
@misc{ding_traced_2023,
	title = {{TRACED}: {Execution}-aware {Pre}-training for {Source} {Code}},
	shorttitle = {{TRACED}},
	url = {http://arxiv.org/abs/2306.07487},
	abstract = {Most existing pre-trained language models for source code focus on learning the static code text, typically augmented with static code structures (abstract syntax tree, dependency graphs, etc.). However, program semantics will not be fully exposed before the real execution. Without an understanding of the program execution, statically pre-trained models fail to comprehensively capture the dynamic code properties, such as the branch coverage and the runtime variable values, and they are consequently less effective at code understanding tasks, such as retrieving semantic clones and detecting software vulnerabilities. To close the gap between the static nature of language models and the dynamic characteristics of programs, we introduce TRACED, an execution-aware pre-training strategy for source code. Specifically, we pre-train code language models with a combination of source code, executable inputs, and corresponding execution traces. Our goal is to teach code models the complicated execution logic during the pre-training, enabling the model to statically estimate the dynamic code properties without repeatedly executing code during task-specific fine-tuning. To illustrate the effectiveness of our proposed approach, we fine-tune and evaluate TRACED on three downstream tasks: static execution estimation, clone retrieval, and vulnerability detection. The empirical results show that TRACED relatively improves the statically pre-trained code models by 12.4\% for complete execution path prediction and by 25.2\% for runtime variable value predictions. TRACED also significantly outperforms statically pre-trained models in clone retrieval and vulnerability detection across four public benchmarks.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Ding, Yangruibo and Steenhoek, Ben and Pei, Kexin and Kaiser, Gail and Le, Wei and Ray, Baishakhi},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07487 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{bouzenia_tracefixer_2023,
	title = {{TraceFixer}: {Execution} {Trace}-{Driven} {Program} {Repair}},
	shorttitle = {{TraceFixer}},
	url = {http://arxiv.org/abs/2304.12743},
	abstract = {When debugging unintended program behavior, developers can often identify the point in the execution where the actual behavior diverges from the desired behavior. For example, a variable may get assigned a wrong value, which then negatively influences the remaining computation. Once a developer identifies such a divergence, how to fix the code so that it provides the desired behavior? This paper presents TraceFixer, a technique for predicting how to edit source code so that it does not diverge from the expected behavior anymore. The key idea is to train a neural program repair model that not only learns from source code edits but also exploits excerpts of runtime traces. The input to the model is a partial execution trace of the incorrect code, which can be obtained automatically through code instrumentation, and the correct state that the program should reach at the divergence point, which the user provides, e.g., in an interactive debugger. Our approach fundamentally differs from current program repair techniques, which share a similar goal but exploit neither execution traces nor information about the desired program state. We evaluate TraceFixer on single-line mistakes in Python code. After training the model on hundreds of thousands of code edits created by a neural model that mimics real-world bugs, we find that exploiting execution traces improves the bug-fixing ability by 13\% to 20\% (depending on the dataset, within the top-10 predictions) compared to a baseline that learns from source code edits only. Applying TraceFixer to 20 real-world Python bugs shows that the approach successfully fixes 10 of them.},
	language = {en},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Bouzenia, Islem and Ding, Yangruibo and Pei, Kexin and Ray, Baishakhi and Pradel, Michael},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12743 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{saieva_contrastive_2023,
	title = {On {Contrastive} {Learning} of {Semantic} {Similarity} {forCode} to {Code} {Search}},
	url = {http://arxiv.org/abs/2305.03843},
	abstract = {This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models (LLMs) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training. We present the first-ever code search method that encodes dynamic runtime information during training without the need to execute either the corpus under search or the search query at inference time and the first code search technique that trains on both positive and negative reference samples. To validate the efficacy of our approach, we perform a set of studies demonstrating the capability of enhanced LLMs to perform cross-language code-to-code search. Our evaluation demonstrates that the effectiveness of our approach is consistent across various model architectures and programming languages. We outperform the state-of-the-art cross-language search tool by up to 44.7{\textbackslash}\%. Moreover, our ablation studies reveal that even a single positive and negative reference sample in the training process results in substantial performance improvements demonstrating both similar and dissimilar references are important parts of code search. Importantly, we show that enhanced well-crafted, fine-tuned models consistently outperform enhanced larger modern LLMs without fine tuning, even when enhancing the largest available LLMs highlighting the importance for open-sourced models. To ensure the reproducibility and extensibility of our research, we present an open-sourced implementation of our tool and training procedures called Cosco.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Saieva, Anthony and Chakraborty, Saikat and Kaiser, Gail},
	month = may,
	year = {2023},
	note = {arXiv:2305.03843 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@misc{saieva_contrastive_2023-1,
	title = {On {Contrastive} {Learning} of {Semantic} {Similarity} {forCode} to {Code} {Search}},
	url = {http://arxiv.org/abs/2305.03843},
	abstract = {This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models (LLMs) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training. We present the first-ever code search method that encodes dynamic runtime information during training without the need to execute either the corpus under search or the search query at inference time and the first code search technique that trains on both positive and negative reference samples. To validate the efficacy of our approach, we perform a set of studies demonstrating the capability of enhanced LLMs to perform cross-language code-to-code search. Our evaluation demonstrates that the effectiveness of our approach is consistent across various model architectures and programming languages. We outperform the state-of-the-art cross-language search tool by up to 44.7{\textbackslash}\%. Moreover, our ablation studies reveal that even a single positive and negative reference sample in the training process results in substantial performance improvements demonstrating both similar and dissimilar references are important parts of code search. Importantly, we show that enhanced well-crafted, fine-tuned models consistently outperform enhanced larger modern LLMs without fine tuning, even when enhancing the largest available LLMs highlighting the importance for open-sourced models. To ensure the reproducibility and extensibility of our research, we present an open-sourced implementation of our tool and training procedures called Cosco.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Saieva, Anthony and Chakraborty, Saikat and Kaiser, Gail},
	month = may,
	year = {2023},
	note = {arXiv:2305.03843 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@misc{Kuckertz2023MetadataBasedEcosystemtoImprovetheFAIRnessofResearchSoftware,
	title = {A {Metadata}-{Based} {Ecosystem} to {Improve} the {FAIRness} of {Research} {Software}},
	url = {http://arxiv.org/abs/2306.10620},
	abstract = {The reuse of research software is central to research efficiency and academic exchange. The application of software enables researchers with varied backgrounds to reproduce, validate, and expand upon study findings. Furthermore, the analysis of open source code aids in the comprehension, comparison, and integration of approaches. Often, however, no further use occurs because relevant software cannot be found or is incompatible with existing research processes. This results in repetitive software development, which impedes the advancement of individual researchers and entire research communities. In this article, the DataDesc ecosystem is presented, an approach to describing data models of software interfaces with detailed and machine-actionable metadata. In addition to a specialized metadata schema, an exchange format and support tools for easy collection and the automated publishing of software documentation are introduced. This approach practically increases the FAIRness, i.e., findability, accessibility, interoperability, and so the reusability of research software, as well as effectively promotes its impact on research.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Kuckertz, Patrick and Göpfert, Jan and Karras, Oliver and Neuroth, David and Schönau, Julian and Pueblas, Rodrigo and Ferenz, Stephan and Engel, Felix and Pflugradt, Noah and Weinand, Jann M. and Nieße, Astrid and Auer, Sören and Stolten, Detlef},
	month = jun,
	year = {2023},
	keywords = {Computer Science - Software Engineering},
}

@article{piorkowski_aimee_2023,
	title = {{AIMEE}: {An} {Exploratory} {Study} of {How} {Rules} {Support} {AI} {Developers} to {Explain} and {Edit} {Models}},
	volume = {7},
	issn = {2573-0142},
	shorttitle = {{AIMEE}},
	url = {https://dl.acm.org/doi/10.1145/3610046},
	doi = {10.1145/3610046},
	abstract = {In real-world applications when deploying Machine Learning (ML) models, initial model development includes close analysis of the model results and behavior by a data scientist. Once trained, however, models may need to be retrained with new data or updated to adhere to new rules or regulations. This presents two challenges. First, how to communicate how a model is making its decisions before and after retraining, and second how to support model editing to take into account new requirements. To address these needs, we built AIMEE (AI Model Explorer and Editor), a tool created to address these challenges by providing interactive methods to explain, visualize, and modify model decision boundaries using rules. Rules should benefit model builders by providing a layer of abstraction for understanding and manipulating the model and reduces the need to modify individual rows of data directly. To evaluate if this was the case, we conducted a pair of user studies totaling 23 participants to evaluate AIMEE's rules-based approach for model explainability and editing. We found that participants correctly interpreted rules and report on their perspectives of how rules are beneficial (and not), ways that rules could support collaboration, and provide a usability evaluation of the tool.},
	language = {en},
	number = {CSCW},
	urldate = {2023-10-25},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Piorkowski, David and Vejsbjerg, Inge and Cornec, Owen and Daly, Elizabeth M. and Alkan, Öznur},
	month = sep,
	year = {2023},
	pages = {1--25},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{arnold_factsheets_2019,
	title = {{FactSheets}: {Increasing} trust in {AI} services through supplier's declarations of conformity},
	volume = {63},
	issn = {0018-8646, 0018-8646},
	shorttitle = {{FactSheets}},
	url = {https://ieeexplore.ieee.org/document/8843893/},
	doi = {10.1147/JRD.2019.2942288},
	language = {en},
	number = {4/5},
	urldate = {2023-10-25},
	journal = {IBM Journal of Research and Development},
	author = {Arnold, M. and Bellamy, R. K. E. and Hind, M. and Houde, S. and Mehta, S. and Mojsilovic, A. and Nair, R. and Ramamurthy, K. Natesan and Olteanu, A. and Piorkowski, D. and Reimer, D. and Richards, J. and Tsay, J. and Varshney, K. R.},
	month = jul,
	year = {2019},
	pages = {6:1--6:13},
}

@article{piorkowski_how_2021,
	title = {How {AI} {Developers} {Overcome} {Communication} {Challenges} in a {Multidisciplinary} {Team}: {A} {Case} {Study}},
	volume = {5},
	issn = {2573-0142},
	shorttitle = {How {AI} {Developers} {Overcome} {Communication} {Challenges} in a {Multidisciplinary} {Team}},
	url = {https://dl.acm.org/doi/10.1145/3449205},
	doi = {10.1145/3449205},
	abstract = {The development of AI applications is a multidisciplinary effort, involving multiple roles collaborating with the AI developers, an umbrella term we use to include data scientists and other AI-adjacent roles on the same team. During these collaborations, there is a knowledge mismatch between AI developers, who are skilled in data science, and external stakeholders who are typically not. This difference leads to communication gaps, and the onus falls on AI developers to explain data science concepts to their collaborators. In this paper, we report on a study including analyses of both interviews with AI developers and artifacts they produced for communication. Using the analytic lens of shared mental models, we report on the types of communication gaps that AI developers face, how AI developers communicate across disciplinary and organizational boundaries, and how they simultaneously manage issues regarding trust and expectations.},
	language = {en},
	number = {CSCW1},
	urldate = {2023-10-25},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Piorkowski, David and Park, Soya and Wang, April Yi and Wang, Dakuo and Muller, Michael and Portnoy, Felix},
	month = apr,
	year = {2021},
	pages = {1--25},
}

@misc{piorkowski_evaluating_2022,
	title = {Evaluating a {Methodology} for {Increasing} {AI} {Transparency}: {A} {Case} {Study}},
	shorttitle = {Evaluating a {Methodology} for {Increasing} {AI} {Transparency}},
	url = {http://arxiv.org/abs/2201.13224},
	abstract = {In reaction to growing concerns about the potential harms of artificial intelligence (AI), societies have begun to demand more transparency about how AI models and systems are created and used. To address these concerns, several efforts have proposed documentation templates containing questions to be answered by model developers. These templates provide a useful starting point, but no single template can cover the needs of diverse documentation consumers. It is possible in principle, however, to create a repeatable methodology to generate truly useful documentation. Richards et al. [25] proposed such a methodology for identifying specific documentation needs and creating templates to address those needs. Although this is a promising proposal, it has not been evaluated. This paper presents the first evaluation of this user-centered methodology in practice, reporting on the experiences of a team in the domain of AI for healthcare that adopted it to increase transparency for several AI models. The methodology was found to be usable by developers not trained in user-centered techniques, guiding them to creating a documentation template that addressed the specific needs of their consumers while still being reusable across different models and use cases. Analysis of the benefits and costs of this methodology are reviewed and suggestions for further improvement in both the methodology and supporting tools are summarized.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Piorkowski, David and Richards, John and Hind, Michael},
	month = jan,
	year = {2022},
	note = {arXiv:2201.13224 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@inproceedings{piorkowski_assessing_2022,
	title = {Assessing {Users}\&\#039; {Ability} {To} {Modify} {And} {Communicate} {Ai} {Models}\&\#039; {Decision} {Boundaries} {Via} {A} {Low}-code, {Rules}-based {Approach}},
	copyright = {© Copyright IBM Corp. 2021},
	url = {https://research.ibm.com/publications/assessing-users-ability-to-modify-and-communicate-ai-models-decision-boundaries-via-a-low-code-rules-based-approach},
	abstract = {Assessing Users' Ability To Modify And Communicate Ai Models' Decision Boundaries Via A Low-code, Rules-based Approach for INFORMS 2022 by David Piorkowski et al.},
	language = {en-US},
	urldate = {2023-10-25},
	author = {Piorkowski, David and Vejsbjerg, Inge and Cornec, Owen and Daly, Elizabeth and Nair, Rahul},
	month = oct,
	year = {2022},
}

@misc{piorkowski_quantitative_2023,
	title = {Quantitative {AI} {Risk} {Assessments}: {Opportunities} and {Challenges}},
	shorttitle = {Quantitative {AI} {Risk} {Assessments}},
	url = {http://arxiv.org/abs/2209.06317},
	abstract = {Although AI-based systems are increasingly being leveraged to provide value to organizations, individuals, and society, significant attendant risks have been identified. These risks have led to proposed regulations, litigation, and general societal concerns. As with any promising technology, organizations want to benefit from the positive capabilities of AI technology while reducing the risks. The best way to reduce risks is to implement comprehensive AI lifecycle governance where policies and procedures are described and enforced during the design, development, deployment, and monitoring of an AI system. While support for comprehensive governance is beginning to emerge, organizations often need to identify the risks of deploying an already-built model without knowledge of how it was constructed or access to its original developers. Such an assessment will quantitatively assess the risks of an existing model in a manner analogous to how a home inspector might assess the energy efficiency of an already-built home or a physician might assess overall patient health based on a battery of tests. This paper explores the concept of a quantitative AI Risk Assessment, exploring the opportunities, challenges, and potential impacts of such an approach, and discussing how it might improve AI regulations.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Piorkowski, David and Hind, Michael and Richards, John},
	month = jan,
	year = {2023},
	note = {arXiv:2209.06317 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{dominique_factsheets_2023,
	address = {Chicago, IL, USA},
	title = {{FactSheets} for {Hardware}-{Aware} {AI} {Models}: {A} {Case} {Study} of {Analog} {In} {Memory} {Computing} {AI} {Models}},
	isbn = {9798350340754},
	shorttitle = {{FactSheets} for {Hardware}-{Aware} {AI} {Models}},
	url = {https://ieeexplore.ieee.org/document/10234354/},
	doi = {10.1109/SSE60056.2023.00029},
	abstract = {In the last few years, documenting and tracking the lineage of AI models has emerged as a important research area that can help to improve the transparency, traceability and overall effectiveness of a model when it is used or deployed by an entity that did not create it. This is a crucial step towards responsible AI in the services computing paradigm especially as AI-enabled software service engineering is becoming more prevalent and mainstream. Multiple documentation methods have been proposed and their adoption has slowly begun, but these methods tend to focus on the data science aspects of the model creation, such as the datasets used to design and train the model, the neural network structure of the model, the F1 score, the modal bias, etc. When adapted to the emerging AI hardware accelerators ﬁeld of analog in-memory computing (IMC), additional documentation requirements need to be considered. Analog IMC accelerators offer increased area and power efﬁciency, which are paramount in IOT and edge resource-constrained environments. We use the AI FactSheets (FS) 360 documentation methodology to understand and evaluate the documentation needs in this emerging domain. To do so, we interviewed 12 participants who represent various roles throughout the lifecycle of designing, training, evaluating, deploying and consuming an analog-aware AI model. From these interviews we capture these roles’ documentation and collaborative needs, develop FactSheets to meet those needs, and evaluate the quality of completed FactSheets. We show that the FactSheets methodology can be applied to Analog AI models to successfully create meaningful documentation that is suitable across multiple roles and a key step towards responsible AI models.},
	language = {en},
	urldate = {2023-10-25},
	booktitle = {2023 {IEEE} {International} {Conference} on {Software} {Services} {Engineering} ({SSE})},
	publisher = {IEEE},
	author = {Dominique, Brandon and Maghraoui, Kaoutar El and Piorkowski, David and Herger, Lorraine},
	month = jul,
	year = {2023},
	pages = {148--158},
}

@misc{wang_understanding_2021,
	title = {Understanding the {Behaviour} of {Contrastive} {Loss}},
	url = {http://arxiv.org/abs/2012.09740},
	abstract = {Unsupervised contrastive learning has achieved outstanding success, while the mechanism of contrastive loss has been less studied. In this paper, we concentrate on the understanding of the behaviours of unsupervised contrastive loss. We will show that the contrastive loss is a hardness-aware loss function, and the temperature \{{\textbackslash}tau\} controls the strength of penalties on hard negative samples. The previous study has shown that uniformity is a key property of contrastive learning. We build relations between the uniformity and the temperature \{{\textbackslash}tau\} . We will show that uniformity helps the contrastive learning to learn separable features, however excessive pursuit to the uniformity makes the contrastive loss not tolerant to semantically similar samples, which may break the underlying semantic structure and be harmful to the formation of features useful for downstream tasks. This is caused by the inherent defect of the instance discrimination objective. Specifically, instance discrimination objective tries to push all different instances apart, ignoring the underlying relations between samples. Pushing semantically consistent samples apart has no positive effect for acquiring a prior informative to general downstream tasks. A well-designed contrastive loss should have some extents of tolerance to the closeness of semantically similar samples. Therefore, we find that the contrastive loss meets a uniformity-tolerance dilemma, and a good choice of temperature can compromise these two properties properly to both learn separable features and tolerant to semantically similar samples, improving the feature qualities and the downstream performances.},
	urldate = {2023-10-23},
	publisher = {arXiv},
	author = {Wang, Feng and Liu, Huaping},
	month = mar,
	year = {2021},
	note = {arXiv:2012.09740 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{andriushchenko_sharpness-aware_2023,
	title = {Sharpness-{Aware} {Minimization} {Leads} to {Low}-{Rank} {Features}},
	url = {http://arxiv.org/abs/2305.16292},
	abstract = {Sharpness-aware minimization (SAM) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is well-known and is the primary motivation, we uncover an additional intriguing effect of SAM: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank reduction mechanism can be more complex, especially for deep networks with pre-activation skip connections and self-attention layers. We make our code available at https://github.com/tml-epfl/sam-low-rank-features.},
	urldate = {2023-10-23},
	publisher = {arXiv},
	author = {Andriushchenko, Maksym and Bahri, Dara and Mobahi, Hossein and Flammarion, Nicolas},
	month = may,
	year = {2023},
	note = {arXiv:2305.16292 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{andriushchenko_sharpness-aware_2023-1,
	title = {Sharpness-{Aware} {Minimization} {Leads} to {Low}-{Rank} {Features}},
	url = {http://arxiv.org/abs/2305.16292},
	abstract = {Sharpness-aware minimization (SAM) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is well-known and is the primary motivation, we uncover an additional intriguing effect of SAM: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank reduction mechanism can be more complex, especially for deep networks with pre-activation skip connections and self-attention layers. We make our code available at https://github.com/tml-epfl/sam-low-rank-features.},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Andriushchenko, Maksym and Bahri, Dara and Mobahi, Hossein and Flammarion, Nicolas},
	month = may,
	year = {2023},
	note = {arXiv:2305.16292 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{andriushchenko_sharpness-aware_2023-2,
	title = {Sharpness-{Aware} {Minimization} {Leads} to {Low}-{Rank} {Features}},
	url = {http://arxiv.org/abs/2305.16292},
	abstract = {Sharpness-aware minimization (SAM) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is well-known and is the primary motivation, we uncover an additional intriguing effect of SAM: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank reduction mechanism can be more complex, especially for deep networks with pre-activation skip connections and self-attention layers. We make our code available at https://github.com/tml-epfl/sam-low-rank-features.},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Andriushchenko, Maksym and Bahri, Dara and Mobahi, Hossein and Flammarion, Nicolas},
	month = may,
	year = {2023},
	note = {arXiv:2305.16292 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{huang_exploiting_2021,
	title = {Exploiting a {Zoo} of {Checkpoints} for {Unseen} {Tasks}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/a1c3ae6c49a89d92aef2d423dadb477f-Abstract.html},
	abstract = {There are so many models in the literature that it is difficult for practitioners to decide which combinations are likely to be effective for a new task. This paper attempts to address this question by capturing relationships among checkpoints published on the web. We model the space of tasks as a Gaussian process. The covariance can be estimated from checkpoints and unlabeled probing data. With the Gaussian process, we can identify representative checkpoints by a maximum mutual information criterion. This objective is submodular. A greedy method identifies representatives that are likely to "cover'' the task space. These representatives generalize to new tasks with superior performance. Empirical evidence is provided for applications from both computational linguistics as well as computer vision.},
	urldate = {2023-10-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Huang, Jiaji and Qiu, Qiang and Church, Kenneth},
	year = {2021},
	pages = {19423--19434},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yu_dag-gnn_2019,
	title = {{DAG}-{GNN}: {DAG} {Structure} {Learning} with {Graph} {Neural} {Networks}},
	shorttitle = {{DAG}-{GNN}},
	url = {http://arxiv.org/abs/1904.10098},
	abstract = {Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at {\textbackslash}url\{https://github.com/fishmoon1234/DAG-GNN\}.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Yu, Yue and Chen, Jie and Gao, Tian and Yu, Mo},
	month = apr,
	year = {2019},
	note = {arXiv:1904.10098 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hu_lora_nodate,
	title = {{LORA}: {LOW}-{RANK} {ADAPTATION} {OF} {LARGE} {LAN}- {GUAGE} {MODELS}},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	author = {Hu, Edward and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
}

@misc{perot_lmdx_2023,
	title = {{LMDX}: {Language} {Model}-based {Document} {Information} {Extraction} and {Localization}},
	shorttitle = {{LMDX}},
	url = {http://arxiv.org/abs/2309.10952},
	abstract = {Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and localizing the entities within the document. In particular, we apply LMDX to the PaLM 2-S LLM and evaluate it on VRDU and CORD benchmarks, setting a new state-of-the-art and showing how LMDX enables the creation of high quality, data-efficient parsers.},
	urldate = {2023-10-14},
	publisher = {arXiv},
	author = {Perot, Vincent and Kang, Kai and Luisier, Florian and Su, Guolong and Sun, Xiaoyu and Boppana, Ramya Sree and Wang, Zilong and Mu, Jiaqi and Zhang, Hao and Hua, Nan},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10952 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chandrasekaran_test_2023,
	title = {Test \& {Evaluation} {Best} {Practices} for {Machine} {Learning}-{Enabled} {Systems}},
	url = {http://arxiv.org/abs/2310.06800},
	abstract = {Machine learning (ML) - based software systems are rapidly gaining adoption across various domains, making it increasingly essential to ensure they perform as intended. This report presents best practices for the Test and Evaluation (T\&E) of ML-enabled software systems across its lifecycle. We categorize the lifecycle of ML-enabled software systems into three stages: component, integration and deployment, and post-deployment. At the component level, the primary objective is to test and evaluate the ML model as a standalone component. Next, in the integration and deployment stage, the goal is to evaluate an integrated ML-enabled system consisting of both ML and non-ML components. Finally, once the ML-enabled software system is deployed and operationalized, the T\&E objective is to ensure the system performs as intended. Maintenance activities for ML-enabled software systems span the lifecycle and involve maintaining various assets of ML-enabled software systems. Given its unique characteristics, the T\&E of ML-enabled software systems is challenging. While significant research has been reported on T\&E at the component level, limited work is reported on T\&E in the remaining two stages. Furthermore, in many cases, there is a lack of systematic T\&E strategies throughout the ML-enabled system's lifecycle. This leads practitioners to resort to ad-hoc T\&E practices, which can undermine user confidence in the reliability of ML-enabled software systems. New systematic testing approaches, adequacy measurements, and metrics are required to address the T\&E challenges across all stages of the ML-enabled system lifecycle.},
	urldate = {2023-10-14},
	publisher = {arXiv},
	author = {Chandrasekaran, Jaganmohan and Cody, Tyler and McCarthy, Nicola and Lanus, Erin and Freeman, Laura},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06800 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{Li2022MetadataRepresentation4QueryableMLModelZoos,
	title = {Metadata {Representations} for {Queryable} {ML} {Model} {Zoos}},
	url = {http://arxiv.org/abs/2207.09315},
	abstract = {Machine learning (ML) practitioners and organizations are building model zoos of pre-trained models, containing metadata describing properties of the ML models and datasets that are useful for reporting, auditing, reproducibility, and interpretability purposes. The metatada is currently not standardised; its expressivity is limited; and there is no interoperable way to store and query it. Consequently, model search, reuse, comparison, and composition are hindered. In this paper, we advocate for standardized ML model meta-data representation and management, proposing a toolkit supported to help practitioners manage and query that metadata.},
	urldate = {2023-09-02},
	publisher = {arXiv},
	author = {Li, Ziyu and Hai, Rihan and Bozzon, Alessandro and Katsifodimos, Asterios},
	month = jul,
	year = {2022},
	keywords = {Computer Science - Databases, Computer Science - Machine Learning},
}

@article{wang_compatibility_2023,
	title = {Compatibility {Issues} in {Deep} {Learning} {Systems}: {Problems} and {Opportunities}},
	abstract = {Deep learning (DL) systems are complex component-based systems, which consist of core program (code implementation and data), Python (language and interpreter), third-party libraries, lowlevel libraries, development tools, OS, and hardware environments. Incompatible interaction between components would cause serious compatibility issues, substantially affecting the development and deployment processes. What types of compatibility issues are frequently exposed in DL systems? What are the root causes of such issues and how do developers fix them? How far are we from automatically detecting and fixing DL compatibility issues? Although there are many existing studies on DL bugs, the characteristics of DL compatibility issues have rarely been systematically studied and the above questions remain largely unexplored. To fill this gap, we conduct the first comprehensive empirical study to characterize compatibility issues in DL systems. Through analyzing 352 DL compatibility issues classified from 3,072 posts on Stack Overflow, we present their types, manifestation stages, and symptoms. We further summarize the root causes and common fixing strategies, and conduct a tool survey on the current research status of automated detection and repair of DL compatibility issues. Our study allows researchers and practitioners to gain a better understanding of DL compatibility issues and can facilitate future tool development.},
	language = {en},
	author = {Wang, Jun and Xiao, Guanping and Zhang, Shuai and Lei, Huashan and Liu, Yepang and Sui, Yulei},
	year = {2023},
}

@misc{wu_survey_2023,
	title = {A {Survey} on {Large} {Language} {Models} for {Recommendation}},
	url = {http://arxiv.org/abs/2305.19860},
	abstract = {Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation, https://github.com/WLiK/LLM4Rec.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Wu, Likang and Zheng, Zhi and Qiu, Zhaopeng and Wang, Hao and Gu, Hongchao and Shen, Tingjia and Qin, Chuan and Zhu, Chen and Zhu, Hengshu and Liu, Qi and Xiong, Hui and Chen, Enhong},
	month = aug,
	year = {2023},
	note = {arXiv:2305.19860 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@misc{wu_survey_2023-1,
	title = {A {Survey} on {Large} {Language} {Models} for {Recommendation}},
	url = {http://arxiv.org/abs/2305.19860},
	abstract = {Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation, https://github.com/WLiK/LLM4Rec.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Wu, Likang and Zheng, Zhi and Qiu, Zhaopeng and Wang, Hao and Gu, Hongchao and Shen, Tingjia and Qin, Chuan and Zhu, Chen and Zhu, Hengshu and Liu, Qi and Xiong, Hui and Chen, Enhong},
	month = aug,
	year = {2023},
	note = {arXiv:2305.19860 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@misc{wu_survey_2023-2,
	title = {A {Survey} on {Large} {Language} {Models} for {Recommendation}},
	url = {http://arxiv.org/abs/2305.19860},
	abstract = {Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation, https://github.com/WLiK/LLM4Rec.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Wu, Likang and Zheng, Zhi and Qiu, Zhaopeng and Wang, Hao and Gu, Hongchao and Shen, Tingjia and Qin, Chuan and Zhu, Chen and Zhu, Hengshu and Liu, Qi and Xiong, Hui and Chen, Enhong},
	month = aug,
	year = {2023},
	note = {arXiv:2305.19860 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@article{huang_cigar_nodate,
	title = {{CIGAR}: {Contrastive} {Learning} for {GitHub} {Action} {Recommendation}},
	abstract = {GitHub Actions was introduced in 2019 as an integrated solution for CI/CD to automate software development workflow. Since then, it has gained tremendous popularity among developers. In a GitHub Actions workflow, actions refer to custom applications for performing complex but frequently repeated tasks. Actions can be typically found in GitHub Marketplace or public GitHub repositories. Prior studies have already disclosed that developers often reuse actions to reduce double work and improve productivity. However, it is not trivial for developers, especially novices, to figure out which action to reuse due to the large number of actions available and the limited search functionality GitHub Marketplace provides. To address this issue, we propose CIGAR (ContrastIve learning for GitHub Action Recommendation). Given the textual description of a task developers want to execute, CIGAR will recommend the most relevant actions. CIGAR exploits a pre-trained RoBERTa model to convert sequences of words into high-dimensional vector representations, and is fine tuned through a contrastive learning objective. The performance of CIGAR was evaluated on a novel dataset curated based on prior research, and the results demonstrate that CIGAR can reliably recommend actions needed by developers and significantly outperforms the GitHub Marketplace search engine. Our study indicates the promise of employing contrastive learning for GitHub action recommendation. The promising performance achieved can potentially drive a wider adoption of GitHub Actions and facilitate the automation of software development workflows.},
	language = {en},
	author = {Huang, Jiangnan and Lin, Bin},
}

@misc{you_graph_2021,
	title = {Graph {Contrastive} {Learning} with {Augmentations}},
	url = {http://arxiv.org/abs/2010.13902},
	abstract = {Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We first design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at https://github.com/Shen-Lab/GraphCL.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {You, Yuning and Chen, Tianlong and Sui, Yongduo and Chen, Ting and Wang, Zhangyang and Shen, Yang},
	month = apr,
	year = {2021},
	note = {arXiv:2010.13902 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@incollection{Maurice2020OSSSupplyChainAttacks,
	address = {Cham},
	title = {Backstabber’s {Knife} {Collection}: {A} {Review} of {Open} {Source} {Software} {Supply} {Chain} {Attacks}},
	volume = {12223},
	isbn = {978-3-030-52682-5 978-3-030-52683-2},
	shorttitle = {Backstabber’s {Knife} {Collection}},
	url = {http://link.springer.com/10.1007/978-3-030-52683-2_2},
	abstract = {A software supply chain attack is characterized by the injection of malicious code into a software package in order to compromise dependent systems further down the chain. Recent years saw a number of supply chain attacks that leverage the increasing use of open source during software development, which is facilitated by dependency managers that automatically resolve, download and install hundreds of open source packages throughout the software life cycle. Even though many approaches for detection and discovery of vulnerable packages exist, no prior work has focused on malicious packages. This paper presents a dataset as well as analysis of 174 malicious software packages that were used in real-world attacks on open source software supply chains and which were distributed via the popular package repositories npm, PyPI, and RubyGems. Those packages, dating from November 2015 to November 2019, were manually collected and analyzed. This work is meant to facilitate the future development of preventive and detective safeguards by open source and research communities.},
	language = {en},
	urldate = {2023-09-14},
	booktitle = {Detection of {Intrusions} and {Malware}, and {Vulnerability} {Assessment}},
	publisher = {Springer International Publishing},
	author = {Ohm, Marc and Plate, Henrik and Sykosch, Arnold and Meier, Michael},
	editor = {Maurice, Clémentine and Bilge, Leyla and Stringhini, Gianluca and Neves, Nuno},
	year = {2020},
	pages = {23--43},
}

@inproceedings{vanderWeide2017Versioning4ETEMLPipelines,
	address = {Chicago IL USA},
	title = {Versioning for {End}-to-{End} {Machine} {Learning} {Pipelines}},
	isbn = {978-1-4503-5026-6},
	url = {https://dl.acm.org/doi/10.1145/3076246.3076248},
	doi = {10.1145/3076246.3076248},
	abstract = {End-to-end machine learning pipelines that run in shared environments are challenging to implement. Production pipelines typically consist of multiple interdependent processing stages. Between stages, the intermediate results are persisted to reduce redundant computation and to improve robustness. Those results might come in the form of datasets for data processing pipelines or in the form of model coefficients in case of model training pipelines. Reusing persisted results improves efficiency but at the same time creates complicated dependencies. Every time one of the processing stages is changed, either due to code change or due to parameters change, it becomes difficult to find which datasets can be reused and which should be recomputed.},
	language = {en},
	urldate = {2022-11-30},
	booktitle = {Proceedings of the 1st {Workshop} on {Data} {Management} for {End}-to-{End} {Machine} {Learning}},
	publisher = {ACM},
	author = {van der Weide, Tom and Papadopoulos, Dimitris and Smirnov, Oleg and Zielinski, Michal and van Kasteren, Tim},
	month = may,
	year = {2017},
	pages = {1--9},
}

@inproceedings{Lakha2023SEPracticesinGeneralSWandMLStartups,
	title = {Analysis of {Software} {Engineering} {Practices} in {General} {Software} and {Machine} {Learning} {Startups}},
	abstract = {Context: On top of the inherent challenges startup software companies face applying proper software engineering practices, the non-deterministic nature of machine learning techniques makes it even more difficult for machine learning (ML) startups. Objective: Therefore, the objective of our study is to understand the whole picture of software engineering practices followed by ML startups and identify additional needs. Method: To achieve our goal, we conducted a systematic literature review study on 37 papers published in the last 21 years. We selected papers on both general software startups and ML startups. We collected data to understand software engineering (SE) practices in five phases of the software development life-cycle: requirement engineering, design, development, quality assurance, and deployment. Results: We find some interesting differences in software engineering practices in ML startups and general software startups. The data management and model learning phases are the most prominent among them. Conclusion: While ML startups face many similar challenges to general software startups, the additional difficulties of using stochastic ML models require different strategies in using software engineering practices to produce high-quality products.},
	urldate = {2023-04-06},
	booktitle = {International {Conference} on {Software} {Engineering} {Research}, {Management} and {Applications}},
	author = {Lakha, Bishal and Bhetwal, Kalyan and Eisty, Nasir U.},
	year = {2023},
	keywords = {Computer Science - Software Engineering},
	pages = {39--46},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9712079},
	urldate = {2023-09-25},
}

@misc{noauthor_ieee_nodate-1,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256174},
	urldate = {2023-09-24},
}

@misc{noauthor_ieee_nodate-2,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256174},
	urldate = {2023-09-24},
}

@misc{noauthor_ieee_nodate-3,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256174},
	urldate = {2023-09-24},
}

@article{Gong2023IntendedUsageCotextofPTM,
	title = {What {Is} the {Intended} {Usage} {Context} of {This} {Model}? {An} {Exploratory} {Study} of {Pre}-{Trained} {Models} on {Various} {Model} {Repositories}},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	shorttitle = {What {Is} the {Intended} {Usage} {Context} of {This} {Model}?},
	url = {https://dl.acm.org/doi/10.1145/3569934},
	doi = {10.1145/3569934},
	abstract = {There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card’s and FactSheet’s guidance for suppliers of pre-trained models on what information they should be published, we propose
              model contracts
              including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task’s type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories.},
	language = {en},
	number = {3},
	urldate = {2023-09-23},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Gong, Lina and Zhang, Jingxuan and Wei, Mingqiang and Zhang, Haoxiang and Huang, Zhiqiu},
	month = jul,
	year = {2023},
	pages = {1--57},
}

@article{Wang2023PreImplementationMethodNamePrediction4OOP,
	title = {Pre-{Implementation} {Method} {Name} {Prediction} for {Object}-{Oriented} {Programming}},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/3597203},
	doi = {10.1145/3597203},
	abstract = {Method naming is a challenging development task in object-oriented programming. In recent years, several research efforts have been undertaken to provide automated tool support for assisting developers in this task. In general, literature approaches assume the availability of method implementation to infer its name. Methods however are usually named before their implementations. In this work, we fill the gap in the literature about method name prediction by developing an approach that predicts the names of all methods to be implemented within a class. Our work considers the class name as the input: the overall intuition is that classes with semantically similar names tend to provide similar functionalities, and hence similar method names. We first conduct a large-scale empirical analysis on 258K+ classes from real-world projects to validate our hypotheses. Then, we propose a hybrid big code-driven approach, Mario, to predict method names based on the class name: we combine a deep learning model with heuristics summarized from code analysis. Extensive experiments on 22K+ classes yielded promising results: compared to the state-of-the-art code2seq model (which leverages method implementation data), our approach achieves comparable results in terms of F-score at token level prediction; our approach, additionally, outperforms code2seq in prediction at the name level. We further show that our approach significantly outperforms several other baselines.},
	urldate = {2023-09-13},
	journal = {ACM Transactions on Software Engineering and Methodology (TOSEM)},
	author = {Wang, Shangwen and Wen, Ming and Lin, Bo and Liu, Yepang and Bissyandé, Tegawendé F. and Mao, Xiaoguang},
	year = {2023},
	keywords = {Method Name Prediction, Naming Convention.},
}

@inproceedings{khosla_supervised_2020,
	title = {Supervised {Contrastive} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations. In reduced data settings, it outperforms cross-entropy significantly. Our loss function is simple to implement and reference TensorFlow code is released at https://t.ly/supcon.},
	urldate = {2023-09-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	year = {2020},
	pages = {18661--18673},
}

@inproceedings{gao_neural_2019,
	title = {A {Neural} {Model} for {Method} {Name} {Generation} from {Functional} {Description}},
	doi = {10.1109/SANER.2019.8667994},
	abstract = {The names of software artifacts, e.g., method names, are important for software understanding and maintenance, as good names can help developers easily understand others’ code. However, the existing naming guidelines are difficult for developers, especially novices, to come up with meaningful, concise and compact names for the variables, methods, classes and files. With the popularity of open source, an enormous amount of project source code can be accessed, and the exhaustiveness and instability of manually naming methods could now be relieved by automatically learning a naming model from a large code repository. Nevertheless, building a comprehensive naming system is still challenging, due to the gap between natural language functional descriptions and method names. Specifically, there are three challenges: how to model the relationship between the functional descriptions and formal method names, how to handle the explosion of vocabulary when dealing with large repositories, and how to leverage the knowledge learned from large repositories to a specific project. To answer these questions, we propose a neural network to directly generate readable method names from natural language description. The proposed method is built upon the encoder-decoder framework with the attention and copying mechanisms. Our experiments show that our method can generate meaningful and accurate method names and achieve significant improvement over the state-of-the-art baseline models. We also address the cold-start problem using a training trick to utilize big data in Github for specific projects.},
	booktitle = {2019 {IEEE} 26th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Gao, Sa and Chen, Chunyang and Xing, Zhenchang and Ma, Yukun and Song, Wen and Lin, Shang-Wei},
	month = feb,
	year = {2019},
	note = {ISSN: 1534-5351},
	keywords = {Computational modeling, Decoding, Encoder-Decoder Model, Naming Convention, Natural languages, Predictive models, Software, Task analysis, Training, Transfer Learning},
	pages = {414--421},
}

@misc{guo_exploring_2023,
	title = {Exploring the {Potential} of {ChatGPT} in {Automated} {Code} {Refinement}: {An} {Empirical} {Study}},
	shorttitle = {Exploring the {Potential} of {ChatGPT} in {Automated} {Code} {Refinement}},
	url = {http://arxiv.org/abs/2309.08221},
	abstract = {Code review is an essential activity for ensuring the quality and maintainability of software projects. However, it is a time-consuming and often error-prone task that can significantly impact the development process. Recently, ChatGPT, a cutting-edge language model, has demonstrated impressive performance in various natural language processing tasks, suggesting its potential to automate code review processes. However, it is still unclear how well ChatGPT performs in code review tasks. To fill this gap, in this paper, we conduct the first empirical study to understand the capabilities of ChatGPT in code review tasks, specifically focusing on automated code refinement based on given code reviews. To conduct the study, we select the existing benchmark CodeReview and construct a new code review dataset with high quality. We use CodeReviewer, a state-of-the-art code review tool, as a baseline for comparison with ChatGPT. Our results show that ChatGPT outperforms CodeReviewer in code refinement tasks. Specifically, our results show that ChatGPT achieves higher EM and BLEU scores of 22.78 and 76.44 respectively, while the state-of-the-art method achieves only 15.50 and 62.88 on a high-quality code review dataset. We further identify the root causes for ChatGPT's underperformance and propose several strategies to mitigate these challenges. Our study provides insights into the potential of ChatGPT in automating the code review process, and highlights the potential research directions.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Guo, Qi and Cao, Junming and Xie, Xiaofei and Liu, Shangqing and Li, Xiaohong and Chen, Bihuan and Peng, Xin},
	month = sep,
	year = {2023},
	note = {arXiv:2309.08221 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{Qi2023ReusingDNNthroughModelReengineering,
	title = {Reusing {Deep} {Neural} {Network} {Models} through {Model} {Re}-engineering},
	url = {http://arxiv.org/abs/2304.00245},
	abstract = {Training deep neural network (DNN) models, which has become an important task in today's software development, is often costly in terms of computational resources and time. With the inspiration of software reuse, building DNN models through reusing existing ones has gained increasing attention recently. Prior approaches to DNN model reuse have two main limitations: 1) reusing the entire model, while only a small part of the model's functionalities (labels) are required, would cause much overhead (e.g., computational and time costs for inference), and 2) model reuse would inherit the defects and weaknesses of the reused model, and hence put the new system under threats of security attack. To solve the above problem, we propose SeaM, a tool that re-engineers a trained DNN model to improve its reusability. Specifically, given a target problem and a trained model, SeaM utilizes a gradient-based search method to search for the model's weights that are relevant to the target problem. The re-engineered model that only retains the relevant weights is then reused to solve the target problem. Evaluation results on widely-used models show that the re-engineered models produced by SeaM only contain 10.11\% weights of the original models, resulting 42.41\% reduction in terms of inference time. For the target problem, the re-engineered models even outperform the original models in classification accuracy by 5.85\%. Moreover, reusing the re-engineered models inherits an average of 57\% fewer defects than reusing the entire model. We believe our approach to reducing reuse overhead and defect inheritance is one important step forward for practical model reuse.},
	urldate = {2023-04-04},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE}'23)},
	author = {Qi, Binhang and Sun, Hailong and Gao, Xiang and Zhang, Hongyu and Li, Zhaotian and Liu, Xudong},
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{binkley_impact_2013,
	title = {The impact of identifier style on effort and comprehension},
	volume = {18},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-012-9201-4},
	doi = {10.1007/s10664-012-9201-4},
	language = {en},
	number = {2},
	urldate = {2023-09-19},
	journal = {Empirical Software Engineering},
	author = {Binkley, Dave and Davis, Marcia and Lawrie, Dawn and Maletic, Jonathan I. and Morrell, Christopher and Sharif, Bonita},
	month = apr,
	year = {2013},
	pages = {219--276},
}

@inproceedings{binkley_camelcase_2009,
	title = {To camelcase or under\_score},
	doi = {10.1109/ICPC.2009.5090039},
	abstract = {Naming conventions are generally adopted in an effort to improve program comprehension. Two of the most popular conventions are alternatives for composing multi-word identifiers: the use of underscores and the use of camel casing. While most programmers have a personal opinion as to which style is better, empirical study forms a more appropriate basis for choosing between them. The central hypothesis considered herein is that identifier style affects the speed and accuracy of manipulating programs. An empirical study of 135 programmers and non-programmers was conducted to better understand the impact of identifier style on code readability. The experiment builds on past work of others who study how readers of natural language perform such tasks. Results indicate that camel casing leads to higher accuracy among all subjects regardless of training, and those trained in camel casing are able to recognize identifiers in the camel case style faster than identifiers in the underscore style.},
	booktitle = {2009 {IEEE} 17th {International} {Conference} on {Program} {Comprehension}},
	author = {Binkley, Dave and Davis, Marcia and Lawrie, Dawn and Morrell, Christopher},
	month = may,
	year = {2009},
	note = {ISSN: 1092-8138},
	keywords = {Computer languages, Educational institutions, Java, Natural languages, Programming profession, Psychology},
	pages = {158--167},
}

@misc{Lester2021PromptTuning,
	title = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
	url = {http://arxiv.org/abs/2104.08691},
	abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	month = sep,
	year = {2021},
	keywords = {Computer Science - Computation and Language},
}

@misc{Wang2022PromptTuning,
	title = {No {More} {Fine}-{Tuning}? {An} {Experimental} {Evaluation} of {Prompt} {Tuning} in {Code} {Intelligence}},
	shorttitle = {No {More} {Fine}-{Tuning}?},
	url = {http://arxiv.org/abs/2207.11680},
	doi = {10.1145/3540250.3549113},
	abstract = {Pre-trained models have been shown effective in many code intelligence tasks. These models are pre-trained on large-scale unlabeled corpus and then fine-tuned in downstream tasks. However, as the inputs to pre-training and downstream tasks are in different forms, it is hard to fully explore the knowledge of pre-trained models. Besides, the performance of fine-tuning strongly relies on the amount of downstream data, while in practice, the scenarios with scarce data are common. Recent studies in the natural language processing (NLP) field show that prompt tuning, a new paradigm for tuning, alleviates the above issues and achieves promising results in various NLP tasks. In prompt tuning, the prompts inserted during tuning provide task-specific knowledge, which is especially beneficial for tasks with relatively scarce data. In this paper, we empirically evaluate the usage and effect of prompt tuning in code intelligence tasks. We conduct prompt tuning on popular pre-trained models CodeBERT and CodeT5 and experiment with three code intelligence tasks including defect prediction, code summarization, and code translation. Our experimental results show that prompt tuning consistently outperforms fine-tuning in all three tasks. In addition, prompt tuning shows great potential in low-resource scenarios, e.g., improving the BLEU scores of fine-tuning by more than 26{\textbackslash}\% on average for code summarization. Our results suggest that instead of fine-tuning, we could adapt prompt tuning for code intelligence tasks to achieve better performance, especially when lacking task-specific data.},
	urldate = {2022-10-06},
	author = {Wang, Chaozheng and Yang, Yuanhang and Gao, Cuiyun and Peng, Yun and Zhang, Hongyu and Lyu, Michael R.},
	month = jul,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{Bhatia2023ChangeTaxonomyforMLPipelines,
	title = {Towards a change taxonomy for machine learning pipelines: {Empirical} study of {ML} pipelines and forks related to academic publications},
	volume = {28},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Towards a change taxonomy for machine learning pipelines},
	url = {https://link.springer.com/10.1007/s10664-022-10282-8},
	doi = {10.1007/s10664-022-10282-8},
	abstract = {Machine Learning (ML) academic publications commonly provide open-source implementations on GitHub, allowing their audience to replicate, validate, or even extend the ML algorithms, data sets and metadata. However, thus far little is known about the degree of collaboration activity happening on such ML research repositories, in particular regarding (1) the degree to which such repositories receive contributions from forks, (2) the nature of such contributions (i.e., the types of changes), and (3) the nature of changes that are not contributed back to forks, which might represent missed opportunities. In this paper, we empirically study contributions to 1,346 ML research repositories and their 67,369 forks, both quantitatively and qualitatively, by building on Hindle et al.’s seminal taxonomy of code changes. We found that while ML research repositories are heavily forked, only 9\% of the forks made modifications to the forked repository. 42\% of the latter sent changes to the parent repositories, half of which (52\%) were accepted by the parent repositories. Our qualitative analysis on 539 contributed and 378 local (fork-only) changes extends Hindle et al.’s taxonomy with two new top-level change categories related to ML (Data and Dependency Management), and 16 new sub-categories, including nine ML-specific ones (input data, parameter tuning, pre-processing, training infrastructure, model structure, pipeline performance, sharing, validation infrastructure, and output data). While the changes that are not contributed back by the forks mostly concern domain-specific features and local experimentation (e.g., parameter tuning), the origin repositories do miss out on a non-trivial 15.4\% of Documentation changes, 13.6\% of Feature changes and 11.4\% of Bug fix changes.},
	language = {en},
	number = {3},
	urldate = {2023-09-18},
	journal = {Empirical Software Engineering},
	author = {Bhatia, Aaditya and Eghan, Ellis E. and Grichi, Manel and Cavanagh, William G. and Jiang, Zhen Ming and Adams, Bram},
	month = may,
	year = {2023},
	pages = {60},
}

@article{morovati_bugs_2023,
	title = {Bugs in machine learning-based systems: a faultload benchmark},
	volume = {28},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Bugs in machine learning-based systems},
	url = {https://link.springer.com/10.1007/s10664-023-10291-1},
	doi = {10.1007/s10664-023-10291-1},
	abstract = {The rapid escalation of applying Machine Learning (ML) in various domains has led to paying more attention to the quality of ML components. There is then a growth of techniques and tools aiming at improving the quality of ML components and integrating them into the ML-based system safely. Although most of these tools use bugs’ lifecycle, there is no standard benchmark of bugs to assess their performance, compare them and discuss their advantages and weaknesses. In this study, we firstly investigate the reproducibility and verifiability of the bugs in ML-based systems and show the most important factors in each one. Then, we explore the challenges of generating a benchmark of bugs in ML-based software systems and provide a bug benchmark namely defect4ML that satisfies all criteria of standard benchmark, i.e. relevance, reproducibility, fairness, verifiability, and usability. This faultload benchmark contains 100 bugs reported by ML developers in GitHub and Stack Overflow, using two of the most popular ML frameworks: TensorFlow and Keras. defect4ML also addresses important challenges in Software Reliability Engineering of MLbased software systems, like: 1) fast changes in frameworks, by providing various bugs for different versions of frameworks, 2) code portability, by delivering similar bugs in different ML frameworks, 3) bug reproducibility, by providing fully reproducible bugs with complete information about required dependencies and data, and 4) lack of detailed information on bugs, by presenting links to the bugs’ origins. defect4ML can be of interest to ML-based systems practitioners and researchers to assess their testing tools and techniques.},
	language = {en},
	number = {3},
	urldate = {2023-09-18},
	journal = {Empirical Software Engineering},
	author = {Morovati, Mohammad Mehdi and Nikanjam, Amin and Khomh, Foutse and Jiang, Zhen Ming},
	month = may,
	year = {2023},
	pages = {62},
}

@article{Lawrie2007Rules4WellFormedIdentifiers,
	title = {An empirical study of rules for well-formed identifiers},
	volume = {19},
	issn = {1532-0618},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.350},
	doi = {10.1002/smr.350},
	abstract = {Readers of programs have two main sources of domain information: identifier names and comments. In order to efficiently maintain source code, it is important that the identifier names (as well as comments) communicate clearly the concepts they represent. Deißenböck and Pizka recently introduced two rules for creating well-formed identifiers: one considers the consistency of identifiers and the other their conciseness. These rules require a mapping from identifiers to the concepts they represent, which may be costly to develop after the initial release of a system. An approach for verifying whether identifiers are well formed without any additional information (e.g., a concept mapping) is developed. Using a pool of 48 million lines of code, experiments with the resulting syntactic rules for well-formed identifiers illustrate that violations of the syntactic pattern exist. Two case studies show that three-quarters of these violations are ‘real’. That is, they could be identified using a concept mapping. Three related studies show that programmers tend to use a rather limited vocabulary, that, contrary to many other aspects of system evolution, maintenance does not introduce additional rule violations, and that open and proprietary sources differ in their percentage of violations. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {4},
	urldate = {2023-09-14},
	journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	author = {Lawrie, Dawn and Feild, Henry and Binkley, David},
	year = {2007},
	keywords = {identifier quality, part of speech, source code analysis},
	pages = {205--229},
}

@article{Lawrie2007IdentifierNamingStudy,
	title = {Effective identifier names for comprehension and memory},
	volume = {3},
	issn = {1614-5046, 1614-5054},
	url = {http://link.springer.com/10.1007/s11334-007-0031-2},
	doi = {10.1007/s11334-007-0031-2},
	language = {en},
	number = {4},
	urldate = {2023-09-14},
	journal = {Innovations in Systems and Software Engineering},
	author = {Lawrie, Dawn and Morrell, Christopher and Feild, Henry and Binkley, David},
	month = nov,
	year = {2007},
	pages = {303--318},
}

@inproceedings{Butler2012MethodNameConsistency,
	title = {Exploring the {Influence} of {Identifier} {Names} on {Code} {Quality}: {An} {Empirical} {Study}},
	shorttitle = {Exploring the {Influence} of {Identifier} {Names} on {Code} {Quality}},
	doi = {10.1109/CSMR.2010.27},
	abstract = {Given the importance of identifier names and the value of naming conventions to program comprehension, we speculated in previous work whether a connection exists between the quality of identifier names and software quality. We found that flawed identifiers in Java classes were associated with source code found to be of low quality by static analysis. This paper extends that work in three directions. First, we show that the association also holds at the finer granularity level of Java methods. This in turn makes it possible to, secondly, apply existing method-level quality and readability metrics, and see that flawed identifiers still impact on this richer notion of code quality and comprehension. Third, we check whether the association can be used in a practical way. We adopt techniques used to evaluate medical diagnostic tests in order to identify which particular identifier naming flaws could be used as a light-weight diagnostic of potentially problematic Java source code for maintenance.},
	booktitle = {2010 14th {European} {Conference} on {Software} {Maintenance} and {Reengineering}},
	author = {Butler, Simon and Wermelinger, Michel and Yu, Yijun and Sharp, Helen},
	month = mar,
	year = {2010},
	keywords = {Complexity theory, Dictionaries, Guidelines, Java, Readability metrics, Sensitivity, programming, software metrics, software quality},
	pages = {156--165},
}

@inproceedings{Alsuhaibani2021NamingMethodsSurvey,
	title = {On the {Naming} of {Methods}: {A} {Survey} of {Professional} {Developers}},
	shorttitle = {On the {Naming} of {Methods}},
	doi = {10.1109/ICSE43902.2021.00061},
	abstract = {This paper describes the results of a large (+1100 responses) survey of professional software developers concerning standards for naming source code methods. The various standards for source code method names are derived from and supported in the software engineering literature. The goal of the survey is to determine if there is a general consensus among developers that the standards are accepted and used in practice. Additionally, the paper examines factors such as years of experience and programming language knowledge in the context of survey responses. The survey results show that participants very much agree about the importance of various standards and how they apply to names and that years of experience and the programming language has almost no effect on their responses. The results imply that the given standards are both valid and to a large degree complete. The work provides a foundation for automated method name assessment during development and code reviews.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Alsuhaibani, Reem and Newman, Christian and Decker, Michael and Collard, Michael and Maletic, Jonathan},
	month = may,
	year = {2021},
	keywords = {Computer languages, Natural language processing, Software, Software development management, Software engineering, Standards, Tools, coding standards, method names, naming conventions, styling},
	pages = {587--599},
}

@inproceedings{butler_mining_2012,
	title = {Mining {Java} class identifier naming conventions},
	doi = {10.1109/ICSE.2012.6227216},
	abstract = {Classes represent key elements of knowledge in object-orientated source code. Class identifier names describe the knowledge recorded in the class and, much of the time, record some detail of the lineage of the class. We investigate the structure of Java class names identifying common patterns of naming and the way components of class identifier names are repeated in inheritance hierarchies. Detailed knowledge of class identifier name structures can be used to improve the accuracy of concept location tools, to support reverse engineering of domain models and requirements traceability, and to support development teams through class identifier naming recommendation systems.},
	booktitle = {2012 34th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Butler, Simon},
	month = jun,
	year = {2012},
	note = {ISSN: 1558-1225},
	keywords = {Accuracy, Educational institutions, Java, Programming, Software maintenance, Speech, identifier names, inheritance, source code},
	pages = {1641--1643},
}

@inproceedings{butler_mining_2011,
	title = {Mining java class naming conventions},
	doi = {10.1109/ICSM.2011.6080776},
	abstract = {Class names represent the concepts implemented in object-oriented source code and are key elements in program comprehension and, thus, software maintenance. Programming conventions often state that class names should be noun-phrases, but there is little further guidance for developers on the composition of class names. Other researchers have observed that the majority of Java class identifier names are composed of one or more nouns preceded, optionally, by one or more adjectives. However, no detailed analysis of class identifier name structure has been undertaken that could be leveraged to support program comprehension activities. We investigate the lexical and syntactic composition of Java class identifier names in two ways. Firstly, as others have done for C function and Java method names, we identify conventional patterns found in the use of parts of speech. Secondly, we identify the origin of words used in class names within the name of any super class and implemented interfaces to identify patterns of class name construction related to inheritance. Through the analysis of 120,000 unique class names found in 60 open source projects we identify both common and project specific class naming conventions. We apply this knowledge in a case study of the mind-mapping tool Freemind to investigate whether class names that follow unconventional naming schemes are candidates for refactoring either a name refactoring that conforms to established naming conventions within the code base, or refactoring of the class that results in conventionally named classes.},
	booktitle = {2011 27th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
	author = {Butler, Simon and Wermelinger, Michel and Yu, Yijun and Sharp, Helen},
	month = sep,
	year = {2011},
	note = {ISSN: 1063-6773},
	keywords = {Accuracy, Java, Java class naming, Libraries, Programming, Software, Speech, Tagging, identifier names},
	pages = {93--102},
}

@inproceedings{Ladisa2023TaxonomyofAttacksonOSSSC,
	title = {{SoK}: {Taxonomy} of {Attacks} on {Open}-{Source} {Software} {Supply} {Chains}},
	shorttitle = {{SoK}},
	doi = {10.1109/SP46215.2023.10179304},
	abstract = {The widespread dependency on open-source software makes it a fruitful target for malicious actors, as demonstrated by recurring attacks. The complexity of today’s open-source supply chains results in a significant attack surface, giving attackers numerous opportunities to reach the goal of injecting malicious code into open-source artifacts that is then downloaded and executed by victims.This work proposes a general taxonomy for attacks on open-source supply chains, independent of specific programming languages or ecosystems, and covering all supply chain stages from code contributions to package distribution. Taking the form of an attack tree, it covers 107 unique vectors, linked to 94 real-world incidents, and mapped to 33 mitigating safeguards.User surveys conducted with 17 domain experts and 134 software developers positively validated the correctness, comprehensiveness and comprehensibility of the taxonomy, as well as its suitability for various use-cases. Survey participants also assessed the utility and costs of the identified safeguards, and whether they are used.},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Ladisa, Piergiorgio and Plate, Henrik and Martinez, Matias and Barais, Olivier},
	month = may,
	year = {2023},
	keywords = {Attack, Costs, Malware, Open Source, Security, Software Supply Chain, Supply chains, Surveys, Taxonomy, Visualization},
	pages = {1509--1526},
}

@article{chen_sethesaurus_2021,
	title = {{SEthesaurus}: {WordNet} in {Software} {Engineering}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {{SEthesaurus}},
	doi = {10.1109/TSE.2019.2940439},
	abstract = {Informal discussions on social platforms (e.g., Stack Overflow, CodeProject) have accumulated a large body of programming knowledge in the form of natural language text. Natural language process (NLP) techniques can be utilized to harvest this knowledge base for software engineering tasks. However, consistent vocabulary for a concept is essential to make an effective use of these NLP techniques. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms (such as abbreviations, synonyms and misspellings) in informal discussions. Existing techniques to deal with such morphological forms are either designed for general English or mainly resort to domain-specific lexical rules. A thesaurus, which contains software-specific terms and commonly-used morphological forms, is desirable to perform normalization for software engineering text. However, constructing this thesaurus in a manual way is a challenge task. In this paper, we propose an automatic unsupervised approach to build such a thesaurus. In particular, we first identify software-specific terms by utilizing a software-specific corpus (e.g., Stack Overflow) and a general corpus (e.g., Wikipedia). Then we infer morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations. Finally, we perform graph analysis on morphological relations. We evaluate the coverage and accuracy of our constructed thesaurus against community-cumulated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our constructed thesaurus by developing three applications and also verify the generality of our approach in constructing thesauruses from data sources in other domains.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Chen, Xiang and Chen, Chunyang and Zhang, Dun and Xing, Zhenchang},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Electronic publishing, Encyclopedias, Internet, Natural language processing, Software engineering, Software-specific thesaurus, Thesauri, morphological form, natural language processing, word embedding},
	pages = {1960--1979},
}

@inproceedings{butler_mining_2011-1,
	title = {Mining java class naming conventions},
	doi = {10.1109/ICSM.2011.6080776},
	abstract = {Class names represent the concepts implemented in object-oriented source code and are key elements in program comprehension and, thus, software maintenance. Programming conventions often state that class names should be noun-phrases, but there is little further guidance for developers on the composition of class names. Other researchers have observed that the majority of Java class identifier names are composed of one or more nouns preceded, optionally, by one or more adjectives. However, no detailed analysis of class identifier name structure has been undertaken that could be leveraged to support program comprehension activities. We investigate the lexical and syntactic composition of Java class identifier names in two ways. Firstly, as others have done for C function and Java method names, we identify conventional patterns found in the use of parts of speech. Secondly, we identify the origin of words used in class names within the name of any super class and implemented interfaces to identify patterns of class name construction related to inheritance. Through the analysis of 120,000 unique class names found in 60 open source projects we identify both common and project specific class naming conventions. We apply this knowledge in a case study of the mind-mapping tool Freemind to investigate whether class names that follow unconventional naming schemes are candidates for refactoring either a name refactoring that conforms to established naming conventions within the code base, or refactoring of the class that results in conventionally named classes.},
	booktitle = {2011 27th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
	author = {Butler, Simon and Wermelinger, Michel and Yu, Yijun and Sharp, Helen},
	month = sep,
	year = {2011},
	note = {ISSN: 1063-6773},
	keywords = {Accuracy, Java, Java class naming, Libraries, Programming, Software, Speech, Tagging, identifier names},
	pages = {93--102},
}

@inproceedings{allamanis_suggesting_2015,
	address = {Bergamo Italy},
	title = {Suggesting accurate method and class names},
	isbn = {978-1-4503-3675-8},
	url = {https://dl.acm.org/doi/10.1145/2786805.2786849},
	doi = {10.1145/2786805.2786849},
	language = {en},
	urldate = {2023-09-12},
	booktitle = {Proceedings of the 2015 10th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Bird, Christian and Sutton, Charles},
	month = aug,
	year = {2015},
	pages = {38--49},
}

@article{li_survey_2021,
	title = {A {Survey} on {Renamings} of {Software} {Entities}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3379443},
	doi = {10.1145/3379443},
	abstract = {More than 70\% of characters in the source code are used to label identifiers. Consequently, identifiers are one of the most important source for program comprehension. Meaningful identifiers are crucial to understand and maintain programs. However, for reasons like constrained schedule, inexperience, and unplanned evolution, identifiers may fail to convey the semantics of the entities associated with them. As a result, such entities should be renamed to improve software quality. However, manual renaming and recommendation are fastidious, time consuming, and error prone, whereas automating the process of renamings is challenging: (1) It involves complex natural language processing to understand the meaning of identifers; (2) It also involves difficult semantic analysis to determine the role of software entities. Researchers proposed a number of approaches and tools to facilitate renamings. We present a survey on existing approaches and classify them into identification of renaming opportunities, execution of renamings, and detection of renamings. We find that there is an imbalance between the three type of approaches, and most of implementation of approaches and evaluation dataset are not publicly available. We also discuss the challenges and present potential research directions. To the best of our knowledge, this survey is the first comprehensive study on renamings of software entities.},
	language = {en},
	number = {2},
	urldate = {2023-09-12},
	journal = {ACM Computing Surveys},
	author = {Li, Guangjie and Liu, Hui and Nyamawe, Ally S.},
	month = mar,
	year = {2021},
	pages = {1--38},
}

@inproceedings{chen_unsupervised_2017,
	title = {Unsupervised {Software}-{Specific} {Morphological} {Forms} {Inference} from {Informal} {Discussions}},
	doi = {10.1109/ICSE.2017.48},
	abstract = {Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build such a thesaurus. Our approach identifies software-specific terms by contrasting software-specific and general corpuses, and infers morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations, and graph analysis of morphological relations. We evaluate the coverage and accuracy of the resulting thesaurus against community-curated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject.},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Chen, Chunyang and Xing, Zhenchang and Wang, Ximing},
	month = may,
	year = {2017},
	note = {ISSN: 1558-1225},
	keywords = {Dictionaries, Electronic publishing, Encyclopedias, Internet, Software engineering, Stack Overflow, Thesauri, abbreviation, morphological form, synonym, word embedding},
	pages = {450--461},
}

@article{chen_sethesaurus_2021-1,
	title = {{SEthesaurus}: {WordNet} in {Software} {Engineering}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {{SEthesaurus}},
	doi = {10.1109/TSE.2019.2940439},
	abstract = {Informal discussions on social platforms (e.g., Stack Overflow, CodeProject) have accumulated a large body of programming knowledge in the form of natural language text. Natural language process (NLP) techniques can be utilized to harvest this knowledge base for software engineering tasks. However, consistent vocabulary for a concept is essential to make an effective use of these NLP techniques. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms (such as abbreviations, synonyms and misspellings) in informal discussions. Existing techniques to deal with such morphological forms are either designed for general English or mainly resort to domain-specific lexical rules. A thesaurus, which contains software-specific terms and commonly-used morphological forms, is desirable to perform normalization for software engineering text. However, constructing this thesaurus in a manual way is a challenge task. In this paper, we propose an automatic unsupervised approach to build such a thesaurus. In particular, we first identify software-specific terms by utilizing a software-specific corpus (e.g., Stack Overflow) and a general corpus (e.g., Wikipedia). Then we infer morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations. Finally, we perform graph analysis on morphological relations. We evaluate the coverage and accuracy of our constructed thesaurus against community-cumulated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our constructed thesaurus by developing three applications and also verify the generality of our approach in constructing thesauruses from data sources in other domains.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Chen, Xiang and Chen, Chunyang and Zhang, Dun and Xing, Zhenchang},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Electronic publishing, Encyclopedias, Internet, Natural language processing, Software engineering, Software-specific thesaurus, Thesauri, morphological form, natural language processing, word embedding},
	pages = {1960--1979},
}

@misc{wong_mlguard_2023,
	title = {{MLGuard}: {Defend} {Your} {Machine} {Learning} {Model}!},
	shorttitle = {{MLGuard}},
	url = {http://arxiv.org/abs/2309.01379},
	abstract = {Machine Learning (ML) is used in critical highly regulated and high-stakes fields such as finance, medicine, and transportation. The correctness of these ML applications is important for human safety and economic benefit. Progress has been made on improving ML testing and monitoring of ML. However, these approaches do not provide i) pre/post conditions to handle uncertainty, ii) defining corrective actions based on probabilistic outcomes, or iii) continual verification during system operation. In this paper, we propose MLGuard, a new approach to specify contracts for ML applications. Our approach consists of a) an ML contract specification defining pre/post conditions, invariants, and altering behaviours, b) generated validation models to determine the probability of contract violation, and c) an ML wrapper generator to enforce the contract and respond to violations. Our work is intended to provide the overarching framework required for building ML applications and monitoring their safety.},
	urldate = {2023-09-06},
	publisher = {arXiv},
	author = {Wong, Sheng and Barnett, Scott and Rivera-Villicana, Jessica and Simmons, Anj and Abdelkader, Hala and Schneider, Jean-Guy and Vasa, Rajesh},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01379 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{wong_mlguard_2023-1,
	title = {{MLGuard}: {Defend} {Your} {Machine} {Learning} {Model}!},
	shorttitle = {{MLGuard}},
	url = {http://arxiv.org/abs/2309.01379},
	abstract = {Machine Learning (ML) is used in critical highly regulated and high-stakes fields such as finance, medicine, and transportation. The correctness of these ML applications is important for human safety and economic benefit. Progress has been made on improving ML testing and monitoring of ML. However, these approaches do not provide i) pre/post conditions to handle uncertainty, ii) defining corrective actions based on probabilistic outcomes, or iii) continual verification during system operation. In this paper, we propose MLGuard, a new approach to specify contracts for ML applications. Our approach consists of a) an ML contract specification defining pre/post conditions, invariants, and altering behaviours, b) generated validation models to determine the probability of contract violation, and c) an ML wrapper generator to enforce the contract and respond to violations. Our work is intended to provide the overarching framework required for building ML applications and monitoring their safety.},
	urldate = {2023-09-06},
	publisher = {arXiv},
	author = {Wong, Sheng and Barnett, Scott and Rivera-Villicana, Jessica and Simmons, Anj and Abdelkader, Hala and Schneider, Jean-Guy and Vasa, Rajesh},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01379 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{geisler_transformers_2023,
	title = {Transformers {Meet} {Directed} {Graphs}},
	url = {http://arxiv.org/abs/2302.00049},
	abstract = {Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and undirected graphs. However, transformers for directed graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains, including source code and logic circuits. In this work, we propose two direction- and structure-aware positional encodings for directed graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding. Together with a data-flow-centric graph construction, our model outperforms the prior state of the art on the Open Graph Benchmark Code2 relatively by 14.7\%.},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Geisler, Simon and Li, Yujia and Mankowitz, Daniel and Cemgil, Ali Taylan and Günnemann, Stephan and Paduraru, Cosmin},
	month = aug,
	year = {2023},
	note = {arXiv:2302.00049 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{wang_automl_2023,
	address = {Melbourne, Australia},
	title = {{AutoML} from {Software} {Engineering} {Perspective}: {Landscapes} and {Challenges}},
	isbn = {9798350311846},
	shorttitle = {{AutoML} from {Software} {Engineering} {Perspective}},
	url = {https://ieeexplore.ieee.org/document/10173951/},
	doi = {10.1109/MSR59073.2023.00019},
	abstract = {Machine learning (ML) has been widely adopted in modern software, but the manual configuration of ML (e.g., hyper-parameter configuration) poses a significant challenge to software developers. Therefore, automated ML (AutoML), which seeks the optimal configuration of ML automatically, has received increasing attention from the software engineering community. However, to date, there is no comprehensive understanding of how AutoML is used by developers and what challenges developers encounter in using AutoML for software development. To fill this knowledge gap, we conduct the first study on understanding the use and challenges of AutoML from software developers’ perspective. We collect and analyze 1,554 AutoML downstream repositories, 769 AutoML-related Stack Overflow questions, and 1,437 relevant GitHub issues. The results suggest the increasing popularity of AutoML in a wide range of topics, but also the lack of relevant expertise. We manually identify specific challenges faced by developers for AutoML-enabled software. Based on the results, we derive a series of implications for AutoML framework selection, framework development, and research.},
	language = {en},
	urldate = {2023-09-02},
	booktitle = {2023 {IEEE}/{ACM} 20th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Wang, Chao and Chen, Zhenpeng and Zhou, Minghui},
	month = may,
	year = {2023},
	pages = {39--51},
}

@incollection{selvan_carbon_2022,
	title = {Carbon {Footprint} of {Selecting} and {Training} {Deep} {Learning} {Models} for {Medical} {Image} {Analysis}},
	volume = {13435},
	url = {http://arxiv.org/abs/2203.02202},
	abstract = {The increasing energy consumption and carbon footprint of deep learning (DL) due to growing compute requirements has become a cause of concern. In this work, we focus on the carbon footprint of developing DL models for medical image analysis (MIA), where volumetric images of high spatial resolution are handled. In this study, we present and compare the features of four tools from literature to quantify the carbon footprint of DL. Using one of these tools we estimate the carbon footprint of medical image segmentation pipelines. We choose nnU-net as the proxy for a medical image segmentation pipeline and experiment on three common datasets. With our work we hope to inform on the increasing energy costs incurred by MIA. We discuss simple strategies to cut-down the environmental impact that can make model selection and training processes more efficient.},
	urldate = {2023-08-30},
	author = {Selvan, Raghavendra and Bhagwat, Nikhil and Anthony, Lasse F. Wolff and Kanding, Benjamin and Dam, Erik B.},
	year = {2022},
	doi = {10.1007/978-3-031-16443-9_49},
	note = {arXiv:2203.02202 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {506--516},
}

@inproceedings{Nguyen2023SWNER,
	title = {Software {Entity} {Recognition} with {Noise}-{Robust} {Learning}},
	url = {http://arxiv.org/abs/2308.10564},
	abstract = {Recognizing software entities such as library names from free-form text is essential to enable many software engineering (SE) technologies, such as traceability link recovery, automated documentation, and API recommendation. While many approaches have been proposed to address this problem, they suffer from small entity vocabularies or noisy training data, hindering their ability to recognize software entities mentioned in sophisticated narratives. To address this challenge, we leverage the Wikipedia taxonomy to develop a comprehensive entity lexicon with 79K unique software entities in 12 fine-grained types, as well as a large labeled dataset of over 1.7M sentences. Then, we propose self-regularization, a noise-robust learning approach, to the training of our software entity recognition (SER) model by accounting for many dropouts. Results show that models trained with self-regularization outperform both their vanilla counterparts and state-of-the-art approaches on our Wikipedia benchmark and two Stack Overflow benchmarks. We release our models, data, and code for future research.},
	urldate = {2023-08-22},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} ({ASE}'23)},
	publisher = {arXiv},
	author = {Nguyen, Tai and Di, Yifeng and Lee, Joohan and Chen, Muhao and Zhang, Tianyi},
	month = aug,
	year = {2023},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{tabassum_code_2020,
	title = {Code and {Named} {Entity} {Recognition} in {StackOverflow}},
	url = {http://arxiv.org/abs/2005.01634},
	abstract = {There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users. Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences. In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types. We trained in-domain BERT representations (BERTOverflow) on 152 million sentences from StackOverflow, which lead to an absolute increase of +10 F-1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F\$\_1\$ score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERT-based tagging model. Our code and data are available at: https://github.com/jeniyat/StackOverflowNER/},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Tabassum, Jeniya and Maddela, Mounica and Xu, Wei and Ritter, Alan},
	month = nov,
	year = {2020},
	note = {arXiv:2005.01634 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{lian_towards_2020,
	title = {{TOWARDS} {FAST} {ADAPTATION} {OF} {NEURAL} {ARCHITEC}- {TURES} {WITH} {META} {LEARNING}},
	abstract = {Recently, Neural Architecture Search (NAS) has been successfully applied to multiple artiﬁcial intelligence areas and shows better performance compared with hand-designed networks. However, the existing NAS methods only target a speciﬁc task. Most of them usually do well in searching an architecture for single task but are troublesome for multiple datasets or multiple tasks. Generally, the architecture for a new task is either searched from scratch, which is neither efﬁcient nor ﬂexible enough for practical application scenarios, or borrowed from the ones searched on other tasks, which might be not optimal. In order to tackle the transferability of NAS and conduct fast adaptation of neural architectures, we propose a novel Transferable Neural Architecture Search method based on meta-learning in this paper, which is termed as T-NAS. T-NAS learns a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which makes the transferred architecture suitable for the speciﬁc task. Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost, which demonstrates the effectiveness of our method.},
	language = {en},
	author = {Lian, Dongze and Zheng, Yin and Xu, Yintao and Lu, Yanxiong and Lin, Leyu and Zhao, Peilin and Huang, Junzhou and Gao, Shenghua},
	year = {2020},
}

@article{roziere_code_nodate,
	title = {Code {Llama}: {Open} {Foundation} {Models} for {Code}},
	language = {en},
	author = {Rozière, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Ellen, Xiaoqing and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, Jérémy and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and Défossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
}

@inproceedings{zhu_benchmarking_2018,
	address = {Raleigh, NC},
	title = {Benchmarking and {Analyzing} {Deep} {Neural} {Network} {Training}},
	isbn = {978-1-5386-6780-4},
	url = {https://ieeexplore.ieee.org/document/8573476/},
	doi = {10.1109/IISWC.2018.8573476},
	abstract = {The recent popularity of deep neural networks (DNNs) has generated considerable research interest in performing DNN-related computation efﬁciently. However, the primary focus is usually very narrow and limited to (i) inference – i.e. how to efﬁciently execute already trained models and (ii) image classiﬁcation networks as the primary benchmark for evaluation. Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark suite for DNN training, called TBD1, which comprises a representative set of eight DNN models and covers six major machine learning applications: image classiﬁcation, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) performing an extensive performance analysis of these models on three major deep learning frameworks (TensorFlow, MXNet, CNTK) across different hardware conﬁgurations (single-GPU, multi-GPU, and multi-machine). We present a new toolchain for performance analysis for these models that combines the targeted usage of existing performance analysis tools, careful selection of performance metrics, and methodologies to analyze the results. We also build a new set of tools for memory proﬁling in three major frameworks. These tools can shed light on precisely how much memory is consumed by different data structures (weights, activations, gradients, workspace) in DNN training. Using our tools and methodologies, we make several important observations and recommendations on where future DNN training research and optimization should be focused.},
	language = {en},
	urldate = {2023-08-24},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	publisher = {IEEE},
	author = {Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
	month = sep,
	year = {2018},
	pages = {88--100},
}

@misc{hou_large_2023,
	title = {Large {Language} {Models} for {Software} {Engineering}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Large {Language} {Models} for {Software} {Engineering}},
	url = {http://arxiv.org/abs/2308.10620},
	abstract = {Large Language Models (LLMs) have significantly impacted numerous domains, notably including Software Engineering (SE). Nevertheless, a well-rounded understanding of the application, effects, and possible limitations of LLMs within SE is still in its early stages. To bridge this gap, our systematic literature review takes a deep dive into the intersection of LLMs and SE, with a particular focus on understanding how LLMs can be exploited in SE to optimize processes and outcomes. Through a comprehensive review approach, we collect and analyze a total of 229 research papers from 2017 to 2023 to answer four key research questions (RQs). In RQ1, we categorize and provide a comparative analysis of different LLMs that have been employed in SE tasks, laying out their distinctive features and uses. For RQ2, we detail the methods involved in data collection, preprocessing, and application in this realm, shedding light on the critical role of robust, well-curated datasets for successful LLM implementation. RQ3 allows us to examine the specific SE tasks where LLMs have shown remarkable success, illuminating their practical contributions to the field. Finally, RQ4 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE, as well as the common techniques related to prompt optimization. Armed with insights drawn from addressing the aforementioned RQs, we sketch a picture of the current state-of-the-art, pinpointing trends, identifying gaps in existing research, and flagging promising areas for future study.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
	month = aug,
	year = {2023},
	note = {arXiv:2308.10620 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@misc{elharrouss_backbones-review_2022,
	title = {Backbones-{Review}: {Feature} {Extraction} {Networks} for {Deep} {Learning} and {Deep} {Reinforcement} {Learning} {Approaches}},
	shorttitle = {Backbones-{Review}},
	url = {http://arxiv.org/abs/2206.08016},
	doi = {10.48550/arXiv.2206.08016},
	abstract = {To understand the real world using various types of data, Artificial Intelligence (AI) is the most used technique nowadays. While finding the pattern within the analyzed data represents the main task. This is performed by extracting representative features step, which is proceeded using the statistical algorithms or using some specific filters. However, the selection of useful features from large-scale data represented a crucial challenge. Now, with the development of convolution neural networks (CNNs), the feature extraction operation has become more automatic and easier. CNNs allow to work on large-scale size of data, as well as cover different scenarios for a specific task. For computer vision tasks, convolutional networks are used to extract features also for the other parts of a deep learning model. The selection of a suitable network for feature extraction or the other parts of a DL model is not random work. So, the implementation of such a model can be related to the target task as well as the computational complexity of it. Many networks have been proposed and become the famous networks used for any DL models in any AI task. These networks are exploited for feature extraction or at the beginning of any DL model which is named backbones. A backbone is a known network trained in many other tasks before and demonstrates its effectiveness. In this paper, an overview of the existing backbones, e.g. VGGs, ResNets, DenseNet, etc, is given with a detailed description. Also, a couple of computer vision tasks are discussed by providing a review of each task regarding the backbones used. In addition, a comparison in terms of performance is also provided, based on the backbone used for each task.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Elharrouss, Omar and Akbari, Younes and Almaadeed, Noor and Al-Maadeed, Somaya},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08016 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_learning_nodate,
	title = {Learning {Contrastive} {Embedding} in {Low}-{Dimensional} {Space}},
	abstract = {Contrastive learning (CL) pretrains feature embeddings to scatter instances in the feature space so that the training data can be well discriminated. Most existing CL techniques usually encourage learning such feature embeddings in the highdimensional space to maximize the instance discrimination. However, this practice may lead to undesired results where the scattering instances are sparsely distributed in the high-dimensional feature space, making it difficult to capture the underlying similarity between pairwise instances. To this end, we propose a novel framework called contrastive learning with low-dimensional reconstruction (CLLR), which adopts a regularized projection layer to reduce the dimensionality of the feature embedding. In CLLR, we build the sparse / low-rank regularizer to adaptively reconstruct a low-dimensional projection space while preserving the basic objective for instance discrimination, and thus successfully learning contrastive embeddings that alleviate the above issue. Theoretically, we prove a tighter error bound for CLLR; empirically, the superiority of CLLR is demonstrated across multiple domains. Both theoretical and experimental results emphasize the significance of learning low-dimensional contrastive embeddings.},
	language = {en},
	author = {Chen, Shuo and Gong, Chen and Li, Jun and Yang, Jian and Niu, Gang and Sugiyama, Masashi},
}

@article{You2021RankingandTuningPTMs,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} of {Exploiting} {Model} {Hubs}},
	volume = {23},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Pre-trained model hubs with many pre-trained models (PTMs) have been a cornerstone in deep learning. Although built at a high cost, they are in fact {\textbackslash}emph\{under-exploited\}: practitioners usually pick one PTM from the provided model hub by popularity, and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ve but common practice poses two obstacles to sufficiently exploiting pre-trained model hubs: (1) the PTM selection procedure has no optimality guarantee; (2) only one PTM is used while the rest PTMs are overlooked. Ideally, to maximally exploit pre-trained model hubs, trying all combinations of PTMs and extensively fine-tuning each combination of PTMs are required, which incurs exponential combinations and unaffordable computational budget. In this paper, we propose a new paradigm of exploiting model hubs by ranking and tuning pre-trained models: (1) Our conference work{\textasciitilde}{\textbackslash}citep\{you\_logme:\_2021\} proposed LogME to estimate the maximum value of label evidence given features extracted by pre-trained models, which can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) the best ranked PTM can be fine-tuned and deployed if we have no preference for the model's architecture, or the target PTM can be tuned by top-K ranked PTMs via the proposed B-Tuning algorithm. The ranking part is based on the conference paper, and we complete its theoretical analysis (convergence proof of the heuristic evidence maximization procedure, and the influence of feature dimension) in this paper. The tuning part introduces a novel Bayesian Tuning (B-Tuning) method for multiple PTMs tuning, which surpasses dedicated methods designed for homogeneous PTMs tuning and sets up new state of the art for heterogeneous PTMs tuning. We believe the new paradigm of exploiting PTM hubs can interest a large audience of the community.},
	number = {1},
	urldate = {2022-02-10},
	journal = {The Journal of Machine Learning Research (JMLR)},
	author = {You, Kaichao and Liu, Yong and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Machine Learning},
	pages = {9400--9446},
}

@misc{li_prototypical_2021,
	title = {Prototypical {Contrastive} {Learning} of {Unsupervised} {Representations}},
	url = {http://arxiv.org/abs/2005.04966},
	abstract = {This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the fundamental limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.},
	urldate = {2023-08-18},
	publisher = {arXiv},
	author = {Li, Junnan and Zhou, Pan and Xiong, Caiming and Hoi, Steven C. H.},
	month = mar,
	year = {2021},
	note = {arXiv:2005.04966 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{jia_zest_2022,
	title = {A {ZEST} {OF} {LIME}: {TOWARDS} {ARCHITECTURE}-{INDEPENDENT} {MODEL} {DISTANCES}},
	language = {en},
	author = {Jia, Hengrui and Chen, Hongyu and Guan, Jonas and Shamsabadi, Ali Shahin and Papernot, Nicolas},
	year = {2022},
}

@inproceedings{yin_team_2020,
	address = {Virtual Event Australia},
	title = {Team discussions and dynamics during {DevOps} tool adoptions in {OSS} projects},
	isbn = {978-1-4503-6768-4},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416640},
	doi = {10.1145/3324884.3416640},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Yin, Likang and Filkov, Vladimir},
	month = dec,
	year = {2020},
	pages = {697--708},
}

@article{kathikar_assessing_nodate,
	title = {Assessing the {Vulnerabilities} of the {Open}-{Source} {Artificial} {Intelligence} ({AI}) {Landscape}: {A} {Large}-{Scale} {Analysis} of the {Hugging} {Face} {Platform}},
	abstract = {Artificial Intelligence (AI) has rapidly proliferated as a critical disruptive technology in the 21st century. Hugging Face hosts pre-trained models, facilitating the sharing and use of open-source code. Hugging Face has been used by 22,000+ organizations, including Intel and Microsoft, with 2.6+ billion model downloads. While Hugging Face democratizes access to AI models, these models may contain unknown security vulnerabilities. In this research, we automatically collect models from Hugging Face, link them to their underlying code bases on GitHub, and perform a large-scale vulnerability assessment of these repositories. Through our approaches, we collected about 110,000 models from Hugging Face and over 29,000 GitHub repositories. Our vulnerability assessment revealed a larger percentage (35.98\%) of high-severity vulnerabilities compared to low-severity vulnerabilities (6.79\%). This trend in severity levels contradicts the results of severities detected in repositories forked from root repositories and searched repositories. Given that many of the vulnerabilities reside in fundamental AI repositories such as Transformers, the results of this vulnerability assessment have significant implications for supply chain software security and AI risk management more broadly.},
	language = {en},
	author = {Kathikar, Adhishree and Nair, Aishwarya and Lazarine, Ben},
}

@article{Liu2020ReplicabilityandReproducibilityofDLinSE,
	title = {On the {Replicability} and {Reproducibility} of {Deep} {Learning} in {Software} {Engineering}},
	volume = {31},
	abstract = {Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain knowledge. Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) replicability - whether the reported experimental result can be approximately reproduced in high probability with the same DL model and the same data; and (2) reproducibility - whether one reported experimental findings can be reproduced by new experiments with the same experimental protocol and DL model, but different sampled real-world data. Unlike traditional machine learning (ML) models, DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming optimization process. In this study, we conducted a literature review on 93 DL studies recently published in twenty SE journals or conferences. Our statistics show the urgency of investigating these two factors in SE. Moreover, we re-ran four representative DL models in SE. Experimental results show the importance of replicability and reproducibility, where the reported performance of a DL model could not be replicated for an unstable optimization process. Reproducibility could be substantially compromised if the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data. It is therefore urgent for the SE community to provide a long-lasting link to a replication package, enhance DL-based solution stability and convergence, and avoid performance sensitivity on different sampled data.},
	number = {1},
	urldate = {2021-09-29},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Liu, Chao and Gao, Cuiyun and Xia, Xin and Lo, David and Grundy, John and Yang, Xiaohu},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	pages = {1--46},
}

@misc{yang_survey_2020,
	title = {A {Survey} on {Deep} {Learning} for {Software} {Engineering}},
	url = {http://arxiv.org/abs/2011.14597},
	abstract = {In 2006, Geoffrey Hinton proposed the concept of training ''Deep Neural Networks (DNNs)'' and an improved model training method to break the bottleneck of neural network development. More recently, the introduction of AlphaGo in 2016 demonstrated the powerful learning ability of deep learning and its enormous potential. Deep learning has been increasingly used to develop state-of-the-art software engineering (SE) research tools due to its ability to boost performance for various SE tasks. There are many factors, e.g., deep learning model selection, internal structure differences, and model optimization techniques, that may have an impact on the performance of DNNs applied in SE. Few works to date focus on summarizing, classifying, and analyzing the application of deep learning techniques in SE. To fill this gap, we performed a survey to analyse the relevant studies published since 2006. We first provide an example to illustrate how deep learning techniques are used in SE. We then summarize and classify different deep learning techniques used in SE. We analyzed key optimization technologies used in these deep learning models, and finally describe a range of key research topics using DNNs in SE. Based on our findings, we present a set of current challenges remaining to be investigated and outline a proposed research road map highlighting key opportunities for future work.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Yang, Yanming and Xia, Xin and Lo, David and Grundy, John},
	month = nov,
	year = {2020},
	note = {arXiv:2011.14597 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{naumcheva_deep_2021,
	title = {Deep {Learning} {Models} in {Software} {Requirements} {Engineering}},
	url = {http://arxiv.org/abs/2105.07771},
	abstract = {Requirements elicitation is an important phase of any software project: the errors in requirements are more expensive to fix than the errors introduced at later stages of software life cycle. Nevertheless, many projects do not devote sufficient time to requirements. Automated requirements generation can improve the quality of software projects. In this article we have accomplished the first step of the research on this topic: we have applied the vanilla sentence autoencoder to the sentence generation task and evaluated its performance. The generated sentences are not plausible English and contain only a few meaningful words. We believe that applying the model to a larger dataset may produce significantly better results. Further research is needed to improve the quality of generated data.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Naumcheva, Maria},
	month = may,
	year = {2021},
	note = {arXiv:2105.07771 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{Guo2019DLDevelopmentandDeploymentAcrossDifferentFrameworksandPlatforms,
	title = {An {Empirical} {Study} {Towards} {Characterizing} {Deep} {Learning} {Development} and {Deployment} {Across} {Different} {Frameworks} and {Platforms}},
	doi = {10.1109/ASE.2019.00080},
	abstract = {Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Guo, Qianyu and Chen, Sen and Xie, Xiaofei and Ma, Lei and Hu, Qiang and Liu, Hongtao and Liu, Yang and Zhao, Jianjun and Li, Xiaohong},
	month = nov,
	year = {2019},
	keywords = {Computational modeling, Deep learning deployment, Deep learning frameworks, Deep learning platforms, Empirical study, Mobile handsets, Predictive models, Quantization (signal), Runtime, Software, Training},
	pages = {810--822},
}

@misc{Cheng2022Mask2Former,
	title = {Masked-attention {Mask} {Transformer} for {Universal} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/2112.01527},
	abstract = {Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics deﬁnes a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Maskedattention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a signiﬁcant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).},
	language = {en},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
	month = jun,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{kaddour_challenges_2023,
	title = {Challenges and {Applications} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.10169},
	abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
	month = jul,
	year = {2023},
	note = {arXiv:2307.10169 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{castano_exploring_2023,
	title = {Exploring the {Carbon} {Footprint} of {Hugging} {Face}'s {ML} {Models}: {A} {Repository} {Mining} {Study}},
	shorttitle = {Exploring the {Carbon} {Footprint} of {Hugging} {Face}'s {ML} {Models}},
	url = {http://arxiv.org/abs/2305.11164},
	abstract = {The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models. The goal is to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. The study includes the first repository mining study on the Hugging Face Hub API on carbon emissions. This study seeks to answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? The study yielded several key findings. These include a decreasing proportion of carbon emissions-reporting models, a slight decrease in reported carbon footprint on Hugging Face over the past 2 years, and a continued dominance of NLP as the main application domain. Furthermore, the study uncovers correlations between carbon emissions and various attributes such as model size, dataset size, and ML application domains. These results highlight the need for software measurements to improve energy reporting practices and promote carbon-efficient model development within the Hugging Face community. In response to this issue, two classifications are proposed: one for categorizing models based on their carbon emission reporting practices and another for their carbon efficiency. The aim of these classification proposals is to foster transparency and sustainable model development within the ML community.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Castaño, Joel and Martínez-Fernández, Silverio and Franch, Xavier and Bogner, Justus},
	month = may,
	year = {2023},
	note = {arXiv:2305.11164 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{Paulsen2020NeuroDiff,
	address = {Virtual Event Australia},
	title = {{NeuroDiff}: scalable differential verification of neural networks using fine-grained approximation},
	isbn = {978-1-4503-6768-4},
	shorttitle = {{NeuroDiff}},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416560},
	doi = {10.1145/3324884.3416560},
	language = {en},
	urldate = {2023-07-12},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Paulsen, Brandon and Wang, Jingbo and Wang, Jiawei and Wang, Chao},
	month = dec,
	year = {2020},
	pages = {784--796},
}

@inproceedings{Paulsen2020ReluDiff,
	address = {Seoul South Korea},
	title = {{ReluDiff}: differential verification of deep neural networks},
	isbn = {978-1-4503-7121-6},
	shorttitle = {{ReluDiff}},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380337},
	doi = {10.1145/3377811.3380337},
	language = {en},
	urldate = {2023-07-10},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Paulsen, Brandon and Wang, Jingbo and Wang, Chao},
	month = jun,
	year = {2020},
	pages = {714--726},
}

@inproceedings{Laskar2022DNNFailuresbtwAlgorithmicInaccuracyandTransientHWFaults,
	title = {Characterizing {Deep} {Learning} {Neural} {Network} {Failures} {Between} {Algorithmic} {Inaccuracy} and {Transient} {Hardware} {Faults}},
	doi = {10.1109/PRDC55274.2022.00020},
	abstract = {Deep Neural Networks (DNNs) have been widely deployed in safety-critical applications such as autonomous vehicles, healthcare, and space applications. Though DNN models have long suffered intrinsic algorithmic inaccuracies, the increasing number of hardware transient faults in computer systems has been raising safety and reliability concerns in safety-critical applications. This paper investigates the impact of DNN misclassifications that caused by hardware transient faults and intrinsic algorithmic inaccuracy in safety-critical applications. We first extend a state-of-the-art fault injector for TensorFlow application, TensorFI, to support fault injections on modern DNN models in a scalable way, then characterize the outcome classes of the models, analyzing them based on safety related metrics. Finally, we conduct a large-scale fault injection experiment to measure the failures according to the metrics and study their impact on safety. We observe that failures caused by hardware transient faults could have much more significant impact (up to 4 times higher probability) on safety-critical applications than that of the DNN algorithmic inaccuracies, advocating the potential needs to protect DNNs from hardware faults in safety-critical applications.},
	booktitle = {2022 {IEEE} 27th {Pacific} {Rim} {International} {Symposium} on {Dependable} {Computing} ({PRDC})},
	author = {Laskar, Sabuj and Rahman, Md Hasanur and Zhang, Bohan and Li, Guanpeng},
	month = nov,
	year = {2022},
	keywords = {Analytical models, Computational modeling, Deep Neural Networks, Deep learning, Hardware, Measurement, Neural networks, Prediction algorithms, Transient Hardware Faults},
	pages = {54--67},
}

@misc{Li2023ReliabilityAssuranceforDNNArchAgainstNumericalDefects,
	title = {Reliability {Assurance} for {Deep} {Neural} {Network} {Architectures} {Against} {Numerical} {Defects}},
	url = {http://arxiv.org/abs/2302.06086},
	abstract = {With the widespread deployment of deep neural networks (DNNs), ensuring the reliability of DNN-based systems is of great importance. Serious reliability issues such as system failures can be caused by numerical defects, one of the most frequent defects in DNNs. To assure high reliability against numerical defects, in this paper, we propose the RANUM approach including novel techniques for three reliability assurance tasks: detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. To the best of our knowledge, RANUM is the first approach that confirms potential-defect feasibility with failure-exhibiting tests and suggests fixes automatically. Extensive experiments on the benchmarks of 63 real-world DNN architectures show that RANUM outperforms state-of-the-art approaches across the three reliability assurance tasks. In addition, when the RANUM-generated fixes are compared with developers' fixes on open-source projects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or even better than human fixes.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Li, Linyi and Zhang, Yuhao and Ren, Luyao and Xiong, Yingfei and Xie, Tao},
	month = apr,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@article{Bozeman2000TechnologyTransfer,
	title = {Technology transfer and public policy: a review of research and theory},
	volume = {29},
	issn = {00487333},
	shorttitle = {Technology transfer and public policy},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0048733399000931},
	doi = {10.1016/S0048-7333(99)00093-1},
	abstract = {My purpose is to review, synthesize and criticize the voluminous, multidisciplinary literature on technology transfer. To reduce the literature to manageable proportions, I focus chiefly Žnot exclusively. on recent literature on domestic technology transfer from uniÕersities and goÕernment laboratories. I begin by examining a set of fundamental conceptual issues, especially the ways in which the analytical ambiguities surrounding technology transfer concepts affect research and theory. My literature review follows and I emphasize technology transfer’s impact and effectiveness. I employ a ‘‘Contingent Effectiveness Model of Technology Transfer’’ to organize the literature. As the model’s name implies, it assumes that technology effectiveness can take a variety of forms. In addition to examining the more traditional effectiveness criteriathose rooted in market impacts- the model considers a number of alternative effectiveness criteria, including political effectiveness, capacity-building. q 2000 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {4-5},
	urldate = {2023-07-11},
	journal = {Research Policy},
	author = {Bozeman, Barry},
	month = apr,
	year = {2000},
	pages = {627--655},
}

@article{Nidhra2013KnowledgeTransfer,
	title = {Knowledge transfer challenges and mitigation strategies in global software development—{A} systematic literature review and industrial validation},
	volume = {33},
	issn = {02684012},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0268401212001466},
	doi = {10.1016/j.ijinfomgt.2012.11.004},
	abstract = {Objectives: The overall aim of this work is to provide a body of knowledge for enabling successful KT in GSD settings. This is achieved by an in-depth understanding of KT challenges and mitigation strategies, both from the perspective of literature and industry. It also identiﬁes the similarities and differences in challenges and strategies gathered from literature studies and industrial experts.
Methods: In order to fulﬁll the aim of the research, we collected data through a systematic literature review (SLR) and conducted interviews with industrial experts. Through the SLR we found 35 primary studies relevant to our objectives. We also conducted eight interviews of experienced industrial professionals from eight different multinational companies world-wide. For analyzing the data we used grounded theory and cross-case analysis.
Results: In total, 60 different challenges and 79 unique mitigation strategies are identiﬁed from both SLR and interview results. The challenges and mitigation strategies are grouped into three core categories of personnel, project and technology factors, thus giving rise to a conceptualization called as 2PT factors. There are greater numbers of challenges and mitigation strategies in the project and personnel factors, highlighting the complex interplay of project-related and human-intensive issues in GSD projects, while the technology factor plays the role as facilitator in transferring knowledge. The study also maps the mitigation strategies to challenges, which can guide practitioners in their selection of strategies to use for overcoming KT challenges in GSD.
Conclusions: We conclude that effective management of project and personnel factors, facilitated by technological factors, are crucial for a successful transfer of knowledge in GSD projects. Thus in future, the researchers and practitioners need to focus on the 2PT factors for ensuring effective KT in GSD settings.},
	language = {en},
	number = {2},
	urldate = {2023-07-11},
	journal = {International Journal of Information Management},
	author = {Nidhra, Srinivas and Yanamadala, Muralidhar and Afzal, Wasif and Torkar, Richard},
	month = apr,
	year = {2013},
	pages = {333--355},
}

@article{Cartaxo2018KnowledgeTransferModel,
	title = {Towards a {Model} to {Transfer} {Knowledge} from {Software} {Engineering} {Research} to {Practice}},
	volume = {97},
	doi = {10.1016/j.infsof.2018.01.001},
	abstract = {Context: Many researchers argue that Software Engineering (SE) research lacks connection with practice. Objective: We propose a model aimed at supporting researchers to transfer knowledge to SE practice. Method: This model is built upon the foundation of Rapid Reviews and Evidence Briefings. These two key elements have been proven e↵ective in other domains, such as medicine, and initial results suggest that they can play a prominent role in SE as well. Results: We discuss how to apply the model as well as possible challenges that might hinder its adoption. Conclusion: We believe that both SE practitioners and researchers could benefit from the proposed model. We expect replications and instantiations of the model conducted in the future.},
	journal = {Information and Software Technology},
	author = {Cartaxo, Bruno and Pinto, Gustavo and Soares, Sergio},
	month = apr,
	year = {2018},
}

@article{Roscher2020XAIforScientificInsightsandDiscoveries,
	title = {Explainable {Machine} {Learning} for {Scientific} {Insights} and {Discoveries}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2976199},
	abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.},
	journal = {IEEE Access},
	author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen},
	year = {2020},
	keywords = {Approximation algorithms, Biological system modeling, Data mining, Data models, Explainable machine learning, Kernel, Machine learning, Mathematical model, informed machine learning, interpretability, scientific consistency, transparency},
	pages = {42200--42216},
}

@article{Murugesan2019DeepCompare,
	title = {{DeepCompare}: {Visual} and {Interactive} {Comparison} of {Deep} {Learning} {Model} {Performance}},
	volume = {39},
	issn = {1558-1756},
	shorttitle = {{DeepCompare}},
	doi = {10.1109/MCG.2019.2919033},
	abstract = {Deep learning models have become the state-of-the-art for many tasks, from text sentiment analysis to facial image recognition. However, understanding why certain models perform better than others or how one model learns differently than another is often difficult yet critical for increasing their effectiveness, improving prediction accuracy, and enabling fairness. Traditional methods for comparing models’ efficacy, such as accuracy, precision, and recall provide a quantitative view of performance; however, the qualitative intricacies of why one model performs better than another are hidden. In this paper, we interview machine learning practitioners to understand their evaluation and comparison workflow. From there, we iteratively design a visual analytic approach, DeepCompare, to systematically compare the results of deep learning models, in order to provide insight into the model behavior and interactively assess tradeoffs between two such models. The tool allows users to evaluate model results, identify and compare activation patterns for misclassifications, and link the test results back to specific neurons. We conduct a preliminary evaluation through two real-world case studies to show that experts can make more informed decisions about the effectiveness of different types of models, understand in more detail the strengths and weaknesses of the models, and holistically evaluate the behavior of the models.},
	number = {5},
	journal = {IEEE Computer Graphics and Applications},
	author = {Murugesan, Sugeerth and Malik, Sana and Du, Fan and Koh, Eunyee and Lai, Tuan Manh},
	month = sep,
	year = {2019},
	keywords = {Adaptation models, Analytical models, Computational modeling, Computer architecture, Deep learning, Task analysis, data visualization, human-centered computing, machine learning, visual analytics},
	pages = {47--59},
}

@article{Chatzimparmpas2020EnhancingTrustinMLusingVisualization,
	title = {The {State} of the {Art} in {Enhancing} {Trust} in {Machine} {Learning} {Models} with the {Use} of {Visualizations}},
	volume = {39},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14034},
	doi = {10.1111/cgf.14034},
	abstract = {Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.},
	language = {en},
	number = {3},
	urldate = {2023-07-11},
	journal = {Computer Graphics Forum},
	author = {Chatzimparmpas, A. and Martins, R. M. and Jusufi, I. and Kucher, K. and Rossi, F. and Kerren, A.},
	year = {2020},
	keywords = {ACM CCS:, explainable machine learning, interpretable machine learning, trustworthy machine learning, visualization, • Human-centered computing → Information visualization, • Human-centered computing → Visual analytics, • Human-centered computing → Visualization systems and tools, • Information systems → Trust, • Machine learning → Reinforcement learning, • Machine learning → Semi-supervised learning, • Machine learning → Supervised learning, • Machine learning → Unsupervised learning},
	pages = {713--756},
}

@article{spinner_explainer_2020,
	title = {{explAIner}: {A} {Visual} {Analytics} {Framework} for {Interactive} and {Explainable} {Machine} {Learning}},
	volume = {26},
	issn = {1941-0506},
	shorttitle = {{explAIner}},
	doi = {10.1109/TVCG.2019.2934629},
	abstract = {We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Spinner, Thilo and Schlegel, Udo and Schäfer, Hanna and El-Assady, Mennatallah},
	month = jan,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Analytical models, Computational modeling, Data models, Deep Learning, Explainability, Explainable AI, Interactive Machine Learning, Interpretability, Machine learning, Monitoring, Pipelines, Visual Analytics},
	pages = {1064--1074},
}

@article{yuan_survey_2021,
	title = {A survey of visual analytics techniques for machine learning},
	volume = {7},
	issn = {2096-0433, 2096-0662},
	url = {http://link.springer.com/10.1007/s41095-020-0191-7},
	doi = {10.1007/s41095-020-0191-7},
	abstract = {Visual analytics for machine learning has recently evolved as one of the most exciting areas in the ﬁeld of visualization. To better identify which research topics are promising and to learn how to apply relevant techniques in visual analytics, we systematically review 259 papers published in the last ten years together with representative works before 2010. We build a taxonomy, which includes three ﬁrst-level categories: techniques before model building, techniques during modeling building, and techniques after model building. Each category is further characterized by representative analysis tasks, and each task is exempliﬁed by a set of recent inﬂuential works. We also discuss and highlight research challenges and promising potential future research opportunities useful for visual analytics researchers.},
	language = {en},
	number = {1},
	urldate = {2023-07-11},
	journal = {Computational Visual Media},
	author = {Yuan, Jun and Chen, Changjian and Yang, Weikai and Liu, Mengchen and Xia, Jiazhi and Liu, Shixia},
	month = mar,
	year = {2021},
	pages = {3--36},
}

@incollection{chen_fine-tuning_2017,
	address = {Cham},
	title = {Fine-{Tuning} {Deep} {Neural} {Networks} in {Continuous} {Learning} {Scenarios}},
	volume = {10118},
	isbn = {978-3-319-54525-7 978-3-319-54526-4},
	url = {http://link.springer.com/10.1007/978-3-319-54526-4_43},
	abstract = {The revival of deep neural networks and the availability of ImageNet laid the foundation for recent success in highly complex recognition tasks. However, ImageNet does not cover all visual concepts of all possible application scenarios. Hence, application experts still record new data constantly and expect the data to be used upon its availability. In this paper, we follow this observation and apply the classical concept of ﬁne-tuning deep neural networks to scenarios where data from known or completely new classes is continuously added. Besides a straightforward realization of continuous ﬁne-tuning, we empirically analyze how computational burdens of training can be further reduced. Finally, we visualize how the network’s attention maps evolve over time which allows for visually investigating what the network learned during continuous ﬁne-tuning.},
	language = {en},
	urldate = {2023-07-10},
	booktitle = {Computer {Vision} – {ACCV} 2016 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Käding, Christoph and Rodner, Erik and Freytag, Alexander and Denzler, Joachim},
	editor = {Chen, Chu-Song and Lu, Jiwen and Ma, Kai-Kuang},
	year = {2017},
	doi = {10.1007/978-3-319-54526-4_43},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {588--605},
}

@misc{Chen2021CodeLLM,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{Arnab2021ViViT,
	title = {{ViViT}: {A} {Video} {Vision} {Transformer}},
	shorttitle = {{ViViT}},
	abstract = {We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {6836--6846},
}

@misc{Zou2023MetaSegmentEverything,
	title = {Segment {Everything} {Everywhere} {All} at {Once}},
	url = {http://arxiv.org/abs/2304.06718},
	abstract = {Despite the growing demand for interactive AI systems, there have been few comprehensive studies on human-AI interaction in visual understanding e.g. segmentation. Inspired by the development of prompt-based universal interfaces for LLMs, this paper presents SEEM, a promptable, interactive model for Segmenting Everything Everywhere all at once in an image. SEEM has four desiderata: i) Versatility: by introducing a versatile prompting engine for different types of prompts, including points, boxes, scribbles, masks, texts, and referred regions of another image; ii) Compositionality: by learning a joint visual-semantic space for visual and textual prompts to compose queries on the fly for inference as shown in Fig 1; iii)Interactivity: by incorporating learnable memory prompts to retain dialog history information via mask-guided cross-attention; and iv) Semantic-awareness: by using a text encoder to encode text queries and mask labels for open-vocabulary segmentation.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Gao, Jianfeng and Lee, Yong Jae},
	month = may,
	year = {2023},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{Strubell2019NLPCost,
	title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},
	url = {http://arxiv.org/abs/1906.02243},
	abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Computation and Language},
}

@article{Radford2019LMareUnsupervisedMultitaskLearners,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	volume = {1},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	number = {8},
	journal = {OpenAI blog},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
	pages = {9},
}

@inproceedings{Vaswani2017Attention,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-07-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{latifi_polygraphmr_2020,
	title = {{PolygraphMR}: {Enhancing} the {Reliability} and {Dependability} of {CNNs}},
	shorttitle = {{PolygraphMR}},
	doi = {10.1109/DSN48063.2020.00029},
	abstract = {Deep neural networks (DNNs) are now starting to emerge in mission critical applications including autonomous vehicles and precision medicine. An important question is the dependability of DNNs and trustworthiness of their predictions. Considering the irreparable damage that can be caused by mispredictions, assessment of their potential misbehavior is necessary for safe deployment. In this paper, we first show the deficiency of current confidence-based methods as reliability measurement, and assess the effectiveness of traditional architecture reliability methods such as modular redundancy (MR). Then, we propose PolygraphMR and show that the combination of input preprocessing, smarter decision policies, and inclusion of prediction confidences can substantially improve the effectiveness of MR for DNNs. Next, we show how to prohibit explosive growth in the cost of MR by the help of reduced-precision designs and staged activations. Across six benchmarks, PolygraphMR detects an average of 33.5\% of the baseline mispredictions with less than 2x overhead.},
	booktitle = {2020 50th {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} ({DSN})},
	author = {Latifi, Salar and Zamirai, Babak and Mahlke, Scott},
	month = jun,
	year = {2020},
	note = {ISSN: 1530-0889},
	keywords = {Benchmark testing, Calibration, Computer science, Mission critical systems, Reliability, Reliability, Machine vision, Computer performance, Task analysis},
	pages = {99--112},
}

@inproceedings{wang_robot_2021,
	title = {{RobOT}: {Robustness}-{Oriented} {Testing} for {Deep} {Learning} {Systems}},
	shorttitle = {{RobOT}},
	doi = {10.1109/ICSE43902.2021.00038},
	abstract = {Recently, there has been a significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is deep learning testing, where adversarial examples (a.k.a. bugs) of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the commonly used neuron coverage metrics by existing DL testing approaches are not correlated to model robustness. It is also not an effective measurement on the confidence of the model robustness after testing. In this work, we address this gap by proposing a novel testing framework called Robustness-Oriented Testing (RobOT). A key part of RobOT is a quantitative measurement on 1) the value of each test case in improving model robustness (often via retraining), and 2) the convergence quality of the model robustness improvement. RobOT utilizes the proposed metric to automatically generate test cases valuable for improving model robustness. The proposed metric is also a strong indicator on how well robustness improvement has converged through testing. Experiments on multiple benchmark datasets confirm the effectiveness and efficiency of RobOT in improving DL model robustness, with 67.02\% increase on the adversarial robustness that is 50.65\% higher than the state-of-the-art work DeepGini.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Wang, Jingyi and Chen, Jialuo and Sun, Youcheng and Ma, Xingjun and Wang, Dongxia and Sun, Jun and Cheng, Peng},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Benchmark testing, Convergence, Deep learning, Measurement, Robots, Robustness, Software engineering},
	pages = {300--311},
}

@inproceedings{zhang_empirical_2019,
	title = {An {Empirical} {Study} of {Common} {Challenges} in {Developing} {Deep} {Learning} {Applications}},
	doi = {10.1109/ISSRE.2019.00020},
	abstract = {Recent advances in deep learning promote the innovation of many intelligent systems and applications such as autonomous driving and image recognition. Despite enormous efforts and investments in this field, a fundamental question remains under-investigated - what challenges do developers commonly face when building deep learning applications? To seek an answer, this paper presents a large-scale empirical study of deep learning questions in a popular Q\&A website, Stack Overflow. We manually inspect a sample of 715 questions and identify seven kinds of frequently asked questions. We further build a classification model to quantify the distribution of different kinds of deep learning questions in the entire set of 39,628 deep learning questions. We find that program crashes, model migration, and implementation questions are the top three most frequently asked questions. After carefully examining accepted answers of these questions, we summarize five main root causes that may deserve attention from the research community, including API misuse, incorrect hyperparameter selection, GPU computation, static graph computation, and limited debugging and profiling support. Our results highlight the need for new techniques such as cross-framework differential testing to improve software development productivity and software reliability in deep learning.},
	booktitle = {2019 {IEEE} 30th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Zhang, Tianyi and Gao, Cuiyun and Ma, Lei and Lyu, Michael and Kim, Miryung},
	month = oct,
	year = {2019},
	note = {ISSN: 2332-6549},
	keywords = {deep learning, Stack Overflow, programming issues, software reliability},
	pages = {104--115},
}

@article{ben_braiek_testing_2023,
	title = {Testing {Feedforward} {Neural} {Networks} {Training} {Programs}},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3529318},
	doi = {10.1145/3529318},
	abstract = {At present, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model.
            
              In this article, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design
              TheDeepChecker
              , an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of
              TheDeepChecker
              on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (
              SMD
              ). Results show that
              TheDeepChecker
              ’s on-execution validation of DNN-based program’s properties through three sequential phases (pre-, on-, and post-fitting) succeeds in revealing several coding bugs and system misconfigurations errors early on and at a low cost. Moreover, our property-based approach outperforms the
              SMD
              ’s offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs.},
	language = {en},
	number = {4},
	urldate = {2023-07-10},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Ben Braiek, Houssem and Khomh, Foutse},
	month = oct,
	year = {2023},
	pages = {1--61},
}

@inproceedings{schorn_accurate_2018,
	title = {Accurate neuron resilience prediction for a flexible reliability management in neural network accelerators},
	doi = {10.23919/DATE.2018.8342151},
	abstract = {Deep neural networks have become a ubiquitous tool for mastering complex classification tasks. Current research focuses on the development of power-efficient and fast neural network hardware accelerators for mobile and embedded devices. However, when used in safety-critical applications, for example autonomously operating vehicles, the reliability of such accelerators becomes a further optimization criterion which can stand in contrast to power-efficiency and latency. Furthermore, ensuring hardware reliability becomes increasingly challenging for shrinking structure widths and rising power densities in the nanometer semiconductor technology era. One solution to this challenge is the exploitation of fault tolerant parts in deep neural networks. In this paper we propose a new method for predicting the error resilience of neurons in deep neural networks and show that this method significantly improves upon existing methods in terms of accuracy as well as interpretability. We evaluate prediction accuracy by simulating hardware faults in networks trained on the CIFAR-10 and ILSVRC image classification benchmarks and protecting neurons according to the resilience estimations. In addition, we demonstrate how our resilience prediction can be used for a flexible trade-off between reliability and efficiency in neural network hardware accelerators.},
	booktitle = {2018 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Schorn, Christoph and Guntoro, Andre and Ascheid, Gerd},
	month = mar,
	year = {2018},
	note = {ISSN: 1558-1101},
	keywords = {Biological neural networks, Hardware, Management, Neurons, Reliability, Resilience, Training},
	pages = {979--984},
}

@misc{shao_increasing_2020,
	title = {Increasing {Trustworthiness} of {Deep} {Neural} {Networks} via {Accuracy} {Monitoring}},
	url = {http://arxiv.org/abs/2007.01472},
	abstract = {Inference accuracy of deep neural networks (DNNs) is a crucial performance metric, but can vary greatly in practice subject to actual test datasets and is typically unknown due to the lack of ground truth labels. This has raised significant concerns with trustworthiness of DNNs, especially in safety-critical applications. In this paper, we address trustworthiness of DNNs by using post-hoc processing to monitor the true inference accuracy on a user's dataset. Concretely, we propose a neural network-based accuracy monitor model, which only takes the deployed DNN's softmax probability output as its input and directly predicts if the DNN's prediction result is correct or not, thus leading to an estimate of the true inference accuracy. The accuracy monitor model can be pre-trained on a dataset relevant to the target application of interest, and only needs to actively label a small portion (1\% in our experiments) of the user's dataset for model transfer. For estimation robustness, we further employ an ensemble of monitor models based on the Monte-Carlo dropout method. We evaluate our approach on different deployed DNN models for image classification and traffic sign detection over multiple datasets (including adversarial samples). The result shows that our accuracy monitor model provides a close-to-true accuracy estimation and outperforms the existing baseline methods.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Shao, Zhihui and Yang, Jianyi and Ren, Shaolei},
	month = jul,
	year = {2020},
	note = {arXiv:2007.01472 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{Bubeck2023AGISystem,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{Pham2021DLVarianceTestingFramework,
	title = {{DEVIATE}: {A} {Deep} {Learning} {Variance} {Testing} {Framework}},
	shorttitle = {{DEVIATE}},
	doi = {10.1109/ASE51524.2021.9678540},
	abstract = {Deep learning (DL) training is nondeterministic and such nondeterminism was shown to cause significant variance of model accuracy (up to 10.8\%). Such variance may affect the validity of the comparison of newly proposed DL techniques with baselines. To ensure such validity, DL researchers and practitioners must replicate their experiments multiple times with identical settings to quantify the variance of the proposed approaches and baselines. Replicating and measuring DL variances reliably and efficiently is challenging and understudied.We propose a ready-to-deploy framework DEVIATE that (1) measures DL training variance of a DL model with minimal manual efforts, and (2) provides statistical tests of both accuracy and variance. Specifically, DEVIATE automatically analyzes the DL training code and extracts monitored important metrics (such as accuracy and loss). In addition, DEVIATE performs popular statistical tests and provides users with a report of statistical p-values and effect sizes along with various confidence levels when comparing to selected baselines.We demonstrate the effectiveness of DEVIATE by performing case studies with adversarial training. Specifically, for an adversarial training process that uses the Fast Gradient Signed Method to generate adversarial examples as the training data, DEVIATE measures a max difference of accuracy among 8 identical training runs with fixed random seeds to be up to 5.1\%.Tool and demo links: https://github.com/lin-tan/DEVIATE},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Pham, Hung Viet and Kim, Mijung and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
	month = nov,
	year = {2021},
	keywords = {Deep learning, Manuals, Software, Software algorithms, Software measurement, Training, Training data, deep learning, nondeterminism, variance},
	pages = {1286--1290},
}

@article{tian_evaldnn_2020,
	title = {{EvalDNN}: {A} {Toolbox} for {Evaluating} {Deep} {Neural} {Network} {Models}},
	issn = {02705257},
	doi = {10.1145/3377812.3382133},
	abstract = {Recent studies have shown that the performance of deep learning models should be evaluated using various important metrics such as robustness and neuron coverage, besides the widely-used prediction accuracy metric. However, major deep learning frameworks currently only provide APIs to evaluate a model's accuracy. In order to comprehensively assess a deep learning model, framework users and researchers often need to implement new metrics by themselves, which is a tedious job. What is worse, due to the large number of hyper-parameters and inadequate documentation, evaluation results of some deep learning models are hard to reproduce, especially when the models and metrics are both new.To ease the model evaluation in deep learning systems, we have developed EvalDNN, a user-friendly and extensible toolbox supporting multiple frameworks and metrics with a set of carefully designed APIs. Using EvalDNN, evaluation of a pre-trained model with respect to different metrics can be done with a few lines of code. We have evaluated EvalDNN on 79 models from TensorFlow, Keras, GluonCV, and PyTorch. As a result of our effort made to reproduce the evaluation results of existing work, we release a performance benchmark of popular models, which can be a useful reference to facilitate future research. The tool and benchmark are available at https://github.com/yqtianust/EvalDNN and https://yqtianust.github.io/EvalDNN-benchmark/, respectively. A demo video of EvalDNN is available at: Https://youtu.be/v69bNJN2bJc.},
	journal = {Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020},
	author = {Tian, Yongqiang and Zeng, Zhihua and Wen, Ming and Liu, Yepang and Kuo, Tzu Yang and Cheung, Shing Chi},
	year = {2020},
	keywords = {Deep Learning Model, Evaluation},
	pages = {45--48},
}

@article{zhuang_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Transfer} {Learning}},
	volume = {109},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2020.3004555},
	abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	month = jan,
	year = {2021},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Adaptation models, Covariance matrices, Data models, Domain adaptation, Machine learning, Semisupervised learning, Transfer learning, interpretation, machine learning, transfer learning},
	pages = {43--76},
}

@inproceedings{Nguyen2022Manas,
	address = {Pittsburgh Pennsylvania},
	title = {Manas: mining software repositories to assist {AutoML}},
	isbn = {978-1-4503-9221-1},
	shorttitle = {Manas},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510052},
	doi = {10.1145/3510003.3510052},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {ACM},
	author = {Nguyen, Giang and Islam, Md Johirul and Pan, Rangeet and Rajan, Hridesh},
	month = may,
	year = {2022},
	pages = {1368--1380},
}

@incollection{leibe_identity_2016,
	address = {Cham},
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	volume = {9908},
	isbn = {978-3-319-46492-3 978-3-319-46493-0},
	url = {http://link.springer.com/10.1007/978-3-319-46493-0_38},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR10 (4.62 \% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.},
	language = {en},
	urldate = {2023-07-05},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46493-0_38},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {630--645},
}

@misc{gu_knowledge_2023,
	title = {Knowledge {Distillation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2306.08543},
	abstract = {Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge from white-box generative LLMs is still under-explored, which becomes more and more important with the prosperity of LLMs. In this work, we propose MiniLLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. Extensive experiments in the instruction-following setting show that the MiniLLM models generate more precise responses with the higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance. Our method is also scalable for different model families with 120M to 13B parameters. We will release our code and model checkpoints at https://aka.ms/MiniLLM.},
	urldate = {2023-07-04},
	publisher = {arXiv},
	author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08543 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{cote_quality_2023,
	title = {Quality {Issues} in {Machine} {Learning} {Software} {Systems}},
	url = {http://arxiv.org/abs/2306.15007},
	abstract = {Context: An increasing demand is observed in various domains to employ Machine Learning (ML) for solving complex problems. ML models are implemented as software components and deployed in Machine Learning Software Systems (MLSSs). Problem: There is a strong need for ensuring the serving quality of MLSSs. False or poor decisions of such systems can lead to malfunction of other systems, significant financial losses, or even threats to human life. The quality assurance of MLSSs is considered a challenging task and currently is a hot research topic. Objective: This paper aims to investigate the characteristics of real quality issues in MLSSs from the viewpoint of practitioners. This empirical study aims to identify a catalog of quality issues in MLSSs. Method: We conduct a set of interviews with practitioners/experts, to gather insights about their experience and practices when dealing with quality issues. We validate the identified quality issues via a survey with ML practitioners. Results: Based on the content of 37 interviews, we identified 18 recurring quality issues and 24 strategies to mitigate them. For each identified issue, we describe the causes and consequences according to the practitioners' experience. Conclusion: We believe the catalog of issues developed in this study will allow the community to develop efficient quality assurance tools for ML models and MLSSs. A replication package of our study is available on our public GitHub repository.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Côté, Pierre-Olivier and Nikanjam, Amin and Bouchoucha, Rached and Basta, Ilan and Abidi, Mouna and Khomh, Foutse},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15007 [cs]},
	keywords = {68T05, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{gao_characterizing_2023,
	title = {Characterizing {Deep} {Learning} {Package} {Supply} {Chains} in {PyPI}: {Domains}, {Clusters}, and {Disengagement}},
	shorttitle = {Characterizing {Deep} {Learning} {Package} {Supply} {Chains} in {PyPI}},
	url = {http://arxiv.org/abs/2306.16307},
	abstract = {Deep learning (DL) package supply chains (SCs) are critical for DL frameworks to remain competitive. However, vital knowledge on the nature of DL package SCs is still lacking. In this paper, we explore the domains, clusters, and disengagement of packages in two representative PyPI DL package SCs to bridge this knowledge gap. We analyze the metadata of nearly six million PyPI package distributions and construct version-sensitive SCs for two popular DL frameworks: TensorFlow and PyTorch. We find that popular packages (measured by the number of monthly downloads) in the two SCs cover 34 domains belonging to eight categories. Applications, Infrastructure, and Sciences categories account for over 85\% of popular packages in either SC and TensorFlow and PyTorch SC have developed specializations on Infrastructure and Applications packages respectively. We employ the Leiden community detection algorithm and detect 131 and 100 clusters in the two SCs. The clusters mainly exhibit four shapes: Arrow, Star, Tree, and Forest with increasing dependency complexity. Most clusters are Arrow or Star, but Tree and Forest clusters account for most packages (Tensorflow SC: 70\%, PyTorch SC: 90\%). We identify three groups of reasons why packages disengage from the SC (i.e., remove the DL framework and its dependents from their installation dependencies): dependency issues, functional improvements, and ease of installation. The most common disengagement reason in the two SCs are different. Our study provides rich implications on the maintenance and dependency management practices of PyPI DL SCs.},
	language = {en},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Gao, Kai and He, Runzhi and Xie, Bing and Zhou, Minghui},
	month = jun,
	year = {2023},
	note = {arXiv:2306.16307 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{karras_divide_2023,
	title = {Divide and {Conquer} the {EmpiRE}: {A} {Community}-{Maintainable} {Knowledge} {Graph} of {Empirical} {Research} in {Requirements} {Engineering}},
	shorttitle = {Divide and {Conquer} the {EmpiRE}},
	url = {http://arxiv.org/abs/2306.16791},
	abstract = {[Background.] Empirical research in requirements engineering (RE) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its "current" state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph (ORKG) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in RE (KG-EmpiRE) whose data is openly available. Our long-term goal is to continuously maintain KG-EmpiRE with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in RE. [Method.] We conduct a literature review using the ORKG to build and publish KG-EmpiRE which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020 - 2025. [Results.] From 570 papers of the IEEE International Requirements Engineering Conference (2000 - 2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The ORKG is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Karras, Oliver and Wernlein, Felix and Klünder, Jil and Auer, Sören},
	month = jun,
	year = {2023},
	note = {arXiv:2306.16791 [cs]},
	keywords = {Computer Science - Digital Libraries, Computer Science - Software Engineering},
}

@misc{loedeman_prompt_2023,
	title = {Prompt {Generation} {Networks} for {Input}-based {Adaptation} of {Frozen} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2210.06466},
	abstract = {With the introduction of the transformer architecture in computer vision, increasing model scale has been demonstrated as a clear path to achieving performance and robustness gains. However, with model parameter counts reaching the billions, classical finetuning approaches are becoming increasingly limiting and even unfeasible when models become hosted as inference APIs, as in NLP. To this end, visual prompt learning, whereby a model is adapted by learning additional inputs, has emerged as a potential solution for adapting frozen and cloud-hosted models: During inference, this neither requires access to the internals of models' forward pass function, nor requires any post-processing. In this work, we propose the Prompt Generation Network (PGN) that generates high performing, input-dependent prompts by sampling from an end-to-end learned library of tokens. We further introduce the "prompt inversion" trick, with which PGNs can be efficiently trained in a latent space but deployed as strictly input-only prompts for inference. We show the PGN is effective in adapting pre-trained models to various new datasets: It surpasses previous methods by a large margin on 12/12 datasets and even outperforms full-finetuning on 5/12, while requiring 100x less parameters.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Loedeman, Jochem and Stol, Maarten C. and Han, Tengda and Asano, Yuki M.},
	month = apr,
	year = {2023},
	note = {arXiv:2210.06466 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{Alhamed2021HumanComputationofSWEffortEstimates,
	title = {Playing {Planning} {Poker} in {Crowds}: {Human} {Computation} of {Software} {Effort} {Estimates}},
	shorttitle = {Playing {Planning} {Poker} in {Crowds}},
	doi = {10.1109/ICSE43902.2021.00014},
	abstract = {Reliable cost effective effort estimation remains a considerable challenge for software projects. Recent work has demonstrated that the popular Planning Poker practice can produce reliable estimates when undertaken within a software team of knowledgeable domain experts. However, the process depends on the availability of experts and can be time-consuming to perform, making it impractical for large scale or open source projects that may curate many thousands of outstanding tasks. This paper reports on a full study to investigate the feasibility of using crowd workers supplied with limited information about a task to provide comparably accurate estimates using Planning Poker. We describe the design of a Crowd Planning Poker (CPP) process implemented on Amazon Mechanical Turk and the results of a substantial set of trials, involving more than 5000 crowd workers and 39 diverse software tasks. Our results show that a carefully organised and selected crowd of workers can produce effort estimates that are of similar accuracy to those of a single expert.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Alhamed, Mohammed and Storer, Tim},
	month = may,
	year = {2021},
	keywords = {Agile development, Augmented intelligence, Estimation, Human Computation, Planning, Planning poker, Software, Software effort estimation, Software engineering, Software reliability, Task analysis, Team working, crowd planning poker, crowdsourcing, open source},
	pages = {1--12},
}

@inproceedings{Shepperd1996EffortEstimationusingAnalogy,
	title = {Effort estimation using analogy},
	doi = {10.1109/ICSE.1996.493413},
	abstract = {The staff resources or effort required for a software project are notoriously difficult to estimate in advance. To date most work has focused upon algorithmic cost models such as COCOMO and Function Points. These can suffer from the disadvantage of the need to calibrate the model to each individual measurement environment coupled with very variable accuracy levels even after calibration. An alternative approach is to use analogy for estimation. We demonstrate that this method has considerable promise in that we show it to out perform traditional algorithmic methods for six different datasets. A disadvantage of estimation by analogy is that it requires a considerable amount of computation. The paper describes an automated environment known as ANGEL that supports the collection, storage and identification of the most analogous projects in order to estimate the effort for a new project. ANGEL is based upon the minimisation of Euclidean distance in n-dimensional space. The software is flexible and can deal with differing datasets both in terms of the number of observations (projects) and in the variables collected. Our analogy approach is evaluated with six distinct datasets drawn from a range of different environments and is found to outperform other methods. It is widely accepted that effective software effort estimation demands more than one technique. We have shown that estimating by analogy is a candidate technique and that with the aid of an automated environment is an eminently practical technique.},
	booktitle = {Proceedings of {IEEE} 18th {International} {Conference} on {Software} {Engineering}},
	author = {Shepperd, M. and Schofield, C. and Kitchenham, B.},
	month = mar,
	year = {1996},
	keywords = {Calibration, Computer industry, Cost function, Environmental management, Euclidean distance, Performance evaluation, Project management, Resource management, Software development management, Storage automation},
	pages = {170--178},
}

@misc{xie_impact_2023,
	title = {Impact of {Large} {Language} {Models} on {Generating} {Software} {Specifications}},
	url = {http://arxiv.org/abs/2306.03324},
	abstract = {Software specifications are essential for ensuring the reliability of software systems. Existing specification extraction approaches, however, suffer from limited generalizability and require manual efforts. We study the effectiveness of Large Language Models (LLMs) in generating software specifications from software documentation, utilizing Few-Shot Learning (FSL) to enable LLMs to generalize from a small number of examples. We compare the performance of LLMs with FSL to that of state-of-the-art specification extraction techniques and study the impact of prompt construction strategies on LLM performance. In addition, we conduct a comprehensive analysis of their symptoms and root causes of the failures to understand the pros and cons of LLMs and existing approaches. We also compare 11 LLMs' performance, cost, and response time for generating software specifications. Our findings include that (1) the best performing LLM outperforms existing approaches by 9.1--13.7\% with a few similar examples, (2) the two dominant root causes combined (ineffective prompts and missing domain knowledge) result in 57--60\% of LLM failures, and (3) most of the 11 LLMs achieve better or comparable performance compared to traditional techniques. Our study offers valuable insights for future research to improve specification generation.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Xie, Danning and Yoo, Byungwoo and Jiang, Nan and Kim, Mijung and Tan, Lin and Zhang, Xiangyu and Lee, Judy S.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03324 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{tang_does_2023,
	title = {Does {Synthetic} {Data} {Generation} of {LLMs} {Help} {Clinical} {Text} {Mining}?},
	url = {http://arxiv.org/abs/2303.04360},
	abstract = {Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score from 23.37\% to 63.99\% for the named entity recognition task and from 75.86\% to 83.59\% for the relation extraction task. Furthermore, generating data using ChatGPT can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. In summary, the proposed framework presents a promising solution to enhance the applicability of LLM models to clinical text mining.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Tang, Ruixiang and Han, Xiaotian and Jiang, Xiaoqian and Hu, Xia},
	month = apr,
	year = {2023},
	note = {arXiv:2303.04360 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liu_order-disorder_2023,
	title = {Order-{Disorder}: {Imitation} {Adversarial} {Attacks} for {Black}-box {Neural} {Ranking} {Models}},
	shorttitle = {Order-{Disorder}},
	url = {http://arxiv.org/abs/2209.06506},
	abstract = {Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. To equip the trigger camouflages, we add the next sentence prediction loss and the language model fluency constraint to the objective function. Experimental results on passage ranking demonstrate the effectiveness of the ranking imitation attack model and adversarial triggers against various SOTA neural ranking models. Furthermore, various mitigation analyses and human evaluation show the effectiveness of camouflages when facing potential mitigation approaches. To motivate other scholars to further investigate this novel and important problem, we make the experiment data and code publicly available.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Liu, Jiawei and Kang, Yangyang and Tang, Di and Song, Kaisong and Sun, Changlong and Wang, Xiaofeng and Lu, Wei and Liu, Xiaozhong},
	month = apr,
	year = {2023},
	note = {arXiv:2209.06506 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Information Retrieval},
}

@misc{qi_modularizing_2023,
	title = {Modularizing while {Training}: a {New} {Paradigm} for {Modularizing} {DNN} {Models}},
	shorttitle = {Modularizing while {Training}},
	url = {http://arxiv.org/abs/2306.09376},
	abstract = {Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and inter-module coupling. We have implemented the proposed approach for modularizing Convolutional Neural Network (CNN) models in this work. The evaluation results on representative models demonstrate that MwT outperforms the state-of-the-art approach. Specifically, the accuracy loss caused by MwT is only 1.13 percentage points, which is 1.76 percentage points less than that of the baseline. The kernel retention rate of the modules generated by MwT is only 14.58\%, with a reduction of 74.31\% over the state-of-the-art approach. Furthermore, the total time cost required for training and modularizing is only 108 minutes, half of the baseline.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Qi, Binhang and Sun, Hailong and Zhang, Hongyu and Zhao, Ruobing and Gao, Xiang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09376 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{cheng_prompt_2023,
	title = {Prompt {Sapper}: {A} {LLM}-{Empowered} {Production} {Tool} for {Building} {AI} {Chains}},
	shorttitle = {Prompt {Sapper}},
	url = {http://arxiv.org/abs/2306.12028},
	abstract = {The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e. prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we propose the concept of AI chain and introduce the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematise AI chain engineering methodology. We also develop a no-code integrated development environment, Prompt Sapper, which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper. CCS Concepts: • Software and its engineering → Visual languages; Integrated and visual development environments; Designing software; Design patterns.},
	language = {en},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Cheng, Yu and Chen, Jieshan and Huang, Qing and Xing, Zhenchang and Xu, Xiwei and Lu, Qinghua},
	month = jun,
	year = {2023},
	note = {arXiv:2306.12028 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{gopalakrishnan_distinguishing_2004,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Distinguishing between {Knowledge} {Transfer} and {Technology} {Transfer} {Activities}: {The} {Role} of {Key} {Organizational} {Factors}},
	shorttitle = {Distinguishing between {Knowledge} {Transfer} and {Technology} {Transfer} {Activities}},
	url = {https://papers.ssrn.com/abstract=1495508},
	abstract = {The role of key organizational factors in promoting knowledge transfer and technology transfer are discussed. Data were collected by surveys of 189 industrial firms representing 21 different industrial sectors. Knowledge transfer and technology transfer activities were examined within the context of relationships between universities and industry.        Results implied that when there is a high level of trust, a firm is more willing to share its unique technology and knowledge requirements with its university research center partner. Also, flexible cultures and cultures accustomed to change were found to be more associated with technology transfer, whereas more stable, direction-oriented cultures were associated with knowledge transfer activities.        Finally, results indicated that when technology transfer is at issue, firm size matters, but firm size is inconsequential to knowledge transfer. Implications suggest that managers in organizations need to recognize and apply the differences in the organizational levers discussed to facilitate knowledge and technology transfer.(JSD)},
	language = {en},
	urldate = {2023-06-23},
	author = {Gopalakrishnan, Shanthi and Santoro, Michael D.},
	year = {2004},
	keywords = {Colleges \& universities, Firm size, Industrial development, Knowledge transfer, Organizational cultures, Technology transfer, University-industry relations},
}

@article{lung_analogy-based_2006,
	title = {Analogy-based domain analysis approach to software reuse},
	volume = {12},
	issn = {0947-3602, 1432-010X},
	url = {http://link.springer.com/10.1007/s00766-006-0035-8},
	doi = {10.1007/s00766-006-0035-8},
	language = {en},
	number = {1},
	urldate = {2023-06-20},
	journal = {Requirements Engineering},
	author = {Lung, Chung-Horng and Urban, Joseph E. and Mackulak, Gerald T.},
	month = nov,
	year = {2006},
	pages = {1--22},
}

@article{im_knowledge_nodate,
	title = {Knowledge {Reuse} - {Insights} from {Software} {Reuse}},
	abstract = {We are moving towards an economy where competitive advantage will be determined by knowledge. In their knowledge management strategies, many companies currently aim to encourage knowledge reuse. In this paper, we examine insights drawn from a related field — software reuse — for their relevance to the emerging field of knowledge reuse. We first examine different types of reuse: components, patterns, frameworks and general principles. We then evaluate different kinds of reuse activities. Finally, we discuss lessons from cultural issues in software reuse.},
	language = {en},
	author = {Im, Il and Hars, Alexander},
}

@article{nidhra_knowledge_2013,
	title = {Knowledge transfer challenges and mitigation strategies in global software development—{A} systematic literature review and industrial validation},
	volume = {33},
	issn = {02684012},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0268401212001466},
	doi = {10.1016/j.ijinfomgt.2012.11.004},
	abstract = {Context: In this article we considered knowledge transfer (KT) in global software development (GSD) from two perspectives, state-of-the-art and state-of-the-practice, in order to identify what are the challenges that hamper the success of KT in global software teams, as well as to ﬁnd out what are the mitigation strategies that can be used to overcome such challenges. Objectives: The overall aim of this work is to provide a body of knowledge for enabling successful KT in GSD settings. This is achieved by an in-depth understanding of KT challenges and mitigation strategies, both from the perspective of literature and industry. It also identiﬁes the similarities and di↵erences in challenges and strategies gathered from literature studies and industrial experts. Methods: In order to fulﬁll the aim of the research, we collected data through a Systematic Literature Review (SLR) and conducted interviews with industrial experts. Through the SLR we found 35 primary studies relevant to our objectives. We also conducted eight interviews of experienced industrial professionals from eight di↵erent multinational companies world-wide. For analyzing the data we used grounded theory and cross-case analysis. Results: In total, 60 di↵erent challenges and 79 unique mitigation strategies are identiﬁed from both SLR and interview results. The challenges and mitigation strategies are grouped into three core categories of personnel, project and technology factors, thus giving rise to a conceptualization called as 2PT factors. There are greater numbers of challenges and mitigation strategies in the project and personnel factors, highlighting the complex interplay of project-related and human-intensive issues in GSD projects, while the technology factor plays the role as facilitator in transferring knowledge. The study also maps the mitigation strategies to challenges, which can guide practitioners in their selection of strategies to use for overcoming KT challenges in GSD. Conclusions: We conclude that e↵ective management of project and personnel factors, facilitated by technological factors, are crucial for a successful transfer of knowledge in GSD projects. Thus in future, the researchers and practitioners need to focus on the 2PT factors for ensuring e↵ective KT in GSD settings.},
	language = {en},
	number = {2},
	urldate = {2023-06-20},
	journal = {International Journal of Information Management},
	author = {Nidhra, Srinivas and Yanamadala, Muralidhar and Afzal, Wasif and Torkar, Richard},
	month = apr,
	year = {2013},
	pages = {333--355},
}

@inproceedings{salger_knowledge_2010,
	address = {Cape Town South Africa},
	title = {Knowledge transfer in global software development: leveraging acceptance test case specifications},
	isbn = {978-1-60558-719-6},
	shorttitle = {Knowledge transfer in global software development},
	url = {https://dl.acm.org/doi/10.1145/1810295.1810332},
	doi = {10.1145/1810295.1810332},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Proceedings of the 32nd {ACM}/{IEEE} {International} {Conference} on {Software} {Engineering} - {Volume} 2},
	publisher = {ACM},
	author = {Salger, Frank and Engels, Gregor},
	month = may,
	year = {2010},
	pages = {211--214},
}

@article{hutzschenreuter_contingency_2007,
	title = {A contingency view on knowledge transfer: empirical evidence from the software industry},
	volume = {5},
	issn = {1477-8238, 1477-8246},
	shorttitle = {A contingency view on knowledge transfer},
	url = {https://www.tandfonline.com/doi/full/10.1057/palgrave.kmrp.8500136},
	doi = {10.1057/palgrave.kmrp.8500136},
	abstract = {Reports on failures of knowledge transfer (KT) seriously accumulate. A reason for failure, claimed by contingency theory and strongly supported in other disciplines, is the lack of fit between context and configuration. Assessing the reported failures, we found substantial evidence for this view. Unfortunately, literature on KT explored context and configuration isolated, but largely ignored the fits between both and their relationships to success. Thus, we developed a contingency framework on KT including the above contingency concepts and underlying factors evident in the KT literature. Based on that, we addressed the unexplored relationships between fits and success by case study research in the software industry. In-depth interviews yielded audio-recorded statements for theory building leading to nine propositions. We encourage case study research to reach conceptual closure as well as hypotheses-testing research to achieve empirical validation.},
	language = {en},
	number = {2},
	urldate = {2023-06-19},
	journal = {Knowledge Management Research \& Practice},
	author = {Hutzschenreuter, Thomas and Listner, Florian},
	month = may,
	year = {2007},
	pages = {136--150},
}

@article{adorf_how_2019,
	title = {How to {Professionally} {Develop} {Reusable} {Scientific} {Software}—{And} {When} {Not} {To}},
	volume = {21},
	issn = {1521-9615, 1558-366X},
	url = {https://ieeexplore.ieee.org/document/8558687/},
	doi = {10.1109/MCSE.2018.2882355},
	language = {en},
	number = {2},
	urldate = {2023-06-19},
	journal = {Computing in Science \& Engineering},
	author = {Adorf, Carl S. and Ramasubramani, Vyas and Anderson, Joshua A. and Glotzer, Sharon C.},
	month = mar,
	year = {2019},
	pages = {66--79},
}

@article{malony_translating_2022,
	title = {Translating {High}-{Performance} {Computing} {Tools} {From} {Research} to {Practice}: {Experiences} {With} the {TAU} {Performance} {System}},
	volume = {24},
	issn = {1558-366X},
	shorttitle = {Translating {High}-{Performance} {Computing} {Tools} {From} {Research} to {Practice}},
	doi = {10.1109/MCSE.2023.3257420},
	abstract = {The field of high-performance computing (HPC) has always challenged the research community to design and develop performance observation technology (based on instrumentation, measurement, and analysis methods), keeping pace with the rapid and aggressive evolution of HPC systems’ hardware and software. While the scope of observational concerns is broad and complex, it is the HPC innovation flux that poses difficult translation issues, even for performance tools of limited functionality. Both the complexity of HPC performance observation and the HPC translational pressures have kept the performance tools community mostly research oriented, with only a few open source toolkits widely used. The TAU Performance System is a performance toolkit for HPC with more than 30 years of continuous research and development. This project at the University of Oregon has attempted to keep TAU at the forefront of performance observation capabilities, ported to the latest HPC platforms available, and supported by a dedicated core research team. This article briefly describes the project’s research work and the challenges encountered, with a particular emphasis on the translation process necessary to make TAU the leading performance technology it is today.},
	number = {5},
	journal = {Computing in Science \& Engineering},
	author = {Malony, Allen D. and Shende, Sameer S.},
	month = sep,
	year = {2022},
	note = {Conference Name: Computing in Science \& Engineering},
	keywords = {Hardware, High performance computing, Performance evaluation, Research and development, Software engineering, Technology transfer},
	pages = {65--71},
}

@article{simpson_conceptual_2002,
	title = {A conceptual framework for transferring research to practice},
	volume = {22},
	issn = {07405472},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0740547202002313},
	doi = {10.1016/S0740-5472(02)00231-3},
	abstract = {Systematic evaluations of efforts to transfer research-based interventions and procedures into general practice at community drug treatment programs have been limited. However, practical experiences as well as results from studies of technology transfer and organizational behavior in related fields provide a basis for proposing a heuristic model of key factors that influence this process. The successful completion of four stages of activity typically involved in program change (exposure, adoption, implementation, and practice of new interventions) appears to be influenced by several organizational considerations (e.g., institutional readiness for change, resources, and climate) as well as staff attributes. Assessment instruments for measuring organizational functioning (based on ratings aggregated for staff and patients in a program) are introduced, along with preliminary evidence for their validity. A better conceptual understanding of the process of program change and common barriers that may be encountered is needed for effectively transferring research to practice. D 2002 Elsevier Science Inc. All rights reserved.},
	language = {en},
	number = {4},
	urldate = {2023-06-19},
	journal = {Journal of Substance Abuse Treatment},
	author = {Simpson, D.Dwayne},
	month = jun,
	year = {2002},
	pages = {171--182},
}

@inproceedings{neyshabur_what_2020,
	title = {What is being transferred in transfer learning?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html},
	abstract = {One desired capability for machines is the ability to transfer their understanding of one domain to another domain where data is (usually) scarce. Despite ample adaptation of transfer learning in many deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analysis to address these fundamental questions. Through a series of analysis on transferring to block-shuffled images, we separate the effect of feature reuse from learning high-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.},
	urldate = {2023-06-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
	year = {2020},
	pages = {512--523},
}

@misc{karimi_relationship_2023,
	title = {On the {Relationship} {Between} {Explanation} and {Prediction}: {A} {Causal} {View}},
	shorttitle = {On the {Relationship} {Between} {Explanation} and {Prediction}},
	url = {http://arxiv.org/abs/2212.06925},
	abstract = {Being able to provide explanations for a model's decision has become a central requirement for the development, deployment, and adoption of machine learning models. However, we are yet to understand what explanation methods can and cannot do. How do upstream factors such as data, model prediction, hyperparameters, and random initialization influence downstream explanations? While previous work raised concerns that explanations (E) may have little relationship with the prediction (Y), there is a lack of conclusive study to quantify this relationship. Our work borrows tools from causal inference to systematically assay this relationship. More specifically, we study the relationship between E and Y by measuring the treatment effect when intervening on their causal ancestors, i.e., on hyperparameters and inputs used to generate saliency-based Es or Ys. Our results suggest that the relationships between E and Y is far from ideal. In fact, the gap between 'ideal' case only increase in higher-performing models -- models that are likely to be deployed. Our work is a promising first step towards providing a quantitative measure of the relationship between E and Y, which could also inform the future development of methods for E with a quantitative metric.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Karimi, Amir-Hossein and Muandet, Krikamol and Kornblith, Simon and Schölkopf, Bernhard and Kim, Been},
	month = may,
	year = {2023},
	note = {arXiv:2212.06925 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{fang_does_2023,
	title = {Does progress on {ImageNet} transfer to real-world datasets?},
	url = {http://arxiv.org/abs/2301.04644},
	abstract = {Does progress on ImageNet transfer to real-world datasets? We investigate this question by evaluating ImageNet pre-trained models with varying accuracy (57\% - 83\%) on six practical image classification datasets. In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. On multiple datasets, models with higher ImageNet accuracy do not consistently yield performance improvements. For certain tasks, interventions such as data augmentation improve performance even when architectures do not. We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Fang, Alex and Kornblith, Simon and Schmidt, Ludwig},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04644 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{kornblith_better_2019,
	address = {Long Beach, CA, USA},
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954384/},
	doi = {10.1109/CVPR.2019.00277},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on ImageNet provides minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	language = {en},
	urldate = {2023-06-19},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = jun,
	year = {2019},
	pages = {2656--2666},
}

@inproceedings{kornblith_better_2019-1,
	address = {Long Beach, CA, USA},
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954384/},
	doi = {10.1109/CVPR.2019.00277},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on ImageNet provides minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	language = {en},
	urldate = {2023-06-19},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = jun,
	year = {2019},
	pages = {2656--2666},
}

@inproceedings{Gesi2023LeveragingFeatureBias,
	title = {Leveraging {Feature} {Bias} for {Scalable} {Misprediction} {Explanation} of {Machine} {Learning} {Models}},
	abstract = {Interpreting and debugging machine learning models is necessary to ensure the robustness of the machine learning models. Explaining mispredictions can help significantly in doing so. While recent works on misprediction explanation have proven promising in generating interpretable explanations for mispredictions, the state-of-the-art techniques “blindly” deduce misprediction explanation rules from all data features, which may not be scalable depending on the number of features. To alleviate this problem, we propose an efficient misprediction explanation technique named Bias Guided Misprediction Diagnoser (BGMD), which leverages two prior knowledge about data: a) data often exhibit highly-skewed feature distributions and b) trained models in many cases perform poorly on subdataset with under-represented features. Next, we propose a technique named MAPS (Mispredicted Area UPweight Sampling). MAPS increases the weights of subdataset during model retraining that belong to the group that is prone to be mispredicted because of containing under-represented features. Thus, MAPS make retrained model pay more attention to the under-represented features. Our empirical study shows that our proposed BGMD outperformed the state-of-the-art misprediction diagnoser and reduces diagnosis time by 92\%. Furthermore, MAPS outperformed two state-of-the-art techniques on fixing the machine learning model’s performance on mispredicted data without compromising performance on all data. All the research artifacts (i.e., tools, scripts, and data) of this study are available in the accompanying website [1].},
	language = {en},
	booktitle = {Proceedings of the 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Gesi, Jiri and Shen, Xinyun and Geng, Yunfan and Chen, Qihong and Ahmed, Iftekhar},
	year = {2023},
}

@misc{imtiaz_decomposing_2023,
	title = {Decomposing a {Recurrent} {Neural} {Network} into {Modules} for {Enabling} {Reusability} and {Replacement}},
	url = {http://arxiv.org/abs/2212.05970},
	abstract = {Can we take a recurrent neural network (RNN) trained to translate between languages and augment it to support a new natural language without retraining the model from scratch? Can we fix the faulty behavior of the RNN by replacing portions associated with the faulty behavior? Recent works on decomposing a fully connected neural network (FCNN) and convolutional neural network (CNN) into modules have shown the value of engineering deep models in this manner, which is standard in traditional SE but foreign for deep learning models. However, prior works focus on the image-based multiclass classification problems and cannot be applied to RNN due to (a) different layer structures, (b) loop structures, (c) different types of input-output architectures, and (d) usage of both nonlinear and logistic activation functions. In this work, we propose the first approach to decompose an RNN into modules. We study different types of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN modules can be reused and replaced in various scenarios. We evaluate our approach against 5 canonical datasets (i.e., Math QA, Brown Corpus, Wiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset. We found that decomposing a trained model has a small cost (Accuracy: -0.6\%, BLEU score: +0.10\%). Also, the decomposed modules can be reused and replaced without needing to retrain.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Imtiaz, Sayem Mohammad and Batole, Fraol and Singh, Astha and Pan, Rangeet and Cruz, Breno Dantas and Rajan, Hridesh},
	month = feb,
	year = {2023},
	note = {arXiv:2212.05970 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{Deng2020ModelCompressionandHWAccelerationforNN,
	title = {Model {Compression} and {Hardware} {Acceleration} for {Neural} {Networks}: {A} {Comprehensive} {Survey}},
	volume = {108},
	issn = {1558-2256},
	shorttitle = {Model {Compression} and {Hardware} {Acceleration} for {Neural} {Networks}},
	doi = {10.1109/JPROC.2020.2976475},
	abstract = {Domain-specific hardware is becoming a promising topic in the backdrop of improvement slow down for general-purpose processors due to the foreseeable end of Moore's Law. Machine learning, especially deep neural networks (DNNs), has become the most dazzling domain witnessing successful applications in a wide spectrum of artificial intelligence (AI) tasks. The incomparable accuracy of DNNs is achieved by paying the cost of hungry memory consumption and high computational complexity, which greatly impedes their deployment in embedded systems. Therefore, the DNN compression concept was naturally proposed and widely used for memory saving and compute acceleration. In the past few years, a tremendous number of compression techniques have sprung up to pursue a satisfactory tradeoff between processing efficiency and application accuracy. Recently, this wave has spread to the design of neural network accelerators for gaining extremely high performance. However, the amount of related works is incredibly huge and the reported approaches are quite divergent. This research chaos motivates us to provide a comprehensive survey on the recent advances toward the goal of efficient compression and execution of DNNs without significantly compromising accuracy, involving both the high-level algorithms and their applications in hardware design. In this article, we review the mainstream compression approaches such as compact model, tensor decomposition, data quantization, and network sparsification. We explain their compression principles, evaluation metrics, sensitivity analysis, and joint-way use. Then, we answer the question of how to leverage these methods in the design of neural network accelerators and present the state-of-the-art hardware architectures. In the end, we discuss several existing issues such as fair comparison, testing workloads, automatic compression, influence on security, and framework/hardware-level support, and give promising topics in this field and the possible challenges as well. This article attempts to enable readers to quickly build up a big picture of neural network compression and acceleration, clearly evaluate various methods, and confidently get started in the right way.},
	number = {4},
	journal = {Proceedings of the IEEE},
	author = {Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
	month = apr,
	year = {2020},
	keywords = {Acceleration, Compact neural network, Data quantization, Machine learning, Neural networks, Program processors, Task analysis, Tensor decomposition, data quantization, neural network acceleration, neural network compression, sparse neural network, tensor decomposition},
	pages = {485--532},
}

@inproceedings{wang_eagle_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {{EAGLE}: creating equivalent graphs to test deep learning libraries},
	isbn = {978-1-4503-9221-1},
	shorttitle = {{EAGLE}},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510165},
	doi = {10.1145/3510003.3510165},
	abstract = {Testing deep learning (DL) software is crucial and challenging. Recent approaches use differential testing to cross-check pairs of implementations of the same functionality across different libraries. Such approaches require two DL libraries implementing the same functionality, which is often unavailable. In addition, they rely on a high-level library, Keras, that implements missing functionality in all supported DL libraries, which is prohibitively expensive and thus no longer maintained. To address this issue, we propose EAGLE, a new technique that uses differential testing in a different dimension, by using equivalent graphs to test a single DL implementation (e.g., a single DL library). Equivalent graphs use different Application Programming Interfaces (APIs), data types, or optimizations to achieve the same functionality. The rationale is that two equivalent graphs executed on a single DL implementation should produce identical output given the same input. Specifically, we design 16 new DL equivalence rules and propose a technique, EAGLE, that (1) uses these equivalence rules to build concrete pairs of equivalent graphs and (2) cross-checks the output of these equivalent graphs to detect inconsistency bugs in a DL library. Our evaluation on two widely-used DL libraries, i.e., TensorFlow and PyTorch, shows that EAGLE detects 25 bugs (18 in TensorFlow and 7 in PyTorch), including 13 previously unknown bugs.},
	urldate = {2023-06-03},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Jiannan and Lutellier, Thibaud and Qian, Shangshu and Pham, Hung Viet and Tan, Lin},
	year = {2022},
	keywords = {deep learning, differential testing, graph equivalence, software testing},
	pages = {798--810},
}

@inproceedings{Lu2019AutoDNNSelection4EdgeInference,
	title = {Automating {Deep} {Neural} {Network} {Model} {Selection} for {Edge} {Inference}},
	doi = {10.1109/CogMI48466.2019.00035},
	abstract = {The ever increasing size of deep neural network (DNN) models once implied that they were only limited to cloud data centers for runtime inference. Nonetheless, the recent plethora of DNN model compression techniques have successfully overcome this limit, turning into a reality that DNN-based inference can be run on numerous resource-constrained edge devices including mobile phones, drones, robots, medical devices, wearables, Internet of Things devices, among many others. Naturally, edge devices are highly heterogeneous in terms of hardware specification and usage scenarios. On the other hand, compressed DNN models are so diverse that they exhibit different tradeoffs in a multi-dimension space, and not a single model can achieve optimality in terms of all important metrics such as accuracy, latency and energy consumption. Consequently, how to automatically select a compressed DNN model for an edge device to run inference with optimal quality of experience (QoE) arises as a new challenge. The state-of-the-art approaches either choose a common model for all/most devices, which is optimal for a small fraction of edge devices at best, or apply device-specific DNN model compression, which is not scalable. In this paper, by leveraging the predictive power of machine learning and keeping end users in the loop, we envision an automated device-level DNN model selection engine for QoE-optimal edge inference. To concretize our vision, we formulate the DNN model selection problem into a contextual multi-armed bandit framework, where features of edge devices and DNN models are contexts and pre-trained DNN models are arms selected online based on the history of actions and users' QoE feedback. We develop an efficient online learning algorithm to balance exploration and exploitation. Our preliminary simulation results validate our algorithm and highlight the potential of machine learning for automating DNN model selection to achieve QoE-optimal edge inference.},
	booktitle = {2019 {IEEE} {First} {International} {Conference} on {Cognitive} {Machine} {Intelligence} ({CogMI})},
	author = {Lu, Bingqian and Yang, Jianyi and Chen, Lydia Y. and Ren, Shaolei},
	month = dec,
	year = {2019},
	keywords = {Computational modeling, Data models, Deep neural network, Edge inference, Engines, Machine learning, Measurement, Model selection, Multi arm bandit, Online learning, Quality of experience, Solid modeling},
	pages = {184--193},
}

@article{krueger1992software,
	title = {Software reuse},
	volume = {24},
	number = {2},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Krueger, Charles W},
	year = {1992},
	note = {Publisher: ACM New York, NY, USA},
	pages = {131--183},
}

@inproceedings{Yokoyama2020BuildingRobustDNNApplicationsAnIndustrialCaseStudyofDataAug,
	title = {Towards {Building} {Robust} {DNN} {Applications}: {An} {Industrial} {Case} {Study} of {Evolutionary} {Data} {Augmentation}},
	shorttitle = {Towards {Building} {Robust} {DNN} {Applications}},
	abstract = {Data augmentation techniques that increase the amount of training data by adding realistic transformations are used in machine learning to improve the level of accuracy. Recent studies have demonstrated that data augmentation techniques improve the robustness of image classification models with open datasets; however, it has yet to be investigated whether these techniques are effective for industrial datasets. In this study, we investigate the feasibility of data augmentation techniques for industrial use. We evaluate data augmentation techniques in image classification and object detection tasks using an industrial in-house graphical user interface dataset. As the results indicate, the genetic algorithm-based data augmentation technique outperforms two random-based methods in terms of the robustness of the image classification model. In addition, through this evaluation and interviews with the developers, we learned following two lessons: data augmentation techniques should (1) maintain the training speed to avoid slowing the development and (2) include extensibility for a variety of tasks.},
	booktitle = {2020 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Yokoyama, Haruki and Onoue, Satoshi and Kikuchi, Shinji},
	month = sep,
	year = {2020},
	keywords = {Data models, Image classification, Object detection, Robustness, Software algorithms, Task analysis, Training data, data augmentation, datasets, machine learning, neural networks, object detection},
	pages = {1184--1188},
}

@inproceedings{Li2022TestingMLSystemsinIndustry,
	title = {Testing {Machine} {Learning} {Systems} in {Industry}: {An} {Empirical} {Study}},
	shorttitle = {Testing {Machine} {Learning} {Systems} in {Industry}},
	doi = {10.1145/3510457.3513036},
	abstract = {Machine learning becomes increasingly prevalent and integrated into a wide range of software systems. These systems, named ML systems, must be adequately tested to gain confidence that they behave correctly. Although many research efforts have been devoted to testing technologies for ML systems, the industrial teams are faced with new challenges on testing the ML systems in real-world settings. To absorb inspirations from the industry on the problems in ML testing, we conducted an empirical study including a survey with 87 responses and interviews with 7 senior ML practitioners from well-known IT companies. Our study uncovers significant industrial concerns on major testing activities, i.e., test data collection, test execution, and test result analysis, and also the good practices and open challenges from the perspective of the industry. (1) Test data collection is conducted in different ways on ML model, data, and code and faced with different challenges. (2) Test execution in ML systems suffers from two major problems: entanglement among the components and the regression on model performance. (3) Test result analysis centers on quantitative methods, e.g., metric-based evaluation, and is combined with some qualitative methods based on practitioners’ experience. Based on our findings, we highlight the research opportunities and also provide some implications for practitioners.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Li, Shuyue and Guo, Jiaqi and Lou, Jian-Guang and Fan, Ming and Liu‡, Ting and Zhang, Dongmei},
	year = {2022},
	keywords = {Analytical models, Data collection, Industries, Machine learning, Software systems, Software testing, Technological innovation, machine learning, software testing, survey},
	pages = {263--272},
}

@article{Arani2022RealTimeODNetworks,
	title = {A {Comprehensive} {Study} of {Real}-{Time} {Object} {Detection} {Networks} {Across} {Multiple} {Domains}: {A} {Survey}},
	shorttitle = {A {Comprehensive} {Study} of {Real}-{Time} {Object} {Detection} {Networks} {Across} {Multiple} {Domains}},
	url = {http://arxiv.org/abs/2208.10895},
	abstract = {Deep neural network based object detectors are continuously evolving and are used in a multitude of applications, each having its own set of requirements. While safety-critical applications need high accuracy and reliability, low-latency tasks need resource and energy-efficient networks. Real-time detectors, which are a necessity in high-impact real-world applications, are continuously proposed, but they overemphasize the improvements in accuracy and speed while other capabilities such as versatility, robustness, resource and energy efficiency are omitted. A reference benchmark for existing networks does not exist, nor does a standard evaluation guideline for designing new networks, which results in ambiguous and inconsistent comparisons. We, thus, conduct a comprehensive study on multiple real-time detectors (anchor-, keypoint-, and transformer-based) on a wide range of datasets and report results on an extensive set of metrics. We also study the impact of variables such as image size, anchor dimensions, confidence thresholds, and architecture layers on the overall performance. We analyze the robustness of detection networks against distribution shifts, natural corruptions, and adversarial attacks. Also, we provide a calibration analysis to gauge the reliability of the predictions. Finally, to highlight the real-world impact, we conduct two unique case studies, on autonomous driving and healthcare applications. To further gauge the capability of networks in critical real-time applications, we report the performance after deploying the detection networks on edge devices. Our extensive empirical study can act as a guideline for the industrial community to make an informed choice on the existing networks. We also hope to inspire the research community towards a new direction in the design and evaluation of networks that focuses on a bigger and holistic overview for a far-reaching impact.},
	urldate = {2022-08-25},
	journal = {Transactions on Machine Learning Research},
	author = {Arani, Elahe and Gowda, Shruthi and Mukherjee, Ratnajit and Magdy, Omar and Kathiresan, Senthilkumar and Zonooz, Bahram},
	month = aug,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{de_bie_distribution-based_2020,
	title = {Distribution-{Based} {Invariant} {Deep} {Networks} for {Learning} {Meta}-{Features}},
	url = {http://arxiv.org/abs/2006.13708},
	abstract = {Recent advances in deep learning from probability distributions successfully achieve classification or regression from distribution samples, thus invariant under permutation of the samples. The first contribution of the paper is to extend these neural architectures to achieve invariance under permutation of the features, too. The proposed architecture, called Dida, inherits the NN properties of universal approximation, and its robustness w.r.t. Lipschitz-bounded transformations of the input distribution is established. The second contribution is to empirically and comparatively demonstrate the merits of the approach on two tasks defined at the dataset level. On both tasks, Dida learns meta-features supporting the characterization of a (labelled) dataset. The first task consists of predicting whether two dataset patches are extracted from the same initial dataset. The second task consists of predicting whether the learning performance achieved by a hyper-parameter configuration under a fixed algorithm (ranging in k-NN, SVM, logistic regression and linear classifier with SGD) dominates that of another configuration, for a dataset extracted from the OpenML benchmarking suite. On both tasks, Dida outperforms the state of the art: DSS (Maron et al., 2020) and Dataset2Vec (Jomaa et al., 2019) architectures, as well as the models based on the hand-crafted meta-features of the literature.},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {De Bie, Gwendoline and Rakotoarison, Herilalaina and Peyré, Gabriel and Sebag, Michèle},
	month = oct,
	year = {2020},
	note = {arXiv:2006.13708 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ruhkopf_masif_nodate,
	title = {{MASIF}: {Meta}-learned {Algorithm} {Selection} using {Implicit} {Fidelity} {Information}},
	abstract = {Selecting a well-performing algorithm for a given task or dataset can be time-consuming and tedious, but is crucial for the successful day-to-day business of developing new AI \& ML applications. Algorithm Selection (AS) mitigates this through a meta-model leveraging meta-information about previous tasks. However, most of the available AS methods are error-prone because they characterize a task by either cheap-to-compute properties of the dataset or evaluations of cheap proxy algorithms, called landmarks. In this work, we extend the classical AS data setup to include multi-fidelity information and empirically demonstrate how meta-learning on algorithms’ learning behaviour allows us to exploit cheap test-time evidence effectively and combat myopia significantly. We further postulate a budget-regret trade-off w.r.t. the selection process. Our new selector MASIF is able to jointly interpret online evidence on a task in form of varying-length learning curves without any parametric assumption by leveraging a transformer-based encoder. This opens up new possibilities for guided rapid prototyping in data science on cheaply observed partial learning curves.},
	language = {en},
	author = {Ruhkopf, Tim and Mohan, Aditya and Deng, Difan and Tornede, Alexander and Hutter, Frank and Lindauer, Marius},
}

@misc{mohan_towards_2022,
	title = {Towards {Meta}-learned {Algorithm} {Selection} using {Implicit} {Fidelity} {Information}},
	url = {http://arxiv.org/abs/2206.03130},
	abstract = {Automatically selecting the best performing algorithm for a given dataset or ranking multiple algorithms by their expected performance supports users in developing new machine learning applications. Most approaches for this problem rely on pre-computed dataset meta-features and landmarking performances to capture the salient topology of the datasets and those topologies that the algorithms attend to. Landmarking usually exploits cheap algorithms not necessarily in the pool of candidate algorithms to get inexpensive approximations of the topology. While somewhat indicative, hand-crafted dataset meta-features and landmarks are likely insufficient descriptors, strongly depending on the alignment of the topologies that the landmarks and the candidate algorithms search for. We propose IMFAS, a method to exploit multi-fidelity landmarking information directly from the candidate algorithms in the form of non-parametrically non-myopic meta-learned learning curves via LSTMs in a few-shot setting during testing. Using this mechanism, IMFAS jointly learns the topology of the datasets and the inductive biases of the candidate algorithms, without the need to expensively train them to convergence. Our approach produces informative landmarks, easily enriched by arbitrary meta-features at a low computational cost, capable of producing the desired ranking using cheaper fidelities. We additionally show that IMFAS is able to beat Successive Halving with at most 50\% of the fidelity sequence during test time.},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Mohan, Aditya and Ruhkopf, Tim and Lindauer, Marius},
	month = jul,
	year = {2022},
	note = {arXiv:2206.03130 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{rakotoarison_learning_2022,
	title = {Learning {Meta}-features for {AutoML}},
	url = {https://inria.hal.science/hal-03583789},
	abstract = {This paper tackles the AutoML problem, aimed to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed meta-features with the space of distributions on the hyper-parameter configurations. MetaBu meta-features, learned once and for all, induce a topology on the set of datasets that is exploited to define a distribution of promising hyper-parameter configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that using MetaBu meta-features boosts the performance of state of the art AutoML systems, (Feurer et al. 2015) and Probabilistic Matrix Factorization (Fusi et al. 2018). Furthermore, the inspection of MetaBu meta-features gives some hints into when an ML algorithm does well. Finally, the topology based on MetaBu meta-features enables to estimate the intrinsic dimensionality of the OpenML benchmark w.r.t. a given ML algorithm or pipeline.},
	language = {en},
	urldate = {2023-05-31},
	author = {Rakotoarison, Herilalaina and Milijaona, Louisot and Rasoanaivo, Andry and Sebag, Michèle and Schoenauer, Marc},
	month = apr,
	year = {2022},
}

@article{ma_world_2021,
	title = {World of code: enabling a research workflow for mining and analyzing the universe of open source {VCS} data},
	volume = {26},
	issn = {1382-3256, 1573-7616},
	shorttitle = {World of code},
	url = {http://link.springer.com/10.1007/s10664-020-09905-9},
	doi = {10.1007/s10664-020-09905-9},
	abstract = {Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are the tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flow? To answer such questions we: a) create a very large and frequently updated collection of version control data in the entire FLOSS ecosystems named World of Code (WoC), that can completely cross-reference authors, projects, commits, blobs, dependencies, and history of the FLOSS ecosystems and b) provide capabilities to efficiently correct, augment, query, and analyze that data. Our current WoC implementation is capable of being updated on a monthly basis and contains over 18B Git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.},
	language = {en},
	number = {2},
	urldate = {2023-05-28},
	journal = {Empirical Software Engineering},
	author = {Ma, Yuxing and Dey, Tapajit and Bogart, Chris and Amreen, Sadika and Valiev, Marat and Tutko, Adam and Kennard, David and Zaretzki, Russell and Mockus, Audris},
	month = mar,
	year = {2021},
	pages = {22},
}

@misc{guo_rosearch_2021,
	title = {{RoSearch}: {Search} for {Robust} {Student} {Architectures} {When} {Distilling} {Pre}-trained {Language} {Models}},
	shorttitle = {{RoSearch}},
	url = {http://arxiv.org/abs/2106.03613},
	abstract = {Pre-trained language models achieve outstanding performance in NLP tasks. Various knowledge distillation methods have been proposed to reduce the heavy computation and storage requirements of pre-trained language models. However, from our observations, student models acquired by knowledge distillation suffer from adversarial attacks, which limits their usage in security sensitive scenarios. In order to overcome these security problems, RoSearch is proposed as a comprehensive framework to search the student models with better adversarial robustness when performing knowledge distillation. A directed acyclic graph based search space is built and an evolutionary search strategy is utilized to guide the searching approach. Each searched architecture is trained by knowledge distillation on pre-trained language model and then evaluated under a robustness-, accuracy- and efficiency-aware metric as environmental fitness. Experimental results show that RoSearch can improve robustness of student models from 7\%{\textasciitilde}18\% up to 45.8\%{\textasciitilde}47.8\% on different datasets with comparable weight compression ratio to existing distillation methods (4.6\${\textbackslash}times\${\textasciitilde}6.5\${\textbackslash}times\$ improvement from teacher model BERT\_BASE) and low accuracy drop. In addition, we summarize the relationship between student architecture and robustness through statistics of searched models.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Guo, Xin and Yang, Jianlei and Zhou, Haoyi and Ye, Xucheng and Li, Jianxin},
	month = jun,
	year = {2021},
	note = {arXiv:2106.03613 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{taylor_spellbound_2020,
	title = {{SpellBound}: {Defending} {Against} {Package} {Typosquatting}},
	shorttitle = {{SpellBound}},
	url = {http://arxiv.org/abs/2003.03471},
	abstract = {Package managers for software repositories based on a single programming language are very common. Examples include npm (JavaScript), and PyPI (Python). These tools encourage code reuse, making it trivial for developers to import external packages. Unfortunately, repositories' size and the ease with which packages can be published facilitates the practice of typosquatting: the uploading of a package with name similar to that of a highly popular package, typically with the aim of capturing some of the popular package's installs. Typosquatting has serious negative implications, resulting in developers importing malicious packages, or -- as we show -- code clones which do not incorporate recent security updates. In order to tackle this problem, we present SpellBound, a tool for identifying and reporting potentially erroneous imports to developers. SpellBound implements a novel typosquatting detection technique, based on an in-depth analysis of npm and PyPI. Our technique leverages a model of lexical similarity between names, and further incorporates the notion of package popularity. This approach flags cases where unknown/scarcely used packages would be installed in place of popular ones with similar names, before installation occurs. We evaluated SpellBound on both npm and PyPI, with encouraging results: SpellBound flags typosquatting cases while generating limited warnings (0.5\% of total package installs), and low overhead (only 2.5\% of package install time). Furthermore, SpellBound allowed us to confirm known cases of typosquatting and discover one high-profile, unknown case of typosquatting that resulted in a package takedown by the npm security team.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Taylor, Matthew and Vaidya, Ruturaj K. and Davidson, Drew and De Carli, Lorenzo and Rastogi, Vaibhav},
	month = mar,
	year = {2020},
	note = {arXiv:2003.03471 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@inproceedings{taylor_defending_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Defending {Against} {Package} {Typosquatting}},
	isbn = {978-3-030-65745-1},
	doi = {10.1007/978-3-030-65745-1_7},
	abstract = {Software repositories based on a single programming language are common. Examples include npm (JavaScript) and PyPI (Python). They encourage code reuse, making it trivial for developers to import external packages. Unfortunately, the ease with which packages can be published also facilitates typosquatting: uploading a package with name similar to that of a highly popular package, with the aim of capturing some of the popular package’s installs. Typosquatting frequently occurs in the wild, is difficult to detect manually, and has resulted in developers importing incorrect and sometimes malicious packages.},
	language = {en},
	booktitle = {Network and {System} {Security}},
	publisher = {Springer International Publishing},
	author = {Taylor, Matthew and Vaidya, Ruturaj and Davidson, Drew and De Carli, Lorenzo and Rastogi, Vaibhav},
	editor = {Kutyłowski, Mirosław and Zhang, Jun and Chen, Chao},
	year = {2020},
	pages = {112--131},
}

@inproceedings{taylor_defending_2020-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Defending {Against} {Package} {Typosquatting}},
	isbn = {978-3-030-65745-1},
	doi = {10.1007/978-3-030-65745-1_7},
	abstract = {Software repositories based on a single programming language are common. Examples include npm (JavaScript) and PyPI (Python). They encourage code reuse, making it trivial for developers to import external packages. Unfortunately, the ease with which packages can be published also facilitates typosquatting: uploading a package with name similar to that of a highly popular package, with the aim of capturing some of the popular package’s installs. Typosquatting frequently occurs in the wild, is difficult to detect manually, and has resulted in developers importing incorrect and sometimes malicious packages.},
	language = {en},
	booktitle = {Network and {System} {Security}},
	publisher = {Springer International Publishing},
	author = {Taylor, Matthew and Vaidya, Ruturaj and Davidson, Drew and De Carli, Lorenzo and Rastogi, Vaibhav},
	editor = {Kutyłowski, Mirosław and Zhang, Jun and Chen, Chao},
	year = {2020},
	pages = {112--131},
}

@article{Menghani2023EfficientDLSurvey,
	title = {Efficient {Deep} {Learning}: {A} {Survey} on {Making} {Deep} {Learning} {Models} {Smaller}, {Faster}, and {Better}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Efficient {Deep} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3578938},
	doi = {10.1145/3578938},
	abstract = {Deep learning has revolutionized the fields of computer vision, natural language understanding, speech recognition, information retrieval, and more. However, with the progressive improvements in deep learning models, their number of parameters, latency, and resources required to train, among others, have all increased significantly. Consequently, it has become important to pay attention to these footprint metrics of a model as well, not just its quality. We present and motivate the problem of efficiency in deep learning, followed by a thorough survey of the five core areas of model efficiency (spanning modeling techniques, infrastructure, and hardware) and the seminal work there. We also present an experiment-based guide along with code for practitioners to optimize their model training and deployment. We believe this is the first comprehensive survey in the efficient deep learning space that covers the landscape of model efficiency from modeling techniques to hardware support. It is our hope that this survey would provide readers with the mental model and the necessary understanding of the field to apply generic efficiency techniques to immediately get significant improvements, and also equip them with ideas for further research and experimentation to achieve additional gains.},
	language = {en},
	number = {12},
	urldate = {2023-05-12},
	journal = {ACM Computing Surveys},
	author = {Menghani, Gaurav},
	month = dec,
	year = {2023},
	pages = {1--37},
}

@article{Das2020ModelAdaptation,
	title = {Model adaptation and unsupervised learning with non-stationary batch data under smooth concept drift},
	url = {http://arxiv.org/abs/2002.04094},
	abstract = {Most predictive models assume that training and test data are generated from a stationary process. However, this assumption does not hold true in practice. In this paper, we consider the scenario of a gradual concept drift due to the underlying non-stationarity of the data source. While previous work has investigated this scenario under a supervised-learning and adaption conditions, few have addressed the common, real-world scenario when labels are only available during training. We propose a novel, iterative algorithm for unsupervised adaptation of predictive models. We show that the performance of our batch adapted prediction algorithm is better than that of its corresponding unadapted version. The proposed algorithm provides similar (or better, in most cases) performance within significantly less run time compared to other state of the art methods. We validate our claims though extensive numerical evaluations on both synthetic and real data.},
	author = {Das, Subhro and Lade, Prasanth and Srinivasan, Soundar},
	year = {2020},
}

@article{li_comet_2023,
	title = {{COMET}: {Coverage}-guided {Model} {Generation} {For} {Deep} {Learning} {Library} {Testing}},
	issn = {1049-331X, 1557-7392},
	shorttitle = {{COMET}},
	url = {https://dl.acm.org/doi/10.1145/3583566},
	doi = {10.1145/3583566},
	abstract = {Recent deep learning (DL) applications are mostly built on top of DL libraries. The quality assurance of these libraries is critical to the dependable deployment of DL applications. Techniques have been proposed to generate various DL models and apply them to test these libraries. However, their test effectiveness is constrained by the diversity of layer API calls in their generated DL models. Our study reveals that these techniques can cover at most 34.1\% layer inputs, 25.9\% layer parameter values, and 15.6\% layer sequences. As a result, we find that many bugs arising from specific layer API calls (i.e., specific layer inputs, parameter values, or layer sequences) can be missed by existing techniques.
            Because of this limitation, we propose COMET to effectively generate DL models with diverse layer API calls for DL library testing. COMET: (1) designs a set of mutation operators and a coverage-based search algorithm to diversify layer inputs, layer parameter values, and layer sequences in DL models. (2) proposes a model synthesis method to boost the test efficiency without compromising the layer API call diversity. Our evaluation result shows that COMET outperforms baselines by covering twice as many layer inputs (69.7\% vs. 34.1\%), layer parameter values (50.2\% vs. 25.9\%), and layer sequences (39.0\% vs. 15.6\%) as those by the state-of-the-art. Moreover, COMET covers 3.4\% more library branches than those by existing techniques. Finally, COMET detects 32 new bugs in the latest version of eight popular DL libraries, including TensorFlow and MXNet, with 21 of them confirmed by DL library developers and 7 of those confirmed bugs have been fixed by developers.},
	language = {en},
	urldate = {2023-05-11},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Li, Meiziniu and Cao, Jialun and Tian, Yongqiang and Li, Tsz On and Wen*, Ming and Cheung*, Shing-Chi},
	month = feb,
	year = {2023},
	pages = {3583566},
}

@article{sehra_research_2017,
	title = {Research patterns and trends in software effort estimation},
	volume = {91},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584917304317},
	doi = {10.1016/j.infsof.2017.06.002},
	abstract = {Objective. To identify unobserved research patterns through natural language processing from a large set of research articles on SEE published during the period 1996 to 2016.
Method. A generative statistical method, called Latent Dirichlet Allocation (LDA), applied on a literature dataset of 1178 articles published on SEE.
Results. As many as twelve core research areas and sixty research trends have been revealed; and the identiﬁed research trends have been semantically mapped to associate core research areas.
Conclusion. This study summarises the research trends in SEE based upon a corpus of 1178 articles. The patterns and trends identiﬁed through this research can help in ﬁnding the potential research areas.},
	language = {en},
	urldate = {2023-05-07},
	journal = {Information and Software Technology},
	author = {Sehra, Sumeet Kaur and Brar, Yadwinder Singh and Kaur, Navdeep and Sehra, Sukhjit Singh},
	month = nov,
	year = {2017},
	pages = {1--21},
}

@inproceedings{alhamed_evaluation_2022,
	title = {Evaluation of {Context}-{Aware} {Language} {Models} and {Experts} for {Effort} {Estimation} of {Software} {Maintenance} {Issues}},
	doi = {10.1109/ICSME55016.2022.00020},
	abstract = {Reflecting upon recent advances in Natural Language Processing (NLP), this paper evaluates the effectiveness of context-aware NLP models for predicting software task effort estimates. Term Frequency–Inverse Document Frequency (TF-IDF) and Bidirectional Encoder Representations from Transformers (BERT) were used as feature extraction methods; Random forest and BERT feed-forward linear neural networks were used as classifiers. Using three datasets drawn from open-source projects and one from a commercial project, the paper evaluates the models and compares the best performing model with expert estimates from both kinds of datasets. The results suggest that BERT as feature extraction and classifier shows slightly better performance than other combinations, but that there is no significant difference between the presented methods. On the other hand, the results show that expert and Machine Learning (ML) estimate performances are similar, with the experts’ performance being slightly better. Both findings confirmed existing literature, but using substantially different experimental settings.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Alhamed, Mohammed and Storer, Tim},
	month = oct,
	year = {2022},
	note = {ISSN: 2576-3148},
	keywords = {BERT, Bit error rate, Data models, Feature extraction, Maximum likelihood estimation, NLP, Natural language processing, Planing Poker, Software maintenance, TF–IDF, Transformers, datasets, empirical software engineering, machine learning, software effort estimation, software maintenance issues},
	pages = {129--138},
}

@misc{favero_se3m_2020,
	title = {{SE3M}: {A} {Model} for {Software} {Effort} {Estimation} {Using} {Pre}-trained {Embedding} {Models}},
	shorttitle = {{SE3M}},
	url = {http://arxiv.org/abs/2006.16831},
	abstract = {Estimating effort based on requirement texts presents many challenges, especially in obtaining viable features to infer effort. Aiming to explore a more effective technique for representing textual requirements to infer effort estimates by analogy, this paper proposes to evaluate the effectiveness of pre-trained embeddings models. For this, two embeddings approach, context-less and contextualized models are used. Generic pre-trained models for both approaches went through a fine-tuning process. The generated models were used as input in the applied deep learning architecture, with linear output. The results were very promising, realizing that pre-trained incorporation models can be used to estimate software effort based only on requirements texts. We highlight the results obtained to apply the pre-trained BERT model with fine-tuning in a single project repository, whose value is the Mean Absolute Error (MAE) is 4.25 and the standard deviation of only 0.17, which represents a result very positive when compared to similar works. The main advantages of the proposed estimation method are reliability, the possibility of generalization, speed, and low computational cost provided by the fine-tuning process, and the possibility to infer new or existing requirements.},
	urldate = {2023-05-07},
	publisher = {arXiv},
	author = {Fávero, Eliane M. De Bortoli and Casanova, Dalcimar and Pimentel, Andrey Ricardo},
	month = jun,
	year = {2020},
	note = {arXiv:2006.16831 [cs]},
	keywords = {68T50, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering, I.2.7, I.5.4, K.6.3},
}

@article{schurhoff_empirical_2023,
	title = {An empirical study on a single company’s cost estimations of 338 software projects},
	volume = {28},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-022-10245-z},
	doi = {10.1007/s10664-022-10245-z},
	language = {en},
	number = {1},
	urldate = {2023-05-07},
	journal = {Empirical Software Engineering},
	author = {Schürhoff, Christian and Hanenberg, Stefan and Gruhn, Volker},
	month = jan,
	year = {2023},
	pages = {11},
}

@article{ravichandran_software_2003,
	title = {Software reuse strategies and component markets},
	volume = {46},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/859670.859678},
	doi = {10.1145/859670.859678},
	abstract = {Black-box reuse with component markets could be the silver bullet solution that makes software reuse a reality, and advances software development to a robust industrial process---but only if market makers address the growing pains plaguing this immature industry.},
	language = {en},
	number = {8},
	urldate = {2023-05-07},
	journal = {Communications of the ACM},
	author = {Ravichandran, T. and Rothenberger, Marcus A.},
	month = aug,
	year = {2003},
	pages = {109--114},
}

@article{frakes_software_1996,
	title = {Software reuse: metrics and models},
	volume = {28},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Software reuse},
	url = {https://dl.acm.org/doi/10.1145/234528.234531},
	doi = {10.1145/234528.234531},
	abstract = {As organizations implement systematic software reuse programs to improve productivity and quality, they must be able to measure their progress and identify the most effective reuse strategies. This is done with reuse metrics and models. In this article we survey metrics and models of software reuse and reusability, and provide a classification structure that will help users select them. Six types of metrics and models are reviewed: cost-benefit models, maturity assessment models, amount of reuse metrics, failure modes models, reusability assessment models, and reuse library metrics.},
	language = {en},
	number = {2},
	urldate = {2023-05-07},
	journal = {ACM Computing Surveys},
	author = {Frakes, William and Terry, Carol},
	month = jun,
	year = {1996},
	pages = {415--435},
}

@article{mahajan_corean_2013,
	title = {{COREAN}: {A} proposed {Model} for {Predicting} {Effort} {Estimation} having {Reuse}},
	volume = {2},
	abstract = {The estimation accuracy has been focused in various formal estimation models in recent research initiatives. The formal estimation models were developed to measure lines of code and function points in the software projects but most of them failed to improve accuracy in estimation. The concept of reusability in software development in estimating effort using artificial neural network is focused in this paper. Incorporation of reusability metrics in COCOMO II may yield better results. In COCOMO II it is very difficult to find the values of size parameters. A new model called COREAN has been proposed in this paper for better effort estimation accuracy and reliability. The proposed model has focused on two components of COCOMO II. First, instead of using RUSE cost driver, three new reuse cost drivers are introduced. Second, In order to reduce the project cost, three cost drivers such as PEXE, AEXE, LTEX are combined into single cost driver Personnel Experience (PLEX). Finally, this proposed model accuracy is more improved with the help of Enhanced RPROP algorithm and simulated annealing optimization technique.},
	language = {en},
	number = {6},
	author = {Mahajan, Jyoti and Dutta, Simmi},
	year = {2013},
}

@misc{pujar_automated_2023,
	title = {Automated {Code} generation for {Information} {Technology} {Tasks} in {YAML} through {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.02783},
	abstract = {The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code generation models.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Pujar, Saurabh and Buratti, Luca and Guo, Xiaojie and Dupuis, Nicolas and Lewis, Burn and Suneja, Sahil and Sood, Atin and Nalawade, Ganesh and Jones, Matt and Morari, Alessandro and Puri, Ruchir},
	month = may,
	year = {2023},
	note = {arXiv:2305.02783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@article{hu_aries_nodate,
	title = {Aries: {Efficient} {Testing} of {Deep} {Neural} {Networks} via {Labeling}-{Free} {Accuracy} {Estimation}},
	abstract = {Deep learning (DL) plays a more and more important role in our daily life due to its competitive performance in industrial application domains. As the core of DL-enabled systems, deep neural networks (DNNs) need to be carefully evaluated to ensure the produced models match the expected requirements. In practice, the de facto standard to assess the quality of DNNs in the industry is to check their performance (accuracy) on a collected set of labeled test data. However, preparing such labeled data is often not easy partly because of the huge labeling effort, i.e., data labeling is labor-intensive, especially with the massive new incoming unlabeled data every day. Recent studies show that test selection for DNN is a promising direction that tackles this issue by selecting minimal representative data to label and using these data to assess the model. However, it still requires human effort and cannot be automatic. In this paper, we propose a novel technique, named Aries, that can estimate the performance of DNNs on new unlabeled data using only the information obtained from the original test data. The key insight behind our technique is that the model should have similar prediction accuracy on the data which have similar distances to the decision boundary. We performed a large-scale evaluation of our technique on two famous datasets, CIFAR-10 and TinyImageNet, four widely studied DNN models including ResNet101 and DenseNet121, and 13 types of data transformation methods. Results show that the estimated accuracy by Aries is only 0.03\% – 2.60\% off the true accuracy. Besides, Aries also outperforms the state-of-the-art labeling-free methods in 50 out of 52 cases and selection-labeling-based methods in 96 out of 128 cases.},
	language = {en},
	author = {Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Papadakis, Mike and Ma, Lei and Traon, Yves Le},
}

@article{guan_comprehensive_nodate,
	title = {A {Comprehensive} {Study} of {Real}-{World} {Bugs} in {Machine} {Learning} {Model} {Optimization}},
	abstract = {Due to the great advance in machine learning (ML) techniques, numerous ML models are expanding their application domains in recent years. To adapt for resource-constrained platforms such as mobile and Internet of Things (IoT) devices, pre-trained models are often processed to enhance their efficiency and compactness, using optimization techniques such as pruning and quantization. Similar to the optimization process in other complex systems, e.g., program compilers and databases, optimizations for ML models can contain bugs, leading to severe consequences such as system crashes and financial loss. While bugs in training, compiling and deployment stages have been extensively studied, there is still a lack of systematic understanding and characterization of model optimization bugs (MOBs).},
	language = {en},
	author = {Guan, Hao and Xiao, Ying and Li, Jiaying and Liu, Yepang and Bai, Guangdong},
}

@article{francisco_reflection_2023,
	title = {Reflection: {Empirical} {Software} {Engineering} {Must} {Consider} {Policy} and {Process}},
	language = {en},
	author = {Francisco, San},
	year = {2023},
}

@misc{wang_awesome-meta_2023,
	title = {Awesome-{META}+: {Meta}-{Learning} {Research} and {Learning} {Platform}},
	shorttitle = {Awesome-{META}+},
	url = {http://arxiv.org/abs/2304.12921},
	abstract = {Artificial intelligence technology has already had a profound impact in various fields such as economy, industry, and education, but still limited. Meta-learning, also known as "learning to learn", provides an opportunity for general artificial intelligence, which can break through the current AI bottleneck. However, meta learning started late and there are fewer projects compare with CV, NLP etc. Each deployment requires a lot of experience to configure the environment, debug code or even rewrite, and the frameworks are isolated. Moreover, there are currently few platforms that focus exclusively on meta-learning, or provide learning materials for novices, for which the threshold is relatively high. Based on this, Awesome-META+, a meta-learning framework integration and learning platform is proposed to solve the above problems and provide a complete and reliable meta-learning framework application and learning platform. The project aims to promote the development of meta-learning and the expansion of the community, including but not limited to the following functions: 1) Complete and reliable meta-learning framework, which can adapt to multi-field tasks such as target detection, image classification, and reinforcement learning. 2) Convenient and simple model deployment scheme which provide convenient meta-learning transfer methods and usage methods to lower the threshold of meta-learning and improve efficiency. 3) Comprehensive researches for learning. 4) Objective and credible performance analysis and thinking.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Wang, Jingyao and Zhang, Chuyuan and Ding, Ye and Yang, Yuxuan},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12921 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{benyahia_overcoming_2019,
	title = {Overcoming {Multi}-{Model} {Forgetting}},
	url = {http://arxiv.org/abs/1902.08232},
	abstract = {We identify a phenomenon, which we refer to as multi-model forgetting, that occurs when sequentially training multiple deep networks with partially-shared parameters; the performance of previously-trained models degrades as one optimizes a subsequent one, due to the overwriting of shared parameters. To overcome this, we introduce a statistically-justified weight plasticity loss that regularizes the learning of a model's shared parameters according to their importance for the previous models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight plasticity in neural architecture search preserves the best models to the end of the search and yields improved results in both natural language processing and computer vision tasks.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Benyahia, Yassine and Yu, Kaicheng and Bennani-Smires, Kamil and Jaggi, Martin and Davison, Anthony and Salzmann, Mathieu and Musat, Claudiu},
	month = mar,
	year = {2019},
	note = {arXiv:1902.08232 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{shi_gradient_2022,
	title = {{GRADIENT} {MATCHING} {FOR} {DOMAIN} {GENERALIZA}-},
	abstract = {Machine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an inter-domain gradient matching objective that targets domain generalization by maximizing the inner product between gradients from different domains. Since direct optimization of the gradient inner product can be computationally prohibitive — it requires computation of second-order derivatives —- we derive a simpler ﬁrst-order algorithm named Fish that approximates its optimization. We perform experiments on the WILDS benchmark, which captures distribution shift in the real world, as well as the DOMAINBED benchmark that focuses more on syntheticto-real transfer. Our method produces competitive results on both benchmarks, demonstrating its effectiveness across a wide range of domain generalization tasks. Code is available at https://github.com/YugeTen/fish.},
	language = {en},
	author = {Shi, Yuge and Siddharth, N and Seely, Jeffrey and Torr, Philip H S and Hannun, Awni and Usunier, Nicolas and Synnaeve, Gabriel},
	year = {2022},
}

@misc{noauthor_deeptest_nodate,
	title = {{DeepTest} 2023 - {ICSE} 2023},
	url = {https://conf.researchr.org/home/icse-2023/deeptest-2023#event-overview},
	abstract = {DeepTest is a high-quality workshop for research at the intersection of Machine Learning (ML) and software engineering (SE). ML is widely adopted in modern software systems, including safety-critical domains such as autonomous cars, medical diagnosis, and aircraft collision avoidance systems. Thus, it is crucial to rigorously test such applications to ensure high dependability. However, standard notions of software quality and reliability become irrelevant when considering ML systems, due to their non-deterministic nature and the lack of a transparent understanding of the models’ semantics ...},
	urldate = {2023-04-21},
}

@misc{nguyen_dealing_2023,
	title = {Dealing with {Popularity} {Bias} in {Recommender} {Systems} for {Third}-party {Libraries}: {How} far {Are} {We}?},
	shorttitle = {Dealing with {Popularity} {Bias} in {Recommender} {Systems} for {Third}-party {Libraries}},
	url = {http://arxiv.org/abs/2304.10409},
	abstract = {Recommender systems for software engineering (RSSEs) assist software engineers in dealing with a growing information overload when discerning alternative development solutions. While RSSEs are becoming more and more effective in suggesting handy recommendations, they tend to suffer from popularity bias, i.e., favoring items that are relevant mainly because several developers are using them. While this rewards artifacts that are likely more reliable and well-documented, it would also mean that missing artifacts are rarely used because they are very specific or more recent. This paper studies popularity bias in Third-Party Library (TPL) RSSEs. First, we investigate whether state-of-the-art research in RSSEs has already tackled the issue of popularity bias. Then, we quantitatively assess four existing TPL RSSEs, exploring their capability to deal with the recommendation of popular items. Finally, we propose a mechanism to defuse popularity bias in the recommendation list. The empirical study reveals that the issue of dealing with popularity in TPL RSSEs has not received adequate attention from the software engineering community. Among the surveyed work, only one starts investigating the issue, albeit getting a low prediction performance.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Nguyen, Phuong T. and Rubei, Riccardo and Di Rocco, Juri and Di Sipio, Claudio and Di Ruscio, Davide and Di Penta, Massimiliano},
	month = apr,
	year = {2023},
	note = {arXiv:2304.10409 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{Baz2021LessonsLearnedfromMetaDLChallenge,
	title = {Lessons learned from the {NeurIPS} 2021 {MetaDL} challenge: {Backbone} fine-tuning without episodic meta-learning dominates for few-shot learning image classification},
	shorttitle = {Lessons learned from the {NeurIPS} 2021 {MetaDL} challenge},
	url = {https://proceedings.mlr.press/v176/el-baz22a.html},
	abstract = {Although deep neural networks are capable of achieving performance superior to humans on various tasks, they are notorious for requiring large amounts of data and computing resources, restricting their success to domains where such resources are available. Meta-learning methods can address this problem by transferring knowledge from related tasks, thus reducing the amount of data and computing resources needed to learn new tasks. We organize the MetaDL competition series, which provide opportunities for research groups all over the world to create and experimentally assess new meta-(deep)learning solutions for real problems. In this paper, authored collaboratively between the competition organizers and the top-ranked participants, we describe the design of the competition, the datasets, the best experimental results, as well as the top-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active teams who made it to the final phase (by outperforming the baseline), making over 100 code submissions during the feedback phase. The solutions of the top participants have been open-sourced. The lessons learned include that learning good representations is essential for effective transfer learning.},
	language = {en},
	urldate = {2023-04-19},
	booktitle = {Proceedings of the {NeurIPS} 2021 {Competitions} and {Demonstrations} {Track}},
	publisher = {PMLR},
	author = {Baz, Adrian El and Ullah, Ihsan and Alcobaça, Edesio and Carvalho, André C. P. L. F. and Chen, Hong and Ferreira, Fabio and Gouk, Henry and Guan, Chaoyu and Guyon, Isabelle and Hospedales, Timothy and Hu, Shell and Huisman, Mike and Hutter, Frank and Liu, Zhengying and Mohr, Felix and Öztürk, Ekrem and Rijn, Jan N. van and Sun, Haozhe and Wang, Xin and Zhu, Wenwu},
	month = jul,
	year = {2022},
	pages = {80--96},
}

@misc{mohr_learning_2022,
	title = {Learning {Curves} for {Decision} {Making} in {Supervised} {Machine} {Learning} -- {A} {Survey}},
	url = {http://arxiv.org/abs/2201.12150},
	abstract = {Learning curves are a concept from social sciences that has been adopted in the context of machine learning to assess the performance of a learning algorithm with respect to a certain resource, e.g. the number of training examples or the number of training iterations. Learning curves have important applications in several contexts of machine learning, most importantly for the context of data acquisition, early stopping of model training and model selection. For example, by modelling the learning curves, one can assess at an early stage whether the algorithm and hyperparameter configuration have the potential to be a suitable choice, often speeding up the algorithm selection process. A variety of approaches has been proposed to use learning curves for decision making. Some models answer the binary decision question of whether a certain algorithm at a certain budget will outperform a certain reference performance, whereas more complex models predict the entire learning curve of an algorithm. We contribute a framework that categorizes learning curve approaches using three criteria: the decision situation that they address, the intrinsic learning curve question that they answer and the type of resources that they use. We survey papers from literature and classify them into this framework.},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Mohr, Felix and van Rijn, Jan N.},
	month = jan,
	year = {2022},
	note = {arXiv:2201.12150 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{ghareh_mohammadi_parameter_2019,
	title = {On {Parameter} {Tuning} in {Meta}-{Learning} for {Computer} {Vision}},
	doi = {10.1109/CSCI49370.2019.00060},
	abstract = {Learning to learn plays a pivotal role in meta-learning (MTL) to obtain an optimal learning model. In this paper, we investigate image recognition for unseen categories of a given dataset with limited training information. We deploy a zero-shot learning (ZSL) algorithm to achieve this goal. We also explore the effect of parameter tuning on performance of semantic auto-encoder (SAE). We further address the parameter tuning problem for meta-learning, especially focusing on zero-shot learning. By combining different embedded parameters, we improved the accuracy of tuned-SAE. Advantages and disadvantages of parameter tuning and its application in image classification are also explored.},
	booktitle = {2019 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Ghareh Mohammadi, Farid and Arabnia, Hamid R. and Amini, M. Hadi},
	month = dec,
	year = {2019},
	keywords = {Advanced Machine Learning, Data Science, Meta Learning, Zero-Shot Learning, Few-Shot Learning, Optimized Learning, Parameter Tuning, Computer vision, Image recognition, Machine learning, Semantics, Testing, Training, Tuning},
	pages = {300--305},
}

@inproceedings{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/375c71349b295fbe2dcdca9206f20a06-Abstract.html},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
	urldate = {2023-04-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	year = {2014},
}

@inproceedings{You2021LogME,
	title = {{LogME}: {Practical} {Assessment} of {Pre}-trained {Models} for {Transfer} {Learning}},
	shorttitle = {{LogME}},
	url = {https://proceedings.mlr.press/v139/you21b.html},
	abstract = {This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo {\textbackslash}emph\{without fine-tuning\}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is {\textbackslash}emph\{immune to over-fitting\}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is {\textbackslash}emph\{fast, accurate, and general\}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most \$3000{\textbackslash}times\$ speedup in wall-clock time and requires only \$1\%\$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: {\textbackslash}href\{https://github.com/thuml/LogME\}\{https://github.com/thuml/LogME\}.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {You, Kaichao and Liu, Yong and Wang, Jianmin and Long, Mingsheng},
	month = jul,
	year = {2021},
	pages = {12133--12143},
}

@inproceedings{Ozturk2022ZeroShotAutoMLwithPTMs,
	title = {Zero-shot {AutoML} with {Pretrained} {Models}},
	url = {https://proceedings.mlr.press/v162/ozturk22a.html},
	abstract = {Given a new dataset D and a low compute budget, how should we choose a pre-trained model to fine-tune to D, and set the fine-tuning hyperparameters without risking overfitting, particularly if D is small? Here, we extend automated machine learning (AutoML) to best make these choices. Our domain-independent meta-learning approach learns a zero-shot surrogate model which, at test time, allows to select the right deep learning (DL) pipeline (including the pre-trained model and fine-tuning hyperparameters) for a new dataset D given only trivial meta-features describing D such as image resolution or the number of classes. To train this zero-shot model, we collect performance data for many DL pipelines on a large collection of datasets and meta-train on this data to minimize a pairwise ranking objective. We evaluate our approach under the strict time limit of the vision track of the ChaLearn AutoDL challenge benchmark, clearly outperforming all challenge contenders.},
	language = {en},
	urldate = {2023-04-18},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Öztürk, Ekrem and Ferreira, Fabio and Jomaa, Hadi and Schmidt-Thieme, Lars and Grabocka, Josif and Hutter, Frank},
	month = jun,
	year = {2022},
	pages = {17138--17155},
}

@misc{de_rosa_improving_2022,
	title = {Improving {Pre}-{Trained} {Weights} {Through} {Meta}-{Heuristics} {Fine}-{Tuning}},
	url = {http://arxiv.org/abs/2212.09447},
	abstract = {Machine Learning algorithms have been extensively researched throughout the last decade, leading to unprecedented advances in a broad range of applications, such as image classification and reconstruction, object recognition, and text categorization. Nonetheless, most Machine Learning algorithms are trained via derivative-based optimizers, such as the Stochastic Gradient Descent, leading to possible local optimum entrapments and inhibiting them from achieving proper performances. A bio-inspired alternative to traditional optimization techniques, denoted as meta-heuristic, has received significant attention due to its simplicity and ability to avoid local optimums imprisonment. In this work, we propose to use meta-heuristic techniques to fine-tune pre-trained weights, exploring additional regions of the search space, and improving their effectiveness. The experimental evaluation comprises two classification tasks (image and text) and is assessed under four literature datasets. Experimental results show nature-inspired algorithms' capacity in exploring the neighborhood of pre-trained weights, achieving superior results than their counterpart pre-trained architectures. Additionally, a thorough analysis of distinct architectures, such as Multi-Layer Perceptron and Recurrent Neural Networks, attempts to visualize and provide more precise insights into the most critical weights to be fine-tuned in the learning process.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {de Rosa, Gustavo H. and Roder, Mateus and Papa, João Paulo and Santos, Claudio F. G. dos},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09447 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{siegmund_views_2015,
	title = {Views on {Internal} and {External} {Validity} in {Empirical} {Software} {Engineering}},
	volume = {1},
	doi = {10.1109/ICSE.2015.24},
	abstract = {Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	author = {Siegmund, Janet and Siegmund, Norbert and Apel, Sven},
	month = may,
	year = {2015},
	note = {ISSN: 1558-1225},
	keywords = {Bibliographies, Computer languages, Context, Guidelines, History, Software engineering, Standards},
	pages = {9--19},
}

@misc{shi_towards_2023,
	title = {Towards {Efficient} {Fine}-tuning of {Pre}-trained {Code} {Models}: {An} {Experimental} {Study} and {Beyond}},
	shorttitle = {Towards {Efficient} {Fine}-tuning of {Pre}-trained {Code} {Models}},
	url = {http://arxiv.org/abs/2304.05216},
	abstract = {Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that only the representations of the top two layers change most during fine-tuning for various downstream tasks. (3) Based on the above findings, we propose Telly to efficiently fine-tune pre-trained code models via layer freezing. The extensive experimental results on five various downstream tasks demonstrate that training parameters and the corresponding time cost are greatly reduced, while performances are similar or better. Replication package including source code, datasets, and online Appendix is available at: {\textbackslash}url\{https://github.com/DeepSoftwareAnalytics/Telly\}.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Shi, Ensheng and Wang, Yanlin and Zhang, Hongyu and Du, Lun and Han, Shi and Zhang, Dongmei and Sun, Hongbin},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05216 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@article{zamir_taskonomy_nodate,
	title = {Taskonomy: {Disentangling} {Task} {Transfer} {Learning}},
	abstract = {Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity.},
	language = {en},
	author = {Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas J and Malik, Jitendra and Savarese, Silvio},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2023-04-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}

@inproceedings{hasabnis_gitrank_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{GitRank}: a framework to rank {GitHub} repositories},
	isbn = {978-1-4503-9303-4},
	shorttitle = {{GitRank}},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528519},
	doi = {10.1145/3524842.3528519},
	language = {en},
	urldate = {2023-04-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Hasabnis, Niranjan},
	month = may,
	year = {2022},
	pages = {729--731},
}

@misc{giunchiglia_machine_2023,
	title = {Machine {Learning} with {Requirements}: a {Manifesto}},
	shorttitle = {Machine {Learning} with {Requirements}},
	url = {http://arxiv.org/abs/2304.03674},
	abstract = {In the recent years, machine learning has made great advancements that have been at the root of many breakthroughs in different application domains. However, it is still an open issue how make them applicable to high-stakes or safety-critical application domains, as they can often be brittle and unreliable. In this paper, we argue that requirements definition and satisfaction can go a long way to make machine learning models even more fitting to the real world, especially in critical domains. To this end, we present two problems in which (i) requirements arise naturally, (ii) machine learning models are or can be fruitfully deployed, and (iii) neglecting the requirements can have dramatic consequences. We show how the requirements specification can be fruitfully integrated into the standard machine learning development pipeline, proposing a novel pyramid development process in which requirements definition may impact all the subsequent phases in the pipeline, and viceversa.},
	urldate = {2023-04-10},
	publisher = {arXiv},
	author = {Giunchiglia, Eleonora and Imrie, Fergus and van der Schaar, Mihaela and Lukasiewicz, Thomas},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03674 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{mishra_delphi_2020,
	address = {Virtual Event USA},
	title = {Delphi: {A} {Cryptographic} {Inference} {System} for {Neural} {Networks}},
	isbn = {978-1-4503-8088-1},
	shorttitle = {Delphi},
	url = {https://dl.acm.org/doi/10.1145/3411501.3419418},
	doi = {10.1145/3411501.3419418},
	language = {en},
	urldate = {2023-04-07},
	booktitle = {Proceedings of the 2020 {Workshop} on {Privacy}-{Preserving} {Machine} {Learning} in {Practice}},
	publisher = {ACM},
	author = {Mishra, Pratyush and Lehmkuhl, Ryan and Srinivasan, Akshayaram and Zheng, Wenting and Popa, Raluca Ada},
	month = nov,
	year = {2020},
	pages = {27--30},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{nijkamp_codegen_2023,
	title = {{CodeGen}: {An} {Open} {Large} {Language} {Model} for {Code} with {Multi}-{Turn} {Program} {Synthesis}},
	shorttitle = {{CodeGen}},
	url = {http://arxiv.org/abs/2203.13474},
	abstract = {Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
	month = feb,
	year = {2023},
	note = {arXiv:2203.13474 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@misc{noauthor_similarity_nodate,
	title = {Similarity measures for {Collaborative} {Filtering}-based {Recommender} {Systems}: {Review} and experimental comparison {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {Similarity measures for {Collaborative} {Filtering}-based {Recommender} {Systems}},
	url = {https://reader.elsevier.com/reader/sd/pii/S1319157821002652?token=A56F3BD8B6CD8343E00284CB96A9EF368D07BF9583A8F58632E4C89580EC02EE9DE6F938DC3AB890DA5F0C5EC06CFAB4&originRegion=us-east-1&originCreation=20230405153219},
	language = {en},
	urldate = {2023-04-05},
	doi = {10.1016/j.jksuci.2021.09.014},
}

@inproceedings{zhang_remos_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{ReMoS}: reducing defect inheritance in transfer learning via relevant model slicing},
	isbn = {978-1-4503-9221-1},
	shorttitle = {{ReMoS}},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510191},
	doi = {10.1145/3510003.3510191},
	language = {en},
	urldate = {2023-04-04},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zhang, Ziqi and Li, Yuanchun and Wang, Jindong and Liu, Bingyan and Li, Ding and Guo, Yao and Chen, Xiangqun and Liu, Yunxin},
	month = may,
	year = {2022},
	pages = {1856--1868},
}

@inproceedings{meng_improving_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Improving fault localization and program repair with deep semantic features and transferred knowledge},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510147},
	doi = {10.1145/3510003.3510147},
	language = {en},
	urldate = {2023-04-04},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Meng, Xiangxin and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Liu, Xudong},
	month = may,
	year = {2022},
	pages = {1169--1180},
}

@inproceedings{chen_copy_2022,
	title = {Copy, {Right}? {A} {Testing} {Framework} for {Copyright} {Protection} of {Deep} {Learning} {Models}},
	shorttitle = {Copy, {Right}?},
	doi = {10.1109/SP46214.2022.9833747},
	abstract = {Deep learning models, especially those large-scale and high-performance ones, can be very costly to train, demanding a considerable amount of data and computational resources. As a result, deep learning models have become one of the most valuable assets in modern artificial intelligence. Unauthorized duplication or reproduction of deep learning models can lead to copyright infringement and cause huge economic losses to model owners, calling for effective copyright protection techniques. Existing protection techniques are mostly based on watermarking, which embeds an owner-specified watermark into the model. While being able to provide exact ownership verification, these techniques are 1) invasive, i.e., they need to tamper with the training process, which may affect the model utility or introduce new security risks into the model; 2) prone to adaptive attacks that attempt to remove/replace the watermark or adversarially block the retrieval of the watermark; and 3) not robust to the emerging model extraction attacks. Latest fingerprinting work on deep learning models, though being non-invasive, also falls short when facing the diverse and ever-growing attack scenarios.In this paper, we propose a novel testing framework for deep learning copyright protection: DEEPJUDGE. DEEPJUDGE quantitatively tests the similarities between two deep learning models: a victim model and a suspect model. It leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspect model is a copy of the victim model. Advantages of DEEPJUDGE include: 1) non-invasive, as it works directly on the model and does not tamper with the training process; 2) efficient, as it only needs a small set of seed test cases and a quick scan of the two models; 3) flexible, i.e., it can easily incorporate new testing metrics or test case generation methods to obtain more confident and robust judgement; and 4) fairly robust to model extraction attacks and adaptive attacks. We verify the effectiveness of DEEPJUDGE under three typical copyright infringement scenarios, including model finetuning, pruning and extraction, via extensive experiments on both image classification and speech recognition datasets with a variety of model architectures.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Chen, Jialuo and Wang, Jingyi and Peng, Tinglan and Sun, Youcheng and Cheng, Peng and Ji, Shouling and Ma, Xingjun and Li, Bo and Song, Dawn},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	keywords = {Adaptation models, Biological system modeling, Computational modeling, Deep learning, Measurement, Training, Watermarking},
	pages = {824--841},
}

@misc{shen_hugginggpt_2023,
	title = {{HuggingGPT}: {Solving} {AI} {Tasks} with {ChatGPT} and its {Friends} in {HuggingFace}},
	shorttitle = {{HuggingGPT}},
	url = {http://arxiv.org/abs/2303.17580},
	abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards AGI.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17580 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{achille_task2vec_2019,
	address = {Seoul, Korea (South)},
	title = {{Task2Vec}: {Task} {Embedding} for {Meta}-{Learning}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{Task2Vec}},
	url = {https://ieeexplore.ieee.org/document/9008292/},
	doi = {10.1109/ICCV.2019.00653},
	abstract = {We introduce a method to generate vectorial representations of visual classiﬁcation tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function, we process images through a “probe network” and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a ﬁxed-dimensional embedding of the task that is independent of details such as the number of classes and requires no understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks. We demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a novel task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well on which task without actually ﬁne-tuning the model. Selecting a feature extractor with task embedding yields performance close to the best available feature extractor, with substantially less computational effort than exhaustively training and evaluating all available models.},
	language = {en},
	urldate = {2023-03-31},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Achille, Alessandro and Lam, Michael and Tewari, Rahul and Ravichandran, Avinash and Maji, Subhransu and Fowlkes, Charless and Soatto, Stefano and Perona, Pietro},
	month = oct,
	year = {2019},
	pages = {6429--6438},
}

@inproceedings{shu_zoo-tuning_2021,
	title = {Zoo-{Tuning}: {Adaptive} {Transfer} from {A} {Zoo} of {Models}},
	shorttitle = {Zoo-{Tuning}},
	url = {https://proceedings.mlr.press/v139/shu21b.html},
	abstract = {With the development of deep networks on various large-scale datasets, a large zoo of pretrained models are available. When transferring from a model zoo, applying classic single-model-based transfer learning methods to each source model suffers from high computational cost and cannot fully utilize the rich knowledge in the zoo. We propose {\textbackslash}emph\{Zoo-Tuning\} to address these challenges, which learns to adaptively transfer the parameters of pretrained models to the target task. With the learnable channel alignment layer and adaptive aggregation layer, Zoo-Tuning {\textbackslash}emph\{adaptively aggregates channel aligned pretrained parameters to derive the target model\}, which simultaneously promotes knowledge transfer and adapts source models to downstream tasks. The adaptive aggregation substantially reduces the computation cost at both training and inference. We further propose lite Zoo-Tuning with the temporal ensemble of batch average gating values to reduce the storage cost at the inference time. We evaluate our approach on a variety of tasks, including reinforcement learning, image classification, and facial landmark detection. Experiment results demonstrate that the proposed adaptive transfer learning approach can more effectively and efficiently transfer knowledge from a zoo of models.},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shu, Yang and Kou, Zhi and Cao, Zhangjie and Wang, Jianmin and Long, Mingsheng},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {9626--9637},
}

@inproceedings{shu_hub-pathway_2022,
	title = {Hub-{Pathway}: {Transfer} {Learning} from {A} {Hub} of {Pre}-trained {Models}},
	shorttitle = {Hub-{Pathway}},
	url = {https://openreview.net/forum?id=L8ESR8IQ7Gb},
	abstract = {Transfer learning aims to leverage knowledge from pre-trained models to benefit the target task. Prior transfer learning work mainly transfers from a single model. However, with the emergence of deep models pre-trained from different resources, model hubs consisting of diverse models with various architectures, pre-trained datasets and learning paradigms are available. Directly applying single-model transfer learning methods to each model wastes the abundant knowledge of the model hub and suffers from high computational cost. In this paper, we propose a Hub-Pathway framework to enable knowledge transfer from a model hub. The framework generates data-dependent pathway weights, based on which we assign the pathway routes at the input level to decide which pre-trained models are activated and passed through, and then set the pathway aggregation at the output level to aggregate the knowledge from different models to make predictions. The proposed framework can be trained end-to-end with the target task-specific loss, where it learns to explore better pathway configurations and exploit the knowledge in pre-trained models for each target datum. We utilize a noisy pathway generator and design an exploration loss to further explore different pathways throughout the model hub. To fully exploit the knowledge in pre-trained models, each model is further trained by specific data that activate it, which ensures its performance and enhances knowledge transfer. Experiment results on computer vision and reinforcement learning tasks demonstrate that the proposed Hub-Pathway framework achieves the state-of-the-art performance for model hub transfer learning.},
	language = {en},
	urldate = {2023-03-30},
	author = {Shu, Yang and Cao, Zhangjie and Zhang, Ziyang and Wang, Jianmin and Long, Mingsheng},
	month = oct,
	year = {2022},
}

@misc{dong_zood_2022,
	title = {{ZooD}: {Exploiting} {Model} {Zoo} for {Out}-of-{Distribution} {Generalization}},
	shorttitle = {{ZooD}},
	url = {http://arxiv.org/abs/2210.09236},
	abstract = {Recent advances on large-scale pre-training have shown great potentials of leveraging a large set of Pre-Trained Models (PTMs) for improving Out-of-Distribution (OoD) generalization, for which the goal is to perform well on possible unseen domains after fine-tuning on multiple training domains. However, maximally exploiting a zoo of PTMs is challenging since fine-tuning all possible combinations of PTMs is computationally prohibitive while accurate selection of PTMs requires tackling the possible data distribution shift for OoD tasks. In this work, we propose ZooD, a paradigm for PTMs ranking and ensemble with feature selection. Our proposed metric ranks PTMs by quantifying inter-class discriminability and inter-domain stability of the features extracted by the PTMs in a leave-one-domain-out cross-validation manner. The top-K ranked models are then aggregated for the target OoD task. To avoid accumulating noise induced by model ensemble, we propose an efficient variational EM algorithm to select informative features. We evaluate our paradigm on a diverse model zoo consisting of 35 models for various OoD tasks and demonstrate: (i) model ranking is better correlated with fine-tuning ranking than previous methods and up to 9859x faster than brute-force fine-tuning; (ii) OoD generalization after model ensemble with feature selection outperforms the state-of-the-art methods and the accuracy on most challenging task DomainNet is improved from 46.5{\textbackslash}\% to 50.6{\textbackslash}\%. Furthermore, we provide the fine-tuning results of 35 PTMs on 7 OoD datasets, hoping to help the research of model zoo and OoD generalization. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/zood.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Dong, Qishi and Muhammad, Awais and Zhou, Fengwei and Xie, Chuanlong and Hu, Tianyang and Yang, Yongxin and Bae, Sung-Ho and Li, Zhenguo},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09236 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{poncelet_so_2022,
	address = {Rochester MI USA},
	title = {So {Many} {Fuzzers}, {So} {Little} {Time}✱: {Experience} from {Evaluating} {Fuzzers} on the {Contiki}-{NG} {Network} ({Hay}){Stack}},
	isbn = {978-1-4503-9475-8},
	shorttitle = {So {Many} {Fuzzers}, {So} {Little} {Time}✱},
	url = {https://dl.acm.org/doi/10.1145/3551349.3556946},
	doi = {10.1145/3551349.3556946},
	language = {en},
	urldate = {2023-03-29},
	booktitle = {37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Poncelet, Clement and Sagonas, Konstantinos and Tsiftes, Nicolas},
	month = oct,
	year = {2022},
	pages = {1--12},
}

@inproceedings{yang_factorizing_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Factorizing {Knowledge} in {Neural} {Networks}},
	isbn = {978-3-031-19830-4},
	doi = {10.1007/978-3-031-19830-4_5},
	abstract = {In this paper, we explore a novel and ambitious knowledge-transfer task, termed Knowledge Factorization (KF). The core idea of KF lies in the modularization and assemblability of knowledge: given a pretrained network model as input, KF aims to decompose it into several factor networks, each of which handles only a dedicated task and maintains task-specific knowledge factorized from the source network. Such factor networks are task-wise disentangled and can be directly assembled, without any fine-tuning, to produce the more competent combined-task networks. In other words, the factor networks serve as Lego-brick-like building blocks, allowing us to construct customized networks in a plug-and-play manner. Specifically, each factor network comprises two modules, a common-knowledge module that is task-agnostic and shared by all factor networks, alongside with a task-specific module dedicated to the factor network itself. We introduce an information-theoretic objective, InfoMax-Bottleneck (IMB), to carry out KF by optimizing the mutual information between the learned representations and input. Experiments across various benchmarks demonstrate that, the derived factor networks yield gratifying performances on not only the dedicated tasks but also disentanglement, while enjoying much better interpretability and modularity. Moreover, the learned common-knowledge representations give rise to impressive results on transfer learning. Our code is available at https://github.com/Adamdad/KnowledgeFactor.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Yang, Xingyi and Ye, Jingwen and Wang, Xinchao},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Knowledge factorization, Transfer learning},
	pages = {73--91},
}

@inproceedings{matsoukas_what_2022,
	address = {New Orleans, LA, USA},
	title = {What {Makes} {Transfer} {Learning} {Work} for {Medical} {Images}: {Feature} {Reuse} \& {Other} {Factors}},
	isbn = {978-1-66546-946-3},
	shorttitle = {What {Makes} {Transfer} {Learning} {Work} for {Medical} {Images}},
	url = {https://ieeexplore.ieee.org/document/9878482/},
	doi = {10.1109/CVPR52688.2022.00901},
	abstract = {Transfer learning is a standard technique to transfer knowledge from one domain to another. For applications in medical imaging, transfer from ImageNet has become the de-facto approach, despite differences in the tasks and image characteristics between the domains. However, it is unclear what factors determine whether – and to what extent –transfer learning to the medical domain is useful. The longstanding assumption that features from the source domain get reused has recently been called into question. Through a series of experiments on several medical image benchmark datasets, we explore the relationship between transfer learning, data size, the capacity and inductive bias of the model, as well as the distance between the source and target domain. Our ﬁndings suggest that transfer learning is beneﬁcial in most cases, and we characterize the important role feature reuse plays in its success.},
	language = {en},
	urldate = {2023-03-29},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Matsoukas, Christos and Haslum, Johan Fredin and Sorkhei, Moein and Soderberg, Magnus and Smith, Kevin},
	month = jun,
	year = {2022},
	pages = {9215--9224},
}

@misc{pfeiffer_modular_2023,
	title = {Modular {Deep} {Learning}},
	url = {http://arxiv.org/abs/2302.11529},
	abstract = {Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Pfeiffer, Jonas and Ruder, Sebastian and Vulić, Ivan and Ponti, Edoardo Maria},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11529 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{miller_adversarial_2020,
	title = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}: {A} {Comprehensive} {Review} of {Defenses} {Against} {Attacks}},
	volume = {108},
	issn = {1558-2256},
	shorttitle = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}},
	doi = {10.1109/JPROC.2020.2970615},
	abstract = {With wide deployment of machine learning (ML)-based systems for a variety of applications including medical, military, automotive, genomic, multimedia, and social networking, there is great potential for damage from adversarial learning (AL) attacks. In this article, we provide a contemporary survey of AL, focused particularly on defenses against attacks on deep neural network classifiers. After introducing relevant terminology and the goals and range of possible knowledge of both attackers and defenders, we survey recent work on test-time evasion (TTE), data poisoning (DP), backdoor DP, and reverse engineering (RE) attacks and particularly defenses against the same. In so doing, we distinguish robust classification from anomaly detection (AD), unsupervised from supervised, and statistical hypothesis-based defenses from ones that do not have an explicit null (no attack) hypothesis. We also consider several scenarios for detecting backdoors. We provide a technical assessment for reviewed works, including identifying any issues/limitations, required hyperparameters, needed computational complexity, as well as the performance measures evaluated and the obtained quality. We then delve deeper, providing novel insights that challenge conventional AL wisdom and that target unresolved issues, including: robust classification versus AD as a defense strategy; the belief that attack success increases with attack strength, which ignores susceptibility to AD; small perturbations for TTE attacks: a fallacy or a requirement; validity of the universal assumption that a TTE attacker knows the ground-truth class for the example to be attacked; black, gray, or white-box attacks as the standard for defense evaluation; and susceptibility of query-based RE to an AD defense. We also discuss attacks on the privacy of training data. We then present benchmark comparisons of several defenses against TTE, RE, and backdoor DP attacks on images. The article concludes with a discussion of continuing research directions, including the supreme challenge of detecting attacks whose goal is not to alter classification decisions, but rather simply to embed, without detection, “fake news” or other false content.},
	number = {3},
	journal = {Proceedings of the IEEE},
	author = {Miller, David J. and Xiang, Zhen and Kesidis, George},
	month = mar,
	year = {2020},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Adversarial machine learning, Anomaly detection (AD), Feature extraction, Informatics, Machine learning, Neural networks, Reverse engineering, Robustness, Social networking (online), Training data, backdoor, black box, data poisoning (DP), deep neural networks (DNNs), membership inference attack, reverse engineering (RE), robust classification, targeted attacks, test-time-evasion (TTE), transferability, white box},
	pages = {402--433},
}

@article{miller_adversarial_2020-1,
	title = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}: {A} {Comprehensive} {Review} of {Defenses} {Against} {Attacks}},
	volume = {108},
	issn = {1558-2256},
	shorttitle = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}},
	doi = {10.1109/JPROC.2020.2970615},
	abstract = {With wide deployment of machine learning (ML)-based systems for a variety of applications including medical, military, automotive, genomic, multimedia, and social networking, there is great potential for damage from adversarial learning (AL) attacks. In this article, we provide a contemporary survey of AL, focused particularly on defenses against attacks on deep neural network classifiers. After introducing relevant terminology and the goals and range of possible knowledge of both attackers and defenders, we survey recent work on test-time evasion (TTE), data poisoning (DP), backdoor DP, and reverse engineering (RE) attacks and particularly defenses against the same. In so doing, we distinguish robust classification from anomaly detection (AD), unsupervised from supervised, and statistical hypothesis-based defenses from ones that do not have an explicit null (no attack) hypothesis. We also consider several scenarios for detecting backdoors. We provide a technical assessment for reviewed works, including identifying any issues/limitations, required hyperparameters, needed computational complexity, as well as the performance measures evaluated and the obtained quality. We then delve deeper, providing novel insights that challenge conventional AL wisdom and that target unresolved issues, including: robust classification versus AD as a defense strategy; the belief that attack success increases with attack strength, which ignores susceptibility to AD; small perturbations for TTE attacks: a fallacy or a requirement; validity of the universal assumption that a TTE attacker knows the ground-truth class for the example to be attacked; black, gray, or white-box attacks as the standard for defense evaluation; and susceptibility of query-based RE to an AD defense. We also discuss attacks on the privacy of training data. We then present benchmark comparisons of several defenses against TTE, RE, and backdoor DP attacks on images. The article concludes with a discussion of continuing research directions, including the supreme challenge of detecting attacks whose goal is not to alter classification decisions, but rather simply to embed, without detection, “fake news” or other false content.},
	number = {3},
	journal = {Proceedings of the IEEE},
	author = {Miller, David J. and Xiang, Zhen and Kesidis, George},
	month = mar,
	year = {2020},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Adversarial machine learning, Anomaly detection (AD), Feature extraction, Informatics, Machine learning, Neural networks, Reverse engineering, Robustness, Social networking (online), Training data, backdoor, black box, data poisoning (DP), deep neural networks (DNNs), membership inference attack, reverse engineering (RE), robust classification, targeted attacks, test-time-evasion (TTE), transferability, white box},
	pages = {402--433},
}

@book{wu_chatgpt_2023,
	title = {{ChatGPT} or {Grammarly}? {Evaluating} {ChatGPT} on {Grammatical} {Error} {Correction} {Benchmark}},
	shorttitle = {{ChatGPT} or {Grammarly}?},
	abstract = {ChatGPT is a cutting-edge artificial intelligence language model developed by OpenAI, which has attracted a lot of attention due to its surprisingly strong ability in answering follow-up questions. In this report, we aim to evaluate ChatGPT on the Grammatical Error Correction (GEC) task, and compare it with commercial GEC product (e.g., Grammarly) and state-of-the-art models (e.g., GECToR). By testing on the CoNLL2014 benchmark dataset, we find that ChatGPT performs not as well as those baselines in terms of the automatic evaluation metrics (e.g., F0.5 score), particularly on long sentences. We inspect the outputs and find that ChatGPT goes beyond one-by-one corrections. Specifically, it prefers to change the surface expression of certain phrases or sentence structure while maintaining grammatical correctness. Human evaluation quantitatively confirms this and suggests that ChatGPT produces less under-correction or mis-correction issues but more over-corrections. These results demonstrate that ChatGPT is severely underestimated by the automatic evaluation metrics and could be a promising tool for GEC.},
	author = {Wu, Haoran and Wang, Wenxuan and Wan, Yuxuan and Jiao, Wenxiang and Lyu, Michael},
	month = mar,
	year = {2023},
}

@inproceedings{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	volume = {29},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female.  Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	urldate = {2023-03-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	year = {2016},
}

@inproceedings{yang_answer_2022,
	address = {Rochester MI USA},
	title = {Answer {Summarization} for {Technical} {Queries}: {Benchmark} and {New} {Approach}},
	isbn = {978-1-4503-9475-8},
	shorttitle = {Answer {Summarization} for {Technical} {Queries}},
	url = {https://dl.acm.org/doi/10.1145/3551349.3560421},
	doi = {10.1145/3551349.3560421},
	abstract = {Prior studies have demonstrated that approaches to generate an answer summary for a given technical query in Software Question and Answer (SQA) sites are desired. We find that existing approaches are assessed solely through user studies. Hence, a new user study needs to be performed every time a new approach is introduced; this is time-consuming, slows down the development of the new approach, and results from different user studies may not be comparable to each other. There is a need for a benchmark with ground truth summaries as a complement assessment through user studies. Unfortunately, such a benchmark is non-existent for answer summarization for technical queries from SQA sites.},
	language = {en},
	urldate = {2023-03-23},
	booktitle = {37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Yang, Chengran and Xu, Bowen and Thung, Ferdian and Shi, Yucen and Zhang, Ting and Yang, Zhou and Zhou, Xin and Shi, Jieke and He, Junda and Han, Donggyun and Lo, David},
	month = oct,
	year = {2022},
	pages = {1--13},
}

@inproceedings{he_ptm4tag_2022,
	address = {Virtual Event},
	title = {{PTM4Tag}: sharpening tag recommendation of stack overflow posts with pre-trained models},
	isbn = {978-1-4503-9298-3},
	shorttitle = {{PTM4Tag}},
	url = {https://dl.acm.org/doi/10.1145/3524610.3527897},
	doi = {10.1145/3524610.3527897},
	language = {en},
	urldate = {2023-03-23},
	booktitle = {Proceedings of the 30th {IEEE}/{ACM} {International} {Conference} on {Program} {Comprehension}},
	publisher = {ACM},
	author = {He, Junda and Xu, Bowen and Yang, Zhou and Han, DongGyun and Yang, Chengran and Lo, David},
	month = may,
	year = {2022},
	pages = {1--11},
}

@inproceedings{luong_arseek_2022,
	address = {Virtual Event},
	title = {{ARSeek}: identifying {API} resource using code and discussion on stack overflow},
	isbn = {978-1-4503-9298-3},
	shorttitle = {{ARSeek}},
	url = {https://dl.acm.org/doi/10.1145/3524610.3527918},
	doi = {10.1145/3524610.3527918},
	language = {en},
	urldate = {2023-03-23},
	booktitle = {Proceedings of the 30th {IEEE}/{ACM} {International} {Conference} on {Program} {Comprehension}},
	publisher = {ACM},
	author = {Luong, Kien and Hadi, Mohammad and Thung, Ferdian and Fard, Fatemeh and Lo, David},
	month = may,
	year = {2022},
	pages = {331--342},
}

@inproceedings{luong_arsearch_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{ARSearch}: searching for {API} related resources from stack overflow and {GitHub}},
	isbn = {978-1-4503-9223-5},
	shorttitle = {{ARSearch}},
	url = {https://dl.acm.org/doi/10.1145/3510454.3517048},
	doi = {10.1145/3510454.3517048},
	language = {en},
	urldate = {2023-03-23},
	booktitle = {Proceedings of the {ACM}/{IEEE} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {ACM},
	author = {Luong, Kien and Thung, Ferdian and Lo, David},
	month = may,
	year = {2022},
	pages = {11--15},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{robbes_how_2012,
	address = {Cary North Carolina},
	title = {How do developers react to {API} deprecation?: the case of a smalltalk ecosystem},
	isbn = {978-1-4503-1614-9},
	shorttitle = {How do developers react to {API} deprecation?},
	url = {https://dl.acm.org/doi/10.1145/2393596.2393662},
	doi = {10.1145/2393596.2393662},
	language = {en},
	urldate = {2023-03-16},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Robbes, Romain and Lungu, Mircea and Röthlisberger, David},
	month = nov,
	year = {2012},
	pages = {1--11},
}

@article{gasparic_what_2016,
	title = {What recommendation systems for software engineering recommend: {A} systematic literature review},
	volume = {113},
	issn = {01641212},
	shorttitle = {What recommendation systems for software engineering recommend},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121215002605},
	doi = {10.1016/j.jss.2015.11.036},
	abstract = {Background. A recommendation system for software engineering (RSSE) is a software application that provides information items estimated to be valuable for a software engineering task in a given context.
Objective. Present the results of a systematic literature review to reveal the typical functionality oﬀered by existing RSSEs, research gaps, and possible research directions.
Method. We evaluated 46 papers studying the beneﬁts, the data requirements, the information and recommendation types, and the eﬀort requirements of RSSE systems. We include papers describing tools that support source code related development published between 2003 and 2013.
Results. The results show that RSSEs typically visualize source code artifacts. They aim to improve system quality, make the development process more eﬃcient and less expensive, lower developer’s cognitive load, and help developers to make better decisions. They mainly support reuse actions and debugging, implementation, and maintenance phases. The majority of the systems are reactive.
Conclusions. Unexploited opportunities lie in the development of recommender systems outside the source code domain. Furthermore, current RSSE systems use very limited context information and rely on simple models. Contextadapted and proactive behaviour could improve the acceptance of RSSE systems in practice.},
	language = {en},
	urldate = {2023-03-16},
	journal = {Journal of Systems and Software},
	author = {Gasparic, Marko and Janes, Andrea},
	month = mar,
	year = {2016},
	pages = {101--113},
}

@inproceedings{shao_not_2022,
	title = {Not {All} {Models} {Are} {Equal}: {Predicting} {Model} {Transferability} in a {Self}-challenging {Fisher} {Space}},
	shorttitle = {Not {All} {Models} {Are} {Equal}},
	abstract = {This paper addresses an important problem of ranking the pre-trained deep neural networks and screening the most transferable ones for downstream tasks. It is challenging because the ground-truth model ranking for each task can only be generated by fine-tuning the pre-trained models on the target dataset, which is brute-force and computationally expensive. Recent advanced methods proposed several lightweight transferability metrics to predict the fine-tuning results. However, these approaches only capture static representations but neglect the fine-tuning dynamics. To this end, this paper proposes a new transferability metric, called {\textbackslash}textbf\{S\}elf-challenging {\textbackslash}textbf\{F\}isher {\textbackslash}textbf\{D\}iscriminant {\textbackslash}textbf\{A\}nalysis ({\textbackslash}textbf\{SFDA\}), which has many appealing benefits that existing works do not have. First, SFDA can embed the static features into a Fisher space and refine them for better separability between classes. Second, SFDA uses a self-challenging mechanism to encourage different pre-trained models to differentiate on hard examples. Third, SFDA can easily select multiple pre-trained models for the model ensemble. Extensive experiments on \$33\$ pre-trained models of \$11\$ downstream tasks show that SFDA is efficient, effective, and robust when measuring the transferability of pre-trained models. For instance, compared with the state-of-the-art method NLEEP, SFDA demonstrates an average of \$59.1\${\textbackslash}\% gain while bringing \$22.5\$x speedup in wall-clock time. The code will be available at {\textbackslash}url\{https://github.com/TencentARC/SFDA\}.},
	urldate = {2022-09-26},
	booktitle = {The {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer},
	author = {Shao, Wenqi and Zhao, Xun and Ge, Yixiao and Zhang, Zhaoyang and Yang, Lei and Wang, Xiaogang and Shan, Ying and Luo, Ping},
	month = jul,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{neyshabur_what_2020,
	title = {What is being transferred in transfer learning?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html},
	abstract = {One desired capability for machines is the ability to transfer their understanding of one domain to another domain where data is (usually) scarce. Despite ample adaptation of transfer learning in many deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analysis to address these fundamental questions. Through a series of analysis on transferring to block-shuffled images, we separate the effect of feature reuse from learning high-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.},
	urldate = {2023-03-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
	year = {2020},
	pages = {512--523},
}

@article{zeng_knowledge_2021,
	title = {Knowledge {Transfer} via {Pre}-training for {Recommendation}: {A} {Review} and {Prospect}},
	volume = {4},
	issn = {2624-909X},
	shorttitle = {Knowledge {Transfer} via {Pre}-training for {Recommendation}},
	url = {https://www.frontiersin.org/articles/10.3389/fdata.2021.602071/full},
	doi = {10.3389/fdata.2021.602071},
	abstract = {Recommender systems aim to provide item recommendations for users and are usually faced with data sparsity problems (e.g., cold start) in real-world scenarios. Recently pretrained models have shown their effectiveness in knowledge transfer between domains and tasks, which can potentially alleviate the data sparsity problem in recommender systems. In this survey, we ﬁrst provide a review of recommender systems with pretraining. In addition, we show the beneﬁts of pre-training to recommender systems through experiments. Finally, we discuss several promising directions for future research of recommender systems with pre-training. The source code of our experiments will be available to facilitate future research.},
	language = {en},
	urldate = {2023-03-16},
	journal = {Frontiers in Big Data},
	author = {Zeng, Zheni and Xiao, Chaojun and Yao, Yuan and Xie, Ruobing and Liu, Zhiyuan and Lin, Fen and Lin, Leyu and Sun, Maosong},
	month = mar,
	year = {2021},
	pages = {602071},
}

@misc{liu_pre-train_2023,
	title = {Pre-train, {Prompt} and {Recommendation}: {A} {Comprehensive} {Survey} of {Language} {Modelling} {Paradigm} {Adaptations} in {Recommender} {Systems}},
	shorttitle = {Pre-train, {Prompt} and {Recommendation}},
	url = {http://arxiv.org/abs/2302.03735},
	abstract = {The emergency of Pre-trained Language Models (PLMs) has achieved tremendous success in the field of Natural Language Processing (NLP) by learning universal representations on large corpora in a self-supervised manner. The pre-trained models and the learned representations can be beneficial to a series of downstream NLP tasks. This training paradigm has recently been adapted to the recommendation domain and is considered a promising approach by both academia and industry. In this paper, we systematically investigate how to extract and transfer knowledge from pre-trained models learned by different PLM-related training paradigms to improve recommendation performance from various perspectives, such as generality, sparsity, efficiency and effectiveness. Specifically, we propose an orthogonal taxonomy to divide existing PLM-based recommender systems w.r.t. their training strategies and objectives. Then, we analyze and summarize the connection between PLM-based training paradigms and different input data types for recommender systems. Finally, we elaborate on open issues and future research directions in this vibrant field.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Liu, Peng and Zhang, Lemei and Gulla, Jon Atle},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03735 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@incollection{avidan_broad_2022,
	address = {Cham},
	title = {A {Broad} {Study} of {Pre}-training for {Domain} {Generalization} and {Adaptation}},
	volume = {13693},
	isbn = {978-3-031-19826-7 978-3-031-19827-4},
	url = {https://link.springer.com/10.1007/978-3-031-19827-4_36},
	abstract = {Deep models must learn robust and transferable representations in order to perform well on new domains. While domain transfer methods (e.g., domain adaptation, domain generalization) have been proposed to learn transferable representations across domains, they are typically applied to ResNet backbones pre-trained on ImageNet. Thus, existing works pay little attention to the eﬀects of pre-training on domain transfer tasks. In this paper, we provide a broad study and in-depth analysis of pre-training for domain adaptation and generalization, namely: network architectures, size, pre-training loss, and datasets. We observe that simply using a state-of-the-art backbone outperforms existing stateof-the-art domain adaptation baselines and set new baselines on OﬃceHome and DomainNet improving by 10.7\% and 5.5\%. We hope that this work can provide more insights for future domain transfer research.},
	language = {en},
	urldate = {2023-03-16},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Kim, Donghyun and Wang, Kaihong and Sclaroff, Stan and Saenko, Kate},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19827-4_36},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {621--638},
}

@inproceedings{wu_empowering_2021,
	address = {Virtual Event Canada},
	title = {Empowering {News} {Recommendation} with {Pre}-trained {Language} {Models}},
	isbn = {978-1-4503-8037-9},
	url = {https://dl.acm.org/doi/10.1145/3404835.3463069},
	doi = {10.1145/3404835.3463069},
	language = {en},
	urldate = {2023-03-16},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng},
	month = jul,
	year = {2021},
	pages = {1652--1656},
}

@article{vilalta_perspective_nodate,
	title = {A {Perspective} {View} and {Survey} of {Meta}-{Learning}},
	abstract = {Different researchers hold different views of what the term meta-learning exactly means. The ﬁrst part of this paper provides our own perspective view in which the goal is to build self-adaptive learners (i.e. learning algorithms that improve their bias dynamically through experience by accumulating meta-knowledge). The second part provides a survey of meta-learning as reported by the machine-learning literature. We ﬁnd that, despite different views and research lines, a question remains constant: how can we exploit knowledge about learning (i.e. meta-knowledge) to improve the performance of learning algorithms? Clearly the answer to this question is key to the advancement of the ﬁeld and continues being the subject of intensive research.},
	language = {en},
	author = {Vilalta, Ricardo and Drissi, Youssef},
}

@inproceedings{zhu_pass_2022,
	address = {Dublin, Ireland},
	title = {Pass off {Fish} {Eyes} for {Pearls}: {Attacking} {Model} {Selection} of {Pre}-trained {Models}},
	shorttitle = {Pass off {Fish} {Eyes} for {Pearls}},
	url = {https://aclanthology.org/2022.acl-long.347},
	doi = {10.18653/v1/2022.acl-long.347},
	abstract = {Selecting an appropriate pre-trained model (PTM) for a speciﬁc downstream task typically requires signiﬁcant efforts of ﬁne-tuning. To accelerate this process, researchers propose feature-based model selection (FMS) methods, which assess PTMs’ transferability to a speciﬁc task in a fast way without ﬁne-tuning. In this work, we argue that current FMS methods are vulnerable, as the assessment mainly relies on the static features extracted from PTMs. However, such features are derived without training PTMs on downstream tasks, and are not necessarily reliable indicators for the PTM’s transferability. To validate our viewpoints, we design two methods to evaluate the robustness of FMS: (1) model disguise attack, which post-trains an inferior PTM with a contrastive objective, and (2) evaluation data selection, which selects a subset of the data points for FMS evaluation based on K-means clustering. Experimental results prove that both methods can successfully make FMS mistakenly judge the transferability of PTMs. Moreover, we ﬁnd that these two methods can further be combined with the backdoor attack to misguide the FMS to select poisoned models. To the best of our knowledge, this is the ﬁrst work to demonstrate the defects of current FMS algorithms and evaluate their potential security risks. By identifying previously unseen risks of FMS, our study indicates new directions for improving the robustness of FMS.},
	language = {en},
	urldate = {2023-03-13},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Biru and Qin, Yujia and Qi, Fanchao and Deng, Yangdong and Liu, Zhiyuan and Sun, Maosong and Gu, Ming},
	year = {2022},
	pages = {5060--5072},
}

@misc{mazumder_dataperf_2022,
	title = {{DataPerf}: {Benchmarks} for {Data}-{Centric} {AI} {Development}},
	shorttitle = {{DataPerf}},
	url = {http://arxiv.org/abs/2207.10062},
	abstract = {Machine learning (ML) research has generally focused on models, while the most prominent datasets have been employed for everyday ML tasks without regard for the breadth, difficulty, and faithfulness of these datasets to the underlying problem. Neglecting the fundamental importance of datasets has caused major problems involving data cascades in real-world applications and saturation of dataset-driven criteria for model quality, hindering research growth. To solve this problem, we present DataPerf, a benchmark package for evaluating ML datasets and dataset-working algorithms. We intend it to enable the "data ratchet," in which training sets will aid in evaluating test sets on the same problems, and vice versa. Such a feedback-driven strategy will generate a virtuous loop that will accelerate development of data-centric AI. The MLCommons Association will maintain DataPerf.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Mazumder, Mark and Banbury, Colby and Yao, Xiaozhe and Karlaš, Bojan and Rojas, William Gaviria and Diamos, Sudnya and Diamos, Greg and He, Lynn and Kiela, Douwe and Jurado, David and Kanter, David and Mosquera, Rafael and Ciro, Juan and Aroyo, Lora and Acun, Bilge and Eyuboglu, Sabri and Ghorbani, Amirata and Goodman, Emmett and Kane, Tariq and Kirkpatrick, Christine R. and Kuo, Tzu-Sheng and Mueller, Jonas and Thrush, Tristan and Vanschoren, Joaquin and Warren, Margaret and Williams, Adina and Yeung, Serena and Ardalani, Newsha and Paritosh, Praveen and Zhang, Ce and Zou, James and Wu, Carole-Jean and Coleman, Cody and Ng, Andrew and Mattson, Peter and Reddi, Vijay Janapa},
	month = jul,
	year = {2022},
	note = {arXiv:2207.10062 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{iyyer_deep_2015,
	address = {Beijing, China},
	title = {Deep {Unordered} {Composition} {Rivals} {Syntactic} {Methods} for {Text} {Classification}},
	url = {https://aclanthology.org/P15-1162},
	doi = {10.3115/v1/P15-1162},
	urldate = {2023-03-13},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Iyyer, Mohit and Manjunatha, Varun and Boyd-Graber, Jordan and Daumé III, Hal},
	month = jul,
	year = {2015},
	pages = {1681--1691},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {{GloVe}},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2023-03-13},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	month = oct,
	year = {2014},
	pages = {1532--1543},
}

@misc{pilehvar_wic_2019,
	title = {{WiC}: the {Word}-in-{Context} {Dataset} for {Evaluating} {Context}-{Sensitive} {Meaning} {Representations}},
	shorttitle = {{WiC}},
	url = {http://arxiv.org/abs/1808.09121},
	abstract = {By design, word embeddings are unable to model the dynamic nature of words' semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
	month = apr,
	year = {2019},
	note = {arXiv:1808.09121 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wu_visual_2023,
	title = {Visual {ChatGPT}: {Talking}, {Drawing} and {Editing} with {Visual} {Foundation} {Models}},
	shorttitle = {Visual {ChatGPT}},
	url = {http://arxiv.org/abs/2303.04671},
	abstract = {ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called {\textbackslash}textbf\{Visual ChatGPT\}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at {\textbackslash}url\{https://github.com/microsoft/visual-chatgpt\}.},
	urldate = {2023-03-12},
	publisher = {arXiv},
	author = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04671 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{soni_evaluation_2020,
	address = {Marseille, France},
	title = {Evaluation of {Dataset} {Selection} for {Pre}-{Training} and {Fine}-{Tuning} {Transformer} {Language} {Models} for {Clinical} {Question} {Answering}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.679},
	abstract = {We evaluate the performance of various Transformer language models, when pre-trained and fine-tuned on different combinations of open-domain, biomedical, and clinical corpora on two clinical question answering (QA) datasets (CliCR and emrQA). We perform our evaluations on the task of machine reading comprehension, which involves training the model to answer a question given an unstructured context paragraph. We conduct a total of 48 experiments on different combinations of the large open-domain and domain-specific corpora. We found that an initial fine-tuning on an open-domain dataset, SQuAD, consistently improves the clinical QA performance across all the model variants.},
	language = {English},
	urldate = {2023-03-11},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Soni, Sarvesh and Roberts, Kirk},
	month = may,
	year = {2020},
	pages = {5532--5538},
}

@misc{deshpande_linearized_2021,
	title = {A linearized framework and a new benchmark for model selection for fine-tuning},
	url = {http://arxiv.org/abs/2102.00084},
	abstract = {Fine-tuning from a collection of models pre-trained on different domains (a "model zoo") is emerging as a technique to improve test accuracy in the low-data regime. However, model selection, i.e. how to pre-select the right model to fine-tune from a model zoo without performing any training, remains an open topic. We use a linearized framework to approximate fine-tuning, and introduce two new baselines for model selection -- Label-Gradient and Label-Feature Correlation. Since all model selection algorithms in the literature have been tested on different use-cases and never compared directly, we introduce a new comprehensive benchmark for model selection comprising of: i) A model zoo of single and multi-domain models, and ii) Many target tasks. Our benchmark highlights accuracy gain with model zoo compared to fine-tuning Imagenet models. We show our model selection baseline can select optimal models to fine-tune in few selections and has the highest ranking correlation to fine-tuning accuracy compared to existing algorithms.},
	urldate = {2023-03-11},
	publisher = {arXiv},
	author = {Deshpande, Aditya and Achille, Alessandro and Ravichandran, Avinash and Li, Hao and Zancato, Luca and Fowlkes, Charless and Bhotika, Rahul and Soatto, Stefano and Perona, Pietro},
	month = jan,
	year = {2021},
	note = {arXiv:2102.00084 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{Dota2019,
	title = {Dota 2 with {Large} {Scale} {Deep} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1912.06680},
	abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
	journal = {arXiv preprint arXiv:1912.06680},
	author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemysław and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
	year = {2019},
}

@article{Mckeeman1998DifferentialTesting,
	title = {Differential {Testing} for {Software}.},
	abstract = {Differential testing, a form of random testing, is a component of a mature testing technology for large software systems. It complements regression testing based on commercial test suites and locally developed tests During prod- uct development and deployment. Differential que testing requires two or more comparable systems be available to the tester. These sys- tems are presented with an exhaustive series of mechanically generated test cases. If (w might say when) the results differ or one of the systems loops indefinitely or crashes, the tester has a candidate for a bug-exposing test. Implementing differential testing is an interest- ing technical problem. Getting it into use is an even more interesting social challenge. This paper is derived from experience in differential testing of compilers and run-time systems at DIGITAL over the last few years and recently at Compaq. A working prototype for testing C compilers is available on the web.},
	journal = {Digital Technical Journal},
	author = {Mckeeman, William M.},
	year = {1998},
	keywords = {Computer architecture, Computer science, Dynamic testing, Integration testing, Manual testing, Non-regression testing, Regression testing, Software performance testing, Software reliability testing, Software system},
}

@misc{Wu2016,
	title = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1609.08144},
	publisher = {arXiv},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Łukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	year = {2016},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{SIGSOFT2020EmpiricalStandards4SEResearch,
	title = {Empirical {Standards} for {Software} {Engineering} {Research}},
	url = {https://arxiv.org/abs/2010.03525},
	abstract = {Empirical Standards are natural-language models of a scientific community's expectations for a specific kind of study (e.g. a questionnaire survey). The ACM SIGSOFT Paper and Peer Review Quality Initiative generated empirical standards for research methods commonly used in software engineering. These living documents, which should be continuously revised to reflect evolving consensus around research best practices, will improve research quality and make peer review more effective, reliable, transparent and fair.},
	journal = {arXiv},
	author = {Ralph, Paul and Ali, Nauman bin and Baltes, Sebastian and Bianculli, Domenico and Diaz, Jessica and Dittrich, Yvonne and Ernst, Neil and Felderer, Michael and Feldt, Robert and Filieri, Antonio and de França, Breno Bernard Nicolau and Furia, Carlo Alberto and Gay, Greg and Gold, Nicolas and Graziotin, Daniel and He, Pinjia and Hoda, Rashina and Juristo, Natalia and Kitchenham, Barbara and Lenarduzzi, Valentina and Martínez, Jorge and Melegati, Jorge and Mendez, Daniel and Menzies, Tim and Molleri, Jefferson and Pfahl, Dietmar and Robbes, Romain and Russo, Daniel and Saarimäki, Nyyti and Sarro, Federica and Taibi, Davide and Siegmund, Janet and Spinellis, Diomidis and Staron, Miroslaw and Stol, Klaas and Storey, Margaret-Anne and Taibi, Davide and Tamburri, Damian and Torchiano, Marco and Treude, Christoph and Turhan, Burak and Wang, Xiaofeng and Vegas, Sira},
	year = {2021},
	keywords = {Computer Science - General Literature, Computer Science - Software Engineering},
}

@inproceedings{Garcia2020AVBugs,
	title = {A comprehensive study of autonomous vehicle bugs},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380397},
	abstract = {Self-driving cars, or Autonomous Vehicles (AVs), are increasingly becoming an integral part of our daily life. About 50 corporations are actively working on AVs, including large companies such as Google, Ford, and Intel. Some AVs are already operating on public roads, with at least one unfortunate fatality recently on record. As a result, understanding bugs in AVs is critical for ensuring their security, safety, robustness, and correctness. While previous studies have focused on a variety of domains (e.g., numerical software; machine learning; and error-handling, concurrency, and performance bugs) to investigate bug characteristics, AVs have not been studied in a similar manner. Recently, two software systems for AVs, Baidu Apollo and Autoware, have emerged as frontrunners in the opensource community and have been used by large companies and governments (e.g., Lincoln, Volvo, Ford, Intel, Hitachi, LG, and the US Department of Transportation). From these two leading AV software systems, this paper describes our investigation of 16,851 commits and 499 AV bugs and introduces our classiﬁcation of those bugs into 13 root causes, 20 bug symptoms, and 18 categories of software components those bugs often affect. We identify 16 major ﬁndings from our study and draw broader lessons from them to guide the research community towards future directions in software bug detection, localization, and repair.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Garcia, Joshua and Feng, Yang and Shen, Junjie and Almanee, Sumaya and Xia, Yuan and Chen, Qi Alfred},
	year = {2020},
}

@inproceedings{Wardat2021DeepLocalize,
	title = {{DeepLocalize}: {Fault} {Localization} for {Deep} {Neural} {Networks}},
	shorttitle = {{DeepLocalize}},
	doi = {10.1109/ICSE43902.2021.00034},
	abstract = {Deep Neural Networks (DNNs) are becoming an integral part of most software systems. Previous work has shown that DNNs have bugs. Unfortunately, existing debugging techniques don't support localizing DNN bugs because of the lack of understanding of model behaviors. The entire DNN model appears as a black box. To address these problems, we propose an approach and a tool that automatically determines whether the model is buggy or not, and identifies the root causes for DNN errors. Our key insight is that historic trends in values propagated between layers can be analyzed to identify faults, and also localize faults. To that end, we first enable dynamic analysis of deep learning applications: by converting it into an imperative representation and alternatively using a callback mechanism. Both mechanisms allows us to insert probes that enable dynamic analysis over the traces produced by the DNN while it is being trained on the training data. We then conduct dynamic analysis over the traces to identify the faulty layer or hyperparameter that causes the error. We propose an algorithm for identifying root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer/parameter on the DNN outcome. We have collected a benchmark containing 40 buggy models and patches that contain real errors in deep learning applications from Stack Overflow and GitHub. Our benchmark can be used to evaluate automated debugging tools and repair techniques. We have evaluated our approach using this DNN bug-and-patch benchmark, and the results showed that our approach is much more effective than the existing debugging approach used in the state-of-the-practice Keras library. For 34/40 cases, our approach was able to detect faults whereas the best debugging approach provided by Keras detected 32/40 faults. Our approach was able to localize 21/40 bugs whereas Keras did not localize any faults.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Wardat, Mohammad and Le, Wei and Rajan, Hridesh},
	month = may,
	year = {2021},
	keywords = {Benchmark testing, Computer bugs, Debugging, Deep Neural Networks, Deep learning bugs, Fault Location, Fault diagnosis, Neural networks, Numerical models, Program Analysis, Tools},
	pages = {251--262},
}

@misc{feder_causal_2022,
	title = {Causal {Inference} in {Natural} {Language} {Processing}: {Estimation}, {Prediction}, {Interpretation} and {Beyond}},
	shorttitle = {Causal {Inference} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2109.00725},
	abstract = {A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and Wood-Doughty, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
	month = jul,
	year = {2022},
	note = {arXiv:2109.00725 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{rak-amnouykit_raise_2022,
	address = {Virtual South Korea},
	title = {The raise of machine learning hyperparameter constraints in {Python} code},
	isbn = {978-1-4503-9379-9},
	url = {https://dl.acm.org/doi/10.1145/3533767.3534400},
	doi = {10.1145/3533767.3534400},
	abstract = {Machine-learning operators often have correctness constraints that cut across multiple hyperparameters and/or data. Violating these constraints causes the operator to raise runtime exceptions, but those are usually documented only informally or not at all. This paper presents the first interprocedural weakest-precondition analysis for Python to extract hyperparameter constraints. The analysis is mostly static, but to make it tractable for typical Python idioms in machine-learning libraries, it selectively switches to the concrete domain for some cases. This paper demonstrates the analysis by extracting hyperparameter constraints for 181 operators from a total of 8 ML libraries, where it achieved high precision and recall and found real bugs. Our technique advances static analysis for Python and is a step towards safer and more robust machine learning.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Rak-amnouykit, Ingkarat and Milanova, Ana and Baudart, Guillaume and Hirzel, Martin and Dolby, Julian},
	month = jul,
	year = {2022},
	pages = {580--592},
}

@inproceedings{alshahwan_software_2023,
	title = {Software {Testing} {Research} {Challenges}: {An} {Industrial} {Perspective}},
	abstract = {There have been rapid recent developments in automated software test design, repair and program improvement. Advances in artificial intelligence also have great potential impact to tackle software testing research problems. In this paper we highlight open research problems and challenges from an industrial perspective. This perspective draws on our experience at Meta Platforms, which has been actively involved in software testing research and development for approximately a decade. As we set out here, there are many exciting opportunities for software testing research to achieve the widest and deepest impact on software practice. With this overview of the research landscape from an industrial perspective, we aim to stimulate further interest in the deployment of software testing research. We hope to be able to collaborate with the scientific community on some of these research challenges.},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Alshahwan, Nadia and Harman, Mark and Marginean, Alexandru},
	month = may,
	year = {2023},
}

@article{Li2020ModelAdaptation,
	title = {Model {Adaptation}: {Unsupervised} {Domain} {Adaptation} without {Source} {Data}},
	issn = {10636919},
	doi = {10.1109/CVPR42600.2020.00966},
	abstract = {In this paper, we investigate a challenging unsupervised domain adaptation setting - - unsupervised model adaptation. We aim to explore how to rely only on unlabeled target data to improve performance of an existing source prediction model on the target domain, since labeled source data may not be available in some real-world scenarios due to data privacy issues. For this purpose, we propose a new framework, which is referred to as collaborative class conditional generative adversarial net to bypass the dependence on the source data. Specifically, the prediction model is to be improved through generated target-style data, which provides more accurate guidance for the generator. As a result, the generator and the prediction model can collaborate with each other without source data. Furthermore, due to the lack of supervision from source data, we propose a weight constraint that encourages similarity to the source model. A clustering-based regularization is also introduced to produce more discriminative features in the target domain. Compared to conventional domain adaptation methods, our model achieves superior performance on multiple adaptation tasks with only unlabeled target data, which verifies its effectiveness in this challenging setting.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Li, Rui and Jiao, Qianfen and Cao, Wenming and Wong, Hau San and Wu, Si},
	year = {2020},
	pages = {9638--9647},
}

@misc{Schreiber2022ModelSelectionAdaptationCombinatinoforTransferLearning,
	title = {Model {Selection}, {Adaptation}, and {Combination} for {Transfer} {Learning} in {Wind} and {Photovoltaic} {Power} {Forecasts}},
	url = {http://arxiv.org/abs/2204.13293},
	abstract = {There is recent interest in using model hubs, a collection of pre-trained models, in computer vision tasks. To utilize the model hub, we first select a source model and then adapt the model for the target to compensate for differences. While there is yet limited research on model selection and adaption for computer vision tasks, this holds even more for the field of renewable power. At the same time, it is a crucial challenge to provide forecasts for the increasing demand for power forecasts based on weather features from a numerical weather prediction. We close these gaps by conducting the first thorough experiment for model selection and adaptation for transfer learning in renewable power forecast, adopting recent results from the field of computer vision on 667 wind and photovoltaic parks. To the best of our knowledge, this makes it the most extensive study for transfer learning in renewable power forecasts reducing the computational effort and improving the forecast error. Therefore, we adopt source models based on target data from different seasons and limit the amount of training data. As an extension of the current state of the art, we utilize a Bayesian linear regression for forecasting the response based on features extracted from a neural network. This approach outperforms the baseline with only seven days of training data. We further show how combining multiple models through ensembles can significantly improve the model selection and adaptation approach.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Schreiber, Jens and Sick, Bernhard},
	month = jul,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{Rahman2019MLSEinPractice,
	title = {Machine learning software engineering in practice: {An} industrial case study},
	doi = {10.48550/arXiv.1906.07154},
	abstract = {SAP is the market leader in enterprise software offering an end-to-end suite of applications and services to enable their customers worldwide to operate their business. Especially, retail customers of SAP deal with millions of sales transactions for their day-to-day business. Transactions are created during retail sales at the point of sale (POS) terminals and then sent to some central servers for validations and other business operations. A considerable proportion of the retail transactions may have inconsistencies due to many technical and human errors. SAP provides an automated process for error detection but still requires a manual process by dedicated employees using workbench software for correction. However, manual corrections of these errors are time-consuming, labor-intensive, and may lead to further errors due to incorrect modifications. This is not only a performance overhead on the customers' business workflow but it also incurs high operational costs. Thus, automated detection and correction of transaction errors are very important regarding their potential business values and the improvement in the business workflow. In this paper, we present an industrial case study where we apply machine learning (ML) to automatically detect transaction errors and propose corrections.We identify and discuss the challenges that we faced during this collaborative research and devel- opment project, from three distinct perspectives: Software Engineering, Machine Learning, and industry-academia collaboration. We report on our experience and insights from the project with guidelines for the identified challenges.We believe that our findings and recommendations can help researchers and practitioners embarking into similar endeavors.},
	journal = {arXiv preprint},
	author = {Rahman, Saidur and River, Emilio and Khomh, Foutse and Guhneuc, Yann Gal and Lehnert, Bernd},
	year = {2019},
}

@misc{cordy_flakime_2019,
	title = {{FlakiMe}: {Laboratory}-{Controlled} {Test} {Flakiness} {Impact} {Assessment}. {A} {Case} {Study} on {Mutation} {Testing} and {Program} {Repair}},
	shorttitle = {{FlakiMe}},
	url = {http://arxiv.org/abs/1912.03197},
	abstract = {Much research on software testing makes an implicit assumption that test failures are deterministic such that they always witness the presence of the same defects. However, this assumption is not always true because some test failures are due to so-called flaky tests, i.e., tests with non-deterministic outcomes. Unfortunately, flaky tests have major implications for testing and test-dependent activities such as mutation testing and automated program repair. To deal with this issue, we introduce a test flakiness assessment and experimentation platform, called FlakiMe, that supports the seeding of a (controllable) degree of flakiness into the behaviour of a given test suite. Thereby, FlakiMe equips researchers with ways to investigate the impact of test flakiness on their techniques under laboratory-controlled conditions. We use FlakiME to report results and insights from case studies that assesses the impact of flakiness on mutation testing and program repair. These results indicate that a 5\% of flakiness failures is enough to affect the mutation score, but the effect size is modest (2\% - 4\% ), while it completely annihilates the ability of program repair to patch 50\% of the subject programs. We also observe that flakiness has case-specific effects, which mainly disrupts the repair of bugs that are covered by many tests. Moreover, we find that a minimal amount of user feedback is sufficient for alleviating the effects of flakiness.},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Cordy, Maxime and Rwemalika, Renaud and Papadakis, Mike and Harman, Mark},
	month = dec,
	year = {2019},
	note = {arXiv:1912.03197 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{Tsay2022AIMetadataExtractionIBM,
	title = {Extracting enhanced artificial intelligence model metadata from software repositories},
	volume = {27},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-022-10206-6},
	doi = {10.1007/s10664-022-10206-6},
	abstract = {While artificial intelligence (AI) models have improved at understanding large-scale data, understanding AI models themselves at any scale is difficult. For example, even two models that implement the same network architecture may differ in frameworks, datasets, or even domains. Furthermore, attempting to use either model often requires much manual effort to understand it. As software engineering and AI development share many of the same languages and tools, techniques in mining software repositories should enable more scalable insights into AI models and AI development. However, much of the relevant metadata around models are not easily extractable. This paper (an extension of our MSR 2020 paper) presents a library called AIMMX for AI Model Metadata eXtraction from software repositories into enhanced metadata that conforms to a flexible metadata schema. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. We also explored how AIMMX can enable studies and tools to advance engineering support for AI development. As preliminary examples, we present an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. We also demonstrate the flexibility of extracted metadata by using the evaluation dataset in an existing natural language processing (NLP) analysis platform to identify trends in the dataset. Overall, we hope AIMMX fosters research towards better AI development.},
	language = {en},
	number = {7},
	urldate = {2022-09-26},
	journal = {Empirical Software Engineering},
	author = {Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar, Avraham and Mummert, Todd},
	month = dec,
	year = {2022},
	pages = {176},
}

@inproceedings{Mitchell2019ModelCardGoogle,
	address = {Atlanta GA USA},
	title = {Model {Cards} for {Model} {Reporting}},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287596},
	doi = {10.1145/3287560.3287596},
	abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
	language = {en},
	urldate = {2023-02-06},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
	month = jan,
	year = {2019},
	pages = {220--229},
}

@misc{bolya_token_2022,
	title = {Token {Merging}: {Your} {ViT} {But} {Faster}},
	shorttitle = {Token {Merging}},
	url = {http://arxiv.org/abs/2210.09461},
	abstract = {We introduce Token Merging (ToMe), a simple method to increase the throughput of existing ViT models without needing to train. ToMe gradually combines similar tokens in a transformer using a general and light-weight matching algorithm that is as fast as pruning while being more accurate. Off-the-shelf, ToMe can 2x the throughput of state-of-the-art ViT-L @ 512 and ViT-H @ 518 models on images and 2.2x the throughput of ViT-L on video with only a 0.2-0.3\% accuracy drop in each case. ToMe can also easily be applied during training, improving in practice training speed up to 2x for MAE fine-tuning on video. Training with ToMe further minimizes accuracy drop, leading to 2x the throughput of ViT-B on audio for only a 0.4\% mAP drop. Qualitatively, we find that ToMe merges object parts into one token, even over multiple frames of video. Overall, ToMe's accuracy and speed are competitive with state-of-the-art on images, video, and audio.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09461 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zeng_extensive_2022,
	address = {Virtual South Korea},
	title = {An extensive study on pre-trained models for program understanding and generation},
	isbn = {978-1-4503-9379-9},
	url = {https://dl.acm.org/doi/10.1145/3533767.3534390},
	doi = {10.1145/3533767.3534390},
	language = {en},
	urldate = {2023-02-24},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Zeng, Zhengran and Tan, Hanzhuo and Zhang, Haotian and Li, Jing and Zhang, Yuqun and Zhang, Lingming},
	month = jul,
	year = {2022},
	pages = {39--51},
}

@inproceedings{zeng_extensive_2022-1,
	address = {Virtual South Korea},
	title = {An extensive study on pre-trained models for program understanding and generation},
	isbn = {978-1-4503-9379-9},
	url = {https://dl.acm.org/doi/10.1145/3533767.3534390},
	doi = {10.1145/3533767.3534390},
	language = {en},
	urldate = {2023-02-24},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Zeng, Zhengran and Tan, Hanzhuo and Zhang, Haotian and Li, Jing and Zhang, Yuqun and Zhang, Lingming},
	month = jul,
	year = {2022},
	pages = {39--51},
}

@inproceedings{liang_gdefects4dl_2022,
	address = {Pittsburgh, PA, USA},
	title = {{gDefects4DL}: {A} {Dataset} of {General} {Real}-{World} {Deep} {Learning} {Program} {Defects}},
	isbn = {978-1-66549-598-1},
	language = {en},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	publisher = {IEEE},
	author = {Liang, Yunkai and Lin, Yun and Song, Xuezhi and Sun, Jun and Feng, Zhiyong and Dong, Jin Song},
	year = {2022},
}

@misc{perez_discovering_2022,
	title = {Discovering {Language} {Model} {Behaviors} with {Model}-{Written} {Evaluations}},
	url = {http://arxiv.org/abs/2212.09251},
	abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09251 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{Chen2022TowardsTrainingReproducibleDLModels,
	title = {Towards {Training} {Reproducible} {Deep} {Learning} {Models}},
	doi = {10.1145/3510003.3510163},
	abstract = {Reproducibility is an increasing concern in Artificial Intelligence (AI), particularly in the area of Deep Learning (DL). Being able to reproduce DL models is crucial for AI-based systems, as it is closely tied to various tasks like training, testing, debugging, and auditing. However, DL models are challenging to be reproduced due to issues like randomness in the software (e.g., DL algorithms) and non-determinism in the hardware (e.g., GPU). There are various practices to mitigate some of the aforementioned issues. However, many of them are either too intrusive or can only work for a specific usage context. In this paper, we propose a systematic approach to training reproducible DL models. Our approach includes three main parts: (1) a set of general criteria to thoroughly evaluate the reproducibility of DL models for two different domains, (2) a unified framework which leverages a record-and-replay technique to mitigate software-related randomness and a profile-and-patch technique to control hardware-related non-determinism, and (3) a reproducibility guideline which explains the rationales and the mitigation strategies on conducting a reproducible training process for DL models. Case study results show our approach can successfully reproduce six open source and one commercial DL models.},
	urldate = {2023-02-13},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Chen, Boyuan and Wen, Mingzhi and Shi, Yong and Lin, Dayi and Rajbahadur, Gopi Krishnan and Ming, Zhen and Jiang},
	month = may,
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
	pages = {2202--2214},
}

@article{francisco_empirical_2023,
	title = {An {Empirical} {Study} on {Bugs} of {Cross}-language {Implementations}},
	language = {en},
	author = {Francisco, San},
	year = {2023},
}

@article{francisco_deep_2023,
	title = {Deep {Neural} {Network} {Reuse} {Detection}: {A} {Neuron} {Functionality} {Analysis} {Approach}},
	language = {en},
	author = {Francisco, San},
	year = {2023},
}

@inproceedings{openja_empirical_2022,
	title = {An {Empirical} {Study} of {Challenges} in {Converting} {Deep} {Learning} {Models}},
	doi = {10.1109/ICSME55016.2022.00010},
	abstract = {There is an increase in deploying Deep Learning (DL)-based software systems in real-world applications. Usually, DL models are developed and trained using DL frameworks like TensorFlow and PyTorch. Each framework has its own internal mechanisms/formats to represent and train DL models (deep neural networks), and usually those formats cannot be recognized by other frameworks. Moreover, trained models are usually deployed in environments different from where they were developed. To solve the interoperability issue and make DL models compatible with different frameworks/environments, some exchange formats are introduced for DL models, like ONNX and CoreML. However, ONNX and CoreML were never empirically evaluated by the community to reveal their prediction accuracy, performance, and robustness after conversion. Poor accuracy or non-robust behavior of converted models may lead to poor quality of deployed DL-based software systems. We conduct, in this paper, the first empirical study to assess ONNX and CoreML for converting trained DL models. In our systematic approach, two popular DL frameworks, Keras and PyTorch, are used to train five widely used DL models on three popular datasets. The trained models are then converted to ONNX and CoreML and transferred to two runtime environments designated for such formats, to be evaluated. We investigate the prediction accuracy before and after conversion. Our results unveil that the prediction accuracy of converted models are at the same level of originals. The performance (time cost and memory consumption) of converted models are studied as well. The size of models are reduced after conversion, which can result in optimized DL-based software deployment. We also study the adversarial robustness of converted models to make sure about the robustness of deployed DL-based software. Leveraging the state-of-the-art adversarial attack approaches, converted models are generally assessed robust at the same level of originals. However, obtained results show that CoreML models are more vulnerable to adversarial attacks compared to ONNX. The general message of our findings is that DL developers should be cautious on the deployment of converted models that may 1) perform poorly while switching from one framework to another, 2) have challenges in robust deployment, or 3) run slowly, leading to poor quality of deployed DL-based software, including DL-based software maintenance tasks, like bug prediction.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Openja, Moses and Nikanjam, Amin and Yahmed, Ahmed Haj and Khomh, Foutse and Jiang, Zhen Ming Jack},
	month = oct,
	year = {2022},
	note = {ISSN: 2576-3148},
	keywords = {Analytical models, Converting Trained Models, Deep Learning, Deep learning, Deploying ML Models, Empirical, Predictive models, Robustness, Runtime environment, Software maintenance, Switches, Systematics},
	pages = {13--23},
}

@inproceedings{lucas_monotonic_2021,
	title = {On {Monotonic} {Linear} {Interpolation} of {Neural} {Network} {Parameters}},
	url = {https://proceedings.mlr.press/v139/lucas21a.html},
	abstract = {Linear interpolation between initial neural network parameters and converged parameters after training with stochastic gradient descent (SGD) typically leads to a monotonic decrease in the training objective. This Monotonic Linear Interpolation (MLI) property, first observed by Goodfellow et al. 2014, persists in spite of the non-convex objectives and highly non-linear training dynamics of neural networks. Extending this work, we evaluate several hypotheses for this property that, to our knowledge, have not yet been explored. Using tools from differential geometry, we draw connections between the interpolated paths in function space and the monotonicity of the network — providing sufficient conditions for the MLI property under mean squared error. While the MLI property holds under various settings (e.g., network architectures and learning problems), we show in practice that networks violating the MLI property can be produced systematically, by encouraging the weights to move far from initialization. The MLI property raises important questions about the loss landscape geometry of neural networks and highlights the need to further study their global properties.},
	language = {en},
	urldate = {2023-02-15},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lucas, James R. and Bae, Juhan and Zhang, Michael R. and Fort, Stanislav and Zemel, Richard and Grosse, Roger B.},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {7168--7179},
}

@inproceedings{thomas_sok_2021,
	title = {{SoK}: {Hate}, {Harassment}, and the {Changing} {Landscape} of {Online} {Abuse}},
	shorttitle = {{SoK}},
	doi = {10.1109/SP40001.2021.00028},
	abstract = {We argue that existing security, privacy, and antiabuse protections fail to address the growing threat of online hate and harassment. In order for our community to understand and address this gap, we propose a taxonomy for reasoning about online hate and harassment. Our taxonomy draws on over 150 interdisciplinary research papers that cover disparate threats ranging from intimate partner violence to coordinated mobs. In the process, we identify seven classes of attacks—such as toxic content and surveillance—that each stem from different attacker capabilities and intents. We also provide longitudinal evidence from a three-year survey that hate and harassment is a pervasive, growing experience for online users, particularly for at-risk communities like young adults and people who identify as LGBTQ+. Responding to each class of hate and harassment requires a unique strategy and we highlight five such potential research directions that ultimately empower individuals, communities, and platforms to do so.},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Thomas, Kurt and Akhawe, Devdatta and Bailey, Michael and Boneh, Dan and Bursztein, Elie and Consolvo, Sunny and Dell, Nicola and Durumeric, Zakir and Kelley, Patrick Gage and Kumar, Deepak and McCoy, Damon and Meiklejohn, Sarah and Ristenpart, Thomas and Stringhini, Gianluca},
	month = may,
	year = {2021},
	note = {ISSN: 2375-1207},
	keywords = {Cognition, Computer security, Distance measurement, Privacy, Social networking (online), Taxonomy, at-risk, emerging-threats, harassment, hate},
	pages = {247--267},
}

@article{haluptzok_language_2023,
	title = {Language {Models} {Can} {Teach} {Themselves} to {Program} {Better}},
	abstract = {Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM’s performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model “improves itself” using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al., 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance.},
	language = {en},
	author = {Haluptzok, Patrick and Bowers, Matthew and Kalai, Adam Tauman},
	year = {2023},
}

@article{gontijo-lopes_no_2022,
	title = {{NO} {ONE} {REPRESENTATION} {TO} {RULE} {THEM} {ALL}: {OVERLAPPING} {FEATURES} {OF} {TRAINING} {METHODS}},
	abstract = {Despite being able to capture a range of features of the data, high accuracy models trained with supervision tend to make similar predictions. This seemingly implies that high-performing models share similar biases regardless of training methodology, which would limit ensembling beneﬁts and render low-accuracy models as having little practical use. Against this backdrop, recent work has developed quite different training techniques, such as large-scale contrastive learning, yielding competitively high accuracy on generalization and robustness benchmarks. This motivates us to revisit the assumption that models necessarily learn similar functions. We conduct a large-scale empirical study of models across hyper-parameters, architectures, frameworks, and datasets. We ﬁnd that model pairs that diverge more in training methodology display categorically different generalization behavior, producing increasingly uncorrelated errors. We show these models specialize in subdomains of the data, leading to higher ensemble performance: with just 2 models (each with ImageNet accuracy 7˜6.5\%), we can create ensembles with 83.4\% (+7\% boost). Surprisingly, we ﬁnd that even signiﬁcantly low-accuracy models can be used to improve high-accuracy models. Finally, we show diverging training methodology yield representations that capture overlapping (but not supersetting) feature sets which, when combined, lead to increased downstream performance.},
	language = {en},
	author = {Gontijo-Lopes, Raphael and Dauphin, Yann and Cubuk, Ekin D},
	year = {2022},
}

@inproceedings{Wang2017ConcurrencyBugsinNodejs,
	title = {A comprehensive study on real world concurrency bugs in {Node}.js},
	doi = {10.1109/ASE.2017.8115663},
	abstract = {Node.js becomes increasingly popular in building server-side JavaScript applications. It adopts an event-driven model, which supports asynchronous I/O and non-deterministic event processing. This asynchrony and non-determinism can introduce intricate concurrency bugs, and leads to unpredictable behaviors. An in-depth understanding of real world concurrency bugs in Node.js applications will significantly promote effective techniques in bug detection, testing and fixing for Node.js. In this paper, we present NodeCB, a comprehensive study on real world concurrency bugs in Node.js applications. Specifically, we have carefully studied 57 real bug cases from open-source Node.js applications, and have analyzed their bug characteristics, e.g., bug patterns and root causes, bug impacts, bug manifestation, and fix strategies. Through this study, we obtain several interesting findings, which may open up many new research directions in combating concurrency bugs in Node.js. For example, one finding is that two thirds of the bugs are caused by atomicity violation. However, due to lack of locks and transaction mechanism, Node.js cannot easily express and guarantee the atomic intention.},
	booktitle = {2017 32nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Wang, Jie and Dou, Wensheng and Gao, Yu and Gao, Chushu and Qin, Feng and Yin, Kang and Wei, Jun},
	month = oct,
	year = {2017},
	keywords = {Computer bugs, Concurrent computing, Databases, Instruction sets, JavaScript, Node.js, Open source software, Testing, concurrency bug, empirical study, event-driven},
	pages = {520--531},
}

@inproceedings{Liu2014PerformanceBugs4SmartPhoneApps,
	address = {Hyderabad India},
	title = {Characterizing and detecting performance bugs for smartphone applications},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568229},
	doi = {10.1145/2568225.2568229},
	abstract = {Smartphone applications’ performance has a vital impact on user experience. However, many smartphone applications suffer from bugs that cause significant performance degradation, thereby losing their competitive edge. Unfortunately, people have little understanding of these performance bugs. They also lack effective techniques to fight with such bugs. To bridge this gap, we conducted a study of 70 real-world performance bugs collected from eight large-scale and popular Android applications. We studied the characteristics (e.g., bug types and how they manifested) of these bugs and identified their common patterns. These findings can support follow-up research on performance bug avoidance, testing, debugging and analysis for smartphone applications. To demonstrate the usefulness of our findings, we implemented a static code analyzer, PerfChecker, to detect our identified performance bug patterns. We experimentally evaluated PerfChecker by applying it to 29 popular Android applications, which comprise 1.1 million lines of Java code. PerfChecker successfully detected 126 matching instances of our performance bug patterns. Among them, 68 were quickly confirmed by developers as previouslyunknown issues that affect application performance, and 20 were fixed soon afterwards by following our optimization suggestions.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Liu, Yepang and Xu, Chang and Cheung, Shing-Chi},
	month = may,
	year = {2014},
	pages = {1013--1024},
}

@inproceedings{Bogart2015HowEcosystemDevelopersReasonabouttheStabilityofDependencies,
	title = {When {It} {Breaks}, {It} {Breaks}: {How} {Ecosystem} {Developers} {Reason} about the {Stability} of {Dependencies}},
	abstract = {Dependencies among software projects and libraries are an indicator of the often implicit collaboration among many developers in software ecosystems. Negotiating change can be tricky: changes to one module may cause ripple effects to many other modules that depend on it, yet insisting on only backward-compatible changes may incur significant opportunity cost and stifle change. We argue that awareness mechanisms based on various notions of stability can enable developers to make decisions that are independent yet wise and provide stewardship rather than disruption to the ecosystem. In ongoing interviews with developers in two software ecosystems (CRAN and Node.js), we are finding that developers in fact struggle with change, that they often use adhoc mechanisms to negotiate change, and that existing awareness mechanisms like Github notification feeds are rarely used due to information overload. We study the state of the art and current information needs and outline a vision toward a change-based awareness system.},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} {Workshop} ({ASEW})},
	author = {Bogart, Christopher and Kästner, Christian and Herbsleb, James},
	year = {2015},
	keywords = {Computer science, Ecosystems, History, Interviews, Planning, Software, Stability analysis},
}

@inproceedings{shivashankar_maintainability_2022,
	title = {Maintainability {Challenges} in {ML}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Maintainability {Challenges} in {ML}},
	doi = {10.1109/SEAA56994.2022.00018},
	abstract = {Background: As Machine Learning (ML) advances rapidly in many fields, it is being adopted by academics and businesses alike. However, ML has a number of different challenges in terms of maintenance not found in traditional software projects. Identifying what causes these maintainability challenges can help mitigate them early and continue delivering value in the long run without degrading ML performance. Aim: This study aims to identify and synthesise the maintainability challenges in different stages of the ML workflow and understand how these stages are interdependent and impact each other’s maintainability. Method: Using a systematic literature review, we screened more than 13000 papers, then selected and qualitatively analysed 56 of them. Results: (i) a catalogue of maintainability challenges in different stages of Data Engineering, Model Engineering workflows and the current challenges when building ML systems are discussed; (ii) a map of 13 maintainability challenges to different interdependent stages of ML that impact the overall workflow; (iii) Provided insights to developers of ML tools and researchers. Conclusions: In this study, practitioners and organisations will learn about maintainability challenges and their impact at different stages of ML workflow. This will enable them to avoid pitfalls and help to build a maintainable ML system. The implications and challenges will also serve as a basis for future research to strengthen our understanding of the ML system’s maintainability.},
	booktitle = {2022 48th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	author = {Shivashankar, Karthik and Martini, Antonio},
	month = aug,
	year = {2022},
	keywords = {Artificial Intelligence, Bibliographies, Buildings, Data engineering, Deep Learning, Machine Learning, Machine learning, Maintainability, Maintenance engineering, Software, Systematic Literature Review, Systematics},
	pages = {60--67},
}

@inproceedings{shivashankar_maintainability_2022-1,
	title = {Maintainability {Challenges} in {ML}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Maintainability {Challenges} in {ML}},
	doi = {10.1109/SEAA56994.2022.00018},
	abstract = {Background: As Machine Learning (ML) advances rapidly in many fields, it is being adopted by academics and businesses alike. However, ML has a number of different challenges in terms of maintenance not found in traditional software projects. Identifying what causes these maintainability challenges can help mitigate them early and continue delivering value in the long run without degrading ML performance. Aim: This study aims to identify and synthesise the maintainability challenges in different stages of the ML workflow and understand how these stages are interdependent and impact each other’s maintainability. Method: Using a systematic literature review, we screened more than 13000 papers, then selected and qualitatively analysed 56 of them. Results: (i) a catalogue of maintainability challenges in different stages of Data Engineering, Model Engineering workflows and the current challenges when building ML systems are discussed; (ii) a map of 13 maintainability challenges to different interdependent stages of ML that impact the overall workflow; (iii) Provided insights to developers of ML tools and researchers. Conclusions: In this study, practitioners and organisations will learn about maintainability challenges and their impact at different stages of ML workflow. This will enable them to avoid pitfalls and help to build a maintainable ML system. The implications and challenges will also serve as a basis for future research to strengthen our understanding of the ML system’s maintainability.},
	booktitle = {2022 48th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	author = {Shivashankar, Karthik and Martini, Antonio},
	month = aug,
	year = {2022},
	keywords = {Artificial Intelligence, Bibliographies, Buildings, Data engineering, Deep Learning, Machine Learning, Machine learning, Maintainability, Maintenance engineering, Software, Systematic Literature Review, Systematics},
	pages = {60--67},
}

@inproceedings{gonzalez_state_2020,
	address = {Seoul Republic of Korea},
	title = {The {State} of the {ML}-universe: 10 {Years} of {Artificial} {Intelligence} \& {Machine} {Learning} {Software} {Development} on {GitHub}},
	isbn = {978-1-4503-7517-7},
	shorttitle = {The {State} of the {ML}-universe},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387473},
	doi = {10.1145/3379597.3387473},
	abstract = {In the last few years, artificial intelligence (AI) and machine learning (ML) have become ubiquitous terms. These powerful techniques have escaped obscurity in academic communities with the recent onslaught of AI \& ML tools, frameworks, and libraries that make these techniques accessible to a wider audience of developers. As a result, applying AI \& ML to solve existing and emergent problems is an increasingly popular practice. However, little is known about this domain from the software engineering perspective. Many AI \& ML tools and applications are open source, hosted on platforms such as GitHub that provide rich tools for large-scale distributed software development. Despite widespread use and popularity, these repositories have never been examined as a community to identify unique properties, development patterns, and trends.},
	language = {en},
	urldate = {2023-02-06},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Gonzalez, Danielle and Zimmermann, Thomas and Nagappan, Nachiappan},
	month = jun,
	year = {2020},
	pages = {431--442},
}

@misc{noauthor_explainable_nodate,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI} {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://reader.elsevier.com/reader/sd/pii/S1566253519308103?token=9E77630E8A6219121AADEC397F8CA9D0DD508C4D25240EF952FFB6A967A1C441297EBCFE86B72D5790B5CA6C9CF73C1E&originRegion=us-east-1&originCreation=20230206204745},
	language = {en},
	urldate = {2023-02-06},
	doi = {10.1016/j.inffus.2019.12.012},
}

@article{stoyanovich_nutritional_nodate,
	title = {Nutritional {Labels} for {Data} and {Models}},
	abstract = {An essential ingredient of successful machine-assisted decision-making, particularly in high-stakes decisions, is interpretability –– allowing humans to understand, trust and, if necessary, contest, the computational process and its outcomes. These decision-making processes are typically complex: carried out in multiple steps, employing models with many hidden assumptions, and relying on datasets that are often used outside of the original context for which they were intended. In response, humans need to be able to determine the “ﬁtness for use” of a given model or dataset, and to assess the methodology that was used to produce it.},
	language = {en},
	author = {Stoyanovich, Julia and Howe, Bill},
}

@article{chmielinski_dataset_nodate,
	title = {The {Dataset} {Nutrition} {Label} (2nd {Gen}): {Leveraging} {Context} to {Mitigate} {Harms} in {Artiﬁcial} {Intelligence}},
	abstract = {As the production of and reliance on datasets to produce automated decisionmaking systems (ADS) increases, so does the need for processes for evaluating and interrogating the underlying data. After launching the Dataset Nutrition Label in 2018, the Data Nutrition Project has made signiﬁcant updates to the design and purpose of the Label, and is launching an updated Label in late 2020, which is previewed in this paper. The new Label includes context-speciﬁc Use Cases Alerts presented through an updated design and user interface targeted towards the data scientist proﬁle. This paper discusses the harm and bias from underlying training data that the Label is intended to mitigate, the current state of the work including new datasets being labeled, new and existing challenges, and further directions of the work, as well as Figures previewing the new label.},
	language = {en},
	author = {Chmielinski, Kasia S and Newman, Sarah and Taylor, Matt and Joseph, Josh and Thomas, Kemi and Yurkofsky, Jessica and Qiu, Yue Chelsea},
}

@inproceedings{mauri_stride-ai_2021,
	title = {{STRIDE}-{AI}: {An} {Approach} to {Identifying} {Vulnerabilities} of {Machine} {Learning} {Assets}},
	shorttitle = {{STRIDE}-{AI}},
	doi = {10.1109/CSR51186.2021.9527917},
	abstract = {We propose a security methodology for Machine Learning (ML) pipelines, supporting the definition of key security properties of ML assets, the identification of threats to them as well as the selection, test and verification of security controls. Our proposal is based on STRIDE, a widely used approach to threat modeling originally developed by Microsoft. We adapt STRIDE to the Artificial Intelligence domain by taking a security property-driven approach that also provides guidance in selecting the security controls needed to alleviate the identified threats. Our proposal is illustrated via an industrial case study.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Cyber} {Security} and {Resilience} ({CSR})},
	author = {Mauri, Lara and Damiani, Ernesto},
	month = jul,
	year = {2021},
	keywords = {Adaptation models, Artificial Intelligence security, Computer crime, Conferences, Machine learning, Pipelines, Proposals, Security, Threat modeling, Vulnerability assessment},
	pages = {147--154},
}

@inproceedings{filgueira_inspect4py_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Inspect4py: a knowledge extraction framework for python code repositories},
	isbn = {978-1-4503-9303-4},
	shorttitle = {Inspect4py},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528497},
	doi = {10.1145/3524842.3528497},
	language = {en},
	urldate = {2023-02-06},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Filgueira, Rosa and Garijo, Daniel},
	month = may,
	year = {2022},
	pages = {232--236},
}

@article{Anasuodei2021SWReusabilityApproachesandChallenges,
	title = {Software {Reusability}: {Approaches} and {Challenges}},
	volume = {06},
	issn = {24546194},
	shorttitle = {Software {Reusability}},
	doi = {10.51584/IJRIAS.2021.6510},
	abstract = {Software reuse is used to aid the software development process which in recent times can improve the resulting quality and productivity of software development, by assisting software engineers throughout various software engineering phases to enhance the quality of software, provide quick turnaround time for software development using few people, tools, and methods, which creates a good software quality by enhancing integration of the software system to provide a competitive advantage. This paper examines the concept of software reuse, the approaches to be considered for software reuse, which is broadly shared into three categories: component-based software reuse, domain engineering and software product lines, architecture-based software reuse and challenges that affect the software reuse development process.},
	language = {en},
	number = {05},
	urldate = {2022-12-22},
	journal = {International Journal of Research and Innovation in Applied Science},
	author = {Anasuodei, Moko and {Ojekudo} and Akpofure, Nathaniel},
	year = {2021},
}

@inproceedings{Gousios2012GHTorrent,
	title = {{GHTorrent}: {Github}'s data from a firehose},
	abstract = {A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.},
	booktitle = {International {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Gousios, Georgios and Spinellis, Diomidis},
	year = {2012},
	keywords = {Communities, Distributed databases, Electronic mail, GitHub, Organizations, Peer to peer computing, Protocols, commits, dataset, events, repository},
}

@article{Pan2010TransferLearning,
	title = {A {Survey} on {Transfer} {Learning}},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	journal = {Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	year = {2010},
	keywords = {Data mining, Knowledge engineering, Knowledge transfer, Labeling, Learning systems, Machine learning, Machine learning algorithms, Space technology, Testing, Training data, Transfer learning, data mining., machine learning, survey},
}

@inproceedings{Pham2020AnalysisofVarianceinDLSWSystems,
	title = {Problems and {Opportunities} in {Training} {Deep} {Learning} {Software} {Systems}: {An} {Analysis} of {Variance}},
	doi = {10.1145/3324884.3416545},
	abstract = {Deep learning (DL) training algorithms utilize nondeterminism to improve models’ accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
	year = {2020},
}

@article{Jadhav2011Framework4EvaluationaandSelectionofSWPackages,
	title = {Framework for evaluation and selection of the software packages: {A} hybrid knowledge based system approach},
	abstract = {Evaluation and selection of the software packages is complicated and time consuming decision making process. Selection of inappropriate software package can turn out to be costly and adversely affects business processes and functioning of the organization. In this paper we describe (i) generic methodology for software selection, (ii) software evaluation criteria, and (iii) hybrid knowledge based system (HKBS) approach to assist decision makers in evaluation and selection of the software packages. The proposed HKBS approach employs an integrated rule based and case based reasoning techniques. Rule based reasoning is used to capture user needs of the software package and formulate a problem case. Case based reasoning is used to retrieve and compare candidate software packages with the user needs of the package. This paper also evaluates and compares HKBS approach with the widely used existing software evaluation techniques such as analytic hierarchy process (AHP) and weighted scoring method (WSM).},
	journal = {Journal of Systems and Software (JSS)},
	author = {Jadhav, Anil S. and Sonar, Rajendra M.},
	year = {2011},
}

@article{Huston2018AIfacesReproducibilityCrisis,
	title = {Artificial intelligence faces reproducibility crisis},
	volume = {359},
	doi = {10.1126/science.359.6377.725},
	abstract = {Unpublished code and sensitivity to training conditions make many claims hard to verify.
          , 
            The booming field of artificial intelligence (AI) is grappling with a replication crisis, much like the ones that have afflicted psychology, medicine, and other fields over the past decade. Just because algorithms are based on code doesn't mean experiments are easily replicated. Far from it. Unpublished codes and a sensitivity to training conditions have made it difficult for AI researchers to reproduce many key results. That is leading to a new conscientiousness about research methods and publication protocols. Last week, at a meeting of the Association for the Advancement of Artificial Intelligence in New Orleans, Louisiana, reproducibility was on the agenda, with some teams diagnosing the problem—and one laying out tools to mitigate it.},
	number = {6377},
	journal = {American Association for the Advancement of Science},
	author = {Hutson, Matthew},
	year = {2018},
	pages = {725--726},
}

@inproceedings{zhang_empirical_2019,
	title = {An {Empirical} {Study} of {Common} {Challenges} in {Developing} {Deep} {Learning} {Applications}},
	doi = {10.1109/ISSRE.2019.00020},
	abstract = {Recent advances in deep learning promote the innovation of many intelligent systems and applications such as autonomous driving and image recognition. Despite enormous efforts and investments in this field, a fundamental question remains under-investigated - what challenges do developers commonly face when building deep learning applications? To seek an answer, this paper presents a large-scale empirical study of deep learning questions in a popular Q\&A website, Stack Overflow. We manually inspect a sample of 715 questions and identify seven kinds of frequently asked questions. We further build a classification model to quantify the distribution of different kinds of deep learning questions in the entire set of 39,628 deep learning questions. We find that program crashes, model migration, and implementation questions are the top three most frequently asked questions. After carefully examining accepted answers of these questions, we summarize five main root causes that may deserve attention from the research community, including API misuse, incorrect hyperparameter selection, GPU computation, static graph computation, and limited debugging and profiling support. Our results highlight the need for new techniques such as cross-framework differential testing to improve software development productivity and software reliability in deep learning.},
	booktitle = {2019 {IEEE} 30th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Zhang, Tianyi and Gao, Cuiyun and Ma, Lei and Lyu, Michael and Kim, Miryung},
	month = oct,
	year = {2019},
	note = {ISSN: 2332-6549},
	keywords = {deep learning, Stack Overflow, programming issues, software reliability},
	pages = {104--115},
}

@inproceedings{boyalakuntla_repoquester_2022,
	title = {{RepoQuester}: {A} {Tool} {Towards} {Evaluating} {GitHub} {Projects}},
	shorttitle = {{RepoQuester}},
	doi = {10.1109/ICSME55016.2022.00069},
	abstract = {Given the drastic rise of repositories on GitHub, it is often hard for developers to find relevant projects meeting their requirements as analyzing source code and other artifacts is effort-intensive. In our prior work, we proposed Repo Reaper (or simply Reaper) that assesses GitHub projects based on seven metrics spanning across project collaboration, quality, and maintenance. Reaper identified 1.4 million projects out of nearly 1.8 million projects to have no purpose for collaboration or software development by classifying them into ‘engineered’ and ‘non-engineered’ software projects. While Reaper can be used to assess millions of repositories based on GHTorrent, it is not designed to be used by developers for standalone repositories on local machines and is dependent on GHTorrent. Hence, in this paper, we propose a re-engineered and extended command-line tool named RepoQuester that aims to assist developers in evaluating GitHub projects on their local machines. RepoQuester computes metrics for projects and does not classify projects into ‘engineered’ and ‘non-engineered’ ones. However, to demonstrate the correctness of metric scores produced by RepoQuester, we have performed the project classification on the Reaper’s training and validation datasets by updating them with the latest metric scores (as reported by RepoQuester). These datasets have their ground truth manually established. During the analysis, we observed that the machine learning classifiers built on the updated datasets produced an F1 score of 72\%. During the evaluation, for each project, we found that RepoQuester can analyze metric scores in less than 10 seconds. A demo video explaining the tool highlights and usage is available at https://youtu.be/Q8OdmNzUfN0, and source code at https://github.com/Kowndinya2000/Repoquester.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Boyalakuntla, Kowndinya and Nagappan, Meiyappan and Chimalakonda, Sridhar and Munaiah, Nuthan},
	month = oct,
	year = {2022},
	note = {ISSN: 2576-3148},
	keywords = {Collaboration, GHTorrent, GitHub, Machine learning, Measurement, Mining Software Repositories, Repo Reaper, Software maintenance, Source coding, Training, Visualization},
	pages = {509--513},
}

@inproceedings{dror_deep_2019,
	address = {Florence, Italy},
	title = {Deep {Dominance} - {How} to {Properly} {Compare} {Deep} {Neural} {Models}},
	url = {https://aclanthology.org/P19-1266},
	doi = {10.18653/v1/P19-1266},
	abstract = {Comparing between Deep Neural Network (DNN) models based on their performance on unseen data is crucial for the progress of the NLP field. However, these models have a large number of hyper-parameters and, being non-convex, their convergence point depends on the random values chosen at initialization and during training. Proper DNN comparison hence requires a comparison between their empirical score distributions on unseen data, rather than between single evaluation scores as is standard for more simple, convex models. In this paper, we propose to adapt to this problem a recently proposed test for the Almost Stochastic Dominance relation between two distributions. We define the criteria for a high quality comparison method between DNNs, and show, both theoretically and through analysis of extensive experimental results with leading DNN models for sequence tagging tasks, that the proposed test meets all criteria while previously proposed methods fail to do so. We hope the test we propose here will set a new working practice in the NLP community.},
	urldate = {2023-02-03},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Dror, Rotem and Shlomov, Segev and Reichart, Roi},
	month = jul,
	year = {2019},
	pages = {2773--2785},
}

@misc{benjamin_measuring_2019,
	title = {Measuring and regularizing networks in function space},
	url = {http://arxiv.org/abs/1805.08289},
	abstract = {To optimize a neural network one often thinks of optimizing its parameters, but it is ultimately a matter of optimizing the function that maps inputs to outputs. Since a change in the parameters might serve as a poor proxy for the change in the function, it is of some concern that primacy is given to parameters but that the correspondence has not been tested. Here, we show that it is simple and computationally feasible to calculate distances between functions in a \$L{\textasciicircum}2\$ Hilbert space. We examine how typical networks behave in this space, and compare how parameter \${\textbackslash}ell{\textasciicircum}2\$ distances compare to function \$L{\textasciicircum}2\$ distances between various points of an optimization trajectory. We find that the two distances are nontrivially related. In particular, the \$L{\textasciicircum}2/{\textbackslash}ell{\textasciicircum}2\$ ratio decreases throughout optimization, reaching a steady value around when test error plateaus. We then investigate how the \$L{\textasciicircum}2\$ distance could be applied directly to optimization. We first propose that in multitask learning, one can avoid catastrophic forgetting by directly limiting how much the input/output function changes between tasks. Secondly, we propose a new learning rule that constrains the distance a network can travel through \$L{\textasciicircum}2\$-space in any one update. This allows new examples to be learned in a way that minimally interferes with what has previously been learned. These applications demonstrate how one can measure and regularize function distances directly, without relying on parameters or local approximations like loss curvature.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Benjamin, Ari S. and Rolnick, David and Kording, Konrad},
	month = jun,
	year = {2019},
	note = {arXiv:1805.08289 [cs, stat]
version: 3},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{noauthor_pii_nodate,
	title = {{PII}: 0164-1212(89)90016-2 {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {{PII}},
	url = {https://reader.elsevier.com/reader/sd/pii/0164121289900162?token=4B90B2DC25E01ED81AC699696137B78E189AC233F0AA81BC29C8FDD986222F28706E172C1300D449F2A3ADB0F9E9A437&originRegion=us-east-1&originCreation=20230202205243},
	language = {en},
	urldate = {2023-02-02},
	doi = {10.1016/0164-1212(89)90016-2},
	note = {ISSN: 0164-1212},
}

@inproceedings{Buse2012InforNeedsforSWDevelopmentAnalytics,
	title = {Information needs for software development analytics},
	doi = {10.1109/ICSE.2012.6227122},
	abstract = {Software development is a data rich activity with many sophisticated metrics. Yet engineers often lack the tools and techniques necessary to leverage these potentially powerful information resources toward decision making. In this paper, we present the data and analysis needs of professional software engineers, which we identified among 110 developers and managers in a survey. We asked about their decision making process, their needs for artifacts and indicators, and scenarios in which they would use analytics. The survey responses lead us to propose several guidelines for analytics tools in software development including: Engineers do not necessarily have much expertise in data analysis; thus tools should be easy to use, fast, and produce concise output. Engineers have diverse analysis needs and consider most indicators to be important; thus tools should at the same time support many different types of artifacts and many indicators. In addition, engineers want to drill down into data based on time, organizational structure, and system architecture.},
	booktitle = {2012 34th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Buse, Raymond P. L. and Zimmermann, Thomas},
	month = jun,
	year = {2012},
	keywords = {Complexity theory, Decision making, Guidelines, Measurement, Programming, Software, Software engineering},
	pages = {987--996},
}

@article{Hinton2015KnowledgeDistilling,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2022-02-23},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	year = {2015},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{cheng_supplementary_nodate,
	title = {Supplementary {Materials}: {Per}-{Pixel} {Classiﬁcation} is {Not} {All} {You} {Need} for {Semantic} {Segmentation}},
	language = {en},
	author = {Cheng, Bowen and Schwing, Alexander G and Kirillov, Alexander},
}

@inproceedings{jurafsky_extracting_2009,
	address = {Boulder, Colorado},
	title = {Extracting {Social} {Meaning}: {Identifying} {Interactional} {Style} in {Spoken} {Conversation}},
	shorttitle = {Extracting {Social} {Meaning}},
	url = {https://aclanthology.org/N09-1072},
	urldate = {2023-01-30},
	booktitle = {Proceedings of {Human} {Language} {Technologies}: {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jurafsky, Dan and Ranganath, Rajesh and McFarland, Dan},
	month = jun,
	year = {2009},
	pages = {638--646},
}

@article{Tan2018DeepTransferLearningSurvey,
	title = {A {Survey} on {Deep} {Transfer} {Learning}},
	abstract = {As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.},
	journal = {IEEE Transactions on knowledge and data engineering},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Robillard2010RecommendationSystems4SE,
	title = {Recommendation {Systems} for {Software} {Engineering}},
	volume = {27},
	issn = {1937-4194},
	doi = {10.1109/MS.2009.161},
	abstract = {Software development can be challenging because of the large information spaces that developers must navigate. Without assistance, developers can become bogged down and spend a disproportionate amount of their time seeking information at the expense of other value-producing tasks. Recommendation systems for software engineering (RSSEs) are software tools that can assist developers with a wide range of activities, from reusing code to writing effective bug reports. The authors provide an overview of recommendation systems for software engineering: what they are, what they can do for developers, and what they might do in the future.},
	number = {4},
	journal = {IEEE Software},
	author = {Robillard, Martin and Walker, Robert and Zimmermann, Thomas},
	month = jul,
	year = {2010},
	keywords = {Navigation, Programming, Software engineering, Software tools, Writing, coding tools and techniques, design tools and techniques, development tools, programming environments, software construction tools, software engineering},
	pages = {80--86},
}

@inproceedings{Ray2014StudyofProgrammingLanguagesandCodeQualityinGitHub,
	title = {A {Large} {Scale} {Study} of {Programming} {Languages} and {Code} {Quality} in {Github}},
	abstract = {What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static v.s. dynamic typing, strong v.s. weak typing on software quality. By triangulating ﬁndings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a signiﬁcant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing. We also ﬁnd that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages.},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Software} {Maintenance}},
	author = {Ray, Baishakhi and Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar},
	year = {2009},
}

@article{Malinen2015UserParticipationinOnlineCommunities,
	title = {Understanding user participation in online communities: {A} systematic literature review of empirical studies},
	volume = {46},
	issn = {07475632},
	shorttitle = {Understanding user participation in online communities},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0747563215000163},
	doi = {10.1016/j.chb.2015.01.004},
	abstract = {Online communities have become a popular and widely studied research topic. As active participation has been acknowledged as essential for the sustainability of the communities, research has focused largely on the most visible participants with the greatest ﬁnancial value for community providers. However, users can engage with the sites in different ways, which calls for a more diverse classiﬁcation of participation, instead of a simple active–passive dichotomy. This systematic literature review discusses empirical studies on online community participation. The results indicate that despite the large amount of research conducted on the topic, a theoretical and conceptual framework for user participation remains undeﬁned as most of the research has approached participation in terms of its quantity. The complexity of online participation and its implications for methodology in future studies is discussed.},
	language = {en},
	urldate = {2023-01-25},
	journal = {Computers in Human Behavior},
	author = {Malinen, Sanna},
	month = may,
	year = {2015},
	pages = {228--238},
}

@article{fan_automatic_2012,
	title = {Automatic knowledge extraction from documents},
	volume = {56},
	issn = {0018-8646},
	doi = {10.1147/JRD.2012.2186519},
	abstract = {Access to a large amount of knowledge is critical for success at answering open-domain questions for DeepQA systems such as IBM Watson™. Formal representation of knowledge has the advantage of being easy to reason with, but acquisition of structured knowledge in open domains from unstructured data is often difficult and expensive. Our central hypothesis is that shallow syntactic knowledge and its implied semantics can be easily acquired and can be used in many areas of a question-answering system. We take a two-stage approach to extract the syntactic knowledge and implied semantics. First, shallow knowledge from large collections of documents is automatically extracted. Second, additional semantics are inferred from aggregate statistics of the automatically extracted shallow knowledge. In this paper, we describe in detail what kind of shallow knowledge is extracted, how it is automatically done from a large corpus, and how additional semantics are inferred from aggregate statistics. We also briefly discuss the various ways extracted knowledge is used throughout the IBM DeepQA system.},
	number = {3.4},
	journal = {IBM Journal of Research and Development},
	author = {Fan, J. and Kalyanpur, A. and Gondek, D. C. and Ferrucci, D. A.},
	month = may,
	year = {2012},
	note = {Conference Name: IBM Journal of Research and Development},
	keywords = {Computational linguistics, Context awareness, Information analysis, Knowledge based systems, Knowledge management, Semantics, Syntactics},
	pages = {5:1--5:10},
}

@inproceedings{Li2008RegexLearning4IE,
	address = {Honolulu, Hawaii},
	title = {Regular {Expression} {Learning} for {Information} {Extraction}},
	url = {https://aclanthology.org/D08-1003},
	urldate = {2023-01-24},
	booktitle = {Proceedings of the 2008 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Yunyao and Krishnamurthy, Rajasekar and Raghavan, Sriram and Vaithyanathan, Shivakumar and Jagadish, H. V.},
	month = oct,
	year = {2008},
	pages = {21--30},
}

@article{Hong2022BROSKeyInformationExtractionfromDocs,
	title = {{BROS}: {A} {Pre}-trained {Language} {Model} {Focusing} on {Text} and {Layout} for {Better} {Key} {Information} {Extraction} from {Documents}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{BROS}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21322},
	doi = {10.1609/aaai.v36i10.21322},
	abstract = {Key information extraction (KIE) from document images requires understanding the contextual and spatial semantics of texts in two-dimensional (2D) space. Many recent studies try to solve the task by developing pre-trained language models focusing on combining visual features from document images with texts and their layout. On the other hand, this paper tackles the problem by going back to the basic: effective combination of text and layout. Speciﬁcally, we propose a pre-trained language model, named BROS (BERT Relying On Spatiality), that encodes relative positions of texts in 2D space and learns from unlabeled documents with area-masking strategy. With this optimized training scheme for understanding texts in 2D space, BROS shows comparable or better performance compared to previous methods on four KIE benchmarks (FUNSD, SROIE∗, CORD, and SciTSR) without relying on visual features. This paper also reveals two real-world challenges in KIE tasks–(1) minimizing the error from incorrect text ordering and (2) efﬁcient learning from fewer downstream examples–and demonstrates the superiority of BROS over previous methods.},
	language = {en},
	number = {10},
	urldate = {2023-01-24},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Hong, Teakgyu and Kim, DongHyun and Ji, Mingi and Hwang, Wonseok and Nam, Daehyun and Park, Sungrae},
	month = jun,
	year = {2022},
	pages = {10767--10775},
}

@article{Shafiq2021LitReviewofMLinSWDevLifeCycle,
	title = {A {Literature} {Review} of {Using} {Machine} {Learning} in {Software} {Development} {Life} {Cycle} {Stages}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3119746},
	abstract = {The software engineering community is rapidly adopting machine learning for transitioning modern-day software towards highly intelligent and self-learning systems. However, the software engineering community is still discovering new ways how machine learning can offer help for various software development life cycle stages. In this article, we present a study on the use of machine learning across various software development life cycle stages. The overall aim of this article is to investigate the relationship between software development life cycle stages, and machine learning tools, techniques, and types. We attempt a holistic investigation in part to answer the question of whether machine learning favors certain stages and/or certain techniques.},
	journal = {IEEE Access},
	author = {Shafiq, Saad and Mashkoor, Atif and Mayr-Dorn, Christoph and Egyed, Alexander},
	year = {2021},
	keywords = {Data mining, Machine learning, Software engineering, Software systems, Software testing, Support vector machines, Tools, literature review, machine learning},
	pages = {140896--140920},
}

@misc{watson_systematic_2021,
	title = {A {Systematic} {Literature} {Review} on the {Use} of {Deep} {Learning} in {Software} {Engineering} {Research}},
	url = {http://arxiv.org/abs/2009.06520},
	abstract = {An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this crosscutting area of work, from its modern inception to the present, this paper presents a systematic literature review of research at the intersection of SE \& DL. The review canvases work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23 unique SE tasks. We center our analysis around the components of learning, a set of principles that govern the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research, and highlights likely areas of fertile exploration for the future.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Watson, Cody and Cooper, Nathan and Palacio, David Nader and Moran, Kevin and Poshyvanyk, Denys},
	month = sep,
	year = {2021},
	note = {arXiv:2009.06520 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Software Engineering},
}

@inproceedings{tawosi_versatile_2022,
	address = {Pittsburgh Pennsylvania},
	title = {A versatile dataset of agile open source software projects},
	isbn = {978-1-4503-9303-4},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528029},
	doi = {10.1145/3524842.3528029},
	abstract = {Agile software development is nowadays a widely adopted practise in both open-source and industrial software projects. Agile teams typically heavily rely on issue management tools to document new issues and keep track of outstanding ones, in addition to storing their technical details, effort estimates, assignment to developers, and more. Previous work utilised the historical information stored in issue management systems for various purposes; however, when researchers make their empirical data public, it is usually relevant solely to the study’s objective. In this paper, we present a more holistic and versatile dataset containing a wealth of information on more than half a million issues from 44 open-source Agile software, making it well-suited to several research avenues, and cross-analyses therein, including effort estimation, issue prioritization, issue assignment and many more. We make this data publicly available on GitHub to facilitate ease of use, maintenance, and extensibility.},
	language = {en},
	urldate = {2023-01-20},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Tawosi, Vali and Al-Subaihin, Afnan and Moussa, Rebecca and Sarro, Federica},
	month = may,
	year = {2022},
	pages = {707--711},
}

@misc{cheng_per-pixel_2021,
	title = {Per-{Pixel} {Classification} is {Not} {All} {You} {Need} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2107.06278},
	abstract = {Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Cheng, Bowen and Schwing, Alexander G. and Kirillov, Alexander},
	month = oct,
	year = {2021},
	note = {arXiv:2107.06278 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{Slankas2013AutomatedExtractionofNonfunctionalRequirements,
	title = {Automated extraction of non-functional requirements in available documentation},
	doi = {10.1109/NAturaLiSE.2013.6611715},
	abstract = {While all systems have non-functional requirements (NFRs), they may not be explicitly stated in a formal requirements specification. Furthermore, NFRs may also be externally imposed via government regulations or industry standards. As some NFRs represent emergent system proprieties, those NFRs require appropriate analysis and design efforts to ensure they are met. When the specified NFRs are not met, projects incur costly re-work to correct the issues. The goal of our research is to aid analysts in more effectively extracting relevant non-functional requirements in available unconstrained natural language documents through automated natural language processing. Specifically, we examine which document types (data use agreements, install manuals, regulations, request for proposals, requirements specifications, and user manuals) contain NFRs categorized to 14 NFR categories (e.g. capacity, reliability, and security). We measure how effectively we can identify and classify NFR statements within these documents. In each of the documents evaluated, we found NFRs present. Using a word vector representation of the NFRs, a support vector machine algorithm performed twice as effectively compared to the same input to a multinomial naïve Bayes classifier. Our k-nearest neighbor classifier with a unique distance metric had an F1 measure of 0.54, outperforming in our experiments the optimal naïve Bayes classifier which had a F1 measure of 0.32. We also found that stop word lists beyond common determiners had no minimal performance effect.},
	booktitle = {2013 1st {International} {Workshop} on {Natural} {Language} {Analysis} in {Software} {Engineering} ({NaturaLiSE})},
	author = {Slankas, John and Williams, Laurie},
	month = may,
	year = {2013},
	keywords = {Classification algorithms, Documentation, Machine learning algorithms, Measurement, Natural languages, Security, Standards, classification, documentation, machine learning, natural language processing, non-functional requirements},
	pages = {9--16},
}

@article{Allamanis2016AttentionNetwork4ExtremeSummarizationofSourceCode,
	title = {A {Convolutional} {Attention} {Network}  for {Extreme} {Summarization} of {Source} {Code}},
	abstract = {Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have ﬁxed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features speciﬁcally. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.},
	language = {en},
	author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
}

@inproceedings{jesse_manytypes4typescript_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{ManyTypes4TypeScript}: a comprehensive {TypeScript} dataset for sequence-based type inference},
	isbn = {978-1-4503-9303-4},
	shorttitle = {{ManyTypes4TypeScript}},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528507},
	doi = {10.1145/3524842.3528507},
	abstract = {In this paper, we present ManyTypes4TypeScript, a very large corpus for training and evaluating machine-learning models for sequence-based type inference in TypeScript. The dataset includes over 9 million type annotations, across 13,953 projects and 539,571 files. The dataset is approximately 10x larger than analogous type inference datasets for Python, and is the largest available for TypeScript. We also provide API access to the dataset, which can be integrated into any tokenizer and used with any state-of-the-art sequence-based model. Finally, we provide analysis and performance results for state-of-the-art code-specific models, for baselining. ManyTypes4TypeScript is available on Huggingface, Zenodo, and CodeXGLUE.},
	language = {en},
	urldate = {2023-01-17},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Jesse, Kevin and Devanbu, Premkumar T.},
	month = may,
	year = {2022},
	pages = {294--298},
}

@article{Ma2021OpenSourceVCSData,
	title = {World of code: enabling a research workflow for mining and analyzing the universe of open source {VCS} data},
	volume = {26},
	issn = {1382-3256, 1573-7616},
	shorttitle = {World of code},
	url = {http://link.springer.com/10.1007/s10664-020-09905-9},
	doi = {10.1007/s10664-020-09905-9},
	abstract = {Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are the tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flow? To answer such questions we: a) create a very large and frequently updated collection of version control data in the entire FLOSS ecosystems named World of Code (WoC), that can completely cross-reference authors, projects, commits, blobs, dependencies, and history of the FLOSS ecosystems and b) provide capabilities to efficiently correct, augment, query, and analyze that data. Our current WoC implementation is capable of being updated on a monthly basis and contains over 18B Git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.},
	language = {en},
	number = {2},
	urldate = {2023-01-17},
	journal = {Empirical Software Engineering},
	author = {Ma, Yuxing and Dey, Tapajit and Bogart, Chris and Amreen, Sadika and Valiev, Marat and Tutko, Adam and Kennard, David and Zaretzki, Russell and Mockus, Audris},
	month = mar,
	year = {2021},
	pages = {22},
}

@inproceedings{Alfadel2021SecurityAnalysisinPythonPackages,
	title = {Empirical {Analysis} of {Security} {Vulnerabilities} in {Python} {Packages}},
	doi = {10.1109/SANER50967.2021.00048},
	abstract = {Software ecosystems play an important role in modern software development, providing an open platform of reusable packages that speed up and facilitate development tasks. However, this level of code reusability supported by software ecosystems also makes the discovery of security vulnerabilities much more difficult, as software systems depend on an increasingly high number of packages. Recently, security vulnerabilities in the npm ecosystem, the ecosystem of Node.js packages, have been studied in the literature. As different software ecosystems embodied different programming languages and particularities, we argue that it is also important to study other popular programming languages to build stronger empirical evidence about vulnerabilities in software ecosystems.In this paper, we present an empirical study of 550 vulnerability reports affecting 252 Python packages in the Python ecosystem (PyPi). In particular, we study the propagation and life span of security vulnerabilities, accounting for how long they take to be discovered and fixed. Our findings show that the discovered vulnerabilities in Python packages are increasing over time, and they take more than 3 years to be discovered. The majority of these vulnerabilities (50.55\%) are only fixed after being publicly announced, giving ample time for attackers exploitation. We find similarities in some characteristics of vulnerabilities in PyPi and npm and divergences that can be attributed to specific PyPi policies. By leveraging our findings, we provide a series of implications that can help the security of software ecosystems by improving the process of discovering, fixing and managing package vulnerabilities.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Alfadel, Mahmoud and Costa, Diego Elias and Shihab, Emad},
	month = mar,
	year = {2021},
	keywords = {Conferences, Ecosystems, Software, Software packages, Software systems, Timing, Tools, empirical studies, packages, pypi, python, vulnerabilities},
	pages = {446--457},
}

@article{Elsner2021RegressionTestOptimizationinCI,
	title = {Empirically {Evaluating} {Readily} {Available} {Information} for {Regression} {Test} {Optimization} in {Continuous} {Integration}},
	abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90\% of the failures, we show that practitioners can expect to save on average 84\% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
	language = {en},
	author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
	year = {2021},
}

@inproceedings{Gousios2014PullBasedSWDevelopment,
	address = {Hyderabad India},
	title = {An exploratory study of the pull-based software development model},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568260},
	doi = {10.1145/2568225.2568260},
	abstract = {The advent of distributed version control systems has led to the development of a new paradigm for distributed software development; instead of pushing changes to a central repository, developers pull them from other repositories and merge them locally. Various code hosting sites, notably Github, have tapped on the opportunity to facilitate pull-based development by offering workﬂow support tools, such as code reviewing systems and integrated issue trackers. In this work, we explore how pull-based software development works, ﬁrst on the GHTorrent corpus and then on a carefully selected sample of 291 projects. We ﬁnd that the pull request model offers fast turnaround, increased opportunities for community engagement and decreased time to incorporate contributions. We show that a relatively small number of factors affect both the decision to merge a pull request and the time to process it. We also examine the reasons for pull request rejection and ﬁnd that technical ones are only a small minority.},
	language = {en},
	urldate = {2023-01-17},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Gousios, Georgios and Pinzger, Martin and Deursen, Arie van},
	month = may,
	year = {2014},
	pages = {345--355},
}

@article{allamanis_convolutional_nodate,
	title = {A {Convolutional} {Attention} {Network}  for {Extreme} {Summarization} of {Source} {Code}},
	abstract = {Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have ﬁxed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features speciﬁcally. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.},
	language = {en},
	author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
}

@inproceedings{Mattis2020RTPTorrent,
	address = {Seoul Republic of Korea},
	title = {{RTPTorrent}: {An} {Open}-source {Dataset} for {Evaluating} {Regression} {Test} {Prioritization}},
	isbn = {978-1-4503-7517-7},
	shorttitle = {{RTPTorrent}},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387458},
	doi = {10.1145/3379597.3387458},
	abstract = {The software engineering practice of automated testing helps programmers find defects earlier during development. With growing software projects and longer-running test suites, frequency and immediacy of feedback decline, thereby making defects harder to repair. Regression test prioritization (RTP) is concerned with running relevant tests earlier to lower the costs of defect localization and to improve feedback. Finding representative data to evaluate RTP techniques is nontrivial, as most software is published without failing tests. In this work, we systematically survey a wide range of RTP literature regarding whether their dataset uses real or synthetic defects or tests, whether they are publicly available, and whether datasets are reused. We observed that some datasets are reused, however, many projects study only few projects and these rarely resemble real-world development activity.},
	language = {en},
	urldate = {2023-01-11},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Mattis, Toni and Rein, Patrick and Dürsch, Falco and Hirschfeld, Robert},
	month = jun,
	year = {2020},
	pages = {385--396},
}

@inproceedings{Beller2017TravisTorrent,
	title = {{TravisTorrent}: {Synthesizing} {Travis} {CI} and {GitHub} for {Full}-{Stack} {Research} on {Continuous} {Integration}},
	shorttitle = {{TravisTorrent}},
	doi = {10.1109/MSR.2017.24},
	abstract = {Continuous Integration (CI) has become a best practice of modern software development. Thanks in part to its tight integration with GitHub, Travis CI has emerged as arguably the most widely used CI platform for Open-Source Software (OSS) development. However, despite its prominent role in Software Engineering in practice, the benefits, costs, and implications of doing CI are all but clear from an academic standpoint. Little research has been done, and even less was of quantitative nature. In order to lay the groundwork for data-driven research on CI, we built TravisTorrent, travistorrent.testroots.org, a freely available data set based on Travis CI and GitHub that provides easy access to hundreds of thousands of analyzed builds from more than 1,000 projects. Unique to TravisTorrent is that each of its 2,640,825 Travis builds is synthesized with meta data from Travis CI's API, the results of analyzing its textual build log, a link to the GitHub commit which triggered the build, and dynamically aggregated project data from the time of commit extracted through GHTorrent.},
	booktitle = {{IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Beller, Moritz and Gousios, Georgios and Zaidman, Andy},
	month = may,
	year = {2017},
	keywords = {Data mining, History, Java, Rails, Software, Testing},
	pages = {447--450},
}

@inproceedings{Beller2017ExplorativeStudyofTravisCI,
	title = {Oops, {My} {Tests} {Broke} the {Build}: {An} {Explorative} {Analysis} of {Travis} {CI} with {GitHub}},
	shorttitle = {Oops, {My} {Tests} {Broke} the {Build}},
	doi = {10.1109/MSR.2017.62},
	abstract = {Continuous Integration (CI) has become a best practice of modern software development. Yet, at present, we have a shortfall of insight into the testing practices that are common in CI-based software development. In particular, we seek quantifiable evidence on how central testing is to the CI process, how strongly the project language influences testing, whether different integration environments are valuable and if testing on the CI can serve as a surrogate to local testing in the IDE. In an analysis of 2,640,825 Java and Ruby builds on Travis CI, we find that testing is the single most important reason why builds fail. Moreover, the programming language has a strong influence on both the number of executed tests, their run time, and proneness to fail. The use of multiple integration environments leads to 10\% more failures being caught at build time. However, testing on Travis CI does not seem an adequate surrogate for running tests locally in the IDE. To further research on Travis CI with GitHub, we introduce TravisTorrent.},
	booktitle = {{IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Beller, Moritz and Gousios, Georgios and Zaidman, Andy},
	month = may,
	year = {2017},
	keywords = {Best practices, Java, Programming, Software, Testing, Tools},
	pages = {356--367},
}

@inproceedings{Zampetti2017HowOSProjectsUseStaticCodeAnalysisinCIPipelines,
	title = {How {Open} {Source} {Projects} {Use} {Static} {Code} {Analysis} {Tools} in {Continuous} {Integration} {Pipelines}},
	doi = {10.1109/MSR.2017.2},
	abstract = {Static analysis tools are often used by software developers to entail early detection of potential faults, vulnerabilities, code smells, or to assess the source code adherence to coding standards and guidelines. Also, their adoption within Continuous Integration (CI) pipelines has been advocated by researchers and practitioners. This paper studies the usage of static analysis tools in 20 Java open source projects hosted on GitHub and using Travis CI as continuous integration infrastructure. Specifically, we investigate (i) which tools are being used and how they are configured for the CI, (ii) what types of issues make the build fail or raise warnings, and (iii) whether, how, and after how long are broken builds and warnings resolved. Results indicate that in the analyzed projects build breakages due to static analysis tools are mainly related to adherence to coding standards, and there is also some attention to missing licenses. Build failures related to tools identifying potential bugs or vulnerabilities occur less frequently, and in some cases such tools are activated in a "softer" mode, without making the build fail. Also, the study reveals that build breakages due to static analysis tools are quickly fixed by actually solving the problem, rather than by disabling the warning, and are often properly documented.},
	booktitle = {{IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Zampetti, Fiorella and Scalabrino, Simone and Oliveto, Rocco and Canfora, Gerardo and Di Penta, Massimiliano},
	month = may,
	year = {2017},
	keywords = {Continuous Integration, Data mining, Empirical Study, Encoding, History, Java, Open Source Projects, Pipelines, Software, Static Analysis Tools, Tools},
	pages = {334--344},
}

@misc{sinha_extractive_2018,
	title = {Extractive {Text} {Summarization} using {Neural} {Networks}},
	url = {http://arxiv.org/abs/1802.10137},
	abstract = {Text Summarization has been an extensively studied problem. Traditional approaches to text summarization rely heavily on feature engineering. In contrast to this, we propose a fully data-driven approach using feedforward neural networks for single document summarization. We train and evaluate the model on standard DUC 2002 dataset which shows results comparable to the state of the art models. The proposed model is scalable and is able to produce the summary of arbitrarily sized documents by breaking the original document into fixed sized parts and then feeding it recursively to the network.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Sinha, Aakash and Yadav, Abhishek and Gahlot, Akshay},
	month = feb,
	year = {2018},
	note = {arXiv:1802.10137 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
}

@misc{wu_learning_2018,
	title = {Learning to {Extract} {Coherent} {Summary} via {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1804.07036},
	abstract = {Coherence plays a critical role in producing a high-quality summary from a document. In recent years, neural extractive summarization is becoming increasingly attractive. However, most of them ignore the coherence of summaries when extracting sentences. As an effort towards extracting coherent summaries, we propose a neural coherence model to capture the cross-sentence semantic and syntactic coherence patterns. The proposed neural coherence model obviates the need for feature engineering and can be trained in an end-to-end fashion using unlabeled data. Empirical results show that the proposed neural coherence model can efficiently capture the cross-sentence coherence patterns. Using the combined output of the neural coherence model and ROUGE package as the reward, we design a reinforcement learning method to train a proposed neural extractive summarizer which is named Reinforced Neural Extractive Summarization (RNES) model. The RNES model learns to optimize coherence and informative importance of the summary simultaneously. Experimental results show that the proposed RNES outperforms existing baselines and achieves state-of-the-art performance in term of ROUGE on CNN/Daily Mail dataset. The qualitative evaluation indicates that summaries produced by RNES are more coherent and readable.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Wu, Yuxiang and Hu, Baotian},
	month = apr,
	year = {2018},
	note = {arXiv:1804.07036 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhu_automatic_2019,
	title = {Automatic {Code} {Summarization}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Automatic {Code} {Summarization}},
	url = {http://arxiv.org/abs/1909.04352},
	abstract = {Background: During software maintenance and development, the comprehension of program code is key to success. High-quality comments can help us better understand programs, but they're often missing or outmoded in today's programs. Automatic code summarization is proposed to solve these problems. During the last decade, huge progress has been made in this field, but there is a lack of an up-to-date survey. Aims: We studied publications concerning code summarization in the field of program comprehension to investigate state-of-the-art approaches. By reading and analyzing relevant articles, we aim at obtaining a comprehensive understanding of the current status of automatic code summarization. Method: In this paper, we performed a systematic literature review over the automatic source code summarization field. Furthermore, we synthesized the obtained data and investigated different approaches. Results: We successfully collected and analyzed 41 selected studies from the different research communities. We exhaustively investigated and described the data extraction techniques, description generation methods, evaluation methods and relevant artifacts of those works. Conclusions: Our systematic review provides an overview of the state of the art, and we also discuss further research directions. By fully elaborating current approaches in the field, our work sheds light on future research directions of program comprehension and comment generation.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Zhu, Yuxiang and Pan, Minxue},
	month = oct,
	year = {2019},
	note = {arXiv:1909.04352 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{allahyari_text_2017,
	title = {Text {Summarization} {Techniques}: {A} {Brief} {Survey}},
	shorttitle = {Text {Summarization} {Techniques}},
	url = {http://arxiv.org/abs/1707.02268},
	abstract = {In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saeid and Trippe, Elizabeth D. and Gutierrez, Juan B. and Kochut, Krys},
	month = jul,
	year = {2017},
	note = {arXiv:1707.02268 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{hendrycks_using_2019,
	title = {Using {Pre}-{Training} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	url = {http://arxiv.org/abs/1901.09960},
	abstract = {He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10\% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
	month = oct,
	year = {2019},
	note = {arXiv:1901.09960 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cox_surviving_2019,
	title = {Surviving software dependencies},
	volume = {62},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3347446},
	doi = {10.1145/3347446},
	abstract = {Software reuse is finally here but comes with risks.},
	language = {en},
	number = {9},
	urldate = {2023-01-05},
	journal = {Communications of the ACM},
	author = {Cox, Russ},
	month = aug,
	year = {2019},
	pages = {36--43},
}

@misc{noauthor_towards_nodate,
	title = {Towards {Reliability} in {Deep} {Learning} {Systems}},
	url = {https://ai.googleblog.com/2022/07/towards-reliability-in-deep-learning.html},
	language = {en},
	urldate = {2023-01-05},
}

@article{neelamegam_survey_nodate,
	title = {A {Survey} - {Object} {Oriented} {Quality} {Metrics}},
	abstract = {Object oriented design is becoming more popular in software development environment and object oriented design metrics is an essential part of software environment. This study focus on a set of object oriented metrics that can be used to measure the quality of an object oriented design. The metrics for object oriented design focus on measurements that are applied to the class and design characteristics. These measurements permit designers to access the software early in process, making changes that will reduce complexity and improve the continuing capability of the design. This report summarizes the existing metrics, which will guide the designers to support their design. We have categorized metrics and discussed in such a way that novice designers can apply metrics in their design as needed.},
	language = {en},
	author = {Neelamegam, C and Punithavalli, Dr M},
}

@inproceedings{goyal_qmood_2014,
	title = {{QMOOD} metric sets to assess quality of {Java} program},
	doi = {10.1109/ICICICT.2014.6781337},
	abstract = {This paper describes the model to evaluate and grade the Java programs, based on QMOOD (Quality Model for Object Oriented Design). QMOOD is the hierarchical model that defines relation between qualities attributes(like reusability, functionality, effectiveness, understand ability, extendibility, flexibility) and design properties with the help of equations. In this research we have developed the system based on QMOOD which is use to evaluate the quality of JAVA programs. In this we would try evaluate all quality attributes by finding the design metrics that is based on design property of object oriented design. Though QMOOD metrics are subjective in nature but still with the help of relationship between quality attributes and design property defined, we have calculated and aggregated these qualities attributes and evaluate the quality of program input We have tested the system with number of different programs that vary in their complexities and functionalities.},
	booktitle = {2014 {International} {Conference} on {Issues} and {Challenges} in {Intelligent} {Computing} {Techniques} ({ICICT})},
	author = {Goyal, Puneet Kumar and Joshi, Gamini},
	month = feb,
	year = {2014},
	keywords = {Computational modeling, Couplings, Encapsulation, ISO, Java, Java Programming language, Measurement, QMOOD metrics, QMOOD model, software engineering, software quality},
	pages = {520--533},
}

@article{Guest2006InterviewDataSaturationandVariability,
	title = {How {Many} {Interviews} {Are} {Enough}?: {An} {Experiment} with {Data} {Saturation} and {Variability}},
	issn = {1525-822X, 1552-3969},
	shorttitle = {How {Many} {Interviews} {Are} {Enough}?},
	url = {http://journals.sagepub.com/doi/10.1177/1525822X05279903},
	doi = {10.1177/1525822X05279903},
	abstract = {Guidelines for determining nonprobabilistic sample sizes are virtually nonexistent. Purposive samples are the most commonly used form of nonprobabilistic sampling, and their size typically relies on the concept of “saturation,” or the point at which no new information or themes are observed in the data. Although the idea of saturation is helpful at the conceptual level, it provides little practical guidance for estimating sample sizes, prior to data collection, necessary for conducting quality research. Using data from a study involving sixty in-depth interviews with women in two West African countries, the authors systematically document the degree of data saturation and variability over the course of thematic analysis. They operationalize saturation and make evidence-based recommendations regarding nonprobabilistic sample sizes for interviews. Based on the data set, they found that saturation occurred within the first twelve interviews, although basic elements for metathemes were present as early as six interviews. Variability within the data followed similar patterns.},
	language = {en},
	urldate = {2022-12-27},
	journal = {Field Methods},
	author = {Guest, Greg and Bunce, Arwen and Johnson, Laura},
	year = {2006},
}

@inproceedings{Abdalkareem2017TrivialPackages,
	title = {Why do developers use trivial packages? an empirical case study on npm},
	abstract = {Code reuse is traditionally seen as good practice. Recent trends have pushed the concept of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call ‘trivial packages’. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix made it imperative to question the growing use of trivial packages.},
	booktitle = {European {Software} {Engineering} {Conference}/{Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Abdalkareem, Rabe and Nourry, Olivier and Wehaibi, Sultan and Mujahid, Suhaib and Shihab, Emad},
	year = {2017},
}

@article{Makitalo2020OpportunisticSWReuse,
	title = {On opportunistic software reuse},
	volume = {102},
	issn = {0010-485X, 1436-5057},
	url = {https://link.springer.com/10.1007/s00607-020-00833-6},
	doi = {10.1007/s00607-020-00833-6},
	abstract = {The availability of open source assets for almost all imaginable domains has led the software industry to opportunistic design—an approach in which people develop new software systems in an ad hoc fashion by reusing and combining components that were not designed to be used together. In this paper we investigate this emerging approach. We demonstrate the approach with an industrial example in which Node.js modules and various subsystems are used in an opportunistic way. Furthermore, to study opportunistic reuse as a phenomenon, we present the results of three contextual interviews and a survey with reuse practitioners to understand to what extent opportunistic reuse offers improvements over traditional systematic reuse approaches.},
	language = {en},
	number = {11},
	urldate = {2022-12-30},
	journal = {Computing},
	author = {Mäkitalo, Niko and Taivalsaari, Antero and Kiviluoto, Arto and Mikkonen, Tommi and Capilla, Rafael},
	month = nov,
	year = {2020},
	pages = {2385--2408},
}

@misc{HuggingFaceWeb,
	title = {Hugging {Face} – {The} {AI} community building the future.},
	url = {https://huggingface.co/},
	urldate = {2021-11-17},
	author = {Hugging Face},
	year = {2021},
}

@misc{ModelhubWeb,
	title = {Modelhub},
	url = {http://modelhub.ai/},
	author = {Computational Imaging {and} Bioinformatics Lab},
	year = {2022},
}

@inproceedings{Vu2021LastPyMile,
	title = {{LastPyMile}: identifying the discrepancy between sources and packages},
	abstract = {Open source packages have source code available on repositories for inspection (e.g. on GitHub) but developers use pre-built packages directly from the package repositories (such as npm for JavaScript, PyPI for Python, or RubyGems for Ruby).},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Vu, Duc-Ly and Massacci, Fabio and Pashchenko, Ivan and Plate, Henrik and Sabetta, Antonino},
	year = {2021},
}

@article{Wang2019SurveyofDLMLFrameworkandLibrary,
	title = {Various {Frameworks} and {Libraries} of {Machine} {Learning} and {Deep} {Learning}: {A} {Survey}},
	issn = {1134-3060, 1886-1784},
	shorttitle = {Various {Frameworks} and {Libraries} of {Machine} {Learning} and {Deep} {Learning}},
	url = {http://link.springer.com/10.1007/s11831-018-09312-w},
	doi = {10.1007/s11831-018-09312-w},
	abstract = {With the rapid development of deep learning in various fields, the big companies and research teams have developed independent and unique tools. This paper collects 18 common deep learning frameworks and libraries (Caffe, Caffe2, Tensorflow, Theano include Keras Lasagnes and Blocks, MXNet, CNTK, Torch, PyTorch, Pylearn2, Scikit-learn, Matlab include MatconvNet Matlab deep learning and Deep learning tool box, Chainer, Deeplearning4j) and introduces a large number of benchmarking data. In addition, we give the overall score of the current eight mainstream deep learning frameworks from six aspects (model design ability, interface property, deployment ability, performance, framework design and prospects for development). Based on our overview, the deep learning researchers can choose the appropriate development tools according to the evaluation criteria. By summarizing the 18 deep learning frameworks and libraries, we have found that most of the deep learning tools are moving closer to the mobile terminal, and the role of ASICs is gradually emerging. It is believed that the future deep learning applications will be inseparable from the ASIC support.},
	language = {en},
	urldate = {2022-01-12},
	journal = {Archives of Computational Methods in Engineering},
	author = {Wang, Zhaobin and Liu, Ke and Li, Jian and Zhu, Ying and Zhang, Yaonan},
	month = feb,
	year = {2019},
}

@misc{Chen2022DLFrameworkBug,
	title = {Toward {Understanding} {Deep} {Learning} {Framework} {Bugs}},
	url = {http://arxiv.org/abs/2203.04026},
	abstract = {DL frameworks are the basis of constructing all DL programs and models, and thus their bugs could lead to the unexpected behaviors of any DL program or model relying on them. Such wide effect demonstrates the necessity and importance of guaranteeing DL frameworks' quality. Understanding the characteristics of DL framework bugs is a fundamental step for this quality assurance task, facilitating to design effective bug detection and debugging approaches. Hence, in this work we conduct the most large-scale study on 800 bugs from four popular and diverse DL frameworks (i.e., TensorFlow, PyTorch, MXNet, and DL4J). By analyzing the root causes and symptoms of DL framework bugs associated with 5 components decomposed from DL frameworks, as well as measuring test coverage achieved by three state-of-the-art testing techniques and developers' efforts on fixing those bugs, we obtain 14 major findings for the comprehensive understanding of DL framework bugs and the current status of existing DL framework testing and debugging practice, and then provide a series of actionable guidelines for better DL framework bug detection and debugging.},
	urldate = {2022-10-04},
	publisher = {arXiv},
	author = {Chen, Junjie and Liang, Yihua and Shen, Qingchao and Jiang, Jiajun},
	month = mar,
	year = {2022},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{Wang2022WechatBugStudy,
	title = {Characterizing and {Detecting} {Bugs} in {WeChat} {Mini}-{Programs}},
	doi = {10.1145/3510003.3510114},
	abstract = {Built on the WeChat social platform, WeChat Mini-Programs are widely used by more than 400 million users every day. Consequently, the reliability of Mini-Programs is particularly crucial. However, WeChat Mini-Programs suffer from various bugs related to execution environment, lifecycle management, asynchronous mechanism, etc. These bugs have seriously affected users' experience and caused serious impacts. In this paper, we conduct the first empirical study on 83 WeChat Mini-Program bugs, and perform an in-depth analysis of their root causes, impacts and fixes. From this study, we obtain many interesting findings that can open up new research directions for combating WeChat Mini-Program bugs. Based on the bug patterns found in our study, we further develop WeDetector to detect WeChat Mini-Program bugs. Our evaluation on 25 real-world Mini-Programs has found 11 previously unknown bugs, and 7 of them have been confirmed by developers.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Wang, Tao and Xu, Qingxin and Chang, Xiaoning and Dou, Wensheng and Zhu, Jiaxin and Xie, Jinhui and Deng, Yuetang and Yang, Jianbo and Yang, Jiaheng and Wei, Jun and Huang, Tao},
	year = {2022},
	keywords = {Computer bugs, Message service, Reliability, Social networking (online), Software engineering, User experience, WeChat Mini-Programs, bug detection, empirical study},
	pages = {363--375},
}

@article{baltes_sampling_2022,
	title = {Sampling in software engineering research: a critical review and guidelines},
	volume = {27},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Sampling in software engineering research},
	url = {https://link.springer.com/10.1007/s10664-021-10072-8},
	doi = {10.1007/s10664-021-10072-8},
	abstract = {Representative sampling appears rare in empirical software engineering research. Not all studies need representative samples, but a general lack of representative sampling undermines a scientific field. This article therefore reports a critical review of the state of sampling in recent, high-quality software engineering research. The key findings are: (1) random sampling is rare; (2) sophisticated sampling strategies are very rare; (3) sampling, representativeness and randomness often appear misunderstood. These findings suggest that software engineering research has a generalizability crisis. To address these problems, this paper synthesizes existing knowledge of sampling into a succinct primer and proposes extensive guidelines for improving the conduct, presentation and evaluation of sampling in software engineering research. It is further recommended that while researchers should strive for more representative samples, disparaging non-probability sampling is generally capricious and particularly misguided for predominately qualitative research.},
	language = {en},
	number = {4},
	urldate = {2022-12-27},
	journal = {Empirical Software Engineering},
	author = {Baltes, Sebastian and Ralph, Paul},
	month = jul,
	year = {2022},
	pages = {94},
}

@article{hristov_structuring_2012,
	title = {Structuring {Software} {Reusability} {Metrics} for {Component}-{Based} {Software} {Development}},
	abstract = {The idea of reusing software components has been present in software engineering for several decades. Although the software industry developed massively in recent decades, component reuse is still facing numerous challenges and lacking adoption by practitioners. One of the impediments preventing efficient and effective reuse is the difficulty to determine which artifacts are best suited to solve a particular problem in a given context and how easy it will be to reuse them there. So far, no clear framework is describing the reusability of software and structuring appropriate metrics that can be found in literature. Nevertheless, a good understanding of reusability as well as adequate and easy to use metrics for quantification of reusability are necessary to simplify and accelerate the adoption of component reuse in software development. Thus, we propose an initial version of such a framework intended to structure existing reusability metrics for component-based software development that we have collected for this paper.},
	language = {en},
	author = {Hristov, Danail and Hummel, Oliver and Huq, Mahmudul and Janjic, Werner},
	year = {2012},
}

@inproceedings{Shiva2007SWReuseResearchandPractice,
	address = {Las Vegas, NV, USA},
	title = {Software {Reuse}: {Research} and {Practice}},
	isbn = {978-0-7695-2776-5},
	shorttitle = {Software {Reuse}},
	url = {http://ieeexplore.ieee.org/document/4151749/},
	doi = {10.1109/ITNG.2007.182},
	abstract = {It has been more than three decades since the idea of software reuse was proposed. Many success stories have been told, yet it is believed that software reuse is still in the development phase and has not reached its full potential. How far are we with software reuse research and practice? This paper is an attempt to answer this question. Keywords: Software Reuse, Architecture, Domain Engineering, Metrics, Component.},
	language = {en},
	urldate = {2022-12-22},
	booktitle = {Fourth {International} {Conference} on {Information} {Technology} ({ITNG}'07)},
	publisher = {IEEE},
	author = {Shiva, Sajjan G. and Shala, Lubna Abou},
	month = apr,
	year = {2007},
	pages = {603--609},
}

@misc{MicrosoftSTRIDE,
	title = {The {STRIDE} {Threat} {Model}},
	url = {https://learn.microsoft.com/en-us/previous-versions/commerce-server/ee823878(v=cs.20)},
	language = {en-us},
	urldate = {2022-12-20},
	author = {Microsoft},
	year = {2021},
}

@misc{AWSSTRIDE,
	title = {How to approach threat modeling},
	url = {https://aws.amazon.com/blogs/security/how-to-approach-threat-modeling/},
	urldate = {2022-12-20},
	author = {Boyd, Darran},
	year = {2021},
}

@article{monteil_nine_2020,
	title = {Nine {Best} {Practices} for {Research} {Software} {Registries} and {Repositories}: {A} {Concise} {Guide}},
	shorttitle = {Nine {Best} {Practices} for {Research} {Software} {Registries} and {Repositories}},
	url = {http://arxiv.org/abs/2012.13117},
	abstract = {Scientific software registries and repositories serve various roles in their respective disciplines. These resources improve software discoverability and research transparency, provide information for software citations, and foster preservation of computational methods that might otherwise be lost over time, thereby supporting research reproducibility and replicability. However, developing these resources takes effort, and few guidelines are available to help prospective creators of registries and repositories. To address this need, we present a set of nine best practices that can help managers define the scope, practices, and rules that govern individual registries and repositories. These best practices were distilled from the experiences of the creators of existing resources, convened by a Task Force of the FORCE11 Software Citation Implementation Working Group during the years 2019-2020. We believe that putting in place specific policies such as those presented here will help scientific software registries and repositories better serve their users and their disciplines.},
	urldate = {2022-05-05},
	journal = {arXiv:2012.13117 [cs]},
	author = {Monteil, Alain and Gonzalez-Beltran, Alejandra and Ioannidis, Alexandros and Allen, Alice and Lee, Allen and Bandrowski, Anita and Wilson, Bruce E. and Mecum, Bryce and Du, Cai Fan and Robinson, Carly and Garijo, Daniel and Katz, Daniel S. and Long, David and Milliken, Genevieve and Ménager, Hervé and Hausman, Jessica and Spaaks, Jurriaan H. and Fenlon, Katrina and Vanderbilt, Kristin and Hwang, Lorraine and Davis, Lynn and Fenner, Martin and Crusoe, Michael R. and Hucka, Michael and Wu, Mingfang and Hong, Neil Chue and Teuben, Peter and Stall, Shelley and Druskat, Stephan and Carnevale, Ted and Morrell, Thomas},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.13117},
	keywords = {Computer Science - Computers and Society, Computer Science - Digital Libraries},
}

@inproceedings{mmdnn,
	title = {Enhancing the interoperability between deep learning frameworks by model conversion},
	abstract = {Deep learning (DL) has become one of the most successful machine learning techniques. To achieve the optimal development result, there are emerging requirements on the interoperability between DL frameworks that the trained model files and training/serving programs can be re-utilized. Faithful model conversion is a promising technology to enhance the framework interoperability in which a source model is transformed into the semantic equivalent in another target framework format. However, several major challenges need to be addressed. First, there are apparent discrepancies between DL frameworks. Second, understanding the semantics of a source model could be difficult due to the framework scheme and optimization. Lastly, there exist a large number of DL frameworks, bringing potential significant engineering efforts.},
	booktitle = {European {Software} {Engineering} {Conference}/{Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Liu, Yu and Chen, Cheng and Zhang, Ru and Qin, Tingting and Ji, Xiang and Lin, Haoxiang and Yang, Mao},
	year = {2020},
}

@inproceedings{kaplunovich_refactoring_2020,
	address = {Seoul Republic of Korea},
	title = {Refactoring of {Neural} {Network} {Models} for {Hyperparameter} {Optimization} in {Serverless} {Cloud}},
	isbn = {978-1-4503-7963-2},
	url = {https://dl.acm.org/doi/10.1145/3387940.3392268},
	doi = {10.1145/3387940.3392268},
	abstract = {Machine Learning and Neural Networks in particular have become hot topics in Computer Science. The recent 2019 Turing award to the forefathers of Deep Learning and AI - Yoshua Bengio, Geoffrey Hinton, and Yann LeCun proves the importance of the technology and its effect on science and industry. However, we have realized that even nowadays, the state of the art methods require several manual steps for neural network hyperparameter optimization. Our approach automates the model tuning by refactoring the original Python code using open-source libraries for processing. We were able to identify hyperparameters by parsing the original source and analyzing it. Given these parameters, we refactor the model, add the state of the art optimization library calls, and run the updated code in the Serverless Cloud. Our approach has proven to eliminate manual steps for an arbitrary TensorFlow and Keras tuning. We have created a tool called OptPar which automatically refactors an arbitrary Deep Neural Network optimizing its hyperparameters. Such a transformation can save hours of time for Data Scientists, giving them an opportunity to concentrate on designing their Machine Learning algorithms.},
	language = {en},
	urldate = {2022-12-13},
	booktitle = {Proceedings of the {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} {Workshops}},
	publisher = {ACM},
	author = {Kaplunovich, Alex and Yesha, Yelena},
	month = jun,
	year = {2020},
	pages = {311--314},
}

@inproceedings{kaur_improving_2017,
	title = {Improving the quality of software by refactoring},
	doi = {10.1109/ICCONS.2017.8250707},
	abstract = {Software code management has become another key skill required by software architects and software developers. Size of software increases with increase in count of features in software. Code refactoring is process of reducing code maintenance cost. It is achieved by many different techniques like extract, move methods, fields or classes in code. In this research we focused on improving the maintainability of the code by looking into the different refactoring techniques and improving upon them. We proposed an algorithm to improve the refactoring process which results in higher maintainability. To look into the validity of our proposed algorithm, we have used Junit and reffinder to analyse the code and generate the result metrics. We have observed the effectiveness of our work by comparing the different code maintainability indexes generated by the tool. In our research we have examined four releases of the software project for code refactoring and maintainability. Adding some extra features and using enhanced refactoring techniques measuring the code metrics and comparing the results of current releases with the previous releases.},
	booktitle = {2017 {International} {Conference} on {Intelligent} {Computing} and {Control} {Systems} ({ICICCS})},
	author = {Kaur, Gurpreet and Singh, Balraj},
	month = jun,
	year = {2017},
	keywords = {Cloning, Code refactoring, Couplings, Data mining, Maintenance engineering, Size measurement, Software, bad smells, refactoring process, software metrics, software quality attributes},
	pages = {185--191},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1374314},
	urldate = {2022-12-13},
}

@inproceedings{kaur_improving_2017-1,
	title = {Improving the quality of software by refactoring},
	doi = {10.1109/ICCONS.2017.8250707},
	abstract = {Software code management has become another key skill required by software architects and software developers. Size of software increases with increase in count of features in software. Code refactoring is process of reducing code maintenance cost. It is achieved by many different techniques like extract, move methods, fields or classes in code. In this research we focused on improving the maintainability of the code by looking into the different refactoring techniques and improving upon them. We proposed an algorithm to improve the refactoring process which results in higher maintainability. To look into the validity of our proposed algorithm, we have used Junit and reffinder to analyse the code and generate the result metrics. We have observed the effectiveness of our work by comparing the different code maintainability indexes generated by the tool. In our research we have examined four releases of the software project for code refactoring and maintainability. Adding some extra features and using enhanced refactoring techniques measuring the code metrics and comparing the results of current releases with the previous releases.},
	booktitle = {2017 {International} {Conference} on {Intelligent} {Computing} and {Control} {Systems} ({ICICCS})},
	author = {Kaur, Gurpreet and Singh, Balraj},
	month = jun,
	year = {2017},
	keywords = {Cloning, Code refactoring, Couplings, Data mining, Maintenance engineering, Size measurement, Software, bad smells, refactoring process, software metrics, software quality attributes},
	pages = {185--191},
}

@inproceedings{murgia_parameter-based_2011,
	title = {Parameter-{Based} {Refactoring} and the {Relationship} with {Fan}-in/{Fan}-out {Coupling}},
	doi = {10.1109/ICSTW.2011.26},
	abstract = {Refactoring is an activity which, in theory, should have minimal impact on the overall structure of a system. That said, certain refactorings change the coupling profile of a system and over time those cumulative changes in coupling can have serious implications for system maintenance effort. In this paper, we analyse effect of the fan-in and fan-out metrics from the perspective of two refactorings - namely 'Add parameter' to, and 'Remove Parameter' from, a method. We developed a bespoke pattern-matching tool to collect these two refactorings from multiple releases of the Tomcat open-source system using the Evolizer tool to extract method signature data and the JHawk metrics tool to collect the two coupling metrics. Results point to significant differences in the profiles of fan-in and fan-out between refactored and non-refactored classes. We describe how software company can take advantage from this knowledge by defining a priority list of classes which could require a refactoring. A strong over-arching theme emerged: developers seemed to focus on the refactoring of classes with relatively high fan-in and fan-out rather than classes with high values in any one. The study is the first that we know of to analyse the direct effect of a subset of Fowler's refactorings on fan-in and fan-out - relevant metrics of the overall structure of a system.},
	booktitle = {2011 {IEEE} {Fourth} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops}},
	author = {Murgia, A. and Marchesi, M. and Concas, G. and Tonelli, R. and Counsell, S.},
	month = mar,
	year = {2011},
	keywords = {Conferences, Helium, Refactoring, Software testing, fan-in, fan-out, parameters},
	pages = {430--436},
}

@inproceedings{alizadeh_refbot_2019,
	title = {{RefBot}: {Intelligent} {Software} {Refactoring} {Bot}},
	shorttitle = {{RefBot}},
	doi = {10.1109/ASE.2019.00081},
	abstract = {The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost. In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any "open" or "merge" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Alizadeh, Vahid and Ouali, Mohamed Amine and Kessentini, Marouane and Chater, Meriem},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Bot (Internet), Manuals, Measurement, Object oriented modeling, Pipelines, Software, Software bot, Software quality, Tools, refactoring},
	pages = {823--834},
}

@misc{noauthor_ieee_nodate-1,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9835158},
	urldate = {2022-12-05},
}

@misc{noauthor_ieee_nodate-2,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8945075},
	urldate = {2022-12-05},
}

@inproceedings{hohman_understanding_2020,
	address = {Honolulu HI USA},
	title = {Understanding and {Visualizing} {Data} {Iteration} in {Machine} {Learning}},
	isbn = {978-1-4503-6708-0},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376177},
	doi = {10.1145/3313831.3376177},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Hohman, Fred and Wongsuphasawat, Kanit and Kery, Mary Beth and Patel, Kayur},
	month = apr,
	year = {2020},
	pages = {1--13},
}

@book{wang_atmseer_2019,
	title = {{ATMSeer}: {Increasing} {Transparency} and {Controllability} in {Automated} {Machine} {Learning}},
	shorttitle = {{ATMSeer}},
	abstract = {To relieve the pain of manually selecting machine learning algorithms and tuning hyperparameters, automated machine learning (AutoML) methods have been developed to automatically search for good models. Due to the huge model search space, it is impossible to try all models. Users tend to distrust automatic results and increase the search budget as much as they can, thereby undermining the efficiency of AutoML. To address these issues, we design and implement ATMSeer, an interactive visualization tool that supports users in refining the search space of AutoML and analyzing the results. To guide the design of ATMSeer, we derive a workflow of using AutoML based on interviews with machine learning experts. A multi-granularity visualization is proposed to enable users to monitor the AutoML process, analyze the searched models, and refine the search space in real time. We demonstrate the utility and usability of ATMSeer through two case studies, expert interviews, and a user study with 13 end users.},
	author = {Wang, Qianwen and Ming, Yao and Jin, Zhihua and Shen, Qiaomu and Liu, Dongyu and Smith, Micah and Veeramachaneni, Kalyan and Qu, Huamin},
	month = feb,
	year = {2019},
	doi = {10.1145/3290605.3300911},
}

@inproceedings{bogart_increasing_2020,
	address = {Seoul Republic of Korea},
	title = {Increasing the {Trust} {In} {Refactoring} {Through} {Visualization}},
	isbn = {978-1-4503-7963-2},
	url = {https://dl.acm.org/doi/10.1145/3387940.3392190},
	doi = {10.1145/3387940.3392190},
	abstract = {In software development, maintaining good design is essential. The process of refactoring enables developers to improve this design during development without altering the program’s existing behavior. However, this process can be time-consuming, introduce semantic errors, and be diﬃcult for developers inexperienced with refactoring or unfamiliar with a given code base. Automated refactoring tools can help not only by applying these changes, but by identifying opportunities for refactoring. Yet, developers have not been quick to adopt these tools due to a lack of trust between the developer and the tool. We propose an approach in the form of a visualization to aid developers in understanding these suggested operations and increasing familiarity with automated refactoring tools. We also provide a manual validation of this approach and identify options to continue experimentation.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} {Workshops}},
	publisher = {ACM},
	author = {Bogart, Alex and AlOmar, Eman Abdullah and Mkaouer, Mohamed Wiem and Ouni, Ali},
	month = jun,
	year = {2020},
	pages = {334--341},
}

@article{murphy-hill_how_2012,
	title = {How {We} {Refactor}, and {How} {We} {Know} {It}},
	volume = {38},
	issn = {1939-3520},
	doi = {10.1109/TSE.2011.41},
	abstract = {Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Murphy-Hill, Emerson and Parnin, Chris and Black, Andrew P.},
	month = jan,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Java, Refactoring, Software tools, floss refactoring, refactoring tools, root-canal refactoring.},
	pages = {5--18},
}

@inproceedings{sellitto_toward_2022,
	address = {Honolulu, HI, USA},
	title = {Toward {Understanding} the {Impact} of {Refactoring} on {Program} {Comprehension}},
	isbn = {978-1-66543-786-8},
	url = {https://ieeexplore.ieee.org/document/9825885/},
	doi = {10.1109/SANER53432.2022.00090},
	abstract = {Software refactoring is the activity associated with developers changing the internal structure of source code without modifying its external behavior. The literature argues that refactoring might have beneﬁcial and harmful implications for software maintainability, primarily when performed without the support of automated tools. This paper continues the narrative on the effects of refactoring by exploring the dimension of program comprehension, namely the property that describes how easy it is for developers to understand source code. We start our investigation by assessing the basic unit of program comprehension, namely program readability. Next, we set up a large-scale empirical investigation – conducted on 156 opensource projects – to quantify the impact of refactoring on program readability. First, we mine refactoring data and, for each commit involving a refactoring, we compute (i) the amount and type(s) of refactoring actions performed and (ii) eight stateof-the-art program comprehension metrics. Afterwards, we build statistical models relating the various refactoring operations to each of the readability metrics considered to quantify the extent to which each refactoring impacts the metrics in either a positive or negative manner. The key results are that refactoring has a notable impact on most of the readability metrics considered.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {2022 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	publisher = {IEEE},
	author = {Sellitto, Giulia and Iannone, Emanuele and Codabux, Zadia and Lenarduzzi, Valentina and De Lucia, Andrea and Palomba, Fabio and Ferrucci, Filomena},
	month = mar,
	year = {2022},
	pages = {731--742},
}

@inproceedings{silva_recommending_2014,
	address = {Hyderabad, India},
	title = {Recommending automated extract method refactorings},
	isbn = {978-1-4503-2879-1},
	url = {http://dl.acm.org/citation.cfm?doid=2597008.2597141},
	doi = {10.1145/2597008.2597141},
	abstract = {Extract Method is a key refactoring for improving program comprehension. However, recent empirical research shows that refactoring tools designed to automate Extract Methods are often underused. To tackle this issue, we propose a novel approach to identify and rank Extract Method refactoring opportunities that are directly automated by IDE-based refactoring tools. Our approach aims to recommend new methods that hide structural dependencies that are rarely used by the remaining statements in the original method. We conducted an exploratory study to experiment and deﬁne the best strategies to compute the dependencies and the similarity measures used by the proposed approach. We also evaluated our approach in a sample of 81 extract method opportunities generated for JUnit and JHotDraw, achieving a precision of 48\% (JUnit) and 38\% (JHotDraw).},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Program} {Comprehension} - {ICPC} 2014},
	publisher = {ACM Press},
	author = {Silva, Danilo and Terra, Ricardo and Valente, Marco Tulio},
	year = {2014},
	pages = {146--156},
}

@article{murphy-hill_breaking_nodate,
	title = {Breaking the {Barriers} to {Successful} {Refactoring}: {Observations} and {Tools} for {Extract} {Method}},
	abstract = {Refactoring is the process of changing the structure of code without changing its behavior. Refactoring can be semi-automated with tools, which should make it easier for programmers to refactor quickly and correctly. However, we have observed that many tools do a poor job of communicating errors triggered by the refactoring process and that programmers using them sometimes refactor slowly, conservatively, and incorrectly. In this paper we characterize problems with current refactoring tools, demonstrate three new tools to assist in refactoring, and report on a user study that compares these new tools against existing tools. The results of the study show that speed, accuracy, and user satisfaction can be significantly increased. From the new tools we induce a set of usability recommendations that we hope will help inspire a new generation of programmer-friendly refactoring tools.},
	language = {en},
	author = {Murphy-Hill, Emerson and Black, Andrew P},
	pages = {10},
}

@phdthesis{nalepa_oizumi_identification_2022,
	address = {Rio de Janeiro, Brazil},
	type = {{DOUTOR} {EM} {CIÊNCIAS} - {INFORMÁTICA}},
	title = {{IDENTIFICATION} {AND} {REFACTORING} {OF} {DESIGN} {PROBLEMS} {IN} {SOFTWARE} {SYSTEMS}},
	url = {http://www.maxwell.vrac.puc-rio.br/Busca_etds.php?strSecao=resultado&nrSeq=60990@2},
	abstract = {Oizumi, Willian Nalepa; Garcia, Alessandro Fabricio (Advisor). Identification and Refactoring of Design Problems in Software Systems. Rio de Janeiro, 2022. 235p. Tese de doutorado –Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro.},
	language = {en},
	urldate = {2022-12-05},
	school = {PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO},
	author = {Nalepa Oizumi, Willian},
	month = apr,
	year = {2022},
	doi = {10.17771/PUCRio.acad.60990},
}

@article{ratzinger_improving_nodate,
	title = {Improving {Evolvability} through {Refactoring}},
	abstract = {Refactoring is one means of improving the structure of existing software. Locations for the application of refactoring are often based on subjective perceptions such as ”bad smells”, which are vague suspicions of design shortcomings. We exploit historical data extracted from repositories such as CVS and focus on change couplings: if some software parts change at the same time very often over several releases, this data can be used to point to candidates for refactoring. We adopt the concept of bad smells and provide additional change smells. Such a smell is hardly visible in the code, but easy to spot when viewing the change history. Our approach enables the detection of such smells allowing an engineer to apply refactoring on these parts of the source code to improve the evolvability of the software. For that, we analyzed the history of a large industrial system for a period of 15 months, proposed spots for refactorings based on change couplings, and performed them with the developers. After observing the system for another 15 months we ﬁnally analyzed the eﬀectiveness of our approach. Our results support our hypothesis that the combination of change dependency analysis and refactoring is applicable and eﬀective.},
	language = {en},
	author = {Ratzinger, Jacek and Fischer, Michael and Gall, Harald},
	pages = {5},
}

@inproceedings{advani_extracting_2006,
	address = {Dijon, France},
	title = {Extracting refactoring trends from open-source software and a possible solution to the 'related refactoring' conundrum},
	isbn = {978-1-59593-108-5},
	url = {http://portal.acm.org/citation.cfm?doid=1141277.1141685},
	doi = {10.1145/1141277.1141685},
	abstract = {Refactoring, as a software engineering discipline has emerged over recent years to become an important aspect of maintaining software. Refactoring refers to the restructuring of software according to specific mechanics and principles. In this paper, we describe a tool that allows refactoring data across multiple versions of seven opensource software systems to be collected. The tool automates the identification of refactorings as program transformations between consecutive software releases. The same tool thus allowed an empirical analysis of software development across versions from the perspective of those transformations. We describe results for the systems analysed and point to key conclusions from our analysis. In particular, we investigate a problematic empirical question as to whether certain refactorings are related, i.e., they cannot be undertaken in isolation without other refactorings being undertaken in parallel. In this context, we focus specifically on the four most common refactorings identified by the tool from three of the opensource systems and use a dependency graph to inform conclusions about the empirical data extracted by the tool. An interesting result relating to some common refactorings is described.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 2006 {ACM} symposium on {Applied} computing  - {SAC} '06},
	publisher = {ACM Press},
	author = {Advani, Deepak and Hassoun, Youssef and Counsell, Steve},
	year = {2006},
	pages = {1713},
}

@article{sculley_machine_nodate,
	title = {Machine {Learning}: {The} {High}-{Interest} {Credit} {Card} of {Technical} {Debt}},
	abstract = {Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning speciﬁc risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
	language = {en},
	author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
	pages = {9},
}

@inproceedings{velez_challenges_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Challenges in migrating imperative deep learning programs to graph execution: an empirical study},
	isbn = {978-1-4503-9303-4},
	shorttitle = {Challenges in migrating imperative deep learning programs to graph execution},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528455},
	doi = {10.1145/3524842.3528455},
	abstract = {Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code that supports symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development tends to produce DL code that is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, less error-prone imperative DL frameworks encouraging eager execution have emerged at the expense of run-time performance. While hybrid approaches aim for the “best of both worlds,” the challenges in applying them in the real world are largely unknown. We conduct a data-driven analysis of challenges—and resultant bugs—involved in writing reliable yet performant imperative DL code by studying 250 open-source projects, consisting of 19.7 MLOC, along with 470 and 446 manually examined code patches and bug reports, respectively. The results indicate that hybridization: (i) is prone to API misuse, (ii) can result in performance degradation—the opposite of its intention, and (iii) has limited application due to execution mode incompatibility. We put forth several recommendations, best practices, and anti-patterns for effectively hybridizing imperative DL code, potentially benefiting DL practitioners, API designers, tool developers, and educators.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Vélez, Tatiana Castro and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Raja, Anita},
	month = may,
	year = {2022},
	pages = {469--481},
}

@inproceedings{dilhara_discovering_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Discovering repetitive code changes in python {ML} systems},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510225},
	doi = {10.1145/3510003.3510225},
	abstract = {Over the years, researchers capitalized on the repetitiveness of software changes to automate many software evolution tasks. Despite the extraordinary rise in popularity of Python-based ML systems, they do not benefit from these advances. Without knowing what are the repetitive changes that ML developers make, researchers, tool, and library designers miss opportunities for automation, and ML developers fail to learn and use best coding practices.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Dilhara, Malinda and Ketkar, Ameya and Sannidhi, Nikhith and Dig, Danny},
	month = may,
	year = {2022},
	pages = {736--748},
}

@inproceedings{li_scaling_2014,
	address = {Beijing, China},
	title = {Scaling {Distributed} {Machine} {Learning} with the {Parameter} {Server}},
	isbn = {978-1-4503-2891-3},
	url = {http://dl.acm.org/citation.cfm?doid=2640087.2644155},
	doi = {10.1145/2640087.2644155},
	abstract = {We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports ﬂexible consistency models, elastic scalability, and continuous fault tolerance.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Big} {Data} {Science} and {Computing} - {BigDataScience} '14},
	publisher = {ACM Press},
	author = {Li, Mu},
	year = {2014},
	pages = {1--1},
}

@inproceedings{tang_empirical_2021,
	title = {An {Empirical} {Study} of {Refactorings} and {Technical} {Debt} in {Machine} {Learning} {Systems}},
	doi = {10.1109/ICSE43902.2021.00033},
	abstract = {Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today's data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semantics-preserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major cross-cutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Tang, Yiming and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Singh, Rhia and Stewart, Ajani and Raja, Anita},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Best practices, Complex systems, Data mining, Deep learning, Open source software, Software engineering, Tools, empirical studies, machine learning systems, refactoring, software evolution, software repository mining, technical debt},
	pages = {238--250},
}

@inproceedings{Ross2018RefactoringML,
	title = {Refactoring {Machine} {Learning}},
	abstract = {Results in machine learning scholarship are sometimes based on untested, difﬁcultto-read code that has only been seen by a single researcher. We argue that this is bad, and that machine learning scholarship could be improved by adhering to software development best practices. We identify several practices which are not widely followed within the research community but we believe would help improve the reliability of results. We describe how to apply these best practices in a machine learning research setting. Finally, we suggest several modiﬁcations to the publication review process to encourage adherence.},
	language = {en},
	booktitle = {Workshop on {Critiquing} and {Correcting} {Trends} in {Machine} {Learning} at {NeuRIPS}},
	author = {Ross, Andrew Slavin and Forde, Jessica Zosa},
	year = {2018},
	pages = {6},
}

@article{olah_research_2017,
	title = {Research {Debt}},
	volume = {2},
	issn = {2476-0757},
	url = {https://distill.pub/2017/research-debt},
	doi = {10.23915/distill.00005},
	abstract = {Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...},
	language = {en},
	number = {3},
	urldate = {2022-12-05},
	journal = {Distill},
	author = {Olah, Chris and Carter, Shan},
	month = mar,
	year = {2017},
	pages = {e5},
}

@article{smith_collaborative_nodate,
	title = {Collaborative, open, and automated data science},
	abstract = {Data science and machine learning have already revolutionized many industries and organizations and are increasingly being used in an open-source setting to address important societal problems. However, there remain many challenges to developing predictive machine learning models in practice, such as the complexity of the steps in the modern data science development process, the involvement of many different people with varying skills and roles, and the necessity of, yet difficulty in, collaborating across steps and people. In this thesis, I describe progress in two directions in supporting the development of predictive models.},
	language = {en},
	author = {Smith, Micah J},
	pages = {187},
}

@article{smith_enabling_2021,
	title = {Enabling {Collaborative} {Data} {Science} {Development} with the {Ballet} {Framework}},
	volume = {5},
	issn = {2573-0142},
	url = {https://dl.acm.org/doi/10.1145/3479575},
	doi = {10.1145/3479575},
	abstract = {While the open-source software development model has led to successful large-scale collaborations in building software systems, data science projects are frequently developed by individuals or small teams. We describe challenges to scaling data science collaborations and present a conceptual framework and ML programming model to address them. We instantiate these ideas in Ballet, the first lightweight framework for collaborative, open-source data science through a focus on feature engineering, and an accompanying cloud-based development environment. Using our framework, collaborators incrementally propose feature definitions to a repository which are each subjected to software and ML performance validation and can be automatically merged into an executable feature engineering pipeline. We leverage Ballet to conduct a case study analysis of an income prediction problem with 27 collaborators, and discuss implications for future designers of collaborative projects.},
	language = {en},
	number = {CSCW2},
	urldate = {2022-12-02},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Smith, Micah J. and Cito, Jürgen and Lu, Kelvin and Veeramachaneni, Kalyan},
	month = oct,
	year = {2021},
	pages = {1--39},
}

@book{Fowler2018SWRefactoring,
	title = {Refactoring},
	isbn = {978-0-13-475770-4},
	abstract = {Fully Revised and Updated–Includes New Refactorings and Code Examples  “Any fool can write code that a computer can understand. Good programmers write code that humans can understand.”  —M. Fowler (1999)For more than twenty years, experienced programmers worldwide have relied on Martin Fowler’s Refactoring to improve the design of existing code and to enhance software maintainability, as well as to make existing code easier to understand.    This eagerly awaited new edition has been fully updated to reflect crucial changes in the programming landscape.  Refactoring, Second Edition,  features an updated catalog of refactorings and includes JavaScript code examples, as well as new functional examples that demonstrate refactoring without classes.    Like the original, this edition explains what refactoring is; why you should refactor; how to recognize code that needs refactoring; and how to actually do it successfully, no matter what language you use.  Understand the process and general principles of refactoring Quickly apply useful refactorings to make a program easier to comprehend and change Recognize “bad smells” in code that signal opportunities to refactor Explore the refactorings, each with explanations, motivation, mechanics, and simple examples Build solid tests for your refactorings Recognize tradeoffs and obstacles to refactoring   Includes free access to the canonical web edition, with even more refactoring resources. (See inside the book for details about how to access the web edition.)},
	language = {en},
	publisher = {Addison-Wesley Professional},
	author = {Fowler, Martin},
	month = nov,
	year = {2018},
	keywords = {Computers / Programming / Object Oriented, Computers / Software Development \& Engineering / General},
}

@article{Opdyke1992Refactoring,
	title = {Refactoring {Object}-{Oriented} {Frameworks}},
	language = {en},
	author = {Opdyke, William},
	year = {1992},
	pages = {206},
}

@article{Mesfin2014SWRefactoringLitReview,
	title = {Trends, {Opportunities} and {Challenges} of {Software} {Refactoring}: {A} {Systematic} {Literature} {Review}},
	abstract = {Software refactoring is a technique that transforms the various types of software artifacts to improve the software internal structure without affecting the external behavior. Refactoring is commonly applied to improve the software quality after a significant amount of features are added. Researchers in the area have studied the different angles of refactoring and developed the right evidence, knowledge and skill. And they published their research findings through journals and conference papers to provide an easy access to everyone. Eventually, the knowledge accumulated in these literatures is huge, so that it needs structuring and organizing. The main purpose of this study is to extend a previously conducted study by covering more literatures and applying a systematic literature review method to increase the accuracy and validity of the study. We study a collection of literature from different electronic databases, published since 1999 to understand and extract the software refactoring knowledge through classification and summarization. The classification and summarization can reveal the research pattern, common concerns and statistics of the published papers in the last fifteen years. The extracted information should help the researchers to formulate better research topics that can solve the crucial problems in software refactoring and save the researchers effort and time.},
	language = {en},
	journal = {International Journal of Software Engineering and Its Applications},
	author = {Abebe, Mesfin and Yoo, Cheol-Jung},
	year = {2014},
	pages = {20},
}

@article{Ward1992TechDebt,
	title = {The {WyCash} portfolio management system},
	volume = {ACM SIGPLAN OOPS Messenger},
	language = {en},
	author = {Cunningham, Ward},
	year = {1992},
	pages = {29--30},
}

@incollection{viale_pereira_technical_2020,
	address = {Cham},
	title = {Technical {Debt} {Management}: {A} {Systematic} {Literature} {Review} and {Research} {Agenda} for {Digital} {Government}},
	volume = {12219},
	isbn = {978-3-030-57598-4 978-3-030-57599-1},
	shorttitle = {Technical {Debt} {Management}},
	url = {https://link.springer.com/10.1007/978-3-030-57599-1_10},
	abstract = {Technical debt is created when software engineers knowingly or unknowingly introduce shortcuts or unsuitable choices in the development or maintenance of the software system, that will have a negative impact on the future evolution of the system until corrected. Therefore, it is crucial to manage established debt particular in the public sector. The aim of this study is to introduce Technical debt to the field of Digital Government. We create an overview of the state of the art of the knowledge on technical debt management, the methods applied to gain this knowledge, and propose a research agenda to Digital Government scholars. We conduct a systematic literature review, which focuses on the concept of technical debt management. Forty-nine papers published within 2017-2020 are selected and analyzed. We identify several gaps in the existing literature: 1) an absence of theory explaining the relation of events, 2) a shortage of studies conducted in the public sector, 3) and an absence of specific techniques such as observation to study actual technical debt management behavior.},
	language = {en},
	urldate = {2022-11-29},
	booktitle = {Electronic {Government}},
	publisher = {Springer International Publishing},
	author = {Nielsen, Mille Edith and Østergaard Madsen, Christian and Lungu, Mircea Filip},
	editor = {Viale Pereira, Gabriela and Janssen, Marijn and Lee, Habin and Lindgren, Ida and Rodríguez Bolívar, Manuel Pedro and Scholl, Hans Jochen and Zuiderwijk, Anneke},
	year = {2020},
	doi = {10.1007/978-3-030-57599-1_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {121--137},
}

@incollection{Viale2022TDManagement,
	address = {Cham},
	title = {Technical {Debt} {Management}: {A} {Systematic} {Literature} {Review} and {Research} {Agenda} for {Digital} {Government}},
	volume = {12219},
	isbn = {978-3-030-57598-4 978-3-030-57599-1},
	shorttitle = {Technical {Debt} {Management}},
	url = {https://link.springer.com/10.1007/978-3-030-57599-1_10},
	abstract = {Technical debt is created when software engineers knowingly or unknowingly introduce shortcuts or unsuitable choices in the development or maintenance of the software system, that will have a negative impact on the future evolution of the system until corrected. Therefore, it is crucial to manage established debt particular in the public sector. The aim of this study is to introduce Technical debt to the field of Digital Government. We create an overview of the state of the art of the knowledge on technical debt management, the methods applied to gain this knowledge, and propose a research agenda to Digital Government scholars. We conduct a systematic literature review, which focuses on the concept of technical debt management. Forty-nine papers published within 2017-2020 are selected and analyzed. We identify several gaps in the existing literature: 1) an absence of theory explaining the relation of events, 2) a shortage of studies conducted in the public sector, 3) and an absence of specific techniques such as observation to study actual technical debt management behavior.},
	language = {en},
	urldate = {2022-11-29},
	booktitle = {Electronic {Government}},
	publisher = {Springer International Publishing},
	author = {Nielsen, Mille Edith and Østergaard Madsen, Christian and Lungu, Mircea Filip},
	editor = {Viale Pereira, Gabriela and Janssen, Marijn and Lee, Habin and Lindgren, Ida and Rodríguez Bolívar, Manuel Pedro and Scholl, Hans Jochen and Zuiderwijk, Anneke},
	year = {2020},
	pages = {121--137},
}

@inproceedings{Ernst2015SWPractitionersandTechnicalDebt,
	address = {Bergamo Italy},
	title = {Measure it? {Manage} it? {Ignore} it? software practitioners and technical debt},
	isbn = {978-1-4503-3675-8},
	shorttitle = {Measure it?},
	url = {https://dl.acm.org/doi/10.1145/2786805.2786848},
	doi = {10.1145/2786805.2786848},
	abstract = {The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and nontechnical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1,831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both nonparametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore, while respondents believe the metaphor is itself important for communication, existing tools are not currently helpful in managing the details. We use our results to motivate a technical debt timeline to focus management and tooling approaches.},
	language = {en},
	urldate = {2022-03-02},
	booktitle = {Proceedings of the 2015 10th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ernst, Neil A. and Bellomo, Stephany and Ozkaya, Ipek and Nord, Robert L. and Gorton, Ian},
	month = aug,
	year = {2015},
	pages = {50--60},
}

@inproceedings{hempel_d_2018,
	address = {Gothenburg Sweden},
	title = {D {\textless}span style="font-variant:small-caps;"{\textgreater}euce{\textless}/span{\textgreater}: a lightweight user interface for structured editing},
	isbn = {978-1-4503-5638-1},
	shorttitle = {D {\textless}span style="font-variant},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180165},
	doi = {10.1145/3180155.3180165},
	abstract = {We present a structure-aware code editor, called Deuce, that is equipped with direct manipulation capabilities for invoking automated program transformations. Compared to traditional refactoring environments, Deuce employs a direct manipulation interface that is tightly integrated within a text-based editing workflow. In particular, Deuce draws (i) clickable widgets atop the source code that allow the user to structurally select the unstructured text for subexpressions and other relevant features, and (ii) a lightweight, interactive menu of potential transformations based on the current selections. We implement and evaluate our design with mostly standard transformations in the context of a small functional programming language. A controlled user study with 21 participants demonstrates that structural selection is preferred to a more traditional text-selection interface and may be faster overall once users gain experience with the tool. These results accord with Deuce’s aim to provide human-friendly structural interactions on top of familiar text-based editing.},
	language = {en},
	urldate = {2022-11-29},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Hempel, Brian and Lubin, Justin and Lu, Grace and Chugh, Ravi},
	month = may,
	year = {2018},
	pages = {654--664},
}

@misc{aniche_effectiveness_2020,
	title = {The {Effectiveness} of {Supervised} {Machine} {Learning} {Algorithms} in {Predicting} {Software} {Refactoring}},
	url = {http://arxiv.org/abs/2001.03338},
	abstract = {Refactoring is the process of changing the internal structure of software to improve its quality without modifying its external behavior. Empirical studies have repeatedly shown that refactoring has a positive impact on the understandability and maintainability of software systems. However, before carrying out refactoring activities, developers need to identify refactoring opportunities. Currently, refactoring opportunity identification heavily relies on developers' expertise and intuition. In this paper, we investigate the effectiveness of machine learning algorithms in predicting software refactorings. More specifically, we train six different machine learning algorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine, Decision Trees, Random Forest, and Neural Network) with a dataset comprising over two million refactorings from 11,149 real-world projects from the Apache, F-Droid, and GitHub ecosystems. The resulting models predict 20 different refactorings at class, method, and variable-levels with an accuracy often higher than 90\%. Our results show that (i) Random Forests are the best models for predicting software refactoring, (ii) process and ownership metrics seem to play a crucial role in the creation of better models, and (iii) models generalize well in different contexts.},
	urldate = {2022-11-29},
	publisher = {arXiv},
	author = {Aniche, Maurício and Maziero, Erick and Durelli, Rafael and Durelli, Vinicius},
	month = sep,
	year = {2020},
	note = {arXiv:2001.03338 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{bokhari_deep_2017,
	address = {Berlin Germany},
	title = {Deep parameter optimisation on {Android} smartphones for energy minimisation: a tale of woe and a proof-of-concept},
	isbn = {978-1-4503-4939-0},
	shorttitle = {Deep parameter optimisation on {Android} smartphones for energy minimisation},
	url = {https://dl.acm.org/doi/10.1145/3067695.3082519},
	doi = {10.1145/3067695.3082519},
	abstract = {With power demands of mobile devices rising, it is becoming increasingly important to make mobile so ware applications more energy e cient. Unfortunately, mobile platforms are diverse and very complex which makes energy behaviours di cult to model. is complexity presents challenges to the e ectiveness of o -line optimisation of mobile applications. In this paper, we demonstrate that it is possible to automatically optimise an application for energy on a mobile device by evaluating energy consumption in-vivo. In contrast to previous work, we use only the device’s own internal meter. Our approach involves many technical challenges but represents a realistic path toward learning hardware speci c energy models for program code features.},
	language = {en},
	urldate = {2022-11-04},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {ACM},
	author = {Bokhari, Mahmoud A. and Bruce, Bobby R. and Alexander, Brad and Wagner, Markus},
	month = jul,
	year = {2017},
	pages = {1501--1508},
}

@inproceedings{xu_empirical_2022,
	address = {Honolulu, HI, USA},
	title = {An {Empirical} {Study} on the {Impact} of {Deep} {Parameters} on {Mobile} {App} {Energy} {Usage}},
	isbn = {978-1-66543-786-8},
	url = {https://ieeexplore.ieee.org/document/9825870/},
	doi = {10.1109/SANER53432.2022.00103},
	abstract = {Improving software performance through conﬁguration parameter tuning is a common activity during software maintenance. Beyond traditional performance metrics like latency, mobile app developers are interested in reducing app energy usage. Some mobile apps have centralized locations for parameter tuning, similar to databases and operating systems, but it is common for mobile apps to have hundreds of parameters scattered around the source code. The correlation between these “deep” parameters and app energy usage is unclear. Researchers have studied the energy effects of deep parameters in speciﬁc modules, but we lack a systematic understanding of the energy impact of mobile deep parameters.},
	language = {en},
	urldate = {2022-10-31},
	booktitle = {2022 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	publisher = {IEEE},
	author = {Xu, Qiang and Davis, James C. and Hu, Y. Charlie and Jindal, Abhilash},
	month = mar,
	year = {2022},
	pages = {844--855},
}

@article{Zaharia2018MLFlow,
	title = {Accelerating the {Machine} {Learning} {Lifecycle} with {MLﬂow}},
	abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLﬂow, an open source platform we recently launched to streamline the machine learning lifecycle. MLﬂow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
	language = {en},
	author = {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
	pages = {7},
}

@inproceedings{Vartak2022ModelDB,
	address = {San Francisco, California},
	title = {{ModelDB}: a system for machine learning model management},
	isbn = {978-1-4503-4207-0},
	shorttitle = {M {\textless}span style="font-variant},
	url = {http://dl.acm.org/citation.cfm?doid=2939502.2939516},
	doi = {10.1145/2939502.2939516},
	abstract = {Building a machine learning model is an iterative process. A data scientist will build many tens to hundreds of models before arriving at one that meets some acceptance criteria (e.g. AUC cutoff, accuracy threshold). However, the current style of model building is ad-hoc and there is no practical way for a data scientist to manage models that are built over time. As a result, the data scientist must attempt to “remember” previously constructed models and insights obtained from them. This task is challenging for more than a handful of models and can hamper the process of sensemaking. Without a means to manage models, there is no easy way for a data scientist to answer questions such as “Which models were built using an incorrect feature?”, “Which model performed best on American customers?” or “How did the two top models compare?” In this paper, we describe our ongoing work on ModelDB, a novel end-to-end system for the management of machine learning models. ModelDB clients automatically track machine learning models in their native environments (e.g. scikit-learn, spark.ml), the ModelDB backend introduces a common layer of abstractions to represent models and pipelines, and the ModelDB frontend allows visual exploration and analyses of models via a web-based interface.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the {Workshop} on {Human}-{In}-the-{Loop} {Data} {Analytics} - {HILDA} '16},
	publisher = {ACM Press},
	author = {Vartak, Manasi and Subramanyam, Harihar and Lee, Wei-En and Viswanathan, Srinidhi and Husnoo, Saadiyah and Madden, Samuel and Zaharia, Matei},
	year = {2016},
	pages = {1--3},
}

@inproceedings{Sculley2015HiddenTechnicalDebtinMLSystems,
	title = {Hidden {Technical} {Debt} in {Machine} {Learning} {Systems}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html},
	abstract = {Machine learning offers a fantastically powerful toolkit for building useful complexprediction systems quickly. This paper argues it is dangerous to think ofthese quick wins as coming for free. Using the software engineering frameworkof technical debt, we find it is common to incur massive ongoing maintenancecosts in real-world ML systems. We explore several ML-specific risk factors toaccount for in system design. These include boundary erosion, entanglement,hidden feedback loops, undeclared consumers, data dependencies, configurationissues, changes in the external world, and a variety of system-level anti-patterns.},
	urldate = {2022-10-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-François and Dennison, Dan},
	year = {2015},
}

@inproceedings{Breck2017MLtestscore,
	title = {The {ML} test score: {A} rubric for {ML} production readiness and technical debt reduction},
	shorttitle = {The {ML} test score},
	doi = {10.1109/BigData.2017.8258038},
	abstract = {Creating reliable, production-level machine learning systems brings on a host of concerns not found in small toy examples or even large offline research experiments. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing technical debt of ML systems. But it can be difficult to formulate specific tests, given that the actual prediction behavior of any given model is difficult to specify a priori. In this paper, we present 28 specific tests and monitoring needs, drawn from experience with a wide range of production ML systems to help quantify these issues and present an easy to follow road-map to improve production readiness and pay down ML technical debt.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D.},
	month = dec,
	year = {2017},
	keywords = {Best Practices, Data models, Machine Learning, Measurement, Monitoring, Production, Reliability, Technical Debt, Testing, Training},
	pages = {1123--1132},
}

@techreport{vanOort2022ProjectsmellsExperiencesinAnalysingtheSWQualityofMLProjectswithMllint,
	title = {"{Project} smells" -- {Experiences} in {Analysing} the {Software} {Quality} of {ML} {Projects} with mllint},
	url = {http://arxiv.org/abs/2201.08246},
	abstract = {Machine Learning (ML) projects incur novel challenges in their development and productionisation over traditional software applications, though established principles and best practices in ensuring the project's software quality still apply. While using static analysis to catch code smells has been shown to improve software quality attributes, it is only a small piece of the software quality puzzle, especially in the case of ML projects given their additional challenges and lower degree of Software Engineering (SE) experience in the data scientists that develop them. We introduce the novel concept of project smells which consider deficits in project management as a more holistic perspective on software quality in ML projects. An open-source static analysis tool mllint was also implemented to help detect and mitigate these. Our research evaluates this novel concept of project smells in the industrial context of ING, a global bank and large software- and data-intensive organisation. We also investigate the perceived importance of these project smells for proof-of-concept versus production-ready ML projects, as well as the perceived obstructions and benefits to using static analysis tools such as mllint. Our findings indicate a need for context-aware static analysis tools, that fit the needs of the project at its current stage of development, while requiring minimal configuration effort from the user.},
	number = {arXiv:2201.08246},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {van Oort, Bart and Cruz, Luís and Loni, Babak and van Deursen, Arie},
	month = jan,
	year = {2022},
	keywords = {68-06, Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{Ma2021AntiPatternsinPythonCodebases,
	address = {Chicago IL USA},
	title = {“{You} have said too much”: {Java}-like verbosity anti-patterns in {Python} codebases},
	isbn = {978-1-4503-9089-7},
	shorttitle = {“{You} have said too much”},
	url = {https://dl.acm.org/doi/10.1145/3484272.3484960},
	doi = {10.1145/3484272.3484960},
	abstract = {As a popular language for teaching introductory programming, Java can profoundly influence beginner programmers with its coding style and idioms. Despite its many advantages, the paradigmatic coding style in Java is often described as verbose. As a result, when writing code in more concise languages, such programmers tend to emulate the familiar Java coding idioms, thus neglecting to take advantage of the more succinct counterparts in those languages. As a result of such verbosity, not only the overall code quality suffers, but the verbose non-idiomatic patterns also render code hard to understand and maintain. In this paper, we study the incidences of Java-like verbosity as they occur in Python codebases. We present a collection of Java-Like Verbosity Anti-patterns and our pilot study of their presence in representative open-source Python codebases. We discuss our findings as a call for action to computing educators, particularly those who work with introductory students. We need novel pedagogical interventions that encourage budding programmers to write concise idiomatic code in any language.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the 2021 {ACM} {SIGPLAN} {International} {Symposium} on {SPLASH}-{E}},
	publisher = {ACM},
	author = {Ma, Yuzhi and Tilevich, Eli},
	month = oct,
	year = {2021},
	pages = {13--18},
}

@inproceedings{ma_you_2021,
	address = {Chicago IL USA},
	title = {“{You} have said too much”: {Java}-like verbosity anti-patterns in {Python} codebases},
	isbn = {978-1-4503-9089-7},
	shorttitle = {“{You} have said too much”},
	url = {https://dl.acm.org/doi/10.1145/3484272.3484960},
	doi = {10.1145/3484272.3484960},
	abstract = {As a popular language for teaching introductory programming, Java can profoundly influence beginner programmers with its coding style and idioms. Despite its many advantages, the paradigmatic coding style in Java is often described as verbose. As a result, when writing code in more concise languages, such programmers tend to emulate the familiar Java coding idioms, thus neglecting to take advantage of the more succinct counterparts in those languages. As a result of such verbosity, not only the overall code quality suffers, but the verbose non-idiomatic patterns also render code hard to understand and maintain. In this paper, we study the incidences of Java-like verbosity as they occur in Python codebases. We present a collection of Java-Like Verbosity Anti-patterns and our pilot study of their presence in representative open-source Python codebases. We discuss our findings as a call for action to computing educators, particularly those who work with introductory students. We need novel pedagogical interventions that encourage budding programmers to write concise idiomatic code in any language.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the 2021 {ACM} {SIGPLAN} {International} {Symposium} on {SPLASH}-{E}},
	publisher = {ACM},
	author = {Ma, Yuzhi and Tilevich, Eli},
	month = oct,
	year = {2021},
	pages = {13--18},
}

@article{wang_yolov7_nodate,
	title = {{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors},
	abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutionalbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https:// github.com/ WongKinYiu/ yolov7.},
	language = {en},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	pages = {17},
}

@article{noauthor_extraction_nodate,
	title = {Extraction and {Management} of {Rationale}},
	language = {en},
	pages = {3},
}

@article{noauthor_extraction_nodate-1,
	title = {Extraction and {Management} of {Rationale}},
	language = {en},
	pages = {3},
}

@inproceedings{gharibi_modelkb_2022,
	title = {{ModelKB}: {Towards} {Automated} {Management} of the {Modeling} {Lifecycle} in {Deep} {Learning}},
	urldate = {2022-10-06},
	booktitle = {International {Workshop} on {Realizing} {Artificial} {Intelligence} {Synergies} in {Software} {Engineering} ({RAISE})},
	publisher = {arXiv},
	author = {Gharibi, Gharib and Walunj, Vijay and Rella, Sirisha and Lee, Yugyung},
	month = sep,
	year = {2022},
	note = {CitationKey: Gharibi2022ModelKB},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{Gharibi2019ModelKB,
	title = {{ModelKB}: {Towards} {Automated} {Management} of the {Modeling} {Lifecycle} in {Deep} {Learning}},
	shorttitle = {{ModelKB}},
	doi = {10.1109/RAISE.2019.00013},
	abstract = {Deep Learning has improved the state-of-the-art results in an ever-growing number of domains. This success heavily relies on the development and training of deep learning models, also known as deep neural networks (DNN). Often, developing a DNN is an ad-hoc, iterative process that results in producing tens to hundreds of models before arriving at a satisfactory result. While there has been a surge in the number of tools and frameworks that aim at facilitating deep learning, the issues of model management have been largely ignored. In particular, deep learning practitioners have to manually track their experiments using text files, spreadsheets or folder hierarchies, which is expensive, time-consuming, and error-prone. In this paper, we present our ongoing work and vision towards automating end-to-end model management in deep learning. Specifically, we introduce a tool prototype, named ModelKB, that can automatically (1) extract and store the model's metadata-including its architecture, weights, and configuration; (2) visualize, query, and compare experiments; and (3) reproduce experiments. Our overarching goal is to automate the model management process with minimal user intervention using the user's favorite framework. We report the current status of ModelKB, a pilot user study, and the challenges of automating model management in deep learning.},
	booktitle = {2019 {IEEE}/{ACM} 7th {International} {Workshop} on {Realizing} {Artificial} {Intelligence} {Synergies} in {Software} {Engineering} ({RAISE})},
	author = {Gharibi, Gharib and Walunj, Vijay and Rella, Sirisha and Lee, Yugyung},
	month = may,
	year = {2019},
	keywords = {Computational modeling, Deep learning, Measurement, Metadata, Software, Training, data management, deep learning, software engineering},
	pages = {28--34},
}

@article{Amazon2017AutomaticallyTrackingMetadataandProvenanceofMLExperiments,
	title = {Automatically {Tracking} {Metadata} and {Provenance} of {Machine} {Learning} {Experiments}},
	volume = {Long Beach},
	abstract = {We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workﬂow, and provides a basis for comparability and repeatability of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.},
	language = {en},
	journal = {NIPS Machine Learning Systems Workshop},
	author = {Schelter, Sebastian and Böse, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
	year = {2017},
	pages = {8},
}

@article{Benton2020MLSystemsandIntelligentApplications,
	title = {Machine {Learning} {Systems} and {Intelligent} {Applications}},
	volume = {37},
	issn = {1937-4194},
	doi = {10.1109/MS.2020.2985224},
	abstract = {Machine learning techniques are useful in a wide range of contexts, but techniques alone are insufficient to solve real business problems. We introduce the intelligent applications concept, which characterizes the structure and responsibilities of contemporary machine learning systems.},
	number = {4},
	journal = {IEEE Software},
	author = {Benton, William C.},
	month = jul,
	year = {2020},
	keywords = {Artificial Intelligence, Data models, Feature extraction, Machine learning, Pipelines, Task analysis, Training data, distributed applications, machine learning},
	pages = {43--49},
}

@misc{Giray2021SEonEngineeringMLSystems,
	title = {A {Software} {Engineering} {Perspective} on {Engineering} {Machine} {Learning} {Systems}: {State} of the {Art} and {Challenges}},
	shorttitle = {A {Software} {Engineering} {Perspective} on {Engineering} {Machine} {Learning} {Systems}},
	url = {http://arxiv.org/abs/2012.07919},
	abstract = {Context: Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems. Method: I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions. Conclusion: The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.},
	urldate = {2022-10-16},
	publisher = {arXiv},
	author = {Giray, Görkem},
	month = jun,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, D.2.0},
}

@article{souza_workflow_2022,
	title = {Workflow provenance in the lifecycle of scientific machine learning},
	volume = {34},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6544},
	doi = {10.1002/cpe.6544},
	abstract = {Machine learning (ML) has already fundamentally changed several businesses. More recently, it has also been profoundly impacting the computational science and engineering domains, like geoscience, climate science, and health science. In these domains, users need to perform comprehensive data analyses combining scientific data and ML models to provide for critical requirements, such as reproducibility, model explainability, and experiment data understanding. However, scientific ML is multidisciplinary, heterogeneous, and affected by the physical constraints of the domain, making such analyses even more challenging. In this work, we leverage workflow provenance techniques to build a holistic view to support the lifecycle of scientific ML. We contribute with (i) characterization of the lifecycle and taxonomy for data analyses; (ii) design decisions to build this view, with a W3C PROV compliant data representation and a reference system architecture; and (iii) lessons learned after an evaluation in an Oil \& Gas case using an HPC cluster with 393 nodes and 946 GPUs. The experiments show that the decisions enable queries that integrate domain semantics with ML models while keeping low overhead ({\textless}1\%), high scalability, and an order of magnitude of query acceleration under certain workloads against without our representation.},
	language = {en},
	number = {14},
	urldate = {2022-10-16},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Souza, Renan and Azevedo, Leonardo G. and Lourenço, Vítor and Soares, Elton and Thiago, Raphael and Brandão, Rafael and Civitarese, Daniel and Vital Brazil, Emilio and Moreno, Marcio and Valduriez, Patrick and Mattoso, Marta and Cerqueira, Renato and Netto, Marco A. S.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.6544},
	keywords = {artificial intelligence, e-Science, explainability, lineage, machine learning lifecycle, provenance, reproducibility, scientific machine learning, scientific workflow, taxonomy},
	pages = {e6544},
}

@article{biessmann_automated_nodate,
	title = {Automated {Data} {Validation} in {Machine} {Learning} {Systems}},
	abstract = {Machine Learning (ML) algorithms are a standard component of modern software systems. The validation of data ingested and produced by ML components has become a central challenge in the deployment and maintenance of ML systems. Subtle changes in the input data can result in unpredictable behavior of an ML algorithm that can lead to unreliable or unfair ML predictions. Responsible usage of ML components thus requires well calibrated and scalable data validation systems. Here, we highlight some challenges associated with data validation in ML systems. We review some of the solutions developed to validate data at the various stages of a data pipeline in modern ML systems, discuss their strengths and weaknesses and assess to what extent these solutions are being used in practice. The research reviewed indicates that the increasing need for data validation in ML systems has driven enormous progress in an emerging community of ML and Data Base Management Systems (DBMS) researchers. While this research has led to a number of technical solutions we ﬁnd that many ML systems deployed in industrial applications are not leveraging the full potential of data validation in practice. The reasons for this are not only technical challenges, but there are also cultural, ethical and legal aspects that need to be taken into account when building data validation solutions for ML systems. We identify the lack of automation in data validation as one of the key factors slowing down adoption of validation solutions and translation of research into useful and robust ML applications. We conclude with an outlook on research directions at the intersection of ML and DBMS research to improve the development, deployment and maintenance of ML systems.},
	language = {en},
	author = {Biessmann, Felix and Golebiowski, Jacek and Rukat, Tammo and Lange, Dustin and Schmidt, Philipp},
	pages = {14},
}

@article{gharibi_automated_2021,
	title = {Automated end-to-end management of the modeling lifecycle in deep learning},
	volume = {26},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-020-09894-9},
	doi = {10.1007/s10664-020-09894-9},
	abstract = {Deep learning has improved the state-of-the-art results in an ever-growing number of domains. This success heavily relies on the development and training of deep learning models–an experimental, iterative process that produces tens to hundreds of models before arriving at a satisfactory result. While there has been a surge in the number of tools and frameworks that aim at facilitating deep learning, the process of managing the models and their artifacts is still surprisingly challenging and time-consuming. Existing modelmanagement solutions are either tailored for commercial platforms or require significant code changes. Moreover, most of the existing solutions address a single phase of the modeling lifecycle, such as experiment monitoring, while ignoring other essential tasks, such as model deployment. In this paper, we present a software system to facilitate and accelerate the deep learning lifecycle, named ModelKB. ModelKB can automatically manage the modeling lifecycle end-to-end, including (1) monitoring and tracking experiments; (2) visualizing, searching for, and comparing models and experiments; (3) deploying models locally and on the cloud; and (4) sharing and publishing trained models. Moreover, our system provides a stepping-stone for enhanced reproducibility. ModelKB currently supports TensorFlow 2.0, Keras, and PyTorch, and it can be extended to other deep learning frameworks easily.},
	language = {en},
	number = {2},
	urldate = {2022-10-16},
	journal = {Empirical Software Engineering},
	author = {Gharibi, Gharib and Walunj, Vijay and Nekadi, Raju and Marri, Raj and Lee, Yugyung},
	month = mar,
	year = {2021},
	pages = {17},
}

@article{Amazon2018MLModelManagementChallenge,
	title = {On {Challenges} in {Machine} {Learning} {Model} {Management}},
	abstract = {The training, maintenance, deployment, monitoring, organization and documentation of machine learning (ML) models – in short model management – is a critical task in virtually all production ML use cases. Wrong model management decisions can lead to poor performance of a ML system and result in high maintenance cost. As both research on infrastructure as well as on algorithms is quickly evolving, there is a lack of understanding of challenges and best practices for ML model management. Therefore, this ﬁeld is receiving increased attention in recent years, both from the data management as well as from the ML community. In this paper, we discuss a selection of ML use cases, develop an overview over conceptual, engineering, and data-related challenges arising in the management of the corresponding ML models, and point out future research directions.},
	journal = {Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
	author = {Schelter, Sebastian and Biessmann, Felix and Januschowski, Tim and Salinas, David and Seufert, Stephan and Szarvas, Gyuri},
	year = {2018},
}

@article{schelter_automatically_nodate,
	title = {Automatically {Tracking} {Metadata} and {Provenance} of {Machine} {Learning} {Experiments}},
	abstract = {We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workﬂow, and provides a basis for comparability and repeatability of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.},
	language = {en},
	author = {Schelter, Sebastian and Böse, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
	pages = {8},
}

@article{martinez-fernandez_software_2022,
	title = {Software {Engineering} for {AI}-{Based} {Systems}: {A} {Survey}},
	volume = {31},
	issn = {1049-331X, 1557-7392},
	shorttitle = {Software {Engineering} for {AI}-{Based} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3487043},
	doi = {10.1145/3487043},
	abstract = {AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on
              Software Engineering (SE)
              approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.},
	language = {en},
	number = {2},
	urldate = {2022-10-16},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Martínez-Fernández, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan},
	month = apr,
	year = {2022},
	pages = {1--59},
}

@article{yang_complex_2022,
	title = {Complex {Python} {Features} in the {Wild}},
	abstract = {While Python is increasingly popular, program analysis tooling for Python is lagging. This is due, in part, to complex features of the Python language—features with difficult to understand and model semantics. Besides the “usual suspects”, reflection and dynamic execution, complex Python features include context managers, decorators, and generators, among others. This paper explores how often and in what ways developers use certain complex features. We analyze over 3 million Python files mined from GitHub to address three research questions: (i) How often do developers use certain complex Python features? (ii) In what ways do developers use these features? (iii) Does use of complex features increase or decrease over time? Our findings show that usage of dynamic features that pose a threat to static analysis is infrequent. On the other hand, usage of context managers and decorators is surprisingly widespread. Our actionable result is a list of Python features that any “minimal syntax” ought to handle in order to capture developers’ use of the Python language. We hope that understanding the usage of Python features will help tool-builders improve Python tools, which can in turn lead to more correct, secure, and performant Python code.},
	language = {en},
	author = {Yang, Yi and Milanova, Ana and Hirzel, Martin},
	year = {2022},
	pages = {12},
}

@article{Xie2011TestingandValidatingMLClassifiersbyMetamorphicTesting,
	title = {Testing and validating machine learning classifiers by metamorphic testing},
	volume = {84},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121210003213},
	doi = {10.1016/j.jss.2010.11.920},
	abstract = {Machine learning algorithms have provided core functionality to many application domains – such as bioinformatics, computational linguistics, etc. However, it is difﬁcult to detect faults in such applications because often there is no “test oracle” to verify the correctness of the computed outputs. To help address the software quality, in this paper we present a technique for testing the implementations of machine learning classiﬁcation algorithms which support such applications. Our approach is based on the technique “metamorphic testing”, which has been shown to be effective to alleviate the oracle problem. Also presented include a case study on a real-world machine learning application framework, and a discussion of how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also conduct mutation analysis and cross-validation, which reveal that our method has high effectiveness in killing mutants, and that observing expected cross-validation result alone is not sufﬁciently effective to detect faults in a supervised classiﬁcation program. The effectiveness of metamorphic testing is further conﬁrmed by the detection of real faults in a popular open-source classiﬁcation program.},
	language = {en},
	number = {4},
	urldate = {2022-09-30},
	journal = {Journal of Systems and Software},
	author = {Xie, Xiaoyuan and Ho, Joshua W.K. and Murphy, Christian and Kaiser, Gail and Xu, Baowen and Chen, Tsong Yueh},
	month = apr,
	year = {2011},
	pages = {544--558},
}

@article{Chen2022DeepPerform,
	title = {{DeepPerform}: {An} {Efficient} {Approach} for {Performance} {Testing} of {Resource}-{Constrained} {Neural} {Networks}},
	abstract = {Today, an increasing number of Adaptive Deep Neural Networks (AdNNs) are being used on resource-constrained embedded devices. We observe that, similar to traditional software, redundant computation exists in AdNNs, resulting in considerable performance degradation. The performance degradation is dependent on the input and is referred to as input-dependent performance bottlenecks (IDPBs). To ensure an AdNN satisfies the performance requirements of resource-constrained applications, it is essential to conduct performance testing to detect IDPBs in the AdNN. Existing neural network testing methods are primarily concerned with correctness testing, which does not involve performance testing. To fill this gap, we propose DeepPerform, a scalable approach to generate test samples to detect the IDPBs in AdNNs. We first demonstrate how the problem of generating performance test samples detecting IDPBs can be formulated as an optimization problem. Following that, we demonstrate how DeepPerform efficiently handles the optimization problem by learning and estimating the distribution of AdNNs’ computational consumption. We evaluate DeepPerform on three widely used datasets against five popular AdNN models. The results show that DeepPerform generates test samples that cause more severe performance degradation (FLOPs: increase up to 552\%). Furthermore, DeepPerform is substantially more efficient than the baseline methods in generating test inputs (runtime overhead: only 6–10 milliseconds).},
	language = {en},
	author = {Chen, Simin and Haque, Mirazul and Liu, Cong and Yang, Wei},
	year = {2022},
	pages = {13},
}

@misc{chatterjee_testing_2022,
	title = {Testing of {Machine} {Learning} {Models} with {Limited} {Samples}: {An} {Industrial} {Vacuum} {Pumping} {Application}},
	shorttitle = {Testing of {Machine} {Learning} {Models} with {Limited} {Samples}},
	url = {http://arxiv.org/abs/2208.04062},
	abstract = {There is often a scarcity of training data for machine learning (ML) classification and regression models in industrial production, especially for time-consuming or sparsely run manufacturing processes. A majority of the limited ground-truth data is used for training, while a handful of samples are left for testing. Here, the number of test samples is inadequate to properly evaluate the robustness of the ML models under test for classification and regression. Furthermore, the output of these ML models may be inaccurate or even fail if the input data differ from the expected. This is the case for ML models used in the Electroslag Remelting (ESR) process in the refined steel industry to predict the pressure in a vacuum chamber. A vacuum pumping event that occurs once a workday generates a few hundred samples in a year of pumping for training and testing. In the absence of adequate training and test samples, this paper first presents a method to generate a fresh set of augmented samples based on vacuum pumping principles. Based on the generated augmented samples, three test scenarios and one test oracle are presented to assess the robustness of an ML model used for production on an industrial scale. Experiments are conducted with real industrial production data obtained from Uddeholms AB steel company. The evaluations indicate that Ensemble and Neural Network are the most robust when trained on augmented data using the proposed testing strategy. The evaluation also demonstrates the proposed method's effectiveness in checking and improving ML algorithms' robustness in such situations. The work improves software testing's state-of-the-art robustness testing in similar settings. Finally, the paper presents an MLOps implementation of the proposed approach for real-time ML model prediction and action on the edge node and automated continuous delivery of ML software from the cloud.},
	urldate = {2022-10-06},
	publisher = {arXiv},
	author = {Chatterjee, Ayan and Ahmed, Bestoun S. and Hallin, Erik and Engman, Anton},
	month = aug,
	year = {2022},
	note = {arXiv:2208.04062 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{zhang_toward_2022,
	title = {Toward {Improving} the {Robustness} of {Deep} {Learning} {Models} via {Model} {Transformation}},
	abstract = {Deep learning (DL) techniques have attracted much attention in recent years, and have been applied to many application scenarios, including those that are safety-critical. Improving the universal robustness of DL models is vital and many approaches have been proposed in the last decades aiming at such a purpose. Among existing approaches, adversarial training is the most representative. It advocates a post model tuning process via incorporating adversarial samples. Although successful, they still suffer from the challenge of generalizability issues in the face of various attacks with unsatisfactory effectiveness. Targeting this problem, in this paper we propose a novel model training framework, which aims at improving the universal robustness of DL models via model transformation incorporated with a data augmentation strategy in a delta debugging fashion. We have implemented our approach in a tool, called Dare, and conducted an extensive evaluation on 9 DL models. The results show that our approach significantly outperforms existing adversarial training techniques. Specifically, Dare has achieved the highest Empirical Robustness in 29 of 45 testing scenarios under various attacks, while the number drops to 5 of 45 for the best baseline approach.},
	language = {en},
	author = {Zhang, Yingyi and Wang, Zan and Jiang, Jiajun and You, Hanmo and Chen, Junjie},
	year = {2022},
	pages = {13},
}

@article{chen_maat_2022,
	title = {{MAAT}: {A} {Novel} {Ensemble} {Approach} to {Addressing} {Fairness} and {Performance} {Bugs} for {Machine} {Learning} {Software}},
	abstract = {Machine Learning (ML) software can lead to unfair and unethical decisions, making software fairness bugs an increasingly significant concern for software engineers. However, addressing fairness bugs often comes at the cost of introducing more ML performance (e.g., accuracy) bugs. In this paper, we propose MAAT, a novel ensemble approach to improving fairness-performance trade-off for ML software. Conventional ensemble methods combine different models with identical learning objectives. MAAT, instead, combines models optimized for different objectives: fairness and ML performance. We conduct an extensive evaluation of MAAT with 5 state-of-the-art methods, 9 software decision tasks, and 15 fairness-performance measurements. The results show that MAAT significantly outperforms the state-of-the-art. In particular, MAAT beats the trade-off baseline constructed by a recent benchmarking tool in 92.2\% of the overall cases evaluated, 12.2 percentage points more than the best technique currently available. Moreover, the superiority of MAAT over the state-of-the-art holds on all the tasks and measurements that we study. We have made publicly available the code and data of this work to allow for future replication and extension.},
	language = {en},
	author = {Chen, Zhenpeng and Sarro, Federica and Zhang, Jie M and Harman, Mark},
	year = {2022},
	pages = {13},
}

@inproceedings{dutta_seed_2022,
	address = {Valencia, Spain},
	title = {To {Seed} or {Not} to {Seed}? {An} {Empirical} {Analysis} of {Usage} of {Seeds} for {Testing} in {Machine} {Learning} {Projects}},
	isbn = {978-1-66546-679-0},
	shorttitle = {To {Seed} or {Not} to {Seed}?},
	url = {https://ieeexplore.ieee.org/document/9787895/},
	doi = {10.1109/ICST53961.2022.00026},
	abstract = {Many Machine Learning (ML) algorithms are inherently random in nature – executing them using the same inputs may lead to slightly different results across different runs. Such randomness makes it challenging for developers to write tests for their implementations of ML algorithms. A natural consequence of randomness is test flakiness – tests both pass and fail non-deterministically for same version of code.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {2022 {IEEE} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	publisher = {IEEE},
	author = {Dutta, Saikat and Arunachalam, Anshul and Misailovic, Sasa},
	month = apr,
	year = {2022},
	pages = {151--161},
}

@article{kelley_framework_2021,
	title = {A framework for creating knowledge graphs of scientific software metadata},
	volume = {2},
	issn = {2641-3337},
	url = {https://direct.mit.edu/qss/article/2/4/1423/108045/A-framework-for-creating-knowledge-graphs-of},
	doi = {10.1162/qss_a_00167},
	abstract = {An increasing number of researchers rely on computational methods to generate or manipulate the results described in their scientific publications. Software created to this end—scientific software—is key to understanding, reproducing, and reusing existing work in many disciplines, ranging from Geosciences to Astronomy or Artificial Intelligence. However, scientific software is usually challenging to find, set up, and compare to similar software due to its disconnected documentation (dispersed in manuals, readme files, websites, and code comments) and the lack of structured metadata to describe it. As a result, researchers have to manually inspect existing tools to understand their differences and incorporate them into their work. This approach scales poorly with the number of publications and tools made available every year. In this paper we address these issues by introducing a framework for automatically extracting scientific software metadata from its documentation (in particular, their readme files); a methodology for structuring the extracted metadata in a Knowledge Graph (KG) of scientific software; and an exploitation framework for browsing and comparing the contents of the generated KG. We demonstrate our approach by creating a KG with metadata from over 10,000 scientific software entries from public code repositories.},
	language = {en},
	number = {4},
	urldate = {2022-09-30},
	journal = {Quantitative Science Studies},
	author = {Kelley, Aidan and Garijo, Daniel},
	month = dec,
	year = {2021},
	pages = {1423--1446},
}

@inproceedings{ponta_beyond_2018,
	title = {Beyond {Metadata}: {Code}-{Centric} and {Usage}-{Based} {Analysis} of {Known} {Vulnerabilities} in {Open}-{Source} {Software}},
	shorttitle = {Beyond {Metadata}},
	doi = {10.1109/ICSME.2018.00054},
	abstract = {The use of open-source software (OSS) is ever-increasing, and so is the number of open-source vulnerabilities being discovered and publicly disclosed. The gains obtained from the reuse of community-developed libraries may be offset by the cost of detecting, assessing, and mitigating their vulnerabilities in a timely manner. In this paper we present a novel method to detect, assess and mitigate OSS vulnerabilities that improves on state-of-the-art approaches, which commonly depend on metadata to identify vulnerable OSS dependencies. Our solution instead is code-centric and combines static and dynamic analysis to determine the reachability of the vulnerable portion of libraries used (directly or transitively) by an application. Taking this usage into account, our approach then supports developers in choosing among the existing non-vulnerable library versions. Vulas, the tool implementing our code-centric and usage-based approach, is officially recommended by SAP to scan its Java software, and has been successfully used to perform more than 250000 scans of about 500 applications since December 2016. We report on our experience and on the lessons we learned when maturing the tool from a research prototype to an industrial-grade solution.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Ponta, Serena Elisa and Plate, Henrik and Sabetta, Antonino},
	month = sep,
	year = {2018},
	note = {ISSN: 2576-3148},
	keywords = {Libraries, Metadata, Open source software, Silicon, Static analysis, Tools, code-centric vulnerability analysis, known vulnerabilities, metric-based update support, open source software},
	pages = {449--460},
}

@inproceedings{gil_ontosoft_2015,
	address = {Palisades NY USA},
	title = {{OntoSoft}: {Capturing} {Scientific} {Software} {Metadata}},
	isbn = {978-1-4503-3849-3},
	shorttitle = {{OntoSoft}},
	url = {https://dl.acm.org/doi/10.1145/2815833.2816955},
	doi = {10.1145/2815833.2816955},
	abstract = {This paper presents OntoSoft, an ontology to describe metadata for scientific software. The ontology is designed considering how scientists would approach the reuse and sharing of software. This includes supporting a scientist to: 1) identify software, 2) understand and assess software, 3) execute software, 4) get support for the software, 5) do research with the software, and 6) update the software. The ontology is available in OWL and contains more than fifty terms. We are using OntoSoft to structure a software registry for geosciences, and to develop user interfaces to capture its metadata.},
	language = {en},
	urldate = {2022-09-30},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {ACM},
	author = {Gil, Yolanda and Ratnakar, Varun and Garijo, Daniel},
	month = oct,
	year = {2015},
	pages = {1--4},
}

@misc{peebles_learning_2022,
	title = {Learning to {Learn} with {Generative} {Models} of {Neural} {Network} {Checkpoints}},
	url = {http://arxiv.org/abs/2209.12892},
	abstract = {We explore a data-driven approach for learning to optimize neural networks. We construct a dataset of neural network checkpoints and train a generative model on the parameters. In particular, our model is a conditional diffusion transformer that, given an initial input parameter vector and a prompted loss, error, or return, predicts the distribution over parameter updates that achieve the desired metric. At test time, it can optimize neural networks with unseen parameters for downstream tasks in just one update. We find that our approach successfully generates parameters for a wide range of loss prompts. Moreover, it can sample multimodal parameter solutions and has favorable scaling properties. We apply our method to different neural network architectures and tasks in supervised and reinforcement learning.},
	urldate = {2022-09-28},
	publisher = {arXiv},
	author = {Peebles, William and Radosavovic, Ilija and Brooks, Tim and Efros, Alexei A. and Malik, Jitendra},
	month = sep,
	year = {2022},
	note = {arXiv:2209.12892 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wang_-target_2021,
	title = {On-target {Adaptation}},
	url = {http://arxiv.org/abs/2109.01087},
	abstract = {Domain adaptation seeks to mitigate the shift between training on the {\textbackslash}emph\{source\} domain and testing on the {\textbackslash}emph\{target\} domain. Most adaptation methods rely on the source data by joint optimization over source data and target data. Source-free methods replace the source data with a source model by fine-tuning it on target. Either way, the majority of the parameter updates for the model representation and the classifier are derived from the source, and not the target. However, target accuracy is the goal, and so we argue for optimizing as much as possible on the target data. We show significant improvement by on-target adaptation, which learns the representation purely from target data while taking only the source predictions for supervision. In the long-tailed classification setting, we show further improvement by on-target class distribution learning, which learns the (im)balance of classes from target data.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Wang, Dequan and Liu, Shaoteng and Ebrahimi, Sayna and Shelhamer, Evan and Darrell, Trevor},
	month = sep,
	year = {2021},
	note = {arXiv:2109.01087 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{you_ranking_2022,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} for {Exploiting} {Model} {Hubs}},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain {\textbackslash}emph\{under-exploited\} -- practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ive but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model's architecture or the target PTM can be tuned by the top \$K\$ ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as {\textbackslash}emph\{B-Tuning\}, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.},
	urldate = {2022-09-26},
	booktitle = {Journal of {Machine} {Learning} {Research}},
	author = {You, Kaichao and Liu, Yong and Zhang, Ziyang and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = jul,
	year = {2022},
	note = {arXiv:2110.10545 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{you_ranking_2022-1,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} for {Exploiting} {Model} {Hubs}},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain {\textbackslash}emph\{under-exploited\} -- practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ive but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model's architecture or the target PTM can be tuned by the top \$K\$ ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as {\textbackslash}emph\{B-Tuning\}, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {You, Kaichao and Liu, Yong and Zhang, Ziyang and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = jul,
	year = {2022},
	note = {arXiv:2110.10545 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{you_ranking_2022-2,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} for {Exploiting} {Model} {Hubs}},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain {\textbackslash}emph\{under-exploited\} -- practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ive but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model's architecture or the target PTM can be tuned by the top \$K\$ ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as {\textbackslash}emph\{B-Tuning\}, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {You, Kaichao and Liu, Yong and Zhang, Ziyang and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = jul,
	year = {2022},
	note = {arXiv:2110.10545 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{you_ranking_2022-3,
	title = {Ranking and {Tuning} {Pre}-trained {Models}: {A} {New} {Paradigm} for {Exploiting} {Model} {Hubs}},
	shorttitle = {Ranking and {Tuning} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2110.10545},
	abstract = {Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain {\textbackslash}emph\{under-exploited\} -- practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This na{\textbackslash}"ive but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks {\textbackslash}emph\{before fine-tuning\}. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model's architecture or the target PTM can be tuned by the top \$K\$ ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as {\textbackslash}emph\{B-Tuning\}, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {You, Kaichao and Liu, Yong and Zhang, Ziyang and Wang, Jianmin and Jordan, Michael I. and Long, Mingsheng},
	month = jul,
	year = {2022},
	note = {arXiv:2110.10545 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{pintor_imagenet-patch_nodate,
	title = {{ImageNet}-{Patch}: {A} {Dataset} for {Benchmarking} {Machine} {Learning} {Robustness} against {Adversarial} {Patches}},
	abstract = {Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machinelearning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and applied to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations.},
	language = {en},
	author = {Pintor, Maura and Angioni, Daniele and Sotgiu, Angelo and Demetrio, Luca and Demontis, Ambra and Biggio, Battista and Roli, Fabio},
	pages = {9},
}

@article{liang_metashift_nodate,
	title = {{MetaShift}: {A} {Dataset} of {Datasets} for {Evaluating} {Contextual} {Distribution} {Shifts}},
	abstract = {We start from the pre-processed and cleaned version of Visual Genome to construct MetaShift, which contains 12,868 sets of natural images from 410 classes. The subsets are characterized by a diverse vocabulary of 1,853 distinct contexts. Beyond 1,702 contexts defined by object presence, the dataset also leverages the 37 distinct general contexts and 114 object attributes from Visual Genome. Appendix A present examples and more information of the contexts.},
	language = {en},
	author = {Liang, Weixin and Yang, Xinyu and Zou, James},
	pages = {8},
}

@misc{rudin_interpretable_2021,
	title = {Interpretable {Machine} {Learning}: {Fundamental} {Principles} and 10 {Grand} {Challenges}},
	shorttitle = {Interpretable {Machine} {Learning}},
	url = {http://arxiv.org/abs/2103.11251},
	abstract = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the “Rashomon set” of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.},
	language = {en},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi},
	month = jul,
	year = {2021},
	note = {arXiv:2103.11251 [cs, stat]},
	keywords = {68T01, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
}

@misc{damour_underspecification_2020,
	title = {Underspecification {Presents} {Challenges} for {Credibility} in {Modern} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2011.03395},
	abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	month = nov,
	year = {2020},
	note = {arXiv:2011.03395 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yamada_does_nodate,
	title = {Does {Robustness} on {ImageNet} {Transfer} to {Downstream} {Tasks}?},
	abstract = {As clean ImageNet accuracy nears its ceiling, the research community is increasingly more concerned about robust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classiﬁcation. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classiﬁcation from different domains. This raises a question: Can these robust image classiﬁers transfer robustness to downstream tasks? For object detection and semantic segmentation, we ﬁnd that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neural Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classiﬁcation, we ﬁnd that models that are robustiﬁed for ImageNet do not retain robustness when fully ﬁne-tuned. These ﬁndings suggest that current robustiﬁcation techniques tend to emphasize ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning.},
	language = {en},
	author = {Yamada, Yutaro and Otani, Mayu},
	pages = {10},
}

@inproceedings{nguyen_leep_2020,
	title = {{LEEP}: {A} {New} {Measure} to {Evaluate} {Transferability} of {Learned} {Representations}},
	shorttitle = {{LEEP}},
	url = {https://proceedings.mlr.press/v119/nguyen20b.html},
	abstract = {We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30\% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nguyen, Cuong and Hassner, Tal and Seeger, Matthias and Archambeau, Cedric},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7294--7305},
}

@misc{poth_what_2021,
	title = {What to {Pre}-{Train} on? {Efficient} {Intermediate} {Task} {Selection}},
	shorttitle = {What to {Pre}-{Train} on?},
	url = {http://arxiv.org/abs/2104.08247},
	abstract = {Intermediate task fine-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to run the cross-product of all combinations to find the best transfer setting. In this work we first establish that similar sequential fine-tuning gains can be achieved in adapter settings, and subsequently consolidate previously proposed methods that efficiently identify beneficial tasks for intermediate transfer learning. We experiment with a diverse set of 42 intermediate and 11 target English classification, multiple choice, question answering, and sequence tagging tasks. Our results show that efficient embedding based methods that rely solely on the respective datasets outperform computational expensive few-shot fine-tuning approaches. Our best methods achieve an average Regret@3 of less than 1\% across all target tasks, demonstrating that we are able to efficiently identify the best datasets for intermediate training.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Poth, Clifton and Pfeiffer, Jonas and Rücklé, Andreas and Gurevych, Iryna},
	month = sep,
	year = {2021},
	note = {arXiv:2104.08247 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{jomaa_dataset2vec_2021,
	title = {{Dataset2Vec}: learning dataset meta-features},
	volume = {35},
	issn = {1384-5810, 1573-756X},
	shorttitle = {{Dataset2Vec}},
	url = {https://link.springer.com/10.1007/s10618-021-00737-9},
	doi = {10.1007/s10618-021-00737-9},
	abstract = {Meta-learning, or learning to learn, is a machine learning approach that utilizes prior learning experiences to expedite the learning process on unseen tasks. As a data-driven approach, meta-learning requires meta-features that represent the primary learning tasks or datasets, and are estimated traditonally as engineered dataset statistics that require expert domain knowledge tailored for every meta-task. In this paper, ﬁrst, we propose a meta-feature extractor called Dataset2Vec that combines the versatility of engineered dataset meta-features with the expressivity of meta-features learned by deep neural networks. Primary learning tasks or datasets are represented as hierarchical sets, i.e., as a set of sets, esp. as a set of predictor/target pairs, and then a DeepSet architecture is employed to regress meta-features on them. Second, we propose a novel auxiliary meta-learning task with abundant data called dataset similarity learning that aims to predict if two batches stem from the same dataset or different ones. In an experiment on a large-scale hyperparameter optimization task for 120 UCI datasets with varying schemas as a meta-learning task, we show that the meta-features of Dataset2Vec outperform the expert engineered meta-features and thus demonstrate the usefulness of learned meta-features for datasets with varying schemas for the ﬁrst time.},
	language = {en},
	number = {3},
	urldate = {2022-09-22},
	journal = {Data Mining and Knowledge Discovery},
	author = {Jomaa, Hadi S. and Schmidt-Thieme, Lars and Grabocka, Josif},
	month = may,
	year = {2021},
	pages = {964--985},
}

@inproceedings{subbaswamy_evaluating_2021,
	title = {Evaluating {Model} {Robustness} and {Stability} to {Dataset} {Shift}},
	url = {https://proceedings.mlr.press/v130/subbaswamy21a.html},
	abstract = {As the use of machine learning in high impact domains becomes widespread, the importance of evaluating safety has increased. An important aspect of this is evaluating how robust a model is to changes in setting or population, which typically requires applying the model to multiple, independent datasets. Since the cost of collecting such datasets is often prohibitive, in this paper, we propose a framework for evaluating this type of stability using the available data. We use the original evaluation data to determine distributions under which the algorithm performs poorly, and estimate the algorithm’s performance on the "worst-case" distribution. We consider shifts in user defined conditional distributions, allowing some distributions to shift while keeping other portions of the data distribution fixed. For example, in a healthcare context, this allows us to consider shifts in clinical practice while keeping the patient population fixed. To address the challenges associated with estimation in complex, high-dimensional distributions, we derive a "debiased" estimator which maintains root-N consistency even when machine learning methods with slower convergence rates are used to estimate the nuisance parameters. In experiments on a real medical risk prediction task, we show this estimator can be used to analyze stability and accounts for realistic shifts that could not previously be expressed. The proposed framework allows practitioners to proactively evaluate the safety of their models without requiring additional data collection.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Subbaswamy, Adarsh and Adams, Roy and Saria, Suchi},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2611--2619},
}

@misc{damour_underspecification_2020-1,
	title = {Underspecification {Presents} {Challenges} for {Credibility} in {Modern} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2011.03395},
	abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	month = nov,
	year = {2020},
	note = {arXiv:2011.03395 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yamada_does_nodate-1,
	title = {Does {Robustness} on {ImageNet} {Transfer} to {Downstream} {Tasks}?},
	abstract = {As clean ImageNet accuracy nears its ceiling, the research community is increasingly more concerned about robust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classiﬁcation. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classiﬁcation from different domains. This raises a question: Can these robust image classiﬁers transfer robustness to downstream tasks? For object detection and semantic segmentation, we ﬁnd that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neural Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classiﬁcation, we ﬁnd that models that are robustiﬁed for ImageNet do not retain robustness when fully ﬁne-tuned. These ﬁndings suggest that current robustiﬁcation techniques tend to emphasize ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning.},
	language = {en},
	author = {Yamada, Yutaro and Otani, Mayu},
	pages = {10},
}

@inproceedings{ghodsi_generating_2021,
	title = {Generating and {Characterizing} {Scenarios} for {Safety} {Testing} of {Autonomous} {Vehicles}},
	doi = {10.1109/IV48863.2021.9576023},
	abstract = {Extracting interesting scenarios from real-world data as well as generating failure cases is important for the development and testing of autonomous systems. We propose efficient mechanisms to both characterize and generate testing scenarios using a state-of-the-art driving simulator. For any scenario, our method generates a set of possible driving paths and identifies all the possible safe driving trajectories that can be taken starting at different times, to compute metrics that quantify the complexity of the scenario. We use our method to characterize real driving data from the Next Generation Simulation (NGSIM) project, as well as adversarial scenarios generated in simulation. We rank the scenarios by defining metrics based on the complexity of avoiding accidents and provide insights into how the AV could have minimized the probability of incurring an accident. We demonstrate a strong correlation between the proposed metrics and human intuition.},
	booktitle = {2021 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Ghodsi, Zahra and Hari, Siva Kumar Sastry and Frosio, Iuri and Tsai, Timothy and Troccoli, Alejandro and Keckler, Stephen W. and Garg, Siddharth and Anandkumar, Anima},
	month = jul,
	year = {2021},
	keywords = {Complexity theory, Computational modeling, Data mining, Data models, Measurement, Safety, Trajectory},
	pages = {157--164},
}

@inproceedings{ghodsi_generating_2021-1,
	title = {Generating and {Characterizing} {Scenarios} for {Safety} {Testing} of {Autonomous} {Vehicles}},
	doi = {10.1109/IV48863.2021.9576023},
	abstract = {Extracting interesting scenarios from real-world data as well as generating failure cases is important for the development and testing of autonomous systems. We propose efficient mechanisms to both characterize and generate testing scenarios using a state-of-the-art driving simulator. For any scenario, our method generates a set of possible driving paths and identifies all the possible safe driving trajectories that can be taken starting at different times, to compute metrics that quantify the complexity of the scenario. We use our method to characterize real driving data from the Next Generation Simulation (NGSIM) project, as well as adversarial scenarios generated in simulation. We rank the scenarios by defining metrics based on the complexity of avoiding accidents and provide insights into how the AV could have minimized the probability of incurring an accident. We demonstrate a strong correlation between the proposed metrics and human intuition.},
	booktitle = {2021 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Ghodsi, Zahra and Hari, Siva Kumar Sastry and Frosio, Iuri and Tsai, Timothy and Troccoli, Alejandro and Keckler, Stephen W. and Garg, Siddharth and Anandkumar, Anima},
	month = jul,
	year = {2021},
	keywords = {Complexity theory, Computational modeling, Data mining, Data models, Measurement, Safety, Trajectory},
	pages = {157--164},
}

@inproceedings{Dube2019DataLabeling4TransferLearning,
	title = {Automatic {Labeling} of {Data} for {Transfer} {Learning}},
	abstract = {Transfer learning uses trained weights from a source model as the initial weights for the training of a target dataset. A well chosen source with a large number of labeled data leads to signiﬁcant improvement in accuracy. We demonstrate a technique that automatically labels large unlabeled datasets so that they can train source models for transfer learning. We experimentally evaluate this method, using a baseline dataset of human-annotated ImageNet1K labels, against ﬁve variations of this technique. We show that the performance of these automatically trained models come within 6\% of baseline.},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}) {Workshops}},
	author = {Dube, Parijat and Bhattacharjee, Bishwaranjan and Huo, Siyu and Watson, Patrick and Belgodere, Brian},
	year = {2019},
	pages = {122--129},
}

@inproceedings{Du2017DeepLog,
	address = {Dallas, Texas, USA},
	title = {{DeepLog}: {Anomaly} {Detection} and {Diagnosis} from {System} {Logs} through {Deep} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3133956.3134015},
	doi = {10.1145/3133956.3134015},
	abstract = {Anomaly detection is a critical step towards building a secure and trustworthy system. e primary purpose of a system log is to record system states and signi cant events at various critical points to help debug system failures and perform root cause analysis. Such log data is universally available in nearly all computer systems. Log data is an important and valuable resource for understanding system status and performance issues; therefore, the various system logs are naturally excellent source of information for online monitoring and anomaly detection. We propose DeepLog, a deep neural network model utilizing Long Short-Term Memory (LSTM), to model a system log as a natural language sequence. is allows DeepLog to automatically learn log pa erns from normal execution, and detect anomalies when log pa erns deviate from the model trained from log data under normal execution. In addition, we demonstrate how to incrementally update the DeepLog model in an online fashion so that it can adapt to new log pa erns over time. Furthermore, DeepLog constructs work ows from the underlying system log so that once an anomaly is detected, users can diagnose the detected anomaly and perform root cause analysis e ectively. Extensive experimental evaluations over large log data have shown that DeepLog has outperformed other existing log-based anomaly detection methods based on traditional data mining methodologies.},
	booktitle = {{ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security} ({CCS})},
	publisher = {Association for Computing Machinery},
	author = {Du, Min and Li, Feifei and Zheng, Guineng and Srikumar, Vivek},
	year = {2017},
	pages = {1285--1298},
}

@inproceedings{He2016ResNet,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://doi.org/10.1109/CVPR.2016.90},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	booktitle = {Conference on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
}

@article{DelaCruz2019PT4DeepRL,
	title = {Pre-training with {Non}-expert {Human} {Demonstration} for {Deep} {Reinforcement} {Learning}},
	volume = {34},
	url = {https://doi.org/10.1017/S0269888919000055},
	doi = {10.1017/S0269888919000055},
	abstract = {Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by using deep neural networks as function approximators to learn directly from raw input images. However, learning directly from raw images is data inefficient. The agent must learn feature representation of complex states in addition to learning a policy. As a result, deep RL typically suffers from slow learning speeds and often requires a prohibitively large amount of training time and data to reach reasonable performance, making it inapplicable to real-world settings where data is expensive. In this work, we improve data efficiency in deep RL by addressing one of the two learning goals, feature learning. We leverage supervised learning to pre-train on a small set of non-expert human demonstrations and empirically evaluate our approach using the asynchronous advantage actor-critic algorithms (A3C) in the Atari domain. Our results show significant improvements in learning speed, even when the provided demonstration is noisy and of low quality.},
	journal = {The Knowledge Engineering Review},
	author = {de la Cruz, Gabriel V. and Du, Yunshu and Taylor, Matthew E.},
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{patterson2021carbon,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	url = {https://arxiv.org/abs/2104.10350},
	doi = {10.48550/arXiv.2104.10350},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	publisher = {arXiv},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	year = {2021},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{Rezende2017MaliciousSWClassificationUsingTLofResNet,
	title = {Malicious {Software} {Classification} {Using} {Transfer} {Learning} of {ResNet}-50 {Deep} {Neural} {Network}},
	abstract = {Malicious software (malware) has been extensively used for illegal activity and new malware variants are discovered at an alarmingly high rate. The ability to group malware variants into families with similar characteristics makes possible to create mitigation strategies that work for a whole class of programs. In this paper, we present a malware family classification approach using a deep neural network based on the ResNet-50 architecture. Malware samples are represented as byteplot grayscale images and a deep neural network is trained freezing the convolutional layers of ResNet-50 pre-trained on the ImageNet dataset and adapting the last layer to malware family classification. The experimental results on a dataset comprising 9,339 samples from 25 different families showed that our approach can effectively be used to classify malware families with an accuracy of 98.62\%.},
	booktitle = {International {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Rezende, Edmar and Ruppert, Guilherme and Carvalho, Tiago and Ramos, Fabio and de Geus, Paulo},
	year = {2017},
	keywords = {Convolutional Neural Networks, Convolutional codes, Deep Learning, Feature extraction, Gray-scale, Malicious Software Classification, Malware, Neural networks, Transfer Learning, Visualization},
	pages = {1011--1014},
}

@inproceedings{Reddy2019TL4MalariaCellImageClassification,
	title = {Transfer {Learning} with {ResNet}-50 for {Malaria} {Cell}-{Image} {Classification}},
	abstract = {Malaria is an infectious disease caused by single-celled parasite of plasmodium group. The disease is more often spread by an Infected Female Anopheles mosquito. In 2017 alone 219 million cases and nearly 435,000 deaths were reported, with more than 40\% of global population at risk. In spite of many advanced evaluation techniques for identifying the infection, microscopists at resource constrained regions face challenge in improving the diagnostic accuracy. Deep learning based classification of cell images prevent the wrong diagnostic decisions. This paper focuses on the implementation of Transfer learning based classification of malarial infected cells to improve the diagnostic accuracy. The experimental results show that transfer learning performs well on microscopic cell-images.},
	booktitle = {International {Conference} on {Communication} and {Signal} {Processing} ({ICCSP})},
	author = {Reddy, A. Sai Bharadwaj and Juliet, D. Sujitha},
	year = {2019},
	keywords = {Biomedical imaging, Deep Learning, Deep learning, Diseases, Inception model, Malaria, Neural networks, Neurons, ResNet50 model, Task analysis, Training, Transfer Learning, VGG-16 model},
	pages = {0945--0949},
}

@article{Qiu2020PTM4NLP,
	title = {Pre-trained models for natural language processing: {A} survey},
	volume = {63},
	journal = {Science China Technological Sciences},
	author = {Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
	year = {2020},
	pages = {1872--1897},
}

@article{Manikas2013SWEcosystem,
	title = {Software ecosystems – {A} systematic literature review},
	volume = {86},
	journal = {Journal of Systems and Software (JSS)},
	author = {Manikas, Konstantinos and Hansen, Klaus Marius},
	year = {2013},
	pages = {1294--1306},
}

@inproceedings{ImageNet,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {6},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	pages = {84--90},
}

@article{Han2021PTM,
	title = {Pre-trained models: {Past}, present and future},
	volume = {2},
	abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
	journal = {AI Open},
	author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
	year = {2021},
	pages = {225--250},
}

@article{Hamon2020RobustnessandExplainabilityofAI,
	title = {Robustness and explainability of {Artificial} {Intelligence}: from technical to policy solutions.},
	shorttitle = {Robustness and explainability of {Artificial} {Intelligence}},
	journal = {Publications Office of the European Union},
	author = {Hamon, Ronan and Junklewitz, Henrik and Sanchez, Ignacio},
	year = {2020},
}

@misc{Goyal2018ImageNetin1Hour,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	url = {https://arxiv.org/abs/1706.02677},
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\textasciitilde}90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{Fujiyoshi2019ImageRecognition4AV,
	title = {Deep learning-based image recognition for autonomous driving},
	volume = {43},
	abstract = {Various image recognition tasks were handled in the image recognition ﬁeld prior to 2010 by combining image local features manually designed by researchers (called handcrafted features) and machine learning method. After entering the 2010, However, many image recognition methods that use deep learning have been proposed. The image recognition methods using deep learning are far superior to the methods used prior to the appearance of deep learning in general object recognition competitions. Hence, this paper will explain how deep learning is applied to the ﬁeld of image recognition, and will also explain the latest trends of deep learning-based autonomous driving.},
	journal = {IATSS Research},
	author = {Fujiyoshi, Hironobu and Hirakawa, Tsubasa and Yamashita, Takayoshi},
	year = {2019},
	pages = {244--252},
}

@misc{DoshiVelez2017RigoriousInterpretableML,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {https://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	author = {Doshi-Velez, Finale and Kim, Been},
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Creswell2018GAN,
	title = {Generative {Adversarial} {Networks}: {An} {Overview}},
	volume = {35},
	doi = {10.1109/MSP.2017.2765202},
	abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
	journal = {IEEE Signal Processing Magazine},
	author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
	year = {2018},
	keywords = {Convolutional codes, Data models, Generators, Image resolution, Machine learning, Semantics, Signal resolution, Training data},
	pages = {53--65},
}

@article{Braiek2020onTestingMLPrograms,
	title = {On testing machine learning programs},
	volume = {164},
	abstract = {Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs.},
	journal = {Journal of Systems and Software (JSS)},
	author = {Braiek, Houssem Ben and Khomh, Foutse},
	year = {2020},
	pages = {110542},
}

@inproceedings{Bibal2016InterpretabilityofMLModelsandRepresentations,
	title = {Interpretability of {Machine} {Learning} {Models} and {Representations}: an {Introduction}},
	abstract = {Interpretability is often a major concern in machine learning. Although many authors agree with this statement, interpretability is often tackled with intuitive arguments, distinct (yet related) terms and heuristic quantifications. This short survey aims to clarify the concepts related to interpretability and emphasises the distinction between interpreting models
and representations, as well as heuristic-based and user-based approaches.},
	booktitle = {European {Symposium} on {Artificial} {Neural} {Networks}},
	author = {Bibal, Adrien and Frénay, Benoît},
	year = {2016},
}

@article{deng_fuzzing_2022,
	title = {Fuzzing {Deep}-{Learning} {Libraries} via {Automated} {Relational} {API} {Inference}},
	abstract = {Deep Learning (DL) has gained wide attention in recent years. Meanwhile, bugs in DL systems can lead to serious consequences, and may even threaten human lives. As a result, a growing body of research has been dedicated to DL model testing. However, there is still limited work on testing DL libraries, e.g., PyTorch and TensorFlow, which serve as the foundations for building, training, and running DL models. Prior work on fuzzing DL libraries can only generate tests for APIs which have been invoked by documentation examples, developer tests, or DL models, leaving a large number of APIs untested. In this paper, we propose DeepREL, the first approach to automatically inferring relational APIs for more effective DL library fuzzing. Our basic hypothesis is that for a DL library under test, there may exist a number of APIs sharing similar input parameters and outputs; in this way, we can easily “borrow” test inputs from invoked APIs to test other relational APIs. Furthermore, we formalize the notion of value equivalence and status equivalence for relational APIs to serve as the oracle for effective bug finding. We have implemented DeepREL as a fully automated end-to-end relational API inference and fuzzing technique for DL libraries, which 1) automatically infers potential API relations based on API syntactic/semantic information, 2) synthesizes concrete test programs for invoking relational APIs, 3) validates the inferred relational APIs via representative test inputs, and finally 4) performs fuzzing on the verified relational APIs to find potential inconsistencies. Our evaluation on two of the most popular DL libraries, PyTorch and TensorFlow, demonstrates that DeepREL can cover 157\% more APIs than state-of-the-art FreeFuzz. To date, DeepREL has detected 162 bugs in total, with 106 already confirmed by the developers as previously unknown bugs. Surprisingly, DeepREL has detected 13.5\% of the high-priority bugs for the entire PyTorch issue-tracking system in a three-month period. Also, besides the 162 code bugs, we have also detected 14 documentation bugs (all confirmed).},
	language = {en},
	author = {Deng, Yinlin and Yang, Chenyuan and Wei, Anjiang and Zhang, Lingming},
	year = {2022},
	pages = {13},
}

@article{chiang_transferability_2022,
	title = {On the {Transferability} of {Pre}-trained {Language} {Models}: {A} {Study} from {Artificial} {Datasets}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	shorttitle = {On the {Transferability} of {Pre}-trained {Language} {Models}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21295},
	doi = {10.1609/aaai.v36i10.21295},
	abstract = {Pre-training language models (LMs) on large-scale unlabeled text data makes the model much easier to achieve exceptional downstream performance than their counterparts directly trained on the downstream tasks. In this work, we study what speciﬁc traits in the pre-training data, other than the semantics, make a pre-trained LM superior to their counterparts trained from scratch on downstream tasks. We propose to use artiﬁcially constructed datasets as the pre-training data to exclude the effect of semantics, and further control what characteristics the pre-training corpora have. By ﬁne-tuning the pre-trained models on GLUE benchmark, we can learn how beneﬁcial it is to transfer the knowledge from the model trained on the dataset possessing that speciﬁc trait. We deﬁne and discuss three different characteristics in the artiﬁcial dataset: 1) matching the token’s uni-gram or bi-gram distribution between pre-training and downstream ﬁne-tuning, 2) the presence of the explicit dependencies among the tokens in a sequence, 3) the length of the implicit dependencies among the tokens in a sequence. Our experiments show that the explicit dependencies in the sequences of the pre-training data are critical to the downstream performance. Our results also reveal that models achieve better downstream performance when pre-trained on a dataset with a longer range of implicit dependencies. Based on our analysis, we ﬁnd that models pretrained with artiﬁcial datasets are prone to learn spurious correlation in downstream tasks. Our work reveals that even if the LMs are not pre-trained on natural language, they still gain transferability on certain human language downstream tasks once the LMs learn to model the token dependencies in the sequences. This result helps us understand the exceptional transferability of pre-trained LMs.},
	language = {en},
	number = {10},
	urldate = {2022-09-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chiang, Cheng-Han and Lee, Hung-yi},
	month = jun,
	year = {2022},
	pages = {10518--10525},
}

@inproceedings{haque_ereba_2022,
	title = {{EREBA}: {Black}-box {Energy} {Testing} of {Adaptive} {Neural} {Networks}},
	shorttitle = {{EREBA}},
	url = {http://arxiv.org/abs/2202.06084},
	doi = {10.1145/3510003.3510088},
	abstract = {Recently, various Deep Neural Network (DNN) models have been proposed for environments like embedded systems with stringent energy constraints. The fundamental problem of determining the robustness of a DNN with respect to its energy consumption (energy robustness) is relatively unexplored compared to accuracy-based robustness. This work investigates the energy robustness of Adaptive Neural Networks (AdNNs), a type of energy-saving DNNs proposed for many energy-sensitive domains and have recently gained traction. We propose EREBA, the first black-box testing method for determining the energy robustness of an AdNN. EREBA explores and infers the relationship between inputs and the energy consumption of AdNNs to generate energy surging samples. Extensive implementation and evaluation using three state-of-the-art AdNNs demonstrate that test inputs generated by EREBA could degrade the performance of the system substantially. The test inputs generated by EREBA can increase the energy consumption of AdNNs by 2,000\% compared to the original inputs. Our results also show that test inputs generated via EREBA are valuable in detecting energy surging inputs.},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	author = {Haque, Mirazul and Yadlapalli, Yaswanth and Yang, Wei and Liu, Cong},
	month = may,
	year = {2022},
	note = {arXiv:2202.06084 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {835--846},
}

@inproceedings{yu_automated_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Automated assertion generation via information retrieval and its integration with deep learning},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510149},
	doi = {10.1145/3510003.3510149},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Yu, Hao and Lou, Yiling and Sun, Ke and Ran, Dezhi and Xie, Tao and Hao, Dan and Li, Ying and Li, Ge and Wang, Qianxiang},
	month = may,
	year = {2022},
	pages = {163--174},
}

@inproceedings{liu_deepstate_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{DeepState}: selecting test suites to enhance the robustness of recurrent neural networks},
	isbn = {978-1-4503-9221-1},
	shorttitle = {{DeepState}},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510231},
	doi = {10.1145/3510003.3510231},
	abstract = {Deep Neural Networks (DNN) have achieved tremendous success in various software applications. However, accompanied by outstanding effectiveness, DNN-driven software systems could also exhibit incorrect behaviors and result in some critical accidents and losses. The testing and optimization of DNN-driven software systems rely on a large number of labeled data that often require many human efforts, resulting in high test costs and low efficiency. Although plenty of coverage-based criteria have been proposed to assist in the data selection of convolutional neural networks, it is difficult to apply them on Recurrent Neural Network (RNN) models due to the difference between the working nature.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Liu, Zixi and Feng, Yang and Yin, Yining and Chen, Zhenyu},
	month = may,
	year = {2022},
	pages = {598--609},
}

@book{fingscheidt_deep_2022,
	address = {Cham},
	title = {Deep {Neural} {Networks} and {Data} for {Automated} {Driving}: {Robustness}, {Uncertainty} {Quantification}, and {Insights} {Towards} {Safety}},
	isbn = {978-3-031-01232-7 978-3-031-01233-4},
	shorttitle = {Deep {Neural} {Networks} and {Data} for {Automated} {Driving}},
	url = {https://link.springer.com/10.1007/978-3-031-01233-4},
	language = {en},
	urldate = {2022-09-18},
	publisher = {Springer International Publishing},
	editor = {Fingscheidt, Tim and Gottschalk, Hanno and Houben, Sebastian},
	year = {2022},
	doi = {10.1007/978-3-031-01233-4},
}

@misc{hendrycks_benchmarking_2019,
	title = {Benchmarking {Neural} {Network} {Robustness} to {Common} {Corruptions} and {Perturbations}},
	url = {http://arxiv.org/abs/1903.12261},
	abstract = {In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	urldate = {2022-09-18},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	month = mar,
	year = {2019},
	note = {arXiv:1903.12261 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hu_characterizing_2022,
	title = {Characterizing and {Understanding} the {Behavior} of {Quantized} {Models} for {Reliable} {Deployment}},
	url = {http://arxiv.org/abs/2204.04220},
	abstract = {Deep Neural Networks (DNNs) have gained considerable attention in the past decades due to their astounding performance in different applications, such as natural language modeling, self-driving assistance, and source code understanding. With rapid exploration, more and more complex DNN architectures have been proposed along with huge pre-trained model parameters. The common way to use such DNN models in user-friendly devices (e.g., mobile phones) is to perform model compression before deployment. However, recent research has demonstrated that model compression, e.g., model quantization, yields accuracy degradation as well as outputs disagreements when tested on unseen data. Since the unseen data always include distribution shifts and often appear in the wild, the quality and reliability of quantized models are not ensured. In this paper, we conduct a comprehensive study to characterize and help users understand the behaviors of quantized models. Our study considers 4 datasets spanning from image to text, 8 DNN architectures including feed-forward neural networks and recurrent neural networks, and 42 shifted sets with both synthetic and natural distribution shifts. The results reveal that 1) data with distribution shifts happen more disagreements than without. 2) Quantization-aware training can produce more stable models than standard, adversarial, and Mixup training. 3) Disagreements often have closer top-1 and top-2 output probabilities, and \$Margin\$ is a better indicator than the other uncertainty metrics to distinguish disagreements. 4) Retraining with disagreements has limited efficiency in removing disagreements. We opensource our code and models as a new benchmark for further studying the quantized models.},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Hu, Qiang and Guo, Yuejun and Cordy, Maxime and Xie, Xiaofei and Ma, Wei and Papadakis, Mike and Traon, Yves Le},
	month = apr,
	year = {2022},
	note = {arXiv:2204.04220 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{zhu_fuzzing_2022,
	title = {Fuzzing: {A} {Survey} for {Roadmap}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Fuzzing},
	url = {https://dl.acm.org/doi/10.1145/3512345},
	doi = {10.1145/3512345},
	abstract = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this article, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
	language = {en},
	number = {11s},
	urldate = {2022-09-16},
	journal = {ACM Computing Surveys},
	author = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
	month = jan,
	year = {2022},
	pages = {1--36},
}

@misc{yu_deeprepair_2020,
	title = {{DeepRepair}: {Style}-{Guided} {Repairing} for {DNNs} in the {Real}-world {Operational} {Environment}},
	shorttitle = {{DeepRepair}},
	url = {http://arxiv.org/abs/2011.09884},
	abstract = {Deep neural networks (DNNs) are being widely applied for various real-world applications across domains due to their high performance (e.g., high accuracy on image classification). Nevertheless, a well-trained DNN after deployment could oftentimes raise errors during practical use in the operational environment due to the mismatching between distributions of the training dataset and the potential unknown noise factors in the operational environment, e.g., weather, blur, noise etc. Hence, it poses a rather important problem for the DNNs' real-world applications: how to repair the deployed DNNs for correcting the failure samples (i.e., incorrect prediction) under the deployed operational environment while not harming their capability of handling normal or clean data. The number of failure samples we can collect in practice, caused by the noise factors in the operational environment, is often limited. Therefore, It is rather challenging how to repair more similar failures based on the limited failure samples we can collect. In this paper, we propose a style-guided data augmentation for repairing DNN in the operational environment. We propose a style transfer method to learn and introduce the unknown failure patterns within the failure data into the training data via data augmentation. Moreover, we further propose the clustering-based failure data generation for much more effective style-guided data augmentation. We conduct a large-scale evaluation with fifteen degradation factors that may happen in the real world and compare with four state-of-the-art data augmentation methods and two DNN repairing methods, demonstrating that our method can significantly enhance the deployed DNNs on the corrupted data in the operational environment, and with even better accuracy on clean datasets.},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Yu, Bing and Qi, Hua and Guo, Qing and Juefei-Xu, Felix and Xie, Xiaofei and Ma, Lei and Zhao, Jianjun},
	month = nov,
	year = {2020},
	note = {arXiv:2011.09884 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{shen_improving_2021,
	title = {Improving {Robustness} of {Learning}-based {Autonomous} {Steering} {Using} {Adversarial} {Images}},
	url = {http://arxiv.org/abs/2102.13262},
	abstract = {For safety of autonomous driving, vehicles need to be able to drive under various lighting, weather, and visibility conditions in different environments. These external and environmental factors, along with internal factors associated with sensors, can pose significant challenges to perceptual data processing, hence affecting the decision-making and control of the vehicle. In this work, we address this critical issue by introducing a framework for analyzing robustness of the learning algorithm w.r.t varying quality in the image input for autonomous driving. Using the results of sensitivity analysis, we further propose an algorithm to improve the overall performance of the task of "learning to steer". The results show that our approach is able to enhance the learning outcomes up to 48\%. A comparative study drawn between our approach and other related techniques, such as data augmentation and adversarial training, confirms the effectiveness of our algorithm as a way to improve the robustness and generalization of neural network training for autonomous driving.},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Shen, Yu and Zheng, Laura and Shu, Manli and Li, Weizi and Goldstein, Tom and Lin, Ming C.},
	month = feb,
	year = {2021},
	note = {arXiv:2102.13262 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{bonomi_fog_nodate,
	title = {Fog computing and its role in the internet of things},
	abstract = {Fog Computing extends the Cloud Computing paradigm to the edge of the network, thus enabling a new breed of applications and services. Deﬁning characteristics of the Fog are: a) Low latency and location awareness; b) Wide-spread geographical distribution; c) Mobility; d) Very large number of nodes, e) Predominant role of wireless access, f) Strong presence of streaming and real time applications, g) Heterogeneity. In this paper we argue that the above characteristics make the Fog the appropriate platform for a number of critical Internet of Things (IoT) services and applications, namely, Connected Vehicle, Smart Grid , Smart Cities, and, in general, Wireless Sensors and Actuators Networks (WSANs).},
	language = {en},
	author = {Bonomi, Flavio and Milito, Rodolfo and Zhu, Jiang and Addepalli, Sateesh},
	pages = {3},
}

@misc{geirhos_imagenet-trained_2019,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {http://arxiv.org/abs/1811.12231},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = jan,
	year = {2019},
	note = {arXiv:1811.12231 [cs, q-bio, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@inproceedings{salman_adversarially_2020,
	title = {Do {Adversarially} {Robust} {ImageNet} {Models} {Transfer} {Better}?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/24357dd085d2c4b1a88a7e0692e60294-Abstract.html},
	abstract = {Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Code and models is available in the supplementary material.},
	urldate = {2022-09-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
	year = {2020},
	pages = {3533--3545},
}

@misc{du_survey_2022,
	title = {A {Survey} of {Vision}-{Language} {Pre}-{Trained} {Models}},
	url = {http://arxiv.org/abs/2202.10936},
	abstract = {As transformer evolves, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve downstream task performance becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, and then we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with synthesis and pointer to related research.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Du, Yifan and Liu, Zikang and Li, Junyi and Zhao, Wayne Xin},
	month = jul,
	year = {2022},
	note = {arXiv:2202.10936 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_benchmarking_2021,
	title = {Benchmarking {Detection} {Transfer} {Learning} with {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2111.11429},
	abstract = {Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4\% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Li, Yanghao and Xie, Saining and Chen, Xinlei and Dollar, Piotr and He, Kaiming and Girshick, Ross},
	month = nov,
	year = {2021},
	note = {arXiv:2111.11429 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_benchmarking_2021-1,
	title = {Benchmarking {Detection} {Transfer} {Learning} with {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2111.11429},
	abstract = {Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4\% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Li, Yanghao and Xie, Saining and Chen, Xinlei and Dollar, Piotr and He, Kaiming and Girshick, Ross},
	month = nov,
	year = {2021},
	note = {arXiv:2111.11429 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chiang_transferability_2022-1,
	title = {On the {Transferability} of {Pre}-trained {Language} {Models}: {A} {Study} from {Artificial} {Datasets}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	shorttitle = {On the {Transferability} of {Pre}-trained {Language} {Models}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21295},
	doi = {10.1609/aaai.v36i10.21295},
	abstract = {Pre-training language models (LMs) on large-scale unlabeled text data makes the model much easier to achieve exceptional downstream performance than their counterparts directly trained on the downstream tasks. In this work, we study what speciﬁc traits in the pre-training data, other than the semantics, make a pre-trained LM superior to their counterparts trained from scratch on downstream tasks. We propose to use artiﬁcially constructed datasets as the pre-training data to exclude the effect of semantics, and further control what characteristics the pre-training corpora have. By ﬁne-tuning the pre-trained models on GLUE benchmark, we can learn how beneﬁcial it is to transfer the knowledge from the model trained on the dataset possessing that speciﬁc trait. We deﬁne and discuss three different characteristics in the artiﬁcial dataset: 1) matching the token’s uni-gram or bi-gram distribution between pre-training and downstream ﬁne-tuning, 2) the presence of the explicit dependencies among the tokens in a sequence, 3) the length of the implicit dependencies among the tokens in a sequence. Our experiments show that the explicit dependencies in the sequences of the pre-training data are critical to the downstream performance. Our results also reveal that models achieve better downstream performance when pre-trained on a dataset with a longer range of implicit dependencies. Based on our analysis, we ﬁnd that models pretrained with artiﬁcial datasets are prone to learn spurious correlation in downstream tasks. Our work reveals that even if the LMs are not pre-trained on natural language, they still gain transferability on certain human language downstream tasks once the LMs learn to model the token dependencies in the sequences. This result helps us understand the exceptional transferability of pre-trained LMs.},
	language = {en},
	number = {10},
	urldate = {2022-09-14},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chiang, Cheng-Han and Lee, Hung-yi},
	month = jun,
	year = {2022},
	pages = {10518--10525},
}

@inproceedings{you_logme_2021,
	title = {{LogME}: {Practical} {Assessment} of {Pre}-trained {Models} for {Transfer} {Learning}},
	shorttitle = {{LogME}},
	url = {https://proceedings.mlr.press/v139/you21b.html},
	abstract = {This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo {\textbackslash}emph\{without fine-tuning\}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is {\textbackslash}emph\{immune to over-fitting\}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is {\textbackslash}emph\{fast, accurate, and general\}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most \$3000{\textbackslash}times\$ speedup in wall-clock time and requires only \$1\%\$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: {\textbackslash}href\{https://github.com/thuml/LogME\}\{https://github.com/thuml/LogME\}.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {You, Kaichao and Liu, Yong and Wang, Jianmin and Long, Mingsheng},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12133--12143},
}

@inproceedings{davis_testing_2019,
	address = {San Diego, CA, USA},
	title = {Testing {Regex} {Generalizability} {And} {Its} {Implications}: {A} {Large}-{Scale} {Many}-{Language} {Measurement} {Study}},
	isbn = {978-1-72812-508-4},
	shorttitle = {Testing {Regex} {Generalizability} {And} {Its} {Implications}},
	url = {https://ieeexplore.ieee.org/document/8952443/},
	doi = {10.1109/ASE.2019.00048},
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and ﬁndings to date are typically generalized from regexes written in only 1–2 programming languages. This is an incomplete foundation.},
	language = {en},
	urldate = {2022-09-13},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Davis, James C and Moyer, Daniel and Kazerouni, Ayaan M and Lee, Dongyoon},
	month = nov,
	year = {2019},
	pages = {427--439},
}

@inproceedings{sillito_failures_2020,
	title = {Failures and {Fixes}: {A} {Study} of {Software} {System} {Incident} {Response}},
	shorttitle = {Failures and {Fixes}},
	doi = {10.1109/ICSME46990.2020.00027},
	abstract = {This paper presents the results of a research study related to software system failures, with the goal of understanding how we might better evolve, maintain and support software systems in production. We have qualitatively analyzed thirty incidents: fifteen collected through in depth interviews with engineers, and fifteen sampled from publicly published incident reports (generally produced as part of postmortem reviews). Our analysis focused on understanding and categorizing how failures occurred, and how they were detected, investigated and mitigated. We also captured analytic insights related to the current state of the practice and associated challenges in the form of 11 key observations. For example, we observed that failures can cascade through a system leading to major outages; and that often engineers do not understand the scaling limits of systems they are supporting until those limits are exceeded. We argue that the challenges we have identified can lead to improvements to how systems are engineered and supported.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Sillito, Jonathan and Kutomi, Esdras},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Conferences, Interviews, Monitoring, Production, Software maintenance, Software systems, empirical studies, incident response, software failures, software monitoring},
	pages = {185--195},
}

@inproceedings{sillito_failures_2020-1,
	title = {Failures and {Fixes}: {A} {Study} of {Software} {System} {Incident} {Response}},
	shorttitle = {Failures and {Fixes}},
	doi = {10.1109/ICSME46990.2020.00027},
	abstract = {This paper presents the results of a research study related to software system failures, with the goal of understanding how we might better evolve, maintain and support software systems in production. We have qualitatively analyzed thirty incidents: fifteen collected through in depth interviews with engineers, and fifteen sampled from publicly published incident reports (generally produced as part of postmortem reviews). Our analysis focused on understanding and categorizing how failures occurred, and how they were detected, investigated and mitigated. We also captured analytic insights related to the current state of the practice and associated challenges in the form of 11 key observations. For example, we observed that failures can cascade through a system leading to major outages; and that often engineers do not understand the scaling limits of systems they are supporting until those limits are exceeded. We argue that the challenges we have identified can lead to improvements to how systems are engineered and supported.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Sillito, Jonathan and Kutomi, Esdras},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Conferences, Interviews, Monitoring, Production, Software maintenance, Software systems, empirical studies, incident response, software failures, software monitoring},
	pages = {185--195},
}

@inproceedings{Wang2021UAVBug,
	title = {An exploratory study of autopilot software bugs in unmanned aerial vehicles},
	doi = {10.1145/3468264.3468559},
	abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly important and widely used in modern society. Software bugs in these systems can cause severe issues, such as system crashes, hangs, and undefined behaviors. Some bugs can also be exploited by hackers to launch security attacks, resulting in catastrophic consequences. Therefore, techniques that can help detect and fix software bugs in UAVs are highly desirable. However, although there are many existing studies on bugs in various types of software, the characteristics of UAV software bugs have never been systematically studied. This impedes the development of tools for assuring the dependability of UAVs. To bridge this gap, we conducted the first large-scale empirical study on two well-known open-source autopilot software platforms for UAVs, namely PX4 and Ardupilot, to characterize bugs in UAVs. Through analyzing 569 bugs from these two projects, we observed eight types of UAV-specific bugs (i.e., limit, math, inconsistency, priority, parameter, hardware support, correction, and initialization) and learned their root causes. Based on the bug taxonomy, we summarized common bug patterns and repairing strategies. We further identified five challenges associated with detecting and fixing such UAV-specific bugs. Our study can help researchers and practitioners to better understand the threats to the dependability of UAV systems and facilitate the future development of UAV bug diagnosis tools.},
	booktitle = {{ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FES})},
	author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei},
	year = {2021},
}

@inproceedings{roth_odds_2019,
	title = {The {Odds} are {Odd}: {A} {Statistical} {Test} for {Detecting} {Adversarial} {Examples}},
	shorttitle = {The {Odds} are {Odd}},
	url = {https://proceedings.mlr.press/v97/roth19a.html},
	abstract = {We investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy.},
	language = {en},
	urldate = {2022-09-08},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Roth, Kevin and Kilcher, Yannic and Hofmann, Thomas},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5498--5507},
}

@techreport{Boyens2015SupplyChainRiskManagementPractices,
	title = {Supply {Chain} {Risk} {Management} {Practices} for {Federal} {Information} {Systems} and {Organizations}},
	abstract = {Federal agencies are concerned about the risks associated with information and communications technology (ICT) products and services that may contain potentially malicious functionality, are counterfeit, or are vulnerable due to poor manufacturing and development practices within the ICT supply chain. These risks are associated with the federal agencies’ decreased visibility into, understanding of, and control over how the technology that they acquire is developed, integrated and deployed, as well as the processes, procedures, and practices used to assure the integrity, security, resilience, and quality of the products and services.},
	number = {NIST SP 800-161},
	institution = {National Institute of Standards and Technology},
	author = {Boyens, Jon M. and Paulsen, Celia and Moorthy, Rama and Bartol, Nadya},
	year = {2015},
	doi = {10.6028/NIST.SP.800-161},
}

@techreport{boyens_supply_2015,
	title = {Supply {Chain} {Risk} {Management} {Practices} for {Federal} {Information} {Systems} and {Organizations}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-161.pdf},
	abstract = {Federal agencies are concerned about the risks associated with information and communications technology (ICT) products and services that may contain potentially malicious functionality, are counterfeit, or are vulnerable due to poor manufacturing and development practices within the ICT supply chain. These risks are associated with the federal agencies’ decreased visibility into, understanding of, and control over how the technology that they acquire is developed, integrated and deployed, as well as the processes, procedures, and practices used to assure the integrity, security, resilience, and quality of the products and services.},
	language = {en},
	number = {NIST SP 800-161},
	urldate = {2022-09-08},
	institution = {National Institute of Standards and Technology},
	author = {Boyens, Jon M. and Paulsen, Celia and Moorthy, Rama and Bartol, Nadya},
	month = apr,
	year = {2015},
	doi = {10.6028/NIST.SP.800-161},
	pages = {NIST SP 800--161},
}

@inproceedings{Zhang2020RetrainRecommenderSystem,
	title = {How to {Retrain} {Recommender} {System}?: {A} {Sequential} {Meta}-{Learning} {Method}},
	doi = {10.1145/3397271.3401167},
	abstract = {Practical recommender systems need be periodically retrained to refresh the model with new interaction data. To pursue high model fidelity, it is usually desirable to retrain the model on both historical and new data, since it can account for both long-term and short-term user preference. However, a full model retraining could be very time-consuming and memory-costly, especially when the scale of historical data is large. In this work, we study the model retraining mechanism for recommender systems, a topic of high practical values but has been relatively little explored in the research community.},
	booktitle = {International {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Zhang, Yang and Feng, Fuli and Wang, Chenxu and He, Xiangnan and Wang, Meng and Li, Yan and Zhang, Yongdong},
	year = {2020},
}

@article{Derakhshan2019CDofMLPipelines,
	title = {Continuous {Deployment} of {Machine} {Learning} {Pipelines}},
	abstract = {Today machine learning is entering many business and scienti c applications. The life cycle of machine learning applications consists of data preprocessing for transforming the raw data into features, training a model using the features, and deploying the model for answering prediction queries. In order to guarantee accurate predictions, one has to continuously monitor and update the deployed model and pipeline. Current deployment platforms update the model using online learning methods. When online learning alone is not adequate to guarantee the prediction accuracy, some deployment platforms provide a mechanism for automatic or manual retraining of the model. While the online training is fast, the retraining of the model is time-consuming and adds extra overhead and complexity to the process of deployment. We propose a novel continuous deployment approach for updating the deployed model using a combination of the incoming realtime data and the historical data. We utilize sampling techniques to include the historical data in the training process, thus eliminating the need for retraining the deployed model. We also o er online statistics computation and dynamic materialization of the preprocessed features, which further reduces the total training and data preprocessing time. In our experiments, we design and deploy two pipelines and models to process two real-world datasets. The experiments show that continuous deployment reduces the total training cost up to 15 times while providing the same level of quality when compared to the state-of-the-art deployment approaches.},
	journal = {EDBT},
	author = {Derakhshan, Behrouz and Mahdiraji, Alireza Rezaei and Rabl, Tilmann and Markl, Volker},
	year = {2019},
}

@inproceedings{WhatsinaGithubStar,
	title = {What's in a {GitHub} {Star}? {Understanding} {Repository} {Starring} {Practices} in a {Social} {Coding} {Platform}},
	doi = {10.1016/j.jss.2018.09.016},
	abstract = {Besides a git-based version control system, GitHub integrates several social coding features. Particularly, GitHub users can star a repository, presumably to manifest interest or satisfaction with an open source project. However, the real and practical meaning of starring a project was never the subject of an in-depth and well-founded empirical investigation. Therefore, we provide in this paper a throughout study on the meaning, characteristics, and dynamic growth of GitHub stars. First, by surveying 791 developers, we report that three out of four developers consider the number of stars before using or contributing to a GitHub project. Then, we report a quantitative analysis on the characteristics of the top-5,000 most starred GitHub repositories. We propose four patterns to describe stars growth, which are derived after clustering the time series representing the number of stars of the studied repositories; we also reveal the perception of 115 developers about these growth patterns. To conclude, we provide a list of recommendations to open source project managers (e.g., on the importance of social media promotion) and to GitHub users and Software Engineering researchers (e.g., on the risks faced when selecting projects by GitHub stars).},
	booktitle = {Journal of {Systems} and {Software} ({JSS})},
	author = {Borges, Hudson and Valente, Marco Tulio},
	year = {2018},
	keywords = {Computer Science - Software Engineering},
}

@article{Sakurada2021Industry4NewJob,
	title = {Analysis of {New} {Job} {Profiles} for the {Factory} of the {Future}},
	doi = {10.1007/978-3-030-69373-2_18},
	abstract = {Industry 4.0 is being promoting the digitisation of manufacturing sector towards smart products, machines, processes and factories. The adoption of disruptive technologies associated to this industrial revolution will lead to re-shaping the manufacturing environment, decreasing the low-skilled activities and increasing the high-skill activities, being expected to grow the complexity and number of new job profiles. In this context, this paper aims to analyse the literature and recruitment repositories to identify the new job profiles in the factory of the future (FoF) across six industrial technological sectors, namely Collaborative Robotics (Cobots), Additive Manufacturing (AM), Mechatronics and Machine Automation (MMA), Data Analytics (DA), Cybersecurity (CS) and Human-Machine Interface (HMI). The performed analysis allowed to compile a catalogue of 100 new job profiles that were characterised and analysed in terms of technical and soft skills, type and level of profile, as well as the frequency demand.},
	journal = {International Workshop on Service Orientation in Holonic and Multi-Agent Manufacturing},
	author = {Sakurada, Lucas and Geraldes, Carla A. S. and Fernandes, Florbela P. and Pontes, Joseane and Leitao, Paulo},
	year = {2020},
}

@inproceedings{IBM2020AIMMX,
	title = {{AIMMX}: {Artificial} {Intelligence} {Model} {Metadata} {Extractor}},
	doi = {10.1145/3379597.3387448},
	abstract = {Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-theart AI papers. Our platform extracted metadata with 87\% precision and 83\% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42\% of models in our sample citing their datasets, method reproducibility is more common at 72\% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar, Avraham and Mummert, Todd},
	year = {2020},
}

@article{Zaidi2022SurveyofObjectDetectionModels,
	title = {A {Survey} of {Modern} {Deep} {Learning} based {Object} {Detection} {Models}},
	abstract = {Object Detection is the task of classification and localization of objects in an image or video. It has gained prominence in recent years due to its widespread applications. This article surveys recent developments in deep learning based object detectors. Concise overview of benchmark datasets and evaluation metrics used in detection is also provided along with some of the prominent backbone architectures used in recognition tasks. It also covers contemporary lightweight classification models used on edge devices. Lastly, we compare the performances of these architectures on multiple metrics.},
	journal = {Digital Signal Processing},
	author = {Zaidi, Syed Sahil Abbas and Ansari, Mohammad Samar and Aslam, Asra and Kanwal, Nadia and Asghar, Mamoona and Lee, Brian},
	year = {2022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{pashchenko_qualitative_2020,
	address = {Virtual Event USA},
	title = {A {Qualitative} {Study} of {Dependency} {Management} and {Its} {Security} {Implications}},
	isbn = {978-1-4503-7089-9},
	url = {https://dl.acm.org/doi/10.1145/3372297.3417232},
	doi = {10.1145/3372297.3417232},
	abstract = {Several large scale studies on the Maven, NPM, and Android ecosystems point out that many developers do not often update their vulnerable software libraries thus exposing the user of their code to security risks. The purpose of this study is to qualitatively investigate the choices and the interplay of functional and security concerns on the developers’ overall decision-making strategies for selecting, managing, and updating software dependencies.},
	language = {en},
	urldate = {2022-09-02},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Pashchenko, Ivan and Vu, Duc-Ly and Massacci, Fabio},
	month = oct,
	year = {2020},
	pages = {1513--1531},
}

@inproceedings{de_souza_empirical_2008,
	address = {Leipzig, Germany},
	title = {An empirical study of software developers' management of dependencies and changes},
	isbn = {978-1-60558-079-1},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368122},
	doi = {10.1145/1368088.1368122},
	abstract = {Different approaches and tools have been proposed to support change impact analysis, i.e., the identification of the potential consequences of a change, or the estimation of what needs to be modified to accomplish a change. However, just a few empirical studies of software developers’ actual change impact analysis approaches have been reported in the literature. To minimize this gap, this paper describes an empirical study of two software development teams. It describes, through the presentation of ethnographic data, the strategies used by software developers to handle the effect of software dependencies and changes in their work. The concept of impact management is proposed as an analytical framework to present these practices and is used to suggest avenues for future research in change impact analysis techniques.},
	language = {en},
	urldate = {2022-09-02},
	booktitle = {Proceedings of the 13th international conference on {Software} engineering  - {ICSE} '08},
	publisher = {ACM Press},
	author = {de Souza, Cleidson R. B. and Redmiles, David F.},
	year = {2008},
	pages = {241},
}

@inproceedings{Thung2016APIRecommendationSystem,
	title = {{API} recommendation system for software development},
	abstract = {Nowadays, software developers often utilize existing third party libraries and make use of Application Programming Interface (API) to develop a software. However, it is not always obvious which library to use or whether the chosen library will play well with other libraries in the system. Furthermore, developers need to spend some time to understand the API to the point that they can freely use the API methods and putting the right parameters inside them. In this work, I plan to automatically recommend relevant APIs to developers. This API recommendation can be divided into multiple stages. First, we can recommend relevant libraries provided a given task to complete. Second, we can recommend relevant API methods that developer can use to program the required task. Third, we can recommend correct parameters for a given method according to its context. Last but not least, we can recommend how different API methods can be combined to achieve a given task. In effort to realize this API recommendation system, I have published two related papers. The first one deals with recommending additional relevant API libraries given known useful API libraries for the target program. This system can achieve recall rate@5 of 0.852 and recall rate@10 of 0.894 in recommending additional relevant libraries. The second one deals with recommending relevant API methods a given target API and a textual description of the task. This system can achieve recall-rate@5 of 0.690 and recall-rate@10 of 0.779. The results for both system indicate that the systems are useful and capable in recommending the right API/library reasonably well. Currently, I am working on another system which can recommend web APIs (i.e., libraries) given a description of the task. I am also working on a system that recommends correct parameters given an API method. In the future, I also plan to realize API composition recommendation for the given task.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Thung, Ferdian},
	year = {2016},
	keywords = {API, Context, Databases, Feature extraction, History, Libraries, Library, Recommendation System, Software, Training},
}

@inproceedings{Wang2020EmpiricalStudyofThirdPartyLibsinJavaProjects,
	title = {An {Empirical} {Study} of {Usages}, {Updates} and {Risks} of {Third}-{Party} {Libraries} in {Java} {Projects}},
	abstract = {Third-party libraries play a key role in software development as they can relieve developers of the heavy burden of re-implementing common functionalities. However, third-party libraries and client projects evolve asynchronously. As a result, out-dated third-party libraries might be used in client projects while developers are not aware of the potential risk (e.g., security bug). Outdated third-party libraries may be updated in client projects in a delayed way, and developers may be less aware of the potential risk (e.g., API incompatibility) in updates. Developers of third-party libraries may be unaware of how their third-party libraries are used or updated in client projects. Therefore, a quantitative and holistic study on usages, updates and risks of third-party libraries in open-source projects can provide concrete evidences on these problems, and practical insights to improve the ecosystem. In this paper, we contribute such a study in Java ecosystem. In particular, we conduct a library usage analysis (e.g., usage intensity and outdatedness) and library update analysis (e.g., update intensity and delay) on 806 open-source projects and 13,565 third- party libraries. Then, we carry out a library risk analysis (e.g., usage risk and update risk) on 806 open-source projects and 544 security bugs. These analyses aim to quantify the usage and update practices and the potential risk of using and updating outdated third-party libraries with respect to security bugs from two holistic perspectives (i.e., open-source projects and third-party libraries). Our findings suggest practical implications to developers and researchers on problems and potential solutions in maintaining third-party libraries (e.g., smart alerting and automated updating of outdated third-party libraries). To indicate the usefulness of our findings, we design a smart alerting system for assisting developers to make confident decisions when updating third-party libraries. 33 and 24 open-source projects have confirmed and updated third-party libraries after receiving our alerts.},
	booktitle = {International {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Wang, Ying and Chen, Bihuan and Huang, Kaifeng and Shi, Bowen and Xu, Congying and Peng, Xin and Wu, Yijian and Liu, Yang},
	year = {2020},
	keywords = {Computer bugs, Java, Libraries, Open source software, Security, Software maintenance, Tools, outdated libraries, security bugs},
}

@inproceedings{LariosVargas2020ThirdPartyLibrariesSelection,
	title = {Selecting third-party libraries: the practitioners’ perspective},
	abstract = {The selection of third-party libraries is an essential element of virtually any software development project. However, deciding which libraries to choose is a challenging practical problem. Selecting the wrong library can severely impact a software project in terms of cost, time, and development effort, with the severity of the impact depending on the role of the library in the software architecture, among others. Despite the importance of following a careful library selection process, in practice, the selection of third-party libraries is still conducted in an ad-hoc manner, where dozens of factors play an influential role in the decision.},
	booktitle = {{ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Larios Vargas, Enrique and Aniche, Maurício and Treude, Christoph and Bruntink, Magiel and Gousios, Georgios},
	year = {2020},
}

@article{Wing2021TrustworthyAI,
	title = {Trustworthy {AI}},
	abstract = {The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.},
	journal = {Communications of the ACM},
	author = {Wing, Jeannette M.},
	year = {2021},
}

@article{Marijan2020SWTesting4ML,
	title = {Software {Testing} for {Machine} {Learning}},
	doi = {10.1609/aaai.v34i09.7084},
	abstract = {Machine learning has become prevalent across a wide variety of applications. Unfortunately, machine learning has also shown to be susceptible to deception, leading to errors, and even fatal failures. This circumstance calls into question the widespread use of machine learning, especially in safety-critical applications, unless we are able to assure its correctness and trustworthiness properties. Software verification and testing are established technique for assuring such properties, for example by detecting errors. However, software testing challenges for machine learning are vast and profuse - yet critical to address. This summary talk discusses the current state-of-the-art of software testing for machine learning. More specifically, it discusses six key challenge areas for software testing of machine learning systems, examines current approaches to these challenges and highlights their limitations. The paper provides a research agenda with elaborated directions for making progress toward advancing the state-of-the-art on testing of machine learning.},
	journal = {AAAI Conference on Artificial Intelligence},
	author = {Marijan, Dusica and Gotlieb, Arnaud},
	year = {2020},
	keywords = {Summary Talks},
}

@article{He2021AutoMLSurvey,
	title = {{AutoML}: {A} survey of the state-of-the-art},
	abstract = {Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition, object detection, and language modeling. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising solution for building a DL system without human assistance and is being extensively studied. This paper presents a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we introduce AutoML methods –covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS) – with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize the representative NAS algorithms’ performance on the CIFAR-10 and ImageNet datasets and further discuss the following subjects of NAS methods: one/two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and resource-aware NAS. Finally, we discuss some open problems related to the existing AutoML methods for future research.},
	journal = {Knowledge-Based Systems},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	year = {2021},
}

@article{mohagheghi_quality_2007,
	title = {Quality, productivity and economic benefits of software reuse: a review of industrial studies},
	volume = {12},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Quality, productivity and economic benefits of software reuse},
	url = {http://link.springer.com/10.1007/s10664-007-9040-x},
	doi = {10.1007/s10664-007-9040-x},
	abstract = {Systematic software reuse is proposed to increase productivity and software quality and lead to economic benefits. Reports of successful software reuse programs in industry have been published. However, there has been little effort to organize the evidence systematically and appraise it. This review aims to assess the effects of software reuse in industrial contexts. Journals and major conferences between 1994 and 2005 were searched to find observational studies and experiments conducted in industry, returning eleven papers of observational type. Systematic software reuse is significantly related to lower problem (defect, fault or error) density in five studies and to decreased effort spent on correcting problems in three studies. The review found evidence for significant gains in apparent productivity in three studies. Other significant benefits of software reuse were reported in single studies or the results were inconsistent. Evidence from industry is sparse and combining results was done by vote-counting. Researchers should pay more attention to using comparable metrics, performing longitudinal studies, and explaining the results and impact on industry. For industry, evaluating reuse of COTS or OSS components, integrating reuse activities in software processes, better data collection and evaluating return on investment are major challenges.},
	language = {en},
	number = {5},
	urldate = {2022-08-31},
	journal = {Empirical Software Engineering},
	author = {Mohagheghi, Parastoo and Conradi, Reidar},
	month = sep,
	year = {2007},
	pages = {471--516},
}

@misc{VanOort2022SWQualityofMLProjects,
	title = {"{Project} smells" -- {Experiences} in {Analysing} the {Software} {Quality} of {ML} {Projects} with mllint},
	url = {http://arxiv.org/abs/2201.08246},
	abstract = {Machine Learning (ML) projects incur novel challenges in their development and productionisation over traditional software applications, though established principles and best practices in ensuring the project's software quality still apply. While using static analysis to catch code smells has been shown to improve software quality attributes, it is only a small piece of the software quality puzzle, especially in the case of ML projects given their additional challenges and lower degree of Software Engineering (SE) experience in the data scientists that develop them. We introduce the novel concept of project smells which consider deficits in project management as a more holistic perspective on software quality in ML projects. An open-source static analysis tool mllint was also implemented to help detect and mitigate these. Our research evaluates this novel concept of project smells in the industrial context of ING, a global bank and large software- and data-intensive organisation. We also investigate the perceived importance of these project smells for proof-of-concept versus production-ready ML projects, as well as the perceived obstructions and benefits to using static analysis tools such as mllint. Our findings indicate a need for context-aware static analysis tools, that fit the needs of the project at its current stage of development, while requiring minimal configuration effort from the user.},
	author = {van Oort, Bart and Cruz, Luís and Loni, Babak and van Deursen, Arie},
	year = {2022},
	keywords = {68-06, Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{michael2019RegexesareHard,
	title = {Regexes are {Hard}: {Decision}-{Making}, {Difficulties}, and {Risks} in {Programming} {Regular} {Expressions}},
	abstract = {Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Michael, Louis G. and Donohue, James and Davis, James C. and Lee, Dongyoon and Servant, Francisco},
	year = {2019},
	keywords = {Decision making, Face, Interviews, Programming profession, Software, Tools, regular expressions, developer process, qualitative research},
}

@inproceedings{Bacchelli2013CodeReview,
	title = {Expectations, outcomes, and challenges of modern code review},
	abstract = {Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more “lightweight” than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classified hundreds of review comments across diverse teams at Microsoft. Our study reveals that while finding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional benefits such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we find that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Bacchelli, Alberto and Bird, Christian},
	year = {2013},
	keywords = {Context, Guidelines, Inspection, Interviews, Knowledge transfer, Software, Sorting},
}

@misc{STRIDEAnalysis,
	title = {The {STRIDE} {Threat} {Model}},
	url = {https://docs.microsoft.com/en-us/azure/security/develop/threat-modeling-tool-threats},
	author = {Microsoft},
	year = {2022},
}

@inproceedings{Smith2020HarnessingMLEcosystem,
	title = {The {Machine} {Learning} {Bazaar}: {Harnessing} the {ML} {Ecosystem} for {Effective} {System} {Development}},
	abstract = {As machine learning is applied more widely, data scientists often struggle to find or create end-to-end machine learning systems for specific tasks. The proliferation of libraries and frameworks and the complexity of the tasks have led to the emergence of “pipeline jungles” — brittle, ad hoc ML systems. To address these problems, we introduce the Machine Learning Bazaar, a new framework for developing machine learning and automated machine learning software systems. First, we introduce ML primitives, a unified API and specification for data processing and ML components from different software libraries. Next, we compose primitives into usable ML pipelines, abstracting away glue code, data flow, and data storage. We further pair these pipelines with a hierarchy of AutoML strategies — Bayesian optimization and bandit learning. We use these components to create a general-purpose, multi-task, end-to-end AutoML system that provides solutions to a variety of data modalities (image, text, graph, tabular, relational, etc.) and problem types (classification, regression, anomaly detection, graph matching, etc.). We demonstrate 5 real-world use cases and 2 case studies of our approach. Finally, we present an evaluation suite of 456 real-world ML tasks and describe the characteristics of 2.5 million pipelines searched over this task suite.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	author = {Smith, Micah J. and Sala, Carles and Kanter, James Max and Veeramachaneni, Kalyan},
	month = jun,
	year = {2020},
}

@misc{Brundage2020TowardTrustworthyAIDevelopment,
	address = {arXiv},
	title = {Toward {Trustworthy} {AI} {Development}: {Mechanisms} for {Supporting} {Verifiable} {Claims}},
	url = {https://arxiv.org/abs/2004.07213},
	abstract = {With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.},
	author = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and Maharaj, Tegan and Koh, Pang Wei and Hooker, Sara and Leung, Jade and Trask, Andrew and Bluemke, Emma and Lebensold, Jonathan and O'Keefe, Cullen and Koren, Mark and Ryffel, Théo and Rubinovitz, J. B. and Besiroglu, Tamay and Carugati, Federica and Clark, Jack and Eckersley, Peter and de Haas, Sarah and Johnson, Maritza and Laurie, Ben and Ingerman, Alex and Krawczuk, Igor and Askell, Amanda and Cammarota, Rosario and Lohn, Andrew and Krueger, David and Stix, Charlotte and Henderson, Peter and Graham, Logan and Prunkl, Carina and Martin, Bianca and Seger, Elizabeth and Zilberman, Noa and hÉigeartaigh, Seán Ó and Kroeger, Frens and Sastry, Girish and Kagan, Rebecca and Weller, Adrian and Tse, Brian and Barnes, Elizabeth and Dafoe, Allan and Scharre, Paul and Herbert-Voss, Ariel and Rasser, Martijn and Sodhani, Shagun and Flynn, Carrick and Gilbert, Thomas Krendl and Dyer, Lisa and Khan, Saif and Bengio, Yoshua and Anderljung, Markus},
	year = {2020},
	keywords = {Computer Science - Computers and Society},
}

@inproceedings{Baltes2018SOTorrent,
	title = {{SOTorrent}: {Reconstructing} and {Analyzing} the {Evolution} of {Stack} {Overflow} {Posts}},
	abstract = {Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-form text on a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, or text surrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text or code blocks. It connects SO posts to other platforms by aggregating URLs from text blocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history of text and code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surrounding text. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Baltes, Sebastian},
	year = {2018},
}

@article{Injadat2021MLtowardsIntelligentSystems,
	title = {Machine learning towards intelligent systems: applications, challenges, and opportunities},
	abstract = {The emergence and continued reliance on the Internet and related technologies has resulted in the generation of large amounts of data that can be made available for analyses. However, humans do not possess the cognitive capabilities to understand such large amounts of data. Machine learning (ML) provides a mechanism for humans to process large amounts of data, gain insights about the behavior of the data, and make more informed decision based on the resulting analysis. ML has applications in various fields. This review focuses on some of the fields and applications such as education, healthcare, network security, banking and finance, and social media. Within these fields, there are multiple unique challenges that exist. However, ML can provide solutions to these challenges, as well as create further research opportunities. Accordingly, this work surveys some of the challenges facing the aforementioned fields and presents some of the previous literature works that tackled them. Moreover, it suggests several research opportunities that benefit from the use of ML to address these challenges.},
	journal = {Artificial Intelligence Review},
	author = {Injadat, MohammadNoor and Moubayed, Abdallah and Nassif, Ali Bou and Shami, Abdallah},
	year = {2021},
}

@misc{Neptune2021MLModelRegistry,
	title = {{ML} {Model} {Registry}: {What} {It} {Is}, {Why} {It} {Matters}, {How} to {Implement} {It}},
	shorttitle = {{ML} {Model} {Registry}},
	url = {https://neptune.ai/blog/ml-model-registry},
	abstract = {Why do you have to know more about model registry? If you were once the only data scientist on your team you can probably relate to this: you start working on a machine learning project and perform a series of experiments that produce various models (and artifacts) that you “track” through non-standard naming conventions. Since […]},
	journal = {neptune.ai},
	year = {2021},
}

@misc{kaplan_survey_2021,
	title = {A {Survey} on {Common} {Threats} in npm and {PyPi} {Registries}},
	url = {http://arxiv.org/abs/2108.09576},
	abstract = {Software engineers regularly use JavaScript and Python for both front-end and back-end automation tasks. On top of JavaScript and Python, there are several frameworks to facilitate automation tasks further. Some of these frameworks are Node Manager Package (npm) and Python Package Index (PyPi), which are open source (OS) package libraries. The public registries npm and PyPi use to host packages allow any user with a verified email to publish code. The lack of a comprehensive scanning tool when publishing to the registry creates security concerns. Users can report malicious code on the registry; however, attackers can still cause damage until they remove their tool from the platform. Furthermore, several packages depend on each other, making them more vulnerable to a bad package in the dependency tree. The heavy code reuse creates security artifacts developers have to consider, such as the package reach. This project will illustrate a high-level overview of common risks associated with OS registries and the package dependency structure. There are several attack types, such as typosquatting and combosquatting, in the OS package registries. Outdated packages pose a security risk, and we will examine the extent of technical lag present in the npm environment. In this paper, our main contribution consists of a survey of common threats in OS registries. Afterward, we will offer countermeasures to mitigate the risks presented. These remedies will heavily focus on the applications of Machine Learning (ML) to detect suspicious activities. To the best of our knowledge, the ML-focused countermeasures are the first proposed possible solutions to the security problems listed. In addition, this project is the first survey of threats in npm and PyPi, although several studies focus on a subset of threats.},
	urldate = {2022-08-26},
	publisher = {arXiv},
	author = {Kaplan, Berkay and Qian, Jingyu},
	month = aug,
	year = {2021},
	note = {arXiv:2108.09576 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{Abdellatif2020SimplifyingSearchofNPMPackages,
	title = {Simplifying the {Search} of npm {Packages}},
	abstract = {Objective: The goal of this paper is to empirically improve the eﬃciency of npms by simplifying the used components without impacting the current npms package ranks.
Method: We use feature selection methods with the aim of simplifying npms’ equations. We remove the features that do not have a signiﬁcant eﬀect on the package’s rank. Then, we study the impact of the simpliﬁed npms on the packages’ rank, the amount of resources saved compared to the original npms, and the performance of the simpliﬁed npms as npm evolves.
Results: Our ﬁndings indicate that (1) 31\% of the unique variables of npms’ equation can be removed without breaking the original packages’ ranks; (2) The simpliﬁed npms, on average, preserves the overlapping of the packages by 98\% and the ranking of those packages by 97\%; (3) Using the simpliﬁed npms saves 10\% of packages scoring time and more than 1.47 million network requests on each scoring run; (4) As the npm evolve through a period of 12 months, the simpliﬁed-npms was able to achieve results similar to the original npms.
Conclusion: Our results show that the simpliﬁed npms preserves the original ranks of packages and is more eﬃcient than the original npms. We believe that using our approach, helps the npms community speed up the scoring process by saving computational resources and time.},
	journal = {Information and Software Technology},
	author = {Abdellatif, Ahmad and Zeng, Yi and Elshafei, Mohamed and Shihab, Emad and Shang, Weiyi},
	year = {2020},
}

@misc{noauthor_trustworthy_nodate,
	title = {Trustworthy {PTNNs}},
	url = {https://www.overleaf.com/project/629661e7c60a7b25a4c76547},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2022-08-19},
}

@book{Guest2011ThematicAnalysi,
	title = {Applied {Thematic} {Analysis}},
	publisher = {SAGE Publications},
	author = {Guest, Greg and MacQueen, Kathleen and Namey, Emily},
	year = {2011},
}

@article{Decan2019DependencyNetworkEvolution,
	title = {An empirical comparison of dependency network evolution in seven software packaging ecosystems},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Decan, Alexandre and Mens, Tom and Grosjean, Philippe},
	year = {2019},
}

@article{Mujahid2022CharacteristicsofHighlySlectedPackages,
	title = {What are the {Characteristics} of {Highly}-{Selected} {Packages}? {A} {Case} {Study} on the {NPM} {Ecosystem}},
	abstract = {With the popularity of software ecosystems, the number of open source components (known as packages) has grown rapidly. Identifying high-quality and well-maintained packages from a large pool of packages to depend on is a basic and important problem, as it is beneﬁcial for various applications, such as package recommendation and package search. However, no systematic and comprehensive work focuses on addressing this problem except in online discussions or informal literature and interviews. To ﬁll this gap, in this paper, we conducted a mixed qualitative and quantitative analysis to understand how developers identify and select relevant open source packages. In particular, we started by surveying 118 JavaScript developers from the npm ecosystem to qualitatively understand the factors that make a package to be highly-selected within the npm ecosystem. The survey results showed that JavaScript developers believe that highly-selected packages are well-documented, receive a high number of stars on GitHub, have a large number of downloads, and do not suﬀer from vulnerabilities. Then, we conducted an experiment to quantitatively validate the developers’ perception of the factors that make a highly-selected package. In this analysis, we collected and mined historical data from 2,527 packages divided into highly-selected and not highly-selected packages. For each package in the dataset, we collected quantitative data to present the factors studied in the developers’ survey. Next, we used regression analysis to quantitatively investigate which of the studied factors are the most important. Our regression analysis complements our survey results about highly-selected packages. In particular, the results showed that highly-selected packages tend to be correlated by the number of downloads, stars, and how large the package’s readme ﬁle is.},
	journal = {SSRN Electronic Journal},
	author = {Mujahid, Suhaib and Abdalkareem, Rabe and Shihab, Emad},
	year = {2022},
}

@inproceedings{Rafiuzzaman2021EfficientConfigurationofPipelinedApplicationsontheEdge,
	title = {π-{Configurator}: {Enabling} {Efficient} {Configuration} of {Pipelined} {Applications} on the {Edge}},
	abstract = {Modern edge computing applications involve a computational pipeline of multiple stages. Each stage typically involves many configuration options that affect application-level quality of service. Identifying an optimal configuration is challenging but important when the applications run under resource constraints. The main challenge is that when pipelines have many stages and each stage has many settings, the overall configuration state space is exceedingly large. We propose π-Configurator, a system for sampling application-level quality of service (QoS) metrics, constructing an approximation of the configuration state space, and finally identifying an optimal configuration for the application. We demonstrate the accuracy and effectiveness of π - Configurator with four multi-stage data processing applications on resource-limited edge computing platforms. π-Configurator incurs low approximation error, and is one to two orders of magnitude faster than complete sampling approaches. The configurations identified by π-Configurator outperform those identified by existing local adaptation approaches by 99\%.},
	booktitle = {International {Conference} on {Internet}-of-{Things} {Design} and {Implementation} ({IoTDI})},
	author = {Rafiuzzaman, Mohammad and Gopalakrishnan, Sathish and Pattabiraman, Karthik},
	year = {2022},
	keywords = {Approximation error, Data processing, Edge computing, IoT, Measurement, Pipelines, Quality of service, complex state-space, general framework, global configuration, pipelined applications, state space approximation, symbolic state space, systematic sampling},
}

@inproceedings{xin_production_2021,
	title = {Production {Machine} {Learning} {Pipelines}: {Empirical} {Analysis} and {Optimization} {Opportunities}},
	shorttitle = {Production {Machine} {Learning} {Pipelines}},
	abstract = {Machine learning (ML) is now commonplace, powering data-driven applications in various organizations. Unlike the traditional perception of ML in research, ML production pipelines are complex, with many interlocking analytical components beyond training, whose sub-parts are often run multiple times on overlapping subsets of data. However, there is a lack of quantitative evidence regarding the lifespan, architecture, frequency, and complexity of these pipelines to understand how data management research can be used to make them more efficient, effective, robust, and reproducible. To that end, we analyze the provenance graphs of 3000 production ML pipelines at Google, comprising over 450,000 models trained, spanning a period of over four months, in an effort to understand the complexity and challenges underlying production ML. Our analysis reveals the characteristics, components, and topologies of typical industrystrength ML pipelines at various granularities. Along the way, we introduce a specialized data model for representing and reasoning about repeatedly run components in these ML pipelines, which we call model graphlets. We identify several rich opportunities for optimization, leveraging traditional data management ideas. We show how targeting even one of these opportunities, i.e., identifying and pruning wasted computation that does not translate to model deployment, can reduce wasted computation cost by 50\% without compromising the model deployment cadence.},
	booktitle = {International {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Xin, Doris and Miao, Hui and Parameswaran, Aditya and Polyzotis, Neoklis},
	year = {2021},
}

@inproceedings{WU2017,
	title = {Domain-specific modelware: to make the machine learning model reusable and reproducible},
	booktitle = {International {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Wu, Xizhu and Zhou, Zhihua},
	year = {2017},
	keywords = {machine learning, model, model specification, modelware},
}

@inproceedings{Nahar2022CollaborationChallengesinBuildingMLSystems,
	title = {Collaboration {Challenges} in {Building} {ML}-{Enabled} {Systems}: {Communication}, {Documentation}, {Engineering}, and {Process}},
	abstract = {The introduction of machine learning (ML) components in software projects has created the need for software engineers to collaborate with data scientists and other specialists. While collaboration can always be challenging, ML introduces additional challenges with its exploratory model development process, additional skills and knowledge needed, difficulties testing ML systems, need for continuous evolution and monitoring, and non-traditional quality requirements such as fairness and explainability. Through interviews with 45 practitioners from 28 organizations, we identified key collaboration challenges that teams face when building and deploying ML systems into production. We report on common collaboration points in the development of production ML systems for requirements, data, and integration, as well as corresponding team patterns and challenges. We find that most of these challenges center around communication, documentation, engineering, and process and collect recommendations to address these challenges.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Nahar, Nadia and Zhou, Shurui and Lewis, Grace and Kästner, Christian},
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{Nikitin2022AutomatedEvolutionaryApproach4theDesignofCompositeMLPipelines,
	title = {Automated evolutionary approach for the design of composite machine learning pipelines},
	abstract = {The effectiveness of the machine learning methods for real-world tasks depends on the proper structure of the modeling pipeline. The proposed approach is aimed to automate the design of composite machine learning pipelines, which is equivalent to computation workflows that consist of models and data operations. The approach combines key ideas of both automated machine learning and workflow management systems. It designs the pipelines with a customizable graph-based structure, analyzes the obtained results, and reproduces them. The evolutionary approach is used for the flexible identification of pipeline structure. The additional algorithms for sensitivity analysis, atomization, and hyperparameter tuning are implemented to improve the effectiveness of the approach. Also, the software implementation on this approach is presented as an open-source framework. The set of experiments is conducted for the different datasets and tasks (classification, regression, time series forecasting). The obtained results confirm the correctness and effectiveness of the proposed approach in the comparison with the state-of-the-art competitors and baseline solutions.},
	journal = {Future Generation Computer Systems},
	author = {Nikitin, Nikolay O. and Vychuzhanin, Pavel and Sarafanov, Mikhail and Polonskaia, Iana S. and Revin, Ilia and Barabanova, Irina V. and Maximov, Gleb and Kalyuzhnaya, Anna V. and Boukhanovsky, Alexander},
	year = {2022},
}

@inproceedings{Chan2021ResilienceofNNEnsemblesAgainstFaultyTrainingData,
	title = {Understanding the {Resilience} of {Neural} {Network} {Ensembles} against {Faulty} {Training} {Data}},
	abstract = {Machine learning is becoming more prevalent in safety-critical systems like autonomous vehicles and medical imaging. Faulty training data, where data is either misla-belled, missing, or duplicated, can increase the chance of misclassification, resulting in serious consequences. In this paper, we evaluate the resilience of ML ensembles against faulty training data, in order to understand how to build better ensembles. To support our evaluation, we develop a fault injection framework to systematically mutate training data, and introduce two diversity metrics that capture the distribution and entropy of predicted labels. Our experiments find that ensemble learning is more resilient than any individual model and that high accuracy neural networks are not necessarily more resilient to faulty training data. Further, we find that simple majority voting suffices in most cases for resilience in ML ensembles. Finally, we observe diminishing returns for resilience as we increase the number of models in an ensemble. These findings can help machine learning developers build ensembles that are both more resilient and more efficient.},
	booktitle = {International {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Chan, Abraham and Narayanan, Niranjhana and Gujarati, Arpan and Pattabiraman, Karthik and Gopalakrishnan, Sathish},
	year = {2021},
	keywords = {Data models, Entropy, Error resilience, Machine learning, Measurement, Neural networks, Software quality, Training, Training data},
}

@inproceedings{Pan2022DecomposingCNN,
	address = {Pittsburgh Pennsylvania},
	title = {Decomposing convolutional neural networks into reusable and replaceable modules},
	abstract = {Training from scratch is the most common way to build a Convolutional Neural Network (CNN) based model. What if we can build new CNN models by reusing parts from previously build CNN models? What if we can improve a CNN model by replacing (possibly faulty) parts with other parts? In both cases, instead of training, can we identify the part responsible for each output class (module) in the model(s) and reuse or replace only the desired output classes to build a model? Prior work has proposed decomposing dense-based networks into modules (one for each output class) to enable reusability and replaceability in various scenarios. However, this work is limited to the dense layers and based on the one-toone relationship between the nodes in consecutive layers. Due to the shared architecture in the CNN model, prior work cannot be adapted directly. In this paper, we propose to decompose a CNN model used for image classification problems into modules for each output class. These modules can further be reused or replaced to build a new model. We have evaluated our approach with CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet models and found that enabling decomposition comes with a small cost (1.77\% and 0.85\% for top-1 and top-5 accuracy, respectively). Also, building a model by reusing or replacing modules can be done with a 2.3\% and 0.5\% average loss of accuracy. Furthermore, reusing and replacing these modules reduces CO2e emission by ∼37 times compared to training the model from scratch.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {ACM},
	author = {Pan, Rangeet and Rajan, Hridesh},
	year = {2022},
}

@inproceedings{chakraborty_bias_2021,
	title = {Bias in {Machine} {Learning} {Software}: {Why}? {How}? {What} to do?},
	shorttitle = {Bias in {Machine} {Learning} {Software}},
	url = {http://arxiv.org/abs/2105.12195},
	doi = {10.1145/3468264.3468537},
	abstract = {Increasingly, software is making autonomous decisions in case of criminal sentencing, approving credit cards, hiring employees, and so on. Some of these decisions show bias and adversely affect certain social groups (e.g. those defined by sex, race, age, marital status). Many prior works on bias mitigation take the following form: change the data or learners in multiple ways, then see if any of that improves fairness. Perhaps a better approach is to postulate root causes of bias and then applying some resolution strategy.},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {Chakraborty, Joymallya and Majumder, Suvodeep and Menzies, Tim},
	month = aug,
	year = {2021},
	note = {arXiv:2105.12195 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	pages = {429--440},
}

@misc{lin_truthfulqa_2022,
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {http://arxiv.org/abs/2109.07958},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2022-08-11},
	publisher = {arXiv},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	month = may,
	year = {2022},
	note = {arXiv:2109.07958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{lin_truthfulqa_2022-1,
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {http://arxiv.org/abs/2109.07958},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2022-08-11},
	publisher = {arXiv},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	month = may,
	year = {2022},
	note = {arXiv:2109.07958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{Vishnu2021TechReport,
	title = {An {Experience} {Report} on {Machine} {Learning} {Reproducibility}: {Guidance} for {Practitioners} and {TensorFlow} {Model} {Garden} {Contributors}},
	url = {http://arxiv.org/abs/2107.00821},
	abstract = {Machine learning techniques are becoming a fundamental tool for scientific and engineering progress. These techniques are applied in contexts as diverse as astronomy and spam filtering. However, correctly applying these techniques requires careful engineering. Much attention has been paid to the technical potential; relatively little attention has been paid to the software engineering process required to bring research-based machine learning techniques into practical utility. Technology companies have supported the engineering community through machine learning frameworks such as TensorFLow and PyTorch, but the details of how to engineer complex machine learning models in these frameworks have remained hidden. To promote best practices within the engineering community, academic institutions and Google have partnered to launch a Special Interest Group on Machine Learning Models (SIGMODELS) whose goal is to develop exemplary implementations of prominent machine learning models in community locations such as the TensorFlow Model Garden (TFMG). The purpose of this report is to define a process for reproducing a state-of-the-art machine learning model at a level of quality suitable for inclusion in the TFMG. We define the engineering process and elaborate on each step, from paper analysis to model release. We report on our experiences implementing the YOLO model family with a team of 26 student researchers, share the tools we developed, and describe the lessons we learned along the way.},
	author = {Banna, Vishnu and Chinnakotla, Akhil and Yan, Zhengxin and Vegesana, Anirudh and Vivek, Naveen and Krishnappa, Kruthi and Jiang, Wenxin and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.},
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{Hosny2019ModelHubPaper,
	title = {{ModelHub}.{AI}: {Dissemination} {Platform} for {Deep} {Learning} {Models}},
	url = {http://arxiv.org/abs/1911.13218},
	abstract = {Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present ModelHub.AI (www.modelhub.ai), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on ModelHub.AI and allows both researchers and developers to utilize models out-of-the-box. ModelHub.AI is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.},
	author = {Hosny, Ahmed and Schwier, Michael and Berger, Christoph and Örnek, Evin P. and Turan, Mehmet and Tran, Phi V. and Weninger, Leon and Isensee, Fabian and Maier-Hein, Klaus H. and McKinley, Richard and Lu, Michael T. and Hoffmann, Udo and Menze, Bjoern and Bakas, Spyridon and Fedorov, Andriy and Aerts, Hugo JWL},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{ModelZooWeb,
	title = {Model {Zoo} - {Deep} learning code and pretrained models},
	url = {https://modelzoo.co/},
	urldate = {2022-01-18},
	author = {Jing, Yu Koh},
	year = {2021},
}

@article{idowu_asset_2022,
	title = {Asset {Management} in {Machine} {Learning}: {State}-of-research and {State}-of-practice},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Asset {Management} in {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3543847},
	doi = {10.1145/3543847},
	abstract = {Machine learning components are essential for today’s software systems, causing a need to adapt traditional software engineering practices when developing machine-learning-based systems. This need is pronounced due to many development-related challenges of machine learning components such as asset, experiment, and dependency management. Recently, many asset management tools addressing these challenges have become available. It is essential to understand the support such tools offer to facilitate research and practice on building new management tools with native supports for machine learning and software engineering assets.
            This article positions machine learning asset management as a discipline that provides improved methods and tools for performing operations on machine learning assets. We present a feature-based survey of 18 state-of-practice and 12 state-of-research tools supporting machine-learning asset management. We overview their features for managing the types of assets used in machine learning experiments. Most state-of-research tools focus on tracking, exploring, and retrieving assets to address development concerns such as reproducibility, while the state-of-practice tools also offer collaboration and workflow-execution-related operations. In addition, assets are primarily tracked intrusively from the source code through APIs and managed via web dashboards or command-line interfaces. We identify asynchronous collaboration and asset reusability as directions for new tools and techniques.},
	language = {en},
	urldate = {2022-08-04},
	journal = {ACM Computing Surveys},
	author = {Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
	month = jun,
	year = {2022},
	pages = {3543847},
}

@inproceedings{idowu_asset_2021,
	title = {Asset {Management} in {Machine} {Learning}: {A} {Survey}},
	shorttitle = {Asset {Management} in {Machine} {Learning}},
	doi = {10.1109/ICSE-SEIP52600.2021.00014},
	abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
	year = {2021},
	keywords = {Asset management, Control systems, Machine learning, Monitoring, Software engineering, Software systems, Tools, asset management, experiment management, machine learning, se4ml},
	pages = {51--60},
}

@misc{kapoor_leakage_2022,
	title = {Leakage and the {Reproducibility} {Crisis} in {ML}-based {Science}},
	url = {http://arxiv.org/abs/2207.07048},
	abstract = {The use of machine learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. In this paper, we systematically investigate reproducibility issues in ML-based science. We show that data leakage is indeed a widespread problem and has led to severe reproducibility failures. Specifically, through a survey of literature in research communities that adopted ML methods, we find 17 fields where errors have been found, collectively affecting 329 papers and in some cases leading to wildly overoptimistic conclusions. Based on our survey, we present a fine-grained taxonomy of 8 types of leakage that range from textbook errors to open research problems. We argue for fundamental methodological changes to ML-based science so that cases of leakage can be caught before publication. To that end, we propose model info sheets for reporting scientific claims based on ML models that would address all types of leakage identified in our survey. To investigate the impact of reproducibility errors and the efficacy of model info sheets, we undertake a reproducibility study in a field where complex ML models are believed to vastly outperform older statistical models such as Logistic Regression (LR): civil war prediction. We find that all papers claiming the superior performance of complex ML models compared to LR models fail to reproduce due to data leakage, and complex ML models don't perform substantively better than decades-old LR models. While none of these errors could have been caught by reading the papers, model info sheets would enable the detection of leakage in each case.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Kapoor, Sayash and Narayanan, Arvind},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07048 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Methodology},
}

@misc{kapoor_leakage_2022-1,
	title = {Leakage and the {Reproducibility} {Crisis} in {ML}-based {Science}},
	url = {http://arxiv.org/abs/2207.07048},
	abstract = {The use of machine learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. In this paper, we systematically investigate reproducibility issues in ML-based science. We show that data leakage is indeed a widespread problem and has led to severe reproducibility failures. Specifically, through a survey of literature in research communities that adopted ML methods, we find 17 fields where errors have been found, collectively affecting 329 papers and in some cases leading to wildly overoptimistic conclusions. Based on our survey, we present a fine-grained taxonomy of 8 types of leakage that range from textbook errors to open research problems. We argue for fundamental methodological changes to ML-based science so that cases of leakage can be caught before publication. To that end, we propose model info sheets for reporting scientific claims based on ML models that would address all types of leakage identified in our survey. To investigate the impact of reproducibility errors and the efficacy of model info sheets, we undertake a reproducibility study in a field where complex ML models are believed to vastly outperform older statistical models such as Logistic Regression (LR): civil war prediction. We find that all papers claiming the superior performance of complex ML models compared to LR models fail to reproduce due to data leakage, and complex ML models don't perform substantively better than decades-old LR models. While none of these errors could have been caught by reading the papers, model info sheets would enable the detection of leakage in each case.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Kapoor, Sayash and Narayanan, Arvind},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07048 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Methodology},
}

@misc{Dang2021AccuracyandLoss,
	title = {Accuracy and {Loss}: {Things} to {Know} about {The} {Top} 1 and {Top} 5 {Accuracy}},
	url = {https://towardsdatascience.com/accuracy-and-loss-things-to-know-about-the-top-1-and-top-5-accuracy-1d6beb8f6df3},
	abstract = {Measure the performance of our model},
	journal = {Medium},
	author = {Dang, Anh T.},
	year = {2021},
}

@article{sanyal_towards_nodate,
	title = {Towards {Data}-{Free} {Model} {Stealing} in a {Hard} {Label} {Setting}},
	abstract = {Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework1 that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim’s gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-ofthe-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.},
	language = {en},
	author = {Sanyal, Sunandini and Addepalli, Sravanti and Babu, R Venkatesh},
	pages = {10},
}

@misc{wojcik_hard_2022,
	title = {Hard hat wearing detection based on head keypoint localization},
	url = {http://arxiv.org/abs/2106.10944},
	abstract = {In recent years, a lot of attention is paid to deep learning methods in the context of vision-based construction site safety systems, especially regarding personal protective equipment. However, despite all this attention, there is still no reliable way to establish the relationship between workers and their hard hats. To answer this problem a combination of deep learning, object detection and head keypoint localization, with simple rule-based reasoning is proposed in this article. In tests, this solution surpassed the previous methods based on the relative bounding box position of different instances, as well as direct detection of hard hat wearers and non-wearers. The results show that the conjunction of novel deep learning methods with humanly-interpretable rule-based systems can result in a solution that is both reliable and can successfully mimic manual, on-site supervision. This work is the next step in the development of fully autonomous construction site safety systems and shows that there is still room for improvement in this area.},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Wójcik, Bartosz and Żarski, Mateusz and Książek, Kamil and Miszczak, Jarosław Adam and Skibniewski, Mirosław Jan},
	month = jun,
	year = {2022},
	note = {arXiv:2106.10944 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{hou_large-scale_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Large-scale security measurements on the android firmware ecosystem},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510072},
	doi = {10.1145/3510003.3510072},
	abstract = {Android is the most popular smartphone platform with over 85\% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous work focuses on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem security and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale comprehensive measurement of the Android firmware ecosystem security. Our study is based on 6,261 firmware images from 153 vendors and 602 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabilities, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, AndScanner, to complete ROM crawling, ROM parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, several interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android images, say 24.2\% and 6.1\% of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In addition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities via 38 newfound vulnerabilities by our framework, 32 of which have been assigned CVE/CNVD numbers. This study provides much new knowledge of the Android firmware ecosystem with deep understanding of software engineering security practices.},
	language = {en},
	urldate = {2022-07-25},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Hou, Qinsheng and Diao, Wenrui and Wang, Yanhao and Liu, Xiaofeng and Liu, Song and Ying, Lingyun and Guo, Shanqing and Li, Yuanzhi and Nie, Meining and Duan, Haixin},
	month = may,
	year = {2022},
	pages = {1257--1268},
}

@inproceedings{Zeager2017AdversarialLearninginCreditCardFraudDetection,
	title = {Adversarial learning in credit card fraud detection},
	abstract = {Credit card fraud is an expensive problem for many financial institutions, costing billions of dollars to companies annually. Many adversaries still evade fraud detection systems because these systems often do not include information about the adversary's knowledge of the fraud detection mechanism. This project aims to include information about the “fraudster's” motivations and knowledge base into an adaptive fraud detection system. In this project, we use a game theoretical adversarial learning approach in order to model the fraudster's best strategy and pre-emptively adapt the fraud detection system to better classify these future fraudulent transactions. Using a logistic regression classifier as the fraud detection mechanism, we initially identify the best strategy for the adversary based on the number of fraudulent transactions that go undetected, and assume that the adversary uses this strategy for future transactions in order to improve our classifier. Prior research has used game theoretic models for adversarial learning in the domains of credit card fraud and email spam, but this project adds to the literature by extending these frameworks to a practical, real-world data set. Test results show that our adversarial framework produces an increasing AUC score on validation sets over several iterations in comparison to the static model usually employed by credit card companies.},
	booktitle = {Systems and {Information} {Engineering} {Design} {Symposium} ({SIEDS})},
	author = {Zeager, Mary Frances and Sridhar, Aksheetha and Fogal, Nathan and Adams, Stephen and Brown, Donald E. and Beling, Peter A.},
	year = {2017},
	keywords = {Adaptation models, Classification algorithms, Credit cards, Game theory, Games, Gaussian mixture model, Hidden Markov models, Logistics, Oversampling, Prediction algorithms, Synthetic data},
}

@inproceedings{zeager_adversarial_2017,
	title = {Adversarial learning in credit card fraud detection},
	doi = {10.1109/SIEDS.2017.7937699},
	abstract = {Credit card fraud is an expensive problem for many financial institutions, costing billions of dollars to companies annually. Many adversaries still evade fraud detection systems because these systems often do not include information about the adversary's knowledge of the fraud detection mechanism. This project aims to include information about the “fraudster's” motivations and knowledge base into an adaptive fraud detection system. In this project, we use a game theoretical adversarial learning approach in order to model the fraudster's best strategy and pre-emptively adapt the fraud detection system to better classify these future fraudulent transactions. Using a logistic regression classifier as the fraud detection mechanism, we initially identify the best strategy for the adversary based on the number of fraudulent transactions that go undetected, and assume that the adversary uses this strategy for future transactions in order to improve our classifier. Prior research has used game theoretic models for adversarial learning in the domains of credit card fraud and email spam, but this project adds to the literature by extending these frameworks to a practical, real-world data set. Test results show that our adversarial framework produces an increasing AUC score on validation sets over several iterations in comparison to the static model usually employed by credit card companies.},
	booktitle = {2017 {Systems} and {Information} {Engineering} {Design} {Symposium} ({SIEDS})},
	author = {Zeager, Mary Frances and Sridhar, Aksheetha and Fogal, Nathan and Adams, Stephen and Brown, Donald E. and Beling, Peter A.},
	month = apr,
	year = {2017},
	keywords = {Adaptation models, Classification algorithms, Credit cards, Game theory, Games, Gaussian mixture model, Hidden Markov models, Logistics, Oversampling, Prediction algorithms, Synthetic data},
	pages = {112--116},
}

@inproceedings{tan_exploratory_2022,
	address = {Pittsburgh Pennsylvania},
	title = {An exploratory study of deep learning supply chain},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510199},
	doi = {10.1145/3510003.3510199},
	abstract = {Deep learning becomes the driving force behind many contemporary technologies and has been successfully applied in many fields. Through software dependencies, a multi-layer supply chain (SC) with a deep learning framework as the core and substantial downstream projects as the periphery has gradually formed and is constantly developing. However, basic knowledge about the structure and characteristics of the SC is lacking, which hinders effective support for its sustainable development. Previous studies on software SC usually focus on the packages in different registries without paying attention to the SCs derived from a single project. We present an empirical study on two deep learning SCs: TensorFlow and PyTorch SCs. By constructing and analyzing their SCs, we aim to understand their structure, application domains, and evolutionary factors. We find that both SCs exhibit a short and sparse hierarchy structure. Overall, the relative growth of new projects increases month by month. Projects have a tendency to attract downstream projects shortly after the release of their packages, later the growth becomes faster and tends to stabilize. We propose three criteria to identify vulnerabilities and identify 51 types of packages and 26 types of projects involved in the two SCs. A comparison reveals their similarities and differences, e.g., TensorFlow SC provides a wealth of packages in experiment result analysis, while PyTorch SC contains more specific framework packages. By fitting the GAM model, we find that the number of dependent packages is significantly negatively associated with the number of downstream projects, but the relationship with the number of authors is nonlinear. Our findings ∗Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.},
	language = {en},
	urldate = {2022-07-22},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Tan, Xin and Gao, Kai and Zhou, Minghui and Zhang, Li},
	month = may,
	year = {2022},
	pages = {86--98},
}

@misc{Khalel2018PixelwiseObjectLabelingusingStackedUNets,
	title = {Automatic {Pixelwise} {Object} {Labeling} for {Aerial} {Imagery} {Using} {Stacked} {U}-{Nets}},
	url = {http://arxiv.org/abs/1803.04953},
	abstract = {Automation of objects labeling in aerial imagery is a computer vision task with numerous practical applications. Fields like energy exploration require an automated method to process a continuous stream of imagery on a daily basis. In this paper we propose a pipeline to tackle this problem using a stack of convolutional neural networks (U-Net architecture) arranged end-to-end. Each network works as post-processor to the previous one. Our model outperforms current state-of-the-art on two different datasets: Inria Aerial Image Labeling dataset and Massachusetts Buildings dataset each with different characteristics such as spatial resolution, object shapes and scales. Moreover, we experimentally validate computation time savings by processing sub-sampled images and later upsampling pixelwise labeling. These savings come at a negligible degradation in segmentation quality. Though the conducted experiments in this paper cover only aerial imagery, the technique presented is general and can handle other types of images.},
	publisher = {arXiv},
	author = {Khalel, Andrew and El-Saban, Motaz},
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{HuggingFacePaper2020,
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	booktitle = {Conference on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	year = {2020},
}

@misc{NeptuneAI2022MLModelRegistry,
	title = {{ML} {Model} {Registry}: {What} {It} {Is}, {Why} {It} {Matters}, {How} to {Implement} {It} - neptune.ai},
	url = {https://neptune.ai/blog/ml-model-registry},
	urldate = {2022-06-10},
	author = {Oladele, Stephen},
	year = {2022},
}

@article{Shakhatreh2019UAVCivilApplicationsandChallenges,
	title = {Unmanned {Aerial} {Vehicles} ({UAVs}): {A} {Survey} on {Civil} {Applications} and {Key} {Research} {Challenges}},
	abstract = {The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than \$45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.},
	journal = {IEEE Access},
	author = {Shakhatreh, Hazim and Sawalmeh, Ahmad H. and Al-Fuqaha, Ala and Dou, Zuochao and Almaita, Eyad and Khalil, Issa and Othman, Noor Shamsiah and Khreishah, Abdallah and Guizani, Mohsen},
	year = {2019},
	keywords = {Civil infrastructure inspection, Communication system security, Market research, Security, Surveillance, UAVs, Unmanned aerial vehicles, Wireless communication, Wireless sensor networks, delivery of goods, precision agriculture, real-time monitoring, remote sensing, search and rescue, security and surveillance, wireless coverage},
}

@misc{ClocTool,
	title = {cloc},
	copyright = {GPL-2.0},
	url = {https://github.com/AlDanial/cloc},
	abstract = {cloc counts blank lines, comment lines, and physical lines of source code in many programming languages.},
	author = {AlDanial},
	year = {2022},
	keywords = {cloc, count-lines, programming-language},
}

@article{Runeson2009Guidelines4CaseStudyinSE,
	title = {Guidelines for conducting and reporting case study research in software engineering},
	abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors’ own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Runeson, Per and Höst, Martin},
	year = {2009},
}

@inproceedings{Perry2004CaseStudy4SE,
	title = {Case studies for software engineers},
	abstract = {The topic of this paper was the correct use and interpretation of case studies as an empirical research method. Using an equal blend of lecture and discussion, it gave a foundation for conducting, reviewing, and reading case studies. There were lessons for software engineers as researchers who conduct and report case studies, reviewers who evaluate papers, and practitioners who are attempting to apply results from papers. The main resource for the course was the book Case Study Research: Design and Methods by Robert K. Yin. This text was supplemented with positive and negative examples from the literature.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Perry, D.E. and Sim, S.E. and Easterbrook, S.M.},
	year = {2004},
	keywords = {Books, Computer aided software engineering, Data analysis, Design methodology, Grounding, History, Paper technology, Software engineering, Solids},
}

@misc{CaffeModelZoo,
	title = {Caffe {\textbar} {Model} {Zoo}},
	url = {https://caffe.berkeleyvision.org/model_zoo.html},
	urldate = {2022-07-13},
	author = {BAIR},
}

@misc{Neptune2022LimitsofDLLanguageModels,
	title = {Can {GPT}-3 or {BERT} {Ever} {Understand} {Language}?⁠—{The} {Limits} of {Deep} {Learning} {Language} {Models}},
	shorttitle = {Can {GPT}-3 or {BERT} {Ever} {Understand} {Language}?},
	url = {https://neptune.ai/blog/gpt-3-bert-limits-of-deep-learning-language-models},
	abstract = {It’s safe to assume a topic can be considered mainstream when it is the basis for an opinion piece in the Guardian. What is unusual is when that topic is a fairly niche area that involves applying Deep Learning techniques to develop natural language models. What is even more unusual is when one of those […]},
	urldate = {2022-07-13},
	journal = {neptune.ai},
	author = {Horan, Cathal},
	year = {2020},
}

@misc{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv:2104.14294 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_yolov7_2022,
	title = {{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors},
	shorttitle = {{YOLOv7}},
	url = {http://arxiv.org/abs/2207.02696},
	abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02696 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{pan_decomposing_2022,
	title = {Decomposing {Convolutional} {Neural} {Networks} into {Reusable} and {Replaceable} {Modules}},
	abstract = {Training from scratch is the most common way to build a Convolutional Neural Network (CNN) based model. What if we can build new CNN models by reusing parts from previously built CNN models? What if we can improve a CNN model by replacing (possibly faulty) parts with other parts? In both cases, instead of training, can we identify the part responsible for each output class (module) in the model(s) and reuse or replace only the desired output classes to build a model? Prior work has proposed decomposing dense-based networks into modules (one for each output class) to enable reusability and replace ability in various scenarios. However, this work is limited to the dense layers and is based on the one-to-one relationship between the nodes in consecutive layers. Due to the shared architecture in the CNN model, prior work cannot be adapted directly. In this paper, we propose to decompose a CNN model used for image classification problems into modules for each output class. These modules can further be reused or replaced to build a new model. We have evaluated our approach with CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet models and found that enabling decomposition comes with a small cost (1.77\% and 0.85\% for top-1 and top-5 accuracy, respectively). Also, building a model by reusing or replacing modules can be done with a 2.3\% and 0.5\% average loss of accuracy. Furthermore, reusing and replacing these modules reduces CO2e emission by {\textasciitilde}37 times compared to training the model from scratch.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Pan, Rangeet and Rajan, Hridesh},
	year = {2022},
	note = {ISSN: 1558-1225},
	keywords = {Adaptation models, Buildings, Computer architecture, Convolutional neural networks, Costs, Software engineering, Training, cnn, decomposition, deep learning, deep neural network, modularity},
	pages = {524--535},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-06-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{PackageFormatWiki,
	title = {Package format},
	url = {https://en.wikipedia.org/w/index.php?title=Package_format&oldid=1081782986},
	abstract = {A package format is a type of archive containing computer programs and additional metadata needed by package managers. While the archive file format itself may be unchanged, package formats bear additional metadata, such as a manifest file or certain directory layouts. Packages may contain either source code or executable files.
Packages may be converted from one type to another with software such as Alien.},
	language = {en},
	urldate = {2022-06-14},
	journal = {Wikipedia},
	year = {2022},
}

@incollection{kruchten_taxonomy_2019,
	address = {Cham},
	title = {A {Taxonomy} of {Software} {Engineering} {Challenges} for {Machine} {Learning} {Systems}: {An} {Empirical} {Investigation}},
	volume = {355},
	isbn = {978-3-030-19033-0 978-3-030-19034-7},
	shorttitle = {A {Taxonomy} of {Software} {Engineering} {Challenges} for {Machine} {Learning} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-030-19034-7_14},
	abstract = {Artiﬁcial intelligence enabled systems have been an inevitable part of everyday life. However, eﬃcient software engineering principles and processes need to be considered and extended when developing AI- enabled systems. The objective of this study is to identify and classify software engineering challenges that are faced by diﬀerent companies when developing software-intensive systems that incorporate machine learning components. Using case study approach, we explored the development of machine learning systems from six diﬀerent companies across various domains and identiﬁed main software engineering challenges. The challenges are mapped into a proposed taxonomy that depicts the evolution of use of ML components in software-intensive system in industrial settings. Our study provides insights to software engineering community and research to guide discussions and future research into applied machine learning.},
	language = {en},
	urldate = {2022-06-13},
	booktitle = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}},
	publisher = {Springer International Publishing},
	author = {Lwakatare, Lucy Ellen and Raj, Aiswarya and Bosch, Jan and Olsson, Helena Holmström and Crnkovic, Ivica},
	editor = {Kruchten, Philippe and Fraser, Steven and Coallier, François},
	year = {2019},
	doi = {10.1007/978-3-030-19034-7_14},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {227--243},
}

@article{muiruri_practices_2022,
	title = {Practices and {Infrastructures} for {Machine} {Learning} {Systems}: {An} {Interview} {Study} in {Finnish} {Organizations}},
	volume = {55},
	issn = {1558-0814},
	shorttitle = {Practices and {Infrastructures} for {Machine} {Learning} {Systems}},
	doi = {10.1109/MC.2022.3161161},
	abstract = {Using interviews, we investigated the practices and toolchains for machine learning (ML)-enabled systems from 16 organizations across various domains in Finland. We observed some well-established artificial intelligence engineering approaches, but practices and tools are still needed for the testing and monitoring of ML-enabled systems.},
	number = {6},
	journal = {Computer},
	author = {Muiruri, Dennis and Lwakatare, Lucy Ellen and Nurminen, Jukka K. and Mikkonen, Tommi},
	month = jun,
	year = {2022},
	note = {Conference Name: Computer},
	keywords = {Artificial intelligence, Business practices, Computational modeling, Data models, Interviews, Learning systems, Machine learning, Monitoring, Organizations, Software engineering},
	pages = {18--29},
}

@article{pfefferkorn_content-oblivious_2022,
	title = {Content-{Oblivious} {Trust} and {Safety} {Techniques}: {Results} from a {Survey} of {Online} {Service} {Providers}},
	volume = {1},
	copyright = {Copyright (c) 2022 Journal of Online Trust and Safety},
	issn = {2770-3142},
	shorttitle = {Content-{Oblivious} {Trust} and {Safety} {Techniques}},
	url = {https://tsjournal.org/index.php/jots/article/view/14},
	doi = {10.54501/jots.v1i2.14},
	abstract = {We present the results of a survey about the trust and safety techniques of a group of online service providers that collectively serve billions of users. We classify techniques that require the provider to be able to access the contents of users’ files and communications at will as content dependent, and content oblivious otherwise. We find that more providers use abuse-reporting features (which are content oblivious) than other abuse-detection techniques, but that participants’ abuse-reporting tools do not consistently cover the types of abuse that users may encounter. We also find that, despite strong consensus among participating providers that automated content scanning (which is content dependent) is the most useful means of detecting child sex abuse imagery, they do not consider it to be nearly as useful for other kinds of abuse. These results indicate that content-dependent techniques do not constitute a silver bullet to protect users against abuse. They also demonstrate that the impact of end-to-end encryption (which, controversially, impedes outside access to user content) on abuse detection may vary by abuse type. These findings have implications for policy debates over the regulation of online service providers’ anti-abuse obligations and their use of end-to-end encryption.},
	language = {en},
	number = {2},
	urldate = {2022-06-12},
	journal = {Journal of Online Trust and Safety},
	author = {Pfefferkorn, Riana},
	month = feb,
	year = {2022},
	note = {Number: 2},
	keywords = {trust and safety},
}

@article{Jeyanthi2020ResNetTL4HumanActionRecognitionusingLSTM,
	title = {Inception {ResNet} deep transfer learning model for human action recognition using {LSTM}},
	abstract = {In recent days, variety of approaches using deep learning features have been proposed for human action recognition due to the worth of deep neural networks. In this work, we put forward a new deep neural network architecture based on transfer learning [TL] for human action recognition. We illustrate how to progress the recognition of human activities with a video data set of small size using transfer learning. The model constructed is based on Inception ResNet convolutional neural networks (CNN) and long short term model (LSTM).We train the model by extracting feature vectors from Inception\_ResNet\_v2 and then the output feature vectors from CNN are applied onto the RNN for learning the action sequence. Then we attempt to classify the input videos according to the model trained. He accuracy score of the model was compared against VGG16, ResNet152 and Inception\_v3 models studied on. The results prove the LSTM architecture using Inception\_ ResNet\_v2 provide the best accuracy score of 92\%, 91\% on UCI 101 and HMDB 51 data sets.},
	journal = {Materials Today: Proceedings},
	author = {Jeyanthi Suresh, A. and Visumathi, J.},
	year = {2020},
}

@inproceedings{Ren2015FasterRCNN,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
}

@article{shakhatreh_unmanned_2019,
	title = {Unmanned {Aerial} {Vehicles} ({UAVs}): {A} {Survey} on {Civil} {Applications} and {Key} {Research} {Challenges}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Unmanned {Aerial} {Vehicles} ({UAVs})},
	doi = {10.1109/ACCESS.2019.2909530},
	abstract = {The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than \$45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.},
	journal = {IEEE Access},
	author = {Shakhatreh, Hazim and Sawalmeh, Ahmad H. and Al-Fuqaha, Ala and Dou, Zuochao and Almaita, Eyad and Khalil, Issa and Othman, Noor Shamsiah and Khreishah, Abdallah and Guizani, Mohsen},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Civil infrastructure inspection, Communication system security, Market research, Security, Surveillance, UAVs, Unmanned aerial vehicles, Wireless communication, Wireless sensor networks, delivery of goods, precision agriculture, real-time monitoring, remote sensing, search and rescue, security and surveillance, wireless coverage},
	pages = {48572--48634},
}

@inproceedings{ji_model-reuse_2018,
	address = {Toronto Canada},
	title = {Model-{Reuse} {Attacks} on {Deep} {Learning} {Systems}},
	isbn = {978-1-4503-5693-0},
	url = {https://dl.acm.org/doi/10.1145/3243734.3243757},
	doi = {10.1145/3243734.3243757},
	abstract = {Many of today’s machine learning (ML) systems are built by reusing an array of, often pre-trained, primitive models, each fulfilling distinct functionality (e.g., feature extraction). The increasing use of primitive models significantly simplifies and expedites the development cycles of ML systems. Yet, because most of such models are contributed and maintained by untrusted sources, their lack of standardization or regulation entails profound security implications, about which little is known thus far.},
	language = {en},
	urldate = {2022-06-10},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Ji, Yujie and Zhang, Xinyang and Ji, Shouling and Luo, Xiapu and Wang, Ting},
	month = oct,
	year = {2018},
	pages = {349--363},
}

@misc{Cui2020PTM4ChineseNLP,
	title = {Revisiting {Pre}-{Trained} {Models} for {Chinese} {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2004.13922},
	abstract = {Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. Resources available: https://github.com/ymcui/MacBERT},
	author = {Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Wang, Shijin and Hu, Guoping},
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@techreport{Chen2021bert2BERT,
	title = {{bert2BERT}: {Towards} {Reusable} {Pretrained} {Language} {Models}},
	url = {http://arxiv.org/abs/2110.07143},
	abstract = {In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model (e.g., BERT\_BASE) to a large model (e.g., BERT\_LARGE) through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving on Transformer-based language model, and further improve it by proposing advanced knowledge for large model's initialization. In addition, a two-stage pre-training method is proposed to further accelerate the training process. We did extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45\% and 47\% computational cost of pre-training BERT\_BASE and GPT\_BASE by reusing the models of almost their half sizes. The source code will be publicly available upon publication.},
	number = {arXiv:2110.07143},
	institution = {arXiv},
	author = {Chen, Cheng and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Qin, Yujia and Wang, Fengyu and Wang, Zhi and Chen, Xiao and Liu, Zhiyuan and Liu, Qun},
	year = {2021},
	keywords = {Computer Science - Computation and Language},
}

@book{thrun_learning_2012,
	title = {Learning to {Learn}},
	isbn = {978-1-4615-5529-2},
	abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications.  Learning to Learn is an exciting new research direction within machine learning. Similar to traditional machine-learning algorithms, the methods described in Learning to Learn induce general functions from experience. However, the book investigates algorithms that can change the way they generalize, i.e., practice the task of learning itself, and improve on it.  To illustrate the utility of learning to learn, it is worthwhile comparing machine learning with human learning. Humans encounter a continual stream of learning tasks. They do not just learn concepts or motor skills, they also learn bias, i.e., they learn how to generalize. As a result, humans are often able to generalize correctly from extremely few examples - often just a single example suffices to teach us a new thing.  A deeper understanding of computer programs that improve their ability to learn can have a large practical impact on the field of machine learning and beyond. In recent years, the field has made significant progress towards a theory of learning to learn along with practical new algorithms, some of which led to impressive results in real-world applications.  Learning to Learn provides a survey of some of the most exciting new research approaches, written by leading researchers in the field. Its objective is to investigate the utility and feasibility of computer programs that can learn how to learn, both from a practical and a theoretical point of view.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Thrun, Sebastian and Pratt, Lorien},
	month = dec,
	year = {2012},
	note = {Google-Books-ID: X\_jpBwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Information Technology},
}

@techreport{Brown2020gpt,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	number = {arXiv:2005.14165},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@techreport{Devlin2019BERT,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {arXiv:1810.04805},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{decan_impact_2018,
	address = {Gothenburg Sweden},
	title = {On the impact of security vulnerabilities in the npm package dependency network},
	isbn = {978-1-4503-5716-6},
	url = {https://dl.acm.org/doi/10.1145/3196398.3196401},
	doi = {10.1145/3196398.3196401},
	abstract = {Security vulnerabilities are among the most pressing problems in open source software package libraries. It may take a long time to discover and fix vulnerabilities in packages. In addition, vulnerabilities may propagate to dependent packages, making them vulnerable too. This paper presents an empirical study of nearly 400 security reports over a 6-year period in the npm dependency network containing over 610k JavaScript packages. Taking into account the severity of vulnerabilities, we analyse how and when these vulnerabilities are discovered and fixed, and to which extent they affect other packages in the packaging ecosystem in presence of dependency constraints. We report our findings and provide guidelines for package maintainers and tool developers to improve the process of dealing with security issues.},
	language = {en},
	urldate = {2022-06-10},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Decan, Alexandre and Mens, Tom and Constantinou, Eleni},
	month = may,
	year = {2018},
	pages = {181--191},
}

@inproceedings{Mens2014ECOS,
	title = {{ECOS}: {Ecological} studies of open source software ecosystems},
	abstract = {Software ecosystems, collections of projects developed by the same community, are among the most complex artefacts constructed by humans. Collaborative development of open source software (OSS) has witnessed an exponential increase in two decades. Our hypothesis is that software ecosystems bear many similarities with natural ecosystems. While natural ecosystems have been the subject of study for many decades, research on software ecosystems is more recent. For this reason, the ECOS research project aims to determine whether and how selected ecological models and theories from natural ecosystems can be adapted and adopted to understand and better explain how OSS projects (akin to biological species) evolve, and to determine what are the main factors that drive the success or popularity of these projects. Expressed in biological terms, we wish to use knowledge on the evolution of natural ecosystems to provide support aiming to optimize the fitness of OSS projects, and to increase the resistance and resilience of OSS ecosystems.},
	booktitle = {{IEEE} {Conference} on {Software} {Maintenance}, {Reengineering}, and {Reverse} {Engineering} ({CSMR}-{WCRE})},
	author = {Mens, Tom and Claes, Maëlick and Grosjean, Philippe},
	year = {2014},
	keywords = {Communities, Ecosystems, Evolution (biology), Open source software},
}

@article{Najafabadi2015DLinBigDataAnalytics,
	title = {Deep learning applications and challenges in big data analytics},
	abstract = {Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.},
	journal = {Journal of Big Data},
	author = {Najafabadi, Maryam M and Villanustre, Flavio and Khoshgoftaar, Taghi M and Seliya, Naeem and Wald, Randall and Muharemagic, Edin},
	year = {2015},
}

@article{Daniele2018LPDLonDrones,
	title = {Ultra {Low} {Power} {Deep}-{Learning}-powered {Autonomous} {Nano} {Drones}},
	author = {Daniele, Palossi and Antonio, Loquercio and Francesco, Conti and Eric, Flamand and Davide, Scaramuzza and Luca, Benini},
	year = {2018},
}

@article{Lee2021Industry4throughML,
	title = {From technological development to social advance: {A} review of {Industry} 4.0 through machine learning},
	abstract = {Industry 4.0 has attracted considerable interest from firms, governments, and individuals as the new concept of future computer, industrial, and social systems. However, the concept has yet to be fully explored in the scientific literature. Given the topic’s broad scope, this work attempts to understand and clarify Industry 4.0 by analyzing 660 journal papers and 3,901 news articles through text mining with unsupervised machine learning algorithms. Based on the results, this work identifies 31 research and application issues related to Industry 4.0. These issues are categorized and described within a five-level hierarchy: 1) infrastructure development for connection, 2) artificial intelligence development for data-driven decision making, 3) system and process optimization, 4) in­ dustrial innovation, and 5) social advance. Further, a framework for convergence in Industry 4.0 is proposed, featuring six dimensions: connection, collection, communication, computation, control, and creation. The research outcomes are consistent with and complementary to existing relevant discussion and debate on Industry 4.0, which validates the utility and efficiency of the data-driven approach of this work to support experts’ in­ sights on Industry 4.0. This work helps establish a common ground for understanding Industry 4.0 across multiple disciplinary perspectives, enabling further research and development for industrial innovation and social advance.},
	journal = {Technological Forecasting and Social Change},
	author = {Lee, Changhun and Lim, Chiehyeon},
	year = {2021},
}

@article{Ghimire2022SurveyonEfficientCNNandHWAcceleration,
	title = {A {Survey} on {Efficient} {Convolutional} {Neural} {Networks} and {Hardware} {Acceleration}},
	abstract = {Over the past decade, deep-learning-based representations have demonstrated remarkable performance in academia and industry. The learning capability of convolutional neural networks (CNNs) originates from a combination of various feature extraction layers that fully utilize a large amount of data. However, they often require substantial computation and memory resources while replacing traditional hand-engineered features in existing systems. In this review, to improve the efﬁciency of deep learning research, we focus on three aspects: quantized/binarized models, optimized architectures, and resource-constrained systems. Recent advances in light-weight deep learning models and network architecture search (NAS) algorithms are reviewed, starting with simpliﬁed layers and efﬁcient convolution and including new architectural design and optimization. In addition, several practical applications of efﬁcient CNNs have been investigated using various types of hardware architectures and platforms.},
	journal = {Electronics},
	author = {Ghimire, Deepak and Kil, Dayoung and Kim, Seong-heum},
	year = {2022},
}

@techreport{kurita_weight_2020,
	title = {Weight {Poisoning} {Attacks} on {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2004.06660},
	abstract = {Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/neulab/RIPPLe.},
	number = {arXiv:2004.06660},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Kurita, Keita and Michel, Paul and Neubig, Graham},
	month = apr,
	year = {2020},
	note = {arXiv:2004.06660 [cs, stat]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{kurita_weight_2020-1,
	title = {Weight {Poisoning} {Attacks} on {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2004.06660},
	abstract = {Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/neulab/RIPPLe.},
	number = {arXiv:2004.06660},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Kurita, Keita and Michel, Paul and Neubig, Graham},
	month = apr,
	year = {2020},
	note = {arXiv:2004.06660 [cs, stat]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Zhang2022OPT,
	title = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
	url = {http://arxiv.org/abs/2205.01068},
	abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
	journal = {arXiv},
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
	year = {2022},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@techreport{cohen_emnist_2017,
	title = {{EMNIST}: an extension of {MNIST} to handwritten letters},
	shorttitle = {{EMNIST}},
	url = {http://arxiv.org/abs/1702.05373},
	abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
	number = {arXiv:1702.05373},
	urldate = {2022-06-06},
	institution = {arXiv},
	author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
	month = mar,
	year = {2017},
	note = {arXiv:1702.05373 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{cohen_emnist_2017-1,
	title = {{EMNIST}: {Extending} {MNIST} to handwritten letters},
	shorttitle = {{EMNIST}},
	doi = {10.1109/IJCNN.2017.7966217},
	abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, the relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a dataset that constitutes a more challenging classification task involving letters and digits, and one that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results using an online ELM algorithm are presented along with a validation of the conversion process through the comparison of the classification results on NIST digits and the MNIST digits.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
	year = {2017},
	note = {ISSN: 2161-4407},
	keywords = {Benchmark testing, Databases, NIST, Training},
	pages = {2921--2926},
}

@inproceedings{Mitropoulos2014BugCataLogofMaven,
	title = {The bug catalog of the maven ecosystem},
	abstract = {Examining software ecosystems can provide the research community with data regarding artifacts, processes, and communities. We present a dataset obtained from the Maven central repository ecosystem (approximately 265gb of data) by statically analyzing the repository to detect potential software bugs. For our analysis we used FindBugs, a tool that examines Java bytecode to detect numerous types of bugs. The dataset contains the metrics results that FindBugs reports for every project version (a jar) included in the ecosystem. For every version we also stored speciﬁc metadata such as the jar’s size, its dependencies and others. Our dataset can be used to produce interesting research results, as we show in speciﬁc examples.},
	booktitle = {Working {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Mitropoulos, Dimitris and Karakoidas, Vassilios and Louridas, Panos and Gousios, Georgios and Spinellis, Diomidis},
	year = {2014},
}

@inproceedings{Zerouali2019DiversityofSWPackagePopularityMetrics,
	title = {On the {Diversity} of {Software} {Package} {Popularity} {Metrics}: {An} {Empirical} {Study} of npm},
	abstract = {Software systems often leverage on open source software libraries to reuse functionalities. Such libraries are readily available through software package managers like npm for JavaScript. Due to the huge amount of packages available in such package distributions, developers often decide to rely on or contribute to a software package based on its popularity. Moreover, it is a common practice for researchers to depend on popularity metrics for data sampling and choosing the right candidates for their studies. However, the meaning of popularity is relative and can be defined and measured in a diversity of ways, that might produce different outcomes even when considered for the same studies. In this paper, we show evidence of how different is the meaning of popularity in software engineering research. Moreover, we empirically analyse the relationship between different software popularity measures. As a case study, for a large dataset of 175k npm packages, we computed and extracted 9 different popularity metrics from three open source tracking systems: libraries.io, npmjs.com and GitHub. We found that indeed popularity can be measured with different unrelated metrics, each metric can be defined within a specific context. This indicates a need for a generic framework that would use a portfolio of popularity metrics drawing from different concepts.},
	booktitle = {International {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Zerouali, Ahmed and Mens, Tom and Robles, Gregorio and Gonzalez-Barahona, Jesus M.},
	year = {2019},
	keywords = {Correlation, Data mining, Libraries, Measurement, Runtime, Software packages, empirical analysis, npm, popularity, software package},
}

@inproceedings{SotoValero2019EmergenceofSWDiversityinMavenCentral,
	title = {The {Emergence} of {Software} {Diversity} in {Maven} {Central}},
	abstract = {Maven artifacts are immutable: an artifact that is uploaded on Maven Central cannot be removed nor modified. The only way for developers to upgrade their library is to release a new version. Consequently, Maven Central accumulates all the versions of all the libraries that are published there, and applications that declare a dependency towards a library can pick any version. In this work, we hypothesize that the immutability of Maven artifacts and the ability to choose any version naturally support the emergence of software diversity within Maven Central. We analyze 1,487,956 artifacts that represent all the versions of 73,653 libraries. We observe that more than 30\% of libraries have multiple versions that are actively used by latest artifacts. In the case of popular libraries, more than 50\% of their versions are used. We also observe that more than 17\% of libraries have several versions that are significantly more used than the other versions. Our results indicate that the immutability of artifacts in Maven Central does support a sustained level of diversity among versions of libraries in the repository.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Soto-Valero, César and Benelallam, Amine and Harrand, Nicolas and Barais, Olivier and Baudry, Benoit},
	year = {2019},
	keywords = {Data mining, History, Indexes, Java, Labeling, Libraries, Software, dataset, development history graph, digital preservation, free software, mining software repositories, open source software, source code},
}

@inproceedings{Vu2021py2src,
	title = {py2src: {Towards} the {Automatic} (and {Reliable}) {Identification} of {Sources} for {PyPI} {Package}},
	abstract = {Selecting which libraries (‘dependencies’ or ‘packages’ in the industry’s jargon) to adopt in a project is an essential task in software development. The quality of the corresponding source code is a key factor behind this selection (from security to timeliness). Yet, how easy is it to find the ‘actual’ source? How reliable is this information? To address this problem, we developed an approach called py2src to automatically identify GitHub source code repositories corresponding to packages in PyPI and automatically provide an indicator of the reliability of such information. We also report a preliminary empirical evaluation of the approach on the top PyPI packages.},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Vu, Duc-Ly},
	year = {2021},
	keywords = {Codes, Libraries, Mining software repository, PyPI, Python packages, Reliability engineering, Security, Software, Software factors, Software reliability, Software supply chain, Supply chains, quantitavie study},
}

@inproceedings{Goswami2020ReproducibilityofNPMPackages,
	title = {Investigating {The} {Reproducibility} of {NPM} {Packages}},
	abstract = {Node.js has been popularly used for web application development, partially because of its large software ecosystem known as NPM (Node Package Manager) packages. When using open-source NPM packages, most developers download prebuilt packages on npmjs.com instead of building those packages from available source, and implicitly trust the downloaded packages. However, it is unknown whether the blindly trusted prebuilt NPM packages are reproducible (i.e., whether there is always a verifiable path from source code to any published NPM package). Therefore, for this paper, we conducted an empirical study to examine the reproducibility of NPM packages, and to understand why some packages are not reproducible.Specifically, we downloaded versions/releases of 226 most popularly used NPM packages and then built each version with the available source on GitHub. Next, we applied a differencing tool to compare the versions we built against versions downloaded from NPM, and further inspected any reported difference. Among the 3,390 versions of the 226 packages, only 2,087 versions are reproducible. Based on our manual analysis, multiple factors contribute to the non-reproducibility issues, such as flexible versioning information in package.json file and the divergent behaviors between distinct versions of tools used in the build process. Our investigation reveals challenges of verifying NPM reproducibility with existing tools, and provides insights for future verifiable build procedures.},
	booktitle = {International {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Goswami, Pronnoy and Gupta, Saksham and Li, Zhiyuan and Meng, Na and Yao, Daphne},
	year = {2020},
	keywords = {JavaScript, Manuals, NPM packages, Open source software, Packaging, Software development management, Software maintenance, Standards, Tools, reproducibility},
}

@techreport{mujahid2022highlySelectedPackages,
	title = {What are the characteristics of highly-selected packages? {A} case study on the npm ecosystem},
	shorttitle = {What are the characteristics of highly-selected packages?},
	url = {http://arxiv.org/abs/2204.04562},
	abstract = {With the popularity of software ecosystems, the number of open source components (known as packages) has grown rapidly. Identifying high-quality and well-maintained packages from a large pool of packages to depend on is a basic and important problem, as it is beneficial for various applications, such as package recommendation and package search. However, no systematic and comprehensive work focuses on addressing this problem except in online discussions or informal literature and interviews. To fill this gap, in this paper, we conducted a mixed qualitative and quantitative analysis to understand how developers identify and select relevant open source packages. In particular, we started by surveying 118 JavaScript developers from the npm ecosystem to qualitatively understand the factors that make a package to be highly-selected within the npm ecosystem. The survey results showed that JavaScript developers believe that highly-selected packages are well-documented, receive a high number of stars on GitHub, have a large number of downloads, and do not suffer from vulnerabilities. Then, we conducted an experiment to quantitatively validate the developers' perception of the factors that make a highly-selected package. In this analysis, we collected and mined historical data from 2,527 packages divided into highly-selected and not highly-selected packages. For each package in the dataset, we collected quantitative data to present the factors studied in the developers' survey. Next, we used regression analysis to quantitatively investigate which of the studied factors are the most important. Our regression analysis complements our survey results about highly-selected packages. In particular, the results showed that highly-selected packages tend to be correlated by the number of downloads, stars, and how large the package's readme file is.},
	number = {arXiv:2204.04562},
	urldate = {2022-06-03},
	institution = {arXiv},
	author = {Mujahid, Suhaib and Abdalkareem, Rabe and Shihab, Emad},
	month = apr,
	year = {2022},
	keywords = {Computer Science - Software Engineering},
}

@article{Bianco2011SurveyonOpenSourceSWTrustworthiness,
	title = {A {Survey} on {Open} {Source} {Software} {Trustworthiness}},
	abstract = {Trustworthiness is a crucial characteristic when it comes to evaluating any product, even more so for open source software, which is now becoming widely used. The authors conducted a survey to identify the reasons and motivations that lead software companies to adopt or reject open source software; they then ranked, according to importance, the specific trust factors used when selecting an open source software component or product. The motivations and importance ranking of factors might be useful for both developers of open source software (to make their products and components more useful for other stakeholders) and to future prospective open source software users.},
	journal = {IEEE Software},
	author = {del Bianco, Vieri and Lavazza, Luigi and Morasca, Sandro and Taibi, Davide},
	year = {2011},
	keywords = {Open source software, Reliability, Software engineering, Software reliability, external software qualities, internal software qualities, open source software, pragmatic software engineering, trustworthiness},
}

@inproceedings{Gao2020FuzzTestingbasedDataAugtoImproveRobustnessofDNN,
	title = {Fuzz {Testing} based {Data} {Augmentation} to {Improve} {Robustness} of {Deep} {Neural} {Networks}},
	abstract = {Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from. Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair. Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances. We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9\% and 5.5\% on average. Further, Sensei-SA can reduce the average DNN training time by 25\%, while still improving robust accuracy.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Gao, Xiang and Saha, Ripon K. and Prasad, Mukul R. and Roychoudhury, Abhik},
	year = {2020},
	keywords = {DNN, Data Augmentation, Genetic Algorithm, Maintenance engineering, Neural networks, Robustness, Software systems, Test pattern generators, Tools, Training},
}

@inproceedings{Hutchison2018RobustnessTestingofAutonomySW,
	title = {Robustness {Testing} of {Autonomy} {Software}},
	abstract = {As robotic and autonomy systems become progressively more present in industrial and human-interactive applications, it is increasingly critical for them to behave safely in the presence of unexpected inputs. While robustness testing for traditional software systems is long-studied, robustness testing for autonomy systems is relatively uncharted territory. In our role as engineers, testers, and researchers we have observed that autonomy systems are importantly different from traditional systems, requiring novel approaches to effectively test them. We present Automated Stress Testing for Autonomy Architectures (ASTAA), a system that effectively, automatically robustness tests autonomy systems by building on classic principles, with important innovations to support this new domain. Over five years, we have used ASTAA to test 17 real-world autonomy systems, robots, and robotics-oriented libraries, across commercial and academic applications, discovering hundreds of bugs. We outline the ASTAA approach and analyze more than 150 bugs we found in real systems. We discuss what we discovered about testing autonomy systems, specifically focusing on how doing so differs from and is similar to traditional software robustness testing and other high-level lessons.},
	booktitle = {International {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} {Track} ({ICSE}-{SEIP})},
	author = {Hutchison, Casidhe and Zizyte, Milda and Lanigan, Patrick E. and Guttendorf, David and Wagner, Michael and Le Goues, Claire and Koopman, Philip},
	year = {2018},
	keywords = {Computer bugs, Dictionaries, Robots, Robustness, Safety, Software, Testing, autonomy, dependability, robustness testing, safety-critical systems},
}

@article{european_commission_joint_research_centre_robustness_2020,
	title = {Robustness and explainability of {Artificial} {Intelligence}: from technical to policy solutions.},
	volume = {Publications Office of the European Union},
	author = {{European Commission. Joint Research Centre.}},
	year = {2020},
}

@inproceedings{Shi2020RobustnessVerification4Transformers,
	title = {Robustness {Verification} for {Transformers}},
	volume = {arXiv},
	url = {https://arxiv.org/abs/2002.06622},
	abstract = {Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the first robustness verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis.},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Shi, Zhouxing and Zhang, Huan and Chang, Kai-Wei and Huang, Minlie and Hsieh, Cho-Jui},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{Tong2019ImprovingRobustnessofMLClassifiers,
	title = {Improving {Robustness} of {ML} {Classiﬁers} against {Realizable} {Evasion} {Attacks} {Using} {Conserved} {Features}},
	abstract = {Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. However, ML models are often susceptible to evasion attacks, in which an adversary makes changes to the input (such as malware) in order to avoid being detected. A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simpliﬁed feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. We investigate the effectiveness of this approach to designing robust ML in the face of attacks that can be realized in actual malware (realizable attacks). We demonstrate that in the context of structure-based PDF malware detection, such techniques appear to have limited effectiveness, but they are effective with content-based detectors. In either case, we show that augmenting the feature space models with conserved features (those that cannot be unilaterally modiﬁed without compromising malicious functionality) signiﬁcantly improves performance. Finally, we show that feature space models enable generalized robustness when faced with a variety of realizable attacks, as compared to classiﬁers which are tuned to be robust to a speciﬁc realizable attack.},
	booktitle = {{USENIX} {Security} {Symposium}},
	author = {Tong, Liang and Li, Bo and Zhang, Ning and Hajaj, Chen and Xiao, Chaowei and Vorobeychik, Yevgeniy},
	year = {2019},
}

@inproceedings{Gehr2018AI2,
	title = {{AI2}: {Safety} and {Robustness} {Certification} of {Neural} {Networks} with {Abstract} {Interpretation}},
	abstract = {We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2 can handle deep convolutional networks, which are beyond the reach of existing methods.},
	booktitle = {{IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
	year = {2018},
	keywords = {Abstract Interpretation, Biological neural networks, Cats, Neural Networks, Neurons, Perturbation methods, Reliable Machine Learning, Robustness, Safety},
}

@article{Rasheed2021ML4healthcare,
	title = {Explainable, {Trustworthy}, and {Ethical} {Machine} {Learning} for {Healthcare}: {A} {Survey}},
	abstract = {With the advent of machine learning (ML) applications in daily life, the questions about liability, trust, and interpretability of their outputs are raising, especially for healthcare applications. The black-box nature of ML models is a roadblock for clinical utilization. Therefore, to gain the trust of clinicians and patients, researchers need to provide explanations of how and why the model is making a speciﬁc decision. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the ﬁeld of eXplainable ML (XML). In this paper, we provide a comprehensive review of explainable and interpretable ML techniques implemented for providing the reasons behind their decisions for various healthcare applications. Along with highlighting various security, safety, and robustness challenges that hinder the trustworthiness of ML we also discussed the ethical issues of healthcare ML and describe how explainable and trustworthy ML can resolve these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.},
	author = {Rasheed, Khansa and Qayyum, Adnan and Ghaly, Mohammed and Al-Fuqaha, Ala and Razi, Adeel and Qadir, Junaid},
	year = {2021},
}

@article{Floridi2019Rules4TrustworthyAI,
	title = {Establishing the rules for building trustworthy {AI}},
	journal = {Nature Machine Intelligence},
	author = {Floridi, Luciano},
	year = {2019},
}

@article{Marcal2021Traceability4TrustworthyAI,
	title = {Traceability for {Trustworthy} {AI}: {A} {Review} of {Models} and {Tools}},
	abstract = {Traceability is considered a key requirement for trustworthy artiﬁcial intelligence (AI), related to the need to maintain a complete account of the provenance of data, processes, and artifacts involved in the production of an AI model. Traceability in AI shares part of its scope with general purpose recommendations for provenance as W3C PROV, and it is also supported to different extents by speciﬁc tools used by practitioners as part of their efforts in making data analytic processes reproducible or repeatable. Here, we review relevant tools, practices, and data models for traceability in their connection to building AI models and systems. We also propose some minimal requirements to consider a model traceable according to the assessment list of the High-Level Expert Group on AI. Our review shows how, although a good number of reproducibility tools are available, a common approach is currently lacking, together with the need for shared semantics. Besides, we have detected that some tools have either not achieved full maturity, or are already falling into obsolescence or in a state of near abandonment by its developers, which might compromise the reproducibility of the research trusted to them.},
	journal = {Big Data and Cognitive Computing},
	author = {Mora-Cantallops, Marçal and Sánchez-Alonso, Salvador and García-Barriocanal, Elena and Sicilia, Miguel-Angel},
	year = {2021},
}

@inproceedings{Franch2002QualityModelBasedApproach4DescribingEvaluatingSWPackages,
	title = {A quality-model-based approach for describing and evaluating software packages},
	abstract = {Selection of software packages from user requirements is a central task in software engineering. Selection of inappropriate packages may compromise business processes and may interfere negatively in the functioning of the involved organization. Success of package selection is endangered because of many factors, one of the most important being the absence of structured descriptions of both package features and user quality requirements. In this paper, we propose a methodology for describing quality factors of software packages using the ISO/IEC quality standard as a framework. Following this standard, relevant attributes for a specific software domain are identified and structured as a hierarchy, and metrics for them are chosen. Software packages in this domain can then be described in a uniform and comprehensive way. Therefore, selection of packages can be ameliorated by transforming user quality requirements into requirements expressed in terms of quality model attributes. We illustrate the approach by presenting, in some depth, a quality model for the mail server domain.},
	booktitle = {{IEEE} {Joint} {International} {Conference} on {Requirements} {Engineering}},
	author = {Franch, X. and Carvallo, J.P.},
	year = {2002},
	keywords = {Buildings, Business, IEC standards, ISO standards, Packaging, Software engineering, Software packages, Software quality, Software standards, Taxonomy},
}

@article{Jadhav2009EvaluatingSelectingSWPackages,
	title = {Evaluating and selecting software packages: {A} review},
	abstract = {Evaluating and selecting software packages that meet an organization’s requirements is a difﬁcult software engineering process. Selection of a wrong software package can turn out to be costly and adversely affect business processes. The aim of this paper is to provide a basis to improve the process of evaluation and selection of the software packages. This paper reports a systematic review of papers published in journals and conference proceedings. The review investigates methodologies for selecting software packages, software evaluation techniques, software evaluation criteria, and systems that support decision makers in evaluating software packages. The key ﬁndings of the review are: (1) analytic hierarchy process has been widely used for evaluation of the software packages, (2) there is lack of a common list of generic software evaluation criteria and its meaning, and (3) there is need to develop a framework comprising of software selection methodology, evaluation technique, evaluation criteria, and system to assist decision makers in software selection.},
	journal = {Information and Software Technology},
	author = {Jadhav, Anil S. and Sonar, Rajendra M.},
	year = {2009},
}

@techreport{haque_ereba_2022,
	title = {{EREBA}: {Black}-box {Energy} {Testing} of {Adaptive} {Neural} {Networks}},
	shorttitle = {{EREBA}},
	url = {http://arxiv.org/abs/2202.06084},
	abstract = {Recently, various Deep Neural Network (DNN) models have been proposed for environments like embedded systems with stringent energy constraints. The fundamental problem of determining the robustness of a DNN with respect to its energy consumption (energy robustness) is relatively unexplored compared to accuracy-based robustness. This work investigates the energy robustness of Adaptive Neural Networks (AdNNs), a type of energy-saving DNNs proposed for many energy-sensitive domains and have recently gained traction. We propose EREBA, the first black-box testing method for determining the energy robustness of an AdNN. EREBA explores and infers the relationship between inputs and the energy consumption of AdNNs to generate energy surging samples. Extensive implementation and evaluation using three state-of-the-art AdNNs demonstrate that test inputs generated by EREBA could degrade the performance of the system substantially. The test inputs generated by EREBA can increase the energy consumption of AdNNs by 2,000\% compared to the original inputs. Our results also show that test inputs generated via EREBA are valuable in detecting energy surging inputs.},
	urldate = {2022-05-31},
	author = {Haque, Mirazul and Yadlapalli, Yaswanth and Yang, Wei and Liu, Cong},
	month = feb,
	year = {2022},
	doi = {10.1145/3510003.3510088},
	note = {arXiv:2202.06084 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@techreport{wan_what_2022,
	title = {What {Do} {They} {Capture}? -- {A} {Structural} {Analysis} of {Pre}-{Trained} {Language} {Models} for {Source} {Code}},
	shorttitle = {What {Do} {They} {Capture}?},
	url = {http://arxiv.org/abs/2202.06840},
	abstract = {Recently, many pre-trained language models for source code have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code completion, code search, and code summarization. These models leverage masked pre-training and Transformer and have achieved promising results. However, currently there is still little progress regarding interpretability of existing pre-trained code models. It is not clear why these models work and what feature correlations they can capture. In this paper, we conduct a thorough structural analysis aiming to provide an interpretation of pre-trained language models for source code (e.g., CodeBERT, and GraphCodeBERT) from three distinctive perspectives: (1) attention analysis, (2) probing on the word embedding, and (3) syntax tree induction. Through comprehensive analysis, this paper reveals several insightful findings that may inspire future studies: (1) Attention aligns strongly with the syntax structure of code. (2) Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer. (3) The pre-trained models of code have the ability of inducing syntax trees of code. Theses findings suggest that it may be helpful to incorporate the syntax structure of code into the process of pre-training for better code representations.},
	number = {arXiv:2202.06840},
	urldate = {2022-05-31},
	institution = {arXiv},
	author = {Wan, Yao and Zhao, Wei and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Jin, Hai},
	month = feb,
	year = {2022},
	note = {arXiv:2202.06840 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{daoudi_lessons_2021,
	title = {Lessons {Learnt} on {Reproducibility} in {Machine} {Learning} {Based} {Android} {Malware} {Detection}},
	volume = {26},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-021-09955-7},
	doi = {10.1007/s10664-021-09955-7},
	abstract = {A well-known curse of computer security research is that it often produces systems that, while technically sound, fail operationally. To overcome this curse, the community generally seeks to assess proposed systems under a variety of settings in order to make explicit every potential bias. In this respect, recently, research achievements on machine learning based malware detection are being considered for thorough evaluation by the community. Such an effort of comprehensive evaluation supposes first and foremost the possibility to perform an independent reproduction study in order to sharpen evaluations presented by approaches’ authors. The question Can published approaches actually be reproduced? thus becomes paramount despite the little interest such mundane and practical aspects seem to attract in the malware detection field. In this paper, we attempt a complete reproduction of five Android Malware Detectors from the literature and discuss to what extent they are “reproducible”. Notably, we provide insights on the implications around the guesswork that may be required to finalise a working implementation. Finally, we discuss how barriers to reproduction could be lifted, and how the malware detection field would benefit from stronger reproducibility standards—like many various fields already have.},
	language = {en},
	number = {4},
	urldate = {2022-05-27},
	journal = {Empirical Software Engineering},
	author = {Daoudi, Nadia and Allix, Kevin and Bissyandé, Tegawendé F. and Klein, Jacques},
	month = jul,
	year = {2021},
	pages = {74},
}

@article{Ashqar2019InvasiveHydrangea_PTNN,
	title = {Identifying {Images} of {Invasive} {Hydrangea} {Using} {Pre}-{Trained} {Deep} {Convolutional} {Neural} {Networks}},
	abstract = {Invasive species are threatening habitats of native species in many countries around the world. The current methods of monitoring them depend on expert knowledge. Trained scientists visit designated areas and take note of the species inhabiting them. Using such a highly qualified workforce is expensive, time inefficient and insufficient since humans cannot cover large areas when sampling. In this paper, machine learning based approach is presented for identifying images of invasive hydrangea (a beautiful invasive species original of Asia) with a dataset that contains approximately 3,800 images taken in a Brazilian national forest and in some of the pictures there is Hydrangea. A deep learning technique that extensively applied to image recognition was used. Our trained model achieved an accuracy of 99.71\% on a held-out test set, demonstrating the feasibility of this approach.},
	journal = {International Journal of Control and Automation (IJCA)},
	author = {Ashqar, Belal A. M. and Abu-Naser, Samy S.},
	year = {2019},
}

@article{Sokipriala2021AV_PTNN,
	title = {Prediction of {Steering} {Angle} for {Autonomous} {Vehicles} {Using} {Pre}-{Trained} {Neural} {Network}},
	abstract = {Autonomous driving is one promising research area that would not only revolutionize the transportation industry but would as well save thousands of lives. accurate correct Steering angle prediction plays a crucial role in the development of the autonomous vehicle .This research attempts to design a model that would be able to clone a drivers behavior using transfer learning from pretrained VGG16, the results showed that the model was able to use less training parameters and achieved a low mean squared error(MSE) of less than 2\% without overfitting to the training set hence was able to drive on new road it was not trained on.},
	journal = {European Journal of Engineering and Technology Research},
	author = {Sokipriala, Jonah},
	year = {2021},
}

@article{subcommittee_ai_nodate,
	title = {{AI} and {Cybersecurity}: {Opportunities} and {Challenges}},
	language = {en},
	author = {Subcommittee, NSTC MLAI and Subcommittee, NSTC NITRD and Science, National and Council, Technology},
	pages = {15},
}

@article{li_empirical_2022,
	title = {An {Empirical} {Study} of {Yanked} {Releases} in the {Rust} {Package} {Registry}},
	issn = {1939-3520},
	doi = {10.1109/TSE.2022.3152148},
	abstract = {Cargo, the software packaging manager of Rust, provides a yank mechanism to support release-level deprecation, which can prevent packages from depending on yanked releases. Most prior studies focused on code-level (i.e., deprecated APIs) and package-level deprecation (i.e., deprecated packages). However, few studies have focused on release-level deprecation. In this study, we investigate how often and how the yank mechanism is used, the rationales behind its usage, and the adoption of yanked releases in the Cargo ecosystem. Our study shows that 9.6\% of the packages in Cargo have at least one yanked release, and the proportion of yanked releases kept increasing from 2014 to 2020. Package owners yank releases for other reasons than withdrawing a defective release, such as fixing a release that does not follow semantic versioning or indicating a package is removed or replaced. In addition, we found that 46\% of the packages directly adopted at least one yanked release and the yanked releases propagated through the dependency network, which leads to 1.4\% of the releases in the ecosystem having unresolved dependencies.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Li, Hao and Cogo, Filipe R. and Bezemer, Cor-Paul},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Cargo, Computer bugs, Ecosystems, Indexes, Libraries, Packaging, Release deprecation, Rust, Safety, Software, Software ecosystems, Yanking},
	pages = {1--1},
}

@inproceedings{Wittern2016JSPackageEcosystem,
	title = {A look at the dynamics of the {JavaScript} package ecosystem},
	abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node.js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem’s growth and activity, into conﬂicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Wittern, Erik and Suter, Philippe and Rajagopalan, Shriram},
	year = {2016},
}

@article{devanbu_deep_2020,
	title = {Deep {Learning} \& {Software} {Engineering}: {State} of {Research} and {Future} {Directions}},
	shorttitle = {Deep {Learning} \& {Software} {Engineering}},
	url = {http://arxiv.org/abs/2009.08525},
	abstract = {Given the current transformative potential of research that sits at the intersection of Deep Learning (DL) and Software Engineering (SE), an NSF-sponsored community workshop was conducted in co-location with the 34th IEEE/ACM International Conference on Automated Software Engineering (ASE'19) in San Diego, California. The goal of this workshop was to outline high priority areas for cross-cutting research. While a multitude of exciting directions for future work were identified, this report provides a general summary of the research areas representing the areas of highest priority which were discussed at the workshop. The intent of this report is to serve as a potential roadmap to guide future work that sits at the intersection of SE \& DL.},
	urldate = {2022-05-17},
	journal = {arXiv:2009.08525 [cs]},
	author = {Devanbu, Prem and Dwyer, Matthew and Elbaum, Sebastian and Lowry, Michael and Moran, Kevin and Poshyvanyk, Denys and Ray, Baishakhi and Singh, Rishabh and Zhang, Xiangyu},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.08525},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{danks_value_2019,
	address = {Honolulu HI USA},
	title = {The {Value} of {Trustworthy} {AI}},
	isbn = {978-1-4503-6324-2},
	url = {https://dl.acm.org/doi/10.1145/3306618.3314228},
	doi = {10.1145/3306618.3314228},
	abstract = {Trust is one of the most critical relations in our human lives, whether trust in one another, trust in the artifacts that we use everyday, or trust of an AI system. Even a cursory examination of the literatures in human-computer interaction, human-robot interaction, and numerous other disciplines reveals a deep, persistent concern with the nature of trust in AI, and the conditions under which it can be generated, reduced, repaired, or influenced.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Danks, David},
	month = jan,
	year = {2019},
	pages = {521--522},
}

@article{liu_trustworthy_2021,
	title = {Trustworthy {AI}: {A} {Computational} {Perspective}},
	shorttitle = {Trustworthy {AI}},
	url = {http://arxiv.org/abs/2107.06641},
	abstract = {In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone's daily life and profoundly altering the course of human society. The intention of developing AI is to benefit humans, by reducing human labor, bringing everyday convenience to human lives, and promoting social good. However, recent research and AI applications show that AI can cause unintentional harm to humans, such as making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against one group. Thus, trustworthy AI has attracted immense attention recently, which requires careful consideration to avoid the adverse effects that AI may bring to humans, so that humans can fully trust and live in harmony with AI technologies. Recent years have witnessed a tremendous amount of research on trustworthy AI. In this survey, we present a comprehensive survey of trustworthy AI from a computational perspective, to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex area, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety \& Robustness, (ii) Non-discrimination \& Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability \& Auditability, and (vi) Environmental Well-Being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.},
	urldate = {2022-05-16},
	journal = {arXiv:2107.06641 [cs]},
	author = {Liu, Haochen and Wang, Yiqi and Fan, Wenqi and Liu, Xiaorui and Li, Yaxin and Jain, Shaili and Liu, Yunhao and Jain, Anil K. and Tang, Jiliang},
	month = aug,
	year = {2021},
	note = {arXiv: 2107.06641},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{franco_toward_2021,
	title = {Toward {Learning} {Trustworthily} from {Data} {Combining} {Privacy}, {Fairness}, and {Explainability}: {An} {Application} to {Face} {Recognition}},
	volume = {23},
	issn = {1099-4300},
	shorttitle = {Toward {Learning} {Trustworthily} from {Data} {Combining} {Privacy}, {Fairness}, and {Explainability}},
	url = {https://www.mdpi.com/1099-4300/23/8/1047},
	doi = {10.3390/e23081047},
	abstract = {In many decision-making scenarios, ranging from recreational activities to healthcare and policing, the use of artiﬁcial intelligence coupled with the ability to learn from historical data is becoming ubiquitous. This widespread adoption of automated systems is accompanied by the increasing concerns regarding their ethical implications. Fundamental rights, such as the ones that require the preservation of privacy, do not discriminate based on sensible attributes (e.g., gender, ethnicity, political/sexual orientation), or require one to provide an explanation for a decision, are daily undermined by the use of increasingly complex and less understandable yet more accurate learning algorithms. For this purpose, in this work, we work toward the development of systems able to ensure trustworthiness by delivering privacy, fairness, and explainability by design. In particular, we show that it is possible to simultaneously learn from data while preserving the privacy of the individuals thanks to the use of Homomorphic Encryption, ensuring fairness by learning a fair representation from the data, and ensuring explainable decisions with local and global explanations without compromising the accuracy of the ﬁnal models. We test our approach on a widespread but still controversial application, namely face recognition, using the recent FairFace dataset to prove the validity of our approach.},
	language = {en},
	number = {8},
	urldate = {2022-05-16},
	journal = {Entropy},
	author = {Franco, Danilo and Oneto, Luca and Navarin, Nicolò and Anguita, Davide},
	month = aug,
	year = {2021},
	pages = {1047},
}

@inproceedings{rawat_secure_2021,
	address = {Online Only, United States},
	title = {Secure and trustworthy machine learning/artificial intelligence for multi-domain operations},
	isbn = {978-1-5106-4329-1 978-1-5106-4330-7},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11746/2592860/Secure-and-trustworthy-machine-learning-artificial-intelligence-for-multi-domain/10.1117/12.2592860.full},
	doi = {10.1117/12.2592860},
	abstract = {Machine Learning (ML) algorithms and Artiﬁcial Intelligence (AI) systems have already had an immense impact on our society as they have shown to be able to create machine cognition comparable to or even better than human cognition for some applications. ML algorithms are now regarded as very useful for data-driven applications including resilient multi-domain operations. However, ML algorithms and AI systems can be controlled, dodged, biased, and misled through ﬂawed learning models and input data, they need robust security features and trust. Furthermore, ML algorithms and AI systems add challenges when we have (unlabeled/labeled) sparse/small data or big data for training and evaluation. It is very important to design, evaluate and test ML algorithms and AI systems that produce reliable, robust, trustworthy, explainable, and fair/unbiased outcomes to make them acceptable and reliable in mission critical multi-domain operations. ML algorithms rely on data and work on the principle of “Garbage In, Garbage Out,” which means that if the input data to learning model is corrupted or compromised, the outcomes of the ML/AI would not be optimal, reliable and trustworthy. This paper focuses on achieving secure and trustworthy machine learning and artiﬁcial intelligence operations using context aware selection of learning models and blockchain for multi-domain battleﬁeld operations.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} for {Multi}-{Domain} {Operations} {Applications} {III}},
	publisher = {SPIE},
	author = {Rawat, Danda},
	editor = {Pham, Tien and Solomon, Latasha and Hohil, Myron E.},
	month = apr,
	year = {2021},
	pages = {99},
}

@article{hamon_bridging_2022,
	title = {Bridging the {Gap} {Between} {AI} and {Explainability} in the {GDPR}: {Towards} {Trustworthiness}-by-{Design} in {Automated} {Decision}-{Making}},
	volume = {17},
	issn = {1556-6048},
	shorttitle = {Bridging the {Gap} {Between} {AI} and {Explainability} in the {GDPR}},
	doi = {10.1109/MCI.2021.3129960},
	abstract = {Can satisfactory explanations for complex machine learning models be achieved in high-risk automated decision-making? How can such explanations be integrated into a data protection framework safeguarding a right to explanation? This article explores from an interdisciplinary point of view the connection between existing legal requirements for the explainability of AI systems set out in the General Data Protection Regulation (GDPR) and the current state of the art in the field of explainable AI. It studies the challenges of providing human legible explanations for current and future AI-based decision-making systems in practice, based on two scenarios of automated decision-making in credit scoring risks and medical diagnosis of COVID-19. These scenarios exemplify the trend towards increasingly complex machine learning algorithms in automated decision-making, both in terms of data and models. Current machine learning techniques, in particular those based on deep learning, are unable to make clear causal links between input data and final decisions. This represents a limitation for providing exact, human-legible reasons behind specific decisions, and presents a serious challenge to the provision of satisfactory, fair and transparent explanations. Therefore, the conclusion is that the quality of explanations might not be considered as an adequate safeguard for automated decision-making processes under the GDPR. Accordingly, additional tools should be considered to complement explanations. These could include algorithmic impact assessments, other forms of algorithmic justifications based on broader AI principles, and new technical developments in trustworthy AI. This suggests that eventually all of these approaches would need to be considered as a whole.},
	number = {1},
	journal = {IEEE Computational Intelligence Magazine},
	author = {Hamon, Ronan and Junklewitz, Henrik and Sanchez, Ignacio and Malgieri, Gianclaudio and De Hert, Paul},
	year = {2022},
	note = {Conference Name: IEEE Computational Intelligence Magazine},
	keywords = {COVID-19, Data models, Decision making, Deep learning, General Data Protection Regulation, Law, Machine learning algorithms, Security},
	pages = {72--85},
}

@inproceedings{dilmaghani_privacy_2019,
	title = {Privacy and {Security} of {Big} {Data} in {AI} {Systems}: {A} {Research} and {Standards} {Perspective}},
	shorttitle = {Privacy and {Security} of {Big} {Data} in {AI} {Systems}},
	doi = {10.1109/BigData47090.2019.9006283},
	abstract = {The huge volume, variety, and velocity of big data have empowered Machine Learning (ML) techniques and Artificial Intelligence (AI) systems. However, the vast portion of data used to train AI systems is sensitive information. Hence, any vulnerability has a potentially disastrous impact on privacy aspects and security issues. Nevertheless, the increased demands for high-quality AI from governments and companies require the utilization of big data in the systems. Several studies have highlighted the threats of big data on different platforms and the countermeasures to reduce the risks caused by attacks. In this paper, we provide an overview of the existing threats which violate privacy aspects and security issues inflicted by big data as a primary driving force within the AI/ML workflow. We define an adversarial model to investigate the attacks. Additionally, we analyze and summarize the defense strategies and countermeasures of these attacks. Furthermore, due to the impact of AI systems in the market and the vast majority of business sectors, we also investigate Standards Developing Organizations (SDOs) that are actively involved in providing guidelines to protect the privacy and ensure the security of big data and AI systems. Our far-reaching goal is to bridge the research and standardization frame to increase the consistency and efficiency of AI systems developments guaranteeing customer satisfaction while transferring a high degree of trustworthiness.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Dilmaghani, Saharnaz and Brust, Matthias R. and Danoy, Grégoire and Cassagnes, Natalia and Pecero, Johnatan and Bouvry, Pascal},
	year = {2019},
	keywords = {Artificial intelligence, Big Data, Data models, Data privacy, IEC Standards, Security},
	pages = {5737--5743},
}

@inproceedings{spring_managing_2020,
	address = {Online USA},
	title = {On managing vulnerabilities in {AI}/{ML} systems},
	isbn = {978-1-4503-8995-2},
	url = {https://dl.acm.org/doi/10.1145/3442167.3442177},
	doi = {10.1145/3442167.3442177},
	abstract = {This paper explores how the current paradigm of vulnerability management might adapt to include machine learning systems through a thought experiment: what if flaws in machine learning (ML) were assigned Common Vulnerabilities and Exposures (CVE) identifiers (CVE-IDs)? We consider both ML algorithms and model objects. The hypothetical scenario is structured around exploring the changes to the six areas of vulnerability management: discovery, report intake, analysis, coordination, disclosure, and response. While algorithm flaws are well-known in academic research community, there is no apparent clear line of communication between this research community and the operational communities that deploy and manage systems that use ML. The thought experiments identify some ways in which CVE-IDs may establish some useful lines of communication between these two communities. In particular, it would start to introduce the research community to operational security concepts, which appears to be a gap left by existing efforts.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {New {Security} {Paradigms} {Workshop} 2020},
	publisher = {ACM},
	author = {Spring, Jonathan M. and Galyardt, April and Householder, Allen D. and VanHoudnos, Nathan},
	month = oct,
	year = {2020},
	pages = {111--126},
}

@inproceedings{rawat_secure_2021-1,
	address = {Online Only, United States},
	title = {Secure and trustworthy machine learning/artificial intelligence for multi-domain operations},
	isbn = {978-1-5106-4329-1 978-1-5106-4330-7},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11746/2592860/Secure-and-trustworthy-machine-learning-artificial-intelligence-for-multi-domain/10.1117/12.2592860.full},
	doi = {10.1117/12.2592860},
	abstract = {Machine Learning (ML) algorithms and Artiﬁcial Intelligence (AI) systems have already had an immense impact on our society as they have shown to be able to create machine cognition comparable to or even better than human cognition for some applications. ML algorithms are now regarded as very useful for data-driven applications including resilient multi-domain operations. However, ML algorithms and AI systems can be controlled, dodged, biased, and misled through ﬂawed learning models and input data, they need robust security features and trust. Furthermore, ML algorithms and AI systems add challenges when we have (unlabeled/labeled) sparse/small data or big data for training and evaluation. It is very important to design, evaluate and test ML algorithms and AI systems that produce reliable, robust, trustworthy, explainable, and fair/unbiased outcomes to make them acceptable and reliable in mission critical multi-domain operations. ML algorithms rely on data and work on the principle of “Garbage In, Garbage Out,” which means that if the input data to learning model is corrupted or compromised, the outcomes of the ML/AI would not be optimal, reliable and trustworthy. This paper focuses on achieving secure and trustworthy machine learning and artiﬁcial intelligence operations using context aware selection of learning models and blockchain for multi-domain battleﬁeld operations.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} for {Multi}-{Domain} {Operations} {Applications} {III}},
	publisher = {SPIE},
	author = {Rawat, Danda},
	editor = {Pham, Tien and Solomon, Latasha and Hohil, Myron E.},
	month = apr,
	year = {2021},
	pages = {99},
}

@misc{noauthor_randomized_nodate,
	title = {A {Randomized} {Trial} of {Intraarterial} {Treatment} for {Acute} {Ischemic} {Stroke} {\textbar} {NEJM}},
	url = {https://www.nejm.org/doi/full/10.1056/NEJMoa1411587},
	urldate = {2022-05-16},
}

@book{ng_animal_2022,
	title = {Animal {Kingdom}: {A} {Large} and {Diverse} {Dataset} for {Animal} {Behavior} {Understanding}},
	shorttitle = {Animal {Kingdom}},
	abstract = {Understanding animals' behaviors is significant for a wide range of applications. However, existing animal behavior datasets have limitations in multiple aspects, including limited numbers of animal classes, data samples and provided tasks, and also limited variations in environmental conditions and viewpoints. To address these limitations , we create a large and diverse dataset, Animal Kingdom, that provides multiple annotated tasks to enable a more thorough understanding of natural animal behaviors. The wild animal footages used in our dataset record different times of the day in extensive range of environments containing variations in backgrounds, viewpoints, illumination and weather conditions. More specifically, our dataset contains 50 hours of annotated videos to localize relevant animal behavior segments in long videos for the video grounding task, 30K video sequences for the fine-grained multi-label action recognition task, and 33K frames for the pose estimation task, which correspond to a diverse range of animals with 850 species across 6 major animal classes. Such a challenging and comprehensive dataset shall be able to facilitate the community to develop, adapt, and evaluate various types of advanced methods for animal behavior analysis. Moreover, we propose a Collabo-rative Action Recognition (CARe) model that learns general and specific features for action recognition with unseen new animals. This method achieves promising performance in our experiments. Our dataset can be found at https://sutdcv.github.io/Animal-Kingdom.},
	author = {Ng, Xun and Ong, Kian Eng and Zheng, Qichen and Ni, Yun and Yeo, Si and Liu, Jun},
	month = apr,
	year = {2022},
}

@inproceedings{chatzidimitriou_npm-miner_2018,
	title = {npm-{Miner}: {An} {Infrastructure} for {Measuring} the {Quality} of the npm {Registry}},
	abstract = {As the popularity of the JavaScript language is constantly increasing, one of the most important challenges today is to assess the quality of JavaScript packages. Developers often employ tools for code linting and for the extraction of static analysis metrics in order to assess and/or improve their code. In this context, we have developed npn-miner, a platform that crawls the npm registry and analyzes the packages using static analysis tools in order to extract detailed quality metrics as well as high-level quality attributes, such as maintainability and security. Our infrastructure includes an index that is accessible through a web interface, while we have also constructed a dataset with the results of a detailed analysis for 2000 popular npm packages.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Chatzidimitriou, Kyriakos and Papamichail, Michail and Diamantopoulos, Themistoklis and Tsapanos, Michail and Symeonidis, Andreas},
	year = {2018},
	keywords = {Computer architecture, Measurement, Security, Software, Static analysis, Tools, Uniform resource locators, javascript, npm, software quality, static analysis},
}

@article{tay_are_2022,
	title = {Are {Pre}-trained {Convolutions} {Better} than {Pre}-trained {Transformers}?},
	url = {http://arxiv.org/abs/2105.03322},
	abstract = {In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.},
	urldate = {2022-05-05},
	journal = {arXiv:2105.03322 [cs]},
	author = {Tay, Yi and Dehghani, Mostafa and Gupta, Jai and Bahri, Dara and Aribandi, Vamsi and Qin, Zhen and Metzler, Donald},
	month = jan,
	year = {2022},
	note = {arXiv: 2105.03322},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{marcelino_transfer_2022,
	title = {Transfer learning from pre-trained models},
	url = {https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751},
	abstract = {How to solve any image classification problem quickly and easily},
	language = {en},
	urldate = {2022-05-05},
	journal = {Medium},
	author = {Marcelino, Pedro},
	month = apr,
	year = {2022},
}

@inproceedings{chen_pre-trained_2021,
	address = {Nashville, TN, USA},
	title = {Pre-{Trained} {Image} {Processing} {Transformer}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577359/},
	doi = {10.1109/CVPR46437.2021.01212},
	language = {en},
	urldate = {2022-05-05},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen},
	month = jun,
	year = {2021},
	pages = {12294--12305},
}

@inproceedings{zerouali_diversity_2019,
	title = {On the {Diversity} of {Software} {Package} {Popularity} {Metrics}: {An} {Empirical} {Study} of npm},
	shorttitle = {On the {Diversity} of {Software} {Package} {Popularity} {Metrics}},
	doi = {10.1109/SANER.2019.8667997},
	abstract = {Software systems often leverage on open source software libraries to reuse functionalities. Such libraries are readily available through software package managers like npm for JavaScript. Due to the huge amount of packages available in such package distributions, developers often decide to rely on or contribute to a software package based on its popularity. Moreover, it is a common practice for researchers to depend on popularity metrics for data sampling and choosing the right candidates for their studies. However, the meaning of popularity is relative and can be defined and measured in a diversity of ways, that might produce different outcomes even when considered for the same studies. In this paper, we show evidence of how different is the meaning of popularity in software engineering research. Moreover, we empirically analyse the relationship between different software popularity measures. As a case study, for a large dataset of 175k npm packages, we computed and extracted 9 different popularity metrics from three open source tracking systems: libraries.io, npmjs.com and GitHub. We found that indeed popularity can be measured with different unrelated metrics, each metric can be defined within a specific context. This indicates a need for a generic framework that would use a portfolio of popularity metrics drawing from different concepts.},
	booktitle = {2019 {IEEE} 26th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Zerouali, Ahmed and Mens, Tom and Robles, Gregorio and Gonzalez-Barahona, Jesus M.},
	year = {2019},
	note = {ISSN: 1534-5351},
	keywords = {Correlation, Data mining, Libraries, Measurement, Runtime, Software packages, empirical analysis, npm, popularity, software package},
	pages = {589--593},
}

@article{rooney_root_nodate,
	title = {Root {Cause} {Analysis} {For} {Beginners}},
	language = {en},
	author = {Rooney, James J and Heuvel, Lee N Vanden},
	pages = {9},
}

@article{abdalkareem_beyond_nodate,
	title = {Beyond {Traditional} {Software} {Development}: {Studying} and {Supporting} the {Role} of {Reusing} {Crowdsourced} {Knowledge} in {Software} {Development}},
	language = {en},
	author = {Abdalkareem, Rabe Muftah B},
	pages = {164},
}

@misc{noauthor_code_nodate,
	title = {Code {Reuse} in {Open} {Source} {Software}},
	url = {https://pubsonline.informs.org/doi/epdf/10.1287/mnsc.1070.0748},
	language = {en},
	urldate = {2022-04-26},
	doi = {10.1287/mnsc.1070.0748},
}

@article{buda_systematic_2018,
	title = {A systematic study of the class imbalance problem in convolutional neural networks},
	volume = {106},
	issn = {08936080},
	url = {http://arxiv.org/abs/1710.05381},
	doi = {10.1016/j.neunet.2018.07.011},
	abstract = {In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.},
	urldate = {2022-04-21},
	journal = {Neural Networks},
	author = {Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A.},
	month = oct,
	year = {2018},
	note = {arXiv: 1710.05381},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {249--259},
}

@article{von_der_mosel_validity_2021,
	title = {On the validity of pre-trained transformers for natural language processing in the software engineering domain},
	url = {http://arxiv.org/abs/2109.04738},
	abstract = {Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.},
	urldate = {2022-04-17},
	journal = {arXiv:2109.04738 [cs]},
	author = {von der Mosel, Julian and Trautsch, Alexander and Herbold, Steffen},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.04738},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{chakraborty_does_2021,
	address = {Athens Greece},
	title = {Does reusing pre-trained {NLP} model propagate bugs?},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3473494},
	doi = {10.1145/3468264.3473494},
	abstract = {In this digital era, the textual content has become a seemingly ubiquitous part of our life. Natural Language Processing (NLP) empowers machines to comprehend the intricacies of textual data and eases human-computer interaction. Advancement in language modeling, continual learning, availability of a large amount of linguistic data, and large-scale computational power have made it feasible to train models for downstream tasks related to text analysis, including safety-critical ones, e.g., medical, airlines, etc. Compared to other deep learning (DL) models, NLP-based models are widely reused for various tasks. However, the reuse of pre-trained models in a new setting is still a complex task due to the limitations of the training dataset, model structure, specification, usage, etc. With this motivation, we study BERT, a vastly used language model (LM), from the direction of reusing in the code. We mined 80 posts from Stack Overflow related to BERT and found 4 types of bugs observed in clients’ code. Our results show that 13.75\% are fairness, 28.75\% are parameter, 15\% are token, and 16.25\% are version-related bugs.},
	language = {en},
	urldate = {2022-04-12},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Chakraborty, Mohna},
	month = aug,
	year = {2021},
	pages = {1686--1688},
}

@article{spinellis_package_2012,
	title = {Package {Management} {Systems}},
	volume = {29},
	issn = {1937-4194},
	doi = {10.1109/MS.2012.38},
	abstract = {A package management system organizes and simplifies the installation and maintenance of software by standardizing and organizing the production and consumption of software collections. As a software developer, you can benefit from package managers in two ways: through a rich and stable development environment and through friction-free reuse. Promisingly, the structure that package managers bring both to the tools we use in our development process and the libraries we reuse in our products ties nicely with the recent move emphasizing DevOps (development operations) as an integration between software development and IT operations.},
	number = {2},
	journal = {IEEE Software},
	author = {Spinellis, Diomidis},
	month = mar,
	year = {2012},
	note = {Conference Name: IEEE Software},
	keywords = {DevOps, Maintenance engineering, Product management, Software libraries, Software reusability, module dependencies, package management system, shared library, software reuse},
	pages = {84--86},
}

@inproceedings{dietrich_dependency_2019,
	title = {Dependency {Versioning} in the {Wild}},
	doi = {10.1109/MSR.2019.00061},
	abstract = {Many modern software systems are built on top of existing packages (modules, components, libraries). The increasing number and complexity of dependencies has given rise to automated dependency management where package managers resolve symbolic dependencies against a central repository. When declaring dependencies, developers face various choices, such as whether or not to declare a fixed version or a range of versions. The former results in runtime behaviour that is easier to predict, whilst the latter enables flexibility in resolution that can, for example, prevent different versions of the same package being included and facilitates the automated deployment of bug fixes. We study the choices developers make across 17 different package managers, investigating over 70 million dependencies. This is complemented by a survey of 170 developers. We find that many package managers support - and the respective community adapts - flexible versioning practices. This does not always work: developers struggle to find the sweet spot between the predictability of fixed version dependencies, and the agility of flexible ones, and depending on their experience, adjust practices. We see some uptake of semantic versioning in some package managers, supported by tools. However, there is no evidence that projects switch to semantic versioning on a large scale. The results of this study can guide further research into better practices for automated dependency management, and aid the adaptation of semantic versioning.},
	booktitle = {2019 {IEEE}/{ACM} 16th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Dietrich, Jens and Pearce, David and Stringer, Jacob and Tahir, Amjed and Blincoe, Kelly},
	year = {2019},
	note = {ISSN: 2574-3864},
	keywords = {Computer bugs, Contracts, Runtime, Semantics, Software, Syntactics, Tools, dependency management, maven, npm, package managers, repository studies, semantic versioning},
	pages = {349--359},
}

@inproceedings{ignatiev_towards_2014,
	address = {Hyderabad India},
	title = {Towards efficient optimization in package management systems},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568306},
	doi = {10.1145/2568225.2568306},
	abstract = {Package management as a means of reuse of software artifacts has become extremely popular, most notably in Linux distributions. At the same time, successful package management brings about a number of computational challenges. Whenever a user requires a new package to be installed, a package manager not only installs the new package but it might also install other packages or uninstall some old ones in order to respect dependencies and conﬂicts of the packages. Coming up with a new conﬁguration of packages is computationally challenging. It is in particular complex when we also wish to optimize for user preferences, such as that the resulting package conﬁguration should not diﬀer too much from the original one. A number of exact approaches for solving this problem have been proposed in recent years. These approaches, however, do not have guaranteed runtime due to the high computational complexity of the problem. This paper addresses this issue by devising a hybrid approach that integrates exact solving with approximate solving by invoking the approximate part whenever the solver is running out of time. Experimental evaluation shows that this approach enables returning high-quality package conﬁgurations with rapid response time.},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Ignatiev, Alexey and Janota, Mikoláš and Marques-Silva, Joao},
	month = may,
	year = {2014},
	pages = {745--755},
}

@article{pham_problems_2020,
	title = {Problems and {Opportunities} in {Training} {Deep} {Learning} {Software} {Systems}: {An} {Analysis} of {Variance}},
	abstract = {Deep learning (DL) training algorithms utilize nondeterminism to improve models’ accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.},
	language = {en},
	author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
	year = {2020},
	pages = {13},
}

@article{qian_are_nodate,
	title = {Are {My} {Deep} {Learning} {Systems} {Fair}? {An} {Empirical} {Study} of {Fixed}-{Seed} {Training}},
	abstract = {Deep learning (DL) systems have been gaining popularity in critical tasks such as credit evaluation and crime prediction. Such systems demand fairness. Recent work shows that DL software implementations introduce variance: identical DL training runs (i.e., identical network, data, conﬁguration, software, and hardware) with a ﬁxed seed produce diﬀerent models. Such variance could make DL models and networks violate fairness compliance laws, resulting in negative social impact. In this paper, we conduct the ﬁrst empirical study to quantify the impact of software implementation on the fairness and its variance of DL systems. Our study of 22 mitigation techniques and ﬁve baselines reveals up to 12.6\% fairness variance across identical training runs with identical seeds. In addition, most debiasing algorithms have a negative impact on the model such as reducing model accuracy, increasing fairness variance, or increasing accuracy variance. Our literature survey shows that while fairness is gaining popularity in artiﬁcial intelligence (AI) related conferences, only 34.4\% of the papers use multiple identical training runs to evaluate their approach, raising concerns about their results’ validity. We call for better fairness evaluation and testing protocols to improve fairness and fairness variance of DL systems as well as DL research validity and reproducibility at large.},
	language = {en},
	author = {Qian, Shangshu and Pham, Hung Viet and Lutellier, Thibaud and Hu, Zeou and Kim, Jungwon and Tan, Lin and Yu, Yaoliang and Chen, Jiahao and Shah, Sameena},
	pages = {24},
}

@article{pham_problems_2020-1,
	title = {Problems and {Opportunities} in {Training} {Deep} {Learning} {Software} {Systems}: {An} {Analysis} of {Variance}},
	abstract = {Deep learning (DL) training algorithms utilize nondeterminism to improve models’ accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.},
	language = {en},
	author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
	year = {2020},
	pages = {13},
}

@article{golendukhina_what_2022,
	title = {What is {Software} {Quality} for {AI} {Engineers}? {Towards} a {Thinning} of the {Fog}},
	shorttitle = {What is {Software} {Quality} for {AI} {Engineers}?},
	url = {http://arxiv.org/abs/2203.12697},
	abstract = {It is often overseen that AI-enabled systems are also software systems and therefore rely on software quality assurance (SQA). Thus, the goal of this study is to investigate the software quality assurance strategies adopted during the development, integration, and maintenance of AI/ML components and code. We conducted semi-structured interviews with representatives of ten Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the interview data identified 12 issues in the development of AI/ML components. Furthermore, we identified when quality issues arise in AI/ML components and how they are detected. The results of this study should guide future work on software quality assurance processes and techniques for AI/ML components.},
	urldate = {2022-03-29},
	journal = {arXiv:2203.12697 [cs]},
	author = {Golendukhina, Valentina and Lenarduzzi, Valentina and Felderer, Michael},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.12697},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{bhatia_towards_2022,
	title = {Towards a {Change} {Taxonomy} for {Machine} {Learning} {Systems}},
	url = {http://arxiv.org/abs/2203.11365},
	abstract = {Machine Learning (ML) research publications commonly provide open-source implementations on GitHub, allowing their audience to replicate, validate, or even extend machine learning algorithms, data sets, and metadata. However, thus far little is known about the degree of collaboration activity happening on such ML research repositories, in particular regarding (1) the degree to which such repositories receive contributions from forks, (2) the nature of such contributions (i.e., the types of changes), and (3) the nature of changes that are not contributed back to forks, which might represent missed opportunities. In this paper, we empirically study contributions to 1,346 ML research repositories and their 67,369 forks, both quantitatively and qualitatively (by building on Hindle et al.'s seminal taxonomy of code changes). We found that while ML research repositories are heavily forked, only 9\% of the forks made modifications to the forked repository. 42\% of the latter sent changes to the parent repositories, half of which (52\%) were accepted by the parent repositories. Our qualitative analysis on 539 contributed and 378 local (fork-only) changes, extends Hindle et al.'s taxonomy with one new top-level change category related to ML (Data), and 15 new sub-categories, including nine ML-specific ones (input data, output data, program data, sharing, change evaluation, parameter tuning, performance, pre-processing, model training). While the changes that are not contributed back by the forks mostly concern domain-specific customizations and local experimentation (e.g., parameter tuning), the origin ML repositories do miss out on a non-negligible 15.4\% of Documentation changes, 13.6\% of Feature changes and 11.4\% of Bug fix changes. The findings in this paper will be useful for practitioners, researchers, toolsmiths, and educators.},
	urldate = {2022-03-28},
	journal = {arXiv:2203.11365 [cs]},
	author = {Bhatia, Aaditya and Eghan, Ellis E. and Grichi, Manel and Cavanagh, William G. and Ming, Zhen and Jiang and Adams, Bram},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.11365},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{jabbarvand_search-based_2019,
	address = {Montreal, QC, Canada},
	title = {Search-{Based} {Energy} {Testing} of {Android}},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8812097/},
	doi = {10.1109/ICSE.2019.00115},
	abstract = {The utility of a smartphone is limited by its battery capacity and the ability of its hardware and software to efﬁciently use the device’s battery. To properly characterize the energy consumption of an app and identify energy defects, it is critical that apps are properly tested, i.e., analyzed dynamically to assess the app’s energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. We present COBWEB, a search-based energy testing technique for Android. By leveraging a set of novel models, representing both the functional behavior of an app as well as the contextual conditions affecting the app’s energy behavior, COBWEB generates a test suite that can effectively ﬁnd energy defects. Our experimental results using real-world apps demonstrate not only its ability to effectively and efﬁciently test energy behavior of apps, but also its superiority over prior techniques by ﬁnding a wider and more diverse set of energy defects.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Jabbarvand, Reyhaneh and Lin, Jun-Wei and Malek, Sam},
	month = may,
	year = {2019},
	pages = {1119--1130},
}

@article{xu_empirical_2022,
	title = {An {Empirical} {Study} on the {Impact} of {Deep} {Parameters} on {Mobile} {App} {Energy} {Usage}},
	url = {http://arxiv.org/abs/2009.12156},
	abstract = {Improving software performance through conﬁguration parameter tuning is a common activity during software maintenance. Beyond traditional performance metrics like latency, mobile app developers are interested in reducing app energy usage. Some mobile apps have centralized locations for parameter tuning, similar to databases and operating systems, but it is common for mobile apps to have hundreds of parameters scattered around the source code. The correlation between these “deep” parameters and app energy usage is unclear. Researchers have studied the energy effects of deep parameters in speciﬁc modules, but we lack a systematic understanding of the energy impact of mobile deep parameters.},
	language = {en},
	urldate = {2022-03-23},
	journal = {arXiv:2009.12156 [cs]},
	author = {Xu, Qiang and Davis, James C. and Hu, Y. Charlie and Jindal, Abhilash},
	month = jan,
	year = {2022},
	note = {arXiv: 2009.12156},
	keywords = {Computer Science - Software Engineering},
}

@article{ousterhout_always_2018,
	title = {Always measure one level deeper},
	volume = {61},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3213770},
	doi = {10.1145/3213770},
	abstract = {Performance measurements often go wrong, reporting surface-level results that are more marketing than science.},
	language = {en},
	number = {7},
	urldate = {2022-03-23},
	journal = {Communications of the ACM},
	author = {Ousterhout, John},
	month = jun,
	year = {2018},
	pages = {74--83},
}

@article{stamoulis_single-path_2019,
	title = {Single-{Path} {NAS}: {Designing} {Hardware}-{Efficient} {ConvNets} in less than 4 {Hours}},
	shorttitle = {Single-{Path} {NAS}},
	url = {http://arxiv.org/abs/1904.02877},
	abstract = {Can we automatically design a Convolutional Network (ConvNet) with the highest image classification accuracy under the runtime constraint of a mobile device? Neural architecture search (NAS) has revolutionized the design of hardware-efficient ConvNets by automating this process. However, the NAS problem remains challenging due to the combinatorially large design space, causing a significant searching time (at least 200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours. Our contributions are as follows: 1. Single-path search space: Compared to previous differentiable NAS methods, Single-Path NAS uses one single-path over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs. 2. Hardware-efficient ImageNet classification: Single-Path NAS achieves 74.96\% top-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with similar constraints ({\textless}80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30 TPU-hours), which is up to 5,000x faster compared to prior work. 4. Reproducibility: Unlike all recent mobile-efficient NAS methods which only release pretrained models, we open-source our entire codebase at: https://github.com/dstamoulis/single-path-nas.},
	urldate = {2022-03-23},
	journal = {arXiv:1904.02877 [cs, stat]},
	author = {Stamoulis, Dimitrios and Ding, Ruizhou and Wang, Di and Lymberopoulos, Dimitrios and Priyantha, Bodhi and Liu, Jie and Marculescu, Diana},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.02877},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{goel_survey_2020,
	title = {A {Survey} of {Methods} for {Low}-{Power} {Deep} {Learning} and {Computer} {Vision}},
	doi = {10.1109/WF-IoT48130.2020.9221198},
	abstract = {Deep neural networks (DNNs) are successful in many computer vision tasks. However, the most accurate DNNs require millions of parameters and operations, making them energy, computation and memory intensive. This impedes the deployment of large DNNs in low-power devices with limited compute resources. Recent research improves DNN models by reducing the memory requirement, energy consumption, and number of operations without significantly decreasing the accuracy. This paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses the methods for compacting and accelerating DNN models. The techniques can be divided into four major categories: (1) parameter quantization and pruning, (2) compressed convolutional filters and matrix factorization, (3) network architecture search, and (4) knowledge distillation. We analyze the accuracy, advantages, disadvantages, and potential solutions to the problems with the techniques in each category. We also discuss new evaluation metrics as a guideline for future research.},
	booktitle = {2020 {IEEE} 6th {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	author = {Goel, Abhinav and Tung, Caleb and Lu, Yung-Hsiang and Thiruvathukal, George K.},
	month = jun,
	year = {2020},
	keywords = {Computational modeling, Computer vision, Deep learning, Knowledge engineering, Measurement, Memory management, Quantization (signal), computer vision, low-power, neural networks},
	pages = {1--6},
}

@inproceedings{phaithoon_fixme_2021,
	title = {{FixMe}: {A} {GitHub} {Bot} for {Detecting} and {Monitoring} {On}-{Hold} {Self}-{Admitted} {Technical} {Debt}},
	shorttitle = {{FixMe}},
	doi = {10.1109/ASE51524.2021.9678680},
	abstract = {Self-Admitted Technical Debt (SATD) is a special form of technical debt in which developers intentionally record their hacks in the code by adding comments for attention. Here, we focus on issue-related "On-hold SATD", where developers suspend proper implementation due to issues reported inside or outside the project. When the referenced issues are resolved, the On-hold SATD also need to be addressed, but since monitoring these issue reports takes a lot of time and effort, developers may not be aware of the resolved issues and leave the On-hold SATD in the code. In this paper, we propose FixMe, a GitHub bot that helps developers detecting and monitoring On-hold SATD in their repositories and notify them whenever the On-hold SATDs are ready to be fixed (i.e. the referenced issues are resolved). The bot can automatically detect On-hold SATD comments from source code using machine learning techniques and discover referenced issues. When the referenced issues are resolved, developers will be notified by FixMe bot. The evaluation conducted with 11 participants shows that our FixMe bot can support them in dealing with On-hold SATD. FixMe is available at https://www.fixmebot.app/ and FixMe's VDO is at https://youtu.be/YSz9kFxN\_YQ.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Phaithoon, Saranphon and Wongnil, Supakarn and Pussawong, Patiphol and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee and Maipradit, Rungroj and Hata, Hideaki and Matsumoto, Kenichi},
	year = {2021},
	note = {ISSN: 2643-1572},
	keywords = {Codes, Computer science, Filtering, Machine learning, Monitoring, Software, Usability},
	pages = {1257--1261},
}

@inproceedings{shen_somanyconflicts_2021,
	title = {{SoManyConflicts}: {Resolve} {Many} {Merge} {Conflicts} {Interactively} and {Systematically}},
	shorttitle = {{SoManyConflicts}},
	doi = {10.1109/ASE51524.2021.9678937},
	abstract = {Code merging plays an important role in collaborative software development. However, it is often tedious and error-prone for developers to manually resolve merge conflicts, especially when there are many conflicts after merging long-lived branches or parallel versions. In this paper, we present SoManyConflicts, a language-agnostic approach to help developers resolve merge conflicts systematically, by utilizing their interrelations (e.g., dependency, similarity, etc.). SoManyConflicts employs a graph representation to model these interrelations and provides 3 major features: 1) cluster and order related conflict based on the graph connectivity; 2) suggest related conflicts of one focused conflict based on the topological sorting, 3) suggest resolution strategies for unresolved conflicts based already resolved ones. We have implemented SoManyConflicts as a Visual Studio Code extension that supports multiple languages (Java, JavaScript, and TypeScript, etc.), which is briefly introduced in the video: https://youtu.be/asWhj1KTU. The source code is publicly available at: https://github.com/Symbolk/somanyconflicts.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Shen, Bo and Zhang, Wei and Yu, Ailun and Shi, Yifan and Zhao, Haiyan and Jin, Zhi},
	year = {2021},
	note = {ISSN: 2643-1572},
	keywords = {Codes, Collaborative software, Merging, Productivity, Software algorithms, Systematics, Version control, Visualization, code merging, conflict resolution, graph partitioning},
	pages = {1291--1295},
}

@inproceedings{ying_nas-bench-101_2019,
	title = {{NAS}-{Bench}-101: {Towards} {Reproducible} {Neural} {Architecture} {Search}},
	shorttitle = {{NAS}-{Bench}-101},
	url = {https://proceedings.mlr.press/v97/ying19a.html},
	abstract = {Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ying, Chris and Klein, Aaron and Christiansen, Eric and Real, Esteban and Murphy, Kevin and Hutter, Frank},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7105--7114},
}

@article{elsken_neural_nodate,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	language = {en},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	pages = {21},
}

@article{Willemink2020MedicalImagingData4ML,
	title = {Preparing {Medical} {Imaging} {Data} for {Machine} {Learning}},
	abstract = {Supervised artificial intelligence (AI) methods for evaluation of medical images require a curation process for data to optimally train, validate, and test algorithms. The chief obstacles to development and clinical implementation of AI algorithms include availability of sufficiently large, curated, and representative training data that includes expert labeling (eg, annotations).},
	journal = {Radiological Society of North America},
	author = {Willemink, Martin J. and Koszek, Wojciech A. and Hardell, Cailin and Wu, Jie and Fleischmann, Dominik and Harvey, Hugh and Folio, Les R. and Summers, Ronald M. and Rubin, Daniel L. and Lungren, Matthew P.},
	year = {2020},
}

@inproceedings{Tucker2010CaseStudyinSWREengineering,
	title = {A {Case} {Study} in {Software} {Reengineering}},
	abstract = {Software reengineering results from several needs including fixing defects (corrective reengineering), modifying the software to address weaknesses or to mitigate potential malfunctions (preventive reengineering), and extending the software to accommodate changes in its external environment (adaptive reengineering). This paper describes a case study in perfective and adaptive reengineering. The rationale for the reengineering decisions, the results of the project, lessons learned and the current state of the system are described.},
	booktitle = {International {Conference} on {Informatio} ({ITNG})n {Technology}: {New} {Generations}},
	author = {Tucker, D. Casey and Devon, M. Simmonds},
	year = {2010},
	keywords = {Advertising, Audio recording, Business, Companies, Databases, Documentation, Hardware, Software design, Software maintenance, Software performance, UML, design, software reengineering},
}

@article{Tan2014OSSBugCharacteristics,
	title = {Bug characteristics in open source software},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Tan, Lin and Liu, Chen and Li, Zhenmin and Wang, Xuanhui and Zhou, Yuanyuan and Zhai, Chengxiang},
	year = {2014},
}

@inproceedings{Sun2017RealBugsforML,
	title = {An {Empirical} {Study} on {Real} {Bugs} for {Machine} {Learning} {Programs}},
	abstract = {Due to the availability of various open source Machine Learning (ML) tools and libraries, developers nowadays can easily implement their purposes by just invoking machine learning APIs without knowing the details of the algorithm. However, the owners of ML tools and libraries usually pay more attention to the correctness and functionality of their algorithm, while spending much less effort on maintaining their code and keeping their code at a high quality level. Considering the popularity of machine learning in today's world, low quality ML tools and libraries can have a huge impact on the software products that use ML algorithms. So in this paper, we conduct an empirical study on real machine learning bugs to examine their patterns and how they evolve over time. We collect three popular machine learning projects on Github, and manually analyzed 329 closed bugs from the perspectives of their bug category, fix pattern, fix scale, fix duration, and type of software maintenance. The results show that (1) there are seven categories of bugs in machine learning programs; (2) twelve different fix patterns are commonly used to fix the bugs; (3) 63.83\% of the patches belong to micro-scale-fix and small-scale-fix, and 68.39\% of the bugs are fixed within one month; (4) 47.77\% of the bug fixes belong to corrective activity from the view of software maintenance.},
	booktitle = {Asia-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Sun, Xiaobing and Zhou, Tianchi and Li, Gengjie and Hu, Jiajun and Yang, Hui and Li, Bin},
	year = {2017},
	keywords = {bug, bug fix, empirical study, machine learning programs},
}

@inproceedings{Serban2020SEBPinMLAdoptionEffects,
	title = {Adoption and effects of software engineering best practices in machine learning},
	abstract = {Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner. Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components. Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models. Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied. Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.},
	booktitle = {Empirical {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Serban, Alex and Blom, Koen Van Der and Hoos, Holger and Visser, Joost},
	year = {2020},
	keywords = {Best practices, Machine learning engineering, Survey},
}

@inproceedings{Seaman2008DefectCategorization,
	title = {Defect categorization: making use of a decade of widely varying historical data},
	abstract = {This paper describes our experience in aggregating a number of historical datasets containing inspection defect data using different categorization schemes. Our goal was to make use of the historical data by creating models to guide future development projects. We describe our approach to reconciling the different choices used in the historical datasets to categorize defects, and the challenges we faced. We also present a set of recommendations for others involved in classifying defects.},
	urldate = {2021-10-25},
	booktitle = {Empirical {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Seaman, Carolyn B. and Shull, Forrest and Regardie, Myrna and Elbert, Denis and Feldmann, Raimund L and Guo, Yuepu and Godfrey, Sally},
	year = {2008},
}

@article{Ren2017FasterRCNN,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2017},
	keywords = {Object detection, convolutional neural network, region proposal},
}

@article{Pineau2020,
	title = {Improving {Reproducibility} in {Machine} {Learning} {Research}},
	abstract = {One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.},
	journal = {Journal of Machine Learning Research},
	author = {Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha, Koustuv and Lariviere, Vincent and Beygelzimer, Alina},
	year = {2020},
}

@article{Lorenzoni2021MLModelDevelopmentfromSEPerspective,
	title = {Machine {Learning} {Model} {Development} from a {Software} {Engineering} {Perspective}: {A} {Systematic} {Literature} {Review}},
	url = {https://arxiv.org/abs/2102.07574},
	abstract = {Data scientists often develop machine learning models to solve a variety of problems in the industry and academy but not without facing several challenges in terms of Model Development. The problems regarding Machine Learning Development involves the fact that such professionals do not realize that they usually perform ad-hoc practices that could be improved by the adoption of activities presented in the Software Engineering Development Lifecycle. Of course, since machine learning systems are different from traditional Software systems, some differences in their respective development processes are to be expected. In this context, this paper is an effort to investigate the challenges and practices that emerge during the development of ML models from the software engineering perspective by focusing on understanding how software developers could benefit from applying or adapting the traditional software engineering process to the Machine Learning workflow.},
	journal = {arXiv},
	author = {Lorenzoni, Giuliano and Alencar, Paulo and Nascimento, Nathalia and Cowan, Donald},
	year = {2021},
}

@inproceedings{COCO,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and others},
	year = {2014},
}

@article{Gundersen2018ReproducibilityinAI,
	title = {State of the art: {Reproducibility} in artificial intelligence},
	abstract = {Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20\% and 30\% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.},
	journal = {AAAI Conference on Artificial Intelligence (AAAI)},
	author = {Gundersen, Odd Erik and Kjensmo, Sigbjørn},
	year = {2018},
}

@inproceedings{Goel2020LPDL,
	title = {A {Survey} of {Methods} for {Low}-{Power} {Deep} {Learning} and {Computer} {Vision}},
	abstract = {Deep neural networks (DNNs) are successful in many computer vision tasks. However, the most accurate DNNs require millions of parameters and operations, making them energy, computation and memory intensive. This impedes the deployment of large DNNs in low-power devices with limited compute resources. Recent research improves DNN models by reducing the memory requirement, energy consumption, and number of operations without significantly decreasing the accuracy. This paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses the methods for compacting and accelerating DNN models. The techniques can be divided into four major categories: (1) parameter quantization and pruning, (2) compressed convolutional filters and matrix factorization, (3) network architecture search, and (4) knowledge distillation. We analyze the accuracy, advantages, disadvantages, and potential solutions to the problems with the techniques in each category. We also discuss new evaluation metrics as a guideline for future research.},
	booktitle = {{IEEE} {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	author = {Goel, Abhinav and Tung, Caleb and Lu, Yung Hsiang and Thiruvathukal, George K.},
	year = {2020},
	keywords = {computer vision, low-power, neural networks},
}

@article{Devanbu2020SE4DLVision,
	title = {Deep {Learning} \& {Software} {Engineering}: {State} of {Research} and {Future} {Directions}},
	shorttitle = {Deep {Learning} \& {Software} {Engineering}},
	url = {https://arxiv.org/abs/2009.08525},
	abstract = {Given the current transformative potential of research that sits at the intersection of Deep Learning (DL) and Software Engineering (SE), an NSF-sponsored community workshop was conducted in co-location with the 34th IEEE/ACM International Conference on Automated Software Engineering (ASE'19) in San Diego, California. The goal of this workshop was to outline high priority areas for cross-cutting research. While a multitude of exciting directions for future work were identified, this report provides a general summary of the research areas representing the areas of highest priority which were discussed at the workshop. The intent of this report is to serve as a potential roadmap to guide future work that sits at the intersection of SE \& DL.},
	journal = {arXiv},
	author = {Devanbu, Prem and Dwyer, Matthew and Elbaum, Sebastian and Lowry, Michael and Moran, Kevin and Poshyvanyk, Denys and Ray, Baishakhi and Singh, Rishabh and Zhang, Xiangyu},
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{Behera2017SurveyonMLConceptAlgorithmsApplications,
	title = {A {Survey} on {Machine} {Learning}: {Concept}, {Algorithms} and {Applications}},
	journal = {International Journal of Innovative Research in Computer and Communication Engineering (IJIRCCE)},
	author = {Behera, Rabi and Das, Kajaree},
	year = {2017},
}

@inproceedings{Byrne1992ConceptualFoundation4SWReengineering,
	title = {A conceptual foundation for software re-engineering},
	abstract = {The author presents a conceptual foundation for software re-engineering. The foundation is composed of properties and principles that underlie re-engineering methods, and assumptions about re-engineering. The value of this conceptual foundation is its ability to model understanding of re-engineering, how it is practiced, and how it can be practiced. A general model of software re-engineering is established, based on this foundation. This model, along with its underlying foundation, proves useful for examining issues such as the re-engineering process and strategies.{\textless}{\textgreater}},
	booktitle = {Conference on {Software} {Maintenance}},
	author = {Byrne, E.J.},
	year = {1992},
	keywords = {Control systems, Databases, Documentation, Programming, Software maintenance, Software quality, Software reusability, Software systems, Software testing, Usability},
}

@article{Zhang2020NumericalBugsinNN,
	title = {Detecting numerical bugs in neural network architectures},
	abstract = {Detecting bugs in deep learning software at the architecture level provides additional benefits that detecting bugs at the model level does not provide. This paper makes the first attempt to conduct static analysis for detecting numerical bugs at the architecture level. We propose a static analysis approach for detecting numerical bugs in neural architectures based on abstract interpretation. Our approach mainly comprises two kinds of abstraction techniques, i.e., one for tensors and one for numerical values. Moreover, to scale up while maintaining adequate detection precision, we propose two abstraction techniques: tensor partitioning and (elementwise) affine relation analysis to abstract tensors and numerical values, respectively. We realize the combination scheme of tensor partitioning and affine relation analysis (together with interval analysis) as DEBAR, and evaluate it on two datasets: neural architectures with known bugs (collected from existing studies) and real-world neural architectures. The evaluation results show that DEBAR outperforms other tensor and numerical abstraction techniques on accuracy without losing scalability. DEBAR successfully detects all known numerical bugs with no false positives within 1.7-2.3 seconds per architecture. On the real-world architectures, DEBAR reports 529 warnings within 2.6-135.4 seconds per architecture, where 299 warnings are true positives.},
	journal = {European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)},
	author = {Zhang, Yuhao and Ren, Luyao and Chen, Liqian and Xiong, Yingfei and Cheung, Shing-Chi and Xie, Tao},
	year = {2020},
	keywords = {Neural Network, Numerical Bugs, Static Analysis},
}

@article{Zhang2018TFBugs,
	title = {An empirical study on {TensorFlow} program bugs},
	abstract = {Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research e orts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To ll this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These ndings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.},
	urldate = {2021-10-04},
	journal = {International Symposium on Software Testing and Analysis (ISSTA)},
	author = {Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu},
	year = {2018},
}

@article{Zhang2020DifferentialFuzzing4DLOps,
	title = {Duo: {Differential} {Fuzzing} for {Deep} {Learning} {Operators}},
	shorttitle = {Duo},
	abstract = {Deep learning (DL) libraries reduce the barriers to the DL model construction. In DL libraries, various building blocks are DL operators with different functionality, responsible for processing high-dimensional tensors during training and inference. Thus, the quality of operators could directly impact the quality of models. However, existing DL testing techniques mainly focus on robustness testing of trained neural network models and cannot locate DL operators’ defects. The insufficient test input and undetermined test output in operator testing have become challenging for DL library developers. In this article, we propose an approach, namely Duo, which combines fuzzing techniques and differential testing techniques to generate input and evaluate corresponding output. It implements mutation-based fuzzing to produce tensor inputs by employing nine mutation operators derived from genetic algorithms and differential testing to evaluate outputs’ correctness from multiple operator instances. Duo is implemented in a tool and used to evaluate seven operators from TensorFlow, PyTorch, MNN, and MXNet in an experiment. The result shows that Duo can expose defects of DL operators and realize multidimension evaluation for DL operators from different DL libraries.},
	journal = {IEEE Transactions on Reliability},
	author = {Zhang, Xufan and Liu, Jiawei and Sun, Ning and Fang, Chunrong and Liu, Jia and Wang, Jiang and Chai, Dong and Chen, Zhenyu},
	year = {2021},
	keywords = {Computer bugs, Deep learning, Deep learning operators, Fuzzing, Learning systems, Tensors, Testing, Training data, deep learning libraries, defect detection, differential testing, fuzzing},
}

@inproceedings{Zhang2019CommonChallengesinDevelopingDLApplications,
	title = {An {Empirical} {Study} of {Common} {Challenges} in {Developing} {Deep} {Learning} {Applications}},
	abstract = {Recent advances in deep learning promote the innovation of many intelligent systems and applications such as autonomous driving and image recognition. Despite enormous efforts and investments in this field, a fundamental question remains under-investigated - what challenges do developers commonly face when building deep learning applications? To seek an answer, this paper presents a large-scale empirical study of deep learning questions in a popular Q\&A website, Stack Overflow. We manually inspect a sample of 715 questions and identify seven kinds of frequently asked questions. We further build a classification model to quantify the distribution of different kinds of deep learning questions in the entire set of 39,628 deep learning questions. We find that program crashes, model migration, and implementation questions are the top three most frequently asked questions. After carefully examining accepted answers of these questions, we summarize five main root causes that may deserve attention from the research community, including API misuse, incorrect hyperparameter selection, GPU computation, static graph computation, and limited debugging and profiling support. Our results highlight the need for new techniques such as cross-framework differential testing to improve software development productivity and software reliability in deep learning.},
	booktitle = {International {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Zhang, Tianyi and Gao, Cuiyun and Ma, Lei and Lyu, Michael and Kim, Miryung},
	year = {2019},
	keywords = {deep learning, Stack Overflow, programming issues, software reliability},
}

@inproceedings{Zhang2020ProgramFailuresofDLjobs,
	title = {An empirical study on program failures of deep learning jobs},
	abstract = {Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Zhang, Ru and Xiao, Wencong and Zhang, Hongyu and Liu, Yu and Lin, Haoxiang and Yang, Mao},
	year = {2020},
}

@article{Xu2021CVinConstructionACriticalReview,
	title = {Computer {Vision} {Techniques} in {Construction}: {A} {Critical} {Review}},
	abstract = {Computer vision has been gaining interest in a wide range of research areas in recent years, from medical to industrial robotics. The architecture, engineering and construction and facility management sector ranks as one of the most intensive fields where vision-based systems/methods are used to facilitate decision making processes during the construction phase. Construction sites make efficient monitoring extremely tedious and difficult due to clutter and disorder. Extensive research has been carried out to investigate the potential to utilise computer vision for assisting on-site managerial tasks. This paper reviews studies on computer vision in the past decade, with a focus on state-of-the-art methods in a typical vision-based scheme, and discusses challenges associated with their application. This research aims to guide practitioners to successfully find suitable approaches for a particular project.},
	journal = {Archives of Computational Methods in Engineering},
	author = {Xu, Shuyuan and Wang, Jun and Shou, Wenchi and Ngo, Tuan and Sadick, Abdul-Manan and Wang, Xiangyu},
	year = {2021},
}

@article{DL4CV,
	title = {Deep {Learning} for {Computer} {Vision}: {A} {Brief} {Review}},
	shorttitle = {Deep {Learning} for {Computer} {Vision}},
	abstract = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
	journal = {Computational Intelligence and Neuroscience},
	author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
	year = {2018},
}

@article{StarCraft2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Michael and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
	year = {2019},
}

@inproceedings{Sculley2014MLTechDebt,
	title = {Machine {Learning} : {The} {High}-{Interest} {Credit} {Card} of {Technical} {Debt}},
	abstract = {Machine learning offers a fantastically powerful toolkit for building complex sys-tems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is re-markably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several ma-chine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
	booktitle = {{NIPS} {Workshop} on {Software} {Engineering} for {Machine} {Learning} ({SE4ML})},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
	year = {2014},
}

@inproceedings{Islam2020RepairingDNN:FixpatternsChallenges,
	title = {Repairing deep neural networks: fix patterns and challenges},
	shorttitle = {Repairing deep neural networks},
	abstract = {Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh},
	month = jun,
	year = {2020},
}

@inproceedings{Islam2019DLBugCharacteristics,
	title = {A comprehensive study on deep learning bug characteristics},
	abstract = {Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48\% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43\% of the times. We have also found that the bugs in the usage of deep learning libraries have some common antipatterns that lead to a strong correlation of bug types among the libraries.},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Islam, Md Johirul and Nguyen, Giang and Pan, Rangeet and Rajan, Hridesh},
	year = {2019},
	keywords = {Bugs, Deep learning bugs, Deep learning software, Empirical study of bugs, Q\&A forums},
}

@inproceedings{Humbatova2020TaxonomyofRealFaultsinDLSystems,
	title = {Taxonomy of real faults in deep learning systems},
	abstract = {The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems.We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50\% of the survey participants.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Humbatova, Nargiz and Jahangirova, Gunel and Bavota, Gabriele and Riccio, Vincenzo and Stocco, Andrea and Tonella, Paolo},
	year = {2020},
	keywords = {Deep learning, Real faults, Software testing, Taxonomy},
}

@inproceedings{Guo2018DLFuzz,
	title = {{DLFuzz}: {Differential} {Fuzzing} {Testing} of {Deep} {Learning} {Systems}},
	abstract = {Deep learning (DL) systems are increasingly applied to safety-critical domains such as autonomous driving cars. It is of significant importance to ensure the reliability and robustness of DL systems. Existing testing methodologies always fail to include rare inputs in the testing dataset and exhibit low neuron coverage. In this paper, we propose DLFuzz, the frst differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality. We present empirical evaluations on two well-known datasets to demonstrate its efficiency. Compared with DeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not require extra efforts to find similar functional DL systems for cross-referencing check, but could generate 338.59\% more adversarial inputs with 89.82\% smaller perturbations, averagely obtain 2.86\% higher neuron coverage, and save 20.11\% time consumption.},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Guo, Jianmin and Jiang, Yu and Zhao, Yue and Chen, Quan and Sun, Jiaguang},
	year = {2018},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{Breck2019DataValidation4ML,
	title = {Data {Validation} for {Machine} {Learning}},
	abstract = {Machine learning is a powerful tool for gleaning knowledge from massive amounts of data. While a great deal of machine learning research has focused on improving the accuracy and efﬁciency of training and inference algorithms, there is less attention in the equally important problem of monitoring the quality of data fed to machine learning. The importance of this problem is hard to dispute: errors in the input data can nullify any beneﬁts on speed and accuracy for training and inference. This argument points to a data-centric approach to machine learning that treats training and serving data as an important production asset, on par with the algorithm and infrastructure used for learning.},
	booktitle = {the {Conference} on {Machine} {Learning} and {Systems} ({MLSys})},
	author = {Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven and Zinkevich, Martin},
	year = {2019},
}

@article{openai_dota_2019,
	title = {Dota 2 with {Large} {Scale} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1912.06680},
	abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
	urldate = {2022-03-17},
	journal = {arXiv:1912.06680 [cs, stat]},
	author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemysław and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.06680},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{Amershi2019SE4MLCaseStudy,
	title = {Software {Engineering} for {Machine} {Learning}: {A} {Case} {Study}},
	shorttitle = {Software {Engineering} for {Machine} {Learning}},
	abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workﬂow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workﬂow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identiﬁed three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difﬁcult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difﬁcult to handle as distinct modules than traditional software components — models may be “entangled” in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.},
	booktitle = {International {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald},
	year = {2019},
}

@inproceedings{Gopalakrishna2022IoTPractices,
	title = {``{If} security is required'': {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	abstract = {The latest generation of IoT systems incorporate machine learning (ML) technologies on edge devices. This introduces new engineering challenges to bring ML onto resource-constrained hardware, and complications for ensuring system security and privacy. Existing research prescribes iterative processes for machine learning enabled IoT products to ease development and increase product success. However, these processes mostly focus on existing practices used in other generic software development areas and are not specialized for the purpose of machine learning or IoT devices.},
	booktitle = {International {Workshop} on {Software} {Engineering} {Research} \& {Practices} for the {Internet} of {Things} ({SERP4IoT})},
	author = {Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C},
	year = {2022},
}

@inproceedings{Shen2021DLCompilerBugs,
	title = {A comprehensive study of deep learning compiler bugs},
	abstract = {There are increasing uses of deep learning (DL) compilers to generate optimized code, boosting the runtime performance of DL models on speci�c hardware. Like their traditional counterparts, DL compilers can generate incorrect code, resulting in unexpected model behaviors that may cause catastrophic consequences in missioncritical systems. On the other hand, the DL models processed by DL compilers di�er fundamentally from imperative programs in that the program logic in DL models is implicit. As such, various characteristics of the bugs arising from traditional compilers need to be revisited in the context of DL compilers.},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Shen, Qingchao and Ma, Haoyang and Chen, Junjie and Tian, Yongqiang and Cheung, Shing-Chi and Chen, Xiang},
	year = {2021},
}

@article{manes_art_2021,
	title = {The {Art}, {Science}, and {Engineering} of {Fuzzing}: {A} {Survey}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {The {Art}, {Science}, and {Engineering} of {Fuzzing}},
	doi = {10.1109/TSE.2019.2946563},
	abstract = {Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.},
	number = {11},
	journal = {IEEE Transactions on Software Engineering},
	author = {Manès, Valentin J.M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Computer bugs, Fuzzing, Security, Software security, Terminology, automated software testing, fuzz testing, fuzzing},
	pages = {2312--2331},
}

@article{zhu_fuzzing_2022,
	title = {Fuzzing: {A} {Survey} for {Roadmap}},
	issn = {0360-0300},
	shorttitle = {Fuzzing},
	url = {https://doi.org/10.1145/3512345},
	doi = {10.1145/3512345},
	abstract = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this paper, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
	urldate = {2022-03-14},
	journal = {ACM Computing Surveys},
	author = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
	year = {2022},
	note = {Just Accepted},
	keywords = {Automation, Fuzz Testing, Fuzzing Theory, Input Space, Security},
}

@article{Shorten2019SurveyonImageDataAug,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year = {2019},
}

@article{Lettner2021DaQL2,
	title = {{DaQL} 2.0: {Measure} {Data} {Quality} based on {Entity} {Models}},
	abstract = {IAnbosrtdrearcto make good decisions, the data used for decision-making needs to be of high quality. As the volume of data continually increases, ensuring high data quality is a big challenge nowadays and needs to be automated with tools. The goal of the Data QInuoarlditeyrLtoibmraarkye(DgoaoQdLd)eicsistoiopnrso, vthideedaattaooulsetod cfoorntdiencuiosuiosnly-menaksuinreg annededms etoasbuereodf ahtiaghquqaulaitlyitya.sApsrotphoesveodluinm[e5]o.fIdnatthaiscopnatpineur,awllye pinrcerseeanstetsh,eecnusrurreinntgsthaitguhs odfattaheqdueavlietylopismaenbtigofcthhaellneenwgeDnaoQwLadvaeyrssioannd2.0n.eeTdhse tmo abine caountotrmibautetidonwoifthDtaoQolLs.2T.0hies gthoealpoosfstihbeiliDtyattoa dQeuﬁanlietydaLtiabqraurayli(tDy aruQleLs) fiosrtocopmropvleidxedaattaooolbtjoecctson(ctianluleoduselnytietinessu),rewahnicdhmreeparseusreendtabtuasqinueaslistyobajsecptrso.pIonsceodnitnra[s5t]t.oInextihsitsinpgaptoeor,lsw, ae upsreesrednotetshenocturrreeqnutirsetadtuestaoilfetdhekndoewvelelodpgme eanbtoouft tthhee ndeawtabDaaseQsLchveemrsiaotnha2t.0is. Tohbesemrvaeidn.contribution of DaQL 2.0 is the possibility to deﬁne data quality rules for complex data objects (called entities), which represent business objects. In contrast to existing tools, a ucse2r0d2o1esThneotAreuqthuoirres.dPeutabilliesdhekdnobwy lEedlsgeeviaebroBu.tVt.he database schema that is observed.},
	journal = {Procedia Computer Science},
	author = {Lettner, Christian and Stumptner, Reinhard and Fragner, Werner and Rauchenzauner, Franz and Ehrlinger, Lisa},
	year = {2021},
}

@inproceedings{Aranda2009SecretLifeofBugs,
	title = {The secret life of bugs: {Going} past the errors and omissions in software repositories},
	abstract = {Every bug has a story behind it. The people that discover and resolve it need to coordinate, to get information from documents, tools, or other people, and to navigate through issues of accountability, ownership, and organizational structure. This paper reports on a field study of coordination activities around bug fixing that used a combination of case study research and a survey of software professionals. Results show that the histories of even simple bugs are strongly dependent on social, organizational, and technical knowledge that cannot be solely extracted through automation of electronic repositories, and that such automation provides incomplete and often erroneous accounts of coordination. The paper uses rich bug histories and survey results to identify common bug fixing coordination patterns and to provide implications for tool designers and researchers of coordination in software development.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Aranda, Jorge and Venolia, Gina},
	year = {2009},
	keywords = {Automation, Computer bugs, Data mining, History, Navigation, Productivity, Programming, Software debugging, Software development management, Spatial databases},
}

@article{Ocariza2017JavaScriptBugs,
	title = {A {Study} of {Causes} and {Consequences} of {Client}-{Side} {JavaScript} {Bugs}},
	abstract = {Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ocariza, Frolin S. and Bajaj, Kartik and Pattabiraman, Karthik and others},
	year = {2017},
	keywords = {Cascading style sheets, Computer bugs, Data mining, Document Object Model (DOM), Faults, HTML, JavaScript, Market research, Reliability, Servers, bug reports, empirical study},
}

@article{fang_you_2021,
	title = {You {Only} {Look} at {One} {Sequence}: {Rethinking} {Transformer} in {Vision} through {Object} {Detection}},
	shorttitle = {You {Only} {Look} at {One} {Sequence}},
	url = {http://arxiv.org/abs/2106.00666},
	abstract = {Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.},
	urldate = {2022-03-07},
	journal = {arXiv:2106.00666 [cs]},
	author = {Fang, Yuxin and Liao, Bencheng and Wang, Xinggang and Fang, Jiemin and Qi, Jiyang and Wu, Rui and Niu, Jianwei and Liu, Wenyu},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.00666},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{ahmed_impact_2020,
	title = {The {Impact} of {Filter} {Size} and {Number} of {Filters} on {Classification} {Accuracy} in {CNN}},
	doi = {10.1109/CSASE48920.2020.9142089},
	abstract = {Convolution Neural Networks (CNNs) have received considerable attention due to their ability to learn directly from data classification features. CNNs used for human motion classification, where predefined and fixed convolutional filter size used. In this paper, different sizes and numbers of filters were used with CNN to determine their effect on accuracy of human motion classification. This work has been done through series of experiments; in each experiment, different filter size and number of filters have been applied. The best performance has been obtained when using 4 convolution layers and 2 pooling layers, whereas has been used the large filter size with upper convolution layer and with each layer the size of filter decreased and number of filters increased, so that, the maximum value of the accuracy classification was 98.98\%.},
	booktitle = {2020 {International} {Conference} on {Computer} {Science} and {Software} {Engineering} ({CSASE})},
	author = {Ahmed, Wafaa Shihab and Karim, Abdul amir A.},
	month = apr,
	year = {2020},
	keywords = {Computer architecture, Computer science, Convolution, Convolution Neural Networks (CNNs), Convolutional filters size, Neural networks, Software engineering, Testing, Training, human Activity Recognition (HAR) and Convolutional layers},
	pages = {88--93},
}

@article{benz_adversarial_2021,
	title = {Adversarial {Robustness} {Comparison} of {Vision} {Transformer} and {MLP}-{Mixer} to {CNNs}},
	url = {http://arxiv.org/abs/2110.02797},
	abstract = {Convolutional Neural Networks (CNNs) have become the de facto gold standard in computer vision applications in the past years. Recently, however, new model architectures have been proposed challenging the status quo. The Vision Transformer (ViT) relies solely on attention modules, while the MLP-Mixer architecture substitutes the self-attention modules with Multi-Layer Perceptrons (MLPs). Despite their great success, CNNs have been widely known to be vulnerable to adversarial attacks, causing serious concerns for security-sensitive applications. Thus, it is critical for the community to know whether the newly proposed ViT and MLP-Mixer are also vulnerable to adversarial attacks. To this end, we empirically evaluate their adversarial robustness under several adversarial attack setups and benchmark them against the widely used CNNs. Overall, we find that the two architectures, especially ViT, are more robust than their CNN models. Using a toy example, we also provide empirical evidence that the lower adversarial robustness of CNNs can be partially attributed to their shift-invariant property. Our frequency analysis suggests that the most robust ViT architectures tend to rely more on low-frequency features compared with CNNs. Additionally, we have an intriguing finding that MLP-Mixer is extremely vulnerable to universal adversarial perturbations.},
	urldate = {2022-03-07},
	journal = {arXiv:2110.02797 [cs]},
	author = {Benz, Philipp and Ham, Soomin and Zhang, Chaoning and Karjauv, Adil and Kweon, In So},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.02797},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{mahmood_robustness_2021,
	address = {Montreal, QC, Canada},
	title = {On the {Robustness} of {Vision} {Transformers} to {Adversarial} {Examples}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710333/},
	doi = {10.1109/ICCV48922.2021.00774},
	abstract = {Recent advances in attention-based networks have shown that Vision Transformers can achieve state-of-the-art or near state-of-the-art results on many image classiﬁcation tasks. This puts transformers in the unique position of being a promising alternative to traditional convolutional neural networks (CNNs). While CNNs have been carefully studied with respect to adversarial attacks, the same cannot be said of Vision Transformers. In this paper, we study the robustness of Vision Transformers to adversarial examples. Our analyses of transformer security is divided into three parts. First, we test the transformer under standard whitebox and black-box attacks. Second, we study the transferability of adversarial examples between CNNs and transformers. We show that adversarial examples do not readily transfer between CNNs and transformers. Based on this ﬁnding, we analyze the security of a simple ensemble defense of CNNs and transformers. By creating a new attack, the self-attention blended gradient attack, we show that such an ensemble is not secure under a white-box adversary. However, under a black-box adversary, we show that an ensemble can achieve unprecedented robustness without sacriﬁcing clean accuracy. Our analysis for this work is done using six types of white-box attacks and two types of black-box attacks. Our study encompasses multiple Vision Transformers, Big Transfer Models and CNN architectures trained on CIFAR-10, CIFAR-100 and ImageNet.},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Mahmood, Kaleel and Mahmood, Rigel and van Dijk, Marten},
	month = oct,
	year = {2021},
	pages = {7818--7827},
}

@article{mao_towards_2021,
	title = {Towards {Robust} {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2105.07926},
	abstract = {Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By using and combining robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. We further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT*. The experimental results on ImageNet and six robustness benchmarks show the advanced robustness and generalization ability of RVT compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S* also achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C and ImageNet-Sketch. The code will be available at {\textbackslash}url\{https://git.io/Jswdk\}.},
	urldate = {2022-03-07},
	journal = {arXiv:2105.07926 [cs]},
	author = {Mao, Xiaofeng and Qi, Gege and Chen, Yuefeng and Li, Xiaodan and Duan, Ranjie and Ye, Shaokai and He, Yuan and Xue, Hui},
	month = may,
	year = {2021},
	note = {arXiv: 2105.07926},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_hierarchical_nodate,
	title = {Hierarchical multimodal transformer to summarize videos {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0925231221015253?token=A480B732FEA04B74A20DDBD67A0CC28A2B91FD9056B487CAF437F4B139AAB37975F505BFA65F58F054634DC87178D63B&originRegion=us-east-1&originCreation=20220307232017},
	language = {en},
	urldate = {2022-03-07},
	doi = {10.1016/j.neucom.2021.10.039},
}

@article{ahmad_transformer-based_2020,
	title = {A {Transformer}-based {Approach} for {Source} {Code} {Summarization}},
	url = {http://arxiv.org/abs/2005.00653},
	abstract = {Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens' position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.},
	urldate = {2022-03-07},
	journal = {arXiv:2005.00653 [cs, stat]},
	author = {Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
	month = may,
	year = {2020},
	note = {arXiv: 2005.00653},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Machine Learning},
}

@inproceedings{zhang_sentiment_2020,
	title = {Sentiment {Analysis} for {Software} {Engineering}: {How} {Far} {Can} {Pre}-trained {Transformer} {Models} {Go}?},
	shorttitle = {Sentiment {Analysis} for {Software} {Engineering}},
	doi = {10.1109/ICSME46990.2020.00017},
	abstract = {Extensive research has been conducted on sentiment analysis for software engineering (SA4SE). Researchers have invested much effort in developing customized tools (e.g., SentiStrength-SE, SentiCR) to classify the sentiment polarity for Software Engineering (SE) specific contents (e.g., discussions in Stack Overflow and code review comments). Even so, there is still much room for improvement. Recently, pre-trained Transformer-based models (e.g., BERT, XLNet) have brought considerable breakthroughs in the field of natural language processing (NLP). In this work, we conducted a systematic evaluation of five existing SA4SE tools and variants of four state-of-the-art pre-trained Transformer-based models on six SE datasets. Our work is the first to fine-tune pre-trained Transformer-based models for the SA4SE task. Empirically, across all six datasets, our fine-tuned pre-trained Transformer-based models outperform the existing SA4SE tools by 6.5-35.6\% in terms of macro/micro-averaged F1 scores.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Zhang, Ting and Xu, Bowen and Thung, Ferdian and Haryono, Stefanus Agus and Lo, David and Jiang, Lingxiao},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Analytical models, Natural Language Processing, Pre-trained Models, Sentiment Analysis, Sentiment analysis, Software Mining, Software engineering, Software maintenance, Systematics, Task analysis, Tools},
	pages = {70--80},
}

@inproceedings{dong_image_2021,
	title = {Image transformer for explainable autonomous driving system},
	doi = {10.1109/ITSC48978.2021.9565103},
	abstract = {In the last decade, deep learning (DL) approaches have been used successfully in computer vision (CV) applications. However, DL-based CV models are generally considered to be black boxes due to their lack of interpretability. This black box behavior has exacerbated user distrust and therefore has prevented widespread deployment DLCV models in autonomous driving tasks even though some of these models exhibit superiority over human performance. For this reason, it is essential to develop explainable DL models for autonomous driving task. Explainable DL models are able to not only boost user trust in autonomy but also serve as a diagnostic approach to identify the defects and weaknesses of the model during the system development phase. In this paper, we propose such an explainable end-to-end autonomous driving system using “Transformer,” a state-of-the-art (SOTA) self-attention based model, to map visual features from images collected by onboard cameras to guide potential driving actions with corresponding explanations. The results demonstrate the efficacy of our proposed model as it outperforms the benchmark model by a significant margin in terms of actions and explanations prediction with lower computational cost.},
	booktitle = {2021 {IEEE} {International} {Intelligent} {Transportation} {Systems} {Conference} ({ITSC})},
	author = {Dong, Jiqian and Chen, Sikai and Zong, Shuya and Chen, Tiantian and Labi, Samuel},
	month = sep,
	year = {2021},
	keywords = {Computational modeling, Computer architecture, Predictive models, Training, Transformers, Vehicular ad hoc networks, Visualization},
	pages = {2732--2737},
}

@inproceedings{prakash_multi-modal_2021,
	address = {Nashville, TN, USA},
	title = {Multi-{Modal} {Fusion} {Transformer} for {End}-to-{End} {Autonomous} {Driving}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578103/},
	doi = {10.1109/CVPR46437.2021.00700},
	abstract = {How should representations from complementary sensors be integrated for autonomous driving? Geometrybased sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in trafﬁc light state can affect the behavior of a vehicle geometrically distant from that trafﬁc light. Geometry alone may therefore be insufﬁcient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling trafﬁc oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efﬁcacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76\% compared to geometry-based fusion.},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
	month = jun,
	year = {2021},
	pages = {7073--7083},
}

@article{yuan_temporal-channel_2021,
	title = {Temporal-{Channel} {Transformer} for {3D} {Lidar}-{Based} {Video} {Object} {Detection} for {Autonomous} {Driving}},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2021.3082763},
	abstract = {The strong demand of autonomous driving in the industry has led to vigorous interest in 3D object detection and resulted in many excellent 3D object detection algorithms. However, the vast majority of algorithms only model single-frame data, ignoring the temporal clue in video sequence. In this work, we propose a new transformer, called Temporal-Channel Transformer (TCTR), to model the temporal-channel domain and spatial-wise relationships for video object detecting from Lidar data. As the special design of this transformer, the information encoded in the encoder is different from that in the decoder. The encoder encodes temporal-channel information of multiple frames while the decoder decodes the spatial-wise information for the current frame in a voxel-wise manner. Specifically, the temporal-channel encoder of the transformer is designed to encode the information of different channels and frames by utilizing the correlation among features from different channels and frames. On the other hand, the spatial decoder of the transformer decodes the information for each location of the current frame. Before conducting the object detection with detection head, a gate mechanism is further deployed for re-calibrating the features of current frame, which filters out the object-irrelevant information by repetitively refining the representation of target frame along with the up-sampling process. Experimental results reveal that TCTR achieves the state-of-the-art performance in grid voxel-based 3D object detection on the nuScenes benchmark.},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Yuan, Zhenxun and Song, Xiao and Bai, Lei and Wang, Zhe and Ouyang, Wanli},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {3D object detection, Correlation, Decoding, Feature extraction, Head, Laser radar, Lidar-based video, Object detection, Temporal-channel attention, Three-dimensional displays, Transformer},
	pages = {1--1},
}

@article{soleymani_construction_2021,
	title = {Construction material classification on imbalanced datasets for construction monitoring automation using {Vision} {Transformer} ({ViT}) architecture},
	url = {http://arxiv.org/abs/2108.09527},
	abstract = {Nowadays, automation is a critical topic due to its significant impacts on the productivity of construction projects. Utilizing automation in this industry brings about great results, such as remarkable improvements in the efficiency, quality, and safety of construction activities. The scope of automation in construction includes a wide range of stages, and monitoring construction projects is no exception. Additionally, it is of great importance in project management since an accurate and timely assessment of project progress enables managers to quickly identify deviations from the schedule and take the required actions at the right time. In this stage, one of the most important tasks is to daily keep track of the project progress, which is very time-consuming and labor-intensive, but automation has facilitated and accelerated this task. It also eliminated or at least decreased the risk of many dangerous tasks. In this way, the first step of construction automation is to detect used materials in a project site automatically. In this paper, a novel deep learning architecture is utilized, called Vision Transformer (ViT), for detecting and classifying construction materials. To evaluate the applicability and performance of the proposed method, it is trained and tested on three large imbalanced datasets, namely Construction Material Library (CML) and Building Material Dataset (BMD), used in the previous papers, as well as a new dataset created by combining them. The achieved results revealed an accuracy of 100 percent in all parameters and also in each material category. It is believed that the proposed method provides a novel and robust tool for detecting and classifying different material types.},
	urldate = {2022-03-07},
	journal = {arXiv:2108.09527 [cs]},
	author = {Soleymani, Maryam and Bonyani, Mahdi and Mahami, Hadi and Nasirzadeh, Farnad},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.09527},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{postnikov_transformer_2021,
	title = {Transformer based trajectory prediction},
	url = {http://arxiv.org/abs/2112.04350},
	abstract = {To plan a safe and efficient route, an autonomous vehicle should anticipate future motions of other agents around it. Motion prediction is an extremely challenging task which recently gained significant attention of the research community. In this work, we present a simple and yet strong baseline for uncertainty aware motion prediction based purely on transformer neural networks, which has shown its effectiveness in conditions of domain change. While being easy-to-implement, the proposed approach achieves competitive performance and ranks 1\${\textasciicircum}\{st\}\$ on the 2021 Shifts Vehicle Motion Prediction Competition.},
	urldate = {2022-03-07},
	journal = {arXiv:2112.04350 [cs]},
	author = {Postnikov, Aleksey and Gamayunov, Aleksander and Ferrer, Gonzalo},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.04350},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{postnikov_transformer_2021-1,
	title = {Transformer based trajectory prediction},
	url = {http://arxiv.org/abs/2112.04350},
	abstract = {To plan a safe and efficient route, an autonomous vehicle should anticipate future motions of other agents around it. Motion prediction is an extremely challenging task which recently gained significant attention of the research community. In this work, we present a simple and yet strong baseline for uncertainty aware motion prediction based purely on transformer neural networks, which has shown its effectiveness in conditions of domain change. While being easy-to-implement, the proposed approach achieves competitive performance and ranks 1\${\textasciicircum}\{st\}\$ on the 2021 Shifts Vehicle Motion Prediction Competition.},
	urldate = {2022-03-07},
	journal = {arXiv:2112.04350 [cs]},
	author = {Postnikov, Aleksey and Gamayunov, Aleksander and Ferrer, Gonzalo},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.04350},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{ivanov_comparative_2021,
	title = {Comparative {Analysis} of {Deep} {Neural} {Networks} {Architectures} for {Visual} {Recognition} in the {Autonomous} {Transport} {Systems}},
	volume = {2096},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/2096/1/012101},
	doi = {10.1088/1742-6596/2096/1/012101},
	abstract = {Abstract
            This paper analyses and presents an experimental investigation of the efficiency of modern models for object recognition in computer vision systems of robotic complexes. In this article, the applicability of transformers for experimental classification problems has been investigated. The comparison results are presented taking into account various limitations specific to robotics. Based on the results of the undertaken studies, recommendations on the use of models in the marine vessels classification problem are proposed},
	language = {en},
	number = {1},
	urldate = {2022-03-07},
	journal = {Journal of Physics: Conference Series},
	author = {Ivanov, Y S and Zhiganov, S V and Liubushkina, N N},
	month = nov,
	year = {2021},
	pages = {012101},
}

@article{kargar_vision_2021,
	title = {Vision {Transformer} for {Learning} {Driving} {Policies} in {Complex} {Multi}-{Agent} {Environments}},
	url = {http://arxiv.org/abs/2109.06514},
	abstract = {Driving in a complex urban environment is a difficult task that requires a complex decision policy. In order to make informed decisions, one needs to gain an understanding of the long-range context and the importance of other vehicles. In this work, we propose to use Vision Transformer (ViT) to learn a driving policy in urban settings with birds-eye-view (BEV) input images. The ViT network learns the global context of the scene more effectively than with earlier proposed Convolutional Neural Networks (ConvNets). Furthermore, ViT's attention mechanism helps to learn an attention map for the scene which allows the ego car to determine which surrounding cars are important to its next decision. We demonstrate that a DQN agent with a ViT backbone outperforms baseline algorithms with ConvNet backbones pre-trained in various ways. In particular, the proposed method helps reinforcement learning algorithms to learn faster, with increased performance and less data than baselines.},
	urldate = {2022-03-07},
	journal = {arXiv:2109.06514 [cs]},
	author = {Kargar, Eshagh and Kyrki, Ville},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.06514},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Robotics},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	urldate = {2022-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@article{kolesnikov_big_2020,
	title = {Big {Transfer} ({BiT}): {General} {Visual} {Representation} {Learning}},
	shorttitle = {Big {Transfer} ({BiT})},
	url = {http://arxiv.org/abs/1912.11370},
	abstract = {Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5\% top-1 accuracy on ILSVRC-2012, 99.4\% on CIFAR-10, and 76.3\% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8\% on ILSVRC-2012 with 10 examples per class, and 97.0\% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.},
	urldate = {2022-03-07},
	journal = {arXiv:1912.11370 [cs]},
	author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
	month = may,
	year = {2020},
	note = {arXiv: 1912.11370},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-03-07},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{noauthor_overview_nodate,
	title = {An overview of {Transformer} {Architectures} in {Computer} {Vision}},
	url = {https://broutonlab.com/blog/broutonlab.com/blog/an-overview-of-transformer-architectures-in-computer-vision},
	abstract = {In the article, we explore novel vision transformers architectures and their application to сomputer vision problems.},
	language = {en},
	urldate = {2022-03-07},
	journal = {BroutonLab},
}

@article{khan_transformers_2022,
	title = {Transformers in {Vision}: {A} {Survey}},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Transformers in {Vision}},
	url = {http://arxiv.org/abs/2101.01169},
	doi = {10.1145/3505244},
	abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
	urldate = {2022-03-07},
	journal = {ACM Computing Surveys},
	author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
	month = jan,
	year = {2022},
	note = {arXiv: 2101.01169},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {3505244},
}

@misc{noauthor_elsevier_nodate,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0743731518308773?token=79A9BBCFF621EC1AEB097349D1692BED717F04D949C9517DBF01B885A909258CC6C2CC0A9E8293E1C3409648722BAC5F&originRegion=us-east-1&originCreation=20220304021905},
	language = {en},
	urldate = {2022-03-04},
	doi = {10.1016/j.jpdc.2019.07.007},
}

@article{han_ghostnet_2020,
	title = {{GhostNet}: {More} {Features} from {Cheap} {Operations}},
	shorttitle = {{GhostNet}},
	url = {http://arxiv.org/abs/1911.11907},
	abstract = {Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. \$75.7{\textbackslash}\%\$ top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at https://github.com/huawei-noah/ghostnet},
	urldate = {2022-02-28},
	journal = {arXiv:1911.11907 [cs]},
	author = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
	month = mar,
	year = {2020},
	note = {arXiv: 1911.11907},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{redmon_yolo9000_2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO9000}},
	url = {http://arxiv.org/abs/1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	urldate = {2022-02-28},
	journal = {arXiv:1612.08242 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.08242},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{miller_empirical_1990,
	title = {An empirical study of the reliability of {UNIX} utilities},
	volume = {33},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/96267.96279},
	doi = {10.1145/96267.96279},
	abstract = {Operating system facilities, such as the kernel and utility programs, are typically assumed to be reliable. In our recent experiments, we have been able to crash 25-33\% of the utility programs on any version of UNIX that was tested. This report describes these tests and an analysis of the program bugs that caused the crashes.},
	language = {en},
	number = {12},
	urldate = {2022-02-24},
	journal = {Communications of the ACM},
	author = {Miller, Barton P. and Fredriksen, Louis and So, Bryan},
	month = dec,
	year = {1990},
	pages = {32--44},
}

@inproceedings{chen_performance_2018,
	address = {Taipei City, Taiwan},
	title = {Performance {Evaluation} of {Edge} {Computing}-{Based} {Deep} {Learning} {Object} {Detection}},
	isbn = {978-1-4503-6553-6},
	url = {http://dl.acm.org/citation.cfm?doid=3301326.3301369},
	doi = {10.1145/3301326.3301369},
	abstract = {This article presents a method for implementing the deep learning object detection based on a low-cost edge computing IoT device. The limit of the hardware is a challenge for working the pretrained neural network model on a low-cost IoT device. Hence, we utilize the Neural Compute Stick (NCS) to accelerate the neural network model on a low-cost IoT device by its high efficiency floating-point operation. With the NCS, the low-cost IoT device can successfully work the pre-trained neural network model and become an edge computing device. The experimental results show the proposed method can effectively detect the objects based on deep learning on an edge computing IoT device. Furthermore, the objective experiment demonstrates the proposed method can immediately infer the neural network model for images in average 1.7 seconds with only one of the NCS and the neural network model can reach average 9.2 fps for the video sequences with four NCSs acceleration. In addition, the discrepancy of the neural network model between the edge device and the edge server is less than 2\% mean average precision (mAP).},
	language = {en},
	urldate = {2022-02-24},
	booktitle = {Proceedings of the 2018 {VII} {International} {Conference} on {Network}, {Communication} and {Computing} - {ICNCC} 2018},
	publisher = {ACM Press},
	author = {Chen, Chuan-Wen and Ruan, Shanq-Jang and Lin, Chang-Hong and Hung, Chun-Chi},
	year = {2018},
	pages = {40--43},
}

@inproceedings{tuli_edgelens_2019,
	title = {{EdgeLens}: {Deep} {Learning} based {Object} {Detection} in {Integrated} {IoT}, {Fog} and {Cloud} {Computing} {Environments}},
	shorttitle = {{EdgeLens}},
	doi = {10.1109/ISCON47742.2019.9036216},
	abstract = {Data-intensive applications are growing at an increasing rate and there is a growing need to solve scalability and high-performance issues in them. By the advent of Cloud computing paradigm, it became possible to harness remote resources to build and deploy these applications. In recent years, new set of applications and services based on Internet of Things (IoT) paradigm, require to process large amount of data in very less time. Among them surveillance and object detection have gained prime importance, but cloud is unable to bring down the network latencies to meet the response time requirements. This problem is solved by Fog computing which harnesses resources in the edge of the network along with remote cloud resources as required. However, there is still a lack of frameworks that are successfully able to integrate sophisticated software and applications, especially deep learning, with fog and cloud computing environments. In this work, we propose a framework to deploy deep learning-based applications in fog-cloud environments to harness edge and cloud resources to provide better service quality for such applications. Our proposed framework, called EdgeLens, adapts to the application or user requirements to provide high accuracy or low latency modes of services. We also tested the performance of the software in terms of accuracy, response time, jitter, network bandwidth and power consumption and show how EdgeLens adapts to different service requirements.},
	booktitle = {2019 4th {International} {Conference} on {Information} {Systems} and {Computer} {Networks} ({ISCON})},
	author = {Tuli, Shreshth and Basumatary, Nipam and Buyya, Rajkumar},
	year = {2019},
	keywords = {Cloud computing, Computational modeling, Deep Learning, Deep learning, Fog computing, Image edge detection, Internet of Things, Logic gates, Object Detection, Object detection, Task analysis},
	pages = {496--502},
}

@article{muhawenayo_compressed_2021,
	title = {Compressed {Object} {Detection}},
	url = {http://arxiv.org/abs/2102.02896},
	abstract = {Deep learning approaches have achieved unprecedented performance in visual recognition tasks such as object detection and pose estimation. However, state-of-the-art models have millions of parameters represented as floats which make them computationally expensive and constrain their deployment on hardware such as mobile phones and IoT nodes. Most commonly, activations of deep neural networks tend to be sparse thus proving that models are over parametrized with redundant neurons. Model compression techniques, such as pruning and quantization, have recently shown promising results by improving model complexity with little loss in performance. In this work, we extended pruning, a compression technique that discards unnecessary model connections, and weight sharing techniques for the task of object detection. With our approach, we are able to compress a state-of-the-art object detection model by 30.0\% without a loss in performance. We also show that our compressed model can be easily initialized with existing pre-trained weights, and thus is able to fully utilize published state-of-the-art model zoos.},
	urldate = {2022-02-23},
	journal = {arXiv:2102.02896 [cs, eess]},
	author = {Muhawenayo, Gedeon and Gkioxari, Georgia},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.02896},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{yang_method_2017,
	title = {A method to estimate the energy consumption of deep neural networks},
	doi = {10.1109/ACSSC.2017.8335698},
	abstract = {Deep Neural Networks (DNNs) have enabled state-of-the-art accuracy on many challenging artificial intelligence tasks. While most of the computation currently resides in the cloud, it is desirable to embed DNN processing locally near the sensor due to privacy, security, and latency concerns or limitations in communication bandwidth. Accordingly, there has been increasing interest in the research community to design energy-efficient DNNs. However, estimating energy consumption from the DNN model is much more difficult than other metrics such as storage cost (model size) and throughput (number of operations). This is due to the fact that a significant portion of the energy is consumed by data movement, which is difficult to extract directly from the DNN model. This work proposes an energy estimation methodology that can estimate the energy consumption of a DNN based on its architecture, sparsity, and bitwidth. This methodology can be used to evaluate the various DNN architectures and energy-efficient techniques that are currently being proposed in the field and guide the design of energy-efficient DNNs. We have released an online version of the energy estimation tool at energyestimation.mit.edu. We believe that this method will play a critical role in bridging the gap between algorithm and hardware design and provide useful insights for the development of energy-efficient DNNs.},
	booktitle = {2017 51st {Asilomar} {Conference} on {Signals}, {Systems}, and {Computers}},
	author = {Yang, Tien-Ju and Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
	year = {2017},
	note = {ISSN: 2576-2303},
	keywords = {Deep learning, Energy consumption, Estimation, Hardware, Measurement, Memory management, Neural networks, Optimization, deep neural network, energy estimation, energy metric, machine learning},
	pages = {1916--1920},
}

@article{zou_object_2019,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	shorttitle = {Object {Detection} in 20 {Years}},
	url = {http://arxiv.org/abs/1905.05055},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	urldate = {2022-02-23},
	journal = {arXiv:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{sinha_deep_2018,
	title = {Deep {Learning} {For} {Computer} {Vision} {Tasks}: {A} review},
	shorttitle = {Deep {Learning} {For} {Computer} {Vision} {Tasks}},
	url = {http://arxiv.org/abs/1804.03928},
	abstract = {Deep learning has recently become one of the most popular sub-fields of machine learning owing to its distributed data representation with multiple levels of abstraction. A diverse range of deep learning algorithms are being employed to solve conventional artificial intelligence problems. This paper gives an overview of some of the most widely used deep learning algorithms applied in the field of computer vision. It first inspects the various approaches of deep learning algorithms, followed by a description of their applications in image classification, object identification, image extraction and semantic segmentation in the presence of noise. The paper concludes with the discussion of the future scope and challenges for construction and training of deep neural networks.},
	urldate = {2022-02-23},
	journal = {arXiv:1804.03928 [cs]},
	author = {Sinha, Rajat Kumar and Pandey, Ruchi and Pattnaik, Rohan},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.03928},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{Schmidhuber2015DLinNN,
	title = {Deep learning in neural networks: {An} overview},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	year = {2015},
}

@inproceedings{jose_real-time_2019,
	address = {Seoul, Korea (South)},
	title = {Real-{Time} {Object} {Detection} {On} {Low} {Power} {Embedded} {Platforms}},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9021990/},
	doi = {10.1109/ICCVW.2019.00304},
	abstract = {Low power real-time object detection is an interesting application in deep learning with applications in smart wearables, Advanced Driver Assistance Systems (ADAS), drone surveillance systems, etc. In this paper, we discuss the limitations with existing networks and enumerate the various factors to keep in mind while designing neural networks for a target hardware. Based on our experience of working with TI embedded platform, we provide a systematic approach for designing real time object detection networks on low power embedded platforms. First stage involves identifying the optimal layers for the hardware, by understanding it’s computational and memory limitations. The next step is to use these layers to come up with a basic building block that has low computational complexity. The ﬁnal stage involves using model compression techniques like sparsiﬁcation/quantization to accelerate the inference process. Based on this design approach, we were able to come up with a low latency object detection model HX-LPNet that operates at 22 FPS on low power TDA2PX System on Chip(SoC) provided by Texas Instruments (TI).},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	publisher = {IEEE},
	author = {Jose, George and Kumar, Aashish and Kruthiventi, Srinivas and Saha, Sambuddha and Muralidhara, Harikrishna},
	month = oct,
	year = {2019},
	pages = {2485--2492},
}

@article{liu_deep_2020,
	title = {Deep {Learning} for {Generic} {Object} {Detection}: {A} {Survey}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Deep {Learning} for {Generic} {Object} {Detection}},
	url = {https://link.springer.com/10.1007/s11263-019-01247-4},
	doi = {10.1007/s11263-019-01247-4},
	abstract = {Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predeﬁned categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the ﬁeld of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this ﬁeld brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We ﬁnish the survey by identifying promising directions for future research.},
	language = {en},
	number = {2},
	urldate = {2022-02-22},
	journal = {International Journal of Computer Vision},
	author = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietikäinen, Matti},
	month = feb,
	year = {2020},
	pages = {261--318},
}

@inproceedings{singh_leveraging_2020,
	address = {Snowmass Village, CO, USA},
	title = {Leveraging {Filter} {Correlations} for {Deep} {Model} {Compression}},
	isbn = {978-1-72816-553-0},
	url = {https://ieeexplore.ieee.org/document/9093331/},
	doi = {10.1109/WACV45572.2020.9093331},
	abstract = {We present a ﬁlter correlation based model compression approach for deep convolutional neural networks. Our approach iteratively identiﬁes pairs of ﬁlters with the largest pairwise correlations and drops one of the ﬁlters from each such pair. However, instead of discarding one of the ﬁlters from each such pair na¨ıvely, the model is re-optimized to make the ﬁlters in these pairs maximally correlated, so that discarding one of the ﬁlters from the pair results in minimal information loss. Moreover, after discarding the ﬁlters in each round, we further ﬁnetune the model to recover from the potential small loss incurred by the compression. We evaluate our proposed approach using a comprehensive set of experiments and ablation studies. Our compression method yields state-of-the-art FLOPs compression rates on various benchmarks, such as LeNet-5, VGG-16, and ResNet-50,56, while still achieving excellent predictive performance for tasks such as object detection on benchmark datasets.},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Singh, Pravendra and Verma, Vinay Kumar and Rai, Piyush and Namboodiri, Vinay P.},
	month = mar,
	year = {2020},
	pages = {824--833},
}

@inproceedings{hu_2020_2021,
	title = {The 2020 {Low}-{Power} {Computer} {Vision} {Challenge}},
	doi = {10.1109/AICAS51828.2021.9458522},
	abstract = {AI computer vision has advanced significantly in recent years. IoT and edge computing devices such as mobile phones have become the primary computing platform for many end users. Mobile devices such as robots and drones that rely on batteries demand for energy efficient computation. Since 2015, the IEEE Annual International Low-Power Computer Vision Challenge (LPCVC) was held to identify energy-efficient AI and computer vision solutions. The 2020 LPCVC includes three challenge tracks: (1) PyTorch UAV Video Track, (2) FPGA Image Track, and (3) On-device Visual Intelligence Competition (OVIC) Tenforflow Track. This paper summarizes the 2020 winning solutions from the three tracks of LPCVC competitions. Methods and future directions for energy-efficient AI and computer vision research are discussed.},
	booktitle = {2021 {IEEE} 3rd {International} {Conference} on {Artificial} {Intelligence} {Circuits} and {Systems} ({AICAS})},
	author = {Hu, Xiao and Chang, Ming-Ching and Chen, Yuwei and Sridhar, Rahul and Hu, Zhenyu and Xue, Yunhe and Wu, Zhenyu and Pi, Pengcheng and Shen, Jiayi and Tan, Jianchao and Lian, Xiangru and Liu, Ji and Wang, Zhangyang and Liu, Chia-Hsiang and Han, Yu-Shin and Sung, Yuan-Yao and Lee, Yi and Wu, Kai-Chiang and Guo, Wei-Xiang and Lee, Rick and Liang, Shengwen and Wang, Zerun and Ding, Guiguang and Zhang, Gang and Xi, Teng and Chen, Yubei and Cai, Han and Zhu, Ligeng and Zhang, Zhekai and Han, Song and Jeong, Seonghwan and Kwon, YoungMin and Wang, Tianzhe and Pan, Jeffery},
	month = jun,
	year = {2021},
	keywords = {Artificial intelligence, Computational efficiency, Computer vision, Conferences, Energy efficiency, FPGA, Low-power, Mobile handsets, NAS, Visualization, challenge, computer vision, drone, knowledge distilling, model compression, scene text},
	pages = {1--4},
}

@inproceedings{davis_testing_2019,
	address = {San Diego, CA, USA},
	title = {Testing {Regex} {Generalizability} {And} {Its} {Implications}: {A} {Large}-{Scale} {Many}-{Language} {Measurement} {Study}},
	isbn = {978-1-72812-508-4},
	shorttitle = {Testing {Regex} {Generalizability} {And} {Its} {Implications}},
	url = {https://ieeexplore.ieee.org/document/8952443/},
	doi = {10.1109/ASE.2019.00048},
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and ﬁndings to date are typically generalized from regexes written in only 1–2 programming languages. This is an incomplete foundation.},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Davis, James C and Moyer, Daniel and Kazerouni, Ayaan M and Lee, Dongyoon},
	month = nov,
	year = {2019},
	pages = {427--439},
}

@article{cheng_survey_2020,
	title = {A {Survey} of {Model} {Compression} and {Acceleration} for {Deep} {Neural} {Networks}},
	abstract = {Deep neural networks (DNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. During the past five years, tremendous progress has been made in this area. In this paper, we review the recent techniques for compacting and accelerating DNN models. In general, these techniques are divided into four categories: parameter pruning and quantization, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and quantization are described first, after that the other techniques are introduced. For each category, we also provide insightful analysis about the performance, related applications, advantages, and drawbacks. Then we go through some very recent successful methods, for example, dynamic capacity networks and stochastic depths networks. After that, we survey the evaluation matrices, the main datasets used for evaluating the model performance, and recent benchmark efforts. Finally, we conclude this paper, discuss remaining the challenges and possible directions for future work.},
	journal = {arXiv},
	author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wu_iou-aware_2020,
	title = {{IoU}-aware single-stage object detector for accurate localization},
	volume = {97},
	issn = {02628856},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0262885620300433},
	doi = {10.1016/j.imavis.2020.103911},
	abstract = {Single-stage object detectors have been widely applied in many computer vision applications due to their simpleness and high efﬁciency. However, the low correlation between the classiﬁcation score and localization accuracy in detection results severely hurts the average precision of the detection model. To solve this problem, an IoUaware single-stage object detector is proposed in this paper. Speciﬁcally, IoU-aware single-stage object detector predicts the IoU for each detected box. Then the predicted IoU is multiplied by the classiﬁcation score to compute the ﬁnal detection conﬁdence, which is more correlated with the localization accuracy. The detection conﬁdence is then used as the input of the subsequent NMS and COCO AP computation, which substantially improves the localization accuracy of model. Sufﬁcient experiments on COCO and PASCOL VOC datasets demonstrate the effectiveness of IoU-aware single-stage object detector on improving model's localization accuracy. Without whistles and bells, the proposed method can substantially improve AP by 1.7\%–1.9\% and AP75 by 2.2\%–2.5\% on COCO testdev. And it can also substantially improve AP by 2.9\%–4.4\% and AP80, AP90 by 4.6\%–10.2\% on PASCAL VOC. The source code will be made publicly available.},
	language = {en},
	urldate = {2022-02-18},
	journal = {Image and Vision Computing},
	author = {Wu, Shengkai and Li, Xiaoping and Wang, Xinggang},
	month = may,
	year = {2020},
	pages = {103911},
}

@article{long_pp-yolo_2020,
	title = {{PP}-{YOLO}: {An} {Effective} and {Efficient} {Implementation} of {Object} {Detector}},
	shorttitle = {{PP}-{YOLO}},
	url = {http://arxiv.org/abs/2007.12099},
	abstract = {Object detection is one of the most important areas in computer vision, which plays a key role in various practical scenarios. Due to limitation of hardware, it is often necessary to sacrifice accuracy to ensure the infer speed of the detector in practice. Therefore, the balance between effectiveness and efficiency of object detector must be considered. The goal of this paper is to implement an object detector with relatively balanced effectiveness and efficiency that can be directly applied in actual application scenarios, rather than propose a novel detection model. Considering that YOLOv3 has been widely used in practice, we develop a new object detector based on YOLOv3. We mainly try to combine various existing tricks that almost not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Since all experiments in this paper are conducted based on PaddlePaddle, we call it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better balance between effectiveness (45.2\% mAP) and efficiency (72.9 FPS), surpassing the existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source code is at https://github.com/PaddlePaddle/PaddleDetection.},
	urldate = {2022-02-18},
	journal = {arXiv:2007.12099 [cs]},
	author = {Long, Xiang and Deng, Kaipeng and Wang, Guanzhong and Zhang, Yang and Dang, Qingqing and Gao, Yuan and Shen, Hui and Ren, Jianguo and Han, Shumin and Ding, Errui and Wen, Shilei},
	month = aug,
	year = {2020},
	note = {arXiv: 2007.12099
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{adelson_problem_1981,
	title = {Problem solving and the development of abstract categories in programming languages},
	volume = {9},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03197568},
	doi = {10.3758/BF03197568},
	language = {en},
	number = {4},
	urldate = {2022-02-16},
	journal = {Memory \& Cognition},
	author = {Adelson, Beth},
	month = jul,
	year = {1981},
	pages = {422--433},
}

@article{adelson_when_nodate,
	title = {When novices surpass experts: {The} difficulty of a task may increase with expertise.},
	volume = {10},
	issn = {1939-1285},
	shorttitle = {When novices surpass experts},
	url = {https://psycnet.apa.org/fulltext/1985-11314-001.pdf},
	doi = {10.1037/0278-7393.10.3.483},
	number = {3},
	urldate = {2022-02-16},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Adelson, Beth},
	note = {Publisher: US: American Psychological Association},
	pages = {483},
}

@inproceedings{louis_where_2020,
	address = {Seoul South Korea},
	title = {Where should {I} comment my code?: a dataset and model for predicting locations that need comments},
	isbn = {978-1-4503-7126-1},
	shorttitle = {Where should {I} comment my code?},
	url = {https://dl.acm.org/doi/10.1145/3377816.3381736},
	doi = {10.1145/3377816.3381736},
	abstract = {Programmers should write code comments, but not on every line of code. We have created a machine learning model that suggests locations where a programmer should write a code comment. We trained it on existing commented code to learn locations that are chosen by developers. Once trained, the model can predict locations in new code. Our models achieved precision of 74\% and recall of 13\% in identifying commentworthy locations. This first success opens the door to future work, both in the new where-to-comment problem and in guiding comment generation. Our code and data is available at http://groups.inf.ed.ac.uk/cup/comment-locator/.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results}},
	publisher = {ACM},
	author = {Louis, Annie and Dash, Santanu Kumar and Barr, Earl T. and Ernst, Michael D. and Sutton, Charles},
	month = jun,
	year = {2020},
	pages = {21--24},
}

@inproceedings{lin_feature_2017,
	address = {Honolulu, HI},
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099589/},
	doi = {10.1109/CVPR.2017.106},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows signiﬁcant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	language = {en},
	urldate = {2022-02-11},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = jul,
	year = {2017},
	pages = {936--944},
}

@article{noauthor_notitle_nodate,
}

@misc{MLOpsWorkflow,
	title = {Machine {Learning} {Operations}},
	url = {https://ml-ops.org/},
	abstract = {Machine Learning Operations},
	language = {en-us},
	urldate = {2021-12-08},
	year = {2021},
}

@inproceedings{Redmon2018YOLO,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016},
}

@inproceedings{Thung2012BugsinMLSystems,
	title = {An empirical study of bugs in machine learning systems},
	abstract = {Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and OpenNLP, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6\% of the bugs belong to the algorithm/method category, 15.6\% of the bugs belong to the non-functional category, and 13\% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research. © 2012 IEEE.},
	booktitle = {International {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Thung, Ferdian and Wang, Shaowei and Lo, David and Jiang, Lingxiao},
	year = {2012},
}

@inproceedings{Tatman2018TaxonomyofReproducibility4MLResearch,
	title = {A {Practical} {Taxonomy} of {Reproducibility} for {Machine} {Learning} {Research}},
	booktitle = {Reproducibility in {Machine} {Learning} {Workshop} at {ICML}},
	author = {Tatman, Rachael and Vanderplas, Jake and Dane, Sohier},
	year = {2018},
}

@inproceedings{Nikanjam2021DesignSmellinDL,
	title = {Design {Smells} in {Deep} {Learning} {Programs}: {An} {Empirical} {Study}},
	shorttitle = {Design {Smells} in {Deep} {Learning} {Programs}},
	abstract = {Nowadays, we are witnessing an increasing adoption of Deep Learning (DL) based software systems in many industries. Designing a DL program requires constructing a deep neural network (DNN) and then training it on a dataset. This process requires that developers make multiple architectural (e.g., type, size, number, and order of layers) and configuration (e.g., optimizer, regularization methods, and activation functions) choices that affect the quality of the DL models, and consequently software quality. An under-specified or poorly-designed DL model may train successfully but is likely to perform poorly when deployed in production. Design smells in DL programs are poor design and-or configuration decisions taken during the development of DL components, that are likely to have a negative impact on the performance (i.e., prediction accuracy) and then quality of DL based software systems. In this paper, we present a catalogue of 8 design smells for a popular DL architecture, namely deep Feedforward Neural Networks which is widely employed in industrial applications. The design smells were identified through a review of the existing literature on DL design and a manual inspection of 659 DL programs with performance issues and design inefficiencies. The smells are specified by describing their context, consequences, and recommended refactorings. To provide empirical evidence on the relevance and perceived impact of the proposed design smells, we conducted a survey with 81 DL developers. In general, the developers perceived the proposed design smells as reflective of design or implementation problems, with agreement levels varying between 47{\textbackslash}\% and 68{\textbackslash}\%.},
	booktitle = {{IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Nikanjam, Amin and Khomh, Foutse},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{pineau_reproducible_2018,
	title = {Reproducible, {Reusable}, and {Robust} {Reinforcement} {Learning}},
	url = {https://media.neurips.cc/Conferences/NIPS2018/Slides/jpineau-NeurIPS-dec18-fb.pdf},
	journal = {Presentation in 31st Neural Information Processing Systems},
	author = {Pineau, Joelle},
	year = {2018},
}

@inproceedings{Kumar2017DataManagementinML,
	title = {Data {Management} in {Machine} {Learning}: {Challenges}, {Techniques}, and {Systems}},
	shorttitle = {Data {Management} in {Machine} {Learning}},
	abstract = {Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.},
	booktitle = {International {Conference} on {Management} of {Data}},
	author = {Kumar, Arun and Boehm, Matthias and Yang, Jun},
	year = {2017},
}

@inproceedings{VOCDataset,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	booktitle = {International {Journal} of {Computer} {Vision} ({IJCV})},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	year = {2010},
}

@inproceedings{Robert2007DifferentialTesting,
	title = {Differential {Testing} {A} new approach to change detection},
	abstract = {Regression testing, as it’s commonly practiced, is unsound due to inconsistent test repair and test addition. This paper presents a new technique, differential testing, that alleviates the test repair problem and detects more changes than regression testing alone. Differential testing works by creating test suites for both the original system and the modified system and contrasting both versions of the system with these two suites.},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Evans, Robert B and Savoia, Alberto},
	year = {2007},
}

@article{Braiek2020c,
	title = {On testing machine learning programs},
	abstract = {Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs.},
	journal = {Journal of Systems and Software (JSS)},
	author = {Braiek, Houssem Ben and Khomh, Foutse},
	year = {2020},
	keywords = {Data cleaning, Feature engineering testing, Implementation testing, Machine learning, Model testing},
}

@misc{CMUMLReproducibility,
	title = {Reproducibility},
	url = {https://blog.ml.cmu.edu/2020/08/31/5-reproducibility/},
	urldate = {2021-12-08},
	author = {Ding, Zihao and Reddy, Aniketh and Joshi, Aparna},
	year = {2021},
}

@inproceedings{Boehm2010ChangingNatureofSWEvolution,
	title = {The changing nature of software evolution; {The} inevitability of evolution},
	abstract = {In "The Changing Nature of Software Evolution," Barry Boehm discusses how a one-size-fits-all approach no longer works and how different types of software now require different types of approaches. In "The Inevitability of Evolution," Kent Beck describes how developers must factor in cost, time, and risk when considering changes to their software.},
	booktitle = {{IEEE} {Software}},
	author = {Boehm, Barry and Beck, Kent},
	year = {2010},
	keywords = {Costs, Lead, Marine vehicles, Programming, development, software change, software engineering, software evolution},
}

@article{han_pre-trained_2021,
	title = {Pre-{Trained} {Models}: {Past}, {Present} and {Future}},
	issn = {26666510},
	shorttitle = {Pre-{Trained} {Models}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651021000231},
	doi = {10.1016/j.aiopen.2021.08.002},
	language = {en},
	urldate = {2022-02-10},
	journal = {AI Open},
	author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
	month = aug,
	year = {2021},
	pages = {S2666651021000231},
}

@inproceedings{you_logme_2021,
	title = {{LogME}: {Practical} {Assessment} of {Pre}-trained {Models} for {Transfer} {Learning}},
	shorttitle = {{LogME}},
	url = {https://proceedings.mlr.press/v139/you21b.html},
	abstract = {This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo {\textbackslash}emph\{without fine-tuning\}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is {\textbackslash}emph\{immune to over-fitting\}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is {\textbackslash}emph\{fast, accurate, and general\}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most \$3000{\textbackslash}times\$ speedup in wall-clock time and requires only \$1\%\$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: {\textbackslash}href\{https://github.com/thuml/LogME\}\{https://github.com/thuml/LogME\}.},
	language = {en},
	urldate = {2022-02-10},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {You, Kaichao and Liu, Yong and Wang, Jianmin and Long, Mingsheng},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12133--12143},
}

@inproceedings{Bahdanau2015,
	title = {Neural machine translation by jointly learning to align and translate},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
	year = {2015},
}

@inproceedings{Bosu2013DataQualityinESE,
	title = {Data quality in empirical software engineering: a targeted review},
	abstract = {Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221 relevant studies met our inclusion criteria and were characterized in terms of their consideration and treatment of data quality. Results: We obtained useful insights as to how the ESE community considers these three elements of data quality. Only 23 of these 221 studies reported on all three elements of data quality considered in this paper. Conclusion: The reporting of data collection procedures is not documented consistently in ESE studies. It will be useful if data collection challenges are reported in order to improve our understanding of why there are problems with software engineering data sets and the models developed from them. More generally, data quality should be given far greater attention by the community. The improvement of data sets through enhanced data collection, pre-processing and quality assessment should lead to more reliable prediction models, thus improving the practice of software engineering.},
	booktitle = {International {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	author = {Bosu, Michael Franklin and MacDonell, Stephen G},
	year = {2013},
}

@inproceedings{Destefanis2018AffectsofGithubIssuesCommenters,
	title = {On measuring affects of github issues' commenters},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Destefanis, Giuseppe and Ortu, Marco and Bowes, David and Marchesi, Michele and Tonelli, Roberto},
	year = {2018},
}

@inproceedings{Eghbali2020String-relatedBugs,
	title = {No strings attached: an empirical study of string-related software bugs},
	abstract = {Strings play many roles in programming because they often contain complex and semantically rich information. For example, programmers use strings to filter inputs via regular expression matching, to express the names of program elements accessed through some form of reflection, to embed code written in another formal language, and to assemble textual output produced by a program. The omnipresence of strings leads to a wide range of mistakes that developers may make, yet little is currently known about these mistakes. The lack of knowledge about string-related bugs leads to developers repeating the same mistakes again and again, and to poor support for finding and fixing such bugs. This paper presents the first empirical study of the root causes, consequences, and other properties of string-related bugs. We systematically study 204 string-related bugs in a diverse set of projects written in JavaScript, a language where strings play a particularly important role. Our findings include (i) that many string-related mistakes are caused by a recurring set of root cause patterns, such as incorrect string literals and regular expressions, (ii) that string-related bugs have a diverse set of consequences, including incorrect output or silent omission of expected behavior, (iii) that fixing string-related bugs often requires changing just a single line, with many of the required repair ingredients available in the surrounding code, (iv) that stringrelated bugs occur across all parts of applications, including the core components, and (v) that almost none of these bugs are detected by existing static analyzers. Our findings not only show the importance and prevalence of string-related bugs, but they help developers to avoid common mistakes and tool builders to tackle the challenge of finding and fixing string-related bugs.},
	booktitle = {International {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Eghbali, Aryaz and Pradel, Michael},
	year = {2020},
}

@inproceedings{Ehrlinger2020DaQLDataQualityinMLApp,
	title = {A {DaQL} to {Monitor} {Data} {Quality} in {Machine} {Learning} {Applications}},
	abstract = {Machine learning models can only be as good as the data used to train them. Despite this obvious correlation, there is little research about data quality measurement to ensure the reliability and trustworthiness of machine learning models. Especially in industrial settings, where sensors produce large amounts of highly volatile data, a one-time measurement of the data quality is not sufficient since errors in new data should be detected as early as possible. Thus, in this paper, we present DaQL (Data Quality Library), a generally-applicable tool to continuously monitor the quality of data to increase the prediction accuracy of machine learning models. We demonstrate and evaluate DaQL within an industrial real-world machine learning application at Siemens.},
	booktitle = {International {Conference} on {Database} and {Expert} {Systems} {Applications}},
	author = {Ehrlinger, Lisa and Haunschmid, Verena and Palazzini, Davide and Lettner, Christian},
	year = {2019},
	keywords = {Data quality, Machine learning, Trust},
}

@article{Gundersen2018onReproducibleAI,
	title = {On reproducible {AI}: {Towards} reproducible research, open science, and digital scholarship in {AI} publications},
	abstract = {Artificial intelligence, like any science, must rely on reproducible experiments to validate results. Our objective is to give practical and pragmatic recommendations for how to document AI research so that results are reproducible. Our analysis of the literature shows that AI publications currently fall short of providing enough documentation to facilitate reproducibility. Our suggested best practices are based on a framework for reproducibility and recommendations for best practices given by scientific organizations, scholars, and publishers. We have made a reproducibility checklist based on our investigation and described how every item in the checklist can be documented by authors and examined by reviewers. We encourage authors and reviewers to use the suggested best practices and author checklist when considering submissions for AAAI publications and conferences.},
	journal = {AI Magazine},
	author = {Gundersen, Odd Erik and Gil, Yolanda and Aha, David W.},
	year = {2018},
	keywords = {Gundersun2018},
}

@inproceedings{Jarzabek1993SWReengineering4Reusability,
	title = {Software reengineering for reusability},
	abstract = {Programs are often reengineered for better maintainability or in order to migrate programs into newer computer/software platforms. However, many of the aging business systems must be also upgraded in order to meet strategic goals of an organization. To meet such ambitious objectives, we must fundamentally redesign programs, rather than merely restructure them for improved maintainability. When much program re-design is involved, the reengineering option becomes challenging at the technical level, expensive and risky. To increase the value of the reengineering solution, we address reusability issues in the context of reengineering. In this paper, we discuss lifecycle phases and outline a possible technical scenario for reengineering for reusability.{\textless}{\textgreater}},
	booktitle = {International {Computer} {Software} and {Applications} {Conference} ({COMPSAC})},
	author = {Jarzabek, S.},
	year = {1993},
	keywords = {Aging, Companies, Computer interfaces, Computer science, Costs, Databases, Electronic mail, Information systems, Maintenance, Software reusability},
}

@article{MixedMethodsResearch,
	title = {Mixed {Methods} {Research}: {A} {Research} {Paradigm} {Whose} {Time} {Has} {Come}},
	abstract = {The purposes of this article are to position mixed methods research (mixed research is a synonym) as the natural complement to traditional qualitative and quantitative research, to present pragmatism as offering an attractive philosophical partner for mixed methods research, and to provide a framework for designing and conducting mixed methods research. In doing this, we briefly review the paradigm ?wars? and incompatibility thesis, we show some commonalities between quantitative and qualitative research, we explain the tenets of pragmatism, we explain the fundamental principle of mixed research and how to apply it, we provide specific sets of designs for the two major types of mixed methods research (mixed-model designs and mixed-method designs), and, finally, we explain mixed methods research as following (recursively) an eight-step process. A key feature of mixed methods research is its methodological pluralism or eclecticism, which frequently results in superior research (compared to monomethod research). Mixed methods research will be successful as more investigators study and help advance its concepts and as they regularly practice it.},
	journal = {Educational Researcher},
	author = {Johnson, R. Burke and Onwuegbuzie, Anthony J.},
	year = {2004},
}

@article{jordan2015ML,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	shorttitle = {Machine learning},
	abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of ...},
	journal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	year = {2015},
}

@article{SEReengineering,
	title = {Software {Re}-engineering},
	abstract = {The essence of software re-engineering is to improve or transform existing software so that it can be understand, controlled, and used anew. The need for software re-engineering has increased greatly, as heritage software systems have become obsolescent in terms of their architecture, the platforms on which they run, and their suitability and stability to support evolution to support changing needs. Software re-engineering is important for recovering and reusing existing software assets, putting high software maintenance costs under control, and establishing a base for future software evolution. The growth in cost and importance of software to NASA, and the aging of many of the Agency's important software systems, has necessitated software reengineering efforts. This technical report is designed to give the reader an overview of the concepts, approaches and risks of re-engineering. It is intended to serve as a basis for understanding software reengineering technology. The information is primarily compiled from experts referenced at the end of the report, modified by approaches taken by NASA Projects. Section 8, however is new. It discusses Hybrid Re-engineering, an approach used by some NASA projects to combine the use of Commercial-Off-The-Shelf (COTS) software with new software development. The technical reports concludes with some industry lessons learned. 1.},
	journal = {Software Assurance Technology Center},
	author = {Linda, Dr and Rosenberg, H. and Hyatt, Lawrence E.},
	year = {1996},
}

@article{Nguyen2019MLDLFrameworksLibraries4LargeScaleDataMining,
	title = {Machine {Learning} and {Deep} {Learning} frameworks and libraries for large-scale data mining: a survey},
	abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.},
	journal = {Artificial Intelligence Review},
	author = {Nguyen, Giang and Dlugolinsky, Stefan and Bobák, Martin and Tran, Viet and García, Álvaro López and Heredia, Ignacio and Malík, Peter and Hluchý, Ladislav},
	year = {2019},
	keywords = {Artificial Intelligence software, Deep Learning, Graphics processing unit (GPU), Intensive computing, Large-scale data mining, Machine Learning, Parallel processing},
}

@inproceedings{Pei2017DeepXplore,
	title = {{DeepXplore}: {Automated} {Whitebox} {Testing} of {Deep} {Learning} {Systems}},
	shorttitle = {{DeepXplore}},
	abstract = {Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system’s behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs.},
	booktitle = {Symposium on {Operating} {Systems} {Principles} ({SOSP})},
	author = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
	month = oct,
	year = {2017},
}

@article{Zou2019ObjectDetectionSurvey,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	shorttitle = {Object {Detection} in 20 {Years}},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	urldate = {2022-01-18},
	journal = {arXiv},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{Alahmari2020RepeatabilityofDLModels,
	title = {Challenges for the {Repeatability} of {Deep} {Learning} {Models}},
	abstract = {Deep learning training typically starts with a random sampling initialization approach to set the weights of trainable layers. Therefore, different and/or uncontrolled weight initialization prevents learning the same model multiple times. Consequently, such models yield different results during testing. However, even with the exact same initialization for the weights, a lack of repeatability, replicability, and reproducibility may still be observed during deep learning for many reasons such as software versions, implementation variations, and hardware differences. In this article, we study repeatability when training deep learning models for segmentation and classification tasks using U-Net and LeNet-5 architectures in two development environments Pytorch and Keras (with TensorFlow backend). We show that even with the available control of randomization in Keras and TensorFlow, there are uncontrolled randomizations. We also show repeatable results for the same deep learning architectures using the Pytorch deep learning library. Finally, we discuss variations in the implementation of the weight initialization algorithm across deep learning libraries as a source of uncontrolled error in deep learning results.},
	journal = {IEEE Access},
	author = {Alahmari, Saeed S. and Goldgof, Dmitry B. and Mouton, Peter R. and Hall, Lawrence O.},
	year = {2020},
	keywords = {Computational modeling, Computer architecture, Deep learning, Keras, Libraries, Microprocessors, Pytorch, Software, TensorFlow, Training, determinism, deterministic models, repeatability, replicability, replicable deep learning models, reproducibility, reproducible, torch},
}

@inproceedings{Tian2018DeepTest,
	title = {{DeepTest}: automated testing of deep-neural-network-driven autonomous cars},
	shorttitle = {{DeepTest}},
	abstract = {Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.},
	urldate = {2022-01-01},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
	year = {2018},
	keywords = {DeepTest},
}

@article{Unceta2020EnvironmentalAdaptationDifferentialReplicationinML,
	title = {Environmental adaptation and differential replication in machine learning},
	abstract = {When deployed in the wild, machine learning models are usually confronted with an environment that imposes severe constraints. As this environment evolves, so do these constraints. As a result, the feasible set of solutions for the considered need is prone to change in time. We refer to this problem as that of environmental adaptation. In this paper, we formalize environmental adaptation and discuss how it differs from other problems in the literature. We propose solutions based on differential replication, a technique where the knowledge acquired by the deployed models is reused in specific ways to train more suitable future generations. We discuss different mechanisms to implement differential replications in practice, depending on the considered level of knowledge. Finally, we present seven examples where the problem of environmental adaptation can be solved through differential replication in real-life applications.},
	journal = {Entropy},
	author = {Unceta, Irene and Nin, Jordi and Pujol, Oriol},
	year = {2020},
	keywords = {Copying, Differential replication, Editing, Knowledge distillation, Machine learning, Natural selection},
}

@misc{DeterminedAIReproducibility,
	title = {Reproducibility in {ML}: why it matters and how to achieve it},
	shorttitle = {Reproducibility in {ML}},
	url = {https://determined.ai/blog/reproducibility-in-ml},
	abstract = {Reproducible machine learning is hard, particularly when training deep learning models. We review common sources of DL non-determinism and how to address them.},
	language = {en},
	urldate = {2021-12-08},
	journal = {Determined AI},
	author = {Villa, Jennifer and Zimmerman, Yoav},
	year = {2018},
}

@article{Wang2020RegexBugs,
	title = {An {Empirical} {Study} on {Regular} {Expression} {Bugs}},
	abstract = {Understanding the nature of regular expression (regex) issues is important to tackle practical issues developers face in regular expression usage. Knowledge about the nature and frequency of various types of regular expression issues, such as those related to performance, API misuse, and code smells, can guide testing, inform documentation writers, and motivate refactoring efforts. However, beyond ReDoS (Regular expression Denial of Service), little is known about to what extent regular expression issues affect software development and how these issues are addressed in practice. This paper presents a comprehensive empirical study of 350 merged regex-related pull requests from Apache, Mozilla, Facebook, and Google GitHub repositories. Through classifying the root causes and manifestations of those bugs, we show that incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3\%). The remaining root causes are incorrect API usage (9.3\%) and other code issues that require regular expression changes in the fix (29.5\%). By studying the code changes of regex-related pull requests, we observe that fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests. The results of this study contribute to a broader understanding of the practical problems faced by developers when using regular expressions.},
	journal = {International Conference on Mining Software Repositories (MSR)},
	author = {Wang, Peipei and Brown, Chris and Jennings, Jamie A. and Stolee, Kathryn T.},
	year = {2020},
	keywords = {Regular expression bug characteristics, bug fixes, pull requests},
}

@article{zhang_software_2019,
	title = {Software engineering practice in the development of deep learning applications},
	url = {https://arxiv.org/abs/1910.03156},
	abstract = {Deep-Learning(DL) applications have been widely employed to assist in various tasks. They are constructed based on a data-driven programming paradigm that is different from conventional software applications. Given the increasing popularity and importance of DL applications, software engineering practitioners have some techniques specifically for them. However, little research is conducted to identify the challenges and lacks in practice. To fill this gap, in this paper, we surveyed 195 practitioners to understand their insight and experience in the software engineering practice of DL applications. Specifically, we asked the respondents to identify lacks and challenges in the practice of the development life cycle of DL applications. The results present 13 findings that provide us with a better understanding of software engineering practice of DL applications. Further, we distil these findings into 7 actionable recommendations for software engineering researchers and practitioners to improve the development of DL applications.},
	journal = {arXiv},
	author = {Zhang, Xufan and Yang, Yilin and Feng, Yang and Chen, Zhenyu},
	year = {2019},
	keywords = {Deep learning applications, Practitioner perception, Software engineering in practice},
}

@misc{FacebookAIReproducibility,
	title = {How the {AI} community can get serious about reproducibility},
	url = {https://ai.facebook.com/blog/how-the-ai-community-can-get-serious-about-reproducibility/},
	abstract = {Facebook AI Managing Director Joelle Pineau discusses the importance of reproducibility in AI research, and shares early results from the first reproducibility challenge at NeurIPS 2019.},
	language = {zh-Hans},
	urldate = {2021-12-08},
	author = {Pineau, Joelle},
	year = {2022},
}

@misc{Jaeyoun2020Intro2ModelGarden,
	title = {Introducing the {Model} {Garden} for {TensorFlow} 2},
	url = {https://blog.tensorflow.org/2020/03/introducing-model-garden-for-tensorflow-2.html},
	abstract = {The TensorFlow blog contains regular news from the TensorFlow team and the community, with articles on Python, TensorFlow.js, TF Lite, TFX, and more.},
	language = {en},
	author = {Kim, Jaeyoun and Li, Jing},
	year = {2021},
}

@misc{noauthor_introducing_nodate,
	title = {Introducing the {Model} {Garden} for {TensorFlow} 2},
	url = {https://blog.tensorflow.org/2020/03/introducing-model-garden-for-tensorflow-2.html},
	abstract = {The TensorFlow blog contains regular news from the TensorFlow team and the community, with articles on Python, TensorFlow.js, TF Lite, TFX, and more.},
	language = {en},
	urldate = {2022-02-04},
}

@article{choromanski_rethinking_2021,
	title = {Rethinking {Attention} with {Performers}},
	url = {http://arxiv.org/abs/2009.14794},
	abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
	urldate = {2022-02-02},
	journal = {arXiv:2009.14794 [cs, stat]},
	author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.14794},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{PytorchWeb,
	title = {{PyTorch}},
	url = {https://pytorch.org/},
	urldate = {2021-11-17},
	year = {2021},
}

@misc{TensorFlowWeb,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	urldate = {2021-11-17},
	year = {2021},
}

@misc{MLReproducibilityChallengeSpring2021,
	title = {Papers with {Code} - {ML} {Reproducibility} {Challenge} 2021 {Edition}},
	url = {https://paperswithcode.com/rc2021},
	abstract = {The ML Reproducibility Challenge 2021 covering paper published in seven major ML conferences: NeurIPS, ACL, EMNLP, ICLR, ICML, CVPR and ECCV.},
	language = {en},
	urldate = {2021-12-08},
	year = {2020},
}

@misc{ONNX,
	title = {{ONNX} {\textbar} {Home}},
	url = {https://onnx.ai/},
	urldate = {2021-12-10},
	year = {2019},
}

@misc{GithubLabels,
	title = {Managing labels},
	url = {https://docs.github.com/en/issues/using-labels-and-milestones-to-track-work/managing-labels},
	abstract = {You can classify issues, pull requests, and discussions by creating, editing, applying, and deleting labels.},
	language = {en},
	urldate = {2021-12-05},
	journal = {GitHub Docs},
	year = {2020},
}

@article{valett1989SWMeasurementExperiencesinSELab,
	title = {A {Summary} of {Software} {Measurement} {Experiences} in the {Software} {Engineering} {Laboratory}},
	volume = {9},
	language = {en},
	journal = {Journal of Systems and Software},
	author = {Valett, Jon D and McGarry, Frank E},
	year = {1989},
	pages = {137 -- 148},
}

@article{rupprecht_improving_2020,
	title = {Improving reproducibility of data science pipelines through transparent provenance capture},
	volume = {13},
	issn = {2150-8097},
	doi = {10.14778/3415478.3415556},
	abstract = {Data science has become prevalent in a large variety of domains. Inherent in its practice is an exploratory, probing, and fact finding journey, which consists of the assembly, adaptation, and execution of complex data science pipelines. The trustworthiness of the results of such pipelines rests entirely on their ability to be reproduced with fidelity, which is difficult if pipelines are not documented or recorded minutely and consistently. This difficulty has led to a reproducibility crisis and presents a major obstacle to the safe adoption of the pipeline results in production environments. The crisis can be resolved if the provenance for each data science pipeline is captured transparently as pipelines are executed. However, due to the complexity of modern data science pipelines, transparently capturing sufficient provenance to allow for reproducibility is challenging. As a result, most existing systems require users to augment their code or use specific tools to capture provenance, which hinders productivity and results in a lack of adoption.In this paper, we present Ursprung,1 a transparent provenance collection system designed for data science environments.2 The Ursprung philosophy is to capture provenance and build lineage by integrating with the execution environment to automatically track static and runtime configuration parameters of data science pipelines. Rather than requiring data scientists to make changes to their code, Ursprung records basic provenance information from system-level sources and combines it with provenance from application-level sources (e.g., log files, stdout), which can be accessed and recorded through a domain-specific language. In our evaluation, we show that Ursprung is able to capture sufficient provenance for a variety of use cases and only adds an overhead of up to 4\%.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Rupprecht, Lukas and Davis, James C. and Arnold, Constantine and Gur, Yaniv and Bhagwat, Deepavali},
	year = {2020},
}

@article{yan_contnet_2021,
	title = {{ConTNet}: {Why} not use convolution and transformer at the same time?},
	shorttitle = {{ConTNet}},
	url = {http://arxiv.org/abs/2104.13497},
	abstract = {Although convolutional networks (ConvNets) have enjoyed great success in computer vision (CV), it suffers from capturing global information crucial to dense prediction tasks such as object detection and segmentation. In this work, we innovatively propose ConTNet (ConvolutionTransformer Network), combining transformer with ConvNet architectures to provide large receptive fields. Unlike the recently-proposed transformer-based models (e.g., ViT, DeiT) that are sensitive to hyper-parameters and extremely dependent on a pile of data augmentations when trained from scratch on a midsize dataset (e.g., ImageNet1k), ConTNet can be optimized like normal ConvNets (e.g., ResNet) and preserve an outstanding robustness. It is also worth pointing that, given identical strong data augmentations, the performance improvement of ConTNet is more remarkable than that of ResNet. We present its superiority and effectiveness on image classification and downstream tasks. For example, our ConTNet achieves 81.8\% top-1 accuracy on ImageNet which is the same as DeiT-B with less than 40\% computational complexity. ConTNet-M also outperforms ResNet50 as the backbone of both Faster-RCNN (by 2.6\%) and Mask-RCNN (by 3.2\%) on COCO2017 dataset. We hope that ConTNet could serve as a useful backbone for CV tasks and bring new ideas for model design},
	urldate = {2022-02-01},
	journal = {arXiv:2104.13497 [cs]},
	author = {Yan, Haotian and Li, Zhe and Li, Weijian and Wang, Changhu and Wu, Ming and Zhang, Chuang},
	month = may,
	year = {2021},
	note = {arXiv: 2104.13497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wang_benchmarking_2019,
	title = {Benchmarking {TPU}, {GPU}, and {CPU} {Platforms} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1907.10701},
	abstract = {Training deep learning models is compute-intensive and there is an industry-wide trend towards hardware specialization to improve performance. To systematically benchmark deep learning platforms, we introduce ParaDnn, a parameterized benchmark suite for deep learning that generates end-to-end models for fully connected (FC), convolutional (CNN), and recurrent (RNN) neural networks. Along with six real-world models, we benchmark Google's Cloud TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep dive into TPU architecture, reveal its bottlenecks, and highlight valuable lessons learned for future specialized system design. We also provide a thorough comparison of the platforms and find that each has unique strengths for some types of models. Finally, we quantify the rapid performance improvements that specialized software stacks provide for the TPU and GPU platforms.},
	urldate = {2022-01-22},
	journal = {arXiv:1907.10701 [cs, stat]},
	author = {Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
	month = oct,
	year = {2019},
	note = {arXiv: 1907.10701},
	keywords = {Computer Science - Machine Learning, Computer Science - Performance, Statistics - Machine Learning},
}

@article{ribeiro_beyond_2020,
	title = {Beyond {Accuracy}: {Behavioral} {Testing} of {NLP} models with {CheckList}},
	shorttitle = {Beyond {Accuracy}},
	url = {http://arxiv.org/abs/2005.04118},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
	urldate = {2022-01-19},
	journal = {arXiv:2005.04118 [cs]},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	month = may,
	year = {2020},
	note = {arXiv: 2005.04118},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{baker_detect_nodate,
	title = {Detect, {Fix}, and {Verify} {TensorFlow} {API} {Misuses}},
	abstract = {The growing application of DL makes detecting and ﬁxing defective DL programs of paramount importance. Recent studies on DL defects report that TensorFlow API misuses represent a common class of DL defects. However to effectively detect, ﬁx, and verify them remains an understudied problem. This paper presents the TensorFlow API misuses Detector And Fixer (TADAF) technique, which relies on 11 common API misuses patterns and corresponding ﬁxes that we extracted from StackOverﬂow. TADAF statically analyses a TensorFlow program for identifying matches of any of the 11 patterns. If it ﬁnds a match, it automatically generates a ﬁxed version of the program. To verify that the misuse brings a tangible negative effect, TADAF reports functional, accuracy, or efﬁciency differences when training and testing (with the same data) the original and ﬁxed versions of the program. Our preliminary evaluation on ﬁve GitHub projects shows that TADAF detected and ﬁxed all the API misuses.},
	language = {en},
	author = {Baker, Wilson and O’Connor, Michael and Shahamiri, Seyed Reza and Terragni, Valerio},
	pages = {5},
}

@article{dilhara_discovering_2022,
	title = {Discovering {Repetitive} {Code} {Changes} in {Python} {ML} {Systems}},
	abstract = {Over the years, researchers capitalized on the repetitiveness of software changes to automate many software evolution tasks. Despite the extraordinary rise in popularity of Python-based ML systems, they do not benefit from these advances. Without knowing what are the repetitive changes that ML developers make, researchers, tool, and library designers miss opportunities for automation, and ML developers fail to learn and use common practices.},
	language = {en},
	author = {Dilhara, Malinda and Sannidhi, Nikhith and Ketkar, Ameya},
	year = {2022},
	pages = {13},
}

@article{del_barrio_review_2020,
	title = {Review of {Mathematical} frameworks for {Fairness} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/2005.13755},
	abstract = {A review of the main fairness definitions and fair learning methodologies proposed in the literature over the last years is presented from a mathematical point of view. Following our independence-based approach, we consider how to build fair algorithms and the consequences on the degradation of their performance compared to the possibly unfair case. This corresponds to the price for fairness given by the criteria \${\textbackslash}textit\{statistical parity\}\$ or \${\textbackslash}textit\{equality of odds\}\$. Novel results giving the expressions of the optimal fair classifier and the optimal fair predictor (under a linear regression gaussian model) in the sense of \${\textbackslash}textit\{equality of odds\}\$ are presented.},
	urldate = {2022-01-12},
	journal = {arXiv:2005.13755 [cs, stat]},
	author = {del Barrio, Eustasio and Gordaliza, Paula and Loubes, Jean-Michel},
	month = may,
	year = {2020},
	note = {arXiv: 2005.13755},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Atkinson2008AccidentalComplexity,
	title = {Reducing accidental complexity in domain models},
	volume = {7},
	issn = {1619-1366, 1619-1374},
	url = {http://link.springer.com/10.1007/s10270-007-0061-0},
	doi = {10.1007/s10270-007-0061-0},
	abstract = {A fundamental principle in engineering, including software engineering, is to minimize the amount of accidental complexity which is introduced into engineering solutions due to mismatches between a problem and the technology used to represent the problem. As model-driven development moves to the center stage of software engineering, it is particularly important that this principle be applied to the technologies used to create and manipulate models, especially models that are intended to be free of solution decisions. At present, however, there is a signiﬁcant mismatch between the “two level” modeling paradigm used to construct mainstream domain models and the conceptual information such models are required to represent—a mismatch that makes such models more complex than they need be. In this paper, we identify the precise nature of the mismatch, discuss a number of more or less satisfactory workarounds, and show how it can be avoided.},
	language = {en},
	number = {3},
	urldate = {2022-01-07},
	journal = {Software \& Systems Modeling},
	author = {Atkinson, Colin and Kühne, Thomas},
	month = jul,
	year = {2008},
	pages = {345--359},
}

@article{biran_explanation_nodate,
	title = {Explanation and {Justification} in {Machine} {Learning}: {A} {Survey}},
	abstract = {We present a survey of the research concerning explanation and justiﬁcation in the Machine Learning literature and several adjacent ﬁelds. Within Machine Learning, we differentiate between two main branches of current research: interpretable models, and prediction interpretation and justiﬁcation.},
	language = {en},
	author = {Biran, Or and Cotton, Courtenay},
	pages = {6},
}

@inproceedings{Bengtsson1998ScenarioBasedSWArchitectureReengineering,
	title = {Scenario-based software architecture reengineering},
	doi = {10.1109/ICSR.1998.685756},
	abstract = {The paper presents a method for reengineering software architectures. The method explicitly addresses the quality attributes of the software architecture. Assessment of quality attributes is performed primarily using scenarios. Design transformations are done to improve quality attributes that do not satisfy the requirements. Assessment and design transformation can be performed for several iterations until all requirements are met. To illustrate the method we use the reengineering of a prototypical measurement system into a domain-specific software architecture as an example.},
	booktitle = {Proceedings. {Fifth} {International} {Conference} on {Software} {Reuse} ({Cat}. {No}.{98TB100203})},
	author = {Bengtsson, P. and Bosch, J.},
	month = jun,
	year = {1998},
	note = {ISSN: 1085-9098},
	keywords = {Application software, Computer architecture, Design methodology, Electrical capacitance tomography, Prototypes, Software architecture, Software maintenance, Software prototyping, Software quality, Software systems},
	pages = {308--317},
}

@article{gudivada_data_2017,
	title = {Data {Quality} {Considerations} for {Big} {Data} and {Machine} {Learning}: {Going} {Beyond} {Data} {Cleaning} and {Transformations}},
	volume = {10},
	shorttitle = {Data {Quality} {Considerations} for {Big} {Data} and {Machine} {Learning}},
	abstract = {Data quality issues trace back their origin to the early days of computing. A wide range of domain-specific techniques to assess and improve the quality of data exist in the literature. These solutions primarily target data which resides in relational databases and data warehouses. The recent emergence of big data analytics and renaissance in machine learning necessitates evaluating the suitability relational database-centric approaches to data quality. In this paper, we describe the nature of the data quality issues in the context of big data and machine learning. We discuss facets of data quality, present a data governance-driven framework for data quality lifecycle for this new scenario, and describe an approach to its implementation. A sampling of the tools available for data quality management is indicated and future trends are discussed.},
	journal = {International Journal on Advances in Software},
	author = {Gudivada, Venkat and Apon, Amy and Ding, Junhua},
	year = {2017},
	pages = {1--20},
}

@inproceedings{odena_tensorfuzz_2019,
	title = {{TensorFuzz}: {Debugging} {Neural} {Networks} with {Coverage}-{Guided} {Fuzzing}},
	shorttitle = {{TensorFuzz}},
	url = {https://proceedings.mlr.press/v97/odena19a.html},
	abstract = {Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how approximate nearest neighbor (ANN) algorithms can provide this coverage metric for neural networks. We then combine these methods with techniques for property-based testing (PBT). In PBT, one asserts properties that a function should satisfy and the system automatically generates tests exercising those properties. We then apply this system to practical goals including (but not limited to) surfacing broken loss functions in popular GitHub repositories and making performance improvements to TensorFlow. Finally, we release an open source library called TensorFuzz that implements the described techniques.},
	language = {en},
	urldate = {2022-01-01},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Odena, Augustus and Olsson, Catherine and Andersen, David and Goodfellow, Ian},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4901--4911},
}

@misc{ONNXBlog,
	title = {Portability between deep learning frameworks – with {ONNX}},
	url = {https://blog.codecentric.de/en/2019/08/portability-deep-learning-frameworks-onnx/},
	abstract = {The need for model portability of deep learning frameworks is greater than ever. This posts explains how ONNX can help here.},
	language = {en-US},
	urldate = {2021-12-31},
	journal = {codecentric AG Blog},
	month = aug,
	year = {2019},
}

@inproceedings{patel_gestalt_2010,
	address = {New York New York USA},
	title = {Gestalt: integrated support for implementation and analysis in machine learning},
	isbn = {978-1-4503-0271-5},
	shorttitle = {Gestalt},
	url = {https://dl.acm.org/doi/10.1145/1866029.1866038},
	doi = {10.1145/1866029.1866038},
	abstract = {We present Gestalt, a development environment designed to support the process of applying machine learning. While traditional programming environments focus on source code, we explicitly support both code and data. Gestalt allows developers to implement a classification pipeline, analyze data as it moves through that pipeline, and easily transition between implementation and analysis. An experiment shows this significantly improves the ability of developers to find and fix bugs in machine learning systems. Our discussion of Gestalt and our experimental observations provide new insight into general-purpose support for the machine learning process.},
	language = {en},
	urldate = {2021-12-31},
	booktitle = {Proceedings of the 23nd annual {ACM} symposium on {User} interface software and technology},
	publisher = {ACM},
	author = {Patel, Kayur and Bancroft, Naomi and Drucker, Steven M. and Fogarty, James and Ko, Amy J. and Landay, James},
	month = oct,
	year = {2010},
	pages = {37--46},
}

@inproceedings{Jebnoun2020DLCodeScent,
	address = {Seoul Republic of Korea},
	title = {The {Scent} of {Deep} {Learning} {Code}: {An} {Empirical} {Study}},
	isbn = {978-1-4503-7517-7},
	shorttitle = {The {Scent} of {Deep} {Learning} {Code}},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387479},
	doi = {10.1145/3379597.3387479},
	abstract = {Deep learning practitioners are often interested in improving their model accuracy rather than the interpretability of their models. As a result, deep learning applications are inherently complex in their structures. They also need to continuously evolve in terms of code changes and model updates. Given these confounding factors, there is a great chance of violating the recommended programming practices by the developers in their deep learning applications. In particular, the code quality might be negatively affected due to their drive for the higher model performance. Unfortunately, the code quality of deep learning applications has rarely been studied to date. In this paper, we conduct an empirical study to investigate the distribution of code smells in deep learning applications. To this end, we perform a comparative analysis between deep learning and traditional open-source applications collected from GitHub. We have several major findings. First, long lambda expression, long ternary conditional expression, and complex container comprehension smells are frequently found in deep learning projects. That is, deep learning code involves more complex or longer expressions than the traditional code does. Second, the number of code smells increases across the releases of deep learning applications. Third, we found that there is a co-existence between code smells and software bugs in the studied deep learning code, which confirms our conjecture on the degraded code quality of deep learning applications.},
	language = {en},
	urldate = {2021-12-30},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Jebnoun, Hadhemi and Ben Braiek, Houssem and Rahman, Mohammad Masudur and Khomh, Foutse},
	month = jun,
	year = {2020},
	pages = {420--430},
}

@inproceedings{singla_analysis_2018,
	title = {Analysis of {Software} {Engineering} for {Agile} {Machine} {Learning} {Projects}},
	doi = {10.1109/INDICON45594.2018.8987154},
	abstract = {The number of machine learning, artificial intelligence or data science related software engineering projects using Agile methodology is increasing. However, there are very few studies on how such projects work in practice. In this paper, we analyze project issues tracking data taken from Scrum (a popular tool for Agile) for several machine learning projects. We compare this data with corresponding data from non-machine learning projects, in an attempt to analyze how machine learning projects are executed differently from normal software engineering projects. On analysis, we find that machine learning project issues use different kinds of words to describe issues, have higher number of exploratory or research oriented tasks as compared to implementation tasks, and have a higher number of issues in the product backlog after each sprint, denoting that it is more difficult to estimate the duration of machine learning project related tasks in advance. After analyzing this data, we propose a few ways in which Agile machine learning projects can be better logged and executed, given their differences with normal software engineering projects.},
	booktitle = {2018 15th {IEEE} {India} {Council} {International} {Conference} ({INDICON})},
	author = {Singla, Kushal and Bose, Joy and Naik, Chetan},
	month = dec,
	year = {2018},
	note = {ISSN: 2325-9418},
	keywords = {Data science, Machine learning, Maximum likelihood estimation, Software, Software engineering, Tag clouds, Task analysis, agile methodology, machine learning project, scrum, software engineering},
	pages = {1--5},
}

@inproceedings{ComputationalCostofDL,
	title = {Predicting the {Computational} {Cost} of {Deep} {Learning} {Models}},
	doi = {10.1109/BigData.2018.8622396},
	abstract = {Deep learning is rapidly becoming a go-to tool for many artificial intelligence problems due to its ability to outperform other approaches and even humans at many problems. Despite its popularity we are still unable to accurately predict the time it will take to train a deep learning network to solve a given problem. This training time can be seen as the product of the training time per epoch and the number of epochs which need to be performed to reach the desired level of accuracy. Some work has been carried out to predict the training time for an epoch - most have been based around the assumption that the training time is linearly related to the number of floating point operations required. However, this relationship is not true and becomes exacerbated in cases where other activities start to dominate the execution time. Such as the time to load data from memory or loss of performance due to non-optimal parallel execution. In this work we propose an alternative approach in which we train a deep learning network to predict the execution time for parts of a deep learning network. Timings for these individual parts can then be combined to provide a prediction for the whole execution time. This has advantages over linear approaches as it can model more complex scenarios. But, also, it has the ability to predict execution times for scenarios unseen in the training data. Therefore, our approach can be used not only to infer the execution time for a batch, or entire epoch, but it can also support making a well-informed choice for the appropriate hardware and model.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Justus, Daniel and Brennan, John and Bonner, Stephen and McGough, Andrew Stephen},
	month = dec,
	year = {2018},
	keywords = {Benchmark, Computational modeling, Deep learning, Hardware, Machine Learning, Neural networks, Performance, Prediction, Predictive models, Timing, Training},
	pages = {3873--3882},
}

@article{shridhar_interoperating_2019,
	title = {Interoperating {Deep} {Learning} models with {ONNX}.jl},
	abstract = {Flux [17] is a machine learning framework, written using the numerical computing language Julia[4]. The framework makes writing layers as simple as writing mathematical formulae, and it’s advanced AD, Zygote [11] , applies automatic differentiation (AD) to calculate derivatives and train the model. It makes heavy use of Julia’s language and compiler features to carry out code analysis and make optimisations. For example, Julia’s GPU compilation support [3] can be used to JIT-compile custom GPU kernels for model layers [19]. Flux also supports a number of a hardware options, from CPUs, GPUs and even TPUs via XLA.jl, that compiles Julia code to XLA: an advanced compiler for linear algebra that is capable of greatly optimizing speed and memory usage in large deep learning models.},
	language = {en},
	author = {Shridhar, Ayush and Tomson, Phil and Innes, Mike},
	year = {2019},
	pages = {6},
}

@article{Washizaki2019SEPatterns4LSystems,
	title = {Studying {Software} {Engineering} {Patterns} for {Designing} {Machine} {Learning} {Systems}},
	doi = {10.1109/IWESEP49350.2019.00017},
	abstract = {Machine-learning (ML) techniques are becoming more prevalent. ML techniques rely on mathematics and software engineering. Researchers and practitioners studying best practices strive to design ML systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. However, a systematic study to collect, classify, and discuss these software-engineering (SE) design patterns for ML techniques have yet to be reported. Our research collects good/bad SE design patterns for ML techniques to provide developers with a comprehensive classification of such patterns. Herein we report the preliminary results of a systematic-literature review (SLR) of good/bad design patterns for ML.},
	journal = {Proceedings - 2019 10th International Workshop on Empirical Software Engineering in Practice, IWESEP 2019},
	author = {Washizaki, Hironori and Uchida, Hiromu and Khomh, Foutse and Guéhéneuc, Yann Gaël},
	year = {2019},
	note = {ISBN: 9781728155906
Publisher: IEEE},
	keywords = {Anti-patterns, Architecture Patterns, Design Patterns, ML Patterns, Machine Learning},
	pages = {49--54},
}

@article{Wang2020DLSE,
	title = {Synergy between {Machine}/{Deep} {Learning} and {Software} {Engineering}: {How} {Far} {Are} {We}?},
	shorttitle = {Synergy between {Machine}/{Deep} {Learning} and {Software} {Engineering}},
	url = {http://arxiv.org/abs/2008.05515},
	abstract = {Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Machine Learning (ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the quality (especially the applicability and generalizability) of ML/DLrelated SE studies, and to stimulate and enhance future collaborations between SE/AI researchers and industry practitioners, we conducted a 10-year Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that ML/DL and SE have had on each other. At the same time, however, we also observed a paucity of replicable and reproducible ML/DL-related SE studies and identiﬁed ﬁve factors that inﬂuence their replicability and reproducibility. To improve the applicability and generalizability of research results, we analyzed what ingredients in a study would facilitate an understanding of why a ML/DL technique was selected for a speciﬁc SE problem. In addition, we identiﬁed the unique trends of impacts of DL models on SE tasks, as well as ﬁve unique challenges that needed to be met in order to better leverage DL to improve the productivity of SE tasks. Finally, we outlined a road-map that we believe can facilitate the transfer of ML/DL-based SE research results into real-world industry practices.},
	language = {en},
	urldate = {2021-12-05},
	journal = {arXiv:2008.05515 [cs]},
	author = {Wang, Simin and Huang, Liguo and Ge, Jidong and Zhang, Tengfei and Feng, Haitao and Li, Ming and Zhang, He and Ng, Vincent},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.05515},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{Thomas2019GlobalDistributionofCVEngineeringWork,
	title = {Migration {Versus} {Management}: the {Global} {Distribution} of {Computer} {Vision} {Engineering} {Work}},
	shorttitle = {Migration {Versus} {Management}},
	doi = {10.1109/ICGSE.2019.00017},
	abstract = {Computer vision professionals develop systems that monitor endangered fish species, alert car dealerships of potential theft, and track inventory on retailers' shelves, to name a few examples. While their products vary, their day-to-day work practices rarely do. Most work in small, co-located, multi-disciplinary teams and rapidly iterate systems built with algorithms, products, and services provided by similar teams but at different companies. Their examples challenge global software engineering research to look beyond the social and technical coordination work of large, internal software development teams. Their stories, culled from ethnographic interviews with eighty computer vision engineers and research scientists, echo the heady days of 1980s Silicon Valley when Bay Area social networks and the global migration of professional talent fueled the rapid growth of the high technology industry. These engineers, then and now, migrate from task to task, versus the task from engineer to engineer.},
	booktitle = {2019 {ACM}/{IEEE} 14th {International} {Conference} on {Global} {Software} {Engineering} ({ICGSE})},
	author = {Thomas, Suzanne L.},
	month = may,
	year = {2019},
	keywords = {Computer vision, Deep learning, Software, Software algorithms, Software engineering, Task analysis, computer vision, ethnography, software engineering},
	pages = {12--17},
}

@misc{CVEngineer2021,
	title = {Being a {Computer} {Vision} {Engineer} in 2021},
	url = {https://viso.ai/computer-vision/computer-vision-engineer/},
	abstract = {Learn what computer vision engineers do, what skills are required to be a successful computer vision engineer, and the job outlook.},
	language = {en-US},
	urldate = {2021-12-08},
	journal = {viso.ai},
	month = jun,
	year = {2021},
}

@article{wang_synergy_2020,
	title = {Synergy between {Machine}/{Deep} {Learning} and {Software} {Engineering}: {How} {Far} {Are} {We}?},
	shorttitle = {Synergy between {Machine}/{Deep} {Learning} and {Software} {Engineering}},
	url = {http://arxiv.org/abs/2008.05515},
	abstract = {Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Machine Learning (ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the quality (especially the applicability and generalizability) of ML/DL-related SE studies, and to stimulate and enhance future collaborations between SE/AI researchers and industry practitioners, we conducted a 10-year Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that ML/DL and SE have had on each other. At the same time, however, we also observed a paucity of replicable and reproducible ML/DL-related SE studies and identified five factors that influence their replicability and reproducibility. To improve the applicability and generalizability of research results, we analyzed what ingredients in a study would facilitate an understanding of why a ML/DL technique was selected for a specific SE problem. In addition, we identified the unique trends of impacts of DL models on SE tasks, as well as five unique challenges that needed to be met in order to better leverage DL to improve the productivity of SE tasks. Finally, we outlined a road-map that we believe can facilitate the transfer of ML/DL-based SE research results into real-world industry practices.},
	urldate = {2021-12-05},
	journal = {arXiv:2008.05515 [cs]},
	author = {Wang, Simin and Huang, Liguo and Ge, Jidong and Zhang, Tengfei and Feng, Haitao and Li, Ming and Zhang, He and Ng, Vincent},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.05515},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{bajammal_survey_2020,
	title = {A {Survey} on the {Use} of {Computer} {Vision} to {Improve} {Software} {Engineering} {Tasks}},
	issn = {1939-3520},
	doi = {10.1109/TSE.2020.3032986},
	abstract = {Software engineering (SE) research has traditionally revolved around engineering the source code. However, novel approaches that analyze software through computer vision have been increasingly adopted in SE. These approaches allow analyzing the software from a different complementary perspective other than the source code, and they are used to either complement existing source code-based methods, or to overcome their limitations. The goal of this manuscript is to survey the use of computer vision techniques in SE with the aim of assessing their potential in advancing the field of SE research. We examined an extensive body of literature from top-tier SE venues, as well as venues from closely related fields (machine learning, computer vision, and human-computer interaction). Our inclusion criteria targeted papers applying computer vision techniques that address problems related to any area of SE. We collected an initial pool of 2,716 papers, from which we obtained 66 final relevant papers covering a variety of SE areas. We analyzed what computer vision techniques have been adopted or designed, for what reasons, how they are used, what benefits they provide, and how they are evaluated. Our findings highlight that visual approaches have been adopted in a wide variety of SE tasks, predominantly for effectively tackling software analysis and testing challenges in the web and mobile domains. The results also show a rapid growth trend of the use of computer vision techniques in SE research.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Bajammal, Mohammad and Stocco, Andrea and Mazinanian, Davood and Mesbah, Ali},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Computer Vision, Computer vision, Graphical user interfaces, Software, Software Engineering, Software engineering, Survey, Task analysis, Testing, Visualization},
	pages = {1--1},
}

@misc{noauthor_state_2019,
	title = {The {State} of {Machine} {Learning} {Frameworks} in 2019},
	url = {https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/},
	abstract = {Since deep learning regained prominence in 2012, many machine learning frameworks have clamored to become the new favorite among researchers and industry practitioners. From the early academic outputs Caffe and Theano to the massive industry-backed PyTorch and TensorFlow, this deluge of options makes it difficult to keep track of what},
	language = {en},
	urldate = {2021-11-19},
	journal = {The Gradient},
	month = oct,
	year = {2019},
}

@article{han_what_2020,
	title = {What do {Programmers} {Discuss} about {Deep} {Learning} {Frameworks}},
	volume = {25},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-020-09819-6},
	doi = {10.1007/s10664-020-09819-6},
	abstract = {Deep learning has gained tremendous traction from the developer and researcher communities. It plays an increasingly significant role in a number of application domains. Deep learning frameworks are proposed to help developers and researchers easily leverage deep learning technologies, and they attract a great number of discussions on popular platforms, i.e., Stack Overflow and GitHub. To understand and compare the insights from these two platforms, we mine the topics of interests from these two platforms. Specifically, we apply Latent Dirichlet Allocation (LDA) topic modeling techniques to derive the discussion topics related to three popular deep learning frameworks, namely, Tensorflow, PyTorch and Theano. Within each platform, we compare the topics across the three deep learning frameworks. Moreover, we make a comparison of topics between the two platforms. Our observations include 1) a wide range of topics that are discussed about the three deep learning frameworks on both platforms, and the most popular workflow stages are Model Training and Preliminary Preparation. 2) the topic distributions at the workflow level and topic category level on Tensorflow and PyTorch are always similar while the topic distribution pattern on Theano is quite different. In addition, the topic trends at the workflow level and topic category level of the three deep learning frameworks are quite different. 3) the topics at the workflow level show different trends across the two platforms. e.g., the trend of the Preliminary Preparation stage topic on Stack Overflow comes to be relatively stable after 2016, while the trend of it on GitHub shows a stronger upward trend after 2016. Besides, the Model Training stage topic still achieves the highest impact scores across two platforms. Based on the findings, we also discuss implications for practitioners and researchers.},
	language = {en},
	number = {4},
	urldate = {2021-11-19},
	journal = {Empirical Software Engineering},
	author = {Han, Junxiao and Shihab, Emad and Wan, Zhiyuan and Deng, Shuiguang and Xia, Xin},
	month = jul,
	year = {2020},
	pages = {2694--2747},
}

@inproceedings{parvat_survey_2017,
	title = {A survey of deep-learning frameworks},
	doi = {10.1109/ICISC.2017.8068684},
	abstract = {Deep learning is a model of machine learning loosely based on our brain. Artificial neural network has been around since the 1950s, but recent advances in hardware like graphical processing units (GPU), software like cuDNN, TensorFlow, Torch, Caffe, Theano, Deeplearning4j, etc. and new training methods have made training artificial neural networks fast and easy. In this paper, we are comparing some of the deep learning frameworks on the basis of parameters like modeling capability, interfaces available, platforms supported, parallelizing techniques supported, availability of pre-trained models, community support and documentation quality.},
	booktitle = {2017 {International} {Conference} on {Inventive} {Systems} and {Control} ({ICISC})},
	author = {Parvat, Aniruddha and Chavan, Jai and Kadam, Siddhesh and Dev, Souradeep and Pathak, Vidhi},
	month = jan,
	year = {2017},
	keywords = {Artificial neural networks, Computational modeling, Deep learning, Graphics processing units, Libraries, Machine learning, Mathematical model, Neural networks, Software libraries, Training},
	pages = {1--7},
}

@article{alexopoulos_how_nodate,
	title = {How {Long} {Do} {Vulnerabilities} {Live} in the {Code}? {A} {Large}-{Scale} {Empirical} {Measurement} {Study} on {FOSS} {Vulnerability} {Lifetimes}},
	abstract = {How long do vulnerabilities live in the repositories of large, evolving projects? Although the question has been identified as an interesting problem by the software community in online forums, it has not been investigated yet in adequate depth and scale, since the process of identifying the exact point in time when a vulnerability was introduced is particularly cumbersome. In this paper, we provide an automatic approach for accurately estimating how long vulnerabilities remain in the code (their lifetimes). Our method relies on the observation that while it is difficult to pinpoint the exact point of introduction for one vulnerability, it is possible to accurately estimate the average lifetime of a large enough sample of vulnerabilities, via a heuristic approach.},
	language = {en},
	author = {Alexopoulos, Nikolaos and Brack, Manuel and Wagner, Jan Philipp and Grube, Tim and Mühlhäuser, Max},
	pages = {18},
}

@inproceedings{xin_production_2021,
	address = {Virtual Event China},
	title = {Production {Machine} {Learning} {Pipelines}: {Empirical} {Analysis} and {Optimization} {Opportunities}},
	isbn = {978-1-4503-8343-1},
	shorttitle = {Production {Machine} {Learning} {Pipelines}},
	url = {https://dl.acm.org/doi/10.1145/3448016.3457566},
	doi = {10.1145/3448016.3457566},
	abstract = {Machine learning (ML) is now commonplace, powering data-driven applications in various organizations. Unlike the traditional perception of ML in research, ML production pipelines are complex, with many interlocking analytical components beyond training, whose sub-parts are often run multiple times on overlapping subsets of data. However, there is a lack of quantitative evidence regarding the lifespan, architecture, frequency, and complexity of these pipelines to understand how data management research can be used to make them more efficient, effective, robust, and reproducible. To that end, we analyze the provenance graphs of 3000 production ML pipelines at Google, comprising over 450,000 models trained, spanning a period of over four months, in an effort to understand the complexity and challenges underlying production ML. Our analysis reveals the characteristics, components, and topologies of typical industrystrength ML pipelines at various granularities. Along the way, we introduce a specialized data model for representing and reasoning about repeatedly run components in these ML pipelines, which we call model graphlets. We identify several rich opportunities for optimization, leveraging traditional data management ideas. We show how targeting even one of these opportunities, i.e., identifying and pruning wasted computation that does not translate to model deployment, can reduce wasted computation cost by 50\% without compromising the model deployment cadence.},
	language = {en},
	urldate = {2021-11-17},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Xin, Doris and Miao, Hui and Parameswaran, Aditya and Polyzotis, Neoklis},
	month = jun,
	year = {2021},
	pages = {2639--2652},
}

@misc{KerasWeb,
	title = {Keras: the {Python} deep learning {API}},
	url = {https://keras.io/},
	urldate = {2021-11-17},
}

@inproceedings{sharma_are_2018,
	title = {Are {Existing} {Knowledge} {Transfer} {Techniques} {Effective} for {Deep} {Learning} with {Edge} {Devices}?},
	doi = {10.1109/EDGE.2018.00013},
	abstract = {With the emergence of edge computing paradigm, many applications such as image recognition and augmented reality require to perform machine learning (ML) and artificial intelligence (AI) tasks on edge devices. Most AI and ML models are large and computational-heavy, whereas edge devices are usually equipped with limited computational and storage resources. Such models can be compressed and reduced for deployment on edge devices, but they may lose their capability and not perform well. Recent works used knowledge transfer techniques to transfer information from a large network (termed teacher) to a small one (termed student) in order to improve the performance of the latter. This approach seems to be promising for learning on edge devices, but a thorough investigation on its effectiveness is lacking. This paper provides an extensive study on the performance (in both accuracy and convergence speed) of knowledge transfer, considering different student architectures and different techniques for transferring knowledge from teacher to student. The results show that the performance of KT does vary by architectures and transfer techniques. A good performance improvement is obtained by transferring knowledge from both the intermediate layers and last layer of the teacher to a shallower student. But other architectures and transfer techniques do not fare so well and some of them even lead to negative performance impact.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Edge} {Computing} ({EDGE})},
	author = {Sharma, Ragini and Biookaghazadeh, Saman and Li, Baoxin and Zhao, Ming},
	month = jul,
	year = {2018},
	keywords = {Biological system modeling, Computational modeling, Data models, Deep neural networks, Image edge detection, Knowledge transfer, Machine learning, Performance evaluation, cloud computing, edge computing, knowledge transfer},
	pages = {42--49},
}

@article{muiruri_practices_2021,
	title = {Practices and {Infrastructures} for {ML} {Systems} – {An} {Interview} {Study}},
	url = {https://www.techrxiv.org/articles/preprint/Practices_and_Infrastructures_for_ML_Systems_An_Interview_Study/16939192/1},
	doi = {10.36227/techrxiv.16939192.v1},
	abstract = {The best practices and infrastructures for developing and maintaining machine learning (ML) enabled software systems are often reported by large and experienced data-driven organizations. However, little is known about the state of practice across other organizations. Using interviews, we investigated practices and tool-chains for ML-enabled systems from 16 organizations in various domains. Our study makes three broad observations related to data management practices, monitoring practices and automation practices in ML model training, and serving workflows. These have limited number of generic practices and tools applicable across organizations in different domains.},
	language = {en},
	urldate = {2021-11-16},
	author = {Muiruri, Dennis and Lwakatare, Lucy Ellen and K. Nurminen, Jukka and Mikkonen, Tommi},
	month = nov,
	year = {2021},
	note = {Publisher: TechRxiv},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	urldate = {2021-11-12},
	journal = {arXiv:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{bochkovskiy_yolov4_2020,
	title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
	shorttitle = {{YOLOv4}},
	url = {http://arxiv.org/abs/2004.10934},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
	urldate = {2021-11-12},
	journal = {arXiv:2004.10934 [cs, eess]},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.10934},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{gafford_synthesis-based_2020,
	address = {Virtual Event Australia},
	title = {Synthesis-based resolution of feature interactions in cyber-physical systems},
	isbn = {978-1-4503-6768-4},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416630},
	doi = {10.1145/3324884.3416630},
	abstract = {The feature interaction problem arises when two or more independent features interact with each other in an undesirable manner. Feature interactions remain a challenging and important problem in emerging domains of cyber-physical systems (CPS), such as intelligent vehicles, unmanned aerial vehicles (UAVs) and the Internet of Things (IoT), where the outcome of an unexpected interaction may result in a safety failure. Existing approaches to resolving feature interactions rely on priority lists or fixed strategies, but may not be effective in scenarios where none of the competing feature actions are satisfactory with respect to system requirements. This paper proposes a novel synthesis-based approach to resolution, where a conflict among features is resolved by synthesizing an action that best satisfies the specification of desirable system behaviors in the given environmental context. Unlike existing resolution methods, our approach is capable of producing a desirable system outcome even when none of the conflicting actions are satisfactory. The effectiveness of the proposed approach is demonstrated using a case study involving interactions among safety-critical features in an autonomous drone.},
	language = {en},
	urldate = {2021-11-07},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Gafford, Benjamin and Dürschmid, Tobias and Moreno, Gabriel A. and Kang, Eunsuk},
	month = dec,
	year = {2020},
	pages = {1090--1102},
}

@article{liu_path_2018,
	title = {Path {Aggregation} {Network} for {Instance} {Segmentation}},
	url = {http://arxiv.org/abs/1803.01534},
	abstract = {The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes. Code is available at https://github.com/ShuLiu1993/PANet},
	urldate = {2021-11-05},
	journal = {arXiv:1803.01534 [cs]},
	author = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
	month = sep,
	year = {2018},
	note = {arXiv: 1803.01534},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{potvin_why_2016,
	title = {Why {Google} stores billions of lines of code in a single repository},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2854146},
	doi = {10.1145/2854146},
	abstract = {Google's monolithic repository provides a common source of truth for tens of thousands of developers around the world.},
	language = {en},
	number = {7},
	urldate = {2021-11-05},
	journal = {Communications of the ACM},
	author = {Potvin, Rachel and Levenberg, Josh},
	month = jun,
	year = {2016},
	pages = {78--87},
}

@article{malhotra_empirical_2020,
	title = {An empirical study on predictability of software maintainability using imbalanced data},
	volume = {28},
	issn = {0963-9314, 1573-1367},
	url = {https://link.springer.com/10.1007/s11219-020-09525-y},
	doi = {10.1007/s11219-020-09525-y},
	abstract = {In software engineering predictive modeling, early prediction of software modules or classes that possess high maintainability effort is a challenging task. Many prediction models are constructed to predict the maintainability of software classes or modules by applying various machine learning (ML) techniques. If the software modules or classes need high maintainability, effort would be reduced in a dataset, and there would be imbalanced data to train the model. The imbalanced datasets make ML techniques bias their predictions towards low maintainability effort or majority classes, and minority class instances get discarded as noise by the machine learning (ML) techniques. In this direction, this paper presents empirical work to improve the performance of software maintainability prediction (SMP) models developed with ML techniques using imbalanced data. For developing the models, the imbalanced data is preprocessed by applying data resampling methods. Fourteen data resampling methods, including oversampling, undersampling, and hybrid resampling, are used in the study. The study results recommend that the safe-level synthetic minority oversampling technique (Safe-LevelSMOTE) is a useful method to deal with the imbalanced datasets and to develop competent prediction models to forecast software maintainability.},
	language = {en},
	number = {4},
	urldate = {2021-11-05},
	journal = {Software Quality Journal},
	author = {Malhotra, Ruchika and Lata, Kusum},
	month = dec,
	year = {2020},
	pages = {1581--1614},
}

@article{elmidaoui_machine_2020,
	title = {Machine {Learning} {Techniques} for {Software} {Maintainability} {Prediction}: {Accuracy} {Analysis}},
	volume = {35},
	issn = {1000-9000, 1860-4749},
	shorttitle = {Machine {Learning} {Techniques} for {Software} {Maintainability} {Prediction}},
	url = {https://link.springer.com/10.1007/s11390-020-9668-1},
	doi = {10.1007/s11390-020-9668-1},
	abstract = {Maintaining software once implemented on the end-user side is laborious and, over its lifetime, is most often considerably more expensive than the initial software development. The prediction of software maintainability has emerged as an important research topic to address industry expectations for reducing costs, in particular, maintenance costs. Researchers and practitioners have been working on proposing and identifying a variety of techniques ranging from statistical to machine learning (ML) for better prediction of software maintainability. This review has been carried out to analyze the empirical evidence on the accuracy of software product maintainability prediction (SPMP) using ML techniques. This paper analyzes and discusses the ﬁndings of 77 selected studies published from 2000 to 2018 according to the following criteria: maintainability prediction techniques, validation methods, accuracy criteria, overall accuracy of ML techniques, and the techniques oﬀering the best performance. The review process followed the well-known systematic review process. The results show that ML techniques are frequently used in predicting maintainability. In particular, artiﬁcial neural network (ANN), support vector machine/regression (SVM/R), regression \& decision trees (DT), and fuzzy \& neuro fuzzy (FNF) techniques are more accurate in terms of PRED and MMRE. The N -fold and leave-one-out cross-validation methods, and the MMRE and PRED accuracy criteria are frequently used in empirical studies. In general, ML techniques outperformed non-machine learning techniques, e.g., regression analysis (RA) techniques, while FNF outperformed SVM/R, DT, and ANN in most experiments. However, while many techniques were reported superior, no speciﬁc one can be identiﬁed as the best.},
	language = {en},
	number = {5},
	urldate = {2021-11-05},
	journal = {Journal of Computer Science and Technology},
	author = {Elmidaoui, Sara and Cheikhi, Laila and Idri, Ali and Abran, Alain},
	month = oct,
	year = {2020},
	pages = {1147--1174},
}

@article{alsolai_systematic_2020,
	title = {A systematic literature review of machine learning techniques for software maintainability prediction},
	volume = {119},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584919302228},
	doi = {10.1016/j.infsof.2019.106214},
	abstract = {Context: Software maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process. Objective: The major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models. Method: The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018. Results: We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other software quality attributes. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level product metrics as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on regression problems and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely. Conclusion: Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other, models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.},
	language = {en},
	urldate = {2021-11-05},
	journal = {Information and Software Technology},
	author = {Alsolai, Hadeel and Roper, Marc},
	month = mar,
	year = {2020},
	pages = {106214},
}

@article{malhotra_software_2012,
	title = {Software {Maintainability} {Prediction} using {Machine} {Learning} {Algorithms}},
	volume = {2},
	abstract = {Software maintainability is one of the most important aspects while evaluating quality of the software product. It is deined as the ease with which a software system or component can be modiied to correct faults, improve performance or other attributes or adapt to a changed environment. Tracking the maintenance behaviour of the software product is very complex. This is precisely the reason that predicting the cost and risk associated with maintenance after delivery is extremely dificult which is widely acknowledged by the researchers and practitioners. In an attempt to address this issue quantitatively, the main purpose of this paper is to propose use of few machine learning algorithms with an objective to predict software maintainability and evaluate them. The proposed models are Group Method of Data Handling (GMDH), Genetic Algorithms (GA) and Probabilistic Neural Network (PNN) with Gaussian activation function. The prediction model is constructed using the above said machine learning techniques. In order to study and evaluate its performance, two commercial datasets UIMS (User Interface Management System) and QUES (Quality Evaluation System) are used. The code for these two systems was written in Classical Ada. The UIMS contains 39 classes and QUES datasets contains 71 classes. To measure the maintainability, number of “CHANGE” is observed over a period of three years. We can deine CHANGE as the number of lines of code which were added, deleted or modiied during a three year maintenance period. After conducting empirical study, performance of these three proposed machine learning algorithms was compared with prevailing models such as GRNN (General Regression Neural Network) Model, ANN (Artiicial Neural Network) Model, Bayesian Model, RT (Regression Tree) Model, Backward Elimination Model, Stepwise Selection Model, MARS (Multiple Adaptive Regression Splines) Model, TreeNets Model, GN (Generalized Regression) Model, ANFIS (Adaptive Neuro Fuzzy inference System) Model, SVM (Support Vector Machine) Model and MLR (Multiple Linear Regressions) Model which were taken from the literature. Based on experiments conducted, it was found that GMDH can be applied as a sound alternative to the existing techniques used for software maintainability prediction since it assists in predicting the maintainability more accurately and precisely than prevailing models.},
	language = {en},
	number = {2},
	journal = {Software engineering},
	author = {Malhotra, Ruchika and Chug, Anuradha},
	year = {2012},
	pages = {19},
}

@article{jha_deep_2019,
	title = {Deep {Learning} {Approach} for {Software} {Maintainability} {Metrics} {Prediction}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2913349},
	abstract = {Software maintainability predicts changes or failures that may occur in software after it has been deployed. Since it deals with the degree to which an application may be understood, repaired, or enhanced, it also takes into account the overall cost of the project. In the past, several measures have been taken into account for predicting metrics that influence software maintainability. However, deep learning is yet to be explored for the same. In this paper, we perform deep learning for software maintainability metrics' prediction on a large number of datasets. Unlike the previous research works, we have relied on large datasets from 299 software and subsequently applied various metrics and functions to the same; 29 object-oriented metrics have been considered along with their impact on software maintainability of open source software. Several metrics have been analyzed and descriptive statistics of these metrics have been pointed out. The proposed long short term memory has been evaluated using measures, such as mean absolute error, root mean square error and accuracy. Five machine learning algorithms, namely, ridge regression with variable selection, decision tree, quantile regression forest, support vector machine, and principal component analysis have been applied to the original datasets, as well as, to the refined datasets. It was found that this paper provides results in the form of metrics that may be used in the prediction of software maintenance and the proposed deep learning model outperforms all of the other methods that were considered. Furthermore, the results of experiment affirm the efficiency of the proposed deep learning model for software maintainability prediction.},
	journal = {IEEE Access},
	author = {Jha, Sudan and Kumar, Raghvendra and Hoang Son, Le and Abdel-Basset, Mohamed and Priyadarshini, Ishaani and Sharma, Rohit and Viet Long, Hoang},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Cloning, Deep learning, Machine learning algorithms, Software, Software measurement, machine learning, prediction, software maintainability, software metrics},
	pages = {61840--61855},
}

@article{gold_ethics_2022,
	title = {Ethics in the mining of software repositories},
	volume = {27},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-021-10057-7},
	doi = {10.1007/s10664-021-10057-7},
	abstract = {Research in Mining Software Repositories (MSR) is research involving human subjects, as the repositories usually contain data about developers’ and users’ interactions with the repositories and with each other. The ethics issues raised by such research therefore need to be considered before beginning. This paper presents a discussion of ethics issues that can arise in MSR research, using the mining challenges from the years 2006 to 2021 as a case study to identify the kinds of data used. On the basis of contemporary research ethics frameworks we discuss ethics challenges that may be encountered in creating and using repositories and associated datasets. We also report some results from a small community survey of approaches to ethics in MSR research. In addition, we present four case studies illustrating typical ethics issues one encounters in projects and how ethics considerations can shape projects before they commence. Based on our experience, we present some guidelines and practices that can help in considering potential ethics issues and reducing risks.},
	language = {en},
	number = {1},
	urldate = {2021-11-03},
	journal = {Empirical Software Engineering},
	author = {Gold, Nicolas E. and Krinke, Jens},
	month = jan,
	year = {2022},
	pages = {17},
}

@inproceedings{luzgin_overview_2020,
	title = {Overview of {Mining} {Software} {Repositories}},
	doi = {10.1109/EIConRus49466.2020.9039225},
	abstract = {Mining of software repositories (MSR) provides opportunities to enhance software engineering activities and to analyze violations to prevent it. Software engineering quality and security challenges can be solved by analyzing unstructured information from various artifacts of software development. This paper describes current challenges and applications in the field. This work also proposes an overview of the directions of mining software repositories and describes the methods and techniques that are used in this area.},
	booktitle = {2020 {IEEE} {Conference} of {Russian} {Young} {Researchers} in {Electrical} and {Electronic} {Engineering} ({EIConRus})},
	author = {Luzgin, Victor A. and Kholod, Ivan I.},
	month = jan,
	year = {2020},
	note = {ISSN: 2376-6565},
	keywords = {Data mining, Deep learning, Natural language processing, Software, Software engineering, Software metrics, mining software repositories, software engineering, unstructured data},
	pages = {400--404},
}

@article{kaur_systematic_nodate,
	title = {Systematic {Literature} {Review} on {Mining} {Software} {Repositories}},
	abstract = {Mining Software Repositories (MSR) explores the complex software systems to unfold valuable and interesting knowledge. The systematic literature review is performed on MSR studies published over a decade. The review is conducted on 300 selected papers. The study aims to identify – i) popular application areas, ii) popular tools, iii) datasets and software projects, along with their type and SDLC phase in which they are used, iv) popular techniques, v) top conferences and journals along with the types of studies and vi) current trends, of MSR research. The important results implied from the reviewed studies are: 1) Bug Prediction (25\%) and Change Prediction (17\%) are the most popular application areas of MSR, 2) Data Modelling and Statistical Tools, (19\%) were employed in most of the reviewed studies, 3) About 71 \% of reviewed studies use datasets from open source repositories, 4) There is a large number of mining techniques with Classification techniques (29\%) dominating the field, 5) The most popular conference for MSR is the International Conference on Mining Software Repositories (MSR), and 6) Research gaps in evolving application areas of clone detection, code reuse and software evolution are identified.},
	language = {en},
	author = {Kaur, Arvinder and Kaur, Kamaldeep and Chopra, Deepti and Kaur, Harguneet},
	pages = {37},
}

@article{barros_mining_2021,
	title = {A {Mining} {Software} {Repository} {Extended} {Cookbook}: {Lessons} learned from a literature review},
	shorttitle = {A {Mining} {Software} {Repository} {Extended} {Cookbook}},
	url = {http://arxiv.org/abs/2110.04095},
	abstract = {The main purpose of Mining Software Repositories (MSR) is to discover the latest enhancements and provide an insight into how to make improvements in a software project. In light of it, this paper updates the MSR findings of the original MSR Cookbook, by first conducting a systematic mapping study to elicit and analyze the state-of-the-art, and then proposing an extended version of the Cookbook. This extended Cookbook was built on four high-level themes, which were derived from the analysis of a list of 112 selected studies. Hence, it was used to consolidate the extended Cookbook as a contribution to practice and research in the following areas by: 1) including studies published in all available and relevant publication venues; 2) including and updating recommendations in all four high-level themes, with an increase of 84\% in comments in this study when compared with the original MSR Cookbook; 3) summarizing the tools employed for each high-level theme; and 4) providing lessons learned for future studies. Thus, the extended Cookbook examined in this work can support new research projects, as upgraded recommendations and the lessons learned are available with the aid of samples and tools.},
	urldate = {2021-11-01},
	journal = {arXiv:2110.04095 [cs]},
	author = {Barros, Daniel and Horita, Flavio and Wiese, Igor and Silva, Kanan},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.04095},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{romano_g-repo_2021,
	title = {G-{Repo}: a {Tool} to {Support} {MSR} {Studies} on {GitHub}},
	shorttitle = {G-{Repo}},
	doi = {10.1109/SANER50967.2021.00064},
	abstract = {GitHub currently hosts more than 100 million public repositories. This has made it very popular to conduct Mining Software Repositories (MSR) studies. Researchers have been exploiting the information stored in GitHub (e.g., commits, pull requests, or issues) to investigate both developer- and project-related aspects. GitHub provides the REST API to make queries without cloning repositories. In this tool-demo paper, we highlight some issues we noticed when conducting an MSR study on GitHub by using the REST API and present G-Repo: a tool developed to support researchers when tackling these issues able to ease the creation of datasets for MSR studies. Also, we provide a manually-annotated dataset with information about the kind and the (spoken) languages of 1,500 repositories hosted on GitHub. A video showing the functioning of G-Repo is available at: https://youtu.be/mb9CIALBFZk.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Romano, Simone and Caulo, Maria and Buompastore, Matteo and Guerra, Leonardo and Mounsif, Anas and Telesca, Michele and Baldassarre, Maria Teresa and Scanniello, Giuseppe},
	month = mar,
	year = {2021},
	note = {ISSN: 1534-5351},
	keywords = {Cloning, Conferences, Data mining, G-Repo, GitHub, MSR, Manuals, Software, Software development management, Tools},
	pages = {551--555},
}

@misc{noauthor_mining_nodate,
	title = {Mining {Industry} {Project} {Management} {Software}},
	url = {https://www.aresprism.com/mining-industry-project-management-software/},
	abstract = {ARES PRISM Software is an enterprise solution vastly used by mining industry giants to digitize their companies \& ensure their projects are on-time \& on budget},
	language = {en-US},
	urldate = {2021-11-01},
	journal = {ARES PRISM},
}

@article{nosco_industrial_nodate,
	title = {The {Industrial} {Age} of {Hacking}},
	abstract = {There is a cognitive bias in the hacker community to select a piece of software and invest signiﬁcant human resources into ﬁnding bugs in that software without any prior indication of success. We label this strategy depth-ﬁrst search and propose an alternative: breadth-ﬁrst search. In breadthﬁrst search, humans perform minimal work to enable automated analysis on a range of targets before committing additional time and effort to research any particular one. We present a repeatable human study that leverages teams of varying skill while using automation to the greatest extent possible. Our goal is a process that is effective at ﬁnding bugs; has a clear plan for the growth, coaching, and efﬁcient use of team members; and supports measurable, incremental progress. We derive an assembly-line process that improves on what was once intricate, manual work. Our work provides evidence that the breadth-ﬁrst approach increases the effectiveness of teams.},
	language = {en},
	author = {Nosco, Tim and Ziegler, Jared and Clark, Zechariah},
	pages = {19},
}

@inproceedings{proksch_enriched_2018,
	address = {Gothenburg Sweden},
	title = {Enriched event streams: a general dataset for empirical studies on in-{IDE} activities of software developers},
	isbn = {978-1-4503-5716-6},
	shorttitle = {Enriched event streams},
	url = {https://dl.acm.org/doi/10.1145/3196398.3196400},
	doi = {10.1145/3196398.3196400},
	abstract = {Developers have been the subject of many empirical studies over the years. To assist developers in their everyday work, an understanding of their activities is necessary, especially how they develop source code. Unfortunately, conducting such studies is very expensive and researchers often resort to studying artifacts after the fact. To pave the road for future empirical studies on developer activities, we built FeedBaG, a general-purpose interaction tracker for Visual Studio that monitors development activities. The observations are stored in enriched event streams that encode a holistic picture of the in-IDE development process. Enriched event streams capture all commands invoked in the IDE with additional context information, such as the test being run or the accompanying fine-grained code edits. We used FeedBaG to collect enriched event streams from 81 developers. Over 1,527 days, we collected more than 11M events that correspond to 15K hours of working time.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Proksch, Sebastian and Amann, Sven and Nadi, Sarah},
	month = may,
	year = {2018},
	pages = {62--65},
}

@article{odom_research_2016,
	title = {From research prototype to research product},
	doi = {10.1145/2858036.2858447},
	abstract = {Prototypes and prototyping have had a long and important history in the HCI community and have played a highly significant role in creating technology that is easier and more fulfilling to use. Yet, as focus in HCI is expanding to investigate complex matters of human relationships with technology over time in the intimate and contested contexts of everyday life, the notion of a 'prototype' may not be fully sufficient to support these kinds of inquiries. We propose the research product as an extension and evolution of the research prototype to support generative inquiries in this emerging research area. We articulate four interrelated qualities of research products-inquiry-driven, finish, fit, and independent-and draw on these qualities to describe and analyze five different yet related design research cases we have collectively conducted over the past six years. We conclude with a discussion of challenges and opportunities for crafting research products and the implications they suggest for future design-oriented HCI research.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Odom, William and Wakkary, Ron and Lim, Youn Kyung and Desjardins, Audrey and Hengeveld, Bart and Banks, Richard},
	year = {2016},
	note = {ISBN: 9781450333627},
	keywords = {Design, Interaction design research, Research product},
	pages = {2549--2561},
}

@article{eisty_use_2019,
	title = {Use of software process in research software development: {A} survey},
	doi = {10.1145/3319008.3319351},
	abstract = {Background: Developers face challenges in building high-quality research software due to its inherent complexity. These challenges can reduce the confidence users have in the quality of the result produced by the software. Use of a defined software development process, which divides the development into distinct phases, results in improved design, more trustworthy results, and better project management. Aims: This paper focuses on gaining a better understanding of the use of software development process for research software. Method: We surveyed research software developers to collect information about their use of software development processes. We analyze whether and demographic factors influence the respondents’ use of and perceived value in defined process. Results: Based on 98 responses, research software developers appear to follow a defined software development process at least some of the time. The respondents also have a strong positive perception about the value of following processes. Conclusions: To produce high-quality and reliable research software, which is critical for many research domains, research software developers must follow a proper software development process. The results indicate a positive perception of value about using defined development processes that should lead to both short-term benefits through improved results and long-term benefits through more maintainable software.},
	journal = {ACM International Conference Proceeding Series},
	author = {Eisty, Nasir U. and Thiruvathukal, George K. and Carver, Jeffrey C.},
	year = {2019},
	note = {ISBN: 9781450371452},
	keywords = {Research software, Software process, Survey},
	pages = {276--282},
}

@article{redmon_yolo_2018,
	title = {{YOLO} v.3},
	url = {https://pjreddie.com/media/files/papers/YOLOv3.pdf},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP 50 in 51 ms on a Titan X, compared to 57.5 AP 50 in 198 ms by RetinaNet, similar performance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.},
	journal = {Tech report},
	author = {Redmon, Joseph and Farhadi, Ali},
	year = {2018},
	pages = {1--6},
}

@article{rushby_evaluation_1990,
	title = {Evaluation of {Safety}- {Critical} {Software}},
	volume = {33},
	number = {6},
	author = {Rushby, John and Parnas, David L. and Schouwen, A. John Van and Kwan, Shu P. O.},
	year = {1990},
}

@article{rekdal_academic_2014,
	title = {Academic urban legends},
	volume = {44},
	issn = {14603659},
	doi = {10.1177/0306312714535679},
	abstract = {Many of the messages presented in respectable scientific publications are, in fact, based on various forms of rumors. Some of these rumors appear so frequently, and in such complex, colorful, and entertaining ways that we can think of them as academic urban legends. The explanation for this phenomenon is usually that authors have lazily, sloppily, or fraudulently employed sources, and peer reviewers and editors have not discovered these weaknesses in the manuscripts during evaluation. To illustrate this phenomenon, I draw upon a remarkable case in which a decimal point error appears to have misled millions into believing that spinach is a good nutritional source of iron. Through this example, I demonstrate how an academic urban legend can be conceived and born, and can continue to grow and reproduce within academia and beyond. © 2014, The Author(s). All rights reserved.},
	number = {4},
	journal = {Social Studies of Science},
	author = {Rekdal, Ole Bjørn},
	year = {2014},
	pmid = {25272616},
	keywords = {academic shortcuts, academic urban legends, citation practices, iron, spinach},
	pages = {638--654},
}

@article{joanne_wendelberger_ross_whitaker_workshop_nodate,
	title = {Workshop on {Quantification}, {Communication}, and {Interpretation} of {Uncertainty} in {Simulatio} and {Data} {Science}},
	abstract = {Eigene Verhaltensweisen zu unterbrechen, zu unterdr\{ü\}cken oder in anderer Weise zu ver\{ä\}ndern, Pl\{ä\}ne zu schmieden und langfristig auch gegen innere und \{ä\}u\{ß\}ere Widerst\{ä\}nde eigene Ziele zu verfolgen sowie Versuchungen oder Ablenkungen zu widerstehen geh\{ö\}rt zu den beeindruckendsten psychischen Funktionen des Menschen (Carver \{\&\} Scheier, 1981; Baumeister, Heatherton \{\&\} Tice, 1994).},
	author = {Joanne Wendelberger Ross Whitaker, William Thompson, James Berger, Baruch Fischhof, Michael Goodchild, Mary Hegarty, Christopher Jermaine, Kathryn S. McKinley, Alex Pang},
}

@article{sampson_expressing_2014,
	title = {Expressing and {Verifying} {Probabilistic} {Assertions}},
	author = {Sampson, Adrian and Panchekha, Pavel and Mytkowicz, Todd and McKinley, Kathryn S. and Grossman, Dan and Ceze, Luis},
	year = {2014},
	note = {ISBN: 9781450327848},
	keywords = {a search, a variable, abilistic data, approximate computing, consume prob-, data obfuscation, differential privacy, however, many applications produce or, of a document to, probabilistic programming, sensors, such as the relevance, symbolic execution, the},
}

@article{bond_probabilistic_2007,
	title = {Probabilistic calling context},
	doi = {10.1145/1297027.1297035},
	abstract = {Calling context enhances program understanding and dynamic analyses by providing a rich representation of program location. Compared to imperative programs, objectoriented programs use more interprocedural and less intraprocedural control flow, increasing the importance of context sensitivity for analysis. However, prior online methods for computing calling context, such as stack-walking or maintaining the current location in a calling context tree, are expensive in time and space. This paper introduces a new online approach called probabilistic calling context (PCC) that continuously maintains a probabilistically unique value representing the current calling context. For millions of unique contexts, a 32-bit PCC value has few conflicts. Computing the PCC value adds 3\% average overhead to a Java virtual machine. PCC is well-suited to clients that detect new or anomalous behavior since PCC values from training and production runs can be compared easily to detect new contextsensitive behavior; clients that query the PCC value at every system call, Java utility call, and Java API call add 0-9\% overhead on average. PCC adds space overhead proportional to the distinct contexts stored by the client (one word per context). Our results indicate PCC is efficient and accurate enough to use in deployed software for residual testing, bug detection, and intrusion detection. Copyright © 2007 ACM.},
	journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
	author = {Bond, Michael D. and McKinley, Kathryn S.},
	year = {2007},
	note = {ISBN: 9781595937865},
	keywords = {Anomaly-based bug detection, Calling context, Dynamic context sensitivity, Intrusion detection, Managed languages, Probabilistic, Residual testing},
	pages = {97--111},
}

@article{bomholt_m_2009,
	title = {M anaging {An} {U} ncertain {F} uture},
	volume = {4},
	number = {October},
	author = {Bomholt, James},
	year = {2009},
	pages = {1--34},
}

@article{bornholt_uncertainlttgt_2014,
	title = {Uncertain\&lt;t\&gt;},
	issn = {0362-1340},
	url = {http://dl.acm.org/citation.cfm?doid=2541940.2541958},
	abstract = {Emerging applications increasingly use estimates such as sensor data (GPS), probabilistic models, machine learning, big data, and human data. Unfortunately, representing this uncertain data with discrete types (floats, integers, and booleans) encourages developers to pretend it is not probabilistic, which causes three types of uncertainty bugs. (1) Using estimates as facts ignores random error in estimates. (2) Computation compounds that error. (3) Boolean questions on probabilistic data induce false positives and negatives. This paper introduces Uncertain〈T〉, a new programming language abstraction for uncertain data. We implement a Bayesian network semantics for computation and conditionals that improves program correctness. The runtime uses sampling and hypothesis tests to evaluate computation and conditionals lazily and efficiently. We illustrate with sensor and machine learning applications that Uncertain〈T〉 improves expressiveness and accuracy. Whereas previous probabilistic programming languages focus on experts, Uncertain〈T〉 serves a wide range of developers. Experts still identify error distributions. However, both experts and application writers compute with distributions, improve estimates with domain knowledge, and ask questions with conditionals. The Uncertain〈T〉 type system and operators encourage developers to expose and reason about uncertainty explicitly, controlling false positives and false negatives. These benefits make Uncertain〈T〉 a compelling programming model for modern applications facing the challenge of uncertainty. Copyright © 2014 ACM.},
	journal = {Proceedings of the 19th international conference on Architectural support for programming languages and operating systems - ASPLOS '14},
	author = {Bornholt, James and Mytkowicz, Todd and McKinley, Kathryn S.},
	year = {2014},
	note = {ISBN: 9781450323055},
	keywords = {estimates, probabilistic programming, uncertain data},
	pages = {51--66},
}

@article{wang_achieving_2009,
	title = {Achieving high and consistent rendering performance of java {AWT}/{Swing} on multiple platforms},
	volume = {39},
	issn = {00380644},
	doi = {10.1002/spe},
	abstract = {Wang et al. (Softw. Pract. Exper. 2007; 37(7):727-745) observed a phenomenon of performance inconsistency in the graphics of Java Abstract Window Toolkit (AWT)/Swing among different Java runtime environments (JREs) on Windows XP. This phenomenon makes it difficult to predict the performance of Java game applications. Therefore, they proposed a portable AWT/Swing architecture, called CYC Window Toolkit (CWT), to provide programmers with high and consistent rendering performance for Java game development among different JREs. They implemented a DirectX version to demonstrate the feasibility of the architecture. This paper extends the above research to other environments in two aspects. First, we evaluate the rendering performance of the original Java AWT with different combinations of JREs, image application programming interfaces, system properties and operating systems (OSs), including Windows XP, Windows Vista, Fedora and Mac OS X. The evaluation results indicate that the performance inconsistency of Java AWT also exists among the four OSs, even if the same hardware configuration is used. Second, we design an OpenGL version of CWT, named CWT-GL, to take advantage of modern 3D graphics cards, and compare the rendering performance of CWT with Java AWT/Swing. The results show that CWT-GL achieves more consistent and higher rendering performance in JREs 1.4to 1.6 on the four OSs. The results also hint at two approaches: (a) decouple the rendering pipelines of Java AWT/Swing from the JREs for faster upgrading and supporting old JREs and (b) use other graphics libraries, such as CWT, instead of Java AWT/Swing to develop cross-platform Java games with higher and more consistent rendering performance © 2009 John Wiley \& Sons, Ltd.},
	number = {7},
	journal = {Software - Practice and Experience},
	author = {Wang, Yi Hsien and Wu, I. Chen},
	year = {2009},
	keywords = {CYC Window Toolkit, Directx, Linux, Mac OS x, OpenGL, Windows},
	pages = {701--736},
}

@article{nandi_debugging_2017,
	title = {Debugging probabilistic programs},
	doi = {10.1145/3088525.3088564},
	abstract = {Many applications compute with estimated and uncertain data. While advances in probabilistic programming help developers build such applications, debugging them remains extremely challenging. New types of errors in probabilistic programs include 1) ignoring dependencies and correlation between random variables and in training data, 2) poorly chosen inference hyper-parameters, and 3) incorrect statistical models. A partial solution to prevent these errors in some languages forbids developers from explicitly invoking inference. While this prevents some dependence errors, it limits composition and control over inference, and does not guarantee absence of other types of errors. This paper presents the FLEXI programming model which supports constructs for invoking inference in the language and reusing the results in other statistical computations. We define a novel formalism for inference with a Decorated Bayesian Network and present a tool, DePP, that analyzes this representation to identify the above errors. We evaluate DePP on a range of prototypical examples to show how it helps developers to detect errors.},
	journal = {MAPL 2017 - Proceedings of the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, co-located with PLDI 2017},
	author = {Nandi, Chandrakana and Grossman, Dan and Sampson, Adrian and Mytkowicz, Todd and McKinley, Kathryn S.},
	year = {2017},
	note = {ISBN: 9781450350716},
	keywords = {Debugging, Probabilistic programming, Program analysis, Statistical inference},
	pages = {18--26},
}

@article{cito_empirical_2020,
	title = {An {Empirical} {Study} on the {Impact} of {Parameters} on {Mobile} {App} {Energy} {Usage}},
	author = {Cito, Jürgen and Rinard, Martin},
	year = {2020},
	note = {ISBN: 9781450375177},
	pages = {598--601},
}

@article{akhlaghi_towards_2021,
	title = {Towards {Long}-term and {Archivable} {Reproducibility}},
	issn = {1558366X},
	doi = {10.1109/MCSE.2021.3072860},
	abstract = {Analysis pipelines commonly use high-level technologies that are popular when created, but are unlikely to be readable, executable, or sustainable in the long term. A set of criteria is introduced to address this problem: Completeness (no execution requirement beyond a minimal Unix-like operating system, no administrator privileges, no network connection, and storage primarily in plain text); modular design; minimal complexity; scalability; verifiable inputs and outputs; version control; linking analysis with narrative; and free software. As a proof of concept, we introduce "Maneage" (Managing data lineage), enabling cheap archiving, provenance extraction, and peer verification that been tested in several research publications. We show that longevity is a realistic requirement that does not sacrifice immediate or short-term reproducibility. The caveats (with proposed solutions) are then discussed and we conclude with the benefits for the various stakeholders. This paper is itself written with Maneage (project commit eeff5de).},
	number = {April},
	journal = {Computing in Science and Engineering},
	author = {Akhlaghi, Mohammad and Infante-Sainz, Raul and Roukema, Boudewijn and Khellat, Mohammadreza and Valls-Gabaud, David and Galle, Roberto Baena},
	year = {2021},
	keywords = {Buildings, Containers, Kernel, Libraries, Software, Tools, Virtual machining},
}

@article{oberhauser_software_2015,
	title = {From {Software} {Engineering} {Process} {Models} to {Operationally} {Relevant} {Context}-aware {Workflows} : {A} {Model}-{Driven} {Method}},
	volume = {8},
	number = {1},
	author = {Oberhauser, Roy},
	year = {2015},
	keywords = {- process-centered software engineering, engineering process modeling, environments, model transformation, software, software engineering environments, software engineering process, spem, uma, unified method},
	pages = {167--181},
}

@article{meyer_software_2001,
	title = {Software engineering in the academy},
	volume = {34},
	issn = {00189162},
	doi = {10.1109/2.920608},
	number = {5},
	journal = {Computer},
	author = {Meyer, Bertrand},
	year = {2001},
	pages = {28},
}

@article{grambow_towards_2011,
	title = {Towards a workflow language for software engineering},
	doi = {10.2316/P.2011.720-020},
	abstract = {Software development processes are broadly used by software providers to ensure the quality and reproducibility of their development endeavors. These processes are typically abstractly defined, individually interpreted by individuals, and manually executed, making governance and compliance difficult. Additionally, process tailoring, reuse, exchange, and any IT-based automation or guidance at the more practical lower level workflows is hindered or more burdensome without a common language for expression. Automated guidance and highly integrated process support holds potential for retaining process-centered advantages while reducing hindrances. In this paper, work on a language for the description of software engineering processes is presented. It unifies the abstract specification and documentation of processes with automated process enactment support, while, in turn, fostering reusability and tailoring of these processes. For enactment, various workflow management systems can be chosen whose models are automatically generated. The approach shows promise for enabling IT process support in the software engineering domain while supporting the exchange and objective comparison of enactable processes and practices.},
	journal = {Proceedings of the 10th IASTED International Conference on Software Engineering, SE 2011},
	author = {Grambow, Gregor and Oberhauser, Roy and Reichert, Manfred},
	year = {2011},
	note = {ISBN: 9780889868649},
	keywords = {Process enactment, Process language, Process modeling, Process reuse, Process-centered software engineering, Software engineering environments, Workflow management},
	pages = {130--137},
}

@article{salayandia_model-based_2006,
	title = {A {Model}-{Based} {Workflow} {Approach} for {Scientific} {Applications}},
	journal = {Proceedings of the 6th OOPSLA Workshop on Domain-Specific Modeling},
	author = {Salayandia, Leonardo and Silva, Paulo Pinheiro Da and Gates, Ann Q. and Rebellon, Alvaro},
	year = {2006},
}

@book{shull_guide_2008,
	title = {Guide to advanced empirical software engineering},
	isbn = {978-1-84800-043-8},
	abstract = {Empirical studies have become an integral element of software engineering research and practice. This unique text/reference includes chapters from some of the top international empirical software engineering researchers and focuses on the practical knowledge necessary for conducting, reporting and using empirical methods in software engineering. Part 1, 'Research Methods and Techniques', examines the proper use of various strategies for collecting and analysing data, and the uses for which those strategies are most appropriate. Part 2, 'Practical Foundations', provides a discussion of several important global issues that need to be considered from the very beginning of research planning. Finally, 'Knowledge Creation' offers insight on using a set of disparate studies to provide useful decision support. Topics and features: Offers information across a range of techniques, methods, and qualitative and quantitative issues, providing a toolkit for the reader that is applicable across the diversity of software development contexts Presents reference material with concrete software engineering examples Provides guidance on how to design, conduct, analyse, interpret and report empirical studies, taking into account the common difficulties and challenges encountered in the field Arms researchers with the information necessary to avoid fundamental risks Tackles appropriate techniques for addressing disparate studies - ensuring the relevance of empirical software engineering, and showing its practical impact Describes methods that are less often used in the field, providing less conventional but still rigorous and useful ways of collecting data Supplies detailed information on topics (such as surveys) that often contain methodological errors This broad-ranging, practical guide will prove an invaluable and useful reference for practising software engineers and researchers. In addition, it will be suitable for graduate students studying empirical methods in software development. © Springer-Verlag London Limited 2008.},
	author = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	year = {2008},
	doi = {10.1007/978-1-84800-044-5},
	note = {Publication Title: Guide to Advanced Empirical Software Engineering},
}

@article{rochaeli_using_2007,
	title = {Using patterns paradigm to refine workflow policies},
	issn = {15294188},
	doi = {10.1109/DEXA.2007.63},
	abstract = {We propose an approach to formalize the patterns and to automatically apply the formalized patterns. In our case, we use the pattern paradigm to refine security policies of a workflow. A policy refinement process derives low-level workflow policies from high-level and abstract policies specified by stakeholders. Such refinement process requires domain-specific expertise knowledge, which will be captured by using the pattern paradigm. These refinement patterns are formalized by using both description logic and temporal logic formalisms. © 2007 IEEE.},
	journal = {Proceedings - International Workshop on Database and Expert Systems Applications, DEXA},
	author = {Rochaeli, Taufiq and Eckert, Claudia},
	year = {2007},
	note = {ISBN: 0769529321},
	pages = {760--764},
}

@article{godoi_final_2021,
	title = {Final report: {National} {Security} {Commission} on {Artificial} {Intelligence}},
	abstract = {National Security Commission on Artificial Intelligence\_Full-Report-Digital-1},
	journal = {Public},
	author = {Godoi, Fernanda C. and Prakash, Sangeeta and Bhandari, Bhesh R.},
	year = {2021},
	note = {ISBN: 9789221336228},
}

@article{thiruvathukal_software_2013,
	title = {Software {Engineering} {Need} not be {Difficult}},
	url = {http://dx.doi.org/10.6084/m9.figshare.830442},
	abstract = {Position paper on how to apply the principles of software engineering and agile methods to support sustainable scientific software. Accepted at the WSSSPE workshop at SC 13.},
	journal = {Workshop on Sustainable Software for Science: Practice and Experiences at SC13},
	author = {Thiruvathukal, G. K. and J, Carver},
	year = {2013},
	pages = {1--6},
}

@article{herrmannsfeldt_1_1990,
	title = {1. {Introducti} o n},
	author = {Herrmannsfeldt, W. B.},
	year = {1990},
	pages = {1--27},
}

@article{iot__nodate,
	title = {“ {If} security is required ”: {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	author = {Iot, Ml-enabled and Iot, Ml-based and Iot, Ml-based and Ml, The},
}

@article{michael_regexes_2019,
	title = {Regexes are hard: {Decision}-making, difficulties, and risks in programming regular expressions},
	doi = {10.1109/ASE.2019.00047},
	abstract = {Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.},
	journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
	author = {Michael, Louis G. and Donohue, James and Davis, James C. and Lee, Dongyoon and Servant, Francisco},
	year = {2019},
	note = {ISBN: 9781728125084},
	keywords = {Developer process, Qualitative research, Regular expressions},
	pages = {415--426},
}

@article{approach_fault-tolerant_1985,
	title = {to {Fault}-{Tolerant} {Software}},
	number = {12},
	author = {Approach, N.-version},
	year = {1985},
	pages = {1491--1501},
}

@article{blackford_updated_2002,
	title = {An {Updated} {Set} of {Basic} {Linear} {Algebra} {Subprograms} ({BLAS})},
	volume = {28},
	issn = {00983500},
	doi = {10.1145/567806.567807},
	number = {2},
	journal = {ACM Transactions on Mathematical Software},
	author = {Blackford, L. Susan and Demmel, James and Dongarra, Jack and Duff, Iain and Hammarling, Sven and Henry, Greg and Heroux, Michael and Kaufman, Linda and Lumsdaine, Andrew and Petitet, Antoine and Pozo, Roldan and Remington, Karin and Whaley, R. Clint},
	year = {2002},
	keywords = {Algorithms, BLAS, G.1.3 [Numerical Analysis]: Numerical Linear Algeb, G.4 [Mathematical Software], Linear algebra, Standardization, Standards},
	pages = {135--151},
}

@article{goswami_investigating_2020,
	title = {Investigating the {Reproducibility} of {NPM} {Packages}},
	doi = {10.1109/ICSME46990.2020.00071},
	abstract = {Node.js has been popularly used for web application development, partially because of its large software ecosystem known as NPM (Node Package Manager) packages. When using open-source NPM packages, most developers download prebuilt packages on npmjs.com instead of building those packages from available source, and implicitly trust the downloaded packages. However, it is unknown whether the blindly trusted prebuilt NPM packages are reproducible (i.e., whether there is always a verifiable path from source code to any published NPM package). Therefore, for this paper, we conducted an empirical study to examine the reproducibility of NPM packages, and to understand why some packages are not reproducible.Specifically, we downloaded versions/releases of 226 most popularly used NPM packages and then built each version with the available source on GitHub. Next, we applied a differencing tool to compare the versions we built against versions downloaded from NPM, and further inspected any reported difference. Among the 3,390 versions of the 226 packages, only 2,087 versions are reproducible. Based on our manual analysis, multiple factors contribute to the non-reproducibility issues, such as flexible versioning information in package.json file and the divergent behaviors between distinct versions of tools used in the build process. Our investigation reveals challenges of verifying NPM reproducibility with existing tools, and provides insights for future verifiable build procedures.},
	journal = {Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020},
	author = {Goswami, Pronnoy and Gupta, Saksham and Li, Zhiyuan and Meng, Na and Yao, Daphne},
	year = {2020},
	note = {ISBN: 9781728156194},
	keywords = {JavaScript, NPM packages, reproducibility},
	pages = {677--681},
}

@article{nascimento_software_2020,
	title = {Software engineering for artificial intelligence and machine learning software: {A} systematic literature review},
	url = {http://arxiv.org/abs/2011.03751},
	abstract = {Artificial Intelligence (AI) or Machine Learning (ML) systems have been widely adopted as value propositions by companies in all industries in order to create or extend the services and products they offer. However, developing AI/ML systems has presented several engineering problems that are different from those that arise in, non-AI/ML software development. This study aims to investigate how software engineering (SE) has been applied in the development of AI/ML systems and identify challenges and practices that are applicable and determine whether they meet the needs of professionals. Also, we assessed whether these SE practices apply to different contexts, and in which areas they may be applicable. We conducted a systematic review of literature from 1990 to 2019 to (i) understand and summarize the current state of the art in this field and (ii) analyze its limitations and open challenges that will drive future research. Our results show these systems are developed on a lab context or a large company and followed a research-driven development process. The main challenges faced by professionals are in areas of testing, AI software quality, and data management. The contribution types of most of the proposed SE practices are guidelines, lessons learned, and tools.},
	author = {Nascimento, Elizamary and Nguyen-Duc, Anh and Sundbø, Ingrid and Conte, Tayana},
	year = {2020},
	keywords = {artificial intelligence, machine learning, practices, software engineering},
}

@article{shetty_neural_2021,
	title = {Neural {Knowledge} {Extraction} {From} {Cloud} {Service} {Incidents}},
	doi = {10.1109/icse-seip52600.2021.00031},
	abstract = {In the last decade, two paradigm shifts have reshaped the software industry - the move from boxed products to services and the widespread adoption of cloud computing. This has had a huge impact on the software development life cycle and the DevOps processes. Particularly, incident management has become critical for developing and operating large-scale services. Incidents are created to ensure timely communication of service issues and, also, their resolution. Prior work on incident management has been heavily focused on the challenges with incident triaging and de-duplication. In this work, we address the fundamental problem of structured knowledge extraction from service incidents. We have built SoftNER, a framework for unsupervised knowledge extraction from service incidents. We frame the knowledge extraction problem as a Named-entity Recognition task for extracting factual information. SoftNER leverages structural patterns like key,value pairs and tables for bootstrapping the training data. Further, we build a novel multi-task learning based BiLSTM-CRF model which leverages not just the semantic context but also the data-types for named-entity extraction. We have deployed SoftNER at Microsoft, a major cloud service provider and have evaluated it on more than 2 months of cloud incidents. We show that the unsupervised machine learning based approach has a high precision of 0.96. Our multi-task learning based deep learning model also outperforms the state of the art NER models. Lastly, using the knowledge extracted by SoftNER we are able to build significantly more accurate models for important downstream tasks like incident triaging.},
	author = {Shetty, Manish and Bansal, Chetan and Kumar, Sumit and Rao, Nikitha and Nagappan, Nachiappan and Zimmermann, Thomas},
	year = {2021},
	keywords = {all or part of, cloud services, deep, knowledge extraction, learning, machine learning, or, or hard copies of, permission to make digital, service incidents, this work for personal},
	pages = {218--227},
}

@article{nguyen_consistent_1994,
	title = {The {Consistent} {Comparison} {Problem} in {N}-{Version} {Software}},
	volume = {6},
	issn = {10414347},
	doi = {10.1109/69.334887},
	abstract = {Knowledge base is the most important component in a knowledge-based system. Because a knowledge base is often built in an incremental, piecemeal fashion, potential errors may be inadvertently brought into it. One of the critical issues in developing reliable knowledge-based systems is how to verify the correctness of a knowledge base. This concise paper describes an automated tool called PREPARE for detecting potential errors in a knowledge base. PREPARE is based on modeling a knowledge base by using a Predicate/Transition net representation. Inconsistent, redundant, subsumed, circular, and incomplete rules in a knowledge base are then defined as patterns of the Predicate/Transition net model, and are detected through a syntactic pattern recognition method. The research results to date have indicated that 1) the methodology can be adopted in knowledge-based systems where logic is used as knowledge representation formalism; 2) the tool can be invoked at any stage of the system’s development, even without a fully functioning inference engine; 3) the Predicate/Transition net model of knowledge bases is easy to implement and provides a clear and understandable display of the knowledge to be used by the system. © 1994 IEEE},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Nguyen, Doan and Zhang, D.},
	year = {1994},
	keywords = {Knowledge base, Predicate/Transition nets, circularity and incompleteness, inconsistency, redundancy, syntactic pattern recognition, verification},
	pages = {983--989},
}

@article{knight_experimental_1986,
	title = {An {Experimental} {Evaluation} of the {Assumption} of {Independence} in {Multiversion} {Programming}},
	volume = {SE-12},
	issn = {00985589},
	doi = {10.1109/TSE.1986.6312924},
	abstract = {N-version programming has been proposed as a method of incorporating fault tolerance into software. Multiple versions of a program (i.e., “N”) are prepared and executed in parallel. Their outputs are collected and examined by a voter, and, if they are not identical, it is assumed that the majority is correct. This method depends for its reliability improvement on the assumption that programs that have been developed independently will fail independently. In this paper an experiment is described in which the fundamental axiom is tested. A total of 27 versions of a program were prepared independently from the same specification at two universities and then subjected to one million tests. The results of the tests revealed that the programs were individually extremely reliable but that the number of tests in which more than one program failed was substantially more than expected. The results of these tests are presented along with an analysis of some of the faults that were found in the programs. Background information on the programmers used is also summarized. The conclusion from this experiment is that N-version programming must be used with care and that analysis of its reliability must include the effect of dependent errors. © 1986 IEEE},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Knight, John C. and Leveson, Nancy G.},
	year = {1986},
	note = {Publisher: IEEE},
	keywords = {Design diversity, N-version programming, fault-tolerant software, multiversion, programming, software reliability},
	pages = {96--109},
}

@article{thiruvathukal_how_2020,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	number = {1},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
	year = {2020},
}

@article{sangeetha_deep_2006,
	title = {Deep {Residual} {Learning} for {Image} {Recognition} {Kaiming}},
	volume = {45},
	issn = {03764699},
	doi = {10.1002/chin.200650130},
	abstract = {An elegant one pot syntheses of the titled compounds 3a-d, 4a-d and 6a-d have been presented starting from 1-hydroxycarbazole-2-carbaldehydes 2a-d in good yields. Treatment of 1-hydroxycarbazole-2-carbaldehydes 2a-d with chloroacetone and with o-aminothiophenol have afforded the novel 2-acetylfuro[2,3-a]carbazoles 3a-d and benzo-[1,2-b]-1,4-thiazepino[2,3-a] carbazoles 4a-d respectively. Further carbazoles 2a-d are treated with phenyl acetic acid in an attempt to synthesize 3-phenyl-2-oxopyrano[2,3-a]-carbazoles 5a-d, but the reaction did not proceed in the anticipated direction and only acetyl derivatives 6a-d are obtained. All the products thus obtained from these reactions are well characterized by spectroscopic and analytical data.},
	number = {8},
	journal = {Indian Journal of Chemistry - Section B Organic and Medicinal Chemistry},
	author = {Sangeetha, V. and Prasad, K. J. Rajendra},
	year = {2006},
	keywords = {1-hydroxycarbazole-2-carbaldehydes, 2-acetylfuro carbazoles, Benzo carbazoles, Phenyl oxopyranocarbazoles, o-aminothiophenol},
	pages = {1951--1954},
}

@article{plastiras_you_2018,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	doi = {10.1145/3243394.3243692},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to per- form detection. Instead, we frame object detection as a re- gression problem to spatially separated bounding boxes and associated class probabilities. A single neural network pre- dicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detec- tors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations ofobjects. It outperforms other de- tection methods, including DPM and R-CNN, when gener- alizing from natural images to other domains like artwork.},
	journal = {ACM International Conference Proceeding Series},
	author = {Plastiras, George and Kyrkou, Christos and Theocharides, Theocharis},
	year = {2018},
	note = {ISBN: 9781450365116},
}

@article{theisen_writing_2017,
	title = {Writing good software engineering research papers: {Revisited}},
	doi = {10.1109/ICSE-C.2017.51},
	abstract = {With the goal of helping software engineering researchers understand how to improve their papers, Mary Shaw presented 'Writing Good Software Engineering Research Papers' in 2003. Shaw analyzed the abstracts of the papers submitted to the 2002 International Conference of Software Engineering (ICSE) to determine trends in research question type, contribution type, and validation approach. We revisit Shaw's work to see how the software engineering research community has evolved since 2002. The goal of this paper is to aid software engineering researchers in understanding trends in research question design, research question type, and validation approach by analyzing the abstracts of the papers submitted to ICSE 2016. We implemented Shaw's recommendation for replicating her study through the use of multiple coders and the calculation of inter-rater reliability and demonstrate that her approach can be repeated. Our results indicate that reviewers have increased expectations that papers have solid evaluations of the research contribution. Additionally, the 2016 results include at least 17\% mining software repository (MSR) papers, a category of papers not seen in 2002. The advent of MSR papers has increased the use of generalization/characterization research questions, the production of empirical report contribution, and validation by evaluation.},
	journal = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
	author = {Theisen, Christopher and Dunaiski, Marcel and Williams, Laurie and Visser, Willem},
	year = {2017},
	note = {ISBN: 9781538615898
Publisher: IEEE},
	keywords = {Abstracts, Guidelines, Research, Writing},
	pages = {402},
}

@article{barba_reproducible_2017,
	title = {Reproducible {Research} for {Computing} in {Science} \& {Engineering}},
	volume = {19},
	issn = {15219615},
	doi = {10.1109/MCSE.2017.3971172},
	abstract = {The editors of the new track for reproducible research outline the parameters for future peer review, submission, and access, highlighting the magazine's previous work in this field and some of the challenges still to come.},
	number = {6},
	journal = {Computing in Science and Engineering},
	author = {Barba, Lorena A. and Thiruvathukal, George K.},
	year = {2017},
	keywords = {open access, peer review, reproducible research, scientific computing},
	pages = {85--87},
}

@article{thiruvathukal_how_nodate,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	number = {2},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{thiruvathukal_how_nodate-1,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	number = {1},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
	pages = {24--26},
}

@article{theisen_writing_2017-1,
	title = {Writing good software engineering research papers: {Revisited}},
	doi = {10.1109/ICSE-C.2017.51},
	abstract = {With the goal of helping software engineering researchers understand how to improve their papers, Mary Shaw presented 'Writing Good Software Engineering Research Papers' in 2003. Shaw analyzed the abstracts of the papers submitted to the 2002 International Conference of Software Engineering (ICSE) to determine trends in research question type, contribution type, and validation approach. We revisit Shaw's work to see how the software engineering research community has evolved since 2002. The goal of this paper is to aid software engineering researchers in understanding trends in research question design, research question type, and validation approach by analyzing the abstracts of the papers submitted to ICSE 2016. We implemented Shaw's recommendation for replicating her study through the use of multiple coders and the calculation of inter-rater reliability and demonstrate that her approach can be repeated. Our results indicate that reviewers have increased expectations that papers have solid evaluations of the research contribution. Additionally, the 2016 results include at least 17\% mining software repository (MSR) papers, a category of papers not seen in 2002. The advent of MSR papers has increased the use of generalization/characterization research questions, the production of empirical report contribution, and validation by evaluation.},
	journal = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
	author = {Theisen, Christopher and Dunaiski, Marcel and Williams, Laurie and Visser, Willem},
	year = {2017},
	note = {ISBN: 9781538615898},
	keywords = {Abstracts, Guidelines, Research, Writing},
	pages = {402},
}

@article{noauthor_orthogonal_1992,
	title = {Orthogonal defect classification-a concept for in-process measurements},
	abstract = {By the early 1990s the need for reengineering legacy systems was already acute, but recently the demand has increased significantly with the shift toward web-based user interfaces. The demand by all business sectors to adapt their information systems to the Web has created a tremendous need for methods, tools, and infrastructures to evolve and exploit existing applications efficiently and cost-effectively. Reverse engineering has been heralded as one of the most promising technologies to combat this.legacy systems problem. This paper presents a roadmap for reverse engineering research for the first decade of the new millennium, building on the program comprehension theories of the 1980s and the reverse engineering technology of the 1990s.},
	year = {1992},
}

@article{thiruvathukal_how_nodate-2,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	number = {2},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{noauthor_understanding_nodate,
	title = {Understanding and {Detecting} {Performance} {Bugs} in {Markdown} {Compilers}},
}

@article{noauthor_empirical_nodate,
	title = {An {Empirical} {Study} on {Information} {Discrepancies} across {Heterogeneous} {Vulnerability} {Databases}},
}

@article{lagouvardos_static_2020,
	title = {Static {Analysis} of {Shape} in {TensorFlow} {Programs}},
	volume = {6},
	abstract = {Machine learning has been widely adopted in diverse science and engineering domains, aided by reusable libraries and quick development patterns. The TensorFlow library is probably the best- known representative of this trend and most users employ the Python API to its powerful back-end. TensorFlow programs are susceptible to several systematic errors, especially in the dynamic typing setting of Python. We present Pythia, a static analysis that tracks the shapes of tensors across Python library calls and warns of several possible mismatches. The key technical aspects are a close modeling of library semantics with respect to tensor shape, and an identification of violations and error-prone patterns. Pythia is powerful enough to statically detect (with 84.62\% precision) 11 of the 14 shape-related TensorFlow bugs in the recent Zhang et al. empirical study—an independent slice of real-world bugs.},
	number = {15},
	journal = {34th European Conference on Object-Oriented Programming (ECOOP 2020).},
	author = {Lagouvardos, Sifis and Dolby, Julian and Grech, Neville},
	year = {2020},
	keywords = {Doop, Python, TensorFlow, Wala, static analysis},
	pages = {1--15},
}

@article{verma_shapeflow_2020,
	title = {{ShapeFlow}: {Dynamic} shape interpreter for {TensorFlow}},
	issn = {23318422},
	abstract = {We present ShapeFlow, a dynamic abstract interpreter for TensorFlow which quickly catches tensor shape incompatibility errors, one of the most common bugs in deep learning code. ShapeFlow shares the same APIs as TensorFlow but only captures and emits tensor shapes, its abstract domain. ShapeFlow constructs a custom shape computational graph, similar to the computational graph used by TensorFlow. ShapeFlow requires no code annotation or code modification by the programmer, and therefore is convenient to use. We evaluate ShapeFlow on 52 programs collected by prior empirical studies to show how fast and accurately it can catch shape incompatibility errors compared to TensorFlow. We use two baselines: a worst-case training dataset size and a more realistic dataset size. ShapeFlow detects shape incompatibility errors highly accurately — with no false positives and a single false negative — and highly efficiently —with an average speed-up of 499X and 24X for the first and second baseline, respectively. We believe ShapeFlow is a practical tool that benefits machine learning developers. We will open-source ShapeFlow on GitHub to make it publicly available to both the developer and research communities.},
	journal = {arXiv},
	author = {Verma, Sahil and Su, Zhendong},
	year = {2020},
}

@article{monat_static_2020,
	title = {Static {Type} {Analysis} by {Abstract} {Interpretation} of {Python} {Programs}},
	number = {17},
	author = {Monat, Raphaël and Miné, Antoine},
	year = {2020},
	keywords = {17, 2020, 4230, abstract interpretation, acknowledgements we thank the, and phrases formal methods, anonymous reviewers for their, digital object identifier 10, dynamic programming language, ecoop, lipics, python semantics, static analysis, type analysis, valuable comments and feedback},
	pages = {1--17},
}

@article{miao_towards_2017,
	title = {Towards unified data and lifecycle management for deep learning},
	issn = {10844627},
	doi = {10.1109/ICDE.2017.112},
	abstract = {Deep learning has improved state-of-The-Art results in many important fields, and has been the subject of much research in recent years, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of data management, model sharing, and lifecycle management are largely ignored. Deep learning modeling lifecycle generates a rich set of data artifacts, e.g., learned parameters and training logs, and it comprises of several frequently conducted tasks, e.g., to understand the model behaviors and to try out new models. Dealing with such artifacts and tasks is cumbersome and largely left to the users. This paper describes our vision and implementation of a data and lifecycle management system for deep learning. First, we generalize model exploration and model enumeration queries from commonly conducted tasks by deep learning modelers, and propose a high-level domain specific language (DSL), inspired by SQL, to raise the abstraction level and thereby accelerate the modeling process. To manage the variety of data artifacts, especially the large amount of checkpointed float parameters, we design a novel model versioning system (dlv), and a read-optimized parameter archival storage system (PAS) that minimizes storage footprint and accelerates query workloads with minimal loss of accuracy. PAS archives versioned models using deltas in a multi-resolution fashion by separately storing the less significant bits, and features a novel progressive query (inference) evaluation algorithm. Third, we develop efficient algorithms for archiving versioned models using deltas under co-retrieval constraints. We conduct extensive experiments over several real datasets from computer vision domain to show the efficiency of the proposed techniques.},
	journal = {Proceedings - International Conference on Data Engineering},
	author = {Miao, Hui and Li, Ang and Davis, Larry S. and Deshpande, Amol},
	year = {2017},
	note = {ISBN: 9781509065431
Publisher: IEEE},
	pages = {571--582},
}

@article{yang_deep_2017,
	title = {Deep learning for fixed model reuse},
	abstract = {Model reuse attempts to construct a model by utilizing existing available models, mostly trained for other tasks, rather than building a model from scratch. It is helpful to reduce the time cost, data amount, and expertise required. Deep learning has achieved great success in various tasks involving images, voices and videos. There are several studies have the sense of model reuse, by trying to reuse pre-trained deep networks architectures or deep model features to train a new deep model. They, however, neglect the fact that there are many other fixed models or features available. In this paper, we propose a more thorough model reuse scheme, FMR (Fixed Model Reuse). FMR utilizes the learning power of deep models to implicitly grab the useful discriminative information from fixed model/features that have been widely used in general tasks. We firstly arrange the convolution layers of a deep network and the provided fixed model/features in parallel, fully connecting to the output layer nodes. Then, the dependencies between the output layer nodes and the fixed model/features are knockdown such that only the raw feature inputs are needed when the model is being used for testing, though the helpful information in the fixed model/features have already been incorporated into the model. On one hand, by the FMR scheme, the required amount of training data can be significantly reduced because of the reuse of fixed model/features. On the other hand, the fixed model/features are not explicitly used in testing, and thus, the scheme can be quite useful in applications where the fixed model/features are protected by patents or commercial secrets. Experiments on five real-world datasets validate the effectiveness of FMR compared with state-of-the-art deep methods.},
	journal = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
	author = {Yang, Yang and Zhan, De Chuan and Fan, Ying and Jiang, Yuan and Zhou, Zhi Hua},
	year = {2017},
	keywords = {Machine Learning Methods},
	pages = {2831--2837},
}

@article{zhou_learnware_2016,
	title = {Learnware: on the future of machine learning},
	volume = {10},
	issn = {20952236},
	doi = {10.1007/s11704-016-6906-3},
	number = {4},
	journal = {Frontiers of Computer Science},
	author = {Zhou, Zhi Hua},
	year = {2016},
	pages = {589--590},
}

@article{monat_static_2020-1,
	title = {Static {Type} {Analysis} by {Abstract} {Interpretation} of {Python} {Programs}},
	volume = {6},
	number = {17},
	author = {Monat, Raphaël and Miné, Antoine},
	year = {2020},
	keywords = {17, 2020, 4230, abstract interpretation, acknowledgements we thank the, and phrases formal methods, anonymous reviewers for their, digital object identifier 10, dynamic programming language, ecoop, lipics, python semantics, static analysis, type analysis, valuable comments and feedback},
	pages = {1--17},
}

@article{wu_model_2017,
	title = {Model reuse with domain knowledge},
	volume = {47},
	issn = {1674-7267},
	doi = {10.1360/n112017-00106},
	number = {11},
	journal = {SCIENTIA SINICA Informationis},
	author = {WU, Xizhu and ZHOU, Zhihua},
	year = {2017},
	pages = {1483--1492},
}

@article{allamanis_mining_2014,
	title = {Mining idioms from source code},
	volume = {16-21-Nove},
	doi = {10.1145/2635868.2635901},
	abstract = {We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic purpose. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present Haggis, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply Haggis to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q\&A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management.},
	journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
	author = {Allamanis, Miltiadis and Sutton, Charles},
	year = {2014},
	note = {ISBN: 9781450330565},
	keywords = {Code idioms, Naturalness of source code, Syntactic code patterns},
	pages = {472--483},
}

@article{zhang_end--end_2019,
	title = {An end-to-end automatic cloud database tuning system using deep reinforcement learning},
	issn = {07308078},
	doi = {10.1145/3299869.3300085},
	abstract = {Configuration tuning is vital to optimize the performance of database management system (DBMS). It becomes more tedious and urgent for cloud databases (CDB) due to the diverse database instances and query workloads, which make the database administrator (DBA) incompetent. Although there are some studies on automatic DBMS configuration tuning, they have several limitations. Firstly, they adopt a pipelined learning model but cannot optimize the overall performance in an end-to-end manner. Secondly, they rely on large-scale high-quality training samples which are hard to obtain. Thirdly, there are a large number of knobs that are in continuous space and have unseen dependencies, and they cannot recommend reasonable configurations in such high-dimensional continuous space. Lastly, in cloud environment, they can hardly cope with the changes of hardware configurations and workloads, and have poor adaptability. To address these challenges, we design an end-to-end automatic CDB tuning system, CDBTune, using deep reinforcement learning (RL). CDBTune utilizes the deep deterministic policy gradient method to find the optimal configurations in high-dimensional continuous space. CDBTune adopts a try-and-error strategy to learn knob settings with a limited number of samples to accomplish the initial training, which alleviates the difficulty of collecting massive high-quality samples. CDBTune adopts the reward-feedback mechanism in RL instead of traditional regression, which enables end-to-end learning and accelerates the convergence speed of our model and improves efficiency of online tuning. We conducted extensive experiments under 6 different workloads on real cloud databases to demonstrate the superiority of CDBTune. Experimental results showed that CDBTune had a good adaptability and significantly outperformed the state-of-the-art tuning tools and DBA experts.},
	journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
	author = {Zhang, Ji and Liu, Yu and Zhou, Ke and Li, Guoliang and Xiao, Zhili and Cheng, Bin and Xing, Jiashu and Wang, Yangtao and Cheng, Tianheng and Liu, Li and Ran, Minwei and Li, Zekang},
	year = {2019},
	note = {ISBN: 9781450356435},
	pages = {415--432},
}

@article{grossner_new_2021,
	title = {New {Compute} {Ecosystem} from {Cloud} to {Edge} {Report} - 2021},
	author = {Grossner, Cliff},
	year = {2021},
}

@article{granger_python_2011,
	title = {Python : {An} {Ecosystem}},
	author = {Granger, Brian E. and Hunter, John D.},
	year = {2011},
	pages = {13--21},
}

@article{margolis_institutionalized_2001,
	title = {Institutionalized older adults in a health district in the {United} {Arab} {Emirates}: {Health} status and utilization rate},
	volume = {47},
	issn = {0304324X},
	doi = {10.1159/000052791},
	abstract = {Background: Little is known about the rate of institutionalization and health status of nursing home (NH) type patients living in the Middle East. This study was set in the Al-Ain Medical District, a geographically discrete region of the United Arab Emirates, a country with a developing economy located on the shores of the Arabian Gulf. NH-type patients were defined as people aged 60 years and older who were admitted to a hospital or a long-term institutionalized setting for at least 6 weeks and with no evidence of an expectation of discharge at the time of the evaluation. Objective: To determine the clinical, functional, cognitive, and nutritional status of NH-type patients living in a defined community within a developing country. Method: Cross-sectional survey. Results: All NH-type patients were identified, and all were included in this study (n = 47, 100\% participation rate). All were located within three public institutions, none of which was a dedicated NH facility. The rate of institutionalization was 7.0-14.0 per 1,000 people aged 65 or older. The age distribution was 30\% (60-74 years), 49\% (75-84 years), and 21\% (85+ years). The length of stay was 3.8 years. The female:male ratio was 1.6. All except 1 had a neurological disorder, and 89\% had dementia. The cognitive deficits were severe with only 61\% alert, 41\% able to speak, 17\% orientated in place, and 15\% orientated in time. The functional status was also poor: 98\% received assistance with all instrumental activities of daily living, 85\% received assistance with five activities of daily living, and 94\% were bed bound. The nutritional status was also impaired with a mean body weight of 45 ± 14 kg and a mean albumin level of 3.1 ± 0.6 g/dl. When compared with the USA data from the National Center for Health Statistics, the study population was younger, had a longer length of stay, a lower female:male ratio, a higher rate of neurological diseases and dementia, and were far more dependent and disorientated. The rate of institutionalization was one sixth to one third of that in the USA. Conclusion: From these data we concluded that this region has a distinctly different population of institutionalized older people who demonstrate greater impairments in all domains of health status. Copyright © 2001 S. Karger AG, Basel.},
	number = {3},
	journal = {Gerontology},
	author = {Margolis, Stephen A. and Reed, Richard L.},
	year = {2001},
	pmid = {11340323},
	keywords = {Developing countries, Nursing homes, Older people, institutionalization},
	pages = {161--167},
}

@article{this_rules_2021,
	title = {Rules of {Machine} {Learning} : {Best} {Practices} for {ML} {Engineering}},
	abstract = {This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine?learned model, then you have the necessary background to read this document. Terminology},
	author = {This, Martin Zinkevich and Guide, Style},
	year = {2021},
	pages = {1--28},
}

@article{molina_training_2019,
	title = {Training {Binary} {Classifiers} as {Data} {Structure} {Invariants}},
	volume = {2019-May},
	issn = {02705257},
	doi = {10.1109/ICSE.2019.00084},
	abstract = {We present a technique to distinguish valid from invalid data structure objects. The technique is based on building an artificial neural network, more precisely a binary classifier, and training it to identify valid and invalid instances of a data structure. The obtained classifier can then be used in place of the data structure's invariant, in order to attempt to identify (in)correct behaviors in programs manipulating the structure. In order to produce the valid objects to train the network, an assumed-correct set of object building routines is randomly executed. Invalid instances are produced by generating values for object fields that 'break' the collected valid values, i.e., that assign values to object fields that have not been observed as feasible in the assumed-correct executions that led to the collected valid instances. We experimentally assess this approach, over a benchmark of data structures. We show that this learning technique produces classifiers that achieve significantly better accuracy in classifying valid/invalid objects compared to a technique for dynamic invariant detection, and leads to improved bug finding.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Molina, Facundo and Degiovanni, Renzo and Ponzio, Pablo and Regis, German and Aguirre, Nazareno and Frias, Marcelo},
	year = {2019},
	note = {ISBN: 9781728108698},
	keywords = {Bug finding, Machine learning, Specification inference},
	pages = {759--770},
}

@article{freese_defining_2014,
	title = {Defining a cloud ecosystem},
	url = {https://www.ibm.com/downloads/cas/AP9YY90D},
	journal = {Ibm},
	author = {Freese, Bob},
	year = {2014},
}

@article{systems_cloud_nodate,
	title = {{THE} {CLOUD} {COMPUTING} {ECOSYSTEM} {Evolu0on} ( of ( {IT} (},
	author = {Systems, Backend},
}

@article{michael_regexes_2019-1,
	title = {Regexes are hard: {Decision}-making, difficulties, and risks in programming regular expressions},
	doi = {10.1109/ASE.2019.00047},
	abstract = {Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.},
	journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
	author = {Michael, Louis G. and Donohue, James and Davis, James C. and Lee, Dongyoon and Servant, Francisco},
	year = {2019},
	note = {ISBN: 9781728125084},
	keywords = {Developer process, Qualitative research, Regular expressions},
	pages = {415--426},
}

@article{park_systematic_2015,
	title = {Systematic {Testing} of {Reactive} {Software} with {Non}-{Deterministic} {Events}: {A} {Case} {Study} on {LG} {Electric} {Oven}},
	volume = {2},
	issn = {02705257},
	doi = {10.1109/ICSE.2015.132},
	abstract = {Most home appliance devices such as electric ovens are reactive systems which repeat receiving a user input/event through an event handler, updating their internal state based on the input, and generating outputs. A challenge to test a reactive program is to check if the program correctly reacts to various non-deterministic sequence of events because an unexpected sequence of events may make the system fail due to the race conditions between the main loop and asynchronous event handlers. Thus, it is important to systematically generate/test various sequences of events by controlling the order of events and relative timing of event occurrences with respect to the main loop execution. In this paper, we report our industrial experience to solve the aforementioned problem by developing a systematic event generation framework based on concolic testing technique. We have applied the framework to a LG electric oven and detected several critical bugs including one that makes the oven ignore user inputs due to the illegal state transition.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Park, Yongbae and Hong, Shin and Kim, Moonzoo and Lee, Dongju and Cho, Junhee},
	year = {2015},
	note = {ISBN: 9781479919345
Publisher: IEEE},
	pages = {29--38},
}

@article{koziolek_openpnp_2019,
	title = {{OpenPnP}: {A} {Plug}-{And}-{Produce} {Architecture} for the {Industrial} {Internet} of {Things}},
	doi = {10.1109/ICSE-SEIP.2019.00022},
	abstract = {Industrial control systems are complex, software-intensive systems that manage mission-critical production processes.commissioning such systems requires installing, configuring, and integrating thousands of sensors, actuators, and controllers and is still a largely manual and costly process. Therefore, practitioners and researchers have been working on 'plug and produce' approaches that automate commissioning for more than 15 years, but have often focused on network discovery and proprietary technologies. We introduce the vendor-neutral OpenPnP reference architecture, which can largely automate the configuration and integration tasks for commissioning. Using an example implementation, we demonstrate that OpenPnP can reduce the configuration and integration effort up to 90 percent and scales up to tens of thousands of communicated signals per second for large Industrial Internet-of-Things (IIoT) systems. OpenPnP can serve as a template for practitioners implementing IIoT applications throughout the automation industry and streamline commissioning processes in many thousands of control system installations.},
	journal = {Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice, ICSE-SEIP 2019},
	author = {Koziolek, Heiko and Burger, Andreas and Platenius-Mohr, Marie and Ruckert, Julius and Stomberg, Gosta},
	year = {2019},
	note = {ISBN: 9781728117607},
	keywords = {Client-server systems, Control engineering, Internet of Things, Real-Time systems, Software architecture},
	pages = {131--140},
}

@article{davis_impact_2020,
	title = {On the {Impact} and {Defeat} of {Regular} {Expression} {Denial} of {Service} {On} the {Impact} and {Defeat} of {Regular} {Expression} {Denial} of {Service}},
	author = {Davis, James C. and Yao, Danfeng Daphne and Butt, Ali R. and Davis, James C.},
	year = {2020},
	keywords = {denial of service, empirical software engineering, redos, regular expressions},
}

@article{botchkarev_evaluating_2018,
	title = {Evaluating {Performance} of {Regression} {Machine} {Learning} {Models} {Using} {Multiple} {Error} {Metrics} in {Azure} {Machine} {Learning} {Studio}},
	issn = {1556-5068},
	doi = {10.2139/ssrn.3177507},
	abstract = {Data driven companies effectively use regression machine learning methods for making predictions in many sectors. Cloud-based Azure Machine Learning Studio (MLS) has a potential of expediting machine learning experiments by offering a convenient and powerful integrated development environment. The process of evaluating machine learning models in Azure MLS has certain limitations, e.g. small number of performance metrics and lack of functionality to evaluate custom built regression models with R language. This paper reports the results of an effort to build an Enhanced Evaluate Model (EEM) module which facilitates and accelerates Azure experiments development and evaluation. The EEM combines multiple performance metrics allowing for multisided evaluation of the regression models. EEM offers 4 times more metrics than built-in Azure Evaluate Model module. EEM metrics include: CoD, GMRAE, MAE, MAPE, MASE, MdAE, MdAPE, MdRAE, ME, MPE, MRAE, MSE, NRMSE\_mm, NRMSE\_sd, RAE, RMdSPE, RMSE, RMSPE, RSE, sMAPE, SMdAPE, SSE. Also, EEM supports evaluation of the R language based regression models. The operational Enhanced Evaluate Model module has been published to the web and openly available for experiments and extensions.},
	journal = {SSRN Electronic Journal},
	author = {Botchkarev, Alexei},
	year = {2018},
	keywords = {accuracy, azure machine learning studio, error, error measure, evaluation, forecast, machine learning, models, multiple types, performance metrics, prediction, r, regression},
	pages = {1--16},
}

@inproceedings{egziabher_performance_2013,
	title = {Performance {Metrics} ({Error} {Measures}) in {Machine} {Learning} {Regression}, {Forecasting} and {Prognostics}: {Properties} and {Typology}},
	volume = {53},
	isbn = {978-85-7811-079-6},
	abstract = {Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. The intention of this study was to overview of a variety of performance metrics and approaches to their classification. The main goal of the study was to develop a typology that will help to improve our knowledge and understanding of metrics and facilitate their selection in machine learning regression, forecasting and prognostics. Based on the analysis of the structure of numerous performance metrics, we propose a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set. The paper proposed a new primary metrics typology designed around the key metrics components. The suggested typology has been shown to cover most of the commonly used primary metrics – total of over 40. The main contribution of this paper is in ordering knowledge of performance metrics and enhancing understanding of their structure and properties by proposing a new typology, generic primary metrics mathematic formula and a visualization chart. Keywords:},
	booktitle = {Africa’s potential for the ecological intensification of agriculture},
	author = {Egziabher, Tewolde Berhan Gebre and Edwards, Sue},
	year = {2013},
	pmid = {25246403},
	note = {ISSN: 1098-6596
Issue: 9},
	keywords = {icle},
	pages = {1689--1699},
}

@article{rauber_foolbox_2017,
	title = {Foolbox: {A} {Python} toolbox to benchmark the robustness of machine learning models},
	issn = {23318422},
	abstract = {Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox. The most up-to-date documentation can be found at http://foolbox.readthedocs.io.},
	journal = {arXiv},
	author = {Rauber, Jonas and Brendel, Wieland and Bethge, Matthias},
	year = {2017},
}

@article{shami_evaluation_2007,
	title = {An evaluation of the robustness of existing supervised machine learning approaches to the classification of emotions in speech},
	volume = {49},
	issn = {01676393},
	doi = {10.1016/j.specom.2007.01.006},
	abstract = {In this study, the robustness of approaches to the automatic classification of emotions in speech is addressed. Among the many types of emotions that exist, two groups of emotions are considered, adult-to-adult acted vocal expressions of common types of emotions like happiness, sadness, and anger and adult-to-infant vocal expressions of affective intents also known as "motherese". Specifically, we estimate the generalization capability of two feature extraction approaches, the approach developed for Sony's robotic dog AIBO (AIBO) and the segment-based approach (SBA) of [Shami, M., Kamel, M., 2005. Segment-based approach to the recognition of emotions in speech. In: IEEE Conf. on Multimedia and Expo (ICME05), Amsterdam, The Netherlands]. Three machine learning approaches are considered, K-nearest neighbors (KNN), Support vector machines (SVM) and Ada-boosted decision trees and four emotional speech databases are employed, Kismet, BabyEars, Danish, and Berlin databases. Single corpus experiments show that the considered feature extraction approaches AIBO and SBA are competitive on the four databases considered and that their performance is comparable with previously published results on the same databases. The best choice of machine learning algorithm seems to depend on the feature extraction approach considered. Multi-corpus experiments are performed with the Kismet-BabyEars and the Danish-Berlin database pairs that contain parallel emotional classes. Automatic clustering of the emotional classes in the database pairs shows that the patterns behind the emotions in the Kismet-BabyEars pair are less database dependent than the patterns in the Danish-Berlin pair. In off-corpus testing the classifier is trained on one database of a pair and tested on the other. This provides little improvement over baseline classification. In integrated corpus testing, however, the classifier is machine learned on the merged databases and this gives promisingly robust classification results, which suggest that emotional corpora with parallel emotion classes recorded under different conditions can be used to construct a single classifier capable of distinguishing the emotions in the merged corpora. Such a classifier is more robust than a classifier learned on a single corpus as it can recognize more varied expressions of the same emotional classes. These findings suggest that the existing approaches for the classification of emotions in speech are efficient enough to handle larger amounts of training data without any reduction in classification accuracy. © 2007 Elsevier B.V. All rights reserved.},
	number = {3},
	journal = {Speech Communication},
	author = {Shami, Mohammad and Verhelst, Werner},
	year = {2007},
	keywords = {Analysis of intent, Emotion recognition, Speech processing, Vocal expressiveness},
	pages = {201--212},
}

@article{bhagoji_enhancing_2017,
	title = {Enhancing robustness of machine learning systems via data transformations},
	issn = {23318422},
	abstract = {We propose the use of data transformations as a defense against evasion attacks on ML classifiers. We present and investigate strategies for incorporating a variety of data transformations including dimensionality reduction via Principal Component Analysis and data 'anti-whitening' to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of linear transformations of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defense is (i) effective against the best known evasion attacks from the literature, resulting in a two-fold increase in the resources required by a white-box adversary with knowledge of the defense for a successful attack, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification and human activity classification.},
	journal = {arXiv},
	author = {Bhagoji, Arjun Nitin and Cullina, Daniel and Sitawarin, Chawin and Mittal, Prateek},
	year = {2017},
	note = {ISBN: 9781538605790
Publisher: IEEE},
	pages = {1--5},
}

@article{botchkarev_evaluating_2018-1,
	title = {Evaluating {Performance} of {Regression} {Machine} {Learning} {Models} {Using} {Multiple} {Error} {Metrics} in {Azure} {Machine} {Learning} {Studio}},
	issn = {1556-5068},
	doi = {10.2139/ssrn.3177507},
	abstract = {Data driven companies effectively use regression machine learning methods for making predictions in many sectors. Cloud-based Azure Machine Learning Studio (MLS) has a potential of expediting machine learning experiments by offering a convenient and powerful integrated development environment. The process of evaluating machine learning models in Azure MLS has certain limitations, e.g. small number of performance metrics and lack of functionality to evaluate custom built regression models with R language. This paper reports the results of an effort to build an Enhanced Evaluate Model (EEM) module which facilitates and accelerates Azure experiments development and evaluation. The EEM combines multiple performance metrics allowing for multisided evaluation of the regression models. EEM offers 4 times more metrics than built-in Azure Evaluate Model module. EEM metrics include: CoD, GMRAE, MAE, MAPE, MASE, MdAE, MdAPE, MdRAE, ME, MPE, MRAE, MSE, NRMSE\_mm, NRMSE\_sd, RAE, RMdSPE, RMSE, RMSPE, RSE, sMAPE, SMdAPE, SSE. Also, EEM supports evaluation of the R language based regression models. The operational Enhanced Evaluate Model module has been published to the web and openly available for experiments and extensions.},
	journal = {SSRN Electronic Journal},
	author = {Botchkarev, Alexei},
	year = {2018},
	keywords = {accuracy, azure machine learning studio, error, error measure, evaluation, forecast, machine learning, models, multiple types, performance metrics, prediction, r, regression},
	pages = {1--16},
}

@article{schelter_jenga-framework_2021,
	title = {{JENGA}-{A} {Framework} to {Study} the {Impact} of {Data} {Errors} on the {Predictions} of {Machine} {Learning} {Models}},
	url = {https://github.com/awslabs/deequ},
	abstract = {Machine learning (ML) is increasingly used to automate decision making in various domains. Almost all common ML models are susceptible to data errors in the serving data (for which the model makes predictions). Such errors frequently occur in practice , caused for example by program bugs in data preprocessing code or non-anticipated schema changes in external data sources. These errors can have devastating effects on the prediction quality of ML models and are, at the same time, hard to anticipate and capture. In order to empower data scientists to study the impact as well as mitigation techniques for data errors in ML models, we propose Jenga, a lightweight , open source experimentation library. Jenga allows its users to test their models for robustness against common data errors. Jenga contains an abstraction for prediction tasks based on a dataset and a model, an easily extendable set of synthethic data corruptions (e.g., for missing values, outliers, ty-pos and noisy measurements) as well as evaluation functionality to experiment with different data corruptions. Jenga supports researchers and practitioners in the difficult task of data validation for ML applications. As a showcase for this, we discuss two use cases of Jenga: studying the robustness of a model against incomplete data, as well as automatically stress testing integrity constraints for ML data expressed with tensorflow data validation.},
	author = {Schelter, Sebastian and Rukat, Tammo and Biessmann, Felix},
	year = {2021},
	note = {ISBN: 9783893180844},
}

@article{sehwag_analyzing_2019,
	title = {Analyzing the robustness of open-world machine learning},
	issn = {15437221},
	doi = {10.1145/3338501.335737},
	abstract = {When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing OOD adversarial examples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that OOD adversarial examples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.},
	journal = {Proceedings of the ACM Conference on Computer and Communications Security},
	author = {Sehwag, Vikash and Sitawarin, Chawin and Bhagoji, Arjun Nitin and Cullina, Daniel and Mittal, Prateek and Song, Liwei and Chiang, Mung},
	year = {2019},
	note = {ISBN: 9781450368339},
	keywords = {Adversarial example, Deep learning, Open world recognition},
	pages = {105--116},
}

@article{elmes_accounting_2020,
	title = {Accounting for training data error in machine learning applied to earth observations},
	volume = {12},
	issn = {20724292},
	doi = {10.3390/rs12061034},
	abstract = {Remote sensing, or Earth Observation (EO), is increasingly used to understand Earth system dynamics and create continuous and categorical maps of biophysical properties and land cover, especially based on recent advances in machine learning (ML). ML models typically require large, spatially explicit training datasets to make accurate predictions. Training data (TD) are typically generated by digitizing polygons on high spatial-resolution imagery, by collecting in situ data, or by using pre-existing datasets. TD are often assumed to accurately represent the truth, but in practice almost always have error, stemming from (1) sample design, and (2) sample collection errors. The latter is particularly relevant for image-interpreted TD, an increasingly commonly used method due to its practicality and the increasing training sample size requirements of modern ML algorithms. TD errors can cause substantial errors in the maps created using ML algorithms, which may impact map use and interpretation. Despite these potential errors and their real-world consequences for map-based decisions, TD error is often not accounted for or reported in EO research. Here we review the current practices for collecting and handling TD. We identify the sources of TD error, and illustrate their impacts using several case studies representing different EO applications (infrastructure mapping, global surface flux estimates, and agricultural monitoring), and provide guidelines for minimizing and accounting for TD errors. To harmonize terminology, we distinguish TD from three other classes of data that should be used to create and assess ML models: training reference data, used to assess the quality of TD during data generation; validation data, used to iteratively improve models; and map reference data, used only for final accuracy assessment. We focus primarily on TD, but our advice is generally applicable to all four classes, and we ground our review in established best practices for map accuracy assessment literature. EO researchers should start by determining the tolerable levels of map error and appropriate error metrics. Next, TD error should be minimized during sample design by choosing a representative spatio-temporal collection strategy, by using spatially and temporally relevant imagery and ancillary data sources during TD creation, and by selecting a set of legend definitions supported by the data. Furthermore, TD error can be minimized during the collection of individual samples by using consensus-based collection strategies, by directly comparing interpreted training observations against expert-generated training reference data to derive TD error metrics, and by providing image interpreters with thorough application-specific training. We strongly advise that TD error is incorporated in model outputs, either directly in bias and variance estimates or, at a minimum, by documenting the sources and implications of error. TD should be fully documented and made available via an open TD repository, allowing others to replicate and assess its use. To guide researchers in this process, we propose three tiers of TD error accounting standards. Finally, we advise researchers to clearly communicate the magnitude and impacts of TD error on map outputs, with specific consideration given to the likely map audience.},
	number = {6},
	journal = {Remote Sensing},
	author = {Elmes, Arthur and Alemohammad, Hamed and Avery, Ryan and Caylor, Kelly and Eastman, J. Ronald and Fishgold, Lewis and Friedl, Mark A. and Jain, Meha and Kohli, Divyani and Bayas, Juan Carlos Laso and Lunga, Dalton and McCarty, Jessica L. and Pontius, Robert Gilmore and Reinmann, Andrew B. and Rogan, John and Song, Lei and Stoynova, Hristiana and Ye, Su and Yi, Zhuang Fang and Estes, Lyndon},
	year = {2020},
	keywords = {Error propagation, Machine learning, Map accuracy, Training data},
	pages = {1--39},
}

@article{didona_enhancing_2015,
	title = {Enhancing performance prediction robustness by combining analytical modeling and machine learning},
	doi = {10.1145/2668930.2688047},
	abstract = {Classical approaches to performance prediction rely on two, typically antithetic, techniques: Machine Learning (ML) and Analytical Modeling (AM). ML takes a black box ap- proach, whose accuracy strongly depends on the represen- tativeness of the dataset used during the initial training phase. Specifically, it can achieve very good accuracy in areas of the features' space that have been sufficiently ex- plored during the training process. Conversely, AM tech- niques require no or minimal training, hence exhibiting the potential for supporting prompt instantiation of the perfor- mance model of the target system. However, in order to ensure their tractability, they typically rely on a set of sim- plifying assumptions. Consequently, AM's accuracy can be seriously challenged in scenarios (e.g., workload conditions) in which such assumptions are not matched. In this paper we explore several hybrid/gray box techniques that exploit AM and ML in synergy in order to get the best of the two worlds. We evaluate the proposed techniques in case stud- ies targeting two complex and widely adopted middleware systems: a NoSQL distributed key-value store and a Total Order Broadcast (TOB) service.},
	journal = {ICPE 2015 - Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
	author = {Didona, Diego and Quaglia, Francesco and Romano, Paolo and Torre, Ennio},
	year = {2015},
	note = {ISBN: 9781450332484},
	pages = {145--156},
}

@article{munaiah_curating_2017,
	title = {Curating {GitHub} for engineered software projects},
	volume = {22},
	issn = {15737616},
	doi = {10.1007/s10664-017-9512-6},
	abstract = {Software forges like GitHub host millions of repositories. Software engineering researchers have been able to take advantage of such a large corpora of potential study subjects with the help of tools like GHTorrent and Boa. However, the simplicity in querying comes with a caveat: there are limited means of separating the signal (e.g. repositories containing engineered software projects) from the noise (e.g. repositories containing home work assignments). The proportion of noise in a random sample of repositories could skew the study and may lead to researchers reaching unrealistic, potentially inaccurate, conclusions. We argue that it is imperative to have the ability to sieve out the noise in such large repository forges. We propose a framework, and present a reference implementation of the framework as a tool called reaper, to enable researchers to select GitHub repositories that contain evidence of an engineered software project. We identify software engineering practices (called dimensions) and propose means for validating their existence in a GitHub repository. We used reaper to measure the dimensions of 1,857,423 GitHub repositories. We then used manually classified data sets of repositories to train classifiers capable of predicting if a given GitHub repository contains an engineered software project. The performance of the classifiers was evaluated using a set of 200 repositories with known ground truth classification. We also compared the performance of the classifiers to other approaches to classification (e.g. number of GitHub Stargazers) and found our classifiers to outperform existing approaches. We found stargazers-based classifier (with 10 as the threshold for number of stargazers) to exhibit high precision (97\%) but an inversely proportional recall (32\%). On the other hand, our best classifier exhibited a high precision (82\%) and a high recall (86\%). The stargazer-based criteria offers precision but fails to recall a significant portion of the population.},
	number = {6},
	journal = {Empirical Software Engineering},
	author = {Munaiah, Nuthan and Kroh, Steven and Cabrey, Craig and Nagappan, Meiyappan},
	year = {2017},
	keywords = {Curation tools, Data curation, GitHub, Mining software repositories},
	pages = {3219--3253},
}

@article{thiruvathukal_how_nodate,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden} : {An} {Experience} {Report} on {Machine} {Learning} {Reproducibility}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{rozsa_are_2017,
	title = {Are accuracy and robustness correlated?},
	doi = {10.1109/ICMLA.2016.42},
	abstract = {Machine learning models are vulnerable to adversarial examples formed by applying small carefully chosen perturbations to inputs that cause unexpected classification errors. In this paper, we perform experiments on various adversarial example generation approaches with multiple deep convolutional neural networks including Residual Networks, the best performing models on ImageNet Large-Scale Visual Recognition Challenge 2015. We compare the adversarial example generation techniques with respect to the quality of the produced images, and measure the robustness of the tested machine learning models to adversarial examples. Finally, we conduct large-scale experiments on cross-model adversarial portability. We find that adversarial examples are mostly transferable across similar network topologies, and we demonstrate that better machine learning models are less vulnerable to adversarial examples.},
	journal = {Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016},
	author = {Rozsa, Andras and Günther, Manuel and Boult, Terrance E.},
	year = {2017},
	note = {ISBN: 9781509061662},
	pages = {227--232},
}

@article{fagan_foundational_nodate,
	title = {Foundational {Cybersecurity} {Activities} for {IoT} {Device} {Manufacturers} {Foundational} {Cybersecurity} {Activities} for {IoT} {Device} {Manufacturers}},
	abstract = {Internet of Things (IoT) devices often lack device cybersecurity capabilities their customers—organizations and individuals—can use to help mitigate their cybersecurity risks. Manufacturers can help their customers by improving how securable the IoT devices they make are by providing necessary cybersecurity functionality and by providing customers with the cybersecurity-related information they need. This publication describes recommended activities related to cybersecurity that manufacturers should consider performing before their IoT devices are sold to customers. These foundational cybersecurity activities can help manufacturers lessen the cybersecurity-related efforts needed by customers, which in turn can reduce the prevalence and severity of IoT device compromises and the attacks performed using compromised devices.},
	author = {Fagan, Michael and Megas, Katerina N. and Fagan, Michael and Megas, Katerina N.},
	keywords = {Internet of Things (IoT), cybersecurity risk, manufacturing, risk management, risk mitigation, securable computing devices, software development},
}

@article{cha_principled_2020,
	title = {A principled approach to {GraphQL} {Query} {Cost} {Analysis}},
	issn = {23318422},
	abstract = {The landscape of web APIs is evolving to meet new client requirements and to facilitate how providers fulfill them. A recent web API model is GraphQL, which is both a query language and a runtime. Using GraphQL, client queries express the data they want to retrieve or mutate, and servers respond with exactly those data or changes. GraphQL’s expressiveness is risky for service providers because clients can succinctly request stupendous amounts of data, and responding to overly complex queries can be costly or disrupt service availability. Recent empirical work has shown that many service providers are at risk. Using traditional API management methods is not sufficient, and practitioners lack principled means of estimating and measuring the cost of the GraphQL queries they receive. In this work, we present a linear-time GraphQL query analysis that can measure the cost of a query without executing it. Our approach can be applied in a separate API management layer and used with arbitrary GraphQL backends. In contrast to existing static approaches, our analysis supports common GraphQL conventions that affect query cost, and our analysis is provably correct based on our formal specification of GraphQL semantics. We demonstrate the potential of our approach using a novel GraphQL query-response corpus for two commercial GraphQL APIs. Our query analysis consistently obtains upper cost bounds, tight enough relative to the true response sizes to be actionable for service providers. In contrast, existing static GraphQL query analyses exhibit over-estimates and under-estimates because they fail to support GraphQL conventions.},
	journal = {arXiv},
	author = {Cha, Alan and Wittern, Erik and Baudart, Guillaume and Davis, James C. and Mandel, Louis and Laredo, Jim A.},
	year = {2020},
	note = {ISBN: 9781450370431},
	keywords = {Algorithmic complexity attacks, GraphQL, Static analysis},
	pages = {257--268},
}

@article{pirelli_requirements_2019,
	title = {Requirements elicitation with a service canvas for packaged enterprise systems},
	volume = {2019-Septe},
	issn = {23326441},
	doi = {10.1109/RE.2019.00043},
	abstract = {We present a technique for eliciting requirements based on the use of a service canvas and the results of its application in the early phase of a customer relationship management integration project. The project was a collaboration between a research group and two industry partners. We describe (1) our service canvas, (2) how we designed a set of workshops to elicit the requirements, (3) the support tools used for running the workshops, and (4) the resulting canvas, listing the customer relationship management requirements, that was the basis for the project proposal. We explain how, as participant observers, we conducted the project and how we collected and analyzed the data. We describe what worked well and the lessons we learned. We outline some practical problems that remain unsolved.},
	journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
	author = {Pirelli, Blagovesta and Etzlinger, Lucien and Derrier, David and Regev, Gil and Wegmann, Alain},
	year = {2019},
	note = {ISBN: 9781728139128},
	keywords = {CRM, Canvas, Packaged enterprise system, Requirements elicitation, Service, Workshop},
	pages = {340--350},
}

@article{chitchyan_sustainability_2016,
	title = {Sustainability design in requirements engineering: {State} of practice},
	issn = {02705257},
	doi = {10.1145/2889160.2889217},
	abstract = {Sustainability is now a major concern in society, but there is little understanding of how it is perceived by software engineering professionals and how sustainability design can become an embedded part of software engineering process. This paper presents the results of a qualitative study exploring requirements engineering practitioners' perceptions and attitudes towards sustainability. It identifies obstacles and mitigation strategies regarding the application of sustainability design principles in daily work life. The results of this study reveal several factors that can prevent sustainability design from becoming a first class citizen in software engineering: software practitioners tend to have a narrow understanding of the concept of sustainability; organizations show limited awareness of its potential opportunities and benefits; and the norms in the discipline are not conducive to sustainable outcomes. These findings suggest the need for focused efforts in sustainability education, but also a need to rethink professional norms and practices.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Chitchyan, Ruzanna and Becker, Christoph and Betz, Stefanie and Duboc, Leticia and Penzenstadler, Birgit and Seyff, Norbert and Venters, Colin C.},
	year = {2016},
	note = {ISBN: 9781450341615},
	keywords = {Obstacles, Perceptions, Requirements engineering, Sustainability, Sustainability design},
	pages = {533--542},
}

@article{thiruvathukal_how_nodate-1,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{iv_exploring_2019,
	title = {Exploring the {Process} and {Challenges} of {Programming} with {Regular} {Expressions}},
	author = {IV, Louis G. Michael},
	year = {2019},
	keywords = {challenges of programming with, copyright 2019, developer process, exploring the process and, louis g, michael iv, qualitative research, redos, regex},
}

@article{garlan_update_2009,
	title = {update},
	author = {Garlan, David and Allen, Robert},
	year = {2009},
}

@article{lago_framing_2015,
	title = {Framing sustainability as a property of software quality},
	volume = {58},
	issn = {15577317},
	doi = {10.1145/2714560},
	number = {10},
	journal = {Communications of the ACM},
	author = {Lago, Patricia and Koçak, Sedef Akinli and Crnkovic, Ivica and Penzenstadler, Birgit},
	year = {2015},
	pages = {70--78},
}

@article{penzenstadler_safety_2014,
	title = {Safety, security, now sustainability: {The} nonfunctional requirement for the 21st century},
	volume = {31},
	issn = {07407459},
	doi = {10.1109/MS.2014.22},
	abstract = {Many software systems today control large-scale sociotechnical systems. These systems aren't just entangled with the environment but also with our dwindling resources and mostly unsustainable way of living, while the planet's population continues to grow. Dealing with sustainability requirements and systematically supporting their elicitation, analysis, and realization is a problem that has yet to be solved. Decades ago, the discipline of software engineering dealt with similar shortcomings in its processes by including safety and security as new system qualities. In light of the increasing consequences of inadequately addressing sustainability in developing software systems, software engineers must apply the lessons learned from these prior research efforts and identify the necessary research agenda. Considering sustainability in software engineering means more than energy efficiency and green IT, which are concerned with the first-order impacts of software systems. Software engineers must also take into account the second-and third-order impacts in the system context, even if they're hard to assess. By doing so, engineers have the potential to considerably improve civilization's sustainability. The Web extra at http://youtu.be/VC07j6a1XUw is a video in which author Birgit Penzenstadler talks about how software engineers can considerably improve civilization's sustainability by taking into account not just the first-order impacts of software systems but also their second-and third-order impacts. © 2014 IEEE.},
	number = {3},
	journal = {IEEE Software},
	author = {Penzenstadler, Birgit and Raturi, Ankita and Richardson, Debra and Tomlinson, Bill},
	year = {2014},
	note = {Publisher: IEEE},
	keywords = {green software, nonfunctional requirements, requirements engineering, security, software, sustainability},
	pages = {40--47},
}

@article{thiruvathukal_how_nodate-2,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{garlan_architectural_1995,
	title = {Architectural {Mismatch}: {Why} {Reuse} {Is} {So} {Hard}},
	volume = {12},
	issn = {07407459},
	doi = {10.1109/52.469757},
	number = {6},
	journal = {IEEE Software},
	author = {Garlan, David and Allen, Robert and Ockerbloom, John},
	year = {1995},
	pages = {17--26},
}

@article{kalleberg_finding_2011,
	title = {Finding {Software} {License} {Violations} {Through} {Binary} {Code} {Clone} {Detection}},
	author = {Kalleberg, Karl Trygve and Vermaas, Rob and Dolstra, Eelco},
	year = {2011},
	note = {ISBN: 9781450305747},
	keywords = {binary analysis, code clone detection, repository mining},
	pages = {63--72},
}

@article{chapman_short_2010,
	title = {A {Short} {Guide} {To} {Open} {Source} {Licenses}},
	url = {https://www.smashingmagazine.com/2010/03/a-short-guide-to-open-source-and-similar-licenses/},
	journal = {Smashing Magazine},
	author = {Chapman, Cameron},
	year = {2010},
	pages = {1--9},
}

@article{thiruvathukal_how_nodate-3,
	title = {How to {Engineer} an {Exemplar} for the {TensorFlow} {Model} {Garden}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{schroepfer_accelerating_2016,
	title = {Accelerating {Innovation} and {Powering} {New} {Experiences} with {AI}},
	url = {http://newsroom.fb.com/news/2016/11/accelerating-innovation-and-powering-new-experiences-with-ai/},
	author = {Schroepfer, Mike},
	year = {2016},
}

@article{neelamegam_survey_2012,
	title = {A {Survey} - {Object} {Oriented} {Quality} {Metrics}},
	volume = {12},
	url = {http://paper.ijcsns.org/07_book/201204/20120420.pdf},
	number = {4},
	journal = {Ijcsns},
	author = {Neelamegam, C. and Punithavalli, M.},
	year = {2012},
	keywords = {Tesi, class complexity, fault prediction, object oriented quality metrics, software, support vector machine},
	pages = {183--186},
}

@article{bassi_measuring_2018,
	title = {Measuring {Developers} ’ {Contribution} in {Source} {Code} using {Quality} {Metrics}},
	author = {Bassi, Patricia Rücker De},
	year = {2018},
	note = {ISBN: 9781538614822},
	keywords = {collaborative software development, developer contribution, metrics, open source projects, quality},
	pages = {39--44},
}

@article{lee_software_2014,
	title = {Software {Quality} {Factors} and {Software} {Quality} {Metrics} to {Enhance} {Software} {Quality} {Assurance}},
	volume = {4},
	number = {21},
	author = {Lee, Ming-chang},
	year = {2014},
	pages = {3069--3095},
}

@article{sadowski_how_2015,
	title = {How {Developers} {Search} for {Code} : {A} {Case} {Study}},
	author = {Sadowski, Caitlin and Stolee, Kathryn T. and Elbaum, Sebastian},
	year = {2015},
	note = {ISBN: 9781450336758},
	keywords = {17, 28, 3, 31, code search, code search appears to, developer tools, have ce-, mented its role in, software development, throughout this evolution, user evaluation},
	pages = {191--201},
}

@article{andersson_survey_1990,
	title = {A survey on software quality metrics},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.7502&rep=rep1&type=pdf},
	abstract = {Software Engineering is defined as “The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is, the application of engineering to software.”This is a survey of Software Quality Metrics. Definitions of the terms Software Crisis and Software Metrics found in the literature are presented. A need for the software metrics is given. A classification of software quality metrics is presented. The classification is: classical, product and process metrics. A number of different metrics are described. This will be followed by brief discussions about Measurement Scales and Current state of Software Metrics.},
	author = {Andersson, Thorbjorn},
	year = {1990},
	keywords = {data collection of software, metrics, quality, software quality metrics},
}

@article{pantiuchina_improving_2018,
	title = {Improving {Code} : {The} ( {Mis} ) perception of {Quality} {Metrics}},
	doi = {10.1109/ICSME.2018.00017},
	number = {Section V},
	author = {Pantiuchina, Jevgenija and Lanza, Michele and Bavota, Gabriele},
	year = {2018},
	note = {Publisher: IEEE},
}

@article{noauthor_quality_2020,
	title = {Quality assurance in research software},
	year = {2020},
}

@article{thiruvathukal_use_2019,
	title = {Use of {Software} {Process} in {Research} {Software} {Development} : {A} {Survey}},
	author = {Thiruvathukal, George K. and Carver, Jeffrey C.},
	year = {2019},
	note = {ISBN: 9781450371452},
	keywords = {2019, acm reference format, and jeffrey c, carver, eisty, george k, nasir u, research software, software process, survey, thiruvathukal, use},
}

@article{noauthor_no_nodate,
	title = {No {Title}},
}

@article{xu_quantitative_2020,
	title = {A {Quantitative} {Comparison} of {Different} {Machine} {Learning} {Approaches} for {Human} {Spermatozoa} {Quality} {Prediction} {Using} {Multimodal} {Datasets}},
	author = {Xu, Kele and Wang, Yin},
	year = {2020},
	note = {ISBN: 9781450379885},
	keywords = {2020, Machine learning, a quantitative comparison of, acm reference format, and yin wang, kele xu, machine learning, ming feng, multimo, multimodal, quantitative comparison},
	pages = {4659--4663},
}

@article{rawat_survey_2012,
	title = {Survey on {Impact} of {Software} {Metrics} on {Software} {Quality}},
	volume = {3},
	number = {1},
	author = {Rawat, Mrinal Singh},
	year = {2012},
	keywords = {- software metrics, and, function points, intended to measure, lines of code, object oriented metrics, software quality, software reliability, the metric should measure, valid, what it is},
	pages = {137--141},
}

@article{bamizadeh_analytical_1848,
	title = {An {Analytical} {Study} of {Code} {Smells}},
	volume = {6168},
	author = {Bamizadeh, Lida and Kumar, Binod and Kumar, Ajay and Shirwaikar, Shailaja},
	year = {1848},
	note = {ISBN: 2021020509},
	keywords = {code smells, data mining, knowledge repository, refactoring methods, software metrics},
	pages = {121--126},
}

@article{thiruvathukal_metrics_nodate,
	title = {Metrics {Dashboard} : {A} {Hosted} {Platform} for {Software} {Quality} {Metrics}},
	author = {Thiruvathukal, George K. and Hayward, Nicholas J. and Konstantin, L.},
}

@article{leotta_how_2019,
	title = {How {Do} {Implementation} {Bugs} {Affect} the {Results} of {Machine} {Learning} {Algorithms} ?},
	author = {Leotta, Maurizio and Informatica, Dip and Ingegneria, Robotica and Dibris, Sistemi and Genova, Università and Olianas, Dario and Informatica, Dip and Ingegneria, Robotica and Dibris, Sistemi and Genova, Università and Ricca, Filippo and Informatica, Dip and Ingegneria, Robotica and Dibris, Sistemi and Genova, Università and Dibris, Sistemi and Genova, Università},
	year = {2019},
	note = {ISBN: 9781450359337},
	keywords = {Accuracy, Bug, Machine Learning, Oracle P, Testing, accuracy, all or part of, bug, machine learning, or, or hard copies of, oracle problem, permission to make digital, soft-, testing, this work for personal, ware quality assurance},
	pages = {1304--1313},
}

@article{pecorelli_role_nodate,
	title = {On the {Role} of {Data} {Balancing} for {Machine} {Learning}-{Based} {Code} {Smell} {Detection}},
	author = {Pecorelli, Fabiano and Nucci, Dario Di and Lucia, Andrea De},
	note = {ISBN: 9781450368551},
	keywords = {acm reference format, and andrea de lucia, code smells, coen de roover, dario di nucci, data balancing, fabiano pecorelli, machine learning},
}

@article{jain_overview_2020,
	title = {Overview and {Importance} of {Data} {Quality} for {Machine} {Learning} {Tasks}},
	author = {Jain, Abhinav and Patel, Hima and Nagalapatti, Lokesh and Gupta, Nitin and Mehta, Sameep and Guttula, Shanmukha and Mujumdar, Shashank and Afzal, Shazia and Mittal, Ruhi Sharma and Munigala, Vitobha},
	year = {2020},
	note = {ISBN: 9781450379984},
	keywords = {abhinav jain, acm reference format, data quality, hima patel, lokesh nagalapatti, machine learning, nitin gupta, quality metrics, sameep mehta},
	pages = {3561--3562},
}

@article{shin_cross-domain_nodate,
	title = {Cross-domain meta-learning for bug finding in the source codes with a small dataset},
	author = {Shin, Jongho},
	note = {ISBN: 9781450375993},
	keywords = {all or part of, cross domain, cyber security, deep neural network, few-shot learning, learning, meta-, or, or hard copies of, permission to make digital, this work for personal, vulnerability detection},
}

@article{cosentino_assessing_2015,
	title = {Assessing the bus factor of {Git} repositories},
	doi = {10.1109/SANER.2015.7081864},
	abstract = {Software development projects face a lot of risks (requirements inflation, poor scheduling, technical problems, etc.). Underestimating those risks may put in danger the project success. One of the most critical risks is the employee turnover, that is the risk of key personnel leaving the project. A good indicator to evaluate this risk is to measure the concentration of information in individual developers. This is also popularly known as the bus factor ('number of key developers who would need to be incapacitated, i.e. hit by a bus, to make a project unable to proceed'). Despite the simplicity of the concept, calculating the actual bus factor for specific projects can quickly turn into an error-prone and time-consuming activity as soon as the size of the project and development team increase. In order to help project managers to assess the bus factor of their projects, in this paper we present a tool that, given a Git-based repository, automatically measures the bus factor for any file, directory and branch in the repository and for the project itself. You can also simulate with the tool what would happen to the project (e.g., which files would become orphans) if one or more developers disappeared.},
	journal = {2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
	author = {Cosentino, Valerio and Izquierdo, Javier Luis Canovas and Cabot, Jordi},
	year = {2015},
	note = {ISBN: 9781479984695
Publisher: IEEE},
	pages = {499--503},
}

@article{pineau_iclr_2018,
	title = {{ICLR} {Reproducibility} {Challenge}},
	volume = {2},
	url = {https://reproducibility-challenge.github.io/iclr_2019/},
	doi = {10.5281/zenodo.3158244},
	abstract = {Second Edition, 2019},
	journal = {ICLR Reproducibility Challenge},
	author = {Pineau, Joelle and Sinha, Koustuv and Fried, Genevieve and Ke, Rosemary Nan and Larochelle, Hugo},
	year = {2018},
	pages = {4--6},
}

@article{lin_perceptual_2011,
	title = {Perceptual visual quality metrics: {A} survey},
	volume = {22},
	issn = {10473203},
	url = {http://dx.doi.org/10.1016/j.jvcir.2011.01.005},
	doi = {10.1016/j.jvcir.2011.01.005},
	abstract = {Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics (namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally 3832 test images). We highlight the most significant research work for each topic and provide the links to the extensive relevant literature. © 2011 Elsevier Inc. All rights reserved.},
	number = {4},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Lin, Weisi and Kuo, C. C. Jay},
	year = {2011},
	note = {Publisher: Elsevier Inc.},
	keywords = {Common feature and artifact detection, Full reference, Human visual system (HVS), Just-noticeable distortion, No reference, Reduced reference, Signal decomposition, Signal-driven model, Vision-based model, Visual attention},
	pages = {297--312},
}

@article{potvin_why_2016,
	title = {Why google stores billions of lines of code in a single repository},
	volume = {59},
	issn = {15577317},
	doi = {10.1145/2854146},
	number = {7},
	journal = {Communications of the ACM},
	author = {Potvin, Rachel and Levenberg, Josh},
	year = {2016},
	pages = {78--87},
}

@article{bacchelli_expectations_2013,
	title = {Expectations, outcomes, and challenges of modern code review},
	issn = {02705257},
	doi = {10.1109/ICSE.2013.6606617},
	abstract = {Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more 'lightweight' than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classified hundreds of review comments across diverse teams at Microsoft. Our study reveals that while finding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional benefits such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we find that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers. © 2013 IEEE.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Bacchelli, Alberto and Bird, Christian},
	year = {2013},
	note = {ISBN: 9781467330763},
	pages = {712--721},
}

@article{ananthanarayanan_keeping_2019,
	title = {Keeping master green at scale},
	doi = {10.1145/3302424.3303970},
	abstract = {Giant monolithic source-code repositories are one of the fundamental pillars of the back end infrastructure in large and fast-paced software companies. The sheer volume of everyday code changes demands a reliable and efficient change management system with three uncompromisable key requirements — always green master, high throughput, and low commit turnaround time. Green refers to a master branch that always successfully compiles and passes all build steps, the opposite being red. A broken master (red) leads to delayed feature rollouts because a faulty code commit needs to be detected and rolled backed. Additionally, a red master has a cascading effect that hampers developer productivity—developers might face local test/build failures, or might end up working on a codebase that will eventually be rolled back. This paper presents the design and implementation of SubmitQueue. It guarantees an always green master branch at scale: all build steps (e.g., compilation, unit tests, UI tests) successfully execute for every commit point. SubmitQueue has been in production for over a year, and can scale to thousands of daily commits to giant monolithic repositories.},
	journal = {Proceedings of the 14th EuroSys Conference 2019},
	author = {Ananthanarayanan, Sundaram and Ardekani, Masoud Saeida and Haenikel, Denis and Varadarajan, Balaji and Soriano, Simon and Patel, Dhaval and Adl-Tabatabai, Ali Reza},
	year = {2019},
	note = {ISBN: 9781450362818},
}

@article{mcdermott_reproducibility_2019,
	title = {Reproducibility in machine learning for health},
	issn = {23318422},
	abstract = {Machine learning algorithms designed to characterize, monitor, and intervene on human health (ML4H) are expected to perform safely and reliably when operating at scale, potentially outside strict human supervision. This requirement warrants a stricter attention to issues of reproducibility than other fields of machine learning. In this work, we conduct a systematic evaluation of over 100 recently published ML4H research papers along several dimensions related to reproducibility. We find that the field of ML4H compares poorly to more established machine learning fields, particularly concerning data and code accessibility. Finally, drawing from success in other fields of science, we propose recommendations to data providers, academic publishers, and the ML4H research community in order to promote reproducible research moving forward.},
	journal = {arXiv},
	author = {McDermott, Matthew B. A. and Wang, Shirly and Marinsek, Nikki and Ranganath, Rajesh and Ghassemi, Marzyeh and Foschini, Luca},
	year = {2019},
}

@article{schroder_reproducible_2019,
	title = {Reproducible {Research} is more than {Publishing} {Research} {Artefacts}: {A} systematic analysis of jupyter notebooks from research articles},
	journal = {arXiv},
	author = {Schröder, Max and Krüger, Frank and Spors, Sascha},
	year = {2019},
	pages = {2--5},
}

@article{rule_ten_2018,
	title = {Ten simple rules for reproducible research in jupyter notebooks},
	issn = {23318422},
	abstract = {Reproducibility of computational studies is a hallmark of scientific methodology. It enables researchers to build with confidence on the methods and findings of others, reuse and extend computational pipelines, and thereby drive scientific progress. Since many experimental studies rely on computational analyses, biologists need guidance on how to set up and document reproducible data analyses or simulations. In this paper, we address several questions about reproducibility. For example, what are the technical and non-technical barriers to reproducible computational studies? What opportunities and challenges do computational notebooks offer to overcome some of these barriers? What tools are available and how can they be used effectively? We have developed a set of rules to serve as a guide to scientists with a specific focus on computational notebook systems, such as Jupyter Notebooks, which have become a tool of choice for many applications. Notebooks combine detailed workflows with narrative text and visualization of results. Combined with software repositories and open source licensing, notebooks are powerful tools for transparent, collaborative, reproducible, and reusable data analyses.},
	journal = {arXiv},
	author = {Rule, Adam and Birmingham, Amanda and Zuniga, Cristal and Altintas, Ilkay and Huang, Shih Cheng and Knight, Rob and Moshiri, Niema and Nguyen, Mai H. and Rosenthal, Sara Brin and Pérez, Fernando and Rose, Peter W.},
	year = {2018},
}

@article{graphs_neurips_2020,
	title = {{NeurIPS} 2020},
	author = {Graphs, Dynamic and Pooling, Graph and Power, Expressive},
	year = {2020},
	pages = {3--7},
}

@article{science_machine_2020,
	title = {The {Machine} {Learning} {Paper} {Paper} {Reproducibility} {Checklist}},
	author = {Science, McGill School of Computer},
	year = {2020},
	pages = {0--1},
}

@article{wang_assessing_2020,
	title = {Assessing and {Restoring} {Reproducibility} of {Jupyter} {Notebooks}},
	doi = {10.1145/3324884.3416585},
	abstract = {Jupyter notebooks-documents that contain live code, equations, visualizations, and narrative text-now are among the most popular means to compute, present, discuss and disseminate scientific findings. In principle, Jupyter notebooks should easily allow to reproduce and extend scientific computations and their findings; but in practice, this is not the case. The individual code cells in Jupyter notebooks can be executed in any order, with identifier usages preceding their definitions and results preceding their computations. In a sample of 936 published notebooks that would be executable in principle, we found that 73\% of them would not be reproducible with straightforward approaches, requiring humans to infer (and often guess) the order in which the authors created the cells. In this paper, we present an approach to (1) automatically satisfy dependencies between code cells to reconstruct possible execution orders of the cells; and (2) instrument code cells to mitigate the impact of non-reproducible statements (i.e., random functions) in Jupyter notebooks. Our Osiris prototype takes a notebook as input and outputs the possible execution schemes that reproduce the exact notebook results. In our sample, Osiris was able to reconstruct such schemes for 82.23\% of all executable notebooks, which has more than three times better than the state-of-the-art; the resulting reordered code is valid program code and thus available for further testing and analysis.},
	journal = {Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020},
	author = {Wang, Jiawei and Kuo, Tzu Yang and Li, Li and Zeller, Andreas},
	year = {2020},
	note = {ISBN: 9781450367684},
	pages = {138--149},
}

@article{zhou_metamorphic_2016,
	title = {Metamorphic {Testing} for {Software} {Quality} {Assessment}: {A} {Study} of {Search} {Engines}},
	volume = {42},
	issn = {19393520},
	doi = {10.1109/TSE.2015.2478001},
	abstract = {Metamorphic testing is a testing technique that can be used to verify the functional correctness of software in the absence of an ideal oracle. This paper extends metamorphic testing into a user-oriented approach to software verification, validation, and quality assessment, and conducts large scale empirical studies with four major web search engines: Google, Bing, Chinese Bing, and Baidu. These search engines are very difficult to test and assess using conventional approaches owing to the lack of an objective and generally recognized oracle. The results are useful for both search engine developers and users, and demonstrate that our approach can effectively alleviate the oracle problem and challenges surrounding a lack of specifications when verifying, validating, and evaluating large and complex software systems.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhou, Zhi Quan and Xiang, Shaowen and Chen, Tsong Yueh},
	year = {2016},
	keywords = {Software quality, lack of system specification, metamorphic testing, oracle problem, quality assessment, search engine, user-oriented testing, validation, verification},
	pages = {260--280},
}

@article{id_based_2006,
	title = {Based {Systems}},
	volume = {66},
	number = {March},
	author = {Id, Publication},
	year = {2006},
	pages = {3714},
}

@article{noauthor_mlreproducibility_2019,
	title = {{MLReproducibility} {Checklist} from {NeurIPS}},
	year = {2019},
	pages = {2019},
}

@article{information_neurips_2021,
	title = {{NeurIPS} 2019},
	author = {Information, Neural and Systems, Processing},
	year = {2021},
	pages = {2019--2021},
}

@article{hoang_reproducibility_2020,
	title = {Reproducibility {Companion} {Paper}: {Selective} {Deep} {Convolutional} {Features} for {Image} {Retrieval}},
	doi = {10.1145/3394171.3414814},
	author = {Hoang, Tuan and Do, Thanh-Toan and Cheung, Ngai-Man and Riegler, Michael and Zahálka, Jan},
	year = {2020},
	note = {ISBN: 9781450379885},
	keywords = {Aggregat, Content-Based Image Retrieval, Embedding, aggregating, content-based image retrieval, deep, embedding},
	pages = {4448--4452},
}

@article{chen_replication_2019,
	title = {Replication can improve prior results: {A} github study of pull request acceptance},
	volume = {2019-May},
	doi = {10.1109/ICPC.2019.00037},
	abstract = {Crowdsourcing and data mining can be used to effectively reduce the effort associated with the partial replication and enhancement of qualitative studies. For example, in a primary study, other researchers explored factors influencing the fate of GitHub pull requests using an extensive qualitative analysis of 20 pull requests. Guided by their findings, we mapped some of their qualitative insights onto quantitative questions. To determine how well their findings generalize, we collected much more data (170 additional pull requests from 142 GitHub projects). Using crowdsourcing, that data was augmented with subjective qualitative human opinions about how pull requests extended the original issue. The crowd's answers were then combined with quantitative features and, using data mining, used to build a predictor for whether code would be merged. That predictor was far more accurate than the one built from the primary study's qualitative factors (F1=90 vs 68\%), illustrating the value of a mixed-methods approach and replication to improve prior results. To test the generality of this approach, the next step in future work is to conduct other studies that extend qualitative studies with crowdsourcing and data mining.},
	journal = {IEEE International Conference on Program Comprehension},
	author = {Chen, Di and Stolee, Kathyrn and Menzies, Tim},
	year = {2019},
	note = {ISBN: 9781728115191},
	keywords = {Crowdsourcing, Github, Replication, Software Engineering},
	pages = {179--190},
}

@article{supeshala_yolo_2020,
	title = {{YOLO} v4 or {YOLO} v5 or {PP}-{YOLO}?},
	author = {Supeshala, Chamidu},
	year = {2020},
	pages = {1--11},
}

@article{bell_deflaker_2018,
	title = {{DeFlaker}: {Automatically} {Detecting} {Flaky} {Tests}},
	url = {http://dl.acm.org/citation.cfm?doid=3180155.3180164},
	abstract = {Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle. We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in the 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1, 874 flaky tests from 4, 846 failures, with a low false alarm rate (1.5\%). DeFlaker had a higher recall (95.5\% vs. 23\%) of confirmed flaky tests than Maven's default flaky test detector. CCS CONCEPTS • Software and its engineering → Software testing and de-bugging;},
	journal = {Proceedings of the 40th International Conference on Software Engineering - ICSE '18},
	author = {Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},
	year = {2018},
	note = {ISBN: 9781450356381},
	keywords = {acm reference format, code coverage, flaky tests, jonathan bell, lamyaa eloussi, michael hilton, owolabi legunsen, software testing, tifany},
	pages = {433--444},
}

@article{chen_metamorphic_1998,
	title = {Metamorphic testing: {A} new approach for generating next test cases},
	issn = {23318422},
	abstract = {In software testing, a set of test cases is constructed according to some predefined selection criteria. The software is then examined against these test cases. Three interesting observations have been made on the current artifacts of software testing. Firstly, an errorrevealing test case is considered useful while a successful test case which does not reveal software errors is usually not further investigated. Whether these successful test cases still contain useful information for revealing software errors has not been properly studied. Secondly, no matter how extensive the testing has been conducted in the development phase, errors may still exist in the software [5]. These errors, if left undetected, may eventually cause damage to the production system. The study of techniques for uncovering software errors in the production phase is seldom addressed in the literature. Thirdly, as indicated by Weyuker in [6], the availability of test oracles is pragmatically unattainable in most situations. However, the availability of test oracles is generally assumed in conventional software testing techniques. In this paper, we propose a novel test case selection technique that derives new test cases from the successful ones. The selection aims at revealing software errors that are possibly left undetected in successful test cases which may be generated using some existing strategies. As such, the proposed technique augments the effectiveness of existing test selection strategies. The technique also helps uncover software errors in the production phase and can be used in the absence of test oracles.},
	journal = {arXiv},
	author = {Chen, T. Y. and Cheung, S. C. and Yiu, S. M.},
	year = {1998},
	keywords = {Selection of Test Cases, Software Quality, Software Testing},
	pages = {1--11},
}

@article{islam_reproducibility_2017,
	title = {Reproducibility of benchmarked deep reinforcement learning tasks for continuous control},
	issn = {23318422},
	abstract = {Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.},
	journal = {arXiv},
	author = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
	year = {2017},
}

@article{raff_step_2019,
	title = {A step toward quantifying independently reproducible machine learning research},
	issn = {23318422},
	abstract = {What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.},
	number = {NeurIPS},
	journal = {arXiv},
	author = {Raff, Edward},
	year = {2019},
}

@article{bouthillier_unreproducible_2019,
	title = {Unreproducible research is reproducible},
	volume = {2019-June},
	abstract = {The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.},
	journal = {36th International Conference on Machine Learning, ICML 2019},
	author = {Bouthillier, Xavier and Laurent, César and Vincent, Pascal},
	year = {2019},
	note = {ISBN: 9781510886988},
	pages = {1150--1159},
}

@article{thulasidasan_mixup_2019,
	title = {On mixup training: {Improved} calibration and predictive uncertainty for deep neural networks},
	issn = {23318422},
	abstract = {Mixup [28] is a recently proposed method for training deep neural networks where additional samples are generated during training by convexly combining random pairs of images and their associated labels. While simple to implement, it has shown to be a surprisingly effective method of data augmentation for image classification; DNNs trained with mixup show noticeable gains in classification performance on a number of image classification benchmarks. In this work, we discuss a hitherto untouched aspect of mixup training-the calibration and predictive uncertainty of models trained with mixup. We find that DNNs trained with mixup are significantly better calibrated-i.e the predicted softmax scores are much better indicators of the actual likelihood of a correct prediction-than DNNs trained in the regular fashion. We conduct experiments on a number of image classification architectures and datasets-including large-scale datasets like ImageNet-and find this to be the case. Additionally, we find that merely mixing features does not result in the same calibration benefit and that the label smoothing in mixup training plays a significant role in improving calibration. Finally, we also observe that mixuptrained DNNs are less prone to over-confident predictions on out-of-distribution and random-noise data. We conclude that the typical overconfidence seen in neural networks, even on in-distribution data is likely a consequence of training with hard labels, suggesting that mixup training be employed for classification tasks where predictive uncertainty is a significant concern.},
	number = {1},
	journal = {arXiv},
	author = {Thulasidasan, Sunil and Chennupati1, Gopinath and Bilmes, Jeff and Bhattacharya1, Tanmoy and Michalak, Sarah},
	year = {2019},
	pages = {1--7},
}

@article{olorisade_reproducibility_2017,
	title = {Reproducibility in {Machine} {Learning}-{Based} {Studies}: {An} {Example} of {Text} {Mining}},
	abstract = {Reproducibility is an essential requirement for computational studies including those based on machine learning techniques. However, many machine learning studies are either not reproducible or are difficult to reproduce. In this paper, we consider what information about text mining studies is crucial to successful reproduction of such studies. We identify a set of factors that affect reproducibility based on our experience of attempting to reproduce six studies proposing text mining techniques for the automation of the citation screening stage in the systematic review process. Subsequently, the reproducibility of 30 studies was evaluated based on the presence or otherwise of information relating to the factors. While the studies provide useful reports of their results, they lack information on access to the dataset in the form and order as used in the original study (as against raw data), the software environment used, randomization control and the implementation of proposed techniques. In order to increase the chances of being reproduced, researchers should ensure that details about and/or access to information about these factors are provided in their reports.},
	number = {Icml 2017},
	journal = {Reproducibility in ML Workshop at the 34th International Conference on Machine Learnin},
	author = {Olorisade, Babatunde K. and Brereton, Pearl and Andras, Peter},
	year = {2017},
	pages = {471--486},
}

@article{freire_computational_2012,
	title = {Computational {Reproducibility}: {State}-of-the-{Art}, {Challenges}, and {Database} {Research} {Opportunities}},
	doi = {10.1145/2213836.2213908},
	abstract = {Computational experiments have become an integral part of the scientific method, but reproducing, archiving, and querying them is still a challenge. The first barrier to a wider adoption is the fact that it is hard both for authors to derive a compendium that encapsulates all the components needed to reproduce a result and for reviewers to verify the results. In this tutorial, we will present a series of guidelines and, through hands-on examples, review existing tools to help authors create of reproducible results. We will also outline open problems and new directions for database-related research having to do with querying computational experiments.},
	author = {Freire, Juliana and Bonnet, Philippe and Shasha, Dennis},
	year = {2012},
	note = {ISBN: 9781450312479},
	keywords = {but also natural science, computational experiments support not, computational reproducibily, only computer science, provenance, re-, research, social science and humanities},
	pages = {593},
}

@article{song_generative_2019,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	issn = {23318422},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients might be ill-defined when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.91 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	number = {Section 4},
	journal = {arXiv},
	author = {Song, Yang and Ermon, Stefano},
	year = {2019},
	pages = {1--15},
}

@article{samuel_machine_2020,
	title = {Machine learning pipelines: {Provenance}, reproducibility and {FAIR} data principles},
	issn = {23318422},
	abstract = {Machine learning (ML) is an increasingly important scientific tool supporting decision making and knowledge generation in numerous fields. With this, it also becomes more and more important that the results of ML experiments are reproducible. Unfortunately, that often is not the case. Rather, ML, similar to many other disciplines, faces a reproducibility crisis. In this paper, we describe our goals and initial steps in supporting the end-to-end reproducibility of ML pipelines. We investigate which factors beyond the availability of source code and datasets influence reproducibility of ML experiments. We propose ways to apply FAIR data practices to ML workflows. We present our preliminary results on the role of our tool, ProvBook, in capturing and comparing provenance of ML experiments and their reproducibility using Jupyter Notebooks.},
	journal = {arXiv},
	author = {Samuel, Sheeba and Löffler, Frank and König-Ries, Birgitta},
	year = {2020},
	pages = {1--2},
}

@article{wang_exploring_2019,
	title = {Exploring {Regular} {Expression} {Evolution}},
	doi = {10.1109/SANER.2019.8667972},
	abstract = {Although there are tools to help developers understand the matching behaviors between a regular expression and a string, regular-expression related faults are still common. Learning developers' behavior through the change history of regular expressions can identify common edit patterns, which can inform the creation of mutation and repair operators to assist with testing and fixing regular expressions. In this work, we explore how regular expressions evolve over time, focusing on the characteristics of regular expression edits, the syntactic and semantic difference of the edits, and the feature changes of edits. Our exploration uses two datasets. First, we look at GitHub projects that have a regular expression in their current version and look back through the commit logs to collect the regular expressions' edit history. Second, we collect regular expressions composed by study participants during problem-solving tasks. Our results show that 1) 95\% of the regular expressions from GitHub are not edited, 2) most edited regular expressions have a syntactic distance of 4-6 characters from their predecessors, 3) over 50\% of the edits in GitHub tend to expand the scope of regular expression, and 4) the number of features used indicates the regular expression language usage increases over time. This work has implications for supporting regular expression repair and mutation to ensure test suite quality.},
	journal = {SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
	author = {Wang, Peipei and Bai, Gina R. and Stolee, Kathryn T.},
	year = {2019},
	note = {ISBN: 9781728105918},
	keywords = {Regular expressions, empirical studies, evolution},
	pages = {502--513},
}

@article{bai_exploring_2019,
	title = {Exploring tools and strategies used during regular expression composition tasks},
	volume = {2019-May},
	doi = {10.1109/ICPC.2019.00039},
	abstract = {Regular expressions are frequently found in programming projects. Studies have found that developers can accurately determine whether a string matches a regular expression. However, we still do not know the challenges associated with composing regular expressions. We conduct an exploratory case study to reveal the tools and strategies developers use during regular expression composition. In this study, 29 students are tasked with composing regular expressions that pass unit tests illustrating the intended behavior. The tasks are in Java and the Eclipse IDE was set up with JUnit tests. Participants had one hour to work and could use any Eclipse tools, web search, or web-based tools they desired. Screen-capture software recorded all interactions with browsers and the IDE. We analyzed the videos quantitatively by transcribing logs and extracting personas. Our results show that participants were 30\% successful (28 of 94 attempts) at achieving a 100\% pass rate on the unit tests. When participants used tools frequently, as in the case of the novice tester and the knowledgeable tester personas, or when they guess at a solution prior to searching, they are more likely to pass all the unit tests. We also found that compile errors often arise when participants searched for a result and copy/pasted the regular expression from another language into their Java files. These results point to future research into making regular expression composition easier for programmers, such as integrating visualization into the IDE to reduce context switching or providing language migration support when reusing regular expressions written in another language to reduce compile errors.},
	journal = {IEEE International Conference on Program Comprehension},
	author = {Bai, Gina R. and Clee, Brian and Shrestha, Nischal and Chapman, Carl and Wright, Cimone and Stolee, Kathryn T.},
	year = {2019},
	note = {ISBN: 9781728115191},
	keywords = {Exploratory study, Personas, Problem solving strategies, Regular expressions},
	pages = {197--208},
}

@article{davis_testing_2019,
	title = {Testing regex generalizability and its implications: {A} large-scale many-language measurement study},
	doi = {10.1109/ASE.2019.00048},
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines.},
	journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
	author = {Davis, James C. and Moyer, Daniel and Kazerouni, Ayaan M. and Lee, Dongyoon},
	year = {2019},
	note = {ISBN: 9781728125084
Publisher: IEEE},
	keywords = {Data driven design, Empirical software engineering, Methods, Regular expressions},
	pages = {427--439},
}

@article{thiruvathukal_best_nodate,
	title = {Best {Practices} for {Reproducing} {Models} in {TensorFlow}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{park_empirical_2012,
	title = {An empirical study of supplementary bug fixes},
	issn = {21601852},
	doi = {10.1109/MSR.2012.6224298},
	abstract = {A recent study finds that errors of omission are harder for programmers to detect than errors of commission. While several change recommendation systems already exist to prevent or reduce omission errors during software development, there have been very few studies on why errors of omission occur in practice and how such errors could be prevented. In order to understand the characteristics of omission errors, this paper investigates a group of bugs that were fixed more than once in open source projects - those bugs whose initial patches were later considered incomplete and to which programmers applied supplementary patches. Our study on Eclipse JDT core, Eclipse SWT, and Mozilla shows that a significant portion of resolved bugs (22\% to 33\%) involves more than one fix attempt. Our manual inspection shows that the causes of omission errors are diverse, including missed porting changes, incorrect handling of conditional statements, or incomplete refactorings, etc. While many consider that missed updates to code clones often lead to omission errors, only a very small portion of supplementary patches (12\% in JDT, 25\% in SWT, and 9\% in Mozilla) have a content similar to their initial patches. This implies that supplementary change locations cannot be predicted by code clone analysis alone. Furthermore, 14\% to 15\% of files in supplementary patches are beyond the scope of immediate neighbors of their initial patch locations - they did not overlap with the initial patch locations nor had direct structural dependencies on them (e.g. calls, accesses, subtyping relations, etc.). These results call for new types of omission error prevention approaches that complement existing change recommendation systems. © 2012 IEEE.},
	journal = {IEEE International Working Conference on Mining Software Repositories},
	author = {Park, Jihun and Kim, Miryung and Ray, Baishakhi and Bae, Doo Hwan},
	year = {2012},
	note = {ISBN: 9781467317610
Publisher: IEEE},
	keywords = {bug fixes, empirical study, patches, software evolution},
	pages = {40--49},
}

@article{franco_comprehensive_2017,
	title = {A {Comprehensive} {Study} of {Real}-{World} {Numerical} {Bug} {Characteristics}},
	url = {https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=A+Comprehensive+Study+of+Real-World+Numerical+Bug+Characteristics&btnG=},
	journal = {Proceedings of the 32Nd IEEE/ACM International Conference on Automated Software Engineering},
	author = {Franco, Anthony Di and Guo, Hui and Rubio-gonzález, Cindy},
	year = {2017},
	note = {ISBN: 978-1-5386-2684-9},
	keywords = {empirical study, numerical bugs},
	pages = {509--519},
}

@article{thiruvathukal_best_nodate-1,
	title = {Best {Practices} for {Reproducing} {Models} in {TensorFlow}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{dolby_ariadne_2018,
	title = {Ariadne: {Analysis} for machine learning programs},
	doi = {10.1145/3211346.3211349},
	abstract = {Machine learning has transformed domains like vision and translation, and is now increasingly used in science, where the correctness of such code is vital. Python is popular for machine learning, in part because of its wealth of machine learning libraries, and is felt to make development faster; however, this dynamic language has less support for error detection at code creation time than tools like Eclipse. This is especially problematic for machine learning: given its statistical nature, code with subtle errors may run and produce results that look plausible but are meaningless. This can vitiate scientific results. We report on: applying a static framework, WALA, to machine learning code that uses TensorFlow. We have created static analysis for Python, a type system for tracking tensorsTensorflows core data structuresand a data flow analysis to track their usage. We report on how it was built and present some early results.},
	journal = {MAPL 2018 - Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, co-located with PLDI 2018},
	author = {Dolby, J. Julian and Shinnar, A. Avraham and Allain, A. Allison and Reinen, J. Jenna},
	year = {2018},
	note = {ISBN: 9781450358347},
	keywords = {Machine learning, Program analysis},
	pages = {1--10},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {14764687},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	pmid = {26017442},
	pages = {436--444},
}

@article{odena_tensorfuzz_2019,
	title = {{TensorFuzz}: {Debugging} neural networks with coverage-guided fuzzing},
	volume = {2019-June},
	abstract = {Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Specifically, wc develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how approximate nearest neighbor (ANN) algorithms can provide this coverage metric for neural networks. We then combine these methods with techniques for property-based testing (PBT). In PBT, one asserts properties that a function should satisfy and the system automatically generates tests exercising those properties. We then apply this system to practical goals including (but not limited to) surfacing broken loss functions in popular GitHub repositories and making performance improvements to TensorFlow. Finally, we release an open source library called TensorFuzz that implements the described techniques.},
	journal = {36th International Conference on Machine Learning, ICML 2019},
	author = {Odena, Augustus and Olsson, Catherine and Andersen, David G. and Goodfellow, Ian},
	year = {2019},
	note = {ISBN: 9781510886988},
	pages = {8603--8613},
}

@article{lin_empirical_2009,
	title = {An empirical study on bug assignment automation using {Chinese} bug data},
	doi = {10.1109/ESEM.2009.5315994},
	abstract = {Bug assignment is an important step in bug life-cycle management. In large projects, this task would consume a substantial amount of human effort. To compare with the previous studies on automatic bug assignment in FOSS (Free/Open Source Software) projects, we conduct a case study on a proprietary software project in China. Our study consists of two experiments of automatic bug assignment, using Chinese text and the other non-text information of bug data respectively. Based on text data of the bug repository, the first experiment uses SVM to predict bug assignments and achieve accuracy close to that by human triagers. The second one explores the usefulness of non-text data in making such prediction. The main results from our study includes that text data are most useful data in the bug tracking system to triage bugs, and automation based on text data could effectively reduce the manual effort. © 2009 IEEE.},
	journal = {2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009},
	author = {Lin, Zhongpeng and Shu, Fengdi and Yang, Ye and Hu, Chenyong and Wang, Qing},
	year = {2009},
	note = {ISBN: 9781424448418
Publisher: IEEE},
	pages = {451--455},
}

@article{zhong_empirical_2015,
	title = {An empirical study on real bug fixes},
	volume = {1},
	issn = {02705257},
	doi = {10.1109/ICSE.2015.101},
	abstract = {Software bugs can cause significant financial loss and even the loss of human lives. To reduce such loss, developers devote substantial efforts to fixing bugs, which generally requires much expertise and experience. Various approaches have been proposed to aid debugging. An interesting recent research direction is automatic program repair, which achieves promising results, and attracts much academic and industrial attention. However, people also cast doubt on the effectiveness and promise of this direction. A key criticism is to what extent such approaches can fix real bugs. As only research prototypes for these approaches are available, it is infeasible to address the criticism by evaluating them directly on real bugs. Instead, in this paper, we design and develop BUGSTAT, a tool that extracts and analyzes bug fixes. With BUGSTAT's support, we conduct an empirical study on more than 9,000 real-world bug fixes from six popular Java projects. Comparing the nature of manual fixes with automatic program repair, we distill 15 findings, which are further summarized into four insights on the two key ingredients of automatic program repair: fault localization and faulty code fix. In addition, we provide indirect evidence on the size of the search space to fix real bugs and find that bugs may also reside in non-source files. Our results provide useful guidance and insights for improving the state-of-the-art of automatic program repair.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Zhong, Hao and Su, Zhendong},
	year = {2015},
	note = {ISBN: 9781479919345
Publisher: IEEE},
	pages = {913--923},
}

@article{zhang_empirical_2012,
	title = {An empirical study on factors impacting bug fixing time},
	issn = {10951350},
	doi = {10.1109/WCRE.2012.32},
	abstract = {Fixing bugs is an important activity of the software development process. A typical process of bug fixing consists of the following steps: 1) a user files a bug report, 2) the bug is assigned to a developer, 3) the developer fixes the bug, 4) changed code is reviewed and verified, and 5) the bug is resolved. Many studies have investigated the process of bug fixing. However, to the best of our knowledge, none has explicitly analyzed the interval between bug assignment and the time when bug fixing starts. After a bug assignment, some developers will immediately start fixing the bug while others will start bug fixing after a long period. We are blind on developer's delays when fixing bugs. This paper explores such delays of developers through an empirical study on three open source software systems. We examine factors affecting bug fixing time along three dimensions: bug reports, source code involved in the fix, and code changes that are required to fix the bug. We further compare different factors by descriptive logistic regression models. Our results can help development teams better understand factors behind delays, and then improve bug fixing process. © 2012 IEEE.},
	journal = {Proceedings - Working Conference on Reverse Engineering, WCRE},
	author = {Zhang, Feng and Khomh, Foutse and Zou, Ying and Hassan, Ahmed E.},
	year = {2012},
	note = {ISBN: 9780769548913
Publisher: IEEE},
	keywords = {bug fixing process, bug report, change request, empirical software engineering, fixing time, mylyn},
	pages = {225--234},
}

@article{zhang_predicting_2013,
	title = {Predicting bug-fixing time: {An} empirical study of commercial software projects},
	issn = {02705257},
	doi = {10.1109/ICSE.2013.6606654},
	abstract = {For a large and evolving software system, the project team could receive many bug reports over a long period of time. It is important to achieve a quantitative understanding of bug-fixing time. The ability to predict bug-fixing time can help a project team better estimate software maintenance efforts and better manage software projects. In this paper, we perform an empirical study of bug-fixing time for three CA Technologies projects. We propose a Markov-based method for predicting the number of bugs that will be fixed in future. For a given number of defects, we propose a method for estimating the total amount of time required to fix them based on the empirical distribution of bug-fixing time derived from historical data. For a given bug report, we can also construct a classification model to predict slow or quick fix (e.g., below or above a time threshold). We evaluate our methods using real maintenance data from three CA Technologies projects. The results show that the proposed methods are effective. © 2013 IEEE.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Zhang, Hongyu and Gong, Liang and Versteeg, Steve},
	year = {2013},
	note = {ISBN: 9781467330763
Publisher: IEEE},
	keywords = {Bugs, bug-fixing time, effort estimation, prediction, software maintenance},
	pages = {1042--1051},
}

@article{davis_testing_2019-1,
	title = {Testing regex generalizability and its implications: {A} large-scale many-language measurement study},
	doi = {10.1109/ASE.2019.00048},
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines.},
	journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
	author = {Davis, James C. and Moyer, Daniel and Kazerouni, Ayaan M. and Lee, Dongyoon},
	year = {2019},
	note = {ISBN: 9781728125084
Publisher: IEEE},
	keywords = {Data driven design, Empirical software engineering, Methods, Regular expressions},
	pages = {427--439},
}

@article{islam_bug_2016,
	title = {Bug {Replication} in {Code} {Clones}: {An} {Empirical} {Study}},
	doi = {10.1109/saner.2016.78},
	abstract = {Cholesterol oxidase (ChOx) has been immobilized onto sol-gel derived nano-structured cerium oxide (NS-CeO 2 ) film deposited on indium-tin-oxide (ITO) coated glass substrate. Phase identification of sol-gel NS-CeO 2 film carried out using X-ray diffraction (XRD) yields reflection peak at 29.4° corresponding to (1 1 1) plane with oriented crystallite (34 nm) along c-axis normal to the substrate. Electrochemical studies reveal that NS-CeO 2 provides electroactive surface for the loading of ChOx and enhances electron transfer rate in the ChOx/NS-CeO 2 /ITO bioelectrode. The low value of Michaelis-Menten constant (K m ) obtained as 2.08 mM indicates enhanced ChOx affinity to cholesterol. The observed results show application of sol-gel derived NS-CeO 2 for biosensing without any functionalization. © 2008 Elsevier B.V. All rights reserved.},
	author = {Islam, Judith F. and Mondal, Manishankar and Roy, Chanchal K.},
	year = {2016},
	note = {ISBN: 9781509018550
Publisher: IEEE},
	pages = {68--78},
}

@article{asaduzzaman_bug_2012,
	title = {Bug introducing changes: {A} case study with {Android}},
	issn = {21601852},
	doi = {10.1109/MSR.2012.6224267},
	abstract = {Changes, a rather inevitable part of software development can cause maintenance implications if they introduce bugs into the system. By isolating and characterizing these bug introducing changes it is possible to uncover potential risky source code entities or issues that produce bugs. In this paper, we mine the bug introducing changes in the Android platform by mapping bug reports to the changes that introduced the bugs. We then use the change information to look for both potential problematic parts and dynamics in development that can cause maintenance implications. We believe that the results of our study can help better manage Android software development. © 2012 IEEE.},
	journal = {IEEE International Working Conference on Mining Software Repositories},
	author = {Asaduzzaman, Muhammad and Bullock, Michael C. and Roy, Chanchal K. and Schneider, Kevin A.},
	year = {2012},
	note = {ISBN: 9781467317610
Publisher: IEEE},
	keywords = {Bug, bug report, change log, fixes},
	pages = {116--119},
}

@article{tarar_automated_2020,
	title = {Automated summarization of bug reports to speed-up software development/maintenance process by using natural language processing ({NLP})},
	doi = {10.1109/ICCSE49874.2020.9201846},
	abstract = {Bug reports can provide a great deal of assistance for developers during the process of development. But due to the large size of bug repositories, it is sometimes difficult to take advantage of these artifacts in the available time. One way of helping developers to provide summaries of these reports and provide relevant details only. Once it's decided that this is the required report then one can study the details. As text mining technology advances, many substantial approaches have been proposed to generate optimized summaries for bug reports. In this paper, we have proposed an extractive based methodology for the generation of summaries of bug reports by using the sentence embedding. We achieved improved rouge-1 and rouge- 2 results than the previous state of the art systems for the bug report summary generation.},
	number = {Iccse},
	journal = {15th International Conference on Computer Science and Education, ICCSE 2020},
	author = {Tarar, M. Irtaza Nawaz and Ahmed, Faizan and Butt, Wasi Haider},
	year = {2020},
	note = {ISBN: 9781728172675},
	keywords = {Bug reports, Machine Learning, Natural Language Processing, Software Artifacts, Summarization},
	pages = {483--488},
}

@article{karpathy_software_2017,
	title = {Software 2.0},
	author = {Karpathy, Andrej},
	year = {2017},
	pages = {1--8},
}

@article{wan_are_2021,
	title = {Are {Machine} {Learning} {Cloud} {APIs} {Used} {Correctly} ?},
	number = {Ml},
	author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
	year = {2021},
}

@article{marta_approach_2007,
	title = {An {Approach} to {Software} {Testing} of {Machine} {Learning} {Applications} {Christian}},
	url = {http://www.psl.cs.columbia.edu/publications/pubs/Murphy-SEKE2007.pdf},
	abstract = {Some learn are not already known to human users. It is challenging no approach aimed at addressing this problem. We present our findings from testing implementations of two different ML ranking algorithms: Support Vector Machines 1..Introduction We investigate the problem of making machine learning (ML) applications dependable, focusing on software testing. Conventional software engineering processes is anomalies (henceforth " bugs ") in the ML applications of interest because there is no reliable indicate input. The general class of software systems with no reliable test oracle available is sometimes known as " non-testable programs " [1]. These ML applications fall describe determine no were optimal quality do not guarantee that an application implements or uses the algorithm correctly, and thus software testing is needed. Our testing, then, does not seek to determine whether an ML algorithm learns well, algorithm correctly implements the specification and fulfills In this paper, we describe our approach to testing ML applications, in particular those that implement ranking algorithms (a requirement of the real-world problem is possible only to show the presence of bugs but not their absence. Usually when input or output equivalence classes are applied to developing test cases, known in advance. Our research seeks to address the issue bugs, and how one can indeed know whether a test actually what Our approach for creating test cases consists of three facets: analyzing the problem domain and the corresponding real-world data sets; analyzing the algorithm as it is defined; and analyzing the implementation's is conventional, not novel, a number of issues arise when applying it to determining equivalence classes and We present our findings to date from two case studies: our first concerns the Martingale Boosting algorithm, [3] initially as a classification algorithm and then adapted by Long and others into a ranking algorithm called and Machines 2..Background},
	author = {Marta, Gail},
	year = {2007},
}

@article{herbold_smoke_2020,
	title = {Smoke testing for machine learning: {Simple} tests to discover severe defects},
	volume = {0},
	issn = {23318422},
	abstract = {Machine learning is nowadays a standard technique for data analysis within software applications. Software engineers need quality assurance techniques that are suitable for these new kinds of systems. Within this article, we discuss the question whether standard software testing techniques that have been part of textbooks since decades are also useful for the testing of machine learning software. Concretely, we try to determine generic smoke tests that can be used to assert that basic functions can be executed without crashing. We found that we can derive such tests using techniques similar to equivalence classes and boundary value analysis. Moreover, we found that these concepts can also be applied to hyperparameters, to further improve the quality of the smoke tests. Even though our approach is almost trivial, we were able to find bugs in all three machine learning libraries that we tested and severe bugs in two of the three libraries. This demonstrates that common software testing techniques are still valid in the age of machine learning and that they are suitable to find and prevent severe bugs, even in mature machine learning libraries.},
	number = {0},
	journal = {arXiv},
	author = {Herbold, Steffen and Haar, Tobias},
	year = {2020},
	keywords = {Boundary-value analysis, Classification, Combinatorial testing, Equivalence classes, Machine learning, Smoke testing, Software testing},
}

@article{abid_gradio_2019,
	title = {Gradio: {Hassle}-free sharing and testing of {ML} models in the wild},
	issn = {23318422},
	abstract = {Accessibility is a major challenge of machine learning (ML). Typical ML models are built by specialists and require specialized hardware/software as well as ML experience to validate. This makes it challenging for non-technical collaborators and endpoint users (e.g. physicians) to easily provide feedback on model development and to gain trust in ML. The accessibility challenge also makes collaboration more difficult and limits the ML researcher's exposure to realistic data and scenarios that occur in the wild. To improve accessibility and facilitate collaboration, we developed an open-source Python package, Gradio, which allows researchers to rapidly generate a visual interface for their ML models. Gradio makes accessing any ML model as easy as sharing a URL. Our development of Gradio is informed by interviews with a number of machine learning researchers who participate in interdisciplinary collaborations. Their feedback identified that Gradio should support a variety of interfaces and frameworks, allow for easy sharing of the interface, allow for input manipulation and interactive inference by the domain expert, as well as allow embedding the interface in iPython notebooks. We developed these features and carried out a case study to understand Gradio's usefulness and usability in the setting of a machine learning collaboration between a researcher and a cardiologist.},
	number = {Ml},
	journal = {arXiv},
	author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},
	year = {2019},
}

@article{hambali_software_2020,
	title = {{SOFTWARE} {TESTING} {CHALLENGES} {IN} {MACHINE} {LEARNING} {APPLICATION}},
	volume = {5},
	number = {November},
	author = {Hambali, Muhamad Adham and Nazeer, Ahmad and Arifin, Zainal},
	year = {2020},
	pages = {720--723},
}

@article{santhanam_engineering_2019,
	title = {Engineering reliable deep learning systems},
	volume = {3},
	issn = {23318422},
	abstract = {Recent progress in artificial intelligence (AI) using deep learning techniques has triggered its wide-scale use across a broad range of applications. These systems can already perform tasks such as natural language processing of voice and text, visual recognition, question-answering, recom-mendations and decision support. However, at the current level of maturity, the use of an AI component in mission-critical or safety-critical applications can have unexpected consequences. Consequently, serious concerns about relia-bility, repeatability, trust, and maintainability of AI applica-tions remain. As AI becomes pervasive despite its short-comings, more systematic ways of approaching AI software development and certification are needed. These fundamen-tal aspects establish the need for a discipline on "AI engi-neering". This paper presents the current perspective of rel-evant AI engineering concepts and some key challenges that need to be overcome to make significant progress in this important area.},
	journal = {arXiv},
	author = {Santhanam, P. and Farchi, Eitan and Pankratius, Victor},
	year = {2019},
	pages = {1--8},
}

@article{xie_testing_2020,
	title = {Testing {Coverage} {Criteria} for {Deep} {Forests}},
	doi = {10.1109/DSA.2019.00091},
	abstract = {In practice, many unknown errors have emerged in deep learning systems. One of the main reasons is that the behaviors of deep learning systems are unpredictable and difficult to test. Proper testing criteria are vitally important to evaluate the adequacy of testing deep learning systems. However, there is no testing criterion available for the deep forest, which is a deep learning model that has achieved good performance on small-scale data sets and low-computing-power platform projects. To address this problem, we propose a set of testing coverage criteria for deep forests in this paper. The set of testing coverage criteria is composed of multi-grained scanning node coverage (MGNC), multi-grained scanning leaf coverage (MGLC), cascade forest output coverage (CFOC) and cascade forest class coverage (CFCC).},
	journal = {Proceedings - 2019 6th International Conference on Dependable Systems and Their Applications, DSA 2019},
	author = {Xie, Ruilin and Cui, Zhanqi and Jia, Minghua and Wen, Yuan and Hao, Baoshui},
	year = {2020},
	note = {ISBN: 9781728160573
Publisher: IEEE},
	keywords = {cascade forest coverage, deep forest, multi-grained scanning coverage, testing coverage criteria},
	pages = {513--514},
}

@article{afzal_study_2020,
	title = {A {Study} on {Challenges} of {Testing} {Robotic} {Systems}},
	doi = {10.1109/ICST46399.2020.00020},
	abstract = {Robotic systems are increasingly a part of everyday life. Characteristics of robotic systems such as interaction with the physical world, and integration of hardware and software components, differentiate robotic systems from conventional software systems. Although numerous studies have investigated the challenges of software testing in practice, no such study has focused on testing of robotic systems. In this paper, we conduct a qualitative study to better understand the testing practices used by the robotics community, and identify the challenges faced by practitioners when testing their systems. We identify a total of 12 testing practices and 9 testing challenges from our participants' responses. We group these challenges into 3 major themes: Real-world complexities, Community and standards, and Component integration. We believe that further research on addressing challenges described with these three major themes can result in higher adoption of robotics testing practices, more testing automation, and higher-quality robotic systems.},
	journal = {Proceedings - 2020 IEEE 13th International Conference on Software Testing, Verification and Validation, ICST 2020},
	author = {Afzal, Afsoon and Goues, Claire Le and Hilton, Michael and Timperley, Christopher Steven},
	year = {2020},
	note = {ISBN: 9781728157771},
	keywords = {qualitative study, robotics testing, testing challenges},
	pages = {96--107},
}

@article{durelli_machine_2019,
	title = {Machine learning applied to software testing: {A} systematic mapping study},
	volume = {68},
	issn = {15581721},
	doi = {10.1109/TR.2019.2892517},
	abstract = {Software testing involves probing into the behavior of software systems to uncover faults. Most testing activities are complex and costly, so a practical strategy that has been adopted to circumvent these issues is to automate software testing. There has been a growing interest in applying machine learning (ML) to automate various software engineering activities, including testingrelated ones. In this paper, we set out to review the state-of-the art of howML has been explored to automate and streamline software testing and provide an overview of the research at the intersection of these two fields by conducting a systematic mapping study. We selected 48 primary studies. These selected studies were then categorized according to study type, testing activity, andMLalgorithm employed to automate the testing activity. The results highlight the most widely used ML algorithms and identify several avenues for future research. We found that ML algorithms have been used mainly for test-case generation, refinement, and evaluation. Also, ML has been used to evaluate test oracle construction and to predict the cost of testing-related activities. The results of this paper outline the ML algorithms that are most commonly used to automate software-testing activities, helping researchers to understand the current state of research concerning ML applied to software testing. We also found that there is a need for better empirical studies examining how ML algorithms have been used to automate software-testing activities.},
	number = {3},
	journal = {IEEE Transactions on Reliability},
	author = {Durelli, Vinicius H. S. and Durelli, Rafael S. and Borges, Simone S. and Endo, Andre T. and Eler, Marcelo M. and Dias, Diego R. C. and Guimarães, Marcelo P.},
	year = {2019},
	keywords = {Machine learning (ML), Software testing, Systematic mapping study},
	pages = {1189--1212},
}

@article{sherin_systematic_2019,
	title = {A systematic mapping study on testing of machine learning programs},
	issn = {23318422},
	abstract = {Context: Machine learning (ML) has made tremendous progress in the last few years leading to usage in mission-critical and safety-critical systems. This has led researchers to focus on the techniques for testing ML-enabled systems, and has been further emphasized by recent hazardous incidents (e.g., Tesla car accident). Objective: We aim to conduct a systematic mapping in the area of testing ML programs. We identify, analyze and classify the existing literature to provide an overview of the area. Methodology: We followed well-established guidelines of systematic mapping to develop a systematic protocol to identify and review the existing literature. We formulate three sets of research questions, define inclusion and exclusion criteria and systematically identify themes for the classification of existing techniques. We also report the quality of the published works using established assessment criteria. Results: we finally selected 37 papers out of 1654 based on our selection criteria up to January 2019. We analyze trends such as contribution facet, research facet, test approach, type of ML and the kind of testing with several other attributes. We also discuss the empirical evidence and reporting quality of selected papers. The data from the study is made publicly available for other researchers and practitioners. Conclusion: We present an overview of the area by answering several research questions. The area is growing rapidly, however, there is lack of enough empirical evidence to compare and assess the effectiveness of the techniques. More publicly available tools are required for use of practitioners and researchers. Further attention is needed on non-functional testing and testing of ML programs using reinforcement learning. We believe that this study can help researchers and practitioners to obtain an overview of the area and identify several sub-areas where more research is required.},
	journal = {arXiv},
	author = {Sherin, Salman and Khan, Muhammad Uzair and Iqbal, Muhammad Zohaib},
	year = {2019},
	keywords = {Deep learning, Machine learning, Software testing, Systematic mapping study},
}

@article{riccio_testing_2020,
	title = {Testing machine learning based systems: a systematic mapping},
	volume = {25},
	issn = {15737616},
	doi = {10.1007/s10664-020-09881-0},
	abstract = {Context:: A Machine Learning based System (MLS) is a software system including one or more components that learn how to perform a task from a given data set. The increasing adoption of MLSs in safety critical domains such as autonomous driving, healthcare, and finance has fostered much attention towards the quality assurance of such systems. Despite the advances in software testing, MLSs bring novel and unprecedented challenges, since their behaviour is defined jointly by the code that implements them and the data used for training them. Objective:: To identify the existing solutions for functional testing of MLSs, and classify them from three different perspectives: (1) the context of the problem they address, (2) their features, and (3) their empirical evaluation. To report demographic information about the ongoing research. To identify open challenges for future research. Method:: We conducted a systematic mapping study about testing techniques for MLSs driven by 33 research questions. We followed existing guidelines when defining our research protocol so as to increase the repeatability and reliability of our results. Results:: We identified 70 relevant primary studies, mostly published in the last years. We identified 11 problems addressed in the literature. We investigated multiple aspects of the testing approaches, such as the used/proposed adequacy criteria, the algorithms for test input generation, and the test oracles. Conclusions:: The most active research areas in MLS testing address automated scenario/input generation and test oracle creation. MLS testing is a rapidly growing and developing research area, with many open challenges, such as the generation of realistic inputs and the definition of reliable evaluation metrics and benchmarks.},
	number = {6},
	journal = {Empirical Software Engineering},
	author = {Riccio, Vincenzo and Jahangirova, Gunel and Stocco, Andrea and Humbatova, Nargiz and Weiss, Michael and Tonella, Paolo},
	year = {2020},
	note = {Publisher: Empirical Software Engineering},
	keywords = {Machine learning, Software testing, Systematic mapping, Systematic review},
	pages = {5193--5254},
}

@article{dilhara_understanding_2018,
	title = {Understanding {Software}-2 . 0 : {A} {Study} of {Machine} {Learning} library usage and evolution},
	volume = {1},
	number = {1},
	author = {Dilhara, Malinda and Ketkar, Ameya and Dig, Danny},
	year = {2018},
	pages = {1--41},
}

@article{chen_behavior_2019,
	title = {Behavior pattern-driven test case selection for deep neural networks},
	doi = {10.1109/AITest.2019.000-2},
	abstract = {With the widespread application of deep learning systems, the robustness of deep neural networks (DNNs) is received increasing attentions recently. By studying the distribution of neurons outputs in DNN models, we found that the behavior patterns of neurons are different for different kinds of DNNs' inputs, e.g. test cases generated by different adversarial attack techniques. In this paper, we extract the neuron behavior patterns of DNNs under different adversarial attack techniques, use them as the guidance for test case selection. Experimental results show that this method is more efficient than random technology.},
	number = {4},
	journal = {Proceedings - 2019 IEEE International Conference on Artificial Intelligence Testing, AITest 2019},
	author = {Chen, Yanshan and Wang, Ziyuan and Wang, Dong and Yao, Yongming and Chen, Zhenyu},
	year = {2019},
	note = {ISBN: 9781728104928
Publisher: IEEE},
	keywords = {Behavior pattern, Deep neural network, Test case prioritization, Test case selection},
	pages = {89--90},
}

@article{liem_oracle_2020,
	title = {Oracle {Issues} in {Machine} {Learning} and {Where} to {Find} {Them}},
	doi = {10.1145/3387940.3391490},
	abstract = {The rise in popularity of machine learning (ML), and deep learning in particular, has both led to optimism about achievements of artificial intelligence, as well as concerns about possible weaknesses and vulnerabilities of ML pipelines. Within the software engineering community, this has led to a considerable body of work on ML testing techniques, including white- and black-box testing for ML models. This means the oracle problem needs to be addressed. For supervised ML applications, oracle information is indeed available in the form of dataset 'ground truth', that encodes input data with corresponding desired output labels. However, while ground truth forms a gold standard, there still is no guarantee it is truly correct. Indeed, syntactic, semantic, and conceptual framing issues in the oracle may negatively affect the ML system's integrity. While syntactic issues may automatically be verified and corrected, the higher-level issues traditionally need human judgment and manual analysis. In this paper, we employ two heuristics based on information entropy and semantic analysis on well-known computer vision models and benchmark data from ImageNet. The heuristics are used to semi-automatically uncover potential higher-level issues in (i) the label taxonomy used to define the ground truth oracle (labels), and (ii) data encoding and representation. In doing this, beyond existing ML testing efforts, we illustrate the need for software engineering strategies that especially target and assess the oracle.},
	journal = {Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020},
	author = {Liem, Cynthia C. S. and Panichella, Annibale},
	year = {2020},
	note = {ISBN: 9781450379632},
	pages = {483--488},
}

@article{zhou_using_2020,
	title = {Using {Metamorphic} {Testing} to {Evaluate} {DNN} {Coverage} {Criteria}},
	doi = {10.1109/ISSREW51248.2020.00055},
	abstract = {Generating test cases and further evaluating their 'quality' are two critical topics in the area of Deep Neural Networks (DNNs). In this domain, different studies (e.g., [1], [2]) have reported that metamorphic testing (MT) serves as an effective test case generation method, where an initial set of source test cases is augmented with identified metamorphic relations (MRs) to produce the corresponding set of follow-up test cases. As a result, the fault detection effectiveness (and, hence, the 'quality') of the resulting test suite T, containing these source and follow-up test cases, will most likely be increased.},
	journal = {Proceedings - 2020 IEEE 31st International Symposium on Software Reliability Engineering Workshops, ISSREW 2020},
	author = {Zhou, Jinyi and Qiu, Kun and Zheng, Zheng and Chen, Tsong Yueh and Poon, Pak Lok},
	year = {2020},
	note = {ISBN: 9781728198705},
	keywords = {Cover, Deep Neural Networks, Metamorphic relations},
	pages = {147--148},
}

@article{ferdinandy_challenges_2020,
	title = {Challenges of machine learning model validation using correlated behaviour data: {Evaluation} of cross-validation strategies and accuracy measures},
	volume = {15},
	issn = {19326203},
	doi = {10.1371/journal.pone.0236092},
	abstract = {Automated monitoring of the movements and behaviour of animals is a valuable research tool. Recently, machine learning tools were applied to many species to classify units of behaviour. For the monitoring of wild species, collecting enough data for training models might be problematic, thus we examine how machine learning models trained on one species can be applied to another closely related species with similar behavioural conformation. We contrast two ways to calculate accuracies, termed here as overall and threshold accuracy, because the field has yet to define solid standards for reporting and measuring classification performances. We measure 21 dogs and 7 wolves, and find that overall accuracies are between 51 and 60\% for classifying 8 behaviours (lay, sit, stand, walk, trot, run, eat, drink) when training and testing data are from the same species and between 41 and 51\% when training and testing is cross-species. We show that using data from dogs to predict the behaviour of wolves is feasible. We also show that optimising the model for overall accuracy leads to similar overall and threshold accuracies, while optimizing for threshold accuracy leads to threshold accuracies well above 80\%, but yielding very low overall accuracies, often below the chance level. Moreover, we show that the most common method for dividing the data between training and testing data (random selection of test data) overestimates the accuracy of models when applied to data of new specimens. Consequently, we argue that for the most common goals of animal behaviour recognition overall accuracy should be the preferred metric. Considering, that often the goal is to collect movement data without other methods of observation, we argue that training data and testing data should be divided by individual and not randomly.},
	number = {7},
	journal = {PLoS ONE},
	author = {Ferdinandy, Bence and Gerencsér, Linda and Corrieri, Luca and Perez, Paula and Újváry, Dóra and Csizmadia, Gábor and Miklósi, Ádám},
	year = {2020},
	pmid = {32687528},
	note = {ISBN: 1111111111},
	pages = {1--14},
}

@article{wu_testing_2020,
	title = {Testing artificial intelligence system towards safety and robustness: {State} of the art},
	volume = {47},
	issn = {18199224},
	abstract = {With the increasing development of machine learning, conventional embedded systems cannot meet the requirement of current academic researches and industrial applications. Artificial Intelligence System (AIS) based on machine learning has been widely used in various safety-critical systems, such as machine vision, autonomous vehicles, collision avoidance system. Different from conventional embedded systems, AIS generates and updates control strategies through learning algorithms which make the control behaviors nondeterministic and bring about the test oracle problem in AIS testing procedure. There have been various testing approaches for AIS to guarantee the safety and robustness. However, few researches explain how to conduct AIS testing with a complete workflow systematically. This paper provides a comprehensive survey of existing testing techniques to detect the erroneous behaviors of AIS, and sums up the involved key steps and testing components in terms of test coverage criterion, test data generation, testing approach and common dataset. This literature review aims at organizing a standardized workflow and leading to a practicable insight and research trend towards AIS testing.},
	number = {3},
	journal = {IAENG International Journal of Computer Science},
	author = {Wu, Tingting and Dong, Yunwei and Dong, Zhiwei and Singa, Aziz and Chen, Xiong and Zhang, Yu},
	year = {2020},
	keywords = {Artificial intelligence system, Machine learning, Neural network, Testing, Verification},
	pages = {449--462},
}

@article{jha_data_2019,
	title = {Data {Infrastructure} for {Machine} {Learning}},
	volume = {7},
	doi = {10.22214/ijraset.2019.4133},
	abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
	number = {4},
	journal = {International Journal for Research in Applied Science and Engineering Technology},
	author = {Jha, Samridhi},
	year = {2019},
	pages = {740--742},
}

@book{gamma_design_1995,
	title = {Design patterns : elements of reusable object-oriented software},
	isbn = {0-201-63361-2},
	publisher = {Addison-Wesley},
	author = {Gamma, Erich},
	editor = {Gamma, Erich},
	year = {1995},
	keywords = {Computer software – Reusability, Object-oriented programming (Computer science), Software patterns},
}

@inproceedings{hazelwood_applied_2018,
	title = {Applied {Machine} {Learning} at {Facebook}: {A} {Datacenter} {Infrastructure} {Perspective}},
	isbn = {978-1-5386-3659-6},
	url = {https://research.fb.com/publications/applied-machine-learning-at-facebook-a-datacenter-infrastructure-perspective/},
	doi = {10.1109/HPCA.2018.00059},
	booktitle = {{HPCA}},
	author = {Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
	year = {2018},
	pmid = {23621169},
	note = {ISSN: 15300897},
}

@article{leesatapornwongsa_taxdc_2016,
	title = {{TaxDC}: {A} {Taxonomy} of {Non}-{Deterministic} {Concurrency} {Bugs} in {Datacenter} {Distributed} {Systems}},
	volume = {44},
	issn = {0163-5964},
	doi = {10.1145/2980024.2872374},
	abstract = {We present TaxDC, the largest and most comprehensive taxonomy of non-deterministic concurrency bugs in dis-tributed systems. We study 104 distributed concurrency (DC) bugs from four widely-deployed cloud-scale datacenter dis-tributed systems, Cassandra, Hadoop MapReduce, HBase and ZooKeeper. We study DC-bug characteristics along sev-eral axes of analysis such as the triggering timing condition and input preconditions, error and failure symptoms, and fix strategies, collectively stored as 2,083 classification labels in TaxDC database. We discuss how our study can open up many new research directions in combating DC bugs.},
	number = {2},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Leesatapornwongsa, Tanakorn and Lukman, Jeffrey F. and Lu, Shan and Gunawi, Haryadi S.},
	year = {2016},
	note = {ISBN: 9781450340915},
	keywords = {concurrency bugs, distributed systems, soft-},
	pages = {517--530},
}

@article{febrero_systematic_2014,
	title = {A systematic mapping study of software reliability modeling},
	volume = {56},
	issn = {09505849},
	url = {http://dx.doi.org/10.1016/j.infsof.2014.03.006},
	doi = {10.1016/j.infsof.2014.03.006},
	abstract = {Context Software Reliability (SR) is a highly active and dynamic research area. Published papers have approached this topic from various and heterogeneous points of view, resulting in a rich body of literature on this topic. The counterpart to this is the considerable complexity of this body of knowledge. Objective The objective of this study is to obtain a panorama and a taxonomy of Software Reliability Modeling (SRM). Method In order to do this, a Systematic Mapping Study (SMS) which analyzes and structures the literature on Software Reliability Modeling has been carried out. Results A total of 972 works were obtained as a result of the Systematic Mapping Study. On the basis of the more than 500 selected primary studies found, the results obtained show an increasing diversity of work. Conclusion Although it was discovered that Software Reliability Growth Models (SRGM) are still the most common modeling technique, it was also found that both the modeling based on static and architectural characteristics and the models based on Artificial Intelligence and automatic learning techniques are increasingly more apparent in literature. We have also observed that most Software Reliability Modeling efforts take place in the Pacific Rim area and in academic environments. Industrial initiatives are as yet marginal, and would appear to be primarily located in the USA. © 2014 Elsevier B.V. All rights reserved.},
	number = {8},
	journal = {Information and Software Technology},
	author = {Febrero, Felipe and Calero, Coral and Moraga, Ma Ángeles},
	year = {2014},
	note = {Publisher: Elsevier B.V.},
	keywords = {Modeling, Software Reliability, Systematic Mapping, Taxonomy},
	pages = {839--849},
}

@article{biesialska_big_2021,
	title = {Big {Data} analytics in {Agile} software development: {A} systematic mapping study},
	volume = {132},
	issn = {09505849},
	url = {https://doi.org/10.1016/j.infsof.2020.106448},
	doi = {10.1016/j.infsof.2020.106448},
	abstract = {Context: Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity. Objective: Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA). Method: As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019. Results: In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics. Conclusions: As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.},
	number = {October 2020},
	journal = {Information and Software Technology},
	author = {Biesialska, Katarzyna and Franch, Xavier and Muntés-Mulero, Victor},
	year = {2021},
	note = {Publisher: Elsevier B.V.},
	keywords = {Agile software development, Artificial intelligence, Data analytics, Literature review, Machine learning, Software analytics},
	pages = {106448},
}

@article{yang_how_2018,
	title = {How not to structure your database-backed web applications: {A} study of performance bugs in the wild},
	issn = {02705257},
	doi = {10.1145/3180155.3180194},
	abstract = {Many web applications use databases for persistent data storage, and using Object Relational Mapping (ORM) frameworks is a common way to develop such database-backed web applications. Unfortunately, developing efficient ORM applications is challenging, as the ORM framework hides the underlying database query generation and execution. This problem is becoming more severe as these applications need to process an increasingly large amount of persistent data. Recent research has targeted specific aspects of performance problems in ORM applications. However, there has not been any systematic study to identify common performance anti-patterns in real-world such applications, how they affect resulting application performance, and remedies for them. In this paper, we try to answer these questions through a comprehensive study of 12 representative real-world ORM applications. We generalize 9 ORM performance anti-patterns from more than 200 performance issues that we obtain by studying their bug-tracking systems and profiling their latest versions. To prove our point, we manually fix 64 performance issues in their latest versions and obtain a median speedup of 2× (and up to 39× max) with fewer than 5 lines of code change in most cases. Many of the issues we found have been confirmed by developers, and we have implemented ways to identify other code fragments with similar issues as well.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Yang, Junwen and Subramaniam, Pranav and Lu, Shan and Yan, Cong and Cheung, Alvin},
	year = {2018},
	note = {ISBN: 9781450356381},
	keywords = {Bug study, Database-backed applications, Object-relational mapping frameworks, Performance anti-patterns},
	pages = {800--810},
}

@article{thiruvathukal_concretizing_nodate,
	title = {Concretizing {Best} {Practices} using {Model} {Reconstruction} in {TensorFlow}},
	author = {Thiruvathukal, George K. and Lu, Yung-hsiang and Davis, James C.},
}

@article{guo_7_2017,
	title = {The 7 {Steps} of {Machine} {Learning} – {Towards} {Data} {Science}},
	abstract = {From detecting skin cancer, to sorting cucumbers, to detecting escalators in need of repairs, machine learning has granted computer systems entirely new abilities. But how does it really work under the hood? Let’s walk through a basic example, and use it as an excuse talk about the process of getting answers from your data using machine learning.},
	journal = {Towardsdatascience},
	author = {Guo, Yufeng},
	year = {2017},
	keywords = {BT-Machine Learning workflow},
	pages = {1--15},
}

@article{agrawal_cloudy_2019,
	title = {Cloudy with high chance of {DBMS}: {A} 10-year prediction for {Enterprise}-{Grade} {ML}},
	issn = {23318422},
	abstract = {Machine learning (ML) has proven itself in high-value web applications such as search ranking and is emerging as a powerful tool in a much broader range of enterprise scenarios including voice recognition and conversational understanding for customer support, autotuning for videoconferencing, inteligent feedback loops in large-scale sysops, manufacturing and autonomous vehicle management, complex financial predictions, just to name a few. Meanwhile, as the value of data is increasingly recognized and monetized, concerns about securing valuable data and risks to individual privacy have been growing. Consequently, rigorous data management has emerged as a key requirement in enterprise settings. How will these trends (ML growing popularity, and stricter data governance) intersect? What are the unmet requirements for applying ML in enterprise settings? What are the technical challenges for the DB community to solve? In this paper, we present our vision of how ML and database systems are likely to come together, and early steps we take towards making this vision a reality.},
	journal = {arXiv},
	author = {Agrawal, Ashvin and Chatterjee, Rony and Curino, Carlo and Floratou, Avrilia and Gowdal, Neha and Interlandi, Matteo and Jindal, Alekh and Karanasos, Kostantinos and Krishnan, Subru and Kroth, Brian and Leeka, Jyoti and Park, Kwanghyun and Patel, Hiren and Poppe, Olga and Psallidas, Fotis and Ramakrishnan, Raghu and Roy, Abhishek and Saur, Karla and Sen, Rathijit and Weimer, Markus and Wright, Travis and Zhu, Yiwen},
	year = {2019},
}

@book{kruchten_agile_2018,
	title = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}: 19th {International} {Conference}, {XP} 2018, {Porto}, {Portugal}, {May} 21–25, 2018, {Proceedings}},
	volume = {314},
	isbn = {978-3-319-91601-9},
	url = {http://link.springer.com/10.1007/978-3-319-91602-6},
	abstract = {This open access book constitutes the proceedings of the 19th International Conference on Agile Software Development, XP 2018, held in Porto, Portugal, in May 2018. XP is the premier agile software development conference combining research and practice, and XP 2018 provided a playful and informal environment to learn and trigger discussions around its main theme – make, inspect, adapt. The 21 papers presented in this volume were carefully reviewed and selected from 62 submissions. They were organized in topical sections named: agile requirements; agile testing; agile transformation; scaling agile; human-centric agile; and continuous experimentation.},
	author = {Kruchten, Philippe and Eds, François Coallier},
	year = {2018},
	doi = {10.1007/978-3-030-19034-7},
}

@article{bibliographies_102720_2019,
	title = {10/27/20 {Agenda}},
	author = {Bibliographies, Annotated and Engineering, Software and Learning, Machine and Study, Case},
	year = {2019},
}

@article{allamanis_survey_2017,
	title = {A survey of machine learning for big code and naturalness},
	volume = {51},
	issn = {23318422},
	abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit code's abundance of patterns. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss crosscutting and application-specific challenges and opportunities.},
	number = {4},
	journal = {arXiv},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
	year = {2017},
	keywords = {Big Code, Code Naturalness, Machine Learning, Software Engineering Tools},
}

@article{xie_survey_2019,
	title = {A survey of machine learning techniques applied to software defined networking ({SDN}): {Research} issues and challenges},
	volume = {21},
	issn = {1553877X},
	doi = {10.1109/COMST.2018.2866942},
	abstract = {In recent years, with the rapid development of current Internet and mobile communication technologies, the infrastructure, devices and resources in networking systems are becoming more complex and heterogeneous. In order to efficiently organize, manage, maintain and optimize networking systems, more intelligence needs to be deployed. However, due to the inherently distributed feature of traditional networks, machine learning techniques are hard to be applied and deployed to control and operate networks. Software defined networking (SDN) brings us new chances to provide intelligence inside the networks. The capabilities of SDN (e.g., logically centralized control, global view of the network, software-based traffic analysis, and dynamic updating of forwarding rules) make it easier to apply machine learning techniques. In this paper, we provide a comprehensive survey on the literature involving machine learning algorithms applied to SDN. First, the related works and background knowledge are introduced. Then, we present an overview of machine learning algorithms. In addition, we review how machine learning algorithms are applied in the realm of SDN, from the perspective of traffic classification, routing optimization, quality of service/quality of experience prediction, resource management and security. Finally, challenges and broader perspectives are discussed.},
	number = {1},
	journal = {IEEE Communications Surveys and Tutorials},
	author = {Xie, Junfeng and Yu, F. Richard and Huang, Tao and Xie, Renchao and Liu, Jiang and Wang, Chenmeng and Liu, Yunjie},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Machine learning, Resource management, Software defined networking, Traffic classification},
	pages = {393--430},
}

@article{roh_survey_2018,
	title = {A survey on data collection for machine learning: {A} big data - {AI} integration perspective},
	volume = {4347},
	issn = {23318422},
	doi = {10.1109/tkde.2019.2946162},
	abstract = {Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.},
	number = {c},
	journal = {arXiv},
	author = {Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
	year = {2018},
	note = {Publisher: IEEE},
	keywords = {Data acquisition, Data collection, Data labeling, Machine learning},
	pages = {1--20},
}

@article{this_rules_nodate,
	title = {Rules of {Machine} {Learning} : {Best} {Practices} for {ML} {Engineering}},
	abstract = {This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine?learned model, then you have the necessary background to read this document. Terminology},
	author = {This, Martin Zinkevich and Guide, Style},
}

@article{sugimura_building_2018,
	title = {Building a reproducible machine learning pipeline},
	issn = {23318422},
	abstract = {Reproducibility of modeling is a problem that exists for any machine learning practitioner, whether in industry or academia. The consequences of an irreproducible model can include significant financial costs, lost time, and even loss of personal reputation (if results prove unable to be replicated). This paper will first discuss the problems we have encountered while building a variety of machine learning models, and subsequently describe the framework we built to tackle the problem of model reproducibility. The framework is comprised of four main components (data, feature, scoring, and evaluation layers), which are themselves comprised of well defined transformations. This enables us to not only exactly replicate a model, but also to reuse the transformations across different models. As a result, the platform has dramatically increased the speed of both offline and online experimentation while also ensuring model reproducibility.},
	journal = {arXiv},
	author = {Sugimura, Peter and Hartl, Florian},
	year = {2018},
}

@article{parker_software_nodate,
	title = {Software 2.0},
	author = {Parker, David},
}

@article{arpteg_software_2018,
	title = {Software engineering challenges of deep learning},
	doi = {10.1109/SEAA.2018.00018},
	abstract = {Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.},
	journal = {Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018},
	author = {Arpteg, Anders and Brinne, Björn and Crnkovic-Friis, Luka and Bosch, Jan},
	year = {2018},
	note = {ISBN: 9781538673829
Publisher: IEEE},
	keywords = {Artificial intelligence, Deep learning, Machine learning, Software engineering challenges},
	pages = {50--59},
}

@article{shafiq_machine_2020,
	title = {Machine learning for software engineering: {A} systematic mapping},
	issn = {23318422},
	abstract = {Context: The software development industry is rapidly adopting machine learning for transitioning modern day software systems towards highly intelligent and self-learning systems. However, the full potential of machine learning for improving the software engineering life cycle itself is yet to be discovered, i.e., up to what extent machine learning can help reducing the effort/complexity of software engineering and improving the quality of resulting software systems. To date, no comprehensive study exists that explores the current state-of-the-art on the adoption of machine learning across software engineering life cycle stages. Objective: This article addresses the aforementioned problem and aims to present a state-of-the-art on the growing number of uses of machine learning in software engineering. Method: We conduct a systematic mapping study on applications of machine learning to software engineering following the standard guidelines and principles of empirical software engineering. Results: This study introduces a machine learning for software engineering (MLSE) taxonomy classifying the state-of-the-art machine learning techniques according to their applicability to various software engineering life cycle stages. Overall, 227 articles were rigorously selected and analyzed as a result of this study. Conclusion: From the selected articles, we explore a variety of aspects that should be helpful to academics and practitioners alike in understanding the potential of adopting machine learning techniques during software engineering projects.},
	journal = {arXiv},
	author = {Shafiq, Saad and Mashkoor, Atif and Mayr-Dorn, Christoph and Egyed, Alexander},
	year = {2020},
	keywords = {Machine learning, Software engineering, Systematic mapping},
}

@article{zhang_machine_2019,
	title = {Machine learning testing: {Survey}, landscapes and horizons},
	volume = {X},
	issn = {23318422},
	doi = {10.1109/tse.2019.2962027},
	abstract = {This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research. It covers 128 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.},
	number = {X},
	journal = {arXiv},
	author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},
	year = {2019},
	keywords = {Deep neural network, Machine learning, Software testing},
}

@article{lee_human---loop_2019,
	title = {A {Human}-in-the-loop {Perspective} on {AutoML}: {Milestones} and the {Road} {Ahead}},
	journal = {Data Engineering},
	author = {Lee, Doris Jung-Lin and Macke, Stephen and Xin, Doris and Lee, Angela and Huang, Silu and Parameswaran, Aditya},
	year = {2019},
	pages = {58},
}

@article{olukotun_designing_2018,
	title = {Designing {Computer} {Systems} for {Software} 2.0 ({Tutorial})},
	journal = {Nips 2018},
	author = {Olukotun, Kunle},
	year = {2018},
}

@article{zhang_ml_2003,
	title = {{ML} and {SE}},
	volume = {7491},
	number = {02},
	author = {ZHANG, DU and J.P.TSAI, JEFFREY},
	year = {2003},
	keywords = {minke whales, north atlantic, organochlorine pesticides, pcbs, stock delineation},
	pages = {87--119},
}

@article{carbin_overparameterization_2019,
	title = {Overparameterization: {A} connection between software 1.0 and software 2.0},
	volume = {136},
	issn = {18688969},
	doi = {10.4230/LIPIcs.SNAPL.2019.1},
	abstract = {A new ecosystem of machine-learning driven applications, titled Software 2.0, has arisen that integrates neural networks into a variety of computational tasks. Such applications include image recognition, natural language processing, and other traditional machine learning tasks. However, these techniques have also grown to include other structured domains, such as program analysis and program optimization for which novel, domain-specific insights mate with model design. In this paper, we connect the world of Software 2.0 with that of traditional software – Software 1.0 –through overparameterization: a program may provide more computational capacity and precision than is necessary for the task at hand. In Software 2.0, overparamterization – when a machine learning model has more parameters than datapoints in the dataset – arises as a contemporary understanding of the ability for modern, gradient-based learning methods to learn models over complex datasets with high-accuracy. Specifically, the more parameters a model has, the better it learns. In Software 1.0, the results of the approximate computing community show that traditional software is also overparameterized in that software often simply computes results that are more precise than is required by the user. Approximate computing exploits this overparameterization to improve performance by eliminating unnecessary, excess computation. For example, one – of many techniques – is to reduce the precision of arithmetic in the application. In this paper, we argue that the gap between available precision and that that is required for either Software 1.0 or Software 2.0 is a fundamental aspect of software design that illustrates the balance between software designed for general-purposes and domain-adapted solutions. A general-purpose solution is easier to develop and maintain versus a domain-adapted solution. However, that ease comes at the expense of performance. We show that the approximate computing community and the machine learning community have developed overlapping techniques to improve performance by reducing overparameterization. We also show that because of these shared techniques, questions, concerns, and answers on how to construct software can translate from one software variant to the other.},
	number = {1},
	journal = {Leibniz International Proceedings in Informatics, LIPIcs},
	author = {Carbin, Michael},
	year = {2019},
	note = {ISBN: 9783959771139},
	keywords = {Approximate computing, Machine learning, Software 2.0},
	pages = {1--13},
}

@article{nakamichi_requirements-driven_2020,
	title = {Requirements-driven method to determine quality characteristics and measurements for machine learning software and its evaluation},
	volume = {2020-Augus},
	issn = {23326441},
	doi = {10.1109/RE48521.2020.00036},
	abstract = {As the applications of machine learning algorithms in various fields are widely demanded, the development of machine learning software systems (MLS) is rapidly increasing. The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise. In this paper, we propose a requirements-driven method to determine the quality characteristics of the MLS. Major contributions of this paper include: (1) Extending the quality characteristics of ISO 25010, which defines the conventional software quality, to those unique to MLS; this paper also defines its measuring method. (2) A method to identify requirements, i.e., issues to be determined in the requirements definition, in order to derive the quality characteristics and measurement methods for MLS, since the quality characteristics and the measurement method depend on the goals of the system under development. In order to evaluate the proposed method, we carried out an empirical study of the quality characteristics and measurement methods related to functional correctness and the maturity of the MLS for the enterprise. Based on the study, we compare the quality characteristics and measurement methods derived by the proposed method with those suggested by developers, and demonstrate the effectiveness of the proposed method.},
	journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
	author = {Nakamichi, Koji and Ohashi, Kyoko and Namba, Isao and Yamamoto, Rieko and Aoyama, Mikio and Joeckel, Lisa and Siebert, Julien and Heidrich, Jens},
	year = {2020},
	note = {ISBN: 9781728174389},
	keywords = {machine learning, quality assurance, quality characteristics, quality measures, quality requirements, software quality model},
	pages = {260--270},
}

@article{rupprecht_ursprung_2019,
	title = {Ursprung: {Provenance} for large-scale analytics environments},
	issn = {07308078},
	doi = {10.1145/3299869.3320235},
	abstract = {Modern analytics has produced wonders, but reproducing and verifying these wonders is difficult. Data provenance helps to solve this problem by collecting information on how data is created and accessed. Although provenance collection techniques have been used successfully on a smaller scale, tracking provenance in large-scale analytics environments is challenging due to the scale of provenance generated and the heterogeneous domains. Without provenance, analysts struggle to keep track of and reproduce their analyses. We demonstrate Ursprung1, a provenance collection system specifically targeted at such environments. Ursprung transparently collects the minimal set of system-level provenance required to track the relationships between data and processes. To collect domain specific provenance, Ursprung enables users to specify capture rules to curate application-specific logs, intermediate results etc. To reduce storage overhead and accelerate queries, it uses event hierarchies to synthesize raw provenance into compact summaries.},
	journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
	author = {Rupprecht, Lukas and Lubbock, Alexander and Davis, James C. and Tyson, Darren and Arnold, Constantine and Bhagwat, Deepavali},
	year = {2019},
	note = {ISBN: 9781450356435},
	pages = {1989--1992},
}

@article{qin_prediction-guided_2020,
	title = {Prediction-{Guided} {Design} for {Software} {Systems}},
	abstract = {While software system development is commonly conducted with explicit rules, machine learning (ML) has been driving a revolution in modern system design. In this paper, we in- troduce a new prediction-guided paradigm, which leverages ML techniques to support decision-makings for the system itself. In the proposed design, the system would be automat- ically driven by various type of data, e.g., system workloads, user behaviors, and platform operations, etc. More impor- tantly, it brings a mindset of “proactive” to developers. Some significant issues can be thus eliminated before becoming catastrophe. In order to illustrate the benefits of the proposed paradigm, we present a project showcase, intelligent buffer management, which is used to achieve an optimal trade-off between having sufficiently large buffers to avoid failures and minimizing excess capacity in Microsoft Azure. It is designed in the prediction-guided paradigm to dynamically and proac- tively adjust the reserved buffer based on customer workload patterns and platform operations. The project not only sig- nificantly improves CFR (capacity fulfillment reliability) of tenant growth, but also reduces millions of dollars in COGS (cost of goods sold) for Microsoft. Prediction-Guided},
	number = {Ml},
	author = {Qin, Si and Xu, Yong and Zhou, Shandan and Lin, Qingwei and Zhang, Hongyu and Agarwal, Saurabh and Subramanian, Karthikeyan and Cortez, Eli and Miller, John and Cowdery, Chris and Kemburu, Shanti and Zhang, Dongmei and Moscibroda, Thomas},
	year = {2020},
	pages = {1--2},
}

@article{bajic_compute_nodate,
	title = {Compute substrate for {Software} 2 . 0 {Moore} ’ s {Law}},
	author = {Bajić, Ljubisa and Vasiljević, Jasmina},
}

@article{weidele_autoaiviz_2020,
	title = {{AutoAIViz}: {Opening} the blackbox of automated artificial intelligence with conditional parallel coordinates},
	doi = {10.1145/3377325.3377538},
	abstract = {Artificial Intelligence (AI) can now automate the algorithm selection, feature engineering, and hyperparameter tuning steps in a machine learning workflow. Commonly known as AutoML or AutoAI, these technologies aim to relieve data scientists from the tedious manual work. However, today's AutoAI systems often present only limited to no information about the process of how they select and generate model results. Thus, users often do not understand the process, neither do they trust the outputs. In this short paper, we provide a first user evaluation by 10 data scientists of an experimental system, AutoAIViz, that aims to visualize AutoAI's model generation process. We find that the proposed system helps users to complete the data science tasks, and increases their understanding, toward the goal of increasing trust in the AutoAI system.},
	journal = {International Conference on Intelligent User Interfaces, Proceedings IUI},
	author = {Weidele, Daniel Karl I. and Weisz, Justin D. and Oduor, Erick and Muller, Michael and Andres, Josh and Gray, Alexander and Wang, Dakuo},
	year = {2020},
	note = {ISBN: 9781450371186},
	keywords = {AutoAI, AutoML, democratizing AI, human-AI collaboration, parallel coordinates, visualization},
	pages = {308--312},
}

@article{tanaka_data_2015,
	title = {Data {Driven} {Debugging}},
	author = {Tanaka, Daigo},
	year = {2015},
	pages = {1--5},
}

@article{polyzotis_data_2017,
	title = {Data management challenges in production machine learning},
	volume = {Part F1277},
	issn = {07308078},
	doi = {10.1145/3035918.3054782},
	abstract = {The tutorial discusses data-management issues that arise in the context of machine learning pipelines deployed in production. Informed by our own experience with such largescale pipelines, we focus on issues related to understanding, validating, cleaning, and enriching training data. The goal of the tutorial is to bring forth these issues, draw connections to prior work in the database literature, and outline the open research questions that are not addressed by prior art.},
	journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
	author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
	year = {2017},
	note = {ISBN: 9781450341974},
	pages = {1723--1726},
}

@article{alberti_deepdiva_2018,
	title = {{DeepDIVA}: {A} highly-functional python framework for reproducible experiments},
	volume = {2018-Augus},
	issn = {21676453},
	doi = {10.1109/ICFHR-2018.2018.00080},
	abstract = {We introduce DeepDIVA: An infrastructure designed to enable quick and intuitive setup of reproducible experiments with a large range of useful analysis functionality. Reproducing scientific results can be a frustrating experience, not only in document image analysis but in machine learning in general. Using DeepDIVA a researcher can either reproduce a given experiment or share their own experiments with others. Moreover, the framework offers a large range of functions, such as boilerplate code, keeping track of experiments, hyper-parameter optimization, and visualization of data and results. To demonstrate the effectiveness of this framework, this paper presents case studies in the area of handwritten document analysis where researchers benefit from the integrated functionality. DeepDIVA is implemented in Python and uses the deep learning framework PyTorch. It is completely open source, and accessible as Web Service through DIVAServices.},
	journal = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
	author = {Alberti, Michele and Pondenkandath, Vinaychandran and Wursch, Marcel and Ingold, Rolf and Liwicki, Marcus},
	year = {2018},
	note = {ISBN: 9781538658758},
	keywords = {Deep Learning, Framework, Hyper paramters Optimization, Machine Learning, Neural Networks, Open-source, Python, Reproducible Research},
	pages = {423--428},
}

@article{rezig_dagger_2020,
	title = {Dagger: {A} {Data} (not code) {Debugger}},
	url = {http://cidrdb.org/cidr2020/papers/p35-rezig-cidr20.pdf},
	abstract = {With the democratization of data science libraries and frameworks, most data scientists manage and generate their data analytics pipelines using a collection of scripts (e.g., Python, R). This marks a shift from traditional applications that communicate back and forth with a DBMS that stores and manages the application data. While code debuggers have reached impressive maturity over the past decades, they fall short in assisting users to explore data-driven what-if scenarios (e.g., split the training set into two and build two ML models). Those scenarios, while doable programmatically, are a substantial burden for users to manage themselves. Dagger (Data Debugger) is an end-to-end data debugger that abstracts key data-centric primitives to enable users to quickly identify and mitigate data-related problems in a given pipeline. Dagger was motivated by a series of interviews we conducted with data scientists across several organizations. A preliminary version of Dagger has been incorporated into Data Civilizer 2.0 to help physicians at the Massachusetts General Hospital process complex pipelines.},
	number = {1},
	journal = {\{CIDR\} 2020, 10th Conference on Innovative Data Systems Research, Amsterdam, The Netherlands, January 12-15, 2020, Online Proceedings},
	author = {Rezig, El Kindi and Cao, Lei and Simonini, Giovanni and Schoemans, Maxime and Madden, Samuel and Tang, Nan and Ouzzani, Mourad and Stonebraker, Michael},
	year = {2020},
}

@article{hall_editorial_2010,
	title = {Editorial: {Data} mining in software engineering},
	volume = {17},
	issn = {15737535},
	doi = {10.1007/s10515-010-0073-9},
	number = {4},
	journal = {Automated Software Engineering},
	author = {Hall, Robert J.},
	year = {2010},
	note = {ISBN: 1051501000739},
	pages = {373--374},
}

@article{wan_how_2019,
	title = {How does {Machine} {Learning} {Change} {Software} {Development} {Practices}?},
	volume = {PP},
	issn = {0098-5589},
	doi = {10.1109/tse.2019.2937083},
	abstract = {Adding an ability for a system to learn inherently adds non-determinism into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work features (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners.},
	number = {c},
	journal = {IEEE Transactions on Software Engineering},
	author = {Wan, Zhiyuan and Xia, Xin and Lo, David and Murphy, Gail C.},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {1--1},
}

@article{vivanti_search-based_nodate,
	title = {Search-based {Data}-flow {Test} {Generation}},
	abstract = {Coverage criteria based on data-flow have long been discussed in the literature, yet to date they are still of surprising little practical relevance. This is in part because 1) manually writing a unit test for a data-flow aspect is more challenging than writing a unit test that simply covers a branch or statement, 2) there is a lack of tools to support data-flow testing, and 3) there is a lack of empirical evidence on how well data-flow testing scales in practice. To overcome these problems, we present 1) a search- based technique to automatically generate unit tests for data-flow criteria, 2) an implementation of this technique in the EVOSUITE test generation tool, and 3) a large empirical study applying this tool to the SF100 corpus of 100 open source Java projects. On average, the number of coverage objectives is three times as high as for branch coverage. However, the level of coverage achieved by EVOSUITE is comparable to other criteria, and the increase in size is only 15\%, leading to higher mutation scores. These results counter the common assumption that data-flow testing does not scale, and should help to re-establish data-flow testing as a viable alternative in practice.},
	author = {Vivanti, Mattia and Mis, Andre and Gorla, Alessandra and Fraser, Gordon},
	keywords = {-data-flow coverage, search based testing, unit testing},
}

@article{re_software_2018,
	title = {Software 2 . 0 and {Snorkel} : {Beyond} {Hand}-{Labeled} {Data}},
	volume = {11},
	number = {3},
	author = {Ré, Christopher},
	year = {2018},
	note = {ISBN: 9781450355520},
	pages = {3575},
}

@article{lwakatare_large-scale_2020,
	title = {Large-scale machine learning systems in real-world industrial settings: {A} review of challenges and solutions},
	volume = {127},
	issn = {09505849},
	doi = {10.1016/j.infsof.2020.106368},
	abstract = {Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.},
	number = {July},
	journal = {Information and Software Technology},
	author = {Lwakatare, Lucy Ellen and Raj, Aiswarya and Crnkovic, Ivica and Bosch, Jan and Olsson, Helena Holmström},
	year = {2020},
	keywords = {Challenges, Industrial settings, Machine learning systems, SLR, Software engineering, Solutions},
}

@article{oreilly_what_2012,
	title = {What is web 2.0?: {Design} patterns and business models for the next generation of software},
	abstract = {This paper was the first initiative to try to define Web2.0 and understand its implications for the next generation of software, looking at both design patterns and business modes. Web 2.0 is the network as platform, spanning all connected devices; Web 2.0 applications are those that make the most of the intrinsic advantages of that platform: delivering software as a continually-updated service that gets better the more people use it, consuming and remixing data from multiple sources, including individual users, while providing their own data and services in a form that allows remixing by others, creating network effects through an "architecture of participation," and going beyond the page metaphor of Web 1.0 to deliver rich user experiences.},
	number = {65},
	journal = {The Social Media Reader},
	author = {O'Reilly, Tim},
	year = {2012},
	note = {ISBN: 0814764053},
	keywords = {collective intelligence, data, long tail and, rich client, software as a service},
	pages = {32--52},
}

@article{vivek_exemplars_nodate,
	title = {Exemplars for {Machine} {Learning} : {Towards} {Software} {Engineering} and {Reproducibility}},
	author = {Vivek, Naveen and Chinnakotla, Akhil and Banna, Vishnu and Vegesana, Anirudh and Yan, Tristan and Davis, James and Lu, Yung-hsiang and Thiruvathukal, George K.},
	keywords = {best practices, computer vision, ducibility, engineers and scientists across, machine, machine learning, many industries are adopting, repro-, software engineering, tensorflow},
	pages = {2},
}

@article{microsoft_database_nodate,
	title = {Database {Systems} 2 . 0},
	author = {Microsoft, Johannes Gehrke},
	pages = {2399},
}

@article{gulzar_bigdebug_2016,
	title = {{BigDebug}: {Debugging} primitives for interactive big data processing in},
	volume = {14-22-May-},
	issn = {02705257},
	doi = {10.1145/2884781.2884813},
	abstract = {Developers use cloud computing platforms to process a large quantity of data in parallel when developing big data analytics. Debugging the massive parallel computations that run in today's datacenters is time consuming and error-prone. To address this challenge, we design a set of interactive, real-time debugging primitives for big data processing in Apache Spark, the next generation data-intensive scalable cloud computing platform. This requires rethinking the notion of step-through debugging in a traditional debugger such as gdb, because pausing the entire computation across distributed worker nodes causes significant delay and naively inspecting millions of records using a watchpoint is too time consuming for an end user. First, BIGDEBUG's simulated breakpoints and on-demand watchpoints allow users to selectively examine distributed, intermediate data on the cloud with little overhead. Second, a user can also pinpoint a crash-inducing record and selectively resume relevant sub-computations after a quick fix. Third, a user can determine the root causes of errors (or delays) at the level of individual records through a fine-grained data provenance capability. Our evaluation shows that BIGDEBUG scales to terabytes and its record-level tracing incurs less than 25\% overhead on average. It determines crash culprits orders of magnitude more accurately and provides up to 100\% time saving compared to the baseline replay debugger. The results show that BIGDEBUG supports debugging at interactive speeds with minimal performance impact.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Gulzar, Muhammad Ali and Interlandi, Matteo and Yoo, Seunghyun and Tetali, Sai Deep and Condie, Tyson and Millstein, Todd and Kim, Miryung},
	year = {2016},
	note = {ISBN: 9781450339001},
	keywords = {Big data analytics, Data-intensive scalable computing (DISC), Debugging, Fault localization and recovery, Interactive tools},
	pages = {784--795},
}

@article{camino_improving_2019,
	title = {Improving missing data imputation with deep generative models},
	issn = {23318422},
	abstract = {Datasets with missing values are very common on industry applications, and they can have a negative impact on machine learning models. Recent studies introduced solutions to the problem of imputing missing values based on deep generative models. Previous experiments with Generative Adversarial Networks and Variational Autoencoders showed interesting results in this domain, but it is not clear which method is preferable for different use cases. The goal of this work is twofold: we present a comparison between missing data imputation solutions based on deep generative models, and we propose improvements over those methodologies. We run our experiments using known real life datasets with different characteristics, removing values at random and reconstructing them with several imputation techniques. Our results show that the presence or absence of categorical variables can alter the selection of the best model, and that some models are more stable than others after similar runs with different random number generator seeds.},
	journal = {arXiv},
	author = {Camino, Ramiro D. and Hammerschmidt, Christian A. and State, Radu},
	year = {2019},
}

@article{menzies_five_2020,
	title = {The {Five} {Laws} of {SE} for {AI}},
	volume = {37},
	issn = {19374194},
	doi = {10.1109/MS.2019.2954841},
	abstract = {It is time to talk about software engineering (SE) for artificial intelligence (AI). As shown in Figure 1, industry is becoming increasingly dependent on AI software. Clearly, AI is useful for SE. But what about the other way around? How important is SE for AI? Many thought leaders in the AI industry are asking how to better develop and maintain AI software (see Figure 2).},
	number = {1},
	journal = {IEEE Software},
	author = {Menzies, Tim},
	year = {2020},
	pages = {81--85},
}

@book{horak_raslan_2013,
	title = {{RASLAN} 2013 {Recent} {Advances} in {Slavonic} {Natural} {Language} {Processing}},
	isbn = {978-80-263-0520-0},
	author = {Horak, P. Rychly A.},
	year = {2013},
}

@article{souza_workflow_2020,
	title = {Workflow provenance in the lifecycle of scientific machine learning},
	issn = {23318422},
	abstract = {Machine Learning (ML) has already fundamentally changed several businesses. More recently, it has also been profoundly impacting the computational science and engineering domains, like geoscience, climate science, and health science. In these domains, users need to perform comprehensive data analyses combining scientific data and ML models to provide for critical requirements, such as reproducibility, model explainability, and experiment data understanding. However, scientific ML is multidisciplinary, heterogeneous, and affected by the physical constraints of the domain, making such analyses even more challenging. In this work, we leverage workflow provenance techniques to build a holistic view to support the lifecycle of scientific ML. We contribute with (i) characterization of the lifecycle and taxonomy for data analyses; (ii) design principles to build this view, with a W3C PROV compliant data representation and a reference system architecture; and (iii) lessons learned after an evaluation in an Oil \& Gas case using an HPC cluster with 393 nodes and 946 GPUs. The experiments show that the principles enable queries that integrate domain semantics with ML models while keeping low overhead ({\textless}1\%), high scalability, and an order of magnitude of query acceleration under certain workloads against without our representation. MSC Codes 65Y05, 68P15},
	journal = {arXiv},
	author = {Souza, Renan and Azevedo, Leonardo G. and Lourenço, Vítor and Soares, Elton and Thiago, Raphael and Brandão, Rafael and Civitarese, Daniel and Brazil, Emilio Vital and Moreno, Marcio and Valduriez, Patrick and Mattoso, Marta and Cerqueira, Renato and Netto, Marco A. S.},
	year = {2020},
	keywords = {Artificial Intelligence, Data Science, Data lake, Design Principles, E-Science, Explainability, Lineage, Machine Learning Lifecycle, Provenance, Reproducibility, Scientific Machine Learning, Scientific Workflow, Taxonomy},
	pages = {1--21},
}

@article{rezig_data_2018,
	title = {Data civilizer 2.0: {A} holistic framework for data preparation and analytics},
	volume = {12},
	issn = {21508097},
	doi = {10.14778/3352063.3352108},
	abstract = {Data scientists spend over 80\% of their time (1) parameter-tuning machine learning models and (2) iterating between data cleaning and machine learning model execution. While there are existing efforts to support the first requirement, there is currently no integrated workflow system that couples data cleaning and machine learning development. The previous version of Data Civilizer was geared towards data cleaning and discovery using a set of pre-defined tools. In this paper, we introduce Data Civilizer 2.0, an end-to-end workflow system satisfying both requirements. In addition, this system also supports a sophisticated data debugger and a workflow visualization system. In this demo, we will show how we used Data Civilizer 2.0 to help scientists at the Massachusetts General Hospital build their cleaning and machine learning pipeline on their 30TB brain activity dataset.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Rezig, El Kindi and Cao, Lei and Stonebraker, Michael and Simonini, Giovanni and Tao, Wenbo and Madden, Samuel and Ouzzani, Mourad and Tang, Nan and Elmagarmid, Ahmed K.},
	year = {2018},
	pages = {1954--1957},
}

@article{ratner_role_2019,
	title = {The role of massively multi-task and weak supervision in software 2.0},
	abstract = {Over the last several years, machine learning models have reached new levels of empirical performance across a broad range of domains. Driven both by accuracy improvements and deployment advantages, many organizations have begun to shift to learning-centered software stacks—a new mode that has been called Software 2.0. This approach holds the promise of radically accelerating the construction, maintenance, and deployment of software systems, and opens up a broad research agenda around changes to hardware, systems, and interaction models. However, these approaches require one critical and often prohibitively expensive ingredient: labeled training data. We outline a vision for a Software 2.0 lifecycle centered around the idea that labeling training data can be the primary interface to Software 2.0 systems. In our envisioned approach, Software 2.0 stacks are programmed using weak supervision—i.e. noisier, programmatically-generated training data—which is specified at various levels of declarative abstraction and precision, and then combined using unsupervised statistical techniques. The codebase for Software 2.0 is also radically different: we envision labels for tens or hundreds of different tasks across an organization combined in a massively multitask central model, leading to amortization of labeling costs and new models of software reuse and development. Finally, we envision Software 2.0 stacks deployed by using collected training labels to supervise commodity model architectures over different servable feature sets. We outline the components of this lifecycle, and provide an interim report on Snorkel, our prototype Software 2.0 system, based on our experiences working on problems ranging from ad fraud to medical diagnostics with some of the world’s largest organizations.},
	journal = {CIDR 2019 - 9th Biennial Conference on Innovative Data Systems Research},
	author = {Ratner, Alexander and Hancock, Braden and Ré, Christopher},
	year = {2019},
}

@article{yao_taking_2018,
	title = {Taking the human out of learning applications: {A} survey on automated machine learning},
	issn = {23318422},
	abstract = {Machine learning techniques have deeply rooted in our everyday life. However, since it is knowledge- and labor-intensive to pursue good learning performance, humans are heavily involved in every aspect of machine learning. To make machine learning techniques easier to apply and reduce the demand for experienced human experts, automated machine learning (AutoML) has emerged as a hot topic with both industrial and academic interest. In this paper, we provide an up to date survey on AutoML. First, we introduce and define the AutoML problem, with inspiration from both realms of automation and machine learning. Then, we propose a general AutoML framework that not only covers most existing approaches to date, but also can guide the design for new methods. Subsequently, we categorize and review the existing works from two aspects, i.e., the problem setup and the employed techniques. The proposed framework and taxonomies provide a detailed analysis of AutoML approaches and explain the reasons underneath their successful applications. We hope this survey can serve as not only an insightful guideline for AutoML beginners but also an inspiration for future research.},
	journal = {arXiv},
	author = {Yao, Quanming and Wang, Mengshuo and Chen, Yuqiang and Dai, Wenyuan and Hu, Yi Qi and Li, Yu Feng and Tu, Wei Wei and Yang, Qiang and Yu, Yang},
	year = {2018},
	keywords = {Automated machine learning, Hyper-parameter optimization, Meta-learning, Neural architecture search, Transfer-learning},
	pages = {1--20},
}

@article{dam_towards_2019,
	title = {Towards effective {AI}-powered agile project management},
	doi = {10.1109/ICSE-NIER.2019.00019},
	abstract = {The rise of Artificial intelligence (AI) has the potential to significantly transform the practice of project management. Project management has a large socio-technical element with many uncertainties arising from variability in human aspects, e.g. customers' needs, developers' performance and team dynamics. AI can assist project managers and team members by automating repetitive, high-volume tasks to enable project analytics for estimation and risk prediction, providing actionable recommendations, and even making decisions. AI is potentially a game changer for project management in helping to accelerate productivity and increase project success rates. In this paper, we propose a framework where AI technologies can be leveraged to offer support for managing agile projects, which have become increasingly popular in the industry.},
	journal = {Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER 2019},
	author = {Dam, Hoa Khanh and Tran, Truyen and Grundy, John and Ghose, Aditya and Kamei, Yasutaka},
	year = {2019},
	note = {ISBN: 9781728117584},
	keywords = {Artificial Intelligence, Software Engineering},
	pages = {41--44},
}

@article{odegua_how_2020,
	title = {How {To} {Put} {Machine} {Learning} {Models} {Into} {Production}},
	volume = {28},
	number = {2},
	journal = {Stack Overflow},
	author = {Odegua, Rising},
	year = {2020},
	pages = {1--19},
}

@article{sekhon_towards_2019,
	title = {Towards improved testing for deep learning},
	doi = {10.1109/ICSE-NIER.2019.00030},
	abstract = {The growing use of deep neural networks in safety-critical applications makes it necessary to carry out adequate testing to detect and correct any incorrect behavior for corner case inputs before they can be actually used. Deep neural networks lack an explicit control-flow structure, making it impossible to apply to them traditional software testing criteria such as code coverage. In this paper, we examine existing testing methods for deep neural networks, the opportunities for improvement and the need for a fast, scalable, generalizable end-to-end testing method. We also propose a coverage criterion for deep neural networks that tries to capture all possible parts of the deep neural network's logic.},
	journal = {Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER 2019},
	author = {Sekhon, Jasmine and Fleming, Cody},
	year = {2019},
	note = {ISBN: 9781728117584},
	keywords = {Coverage criterion, Deep neural networks, Whitebox testing},
	pages = {85--88},
}

@phdthesis{islam_towards_2020,
	type = {Doctor of {Philosophy}},
	title = {Towards understanding the challenges faced by machine learning software developers and enabling automated solutions},
	url = {https://lib.dr.iastate.edu/etd/18149},
	language = {en},
	urldate = {2021-10-04},
	school = {Iowa State University},
	author = {Islam, Md Johirul},
	year = {2020},
	doi = {10.31274/etd-20200902-68},
}

@inproceedings{wan_bug_2017,
	address = {Buenos Aires, Argentina},
	title = {Bug {Characteristics} in {Blockchain} {Systems}: {A} {Large}-{Scale} {Empirical} {Study}},
	isbn = {978-1-5386-1544-7},
	shorttitle = {Bug {Characteristics} in {Blockchain} {Systems}},
	url = {http://ieeexplore.ieee.org/document/7962390/},
	doi = {10.1109/MSR.2017.59},
	abstract = {Bugs severely hurt blockchain system dependability. A thorough understanding of blockchain bug characteristics is required to design effective tools for preventing, detecting and mitigating bugs. We perform an empirical study on bug characteristics in eight representative open source blockchain systems. First, we manually examine 1,108 bug reports to understand the nature of the reported bugs. Second, we leverage card sorting to label the bug reports, and obtain ten bug categories in blockchain systems. We further investigate the frequency distribution of bug categories across projects and programming languages. Finally, we study the relationship between bug categories and bug ﬁxing time. The ﬁndings include: (1) semantic bugs are the dominant runtime bug category; (2) frequency distributions of bug types show similar trends across different projects and programming languages; (3) security bugs take the longest median time to be ﬁxed; (4) 35.71\% performance bugs are ﬁxed in more than one year; performance bugs take the longest average time to be ﬁxed.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Wan, Zhiyuan and Lo, David and Xia, Xin and Cai, Liang},
	month = may,
	year = {2017},
	pages = {413--424},
}

@inproceedings{vahabzadeh_empirical_2015,
	address = {Bremen, Germany},
	title = {An empirical study of bugs in test code},
	isbn = {978-1-4673-7532-0},
	url = {http://ieeexplore.ieee.org/document/7332456/},
	doi = {10.1109/ICSM.2015.7332456},
	abstract = {Testing aims at detecting (regression) bugs in production code. However, testing code is just as likely to contain bugs as the code it tests. Buggy test cases can silently miss bugs in the production code or loudly ring false alarms when the production code is correct. We present the ﬁrst empirical study of bugs in test code to characterize their prevalence and root cause categories. We mine the bug repositories and version control systems of 211 Apache Software Foundation (ASF) projects and ﬁnd 5,556 test-related bug reports. We (1) compare properties of test bugs with production bugs, such as active time and ﬁxing effort needed, and (2) qualitatively study 443 randomly sampled test bug reports in detail and categorize them based on their impact and root causes. Our results show that (1) around half of all the projects had bugs in their test code; (2) the majority of test bugs are false alarms, i.e., test fails while the production code is correct, while a minority of these bugs result in silent horrors, i.e., test passes while the production code is incorrect; (3) incorrect and missing assertions are the dominant root cause of silent horror bugs; (4) semantic (25\%), ﬂaky (21\%), environmentrelated (18\%) bugs are the dominant root cause categories of false alarms; (5) the majority of false alarm bugs happen in the exercise portion of the tests, and (6) developers contribute more actively to ﬁxing test bugs and test bugs are ﬁxed sooner compared to production bugs. In addition, we evaluate whether existing bug detection tools can detect bugs in test code.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Vahabzadeh, Arash and Fard, Amin Milani and Mesbah, Ali},
	month = sep,
	year = {2015},
	pages = {101--110},
}

@inproceedings{saha_empirical_2014,
	address = {Antwerp, Belgium},
	title = {An empirical study of long lived bugs},
	isbn = {978-1-4799-3752-3},
	url = {http://ieeexplore.ieee.org/document/6747164/},
	doi = {10.1109/CSMR-WCRE.2014.6747164},
	abstract = {Bug ﬁxing is a crucial part of software development and maintenance. A large number of bugs often indicate poor software quality since buggy behavior not only causes failures that may be costly but also has a detrimental effect on the user’s overall experience with the software product. The impact of long lived bugs can be even more critical since experiencing the same bug version after version can be particularly frustrating for user. While there are many studies that investigate factors affecting bug ﬁxing time for entire bug repositories, to the best of our knowledge, none of these studies investigates the extent and reasons of long lived bugs. In this paper, we analyzed long lived bugs from ﬁve different perspectives: their proportion, severity, assignment, reasons, as well as the nature of ﬁxes. Our study on four open-source projects shows that there are a considerable number of long lived bugs in each system and over 90\% of them adversely affect the user’s experience. The reasons of these long lived bugs are diverse including long assignment time, not understanding their importance in advance etc. However, many bug-ﬁxes were delayed without any speciﬁc reasons. Our analysis of bug ﬁxing changes further shows that many long lived bugs can be ﬁxed quickly through careful prioritization. We believe our results will help both developers and researchers to better understand factors behind delays, improve the overall bug ﬁxing process, and investigate analytical approaches for prioritizing bugs based on bug severity as well as expected bug ﬁxing effort. Index Terms—Bug tracking system, bug triaging, bug survival time I. INTRODUCTION Software development and maintenance is a complex process. Although developers and testers try their best to make their software error free, in practice software ships with bugs. The number of bugs in software is a signiﬁcant indicator of software quality since bugs can adversely affect users experience directly. Therefore, developers are generally very active in ﬁnding and removing bugs.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2014 {Software} {Evolution} {Week} - {IEEE} {Conference} on {Software} {Maintenance}, {Reengineering}, and {Reverse} {Engineering} ({CSMR}-{WCRE})},
	publisher = {IEEE},
	author = {Saha, Ripon K. and Khurshid, Sarfraz and Perry, Dewayne E.},
	month = feb,
	year = {2014},
	pages = {144--153},
}

@article{satija_blockene_nodate,
	title = {Blockene: {A} {High}-throughput {Blockchain} {Over} {Mobile} {Devices}},
	abstract = {We introduce Blockene, a blockchain that reduces resource usage at member nodes by orders of magnitude, requiring only a smartphone to participate in block validation and consensus. Despite being lightweight, Blockene provides a high throughput of transactions and scales to a large number of participants. Blockene consumes negligible battery and data in smartphones, enabling millions of users to participate in the blockchain without incentives, to secure transactions with their collective honesty. Blockene achieves these properties with a novel split-trust design based on delegating storage and gossip to untrusted nodes.},
	language = {en},
	author = {Satija, Sambhav and Mehra, Apurv and Singanamalla, Sudheesh and Grover, Karan and Sivathanu, Muthian and Chandran, Nishanth and Gupta, Divya and Lokam, Satya},
	pages = {17},
}

@article{nguyen_improving_2021,
	title = {Improving {Object} {Detection} by {Label} {Assignment} {Distillation}},
	url = {http://arxiv.org/abs/2108.10520},
	abstract = {Label assignment in object detection aims to assign targets, foreground or background, to sampled regions in an image. Unlike labeling for image classification, this problem is not well defined due to the object's bounding box. In this paper, we investigate the problem from a perspective of distillation, hence we call Label Assignment Distillation (LAD). Our initial motivation is very simple, we use a teacher network to generate labels for the student. This can be achieved in two ways: either using the teacher's prediction as the direct targets (soft label), or through the hard labels dynamically assigned by the teacher (LAD). Our experiments reveal that: (i) LAD is more effective than soft-label, but they are complementary. (ii) Using LAD, a smaller teacher can also improve a larger student significantly, while soft-label can't. We then introduce Co-learning LAD, in which two networks simultaneously learn from scratch and the role of teacher and student are dynamically interchanged. Using PAA-ResNet50 as a teacher, our LAD techniques can improve detectors PAA-ResNet101 and PAA-ResNeXt101 to \$46 {\textbackslash}rm AP\$ and \$47.5{\textbackslash}rm AP\$ on the COCO test-dev set. With a strong teacher PAA-SwinB, we improve the PAA-ResNet50 to \$43.9{\textbackslash}rm AP\$ with only {\textbackslash}1x schedule training, and PAA-ResNet101 to \$47.9{\textbackslash}rm AP\$, significantly surpassing the current methods. Our source code and checkpoints will be released at https://github.com/cybercore-co-ltd/CoLAD\_paper.},
	urldate = {2021-10-01},
	journal = {arXiv:2108.10520 [cs]},
	author = {Nguyen, Chuong H. and Nguyen, Thuy C. and Tang, Tuan N. and Phan, Nam L. H.},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.10520},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ge_ota_2021,
	title = {{OTA}: {Optimal} {Transport} {Assignment} for {Object} {Detection}},
	shorttitle = {{OTA}},
	url = {http://arxiv.org/abs/2103.14259},
	abstract = {Recent advances in label assignment in object detection mainly seek to independently define positive/negative training samples for each ground-truth (gt) object. In this paper, we innovatively revisit the label assignment from a global perspective and propose to formulate the assigning procedure as an Optimal Transport (OT) problem -- a well-studied topic in Optimization Theory. Concretely, we define the unit transportation cost between each demander (anchor) and supplier (gt) pair as the weighted summation of their classification and regression losses. After formulation, finding the best assignment solution is converted to solve the optimal transport plan at minimal transportation costs, which can be solved via Sinkhorn-Knopp Iteration. On COCO, a single FCOS-ResNet-50 detector equipped with Optimal Transport Assignment (OTA) can reach 40.7\% mAP under 1X scheduler, outperforming all other existing assigning methods. Extensive experiments conducted on COCO and CrowdHuman further validate the effectiveness of our proposed OTA, especially its superiority in crowd scenarios. The code is available at https://github.com/Megvii-BaseDetection/OTA.},
	urldate = {2021-10-01},
	journal = {arXiv:2103.14259 [cs]},
	author = {Ge, Zheng and Liu, Songtao and Li, Zeming and Yoshie, Osamu and Sun, Jian},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.14259},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{gladisch_experience_2019,
	title = {Experience {Paper}: {Search}-{Based} {Testing} in {Automated} {Driving} {Control} {Applications}},
	shorttitle = {Experience {Paper}},
	doi = {10.1109/ASE.2019.00013},
	abstract = {Automated test generation and evaluation in simulation environments is a key technology for verification of automated driving (AD) applications. Search-based testing (SBT) is an approach for automated test generation that leverages optimization to efficiently generate interesting concrete tests from abstract test descriptions. In this experience paper, we report on our observations after successfully applying SBT to AD control applications in several use cases with different characteristics. Based on our experiences, we derive a number of lessons learned that we consider important for the adoption of SBT methods and tools in industrial settings. The key lesson is that SBT finds relevant errors and provides valuable feedback to the developers, but requires tool support for writing specifications.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Gladisch, Christoph and Heinz, Thomas and Heinzemann, Christian and Oehlerking, Jens and von Vietinghoff, Anne and Pfitzer, Tim},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Measurement, Monitoring, Optimization, Software, Test pattern generators, Tools, automated driving, automated test generation, experience paper, search-based testing},
	pages = {26--37},
}

@article{islam_towards_nodate,
	title = {Towards understanding the challenges faced by machine learning software developers and enabling automated solutions},
	language = {en},
	author = {Islam, Johirul},
	pages = {162},
}

@article{zhou_centernet_2019,
	title = {{CenterNet}: {Objects} as {Points}},
	url = {http://arxiv.org/abs/1904.07850},
	abstract = {Detection identiﬁes objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefﬁcient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point — the center point of its bounding box. Our detector uses keypoint estimation to ﬁnd center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
	language = {en},
	urldate = {2021-09-22},
	journal = {arXiv:1904.07850 [cs]},
	author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.07850},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ge_yolox_2021,
	title = {{YOLOX}: {Exceeding} {YOLO} {Series} in 2021},
	shorttitle = {{YOLOX}},
	url = {http://arxiv.org/abs/2107.08430},
	abstract = {In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3\% AP on COCO, surpassing NanoDet by 1.8\% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3\% AP on COCO, outperforming the current best practice by 3.0\% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0\% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8\% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/Megvii-BaseDetection/YOLOX.},
	language = {en},
	urldate = {2021-09-22},
	journal = {arXiv:2107.08430 [cs]},
	author = {Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},
	month = aug,
	year = {2021},
	note = {arXiv: 2107.08430},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{gkioxari_mesh_nodate,
	title = {Mesh {R}-{CNN}},
	abstract = {Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by ﬁrst predicting coarse voxel representations which are converted to meshes and reﬁned with a graph convolution network operating over the mesh’s vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes. Project page: https://gkioxari.github.io/meshrcnn/.},
	language = {en},
	author = {Gkioxari, Georgia and Malik, Jitendra and Johnson, Justin},
	pages = {15},
}

@article{tian_fcos_2020,
	title = {{FCOS}: {A} {Simple} and {Strong} {Anchor}-free {Object} {Detector}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {{FCOS}},
	url = {https://ieeexplore.ieee.org/document/9229517/},
	doi = {10.1109/TPAMI.2020.3032166},
	language = {en},
	urldate = {2021-09-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
	year = {2020},
	pages = {1--1},
}

@misc{noauthor_notitle_nodate,
}

@misc{noauthor_elsevier_nodate,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/0164121289900162?token=71D919BDA5333D78B51B87F17DBDA4B2A3214D8864E80C3F1774B6AF7B1DE0893FB7CFAA7B62969C98F0ADFA2D302459&originRegion=eu-west-1&originCreation=20210920153903},
	urldate = {2021-09-20},
}

@misc{noauthor_elsevier_nodate-1,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/0164121289900162?token=71D919BDA5333D78B51B87F17DBDA4B2A3214D8864E80C3F1774B6AF7B1DE0893FB7CFAA7B62969C98F0ADFA2D302459&originRegion=eu-west-1&originCreation=20210920153903},
	urldate = {2021-09-20},
}
