% SmartSquat: Detecting Malicious Typosquatting in Large-Scale Ecosystems with Enhanced Embedding Search and Metadata Postprocessing


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for USENIX papers.
%
% History:
%
% - TEMPLATE for Usenix papers, specifically to meet requirements of
%   USENIX '05. originally a template for producing IEEE-format
%   articles using LaTeX. written by Matthew Ward, CS Department,
%   Worcester Polytechnic Institute. adapted by David Beazley for his
%   excellent SWIG paper in Proceedings, Tcl 96. turned into a
%   smartass generic template by De Clarke, with thanks to both the
%   above pioneers. Use at your own risk. Complaints to /dev/null.
%   Make it two column with no page numbering, default is 10 point.
%
% - Munged by Fred Douglis <douglis@research.att.com> 10/97 to
%   separate the .sty file from the LaTeX source template, so that
%   people can more easily include the .sty file into an existing
%   document. Also changed to more closely follow the style guidelines
%   as represented by the Word sample file.
%
% - Note that since 2010, USENIX does not require endnotes. If you
%   want foot of page notes, don't include the endnotes package in the
%   usepackage command, below.
% - This version uses the latex2e styles, not the very ancient 2.09
%   stuff.
%
% - Updated July 2018: Text block size changed from 6.5" to 7"
%
% - Updated Dec 2018 for ATC'19:
%
%   * Revised text to pass HotCRP's auto-formatting check, with
%     hotcrp.settings.submission_form.body_font_size=10pt, and
%     hotcrp.settings.submission_form.line_height=12pt
%
%   * Switched from \endnote-s to \footnote-s to match Usenix's policy.
%
%   * \section* => \begin{abstract} ... \end{abstract}
%
%   * Make template self-contained in terms of bibtex entires, to allow
%     this file to be compiled. (And changing refs style to 'plain'.)
%
%   * Make template self-contained in terms of figures, to
%     allow this file to be compiled.
%
%   * Added packages for hyperref, embedding fonts, and improving
%     appearance.
%
%   * Removed outdated text.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}

\newif\ifDEBUG
% \DEBUGtrue
\DEBUGfalse

\newif\ifNOBOXES
%\NOBOXEStrue
\NOBOXESfalse

\newif\ifSPACEHACK
% \SPACEHACKtrue
\SPACEHACKfalse


\newif\ifANONYMOUS
% \ANONYMOUStrue
\ANONYMOUSfalse

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}

% inlined bib file
\usepackage{filecontents}

%-------------------------------------------------------------------------------

\input{misc/typesetting}
\input{data/data}

\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
% \title{\Large \bf TypoSmart: Low False-Positive Detection on ``Sleepy'' Typosquatting Attacks \\for Package Registries
% }

\title{\Large \bf TypoSmart: A Low False-Positive System for Detecting Malicious and Stealthy Typosquatting Threats in Package Registries}
\title{\Large \bf Detecting Active and Stealthy Typosquatting Threats in Package Registries}
% SmartSquat: Detecting Malicious Typosquatting in Large-Scale Ecosystems with Enhanced Embedding Search and Metadata Postprocessing


%for single author (just remove % characters)
% \author{
% {\rm Your N.\ Here}\\
% Your Institution
% \and
% {\rm Second Name}\\
% Second Institution
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
% } % end author

\maketitle

\begin{abstract}
Typosquatting attacks, also known as package confusion attacks, threaten software supply chains.
Attackers make packages with names that resemble legitimate ones, tricking engineers into installing malware.
While prior work has developed defenses against typosquatting in some software package registries, notably npm and PyPI, gaps remain:
  addressing high false-positive rates;
  generalizing to more software package ecosystems;
  and
  gaining insight from real-world deployment.
%\JD{The following remark is suitable for the body but not the abstract. I cleaned up the opening part of this paragraph, so just move the rest to the body or Discussoin.}
%Prior works focused on user-level where they deploy the defense in CI/CD pipeline and the false-positive is acceptable, while our work focused on a registry-level where the FPs are important to maintain the reputation of package contributors and organizations in the community.

% \JD{The next paragraph needs to be reworked. We have a run-on style and the order of presentation does not seem suitable.}
\GKT{I am guessing this is a tools or experience type of track. But this also reads as a research paper as an empirical study.w}
In this work, we introduce \TypoSmart, a solution designed to address the challenges posed by typosquatting attacks. We begin by conducting a novel analysis of typosquatting data to gain deeper insights into attack patterns and engineering practices.
Building on state-of-the-art approaches, we extend support to six software package registries using embedding-based similarity search, achieving a 73\%–91\% improvement in speed. Additionally, our approach significantly reduces \FPReducedRate false-positive compared to prior work results.
\TypoSmart is being used in production at our industry partner and contributed to the removal of \NumRemovedPkgs typosquatting packages in one month.
We share lessons learned from the production deployment.



% This paper presents a typosquatting detection system that integrates an embedding-based name similarity approach, popularity metadata, and fine-grained filtering with package-level metadata. We perform a large-scale analysis across six registries---npm, PyPI, RubyGems, Maven, Golang, and Hugging Face---unifying prior approaches and introducing novel optimizations to handle ecosystems lacking direct popularity metrics. Our evaluation demonstrates that the system not only improves detection accuracy but also substantially reduces false positives by leveraging metadata verification. Deployed in a production setting, our approach yields actionable alerts for practitioners, and our disclosures to registry maintainers have led to the removal of \TODO{xx} reported malicious packages. By bridging gaps in both methodology and coverage, this work offers a comprehensive, faster, and more accurate solution for detecting typosquatting attacks in modern software and AI model ecosystems. Our work has \TODO{...}

\iffalse
\vspace{0.1cm}
%\textit{``There are only two hard things in computer science: cache invalidation and naming things.'' --- P. Karlton}
\hspace{0.5cm} \textit{``Don't judge a book by its cover'' --- Anon.}
\fi

\end{abstract}


%-------------------------------------------------------------------------------
\section{Introduction}
% \JD{Global change needed: I changed the title to describe `typosquat packages with malware in' as `active typosquat', so that there is a complementary word for `stealth typosquat'}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/threat_model - Page 1 (1).pdf}
    \caption{
    Threat model depicting typical typosquatting attacks involving active and stealth typosquatting threats.
    % \BC{I am working on it}\WJ{We need larger font size.}
    % \JD{No Need For Upper Case Everywhere In The Caption}
    % \JD{Use the terms `active typosquat' and `stealth typosquat'.}
    \JD{This figure states the attacker can compromise a benign package. That sounds like hijacking which we exclude. Please correct the figure.}
}
\vspace{-0.5cm}

    \label{fig:ThreatModel}
\end{figure}

%-------------------------------------------------------------------------------
% \JD{Mention HF in this intro as well}
Software package registries (SPRs) have become indispensable in modern development, providing open-source packages which greatly reduce costs and accelerate product cycles~\cite{SotoValero2019EmergenceofSWDiversityinMavenCentral, Wittern2016JSPackageEcosystem}.
% \JD{The next sentence does not illustrate criticality at all. Needs statistics/evidence...not `popular'. Popular to whom? Students and hobbyists are not critical. As an example reference, Kelechi's USENIX has a nice reference to an industry study that showed how much open-source software is integrated into commercial code.}
% Popular registries
% such as npm for JavaScript and PyPI for Python
% illustrate how critical these repositories have become
% ~\cite{gu2023investigatingPackageRelatedSecurityThreatsinSoftwareRegistries, Ladisa2023TaxonomyofAttacksonOSSSC, okafor_sok_2022}. However, this deep reliance on third-party components also heightens risks to the software supply chain~\cite{Ohm2020ReviewofOpenSourceSSCAttacks} \JD{To fix limited references: add 3-5 more papers here in a block.}.
Open-source packages are used by industry and governments~\cite{18fopensourcepolicy, synopsys2024ossra}, including in AI and safety-critical applications~\cite{Jiang2022PTMReuse, stenbit2003OSSinDOD}.
Attackers seek to disrupt or exploit the resulting supply chains by creating packages with deceptive or look-alike names~\cite{gu2023investigatingPackageRelatedSecurityThreatsinSoftwareRegistries, Ladisa2023TaxonomyofAttacksonOSSSC, okafor_sok_2022}.
This practice is commonly called typosquatting or package confusion~\cite{kaplan_survey_2021, neupane2023beyondTyposquatting}.
\cref{fig:ThreatModel} shows the threat model.

\iffalse
An adversary publishes a package that may be confused with a legitimate one.
Through techniques like choosing a similar name, adversaries trick engineers into unintentionally installing a malicious package, leading to data stealing, cryptojacking, or other breaches~\cite{neupane2023beyondTyposquatting}.
Addressing the typosquatting attack problems is critical not just for traditional software packages but also for open-source pre-trained AI models. Recent research shows that the AI model supply chain is equally susceptible to malicious uploads, with typosquatting enabling attackers to spread harmful or backdoored models through platforms like Hugging Face~\cite{Jiang2022PTMReuse}.
% \JD{I feel that we should write something positive about the approaches first, eg ``Researchers have proposed many approaches to address typosquatting, using techniques like X [cite, cite] and Y [cite, cite]. However, three problems remain:''}
\fi

Researchers have proposed many approaches to address package typosquatting, including Levenshtein distance~\cite{vu2020typosquatting}, lexical similarity~\cite{neupane2023beyondTyposquatting, taylor2020spellboundDefendingAgainstPackageTyposquatting}, or imaging approach~\cite{sern2021typoswype}.
% Although many approaches for detecting typosquatting already exist~\cite{},
However, three gaps
% \JD{not sure we need ``practical'' here} practical
 remain: (1) high false positive rate, (2) limited coverage across SPRs, and (3) limited insights from SPR perspectives.
Prior work --- triggering alarms approximately every 200 to 1,000 package installations --- falls short of customer expectations, and has limited coverage on only three SPRs~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting}.

% \PA{These items sounds more like limitations of existing work, and not necessarily challenges. Do you want to frame it as such? Also, at this point, I don't think it is obvious why we should care about solving these challenges/limitations.}
% \JD{Ultimately I feel that problem 1 and problem 3 are the same problem --- ``Existing techniques do not generalize to all registries. New registries introduce new forms of typosquatting that must be addressed.'' Can we reduce to two problems in this way?}
\iffalse
\begin{enumerate}[leftmargin=*, itemsep=0.1ex]
    % \item \textbf{Lack of Support for Hierarchical Naming.} Registries such as Maven, Golang, and Hugging Face employ hierarchical naming conventions, which have not been thoroughly explored in current research.

    \item \textbf{High False Positive Rate.} Existing detection methods primarily rely on name similarity, resulting in a large number of false alarms when applied to millions of packages. While this may be acceptable for frontend CLI tools during installation, it could have adverse effects if implemented at the package registry level.

    \item \textbf{Limited Coverage Across Registries.} Most large-scale studies focus on npm, PyPI, and RubyGems, whereas registries like Maven and Golang remain understudied despite their widespread use. Additionally, Hugging Face—today's leading AI model registry with over one million models—has received minimal attention concerning package confusion attacks.
    % \JD{This third problem definitely sounds like a practical problem (``incremental advance''), but the other two seem bigger to me?}
\end{enumerate}
\fi

%
% (1) the need for more scalable package comparison methods\JD{Add a bit of motivation, eg the scale of registries and the need to handle millions of packages. But later you say the detection speed is 0.22 ms/package so that's just about 4 minutes to scan a million-package registry (0.22ms*1millionpkgs/1000 = 220 seconds = 4 minutes). Sounds like there is no problem.}, (2) \JD{Number 2 does not sound like an essential problem. Registry can easily publish popularity and remove this gap so your paper shouldn't depend on this for motivation.} the absence of popularity metrics for certain registries, (3) high false positive rates in the state-of-the-art detection systems, and (4) the lack of data and insights from production deployments. \JD{Not sure about this gap and the next sentence. Essential for what? (This paragraph is about the knowledge gap --- why do we need this data/insight?)}
% A deeper understanding of the challenges and solutions for deploying typosquat detectors in production environments and at scale is essential.
%

% \JD{Next paragraph should be aligned and ordered like the gaps you identify in the preceding paragraph. The deployment experience part should be in the Evaluation paragraph.}
We present \TypoSmart, the first scalable deployment of a novel typosquatting detection system.
Our approach addresses key limitations by leveraging comprehensive package metadata to reduce false-positive rates, integrating enhanced embedding-based name comparisons to scale on production, and deployed to a production environment.
% Achieving both continuous coverage and high accuracy in production introduces unique complexities.
% For package registries, this necessitates achieving continuous coverage and high accuracy in production while minimizing false-positive rates and therefore address \ul{problem 1}.
% We also extend the state-of-the-art to six ecosystems while significantly reducing the false-positive rate to address \ul{problem 2}.
% We deployed our system across six registries—npm, PyPI, RubyGems, Maven, Golang, and Hugging Face—encompassing both established package managers and the leading AI model hub.
Additionally, we refined the traditional definition of typosquatting --- moving from merely identifying suspicious package names to incorporating both active and stealth attacks.
This approach improved the accuracy of identifying real threats and redefined false-positive criteria, aligning them more closely with practical concerns.

% To address this, we developed a false-positive verifier that leverages package metadata and large-language models (LLMs) to substantially reduce spurious alerts. Specifically, we employ embedding similarity search to optimize name-similarity computations and implement a fine-grained metadata verification process to further minimize false alarms, ensuring practical and reliable system performance in real-world deployments.
% \WJ{\textbf{Production validation.} Deploying typosquat detectors in real-world CI/CD pipelines raises questions of computational overhead, data completeness, and how best to mitigate false positives in a large volume of alerts—issues rarely explored in depth.}
% Consequently, we developed an optimized pipeline featuring \textbf{quantized embedding models} and \textbf{approximate nearest neighbor (ANN) searches}, boosting efficiency and coverage.
%
%
% % To address these gaps, we deployed a state-of-the-art typosquat detection approach in a production environment and iteratively optimized the system based on real-world usage.
% \JD{The next sentence is a bit boring, you see, which affirms my concern about the lack of real complexity/challenge for popularity data}
% We utilized popularity metadata from \emph{ecosyste.ms} to compensate for missing information in registries like Maven and Golang. Our deployment experience informed further optimizations to enhance system accuracy and efficiency.
% Building upon our deployment experience, we optimized the typosquat detection approach by employing a quantized embedding model and utilizing approximate nearest neighbor (ANN) search, which significantly accelerated the detection process. To reduce the false positive rate, we also leveraged package metadata to verify the legitimacy of each detected package.
% \JD{This paragraph sounds like it's more of your system design and implementation. I was expecting it to be about your findings/results.}

In empirical tests, our \texttt{TypoSmart} system improved 73\%-91\% neighbor search speeds, and reduced false positives by \FPReducedRate through metadata-driven checks.
% , which incur a latency of \TypoSmartFPVerificationLatency.
% The overall system operates at acceptable speeds of \TypoSmartNeighborSearchLatency per pair of packages during neighbor searches.
By prioritizing true positives that practitioners find valuable, our methodology improves the relevance and reliability of detection outcomes.
%further bolstering the security of the software supply chain.
Additionally, this work presents 4 lessons we learned from production deployment from the perspective of SPRs, offering insights into optimal deployment strategies.
\TypoSmart is being used in production at our industry partner, and contributed to the removal of \NumRemovedPkgs typosquatting packages in one month.

To summarize our contributions:
% \JD{This is too many contributions. Make fewer richer contributions, like ``We accomplish X via Y'' instead of ``(1) We accomplish X. (2) Via Y''. Let's aim for three total. I suggest you keep contribution 1 as-is, merge contributions 2 and 4, and merge contributions 3 and 5.}
\begin{enumerate}[leftmargin=1.2em, itemsep=0.1ex]
    \item  Drawing from production deployment experience and analysis of typosquatting-relevant practices, we refined the definition of typosquatting to encompass both active and stealth typosquatting threats.
    % attack and engineering practices to provide a better understanding of the current gap.
    \item We present \TypoSmart, an embedding-based detection system, extending state-of-the-art work to SPRs and substantially reducing the false positive rates.
\item Our real-world deployment provides practical lessons for implementing robust supply chain defenses from the perspective of package registries.
\end{enumerate}

\iffalse
\begin{enumerate}[leftmargin=1.2em, itemsep=0.1ex]
    \item
    % \textbf{False-Positive Mitigation via Metadata.}
    Drawing from production deployment experience, we refined the definition of typosquatting to include both active and stealth typosquatting attacks.
    \item We developed a novel system aimed for package registries that incorporates package metadata, substantially reducing false positives to align with practitioner needs.
    \item
    % \textbf{Expanded Multi-Registry Analysis.}
    % \PA{I didn't find this in the paper though.}
    We advance the state-of-the-art by extending typosquatting analysis to six software package registries, including Maven, Golang, and Hugging Face --- the first study to address typosquatting in pre-trained model packages --- alongside npm, PyPI, and RubyGems.
    \item
    We leverage embedding-based package neighbor search and external popularity metrics to achieve faster and more accurate detection at scale, optimizing performance for production environments.
    % % \textbf{Embedding-Based Approach with Production Optimizations.}
    % We employ
    % % quantized embeddings,
    % ANN search and external popularity metrics, enabling faster, more accurate detection at large scale.
    % from \TODO{xx}\% to \TODO{yy}\%.
    \item
    % \textbf{Deployment Experience.}
    Our real-world deployment offers valuable lessons for implementing supply chain defenses from the package registry perspective, covering aspects such as system efficiency, responsible disclosure practices, and directions for future work.
\end{enumerate}
\fi

\iffalse
\section{Background and Related Work}
%-------------------------------------------------------------------------------
\subsection{Software Supply Chain Security}

% Why software supply chain is important

% What impact it can have. Examples of supply chain attacks e.g. xzutil, xxx.

% AI Model supply chain security
\TODO{Models Are Codes: Towards Measuring Malicious Code Poisoning
Attacks on Pre-trained Model Hubs

Typosquat is one of the threat model.}


\subsection{Typosquatting and Package Confusion}

% Origin of typosquat attack
Typosquatting originated from DNS domain names~\cite{koide2023phishreplicantLanguageModelBasedDNSSquattingDetection, moubayed2018DNSTyposquatDomainDetection}

% Taxonomy of detections
Package typosquatting, or package confusion...


% Detection methods
Previous methods have focused on maximizing detection by applying typosquatting detectors across the Cartesian product of popular and less popular packages, aiming to catch all potential typosquats.
The simplest approach utilizes Levenshtein distance (edit distance) measurements~\cite{taylor2020spellbound}. A more advanced method employs \textit{FastText} embeddings to capture semantic similarities and phonetic resemblance~\cite{neupane2023beyondTyposquatting}.

% \subsection{Embedding Models and Efficient Search Approaches}
\fi
\section{Background and Related Work} %-------------------------------------------------------------------------------

%This section discusses software package registries, typosquatting attacks and existing defenses for these attacks.

% \subsection{Software Package Registries and Package Naming}

Modern software development depends on ecosystems of third-party libraries and frameworks that facilitate software reuse~\cite{Wittern2016JSPackageEcosystem, Zimmermann2019SecurityThreatsinNPMEcosystem}.
% , build tools, CI pipelines, and distribution services.
We refer to these reusable software artifacts as \textit{software packages}.
These packages are distributed by \textit{software package registries (SPRs)}~\cite{SotoValero2019EmergenceofSWDiversityinMavenCentral, schorlemmer2024signing}.
\cref{tab:registry-overview} shows the SPRs for six popular ecosystems.
Together, these SPRs contain over 7 million software packages that are used by organizations worldwide.

SPRs accelerate engineering practice but their use exposes applications to many software supply chain security attack vectors~\cite{ohm_backstabbers_2020, okafor_sok_2022, Ladisa2023TaxonomyofAttacksonOSSSC, Zahan2022WeakLinksinNPMSupplyChain, gu2023investigatingPackageRelatedSecurityThreats}
% \JD{To fix limited references: add 3-5 more papers here in a block.}.
The most common and longstanding supply chain attack vector is called \textit{typosquatting}, or \textit{package confusion}.
A recent study indicates that, as of 2024, typosquatting campaigns continue to target developers by exploiting hundreds of popular JavaScript packages, with over 250 typosquatting packages published in total~\cite{lyons2024typosquatting}. Previous research has also demonstrated that a single typosquatting package can result in hundreds of user downloads~\cite{reversinglabs_r77_typosquatting}.
% \JD{Insert here: some statistics about typosquat in practice (eg to show how common it is). This may echo things in the Intro.}
Our goal is to detect such attacks.
In preparation, we next describe the current taxonomy of typosquatting attacks (\cref{sec:background-Taxonomy}) and then examine existing defenses (\cref{sec:background-Defenses}).

\begin{table}[h]
\footnotesize
\setlength{\tabcolsep}{2pt}
\caption{Ecosystems examined in this study, highlighting their primary domains, naming conventions, and popularity metrics.
Registry sizes are as of Jan. 2025.
%The Hugging Face model counts are sourced from \cite{HuggingFaceWeb}, while the other figures are derived from a commercial software supply chain database, as collected in January 2025.
An example 1-level naming pattern is PyPI's \texttt{requests} module.
An example hierarchical name is Hugging Face's \texttt{google-bert/bert-base-uncased}.
\iffalse
Example naming patterns are:
  npm (\texttt{lodash}),
  or \texttt{@namespace/pkg\_name}),
  PyPI (\texttt{requests}), RubyGems (\texttt{rails}),
  Maven (\texttt{groupId:artifactId}),
  Golang (\texttt{domain/author/repo}),
  and
  Hugging Face (\texttt{author/model}).
  \fi
% \PA{I feel these example names should be in the table. Would reduce the caption length. To do this, I would suggest removing the pop metric (not sure how useful it is), domain and # pkgs (we know these are the top registries for these ecosystems) columns. The domains can be bracketed after the registry name.}
% The numbers of packages are collected in Jan 2025.
% \WJ{Do we need to add the total number of packages in each ecosystem in this table? If so, that will be more than one column.}
}
\label{tab:registry-overview}
\footnotesize
\centering
\resizebox{0.95\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Registry (\# pkgs)} & \textbf{Domain} & \textbf{Name Structure} & \textbf{Pop. metric?}\\
\midrule
npm (\NumAllNPMPkgs)
& JavaScript
& Both
& Yes \\

PyPI (\NumAllPyPIPkgs)
& Python
& 1-level
& Yes \\

RubyGems (\NumAllRubyPkgs)
& Ruby
& 1-level
& Yes \\

Maven (\NumAllMavenPkgs)
& Java
& Hierarchical
& External \\

Golang (\NumAllGolangPkgs)
& Go
& Hierarchical
& External \\

Hugging Face (\NumAllHFPkgs)
& AI Models
& Hierarchical
& Yes \\
\bottomrule
\end{tabular}
}
\vspace{-0.4cm}

\end{table}

% --- reusable code libraries and frameworks
% Central to this ecosystem is software package managers, commonly known as
% Recently, AI model package registries like Hugging Face have emerged, streamlining AI development and introducing a pre-trained model supply chain as a new dimension of the software supply chain~\cite{Jiang2022PTMReuse, Jiang2022PTMSupplyChain}.
% While SPRs enhance engineering efficiency, they also expand the potential attack surface.

% \subsection{Software Supply Chain Security}
% This paper focuses on one aspect in which the attack surface is expanded: that engineers must identify and encode a package to depend on, and that they make mistakes in this process.
% We start by reviewing concepts in typosquatting ---
%   the existing taxonomy (\cref{X}) and
%   the state-of-the-art detection methods (\cref{Y}).
% %  software supply chain and software registries (\cref{sec:background-SSCandSPR}),
% % describes
% % typosquatting in SPRs (\cref{sec:background-typosquatinSPRs}), and
%   and the state-of-the-art in typosquatting taxonomy detection (\cref{sec:background-TaxonomyAndDetection}).

  %%%

% \JD{The rest of this paragraph is describing a hijack attack, not a typosquat, so it feels out of place. Can you give an example of a famous typosquat. You need to talk about the aspect of the attack surface that is relevant to this paprt.}
% A compromise in any package can cascade through the supply chain of dependent projects~\cite{Ohm2020ReviewofOpenSourceSSCAttacks}.
% Notable incidents, such as the malicious update to the npm package \texttt{event-stream} (which impacted hundreds of applications) underscore the severe consequences of such vulnerabilities, eroding trust across diverse platforms~\cite{arvanitis2022systematicAnalysisofEventStreamIncident, npm_event_stream_2018}.


% \PA{AT the end of almost every paragraph, you try to contextualize your work with respect to that paragraph. But this confuses me as to what your key contribution is. I don't think you should talk about your work so much in the background section.}

%\subsection{Software Ecosystems}
%\label{sec:background-SSCandSPR}


% Modern software development relies on a complex supply chain composed of third-party libraries, build tools, continuous integration (CI) pipelines, and distribution services. A key component of this supply chain is \textit{software packages}, which encompass reusable code libraries and frameworks~\cite{Wittern2016JSPackageEcosystem, Zimmermann2019SecurityThreatsinNPMEcosystem}.
% Central to this ecosystem is software package managers, commonly known as \textit{package registries}~\cite{SotoValero2019EmergenceofSWDiversityinMavenCentral, schorlemmer2024signing}.
% Recently, registries for AI model packages, such as Hugging Face, have also gained significant traction among practitioners, not only reducing engineering costs in AI software development but also establishing a pre-trained model supply chain as a new type of software supply chain~\cite{Jiang2022PTMReuse, Jiang2022PTMSupplyChain}.



% \JD{Should this topic sentence refer to packages or to ecosystems?}

% \Andreas{Nit: I believe that SolarWinds was distributed directly by the vendor as proprietary software, and not through package registries - so if ``such vulnerabilities'' is meant to refer to malware distributed through software registries, then I don't think it fits.}
% \JD{Seems like the next sentences belong in \$2.1, not here.}

% \Andreas{Table 1 introduces ``Name Structure'' and ``Pop. metric'' without explaining why they're relevant to registries and typosquatting attacks. I suggest that the first paragraph of \cref{sec:registry-model} should appear in this Background section.}
% \PA{Yeah, I agree with Andreas that this next paragraph needs more context. Alternatively, I think you can consider removing the paragraph, and reference figure 1 and your work after discussing existing taxonomy or typosquat detection and their limitations.}
% \WJ{@Jamie this is the structural change I would like to discuss.}

% \MIK{NPM is technically multi level since you can have namespaces}
% \WJ{Added examples for that}

\iffalse
\subsection{Typosquatting in Software Package Registries}
\label{sec:background-typosquatinSPRs}
% Typosquatting attacks are the most common software supply chain attack, ~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, neupane2023beyondTyposquatting}.
% A more detailed definition of typosquatting is provided in \cref{sec:registry-model}.

\textit{Typosquatting}---or \textit{package confusion}---is the most common and long-known supply chain attack vector in the traditional software registries, wherein adversaries register deceptive names closely resembling those of legitimate packages~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, Ladisa2023TaxonomyofAttacksonOSSSC}. Originally observed in domain squatting~\cite{moubayed2018dnstyposquat,koide2023phishreplicant}, it has since spread to software registries such as npm, PyPI, and Maven~\cite{Ohm2020ReviewofOpenSourceSSCAttacks}.
Even small changes, such as a single-character substitution, omission, or insertion, can lead developers --- or automated systems reliant on imperfect input handling, such as copy-paste errors or integrations with AI agents --- to unintentionally install malicious code.
We provide a more detailed discussion of typosquatting attacks and present a refined definition in \cref{sec:registry-model}.


% \subsection{Typosquatting in AI Model Repositories}

In parallel with traditional software repositories, typosquatting in the AI supply chain exploits the same fundamental dynamic: publishing model names that closely mimic well-known ones to deceive users or automated pipelines into downloading harmful variants~\cite{protectai_huggingface, 2022JiangEmpirical, jiang2023PTMNaming}. As machine learning workflows are increasingly integrated into critical infrastructure --- spanning domains such as financial services and autonomous vehicles~\cite{liu2021finbert, cui2024surveyonMultiLLM4AutoDriving} --- the accidental inclusion of a malicious model can have severe operational and security consequences~\cite{hu2021AISecurity}. AI model registries, such as Hugging Face~\cite{HuggingFaceWeb} and PyTorch Hub~\cite{PytorchHub}, play a central role in these workflows and are indispensable for machine learning practitioners~\cite{Jiang2022PTMReuse, 2022JiangEmpirical}. Prior research underscores that ``models are code'', highlighting how backdoors or malicious payloads embedded in pre-trained AI models can jeopardize user data integrity, compromise system functionality, or produce erroneous results in production systems~\cite{zhao2024modelsareCodes, Jiang2022PTMReuse}.
To address these concerns, we present the first study of typosquatting attacks in the context of the AI supply chain.
\fi

% \subsection{Typosquat: Taxonomy and Detection}
% \label{sec:background-TaxonomyAndDetection}

\subsection{Typosquatting Attacks and Taxonomy}
\label{sec:background-Taxonomy}
% \JD{Merge 2.2 and 2.3 into one subsection with two subsubsections}

% \TODO{Reorder or cut}
%\textit{Typosquatting}, also called \textit{package confusion}, is the most common and longstanding supply chain attack vector.
Typosquatting attacks on software packages are enabled by permissive package naming policies.\footnote{We acknowledge that typosquatting plagues all IT endeavors (\eg DNS~\cite{coyle2023chrome,ulikowski2025dnstwist} and Blocke~\cite{muzammil2024BlockchainTyposquatting}). We focus on software packages.}
In all major SPRs (\eg those of~\cref{tab:registry-overview}), if a name is not already registered, then an attacker can claim it for himself.
To carry out a typosquatting attack, an adversary publishes a software package whose name closely resembles that of a legitimate package.
The attacker's goal is to cause engineers to accidentally rely on their package instead, allowing the attacker to deliver malware to the developers or users of downstream software~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, Ladisa2023TaxonomyofAttacksonOSSSC}.
% We provide a more detailed discussion of typosquatting attacks and present a refined definition in \cref{sec:ThreatDefandThreatModel}.
Typosquatting occurs in the AI software supply chain too ---
%Similarly, with the rise of AI pretrained model hubs, typosquatting in the AI supply chain exploits the same fundamental dynamic:
on the Hugging Face SPR, attackers publish pre-trained models with names mimicking famous models to deceive engineers into using harmful variants~\cite{protectai_huggingface, 2022JiangEmpirical, jiang2023PTMNaming}.
% To address these concerns, we present the first study of typosquatting attacks in the context of the AI supply chain by running our system on Hugging Face model packages.
% \PA{What section contains the study you referenced here?}
% There has been
% Prior work~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting, sern2021typoswype} classifies typosquatting techniques based on specific \textit{textual manipulations}.

\cref{tab:typosquat_taxonomy} summarizes the state-of-the-art taxonomy of typosquatting attacks, based on Neupane \etal~\cite{neupane2023beyondTyposquatting}.
%This taxonomy provides illustrative examples of different forms of textual manipulations commonly employed by typosquatting attacks.
The variations in naming structure in different SPRs also affect the nature of typosquatting by SPR.
In SPRs with 1-level names, typosquats look like the top portion of the table, while in hierarchical SPRs the attacker has a larger naming surface to exploit (bottom portion of table).

% These strategies leverage common patterns, such as typos, visual ambiguities, or naming confusion, to deceive users.
% While some techniques (\eg homophonic similarity) may appear trivial, they exploit real-world scenarios where developers rely on automated tools or conduct only cursory manual checks during dependency installation, leaving them vulnerable to subtle, malicious alterations.

\iffalse
As shown in~\cref{tab:registry-overview}, SPRs usually use two different kinds of naming structure to identify the software packages they distribute.
This naming structure also affects the SPR's susceptibility to typosquatting attacks.
Single-level naming systems (\eg PyPI, RubyGems) use a straightforward mapping where each package is identified by a single string (\eg \texttt{lodash} or \texttt{requests}), making them easier to manage but also prone to direct typosquatting.
In contrast, hierarchical naming schemes (\eg Maven’s \texttt{groupId:artifactId} or Hugging Face’s \texttt{author/model}) offer greater flexibility for organizing packages but also broaden the attack surface.
By exploiting these complex naming structures, attackers can employ more sophisticated strategies to deceive users.
\fi


{
\renewcommand{\arraystretch}{0.5}
\tiny
\begin{table}[h]
\centering
\caption{
Common typosquatting techniques with examples.~\GKT{Can you clarify whether these examples are ones that show up in the wild? Are there any "famous" examples of these that you could mention, perhaps in a third column? If the second column reflects this, can you mention the package where it occurred? Not all of your examples here look harmful at first glance.}
The top section presents the taxonomy proposed by Neupane \etal~\cite{neupane2023beyondTyposquatting}.
We added two patterns (bottom, cf.~\cref{sec:ProblemState-CaseStudy1-TP}), generalizing the prior taxonomy to SPRs with hierarchical names.
%We consider \textit{command squatting} out of scope in this work.
% \JD{Can we highlight the changed letters, using underline and boldface in the `Example Transformation' column? I would like to be able to check for myself whether it seems like you've really found two new techniques, but I cannot easily do so without a lot of work. Make it easy for me!}
% \WJ{Highlighted}
}
\label{tab:typosquat_taxonomy}
\scriptsize
\begin{tabular}{p{0.17\textwidth}p{0.26\textwidth}}
\toprule
\textbf{Technique} & \textbf{Example Transformation} \\
\midrule
1-step Damerau-Levenshtein Distance     & \texttt{crypt\textbf{\ul{o}}} $\to$ \texttt{crypt} \\
\\
Prefix/suffix augmentation & \texttt{dateutil} $\to$ \texttt{\textbf{\ul{python3-}}dateutil} \\
\\
Sequence reordering      & \texttt{python-nmap} $\to$ \texttt{\textbf{\ul{nmap-python}}} \\
\\
Delimiter modification & \texttt{active\textbf{\ul{-}}support} $\to$ \texttt{activesupport} \\
\\
Grammatical substitution  & \texttt{serialize} $\to$ \texttt{serialize\textbf{\ul{s}}} \\
\\
Scope confusion  & \texttt{\textbf{\ul{@}}cicada\textbf{\ul{/}}render} $\to$ \texttt{cicada\textbf{\ul{-}}render} \\
\\
Semantic substitution  & \texttt{b\textbf{\ul{z2file}}} $\to$ \texttt{b\textbf{\ul{zip}}} \\
\\
Asemantic substitution  & \texttt{discord.\textbf{\ul{js}}} $\to$ \texttt{discord.\textbf{\ul{app}}} \\
\\
Homophonic similarity  & \texttt{uglify\textbf{\ul{-}}js} $\to$ \texttt{uglifi\textbf{\ul{.}}js} \\
\\
Simplification  & \texttt{pwd\textbf{\ul{hash}}} $\to$ \texttt{pwd} \\
\\
Alternate spelling  & \texttt{colorama} $\to$ \texttt{colo\textbf{\ul{u}}rama} \\
\\
Homographic replacement  & \texttt{d\textbf{\ul{j}}ango} $\to$ \texttt{d\textbf{\ul{i}}ango} \\
\midrule
\textbf{Impersonation Squatting}  & \texttt{\textbf{\ul{meta}}-llama/Llama-2-7b-chat-hf} $\to$ \texttt{\textbf{\ul{facebook}}-llama/Llama-2-7b-chat-hf}\\
\\
\textbf{Compound Squatting}  & \texttt{@typescript\textbf{\ul{-}}eslint/eslint\textbf{\ul{-plugin}}} $\to$ \texttt{@typescript\textbf{\ul{\_}}eslint\textbf{\ul{er}}/eslint} \\
% \textbf{Command Squatting}  & \texttt{npm i \ul{\textbf{$--$help}}} $\to$ \texttt{npm i
 % \ul{\textbf{\texttt{help}}}} \\
\bottomrule
\end{tabular}
\end{table}
}


% \MIK{Did we end up implementing any other types of typosquat detection?}

% \cref{tab:registry-overview} summarizes the SPRs for six ecosystems.
% The naming structure of a registry plays a significant role in its susceptibility to attacks. Single-level naming systems (\eg PyPI, RubyGems) use a straightforward mapping where each package is identified by a single string (\eg \texttt{lodash} or \texttt{requests}), making them easier to manage but also prone to direct typosquatting. In contrast, hierarchical naming schemes (\eg Maven’s \texttt{groupId:artifactId} or Hugging Face’s \texttt{author/model}) offer greater flexibility for organizing packages but also broaden the attack surface. By exploiting these complex naming structures, attackers can employ more sophisticated strategies to deceive users.
% These ecosystems reflect contemporary designs, encompassing both single-level naming schemes (\eg npm, PyPI, RubyGems) and hierarchical naming structures (\eg Maven, Golang, Hugging Face), as detailed in \cref{tab:registry-overview}. Each registry demonstrates unique naming conventions, popularity metrics, and policies, which can be leveraged for various software supply chain attacks, including typosquatting, code injection, package manipulation~\cite{ohm2023SoKPracticalDetectionofSSCAttacks, gokkaya2023SSCReviewofAttacks}.
% While some registries (\eg npm, PyPI) feature simple name-to-package mappings, others (\eg Maven, Hugging Face) rely on hierarchical coordinates or author-based namespaces. Understanding these structural differences is critical for analyzing how attackers may deploy malicious packages.
% Our work examines the naming conventions of six ecosystems and proposes tailored detection approaches for each.

\subsection{Defenses Against Typosquatting}
\label{sec:background-Defenses}
% \JD{I suggest you use paragraph headings for `considerations for defense' (here you would talk about false positives and any other major issuse), `academic defenses', `production defenses'}
% \JD{This paragraph does not analyze the cited works in a coherent way. I want to know the same information (dimensions of analysis) about each work. You say WHERE Spellbound is implemented and then HOW typomind works. Confusing.}
% \JD{Please add the phrase ``The state of the art technique is'' (or `techniqueS ARE') so the reader knows.}
% This section explores three key aspects of typosquatting defense: considerations for minimizing false positives, academic approaches, and production tools.


\iffalse
\subsubsection{Considerations for Defenses}
% \JD{This is a bad topic sentence. Also I cannot think of a good topic sentence because this paragraph is doing too many things. It does not fit under the heading 2.2.2 ``Typosquat defense'' which should only contain analysis of existing defenses.}
% \PA{This paragraph feels like it is explaining another dimension of typosquat attacks. Do you want to have a separate subsection for typosquat attacks? Then this section just focuses on the academic and production defenses. Alternatively, you can edit this paragraph so it is obvious what the defense considerations are (which is not currently stated)}
Considering typosquatting \ul{attack behavior}, attackers exploit permissive naming policies in registries by mimicking the names of legitimate packages. Current state-of-the-art detection techniques primarily rely on lexical similarity as their main criterion, often leading to high false-positive rates due to the limited information encoded in package names~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}. Beyond simple naming mimicry, attackers can deploy \textit{stealth typosquatting attacks}, where malicious packages not only imitate the names but also replicate the functionality of legitimate packages.
These attacks enable silent malware injection and compromise systems, targeting either maliciously designed placeholder packages or legitimate packages with similar names that might later be compromised, posing significant risks to supply chain security.
% The growing reliance on large language models (LLMs) for code generation has further exacerbated these threats. LLMs often hallucinate package names, generating installation commands for nonexistent or similarly named packages. Recent studies reveal that such hallucinations are prevalent across all tested LLMs~\cite{spracklen2024LLMPackageHallucinations}.
To address these challenges, we propose a refined definition of typosquatting attacks that includes \textit{stealth typosquatting} in \cref{sec:ThreatDefandThreatModel}, explicitly identifying such packages by analyzing the malicious intent behind each package.
\fi

Typosquat attacks can be mitigated at many points in the reuse process, \eg by scanning for malware in registries~\cite{vu_lastpymile_2021} or during dependency installation~\cite{latendresse2022PruductionDependenciesinNPM} or by sandboxing dependencies at runtime~\cite{vasilakis_breakapp_2018,ferreira_containing_2021,amusuo2025ztd}.
Since an ounce of prevention is worth a pound of cure~\GKT{I like this phrase but it is colloquial and may irritate picky ICSE reviewers}, many works have focused on detecting typosquats prior to dependency installation, \eg identifying possible typosquats during the dependency selection process.
We describe typosquat defenses
  from academia (\cref{sec:back-defense-academic})
  and
  from industry (\cref{sec:back-defense-industry}),
  and then describe the weaknesses and knowledge gaps (\cref{sec:back-defense-gaps}).

\subsubsection{Academic Defenses}
\label{sec:back-defense-academic}

\GKT{Wenxin, I debated whether to add a comment to the intro or here. I think a short paragraph about "prominent typosquat incidents" would be helpful before you go into the defenses. These could be from news sources or anecdotal. I feel like you did a great job of explaining the problem to this point but also feel like we're not doing enough to make it clear that there is a real problem that has real consequences. For example, has any typosquatting incident affected mission critical software? Did these incidents inflict significant damage, e.g. monetary, reputation, etc.? I think a short paragraph in the intro could really help to tell the story better.}
% \JD{Wenxin, we need to cite all the works on typosquat detection.}
Although there are many academic works on software supply chain security~\cite{okafor_sok_2022}, there have been relatively few academic works that specifically target the detection of typosquat attacks.
Prior typosquatting detection has explored multiple areas, including DNS domains~\cite{moubayed2018dns, kintis2017LongitudinalStsudyofCombosquattingAbuse}, mobile apps~\cite{hu2020mobileSquatting}, blockchain~\cite{muzammil2024BlockchainTyposquatting}, and container registry~\cite{liu2022exploringContainerRegistryTyposquatting}.
The earliest solution to package typosquatting was Taylor \etals \texttt{Spellbound} which was integrated into the installation pipeline to present npm users from potential typosquatting attacks~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting}.
Vu \etals concurrent work applied Levenshtein distance to identify PyPI package typosquatting attacks~\cite{vu2020typosquatting}.
Their scheme was based on lexical similarity --- they detected typosquats by identifying minor textual alterations in package names.
To detect more complex attacks, Neupane \etal proposed \texttt{Typomind}, a system that employs 12 heuristic rules for detecting typosquats~\cite{neupane2023beyondTyposquatting}.
Of particular note is their approach to addressing the ``semantic substitution'' class of attacks (\cref{tab:typosquat_taxonomy}).
They used FastText embeddings~\cite{bojanowski2017FasttextEmbedding} to encode elements of a package name into high-dimensional vectors.
By analyzing cosine similarity between these vectors, a wider range of typosquat techniques could be detected.

%Effective defenses against typosquatting attacks in package registries require solutions that balance accuracy with practical deployability. Taylor \etal introduced \texttt{Spellbound}, a lexical similarity based tool designed to detect typosquatting by identifying minor textual alterations in package names~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting}. To address more complex patterns of similarity, recent approaches have incorporated embedding-based techniques.
%Notably, Neupane \etal proposed \texttt{Typomind}, a system that employs 12 heuristic rules for detecting package confusion~\cite{neupane2023beyondTyposquatting}.
%Among these, one key rule utilizes FastText embeddings~\cite{bojanowski2017FasttextEmbedding} to encode split naming components of packages as high-dimensional vectors. By analyzing cosine similarity between these components, \texttt{Typomind} effectively identifies near-duplicate or subtly altered names that traditional edit-distance methods or simple lexical approach might fail to detect~\cite{neupane2023beyondTyposquatting}.
% \PA{I feel I would be more interested in if these approaches support or dont support hierarchical naming conventions (not if they investigated it). Also, more specificity on how they struggle with hierarchical names may be helpful.}




\subsubsection{Industry Defenses}
\label{sec:back-defense-industry}

% The emergence of ``sleep'' typosquatting attacks --- where packages with similar names and functionality silently inject malware and compromise dependents --- poses a significant threat to the supply chain. These so-called \textit{defensive typosquatting} attacks exploit the allowance of similar package names by registries, creating a broader attack surface.


% \WJ{@Mik, could you check this paragraph and let me know if it looks good and accurate?}
% \TODO{Mik:
% for similar tools, there are a bunch of typosquat tools for urls, but not much for packages.  all of the registries have some internal measures for preventing typosquats, but they are generally not sufficient
% \\
% note that npm does not actually say how they classify typosquats, but from what i gather in conversations, they just use levenshtein distance and also hold a few popular names internally
% \\
% the main registry is not well staffed and is more reactive than proactive at handling typosquats
% https://docs.npmjs.com/threats-and-mitigations#by-typosquatting--dependency-confusion}
% \PA{Maybe use the same frontend vs backend to analyze production defenses. Providing tools for users sounds like front-end support. More work is needed to help registries identify, flag or remove these packages.}
% \WJ{@Berk, one sentence each please.}
\GKT{Before going into specific companies (realizing that you are working with one of them) can you broadly characterize the industry. You may even want to signpost the general approaches and then mention companies. The opening to this section needs to be stronger.}
One class of industry tools supports engineers at package installation time.
% \JD{Fill in some details based on the links, or say ``no details provided''.}
Socket defends against typosquatting by providing real-time detection.
% of malicious packages in a software project during pull requests, inspecting introduced dependencies via their GitHub app. Furthermore, their CLI tool can be used as a prefix to \texttt{npm install}, proactively analyzing dependencies at installation-time~\cite{socket_typosquatting}.
Stacklok's approach is to identify typosquatting by analyzing package names using Levenshtein distance, evaluating repository and author activity metrics, and assigning a risk score through their platforms~\cite{stacklok_typosquatting}.
% GitGuardian...~\cite{gitguardian_typosquatting}.
Microsoft's OSSGadget provides a CLI tool to detect typosquats across multiple ecosystems~\cite{MicrosoftOSSGadget}.
%These solutions often rely on known CVEs or malware scanners.

Some software package registries also seek to detect typosquatting.
\texttt{npm} uses Levenshtein-based detection to identify and block package names that are deceptively similar to popular packages, thereby preventing typosquats from entering the registry~\cite{npm_threats_mitigations}.
\texttt{PyPI} has an impersonation policy, which prohibits deceptively similar usernames, organization names, and project names, reducing the risk of typosquatting and related attacks~\cite{psf_acceptable_use_policy}.
In addition, all major SPRs remove malware (including typosquat packages) when they become aware of it~\cite{ferreira2021containing, guo2023empiricalStudyonMaliciousCodeinPyPI, gu2023investigatingPKGRelatedSecurityThreatsinSRs}.
However, some typosquats are subjective; in the absence of a clear malware signal, human analysts remain necessary to triage reports.
%However, these measures are often insufficient and reactive due to resource constraints.

%These limitations highlight the need for robust techniques that combine advanced detection with effective takedown policies, as demonstrated in our findings from real-world deployments. Furthermore, existing research has largely overlooked typosquatting in hierarchical registries like Maven, Golang, and Hugging Face~\cite{mavenWeb, GolangWeb, HuggingFaceWeb}.
%To bridge this gap, we present a system model specifically designed for SPRs and provide valuable insights into additional underexplored registries.

\subsubsection{Gap Analysis and Contributions}
\label{sec:back-defense-gaps}

%New typosquatting packages continue to stymie existing  defenses~\cite{infosecurity_new_typosquatting_2024, socket_new_typosquatting_2025}.

%In the open-source community, package registries --- being central to all package distribution --- are uniquely positioned to provide critical support in combating these attacks.
Our work addresses several gaps across the existing knowledge described in~\cref{sec:back-defense-academic,sec:back-defense-industry}.

\begin{enumerate}[left=0.1cm]
\item \textbf{High false positive rate:}
  Existing tools say that only $\sim$0.1\% of their reports were malware~\cite{neupane2023beyondTyposquatting} because they do not have a clear definition of typosquatting false-positive.
  % \JD{Comment here about accuracy and whether the issue is false positives or false negatives or both.}
  What is the real typosquatting false positive and how to reduce the rate remains an open problem.
\item \textbf{Limited registry focus:}
  So far, the major research papers have focused on only 3 SPRs: npm, PyPI, and RubyGems~\cite{vu2020typosquatting, taylor2020spellbound, neupane2023beyondTyposquatting}.
  As indicated by~\cref{tab:registry-overview}, there are many SPRs and these SPRs vary in typosquat-relevant ways --- \eg how names are constructed and whether popularity information is available.
\item \textbf{Limited insight from SPR perspective:}
  Both academic defenses and many industry tools are focused on supporting the individual engineer with engineer-side tooling based on limited local information.
  Such tooling is certainly useful, but we believe it is a stopgap until SPRs or other ecosystem players can deploy a viable typosquat detection system at scale to proactively prevent typosquat creation or automatically take them down more quickly.
  False positives are particularly problematic for such a venture because inaccurate notifications erode trust ecosystem-wide.
  We lack reports from production deployments to understand what challenges arise. % and what lessons can be shared across the industry.
\end{enumerate}

We address these gaps as follows.
First, we conduct a novel analysis of typosquat true and false positives in order to improve accuracy over prior work.
Second, our work expands on prior work to consider 6 registries, which (to the best of our knowledge) represent all major typosquat-relevant variations across all public SPRs.
Third, we have deployed our system in collaboration with an industry partner and share lessons learned.

% \Andreas{This paragraph refers to ``frontend'' applications and ``backend'' environments. I think they should be defined, especially since a major part of your contribution is a focus on the backend. Perhaps in \$2.1 you can describe the actors / components involved in software installation via registries so that you can now refer to the components the prior work focuses on.}
\iffalse
Considering the \ul{target} of the defense mechanisms, current state-of-the-art techniques are primarily designed for frontend users during the package installation process~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting, MicrosoftOSSGadget}. While these highly sensitive detection systems are effective at identifying potential threats, they often produce substantial false positives during large-scale deployment.
In such systems, inaccurate notifications can harm the reputation of legitimate contributors and organizations, therefore eroding trust within the ecosystem.
False positives are more manageable in \textit{frontend} applications, where they serve to alert end users to potential risks when installing packages. However, in \textit{backend} environments, they pose significant challenges --- systems with a high false-positive rate are unsuitable as registries alarms, or \textit{blocking policy}, which is highly desirable and beneficial for SPRs.
To achieve this goal, we first developed a taxonomy that extends beyond simple naming patterns by incorporating package metadata, enabling a more robust and comprehensive threat detection framework.
We propose a backend solution that emphasizes accuracy, significantly reducing false-positive rates while maintaining operational efficiency.
\fi

\iffalse
However, none of these approaches have investigated typosquatting attacks on SLRs with hierarchical naming conventions, relying solely on package names --- a method that can be prone to inaccuracies and lead to high false-positive rates.
They also claimed that their system is designed to detect potential package confusion and serves solely as a warning mechanism.
In contrast, our approach leverages package metadata and extends state-of-the-art analysis across six ecosystems, achieving a significantly lower false-positive rate. This makes it suitable not only as a warning system but also potentially as a \textit{blocking policy} for SPRs.
\fi










\iffalse

\WJ{Below is old content.}

The typosquatting problem has been a persistent challenge in the software supply chain, prompting extensive research and development of defenses aimed at safeguarding supply chain security~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, vu2020typosquatting}. These defenses encompass a range of strategies, including string-based~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, MicrosoftOSSGadget} and embedding-combined approach~\cite{neupane2023beyondTyposquatting} approaches, alongside other innovative methodologies, such as keyboard typing based ~\cite{sern2021typoswype, le2019smorgaasbordofTypoes}.
Though there are some state-of-the-art solutions deployed in production environments, but specifically used for detecting domain name typosquatting and package typosquatting attack in practice remains unclear~\cite{, }.
\paragraph{String-Based Methods}

A straightforward approach is to measure string similarity between a suspicious package name and known popular targets using edit-distance metrics (\eg Levenshtein distance~\cite{levenshtein1966binary}). For example, \texttt{SpellBound} by Taylor \etal~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting} flags packages that lie within a small edit distance of widely used libraries. While simple and transparent, these methods can yield excessive false positives in repositories where many legitimate packages share overlapping or derivative names.

\paragraph{Embedding-Based Methods}

To capture more nuanced phonetic and semantic resemblances, recent work has turned to embedding-based detection. For instance, Neupane \etal~\cite{neupane2023beyondTyposquatting} propose \texttt{typomind}, which employs FastText embeddings~\cite{bojanowski2017FasttextEmbedding} to represent package names as vectors in a high-dimensional space. By comparing the cosine similarity of these embeddings, \texttt{typomind} uncovers near-duplicate or deceptively modified names that naive edit-distance checks might overlook. However, as with most highly sensitive detectors, large-scale deployment can still produce significant false alarms without further refinement.

Despite these advances, not all registries supply the same level of detail about packages, and some---particularly Maven or certain AI model repositories---offer only limited data regarding package popularity or ownership. Consequently, balancing coverage (high recall) with false-positive suppression remains an open challenge in practical deployments. Addressing these inconsistent signals and curbing superfluous alerts is central to our approach.

\paragraph{Other typosquat defenses}

\paragraph{Production Defense}
Microsoft has published an open-source supply chain defense package called OSSGadget which can  ~\cite{MicrosoftOSSGadget}
\fi

% \PA{Based on the last paragraph, it sounds like suppression of false positives is central to your approach. I think this should be more emphasized in the introduction.}

% \PA{So far, it seems like prior typosquatting works do not properly consider hierarchical package registries or detect your new classes. Is this true? if yes, I think it should be more emphasized in the intro and this comparison to related works.}

\section{Empirical Study of Typosquats}
\label{sec:ThreatDefandThreatModel}

In this section we report on several complementary investigations into attacker behaviors and benign engineering behaviors.
Our findings allow us to refine the definition of typosquatting, extend the state-of-the-art taxonomy, and learn legitimate behaviors that lead to false positives.

\GKT{One thing that I have been wondering about this topic, even since the time we worked on the PTM naming paper, is how to distinguish between deliberate and accidental? People do dumb stuff all the time; I know that I do. I believe the latter could pose a slight risk (threat?) to how we think about typosquatting in general. I'm not all that far into the paper yet but feel like something should be said about this. My gut feeling is that so many developers (who start as our students) are really bad at naming, often attributable to laziness. I can see this playing a role in modern software engineering. This doesn't invalidate what we are doing at all. I would just like to see a bit more clarity. This could be left to discussions, future implications, etc.}
%introduces the practices of typosquatting attacks (\cref{sec:PracticeAnalysis}) and refined definition in our work.
\subsection{Analysis of Attacker Practices}
\label{sec:PracticeAnalysis}

Although previous research has proposed various methods for detecting typosquatting attacks, the issue persists across multiple registries.~\GKT{...with what consequences?}
We analyzed attacker practices and identified key shortcomings in prior work.\GKT{Seems like two thoughts in this sentence. I recommend splitting into two sentences.}

%However, this taxonomy was created based on open-source data collected from only three ecosystems that employ single-level naming structure --- npm, PyPI, and RubyGems --- and has not been evaluated for adaptability to SPRs with hierarchical naming conventions, such as Maven, Go modules, and Hugging Face.
%Extending this work, our study explores how these manipulations can be adapted to hierarchical naming ecosystems, including emerging AI model registries, shedding light on the evolving threat landscape across diverse platforms.


% \subsubsection{Analysis of Attackers Behavior}
% \subsection{Case Study 1: True-Positive Analysis}
\label{sec:ProblemState-CaseStudy1-TP}

% \JD{This material feels very strange here. I think we should move this and the next subsection to \$3.}

\subsubsection{Stealthy Typosquats}
We analyzed the versions of confirmed typosquatting ``true-positives'', defined as \ul{typosquatting attack packages that include malware}. These data were collected and reported by Neupane \etal~\cite{neupane2023beyondTyposquatting}. We analyzed the number of days these malicious packages were available before malware was injected. Due to the lack of comprehensive package content (the SPR removed the packages), we estimated the injection time based on the last version updated before removal.

\Cref{fig:TPAnalysis} illustrates our findings on the release time before malware injection.
\textbf{While most typosquatting attacks injected malware within the first day of release, a notable proportion of packages ($\sim$14\%) represent stealth attacks}, lying dormant for a period before deploying malware.
The stealth typosquatting attack strategy allows attackers to evade detection for extended periods, increasing the likelihood of successful exploitation before the malicious activity is discovered.
% \JD{I added the next sentence, please check it.}
Prior work on typosquatting did not report this behavior nor take it into account in their system designs.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/days_histogram.pdf}
    \caption{
    Frequency of released time before malware added to a typosquat package.
    Over 10\% of typosquats are stealthy.
    }
    \label{fig:TPAnalysis}
\end{figure}

% \PA{Do you want to add a sentence or two about how these additional attack patterns were identified. I am wondering if these are all the new patterns we should be concerned about (completeness).}
% \BC{Which risks?}
% \WJ{TODO: How to be systematic.}
\subsubsection{Extending the Taxonomy to New SPRs}
% \WJ{@Jamie Is this paragraph okay as data source?}
% \JD{Feels a bit weird --- notably because typically taxonomies are developed through iterative card sort type methods. Mikola is on the author team and so this paper can be written with the perspective of Socket. I suggest that you say instead ``We analyzed the existing taxonomy and some real attacks and concluded that two-three additional categories would be needed'' --- here the `we' is `Socket'. But to extend a taxonomy you must argue that these don't fit into the existing taxonomy. Right now you are just saying ``because we said so''. Why is impersonation not captured already? Why not compound? I think that `command squat' in particular looks like it is described by the existing taxonomy --- the difference is not in how the text is changed (add/remove a hyphen) but in which thing is similar (another package or another ecosystem construct like the command line installer's options). I think you should just drop command squat, it seems hard to defend taxonomically and your system doesn't support it anyway right?.}
We analyzed the existing taxonomy and real-world attack patterns observed in production data by our industry partner.
We found that in registries with hierarchical naming conventions, two additional categories occur:
% To get a better understanding of the SOTA typosquatting attacks, we shared the existing taxonomy with a software supply chain security company team and sought their feedback on gaps in the current taxonomy, based on their observations of emerging threats across multiple SPRs.
% Following discussions with their team, we extended the package confusion taxonomy introduced by Neupane \etal~\cite{neupane2023beyondTyposquatting}, adding two new categories specifically targeting hierarchical naming systems (\cref{tab:typosquat_taxonomy}). These extensions help to capture additional attack vectors that exploit the structural complexities of modern SPRs:

\begin{itemize}[leftmargin=*, itemsep=0.1ex]
  \item \textbf{Impersonation Squatting.} Impersonating a legitimate maintainer or organization by registering a deceptively similar \texttt{author} or \texttt{groupId}.
  %, then publishing malicious packages with well-known project names.
  For instance, \texttt{meta-llama/Llama-2-7b-chat-hf} vs. \texttt{facebook-llama/Llama-2-7b-chat-hf}~\cite{protectai_huggingface}.
  \item \textbf{Compound Squatting.} Making multiple coordinated edits to the hierarchical name, such as altering both scope and delimiters at once. For example, \texttt{@typescript-eslint/eslint-plugin} becoming \texttt{@typescript\_eslinter/eslint}. %, merging multiple confusion tactics into a single new name.
  % \item \textbf{Command Squatting.} For example, npm package \texttt{help} (\texttt{npm i help}) impersonate the command line argument for help (\texttt{npm i --help}).
\end{itemize}

\subsection{Analysis of Engineering Practices}
\label{sec:ProblemState-CaseStudy2-FP}
% \subsection{Case Study 2: False-Positive Analysis}

% \WJ{I don't think FP data is accurate but couldn't figure out a better name. Maybe suspicious data?}
% \Andreas{This subsection seems out of place - I would have expected it in \$3, not in a ``Requirements'' section.}
% \subsubsection{Preliminary Analysis}

% \TODO{Move this subsection to appendix and only talk about the taxonomy we identified here.}

% \JD{This material feels very strange here.}

% \subsubsection{Motivation.}
%Name similarity alone fails to capture \emph{stealth attacks}, leading
Prior typosquatting detectors
%, such as the system proposed by Neupane \etal~\cite{neupane2023beyondTyposquatting}, flag packages based solely on naming traits.
have high false positive rates.
To address this, we must know why engineers benignly make packages that resemble typosquats.
%These false positives, defined as \ul{possible typosquatting packages without malware injected}, often include legitimate forks, relocated packages, or developer tests that pose no actual threat to the software supply chain (\cref{sec:ProblemStatement-Definition}).

% \subsubsection{Methodology.}
% \PA{What question are you trying to answer here?}
\paragraph{Method}
% \BC{I think numbers doesn't hold. I remember we had 665 samples and 99\% confidence level}
To understand the causes of such package names,
% \JD{randomly sampled? otherwise you don't get to talk about confidence levels}
we randomly sampled \SampledTypomindFPNum packages from the original \TypomindFPNum false positives reported by Neupane \etal, which have suspicious package names while do not include any malware. This sample size was selected to obtain a confidence level of \FPPrelimDataConfidenceLevel with a margin of error of \FPPrelimDataMarginError on the resulting distribution of causes.

To identify the engineering practices in the existing typosquatting data,
% false-positives as defined in \cref{sec:ProblemStatement-Definition},
we began by having two researchers with expertise in the software supply chain analyze 200 of these packages.
They analyzed each package’s content and metadata (READMEs, maintainers, available source code, etc.).
Each analyst independently proposed a list of engineering practices based on this analysis (codebook) and then they discussed this codebook together to reach agreement on terms and definitions.
To test valididty, they then independently applied this codebook to the 200 packages and measured agreement using Cohen's Kappa~\cite{cohen1960coefficient}.
The initial inter-rater reliability was  \InterRaterAgreement (substantial~\cite{mchugh2012interrater}).
The researchers subsequently discussed to resolve discrepancies and refine their analysis.
Through discussion, they reached consensus on measurable attributes that could indicate malicious intent or the possibility of a stealthy attack.
% They also identified false-positive categories and additional categories for true-positives.

\iffalse
Based on the initial analysis, the researchers proposed an automated filter to analyze factors such as the presence of a README file and the similarity of package purposes.
This filter was developed with \texttt{gpt-4o} as a filter.
We provide implementation details including our prompt in Appendix~\ref{Appendix-LLMFilter}.
\fi

Based on the high agreement in this process, one of these researchers analyzed the remaining 465 packages, assisted by a tool developed by both researchers. %\footnote{Details of this tool are in Appendix~\ref{Appendix-LLMFilter}.}

\iffalse
assisted by the LLM filter, mapping non-malicious factors to identify potential metadata cues for distinguishing legitimate from malicious naming overlaps.
\fi

\paragraph{Results}
% \JD{Need to place some of the results here to illustrate, I left placeholders.}
This analysis identified eight measurable attributes indicative of malicious intent or stealthy attack potential. These attributes include factors such as a distinct purpose, adversarial package naming, and the comprehensiveness of metadata, including README files.
Due to space limitations, we have placed the detailed findings in \cref{Appendix-taxonomy}.

This manually labeled dataset was also used as the evaluation dataset in \cref{sec:Eval}.
We will include the dataset in our artifact (\cref{sec:OpenScience}).

%\cref{fig:FP-taxonomy} presents the taxonomy for false positives and true positives in package pairs with similar names.
%The remaining data was deemed reliable, eliminating the need for further review.

\iffalse
Our analysis highlights the need for improved categorization methods to minimize the false-positive rate and effectively classify stealth typosquatting attacks. To address this, we propose an additional requirement:

The frequency of each category is summarized in Appendix~\ref{Appendix-LLMFilter}.
\fi

\subsection{Refined Typosquat Definition}
\label{sec:ProblemStatement-Definition}

Integrating these findings we obtain:

\GKT{Did I ever tell anyone how much I love these boxes? So glad that we are continuing to use them!}
\begin{tcolorbox}[title=Definition: Typosquatting Threat]
\textbf{A typosquatting threat} is the creation of a software package whose name mimics a popular, trusted resource, with the intent of deceiving developers into installing code that is actively malicious \ul{or may become so (stealth)}.
\end{tcolorbox}

\GKT{Thinking as a reviewer, I don't like the following sentence. Your definition is clearly stating "deceiving developers into installing code that is actively malicious". Soulds a lot like malware to me. Nevertheless, your points are valid but I'd consider replacing "or may become so" with "being part of the preconditions leading to malware". THis would likely save a few sentences below the box.}
Under our definition, the current absence of malware does not imply future innocence.
In contrast, prior research on typosquatting supposes that typosquat packages always contain malicious code~\cite{taylor2020spellbound,neupane2023beyondTyposquatting}.

Without being able to use malware signatures or CVEs to pinpoint active typosquat attacks, our inclusion of intent raises the possibility of high false positive rates.
Our analysis of false positives from prior lexical analyses (\cref{sec:ProblemState-CaseStudy2-FP}) found usable signals to distinguish benign engineering practices from likely-stealthy typosquats.

%As a result, typosquat attacks are counted as ``true positives'' only if they contain or deliver explicit malicious payloads.
%An attacker could publish a benign fork that adopts a misleadingly similar name, only to introduce malicious code in a later version.
% Hence, \emph{our system flags name-based threats even when immediate harm is absent}.

%\paragraph{Comparisons with definitions in prior works:}
%We distinguish our refined definition with those used by other typosquatting detection research.

\iffalse
\textit{Prior works focus on malicious \ul{content}:}
% \BC{change content => malicious content}
Prior research on studying or detecting typosquatting~\cite{taylor2020spellbound, neupane2023beyondTyposquatting} typically assume typosquatt packages always contain malicious code.
As a result, attacks are counted as “true positives” only if they contain or deliver explicit malicious payloads.
While this approach pinpoints dangerous packages, it can miss harmful \emph{intent}, such as a name that mimics a popular project but has not yet weaponized its code.
Our preliminary analysis in \cref{sec:ProblemState-CaseStudy1-TP} demonstrate the occurrence of these threats in the popular package registries.
% Additionally, a initially benign package could later be compromised by an attacker.
% \PA{Do you have an example to show that this is important?}

\textit{Our work focuses on malicious \ul{intent}:}
% \BC{change intent => malicious intent}
In this work, we expand the definition of a typosquatt threat to cover any \emph{malicious intent} in the package naming strategy itself --- even if the code is not yet malicious:
% \PA{I'll be interested in seeing how you define and measure malicious intent later on.}



\paragraph{False Positives in This Definition:}
One risk introduced by this refined definition is the potential of high false positives due to the lexical similarity of benign package names.
For instance, programming practices like forking frequently result in similar package names~\cite{hadian2022exploringTrendsandPracticesofForks, jiang2023PTMNaming} and a naive implementation of typosquatting threat detection will flag legitimate forks as threats in the software supply chain.
Therefore, we need a \textit{smarter} approach to detect malicious intent between packages of lexically similar names.


With this in mind, we now
clarify how our system determines ``threat''.
We assess various signals
  ---
  similarity of its name to popular packages,
  unusual publisher activity,
  and potential overlaps with known malicious tactics
  ---
  to determine whether a package is \emph{deliberately} masquerading as a legitimate resource or if users should be alerted to the possibility of a potential compromise.
This multi-factor assessment establishes what we call an \emph{threat threshold}:
  packages exceeding that threshold (\ie strongly suggesting deceptive or harmful motivation) are flagged as typosquatting attempts.

% \PA{This definition of false positive sounds very subjective though. I'll be interested in seeing how sound your operationalization and measurements are.}
A flagged package constitutes a false positive if it \emph{does not meet our threshold for malicious intent}—in other words, if it lacks both demonstrably harmful code and any strong indication that it is poised for future malicious use.
Typical examples include legitimate forks, test packages, or brand extensions that resemble well-known projects but pose no real threat.

\begin{tcolorbox}[title=Definition of A False-Positive in This Work]
\textbf{False Positive:}
% \WJ{We consider "packages that is not evidently designed for malicious intent but could be compromised and deceive users as true-positive (with a lower risk level)."
% How can we  modify the definition here to indicate this?}
A package
% flagged by our system
that is neither clearly malicious nor evidently risky for a stealth attack.
\end{tcolorbox}

\noindent
We acknowledge that this definition of false positives introduces subjectivity. To mitigate this, we detail our data-driven design process in \cref{sec:SystemDesign-Step5}.

\fi

\iffalse

\section{Study Context and Threat Model}
\label{sec:StudyContextandThreatModel}
%-------------------------------------------------------------------------------

% \PA{It is unclear how characterizing SPRs relate to threat model.}
% \WJ{Maybe the section heading should be "Study Context and Threat Model"?}
% \WJ{Should we move the ecosystem context into background? Then in the background we have a new subsection for "package registries and supply chain"}
% \JD{Yes, please do. That will help with some of the issues in \$2.}
% \Andreas{I suggest a sentence here motivating this section. Why is a study needed?}
\BC{Change ``encompassing multiple registries'' with ``encompassing multiple ecosystems'', maybe?}
Although typosquatting attacks have long been a persistent issue, effective defense techniques for mitigating them in production environments within SPRs remain scarce. As an initial effort, we developed a system in collaboration with a software supply chain security company that operates an SPR-like platform encompassing multiple registries.
In this section, we present the context of our study by characterizing SPRs (\cref{sec:registry-model}), outlining attacker capabilities (\cref{sec:threat-actor-capabilities}), and summarizing the threat model in this work (\cref{sec:ThreatModel}).

% \Andreas{After reading this section, I'm left wondering whether ``stealth'' typosquatting is a real problem, or hypothetical. Can you motivate the need for a solution for these attacks in this section? I understand that a key distinguishing feature of your solution vs prior work is finding these stealth packages, so I think it should be motivated.}

% \PA{I think you may want to use a more recognized term here (\eg System Model). Then the section heading can be "System and Threat Model". Also, this subsection feels like background information to me.}
\subsection{Ecosystem Context and Registry Model}
\label{sec:registry-model}





% We consider six major public registries that store code libraries or AI models, each with different policies, naming conventions, and exposure to typosquatting. Single-level naming (\eg npm) identifies packages with a single string, whereas hierarchical conventions
% % (\eg Maven’s \texttt{groupId:artifactId}, Hugging Face’s \texttt{author/model})
% grant more flexibility but also expand the attack surface. In addition to basic character-based typosquatting, we extended the existing package confusion taxonomy created by Neupane \etal~\cite{neupane2023beyondTyposquatting} with two additional categories. Attackers can exploit \emph{Impersonation Squatting} and \emph{Compund Squatting} within hierarchical naming structures (\cref{tab:typosquat_taxonomy}):

% \PA{Are you the one identifying these typosquatting types? Or are they part of an existing taxonomy? It is not clear to me here.}
% \WJ{Making it clearer here.}
\PA{Do you want to add a sentence or two about how these additional attack patterns were identified. I am wondering if these are all the new patterns we should be concerned about (completeness).}
\BC{Which risks?}
We extend the existing package confusion taxonomy introduced by Neupane \etal~\cite{neupane2023beyondTyposquatting}, adding two new categories specifically targeting hierarchical naming systems (\cref{tab:typosquat_taxonomy}). These extensions help to capture additional attack vectors that exploit the structural complexities of modern SPRs:

\begin{itemize}[leftmargin=*, itemsep=0.5ex]
  \item \textbf{Impersonation Squatting.} Impersonating a legitimate maintainer or organization by registering a deceptively similar \texttt{author} or \texttt{groupId}, then publishing malicious packages with well-known project names. For instance, \texttt{meta-llama/Llama-2-7b-chat-hf} vs. \texttt{facebook-llama/Llama-2-7b-chat-hf}~\cite{protectai_huggingface}.
  \item \textbf{Compound Squatting.} Making multiple coordinated edits to the hierarchical name, such as altering both scope and delimiters at once. For example, \texttt{@typescript-eslint/eslint-plugin} becoming \texttt{@typescript\_eslinter/eslint}, merging multiple confusion tactics into a single new name.
\end{itemize}

Popularity metrics, which indicate the usage or visibility of packages, also vary across ecosystems. Registries like npm, PyPI, RubyGems, and Hugging Face provide direct popularity indicators such as download counts or star ratings. In contrast, Maven and Golang often require external sources to assess package popularity, making them less transparent. These inconsistencies in visibility make it easier for attackers to exploit less-monitored areas of the ecosystem, where suspicious activities may go unnoticed.



\iffalse
\subsection{Model of Ecosystem Context / Software Package Registries}

\JD{Seems like there should be a table here to summarize the registries of interest. This subsection should open by referring to the table. Do you unify these specific examples into a general statement of an SPR? Right now the section does not, but I think you could open with ``Here's our model of an SPR. It's based on our analysis of X registries, summarized in Table Y.'' The rest of the section would then expand on the dimensions included in the table, possibly grouping by dimension (with per-registry analysis) or by registry (going through each dimension, like you currently do). Not sure which one makes more sense.}

\iffalse
\cref{Fig:TODO} shows the open-source package ecosystems or software package registries.

\paragraph{Maven}

\paragraph{Golang}
\href{https://portswigger.net/daily-swig/suspicious-finds-researcher-discovers-go-typosquatting-package-that-relays-system-information-to-chinese-tech-firm}{Suspicious finds: Researcher discovers Go typosquatting package that relays system information to Chinese tech firm}

\href{https://michenriksen.com/archive/blog/finding-evil-go-packages/}{Finding Evil Go Packages
}
"The novel Dependency Confusion attack vector is luckily not something a Go developer has to worry about since the source is always explicitly specified when importing a package, so when Go fetches the external dependency, it can’t be confused about where to fetch it"

\paragraph{Hugging Face}
\fi

\paragraph{Maven:}
Maven is a widely adopted build automation and dependency management tool for Java projects. It relies on a centralized repository system, with Maven Central being the primary public repository. Packages in Maven are uniquely identified by a combination of \texttt{groupId}, \texttt{artifactId}, and \texttt{version}, forming a coordinate that specifies the exact package. The \texttt{groupId} typically represents the organization or author, while the \texttt{artifactId} denotes the specific project or library name. This hierarchical naming convention facilitates organized package management but also introduces potential vectors for typosquatting attacks, targeting either the \texttt{groupId}, the \texttt{artifactId}, or both.

\paragraph{Golang:}
Golang, commonly known as Go, is an open-source programming language developed by Google. The Go ecosystem uses a decentralized module system where packages are retrieved directly from their version control system (VCS) repositories based on import paths that often resemble URLs. Developers explicitly specify the full path to the package, including the domain, repository owner, and package name. This explicit specification reduces the risk of dependency confusion attacks because Go fetches dependencies from the specified locations without ambiguity. However, it opens up possibilities for typosquatting through malicious packages hosted at deceptive domains or repositories with similar names. Recent reports have highlighted such threats, including the discovery of Go packages that relay system information to unauthorized servers~\cite{portswigger2021suspicious} and methodologies for identifying malicious Go packages~\cite{henriksen2021finding}.

The inherent trust in the specified import paths means that if an attacker controls a domain or repository with a misleading name, they can potentially distribute malicious code to unsuspecting developers. This scenario underscores the importance of verifying the authenticity of package sources in Go.

\paragraph{Hugging Face: }
Hugging Face is a leading platform for open-source pre-trained AI models~\cite{Jiang2022PTMReuse, jiang2024peatmoss}, providing repositories for sharing pre-trained models and datasets. Packages in Hugging Face are identified by a combination of \texttt{author} (or organization) names and \texttt{model} or \texttt{dataset} names, forming a hierarchical structure similar to that of Maven. This setup allows for efficient organization and discovery of resources but also presents opportunities for typosquatting attacks. Malicious actors can create models or datasets with names that are slight variations of popular ones or impersonate well-known authors by using similar author names, thereby tricking users into downloading compromised resources.

The open nature of the platform encourages community contributions but also requires vigilance to ensure that shared models and datasets are trustworthy. As machine learning models are increasingly integrated into applications, the risks associated with malicious or tampered models become more significant.
\fi

\subsection{Refined Definition and Novelty}
\label{sec:ProblemStatement-Definition}
\paragraph{Prior work focusing on \ul{content}:} \BC{change content => malicious content}
Prior research identifies ``package confusion'' by comparing package names to well-known libraries or models~\cite{taylor2020spellbound, neupane2023beyondTyposquatting}. Attacks are commonly counted as “true positives” only if they contain or deliver explicit malicious payloads. While this approach pinpoints dangerous packages, it can miss harmful \emph{intent}, such as a name that mimics a popular project but has not yet weaponized its code. Additionally, a initially benign package could later be compromised by an attacker.
% \PA{Do you have an example to show that this is important?}

\paragraph{Our work focusing on \ul{intent}:} \BC{change intent => malicious intent}
We expand the criteria for what constitutes a true threat by prioritizing \emph{malicious intent} in the naming strategy itself—even if the code is not yet malicious:
% \PA{I'll be interested in seeing how you define and measure malicious intent later on.}

\begin{tcolorbox}[title=Definition: Typosquatting Threat]
\textbf{A typosquatting threat} is the deliberate creation of a package (or AI model) with a name that closely mimics a popular, trusted resource, done specifically to deceive developers into installing code that either is malicious, \ul{or stealth (will become so)}.
\end{tcolorbox}

Under this definition, the absence of malware does not imply innocence.
An attacker could publish a benign fork that adopts a misleadingly similar name, only to introduce malicious code in a later version.
Hence, \emph{our system flags name-based threats even when immediate harm is absent}.

\paragraph{False Positives in This Definition:}
Clearly, this definition may result in many false positives based solely on the lexical similarity of package names. This outcome is an inherent challenge given the nature of open-source software development.
For instance, forking is a common practice in the open-source ecosystem, often resulting in similar package names~\cite{hadian2022exploringTrendsandPracticesofForks, jiang2023PTMNaming}.
\BC{OK, I think we should argue that "forking is benign." If we say that attackers can exploit forks, then they are not FPs anymore.}
While such practices are typically benign, malicious attackers can exploit forks to introduce malware into the ecosystem, either directly or by compromising the authors of existing forks~\cite{cao2022WhatThefork}.

With this in mind, we now
clarify how our system determines ``threat''.
We assess various signals
  ---
  similarity of its name to popular packages,
  unusual publisher activity,
  and potential overlaps with known malicious tactics
  ---
  to determine whether a package is \emph{deliberately} masquerading as a legitimate resource or if users should be alerted to the possibility of a potential compromise.
This multi-factor assessment establishes what we call an \emph{threat threshold}:
  packages exceeding that threshold (\ie strongly suggesting deceptive or harmful motivation) are flagged as typosquatting attempts.

% \PA{This definition of false positive sounds very subjective though. I'll be interested in seeing how sound your operationalization and measurements are.}
A flagged package constitutes a false positive if it \emph{does not meet our threshold for malicious intent}—in other words, if it lacks both demonstrably harmful code and any strong indication that it is poised for future malicious use.
Typical examples include legitimate forks, test packages, or brand extensions that resemble well-known projects but pose no real threat.

\begin{tcolorbox}[title=Definition of A False-Positive in This Work]
\textbf{False Positive:}
% \WJ{We consider "packages that is not evidently designed for malicious intent but could be compromised and deceive users as true-positive (with a lower risk level)."
% How can we  modify the definition here to indicate this?}
A package
% flagged by our system
that is neither clearly malicious nor evidently risky for a stealth attack.
\end{tcolorbox}

\noindent
We acknowledge that this definition of false positives introduces subjectivity. To mitigate this, we detail our data-driven design process in \cref{sec:SystemDesign-Step5}.

\iffalse
\subsection{Observed Attacker Behavior}
% Attacker Tactics and Capabilities}
\label{sec:threat-actor-capabilities}
\WJ{Merge this with \$2.3 and cut this.}
% \WJ{This subsection has overlap with threat model.}
% \Andreas{I \textit{think} that the purpose of this section is to describe how attackers have been observed to use typosquatting attacks. Perhaps rename to ``Categorizing Observed Attacker Abuses of Registries'' or similar? The current name sounds like ``Threat Model'' without saying threat model. This may also be background to frame the problem?}
% \TODO{Add more references}
\cref{fig:ThreatModel} illustrates the workflow of a typical typosquatting attack within the software supply chain. Our threat model assumes that attackers can \textit{publish} new packages or \textit{hijack} existing packages in targeted SPRs by leveraging multiple accounts or creating misleadingly named organizations. To gather insights, we conducted a systematic web search using the keyword ``\textit{typosquatting attack AND software package}''. We analyzed the top 50 results published from 2022 to 2024, and summarized the attackers' behavior.

\PA{Hi Wenxin, do you have counts for these objectives and approaches? I think providing counts will demonstrate more transparency.}
The primary objectives of these attacks can be categorized as follows:

% \Andreas{How do you come up with this list of behaviors? Is it exhaustive? Are these from reports of attacker behaviors?}
\setlist[enumerate]{leftmargin=1.2em, itemsep=0.3ex}
\begin{enumerate}
% [leftmargin=*, label=(\arabic*)]

    \item \textbf{Malware Injection~\cite{lyons2024typosquatting, regeski2024typosquattingTargetsPythonDevelopers}:}
    Attackers exploit subtle alterations to package names to deceive users into downloading \ul{\textit{malicious content}}. In single-level registries, these tactics include minor typographical modifications, such as insertions, deletions, or substitutions. For hierarchical registries, strategies extend to author impersonation and reordering of package names, misleading users into installing harmful or incorrect software.


    \item \textbf{Stealth Typosquatting Threat~\cite{zornstein2023stealthy_typosquatting}.}
% \WJ{Maybe ``sleepy'' is not the right word here since I didn't find a good citation...}
% \JD{I don't think there is a term in the literature. I think ``sleeper'' or ``stealth'' would be fine, but I guess `defensive typosquat' is also present in the literature so why not use that?}
\PA{I am not sure these two citations here best represent the prose. They point to stealthier typosquat attack but the uploaded packages seem to be malicious from the start.}
    Attackers create or compromise packages with \ul{\textit{malicious intent}}, explicitly aiming to deliver harmful payloads.
    A common tactic involves ``stealth'' attacks
    % --- also referred to as \textit{defensive typosquatting}~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting} \JD{I'm confused, why would anyone call this behavior `defensive'? Can you reply to this comment and explain?} ---
    where packages initially appear benign to evade detection but are later updated with harmful content.
    % \JD{The next sentence describes behavior that is out of scope for this project. It is a supply chain attack but not typosquatting. So I think it should be omitted here. Could go in Discussion or Limitations or Threat Model.}
    % Similarly to the \texttt{event-stream} incident~\cite{npm2018eventstream}, attackers may leverage a similar process to compromise trusted packages by targeting their maintainers, either through credential theft or coercion~\cite{nagra2009surreptitious, kaplan2021surveyonCommonThreatsinNPMPyPI}.
    Unlike direct malware injection, these attacks often involve deploying a package with a name similar to a legitimate one, exploiting the proximity in the namespace to target the original package and its users.



\end{enumerate}

\noindent
The key attacker approaches are as follows:
% \WJ{We are focusing on detecting the two kinds of activities but our work is not directly relevant to the approaches. Should we cut it?}
% \JD{Well, with security the reader always wants to know if the attacker can bypass your defense (They always can, so this question might be asked more usefully as ``how much more it would cost them compared to the absence of the defense''). So knowing the typical techniques seems like it would be helpful for analyzing the security properties you achieve. You would typically do that analysis as part of \$5 or \$6, with subsection titled  ``Security Analysis'' that gives a conceptual/theoretical discussion of how much of the attack surface your system is addressing and how easy it might be to bypass.}
% \Andreas{What is the difference between a ``tactic'' and ``approach''? I think that what you're describing are: attacker goals (deliver malware to victim), approaches to achieve the goal (malware injection, stealth typosquatting), and techniques the attacker employs to actualize the approach (name deception, etc.).}

\begin{enumerate}
    \item \textbf{Package Name Deception.}
    Attackers create package names that closely resemble legitimate ones by introducing minor typographical errors, such as misspellings or character substitutions. This strategy exploits users' typing mistakes to trick them into inadvertently downloading malicious packages instead of the intended legitimate ones.




    \item \textbf{Popularity Manipulation.}
    To enhance the credibility of malicious packages, attackers artificially inflate metrics like download counts, stars, and forks. Techniques such as "download farming" and "citation farming" are used to make these packages appear more trustworthy and widely adopted~\cite{he2024FakeStars}.


    \item \textbf{Exploiting Repository Trust.}
    Attackers leverage the inherent trust in repositories by spoofing legitimate domains or manipulating import paths. For example, in Golang ecosystems, they may register domains similar to trusted ones to deliver malicious code. On platforms like Hugging Face, attackers publish trojaned models under authoritative-sounding author names to deceive users into installing harmful content.



    \item \textbf{Dependency Confusion.}
    Dependency confusion is a supply chain attack where attackers publish malicious packages to public registries using names identical to an organization’s internal or private packages. When build systems are misconfigured to prioritize public sources, they inadvertently download and execute these malicious versions, allowing attackers to inject harmful code into the organization’s software~\cite{birsan2021dependencyconfusion}.
\end{enumerate}

% \paragraph{Constraints on the Attacker.}
% We assume attackers cannot forcibly modify legitimate packages owned by others, nor can they directly falsify official popularity statistics at the registry backend. However, ecosystem policies often lack stringent checks on new package names, enabling a range of creative “look-alike” or “sound-alike” naming attacks.

% \paragraph{Security Implications.}
% A successful typosquat in any of these registries can compromise user data, introduce cryptominers, or sabotage AI-based outcomes—especially troubling when integrated deep into CI/CD pipelines or enterprise ML workflows. Through consistent naming deceptions, adversaries exploit both user oversight and automation-driven dependency fetching, making name-based attacks a sustained problem despite improved authentication and scanning practices.

\fi

\subsection{System and Threat Model}

We describe the system we secure and the threats covered in this work.

\textbf{System Model:}
We focus on software package registries, including registries hosting traditional software packages (\eg NPM) and pretrained AI models (\eg Huggingface).
These registries allow users to publish and share software artifacts (traditional packages and pretrained models) with other users, thereby facilitating software reuse.
The number of packages hosted in these registries, as shown in \cref{tab:registry-overview}, demonstrate their popularity across software development communities.

\textbf{Threat Model:}
Our threat model focuses on attackers who can publish packages to software package registries and use the published package to deliver malicious code to unsuspecting users and applications.
We include some threats and exclude others.
\begin{itemize}
    \item \textit{In-scope:} We consider attacks where packages with deceptively similar names to legitimate packages are published in the software package registry. However, the published package may or may not contain any malicious payload.
    \item \textit{Out-of-scope:} We do not consider attackers that either directly publish malicious packages with new names or compromise existing legitimate packages.
    These attacks are mitigated by the broader software supply chain security measures~\cite{okafor_sok_2022}.
\end{itemize}
This threat model is substantially stronger than those of existing typosquat detection techniques that only considers typosquat packages with malicious code.
This threat model also protects against currently benign typosquat packages that can introduce malicious code in the future.
\cref{fig:TPAnalysis} demonstrates the viability of this threat class as malicious code can be introduced months after the original package is published, and after it may have gotten tens or hundreds of downloads.

\iffalse

\subsection{System and Threat Model}
\label{sec:ThreatModel}
% \subsection{Why Existing Solutions Fall Short}
% \label{sec:ProblemStatement-SOTALimitation}

% \JD{This probably needs to come earlier.}
% \JD{Give two examples, early on --- back in \$3 after you give the threat model. One example should be a typosquat that was malicious from the start (or near its creation, anyway), and the other should be a typosquat *that became malicious later on*. That will make the description of the gap in prior work here more compelling, as well as motivating the problem definition that you provide.}
% \WJ{Is the description of tactics in \cref{sec:threat-actor-capabilities} concret or we need more specific example, like providing a list of packages?
% }

% \PA{Looks like the next couple words are hanging}
Prior research highlights several key shortcomings of SOTA typosquatting detection systems, including high false-positive rates, limited adaptability, and insufficient utilization of contextual information.
and needs a generic approach for
We aim to enhance the infrastructure for SPRs and outline the specific adversarial tactics associated with typosquatting that our system seeks to mitigate as follows:

\paragraph{System Model:} \Andreas{I think "System Model" should define the system that you aim to defend, not the security solution that you implement. What are the properties of the system that you aim to protect?}
We propose a system designed to support software package registries (SPRs) in identifying and mitigating typosquatting threats through backend operations. Existing solutions predominantly focus on frontend interactions, such as command-line interface (CLI) tools for end-users~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}. While these solutions are effective in assisting users at the point of package installation, backend-focused tools --- requiring much higher accuracy alongside acceptable latency and throughput --- tailored for package registry maintainers remain significantly underexplored.

Our system addresses this gap by providing a backend typosquatting detection system that prioritizes accuracy and efficiency while maintaining low false-positive rates. This system is designed to integrate seamlessly into registry workflows, enabling proactive detection of suspicious packages before they reach end-users. By leveraging this backend capability, SPRs can bolster their defenses against typosquatting threats and better protect their ecosystems.




\paragraph{Threat Model:}

Our threat model considers adversaries targeting SPRs to distribute malicious software packages. Specifically, adversaries who attempt to publish malicious packages to the registries and coerce users to install them by employing typosquatting techniques.

Our threat model considers adversaries targeting SPRs through typosquatting attacks, as well as stealth threats in the ecosystems. The attackers register or hijack packages with deceptively similar names to legitimate ones, aiming to exploit user mistakes or trust to introduce malicious code into the software supply chain.
Our threat model in this work is defined as follows:

\begin{itemize} [leftmargin=*, itemsep=0.1ex]
    \item \textit{In scope:} This includes immediate maliciousness from typosquatting attacks where attackers use package name deception to deliver harmful payloads. It also covers deferred maliciousness, where packages appear benign initially but could introduce malicious updates later, as well as limited cases of popularity manipulation to enhance the credibility of malicious packages.
    \item \textit{Out of scope:} The model does not address metadata manipulation, such as altering descriptions or version histories, nor does it cover attacks targeting high-profile maintainers or organizations through phishing or account compromise. Broader supply chain attacks, including dependency confusion and substitution, also fall outside the scope of this work.
\end{itemize}
% \PA{Is the lack of production-focused insight a design or implementation limitation? If design, you should discuss how. If implementation, it may not be interesting to emphasize.}
This threat model supports SPRs from the backend by detecting and mitigating typosquatting threats, including immediate maliciousness through package name deception, deferred maliciousness via delayed updates. Our system aims to (1) raise alarms for both immediate and deferred threats for SPR users and (2) provide SPRs with a focused list of typosquatting threats, enabling efficient monitoring and scanning of suspicious packages. While addressing typosquatting-specific risks, the model excludes broader supply chain attacks like dependency confusion, substitution, and metadata manipulation.

\iffalse

\fi

\subsection{Model of Attacker Capabilities}
% \label{sec:ThreatModel-AttackerCap}

\JD{This section does not define the attacker's capabilities. Need to rewrite. Here are some of the capabilities you should include: (1) Can the attacker publish their own modules? (2) Can the attacker modify existing modules they did not originally publish? (3) Can the attacker manipulate popularity metrics, and if so, to what extent? (this one is very important since it is a sometimes-implicit assumption in prior work that I think is actually a bit questionable...let's write it down and then in the Discussion we can reflect on how plausible the assumption is, it would be nice to be able to cite work on download farms and citation farming etc.) (4) Can the attacker access the source code etc of popular modules?}
Attackers can exploit ecosystem vulnerabilities even with security measures like Two-Factor Authentication (2FA) and Automated Dependency Scanning in place. Common strategies include hiding malicious content within packages or employing "stealth attacks," \JD{are you defining this term? there should be a citation I think} where a benign package is published initially and malicious code is introduced later. These methods are often used for cryptojacking, gaining unauthorized network or shell access, and other forms of exploitation~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, taylor2020spellbound}.

One prevalent tactic is typosquatting, where attackers create package or author names resembling popular ones by introducing minor typographical errors. This includes both author name typosquatting (\eg using \texttt{expresss} instead of \texttt{express}) and package name typosquatting (\eg \texttt{lodashs} instead of \texttt{lodash}). These techniques rely on human error and automated scripts to deceive users into installing malicious packages~\cite{neupane2023beyondtyposquatting, moubayed2018dnstyposquat}. Another related tactic is dependency confusion, where attackers publish packages mimicking internal dependencies, exploiting build systems that inadvertently fetch public versions instead of intended private ones~\cite{koide2023phishreplicant}.



\iffalse
%%%%%

\subsection{Existing Threats}

\TODO{Use the theory from Typomind paper~\cite{neupane2023beyondTyposquatting}}

\subsection{Additional Threats}
\TODO{Define additional threats specifically for author name typosquats which happens more on Maven, Golang, and HF because their package name include author names.}
\fi
\fi


\fi

% \section{Problem Statement and Requirements}
\section{Threat Model and System Requirements}
\label{sec:ThreatModelandSysRequirements}
% \JD{These opening remarks feel repetitive. Open more directly with ``Our goal is to...''}
% Typosquatting attacks capitalize on naming similarities to deceive developers into installing malicious or soon-to-be malicious packages. Prior work has examined string-based detection~\cite{taylor2020spellbound, neupane2023beyondTyposquatting}, but it typically measures success by detecting packages that \emph{already contain} harmful code, overlooking the central issue of \textbf{``stealth'' typosquats}.
Our goal is to tackle the challenges of high false-positive rates and limited registry coverage while enhancing the ability to detect stealthy typosquatting attempts.
This section defines our system and threat model (\cref{sec:SystemandThreatModel}), and articulates our system requirements (\cref{sec: ProblemState-SysReq}).





% \WJ{Should we talk about the accuracy and FP rate in separate requirements? Currently they are both in req1 because both are under accuracy requirement.}
% \JD{I think they should stay in the shared requirement, it will seem silly to split them. Everyone knows this requirement.}

\subsection{System and Threat Model}
\label{sec:SystemandThreatModel}

We describe the system we secure and the threats covered in this work.

\textbf{System Model:}
We focus on software package registries, including registries hosting traditional software packages (\eg NPM) and pretrained AI models (\eg Huggingface).
These registries allow users to publish and share software artifacts (traditional packages and pretrained models) with other users, thereby facilitating software reuse.
The number of packages hosted in these registries, as shown in \cref{tab:registry-overview}, demonstrate their popularity across software development communities.

\textbf{Threat Model:}
Our threat model focuses on attackers who can publish packages to software package registries and use the published package to deliver malicious code to unsuspecting users and applications.
We include some threats and exclude others.

\begin{itemize}[left=0.1cm]
    \item \textit{In-scope:} We consider attacks where packages with deceptively similar names to legitimate packages are published in the software package registry. The attacker may initially publish with non-malicious content (stealthy) and later introduce malware (active).
    \item \textit{Out-of-scope:} We do not consider attackers that either directly publish malicious packages with new names or compromise existing legitimate packages.
    These attacks are mitigated by the broader software supply chain security measures~\cite{okafor_sok_2022}.
    % We also exclude \texttt{command squatting} from our scope, as this attack targets registry commands, which fall outside the bounds of our defined focus.
\end{itemize}

% \JD{This is a strong claim so I want to be sure that it is true. Please check it.}
\textit{This threat model is substantially stronger than those of existing typosquat detection techniques}~\cite{vu2020typosquatting, taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}.
% \JD{Cite the specific papers here}.
They only consider typosquat packages with malicious code (\ie active threats, not stealthy ones).
%This threat model also protects against currently benign typosquat packages that can introduce malicious code in the future.
\cref{fig:TPAnalysis} demonstrates stealthy threats in the wild, with malicious code introduced months after the original package is published. %, and after it may have gotten tens or hundreds of downloads.


\subsection{System Requirements}

\GKT{Wenxin, I like what i see here but do have a question. If we are reorganizing this as an ICSE submmission, should we not consider formulating proper RQs? Or is this some sort of experience/industry track where this is not expected?}
\label{sec: ProblemState-SysReq}
Effective detection systems for package registries must meet both security and operational goals. These requirements ensure robust identification of malicious intent, compatibility with diverse ecosystems, and practical performance for real-world deployment. A production-ready system must address the following key requirements:


\paragraph{\ul{Req$_{1}$}: High Accuracy and Low False-Positive Rate}
A typosquat detection system must achieve high accuracy, effectively capturing true positives while maintaining a low false-negative rate
Additionally, to avoid “alert fatigue”\cite{ahrq_alert_fatigue} and protect the reputation of influential open-source contributors, the system must minimize unnecessary alerts, aiming for a false-positive rate of only a few percent. Packages with similar names, such as legitimate forks\cite{wyss2022whatTheForkFindingHiddenCodeClonesinNPM}, should not be flagged unless they exhibit clear signs of malicious intent.

\paragraph{\ul{Req$_{2}$}: Efficient and Timely Detection}
The system must be able to handle large registries, ensuring scalability
while providing low-latency, on-demand checks for real-time feedback.
% It must deliver acceptable throughput and latency to enable large-scale scans and real-time classification, all while ensuring cost-effectiveness.

\paragraph{\ul{Req$_{3}$}: Compatibility Across Ecosystems}
The system must provide support for diverse naming schemes, encompassing both one-level and hierarchical naming conventions, along with comprehensive metadata integration.
% from single-level systems
% (\eg npm, PyPI) to hierarchical ones (\eg Maven, Hugging Face).
% The system should efficiently leverage internal and external data sources to retrieve popularity metadata.

\paragraph{\ul{Req$_{4}$}: Frequent Metadata Updates}
The system must regularly ingest data from sources like package registries, version histories, and domain checks to maintain up-to-date threat intelligence. This ensures developers are promptly informed about suspicious packages released since the last scan, independent of popularity metrics.

\paragraph{\ul{Req$_{5}$}: High Recall of Stealth Typosquatting Attack}
The system must identify both active and stealth typosquatting packages.


% \paragraph{Req$_{6}$: Manageable False-Positive Load}
% To prevent “alert fatigue,” robust verification methods such as code similarity checks, publisher identity assessments, or metadata validation must help distinguish genuine threats from legitimate forks. A precision rate of 95\% or higher is typically targeted, adjustable based on an organization’s risk tolerance.

% \paragraph{Req$_{6}$: Cost-Effective Performance}
% The system must optimize throughput and latency to enable large-scale scans and real-time classification while keeping costs manageable.
% For instance, it should handle 1 million packages within six hours on a moderately provisioned server and maintain sub-second response times for CI workflows.



\iffalse
We seek four key guarantees that expand on, and differ from, prior typosquatting defenses:

\PA{Do you want to call these System Requirements? The item does not sound like security guarantees.}
\begin{enumerate}
  \item \textbf{G$_{1}$: High Recall of Malicious-Intent Names.}
    The system must capture both actively malicious packages \emph{and} those evidently designed to confuse or mislead, thereby minimizing false negatives.

  \item \textbf{G$_{2}$: Low False-Positive Rate.}
    Although perfect precision is unattainable, we target minimal disruption to developer workflows. In practice, we strive for a false-positive rate under a few percent to avoid overwhelming teams with unnecessary alerts (\TODO{xx}). Many packages have superficially similar names or are legitimate forks~\cite{wyss2022whatTheForkFindingHiddenCodeClonesinNPM}, so we only flag those that signal genuine malicious intent.

  \item \textbf{G$_{3}$: Efficient and Timely Detection.}
    Our approach must handle large registries (\eg  millions of packages) in a timely manner, such as completing nightly full scans or near-real-time checks for newly published packages. Throughput should be sufficient for scanning $\sim$1M entries overnight on a standard server, and latency should be low enough to provide quick feedback in CI/CD pipelines.

  \item \textbf{G$_{4}$: Compatibility Across Ecosystems.}
    The solution should accommodate single-level (\eg npm, PyPI) and hierarchical (\eg Maven, Hugging Face) naming schemes without assuming uniform metadata or popularity signals. This ensures that partial or external data sources suffice to identify malicious naming trends.
\end{enumerate}

By centering our approach on malicious intent, we go beyond the simpler “contains malicious code” criteria. This reduces the window of exploitation, discourages “stealth attacks”, and addresses the real motive behind suspicious naming.

\subsection{Production Requirements}
\label{sec: ProblemState-Prod}

While the security guarantees outline the system’s detection goals, real-world deployment imposes additional constraints. We enumerate three main requirements:

\paragraph{Req$_{1}$: Frequent Metadata Updates.}
Practitioners need reliable, up-to-date information about newly published packages—even if popularity metrics are incomplete. The system must regularly ingest data from various sources (\eg package registries, version histories, or domain name checks) so that emergent threats are caught early. Critically, the intent is to keep developers informed about suspiciously named packages released \emph{since the last scan}, rather than to incorporate popularity as a direct detection mechanism.

\paragraph{Req$_{2}$: Manageable False-Positive Load.}
High false-positive rates lead to ``alert fatigue'', which often results in developers ignoring warnings~\cite{xx}.
Hence, the system must offer robust verification methods—such as code similarity checks, publisher identity assessment, or metadata validation—to distinguish purposeful name confusion from legitimate forks. The acceptable threshold (\eg 95\% precision) may vary by organization’s tolerance for risk.

\paragraph{Req$_{3}$: Cost-Effective Performance.}
Large ecosystems necessitate scanning millions of packages within a limited time (\eg overnight or in short CI windows). While one could theoretically purchase more compute resources to reduce latency, such an approach is not always financially viable. Thus, the system must balance:
\begin{itemize}
  \item \textbf{Throughput}—how many packages can be processed per hour to handle full ecosystem scans.
  \item \textbf{Latency}—how quickly each new package is classified to block malicious installs in real-time.
\end{itemize}
For instance, a typical target is to process \TODO{1 million packages in under 6 hours on a moderate CPU server}.
% , while also providing sub-second responses to queries in CI environments.
\fi



\iffalse

\JD{The reader wants to know whether, and how, this formulation of problem and requirements is novel/different from prior work. Please be clear about this.}

To protect package users from the threat model we discussed above, we describe the detailed problem statement in this section, including the desired security guarantees (\cref{sec: ProblemState-Security}) and the systems requirements for production (\cref{sec: ProblemState-Prod})

\JD{This part does not clarify what you are changing or why.}
Prior work defined pacakge confusion attack as~\cite{neupane2023beyondTyposquatting}.
In this work, we scope-down the definition:
\begin{tcolorbox}
    A \textbf{typosquatting attack} involves the creation of packages with names that are deliberately similar to those of popular packages, typically with \ul{\textit{malicious intent}}. These packages may either contain active malicious code or be designed to introduce harmful content at a later stage.
    %
    \textit{Incorporating malicious intent into this definition is different from exiting typosquatting defense techniques.}
\end{tcolorbox}
\subsection{Security Guarantees}
\label{sec: ProblemState-Security}

\JD{This subsection currently does not describe security guarantees. What guarantee(s) is your approach providing? (Your guarantee can be probabilistic -- false positive/negative are inevitable, but you should indicate the criterion that Socket has for deployability.}
To protect users from typosquatting attacks, an automated detection mechanism is essential.
A straightforward approach is to identify packages with similar names that have a significant difference in popularity. \JD{Yep, and you should cite the prior work that did so, as well as any challenge like missing popularity info}
\JD{seems to ignore the `malicious intent' part --- that part becomes quite tricky so you should not call it `straightforward'.}

\subsection{Production Requirements}
\label{sec: ProblemState-Prod}
For practical deployment in a production environment, the typosquatting detection system must meet four requirements:
% \JD{``four requirements'' is a lot nicer than ``several key''. If something isn't important, don't mention it in a big way. We assume you are only telling us important things, so don't inflate with adjectives like `key'.}

\paragraph{Req$_{1}$: Regularly Updated Package Metadata:}
An effective detection mechanism must be integrated with a continuously \JD{I don't think `continuous' is possible because computers are discrete? `regular' might be a better term.} updated database that includes the latest package metadata and popularity metrics. This ensures that newly published packages and changes in package popularity are promptly reflected in the detection process.
\JD{I think that including popularity in this paragraph is a mistake --- this is combining the requirements with the solution approach, which is not correct. The requirement exists **only** because users need warnings about the most recent published packages --- that's a ``user'' reason for the requirmeent, as opposed to a ``system'' reason.}

\paragraph{Req$_{2}$: Standardized Support for Popularity Metrics Across Ecosystems:}
Ecosystems like Maven and Golang do not consistently provide direct popularity metrics, such as download counts. To address this challenge, the system must incorporate standardized methods for estimating package popularity across all ecosystems. This may involve aggregating alternative metrics such as repository stars, forks, dependent packages, or leveraging external datasets like \texttt{ecosyste.ms} that provide normalized popularity indicators.
\JD{I think this requirement overlaps with Req$_{1}$, but it's more of a problem than Req$_{1}$. This requirement is solely due to how you designed your specific system, so it's not appropriate to give it as a general requirement for a typosquat technique. You should remove this.}

\paragraph{Req$_{3}$: Minimize False Positives While Maintaining High True Positive Rates:}
In a production environment, particularly within continuous integration (CI) pipelines, a high false-positive rate can be disruptive and lead to alert fatigue \JD{cite}. Many packages may have similar names intentionally or be legitimate forks for customized use cases~\cite{wyss2022whatTheForkFindingHiddenCodeClonesinNPM}. Therefore, the detection system must balance accurately identifying malicious typosquatting packages (true positives) while minimizing the incorrect flagging of legitimate packages (false positives). This requires sophisticated analysis that goes beyond name similarity, potentially incorporating code analysis, publisher reputation, and historical behavior.
\JD{The previous sentence is inappropriate because it is about design, not user requirement. Instead, we want to know ``What false positive rate is acceptable?'' Cite prior work on this point and include Socket's own standard.}

\paragraph{Req$_{4}$: Low Latency and Cost-Effectiveness:}
For integration into CI/CD pipelines and real-time applications, the detection mechanism must operate with low latency to avoid hindering development workflows. Additionally, it should be cost-effective in terms of computational resources and scalability, enabling widespread adoption without imposing significant overhead.
\JD{Not sure about this statement about `widespread adoption'. Presumably it just needs to be cost effective by a single company so they can sell it as a service. This paragraph overall felt a little confused, you are talking about low latency as well as cost effectiveness and I think these are related but not identical. I can buy a bigger computer to get lower latency but this is not cost effective. Maybe the REQ should be `cost effective' and you can talk within it about obtaining low latency without excessive compute resource cost. And again, a specific target from Socket will be helpful: ``throughput or latency of X packages/time unit on a machine with resources Y''. Also note that it's not clear whether what you want is latency or throughput --- which measure is more appropriate (or both?) and why? I presume that latency can be resolved by storing the results in a DB, so then it seems that throughptu is the appropriate measure: ``We want to run an update every night and our budget is machines like Y and so we need to be able to process X packages/second in order to scan all of these registries.''}

By meeting these requirements, the proposed system can effectively detect typosquatting attacks in a production setting, providing robust security guarantees while maintaining usability and efficiency.
\fi

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/system-overview.pdf}
    \caption{
    Overview of our typosquatting detection pipeline.
    The pipeline includes five primary steps, each labeled with a blue circled number. Part of Steps 2 and All Step 3 are also pursued in prior work~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}.
    In Step 2, we employed an embedding-similarity-based approach, which differs from conventional methods. Additionally, we observed that incorporating Steps 4 and 5 significantly enhances system accuracy.
    The red circles indicate the evaluation questions (EQs).
    % \WJ{TODO: We need to update the EQs.}
    % \JD{Proposal: (1) add the titles of the steps into the diagram; and (2) In the caption, add a sentence like: Steps 1-2 are also pursued in prior work (CITE). Our approach to Steps 1-2 differ in XYZ, and we find that including Steps 3-5 offer a substantial improvement in F1-score (CREF).}
    }
\vspace{-5mm}
    \label{fig:pipeline}
\end{figure*}
% \vspace{-10mm}

\section{TypoSmart Design and Implementation}
\label{sec:SystemDesign}

% \WJ{Does the order of the next three paragraphs look reasonable? (background -> our work -> comparison)}
Existing methods often rely on simplistic criteria, such as the absence of current malware activity or name similarity to legitimate packages, to classify packages as benign or potential typosquats (\cref{sec:background-Defenses}). These approaches are prone to ambiguity and result in high false-positive rates, failing to account for the range of intents behind suspicious package names~\cite{taylor2020spellbound, neupane2023beyondTyposquatting}.


We introduce \TypoSmart, a comprehensive typosquatting detection system designed to reduce false positives while prioritizing the detection of \textit{malicious intent}. Our system integrates embedding-driven name analysis, hierarchical naming checks, and metadata-based verification to enhance threat detection and mitigation.
\TypoSmart is specifically designed for \textit{backend} use in package registries or similar platforms, prioritizing accuracy over low latency to ensure robust and reliable detection.


Our system proactively addresses existing shortcomings by \ul{analyzing suspicious package names and leveraging LLMs to detect the existence of malicious intent}, thereby flagging harmful naming patterns before they can cause damage.

% The following sections outline how we operationalize these techniques to meet stringent security guarantees (\cref{sec: ProblemState-Security}), satisfy production requirements (\cref{sec: ProblemState-Prod}), and overcome the limitations of existing approaches (\cref{sec:ProblemStatement-SOTALimitation}).


% \PA{Can I achieve malicious intent detection using prior works by simply removing the check for harmful content?}
% \WJ{The problem is prior work does not have the harmful check part. They ran a malware scanner on the suspicious packages they identified which gave less than 1\% actual malware.}

% \TODO{Merge 7 and 8}
% \label{sec: SystemDesign}
% \PA{It is not clear which aspects of your design are novel vs copied from other works.}
% \PA{I think steps 2, 4 and 5 are the most interesting. 1 and 3 sounds like low-level implementation decisions.}
% \WJ{Yes I agree with this.}

\cref{fig:pipeline} presents our typosquatting detection pipeline, including five steps:
% \WJ{Should we integrate the malware scanner as well in the pipeline? Or at least indicate that we also have a scanning system after we identified these suspicious typosquatting packages.}

\setlist[enumerate]{leftmargin=1.2em, itemsep=0.1ex}
\begin{enumerate}
    \item Maintaining an \emph{up-to-date metadata database}, ensuring real-time awareness of new and evolving packages (\textbf{Req$_{4}$}).
    \item Generating and storing \emph{fine-tuned embeddings} to capture domain-specific name similarities essential for detecting malicious intent (\textbf{Req$_{1}$},
    % \textbf{Req$_{2}$},
    \textbf{Req$_{3}$}).
    \item Integrating \emph{popularity metrics} (where available) so that the system focuses on high-value targets without excluding lower-profile threats (\textbf{Req$_{3}$}).
    \item Running \emph{ML-based package confusion searches}, using approximate nearest neighbors (ANN) and quantization to scale efficiently (\textbf{Req$_{2}$}).
    \item Verifying \emph{potentially legitimate packages} through metadata heuristics, mitigating false positives and maintaining developer trust (\textbf{Req$_{1}$}, \textbf{Req$_{5}$}).
\end{enumerate}

Each step builds upon the artifacts of the previous ones
% --- metadata drives embedding creation; embeddings and popularity signals fuel confusion searches; and suspicious results are then verified with richer heuristics.
In the remainder of this section, we first detail the rationale and implementation details of each step (\cref{sec:SystemDesign-Step1-Metadata} -- \cref{sec:SystemDesign-Step6}), as well as the system limitation (\cref{sec:SystemDesign-Limitation}).

\subsection{Step \textcolor{blue}{\textcircled{\small{1}}}: Package Metadata Database }
\label{sec:SystemDesign-Step1-Metadata}

\subsubsection{Rationale (\ul{\textit{Req$_{4}$}})}
% Keeping pace with new or updated packages across multiple ecosystems requires regularly refreshed metadata. Without such updates, attackers could publish deceptive names and remain undetected for extended periods. Hence,
\emph{Frequent metadata ingestion} is essential for early threat detection and aligns with \textbf{Req$_{4}$}, which mandates a reliable feed of newly published packages.

\subsubsection{Approach}
We employ a \textit{commercially maintained platform} from a software supply chain security company, which consolidates metadata from npm, PyPI, RubyGems, Maven, Golang, and Hugging Face. This database contains package names, version histories, commit logs, license info, and maintainer records, all updated on a near-daily basis. By ingesting these details, we mitigate stale data concerns and ensure our pipeline quickly analyzes newly introduced packages. This step forms the basis for all subsequent steps in the system.



\iffalse
In this work, we utilize the package metadata dataset provided by Socket\footnote{\url{https://socket.dev/}}.
Socket is a comprehensive platform that aggregates and maintains extensive metadata for a wide variety of software packages across multiple ecosystems, including JavaScript, Python, Ruby, Java, and Go. This dataset offers regularly updated and comprehensive package information, enabling the detection of both existing ecosystem attacks and emerging zero-day vulnerabilities. By leveraging this dataset, we effectively address \ul{\textit{Req$_{1}$}}, which requires access to up-to-date metadata.
\JD{Is this a paid/subscription database? And the popularity stuff should be given in this part as an extension you're making to their database, which makes this step more interesting.}
\fi

\subsection{Step \textcolor{blue}{\textcircled{\small{2}}}: Package Name Embedding Database Creation}
\label{sec:SystemDesign-Step2}

\subsubsection{Rationale (\ul{\textit{Req$_{1}$}},
% \ul{\textit{Req$_{2}$}},
\ul{\textit{Req$_{3}$}})}
Detecting maliciously similar names requires accurately capturing subtle lexical variations. Traditional Levenshtein edit distance methods often fail to account for domain-specific semantic nuances (\eg \texttt{meta-llama} vs. \texttt{facebook-llama}), while generic embedding models can introduce inaccuracies for short names, resulting in higher false-positive or false-negative rates. Robust, fine-tuned embeddings address these shortcomings by providing enhanced semantic sensitivity, reducing erroneous alerts (\textbf{Req$_{1}$}).
Moreover, this embedding approach is also generic and unified to support various naming convention of SPRs (\textbf{Req$_{3}$}).

\subsubsection{Approach}
An \emph{embedding fine-tuned on real package names} enhances semantic sensitivity, enabling the detection of adversarial or suspicious names. This capability is integral to assessing the risk associated with a package and serves as a cornerstone of our \textit{intent-centric} approach.


We build upon FastText~\cite{bojanowski2017FasttextEmbedding}, starting with the pre-trained model pre-trained on \texttt{cc.en.300.bin}, and \emph{fine-tune} it using a list of all (totally $ \sim $9.1 billion) package names extracted from the metadata database in Nov 2024 (\S\ref{sec:SystemDesign-Step1-Metadata}).
By fine-tuning, our version can capture:
\begin{itemize}[leftmargin=*, itemsep=0.1ex]
    \item \textbf{Domain-Specific Subwords}: Frequent domain specific terms in package names are better captured.
    \item \textbf{Hierarchical Structures}: Splitting names in Maven (\texttt{groupId:artifactId}) or Hugging Face (\texttt{author/model}) to create multiple embeddings per package.
\end{itemize}
We then use the fine-tuned embedding model to create an embedding vector database utilizing the vector format provided by \texttt{pgvector} due to its efficient vector operations for databases~\cite{pgvector}. The complete embedding database occupies 24~GB, with each embedding vector corresponding to a single package name (or its \texttt{author name} and \texttt{package identifier}). This setup not only facilitates rapid query-based lookups but also supports subsequent steps in package neighbor searching.
Visualization of our embedding model is available in \cref{Appendix-EmbeddingVis}.


% \subsubsection{Illustrative Example}


% \cref{fig:embeddingSpace} presents a t-SNE visualization of the embedding space for 10,000 npm package names. This visualization illustrates how the embeddings capture both semantic relationships and syntactic similarities among package names. In the plot, package names that are semantically related or have minor syntactic differences are clustered closely together. For instance, packages that are common targets for typosquatting—those with names differing by a single character or with transposed letters—are located in proximity within the embedding space.

% The clustering behavior observed in \cref{fig:embeddingSpace} indicates that the fine-tuned embeddings effectively encode meaningful patterns in package names. Packages with similar functionalities or those that are potential typosquatting variants of popular packages naturally group together, demonstrating that the embeddings are sensitive to both the lexical composition of the package names and their contextual relevance within the ecosystem. By capturing these nuanced relationships, the embeddings provide a robust foundation for typosquatting detection, enabling similarity searches that effectively retrieve genuinely related or suspiciously similar packages. This capability is crucial for identifying malicious packages that exploit naming similarities to deceive users.

% All extracted embeddings are stored in a PostgreSQL database to facilitate efficient querying and analysis in subsequent steps. The total size of the embeddings amounts to approximately 24GB, reflecting the comprehensive scope of our dataset and the high dimensionality of the embeddings.

\subsection{Step \textcolor{blue}{\textcircled{\small{3}}}: Integrating Popularity Signals}
\label{sec:SystemDesign-Step3}

\subsubsection{Rationale (\ul{\textbf{Req$_{3}$}})}
Including popularity signals is crucial because attackers often target widely used libraries or models to maximize their impact. Building on insights from prior work~\cite{neupane2023beyondTyposquatting, taylor2020spellbound}, identifying packages with high download counts or strong community engagement allows us to concentrate detection efforts where they are most needed, minimizing overhead and reducing alert fatigue from infrequently used packages.
% However, we do not treat popularity as the sole criterion; packages exhibiting suspicious naming patterns are still flagged under our intent-centric approach, even in the absence of popularity data. This balanced reliance on popularity enables us to prioritize scanning likely targets while maintaining coverage for less popular projects that may pose malicious risks.

\subsubsection{Approach}
% \emph{Direct Metrics.}
Building on prior work, we prioritize packages based on popularity, treating widely-used ones as more likely to be legitimate, while still scrutinizing all packages for signs of stealth typosquatting attacks to balance resource efficiency and detection accuracy.

For registries like npm, PyPI, RubyGems, and Hugging Face—which offer weekly or monthly download counts. We mark the top packages as \textit{legitimate} packages. Prior works~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting} also adopt this strategy, using popularity as a gating mechanism to identify prime attack targets.

% \emph{Aggregated Metrics for Incomplete Data.}

Maven and Golang lack consistent download metrics, prompting us to integrate external data sources such as \texttt{ecosyste.ms}, which estimate popularity using indicators like stargazers, forks, dependent repositories, and Docker pulls~\cite{ecosystemsWeb}. While the resulting composite \texttt{average\_ranking} metric is not without limitations, it effectively identifies libraries that are frequently referenced or heavily relied upon.

To address the size of each ecosystem and satisfy Req$_{3}$, we made the following design decision: For a given input package, we categorize packages into two groups: a popular list and an unpopular list.
If the package belongs to the popular list, we compare it only with significantly more popular packages --- those with download rates at least 10 times higher (for registries with download metrics) or ranking scores at least twice as high (for registries with relative popularity rankings). If the package is flagged as suspicious, it is removed from the popular list, which is cached in memory for efficient processing. For packages in the unpopular list, we compare them against all packages in the popular list.


% \paragraph{Partial Usage.}
% Crucially, any package—\emph{popular or not}—can still trigger alerts if our embedding and metadata checks (\S\ref{sec:SystemDesign-Step2}, \S\ref{sec:SystemDesign-Step5}) detect strong evidence of malicious intent. In other words, popularity helps us \emph{prioritize} suspicious candidates but does not exclude low-profile packages from scrutiny. This dual approach addresses real-world constraints by dedicating more resources to heavily used packages (and thus reducing false positives on obscure ones), while maintaining the flexibility to catch crafty attackers who start with low-visibility repos. As such, Step 3 serves both \textbf{Req$_{2}$} (manageable false positives through targeted scanning) and partially aids \textbf{Req$_{3}$} (cost-effectiveness) by allocating computational effort where it is most impactful.


\subsection{Step \textcolor{blue}{\textcircled{\small{4}}}: ML-Based Package Confusion Search}
\label{sec:SystemDesign-Step4}

\subsubsection{Rationale (\textbf{Req$_{2}$})}
Despite the use of robust embeddings (\cref{sec:SystemDesign-Step2}), scanning entire ecosystems for name-based threats remains computationally intensive. A scalable strategy is essential to identify suspicious package similarities across millions of entries in near-real-time (\textbf{Req${2}$}). Moreover, hierarchical naming conventions (\eg \texttt{groupId:artifactId} in Maven) add complexity, as attackers may target authorship (\ie \emph{author squatting}) rather than solely manipulating package names. Thus, the solution must achieve high recall while maintaining efficiency and minimizing false positives, ensuring developers are not overwhelmed with excessive alerts (\textbf{Req$_{2}$}).









\subsubsection{Approach}

\paragraph{Package Name Similarity Search}
To efficiently detect suspiciously similar names, we employ the \emph{Approximate Nearest Neighbors} (ANN) method, utilizing the \emph{HNSW} index~\cite{malkov2018HNSW} in PostgreSQL. We selected HNSW over \emph{IVFFlat} due to its superior benchmarking results, which demonstrate faster search speeds and negligible changes to vectors~\cite{tembo_pgvector_2024}. The HNSW index partitions the embedding space into multiple clusters, enabling us to focus distance computations on a smaller subset of candidate packages, rather than exhaustively comparing all possible pairs.

\paragraph{Data-Driven Optimization} We deployed an initial version of our neighbor search algorithm to production for evaluation. Based on empirical data, we identified opportunities to enhance its performance. To improve accuracy across ecosystems, we tailored the neighbor search query. For example, in Maven, the \texttt{artifact
\_id}, and in Golang, the \texttt{domain name}, are lengthy and reduce the effectiveness of embedding similarity. To address this, we relied exclusively on edit distance for author names. Additionally, within our threat model, we determined that changes to both identifiers in Maven, Golang, and Hugging Face were unlikely to significantly confuse users. Consequently, we excluded these from our query.


\iffalse
\begin{enumerate}[leftmargin=*]
\item \textbf{Index Construction.}
We first run k-means clustering on the embedding vectors (extracted in \S\ref{sec:SystemDesign-Step2}) to produce a predetermined number of centroids. Each package name’s vector is then assigned to its nearest centroid.

\item \textbf{Query Process.}
When a new or updated package is evaluated, we determine the centroid nearest to its embedding. We then compute similarity only against vectors in that cluster, dramatically cutting the computational cost relative to brute-force searches. This design supports real-time lookups and batch scans alike.

\item \textbf{Author-Level Similarity.}
In Maven, Golang, and Hugging Face, package identities often include author or group-level namespaces. We separately embed these author IDs to identify potential \emph{impersonation attacks}, in which malicious packages appear to originate from well-known maintainers. By comparing embedding distances among author names, we can flag suspiciously similar accounts, a tactic that single-level matching might miss entirely.
\end{enumerate}
\fi

\paragraph{Quantization for Performance.}
While ANN indexing accelerates queries, the high dimensionality of fine-tuned FastText vectors can still impose significant computational and storage demands. To address this, we applied the quantization technique to compress model weights by reducing precision, and maintaining high-quality embeddings while reducing model size. We then regenerate and store all package name vectors using the quantized model in PostgreSQL, enhancing both batch and real-time search performance while contributing to \textbf{Req$_{3}$} by lowering compute costs.

% \paragraph{Tailored Algorithm per ecosystem}
% \WJ{There is something we need to say here, but maybe we should move it to appendix}

\TODO{HuggingFace challenges:semantic difference vs. typos}
\iffalse
\begin{enumerate}[leftmargin=*]
\item \textbf{Quantizing FastText Models.}
We apply the built-in \texttt{quantization} function in FastText~\cite{joulin2016fasttext}, compressing weight parameters by lowering precision. This preserves the model’s ability to generate high-quality embeddings while shrinking its size.

\item \textbf{Re-Embedding and Storage.}
All package name vectors are regenerated using the quantized model, then stored in PostgreSQL. This further speeds up both batch and real-time searches, contributing to \textbf{Req$_{3}$} (cost-effectiveness) by minimizing compute demands.

\item \textbf{Accuracy-Performance Trade-off.}

In \cref{sec:eval-quantization}, we evaluate how much retrieval precision we sacrifice for these gains. \TODO{Our findings show only a small drop in F1 score, outweighed by faster inference times and reduced memory footprints.} This ensures feasible deployment at scale without requiring specialized hardware or advanced GPU clusters.
\end{enumerate}
\fi

% \subsubsection{Why This Matters.}
% \WJ{TODO: This paragraph can be cut}
% By combining approximate nearest neighbors with quantized embeddings, we achieve strong recall for name-similarity attacks while controlling latency and resource usage --- thereby maintaining a balance between high detection coverage and real-world deployability (\textbf{Req$_{2}$} and \textbf{Req$_{3}$}). In particular:
% \begin{itemize}[leftmargin=*]
%     \item \textbf{Author-level checks} catch hierarchical attacks (\eg \texttt{facebook-llama} impersonating \texttt{meta-llama}).
%     \item \textbf{Quantized embeddings} keep the per-query cost low, even as the registry expands to millions of packages.
%     \item \textbf{IVFFlat indexing} ensures real-time responsiveness for suspicious packages, fostering immediate alerts in CI/CD environments.
% \end{itemize}

% In short, \emph{Step 4} forms the core matching engine of our pipeline, detecting known typosquatting tactics and adapting to structural nuances across ecosystems without saturating computational budgets or developer attention.


\iffalse
\subsection{Step 4: ML-based Package Confusion Search}

\subsubsection{Package Name Similarity Search}

\paragraph{Approximate Nearest Neighbors (ANN) Search}

To efficiently identify packages with similar names, we employ an Approximate Nearest Neighbors (ANN) search using the \texttt{Inverted File Flat (IVFFlat)} index within our PostgreSQL database. The IVFFlat index is a quantization-based indexing method that partitions the dataset into multiple buckets, enabling faster similarity searches by limiting the search space to a subset of relevant clusters~\cite{zhu2023ivfflat}. This approach significantly accelerates vector similarity computations compared to brute-force methods, making it feasible to perform real-time searches over large-scale embedding datasets.

In our implementation, we first cluster the package name embeddings into a predefined number of centroids using k-means clustering. During a query, the search process identifies the nearest centroid to the query embedding and only computes exact distances within that cluster. This reduces the computational overhead while maintaining high recall for nearest neighbor searches.

\paragraph{Detecting Confusing Author Accounts}

In ecosystems like Maven, Golang, and Hugging Face, package names often include author or group identifiers as part of a hierarchical naming structure. Attackers can exploit this by creating typosquatting packages that mimic not only popular package names but also author names. To detect such attacks, we separately analyze the embeddings of author names and package identifiers.

First, we extract embeddings for both the author names (or \texttt{groupId}) and the package identifiers (or \texttt{artifactId}) independently. By doing so, we can perform similarity searches on author names to identify accounts that may be impersonating legitimate authors. This involves computing the similarity between the embeddings of known legitimate authors and those of other authors in the ecosystem.

We compute the similarity between the embedding vector of a legitimate author (\eg 'legit\_author') and all other author embeddings in the database. We then order the results by the highest similarity scores, effectively identifying author accounts that are most similar to the legitimate one. Packages associated with these accounts can then be flagged for further inspection.

By focusing on the author-level embeddings, we can detect typosquatting attacks that might not be apparent when only analyzing package names. This method is crucial for ecosystems where the author or group identifier is a significant component of the package's identity.

\subsubsection{Quantization to Improve Inference Speed}
\JD{Including this as a subsubsection is a bit unusual, usually this is part of a section or subsection called Implementation which is placed at the end of Design\&Impl or as its own section. Placed within this Step, it raises questions like `Is this the only place where you made changes for performance?'' This is not fatal but be thoughtful about where to put this info.}
Due to the high computational cost associated with similarity comparisons over high-dimensional embedding vectors in large datasets, we implement quantization techniques to reduce both storage requirements and inference time.

We apply quantization directly to the pre-trained FastText models using the built-in \texttt{quantization} function provided by FastText~\cite{joulin2016fasttext}. This process compresses the original models by reducing the precision of the weight parameters, effectively decreasing the model size without significantly compromising accuracy. The quantized models retain the ability to generate embeddings for package names but operate with improved efficiency.

By using the quantized FastText models, we regenerate the embeddings for all package names across the six ecosystems. The quantization reduces the computational load during both the embedding generation and the similarity search phases. This optimization is crucial for handling large-scale data and performing real-time similarity searches in a production environment.

The quantized embeddings are then stored in our PostgreSQL database, replacing the original, uncompressed embeddings. This setup allows us to perform similarity searches using the more compact embeddings, significantly improving query performance while maintaining acceptable levels of precision.

We evaluated the performance of the quantized models in Section~\ref{sec
}, where we compare retrieval accuracy and query latency against the original models. The results demonstrate that using quantized FastText models provides a favorable trade-off between speed and accuracy. Specifically, we observe substantial reductions in inference time and storage requirements with minimal impact on the quality of the similarity search results.

By integrating quantization into our system in this manner, we meet the production requirement of low latency and cost-effectiveness (\textit{Req$_{4}$}), ensuring that the typosquatting detection mechanism operates efficiently at scale without the need for additional complex quantization techniques or database modifications.
\fi


% \subsubsection{Structure Design of the Final System}



\subsection{Step \textcolor{blue}{\textcircled{\small{5}}}: Metadata Verification}
\label{sec:SystemDesign-Step5}

{
\renewcommand{\arraystretch}{0.3}
\begin{table*}[h]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\caption{
Overview of the 10 metadata-based verification rules. Each rule includes a description of its purpose and the specific implementation steps taken to verify flagged packages. The final two rules (R9 and R10)
% \JD{The final two rules are R9, R10 , not R8 and R9?}
were added as part of our refinement process based on further observed false-positive patterns after we deployed our system in production.
% \WJ{Can we move this table into appendix? I think the rules are overlapped with our taxonomy but the implementation details are not.}
\JD{We do not have an ablation of this, right?}
}
\label{tab:metadata_rules}
\scriptsize
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{p{0.2\textwidth} p{0.33\textwidth} p{0.44\textwidth}}
\toprule
\textbf{Rule} & \textbf{Description} & \textbf{Implementation} \\
\midrule
\textbf{R1: Intentional Naming}
& Identify brand-related or deliberately extended names, such as \texttt{express-plus}, that suggest a legitimate project rather than a malicious clone.
& Compare flagged package names to legitimate ones, searching for suffixes like \texttt{-plus}, \texttt{-extra}, or \texttt{-utils}. Presence of these terms strongly suggests legitimate extensions. \\
\\
\textbf{R2: Distinct Purpose}
& Distinguish packages with different functionalities, even if names are superficially similar (\eg \texttt{lodash-utils} vs. \texttt{lodash}).
& Extract package descriptions and calculate semantic similarity using TF-IDF cosine scores. A score below 0.5 indicates distinct purposes, reducing suspicion of deception. \\
\\
\textbf{R3: Fork Identification}
& Detect benign forks sharing near-identical code or metadata with a popular package.
& Compare README files, version histories, and file structures for high overlap. Similarities without malicious edits suggest harmless forks. \\
\\
\textbf{R4: Active Development/Maintained}
& Exclude packages that are frequently updated or actively maintained by multiple contributors, as these are less likely to be malicious placeholders.
& Retrieve metadata for the last update, commit history, and version count. Classify packages with recent updates (\eg within 30 days) or more than five versions as legitimate. \\
\\
\textbf{R5: Comprehensive Metadata}
& Identify packages missing critical metadata elements, such as licenses, maintainers, or homepage URLs, which are typical of legitimate projects.
& Check for the presence of licenses, contact details, and repository links. Flag packages missing two or more of these elements as potential typosquats. \\
\\
\textbf{R6: Overlapped Maintainers}
& Distinguish legitimate extensions or rebrands by verifying if the flagged package shares maintainers with the legitimate one.
& Match maintainer identifiers (\eg email, GitHub handle) between flagged and legitimate packages. Overlapping maintainers suggest legitimate intent. \\
\\
\textbf{R7: Adversarial Package Name}
& Filter out name pairs with significant length differences, as these often indicate unrelated projects rather than covert mimicry.
& Compare string lengths of flagged and legitimate package names. A difference exceeding 30\% indicates likely unrelated naming.\\
\\
\textbf{R8: Well-known Maintainers}
& Trust packages maintained by reputable and recognized authors or organizations.
& Leverage knowledge in LLM training data to identify if a maintainer is well-known in the community.
% If a flagged package is published by a recognized maintainer (\eg \texttt{expressjs}), it is considered legitimate regardless of name similarity.
% For example, \texttt{expressjs/body-parser} is trusted even if a similarly named package like \texttt{body-parser} exists.
\\
% \sout{\textbf{R6: Consistent Quality Metrics}} & \sout{Detect artificially boosted popularity by comparing download \midrule against community engagement metrics (\eg commits, contributors).} & \sout{Examine ratios such as downloads-per-contributor. Discrepancies (\eg high downloads but low activity) indicate potential typosquats.} \\
% \\
\midrule
\textbf{R9: Package Relocation}
& Account for legitimate package relocations, common in hierarchical registries like Maven.
& Parse metadata files (\texttt{pom.xml}) for \texttt{<relocation>} tags or analogous fields. Identify and ignore renamed or migrated projects. \\
\\


\textbf{R10: Organization Allowed List}
& Prevent false positives by excluding packages published by trusted or verified organizations.
& Maintain an allowedlist of approved organizations. If a flagged package is published under an allowed organization (\eg \texttt{@oxc-parser/binding-darwin-arm64}), it should be considered legitimate, comparing to \texttt{binding-darwin-arm64}. \\
\\
\bottomrule
\end{tabular}
}
\end{table*}
}

\iffalse
\TODO{Analyze these metrics using data from typomind to specify the threshold for each.}
\WJ{Reuse the metadata feature from \url{https://dl.acm.org/doi/pdf/10.1145/3691620.3695493}}
\fi
Despite promising performance in name-based detection, purely string-oriented methods often misclassify harmless or beneficial packages as typosquats. To mitigate these false alarms, we developed a \textbf{metadata-driven verification procedure} that filters out legitimate packages based on readily observable heuristics (\eg active development, overlap in maintainers). This section details our preliminary analysis, the rules derived from it, and how we evaluate the final false-positive (FP) verifier.

\subsubsection{Rationale (\textbf{\ul{Req$_{1}$}}, \textbf{\ul{Req$_{5}$}}).}
Although name-based detection (Steps~2--4) successfully flags suspiciously similar packages, purely string-oriented methods often over-trigger on benign forks, relocated projects, or rebranded namespaces. This can overload developers with alerts, undermining trust in the system (\textbf{Req$_{1}$}, which calls for manageable false-positive rates). To mitigate these spurious detections, we introduce a metadata-driven verification procedure that check each flagged package for clear signs of legitimacy (\eg active development, overlapping maintainers) and classify them into different categories (\textbf{Req$_{5}$}).


\subsubsection{Approach}
% \paragraph{Heuristic Rules and Refinements}
Our analysis on the false-positive data (\cref{sec:ProblemState-CaseStudy2-FP}) suggested that purely name-based detection can flag numerous legitimate packages, from brand extensions to harmless forks. To address these issues, we iteratively developed nine rules that utilize metadata signals (\eg version history, maintainers) to distinguish genuinely malicious typosquats from benign look-alikes.



\cref{tab:metadata_rules} provides a summary of the goals and implementation details for each rule. Once our name-based detector flags a package as suspicious, we retrieve its metadata and sequentially apply \texttt{R1}–\texttt{R8}, stopping as soon as a rule determines the package to be legitimate. Furthermore, during the deployment of our system in production, we identified the need for two additional rules (\texttt{R9}, \texttt{R10})
% \JD{There is no R11 in the table}
based on observed patterns. These new rules were subsequently integrated into the system, along with their corresponding metrics. Implementation detail of our verifier can be found in \cref{Appendix-MetadataVerification}.






% \PA{I'll check to see how you evaluated your derived rules and what the results were.}


% \paragraph{Initial Set of Six Rules (R1--R6).} Our first iteration addressed issues like brand extensions (\textbf{R1}), legitimate forks (\textbf{R3}), and code re-organization (\textbf{R4}, \textbf{R5}, \textbf{R6}). Yet some false positives persisted, particularly in hierarchical ecosystems.

% \paragraph{Refinements (R7--R9).} Subsequent manual reviews uncovered common relocation patterns in Maven (\textbf{R7}), packages maintained by the same author accounts (\textbf{R8}), and major length discrepancies (\textbf{R9}). Incorporating these checks further reduced extraneous alerts.

% \paragraph{Verification Pipeline}


% \paragraph{Concurrency and Logging.}
% To process large registries efficiently, we rely on multiprocessing: each worker gathers metadata, executes the rules in order, and logs the results. If the package fails all nine checks, it remains flagged as a true-positive typosquat.

\subsection{Step \textcolor{blue}{\textcircled{\small{6}}}: Intervention and Alerting}
\label{sec:SystemDesign-Step6}

% \paragraph{Limited Representation}
To mitigate typosquatting attacks, our system issues notifications to relevant stakeholders whenever it detects suspicious packages across different package registries. The goal is to prompt closer scrutiny of packages that exhibit potentially deceptive naming, to determine whether they constitute genuine attacks, and to assess any malicious behavior within them. We envision this alert mechanism as an additional layer of defense, complementing other software-package metrics such as supply chain security, quality, maintenance, vulnerability management, and licensing compliance.

% \Andreas{At this point, I don't think SBOMs have been introduced, so it's not clear why they're being analyzed.}
% Currently, our system has generated alerts for \TODO{xxx} in production, revealing \TODO{yy} packages with confirmed malicious content and \TODO{yy} with malicious intent. We expect to continue monitoring ecosystems and issuing timely alerts, thereby contributing to improved security within these package communities.


\iffalse

\subsection{Evaluation of the False-Positive Verifier}



\iffalse
\paragraph{Rule 1: Intentional Naming} - Packages with intentional, brand-related naming patterns are likely to be legitimate, reducing the chances of false positives.
% \TODO{For example}

\paragraph{Rule 2: Distinct Purpose} - If the flagged package and the legitimate package serve clearly different purposes or functionalities, it is likely a false positive.

\paragraph{Rule 3: Fork Identification} - Packages with identical or highly similar description files and metadata are more likely to be genuine typosquat cases.

\paragraph{Rule 4: Active Development} - Packages that are actively maintained (\eg recent updates, multiple versions, \TODO{multiple maintainers?}) are typically not typosquats, thus reducing false positives.

\paragraph{Rule 5: Comprehensive Metadata} - Packages missing at least two critical metadata elements (\eg license, official webpage) are more likely to be true positives.

\paragraph{Rule 6: Consistent Quality Metrics} - Discrepancies between quality indicators (\eg high download count but low contributor activity) suggest a higher likelihood of a typosquat.


We ran an initial version of this on 20,000 packages from each ecosystems and manually sampled a set of true positives and false positives to evaluate the effectiveness of our FP verifier. After reviewing the preliminary data, we added the following rules:

\paragraph{Rule 7: Relocation of Java Packages} During our analysis, we found a lot of false positives are from the pacakge relocations. Therefore, we use the metadata from \texttt{pom.xml} file on Maven to identify the relocation information and filter those relocated pacakge out. For example, \textit{io.fnproject.fn:runtime} was relocated from \textit{com.fnproject.fn:runtime}.

\paragraph{Rule 8: Overlapping Maintainers} We observed that many false positives occur when packages share the same authors. To address this, we implemented a maintainer check: if two packages have overlapping maintainers, they are likely to be false positives. For example, \texttt{botocore-a-la-carte-machinelearning} and \texttt{botocore-a-la-carte-chatbot} share the same maintainer, indicating a false positive.

\paragraph{Rule 9: Substantially Different Package Name Lengths}


\subsubsection{Implementation}

To implement our rules for false-positive removal, we developed a pipeline that automates metadata verification and comparison across three registries: npm, PyPI, and Rubygems. The process begins by loading potential typosquats detected by our tool, retrieving metadata for both the flagged and legitimate packages, and applying the six rules to determine if the flagged package should be classified as a false positive.
We describe out detailed implementation below:

\paragraph{Rule 1: Intentional Naming} Packages that exhibit deliberate, brand-related naming patterns are less likely to be typosquats. Our implementation compares the flagged package name with the legitimate one to detect creative variations or intentional play on words. This rule helps filter out cases where a package's naming is intended for branding or marketing purposes.

\paragraph{Rule 2: Distinct Purpose} If the flagged and legitimate packages serve clearly different purposes or functionalities, the flagged package is likely not a typosquat. The implementation extracts and compares package descriptions, analyzing the semantic content. If the descriptions show low similarity (less than 50%), we consider it a distinct purpose, reducing the likelihood of a false positive.

\paragraph{Rule 3: Fork Identification} Packages with identical or highly similar README files, descriptions, or metadata are more likely to be genuine typosquat cases. Our implementation uses text similarity metrics to compare these elements. If there is a high degree of similarity, the flagged package is considered a potential fork, suggesting it may indeed be a typosquat.

\paragraph{Rule 4: Active Development} Packages that are actively maintained (\eg have been recently updated or have multiple versions) are generally not typosquats. Our script retrieves metadata such as the last update date and version count. Packages with updates within the last 30 days or \TODO{more than five versions} are classified as actively developed, indicating they are less likely to be typosquats.

\paragraph{Rule 5: Comprehensive Metadata} Packages missing critical metadata elements (\eg license, official webpage, maintainers) are more likely to be typosquats. Our tool checks for the presence of these key attributes. If several are missing, it raises a flag, marking the package as a potential typosquat, as legitimate packages typically have complete metadata.

\paragraph{Rule 6: Consistent Quality Metrics} Discrepancies between quality indicators, such as high download counts but low contributor activity, suggest a higher likelihood of a typosquat. Our implementation calculates ratios like downloads per version and checks for inconsistencies across metrics. If quality indicators are imbalanced, the package is flagged as a likely typosquat, while consistent metrics suggest legitimacy.

Each of these rules is applied sequentially, and the results are logged to provide transparency in the verification process. We use multiprocessing to handle registries concurrently, allowing efficient processing of large datasets. If a package passes any rule suggesting it is legitimate, it is classified as a false positive; otherwise, it remains flagged as a true positive typosquat. The results are saved for further review and refinement.

\subsubsection{Evaluation of FP verifier}

We conducted an evaluation of our FP verifier on the typosquat dataset from a prior real-world typosquatting attack dataset created by Neupane \etal~\cite{neupane2023beyondTyposquatting}. Among all the 1232 packages in their dataset, our FP verifier was able to classify 1225 true positives and 7 false positives. This achieves an accuracy of 99\% which is substantially effective on classifying true-positives.
\WJ{TODO: False-positive dataset.}
\fi


\iffalse

\subsection{Step 4: Legitimate Package Verification Using Metadata}

\subsubsection{Preliminary Analysis}

\TODO{@Berk Sampling and analysis of FP data}
\WJ{@Jamie Could you take a look at this methodology and let me know if it looks good?}

To refine our typosquatting detector and reduce false alarms, we began by examining a subset of packages flagged as false positives in the work by Neupane \etal~\cite{neupane2023beyondTyposquatting}. Their tool identifies suspicious names by string similarity, without considering actual package content or metadata. As a result, many of the flagged entries potentially include benign forks, relocated packages, or developer experiments --- rather than truly malicious or high-intent typosquats per our definition (\cref{sec:ProblemStatement-Definition}).



\paragraph{Methodology:}
We randomly sampled \SampledTypomindFPNum of these packages from the original \TypomindFPNum false positives, achieving a \FPPrelimDataConfidenceLevel\% confidence level with a \FPPrelimDataMarginError\% margin of error. Two researchers independently inspected each sampled package’s metadata (\eg maintainers, version history), code content, and naming context to assess whether it revealed deceptive or harmful intent. Initial interrater reliability was \TODO{xx} by Cohen’s Kappa~\cite{cohen1960coefficient}, after which the researchers discussed disagreements and reached consensus on all cases.

\paragraph{Results:}
We catalogued four recurring false-positive categories in total, reflecting issues like legitimate package relocation or brand extensions. We also tracked category “saturation” (\cref{fig:xxx}) to ensure our taxonomy comprehensively captured the diverse non-malicious reasons for similar naming. Full details on the sampled dataset and analysis steps appear in \cref{sec: artifact}. This process guided subsequent improvements to our system, striking a better balance between detecting truly malicious typosquats and avoiding false positives (\textit{Req$_{3}$}).


\paragraph{Preliminary Results:}

\JD{The reader may wonder why not learn these rules, why should we use heuristics instead? (I assume it's for explainability?)}
Based on our observations, we initially established six rules to filter out false positives:

\paragraph{Rule 1: Intentional Naming Patterns} Packages with deliberate, brand-related naming patterns are likely to be legitimate, reducing the chances of false positives. For example, a package named \texttt{express-plus} that legitimately extends the \texttt{express} framework may not be a typosquat.

\paragraph{Rule 2: Distinct Purpose or Functionality} If the flagged package and the legitimate package serve clearly different purposes or functionalities, it is likely a false positive. For instance, if \texttt{lodash-utils} provides utility functions unrelated to the original \texttt{lodash} library, it may not be a typosquat.

\paragraph{Rule 3: Fork Identification} Packages with identical or highly similar README files, descriptions, or metadata are more likely to be genuine typosquats. Identifying such similarities helps confirm cases where a package may have been forked with malicious intent.

\paragraph{Rule 4: Active Development} Packages that are actively maintained—evidenced by recent updates or multiple versions—are typically not typosquats, thus reducing false positives. A package with regular commits and a history of releases is less likely to be malicious.

\paragraph{Rule 5: Comprehensive Metadata} Packages missing critical metadata elements such as a license, official webpage, or maintainer information are more likely to be typosquats. Legitimate packages usually provide complete metadata to establish credibility.

\paragraph{Rule 6: Consistent Quality Metrics} Discrepancies between quality indicators—for example, a high download count but low contributor activity—suggest a higher likelihood of a typosquat. Consistent metrics across various indicators are characteristic of legitimate packages.

After applying these rules to our dataset, we reviewed the preliminary results and identified additional patterns that led to false positives.
\JD{Did you refine the original 6?}
Consequently, we added three more rules to further refine our detection:

\paragraph{Rule 7: Package Relocation} We found that many false positives were due to package relocations, especially in Maven. By identifying relocation information in metadata files (\eg the \texttt{pom.xml} file in Maven), we filtered out these cases. For example, \textit{io.fnproject.fn
} was relocated from \textit{com.fnproject.fn
} and should not be considered a typosquat.

\paragraph{Rule 8: Overlapping Maintainers} Many false positives occur when packages share the same maintainers. If two packages have overlapping maintainers, they are likely not typosquats. For instance, \texttt{botocore-a-la-carte-machinelearning} and \texttt{botocore-a-la-carte-chatbot} share the same maintainer, indicating legitimate extensions rather than typosquatting.

\paragraph{Rule 9: Substantially Different Package Name Lengths} Packages with significantly different name lengths compared to the legitimate package are less likely to be typosquats. A package name that is much longer or shorter may serve a different purpose and not be an attempt to impersonate.

\subsubsection{Implementation}

To implement these rules for false-positive removal, we developed a pipeline that automates metadata verification and comparison across the registries. The process begins by loading potential typosquats detected by our tool, retrieving metadata for both the flagged and legitimate packages, and applying the rules to determine if the flagged package should be classified as a false positive. Below, we describe our detailed implementation of each rule:

\paragraph{Rule 1: Intentional Naming Patterns}

Our implementation compares the flagged package name with the legitimate one to detect creative variations or intentional plays on words. We use string similarity metrics and check for the presence of additional terms that indicate purposeful naming (\eg \texttt{-plus}, \texttt{-extra}, \texttt{-utils}). This helps filter out cases where a package's naming is intended for branding or legitimate extension, rather than typosquatting.

\paragraph{Rule 2: Distinct Purpose or Functionality}

We extract and compare the package descriptions, analyzing the semantic content using natural language processing techniques. We calculate the cosine similarity between the TF-IDF vectors of the descriptions. If the similarity score is below a threshold (\eg 0.5), we consider the packages to serve distinct purposes, reducing the likelihood of typosquatting.

\paragraph{Rule 3: Fork Identification}

We compare the README files, code repositories, and other metadata using text and structural similarity metrics. A high degree of similarity suggests that the flagged package may be a fork or a direct copy, which could indicate typosquatting. We also check for identical version histories and file structures.

\paragraph{Rule 4: Active Development}

Our script retrieves metadata such as the last update date, version count, and commit history. Packages that have been updated within the last 30 days or have more than a specified number of versions (\eg five) are classified as actively developed. We also consider the number of contributors and the frequency of updates.

\paragraph{Rule 5: Comprehensive Metadata}

We check for the presence of critical metadata elements such as license information, official webpage URLs, maintainer contact details, and repository links. Packages missing several of these key attributes are flagged as potential typosquats. We consider the absence of at least two critical metadata elements as a strong indicator.

\paragraph{Rule 6: Consistent Quality Metrics}

We calculate quality indicators such as download counts, contributor activity, issue tracker activity, and community engagement. We look for inconsistencies, such as a high download count with minimal contributor activity or lack of community interaction. Such discrepancies may suggest that the package's popularity is artificially inflated.

\paragraph{Rule 7: Package Relocation}

We parse metadata files (\eg the \texttt{pom.xml} file in Maven) to identify packages that have been relocated or renamed. We look for tags or fields indicating relocation, such as \texttt{<relocation>} in Maven. These packages are filtered out, as they are legitimate but may appear similar to previous versions.

\paragraph{Rule 8: Overlapping Maintainers}

We compare the maintainers of the flagged package and the legitimate package by retrieving maintainer information from the package metadata or repository. If there is any overlap in maintainers or contributing authors, the package is likely legitimate. We use unique identifiers like email addresses or usernames for accurate matching.

\paragraph{Rule 9: Substantially Different Package Name Lengths}

We calculate the lengths of the package names and compare them. If the flagged package name is significantly longer or shorter than the legitimate package name (\eg a difference of more than 30%), it may be less likely to be a typosquat. This helps eliminate packages that are unlikely to be mistaken for the legitimate one due to noticeable differences in name length.

Each of these rules is applied sequentially, and the results are logged to provide transparency in the verification process. We use multiprocessing to handle registries concurrently, allowing efficient processing of large datasets. If a package satisfies any rule suggesting it is legitimate, it is classified as a false positive; otherwise, it remains flagged as a true positive typosquat. The results are saved for further review and refinement.

\subsubsection{Evaluation of False-Positive Verifier}

We evaluated our false-positive verifier using two datasets, as shown in \cref{table:verifier_results}: the typosquat dataset from a prior study by Neupane \etal~\cite{neupane2023beyondTyposquatting} and human-evaluated false-positive data provided by Socket. In Neupane’s dataset, which includes 1,232 packages, our verifier correctly classified 1,225 as true positives and identified 7 as false positives, achieving an accuracy of 99\%. This demonstrates the verifier’s effectiveness in distinguishing true typosquats from legitimate packages. For Socket’s dataset of 19,521 packages with similar but legitimate names, the verifier achieved a True Negative Rate (specificity) of 99.33\%, correctly identifying 19,390 non-typosquats while incorrectly flagging 131 as false positives, resulting in a False Positive Rate of 0.67%.


\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Total Packages} & \textbf{True Positives} & \textbf{False Positives} & \textbf{True Negatives} & \textbf{Accuracy} & \textbf{Specificity (TNR)} & \textbf{False Positive Rate} \\ \hline
Neupane \etal~\cite{neupane2023beyondTyposquatting} & 1,232 & 1,225 & 7 & N/A & 99\% & N/A & N/A \\ \hline
Socket & 19,521 & N/A & 131 & 19,390 & N/A & 99.33\% & 0.67\% \\ \hline
\end{tabular}%
}
\caption{Performance evaluation of the false-positive verifier on two datasets.}
\label{table:verifier_results}
\end{table}


By implementing these rules and integrating metadata verification into our detection pipeline, we significantly improved the precision of our typosquatting detector. This approach ensures that legitimate packages are not incorrectly flagged, thereby maintaining trust in the ecosystem and meeting the production requirement of balancing true positives while minimizing false positives (\textit{Req$_{3}$}).
% Total packages processed: 1232
% True Positives: 1225
% False Positives: 7
% Accuracy: 0.99                                    \section{Evaluation}
\label{sec:evaluation}

In this section, we experimentally validate our typosquatting detection system, assessing its accuracy, performance, and ability to discover new threats. We begin by summarizing our experimental setup and datasets (\S\ref{sec:Eval-exp-setup}), then introduce the baselines we compare against (\S\ref{sec:baselines}). Finally, we present our research questions and results (\S\ref{sec:results}), culminating in an analysis of how effectively our approach meets real-world requirements.
\fi
\fi

% suppressing false positives - integrating empirically-derived signals
% hierarchically-aware - new design for identifying name similarity

\subsection{Implementation}

Our system is implemented in Python, with embeddings stored in PostgreSQL.
Quantization and embedding generation rely on a modified FastText pipeline (\cref{sec:SystemDesign-Step2}).
\TypoSmart has over 3K \texttt{LOC}, primarily comprising $\sim$1K \texttt{LOC} for embedding creation, $\sim$500 \texttt{LOC} for popularity
check, $\sim$1K \texttt{LOC} for neighbor search, $\sim$300 \texttt{LOC} for metadata verification, and $\sim$100 \texttt{LOC} for alerting.
% \JD{Can you comment on the lines of code, possily broken down by component?}

\section{Evaluation}
\label{sec:Eval}

\WJ{TODO: We might also need a benchmark on AI model versions. Let's try o1 and o1 mini if possible. At least the mini models will have much lower latency}
To evaluate \TypoSmart, we pose five Evaluation Questions (EQs) to assess its performance at the component level (\textbf{EQ1}–\textbf{EQ3}) and the system level (\textbf{EQ4}–\textbf{EQ5}).
At the component level, we measure the effectiveness of novel mechanisms introduced in \TypoSmart.
At the system level, we evaluate its integrated functionality, scalability, and ability to detect real-world typosquatting threats. Additionally, we compare our approach to SOTA methods to benchmark its effectiveness. An overview of the evaluation process is illustrated in \cref{fig:pipeline}.




% We frame our experimental evaluation around six core questions, each assessing a different aspect of our system:
% \JD{Justify these measurements: ``We measure the novel components, and then we measure properties of the integrated system. See Figure XXX for an illustration.}

% \JD{Now add subheadings, one for the component-level and one for the integrated system. And 1-2 sentences that introduce each question set.}

\paragraph{Component-level}
We assess how individual components contribute effectively to the overall system.

\begin{itemize}[leftmargin=*, itemsep=0.1ex]
    \item \textbf{EQ1: Performance of Embedding Model.}
    What is the accuracy and efficiency of our
    % fine-tuned/quantized
    embedding model?

    \item \textbf{EQ2: Neighbor Search Accuracy.}
    How effective is the neighbor search approach?
    (\cref{sec:SystemDesign-Step4})

    \item \textbf{EQ3: Metadata Verification Accuracy.}
    How much does the FP verifier (\cref{sec:SystemDesign-Step5}) reduce false positive rates?
\end{itemize}

\paragraph{System-level}
We examine the performance of the full \TypoSmart system and compare to other approaches.
%emphasizing its practical applicability, scalability, and capability to detect novel typosquatting threats.


\begin{itemize}[leftmargin=*, itemsep=0.1ex]
    \item \textbf{EQ4: Discovery of New Typosquats.}
    Can \TypoSmart identify previously unknown typosquatting threats?
    % especially among newly published packages or AI model entries?
    \item \textbf{EQ5: System Efficiency and Scalability.}
    Does \TypoSmart have practical throughput and latency for deployment? (Throughput is critical for full registry scans, while low latency ensures viability for on-demand API queries.)
    % \JD{``What is the throughput and latency of system?''. You should justify why both metrics are of interest --- throughput is useful for whole-registry scans, while latency is useful when offering on-demand query via API. Are both relevant?}

    % \JD{System needs a cool name. Ask GPT. Write the title as ``CODENAME: blah blah blah''}

    % \item \textbf{EQ6: Coverage in Hierarchical Ecosystems.}
    % How effectively does \TypoSmart detect complex typosquatting patterns, such as \textit{impersonation squatting} and \textit{compound squatting}, within hierarchical naming ecosystems?

    % \item \textbf{EQ6: Comparison to SOTA  Methods}
    % How does \TypoSmart compare to existing state-of-the-art methods in detecting typosquatting threats, in terms of accuracy and efficiency?
    % , and robustness?   \JD{robustness? what is?}
\end{itemize}

All experiments run on a server with 36 CPU cores (Intel Xeon W-2295 @ 3.00GHz) and 188 GB of RAM.
Notably, the training and fine-tuning of FastText models do not require GPUs.
% \JD{Any GPU? If not, you might note that FastText doesn't need a GPU (if that's the case?)}

% \PA{I'll suggest you rearrange your questions based on importance. In my opinion, the most important question is - does the system as a whole work (EQ4-2, EQ5)? What is the false positivity rate (some variant of EQ4-1)? I also want to see how these metrics compare with those from prior works}

% \PA{EQ1, EQ3 and EQ4-1 can be one EQ on conduction ablasion study - how do the design of each component contribute to the success of the overall system}


\subsection{Baseline and Evaluation Datasets}
\label{sec:Eval-Setup}

\paragraph{State-of-the-Art Baselines}
\label{sec:baselines}

We compare our system to the Levenshtein distance approach, used in~\cite{vu2020typosquatting}, and Typomind~\cite{neupane2023beyondTyposquatting} which is the only embedding-based approach from existing literature.

\iffalse
We compare our system to three representative baselines: Typomind~\cite{neupane2023beyondTyposquatting}, Spellbound~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting}, and OSSGadget~\cite{MicrosoftOSSGadget}.
These are the leading academic and industry tools available for comparison.
\fi
%These baselines allow us to gauge whether hierarchical name support, quantization, and metadata-driven false-positive reduction yield tangible improvements over existing approaches.


\paragraph{Evaluation Datasets}
We evaluate our approach primarily using the dataset from \cite{neupane2023beyondTyposquatting} because it summarized all confirmed typosquatting true positives, while include many suspicious packages that does not include malware.
This dataset includes \NumTypomindData confirmed typosquats labeled typosquatting packages from npm, PyPI, and RubyGems.
% These data focus on string-similarity attacks but do not incorporate stealth attack checks.
% For baseline comparisons, we utilize the dataset from the state-of-the-art study published at USENIX'23~\cite{neupane2023beyondTyposquatting}, incorporating their published evaluation results as reference points. To assess the system's performance on previously unknown typosquat threats, we applied our system to a randomly sampled dataset of 5000 packages from each registry.
% \PA{What are the sizes of these datasets?}
% \JD{It would be better to run ourselves than to reuse the data, but given the time constraint you can simply say "We used the same dataset so we copied the evaluation result from USENIX'23"}
% \begin{itemize}[leftmargin=*, itemsep=0.1ex]
%     \item \textbf{Typomind Ground-Truth}~\cite{neupane2023beyondTyposquatting}: Includes \NumTypomindData confirmed typosquats labeled typosquatting packages from npm, PyPI, and RubyGems. These data focus on string-similarity attacks but do not incorporate malicious intent checks.
%     % \item \textbf{Hugging Face Typosquats}~\cite{protectai_huggingface}: Compiled from public blog posts and suspicious model listings, reflecting hierarchical naming (\texttt{author/model}) and AI-specific threats.
%     % \item \textbf{LLM-Generated Name Hallucinations}~\cite{spracklen2024LLMPackageHallucinations}: \NumHallucinationData synthetic examples created by large language models, testing robustness against unusual or creative manipulations.
%     % \item \textbf{Human-Reviewed False-Positives}: A curated set of \NumProdReviewedTypos flagged packages verified by security analysts from our industry partner. These packages are labeled as truly malicious or high-intent versus benign forks.
% \end{itemize}

% Combining these datasets ensures coverage of both low-level name confusion (\eg edit-distance changes) and higher-level intent-based attacks (\eg \textit{impersonation squatting} in AI model hubs).

% \paragraph{Evaluation Pipeline.}
% All packages are embedded using our fine-tuned (and optionally quantized) FastText model. We then run the similarity search (Step 3 in \S\ref{sec:system_design}) to obtain candidate typosquats for each known “popular” package or author. Finally, we apply the nine-step metadata verification procedure (Step 4) to reduce false positives.



\iffalse
\begin{itemize}[leftmargin=*]
    \item \textbf{Typomind}~\cite{neupane2023beyondTyposquatting}: An embedding-based detector using FastText for name similarity but lacking metadata verification or hierarchical name handling.
    \item \textbf{SpellBound}~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting}: The most state-of-the-art tool employing edit-distance calculations, identifying packages that fall within a small Levenshtein distance of well-known libraries.
    \item \textbf{OSSGadget}~\cite{MicrosoftOSSGadget}: Similar to SpellBound, it uses edit-distance checks to flag packages within a minimal Levenshtein threshold of popular libraries.
    % \item \textbf{Socket v1}: \WJ{How to present this baseline without mentioning ``Socket''?}
    % \PA{Does providing this comparison produce new insight? Especially as there are publicly available baselines to compare with.}
    % A prior version of our system that partially uses popularity metrics and direct name comparisons but omits advanced embedding or code-level heuristics.
\end{itemize}
\fi



% \subsection{Evaluation Questions}
% \label{sec:eval-questions}
\subsection{EQ1: Perf. of Embedding Model}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ1 Summary:}
Our embedding model demonstrates superior effectiveness and efficiency, ensuring an acceptable overhead for SPR during the preparation of the embedding database.
\end{tcolorbox}
\fi


In this evaluation question, we measured both the effectiveness and efficiency of our embedding model.

\subsubsection{Effectiveness}

\paragraph{Method}
We evaluate the effectiveness of embedding-based similarity detection by comparing three approaches:

\begin{enumerate}[leftmargin=*, itemsep=0.2ex]
    \item \textit{Levenshtein-Distance}, calculates the minimum number of single-character edits required to change one package name into another.
    \item \textit{Pre-trained FastText}~\cite{bojanowski2017FasttextEmbedding} (\texttt{cc.en.300.bin}), used in the SOTA work Typomind~\cite{neupane2023beyondTyposquatting}, employs the general-purpose embedding model \texttt{cc.en.300.bin}~\cite{bojanowski2017FasttextEmbedding} to capture semantic relationships.
    \item \textit{Fine-tuned FastText (\textbf{Ours})}, which we have adapted using a corpus specifically composed of package names to better capture domain-specific similarities.
\end{enumerate}

To systematically compare these methods, we construct a balanced test set consisting of both positive and negative pairs, with each category containing 1,239 data points.


\begin{itemize}[leftmargin=*, itemsep=0.2ex]
    \item \textit{Positive Pairs}: These are derived directly from the Typomind Ground-Truth dataset~\cite{neupane2023beyondTyposquatting}, which includes labeled typosquatting packages across npm, PyPI, and RubyGems. Each positive pair consists of a known typosquat and its corresponding legitimate package.

    \item \textit{Negative Pairs}: Created by randomly pairing unrelated package names from registries, ensuring low similarity scores across all tested methods. These pairs are guaranteed not to represent typosquatting relationships.
\end{itemize}

For each approach, we applied a predefined similarity threshold to classify package pairs as potential typosquats. The threshold was selected via a grid search to optimize Precision and Recall, ensuring effective identification of true typosquats. Pairs with similarity scores above the threshold were classified as positive, while those below were classified as negative. False positives were subsequently filtered using our false-positive verification process in Step \textcolor{blue}{\textcircled{\small{5}}}.

% To get the best threshold for each model, we performed a grid search to determine the optimal similarity threshold, ensuring that embedding similarity effectively identifies true typosquats by detecting similar package names. Some false positives, initially misclassified as suspicious packages, are subsequently filtered out by our false-positive verification process in Step \textcolor{blue}{\textcircled{\small{5}}}.


We compute Precision, Recall, and F1 scores for each approach to assess their performance:

\begin{itemize}[leftmargin=*, itemsep=0.2ex]
    \item \textit{Precision}: The proportion of correctly identified typosquats out of all flagged pairs.
    \item \textit{Recall}: The proportion of actual typosquats that were correctly identified.
    \item \textit{F1 Score}: The mean of Precision and Recall, providing a balanced performance metric.
\end{itemize}

%This evaluation highlights the differences in naming patterns captured by each method and demonstrates the advantages of our fine-tuned FastText model in accurately identifying typosquatting attacks while minimizing false positives.


\renewcommand*{\arraystretch}{0.4}
\begin{table*}[h]
\footnotesize
\centering
\caption{
Performance of embedding models.
We selected the fine-tuned model with a threshold of 0.9 to achieve high F1 scores for positive pairs and relatively high F1 scores for negative pairs.
Quantization slightly improves F1 scores because the false positive rate goes up a bit while the false negative rate goes down more substantially.
}
\label{tab:EQ1_effectiveness_updated}
\scriptsize
\begin{tabular}{lllccccccc}
\toprule
\textbf{Model} & \textbf{Quantization} & \textbf{Threshold} & \multicolumn{3}{c}{\textbf{Positive Pairs}} & \multicolumn{3}{c}{\textbf{Negative Pairs}} & \textbf{Overall Score} \\
\cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-10}
& & & Precision & Recall & F1 Score & Precision & Recall & F1 Score & \\
\midrule
Edit Distance & N/A & 4 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.94 \\
\midrule
Pretrained & float32 & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.96 \\
Pretrained & float16 & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.96 \\
Pretrained & int8 & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.95 \\
\midrule
Fine-Tuned & float32 & 0.9 & 1.00 & 0.90 & 0.95 & 1.00 & 0.96 & 0.98 & 0.96 \\
Fine-Tuned & float16 & 0.9 & 1.00 & 0.90 & 0.95 & 1.00 & 0.96 & 0.98 & 0.96 \\
Fine-Tuned & int8 & 0.9 & 1.00 & 0.88 & 0.94 & 1.00 & 0.96 & 0.98 & 0.96 \\
\midrule
Hybrid & N/A & 0.5 & 1.00 & 0.91 & 0.95 & 1.00 & 0.91 & 0.95 & 0.95 \\
\bottomrule
\end{tabular}
\end{table*}

\iffalse
{
\renewcommand*{\arraystretch}{0.4}
\begin{table*}[h]
\footnotesize
\centering
\caption{
Evaluation Metrics (EQ1) for Positive and Negative Pairs Across Various Thresholds.
We selected the fine-tuned model with a threshold of 0.9 to achieve high F1 scores for positive pairs and relatively high F1 scores for negative pairs.
Quantization slightly improves F1 scores because the false positive rate goes up a bit while the false negative rate goes down more substantially.
% \WJ{Is it really necessary to put this table here? Or Figure 5 is enough?}
% \JD{Caption should mention that the quantization slightly improves F1 score and that this is because the false positive rate goes up a little but the false negative rate goes down a lot.}
% \JD{Make sure the tuning curve of 0.8 is also placed in here as a figure. Use the proper name for it I forget :-D}
\JD{Can we omit the rows for 0.6 and 0.8 throughout (and just write ``some thresholds omitted for space'')}
\JD{I find this table analysis a bit hard to follow. Can you expand the caption slightly and also perhaps the prose?}
\JD{DISCUSSION: ``I am thinking maybe we can cut the thresholds and just put the best result from the best threshold for each model''}
}
\label{tab:EQ1_effectiveness}
\scriptsize
\begin{tabular}{lllccccccc}
\toprule
\textbf{Model} & \textbf{Quantization} & \textbf{Threshold} & \multicolumn{3}{c}{\textbf{Positive Pairs}} & \multicolumn{3}{c}{\textbf{Negative Pairs}} & \textbf{Overall Score} \\
\cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-10}
& & & Precision & Recall & F1 Score & Precision & Recall & F1 Score & \\
\midrule
\multirow{3}{*}{Edit Distance} & \multirow{3}{*}{N/A}
 & 2 & 1.00 & 0.78 & 0.87 & 1.00 & 1.00 & 1.00 & 0.94 \\
 & & 3 & 1.00 & 0.79 & 0.88 & 1.00 & 1.00 & 1.00 & 0.94 \\
 & & 4 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.94 \\
\midrule
\multirow{15}{*}{Pretrained}
 & \multirow{5}{*}{float32}
    & 0.5 & 1.00 & 0.87 & 0.93 & 1.00 & 0.92 & 0.96 & 0.94 \\
 & & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.96 \\
 & & 0.7 & 1.00 & 0.83 & 0.91 & 1.00 & 1.00 & 1.00 & 0.95 \\
 & & 0.8 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.94 \\
 & & 0.9 & 1.00 & 0.66 & 0.80 & 1.00 & 1.00 & 1.00 & 0.90 \\
\cmidrule{2-10}
 & \multirow{5}{*}{float16}
    & 0.5 & 1.00 & 0.87 & 0.93 & 1.00 & 0.92 & 0.96 & 0.94 \\
 & & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.96 \\
 & & 0.7 & 1.00 & 0.83 & 0.91 & 1.00 & 1.00 & 1.00 & 0.95 \\
 & & 0.8 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.94 \\
 & & 0.9 & 1.00 & 0.66 & 0.80 & 1.00 & 1.00 & 1.00 & 0.90 \\
\cmidrule{2-10}
 & \multirow{5}{*}{int8}
    & 0.5 & 1.00 & 0.87 & 0.93 & 1.00 & 0.93 & 0.96 & 0.94 \\
 & & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.95 \\
 & & 0.7 & 1.00 & 0.83 & 0.90 & 1.00 & 1.00 & 1.00 & 0.95 \\
 & & 0.8 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.95 \\
 & & 0.9 & 1.00 & 0.66 & 0.80 & 1.00 & 1.00 & 1.00 & 0.90 \\
\midrule
\multirow{15}{*}{Fine-Tuned}
 & \multirow{5}{*}{float32}
    & 0.5 & 1.00 & 0.98 & 0.99 & 1.00 & 0.03 & 0.05 & 0.52 \\
 & & 0.6 & 1.00 & 0.97 & 0.98 & 1.00 & 0.05 & 0.10 & 0.54 \\
 & & 0.7 & 1.00 & 0.96 & 0.98 & 1.00 & 0.21 & 0.35 & 0.66 \\
 & & 0.8 & 1.00 & 0.94 & 0.97 & 1.00 & 0.56 & 0.72 & 0.84 \\
 & & 0.9 & 1.00 & 0.90 & 0.95 & 1.00 & 0.96 & 0.98 & 0.96 \\
\cmidrule{2-10}
 & \multirow{5}{*}{float16}
    & 0.5 & 1.00 & 0.98 & 0.99 & 1.00 & 0.03 & 0.05 & 0.52 \\
 & & 0.6 & 1.00 & 0.97 & 0.98 & 1.00 & 0.05 & 0.10 & 0.54 \\
 & & 0.7 & 1.00 & 0.96 & 0.98 & 1.00 & 0.21 & 0.35 & 0.66 \\
 & & 0.8 & 1.00 & 0.94 & 0.97 & 1.00 & 0.56 & 0.72 & 0.84 \\
 & & 0.9 & 1.00 & 0.90 & 0.95 & 1.00 & 0.96 & 0.98 & 0.96 \\
\cmidrule{2-10}
 & \multirow{5}{*}{int8}
    & 0.5 & 1.00 & 0.98 & 0.99 & 1.00 & 0.04 & 0.09 & 0.54 \\
 & & 0.6 & 1.00 & 0.97 & 0.98 & 1.00 & 0.07 & 0.13 & 0.56 \\
 & & 0.7 & 1.00 & 0.96 & 0.98 & 1.00 & 0.23 & 0.38 & 0.68 \\
 & & 0.8 & 1.00 & 0.93 & 0.97 & 1.00 & 0.61 & 0.76 & 0.86 \\
 & & 0.9 & 1.00 & 0.88 & 0.94 & 1.00 & 0.96 & 0.98 & 0.96 \\
\midrule
\multirow{5}{*}{Hybrid} & \multirow{5}{*}{N/A}
 & 0.5 & 1.00 & 0.91 & 0.95 & 1.00 & 0.91 & 0.95 & 0.95 \\
 & & 0.6 & 1.00 & 0.87 & 0.93 & 1.00 & 0.87 & 0.93 & 0.93 \\
 & & 0.7 & 1.00 & 0.82 & 0.90 & 1.00 & 0.82 & 0.90 & 0.90 \\
 & & 0.8 & 1.00 & 0.72 & 0.84 & 1.00 & 0.72 & 0.84 & 0.84 \\
 & & 0.9 & 1.00 & 0.64 & 0.78 & 1.00 & 0.64 & 0.78 & 0.78 \\
\bottomrule
\end{tabular}
\end{table*}
}
\fi

\paragraph{Result}
% \JD{Maybe I'm bad at reading this table, but it seems that all approaches have at least one row with a good F1 score overall and also good results by positive and negative pairs?}
\cref{tab:EQ1_effectiveness_updated} presents the results comparing our fine-tuned model with baseline approaches. The table shows that the accuracy of the quantized and original versions of our fine-tuned model is nearly identical, indicating minimal loss in performance from quantization. Additionally, the overall F1 score of the fine-tuned model is approximately 1\% higher than the pre-trained model, with a notable 2-3\% improvement in recall for positive pairs.
This improvement in recall is particularly critical for detecting typosquatting attacks, as correctly identifying all positive pairs is essential for comprehensive threat mitigation. At the same time, our approach maintains a relatively high accuracy for negative pairs.

\subsubsection{Efficiency}

\paragraph{Method}
To assess the efficiency of embedding database creation, we evaluated three quantized versions of our fine-tuned models. We evaluated the embedding database creation efficiency by measuring the throughput, latency, and overall overhead associated.


\paragraph{Result}

Our results shows that quantized models substantially increase throughput while decreasing latency. Specifically, both \texttt{float16} and \texttt{int8} quantization methods enhance throughput by approximately $5\times$ and reduce latency by around $10\times$ compared to the \texttt{float32} baseline. However, the performance difference between \texttt{float16} and \texttt{int8} is minimal, likely due to the additional quantization and processing steps involved.

Overall, the efficiency of quantized models indicates that our embedding models achieve strong performance while optimizing resource usage. Additionally, the implementation of \textit{HNSW}  indexing introduces minimal overhead, with each table requiring less than 10 seconds to process. This further enhances overall system efficiency. A detailed results table is available in \cref{tab:EQ1-Efficiency} (Appendix~\ref{Appendix-Efficiency}).

% \cref{tab:EQ1-Efficiency} presents the efficiency metrics for embedding creation across the evaluated models. The results highlight the effectiveness of the quantized versions in maintaining strong performance while optimizing resource usage. Based on this evaluation, we selected the \textit{int8} quantized model for the embedding database, as it offers superior efficiency in both creation and usage while achieving a relatively fast creation speed.

% \cref{tab:EQ1_embeddings} shows that fine-tuning on real package names yields a substantial boost in F1, presumably by capturing domain-specific subwords (\eg py, react, huggingface). Qualitatively, suspiciously similar names (\eg ``lod4sh'' vs. ``lodash'') cluster closer together in t-SNE plots, improving detection recall.






\subsection{EQ2: Neighbor Search Accuracy}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ2 Summary:}
Our embedding-based neighbor search algorithm, combined with the sorting algorithm, achieves accuracy comparable to prior work while capturing richer semantic information.
\end{tcolorbox}
\fi


\paragraph{Methods} Using the results from EQ1, we determined a threshold of 0.93 through grid search, achieving an optimal balance between precision and recall. To evaluate neighbor search performance, we analyzed suspicious packages identified by \texttt{typomind} and additional data provided by our ry partner.


\paragraph{Results} \cref{fig:EQ1-ThresholdCurves} illustrates the ROC and accuracy curves for our fine-tuned model.
% Additionally, \cref{tab:similarity results} showcases the neighbors identified using results from \texttt{typomind}.
Our approach successfully captured all typosquatting cases previously identified by SOTA methods. The neighbor search algorithm demonstrated the ability to detect 99\% of real typosquatting attacks flagged by prior research.

Deployment data revealed that the algorithm effectively identified advanced threats, including compound squatting and impersonation squatting, across platforms such as Maven, Golang, and Hugging Face.
For example, our system detected a \textit{compound squatting} attack on \texttt{@typescripyt-eslint/eslint-plugin}, where the attacker used both a similar namespace and package identifier (\texttt{@typescript\_eslinter/eslint}) to mislead users.
We found that prior tools were unable to identify this type of sophisticated attack.
% \JD{Next sentence: Is this becuase you tried them? If so, write ``We found that prior tools could not identify...''. If not, write ``Prior tools would be unable to...because they...''}
Additionally, we successfully flagged impersonation squatting attacks targeting hierarchical package names in npm. While no suspicious packages were identified in Golang, Maven, or Hugging Face due to the limited sample size in these registries, we are confident in the system’s capability to detect such threats. Notably, it can flag the reported impersonation squatting attacks for Hugging Face from \cite{protectai_huggingface}, which prior work failed to identify.
These findings highlight that our approach achieves SOTA performance in neighbor search accuracy.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/finetuned_threshold_accuracy.pdf}
        \caption{}
        \label{fig:threshold_accuracy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/finetuned_roc_curve.pdf}
        \caption{}
        \label{fig:roc_curve}
    \end{subfigure}
    \caption{Performance Metrics of the Fine-Tuned Model: (a) Threshold Accuracy and (b) ROC Curve.}
    \label{fig:EQ1-ThresholdCurves}
\end{figure}
%\vspace{-5mm}


\subsection{EQ3: Metadata Verification Accuracy}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ3 Summary:}
Our metadata verification process significantly lowers the false-positive rate from 75.4\% to 5\%.
\end{tcolorbox}
\fi

\paragraph{Method}
Starting with the raw embedding output (\ie packages flagged for name-based suspicion), we apply the nine metadata checks described in \cref{sec:SystemDesign-Step5}. We measure false positive rates (FPR)  on the manually labeled data, \ie \SampledTypomindFPNum flagged packages verified in \cref{sec:ProblemState-CaseStudy2-FP}.
% \begin{itemize}[leftmargin=*, itemsep=0.2ex]
    % \item \textbf{Typomind true-positives}~\cite{neupane2023beyondTyposquatting}: Contains \TypomindTPNum flagged packages from npm, PyPI, RubyGems.
    % \item \textbf{Human-Reviewed (Internal)}: \TODO{xx} flagged packages verified by security analysts, including .
    % \item \textbf{Manually labeled Typomind Outputs}: \SampledTypomindFPNum flagged packages verified in \cref{sec:ProblemState-CaseStudy2-FP}.
% \end{itemize}
%
\paragraph{Result}
% \cref{table:verifier_results} shows the results of our metadata verification:
Among the 665 false-positive (\ie suspicious typosquatting package without malware injected), there are 23 packages unavailable. So among the rest 642 packages, our metadata verification step was able to correctly classify 425 as false-positive, \ie we reduced the false-positive rate from 75.4\% to 5\%.

% \begin{enumerate}[leftmargin=*]
%   \item \textbf{Neupane \etal~\cite{neupane2023beyondTyposquatting} Dataset:}
%   % Contains 1,232 suspiciously named packages identified through string similarity. Our verifier classified 1,225 as genuine typosquats (true positives) and 7 as false positives (99\% accuracy).

%   \item \textbf{Socket Human-Reviewed Dataset:}
%   % A large set of 19,521 packages, each analyzed by Socket’s security analysts. Our tool achieved a 99.33\% True Negative Rate, incorrectly flagging only 0.67\% of legitimate packages.
% \end{enumerate}

% \begin{table}[h]
% \centering
% \small
% \caption{Performance of metadata verification on true-positive and false-positive data from \cite{neupane2023beyondTyposquatting}.}
% \label{table:verifier_results}
% \begin{tabular}{lccc}
% \toprule
% \textbf{Dataset} & \textbf{Size} & \textbf{TP} & \textbf{FP} & \textbf{Metadata Unavailable}\\
% \midrule
% % True Positives Data & 1,239  & \TODO{1,225} & \TODO{7} \\
% False Positive Data & 665 & \TODO{41}   & \TODO{425}\\
% \bottomrule
% \end{tabular}
% \end{table}

These results confirm that supplementary heuristics beyond raw name similarity reduce false positives while retaining high recall for genuinely malicious typosquats. For instance, overlapping maintainer checks (\textbf{R6}) and explicit relocation detection (\textbf{R9}) proved especially effective in ecosystems like Maven, where hierarchical naming changes are common. Ultimately, this metadata-driven filtering step (\textbf{Step 4} in our pipeline) aligns our detection approach with the realities of open-source development and AI model publication, ensuring minimal ``false alarms'' (\textit{Req$_{3}$}) without missing critical threats.


\subsection{EQ4: Discovery of New Typosquats}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ4 Summary:}
Our system effectively detected stealthy typosquatting attacks with malicious intent and identified two new attack types—\textit{impersonation squatting} and \textit{compound squatting}—in a production environment. Moreover, it uncovered these threats across three ecosystems that had not been explored in prior research.
\end{tcolorbox}
\fi

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/threat_type_distribution_filtered_log.pdf}
    \caption{Production data from npm and PyPI showing removed packages flagged by our system within the past month.}
    \label{fig:product-removed-pkgs}
\end{figure}


\paragraph{Method}
% We ran the typosquat detector on 5,000 packages on npm, PyPI, Maven, and Hugging Face, then we use the attributes from FP verification step to classify each suspicious alert entry as malicious or benign.
% We also deployed our system in production for one month and report our statistics here.
To evaluate the effectiveness of our typosquatting detection system, we deployed \TypoSmart in a production environment for one month. During this period, flagged packages were analyzed using a malware scanner and reviewed by threat analysts for detailed insights.



\paragraph{Result}
\cref{fig:product-removed-pkgs} provides a detailed breakdown of typosquatting packages detected by \TypoSmart which were flagged and removed from registries in Dec, 2024.
% Each identified package was analyzed using a commercial malware scanner and reviewed by threat analysts for detailed insights.

Out of 3,658 suspicious package names flagged by our system, the majority (3,075, 86.1\%) contained malware. An additional 298 packages (8.4\%) were classified as anomalies due to the presence of dangerous functionality (\eg \texttt{eval}, \texttt{fs}) that did not appropriately handle user input or exhibited other risky coding practices. A smaller subset, 119 packages (3.4\%), was categorized as ``vulnerabilities'', where the code included multiple dangerous functionalities and mishandled user input in ways that could harm users’ systems, such as deleting files outside the \texttt{tmp} folder.
Furthermore, 27 packages were removed from the SPRs by their authors or maintainers before further analysis, leaving insufficient data to assess their content.
In addition, we identified 15 stealth typosquatting threats (0.4\%), which did not contain overtly malicious content but were flagged due to their deceptive intent. These packages impersonated legitimate ones to mislead users. Other less common categories, such as spyware-only attacks, were also observed in the dataset.

In addition to these packages that include


% These results highlight the diverse nature of typosquatting threats and the effectiveness of our system in identifying and categorizing them for further mitigation efforts.
% \TODO{69} were harmless forks/extensions, and \TODO{105} remained under investigation. Several AI-based impersonations on Hugging Face illustrate how attackers could exploit well-known model authors (\eg \texttt{meta-llama}).

\begin{table}[h]
\centering
\footnotesize
\caption{Typosquats uncovered in randomly selected 5000 packages from each SPR.}
\label{tab:EQ4_novelty}
\begin{tabular}{lcc}
\toprule
\textbf{Registry} & \textbf{Suspicious}& \textbf{Benign} \\
\midrule
npm         & 67  & 4933 \\
PyPI        & 29  & 4971 \\
RubyGem       & 239 & 4761 \\
\midrule
Maven       & 0  & 10000 \\
Golang       & 0  & 10000 \\
Hugging Face & 0  & 10000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{EQ5: System Efficiency and Scalability}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ5 Summary:}
Our system exhibits slower processing speeds due to additional sorting and LLM verification steps but maintains acceptable efficiency and strong scalability for deployment on SPRs.
\end{tcolorbox}
\fi

\paragraph{Method}
We evaluated the end-to-end system efficiency by measuring the average latency and throughput across 5,000 package inputs per ecosystem. The latency metrics include similarity comparison, neighbor search, false-positive (FP) verification, and overall system latency. Throughput was calculated as the number of packages processed per second.

\paragraph{Result}
\cref{tab:registry_performance} summarizes the performance metrics of our system across various registries, demonstrating acceptable latency with variations based on registry size and complexity. For example, npm shows a latency of 14.17 seconds per package, while PyPI achieves a faster latency of 7.22 seconds. RubyGems, with its larger package set and more complex naming structures, has a higher latency of 46.94 seconds per package. Our system also effectively supports previously unaddressed registries like Maven, Golang, and Hugging Face, which exhibit latencies of 5.98, 5.04, and 16.05 seconds per package, respectively. Importantly, it achieves substantial speed improvements in the neighbor search step, outperforming prior work by 73\%--91\% across all registries.




Although our system prioritizes accuracy over latency, it maintains performance levels suitable for production deployment. The relatively low throughput is primarily due to reliance on the OpenAI API and GPT-4o for metadata verification, which introduces significant inference times (\eg npm and PyPI require 11.3 and 6.81 seconds per package, respectively, for false-positive verification). However, the inclusion of LLMs greatly enhances detection accuracy, making this trade-off worthwhile. The additional latency from sorting and LLM verification is critical to reducing false positives and ensuring high detection accuracy. Furthermore, the system scales effectively, efficiently managing large registries like npm and PyPI. Future work could explore optimizing LLM inference or incorporating smaller, more efficient models to boost throughput without compromising accuracy.










\begin{table*}[ht]
    \centering
    \caption{System Latency and Throughput Metrics Across Registries: This comparison includes similarity latency, overall neighbor search latency, false-positive verification latency, and system latency. All metrics are evaluated using identical inputs, ensuring fair comparability across systems. Each latency value represents the time taken to compare a given unpopular package with all popular packages in the registry. Prior work does not support Maven, Golang, and Hugging Face because their names are too long to process~\cite{neupane2023beyondTyposquatting}.
    The FP verification for Maven, Golang, and Hugging Face was manually triggered (no suspicious neighbors identified).
    }
    % For npm, PyPI, and RubyGems, we use the same popularity threshold (15,000 weekly downloads) as \cite{neupane2023beyondTyposquatting} to be directly comparable to prior work.}
    % \footnotesize
    \label{tab:registry_performance}
    \resizebox{0.95\textwidth}{!}{%
    \begin{tabular}{lc|cc|cc}
        \toprule
        \textbf{Registry} & \textbf{\# Pop. Pkgs.} & \textbf{\textit{Typomind} Name Comparison Latency (s/pkg)} & \textbf{\textit{Our} Neighbor Search Latency (s/pkg)} & \textbf{FP Verification Latency (s/pkg)} & \textbf{System Latency (s/pkg)} \\
        \midrule
        npm        & \NumAllPopNPMPkgs       & 10.83 & 2.87  & 11.3 & 14.17  \\
        PyPI       & \NumAllPopPyPIPkgs      & 4.45  & 0.41  & 6.67 & 8.94 \\
        RubyGems   & \NumAllPopRubyPkgs      & 34.62  & 1.37  & 15.73 & 23.98  \\
        \midrule
        Maven      & \NumAllPopMavenPkgs     & N/A & 5.98  & (8.08) & (14.06)  \\
        Golang     & \NumAllPopGolangPkgs    & N/A & 5.04  & (12.04) & (17.08)  \\
        Hugging Face         & \NumAllPopHFPkgs        & N/A & 1.23 & (14.82) & (16.05)  \\

        \bottomrule
    \end{tabular}
    }
\end{table*}




% \subsection{EQ6: Coverage in Hierarchical Ecosystems}
% \WJ{Should we move this to EQ4 or simply cut this?}
% We compare single-embedding vs.\ multi-embedding approaches for Maven, Golang, and Hugging Face. Single-embedding treats package names as one string, while multi-embedding splits them into components (\eg \texttt{groupId:artifactId}, \texttt{author/model}).


% \subsection{EQ6: Comparison to SOTA Methods}
% \begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
% \textbf{EQ6 Summary:}
% Compared to SOTA methods, our system is good at capturing additional typosquatting categories, achieves a substantially lower false-positive rate, and maintains acceptable latency, making it well-suited for deployment on SPR backends while remaining effective for frontend on-demand requests.
% \end{tcolorbox}

\iffalse
\WJ{Below are the old evaluation results}
\subsection{Methodology}
\label{sec:Eval-Methodology}

\paragraph{EQ1: Embedding Model Effectiveness}


\paragraph{EQ2: Efficienct and Scalability}
We evaluate throughput (\texttt{packages/min}) and average query latency under two usage patterns:
\begin{itemize}[leftmargin=*]
    \item \textbf{Batch Scanning}: Processing a large snapshot (\TODO{N=500k} packages).
    \item \textbf{Real-Time Checks}: On-demand scanning for newly published packages in a mock CI pipeline.
\end{itemize}
We measure performance with/without embedding quantization, and with different indexing approaches (IVFFlat vs.\ brute force).



\subsection{Results and Analysis}
\label{sec:Eval-Results}

We now present empirical findings that address each research question.
\paragraph{EQ1: Embedding Model Effectiveness}



\paragraph{EQ2: Efficienct and Scalability}
Table~\ref{tab:EQ2_perf} shows placeholder results:




\paragraph{EQ3: }


\paragraph{EQ3: Metadata Verification Accuracy}
\cref{tab:EQ3_metadataFPR} details the reduction in FPR:

\begin{table}[h]
\centering
\footnotesize
\caption{Impact of metadata-based rules on false positives (EQ3).}
\label{tab:EQ3_metadataFPR}
\begin{tabular}{lcc}
\toprule
\textbf{Dataset} & \textbf{FPR (Before)} & \textbf{FPR (After)} \\
\midrule
Typomind       & \TODO{12.1\%} & \TODO{3.4\%} \\
\TypoSmart & \TODO{7.6\%}  & \TODO{1.2\%} \\
\bottomrule
\end{tabular}
\end{table}

We observe a \TODO{80\%} decrease in false positives, especially from rules like \emph{fork identification} (Rule~3) and \emph{overlapping maintainers} (Rule~8), confirming that many suspiciously named packages are in fact benign packages.



\paragraph{EQ4: Discovery of New Typosquats}





\paragraph{EQ5: Coverage in Hierarchical Ecosystems}
Table~\ref{tab:EQ5_hierarchical} shows that hierarchical embeddings catch \TODO{x\%} more attacks involving \emph{impersionation squatting} or \emph{compound squatting}:

\begin{table}[h]
\centering
\footnotesize
\caption{Typosquats identified with single vs.\ hierarchical embeddings (EQ5).}
\label{tab:EQ5_hierarchical}
\begin{tabular}{lcc}
\toprule
\textbf{Ecosystem} & \textbf{Single-Embedding} & \textbf{Hierarchical Embedding}\\
\midrule
Maven         & \TODO{xx} & \TODO{xx} \\
Golang        & \TODO{xx} & \TODO{xx} \\
Hugging Face  & \TODO{xx} & \TODO{xx} \\
\bottomrule
\end{tabular}
\end{table}

Examples include \texttt{facebook-llama} vs.\ \texttt{meta-llama}. Single-level name checks fail to detect this subtle impersonation, but multi-level embeddings spot the near-duplicate \texttt{author} dimension.



\subsection{Summary of Findings and Relation to System Guarantees}

Our empirical results reinforce the efficacy of each design choice. Table~\ref{tab:requirements_fulfillment} summarizes how each of our production requirements (\S\ref{sec: ProblemState-Prod}) and security guarantees (\S\ref{sec: ProblemState-Security}) is met:

% \begin{table}[h]
% \centering
% \footnotesize
% \caption{How we fulfill production requirements and security guarantees. \TODO{Update this table with the latest requirement changes)}}
% \label{tab:requirements_fulfillment}
% \begin{tabular}{p{0.32\columnwidth} p{0.55\columnwidth}}
% \toprule
% \textbf{Req./Guarantee} & \textbf{Implementation/Outcome} \\
% \midrule
% \textbf{Req$_{1}$: Frequent Metadata} & Step~1 ensures daily ingestion; catch newly released packages \\
% \textbf{Req$_{2}$: Manageable FP}    & Step~5 reduces false positives to \TODO{<x\%} in multiple datasets \\
% \textbf{Req$_{3}$: Cost-Effectiveness} & ANN + quantization achieve \TODO{xk pkgs/min} and short CI-latency \\
% \midrule
% \textbf{G$_{1}$: High Recall} & Intent-based name checks detect potential future malware \\
% \textbf{G$_{2}$: Low FPR}     & Nine metadata rules filter forks/brand expansions \\
% \textbf{G$_{3}$: Timely Detection} & Parallel scanning finishes 1M pkgs in \TODO{x hours} \\
% \textbf{G$_{4}$: Ecosystem Compatibility} & Hierarchical approach for Maven/Hugging Face etc. \\
% \bottomrule
% \end{tabular}
% \end{table}

Overall, these experiments confirm that our approach:
\begin{itemize}[leftmargin=*]
    \item \textbf{Captures malicious intent} by embedding domain-specific patterns (\textbf{EQ1}).
    \item \textbf{Operates at scale} via quantization and ANN indexing (\textbf{EQ2}), scanning large registries cost-effectively (\textbf{Req$_{3}$}).
    \item \textbf{Minimizes false positives} (\textbf{EQ3}) by verifying suspicious names against real-world metadata heuristics—achieving an acceptably low alert volume for practitioners (\textbf{Req$_{2}$}).
    \item \textbf{Extends coverage} to hierarchical ecosystems (\textbf{EQ5}), catching author squatting that simpler methods miss.
\end{itemize}

Hence, we conclude that our end-to-end pipeline meets the demands of large-scale software supply chain security (\cref{sec: ProblemState-Prod}), bridging both academic detection criteria and real-world deployability.
\fi
\iffalse

\section{Evaluation}
We evaluate TypoSmart design decisions and compare with existing work.
\cref{sec:Eval-exp-setup} describes our experimental setup, including the dataset preparation

\subsection{Experimental Setup}
\label{sec:Eval-exp-setup}
\subsection{Datasets}
\TODO{@Berk: Dataset structure: typosquat pkg, target pkg/legitimate pkg, registry, TP/FP/TN, typosquat category, source}
\WJ{Not sure if we need separate subsections}
%-------------------------------------------------------------------------------
\textbf{Datasets:}
\begin{itemize}
    \item The ground-truth dataset collected in typomind + latest blog posts including HF typosquats
    \item Ground-truth labeled in typomind new data
    \item Socket human reviewed typosquats
    \item Package name hallucination from LLM~\cite{spracklen2024LLMPackageHallucinations}
\end{itemize}

\subsection{State-Of-The-Art Baselines}

\textbf{Baselines:}
\TODO{@Berk: (1) Set up the environments for typomind and typobound (2) evaluation pipeline infrastructure
% (input: tool (--input=typomind) --registry=npm --suspicious_pkg=loadsh)
(3) connect the pipeline to our typosquat system using REST API.}

\begin{itemize}
    \item Typomind
    \item Typobound
    \item Socket typosquat detection system v1
\end{itemize}

\subsection{Research Questions}
In our evaluation, we ask the following questions:

\TODO{@Berk: Go to check Synk, Reverselab, etc. and add the latest data to our dataset.}
\begin{itemize}
    \item \textbf{RQ1:} How does the embedding model perform? \WJ{TODO: Embedding similarity measure + embedding visualization}
    \item \textbf{RQ2:} What is the efficiency of embedding-based approach?
    \item \textbf{RQ3:} To what extent the metadata validation can help improve the accuracy? \WJ{We will need some ablation study on this}
    \item \textbf{RQ4:} Can the system find new typosquats? \WJ{Production data + full scanning (sampling)}

    all packages = popular pkgs (50K) + unpop pkgs
    500K -> sample 16K (less than 1day)
    \item \textbf{RQ5:} Can we detect typosquats on Maven, Golang?
    \item \textbf{RQ6:} Are there typosquats on Hugging Face?

\end{itemize}

\subsection{Results and Analysis}
%-------------------------------------------------------------------------------
\subsubsection{Empirical Analysis}
\label{sec:results-empiricalAnalysis}
\begin{itemize}
    \item \textbf{RQ1:} What are the malicious \textit{contents} in the packages?
    \item \textbf{RQ2:} What are the \textit{intents} in the suspicious packages?
    \item \textbf{RQ3:} xxxx
\end{itemize}

\paragraph{Golang}
We analyzed the distribution of namespace, author names, and pacakge names.
Init function during package installation.

\paragraph{Maven}
We analyzed the distribution of group\_id and package\_id.

For some Maven packages, the attacker can use relocated package to do typosquat attack. Example:
\texttt{io.github.spring-cattle:cattle-commons} -> \texttt{io.gitlab.spring-cattle:cattle-commons}


\subsection{RQ1: Embedding Model}
\TODO{Compare to Table 3 in typomind paper}
Table 3: Performance of detection rules

\iffalse
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/embedding_comparison_distances.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/embedding_comparison_scores.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}
\fi
\section{Measurement Study}
\fi

\section{Discussion}
% \WJ{@Jamie Please take a look at this section as well.}
% \subsection{Responsible Disclosure}
% We promptly notified the respective package ecosystems of all malicious packages identified in \cref{xx} that remained publicly accessible at the time of discovery. Specifically, we reported \TODO{xx} malicious packages to npm, \TODO{yy} to PyPI, ..., and \TODO{zz} to Hugging Face. Each ecosystem has since taken disciplinary measures against these packages, and as a result, they have been removed from public access.

\subsection{Lessons Learned from Production}
% \TODO{Mik: yeah, I'm wondering about the ontology.  maybe we can add more cases or argue that the number of types could be smaller....}
% \WJ{@Mik, it would be great if you could provide any insights for the next two subsections.}

% \JD{\textbf{Lesson 1: TOOLNAME prevents real typosquats.}}
\paragraph{Lesson 1: \TypoSmart prevents real typosquats}
Over the past two months, we deployed the system in our industry partner's production environment, during which we identified and confirmed \NumProdReviewedTPTypos threats, with \NumProdUnreviewedTypos additional threats under review.
For the typosquatting threats we identified on npm, PyPI, and Golang, we ran a commercial malware scanner on those packages. This results in the removal of \NumRemovedPkgs packages within one month.

\iffalse
Additionally, our system successfully identified 12 suspicious packages on Maven, 4 packages on Golang, and \TODO{xx} packages on Hugging Face during deployment. However, after closer analyzing these packages, we found that the Maven packages are all false-positives which has missing package relocation metadata. Within Maven Central, our system returns 9 alarms while after manual analysis, those are the packages with relocation information while the relocation information is not present in the metadata in our database.
\JD{Were they taken down...? Say more.}
\fi

% \JD{\textbf{Lesson 2: False Positives Harm Our Customers.}}
\paragraph{Lesson 2: False Positives Harm Our Customers.}
% \JD{Write this instead as ``table 3 represents the most recent...'', but I think you should tell more stories about false positive behaviors so the reader sees your system is based on experience}
% \JD{This doesn't include any actual story or lesson about customer impact. If we do not have those, we should cut it. But you told me a story about customer complaining about you flagging their whole thing as malicious, do you have other stories? Don't want to deanonymize but you can be vague.}
Table 3 represents the most recent metadata verification rules. In deployment, we found additional cases where the package has a very similar name and the README of their package was missing which made our system classify it as suspicious stealth attack, while the package is actually legitimate. That package, specifically served as transitive package and therefore the package owner did not write a README. In such case, from a registry perspective, a false-positive will harm the reputation of the customer in the community if the package is flagged as suspicious. To avoid this case, we added an allowed list so that if the package is from our customers, then it should be considered as legitimate.
% For example, for R9, there are many relocated packages that exist both on Golang and Maven.
% Continuous enhancements to the system have been made through meticulous manual data analysis. For instance, we have incorporated additional rules to further minimize false positives, as detailed in \cref{tab:metadata_rules}.

% \JD{\textbf{Lesson 1: Ontology Matters.}}
\paragraph{Lesson 3: Ontology Matters.}
Despite these improvements, we have identified ontological limitations in our current approach to categorizing typosquatting threats. Specifically, the typosquatting categories outlined in \cref{tab:typosquat_taxonomy} do not provide sufficient meaningfulness for end-users in the alarming system (\cref{sec:SystemDesign-Step6}). To address this, we propose refining our categorization system to consider the malicious content and intent behind each package, including a risk-level classification.
This would allow a more nuanced assessment of each threat.
%This enhanced taxonomy can be developed by leveraging the metrics discussed in \cref{sec:SystemDesign-Step5}, allowing for a more nuanced assessment of each threat's severity and nature.

\paragraph{Lesson 4: We need more sophisticated analysis to discern intent.}
To bolster a more straightforward blocking policy, there is still a need to integrate additional tools and algorithms. One such tool is a differential scanner, which compares packages to identify discrepancies that may indicate malicious alterations. Additionally, implementing a grey-list system will enable us to place suspicious packages with low-risk levels into a monitored category. These grey-listed packages will undergo continuous surveillance of their behavior and content, ensuring that any emerging threats are promptly detected and addressed. By adopting these strategies, we aim to enhance the robustness and responsiveness of our typosquatting defense mechanisms.






\subsection{Limitations and Security Analysis}
% \subsection{Security Analysis}
\label{sec:Discussion-LimitatinoAndSecurityAnalysis}

This section discusses our system limitations and how attackers might bypass \TypoSmart.
% \subsection{Limitations and Security Analysis}
\label{sec:SystemDesign-Limitation}

\paragraph{Gaming Metrics}
% \JD{I added this, seemed relevant?}
Our system relies on software metrics to gauge the likelihood that a package is a typosquat.
These metrics might be gamed.
There has been little formal study of the feasibility of gaming these metrics, but recent work suggests both the possibility and some real-world examples~\cite{he2024FakeStars}.

% \WJ{Moved from \$5.7 here.}
% \JD{Distinguishing the primary limitation from your design and the secondary limitation from your implementation.}

% \TODO{Update examples here}
 % \JD{It would be helpful for analyzing the security properties you achieve. You would typically do that analysis as part of \$5 or \$6, with subsection titled  ``Security Analysis'' that gives a conceptual/theoretical discussion of how much of the attack surface your system is addressing and how easy it might be to bypass.}
\paragraph{Limitations in Neighbor Search}
\WJ{This phenomenon is known as the “curse of dimensionality” and it can affect the performance of vector semantic similarity search in certain scenarios}
One significant limitation of \TypoSmart lies in its ability to handle short names or acronyms. FastText, the underlying embedding model, struggles with short words (\eg \texttt{xml} vs. \texttt{yml}, with a similarity score of 0.7). The model’s reliance on character \textit{n}-grams often fails to capture subtle similarities effectively in such cases, providing an avenue for attackers to exploit short package names.
Although FastText emphasizes subword representations to improve embeddings, this focus reduces its ability to account for visual ambiguity or phonetic similarity (\eg \texttt{google} and \texttt{g00gle} appear less similar in the embedding space). To address this, we implemented a list of potential substitutions to identify cases of visual or phonetic ambiguity. However, this approach introduces additional computational overhead, slowing down the system.
To further enhance detection accuracy, we combine embedding similarity with Levenshtein distance. While this hybrid approach improves neighbor search for short names, it increases the computational cost and still does not fully resolve the limitations in representation.
Recent research demonstrates the potential of AI tools to generate sound-based squatting attacks~\cite{valentim2024xsquatter, valentim2022soundsquatting}, further exacerbating threats to the software supply chain.


% Efficient and accurate false-positive verification is critical for maintaining system reliability. Currently, we rely on OpenAI's commercial LLM API, specifically \texttt{GPT-4o}. While effective, transitioning to a local LLM or a model fine-tuned specifically for package typosquatting could significantly reduce latency and enhance the efficiency of FP verification.

\paragraph{Bypassing Metadata Verification}
Our reliance on OpenAI's \texttt{GPT-4o} API introduces obvious correctness issues (hallucination). It also entails security risks, as these models are vulnerable to jailbreaking attacks~\cite{yi2024jailbreak}. Adversaries could exploit this by using techniques like prompt injection or model hijacking~\cite{liu2023promptInjectionAttack, zou2023universalandTransferableAdvAttackonLM} to manipulate the model's responses. For example, attackers might craft prompts or metadata to trick the model into classifying a malicious package as legitimate, such as misrepresenting a malicious username as belonging to a trusted maintainer or aligning fake functionality with that of a legitimate package. Since the LLM is integral to verifying maintainers and functionality, such attacks could compromise the system’s defenses and allow malicious packages to bypass detection.
% \Andreas{Can an attacker who is aware of the defenses provided by your system design a typosquatting package to circumvent them? If so, what would that look like? If not, why not?}


\subsection{Future Directions}
\label{sec:Discussion-FutureWork}

\paragraph{Addressing Other Squatting Attacks:}
%Recent studies highlight the potential of AI-generated multilingual sound squatting attacks, which pose a significant threat to software package registries (SPRs)~\cite{valentim2024xsquatter, valentim2022soundsquatting}.
Our study focused on typosquatting on package names, but other constructs within SPRs and ecosystems are also viable targets.
One example is \textit{command squatting}, where malicious packages mimic command-line arguments (\eg \texttt{npm i help} vs. \texttt{npm i $--$help}). While creating static command lists for each registry would offer a temporary solution, maintaining them would be complex in these decentralized ecosystems. % a more scalable and efficient approach involves dynamically constructing and updating these lists through automated policies and tools. Implementing such dynamic systems could greatly mitigate the risk of command squatting while reducing the need for manual maintenance.


\paragraph{Enhancing Representations for Typosquatting Detection:}
\label{sec:Discussion-FutureWork-Representation}
Improving the representation of typosquats is crucial for more robust detection. While FastText captures semantic similarity through subword embeddings, it struggles with typographical variations, particularly for short words or acronyms. Fine-tuning FastText or training a more efficient model on domain-specific corpora including both correct and misspelled terms can address these limitations.
Augmenting training data with synthetic typos and incorporating typo normalization or correction techniques before embedding generation can significantly reduce errors. Advanced models, such as transformer-based architectures fine-tuned with contrastive learning on typo-specific datasets, present a promising alternative for enhancing detection accuracy and reliability.
This approach has proven effective in combating domain typosquatting, but no research has been conducted targeting package typosquatting~\cite{koide2023phishreplicant}. One challenge is the limited availability of data for verifying package squatting cases.
LLMs might help here~\cite{tozzi2024packageHallucinatinoThreat}.




\paragraph{Mitigating LLM Hallucination in Code Generation}
The increasing use of LLMs for code generation has introduced new challenges, as these models often hallucinate package names or generate commands for nonexistent or maliciously similar packages~\cite{spracklen2024LLMPackageHallucinations}. These hallucinations pose serious threats to the security of the software supply chain~\cite{vulcan2023CanyouTrustChatGPTPKGRecommendation, tozzi2024packageHallucinatinoThreat}. Addressing this issue requires implementing typo or hallucination correction mechanisms in LLM-based package recommenders. Verifying package legitimacy, detecting typos, and integrating contextual checks can prevent the propagation of incorrect package names, reducing the risks associated with hallucinations.

\paragraph{Meta-Learning for Malicious Package Detection}
Meta-learning approaches offer significant potential for improving malicious package detection. By leveraging anomaly detection techniques and metadata analysis~\cite{halder2024maliciousPackageDetectionUsingMetadataInfo}, systems can dynamically adapt to evolving attack strategies. Meta-learning frameworks could analyze patterns across registries and rapidly identify emerging threats, enhancing the scalability and robustness of detection systems. Integrating such frameworks will be key to staying ahead of increasingly sophisticated attackers.

\section{Conclusion}

We present \TypoSmart, an embedding-based typosquatting detection system.
Based on real-world attack patterns, we refined the typosquatting definition and developed a taxonomy based on engineering practices.
\TypoSmart is being used in production at our industry partner and contributed to the removal of \NumRemovedPkgs typosquatting threats in one month.
Compared to SOTA methods, our system is good at capturing additional typosquatting categories, achieves a substantially lower false-positive rate, and maintains acceptable latency, making it well-suited for deployment on SPR backends while remaining effective for frontend on-demand requests.
We shared our insights from production experience, customer feedback, the need for an improved ontology, and outlined future directions.
% \JD{Must finish the conclusion.}

% \JD{Review the references for nits. For example, the first few are missing URLs; [17] does not have a venue; it would be nice to write the acronyms of venues for the lazy reviewer (CCS, USENIX, ICSE, WWW, etc.)}

% \section*{Data availability}
% \label{sec: artifact}
%

\pagebreak
% \JD{Here is copy-paste from Kelechi, please revise}

\textit{The call for papers states that an extra page is allotted to discuss ethics considerations and compliance with the open science policy. This page contains that content.}
% \WJ{TODO: This needs to be shorten to one page.}
\section{Ethical Considerations} \label{sec:ethics}

This section discusses the ethical implications of designing, implementing, and deploying a system to detect typosquatting attacks across multiple software registries.
We also outline the risks and benefits associated with sharing detailed detection methods and results.

We attest that:
\begin{itemize}[itemsep=0.1ex]
    \item Our data collection and handling was consistent with conference guidelines and policies.
    \item Our study uses only publicly available metadata in SPRs, eliminating privacy risks and not requiring IRB approval.
    %\item We will share our methods and, where possible, sanitized datasets under open licenses to enable replication while protecting user security.
    \item We promptly disclosed all confirmed malicious/suspicious packages to the registry maintainers.
    \item To the best of our ability, the research team has evaluated all ethical implications, ensuring that all activities are conducted responsibly and that future actions will continue to uphold these standards.
    \end{itemize}

% Our typosquat detection system analyzes package repositories across six major open-source ecosystems: npm, PyPI, RubyGems, Maven, Golang, and Hugging Face.

\subsection{Data Collection and Scope}
We obtain comprehensive package metadata --- including package names, versions, download statistics, maintainers, and other relevant attributes --- from a private database maintained by our industry partner, a software supply chain security company.
This method ensures that data collection is both systematic and compliant with legal and ethical standards. % as the database aggregates publicly available information without the need for additional scraping or direct interaction with repository maintainers.

These data sources do not include sensitive personal information (SPI) about package maintainers or users.
By relying solely on metadata necessary for detecting suspicious naming patterns and assessing package-level attributes, we minimize privacy concerns. % and focus exclusively on information that is already publicly accessible.

% \JD{Move this block to the opening remarks of \$9 because it's a summary of the whole section.}


\subsection{Responsible Disclosure}
Throughout our study, we promptly reported newly discovered malicious or suspicious packages to the respective registry maintainers.
We delayed publication of any technical indicators that could facilitate ongoing attacks until the registries took corrective actions (\eg removal of malicious packages).
This procedure protects users from live threats while still allowing the broader community to learn from our findings and detection strategies.

\subsection{Risks and Benefits}
% \WJ{Need improvement on this subsection or we can cut it.}
% \JD{LGTM actually, please keep it as-is}
In our judgment, the potential benefits of providing actionable insights and better protective measures against typosquatting outweigh the risks. %, especially when combined with careful disclosure practices.
%We discuss the risks and benefits of our decision in detail below.

We perceive two main risks:

\begin{itemize} \item \textbf{Highlighting Attack Vectors.} By publicizing the detection pipeline, we inform potential attackers about its limitations or thresholds, potentially encouraging them to find new ways to evade detection. \item \textbf{Unintentional Disclosure of Live Threats.} Publishing lists of suspected malicious packages could draw attention to attacks still under investigation, particularly if registry maintainers have not yet removed them. Sharing such data prematurely could increase user exposure to live threats. \end{itemize}

We perceive two main benefits:

\begin{itemize} \item \textbf{Improved Ecosystem Security.} Disclosing our detection approach and findings helps registries, maintainers, and security practitioners find malicious packages faster, reducing exposure to malware.
\item \textbf{Shared Knowledge for the Community.} By openly describing our methods, we enable other researchers and industry professionals to replicate and extend our techniques, contributing to broader, collaborative improvements in supply chain security. \end{itemize}

\section{Compliance with Open Science Policy}
\label{sec:OpenScience}

\WJ{@Mik, please double check the sharable artifact content below.}
We acknowledge the USENIX Security open science policy, which encourages authors to share data and artifacts.
The artifacts associated with this project are:
  (1) the case study we conducted on analyzing prior typosquatting data (\cref{sec:ProblemState-CaseStudy1-TP}, \cref{sec:ProblemState-CaseStudy2-FP});
  (2) the commercial metadata database (\cref{sec:SystemDesign-Step1-Metadata});
  (3) the embedding database we created (\cref{sec:SystemDesign-Step2});
  (4) the other prototypes of our system (\cref{sec:SystemDesign-Step3}--\cref{sec:SystemDesign-Step6});
  and
  (5) our evaluation scripts and datasets (\cref{sec:Eval}).

% \JD{Please begin with an enumeration of all artifacts. ``The artifacts associated with this project are: X Y Z.'' (Then the bullet list should cover all the artifacts}

\begin{itemize}[left=0.1cm]
\item \textbf{Artifacts Shared.}
We will publish our case study, system’s prototype implementation (excluding the commercial database) (\cref{sec:SystemDesign-Step2}, \cref{sec:SystemDesign-Step3}--\cref{sec:SystemDesign-Step6}), and an anonymized summary of evaluation results (\cref{sec:Eval}). This includes our case study scripts and data, as well as the typosquatting taxonomy (\cref{sec:ProblemState-CaseStudy1-TP}).
\item \textbf{Artifacts Withheld.} To avoid enabling new attacks or disrupting ongoing investigations, we will not publicly list any active, unreleased, or unresolved suspicious packages. We likewise will not share raw metadata tied to live threats if doing so could aid adversaries or disclose unresolved vulnerabilities.
As informed by the authors from \cite{spracklen2024LLMPackageHallucinations}, we will not share the raw dataset from their work but their data is available by request. We will also not share the commercial metadata database (\cref{sec:SystemDesign-Step1-Metadata}).
\end{itemize}

%\noindent By diligently balancing transparency and caution, we aim to advance the scientific understanding of typosquatting while protecting the broader software community from emerging threats.

An artifact will be made available following the USENIX process and once it completes the review process from our industry partner.
%Resources will be made available in a public repository at this anonymous GitHub repository: \url{https://github.com/xxx}\TODO{Update the URL}.

\iffalse
\section{Compliance with Open Science Policy} \label{sec:OpenScience}


We acknowledge that USENIX Security has an open science policy: that authors are expected to openly share their research artifacts by default.
The research artifacts associated with this study are:
\begin{itemize}
    \item Raw transcripts of interviews
    \item Anonymized transcripts of interviews
    \item Interview protocol
    \item Codebook
\end{itemize}

\myparagraph{Things we have shared}
The main contribution of this work is the system design and evaluation results.
We share the prototype of our system in the artifact, as well as the data-driven analysis which includes the typosquat taxonomy (\cref{sec: SystemDesign-Step5})
Our full typosquat taxonomy is included in~\cref{sec:appendix-FPTaxonomy}. We also include the disclosed \TODO{xx} suspicious packages from (\cref{sec:Eval-Results}).

\myparagraph{Things we cannot share}
For undisclosed suspicious packages, we were not able to share them. We are also not able to share more details about the company implementation. We are also informed not to share the dataset from \cite{spracklen2024LLMPackageHallucinations}.
\fi

\raggedbottom
\pagebreak

{
\normalsize
% \begin{spacing}{0.5}
\bibliographystyle{plain}
\bibliography{bibliography/Reference, bibliography/WenxinZotero, bibliography/DualityLab}
% \end{spacing}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\newpage

\WJ{Notes for appendices:
These appendices may be included to assist reviewers who may have questions that fall outside the stated contribution of the paper on which your work is to be evaluated, or to provide details that would only be of interest to a small minority of readers. The paper should be self-contained without appendices.
To accommodate additional material requested by reviewers, the revisions for papers that previously received an “Invite for Major Revision” or “Accept on Shepherd Approval” decision can use up to 14 typeset pages for the main body of the paper, excluding the one page for discussing ethics considerations and compliance with the open science policy, the bibliography, and well-marked appendices.
Once accepted, the final version should be no longer than 20 pages, including the bibliography and any appendices.
}

\clearpage

\appendix
% \section{LLM Filter for Data Analysis}

% \WJ{This section should include: (1)This filter was developed using \texttt{gpt-4o}, with implementation details provided in Appendix~\ref{Appendix-LLMFilter}. (2) The frequency of each category is summarized in Appendix~\ref{Appendix-LLMFilter}}.

% This appendix presents the designed of our LLM filter and data used in \cref{sec:ProblemState-CaseStudy2-FP}.


\section{Taxonomy of Engineering Practices}
\label{Appendix-taxonomy}

\cref{fig:FP-taxonomy} presents the taxonomy we created in \cref{sec:ProblemState-CaseStudy2-FP}.


\section{More System Implementation Details}

\subsection{Visualization of Embedding}
\label{Appendix-EmbeddingVis}
\cref{fig:embeddingSpace} shows a \texttt{UMAP} visualization of 100K npm packages~\cite{mcinnes2018umap}.
% starting with the letter ``a''.
Clusters often form around minor spelling variations,
% (\eg \texttt{color-string} vs.\ \texttt{color-name}),
demonstrating how embeddings capture both lexical and semantic relationships. This grouping is central to detecting maliciously similar names that differ by a single character or switched letters.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/npm_embeddings_visualization_100k.pdf}
    \caption{UMAP visualization of embedding space of 10K npm packages.
    % \TODO{Update this figure.}\WJ{Likely we will cut this figure since it does not include much useful knowledge. We can put it in the appendix instead.}
    }
    \label{fig:embeddingSpace}
\end{figure}




\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/FP_taxonomy.pdf}
    \caption{Taxonomy of engineering practices.}
    \label{fig:FP-taxonomy}
\end{figure*}

\subsection{Detailed Embedding Efficiency Measurement}
\label{Appendix-Efficiency}
\cref{tab:EQ1-Efficiency}
highlights the efficiency of embedding models across different quantization formats (\texttt{float32}, \texttt{float16}, and \texttt{int8}), indexing overhead, and memory usage. While \texttt{float16} and \texttt{int8} offer comparable throughput and latency due to similar I/O and storage overheads, \texttt{int8} exhibits slightly higher latency in embedding creation due to additional quantization steps. Across ecosystems, \texttt{float16} and \texttt{int8} achieve significant improvements in processing speed and memory efficiency compared to \texttt{float32}, demonstrating their suitability for scalable embedding-based systems.








\subsection{Implementation of metadata verification rules}
\label{Appendix-MetadataVerification}
Algorithm~\ref{alg:FPRules} shows our implementation of the engineering practice categories. We apply both heuristic rules and LLM-based rules to measure the attributes from \cref{tab:typosquat_taxonomy} and use this algorithm to check if a suspicious pair should be classified as true-positive or false-positive.

\begin{algorithm}[H]
\caption{Heuristic Rules for False-positive Verification}
\begin{algorithmic}[1]

\STATE \textbf{Input:} $\text{typoDoc}, \text{legitDoc}, \text{registry}$
\STATE \textbf{Output:} $\text{Boolean}, \text{metrics}, \text{explanation}$

\STATE \textbf{Initialize:} Set all keys in $\text{metrics}$ to $\text{None}$

\STATE \textbf{for each} key in $\text{metrics}$:
\STATE \quad $\text{metrics[key]} \gets \text{None}$

\STATE Populate $\text{metrics}$ using helper checks:
\STATE \quad $\text{metrics} \gets \_\text{check\_package\_naming\_and\_purpose}(\text{typoDoc}, \text{legitDoc})$
\STATE \quad $\text{metrics["overlapped\_maintainers"]} \gets \_\text{has\_overlapped\_maintainers}(\text{typoDoc}, \text{legitDoc}, \text{registry})$
\STATE \quad $\text{metrics["comprehensive\_metadata"]} \gets \_\text{has\_comprehensive\_metadata}(\text{typoDoc})$
\STATE \quad $\text{metrics["active\_development"]} \gets \_\text{is\_actively\_developed}(\text{typoDoc})$

\STATE \textbf{If} $\neg \text{metrics["is\_adversarial\_name"]}$ \textbf{and} $\neg \text{metrics["is\_suspicious"]}$:
\STATE \quad \textbf{If} $\text{metrics["obvious\_not\_typosquat"]}$ \textbf{or} $\text{metrics["fork\_naming"]}$ \textbf{or} $\text{isTest}$ \textbf{or}
\STATE \quad \quad $\text{metrics["is\_known\_maintainer"]}$ \textbf{or} $\text{metrics["distinct\_purpose"]}$ \textbf{or} $\text{metrics["overlapped\_maintainers"]}$:
\STATE \quad \quad \textbf{return} $(\text{True}, \text{metrics}, \text{explanation})$

\STATE \textbf{If} $\text{metrics["is\_adversarial\_name"]}$ \textbf{and} $\neg \text{metrics["distinct\_purpose"]}$:
\STATE \quad \textbf{return} $(\text{False}, \text{metrics}, \text{explanation})$

\STATE \textbf{If} $\neg \text{metrics["comprehensive\_metadata"]}$ \textbf{or} $\neg \text{metrics["distinct\_purpose"]}$ \textbf{and}
\STATE \quad $\neg \text{metrics["is\_adversarial\_name"]}$ \textbf{or} $\neg \text{metrics["active\_development"]}$:
\STATE \quad \textbf{return} $(\text{False}, \text{metrics}, \text{explanation})$

\STATE \textbf{If} $\text{metrics["is\_adversarial\_name"]}$ \textbf{and} ($\neg \text{metrics["distinct\_purpose"]}$ \textbf{or}
\STATE \quad $\neg \text{metrics["comprehensive\_metadata"]}$):
\STATE \quad \textbf{return} $(\text{False}, \text{metrics}, \text{explanation})$

\STATE $\text{isDeprecated} \gets \_\text{check\_deprecated}(\text{typoDoc})$
\STATE \textbf{If} $\text{isDeprecated}$ \textbf{and} $\text{metrics["is\_adversarial\_name"]}$:
\STATE \quad \textbf{return} $(\text{True}, \text{metrics}, \text{explanation})$

\STATE \textbf{If} $\text{metrics["is\_suspicious"]}$:
\STATE \quad \textbf{return} $(\text{False}, \text{metrics}, \text{explanation})$

\STATE \textbf{Return:} $(\text{True}, \text{metrics}, \text{explanation})$

\end{algorithmic}
\label{alg:FPRules}
\end{algorithm}

\begin{table*}[h]
\centering
\caption{Evaluation of embedding model efficiency, \textit{HNSW} indexing overhead, and memory usage. The throughput and latency differences between \texttt{float16} and \texttt{int8} are minimal due to similar overall I/O and storage overheads, as well as the efficient handling of embeddings using \texttt{pgvector}. However, some increased latency in \texttt{int8} embedding creation is observed due to additional quantization and processing steps required for integer-based representations.}
% Updated \texttt{float16} and \texttt{int8} quantization formats.}
\resizebox{0.95\textwidth}{!}{%
\scriptsize
\begin{tabular}{l l r r r r r r r}
\toprule
\textbf{Quantization} & \textbf{Ecosystem} & \textbf{Throughput} & \textbf{Avg Batch} & \textbf{Avg Pkg} & \textbf{Total} & \textbf{PG Table} & \textbf{PG DB} & \textbf{Indexing} \\
& & \textbf{(batches/s)} & \textbf{Latency (s)} & \textbf{Latency (\textmu s)} & \textbf{Time (s)} & \textbf{Size (GB)} & \textbf{Size (GB)} & \textbf{Time (s)} \\
\midrule
\multirow{6}{*}{float32}
& Hugging Face & 4.20 & 1062.44 & 2380.68 & 1941.68 & 9.25 & 45.26 & 4.46 \\
& Maven & 4.59 & 697.16 & 2180.61 & 1380.76 & 6.57 & 46.97 & 4.91 \\
& Golang & 2.42 & 3778.33 & 4135.75 & 7905.49 & 21.36 & 54.81 & 7.80 \\
& npm & 3.73 & 7463.84 & 2678.89 & 13269.07 & 21.04 & 60.64 & 7.42 \\
& PyPI & 9.57 & 312.03 & 1045.52 & 602.95 & 2.62 & 61.49 & 4.39 \\
& RubyGems & 9.54 & 116.86 & 1049.14 & 220.00 & 0.94 & 61.80 & 4.07 \\
\midrule
\multirow{6}{*}{float16}
& Hugging Face & 19.67 & 203.86 & 508.61 & 414.82 & 3.22 & 36.73 & 4.75 \\
& Maven & 20.44 & 156.54 & 489.42 & 309.90 & 2.51 & 34.74 & 4.77 \\
& Golang & 19.07 & 520.25 & 524.55 & 1002.68 & 7.56 & 29.70 & 4.79 \\
& npm & 36.94 & 669.87 & 270.72 & 1340.91 & 6.69 & 22.20 & 5.97 \\
& PyPI & 38.33 & 76.03 & 261.04 & 150.54 & 0.77 & 21.38 & 4.23 \\
& RubyGems & 37.88 & 28.09 & 264.40 & 55.44 & 0.28 & 21.02 & 4.35 \\
\midrule
\multirow{6}{*}{int8}
& Hugging Face & 22.12 & 192.31 & 452.20 & 368.81 & 3.22 & 36.73 & 4.84 \\
& Maven & 18.32 & 161.71 & 546.01 & 345.73 & 2.51 & 34.72 & 4.81 \\
& Golang & 19.23 & 499.12 & 520.22 & 994.40 & 7.56 & 29.70 & 4.75 \\
& npm & 35.28 & 719.06 & 283.46 & 1404.03 & 6.69 & 22.20 & 5.28 \\
& PyPI & 37.57 & 80.66 & 266.35 & 153.60 & 0.77 & 21.37 & 4.31 \\
& RubyGems & 39.71 & 26.70 & 252.35 & 52.92 & 0.28 & 21.00 & 4.23 \\
\bottomrule
\end{tabular}%
}
\label{tab:EQ1-Efficiency}
\end{table*}

\iffalse
\subsection{Pseudo-code for \texttt{TypoSim} Similarity Function}

The TypoSim similarity function (Algorithm~\ref{alg:typosim}) provides \TypoSmart with the capability to assess similarity between package names and detect potential typosquatting threats.
The algorithm integrates multiple dimensions of similarity, including Normalized Damerau-Levenshtein Distance, N-gram Similarity, Phonetic Similarity, Typosquat-Specific Similarity, and Substring Similarity, each designed to capture distinct patterns of name manipulation.
The function then computes a maximum similarity score across these metrics.
At the end, it returns both the maximum similarity value and the individual similarity scores for further analysis.

\iffalse
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\small, columns=fullflexible]
class TypoSim:
    def __init__(self):
        self.max_sim = 0

    def __call__(self, name1, name2):
        similarities = {
            "normalize_damerau_levenshtein": self.calculate_normalized_damerau_levenshtein(name1, name2),
            "n_gram_similarity": self.calculate_n_gram_similarity(name1, name2),
            "phonetic_similarity": self.calculate_phonetic_similarity(name1, name2),
            "typosquat_similarity": self.calculate_typosquat_similarity(name1, name2),
            "substring_similarity": self.calculate_substring_similarity(name1, name2),
        }
        self.max_sim = max(similarities.values())
        return self.max_sim, similarities

    def calculate_normalized_damerau_levenshtein(self, name1, name2):
        if not name1 or not name2: return 0.0
        max_len = max(len(name1), len(name2))
        return 1 - (distance(name1, name2) / max_len)

    def calculate_n_gram_similarity(self, s1, s2, n=2):
        if len(s1) < n or len(s2) < n: return 0.0
        return intersection(s1_ngrams, s2_ngrams) / union(s1_ngrams, s2_ngrams)

    def calculate_phonetic_similarity(self, s1, s2):
        if not s1 or not s2: return 0.0
        return avg(soundex_similarity(s1, s2), metaphone_similarity(s1, s2))

    def calculate_typosquat_similarity(self, s1, s2):
        s1, s2 = normalize(s1, s2)
        apply_substitutions()
        return calculate_similarity()

    def calculate_substring_similarity(self, s1, s2):
        overlaps = find_overlaps(split(s1), split(s2))
        return compute_similarity(overlaps) + bonuses(overlaps)
\end{lstlisting}
\fi

\begin{algorithm}[h]
\caption{TypoSim Similarity Computation}
\label{alg:typosim}
\begin{algorithmic}[1]

\STATE \textbf{Initialize} max\_sim $\gets$ 0

\STATE \textbf{Function} TypoSim(name1, name2):
\STATE \quad similarities $\gets$ \{
\STATE \quad \quad \texttt{"normalize\_damerau\_levenshtein"} $\gets$ CalculateNormalizedDamerauLevenshtein(name1, name2),
\STATE \quad \quad \texttt{"n\_gram\_similarity"} $\gets$ CalculateNGramSimilarity(name1, name2),
\STATE \quad \quad \texttt{"phonetic\_similarity"} $\gets$ CalculatePhoneticSimilarity(name1, name2),
\STATE \quad \quad \texttt{"typosquat\_similarity"} $\gets$ CalculateTyposquatSimilarity(name1, name2),
\STATE \quad \quad \texttt{"substring\_similarity"} $\gets$ CalculateSubstringSimilarity(name1, name2)
\STATE \quad \}
\STATE \quad max\_sim $\gets$ \textbf{MAX}(similarities.values())
\STATE \quad \textbf{return} (max\_sim, similarities)

\STATE \textbf{Function} CalculateNormalizedDamerauLevenshtein(name1, name2):
\STATE \quad \textbf{if} name1 = \texttt{NULL} or name2 = \texttt{NULL}:
\STATE \quad \quad \textbf{return} 0.0
\STATE \quad max\_len $\gets$ \textbf{MAX}(\textbf{LENGTH}(name1), \textbf{LENGTH}(name2))
\STATE \quad \textbf{return} $1 - \frac{\text{distance}(name1, name2)}{\text{max\_len}}$

\STATE \textbf{Function} CalculateNGramSimilarity(s1, s2, n):
\STATE \quad \textbf{if} \textbf{LENGTH}(s1) $<$ n or \textbf{LENGTH}(s2) $<$ n:
\STATE \quad \quad \textbf{return} 0.0
\STATE \quad \textbf{return} $\frac{\text{INTERSECTION}(s1\_ngrams, s2\_ngrams)}{\text{UNION}(s1\_ngrams, s2\_ngrams)}$

\STATE \textbf{Function} CalculatePhoneticSimilarity(s1, s2):
\STATE \quad \textbf{if} s1 = \texttt{NULL} or s2 = \texttt{NULL}:
\STATE \quad \quad \textbf{return} 0.0
\STATE \quad \textbf{return} $\text{AVG}(\text{SoundexSimilarity}(s1, s2), \text{MetaphoneSimilarity}(s1, s2))$

\STATE \textbf{Function} CalculateTyposquatSimilarity(s1, s2):
\STATE \quad s1, s2 $\gets$ Normalize(s1, s2)
\STATE \quad ApplySubstitutions()
\STATE \quad \textbf{return} CalculateSimilarity()

\STATE \textbf{Function} CalculateSubstringSimilarity(s1, s2):
\STATE \quad overlaps $\gets$ FindOverlaps(Split(s1), Split(s2))
\STATE \quad \textbf{return} ComputeSimilarity(overlaps) + Bonuses(overlaps)

\end{algorithmic}
\end{algorithm}


\iffalse
\begin{algorithm}[h]
\small
% \caption{Heuristic Rules for False-positive Verification. \TODO{Replace with a figure.} \WJ{TODO: Move this to appendix}}
\caption{Heuristic Rules for False-positive Verification}
\label{algo:heuristic_false_positive}

\KwIn{\texttt{typoDoc}, \texttt{legitDoc}, \texttt{registry}}
\KwOut{(\texttt{Boolean}, \texttt{metrics}, \texttt{explanation})}

\BlankLine
\CommentSty{/* Initialize all metrics keys to None */}
\ForEach{key in \texttt{metrics}}{
    \texttt{metrics[key]} $\gets$ \texttt{None}\;
}

\BlankLine
\CommentSty{/* Populate metrics via helper checks */}
\texttt{metrics} $\gets$ \texttt{\_check\_package\_naming\_and\_purpose}(\texttt{typoDoc}, \texttt{legitDoc})\;
\texttt{metrics["overlapped\_maintainers"]} $\gets$ \texttt{\_has\_overlapped\_maintainers}(\texttt{typoDoc}, \texttt{legitDoc}, \texttt{registry})\;
\texttt{metrics["comprehensive\_metadata"]} $\gets$ \texttt{\_has\_comprehensive\_metadata}(\texttt{typoDoc})\;
\texttt{metrics["active\_development"]} $\gets$ \texttt{\_is\_actively\_developed}(\texttt{typoDoc})\;

\BlankLine
\CommentSty{/* Early non-typosquat checks */}
\If{( $\neg$\texttt{metrics["is\_adversarial\_name"]}) \textbf{and} ( $\neg$\texttt{metrics["is\_suspicious"]})}{
    \If{\texttt{metrics["obvious\_not\_typosquat"]} \textbf{or}
       \texttt{metrics["fork\_naming"]} \textbf{or}
       \texttt{isTest} \textbf{or}
       \texttt{metrics["is\_known\_maintainer"]} \textbf{or}
       \texttt{metrics["distinct\_purpose"]} \textbf{or}
       \texttt{metrics["overlapped\_maintainers"]}}{
        \Return (\texttt{True}, \texttt{metrics}, \texttt{explanation})\;
    }
}

\BlankLine
\CommentSty{/* Potential typosquat cases */}
\If{\texttt{metrics["is\_adversarial\_name"]} \textbf{and}
   ($\neg$\texttt{metrics["distinct\_purpose"]})}{
    \Return (\texttt{False}, \texttt{metrics}, \texttt{explanation})\;
}

\If{( $\neg$\texttt{metrics["comprehensive\_metadata"]} \textbf{or}
     $\neg$\texttt{metrics["distinct\_purpose"]})
    \textbf{and}
    ( $\neg$\texttt{metrics["is\_adversarial\_name"]} \textbf{or}
      $\neg$\texttt{metrics["active\_development"]})}{
    \Return (\texttt{False}, \texttt{metrics}, \texttt{explanation})\;
}

\If{\texttt{metrics["is\_adversarial\_name"]} \textbf{and}
   ( $\neg$\texttt{metrics["distinct\_purpose"]} \textbf{or}
     $\neg$\texttt{metrics["comprehensive\_metadata"]})}{
    \Return (\texttt{False}, \texttt{metrics}, \texttt{explanation})\;
}

\BlankLine
\CommentSty{/* Check for deprecation */}
\texttt{isDeprecated} $\gets$ \texttt{\_check\_deprecated}(\texttt{typoDoc})\;
\If{(\texttt{isDeprecated} \textbf{and} \texttt{metrics["is\_adversarial\_name"]})}{
    \Return (\texttt{True}, \texttt{metrics}, \texttt{explanation})\;
}

\BlankLine
\CommentSty{/* Final suspicious check */}
\If{\texttt{metrics["is\_suspicious"]}}{
    \Return (\texttt{False}, \texttt{metrics}, \texttt{explanation})\;
}

\BlankLine
\CommentSty{/* Otherwise, likely false positive */}
\Return (\texttt{True}, \texttt{metrics}, \texttt{explanation})\;

\end{algorithm}
\fi

\fi

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks