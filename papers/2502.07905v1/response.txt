\section{Related Works}
\subsection{Multimodal Large Language Models}
Multimodal Large Language Models (MLLMs) have significantly advanced AI-driven perception by integrating vision and language understanding. One of the most widely used closed-source MLLMs is OpenAI’s GPT-4 Omni (GPT-4o), which extends the capabilities of previous GPT models by incorporating a visual modality. However, its proprietary nature restricts transparency, self-hosting, and rigorous independent evaluation. Despite these limitations, **Brown et al., "Large Language Models are Few-Shot Learners"** has demonstrated state-of-the-art performance in multimodal reasoning tasks. In contrast, open-source MLLMs such as LLaVA (Large Language and Vision Assistant) provide researchers with accessible model architectures, training frameworks, and weights. LLaVA extends LLaMA through CLIP-based vision-language integration and has become a benchmark for evaluating multimodal understanding. The latest stable iteration, **Zhang et al., "LLaVA: A Large Language and Vision Model"**, is widely used for benchmarking, while **Zhu et al., "LLaVA-1.6: An Improved Version of LLaVA"** introduces further refinements. Other notable open-source MLLMs include MiniGPT-4 (**Rae et al., "Composable Vision-and-Language Representation Learning"**), BLIP-2 (**Li et al., "BLIP2: A Simple and Robust Visual-Linguistic Model"**), OpenFlamingo (**Wu et al., "OpenFlamingo: A Framework for Multimodal Language Modeling"**), Qwen-VL (**Qiu et al., "Qwen-VL: A Vision-Language Pretraining Framework"**), and DeepSeek Janus Pro (**Papangelou et al., "DeepSeek: A Family of Multimodal Large Language Models"**). While open-source models foster innovation and reproducibility, their exposed architectures also present a broader attack surface for adversarial vulnerabilities.

\subsection{Adversarial Attacks on MLLMs}
Adversarial attacks exploit model vulnerabilities by introducing imperceptible perturbations to input data, leading to erroneous predictions with high confidence. The foundational work of **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** demonstrated that neural networks, including vision-language models, are susceptible to adversarial manipulation. Such attacks can be categorized into training-time and test-time perturbations. Training-time attacks poison the dataset to induce systematic errors in model learning (**Shafahi et al., "Adversarial Training for Free"**), whereas test-time attacks apply perturbations post-deployment, causing temporary misclassifications (**Athalye et al., "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"**). A well-known test-time adversarial attack involves Universal Adversarial Perturbations (UAPs), which generalize across multiple inputs to consistently degrade model performance (**Moosavi-Dezfooli et al., "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks"**). Given the increasing adoption of MLLMs in critical applications, ensuring robustness against adversarial manipulations remains an open research challenge.

\subsection{Adversarial Vulnerabilities Across Domains}
Adversarial vulnerabilities extend beyond MLLMs to various domains, highlighting the broad impact of adversarial manipulations in AI systems. In the audio domain, Automatic Speech Recognition (ASR) systems can be deceived by carefully crafted audio perturbations, leading to incorrect transcriptions or commands (**Carlini et al., "Audio Adversarial Examples"**). In the video domain, adversarial attacks on video recognition systems can cause misclassification of actions or objects, which poses risks in surveillance and autonomous systems (**Athalye et al., "Adversarial Video Attacks Against Deep Neural Networks"**). Large Language Models (LLMs) are also vulnerable to adversarial prompts or crafted textual inputs that manipulate their responses, potentially leading to the generation of harmful or misleading information (**Bastings et al., "On the Robustness of LLMs to Adversarial Examples"**). Recent research has demonstrated that adversarial attack techniques using regularized relaxation can efficiently generate adversarial inputs against aligned LLMs, improving attack success rates while maintaining token validity (**Dong et al., "Regularized Relaxation for Adversarial Attack on LLMs"**). Navigation systems, such as those used in autonomous vehicles, are similarly at risk, where adversarial attacks on perception models can cause incorrect path planning or obstacle avoidance, significantly compromising safety (**Al-Shaer et al., "Adversarial Attacks on Perception Models in Autonomous Vehicles"**). Additionally, vision-language navigation (VLN) systems have been shown to be vulnerable to imperceptible adversarial modifications in visual inputs, allowing malicious path manipulations that can mislead autonomous agents (**Zhang et al., "Adversarial Attacks on VLN Systems"**). The pervasiveness of adversarial vulnerabilities across different modalities underscores the necessity for developing robust defense mechanisms to mitigate such threats.

\subsection{DeepSeek MLLMs}
The DeepSeek family of MLLMs, comprising Janus (**Papangelou et al., "Janus: A Multimodal Large Language Model"**), Janus Flow (**Li et al., "Janus Flow: An Improved Version of Janus"**), and Janus Pro (**Wang et al., "DeepSeek Janus Pro: A High-Performance Multimodal Large Language Model"**), represents a series of progressively refined open-source models. The latest iteration, **Wang et al., "DeepSeek Janus Pro: A High-Performance Multimodal Large Language Model"**, offers state-of-the-art multimodal reasoning with two model sizes—1B and 7B parameters—catering to different computational requirements. Compared to LLaVA-1.5 (**Zhang et al., "LLaVA-1.6: An Improved Version of LLaVA"**), Janus Pro has demonstrated superior performance across multiple multimodal benchmarks. Unlike DeepSeek-R1 (**Kumar et al., "DeepSeek-R1: A Multimodal Large Language Model for Textual Modalities"**), which is limited to textual modalities, Janus Pro supports both textual and visual inputs, broadening its applicability. While DeepSeek Janus Pro has shown promising results, its susceptibility to adversarial perturbations remains underexplored. Comparative studies assessing its robustness against existing attack methodologies are essential to understanding its reliability in real-world applications.