\documentclass[runningheads]{llncs}
\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{adjustbox}
\begin{document}
\title{DeepSeek on a Trip: Inducing Targeted Visual
Hallucinations via Representation Vulnerabilities}
\author{Chashi Mahiul Islam\quad Samuel Jacob Chacko\quad Preston Horne\quad Xiuwen Liu}
\authorrunning{Islam et al.}
\institute{Department of Computer Science, Florida State University, Tallahassee FL, USA \\
\email{\{cislam,sjacobchacko,phorne,xliu\}@fsu.edu}}

\maketitle              

\begin{abstract}
Multimodal Large Language Models (MLLMs) represent the cutting edge of AI technology, with DeepSeek models emerging as a leading open-source alternative offering competitive performance to closed-source systems. While these models demonstrate remarkable capabilities, their vision-language integration mechanisms introduce specific vulnerabilities. We implement an adapted embedding manipulation attack on DeepSeek Janus that induces targeted visual hallucinations through systematic optimization of image embeddings. Through extensive experimentation across COCO, DALL·E 3, and SVIT datasets, we achieve hallucination rates of up to 98.0\% while maintaining high visual fidelity (SSIM > 0.88) of the manipulated images on open-ended questions. Our analysis demonstrates that both 1B and 7B variants of DeepSeek Janus are susceptible to these attacks, with closed-form evaluation showing consistently higher hallucination rates compared to open-ended questioning. We introduce a novel multi-prompt hallucination detection framework using LLaMA-3.1 8B Instruct for robust evaluation. The implications of these findings are particularly concerning given DeepSeek's open-source nature and widespread deployment potential. This research emphasizes the critical need for embedding-level security measures in MLLM deployment pipelines and contributes to the broader discussion of responsible AI implementation.
\end{abstract}

\keywords{Multimodal Large Language Models \and Embedding Manipulation Attacks \and DeepSeek Janus \and Vision-Language Security \and Targeted Hallucinations \and Adversarial Machine Learning}

\section{Introduction}


\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{dummy_images/hallucination_figure.pdf}
\caption{Examples of induced hallucinations in DeepSeek Janus through adversarial image manipulation. For each pair, we show the original image (\textcolor{green}{green} border), the target image (\textcolor{blue}{blue} border), and the adversarially modified result (\textcolor{red}{red} border), along with the model's responses.}
\label{fig:hallucination_examples}
\end{figure}
Recent advances in artificial intelligence have led to the emergence of Multimodal Large Language Models (MLLMs), which represent a significant evolution in machine learning capabilities~\cite{alayrac2022flamingo, driess2023palm, liu2024visual}. These models, capable of processing both text and visual inputs, have demonstrated remarkable performance across various tasks including visual question answering, image captioning, and multimodal reasoning~\cite{achiam2023gpt}. Among these, DeepSeek Janus has emerged as a pioneering open-source model, offering performance competitive with proprietary alternatives~\cite{wu2024janus}. DeepSeek also released Janus-Pro~\cite{chen2025janus}, an enhanced version of Janus with improved training, data scaling, and model size for better multimodal understanding and text-to-image generation.

The transition from traditional Large Language Models (LLMs) to multimodal architectures marks a crucial advancement in AI development. However, this expansion of capabilities introduces new complexities and potential vulnerabilities in the model's processing pipeline~\cite{liu2024visual, cui2024robustness}. As these models integrate multiple modalities, their attack surface expands correspondingly, creating novel security challenges that demand careful examination.

Open-source models like DeepSeek Janus present a unique scenario in the AI landscape. While their accessibility promotes innovation and democratizes access to advanced AI capabilities, it also raises significant security concerns~\cite{qiu2019review,chakraborty2021survey}. Organizations can freely deploy these models in production environments, potentially without implementing adequate security measures or fully understanding the associated risks.

Our research specifically focuses on DeepSeek Janus Pro~\cite{chen2025janus}, currently the only open-source multimodal model achieving performance comparable to closed-source alternatives. Through systematic experimentation, we demonstrate how adversarial manipulations can exploit vulnerabilities in the model's vision-language processing pipeline, leading to significant hallucinations in model outputs (Fig. ~\ref{fig:hallucination_examples}). These findings have crucial implications for deploying such models in real-world applications. The expanding adoption of MLLMs in business environments, coupled with their open-source availability, necessitates a thorough understanding of their security vulnerabilities. Our key contributions in this paper are:

\begin{itemize}

    \item We present an adapted attack methodology, building on prior work~\cite{salman2024intriguing}, that successfully induces hallucinations in DeepSeek models through targeted adversarial image manipulations. Our approach refines and extends existing techniques, revealing significant vulnerabilities in state-of-the-art open-source multimodal systems.
    
    \item We introduce a novel multi-prompt hallucination detection technique utilizing LLaMA-3.1 8B Instruct~\cite{dubey2024llama}, providing robust statistical evaluation metrics for MLLM hallucination detection. 
    %To our knowledge, this is the first implementation of such a multi-prompt approach for hallucination detection in multimodal models.
    
    \item We curate a novel benchmark dataset, LSD-Hallucination (Latent Space Disruption for
Hallucinations), which introduces over 600 input-target pairs along with both open-ended and closed-form questions and answers. This dataset specifically aims to address multimodal hallucinations, with the inclusion of closed-form questions serving as a key contribution to providing a more structured framework for future hallucination assessment in MLLMs.
\end{itemize}

\section{Related Works}

\subsection{Multimodal Large Language Models}
Multimodal Large Language Models (MLLMs) have significantly advanced AI-driven perception by integrating vision and language understanding. One of the most widely used closed-source MLLMs is OpenAI’s GPT-4 Omni (GPT-4o), which extends the capabilities of previous GPT models by incorporating a visual modality. However, its proprietary nature restricts transparency, self-hosting, and rigorous independent evaluation. Despite these limitations, GPT-4o has demonstrated state-of-the-art performance in multimodal reasoning tasks~\cite{shahriar2024putting}. In contrast, open-source MLLMs such as LLaVA (Large Language and Vision Assistant) provide researchers with accessible model architectures, training frameworks, and weights. LLaVA extends LLaMA through CLIP-based vision-language integration and has become a benchmark for evaluating multimodal understanding. The latest stable iteration, LLaVA-1.5, is widely used for benchmarking, while LLaVA-1.6 introduces further refinements~\cite{liu2024visual}. Other notable open-source MLLMs include MiniGPT-4~\cite{zhu2023minigpt}, BLIP-2~\cite{li2023blip}, OpenFlamingo~\cite{awadalla2023openflamingo}, Qwen-VL~\cite{bai2023qwenvl}, and DeepSeek Janus Pro~\cite{chen2025janus}. While open-source models foster innovation and reproducibility, their exposed architectures also present a broader attack surface for adversarial vulnerabilities.

\subsection{Adversarial Attacks on MLLMs}
Adversarial attacks exploit model vulnerabilities by introducing imperceptible perturbations to input data, leading to erroneous predictions with high confidence. The foundational work of Goodfellow et al. demonstrated that neural networks, including vision-language models, are susceptible to adversarial manipulation~\cite{goodfellow2014explaining, papernot2016limitations,mkadry2017towards}. Such attacks can be categorized into training-time and test-time perturbations. Training-time attacks poison the dataset to induce systematic errors in model learning~\cite{wan2023poisoning}, whereas test-time attacks apply perturbations post-deployment, causing temporary misclassifications~\cite{chakraborty2021survey}. A well-known test-time adversarial attack involves Universal Adversarial Perturbations (UAPs), which generalize across multiple inputs to consistently degrade model performance~\cite{moosavidezfooli2017universaladversarialperturbations,lu2024test}. Given the increasing adoption of MLLMs in critical applications, ensuring robustness against adversarial manipulations remains an open research challenge.

\subsection{Adversarial Vulnerabilities Across Domains}
Adversarial vulnerabilities extend beyond MLLMs to various domains, highlighting the broad impact of adversarial manipulations in AI systems. In the audio domain, Automatic Speech Recognition (ASR) systems can be deceived by carefully crafted audio perturbations, leading to incorrect transcriptions or commands~\cite{carlini2018audioadversarialexamplestargeted}. In the video domain, adversarial attacks on video recognition systems can cause misclassification of actions or objects, which poses risks in surveillance and autonomous systems~\cite{jiang2019blackboxadversarialattacksvideo}. Large Language Models (LLMs) are also vulnerable to adversarial prompts or crafted textual inputs that manipulate their responses, potentially leading to the generation of harmful or misleading information~\cite{zou2023universaltransferableadversarialattacks}. Recent research has demonstrated that adversarial attack techniques using regularized relaxation can efficiently generate adversarial inputs against aligned LLMs, improving attack success rates while maintaining token validity~\cite{chacko2024adversarialattackslargelanguage}. Navigation systems, such as those used in autonomous vehicles, are similarly at risk, where adversarial attacks on perception models can cause incorrect path planning or obstacle avoidance, significantly compromising safety~\cite{mahima2021adversarial}. Additionally, vision-language navigation (VLN) systems have been shown to be vulnerable to imperceptible adversarial modifications in visual inputs, allowing malicious path manipulations that can mislead autonomous agents~\cite{islam2024maliciouspathmanipulationsexploitation}. The pervasiveness of adversarial vulnerabilities across different modalities underscores the necessity for developing robust defense mechanisms to mitigate such threats.


\subsection{DeepSeek MLLMs}
The DeepSeek family of MLLMs, comprising Janus~\cite{wu2024janus}, Janus Flow~\cite{ma2024janusflow}, and Janus Pro~\cite{chen2025janus}, represents a series of progressively refined open-source models. The latest iteration, Janus Pro, offers state-of-the-art multimodal reasoning with two model sizes—1B and 7B parameters—catering to different computational requirements. Compared to LLaVA-1.5, Janus Pro has demonstrated superior performance across multiple multimodal benchmarks. Unlike DeepSeek-R1 \cite{guo2025deepseek}, which is limited to textual modalities, Janus Pro supports both textual and visual inputs, broadening its applicability. While DeepSeek Janus Pro has shown promising results, its susceptibility to adversarial perturbations remains underexplored. Comparative studies assessing its robustness against existing attack methodologies are essential to understanding its reliability in real-world applications.







\section{Methodology}
Our approach focuses on manipulating image embeddings to induce hallucinations in DeepSeek Janus through targeted optimization. The method consists of two main components: (1) an embedding optimization strategy and (2) a systematic process for generating and evaluating adversarial examples.

\subsection{Embedding Optimization}
Given an original image $x_o$ and a target image $x_t$, we aim to generate an adversarial image $x_a$ that maintains visual similarity to $x_o$ while inducing the model to produce embeddings similar to those of $x_t$. The vision encoder first produces patch embeddings which are then averaged to get a single embedding vector. The optimization objective can be formulated as:

\begin{equation}
x_a = \arg\min_{x} \mathcal{L}(g(f_v(x)), g(f_v(x_t)))
\label{eq:main_objective}
\end{equation}
where $f_v(\cdot)$ represents the vision encoder of the DeepSeek model that produces patch embeddings, and $g(\cdot)$ represents the mean pooling operation over patch dimension:

\begin{equation}
g(e) = \frac{1}{N}\sum_{i=1}^N e_i
\end{equation}
where $N$ is the number of patches and $e_i$ is the embedding of the $i$-th patch. The loss function $\mathcal{L}$ combines two distance metrics:

\begin{equation}
\mathcal{L} = \mathcal{L}_{MSE}(g(e_a), g(e_t)) = \|g(e_a) - g(e_t)\|_2^2
\label{eq:mse_loss}
\end{equation}
where $e_a = f_v(x_a)$ and $e_t = f_v(x_t)$ are the patch embeddings of the adversarial and target images respectively. 

The optimization process continues until both of two convergence criteria is met:
\begin{equation}
\|g(e_a) - g(e_t)\|_2^2 \leq \tau_l \text{ and } \cos(g(e_a), g(e_t)) \geq \tau_c
\label{eq:convergence}
\end{equation}
where $\tau_l$ and $\tau_c$ are the L2 distance and cosine similarity thresholds respectively.

\subsection{Implementation Details}
We implement the attack using PyTorch and the offline version of DeepSeek Janus Pro models available in HuggingFace. The optimization process uses the Adam optimizer with a learning rate of $\alpha = 0.007$. The convergence thresholds are set to $\tau_l = 1.44$ for L2 distance and $\tau_c = 0.95$ for cosine similarity. To ensure stable optimization, we normalize the input images to the range [0,1] and apply gradient updates directly in the pixel space. During optimization, we monitor both the L2 distance and cosine similarity between embeddings to ensure effective convergence. The process terminates either when the convergence criteria are met or after a maximum of 10,000 iterations to prevent infinite loops. The complete attack procedure is outlined in Algorithm~\ref{alg:attack}
\begin{algorithm}[ht!]
\caption{DeepSeek Embedding Manipulation Attack}
\label{alg:attack}
\begin{algorithmic}[1]
\REQUIRE Original image $x_o$, target image $x_t$, model $f$, thresholds $\tau_l$, $\tau_c$
\ENSURE Adversarial image $x_a$
\STATE $x_a \gets x_o$ \COMMENT{Initialize with original image}
\STATE $e_t \gets f_v(x_t)$ \COMMENT{Get patch embeddings}
\STATE $\bar{e}_t \gets \frac{1}{N}\sum_{i=1}^N e_{t,i}$ \COMMENT{Mean pool target patch embeddings}
\WHILE{not converged}
    \STATE $e_a \gets f_v(x_a)$ \COMMENT{Current patch embeddings}
    \STATE $\bar{e}_a \gets \frac{1}{N}\sum_{i=1}^N e_{a,i}$ \COMMENT{Mean pool current patch embeddings}
    \STATE $\mathcal{L} \gets \|\bar{e}_a - \bar{e}_t\|_2^2$ \COMMENT{Compute MSE loss on mean embeddings}
    \STATE Update $x_a$ using Adam optimizer
    \IF{$\|\bar{e}_a - \bar{e}_t\|_2^2 \leq \tau_l$ \textbf{and} $\cos(\bar{e}_a, \bar{e}_t) \geq \tau_c$}
        \STATE \textbf{break}
    \ENDIF
\ENDWHILE
\RETURN $x_a$
\end{algorithmic}
\end{algorithm}

The attack pipeline processes image pairs in batches, maintaining the following workflow:
\begin{enumerate}
    \item Load and preprocess image pairs using the DeepSeek processor
    \item Extract vision patch embeddings for both input and target images from the VisionTower
    \item Apply mean pool on vision patch embeddings
    \item Optimize original images to match target average embeddings
    \item Evaluate the effectiveness of the generated adversarial examples
\end{enumerate}

\section{Results and Discussion}

Our experimental evaluation demonstrates the effectiveness of the proposed attack, analyzing both the semantic impact and visual quality of the adversarial perturbations. In this section, we use 'original' and 'source' interchangeably to refer to the same image. We present results from three primary evaluation perspectives: basic semantic metrics, advanced LLaMA-based hallucination detection, and image quality assessment.

\subsection{Experimental Setup}
Our experiments were conducted using two variants of the DeepSeek Janus-Pro model: the 1B and 7B parameter versions. The attack implementation used PyTorch and was executed on two NVIDIA A5000 GPUs with 48GB of memory each. For LLaMA-based evaluation, we employed the LLaMA-3.1 8B Instruct model, using a systematic prompt template to assess hallucination presence in model outputs. 

\subsection{Dataset Creation}
We evaluate the attack’s effectiveness over samples curated from three datasets: COCO~\cite{antol2015vqa}, DALL·E 3~\cite{betker2023improving}, and SVIT~\cite{zhao2023svit}. The COCO dataset provides real-world images spanning 80 object categories, SVIT contributes domain-specific images, while DALL·E 3 offers synthetic images with complex scene compositions. For COCO and SVIT, we randomly select 200 image pairs, ensuring diverse semantic relationships between the original and target images. For DALL·E 3, we use a subset of synthetic images sourced from AnyDoor~\cite{lu2024test}. We also adopt their corresponding open-ended questions for all three datasets to maintain consistency in evaluation.

To further enhance our assessment, we craft a novel prompt for LLaMA-3.1 8B Instruct to generate closed-form questions from the open-ended ones. These closed-form questions explicitly query the presence of objects in the images, enabling a structured evaluation of hallucinations. The full prompt is provided in Appendix~\ref{appendix:question_transform}.

\subsection{Basic Semantic Evaluation}
\begin{table}[ht]
\caption{Comparison of model performance across different datasets with and without adversarial attacks. Results are reported for both Source and Target texts with mean ± standard deviation. Arrows (↑/↓) indicate increase/decrease in adversarial setting, with absolute change shown. Note: Adversarially perturbed images were generated using Janus-Pro 7B and evaluated on both models to assess attack transferability.}

\label{tab:model-comparison}
\centering
\small  
\setlength{\tabcolsep}{3pt}  
\renewcommand{\arraystretch}{1.2} 
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lllcccccc}
\toprule
\multirow{3}{*}{Dataset} & \multirow{3}{*}{Model} & \multirow{3}{*}{Text} & \multicolumn{2}{c}{Original Performance} & \multicolumn{4}{c}{Under Adversarial Attack} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-9}
& & & Orig-BLEU & Orig-Rouge & Adv-BLEU & BLEU-$\Delta$ & Adv-Rouge & Rouge-$\Delta$ \\
\midrule
\multirow{4}{*}{COCO} 
   & \multirow{2}{*}{Janus-Pro 7B} & Source & 0.43±0.21 & 0.68±0.15 & 0.17±0.16 & ↓0.26 & 0.42±0.16 & ↓0.26 \\
   &  & Target & 0.07±0.09 & 0.32±0.14 & 0.17±0.14 & ↑0.10 & 0.45±0.14 & ↑0.13 \\
\cmidrule(lr){2-9}
   & \multirow{2}{*}{Janus-Pro 1B} & Source & 0.19±0.17 & 0.47±0.17 & 0.16±0.15 & ↓0.03 & 0.42±0.17 & ↓0.05 \\
   &  & Target & 0.01±0.02 & 0.17±0.09 & 0.02±0.05 & ↑0.01 & 0.18±0.11 & ↑0.01 \\
\midrule
\multirow{4}{*}{DALL·E 3} 
   & \multirow{2}{*}{Janus-Pro 7B} & Source & 0.36±0.24 & 0.63±0.19 & 0.10±0.13 & ↓0.26 & 0.31±0.13 & ↓0.32 \\
   &  & Target & 0.03±0.05 & 0.23±0.09 & 0.14±0.14 & ↑0.11 & 0.37±0.14 & ↑0.14 \\
\cmidrule(lr){2-9}
   & \multirow{2}{*}{Janus-Pro 1B} & Source & 0.11±0.12 & 0.37±0.14 & 0.08±0.11 & ↓0.03 & 0.31±0.14 & ↓0.06 \\
   &  & Target & 0.01±0.02 & 0.17±0.09 & 0.03±0.04 & ↑0.02 & 0.21±0.09 & ↑0.04 \\
\midrule
\multirow{4}{*}{SVIT} 
   & \multirow{2}{*}{Janus-Pro 7B} & Source & 0.20±0.21 & 0.46±0.21 & 0.07±0.10 & ↓0.13 & 0.28±0.14 & ↓0.18 \\
   &  & Target & 0.04±0.07 & 0.22±0.13 & 0.09±0.12 & ↑0.05 & 0.31±0.15 & ↑0.09 \\
\cmidrule(lr){2-9}
   & \multirow{2}{*}{Janus-Pro 1B} & Source & 0.08±0.10 & 0.30±0.15 & 0.06±0.10 & ↓0.02 & 0.26±0.14 & ↓0.04 \\
   &  & Target & 0.01±0.02 & 0.12±0.08 & 0.01±0.01 & 0.00 & 0.13±0.07 & ↑0.01 \\
\bottomrule
\end{tabular}%
}
\vspace{5pt}
\end{table}


Our experimental results demonstrate the effectiveness and transferability of embedding-based semantic manipulation across multiple datasets and model variants. Adversarially perturbed images were generated using Janus-Pro 7B and evaluated on both models to assess attack transferability. Table~\ref{tab:model-comparison} presents a comprehensive evaluation using BLEU-4 \cite{papineni2002bleu} and ROUGE-L \cite{lin2004rouge} metrics, revealing several key findings. 

\begin{itemize}


        
    \item \textbf{Model Scale Impact:} The larger 7B model demonstrated both stronger base performance and higher vulnerability to attacks. For example, on DALL·E 3, Janus-Pro 7B's source BLEU-4 score started higher (0.36 vs 0.11) and showed larger degradation ($\downarrow$0.26 vs $\downarrow$0.03) compared to the 1B model, suggesting that increased model capacity may lead to higher susceptibility to semantic manipulation.
    
    \item \textbf{Cross-Model Target Alignment:} The transfer of target semantics was notably weaker in Janus-Pro 1B. In DALL·E 3, while Janus-Pro 7B showed substantial target text improvements (BLEU-4: $\uparrow$0.11, ROUGE-L: $\uparrow$0.14), Janus-Pro 1B exhibited minimal changes (BLEU-4: $\uparrow$0.02, ROUGE-L: $\uparrow$0.04), indicating partial transfer of the attack's effectiveness.
    
    \item \textbf{Dataset Sensitivity:} Both models showed varying levels of robustness across datasets. The SVIT dataset demonstrated the most resilient behavior for Janus-Pro 1B, with minimal changes in both source metrics (BLEU-4: $\downarrow$0.02, ROUGE-L: $\downarrow$0.04) and target metrics (BLEU-4: 0.00, ROUGE-L: $\uparrow$0.01), suggesting that domain-specific characteristics may influence attack transferability.

    \item \textbf{Attack Transferability:} While the attacks were generated using Janus-Pro 7B, they showed limited transferability to Janus-Pro 1B. For instance, in the COCO dataset, Janus-Pro 1B showed slight degradation in source text fidelity, with BLEU-4 scores dropping from 0.19 to 0.16 ($\downarrow$0.03) and ROUGE-L decreasing from 0.47 to 0.42 ($\downarrow$0.05), though these changes were less pronounced than in Janus-Pro 7B.


\end{itemize}

These results validate not only our approach's ability to manipulate semantic interpretations but also demonstrate partial transferability across model scales. While the attacks were most effective on their original target (Janus-Pro 7B), their ability to influence Janus-Pro 1B's outputs, albeit to a lesser extent, raises important considerations about the broader implications of such vulnerabilities in multimodal systems.

\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{dummy_images/eval_questions.pdf}
\caption{Examples of sample questions in our evaluation framework, showing the baseline (top), source (middle), and target (bottom) questions.}
\label{fig:eval_questions}
\end{figure}

\subsection{LLaMA-Based Hallucination Detection}
We developed a systematic evaluation framework using LLaMA-3.1 8B Instruct to detect and quantify hallucinations in vision-language model responses. Our methodology employs a structured approach to questioning that varies both in format and specificity.

\subsubsection{Question Formulation}
Our framework utilizes two distinct questioning formats, each designed to elicit different types of model responses:

\begin{itemize}
    \item \textbf{Closed-form Questions:} Direct queries requiring specific yes/no or simple factual responses about object existence or scene attributes (e.g., "Do you see any clouds in this image?")
    \item \textbf{Open-ended Questions:} Descriptive queries that prompt detailed, elaborate responses about scene elements and their characteristics (e.g., "What type of clouds are predominantly featured in the image?")
\end{itemize}

\subsubsection{Evaluation Methodology}
Within each question format, we employ three distinct question types to comprehensively evaluate the model's behavior (see Fig. \ref{fig:eval_questions}). Sample responses for all question types, along with their corresponding original and optimized images, are provided in Appendix~\ref{tab:Comprehensive Evaluation}.

\begin{itemize}
    \item \textbf{Baseline Questions:} Generic scene understanding prompts that assess overall image comprehension. These can be either closed-form (e.g., "Is this an indoor or outdoor scene?") or open-ended (e.g., "What do you see in this image?")
    
    \item \textbf{Source-Specific Questions:} Targeted queries about known source image content, formulated in both closed-form (e.g., "Do you see any sport being played in this image?") and open-ended formats (e.g., "Describe the sporting activity taking place in this scene.")
    
    \item \textbf{Target-Specific Questions:} Probes designed to detect content leakage from target images, presented as both closed-form queries (e.g., "Is there a piano in this image?") and open-ended questions (e.g., "Describe any musical instruments visible in the scene.")
\end{itemize}

\subsubsection{Evaluation Prompts}
Our framework utilizes carefully structured prompts for LLaMA-3.1 8B Instruct evaluation, as shown in Appendix ~\ref{appendix:prompt-template}. This prompt design incorporates several key features:
\begin{itemize}
    \item \textbf{Clear Evaluation Criteria:} Separate criteria for original and optimized images
    \item \textbf{Ground Truth Integration:} Explicit inclusion of reference answers
    \item \textbf{Structured Output:} JSON format requirement for consistent parsing
    \item \textbf{Binary Classification:} PASS/FAIL scoring system
\end{itemize}


\begin{table}[ht!]
\caption{LLaMA-based hallucination detection results across different learning rates and datasets for both open-ended and closed-form questions. For Original Image, responses are evaluated for general baseline questions, original content questions, and target content questions. For Optimized Image, hallucination rates are shown separately for each question type.}
\label{tab:llama-evaluation}
\centering
\small
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.3}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lllcccccc}
\toprule
\multirow{4}{*}{Question Type} & \multirow{4}{*}{Dataset} & \multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Learning\\Rate\end{tabular}} & \multicolumn{6}{c}{Hallucination Rate (\%)} \\
\cmidrule(lr){4-9}
& & & \multicolumn{3}{c}{Original Image} & \multicolumn{3}{c}{Optimized Image} \\
\cmidrule(lr){4-6} \cmidrule(lr){7-9}
& & & Baseline & Original & Target & Baseline & Original & Target \\
& & & Question & Question & Question & Question & Question & Question \\
\midrule
\multirow{6}{*}{Open-ended} & \multirow{2}{*}{COCO} 
    & 0.001 & \multirow{2}{*}{1.0} & \multirow{2}{*}{5.0} & \multirow{2}{*}{10.0} & 98.0 & 93.0 & 97.5 \\
    & & 0.007 & & & & 99.0 & 93.0 & 95.0 \\
\cmidrule(lr){2-9}
& \multirow{2}{*}{DALL·E 3}
    & 0.001 & \multirow{2}{*}{36.0} & \multirow{2}{*}{11.5} & \multirow{2}{*}{32.0} & 85.5 & 55.0 & 80.5\\
    & & 0.007 & & & & 87.8 & 55.4 & 89.6 \\
\cmidrule(lr){2-9}
& \multirow{2}{*}{SVIT}
    & 0.001 & \multirow{2}{*}{16.1} & \multirow{2}{*}{20.5} & \multirow{2}{*}{25.8} & 88.3 & 76.7 & 78.5 \\
    & & 0.007 & & & & 94.5 & 78.0 & 72.0 \\
\midrule
\multirow{6}{*}{Closed-form} & \multirow{2}{*}{COCO} 
    & 0.001 & \multirow{2}{*}{0.5} & \multirow{2}{*}{2.0} & \multirow{2}{*}{10.0} & 99.0 & 96.5 & 96.5 \\
    & & 0.007 & & & & 99.5 & 98.0 & 98.5 \\
\cmidrule(lr){2-9}
& \multirow{2}{*}{DALL·E 3}
    & 0.001 & \multirow{2}{*}{39.5} & \multirow{2}{*}{6.0} & \multirow{2}{*}{4.0} & 92.0 & 66.5 & 94.5 \\
    & & 0.007 & & & & 97.0 & 69.7 & 96.1 \\
\cmidrule(lr){2-9}
& \multirow{2}{*}{SVIT}
    & 0.001 & \multirow{2}{*}{14.3} & \multirow{2}{*}{15.2} & \multirow{2}{*}{23.2} & 90.2 & 87.5 & 88.4 \\
    & & 0.007 & & & & 94.0 & 86.0 & 88.5 \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Results Analysis}
Table~\ref{tab:llama-evaluation} presents comprehensive results across datasets and learning rates. For the \textbf{COCO dataset}, original images demonstrated remarkable reliability with minimal baseline hallucination rates of just 1.0\%, while source-specific and target-specific questions showed slightly higher rates at 5.0\% and 10.0\% respectively. When subjected to optimization, these images exhibited extremely high hallucination rates across all question types, ranging from 93.0\% to 99.0\%. The effect was particularly pronounced in closed-form questions, where hallucination rates reached 99.5\% for baseline questions at 0.007 learning rate. Both original and target content questions maintained consistently high hallucination rates between 96.5\% and 98.5\% in the optimized setting.

The \textbf{DALL·E 3 dataset} revealed notably different characteristics, with higher inherent baseline hallucination rates of 39.5\% for closed-form baseline questions in original images. There was significant variation between question types in the original setting, with open-ended questions showing 36.0\% for baseline, 11.5\% for original content, and 32.0\% for target content queries. Optimized images demonstrated strong hallucination patterns, particularly evident in baseline (85.5-87.8\%) and target questions (80.5-89.6\%) for open-ended formats. Closed-form questions proved even more effective, with hallucination rates climbing to 97.0\% for baseline questions at the higher learning rate.

The \textbf{SVIT dataset} exhibited more moderate and consistent behavior, with original images showing hallucination rates between 14.3\% and 25.8\% across all question types. This dataset displayed a more uniform distribution across question types compared to COCO and DALL·E 3. When optimized, images showed strong hallucination patterns with rates varying from 72.0\% to 94.5\%. Closed-form questions were particularly effective for optimized images, achieving baseline hallucination rates of up to 94.0\% at the 0.007 learning rate.

\subsubsection{Key Findings}
Our evaluation revealed several significant patterns in the data. The \textbf{learning rate} proved to be a crucial factor, with the higher rate of 0.007 consistently producing increased hallucination rates across all datasets. This effect was most pronounced in DALL·E 3, where baseline closed-form hallucination rates rose from 92.0\% to 97.0\%, though the impact was notably smaller in SVIT dataset for target-specific questions.

\textbf{Question type} emerged as another significant factor, with closed-form questions generally achieving higher hallucination rates compared to open-ended ones. This difference was particularly evident in the DALL·E 3 dataset, where closed-form baseline questions reached 97.0\% compared to 87.8\% for open-ended queries. This pattern remained consistent across both learning rates and all datasets, suggesting a fundamental difference in how the model responds to different question formulations.

\textbf{Dataset sensitivity} analysis revealed that the COCO dataset showed the most dramatic contrast between original and optimized images, with baseline hallucination rates increasing dramatically from 1.0\% to 98.0\% for open-ended questions. DALL·E 3 demonstrated the highest inherent hallucination rates in original images, while SVIT showed the most consistent performance across question types in the original setting.

These results validate our framework's effectiveness in detecting and quantifying hallucinations induced by adversarial optimization. The combination of closed-form questions and higher learning rates proved most effective at revealing model vulnerabilities, particularly in the COCO dataset where we achieved near-perfect hallucination rates. The significant variation in baseline hallucination rates across datasets (1.0\% for COCO vs 39.5\% for DALL·E 3 in closed-form) suggests that dataset characteristics play a crucial role in model reliability and vulnerability to adversarial manipulation.

\subsection{Image Quality Assessment}
\begin{table}[ht!]
\caption{SSIM and PSNR (dB) comparison for different datasets and learning rates. JanusPro-7B model was used to generate the Opitmized (Opt.) image}
\label{tab:ssim-psnr-comparison}
\centering
\small  
\setlength{\tabcolsep}{4pt}  
\renewcommand{\arraystretch}{1.4} 
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccc}
\hline
\multirow{3}{*}{Dataset} & 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Learning\\Rate\end{tabular}} & 
\multicolumn{2}{c}{SSIM} & \multicolumn{2}{c}{PSNR (dB)} \\
\cline{3-4} \cline{5-6}
& & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Original \&\\Optimized\end{tabular}} & 
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Target \&\\Optimized\end{tabular}} & 
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Original \&\\Optimized\end{tabular}} & 
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Target \&\\Optimized\end{tabular}} \\
& & & & & \\
\hline
\multirow{2}{*}{COCO}  
    & 0.001 & 0.88 ± 0.03 & 0.25 ± 0.07 & 31.86 ± 1.57 & 9.77 ± 1.52 \\
    & 0.007 & 0.72 ± 0.07 & 0.26 ± 0.07 & 21.74 ± 3.75 & 10.95 ± 1.41 \\
\hline
\multirow{2}{*}{DALL·E 3}  
    & 0.001 & 0.91 ± 0.03 & 0.16 ± 0.07 & 29.81 ± 1.51 & 8.67 ± 1.13 \\
    & 0.007 & 0.74 ± 0.07 & 0.17 ± 0.06 & 20.07 ± 3.23 & 9.92 ± 1.03 \\
\hline
\multirow{2}{*}{SVIT}  
    & 0.001 & 0.88 ± 0.03 & 0.26 ± 0.06 & 31.88 ± 1.50 & 9.95 ± 1.47 \\
    & 0.007 & 0.73 ± 0.08 & 0.26 ± 0.06 & 22.34 ± 3.68 & 11.08 ± 1.24 \\
\hline
\end{tabular}%
}
\vspace{5pt}
\end{table}


To quantify the perceptual impact of our semantic manipulation approach, we conducted comprehensive image quality measurements using SSIM and PSNR metrics. Table~\ref{tab:ssim-psnr-comparison} presents these measurements across different datasets and learning rates, revealing several key relationships between visual quality and semantic manipulation effectiveness.

\subsubsection{Learning Rate Impact}
Our analysis demonstrates a clear trade-off between visual fidelity and semantic manipulation:

At learning rate 0.001, we achieved superior visual quality (SSIM 0.88-0.91 with source) while maintaining high hallucination rates. The COCO dataset showed baseline hallucination rates of 98.0\% for optimized images in open-ended questions and 99.0\% in closed-form evaluation. DALL·E 3 achieved 85.5\% for baseline open-ended questions, while SVIT reached 88.3\%. The higher learning rate of 0.007 showed decreased visual quality (SSIM 0.72-0.74) but generally increased hallucination rates, particularly evident in DALL·E 3 where baseline closed-form questions improved from 92.0\% to 97.0\%.

\subsubsection{Dataset-Specific Performance}
Different datasets exhibited varying levels of robustness:

The \textbf{COCO Dataset} demonstrated optimal balance with high source similarity (SSIM 0.88 ± 0.03, PSNR 31.86 ± 1.57 dB at lr=0.001) while achieving the highest hallucination rates. For closed-form questions, baseline hallucination rates reached 99.0\% at 0.001 learning rate and 99.5\% at 0.007, with original and target content questions showing similarly high rates (96.5-98.5\%).

The \textbf{DALL·E 3 Dataset} showed highest source fidelity (SSIM 0.91 ± 0.03) but demonstrated higher inherent instability, with original image baseline hallucination rates of 36.0\% for open-ended and 39.5\% for closed-form questions. When optimized, baseline questions achieved 85.5-87.8\% hallucination rates for open-ended and 92.0-97.0\% for closed-form evaluation, showing strong susceptibility to manipulation despite high initial fidelity.

The \textbf{SVIT Dataset} maintained consistent performance across metrics (SSIM 0.88 ± 0.03), with moderate original image hallucination rates (14.3-25.8\%) and strong optimized performance. Baseline hallucination rates ranged from 88.3\% to 94.5\% across question types and learning rates, indicating balanced susceptibility to both visual and semantic modifications.

\subsubsection{Quality-Hallucination Correlation}
Our analysis reveals important relationships between visual quality metrics and hallucination effectiveness:

\textbf{Target Similarity:} Low SSIM scores with target images (0.16-0.26) across all configurations indicate that successful semantic manipulation doesn't require visual convergence to target images. This is particularly evident in DALL·E 3, which showed the lowest target SSIM (0.16-0.17) while achieving high hallucination rates (up to 97.0\% for closed-form baseline questions).

\textbf{PSNR Analysis:} The substantial drop in PSNR between source and target comparisons (e.g., 31.86 dB to 9.77 dB for COCO at lr=0.001) confirms that our method preserves source image structure while inducing semantic changes. This pattern holds across datasets, with DALL·E 3 showing the most dramatic PSNR difference (29.81 dB to 8.67 dB) at lower learning rates.

\textbf{Optimal Configuration:} The lower learning rate (0.001) consistently achieved better quality-hallucination balance, with SSIM > 0.88 and strong hallucination rates across all datasets. COCO achieved the best performance with 99.0\% closed-form baseline hallucination rate while maintaining SSIM of 0.88 ± 0.03, demonstrating that high visual fidelity can coexist with effective semantic manipulation.

These findings demonstrate that our approach successfully maintains high visual fidelity while achieving significant semantic manipulation, with the learning rate serving as a crucial control parameter for balancing these competing objectives. The results show particularly strong performance on closed-form questions, where we consistently achieved higher hallucination rates compared to open-ended queries across all datasets and configurations.


\subsection{Cross-Dataset Analysis}
Our comprehensive evaluation across multiple datasets reveals distinct patterns in both semantic manipulation effectiveness and visual quality preservation:

\subsubsection{COCO Dataset Performance}
COCO demonstrated the strongest overall attack success. For semantic impact, it achieved remarkably low baseline hallucination rates in original images (1.0\% for open-ended, 0.5\% for closed-form questions), while optimized images showed extremely high hallucination rates (98.0-99.0\% for baseline questions, 93.0-98.5\% for content-specific questions). In terms of visual quality, it maintained high fidelity with SSIM 0.88 ± 0.03 and PSNR 31.86 ± 1.57 dB at lr=0.001, while showing appropriate divergence from target images (SSIM 0.25 ± 0.07). Higher learning rate (0.007) reduced visual quality (SSIM 0.72 ± 0.07) while providing minimal gains in hallucination effectiveness.

\subsubsection{DALL·E 3 Dataset Characteristics}
DALL·E 3 exhibited unique patterns in both resilience and vulnerability. The dataset achieved the highest source-optimized visual similarity (SSIM 0.91 ± 0.03) while showing the lowest target-optimized similarity (SSIM 0.16 ± 0.07). However, it demonstrated significant baseline instability with high original image hallucination rates (36.0\% for open-ended baseline, 39.5\% for closed-form baseline). When optimized, images showed strong hallucination patterns for baseline questions (85.5-87.8\% open-ended, 92.0-97.0\% closed-form), with particularly high rates for target-specific questions in closed-form evaluation (94.5-96.1\%).

\subsubsection{SVIT Dataset Analysis}
SVIT demonstrated balanced vulnerability across metrics. The dataset showed moderate baseline hallucination rates in original images (16.1\% open-ended, 14.3\% closed-form) with consistent performance across question types (20.5-25.8\% for content-specific questions). When optimized, images exhibited strong hallucination patterns with baseline rates ranging from 88.3\% to 94.0\% depending on learning rate. Visual quality metrics revealed the highest target-optimized similarity (SSIM 0.26 ± 0.06) while maintaining strong source similarity (SSIM 0.88 ± 0.03), suggesting balanced susceptibility to both visual and semantic modifications.

These results demonstrate that DeepSeek models exhibit consistent vulnerability to embedding manipulation attacks, with effectiveness modulated by dataset characteristics and optimization parameters. Three key patterns emerged from our analysis:

First, the learning rate showed consistent impact across datasets, with the lower rate (0.001) providing better balance between visual quality and attack success. This was particularly evident in DALL·E 3, where baseline closed-form hallucination rates reached 92.0\% while maintaining SSIM of 0.91 ± 0.03.

Second, question type proved crucial for detection effectiveness, with closed-form questions consistently achieving higher hallucination rates. The most dramatic example appeared in DALL·E 3, where baseline questions showed a difference of 92.0\% (closed-form) versus 85.5\% (open-ended) at learning rate 0.001.

Finally, dataset characteristics emerged as more influential than model architecture variations. The stark contrast between COCO's clean baseline (1.0\% hallucination rate) and DALL·E 3's inherent instability (36.0-39.5\%) suggests that dataset properties fundamentally shape model vulnerability, regardless of model size or configuration.

\section{Conclusion}
Our research reveals that DeepSeek Janus models are highly vulnerable to embedding manipulation attacks, posing significant risks for open-source multimodal AI deployment. Both 1B and 7B variants are susceptible, though larger models exhibit marginally superior robustness. The attack's effectiveness varies across datasets, emphasizing the context-dependent nature of these vulnerabilities. Notably, the trade-off between attack success and visual quality preservation raises concerns for real-world applications. These findings indicate that current multimodal models may not be suitable for security-sensitive tasks without additional safeguards. Future work should focus on robust defense mechanisms, real-time detection methods, and evaluating how these attacks scale across different architectures to mitigate broader security risks. Promising directions include exploring randomized smoothing techniques for certified robustness \cite{cohen2019certifiedadversarialrobustnessrandomized} and strategic neuron-level defense mechanisms like those proposed for Vision Transformers \cite{islam2025mechanisticunderstandingsrepresentationvulnerabilities}. Future research should evaluate the effectiveness of these and other defense approaches against embedding manipulation attacks to develop more robust multimodal systems.

\bibliographystyle{splncs04}
\bibliography{main}
\newpage
\appendix
\section{Hallucination Detection Prompt}
\label{appendix:prompt-template}
We introduce the template used for LLaMA-based hallucination detection prompt showing evaluation criteria for both original and optimized image responses.
\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{dummy_images/hallucination_prompt.pdf}
\end{figure}

\section{Question Transformation Prompt}
\label{appendix:question_transform}
We present the template used for transforming open-ended questions into closed-form questions.
\begin{figure}[ht!]
\centering
\includegraphics[width=\linewidth]{dummy_images/question_transform.pdf}
\end{figure}

\newpage
\section{Comprehensive Evaluation Examples (Janus Pro)}
\label{tab:Comprehensive Evaluation}
\begin{table}[ht!]
    \centering
    \resizebox{0.95\columnwidth}{!}{%
    \begin{tabular}{|c|p{4.5cm}|p{5cm}|c|}
        \hline
        \textbf{Image} & \textbf{Question} & \textbf{Response} & \textbf{Target Image}\\
        \hline
        \multirow{3}{*}{\adjustbox{valign=c}{\begin{tabular}{c}\includegraphics[width=3cm]{dummy_images/current_image.png}\\[2pt]Input Image\end{tabular}}} & Baseline: What do you see in the image? & The image shows a well-organized workspace with a computer on a wooden desk. There are various office supplies like a keyboard, mouse, pens, and a lamp. The background features a window with a view of the outside, and there are some books and a picture frame on the desk. & \multirow{6}{*}{\includegraphics[width=3cm]{dummy_images/target_image.png}} \\
        \cline{2-3}
        & Current: Do you see any office items next to the desktop computer on the desk? & Yes, there are several office items next to the desktop computer on the desk. These include a keyboard, a mouse, a desk lamp, a pen holder, and a few other small containers holding writing instruments. & \\
        \cline{2-3}
        & Target: Do you see any skiers in this image? & No, there are no skiers in this image. The image depicts a workspace with a computer, keyboard, mouse, desk lamp, and various office supplies. & \\
        \cline{1-3}
       \multirow{3}{*}{\adjustbox{valign=c}{\begin{tabular}{c}\includegraphics[width=3cm]{dummy_images/optimized_image.png}\\[2pt]Optimized Image\end{tabular}}} & Baseline: What do you see in the image? & I see a computer monitor displaying a \textcolor{red}{skier in mid-air, surrounded by snow-covered rocks and trees}. The scene appears to be set in a \textcolor{red}{snowy, mountainous area.} &  \\
        \cline{2-3}
        & Current: Do you see any office item next to the desktop computer on the desk? & \textcolor{red}{No, there are no office items} next to the desktop computer on the desk. The desk is \textcolor{red}{covered in snow, and there are no other objects present.} & \\
        \cline{2-3}
        & Target: Do you see any skiers in this image? & Yes, there is a \textcolor{red}{skier} in the image. \textcolor{red}{The skier is depicted on the screen of a monitor, skiing down a snowy slope.} & \\
        \hline
        \multirow{3}{*}{\adjustbox{valign=c}{\begin{tabular}{c}\includegraphics[width=3cm]{dummy_images/current_image2.png}\\[2pt]Input Image\end{tabular}}} & Baseline: What do you see in the image? & I see an airplane on a runway with a city skyline in the background. The airplane is positioned centrally, and the cityscape is illuminated by the setting sun. & \multirow{6}{*}{\includegraphics[width=3cm]{dummy_images/target_image2.png}} \\
        \cline{2-3}
        & Current: Do you see any airport runway in this image? & Yes, I see an airport runway in this image. The airplane is positioned on the runway, ready for takeoff. & \\
        \cline{2-3}
        & Target: Do you see any steam engines in this image? & No, I do not see any steam engines in this image. The image shows an airplane on a runway with a cityscape in the background. & \\
        \cline{1-3}
        \multirow{3}{*}{\adjustbox{valign=c}{\begin{tabular}{c}\includegraphics[width=3cm]{dummy_images/optimized_image2.png}\\[2pt]Optimized Image\end{tabular}}} & Baseline: What do you see in the image? & The image shows a \textcolor{red}{museum exhibit featuring a large, vintage steam locomotive and a futuristic} aircraft. \textcolor{red}{The locomotive is in the background}, and the aircraft is in the foreground. &  \\
        \cline{2-3}
        & Current: Do you see any airport runway in this image? & \textcolor{red}{No, there is no airport runway} in this image. The image depicts a \textcolor{red}{museum exhibit featuring a large steam locomotive} and an airplane, both \textcolor{red}{displayed in a spacious, well-lit hall}. & \\
        \cline{2-3}
        & Target: Do you see any steam engines in this image? & \textcolor{red}{Yes, there is a steam engine in the background} of the image. It appears to be a \textcolor{red}{large, vintage steam locomotive displayed in a museum setting}. & \\
        \hline
    \end{tabular}
    }
\end{table}
% \begin{figure}[ht!]
% \centering
% \includegraphics[width=0.95\linewidth]{dummy_images/comprehensive_evaluation.pdf}
% \end{figure}
\end{document}