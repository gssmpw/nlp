\section{Literature Review}
\label{sec:a5}
Throughout the years, much research has been conducted in anomaly detection with a multitude of explored methods such as in ____. Since then, this interest has increased significantly as various domains such as cybersecurity ____, fraud detection, and healthcare ____ became more relevant. 
The work on learning from imbalanced datasets was proposed in AAAI 2000 workshop 
%Workshop on learning from imbalanced data) 
and highlighted research on two major problems, types of imbalance that hinder the performance of standard classifiers and the suitable approaches for the same. ____ also showed that the class imbalance problem affects not only standard classifiers like decision trees but also Neural Networks and SVMs. The work by ____
%This was followed by ICML'2003 Workshop on Learning from Imbalanced Datasets[II] ____ and ACM SIGKDD Exploration'2004 ____ that 
gave a further boost to research in imbalanced data classification. Extensive research was done on unsupervised learning methods to address issues such as relying on static thresholds, in turn struggling to adapt to dynamic data, resulting in high false positives and missed anomalies. However, the work by ____ observed that unsupervised anomaly detection can be computationally intensive, especially in high-dimensional datasets. Jacob and Tatbul ____ delved into explainable anomaly detection in time series using real-world data, yet deep learning-based time-series anomaly detection models were not thoroughly explored well enough. %Clustering-based methods like K-Means and Isolation Forest may require prior knowledge of cluster numbers.
With significant growth in applying various ML algorithms
to detect anomalies, there has been an avalanche of anomaly
benchmarking data ____, ____, ____, as well as empirical studies of the performances of existing algorithms ____, ____ on different benchmark data. Due to the importance of the problem, there have also been efforts to produce benchmarks such as AD-Bench____ and ESA-ADB____. %AD-Bench examines the performance of 30 detection algorithms across an extensive set of 57 benchmark datasets, adding to the body of research in the field. 
Researchers have critically examined the suitability of evaluation metrics for machine learning methods in anomaly detection. Kim et al. ____ exposed the limitations of the F1-score with point adjustment, both theoretically and experimentally. 
%Despite the emergence of numerous new metrics for time series anomaly detection, even the most recent ones are not without their shortcomings. 

To conclude, %despite the availability of extensive anomaly benchmark datasets and publicly accessible machine learning-driven anomaly detection methods, 
recent benchmarking studies, concentrated on deep-learning-based anomaly detection techniques mostly, have not examined the performance across varying types of anomalies, such as singleton/point, small, and significantly large numbers, nor across different data types, including univariate, multivariate, temporal, and non-temporal data. Additionally, there has been a lack of exploration into how anomalies should be identified when their frequency is high. These observations prompt several critical questions. Is the current SoTA algorithm the most effective? Are we reaching the peak in anomaly detection using Deep Learning approaches? Are unsupervised learning algorithms truly better than supervised learning algorithms?

%%%%%%%%%%%%%%%%%% ATTACHING PREVIOUS APPENDIX %%%%%%%%%%%%%%%%%%%

\newpage