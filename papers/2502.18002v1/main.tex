\documentclass{article} % for initial submission
%\documentclass[accepted]{uai2025} % after acceptance, for a revised version; 
% also before submission to see how the non-anonymous paper would look like 
                        
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2025} % ptmx math instead of Computer
                                         % Modern (has noticeable issues)
% \documentclass[mathfont=newtx]{uai2025} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams

%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{soul}
\usepackage{floatpag}
\usepackage{multirow}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{arxiv}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%

%% Provided macros
% \tinyer: Because the class footnote size is essentially LaTeX's \tiny,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
%\newcommand{\swap}[3][-]{#3#1#2} % just an example

\fancyhead[LO]{\emph{Radon--Nikod\'{y}m Derivative: Re-imagining Anomaly Detection from a Measure Theoretic Perspective}}

\title{Radon--Nikod\'{y}m Derivative: Re-imagining Anomaly Detection from a Measure Theoretic Perspective}

\author{
  Shlok Mehendale\\
  Dept. of Mathematics and CSIS,  BITS Pilani Goa, India\\
  \texttt{f20221426@goa.bits-pilani.ac.in}
\And
Aditya Challa\\
Dept. of CSIS and APPCAIR,  BITS Pilani Goa, India\\
\texttt{adityac@goa.bits-pilani.ac.in}
\And
Rahul Yedida\\
LexisNexis, North Carolina, United States\\
\texttt{rahul@ryedida.me}
\And
Sravan Danda\\
Dept. of CSIS and APPCAIR,  BITS Pilani Goa, India\\
\texttt{dandas@goa.bits-pilani.ac.in}
\And
Santonu Sarkar\\
Dept. of CSIS and APPCAIR,  BITS Pilani Goa, India\\
\texttt{snehanshus@goa.bits-pilani.ac.in}
\And
Snehanshu Saha\\
Dept. of CSIS and APPCAIR,  BITS Pilani Goa, India\\
\texttt{snehanshus@goa.bits-pilani.ac.in}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MATH COMMANDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\featspace{{\mathcal{X}}}
\def\labelspace{{\mathcal{Y}}}
\def\hypospace{{\mathcal{H}}}
\input{math_commands}

\newcommand{\sbar}[1]{{\color{darkgray}\rule{\dimexpr 1cm * #1 / 100}{5pt}\color{lightgray}\rule{\dimexpr 1cm * (100 - #1) / 100}{5pt}}}

\def\rnthm{{Radon--Nikod\'{y}m}}
  
\begin{document}
\maketitle

\begin{abstract}
% Anomaly detection is a critical task for identifying deviations from expected behavior in various systems, and recent research highlights the growing importance of both supervised and unsupervised approaches in this domain. The aim of this article is to design a loss function for anomaly detection from first principles. 

Which principle underpins the design of an effective anomaly detection loss function? The answer lies in the concept of \rnthm{} theorem, a fundamental concept in measure theory. The key insight is -- Multiplying the vanilla loss function with the \rnthm{} derivative improves the performance across the board. We refer to this as RN-Loss.

%Accordingly, we revisit the PAC learnability of anomaly detection and obtain a loss function inspired by the \rnthm{} theorem, a fundamental concept in measure theory. 

This is established using PAC learnability of anomaly detection. We further show that the \rnthm{} derivative offers important insights into unsupervised clustering based anomaly detections as well. 

% Specifically, the \rnthm{} derivative fits into the narrative of divergence metrics used in those methods. 

% This adaptation also is shown to improve the baseline performance of those methods on the benchmark datasets.

We evaluate our algorithm on 96 datasets, including univariate and multivariate data from diverse domains, including healthcare, cybersecurity, and finance. We show that RN-Derivative algorithms  outperform state-of-the-art methods on 68\% of Multivariate datasets (based on F-1 scores) and also  achieves peak F1-scores on 72\% of time series (Univariate) datasets. 



\end{abstract}
\section{Introduction}
\label{sec:intro}

Anomaly detection, the process of identifying rare yet significant deviations from normal patterns, has become essential in various domains such as finance, healthcare, and cybersecurity, where undetected anomalies can lead to catastrophic consequences. Researchers have explored supervised and unsupervised approaches for anomaly detection. Although unsupervised approaches work well when labeled anomalies are scarce and often not found in training data, shallow unsupervised anomaly detection methods like One-Class SVM~\cite{I59}, Isolation Forest~\cite{I29} are limited in their scalability to large datasets~\cite{I7}. Tree-based algorithms such as Isolation Forest, Random Cut Forest \cite{I60}, d-BTAI \cite{dBTAI}, and MGBTAI \cite{MGBTAI}, report a comparatively better performance. Supervised learning needs to use labeled data and has difficulty identifying unseen anomalies\cite{I58},\cite{I1}; it also offers several advantages that make them a favorable choice. For example, supervised algorithms can leverage domain knowledge to recognize complex patterns indicative of anomalies by training on labeled examples of both normal and anomalous instances. This results in improved accuracy and precision compared to unsupervised methods, which rely on more generalized patterns and assumptions~\cite{I6,I7}. Supervised algorithms suffer from the class imbalance issue because loss functions used in these algorithms assume equally distributed datasets across multiple classes. 

\paragraph{Motivation of the present work:}
Anomaly detection has been a long-standing focus of research, addressing diverse applications such as fraud detection, network security, and medical diagnostics \cite{I1, HILAL2022116429, net}. Numerous approaches have achieved state-of-the-art performance, supported by robust empirical studies \cite{THOCN, SSLAnomaly}. However, to the best of our knowledge, no prior work has derived\emph{ a generic principle for designing the loss function} leaving a critical gap in the theoretical understanding of anomaly detection. This work aims to bridge this gap by deriving such a loss function and providing foundational insights into its properties and implications for anomaly detection tasks.

Significant efforts have been devoted to the principled study of the related problem of out-of-distribution (OOD) detection, as detailed in prior work (See \citet{DBLP:journals/jmlr/0001L00024} and references therein). While these studies provide valuable insights, OOD detection fundamentally differs in it's objectives and assumptions compared to anomaly detection. Specifically, OOD detection focuses on identifying inputs not represented by the training data distribution, whereas anomaly detection typically involves identifying rare or unexpected patterns within the data itself (See \citet{DBLP:journals/tmlr/SalehiMHLRS22} for comparison). For a comprehensive review of the literature addressing both paradigms, please refer to \Cref{sec:a5}.

% Despite the importance of anomaly detection across domains, the literature lacks a rigorous theoretical framework or explicit formulations for an optimal loss function. 

% It is established that traditional loss functions (like MSE, binary cross-entropy, etc.), while effective in many contexts, often fall short in capturing the nuances of rare events or may \emph{struggle with the imbalance between normal and anomalous data}, with a similar result in \cite{I69}. Moreover, to our knowledge, loss functions for anomaly detection were only designed to work with one kind of architecture (CITE). For example, the loss function used in a Convolutional Neural Network (CNN) might not be directly reused or effective when transferred to a Long Short-Term Memory (LSTM) network. A loss function designed for one type of architecture often requires significant modification or replacement when applied to another, making it challenging to achieve consistent performance across different data types and models.


% \paragraph{The motivation, insights from the theorem, discovery of class reweighing and possible flexibility to UNSAD should be discussed here } --\textcolor{red}{informal summary rewritten and expanded}



\subsubsection*{Contributions/Insights}

The key contribution of this paper is the rethinking of anomaly detection from first principles (PAC learnability) and the introduction of a weighted loss function based on the \rnthm{} derivative (termed ``RN-Loss'' in this paper), tailored for anomaly detection. 

\paragraph{Theoretical Insights:} In \cref{sec:2} we introduce the problem of (PAC-)learnability of anomaly detection problem. In \cref{sec:3} we show that anomaly detection is indeed learnable under some mild conditions (see \textit{informal summary}) of (PAC-)learnability. The main insight from this derivation is -- \emph{The generic loss function for learning anomaly detection is the product of the original loss function and the \rnthm{} derivative of the two respective distributions. An operator (Radon–Nikodým derivative) is able to calibrate the two different measures i.e. distributions. This enables identifying anomalies in the data distribution. Further we also show that, this divergence in distributions can be quantified using the  anomaly contamination ratio. This interpretation and formal proof also turned out to be performant when tested on diverse datasets.} Specifically, if one has samples and used the empirical estimate, then the ideal (generic) loss function tailored for anomaly detection becomes a (appropriately) weighted loss. In other cases (unsupervised), it reduces to the ratio of kernel density estimates. 

\paragraph{Empirical Insights:} RN-Loss maintains \emph{computational efficiency} by building on base loss functions like Binary Cross-Entropy, enabling integration with architectures such as Feedforward ReLU and LSTM models. Further unsupervised methods like dBTAI~\cite{dBTAI} can benefit from an adapted version of RN-Loss \textit{thus making it capable of identifying anomalies even when the model is trained solely on normal data. This ability to detect previously unseen anomalies demonstrates its robustness and effectiveness in scenarios where anomalous data is not present during training.} It supports \emph{operational simplicity} with automated hyperparameter tuning via AUC ROC, removing the need for manual thresholding (\cref{sec:a6}, \cref{table:5}).

The loss function demonstrates \emph{flexibility}, fitting varied data distributions such as Weibull and Log-normal without requiring structural changes (\cref{sec:a6}, \cref{table:3}). These properties make RN-Loss a robust and adaptable approach for anomaly detection, combining \emph{improved metrics}, \emph{efficiency}, and \emph{versatility}, making it suitable for diverse applications in real-world scenarios.
\paragraph{Consequences:} The RN-Loss function introduces improvements in anomaly detection, achieving \emph{enhanced performance} and \emph{broad adaptability}. It outperforms prior SoTA methods, improving F-1 Scores in 68\% and Recall in 46\% of the multivariate datasets, with similar trends in time-series (Univariate) data (F-1: 72\%, Recall: 83\%). These results demonstrate its \emph{consistent performance across diverse benchmarks}.\newline
Our experiments demonstrate that RN-Loss significantly improves the performance of unsupervised anomaly detection methods, \emph{specifically vanilla implementation of CBLOF and ECBLOF \cite{ECBLOF}, when combined with clustering algorithms like K-Means \cite{CBLOF} and dBTAI \cite{dBTAI}}. The enhanced KMeans-CBLOF algorithm achieved superior performance on 93\% of univariate datasets (27 out of 29) and 48\% of multivariate datasets (32 out of 67) in comparison to the original KMeans-CBLOF algorithm. While dBTAI previously achieved state-of-the-art results, its metrics—particularly precision—were inflated. RN-Loss helped normalize these metrics and the modified dBTAI further maintained or improved its overall performance across 59 multivariate datasets and demonstrated increased recall values across nearly all univariate datasets. Detailed experimental results are presented in \Cref{table:K-Means-Multi}, \Cref{table:K-Means-Uni}, \Cref{table:dBTAI-Multi} and \Cref{table:dBTAI-Uni} in the \Cref{sec:a6}. For the mathematical foundations of adapting CBLOF and ECBLOF to RN-Loss, please refer to Appendices \Cref{sec:a3} and \Cref{sec:a4}, respectively. 
%\textcolor{blue}{Please check the above modified paragraph}
% RN-Loss is unaffected by percentage of anomaly contamination. We demonstrate the flexibility and generalizability of RN-Loss by applying it to different architectures (Feedforward ReLU and LSTM). The loss function works with Tree-based unsupervised approaches like dBTAI~\cite{dBTAI}.  Important highlights of this approach are listed below.
% \begin{itemize}
%     \item[1.] \textbf{Enhanced Performance:} The RN-Loss function significantly improves the performance of supervised algorithms for anomaly detection. We have shown that our method outperforms prior SoTA on standard datasets in 71\% of datasets, w.r.t F-1 Score and 60\% of the datasets w.r.t Recall. On time-series dataset, our method performs better on 56\% of datasets w.r.t F-1 Score and 70\% of the datasets w.r.t Recall. Table \ref{table:1a)} shows the overall comparison of algorithms on these datasets.
%     \item[2.] \textbf{Efficiency and Adaptability:} Implemented on top of base loss functions, such as Binary Cross-Entropy for binary classification, the proposed loss function maintains computational efficiency while providing better performance (Technical Appendix, Table 16), making it highly adaptable to various deep learning models.
%     \item[3.] \textbf{Anomaly Identification without any exposure:} The proposed loss function is capable of identifying anomalies even when the model is trained solely on normal data. This ability to detect previously unseen anomalies demonstrates its robustness and effectiveness in scenarios where anomalous data is not present during training. Many supervised learning anomaly detection algorithms like DAGMM~\cite{DAGMM} and DevNet~\cite{DevNet} require a minimum of 2\% anomalous contamination of training dataset to achieve optimal performance on anomalous datasets. This limitation is overcome by the proposed RN-Net which is efficient in detecting anomalies even when not exposed to any anomalous samples. 
%     \item[4.] \textbf{Automated Hyperparameter Tuning:} The hyperparameter optimization is done using the AUC ROC, whereas other SoTA algorithms require manual thresholding to achieve optimal performance.  (refer to the threshold table in the Technical Appendix Table 4)
%     \item[5.] \textbf{Agnostic to Data Distribution:} The proposed loss function shows the capability of fitting to varied data distributions like Weibull, Log-normal, etc. (refer to Technical Appendix Table 3), without any changes in its structure.
% \end{itemize}
% \paragraph{Consequences:}
% \paragraph{Outline/big picture of the article:} In \cref{sec:2} we start with defining the problem of PAC learning in the context of anomaly detection and setting up the notation. \cref{sec:3} uses this setup to derive the RN-Loss from first principles. In \cref{sec:4} we test the proposed loss (RN-Loss) on a wide variety of benchmark datasets spanning multivariate, univariate time-series and multivariate time-series with different architectures based on FCNN (fully connected neural networks) and LSTM (long short term memory). We also compare our approach with SoTA approaches and observe that this outperforms the other approaches. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\textwidth]{Images/Diagram1.png}
    \caption{\fontsize{9}{1em}\selectfont Comparative Analysis of Anomaly Detection Algorithms. Performance evaluation prioritizes recall, with precision as the secondary metric for tied cases. The comparison spans three algorithm categories: (1) Deep Learning approaches (AutoEncoders, DAGMM, DevNet, GAN, DeepSAD, FTTransformer (current state-of-the-art), and PReNet), (2) Unsupervised methods (LOF, Elliptic Envelope, Isolation Forest, dBTAI, MGBTAI, and quantile-based approaches including q-LSTM variants and QReg), and (3) RN-Derivative algorithms (RN-Net and RN-LSTM). RN-Net outperforms state-of-the-art methods on 68\% of Multivariate datasets (based on F-1 scores), while the RN-LSTM + RN-Net combination achieves peak F1-scores on 72\% of time series (Univariate) datasets. For detailed numerical comparisons, refer to \Cref{table:1a),table:1b),table:1d),fig:performance} in \Cref{sec:a6}. }
    \label{fig:summary}
\end{figure*}

\section{Problem Setup and Notation}
\label{sec:2}

Let $\featspace \subset \mathbb{R}^d$ denote the feature space and $\labelspace \in \{1, \dots, K\}$ denote the label space. The base distribution is denoted by a joint distribution $D_{X_BY_B}$ over $\featspace \times \labelspace$, where $X_B \in \featspace$ and $Y_B \in \labelspace$. The \emph{anomaly distribution} is denoted by $D_{X_AY_A}$ over the same $\featspace \times \labelspace$ and $X_A \in \featspace$. However, $Y_A$ does not belong to $\labelspace$ for simplicity we assume $Y_A = K+1$. We also often use the mapping $\phi$ which is a function which maps the labels $\{1, \dots, K\} \to 0$ and label $\{K+1\} \to 1$. 

We assume that the distribution we sample from is a \emph{mixture} of the base-distribution and the anomaly distribution, i.e $D_{XY}^{\alpha} = (1-\alpha) D_{X_BY_B} + \alpha D_{X_AY_A}$ where $\alpha \in [0,1)$ is unknown. $\alpha$ is referred to as \emph{anomaly contamination ratio}. We sometimes omit the $\alpha$ when we imply that $\alpha$ can be any unknown value.

Anomaly detection can either be supervised and unsupervised. For completeness, we state both these problems below.
%, although we are only interested in the supervised case.

\paragraph{Supervised Anomaly Detection:} Given a sample of points $S \coloneqq \{(\vx^i, y^i)\}$ drawn i.i.d from $D_{XY}$, the aim is to obtain a classifier $f$ such that, for any sample $\vx$ from marginal $D_X$ : (i) if $\vx$ is sampled from $D_{X_A}$ then identify it as \emph{anomaly} and (ii) if $\vx$ is sampled from $D_{X_B}$ identify it as \emph{non-anomaly}. Note that we have at our disposal the labels -- $\phi \circ \labelspace \in \{0,1\}$, making this a binary classification problem.

\paragraph{Unsupervised Anomaly Detection:} Given a sample of points $S \coloneqq \{(\vx^i, y^i)\}$ drawn i.i.d from $D_{X_BY_B}$, the aim is to obtain a classifier $f$ such that, for any sample $\vx$ from marginal $D_X$ : (i) if $\vx$ is sampled from $D_{X_A}$ then identify it as \emph{anomaly} and (ii) if $\vx$ is sampled from $D_{X_B}$ identify it as \emph{non-anomaly}. 

\paragraph{Remark:} Supervised and unsupervised cases differ by the sampling distribution $D_{XY}$ vs $D_{X_B Y_B}$ and whether we have ``some'' labels to identify the classifier $f$.

\paragraph{Space of Distributions:} Clearly, if there is no restriction on the set of possible distributions $D_{XY}$, no-free-lunch theorem \cite{Wolpert} suggests that anomaly detection is impossible. So, we restrict the set of distributions to a set $\mathscr{D}_{XY}$.

\paragraph{Hypothesis Space, Loss function and Risks :} Let $\hypospace \subset \{h : \featspace \to \{0,1\}\}$ denote the set of functions from which we choose our classifier $f$. Any specific $h \in \hypospace$ is referred to as hypothesis function or simply function if the context is clear. We consider 0-1 loss function -- $\ell : \labelspace \times \labelspace \to \{0,1\}$ where $\ell(y_1, y_2) = 0$ if $y_1 = y_2$ and $\ell(y_1, y_2) = 1$ otherwise.

The risk of a hypothesis is defined as the expected loss when the hypothesis is used for identifying anomalies.  
\begin{equation}
    R_{D}(h) = E_{D_{XY}}[\ell(h(\vx), y)]
\end{equation}
We also define $\alpha-$risk to quantify the expected loss when \emph{anomaly contamination ratio} is $\alpha \in [0,1)$. That is, $D = (1-\alpha) D_{X_BY_B} + \alpha D_{X_AY_A}$
\begin{equation}
    R_{D}^{\alpha}(h) = (1-\alpha) R_{D_{X_BY_B}}(h) + \alpha R_{D_{X_A Y_A}}(h)
\end{equation}

\paragraph{PAC Learning and Anomaly Detection:} We use the following equivalent definition of PAC learning \cite{Ratsaby2008} in this article. A hypothesis class $\hypospace$ is (agnostic-)PAC learnable if there exists (i) an algorithm $\mathbf{A} : \cup_{n=1}^{\infty} (\featspace \times \labelspace)^n \to \hypospace$, (ii) a decreasing sequence $\epsilon(n) \to 0$, and (iii) for all $D_{XY} \in \mathscr{D}_{XY}$
\begin{equation}
    E_{S \sim D_{XY}^{n}} [R_{D_{XY}}(\mathbf{A}(S)) - \inf_{h \in \hypospace}R_{D_{XY}}(h)] \leq \epsilon(n)
\end{equation}
Existing results \cite{Uniconv} show that if $\hypospace$ has finite VC dimension, then it is learnable for all possible distributions $\mathscr{D}_{XY}$. Specifically, neural networks are PAC learnable.

Note that we are interested in the anomaly detection. Which means, while learning may be done with specific $\alpha$, the test distribution is always fixed at $\alpha = 0.5$. Formally, we sample from $D_{XY}^{\alpha}$, but the risk we are interested is when base-distribution and anomalies are equally possible, i.e $\alpha=0.5$. Thus, we need to show that 
\begin{equation}
    E_{S \sim D_{XY}^{\alpha, n}} [R_{D_{XY}}^{0.5}(\mathbf{A}(S)) - \inf_{h \in \hypospace} R_{D_{XY}}^{0.5}(h)] \leq \epsilon(n) \\  
\end{equation}
where $\epsilon(n) \to 0$ as $n \to \infty$. 

\section{RNLoss : Derivation from first principles}
\label{sec:3}

The aim of this article is to obtain the algorithm $\mathbf{A}$ for solving the anomaly detection problem. We achieve this in 2 stages - (i) Assume that $\alpha$ is known and design a loss using the \rnthm{}(RN) derivative, (ii) Design the loss function which approximates $\alpha$. 

We begin by recalling the established \rnthm{} theorem from measure theory.

\begin{theorem}[\rnthm{} \cite{RN_Thm}]
\label{thm:1}
    Let $(\Omega,\mathcal{A})$ be a measurable space with $\mathcal{A}$ as the $\sigma$ algebra and $\mu, \nu$ denote two $\sigma-$finite measures such that $\nu << \mu$ ($\nu$ is absolutely continuous with respect to $\mu$). Then, there exists a function $f$ such that,
    \begin{equation}
        \nu(A) = \int_{A} f d \mu \quad (or) \quad d \nu = f d \mu
    \end{equation}
    where A $\in \mathcal{A}$.
\end{theorem}

\paragraph{Absolute Continuity and bounded Assumption:} For any fixed $\alpha$, let $\nu$ denote the measure induced by $D_{XY}^{0.5}$  and $\mu$ denote the measure induced by $D_{XY}^{\alpha}$. We assume that $\nu$ is absolutely continuous with respect to $\mu$. We further assume that the density $f$ relating these distributions is bounded, i.e $1/b < f < b$ for some $b$ on the support of $\mu$.

We then have the following theorem:
\begin{theorem}
\label{thm:2}
    Let $\nu$ denote the measure induced by $D_{XY}^{0.5}$  and $\mu$ denote the measure induced by $D_{XY}^{\alpha}$. Also, let $d \nu = f d \mu$ (absolutely continuous) where $1/b < f < b$ for some $b < \infty$ on the support of $\mu$. For all $h \in \hypospace$, there exists a $\Delta_{\mu, \nu}$ such that,
    \begin{equation}
        \frac{1}{\Delta_{\mu, \nu}} \leq \frac{R_{D_{XY}^{0.5}}(h)}{R_{D_{XY}^{\alpha}}(h)} \leq \Delta_{\mu, \nu}
    \end{equation}
\end{theorem}
The proof is given in \cref{sec:a1}.

Using \cref{thm:2}, we have
\begin{equation}
    R_{D_{XY}^{0.5}}(\mathbf{A}(S))  \leq R_{D_{XY}^{\alpha}}(\mathbf{A}(S)) \times \Delta_{\mu, \nu}
\end{equation}
and,
\begin{equation}
        \inf_{h \in \hypospace} R_{D_{XY}^{0.5}}(h) \geq \frac{1}{\Delta_{\mu,\nu}} \inf_{h \in \hypospace} R_{D_{XY}^{\alpha}}(h) 
\end{equation}
Hence,
\begin{equation}
\begin{aligned}
    &R_{D_{XY}^{0.5}}(\mathbf{A}(S)) - \inf_{h \in \hypospace} R_{D_{XY}^{0.5}}(h) \\
    & \leq R_{D_{XY}^{\alpha}}(\mathbf{A}(S)) \times \Delta_{\mu, \nu} - \frac{1}{\Delta_{\mu,\nu}} \inf_{h \in \hypospace} R_{D_{XY}^{\alpha}}(h) \\
    &\leq \Delta_{\mu, \nu} \left[ R_{D_{XY}^{\alpha}}(\mathbf{A}(S)) -  \inf_{h \in \hypospace} R_{D_{XY}^{\alpha}}(h) \right] \\
    & + \inf_{h \in \hypospace} R_{D_{XY}^{\alpha}}(h) \left[ - \frac{1}{\Delta_{\mu,\nu}} +  \Delta_{\mu, \nu}\right]
\end{aligned}    
\end{equation}
Taking expectations on both sides,
\begin{equation}
\begin{aligned}
    &E_{S \sim D_{XY}^{\alpha, n}} [R_{D_{XY}^{0.5}}(\mathbf{A}(S)) - \inf_{h \in \hypospace} R_{D_{XY}^{0.5}}(h)] \\
    & \leq \Delta_{\mu, \nu} E_{S \sim D_{XY}^{\alpha, n}} [R_{D_{XY}^{\alpha}}(\mathbf{A}(S)) - \inf_{h \in \hypospace} R_{D_{XY}^{\alpha}}(h)] \\
    & + \left[ - \frac{1}{\Delta_{\mu,\nu}} +  \Delta_{\mu, \nu}\right] E_{S \sim D_{XY}^{\alpha, n}} \left[\inf_{h \in \hypospace} R_{D_{XY}^{\alpha}}(h)\right]
\end{aligned}    
\end{equation}
Using the \underline{Realizability Assumption} of the PAC learning framework, we have that
\begin{equation}
    E_{S \sim D_{XY}^{\alpha, n}} \left[\inf_{h \in \hypospace} R_{D_{XY}^{\alpha}}(h)\right] = 0.
\end{equation}
Assuming that $\hypospace$ is PAC learnable, we have that
\begin{equation}
    E_{S \sim D_{XY}^{\alpha, n}} [R_{D_{XY}^{0.5}}(\mathbf{A}(S)) - \inf_{h \in \hypospace} R_{D_{XY}^{0.5}}(h)] \leq \epsilon(n)
\end{equation}
where $\epsilon(n) \to 0$ as $n \to \infty$. 

\paragraph{Summary (informal):} To summarize, if $\alpha$ is known, the distribution space $\mathscr{D}_{XY}$ is such that (i) absolute continuity holds, (ii) relation between the measures is bounded, and if the $\hypospace$ is such that realizability assumption holds then \underline{the anomaly detection problem is \textbf{learnable}}. 


\paragraph{Estimating empirical distribution function of $f$ to get the loss function:} 

It is easy to see that, if $\alpha$ is known the loss function to be minimized is $\ell(h(x),y)f(x)$, where $f$ is the RN derivative above. (See proof in \cref{sec:a1} for details). So, in the supervised case we can estimate $f \approx \hat{f}$ using the empirical distribution function. 

If we let the weight of the anomalous class $D_{X_AY_A}$ to be $1$, then the samples from the majority class $D_{X_BY_B}$ are reweighted by
\begin{equation}
    \omega = \frac{\alpha}{1-\alpha} \approx \frac{\# \text{samples in Minority}}{\# \text{samples in Majority}}
    \label{eq:approx_omega}
\end{equation}
In the experiments we use $\omega$ as tuning hyper-parameter to the loss. See \cref{sec:a2} for detailed derivation. 


\paragraph{Important Remarks:} 
\begin{itemize}
    \item[(i)] Note that the assumptions made in the above section are reasonable for any practical situation. This is evidenced by the experiments in \cref{sec:4}, where we validate the loss in a variety of settings. 
    \item[(ii)] Note that the loss function $\ell(.,.)$ can be supervised/unsupervised. The \ul{RN-Loss is a generic loss function which mandates multiplying the base loss function with the Radon--Nikod\'{y}m derivative}. One can in theory use \emph{any suitable approximation} to $f$. However, as we see in \cref{sec:4}, \cref{eq:approx_omega} works well when the relevant information is available.
    \item[(iii)] \textit{Generalization:} The \ul{RN-Loss} can be easily adapted to the unsupervised anomaly detection framework, as demonstrated in the CBLOF and ECBLOF examples. The key insight is addressing the discrepancy between the empirical imbalanced distribution ($D_{XY}^\alpha$ - with cluster size bias) and the desired balanced distribution ($D_{XY}^{0.5}$). In clustering-based anomaly detection methods, the data is assumed to come from a mixture of distributions, where each cluster represents a component. The RN-Loss framework corrects the inherent bias introduced by cluster sizes by applying a derivative function that accounts for the density ratio between these distributions. For instance, in CBLOF, this correction leads to an adjusted anomaly score that incorporates both the distance-based rarity and the cluster size, while in ECBLOF, it helps justify the removal of cluster size scaling to achieve unbiased detection. %This theoretical foundation validates why certain modifications to these algorithms work well in practice and provides a principled approach to adapting other unsupervised methods. 
    Detailed mathematical proofs can be found in \cref{sec:a3} and \cref{sec:a4}.
\end{itemize}

\section{Experiments - Anomaly Detection}
\label{sec:4}

\paragraph{Datasets:} A total of 96 datasets (29 Univariate and 67 Multivariate) with anomalous instances in varying degrees were considered for evaluation from AD-Bench, SWaT, etc., with the notable inclusion of ESA-ADB, a recently published time series data set (\cref{sec:a6}, \Cref{table:1}).
Our datasets cover multiple domains such as finance, healthcare, e-commerce, industrial systems, telecommunications, astronautics, computer vision, forensics, botany, sociology, linguistics, etc. The data volume ranges from 80 to 61,936, and the anomaly percentages range from 0.03\% to 43.51\%. We further split the datasets over three anomaly contamination ranges: i) less than 1\% (critically imbalanced datasets), ii) between 1\% to 10\%, and iii) greater than 10\% (moderately imbalanced datasets). %These verify the robustness of the proposed architecture against State-Of-The-Art algorithms and their performance on a huge variety of datasets.
\begin{itemize}
    \item[(i)] \textbf{Anomaly $<1$\%}: In this, we primarily focus on detecting extreme outliers or rare events as they deviate significantly from the normal, also being highly indicative of critical issues such as fraud or system failures. This set consists of 22 univariate and 4 multivariate datasets from different distributions such as Weibull and log-normal other than Gaussian.(\cref{sec:a6}, \Cref{table:3}) 
    \item[(ii)] \textbf{Anomaly between $1-10$\%}: This set also indicates outliers that are less frequent than the normal data but occur often enough to be noticeable. It consists of 47 datasets (40 multivariate and 7 univariate) with distributions such as Exponential, Weibull, Gamma, and Log-normal. Extensive research has been performed on these datasets over the past few years and is simultaneously documented in the form of surveys compiling all the methodologies presented to date, \cite{I4, I5}.
    \item[(iii)] \textbf{Anomaly $>10$\%}: These datasets comprise clusters of sample points that lie away from the general distribution and are termed outliers, such as in some cases of collective anomalies or contextual anomaly groups. This set has 16 multivariate datasets, including time series, with various domains covered in the Appendix.
\end{itemize}
A separate subsection of analysis has been done targeting Time Series anomaly detection in particular, where we use 29 univariate time-dependent and 7 multivariate time-dependent datasets, along with the newly proposed ESA-ADB dataset \cite{I44} along with six other SWaT datasets \cite{SWaT} and a BATADAL dataset.

\paragraph{Network Architecture:} We used two architecture in our study. First, RN-Net is a ReLU feedforward network with RN-Loss, comprising 64 hidden units in a binary classification setting. We train for 50 epochs using the Adam optimizer. Additionally, we use batch normalization~\cite{Norm}, dropout~\cite{Dropout}, and early stopping with a threshold of 10 epochs. We reduce our learning rate by half every 5 epochs until it reaches $10^{-6}$. 
Parallel to this, we integrated L2 regularisation with RN-Net and noticed a further step-up in performance across datasets (Results are in \cref{sec:a6}, \Cref{tab:regularization-comparison-dataset1,tab:regularization-comparison-dataset2}).

Similarly, to demonstrate the flexibility and adaptability of RN-Loss, we create RN-LSTM: A LSTM with 32 hidden units coupled with the RN-Loss function.

%\textcolor{blue}{To illustrate the versatility of RN-Loss, we integrate it with existing unsupervised methods. For instance, one algorithm uses Cluster-Based Local Outlier Factor (CBLOF) distances to detect anomalies. However, relying solely on CBLOF scores skews results—large clusters have lower scores due to denser regions, while small clusters yield higher scores, causing over-detection in sparse regions and under-detection in dense ones. To address this, we scale CBLOF distances by density ratios estimated through Kernel Density Estimation (KDE), which provides continuous density functions. This aligns with RN-Loss’s foundation on continuous measure spaces, enhancing traditional methods with density-based information while maintaining mathematical rigor. Importantly, RN-Loss is adaptable to any supervision level without prior exposure to anomalies, unlike methods such as \cite{DevNet},\cite{I7}.}

\paragraph{Evaluation:} We adopt a modified approach to the traditional 70-30 data splitting technique. We allocate 70\% of the normal data for training, while only 15\% of the anomalous data is used for training. The remaining data, comprising 30\% of the normal data and 85\% of the anomalous data, is reserved for testing. This strategy is designed to evaluate the robustness of the model, particularly given that anomalies typically constitute less than 10\% of the dataset. By using only 15\% of these rare anomalies for training, the exposure to anomalous content is zero or minimal, encasing both scenarios of the model being trained on a completely normal dataset or with some anomaly contamination. The results from both setups are identical, which further eliminates the need for training on anomalous data, as in most supervised learning algorithms for optimal performance, like DevNet \cite{DevNet}, DAGMM \cite{DAGMM}, etc. 

\paragraph{Model-specific Threshold Tuning:} In the domain of anomaly detection, determining optimal threshold values is crucial due to the inherently rare and imbalanced nature of anomalies. Setting the threshold too low may lead to a high number of false positives, reducing the model's significance, and setting it too high may cause the model to miss critical anomalies, which could be disastrous in fields like cybersecurity, fraud detection, etc. Therefore, aiming at the most optimal model performance, we set the following thresholds in accordance with the suggestions from the literature:
For Autoencoders, the lower threshold is set at the $0.75$th percentile, and the upper threshold is at the $99.25$th percentile of Mean Squared Error(MSE) values. All the data points with discriminator scores less than $10$th percentile were considered anomalies for GANs. DAGMM had a dual threshold setting with high and low thresholds, with two standard deviations above and below the mean. For the tree-based approaches, MGBTAI was set to a minimum clustering threshold of 20\% of the dataset size and leaf level threshold of 4, while for d-BTAI, the minimum clustering threshold was set to 10\% of the dataset size. For deep quantile regression, the lower threshold was at $0.9$th percentile and the upper at $99.1$st percentile of the predicted values. The above settings help us in achieving State-of-The-Art results over the AD-Bench and Time series datasets, however, as observed this process becomes extremely meticulous and takes up most of the experimental time.
Hence, for RN-NET and RN-LSTM, we automate our threshold calculation process by maximizing the difference in True positive and False positive rates, which are very important metrics obtained from the AUC-ROC curve. This helps us get the best optimal threshold, and SoTA results across datasets and overall algorithms. The thresholds range from 0.001 to 0.999 and can be found for all the respective datasets in \cref{sec:a6}, \Cref{table:5}.

\paragraph{Statistical Tests:} Each experiment was performed 5 times, after which mean and standard deviations for the repeats were calculated. We use Cohen's $d$ effect size test, with the ``medium'' effect size popularized by \citet{sawilowsky2009new} to declare wins, ties, and losses. A detailed comparison can be observed in \Cref{table:1a)} (\Cref{sec:a6}).

\begin{figure*}[htbp]
  \centering
  \begin{minipage}{1\textwidth}
    \centering
    % Place subfigures side by side by removing \vspace and adjusting positioning
    \begin{subfigure}[b]{0.47\textwidth} 
      \includegraphics[width=\textwidth]{Images/F1_Scores_Multi.png}
      \caption{F1 Scores of algorithms on 60 Multivariate Datasets}
      \label{fig:f1scores}
    \end{subfigure}
    \hspace{0.05\textwidth}  % Add horizontal space between figures
    \begin{subfigure}[b]{0.47\textwidth} 
      \includegraphics[width=\textwidth]{Images/F1_Univariate.png}
      \caption{F1 Scores of algorithms on 29 Univariate datasets}
      \label{fig:f1scores_timeseries}
    \end{subfigure}
  \end{minipage}
  
  \caption{Comparison of algorithm performance on different datasets based on F1 Scores for 60 Multivariate and 29 Univariate Time Series datasets.(Excluding 7 Multivariate Datasets and ESA-ADB dataset). For other metrics (Precision, Recall and AUC-ROC) refer \Cref{fig:performance} in \cref{sec:a6}.}
  \label{fig:performance}
\end{figure*}

\paragraph{Baselines and SoTA:} The current SoTA algorithms used for comparison include Local Outlier Factor(LOF), Isolation Forest (IForest), One-class SVM (OCSVM), Autoencoders (IEEE TSMC, 2022) Deep Autoencoding Gaussian Mixture Model (DAGMM, ICLR, 2018), Quantile LSTM(q-LSTM, TAI, 2024), Deep Quantile Regression \cite{I77}, GNN \cite{GDN}(AAAI, 2021), GAN (NeurIps, 2020), DevNet(CVPR, 2015), MGBTAI and d-BTAI (2023) as covered in \Cref{tab:algo_list} in \cref{sec:a6}. Above is a mix of supervised and unsupervised methods, forming our baselines for comparison on anomalous datasets. Some observations on the above algorithms are as follows: GAN performed well and achieved perfect recall on 14 datasets overall, however it classified all datapoints as anomalies in 13 datasets (including 6 SWaT datasets). This observation stemmed from the uniform discriminator score equal to 1, i.e. the algorithm classified all data points as anomalous. \cite{GDN} proposed an approach based on GNN (GDN) to detect anomalies in multivariate time series datasets. GDN, being a SoTA algorithm in this field, offers competition to RN-Net and RN-LSTM but still lags behind, as seen from table \ref{table:combined}. Some datasets have more than 10\% anomalies and hence make more of a classification problem rather than an anomaly detection problem, so a Linear SVM is used to quantify its performance and compared with RN-Net (\cref{sec:a6}, \Cref{table:11}). Further, to study univariate datasets in particular, we have included 4 quantile-based algorithms, 3 qLSTM with sigmoid, PEF, and tanh activation functions, and a deep quantile regression algorithm (\Cref{sec:a6}, \cref{table:12,table:13,table:14,table:15}). Other important algorithms that have been tested along with the above are ECOD \cite{ECOD}, COPOD \cite{COPOD}, KNN, LUNAR \cite{LUNAR}, PCA, DSVDD (\cref{sec:a6}, \Cref{table21,table22,table23,table24,table25,table26,table27,table28}).

\begin{table*}[h!]
\centering
\scriptsize 
\setlength{\tabcolsep}{0.9mm} 
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{0.9mm}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Dataset     & LOF  & Iforest & AutoEncoders & DAGMM & Elliptic  Envelope & DevNet & GAN  & MGBTAI & dBTAI & Deep-SAD & FTTransformer & PReNet & RN-Net & RN-LSTM       \\
\hline
BATADAL  04 & 0.26 & 0.10    & 0.12         & 0.25  & 0.34               & 0.07   & 0.14 & 0.10   & 0.18  & 0.22     & 0.12          & 0.22   & 0.23          & \textbf{0.57} \\
SWaT  1     & 0.09 & 0.24    & 0.14         & 0.53  & 0.20               & 0.22   & 0.16 & 0.24   & 0.30  & 0.55     & 0.29          & 0.22   & 0.40          & \textbf{0.90} \\
SWaT  2     & 0.08 & 0.15    & 0.13         & 0.15  & 0.18               & 0.16   & 0.04 & 0.15   & 0.11  & 0.13     & \textbf{0.40} & 0.11   & 0.30          & 0.25          \\
SWaT  3     & 0.06 & 0.02    & 0.20         & 0.34  & 0.13               & 0.23   & 0.04 & 0.02   & NA    & NA       & NA            & NA     & 0.69          & \textbf{0.72} \\
SWaT  4     & 0.17 & 0.18    & 0.03         & 0.37  & 0.10               & 0.89   & 0.44 & 0.18   & 0.17  & 0.94     & \textbf{0.99} & 0.37   & 0.08          & 0.96          \\
SWaT  5     & 0.03 & 0.22    & 0.15         & 0.15  & 0.20               & 0.17   & 0.02 & 0.22   & 0.12  & 0.22     & 0.29          & 0.04   & \textbf{0.43} & 0.21          \\
SWaT  6     & 0.09 & 0.57    & 0.46         & 0.33  & 0.32               & 0.20   & 0.06 & 0.57   & 0.22  & 0.46     & \textbf{0.96} & 0.41   & 0.32          & 0.73  \\
\hline
\end{tabular}
}
\caption{Comparative analysis of F1 scores across 14 anomaly detection methods on multivariate time series datasets, including traditional approaches and recent state-of-the-art models (FTTransformer, PReNet). The analysis spans 7 datasets from the BATADAL and SWaT collections. Missing values (NA) in SWaT 3 are attributed to different constraints: d-BTAI reached maximum depth limitations, while FTTransformer, DeepSAD, and PReNet exceeded the 3-hour computational threshold. Results demonstrate the superior performance of RN-LSTM in most scenarios, achieving the highest F1 scores in several test cases. Best F1 scores for each dataset are highlighted in bold.}
\end{table*}

\paragraph{Results:} The following results summarize and include a comprehensive overview of all tables and figures from both the main text and the \cref{sec:a6}.

\paragraph{Multivariate Datasets} We trained and tested RN-Net on a total of 60 multivariate datasets(non-timeseries) spanning across various domains. Figures \ref{fig:precision} through \ref{fig:aucroc} present a summary of our results, where we use 9 of the 14 SoTA algorithms; we defer the full results to the \cref{sec:a6}. 

\textbf{Precision:} As deciphered from the figures, RN-Net leads by a significant margin and achieves peak performance in more than 43 datasets out of 60 across all metrics. The proposed algorithm achieves a perfect precision on 8 datasets and hence correctly identifies all outliers.

\textbf{Recall:} As seen in Fig \ref{fig:recall}, \cref{sec:a6}, the RN-Net model shows exceptional performance by achieving peak recall values in 31 out of 60 datasets, again leaving tree-based approaches and other algorithms lagging behind. The proposed algorithm achieved a perfect recall on 15 datasets, implying no false positives and correctly identifying all outliers.

\textbf{F-1 Score:} As expected from previous results, Fig \ref{fig:f1scores}, \cref{sec:a6}, shows that the proposed model attains the maximum F1 score on $44$ datasets because the F1 score is dependent on precision and recall values. The tree-based and other unsupervised algorithms fall short in F1 Scores, demonstrating their under-performance in this metric.

\textbf{AUC-ROC:} We clearly see that RN-Net outperforms every other algorithm by a significant margin (Fig. \ref{fig:aucroc}, \cref{sec:a6}. This showcases that it is able to clearly differentiate between the two classes, one of which is less than 10\% in some cases. The d-BTAI/MGBTAI algorithms and Elliptic Envelope manages to perform highest in 2 datasets.

The recent state-of-the-art (SoTA) algorithms demonstrate varying performance across multiple datasets. FTTransformer achieves the best results on 8 datasets in terms of precision, 2 datasets for recall, 9 datasets for F1-score, and 7 datasets for AUC-ROC. DeepSAD shows superior performance on 3 datasets for precision, 10 for recall, 9 for F1-score, and 8 for AUC-ROC. Meanwhile, PReNet achieves top performance on 2 datasets in terms of precision, 6 for recall, none for F1-score, and 7 for AUC-ROC. Detailed results can be found in \cref{sec:a6}.

\paragraph{Time Series - Univariate Datasets}
This study considers 29 univariate datasets, varying in size from about 544 to 2500 datapoints with anomaly percentages ranging from 0.04\% to 1.47\%. These may be single data instances (point anomalies) or a few data points scattered at a reasonable distance from the norm. The varied distributions of these datasets were identified based on chi-square (\cref{sec:a6}, \cref{table:3}).

\textbf{Precision:} As seen from \cref{fig:precision_timeseries}, \cref{sec:a6}, RN-Net shows peak precision in 20 datasets. This signifies the capability of correctly identifying many true anomalies out of the total flagged instances, considering the datasets are also critically imbalanced. This is followed by dBTAI/MGBTAI peaking in 17 datasets. 

\textbf{Recall:} As seen from the above \cref{fig:recall_timeseries}, \cref{sec:a6}, all models show a considerable recall performance in the univariate datasets with a least group minimum of highest in 10 datasets(DevNet). The RN-Net model tops the chart with the best recall in 20 out of 29 datasets, followed by Isolation Forest, qLSTM, dBTAI/MGBTAI, and RN-LSTM, respectively.  

\textbf{F-1 Score:} The above-mentioned results give an indication of the results obtained from \cref{fig:f1scores_timeseries}, \cref{sec:a6}, RN-Net outperforms all algorithms in 21/29 datasets based on the F1 score. In the case of tree-based algorithms, the peak is recorded in 7 datasets.

\textbf{AUC-ROC:} RN-Net achieves the highest performance in terms of AUC-ROC values in 17 datasets, closely followed by dBTAI/MGBTAI at 9 datasets, as observed from \cref{fig:aucroc_timeseries}, \cref{sec:a6}. 

The recent state-of-the-art (SoTA) algorithms demonstrate varying performance across multiple datasets. FTTransformer achieves the best results on 8 datasets in terms of precision, 2 datasets for recall, none for F1-score, and 14 datasets for AUC-ROC. DeepSAD shows superior performance on 3 datasets for precision, 2 for recall, none for F1-score, and 7 for AUC-ROC. Meanwhile, PReNet achieves top performance on 5 datasets in terms of precision, none for recall, none for F1-score, and 12 for AUC-ROC. Detailed results can be found in \cref{sec:a6}.
Overall, RN-Net has SoTA performance, whereas, though better than most of the algorithms, RN-LSTM struggles a bit, perhaps because of a fixed number of timesteps, extremely low number of anomalies in some datasets, etc., reasons which can be addressed when dealing with the intricacies of an LSTM. However, it satisfies the motive of showcasing RN-loss adaptability and records SoTA performance on some univariate datasets (with less than 1\% anomalies).

\paragraph{Time Series - Multivariate Datasets}
We propose the RN-LSTM model for Anomaly detection in time series data, which also emphasizes on the flexibility and adaptability of the RN-loss function. The datasets used are SWaT, BATADAL, and ESA-ADB. The Secure Water Treatment (SWaT) plant is a testbed at the Singapore University of Technology and Design \cite{SWaT}[SWaT: A Water Treatment Testbed for Research and Training on ICS Security], and is considered as a world class dataset. Some of the major challenges faced while working with this dataset are covered in the paper, \cite{I54}. The results obtained from LOF, IForest, OCSVM, AutoEncoder, DAGMM, Envelope, DevNet, GDN\cite{GDN}(a modified version of GNN specifically created for Multivariate Time series anomaly detection), GAN, MGBTAI, d-BTAI, RN-Net and RN-LSTM in the form of Recall, F1 score and AUC-ROC are recorded in the table \cref{table:combined}, \cref{sec:a6}. These algorithms were chosen because of their dominance over others across datasets.

As shown in the table, GAN has perfect recall performance on almost every dataset but has low precision in each of them, as can be seen from the previous table, indicating that it classifies almost every sample as an anomaly, which may lead to increased false alarms. Excluding GAN, Tree-based algorithms like MGBTAI show the highest performance on 4 datasets based on Recall, and GDN shows the best performance on two datasets based on AUC-ROC. OCSVM also performs well, with peak performance in two of the datasets. GDN, specifically designed for the task of anomaly detection in multivariate time series datasets, falls short when compared with RN-LSTM. RN-Net and RN-LSTM perform best on all datasets based on F1 scores and showcase decent performance individually.

The recent state-of-the-art (SoTA) algorithms demonstrate varying performance across multiple datasets. FTTransformer achieves the best results on 1 dataset in terms of precision, none for recall, 3 for F1-score, and 5 datasets for AUC-ROC. DeepSAD shows superior performance on 1 dataset for precision, 1 for recall, 1 for F1-score, and 1 for AUC-ROC. Meanwhile, PReNet achieves top performance on 1 dataset in terms of precision, none for recall, F1-score or AUC-ROC. Detailed results can be found in \cref{sec:a6}.

\section{Conclusion}

Anomaly detection is a fundamental problem across multiple domains. Formally, an anomaly is any sample that does not belong to the underlying data distribution. However, identifying anomalies is challenging, particularly when the data distribution exhibits high variability. Despite its importance, the theoretical foundations of anomaly detection remain underexplored.

\emph{What is the right principle to design loss function for anomaly detection?} We show that the right principle should correct the  discrepancies between the distributions. This is easily achieved by weighing the generic loss function with \rnthm{} derivative. We prove this by establishing the PAC learnability of anomaly detection. We refer to this approach as RN-Loss. Notably, we show that (supervised) weight-adjusted loss functions and unsupervised Cluster-Based Local Outlier Factor (CBLOF) naturally emerge as performant and conceptual instances of this correction mechanism.

Empirical evaluations across 96 datasets demonstrate that weighting a standard loss function by the \rnthm{} derivative enhances performance, making RN-Loss a robust, efficient, and adaptable solution that outperforms state-of-the-art methods under varying anomaly contamination levels.

The integral representation of the weighted loss function via \rnthm{} derivative can be used for imbalanced class boundaries. An interesting regularization interpretation can also be handy in future. The sparsity of the derivative implies a sparsity-inducing regularization; conversely, if it is dense, it might be interpreted as a smoothing regularizer.


% We proposed a novel loss function, called RN-Loss, derived from the Radon–Nikod\'ym derivative, for anomaly detection in both supervised and unsupervised settings. We establish the learnability of anomaly detection when the anomaly contamination ratio in the dataset is known. Thus, the proposed RN-Loss is theoretically sound. 

% We  demonstrate that weighting a standard loss function by the RN derivative enhances performance. Empirical evaluations across 96 datasets confirm RN-Loss's robustness, efficiency, and adaptability, outperforming state-of-the-art methods in various anomaly contamination scenarios.

% We show that the impact of RN-Loss extends beyond improved classification performance; it enhances unsupervised clustering-based anomaly detection by addressing imbalanced distributions and optimizing threshold selection. The proposed method is flexible across different data distributions, thus it's applicability is broad, making it a significant advancement in the field.



% References
\bibliography{main}

\newpage

\onecolumn

\title{RN-Loss: A New Measure Theoretic Loss Function for a Generalized, Adaptable Anomaly Detector \\(Supplementary Material)}

\maketitle

\appendix
\section{Proofs}
\label{sec:a1}


\paragraph{Statement (\cref{thm:2}):} Let $\nu$ denote the measure induced by $D_{XY}^{0.5}$ and $\mu$ denote the measure induced by $D_{XY}^{\alpha}$. Also, let $d \nu = f  d \mu$ (absolutely continuous) where $1/b < f < b$ for some $b < \infty$ on the support of $\mu$. Then, there exists a constant $\Delta_{\mu, \nu}$ such that
\begin{equation}
   \frac{1}{\Delta_{\mu, \nu}} \leq \frac{R_{D_{XY}^{0.5}}(h)}{R_{D_{XY}^{\alpha}}(h)} \leq \Delta_{\mu, \nu}
    \quad \text{for all } h,
\end{equation}
where $R_{D_{XY}^{\alpha}}(h) = \mathbb{E}_{(\mathbf{x},y)\sim D_{XY}^{\alpha}}\bigl[\ell(h(\mathbf{x}),y)\bigr]$ and similarly for $R_{D_{XY}^{0.5}}(h)$.


\begin{proof}
Recall, that 
\begin{equation}
    R_{D_{XY}^{0.5}}(h) = \int \ell(h(\vx,y)) d\nu = \int \ell(h(\vx,y)) f d\mu \quad and \quad R_{D_{XY}^{\alpha}} = \int \ell(h(\vx,y)) d\mu
\end{equation}
So, we have
\begin{equation}
    \frac{R_{D_{XY}^{0.5}}(h)}{R_{D_{XY}^{\alpha}}(h)} = \frac{\int \ell(h(\vx,y))f d\mu}{\int \ell(h(\vx,y)) d\mu}
\end{equation}
Now, observe that $\ell$ is taken to be 0-1 loss, $f$ is bounded by $1/b$ and $b$ on the support of $\mu$, and $\int d\mu = 1$ (probability measure). Note that the bound $b$ depends on the distributions $\mu, \nu$. Hence, we have 
\begin{equation}
    \frac{1}{\Delta_{\mu, \nu}} \leq \frac{R_{D_{XY}^{0.5}}(h)}{R_{D_{XY}^{\alpha}}(h)} \leq \Delta_{\mu, \nu}
\end{equation}
for some constant $\Delta_{\mu, \nu}$

\end{proof}


\section{Deriving the Loss function:}
\label{sec:a2}

\paragraph{Recall,} \(D_{X_B Y_B}\) and \(D_{X_A Y_A}\) be two probability distributions on a measurable space \(\mathcal{X}\times\mathcal{Y}\). Denote their respective densities by $p_B(x,y)$ and  $p_A(x,y)$.For any fixed $\alpha \in [0,1]$, we define $D_{XY}^{\alpha}$ by $(1-\alpha) D_{X_B Y_B} + \alpha D_{X_A Y_A}$. Denote by $\mu$ the probability measure induced by $D_{XY}^{\alpha}$. Equivalently, $\mu$ has density
\begin{equation}
  \mu(x,y) = (1-\alpha) p_B(x,y) + \alpha p_A(x,y).
\end{equation}
We also consider distribution \(D_{XY}^{0.5}\), whose induced measure is denoted $\nu$, with density
\begin{equation}
  \nu(x,y) = 0.5 p_B(x,y) + 0.5 p_A(x,y)
\end{equation}

Assume that $\nu$ is absolutely continuous with respect to $\mu$ $\nu << \mu$. By the \rnthm{} theorem, there is a function 
$f(x,y)$ such that
\begin{equation}
   d \nu = f d \mu \quad \text{or} \quad  \frac{d\nu}{d\mu}(x,y) = f(x,y).
\end{equation}
Under the usual condition that $\mu(x,y) > 0$, we obtain
\begin{equation}
  f(x,y) = \frac{\nu(x,y)}{\mu(x,y)} =\frac{0.5p_B(x,y) + 0.5p_A(x,y)}
       {(1-\alpha)p_B(x,y) + \alpha p_A(x,y)}.
  \label{eq:RNderiv}
\end{equation}

\paragraph{Using empirical distribution function to estimate $f$:} Let us now assume we have a sample $\{\vx^i, y^i\}$ from $D_{XY}^{\alpha}$ where $y^{i} = 1$ if it belongs to $D_{X_BY_B}$ and $y^{i} = 0$ if it belongs to $D_{X_AY_A}$. From above we have,
\begin{equation}
    f(\vx, +1) = \frac{0.5}{1-\alpha} \quad and \quad f(\vx, 0) = \frac{0.5}{\alpha}
\end{equation}
If we let the weight of the anomalous class $D_{X_AY_A}$ to be $1$, samples from the majority class $D_{X_BY_B}$ are reweighted by
\begin{equation}
    \omega = \frac{\alpha}{1-\alpha}
\end{equation}

\section{CBLOF as \rnthm{} derivative:}
\label{sec:a3}

CBLOF (Cluster-Based Local Outlier Factor) is a widely known measure for obtaining anomaly scores which are known to work well empirically. In this section, we provide an explanation of CBLOF using the \rnthm{} framework in this article.\newline Let $\mathcal{X} \subset \mathbb{R}^d$ be the data space. Given a sample $S = \{ x_i \}_{i=1}^{n}$ drawn from a mixture distribution $D_{XY}^{\alpha}$, we aim to assign an anomaly score to each $x \in \mathcal{X}$.

\paragraph{Naive definition of anomaly:} Any point can be considered an anomaly if it lies far-away from the center. A standard assumption which works well in practice is to assume that the base distribution is a mixture of Gaussians. Thus, we have the following anomaly score
\begin{equation}
    h^*(x) = \argmin_i \| x - \mu_i \|
    \label{eq:a31}
\end{equation}
where $\mu_i$ denotes the centroid of the $i$th Gaussian. In words, if the point is far-away from the closest center, then it would be considered an anomaly.

\paragraph{Using the naive approach for anomaly detection:} The naive definition applied to identify the outliers within this sample is -
\begin{itemize}
    \item[1.] Cluster the sample into clusters $\{C_1, C_2, \cdots, C_m\}$, obtain the $m$ corresponding means $\mu_k$
    \item[2.] Consider only the largest $k < m$ clusters to estimate the density $p_B$. This filtering step allows us to estimate $p_B$.
    \item[3.] Obtain the scores $h^*(x)$ using \cref{eq:a31} and the cluster centroids.
\end{itemize}

\paragraph{Distribution assumption of $h^*$:} Let $p_{C_i}(x)$ denote the distribution of each cluster $C_i$. Implicit in the above formulation is the \emph{distribution assumption}
\begin{equation}
    D_{XY}^{\alpha} = \frac{1}{m} \sum_{i=1}^{m} \frac{1}{|C_i|} p_{C_i}(x)
\end{equation}
The above distribution can be inferred from the assumption made by CBLOF \cite{CBLOF} - the size of the cluster should not matter for anomalies. \textbf{Remark:} As we shall shortly see, this becomes equivalent to having more anomalies in the small cluster. 
\newline
Furthermore, we assume that each $p_{C_i}(x)$ has a distinct support ${S}_i$, ensuring that samples do not overlap across clusters,i.e.
\begin{equation}
    \mathcal{S}_i \cap \mathcal{S}_j = \emptyset, \quad \forall i \neq j.
    \label{eq:distinct_supports}
\end{equation}
This assumption ensures that each point $x$ belongs uniquely to a single cluster, simplifying density estimation and anomaly detection.


\paragraph{Discrepancy caused by $D_{XY}^{\alpha}$ vs $D_{XY}^{0.5}$:} Following the reasoning from \cref{sec:a2}, we correct the discrepancy using the \rnthm{} derivative. Note that,
\begin{equation}
     D_{XY}^{0.5} \equiv \frac{1}{m} \sum_{i=1}^{m} p_{C_i}(x)
\end{equation}
Then, the \rnthm{} derivative can be computed to be
\begin{equation}
    f(x) = \sum_{i=1}^{m} |C_i| I[x \in C_i]
\end{equation}
Accordingly, the anomaly score is adjusted to be
\begin{equation}
    h^{\dagger}(x) =
    \begin{cases}
        |C_i| d(x, C_i), & \text{if } x \in C_i, C_i \text{ is large} \\
        |C_i| d(x, C_L), & \text{if } x \in C_i, C_L \text{ is the nearest large cluster}
    \end{cases}
    \label{eq:final_cblof}
\end{equation}
where $C_L$ is the closest large cluster. This ensures anomaly scores reflect both distance-based rarity and density ratio correction.


\section{ECBLOF as \rnthm{} derivative:}  
\label{sec:a4}

ECBLOF (Enhanced Cluster-Based Local Outlier Factor)\cite{ECBLOF} is a modified version of CBLOF designed to avoid the bias caused by multiplying the anomaly score with the cluster size. In CBLOF, points in smaller clusters are assigned higher anomaly scores due to this scaling factor, which can lead to biased detection favoring large clusters. In ECBLOF we assume, \( D_{XY}^{0.5} = D_{XY}^{\alpha} = \frac{1}{m} \sum_{i=1}^m p_{C_i}(x) \) implying that both enforce \textbf{cluster-size invariance}, ensuring anomaly scores depend solely on distances, not cluster populations.

Let $\mathcal{X} \subset \mathbb{R}^d$ represent the data space. Given a sample $S = \{ x_i \}_{i=1}^{n}$ drawn from a mixture distribution $D_{XY}^{\alpha}$, the objective is to assign an unbiased anomaly score to each $x \in \mathcal{X}$.

\paragraph{Modifications in ECBLOF compared to CBLOF:}
\begin{itemize}
    \item \textbf{Elimination of cluster size scaling:}  
    In CBLOF, the anomaly score is scaled by the size of the cluster $|C_i|$, which biases the score towards larger clusters.  
    \begin{equation}
        h(x) = |C_i| \cdot d(x, C_i)
    \end{equation}
    ECBLOF removes this scaling, ensuring that the anomaly score is based purely on the distance from the point to the nearest cluster centroid.  
    \begin{equation}
        h^\dagger(x) = d(x, C_i)
    \end{equation}
    
    \item \textbf{Unbiased scoring:}  
    By eliminating the cluster size multiplication, ECBLOF avoids favoring larger clusters and provides a more balanced detection of anomalies.

    \item \textbf{Handling small clusters:}  
    Similar to CBLOF, ECBLOF still identifies small clusters as potential anomaly sources. However, if a point belongs to a small cluster, the distance to the nearest large cluster $C_L$ is used to compute its anomaly score:
    \begin{equation}
        h^\dagger(x) =  
        \begin{cases}  
            d(x, C_i), & \text{if } x \in C_i, C_i \text{ is large} \\  
            d(x, C_L), & \text{if } x \in C_i, C_L \text{ is the nearest large cluster}
        \end{cases}
        \label{eq:ecblof_score}
    \end{equation}
\end{itemize}

\paragraph{Distribution assumption in ECBLOF:}  
Each cluster $C_i$ is assumed to be governed by a distinct distribution $p_{C_i}(x)$, with disjoint support sets $\mathcal{S}_i$ such that:
\begin{equation}
    \mathcal{S}_i \cap \mathcal{S}_j = \emptyset, \quad \forall i \neq j.
    \label{eq:distinct_supports}
\end{equation}

The mixture distribution $D_{XY}^{\alpha}$ is given by:
\begin{equation}
    D_{XY}^{\alpha} = \frac{1}{m} \sum_{i=1}^m p_{C_i}(x)
\end{equation}
For unbiased anomaly detection, we need the distribution $D_{XY}^{0.5}$:
\begin{equation}
    D_{XY}^{0.5} = \frac{1}{m} \sum_{i=1}^{m} p_{C_i}(x)
\end{equation}

By assuming \( D_{XY}^{\alpha} = D_{XY}^{0.5} \), ECBLOF treats all clusters equally in the mixture distribution, removing the bias that favors large clusters in CBLOF; this ensures that anomaly scores depend only on a point’s distance from its cluster centroid, making detection fairer and unbiased.

Thus the \rnthm{} derivative reduces to:
\begin{equation}
    f(x) = 1
\end{equation}
Hence, the corrected anomaly score for ECBLOF is given by:
\begin{equation}
    h^{\dagger}(x) = d(x, C_i)
\end{equation}
\textbf{Remark:} Unlike in CBLOF, we do not multiply by $|C_i|$. This ensures that the score is purely distance-based, without being influenced by cluster size. This ensures a fair and unbiased detection of anomalies.

\section{Literature Review}
\label{sec:a5}
Throughout the years, much research has been conducted in anomaly detection with a multitude of explored methods such as in \cite{I31, I51, I37, I38, I22, I39, I40, I41}. Since then, this interest has increased significantly as various domains such as cybersecurity \cite{I17, I18}, fraud detection, and healthcare \cite{I20, I21} became more relevant. 
The work on learning from imbalanced datasets was proposed in AAAI 2000 workshop 
%Workshop on learning from imbalanced data) 
and highlighted research on two major problems, types of imbalance that hinder the performance of standard classifiers and the suitable approaches for the same. \cite{I68} also showed that the class imbalance problem affects not only standard classifiers like decision trees but also Neural Networks and SVMs. The work by \cite{I66,I67}
%This was followed by ICML'2003 Workshop on Learning from Imbalanced Datasets[II] \cite{I66} and ACM SIGKDD Exploration'2004 \cite{I67} that 
gave a further boost to research in imbalanced data classification. Extensive research was done on unsupervised learning methods to address issues such as relying on static thresholds, in turn struggling to adapt to dynamic data, resulting in high false positives and missed anomalies. However, the work by \cite{I70} observed that unsupervised anomaly detection can be computationally intensive, especially in high-dimensional datasets. Jacob and Tatbul \cite{I71} delved into explainable anomaly detection in time series using real-world data, yet deep learning-based time-series anomaly detection models were not thoroughly explored well enough. %Clustering-based methods like K-Means and Isolation Forest may require prior knowledge of cluster numbers.
With significant growth in applying various ML algorithms
to detect anomalies, there has been an avalanche of anomaly
benchmarking data \cite{I43}, \cite{I18}, \cite{I72}, as well as empirical studies of the performances of existing algorithms \cite{I73}, \cite{I74} on different benchmark data. Due to the importance of the problem, there have also been efforts to produce benchmarks such as AD-Bench~\cite{I43} and ESA-ADB~\cite{I44}. %AD-Bench examines the performance of 30 detection algorithms across an extensive set of 57 benchmark datasets, adding to the body of research in the field. 
Researchers have critically examined the suitability of evaluation metrics for machine learning methods in anomaly detection. Kim et al. \cite{I76} exposed the limitations of the F1-score with point adjustment, both theoretically and experimentally. 
%Despite the emergence of numerous new metrics for time series anomaly detection, even the most recent ones are not without their shortcomings. 

To conclude, %despite the availability of extensive anomaly benchmark datasets and publicly accessible machine learning-driven anomaly detection methods, 
recent benchmarking studies, concentrated on deep-learning-based anomaly detection techniques mostly, have not examined the performance across varying types of anomalies, such as singleton/point, small, and significantly large numbers, nor across different data types, including univariate, multivariate, temporal, and non-temporal data. Additionally, there has been a lack of exploration into how anomalies should be identified when their frequency is high. These observations prompt several critical questions. Is the current SoTA algorithm the most effective? Are we reaching the peak in anomaly detection using Deep Learning approaches? Are unsupervised learning algorithms truly better than supervised learning algorithms?

%%%%%%%%%%%%%%%%%% ATTACHING PREVIOUS APPENDIX %%%%%%%%%%%%%%%%%%%

\newpage
\section{Experimental Results and Data}
\label{sec:a6}

\begin{table*}[h!]
    \centering
    \caption{\textit{Percentage} of wins, ties, and losses on F-1 score across all datasets (96). For brevity, other methods that have no wins or ties are not shown. Wins and ties are determined via a Cohen's {\it d} effect size test.}
    \label{tab:results:summary}
    \begin{tabular}{lllllllll}
        \toprule
        & (RN-Net/LSTM) & LOF & IForest & Ell. Env. & DevNet & GAN & MGBTAI & d-BTAI \\
        \midrule
        wins & \sbar{76} {76} & \sbar{5} {5} & \sbar{3} {3} & \sbar{0} {0} & \sbar{3} {3} & \sbar{2} {2} & \sbar{2} {2} & \sbar{5} {5} \\
        ties & \sbar{6} {6} & \sbar{0} {0} & \sbar{0} {0} & \sbar{2}  {2} & \sbar{0} {0} & \sbar{0} {0} & \sbar{2} {2} & \sbar{0} {0} \\
        losses & \sbar{18} {18} & \sbar{95} {95} & \sbar{97} {97} &\sbar{98} {98} & \sbar{97} {97} &\sbar{98} {98} &\sbar{96} {96} & \sbar{95} {95} \\
        \bottomrule
    \end{tabular}
    \label{table:1a)}
\end{table*}

\begin{table*}[h!]
    \centering
    \caption{\textit{Percentage} of wins, ties, and losses on F-1 score across datasets with different anomaly ratios.For brevity, other methods (such as OCSVM, DevNet and newer methods like PReNet) that have no wins or ties are not shown.}
        \begin{minipage}{0.48\textwidth}
        \centering
        \caption*{Anomalies $< 1\%$ (26)}
        \fontsize{7pt}{8pt}\selectfont
        \begin{tabular}{@{}lllll@{}}
            \toprule
            & (RN-Net/LSTM) & MGBTAI & d-BTAI & FTTransformer\\
            \midrule
            wins & \sbar{46}{46} & \sbar{14}{14} & \sbar{9}{9} & \sbar{4}{4} \\
            ties & \sbar{45}{45} & \sbar{23}{23} & \sbar{36}{36} & \sbar{0}{0}\\
            losses & \sbar{9}{9} & \sbar{63}{63} & \sbar{55}{55} & \sbar{96}{96}\\
            \bottomrule
        \end{tabular}
        \label{table:1b)}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \caption*{Anomalies $> 10\%$ (16)}
        \fontsize{7pt}{8pt}\selectfont
        \begin{tabular}{lllll}
            \toprule
            & (RN-Net/LSTM) & d-BTAI & FTTransformer & DeepSAD\\
            \midrule
            wins & \sbar{94}{94} & \sbar{6}{6} & \sbar{19}{19} & \sbar{13}{13}\\
            ties & \sbar{6}{6} & \sbar{0}{0} & \sbar{6}{6} & \sbar{0}{0}\\
            losses & \sbar{0}{0} & \sbar{94}{94} & \sbar{75}{75} & \sbar{87}{87}\\
            \bottomrule
        \end{tabular}
        \label{table:1d)}
    \end{minipage}
\end{table*}


\begin{figure*}[htbp]
  \centering
  \begin{minipage}{1\textwidth}
    \centering
    % First row of bar charts
    \begin{subfigure}[b]{0.24\textwidth}
      \includegraphics[width=\textwidth]{Images/Precision_Multivariate.png}
      \caption{Precision of algorithms on 60 Multivariate Datasets}
      \label{fig:precision}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
      \includegraphics[width=\textwidth]{Images/Recall_Multivariate.png}
      \caption{Recall of algorithms on 60 Multivariate Datasets}
      \label{fig:recall}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
      \includegraphics[width=\textwidth]{Images/F1_Scores_Multi.png}
      \caption{F1 Scores of algorithms on 60 Multivariate Datasets}
      \label{fig:f1scores}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
      \includegraphics[width=\textwidth]{Images/AUC-ROC_Multivariate.png}
      \caption{AUC ROC values of algorithms on 60 Multivariate Datasets}
      \label{fig:aucroc}
    \end{subfigure}

    \vspace{1em} % Space between rows

    % Second row of bar charts
    \begin{subfigure}[b]{0.24\textwidth}
      \includegraphics[width=\textwidth]{Images/Precision_Univariate.png}
      \caption{Precision of algorithms on 29 Univariate Time series datasets}
      \label{fig:precision_timeseries}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
      \includegraphics[width=\textwidth]{Images/Recall_Univariate.png}
      \caption{Recall of algorithms on 29 Univariate Time series datasets}
      \label{fig:recall_timeseries}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
      \includegraphics[width=\textwidth]{Images/F1_Univariate.png}
      \caption{F1 Scores of algorithms on 29 Univariate Time series datasets}
      \label{fig:f1scores_timeseries}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
      \includegraphics[width=\textwidth]{Images/AUC-ROC_Univariate.png}
      \caption{AUC ROC values of algorithms on 29 Univariate Time series datasets}
      \label{fig:aucroc_timeseries}
    \end{subfigure}
  \end{minipage}
  
  \caption{Comparison of algorithm performance on different datasets: (a) Precision, (b) Recall, (c) F1 Scores, and (d) AUC ROC for 60 Multivariate and 29 Univariate Time Series datasets.(Excluding 7 Multivariate Datasets and ESA-ADB dataset)}
  \label{fig:performance}
\end{figure*}

\begin{table*}[h!]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{1mm}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{|c|p{16cm}|}
    \hline
    \textbf{Algorithm} & \textbf{Anomaly Detection Approach} \\
    \hline
    LOF & Uses data point densities to identify an anomaly by measuring how isolated a point is relative to its nearest neighbors in the feature space. Implemented using the Python Outlier Detection (PyOD) library with default parameters. Trained on 70\% of data and tested on the entire dataset. \\
    \hline
    Iforest & Ensemble-based algorithm that isolates anomalies by constructing decision trees. Reported to perform well in high-dimensional data. The algorithm efficiently separates outliers by requiring fewer splits in the decision tree compared to normal data points. Implemented using scikit-learn with default parameters. Trained on 70\% of data and tested on the entire dataset. \\
    \hline
    OCSVM & Constructs a hyperplane in a high-dimensional space to separate normal data from anomalies. Implemented using scikit-learn with default parameters. Trained on 70\% normal data, or whatever normal data was available. \\
    \hline
    AutoEncoder & Anomalies are identified based on the reconstruction errors generated during the encoding-decoding process. \textit{Requires training on normal data.} Implemented using Keras. Lower threshold set at the 0.75th percentile, and upper threshold at the 99.25th percentile of the Mean Squared Error (MSE) values. Trained on 70\% normal data, or whatever normal data was available. Anomalies are detected by comparing the reconstruction error with the predefined thresholds. \\
    \hline
    DAGMM & Combines .{autoencoder} and Gaussian mixture models to model the data distribution and identify anomalies. It has a compression network to process low-dimensional representations, and the Gaussian mixture model helps capture data complexity. \textit{This algorithm requires at least 2 anomalies to be effective.} It is trained on 70\% normal data, or whatever normal data was available. Anomalies are detected by calculating anomaly scores, with thresholds set at two standard deviations above and below the mean anomaly score. \\
    \hline
    LSTM & Trained on a normal time-series data sequence. Acts as a predictor, and the prediction error, drawn from a multivariate Gaussian distribution, detects the likelihood of anomalous behavior. Implemented using Keras. Lower threshold set at the 5th percentile, and upper threshold at the 95th percentile of the Mean Squared Error (MSE) values. Trained on 70\% normal data, or whatever normal data was available. Anomalies are detected when the prediction error lies outside the defined thresholds. \\
    \hline
    qLSTM & Augments LSTM with quantile thresholds to define the range of normal behavior within the data. Implementation follows the methodology described in the authors' paper, which applies quantile thresholds to LSTM predictions. Anomalies are detected when the prediction error falls outside the defined quantile range. \\
    \hline
    QREG & A multilayered LSTM-based RNN forecasts quantiles of the target distribution to detect anomalies. The core mathematical principle involves modeling the target variable's distribution using multiple quantile functions. Lower threshold set at the 0.9th percentile, and upper threshold at the 99.1st percentile of the predicted values. Anomalies are detected when the predicted value lies outside these quantile thresholds. Trained on 70\% data and tested on the entire dataset. \\
    \hline
    Elliptic Envelope & Fits an ellipse around the central multivariate data points, isolating outliers. It needs a contamination parameter of 0.1 by default, with a support fraction of 0.75, and uses Mahalanobis distance for multivariate outlier detection. Implemented using default parameters from the sklearn package. Trained on 70\% of data and tested on the entire dataset. Anomalies are detected when data points fall outside the fitted ellipse. \\
    \hline
    DevNet & A Deep Learning-based model designed specifically for anomaly detection tasks. Implemented using the Deep Learning-based Outlier Detection (DeepOD) library. Anomalies are detected based on the deviation score, with a threshold defined according to the model's performance and expected anomaly rate. Trained on 70\% data and tested on the entire dataset; \textit{it requires atleast 2 anomalies in its training set to function, and for optimal performance, it is recommended to include at least 2\% anomalies in the training data.} \\
    \hline
    GAN & Creates data distributions and detects anomalies by identifying data points that deviate from the generated distribution. It consists of generator and discriminator networks trained adversarially. Implemented using Keras. All data points whose discriminator score lies in the lowest 10th percentile are considered anomalies. Trained on 70\% normal data. \\
    \hline
    GNN & GDN, which is based on graph neural networks, learns a graph of relationships between parameters and detects deviations from the patterns. Implementation follows the methodology described in the authors' paper. \\
    \hline
    MGBTAI & An unsupervised approach that leverages a multi-generational binary tree structure to identify anomalies in data. Minimum clustering threshold set to 20\% of the dataset size and leaf level threshold set to 4. Used k-means clustering function. No training data required.\\
    \hline
    dBTAI & Like MGBTAI, it does not rely on training data. It adapts dynamically as data environments change.The small cluster threshold is set to 2\% of the data size. The leaf level threshold is set to 3. The minimum cluster threshold is set to 10\% of the data size and the number of clusters are 2 (for KMeans clustering at each split). The split threshold is 0.9 (used in the binary tree function). The anomaly threshold is determined dynamically using the knee/elbow method on the cumulative sum of sorted anomaly scores. The kernel density uses a gaussian kernel with default bandwidth and uses imbalance ratio to weight the density ratios. Used k-means clustering function. No training data required.\\
    \hline
    FTTransformer & It is a sample adaptation of the original transformer architecture for tabular data. The model transforms all features (categorical and numerical) to embeddings and applies a stack of Transformer layers to the embeddings. However, as stated in the original paper's \cite{FTT} limitations: FTTransformer requires more resources (both hardware and time) for training than simple models such as ResNet and may not be easily scaled to datasets when the number of features is “too large”. \\
    \hline
    DeepSAD & It is a generalization of the unsupervised Deep SVDD method to the semi-supervised anomaly detection setting and thus needs labeled data for training. It is also considered as an information-theoretic framework for deep anomaly detection.\\
    \hline
    PReNet & It has a basic ResNet with input and output convolution layers, several residual blocks (ResBlocks) and a recurrent layer implemented using a LSTM. It is particularly created for the task of image deraining as mentioned in \cite{PReNet}.\\
    \hline
  \end{tabular}
  \caption{Descriptions and hyperparameter settings of SOTA algorithms benchmarked in this study}
  \label{tab:algo_list}
\end{table*}

\begin{table*}[h!]
\centering
\scriptsize 
\setlength{\tabcolsep}{1.5mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|l|l|l|l|l|}
\hline
Dataset          & LOF                      & IForest       & AutoEncoders & DAGMM         & Elliptic  Envelope & DevNet        & GAN  & MGBTAI        & dBTAI & Deep-Sad      & FTT           & PReNet        & RN-Net        \\
\hline
ALOI             & 0.10                     & 0.04          & 0.05         & 0.03          & 0.03               & 0.03          & 0.04 & 0.04          & 0.04           & 0.04          & 0.04          & 0.03          & \textbf{0.29} \\
annthyroid       & 0.26                     & 0.25          & 0.15         & 0.24          & 0.40               & 0.12          & 0.09 & 0.07          & 0.10           & 0.46          & 0.53          & 0.06          & \textbf{0.73} \\
backdoor         & 0.11                     & 0.03          & 0.20         & 0.21          & 0.21               & 0.21          & 0.21 & 0.01          & NA             & NA            & NA            & NA            & \textbf{0.65} \\
breastw          & 0.39                     & 0.98          & 0.27         & \textbf{1.00} & \textbf{1.00}      & 0.86          & 0.00 & 0.85          & 0.89           & 0.88          & 0.94          & 0.97          & 0.97          \\
campaign         & 0.07                     & 0.33          & 0.22         & 0.30          & 0.34               & 0.42          & 0.21 & 0.17          & 0.17           & 0.23          & 0.22          & 0.17          & \textbf{0.76} \\
cardio           & 0.17                     & 0.48          & 0.34         & 0.13          & 0.40               & 0.50          & 0.07 & 0.48          & 0.20           & 0.40          & 0.39          & 0.34          & \textbf{0.90} \\
Cardiotocography & 0.32                     & 0.48          & 0.31         & 0.36          & 0.48               & 0.40          & 0.24 & 0.80          & 0.35           & 0.34          & 0.66          & 0.72          & \textbf{0.88} \\
celeba           & 0.01                     & 0.07          & 0.02         & 0.07          & 0.09               & 0.19          & 0.03 & 0.08          & 0.03           & 0.08          & 0.12          & 0.13          & \textbf{0.26} \\
cover            & 0.02                     & 0.05          & 0.09         & 0.02          & 0.01               & 0.10          & 0.01 & 0.00          & 0.02           & 0.05          & 0.03          & 0.00          & \textbf{1.00} \\
donors           & 0.21                     & 0.10          & 0.14         & 0.50          & 0.20               & 0.44          & 0.04 & 0.00          & NA             & \textbf{1.00} & NA            & 0.59          & 0.90          \\
fault            & 0.32                     & 0.49          & 0.19         & 0.36          & 0.24               & 0.36          & 0.47 & 0.42          & 0.50           & 0.57          & 0.45          & 0.69          & \textbf{0.89} \\
fraud            & 0.00 & 0.02          & 0.02         & 0.01          & 0.02               & 0.02          & NA   & 0.01          & 0.01           & 0.05          & \textbf{0.36} & 0.01          & 0.19          \\
glass            & 0.18                     & 0.10          & 0.09         & 0.00          & 0.12               & 0.00          & 0.06 & 0.08          & 0.11           & 0.08          & 0.14          & 0.05          & \textbf{0.80} \\
Hepatitis        & 0.25                     & 0.18          & 0.13         & 0.25          & 0.33               & 0.25          & 0.20 & 0.00          & 0.17           & 0.57          & 0.41          & 0.63          & \textbf{0.71} \\
http             & 0.00 & 0.03          & 0.04         & 0.04          & 0.04               & 0.04          & NA   & 0.05          & 0.00           & 0.00          & NA            & 0.04          & \textbf{0.92} \\
InternetAds      & 0.48                     & 0.64          & 0.53         & 0.20          & 0.63               & 0.32          & 0.11 & \textbf{0.98} & NA             & 0.52          & NA            & 0.65          & 0.84          \\
Ionosphere       & 0.94                     & 0.96          & 0.50         & 0.69          & \textbf{1.00}      & 0.58          & 0.58 & 0.44          & 0.73           & 0.78          & 0.63          & 0.83          & 0.96          \\
landsat          & 0.31                     & 0.15          & 0.19         & 0.18          & 0.04               & 0.24          & 0.38 & 0.01          & 0.25           & 0.56          & 0.68          & \textbf{0.78} & 0.76          \\
letter           & \textbf{0.36}            & 0.08          & 0.05         & 0.06          & 0.17               & 0.07          & 0.06 & 0.11          & NA             & NA            & NA            &               & 0.30          \\
Lymphography     & 0.33                     & 0.40          & 0.13         & 0.07          & 0.38               & 0.11          & 0.13 & 0.43          & 0.15           & 0.31          & 0.56          & 0.33          & \textbf{0.67} \\
magic.gamma      & 0.62                     & 0.78          & 0.55         & 0.56          & \textbf{0.91}      & 0.78          & 0.76 & 0.77          & 0.60           & 0.72          & 0.68          & 0.62          & 0.91          \\
mammography      & 0.08                     & 0.12          & 0.05         & 0.15          & 0.03               & 0.19          & 0.01 & 0.00          & NA             & 0.03          & 0.06          & 0.00          & \textbf{0.44} \\
mnist            & 0.23                     & 0.32          & 0.19         & 0.16          & 0.16               & 0.47          & 0.09 & 0.02          & 0.17           & 0.28          & 0.54          & 0.35          & \textbf{0.89} \\
musk             & 0.01                     & 0.25          & 0.17         & 0.32          & 0.31               & 0.29          & 0.03 & 0.23          & 0.08           & 0.27          & \textbf{1.00} & 0.03          & 0.91          \\
optdigits        & 0.07                     & 0.05          & 0.04         & 0.05          & 0.00               & 0.26          & 0.03 & 0.33          & 0.02           & 0.53          & \textbf{1.00} & 0.28          & \textbf{1.00} \\
PageBlocks       & 0.39                     & 0.41          & 0.29         & 0.27          & 0.56               & 0.17          & 0.31 & 0.67          & 0.17           & 0.11          & 0.22          & 0.17          & \textbf{1.00} \\
pendigits        & 0.04                     & 0.16          & 0.13         & 0.00          & 0.06               & 0.23          & 0.06 & 0.00          & 0.04           & 0.33          & 0.71          & 0.22          & \textbf{0.84} \\
Pima             & 0.32                     & 0.59          & 0.35         & 0.45          & 0.51               & \textbf{0.68} & 0.15 & 0.28          & 0.42           & 0.51          & 0.48          & 0.57          & 0.40          \\
satellite        & 0.48                     & 0.93          & 0.39         & 0.77          & 0.96               & 0.59          & 0.32 & \textbf{1.00} & 0.51           & 0.62          & 0.91          & 0.80          & 0.84          \\
satimage-2       & 0.03                     & 0.10          & 0.00         & 0.08          & 0.12               & 0.12          & 0.01 & 0.14          & 0.03           & 0.77          & 0.73          & 0.11          & \textbf{0.95} \\
shuttle          & 0.00 & 0.98          & 0.47         & 0.51          & \textbf{1.00}      & 0.48          & 0.07 & 0.49          & 0.45           & 0.80          & 0.91          & 0.67          & 0.64          \\
skin             & 0.24                     & 0.06          & 0.08         & 0.57          & 0.30               & 0.76          & 0.10 & 0.62          & 0.33           & 0.99          & 0.94          & 0.89          & \textbf{1.00} \\
smtp             & 0.00                     & 0.00          & 0.00         & 0.00          & 0.00               & 0.00          & 0.00 & 0.00          & 0.00           & \textbf{1.00} & 0.53          & 0.00          & 1.00          \\
SpamBase         & 0.32                     & 0.41          & 0.41         & 0.63          & 0.31               & 0.76          & 0.08 & 0.73          & 0.66           & 0.92          & 0.86          & 0.81          & \textbf{1.00} \\
speech           & 0.02                     & 0.02          & 0.03         & 0.01          & 0.02               & 0.13          & 0.08 & 0.01          & 0.02           & 0.12          & 0.02          & 0.04          & \textbf{0.93} \\
Stamps           & 0.15                     & 0.23          & 0.14         & 0.03          & 0.11               & 0.15          & 0.45 & 0.13          & 0.20           & 0.39          & \textbf{0.81} & 0.71          & 0.06          \\
thyroid          & 0.10                     & 0.19          & 0.15         & 0.17          & 0.23               & 0.18          & 0.17 & 0.13          & 0.07           & 0.29          & 0.36          & 0.25          & \textbf{0.70} \\
vertebral        & 0.04                     & 0.03          & 0.00         & 0.08          & 0.00               & 0.10          & 0.04 & 0.00          & 0.09           & 0.14          & 0.21          & 0.58          & \textbf{0.75} \\
vowels           & 0.26                     & 0.09          & 0.11         & 0.01          & 0.05               & 0.19          & 0.03 & 0.67          & 0.08           & 0.30          & 0.24          & 0.31          & \textbf{0.82} \\
Waveform         & 0.09                     & 0.06          & 0.11         & 0.09          & 0.04               & 0.07          & 0.21 & 0.00          & 0.04           & 0.14          & 0.07          & 0.08          & \textbf{0.67} \\
WBC              & 0.09                     & 0.32          & 0.38         & 0.35          & 0.38               & 0.31          & 0.06 & 0.42          & 0.19           & 0.30          & \textbf{0.50} & 0.43          & 0.36          \\
WDBC             & 0.27                     & 0.21          & 0.26         & 0.24          & 0.26               & 0.26          & 0.01 & 0.21          & 0.13           & 0.83          & \textbf{0.90} & 0.27          & 0.56          \\
Wilt             & 0.10                     & 0.01          & 0.05         & 0.10          & 0.10               & 0.00          & 0.05 & 0.00          & 0.04           & 0.08          & 0.09          & 0.14          & \textbf{1.00} \\
wine             & 0.77                     & 0.18          & 0.50         & 0.31          & 0.36               & 0.45          & 0.51 & 0.53          & 0.25           & \textbf{0.90} & \textbf{0.90} & 0.69          & 0.47          \\
WPBC             & 0.10                     & 0.14          & 0.25         & 0.15          & 0.15               & 0.18          & 0.20 & 0.12          & 0.26           & 0.40          & 0.57          & 0.55          & \textbf{1.00} \\
yeast            & 0.27                     & 0.30          & 0.35         & 0.36          & 0.26               & 0.35          & 0.49 & 0.10          & 0.30           & 0.36          & 0.46          & 0.34          & \textbf{0.82} \\
CIFAR10          & 0.14                     & 0.13          & 0.07         & 0.05          & 0.13               & 0.17          & 0.18 & 0.04          & 0.08           & 0.10          & 0.08          & 0.06          & \textbf{0.81} \\
FashionMNIST     & 0.15                     & 0.19          & 0.12         & 0.11          & 0.18               & 0.29          & 0.21 & 0.06          & 0.10           & 0.22          & 0.15          & 0.12          & \textbf{0.33} \\
MNIST-C          & 0.13                     & 0.08          & 0.04         & 0.08          & 0.08               & 0.38          & 0.09 & 0.05          & 0.10           & 0.49          & 0.31          & 0.27          & \textbf{0.97} \\
MVTec-AD         & 0.87                     & \textbf{1.00} & 0.50         & 0.40          & 0.12               & 0.26          & 0.97 & \textbf{1.00} & 0.77           & 0.93          & \textbf{1.00} & \textbf{1.00} & 0.96          \\
SVHN             & 0.11                     & 0.06          & 0.04         & 0.05          & 0.09               & 0.38          & 0.10 & 0.06          & 0.07           & 0.14          & 0.07          & 0.09          & \textbf{0.49} \\
Agnews           & 0.11                     & 0.06          & 0.05         & 0.06          & 0.07               & 0.07          & 0.10 & 0.05          & 0.06           & 0.09          & 0.05          & 0.16          & \textbf{0.33} \\
Amazon           & 0.06                     & 0.06          & 0.05         & 0.05          & 0.06               & 0.06          & 0.06 & 0.05          & 0.06           & 0.01          & 0.06          & 0.18          & \textbf{0.23} \\
Imdb             & 0.04                     & 0.04          & 0.05         & 0.05          & 0.03               & 0.07          & 0.00 & 0.02          & 0.05           & 0.09          & 0.00          & 0.09          & \textbf{0.36} \\
Yelp             & 0.10                     & 0.09          & 0.03         & 0.06          & 0.07               & 0.06          & 0.04 & 0.04          & 0.07           & 0.13          & 0.11          & 0.08          & \textbf{0.36} \\
20newsgroups     & 0.18                     & 0.07          & 0.05         & 0.06          & 0.10               & 0.05          & 0.04 & 0.00          & 0.08           & 0.08          & 0.05          & 0.14          & \textbf{0.41} \\
BATADAL  04      & 0.20                     & 0.10          & 0.09         & 0.19          & \textbf{0.26}      & 0.05          & 0.10 & 0.10          & 0.10           & 0.13          & 0.07          & 0.17          & 0.17          \\
SWaT  1          & 0.08                     & \textbf{0.63} & 0.13         & 0.50          & 0.19               & 0.22          & 0.09 & \textbf{0.63} & 0.19           & 0.51          & 0.17          & 0.21          & 0.40          \\
SWaT  2          & 0.06                     & \textbf{0.71} & 0.10         & 0.11          & 0.13               & 0.12          & 0.05 & \textbf{0.71} & 0.06           & 0.07          & 0.25          & 0.08          & 0.33          \\
SWaT  3          & 0.04                     & 0.02          & 0.13         & 0.23          & 0.09               & 0.15          & 0.04 & 0.02          & NA             & NA            & NA            & NA            & \textbf{0.66} \\
SWaT  4          & 0.45                     & 0.34          & 0.08         & 0.99          & 0.27               & 0.88          & 0.44 & 0.34          & 0.19           & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.85          \\
SWaT  5          & 0.02                     & 0.22          & 0.09         & 0.10          & 0.12               & 0.12          & 0.03 & 0.22          & 0.06           & 0.15          & 0.17          & 0.03          & \textbf{0.36} \\
SWaT  6          & 0.07                     & \textbf{0.97} & 0.37         & 0.26          & 0.26               & 0.17          & 0.06 & \textbf{0.97} & 0.13           & 0.32          & 0.92          & 0.33          & 0.21          \\
ecoli            & 0.21                     & 0.19          & 0.20         & 0.22          & 0.21               & 0.06          & 0.00 & 0.14          & 0.04           & 0.12          & 0.21          & 0.21          & \textbf{0.44} \\
cmc              & 0.01                     & 0.00          & 0.03         & 0.01          & 0.01               & 0.00          & 0.05 & 0.00          & 0.01           & 0.03          & 0.02          & 0.03          & \textbf{0.15} \\
lympho  h        & 0.33                     & 0.32          & 0.12         & 0.00          & 0.21               & 0.06          & 0.05 & 0.00          & 0.10           & 0.43          & 0.33          & 0.40          & \textbf{0.64} \\
wbc  h           & 0.37                     & 0.34          & 0.27         & 0.32          & 0.35               & 0.40          & 0.04 & 0.61          & 0.15           & 0.06          & 0.38          & 0.37          & \textbf{0.85} \\
\hline
\end{tabular}
}
\caption{Precision value comparison on Multivariate datasets}
\label{table:7}
\end{table*}


\begin{table*}[h!]
\centering
\scriptsize 
\setlength{\tabcolsep}{1.5mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|l|l|l|l|l|}
\hline
Dataset          & LOF                               & Iforest                           & AutoEncoders  & DAGMM         & Elliptic  Envelope & DevNet        & GAN           & MGBTAI        & dBTAI & Deep-Sad      & FTT           & PReNet        & RN-Net        \\
\hline
ALOI             & 0.34                              & 0.15                              & 0.12          & 0.11          & 0.09               & 0.08          & 0.14          & 0.13          & 0.41           & 0.16          & 0.18          & 0.11          & \textbf{0.57} \\
annthyroid       & 0.35                              & 0.41                              & 0.20          & 0.32          & 0.52               & 0.17          & 0.12          & 0.18          & 0.49           & 0.62          & 0.09          & 0.08          & \textbf{0.83} \\
backdoor         & 0.45                              & 0.16                              & 0.19          & 0.87          & 0.84               & 0.91          & 0.86          & 0.02          & NA             & NA            & NA            & NA            & \textbf{0.98} \\
breastw          & \textbf{1.00} & 0.41                              & 0.13          & 0.29 & 0.32      & \textbf{1.00} & 0.00          & 0.33          & 0.85           & 0.95          & 0.98          & 0.28          & 0.95          \\
campaign         & 0.06                              & 0.35                              & 0.19          & 0.27          & 0.31               & 0.52          & 0.19          & 0.16          & \textbf{0.64}  & 0.57          & 0.19          & 0.15          & 0.51 \\
cardio           & 0.18                              & 0.61                              & 0.36          & 0.14          & 0.42               & 0.79          & 0.07          & 0.32          & 0.76           & 0.78          & 0.41          & 0.36          & \textbf{0.89} \\
Cardiotocography & 0.15                              & 0.26                              & 0.13          & 0.16          & 0.21               & 0.23          & 0.32          & 0.20          & 0.47           & 0.05          & 0.29          & 0.33          & \textbf{0.70} \\
celeba           & 0.05                              & 0.38                              & 0.10          & 0.29          & 0.38               & 0.88          & 0.15          & 0.70          & 0.37           & 0.74          & 0.52          & 0.58          & \textbf{0.90} \\
cover            & 0.22                              & 0.57                              & 0.37          & 0.24          & 0.13               & 0.97          & \textbf{1.00} & 0.00          & 0.69           & 0.25          & 0.91          & 0.04          & \textbf{1.00} \\
donors           & 0.33                              & 0.20          & 0.24          & 0.86          & 0.33               & \textbf{1.00} & 0.10          & 0.00          & NA             & 0.99 & NA            & \textbf{1.00} & \textbf{1.00} \\
fault            & 0.09                              & 0.18                              & 0.24          & 0.10          & 0.07               & 0.11          & 0.14          & 0.07          & \textbf{0.63}  & 0.62          & 0.59          & 0.20          & 0.44 \\
fraud            & 0.11                              & 0.89                              & 0.88          & 0.62          & 0.85               & \textbf{0.92} & NA            & 0.62          & 0.85           & 0.84          & 0.80 & 0.83          & 0.83          \\
glass            & 0.44                              & 0.33                              & 0.22          & 0.00          & 0.33               & 0.00          & 0.13          & 0.11          & 0.78           & 0.67          & 0.67          & 0.11          & \textbf{1.00} \\
Hepatitis        & 0.15                              & 0.15                              & 0.08          & 0.15          & 0.31               & 0.15          & 0.12          & 0.00          & 0.31           & \textbf{1.00} & 0.69          & 0.38          & 0.83 \\
http             & 0.03                              & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00}      & \textbf{1.00} & NA            & \textbf{1.00} & 0.02           & \textbf{1.00} & NA            & \textbf{1.00} & 0.95 \\
InternetAds      & 0.26                              & 0.42                              & 0.29          & 0.11          & 0.36               & 0.17          & 0.04          & 0.23 & NA             & 0.85          & NA            & 0.35          & \textbf{0.86} \\
Ionosphere       & 0.26                              & 0.35                              & 0.14          & 0.20          & 0.25      & 0.25          & 0.16          & 0.03          & 0.78           & \textbf{0.91} & 0.84          & 0.24          & 0.89          \\
landsat          & 0.15                              & 0.09                              & 0.09          & 0.09          & 0.02               & 0.12          & 0.19          & 0.00          & 0.44           & 0.78          & 0.46          & 0.38 & \textbf{0.80} \\
letter           & 0.57                     & 0.16                              & 0.08          & 0.09          & 0.28               & 0.12          & 0.10          & 0.18          & NA             & NA            & NA            & NA            & \textbf{0.80} \\
Lymphography     & 0.83                              & 1.00 & 0.33          & 0.17          & 0.83               & 0.33          & 0.33          & \textbf{1.00} & \textbf{1.00}  & 0.83          & 0.83          & 0.83          & 0.67 \\
magic.gamma      & 0.18                              & 0.26                              & 0.16          & 0.16          & 0.26      & 0.57          & 0.22          & 0.21          & 0.49           & \textbf{0.79} & 0.73          & 0.18          & 0.45          \\
mammography      & 0.37                              & 0.64                              & 0.42          & 0.64          & 0.13               & \textbf{0.80} & 0.04          & 0.00          & NA             & 0.55          & 0.25          & 0.01          & 0.71 \\
mnist            & 0.25                              & 0.42                              & 0.21          & 0.18          & 0.18               & 0.71          & \textbf{1.00} & 0.02          & 0.85           & 0.85          & 0.88          & 0.38          & 0.94 \\
musk             & 0.04                              & 1.00 & 0.53          & \textbf{1.00} & \textbf{1.00}      & \textbf{1.00} & \textbf{1.00} & 0.79          & \textbf{1.00}  & 0.97          & \textbf{0.99} & \textbf{1.00} & 0.90          \\
optdigits        & 0.26                              & 0.21                              & 0.15          & 0.16          & 0.01               & 0.99          & 0.12          & 0.81          & 0.34           & 0.97          & 0.98 & 0.99          & \textbf{1.00} \\
PageBlocks       & 0.41                              & 0.52                              & 0.30          & 0.29          & 0.61               & 0.20          & 0.33          & 0.06          & 0.21           & 0.89          & 0.80          & 0.18          & \textbf{1.00} \\
pendigits        & 0.17                              & 0.88                              & 0.56          & 0.01          & 0.28               & 0.98          & 0.25          & 0.03          & 0.71           & \textbf{0.99} & 0.97          & 0.97          & 0.64 \\
Pima             & 0.09                              & 0.23                              & 0.10          & 0.13          & 0.16               & 0.36 & 0.04          & 0.05          & 0.44           & 0.73          & 0.63          & 0.16          & \textbf{0.98} \\
satellite        & 0.15                              & 0.34                              & 0.12          & 0.24          & 0.29               & 0.31          & \textbf{1.00} & 0.30 & 0.59           & 0.56          & 0.64          & 0.25          & 0.48          \\
satimage-2       & 0.28                              & 0.99                              & 0.01          & 0.65          & \textbf{1.00}      & 0.97          & \textbf{1.00} & \textbf{1.00} & \textbf{1.00}  & 0.93          & 0.92          & 0.93          & 0.90 \\
shuttle          & 0.00          & 0.41                              & 0.65          & 0.72          & \textbf{0.31}      & 0.98          & \textbf{1.00} & 0.01          & 0.89           & 0.48          & 0.97          & 0.94          & \textbf{1.00} \\
skin             & 0.12                              & 0.03                              & 0.04          & 0.28          & 0.14               & \textbf{1.00} & 0.05          & 0.15          & 0.33           & 0.99          & 0.45          & 0.43          & \textbf{1.00} \\
smtp             & 0.70                              & 0.77                              & 0.70          & 0.87          & 0.77               & 0.70          & \textbf{1.00} & 0.33          & 0.87           & \textbf{0.57} & 0.63          & 0.67          & 0.62          \\
SpamBase         & 0.08                              & 0.12                              & 0.10          & 0.16          & 0.07               & 0.45          & 0.02          & 0.18          & 0.27           & 0.06          & 0.21          & 0.20          & \textbf{0.72} \\
speech           & 0.15                              & 0.15                              & 0.18          & 0.03          & 0.11               & 0.75          & 0.12          & 0.11          & 0.51           & \textbf{0.97} & 0.57          & 0.25          & 0.78 \\
Stamps           & 0.16                              & 0.32                              & 0.15          & 0.03          & 0.13               & 0.16          & 0.49          & 0.06          & \textbf{0.84}  & 0.77          & 0.81 & 0.77          & 0.71          \\
thyroid          & 0.39                              & 0.97                              & 0.62          & 0.71          & 0.96               & 0.72          & 0.71          & 0.25          & 0.92           & 0.91          & 0.99          & \textbf{1.00} & 0.96 \\
vertebral        & 0.03                              & 0.03                              & 0.00          & 0.07          & 0.00               & 0.07          & 0.03          & 0.00          & 0.24           & 0.93          & 0.53          & 0.47          & \textbf{1.00} \\
vowels           & 0.76                              & 0.30          & 0.33          & 0.02          & 0.14               & 0.62          & 0.08          & 0.16          & \textbf{1.00}  & 0.78          & 0.74          & 0.90          & 0.69 \\
Waveform         & 0.31                              & 0.25                              & 0.37          & 0.32          & 0.16               & 0.24          & 0.72          & 0.00          & 0.71           & 0.51          & 0.13          & 0.27          & \textbf{1.00} \\
WBC              & 0.20                              & \textbf{1.00} & 0.90          & 0.80          & \textbf{1.00}      & \textbf{1.00} & 0.13          & \textbf{1.00} & \textbf{1.00}  & 0.90          & 0.90 & \textbf{1.00} & 0.58          \\
WDBC             & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.90          & 0.90               & \textbf{1.00} & 0.33          & \textbf{1.00} & \textbf{1.00}  & \textbf{1.00} & 0.90 & \textbf{1.00} & \textbf{1.00} \\
Wilt             & 0.19                              & 0.02                              & 0.10          & 0.18          & 0.20               & 0.00          & \textbf{1.00} & 0.00          & 0.17           & 0.51          & 0.86          & 0.26          & \textbf{1.00} \\
wine             & 1.00 & 0.30          & 0.70          & 0.40          & 0.40               & \textbf{1.00} & 0.67          & \textbf{1.00} & \textbf{1.00}  & 0.90 & 0.90 & 0.90          & 0.78          \\
WPBC             & 0.04                              & 0.09                              & 0.11          & 0.06          & 0.06               & 0.06          & 0.09          & 0.04          & 0.45           & 0.72          & 0.26          & 0.23          & \textbf{1.00} \\
yeast            & 0.08                              & 0.10          & 0.10          & 0.11          & 0.08               & 0.10          & 0.14          & 0.00          & 0.30           & \textbf{0.82} & 0.50          & 0.10          & 0.80 \\
CIFAR10          & 0.27                              & 0.31                              & 0.14          & 0.10          & 0.26               & 0.37          & 0.37          & 0.07          & 0.70           & 0.61          & 0.36          & 0.13          & \textbf{0.74} \\
FashionMNIST     & 0.30                              & 0.47                              & 0.24          & 0.22          & 0.36               & 0.71          & 0.42          & 0.12          & \textbf{0.93}  & 0.80          & 0.59          & 0.24          & 0.63 \\
MNIST-C          & 0.27                              & 0.18                              & 0.08          & 0.15          & 0.15               & 0.97          & 0.18          & 0.12          & 0.88           & 0.88          & 0.83          & 0.54          & \textbf{0.99} \\
MVTec-AD         & 0.41                              & 0.51                     & 0.24          & 0.19          & 0.21               & 0.13          & 0.46          & 0.22 & 0.90           & 0.62          & 0.48 & 0.48 & \textbf{0.91} \\
SVHN             & 0.22                              & 0.13                              & 0.08          & 0.10          & 0.20               & \textbf{0.96} & 0.20          & 0.06          & 0.62           & 0.50          & 0.46          & 0.18          & 0.59 \\
Agnews           & 0.22                              & 0.15                              & 0.09          & 0.12          & 0.14               & 0.14          & 0.21          & 0.02          & 0.47           & 0.59          & \textbf{1.00} & 0.33          & 0.73 \\
Amazon           & 0.12                              & 0.14                              & 0.10          & 0.11          & 0.12               & 0.12          & 0.11          & 0.12          & 0.46           & 0.21          & 0.35          & 0.36          & \textbf{0.54} \\
Imdb             & 0.08                              & 0.08                              & 0.10          & 0.10          & 0.06               & 0.13          & 0.00          & 0.02          & 0.33           & 0.19          & 0.00          & 0.19          & \textbf{0.64} \\
Yelp             & 0.20                              & 0.21                              & 0.07          & 0.11          & 0.14               & 0.11          & 0.08          & 0.04          & \textbf{0.57}  & 0.25          & 0.09          & 0.17          & 0.51 \\
20newsgroups     & 0.37                              & 0.17                              & 0.11          & 0.13          & 0.19               & 0.10          & 0.08          & 0.00          & 0.60           & 0.71          & \textbf{0.95} & 0.27          & 0.54 \\
BATADAL  04      & 0.38                              & 0.10          & 0.17          & 0.36          & 0.50      & 0.10          & 0.23          & 0.10          & 0.75           & \textbf{0.76} & 0.42          & 0.32          & 0.37          \\
SWaT  1          & 0.09                              & 0.15                     & 0.15          & 0.57          & 0.21               & 0.23          & \textbf{1.00} & 0.15 & 0.79           & 0.59          & 0.91          & 0.24          & 0.40          \\
SWaT  2          & 0.13                              & 0.08                     & 0.20          & 0.23          & 0.27               & 0.26          & \textbf{1.00} & 0.08 & 0.43           & 0.75          & 0.92          & 0.17          & 0.31          \\
SWaT  3          & 0.12                              & 0.03                              & 0.38          & 0.65          & 0.24               & 0.42          & \textbf{1.00} & 0.03          & NA             & NA            & NA            & NA            & 0.79 \\
SWaT  4          & 0.10                              & 0.12                              & 0.02          & 0.23          & 0.06               & 0.89          & \textbf{1.00} & 0.12          & 0.15           & 0.90 & 0.98 & 0.23 & 0.04          \\
SWaT  5          & 0.08                              & 0.22                              & 0.36          & 0.38          & 0.51               & 0.50          & \textbf{1.00} & 0.22          & 0.85           & 0.40          & 0.94          & 0.11          & 0.55 \\
SWaT  6          & 0.12                              & 0.40 & 0.63          & 0.44          & 0.44               & 0.30          & \textbf{1.00} & 0.40 & 0.80           & 0.79          & 1.00          & 0.56          & 0.72          \\
ecoli            & 0.78                              & 0.78                              & 0.78          & 0.78          & 0.78               & 0.22          & 0.00          & 0.78          & 0.56           & 0.67          & 0.78          & 0.78 & \textbf{1.00} \\
cmc              & 0.06                              & 0.00          & 0.24          & 0.06          & 0.12               & 0.00          & 0.47          & 0.00          & 0.18           & 0.94          & 0.65          & 0.29          & \textbf{1.00} \\
lympho  h        & 0.83                              & 1.00 & 0.33          & 0.00          & 0.67               & 0.17          & 0.11          & 0.00          & \textbf{1.00}  & \textbf{1.00} & 0.67          & \textbf{1.00} & 0.80 \\
wbc  h           & 0.67                              & 0.71                              & 0.48          & 0.57          & 0.67               & \textbf{1.00} & 0.08          & 0.52          & 0.95           & \textbf{1.00} & 0.86          & 0.67          & \textbf{1.00} \\
\hline
\end{tabular}
}
\caption{Recall value comparison on Multivariate datasets}
\label{table:8}
\end{table*}


\begin{table*}[h!]
\centering
\scriptsize 
\setlength{\tabcolsep}{1.5mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{|c|c|c|c|c|c|l|c|c|l|l|l|l|l|}
\hline
Dataset          & LOF           & Iforest       & AutoEncoders  & DAGMM         & Elliptic  Envelope & DevNet                            & GAN           & MGBTAI        & dBTAI & Deep-Sad      & FTT           & PReNet        & RN-Net        \\
\hline
ALOI             & 0.16          & 0.06          & 0.06          & 0.05          & 0.04               & 0.04                              & 0.06          & 0.07          & 0.07           & 0.07          & 0.06          & 0.05          & \textbf{0.39} \\
annthyroid       & 0.30          & 0.31          & 0.17          & 0.28          & 0.45               & 0.14                              & 0.10          & 0.10          & 0.17           & 0.39          & 0.53          & 0.07          & \textbf{0.78} \\
backdoor         & 0.18          & 0.05          & 0.26          & 0.34          & 0.33               & 0.35                              & 0.34          & 0.02          & NA             & NA            & NA            & NA            & \textbf{0.78} \\
breastw          & \textbf{0.56} & 0.57          & 0.26          & \textbf{0.45} & \textbf{0.49}      & \textbf{0.92}                     & 0.00          & 0.48          & 0.87           & 0.92          & \textbf{0.96} & 0.44          & 0.96          \\
campaign         & 0.06          & 0.34          & 0.20          & 0.28          & 0.32               & 0.47                              & 0.20          & 0.16          & \textbf{0.27}  & 0.33          & 0.20          & 0.16          & \textbf{0.61} \\
cardio           & 0.18          & 0.54          & 0.35          & 0.13          & 0.41               & 0.61                              & 0.07          & 0.38          & 0.32           & 0.53          & 0.40          & 0.35          & \textbf{0.90} \\
Cardiotocography & 0.20          & 0.34          & 0.19          & 0.22          & 0.29               & 0.29                              & 0.22          & 0.32          & 0.40           & 0.09          & 0.41          & 0.45          & \textbf{0.78} \\
celeba           & 0.02          & 0.12          & 0.04          & 0.11          & 0.14               & 0.32                              & 0.05          & 0.14          & 0.06           & 0.15          & 0.19          & 0.21          & \textbf{0.41} \\
cover            & 0.04          & 0.09          & 0.17          & 0.04          & 0.02               & 0.17                              & \textbf{0.02} & 0.00          & 0.04           & 0.08          & 0.06          & 0.01          & \textbf{1.00} \\
donors           & 0.26          & 0.13          & 0.19          & 0.63          & 0.25               & \textbf{0.61}                     & 0.06          & 0.00          & NA             & \textbf{0.99} & NA            & \textbf{0.74} & \textbf{0.95} \\
fault            & 0.15          & 0.26          & 0.38          & 0.16          & 0.11               & 0.16                              & 0.21          & 0.12          & \textbf{0.56}  & \textbf{0.59} & 0.52          & 0.31          & \textbf{0.59} \\
fraud            & 0.00          & 0.03          & 0.03          & 0.02          & 0.04               & \textbf{0.03}                     & NA            & 0.02          & 0.01           & 0.09          & \textbf{0.49} & 0.03          & 0.31          \\
glass            & 0.26          & 0.16          & 0.13          & 0.00          & 0.18               & 0.00          & 0.08          & 0.09          & 0.20           & 0.15          & 0.23          & 0.06          & \textbf{0.89} \\
Hepatitis        & 0.19          & 0.17          & 0.10          & 0.19          & 0.32               & 0.19                              & 0.15          & 0.00          & 0.22           & \textbf{0.72} & 0.51          & 0.48          & \textbf{0.77} \\
http             & 0.00          & \textbf{0.06} & \textbf{0.07} & \textbf{0.07} & \textbf{0.07}      & \textbf{0.08}                     & NA            & \textbf{0.08} & 0.00           & \textbf{0.01} & NA            & \textbf{0.07} & \textbf{0.94} \\
InternetAds      & 0.33          & 0.42          & 0.37          & 0.14          & 0.46               & 0.22                              & 0.16          & \textbf{0.38} & NA             & 0.65          & NA            & 0.45          & \textbf{0.85} \\
Ionosphere       & 0.41          & 0.51          & 0.22          & 0.31          & \textbf{0.39}      & 0.35                              & 0.25          & 0.06          & 0.75           & \textbf{0.84} & 0.72          & 0.37          & \textbf{0.92} \\
landsat          & 0.20          & 0.11          & 0.13          & 0.12          & 0.02               & 0.16                              & 0.25          & 0.00          & 0.32           & 0.65          & 0.55          & \textbf{0.51} & \textbf{0.78} \\
letter           & \textbf{0.57} & 0.11          & 0.06          & 0.07          & 0.22               & 0.09                              & 0.07          & 0.13          & NA             & NA            & NA            & NA            & \textbf{0.32} \\
Lymphography     & 0.48          & \textbf{0.57} & 0.18          & 0.10          & 0.53               & 0.17                              & 0.19          & \textbf{0.60} & \textbf{0.26}  & 0.45          & 0.67          & 0.48          & \textbf{0.67} \\
magic.gamma      & 0.27          & 0.39          & 0.24          & 0.25          & \textbf{0.40}      & 0.66                              & 0.34          & 0.33          & 0.54           & \textbf{0.75} & 0.70          & 0.27          & 0.60          \\
mammography      & 0.14          & 0.21          & 0.09          & 0.24          & 0.05               & \textbf{0.30} & 0.02          & 0.00          & NA             & 0.12          & 0.09          & 0.00          & \textbf{0.54} \\
mnist            & 0.24          & 0.37          & 0.20          & 0.17          & 0.17               & 0.56                              & \textbf{0.17} & 0.02          & 0.28           & 0.42          & 0.67          & 0.36          & \textbf{0.91} \\
musk             & 0.02          & \textbf{0.39} & 0.25          & \textbf{0.48} & \textbf{0.47}      & \textbf{0.45}                     & \textbf{0.06} & 0.35          & \textbf{0.15}  & 0.42          & \textbf{0.99} & \textbf{0.06} & 0.91          \\
optdigits        & 0.12          & 0.08          & 0.07          & 0.07          & 0.00               & 0.42                              & 0.05          & 0.47          & 0.04           & 0.68          & \textbf{0.99} & 0.44          & \textbf{1.00} \\
PageBlocks       & 0.40          & 0.46          & 0.30          & 0.28          & 0.58               & 0.18                              & 0.32          & 0.11          & 0.19           & 0.20          & 0.34          & 0.17          & \textbf{1.00} \\
pendigits        & 0.06          & 0.28          & 0.21          & 0.00          & 0.11               & 0.37                              & 0.09          & 0.00          & 0.07           & \textbf{0.49} & \textbf{0.82} & 0.36          & \textbf{0.73} \\
Pima             & 0.14          & 0.33          & 0.16          & 0.20          & 0.24               & \textbf{0.47}                     & 0.07          & 0.09          & 0.43           & \textbf{0.60} & 0.54          & 0.26          & \textbf{0.57} \\
satellite        & 0.23          & 0.50          & 0.19          & 0.37          & 0.45               & 0.41                              & \textbf{0.48} & \textbf{0.46} & 0.55           & 0.59          & \textbf{0.75} & 0.39          & 0.61          \\
satimage-2       & 0.28          & 0.18          & 0.00          & 0.14          & \textbf{0.21}      & 0.22                              & \textbf{0.02} & \textbf{0.25} & \textbf{0.07}  & 0.84          & 0.81          & 0.20          & \textbf{0.92} \\
shuttle          & 0.00          & 0.57          & 0.54          & 0.60          & \textbf{0.48}      & 0.64                              & \textbf{0.13} & 0.01          & 0.59           & 0.59          & \textbf{0.94} & 0.79          & \textbf{0.78} \\
skin             & 0.16          & 0.04          & 0.05          & 0.37          & 0.19               & \textbf{0.86}                     & 0.07          & 0.24          & 0.33           & 0.99          & 0.61          & 0.58          & \textbf{1.00} \\
smtp             & 0.00          & 0.00          & 0.00          & 0.01          & 0.01               & 0.00                              & \textbf{0.00} & 0.00          & 0.00           & \textbf{0.72} & 0.58          & 0.00          & \textbf{0.76} \\
SpamBase         & 0.13          & 0.18          & 0.16          & 0.25          & 0.12               & 0.57                              & 0.03          & 0.29          & 0.38           & 0.11          & 0.34          & 0.32          & \textbf{0.84} \\
speech           & 0.04          & 0.04          & 0.05          & 0.01          & 0.03               & 0.23                              & 0.03          & 0.03          & 0.03           & \textbf{0.21} & 0.04          & 0.07          & \textbf{0.85} \\
Stamps           & 0.15          & 0.27          & 0.14          & 0.03          & 0.12               & 0.15                              & 0.47          & 0.09          & \textbf{0.32}  & 0.52          & \textbf{0.81} & 0.74          & 0.10          \\
thyroid          & 0.15          & 0.32          & 0.24          & 0.28          & 0.37               & 0.29                              & 0.28          & 0.17          & 0.12           & 0.43          & 0.53          & \textbf{0.39} & \textbf{0.81} \\
vertebral        & 0.04          & 0.03          & 0.00          & 0.07          & 0.00               & 0.08                              & 0.04          & 0.00          & 0.13           & 0.24          & 0.30          & 0.52          & \textbf{0.86} \\
vowels           & 0.39          & 0.14          & 0.17          & 0.01          & 0.07               & 0.30          & 0.04          & 0.26          & \textbf{0.14}  & 0.43          & 0.37          & 0.46          & \textbf{0.75} \\
Waveform         & 0.14          & 0.09          & 0.17          & 0.14          & 0.07               & 0.11                              & 0.32          & 0.00          & 0.08           & 0.22          & 0.09          & 0.12          & \textbf{0.80} \\
WBC              & 0.12          & \textbf{0.49} & 0.53          & 0.48          & \textbf{0.56}      & \textbf{0.48}                     & 0.08          & \textbf{0.57} & \textbf{0.31}  & 0.45          & \textbf{0.64} & \textbf{0.61} & 0.45          \\
WDBC             & \textbf{0.43} & \textbf{0.35} & \textbf{0.42} & 0.38          & 0.40               & \textbf{0.41}                     & 0.02          & \textbf{0.34} & \textbf{0.23}  & \textbf{0.91} & \textbf{0.90} & \textbf{0.43} & \textbf{0.72} \\
Wilt             & 0.14          & 0.01          & 0.07          & 0.12          & 0.13               & \multicolumn{1}{c}{0.00}          & \textbf{0.10} & 0.00          & 0.07           & 0.14          & 0.17          & 0.18          & \textbf{1.00} \\
wine             & \textbf{0.87} & 0.22          & 0.58          & 0.35          & 0.38               & \textbf{0.55}                     & 0.58          & \textbf{0.69} & \textbf{0.40}  & \textbf{0.90} & \textbf{0.90} & 0.78          & 0.59          \\
WPBC             & 0.06          & 0.11          & 0.15          & 0.09          & 0.09               & 0.09                              & 0.12          & 0.06          & 0.33           & 0.52          & 0.35          & 0.33          & \textbf{1.00} \\
yeast            & 0.12          & 0.16          & 0.16          & 0.16          & 0.12               & 0.16                              & 0.22          & 0.01          & 0.30           & \textbf{0.50} & 0.48          & 0.16          & \textbf{0.81} \\
CIFAR10          & 0.18          & 0.18          & 0.09          & 0.07          & 0.18               & 0.23                              & 0.25          & 0.05          & 0.14           & 0.18          & 0.13          & 0.09          & \textbf{0.77} \\
FashionMNIST     & 0.20          & 0.27          & 0.16          & 0.15          & 0.24               & 0.41                              & 0.28          & 0.08          & \textbf{0.19}  & 0.34          & 0.24          & 0.16          & \textbf{0.43} \\
MNIST-C          & 0.18          & 0.11          & 0.06          & 0.10          & 0.10               & 0.55                              & 0.12          & 0.07          & 0.18           & 0.63          & 0.45          & 0.36          & \textbf{0.98} \\
MVTec-AD         & 0.56          & \textbf{0.67} & 0.32          & 0.26          & 0.15               & 0.17                              & 0.62          & \textbf{0.36} & 0.83           & 0.74          & \textbf{0.65} & \textbf{0.65} & \textbf{0.93} \\
SVHN             & 0.15          & 0.08          & 0.06          & 0.06          & 0.13               & \textbf{0.55}                     & 0.12          & 0.06          & 0.13           & 0.22          & 0.12          & 0.12          & \textbf{0.53} \\
Agnews           & 0.15          & 0.09          & 0.06          & 0.08          & 0.09               & 0.09                              & 0.14          & 0.03          & 0.11           & 0.16          & \textbf{0.10} & 0.22          & \textbf{0.46} \\
Amazon           & 0.08          & 0.08          & 0.07          & 0.07          & 0.08               & 0.08                              & 0.07          & 0.07          & 0.11           & 0.14          & 0.10          & 0.24          & \textbf{0.32} \\
Imdb             & 0.06          & 0.05          & 0.07          & 0.07          & 0.04               & 0.09                              & 0.00          & 0.02          & 0.08           & 0.12          & 0.00          & 0.12          & \textbf{0.46} \\
Yelp             & 0.13          & 0.12          & 0.06          & 0.07          & 0.10               & 0.07                              & 0.06          & 0.04          & \textbf{0.13}  & 0.17          & 0.10          & 0.11          & \textbf{0.42} \\
20newsgroups     & 0.25          & 0.10          & 0.09          & 0.09          & 0.13               & 0.06                              & 0.06          & 0.00          & 0.14           & 0.15          & \textbf{0.09} & 0.18          & \textbf{0.46} \\
BATADAL  04      & 0.26          & 0.10          & 0.12          & 0.25          & \textbf{0.34}      & 0.07                              & 0.14          & 0.10          & 0.18           & \textbf{0.22} & 0.12          & 0.22          & 0.23          \\
SWaT  1          & 0.09          & \textbf{0.24} & 0.14          & 0.53          & 0.20               & 0.22                              & \textbf{0.16} & \textbf{0.24} & 0.30           & \textbf{0.55} & 0.29          & 0.22          & 0.40          \\
SWaT  2          & 0.08          & \textbf{0.15} & 0.13          & 0.15          & 0.18               & 0.16                              & \textbf{0.04} & \textbf{0.15} & 0.11           & 0.13          & \textbf{0.40} & 0.11          & 0.30          \\
SWaT  3          & 0.06          & 0.02          & 0.20          & 0.34          & 0.13               & 0.23                              & \textbf{0.04} & 0.02          & NA             & NA            & NA            & NA            & \textbf{0.69} \\
SWaT  4          & 0.17          & 0.18          & 0.03          & 0.37          & 0.10               & 0.89                              & \textbf{0.44} & 0.18          & 0.17           & \textbf{0.94} & \textbf{0.99} & \textbf{0.37} & 0.08          \\
SWaT  5          & 0.03          & 0.22          & 0.15          & 0.15          & 0.20               & 0.17                              & \textbf{0.02} & 0.22          & 0.12           & 0.22          & 0.29          & 0.04          & \textbf{0.43} \\
SWaT  6          & 0.09          & \textbf{0.57} & 0.46          & 0.33          & 0.32               & 0.20                              & \textbf{0.06} & \textbf{0.57} & 0.22           & 0.46          & \textbf{0.96} & 0.41          & 0.32          \\
ecoli            & 0.33          & 0.30          & 0.31          & 0.33          & 0.21               & \multicolumn{1}{c}{0.10}          & 0.00          & 0.23          & 0.07           & 0.19          & 0.33          & \textbf{0.33} & \textbf{0.60} \\
cmc              & 0.01          & 0.00          & 0.06          & 0.01          & 0.03               & \multicolumn{1}{c}{0.00}          & 0.10          & 0.00          & 0.01           & 0.06          & 0.04          & 0.06          & \textbf{0.27} \\
lympho  h        & 0.48          & \textbf{0.48} & 0.18          & 0.00          & 0.32               & 0.09                              & 0.07          & 0.00          & \textbf{0.19}  & \textbf{0.60} & 0.44          & \textbf{0.57} & \textbf{0.64} \\
wbc  h           & 0.47          & 0.46          & 0.35          & 0.41          & 0.46               & \textbf{0.57}                     & 0.06          & 0.56          & 0.25           & \textbf{0.11} & 0.52          & 0.47          & \textbf{0.92}\\
\hline
\end{tabular}
}
\caption{F1 Scores comparison on Multivariate datasets}
\label{table:9}
\end{table*}


\begin{table*}[h!]
\centering
\scriptsize 
\setlength{\tabcolsep}{1.5mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{|c|c|l|c|c|c|c|c|c|l|l|l|l|l|}
\hline
Dataset          & LOF           & Iforest                           & AutoEncoders  & DAGMM         & Elliptic  Envelope & DevNet        & GAN           & MGBTAI        & dBTAI & Deep-Sad      & FTT           & PReNet        & RN-Net        \\
\hline
ALOI             & 0.63          & 0.51                              & 0.51          & 0.51          & 0.49               & 0.49          & 0.52          & 0.52          & 0.53           & 0.57          & 0.50          & 0.51          & \textbf{0.71} \\
annthyroid       & 0.63          & 0.66                              & 0.55          & 0.62          & 0.73               & 0.54          & 0.51          & 0.50          & 0.57           & 0.90          & 0.51          & 0.47          & \textbf{0.93} \\
backdoor         & 0.68          & 0.52                              & 0.53          & 0.89          & 0.88               & 0.91          & 0.89          & 0.49          & NA             & NA            & NA            & NA            & \textbf{0.99} \\
breastw          & \textbf{0.57} & 0.70          & 0.63          & \textbf{0.64} & \textbf{0.66}      & \textbf{0.95} & 0.41          & 0.65          & 0.90           & 0.97          & \textbf{0.99} & 0.83          & 0.98          \\
campaign         & 0.48          & 0.63                              & 0.55          & 0.59          & 0.62               & \textbf{0.72} & 0.55          & 0.53          & \textbf{0.62}  & 0.71          & 0.68          & 0.65          & \textbf{0.71} \\
cardio           & 0.55          & 0.77                              & 0.64          & 0.52          & 0.68               & 0.85          & 0.40          & 0.64          & 0.72           & 0.89          & 0.81          & 0.82          & \textbf{0.96} \\
Cardiotocography & 0.53          & 0.59                              & 0.55          & 0.54          & 0.57               & 0.67          & 0.50          & 0.59          & 0.61           & 0.38          & 0.69          & 0.77          & \textbf{0.84} \\
celeba           & 0.47          & 0.63                              & 0.50          & 0.60          & 0.64               & \textbf{0.90} & 0.52          & 0.75          & 0.55           & 0.87          & 0.76          & 0.81          & \textbf{0.89} \\
cover            & 0.56          & 0.73                              & 0.91          & 0.57          & 0.52               & 0.94          & \textbf{0.50} & 0.50          & 0.68           & 0.39          & 0.89          & 0.66          & \textbf{1.00} \\
donors           & 0.63          & 0.54                              & 0.60          & 0.90          & 0.62               & \textbf{0.96} & 0.47          & 0.50          & NA             & \textbf{1.00} & NA            & \textbf{0.99} & \textbf{0.98} \\
fault            & 0.50          & 0.54                              & 0.68          & 0.50          & 0.48               & 0.50          & 0.53          & 0.51          & \textbf{0.65}  & \textbf{0.75} & 0.63          & 0.53          & \textbf{0.68} \\
fraud            & 0.51          & 0.88                              & 0.89          & 0.77          & 0.88               & \textbf{0.91} & NA            & 0.75          & 0.72           & \textbf{0.93} & \textbf{0.92} & 0.93          & 0.92          \\
glass            & 0.68          & 0.60          & 0.56          & 0.50          & 0.61               & 0.50          & 0.52          & 0.53          & 0.75           & 0.73          & 0.82          & 0.83          & \textbf{0.99} \\
Hepatitis        & 0.53          & 0.51                              & 0.49          & 0.53          & 0.59               & 0.53          & 0.51          & 0.44          & 0.51           & \textbf{0.97} & 0.77          & 0.65          & \textbf{0.83} \\
http             & 0.47          & \textbf{0.94}                     & \textbf{0.95} & \textbf{0.95} & \textbf{0.95}      & \textbf{0.95} & NA            & \textbf{0.95} & 0.34           & \textbf{0.01} & NA            & \textbf{1.00} & \textbf{0.95} \\
InternetAds      & 0.60          & 0.68                              & 0.61          & 0.50          & 0.66               & 0.54          & 0.47          & \textbf{0.62} & NA             & 0.91          & NA            & 0.51          & \textbf{0.93} \\
Ionosphere       & 0.63          & 0.67                              & 0.53          & 0.57          & \textbf{0.62}      & 0.57          & 0.55          & 0.50          & 0.81           & \textbf{0.94} & 0.84          & 0.41          & \textbf{0.97} \\
landsat          & 0.53          & 0.48                              & 0.50          & 0.49          & 0.45               & 0.51          & 0.55          & 0.44          & 0.55           & 0.88          & 0.76          & \textbf{0.61} & \textbf{0.88} \\
letter           & \textbf{0.75} & 0.52                              & 0.49          & 0.49          & 0.60               & 0.51          & 0.50          & 0.54          & NA             & NA            & NA            & NA            & \textbf{0.84} \\
Lymphography     & 0.88          & \textbf{0.97}                     & 0.62          & 0.53          & 0.89               & 0.61          & 0.62          & \textbf{0.97} & \textbf{0.88}  & 0.92          & \textbf{1.00} & 0.83          & \textbf{0.74} \\
magic.gamma      & 0.56          & 0.61                              & 0.54          & 0.55          & \textbf{0.62}      & 0.74          & 0.59          & 0.59          & 0.66           & \textbf{0.88} & 0.85          & 0.59          & 0.63          \\
mammography      & 0.64          & 0.77                              & 0.58          & 0.78          & 0.52               & \textbf{0.86} & 0.47          & 0.50          & NA             & 0.67          & 0.77          & 0.31          & \textbf{0.89} \\
mnist            & 0.58          & 0.67                              & 0.56          & 0.54          & 0.54               & 0.82          & \textbf{0.50} & 0.46          & 0.71           & 0.89          & 0.96          & 0.43          & \textbf{0.98} \\
musk             & 0.47          & \textbf{0.95}                     & 0.72          & \textbf{0.96} & \textbf{0.96}      & \textbf{0.96} & \textbf{0.50} & 0.85          & \textbf{0.81}  & 0.98          & \textbf{1.00} & \textbf{0.50} & 0.97          \\
optdigits        & 0.58          & 0.55                              & 0.52          & 0.53          & 0.45               & 0.96          & 0.51          & 0.88          & 0.44           & 0.99          & \textbf{1.00} & 0.99          & \textbf{1.00} \\
PageBlocks       & 0.67          & 0.72                              & 0.61          & 0.60          & 0.78               & 0.55          & 0.62          & 0.53          & 0.55           & 0.49          & 0.82          & 0.40          & \textbf{1.00} \\
pendigits        & 0.53          & 0.89                              & 0.74          & 0.46          & 0.59               & 0.95          & 0.58          & 0.41          & 0.65           & \textbf{0.99} & \textbf{0.99} & 0.99          & \textbf{0.79} \\
Pima             & 0.49          & 0.57                              & 0.50          & 0.52          & 0.54               & \textbf{0.64} & 0.46          & 0.49          & 0.56           & \textbf{0.73} & 0.65          & 0.52          & \textbf{0.99} \\
satellite        & 0.54          & 0.66                              & 0.52          & 0.60          & 0.64               & 0.61          & \textbf{0.50} & \textbf{0.65} & 0.67           & 0.73          & \textbf{0.80} & 0.61          & 0.70          \\
satimage-2       & 0.59          & 0.94                              & 0.46          & 0.78          & \textbf{0.95}      & 0.94          & \textbf{0.50} & \textbf{0.92} & \textbf{0.83}  & \textbf{0.99} & \textbf{0.99} & 0.97          & \textbf{0.97} \\
shuttle          & 0.43          & 0.70          & 0.80          & 0.83          & \textbf{0.66}      & 0.95          & \textbf{0.50} & 0.50          & 0.90           & 0.76          & \textbf{0.99} & 0.97          & \textbf{1.00} \\
skin             & 0.51          & 0.45                              & 0.46          & 0.61          & 0.53               & \textbf{0.96} & 0.47          & 0.56          & 0.58           & 0.99          & 0.84          & 0.97          & \textbf{1.00} \\
smtp             & 0.80          & 0.82                              & 0.80          & \textbf{0.88} & 0.83               & 0.80          & \textbf{0.50} & 0.64          & 0.79           & \textbf{0.81} & 0.77          & 0.79          & \textbf{0.76} \\
SpamBase         & 0.48          & 0.50          & 0.50          & 0.55          & 0.48               & 0.68          & 0.43          & 0.57          & 0.59           & 0.41          & 0.74          & 0.69          & \textbf{0.84} \\
speech           & 0.52          & 0.51                              & 0.54          & 0.47          & 0.51               & 0.84          & 0.51          & 0.49          & 0.48           & \textbf{0.98} & 0.52          & 0.51          & \textbf{0.91} \\
Stamps           & 0.53          & 0.61                              & 0.53          & 0.46          & 0.51               & 0.53          & 0.72          & 0.51          & \textbf{0.75}  & 0.90          & \textbf{0.93} & \textbf{0.94} & 0.57          \\
thyroid          & 0.65          & 0.93                              & 0.76          & 0.81          & 0.94               & 0.82          & 0.81          & 0.60          & 0.80           & 0.98          & \textbf{1.00} & \textbf{1.00} & \textbf{0.94} \\
vertebral        & 0.46          & 0.44                              & 0.44          & 0.48          & 0.44               & 0.49          & 0.31          & 0.49          & 0.44           & 0.42          & 0.65          & 0.75          & \textbf{1.00} \\
vowels           & 0.84          & 0.60          & 0.62          & 0.46          & 0.52               & 0.76          & 0.49          & 0.58          & \textbf{0.79}  & 0.94          & 0.91          & \textbf{0.96} & \textbf{0.89} \\
Waveform         & 0.61          & 0.56                              & 0.64          & 0.61          & 0.53               & 0.57          & 0.82          & 0.50          & 0.61           & 0.77          & 0.51          & 0.65          & \textbf{0.99} \\
WBC              & 0.55          & \textbf{0.95}                     & 0.91          & 0.86          & \textbf{0.96}      & \textbf{0.95} & 0.52          & \textbf{0.97} & \textbf{0.90}  & 0.98          & \textbf{0.99} & \textbf{0.98} & 0.82          \\
WDBC             & \textbf{0.96} & \textbf{0.95}                     & \textbf{0.96} & 0.91          & 0.91               & \textbf{0.96} & 0.46          & \textbf{0.95} & \textbf{0.91}  & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{0.97} \\
Wilt             & 0.55          & 0.44                              & 0.50          & 0.54          & 0.55               & 0.45          & \textbf{0.50} & 0.48          & 0.48           & 0.58          & 0.73          & 0.65          & \textbf{1.00} \\
wine             & \textbf{0.99} & 0.59                              & 0.82          & 0.66          & 0.67               & \textbf{0.95} & 0.81          & \textbf{0.96} & \textbf{0.87}  & \textbf{0.98} & \textbf{1.00} & 1.00          & 0.88          \\
WPBC             & 0.46          & 0.46                              & 0.50          & 0.48          & 0.48               & 0.49          & 0.49          & 0.47          & 0.52           & 0.75          & 0.58          & 0.66          & \textbf{1.00} \\
yeast            & 0.48          & 0.49                              & 0.50          & 0.50          & 0.48               & 0.50          & 0.53          & 0.49          & 0.47           & \textbf{0.53} & 0.61          & 0.49          & \textbf{0.87} \\
CIFAR10          & 0.59          & 0.60          & 0.52          & 0.50          & 0.59               & 0.68          & 0.64          & 0.49          & 0.64           & 0.72          & 0.58          & 0.52          & \textbf{0.77} \\
FashionMNIST     & 0.60          & 0.68                              & 0.58          & 0.56          & 0.64               & 0.81          & 0.67          & 0.51          & \textbf{0.76}  & \textbf{0.90} & 0.73          & 0.60          & \textbf{0.72} \\
MNIST-C          & 0.59          & 0.53                              & 0.49          & 0.53          & 0.53               & 0.94          & 0.54          & 0.50          & 0.73           & 0.97          & 0.93          & 0.76          & \textbf{1.00} \\
MVTec-AD         & 0.70          & \textbf{0.75}                     & 0.59          & 0.56          & 0.39               & 0.51          & 0.73          & \textbf{0.61} & 0.92           & 0.86          & \textbf{0.90} & \textbf{0.95} & \textbf{0.97} \\
SVHN             & 0.56          & 0.51                              & 0.49          & 0.50          & 0.55               & \textbf{0.94} & 0.58          & 0.51          & 0.59           & 0.72          & 0.59          & 0.58          & \textbf{0.81} \\
Agnews           & 0.56          & 0.51                              & 0.50          & 0.51          & 0.52               & 0.51          & 0.56          & 0.50          & 0.55           & 0.68          & \textbf{0.42} & 0.73          & \textbf{0.81} \\
Amazon           & 0.51          & 0.51                              & 0.50          & 0.50          & 0.51               & 0.51          & 0.51          & 0.50          & 0.55           & 0.63          & 0.51          & \textbf{0.75} & \textbf{0.67} \\
Imdb             & 0.49          & 0.48                              & 0.50          & 0.50          & 0.48               & 0.52          & 0.50          & 0.48          & 0.48           & 0.59          & 0.45          & 0.58          & \textbf{0.79} \\
Yelp             & 0.55          & 0.55                              & 0.48          & 0.51          & 0.52               & 0.51          & 0.49          & 0.50          & \textbf{0.59}  & 0.66          & 0.44          & 0.58          & \textbf{0.68} \\
20newsgroups     & 0.64          & 0.53                              & 0.51          & 0.52          & 0.55               & 0.50          & 0.49          & 0.50          & 0.62           & 0.69          & \textbf{0.43} & 0.71          & \textbf{0.75} \\
BATADAL  04      & 0.65          & 0.52                              & 0.54          & 0.64          & \textbf{0.71}      & 0.50          & 0.55          & 0.52          & 0.69           & \textbf{0.80} & 0.56          & 0.64          & 0.57          \\
SWaT  1          & 0.50          & \textbf{0.57}                     & 0.53          & 0.76          & 0.56               & 0.57          & \textbf{0.50} & \textbf{0.57} & 0.73           & \textbf{0.76} & \textbf{0.78} & 0.49          & 0.54          \\
SWaT  2          & 0.51          & \textbf{0.54}                     & 0.55          & 0.57          & 0.59               & 0.58          & \textbf{0.50} & \textbf{0.54} & 0.55           & 0.64          & \textbf{0.96} & 0.44          & 0.56          \\
SWaT  3          & 0.51          & 0.48                              & 0.64          & 0.79          & 0.57               & 0.66          & \textbf{0.50} & 0.48          & NA             & NA            & NA            & NA            & \textbf{0.81} \\
SWaT  4          & 0.50          & 0.47                              & 0.43          & 0.61          & 0.47               & 0.90          & \textbf{0.50} & 0.47          & 0.32           & \textbf{0.96} & \textbf{1.00} & \textbf{0.95} & 0.07          \\
SWaT  5          & 0.49          & 0.60          & 0.63          & 0.65          & 0.71               & 0.70          & \textbf{0.50} & 0.60          & 0.76           & 0.60          & \textbf{0.96} & 0.49          & \textbf{0.64} \\
SWaT  6          & 0.51          & \textbf{0.70} & 0.78          & 0.68          & 0.68               & 0.60          & \textbf{0.50} & \textbf{0.70} & 0.73           & 0.89          & \textbf{1.00} & 0.84          & 0.82          \\
ecoli            & 0.85          & 0.84                              & 0.83          & 0.85          & 0.85               & 0.57          & 0.50          & 0.82          & 0.59           & 0.86          & 0.90          & \textbf{0.92} & \textbf{0.99} \\
cmc              & 0.48          & 0.44                              & 0.59          & 0.48          & 0.51               & 0.45          & 0.69          & 0.46          & 0.43           & 0.85          & 0.70          & 0.56          & \textbf{0.97} \\
lympho  h        & 0.88          & \textbf{0.95}                     & 0.62          & 0.45          & 0.78               & 0.53          & 0.50          & 0.44          & \textbf{0.82}  & \textbf{0.99} & 0.95          & \textbf{0.99} & \textbf{0.87} \\
wbc  h           & 0.80          & 0.82                              & 0.71          & 0.75          & 0.80               & \textbf{0.96} & 0.49          & 0.75          & 0.81           & \textbf{0.04} & 0.94          & 0.92          & \textbf{0.99}\\
\hline
\end{tabular}
}
\caption{AUC-ROC value comparison on Multivariate datasets}
\label{table:10}
\end{table*}

\begin{sidewaystable*}[h!]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{|c|c|l|c|c|c|c|c|c|l|l|l|l|l|l|l|l|l|l|}
\hline
Dataset          & LOF  & Iforest & Autoencoders & DAGMM & Envelope &DevNet & GAN  & MGBTAI        & q-LSTM (tanh) & q-LSTM (sigmoid) & QReg          & q-LSTM (PEF) & dBTAI & Deep-Sad      & FTT           & PReNet        & RN-Net        & RN-LSTM       \\
\hline
yahoo1           & 0.01 & 0.01    & 0.01         & 0.01  & 0.01     & 0.01                                               & 0.01 & \textbf{1.00} & 0.08                                                                                                 & 0.02                                                                                                    & 0.00          & 0.05                                                                                                & NA    & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.02          \\
yahoo2           & 0.06 & 0.04    & 0.05         & 0.05  & 0.05     & 0.06                                               & 0.00 & \textbf{1.00} & 0.02                                                                                                 & 0.22                                                                                                    & 0.18          & \textbf{1.00}                                                                                       & 0.02  & 0.88          & 0.88          & 0.88          & 0.88          & 0.05          \\
yahoo3           & 0.06 & 0.04    & 0.05         & 0.05  & 0.05     & 0.06                                               & 0.00 & 0.50          & 0.25                                                                                                 & 0.78                                                                                                    & 0.15          & 0.28                                                                                                & 0.28  & 0.00          & \textbf{1.00} & 0.07          & \textbf{1.00} & 0.00          \\
yahoo5           & 0.02 & 0.03    & 0.04         & 0.04  & 0.04     & 0.00   & 0.00 & \textbf{1.00} & 0.02                                                                                                 & 0.01                                                                                                    & 0.00          & 0.02                                                                                                & 0.01  & 0.00          & 0.01          & 0.00          & 0.08          & 0.03          \\
yahoo6           & 0.03 & 0.02    & 0.03         & 0.03  & 0.03     & 0.03                                               & 0.00 & \textbf{1.00} & 0.03                                                                                                 & 0.01                                                                                                    & 0.00          & 0.03                                                                                                & 0.01  & 0.00          & 0.50          & 0.00          & \textbf{1.00} & 0.09          \\
yahoo7           & 0.02 & 0.03    & 0.00         & 0.03  & 0.03     & 0.01                                               & 0.01 & \textbf{0.50} & 0.07                                                                                                 & 0.07                                                                                                    & 0.00          & 0.07                                                                                                & 0.01  & 0.01          & 0.02          & 0.01          & 0.06          & 0.15          \\
yahoo8           & 0.01 & 0.01    & 0.01         & 0.01  & 0.01     & 0.01                                               & 0.01 & \textbf{1.00} & 0.03                                                                                                 & 0.03                                                                                                    & 0.00          & 0.03                                                                                                & 0.01  & 0.00          & 0.01          & 0.01          & \textbf{1.00} & 0.13          \\
yahoo9           & 0.01 & 0.01    & 0.05         & 0.05  & 0.01     & 0.02                                               & 0.00 & \textbf{1.00} & 0.02                                                                                                 & \textbf{1.00}                                                                                           & 0.00          & 0.02                                                                                                & 0.01  & 0.00          & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.09          \\
Speed  6005      & 0.01 & 0.00    & 0.00         & 0.00  & 0.00     & 0.00     & 0.00 & 0.20          & 0.01                                                                                                 & 0.01                                                                                                    & 0.00          & 0.01                                                                                                & 0.03  & NA            & NA            & NA            & \textbf{0.50} & 0.00          \\
Speed  7578      & 0.03 & 0.03    & 0.03         & 0.04  & 0.03     & 0.00   & 0.04 & 0.09          & \textbf{0.50}                                                                                        & \textbf{0.50}                                                                                           & \textbf{0.50} & 0.09                                                                                                & NA    & 0.12          & 0.12          & 0.12          & 0.17          & 0.01          \\
Speed  t4013     & 0.02 & 0.01    & 0.01         & 0.00  & 0.01     & 0.00   & 0.01 & 0.29          & 0.01                                                                                                 & 0.03                                                                                                    & 0.04          & 0.05                                                                                                & NA    & 0.00          & \textbf{1.00} & 0.50          & \textbf{1.00} & 0.02          \\
TravelTime  387  & 0.01 & 0.01    & 0.01         & 0.01  & 0.01     & 0.01                                               & 0.00 & 0.05          & 0.01                                                                                                 & 0.00                                                                                                    & 0.00          & 0.01                                                                                                & 0.01  & 0.00          & 0.05          & 0.05          & \textbf{0.16} & 0.04          \\
TravelTime  451  & 0.01 & 0.00    & 0.01         & 0.01  & 0.01     & NA     & 0.00 & 0.04          & 0.00                                                                                                 & 0.01                                                                                                    & 0.00          & 0.01                                                                                                & 0.01  & NA            & NA            & NA            & \textbf{1.00} & 0.00          \\
Occupancy  6005  & 0.00 & 0.00    & 0.00         & 0.00  & 0.00     & NA     & 0.00 & 0.06          & 0.01                                                                                                 & 0.01                                                                                                    & 0.00          & 0.03                                                                                                & 0.01  & NA            & NA            & NA            & \textbf{1.00} & 0.01          \\
Occupancy  t4013 & 0.01 & 0.01    & 0.01         & 0.00  & 0.01     & NA     & 0.00 & 0.33          & 0.09                                                                                                 & 0.01                                                                                                    & 0.00          & 0.06                                                                                                & 0.03  & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.01          \\
yahoo  syn1      & 0.08 & 0.06    & 0.08         & 0.08  & 0.08     & 0.08                                               & 0.00 & \textbf{1.00} & 0.09                                                                                                 & 0.08                                                                                                    & 0.03          & 0.38                                                                                                & NA    & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.04          \\
yahoo  syn2      & 0.13 & 0.10    & 0.12         & 0.12  & 0.13     & 0.14                                               & 0.00 & \textbf{1.00} & \textbf{1.00}                                                                                        & 0.02                                                                                                    & 0.00          & \textbf{1.00}                                                                                       & 0.25  & 0.94          & 0.94          & 0.94          & 0.94          & \textbf{1.00} \\
yahoo  syn3      & 0.10 & 0.10    & 0.11         & 0.11  & 0.11     & 0.14                                               & 0.00 & 0.81          & 0.06                                                                                                 & \textbf{1.00}                                                                                           & 0.08          & 0.60                                                                                                & 0.81  & 0.00          & 0.94          & 0.19          & \textbf{1.00} & \textbf{1.00} \\
yahoo  syn5      & 0.08 & 0.09    & 0.11         & 0.11  & 0.11     & 0.00   & 0.01 & \textbf{1.00} & 0.06                                                                                                 & 0.04                                                                                                    & 0.00          & 0.06                                                                                                & 0.02  & 0.00          & 0.31          & 0.28          & 0.26          & 0.07          \\
yahoo  syn6      & 0.03 & 0.08    & 0.03         & 0.03  & 0.03     & 0.03                                               & 0.01 & \textbf{1.00} & 0.04                                                                                                 & 0.06                                                                                                    & 0.01          & 0.76                                                                                                & 0.02  & 0.00          & 0.05          & 0.04          & \textbf{1.00} & 0.14          \\
yahoo  syn7      & 0.04 & 0.07    & 0.05         & 0.07  & 0.08     & 0.10   & 0.01 & \textbf{0.56} & 0.20                                                                                                 & 0.26                                                                                                    & 0.01          & 0.41                                                                                                & 0.02  & 0.01          & 0.03          & 0.01          & 0.12          & 0.15          \\
yahoo  syn8      & 0.03 & 0.02    & 0.02         & 0.00  & 0.02     & 0.02                                               & 0.01 & \textbf{1.00} & 0.07                                                                                                 & 0.07                                                                                                    & 0.20          & 0.20                                                                                                & 0.01  & 0.00          & 0.00          & 0.00          & \textbf{1.00} & 0.14          \\
yahoo  syn9      & 0.11 & 0.08    & 0.11         & 0.03  & 0.10     & 0.11                                               & 0.00 & \textbf{1.00} & 0.12                                                                                                 & \textbf{1.00}                                                                                           & 0.01          & \textbf{1.00}                                                                                       & 0.04  & 0.00          & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.43          \\
aws1             & 0.01 & 0.01    & 0.01         & 0.01  & 0.01     & NA     & 0.00 & \textbf{1.00} & 0.04                                                                                                 & 0.03                                                                                                    & 0.02          & 0.04                                                                                                & 0.00  & NA            & NA            & NA            & \textbf{1.00} & 0.00          \\
aws2             & 0.09 & 0.01    & 0.00         & 0.00  & 0.04     & 0.01                                               & 0.00 & \textbf{1.00} & 0.01                                                                                                 & 0.01                                                                                                    & 0.00          & 0.00                                                                                                & NA    & 0.10          & 0.10          & 0.10          & \textbf{1.00} & 0.01          \\
aws3             & 0.01 & 0.01    & 0.01         & 0.01  & 0.01     & NA     & 0.00 & 0.50          & 0.01                                                                                                 & 0.01                                                                                                    & 0.01          & 0.02                                                                                                & 0.00  & NA            & NA            & NA            & \textbf{1.00} & 0.00          \\
aws  syn1        & 0.10 & 0.03    & 0.00         & 0.08  & 0.06     & 0.03   & 0.00 & \textbf{0.90} & 0.22                                                                                                 & 0.08                                                                                                    & 0.02          & 0.08                                                                                                & 0.04  & 0.00          & \textbf{0.90} & 0.82          & \textbf{0.90} & 0.50          \\
aws  syn2        & 0.02 & 0.07    & 0.00         & 0.08  & 0.08     & 0.00                                               & 0.01 & 0.67          & 0.05                                                                                                 & 0.04                                                                                                    & 0.00          & 0.00                                                                                                & NA    & 0.66          & 0.66          & 0.66          & 0.81          & \textbf{0.94} \\
aws  syn3        & 0.07 & 0.05    & 0.01         & 0.07  & 0.06     & 0.02   & 0.00 & 0.90          & \textbf{1.00}                                                                                        & 0.06                                                                                                    & 0.01          & 0.06                                                                                                & 0.02  & 0.00          & 0.90          & 0.82          & 0.90          & 0.88 \\
\hline
\end{tabular}
}
\caption{Precision value comparison on Univariate datasets}
\label{table:12}
\end{sidewaystable*}


\begin{sidewaystable*}[h!]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{|c|c|l|c|c|c|c|c|c|l|l|l|l|l|l|l|l|l|l|}
\hline
Dataset          & LOF           & Iforest       & Autoencoders  & DAGMM         & Envelope      & DevNet        & GAN           & MGBTAI        & q-LSTM (tanh) & q-LSTM (sigmoid) & QReg          & q-LSTM (PEF) & dBTAI         & Deep-Sad      & FTT           & PReNet & RN-Net        & RN-LSTM       \\
\hline
yahoo1           & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.50          & \textbf{1.00} & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & 0.00          & \textbf{1.00}                                                                                       & NA            & 0.50          & 0.50          & 0.50   & \textbf{1.00} & \textbf{1.00} \\
yahoo2           & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.00          & 0.25          & 0.25                                                     & 0.63                                                                                                    & 0.63          & 0.38                                                                                                & \textbf{1.00} & 0.88          & 0.88          & 0.88   & \textbf{1.00} & 0.29          \\
yahoo3           & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.88          & 0.88          & \textbf{1.00} & 0.00          & 0.88          & \textbf{1.00}                                                                                        & 0.88                                                                                                    & 0.75          & \textbf{1.00}                                                                                       & 0.88          & 0.00          & 0.75          & 0.88   & \textbf{1.00} & 0.00          \\
yahoo5           & 0.33          & 0.67          & 0.67          & 0.67          & 0.67          & 0.00          & 0.00          & 0.33          & 0.67                                                     & 0.33                                                                                                    & 0.00          & 0.66                                                                                                & \textbf{1.00} & 0.00          & 0.67          & 0.22   & 0.67          & \textbf{1.00} \\
yahoo6           & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & \textbf{1.00} & \textbf{1.00}                                                                                       & \textbf{1.00} & 0.00          & 0.50          & 0.00   & \textbf{1.00} & \textbf{1.00} \\
yahoo7           & 0.36          & 0.55          & 0.00          & 0.45          & 0.45          & 0.09          & \textbf{1.00} & 0.36          & 0.36                                                     & 0.55                                                                                                    & 0.00          & 0.54                                                                                                & 0.73          & 0.91          & 0.82          & 0.55   & 0.80          & 0.30          \\
yahoo8           & 0.20          & 0.20          & 0.10          & 0.20          & 0.20          & 0.20          & \textbf{1.00} & 0.01          & 0.30                                                                                                 & 0.30                                                                                                    & 0.00          & 0.30                                                                                                & 0.60          & 0.00          & 0.80          & 0.80   & 0.89          & 0.11          \\
yahoo9           & 0.20          & 0.20          & \textbf{1.00} & \textbf{1.00} & 0.20          & 0.38          & 0.00          & 0.62          & 0.63                                                     & 0.75                                                                                                    & 0.88          & 0.75                                                                                                & \textbf{1.00} & 0.00          & 0.88          & 0.50   & \textbf{1.00} & 0.86          \\
Speed  6005      & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & NA            & \textbf{1.00} & \textbf{1.00} & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & 0.00          & \textbf{1.00}                                                                                       & \textbf{1.00} & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
Speed  7578      & 0.75          & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.00          & \textbf{1.00} & 0.75          & 0.25                                                     & 0.25                                                                                                    & 0.24          & \textbf{1.00}                                                                                       & NA            & \textbf{1.00} & \textbf{1.00} & 0.00   & 0.75          & \textbf{1.00} \\
Speed  t4013     & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.00          & \textbf{1.00} & \textbf{1.00} & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & \textbf{1.00} & \textbf{1.00}                                                                                       & NA            & \textbf{1.00} & \textbf{1.00} & 0.00   & \textbf{1.00} & 0.50          \\
TravelTime  387  & 0.67          & 0.67          & 0.67          & 0.67          & 0.67          & 0.67          & 0.00          & 0.67          & 0.33                                                     & 0.33                                                                                                    & 0.33          & 0.67                                                                                                & 0.67          & 0.67          & 0.67          & 0.33   & 0.67          & \textbf{1.00} \\
TravelTime  451  & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & NA            & 0.00          & \textbf{1.00} & 0.00                                                                                                 & \textbf{1.00}                                                                                           & 0.00          & \textbf{1.00}                                                                                       & \textbf{1.00} & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
Occupancy  6005  & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.00          & \textbf{1.00} & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & 0.00          & \textbf{1.00}                                                                                       & \textbf{1.00} & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
Occupancy  t4013 & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.00          & \textbf{1.00} & NA            & 0.00          & \textbf{1.00} & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & \textbf{1.00} & \textbf{1.00}                                                                                       & \textbf{1.00} & 0.50          & 0.50          & 0.50   & \textbf{1.00} & \textbf{1.00} \\
yahoo  syn1      & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.00          & \textbf{0.17} & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & 0.08          & \textbf{1.00}                                                                                       & NA            & 0.92          & 0.92          & 0.92   & \textbf{1.00} & 0.91          \\
yahoo  syn2      & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.00          & 0.28          & 0.50                                                                                                 & 0.61                                                                                                    & 0.00          & 0.61                                                                                                & 0.78          & 0.94          & 0.94          & 0.94   & \textbf{1.00} & 0.81          \\
yahoo  syn3      & 0.83          & \textbf{1.00} & 0.94          & 0.94          & \textbf{1.00} & \textbf{1.00} & 0.00          & 0.72          & 0.61                                                     & 0.44                                                                                                    & 0.39          & \textbf{1.00}                                                                                       & 0.95          & 0.00          & 0.89          & 0.94   & \textbf{1.00} & 0.94          \\
yahoo  syn5      & 0.58          & 0.84          & 0.84          & 0.84          & 0.84          & 0.00          & \textbf{1.00} & 0.42          & 0.74                                                     & 0.74                                                                                                    & 0.00          & 0.58                                                                                                & 0.58          & 0.00          & 0.79          & 0.79   & \textbf{1.00} & 0.65          \\
yahoo  syn6      & 0.29          & \textbf{1.00} & 0.29          & 0.29          & 0.29          & 0.29          & \textbf{1.00} & 0.29          & 0.86                                                     & 0.86                                                                                                    & \textbf{1.00} & 0.93                                                                                                & \textbf{1.00} & 0.00          & 0.64          & 0.64   & 0.91          & 0.42          \\
yahoo  syn7      & 0.33          & 0.71          & 0.43          & 0.52          & 0.67          & 0.71          & \textbf{1.00} & 0.24          & 0.62                                                     & 0.62                                                                                                    & 0.19          & 0.66                                                                                                & 0.76          & 0.95          & 0.90          & 0.19   & 0.78          & 0.94          \\
yahoo  syn8      & 0.25          & 0.20          & 0.15          & 0.00          & 0.15          & 0.20          & \textbf{1.00} & 0.05          & 0.40                                                                                                 & 0.40                                                                                                    & \textbf{1.00} & 0.70                                                                                                & 0.60          & 0.00          & 0.00          & 0.00   & 0.94          & 0.18          \\
yahoo  syn9      & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.25          & \textbf{1.00} & \textbf{1.00} & 0.00          & 0.06          & 0.28                                                     & \textbf{0.72}                                                                                           & 0.89          & 0.94                                                                                                & \textbf{1.00} & 0.00          & 0.94          & 0.78   & \textbf{1.00} & 0.94          \\
aws1             & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & NA            & 0.00          & \textbf{1.00} & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & \textbf{1.00} & \textbf{1.00}                                                                                       & \textbf{1.00} & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
aws2             & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.00          & 0.50          & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & 0.00          & \textbf{1.00}                                                                                       & NA            & 0.50          & 0.50          & 0.50   & 0.50          & \textbf{1.00} \\
aws3             & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & NA            & 0.00          & \textbf{1.00} & \textbf{1.00}                                                                                        & \textbf{1.00}                                                                                           & \textbf{1.00} & \textbf{1.00}                                                                                       & \textbf{1.00} & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
aws  syn1        & 0.05          & \textbf{1.00} & 0.00          & 0.82          & \textbf{1.00} & 0.30          & 0.00          & 0.90          & \textbf{1.00}                                                                                        & 0.64                                                                                                    & \textbf{1.00} & \textbf{1.00}                                                                                       & \textbf{1.00} & 0.00          & 0.90          & 0.90   & \textbf{1.00} & 0.78          \\
aws  syn2        & 0.02          & \textbf{1.00} & 0.00          & \textbf{1.00} & \textbf{1.00} & 0.05          & \textbf{1.00} & \textbf{1.00} & \textbf{1.00}                                                                                        & 0.86                                                                                                    & 0.00          & \textbf{1.00}                                                                                       & NA            & 0.95          & 0.95          & 0.95   & \textbf{1.00} & 0.94          \\
aws  syn3        & 0.07          & \textbf{1.00} & 0.20          & \textbf{1.00} & \textbf{1.00} & 0.30          & 0.00          & 0.90          & \textbf{1.00}                                                                                        & 0.64                                                                                                    & \textbf{1.00} & \textbf{1.00}                                                                                       & \textbf{1.00} & 0.00          & 0.90          & 0.90   & \textbf{1.00} & 0.78         \\
\hline
\end{tabular}
}
\caption{Recall value comparison on Univariate datasets}
\label{table:13}
\end{sidewaystable*}

\begin{sidewaystable*}[h!]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{|c|c|l|c|c|c|c|c|c|l|l|l|l|l|l|l|l|l|l|}
\hline
Dataset          & LOF  & Iforest & Autoencoders & DAGMM & Envelope & DevNet & GAN  & MGBTAI        & q-LSTM (tanh) & q-LSTM (sigmoid) & QReg & q-LSTM (PEF) & dBTAI & Deep-Sad & FTT  & PReNet & RN-Net        & RN-LSTM       \\
\hline
yahoo1           & 0.03 & 0.02    & 0.01         & 0.03  & 0.03     & 0.03   & 0.01 & \textbf{1.00} & 0.15                                                                                                 & 0.04                                                                                                    & 0.00 & 0.09                                                                                                & NA    & 0.67     & 0.67 & 0.67   & \textbf{1.00} & 0.03          \\
yahoo2           & 0.11 & 0.08    & 0.10         & 0.10  & 0.10     & 0.12   & 0.00 & 0.40          & 0.03                                                                                                 & 0.33                                                                                                    & 0.28 & 0.55                                                                                                & 0.04  & 0.88     & 0.88 & 0.88   & \textbf{0.94} & 0.09          \\
yahoo3           & 0.11 & 0.09    & 0.10         & 0.09  & 0.09     & 0.11   & 0.00 & 0.64          & 0.40                                                                                                 & 0.82                                                                                                    & 0.26 & 0.44                                                                                                & 0.42  & 0.00     & 0.86 & 0.13   & \textbf{1.00} & 0.00          \\
yahoo5           & 0.04 & 0.06    & 0.08         & 0.08  & 0.08     & 0.00   & 0.00 & \textbf{0.50} & 0.04                                                                                                 & 0.02                                                                                                    & 0.00 & 0.04                                                                                                & 0.03  & 0.00     & 0.01 & 0.01   & 0.15          & 0.05          \\
yahoo6           & 0.05 & 0.04    & 0.05         & 0.05  & 0.06     & 0.05   & 0.01 & \textbf{1.00} & 0.05                                                                                                 & 0.03                                                                                                    & 0.01 & 0.05                                                                                                & 0.01  & 0.00     & 0.50 & 0.00   & \textbf{1.00} & 0.16          \\
yahoo7           & 0.04 & 0.05    & 0.00         & 0.06  & 0.05     & 0.01   & 0.01 & \textbf{0.42} & 0.12                                                                                                 & 0.13                                                                                                    & 0.00 & 0.12                                                                                                & 0.02  & 0.01     & 0.03 & 0.01   & 0.11          & 0.20          \\
yahoo8           & 0.02 & 0.02    & 0.01         & 0.02  & 0.02     & 0.02   & 0.01 & 0.01          & 0.05                                                                                                 & 0.05                                                                                                    & 0.00 & 0.05                                                                                                & 0.01  & 0.00     & 0.02 & 0.02   & \textbf{0.94} & 0.12          \\
yahoo9           & 0.02 & 0.02    & 0.09         & 0.09  & 0.02     & 0.03   & 0.00 & 0.77          & 0.04                                                                                                 & 0.86                                                                                                    & 0.01 & 0.04                                                                                                & 0.03  & 0.00     & 0.93 & 0.67   & \textbf{1.00} & 0.16          \\
Speed  6005      & 0.03 & 0.01    & 0.01         & 0.01  & 0.01     & 0.00   & 0.01 & 0.33          & 0.03                                                                                                 & 0.02                                                                                                    & 0.00 & 0.03                                                                                                & 0.05  & NA       & NA   & NA     & \textbf{0.67} & 0.01          \\
Speed  7578      & 0.06 & 0.06    & 0.06         & 0.07  & 0.06     & 0.00   & 0.07 & 0.16          & \textbf{0.33}                                                                                        & \textbf{0.33}                                                                                           & 0.32 & 0.16                                                                                                & NA    & 0.26     & 0.26 & 0.00   & 0.27          & 0.02          \\
Speed  t4013     & 0.04 & 0.01    & 0.01         & 0.01  & 0.02     & 0.00   & 0.01 & 0.44          & 0.02                                                                                                 & 0.07                                                                                                    & 0.08 & 0.10                                                                                                & NA    & 0.80     & 0.80 & 0.00   & \textbf{1.00} & 0.04          \\
TravelTime  387  & 0.02 & 0.01    & 0.02         & 0.02  & 0.02     & 0.02   & 0.00 & 0.09          & 0.01                                                                                                 & 0.01                                                                                                    & 0.00 & 0.02                                                                                                & 0.02  & 0.00     & 0.08 & 0.08   & \textbf{0.26} & 0.07          \\
TravelTime  451  & 0.01 & 0.01    & 0.01         & 0.01  & 0.01     & 0.00   & 0.00 & 0.08          & 0.00                                                                                                 & 0.01                                                                                                    & 0.00 & 0.01                                                                                                & 0.02  & NA       & NA   & NA     & \textbf{1.00} & 0.00          \\
Occupancy  6005  & 0.01 & 0.01    & 0.01         & 0.01  & 0.01     & 0.01   & 0.00 & 0.12          & 0.02                                                                                                 & 0.02                                                                                                    & 0.00 & 0.06                                                                                                & 0.02  & NA       & NA   & NA     & \textbf{1.00} & 0.01          \\
Occupancy  t4013 & 0.02 & 0.01    & 0.02         & 0.00  & 0.02     & 0.02   & 0.00 & 0.50          & 0.16                                                                                                 & 0.02                                                                                                    & 0.01 & 0.11                                                                                                & 0.05  & 0.67     & 0.67 & 0.67   & \textbf{1.00} & 0.01          \\
yahoo  syn1      & 0.16 & 0.12    & 0.16         & 0.16  & 0.14     & 0.16   & 0.00 & 0.29          & 0.16                                                                                                 & 0.15                                                                                                    & 0.04 & 0.55                                                                                                & NA    & 0.96     & 0.96 & 0.96   & \textbf{1.00} & 0.09          \\
yahoo  syn2      & 0.23 & 0.18    & 0.21         & 0.22  & 0.23     & 0.24   & 0.00 & 0.43          & 0.67                                                                                                 & 0.05                                                                                                    & 0.00 & 0.76                                                                                                & 0.37  & 0.94     & 0.94 & 0.94   & \textbf{0.97} & 0.89          \\
yahoo  syn3      & 0.18 & 0.18    & 0.20         & 0.20  & 0.20     & 0.24   & 0.00 & 0.76          & 0.11                                                                                                 & 0.62                                                                                                    & 0.13 & 0.75                                                                                                & 0.87  & 0.00     & 0.91 & 0.31   & \textbf{1.00} & 0.97          \\
yahoo  syn5      & 0.14 & 0.17    & 0.20         & 0.20  & 0.20     & 0.00   & 0.03 & \textbf{0.59} & 0.10                                                                                                 & 0.07                                                                                                    & 0.00 & 0.11                                                                                                & 0.03  & 0.00     & 0.44 & 0.42   & 0.41          & 0.12          \\
yahoo  syn6      & 0.05 & 0.15    & 0.05         & 0.05  & 0.05     & 0.05   & 0.02 & 0.44          & 0.08                                                                                                 & 0.11                                                                                                    & 0.02 & 0.84                                                                                                & 0.05  & 0.00     & 0.09 & 0.08   & \textbf{0.95} & 0.21          \\
yahoo  syn7      & 0.07 & 0.13    & 0.09         & 0.01  & 0.15     & 0.17   & 0.02 & 0.33          & 0.30                                                                                                 & 0.37                                                                                                    & 0.02 & \textbf{0.51}                                                                                       & 0.05  & 0.02     & 0.07 & 0.02   & 0.20          & 0.26          \\
yahoo  syn8      & 0.05 & 0.04    & 0.03         & 0.00  & 0.03     & 0.04   & 0.02 & 0.10          & 0.12                                                                                                 & 0.12                                                                                                    & 0.34 & 0.31                                                                                                & 0.03  & 0.00     & 0.00 & 0.00   & \textbf{0.97} & 0.15          \\
yahoo  syn9      & 0.19 & 0.15    & 0.19         & 0.05  & 0.18     & 0.19   & 0.00 & 0.11          & 0.16                                                                                                 & 0.84                                                                                                    & 0.02 & 0.97                                                                                                & 0.07  & 0.00     & 0.97 & 0.88   & \textbf{1.00} & 0.59          \\
aws1             & 0.02 & 0.02    & 0.02         & 0.02  & 0.02     & 0.02   & 0.00 & \textbf{1.00} & 0.07                                                                                                 & 0.05                                                                                                    & 0.04 & 0.08                                                                                                & 0.01  & NA       & NA   & NA     & \textbf{1.00} & 0.01          \\
aws2             & 0.17 & 0.02    & 0.00         & 0.01  & 0.08     & 0.02   & 0.00 & \textbf{0.67} & 0.01                                                                                                 & 0.03                                                                                                    & 0.00 & 0.01                                                                                                & NA    & 0.17     & 0.17 & 0.17   & \textbf{0.67} & 0.01          \\
aws3             & 0.01 & 0.01    & 0.01         & 0.01  & 0.01     & 0.01   & 0.00 & \textbf{0.67} & 0.01                                                                                                 & 0.02                                                                                                    & 0.02 & 0.04                                                                                                & 0.00  & NA       & NA   & NA     & \textbf{1.00} & 0.01          \\
aws  syn1        & 0.17 & 0.06    & 0.00         & 0.14  & 0.11     & 0.05   & 0.00 & 0.90          & 0.37                                                                                                 & 0.15                                                                                                    & 0.04 & \textbf{0.99}                                                                                       & 0.07  & 0.00     & 0.90 & 0.86   & 0.95          & 0.61          \\
aws  syn2        & 0.05 & 0.13    & 0.00         & 0.15  & 0.15     & 0.01   & 0.02 & 0.80          & 0.09                                                                                                 & 0.08                                                                                                    & 0.00 & 0.90                                                                                                & NA    & 0.78     & 0.78 & 0.78   & 0.89          & \textbf{0.94} \\
aws  syn3        & 0.13 & 0.10    & 0.03         & 0.12  & 0.12     & 0.04   & 0.00 & 0.90          & \textbf{1.00}                                                                                        & 0.11                                                                                                    & 0.02 & 0.98                                                                                                & 0.05  & 0.00     & 0.90 & 0.86   & 0.95          & 0.82 \\
\hline
\end{tabular}
}
\caption{F-1 Score comparison on Univariate datasets}
\label{table:14}
\end{sidewaystable*}



\begin{sidewaystable*}[h!]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{|c|c|l|c|c|c|c|c|c|l|l|l|l|l|l|l|l|l|l|}
\hline
Dataset          & LOF           & Iforest & Autoencoders  & DAGMM         & Envelope      & DevNet & GAN  & MGBTAI        & q-LSTM (tanh) & q-LSTM (sigmoid) & QReg & q-LSTM (PEF) & dBTAI & Deep-SAD      & FTT           & PReNet        & RN-Net        & RN-LSTM       \\
\hline
yahoo1           & 0.95          & 0.94    & 0.90          & 0.95          & 0.95          & 0.95   & 0.70 & \textbf{1.00} & 0.99                                                                                                 & 0.97                                                                                                    & 0.49 & 0.99                                                                                                & NA    & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1}    & 0.71          \\
yahoo2           & 0.95          & 0.94    & 0.95          & 0.95          & 0.95          & 0.96   & 0.45 & 0.62          & 0.58                                                                                                 & 0.84                                                                                                    & 0.80 & 0.69                                                                                                & 0.85  & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.99          & 0.39          \\
yahoo3           & 0.95          & 0.94    & 0.95          & 0.89          & 0.89          & 0.95   & 0.45 & 0.94          & 0.99                                                                                                 & 0.94                                                                                                    & 0.86 & 0.99                                                                                                & 0.93  & 0.02          & 0.92          & 0.99          & \textbf{1}    & 0.1           \\
yahoo5           & 0.62          & 0.77    & \textbf{0.78} & \textbf{0.78} & \textbf{0.78} & 0.45   & 0.45 & 0.67          & 0.73                                                                                                 & 0.55                                                                                                    & 0.49 & 0.74                                                                                                & 0.76  & 0.33          & 0.63          & 0.25          & 0.65          & 0.4           \\
yahoo6           & 0.95          & 0.94    & 0.95          & 0.95          & 0.95          & 0.95   & 0.50 & \textbf{1.00} & 0.95                                                                                                 & 0.90                                                                                                    & 0.50 & 0.95                                                                                                & 0.80  & 0.00          & \textbf{1.00} & 0.00          & \textbf{1}    & 0.96          \\
yahoo7           & 0.63          & 0.71    & 0.45          & 0.68          & 0.68          & 0.50   & 0.50 & 0.68          & 0.67                                                                                                 & 0.75                                                                                                    & 0.49 & 0.74                                                                                                & 0.67  & 0.17          & \textbf{0.84} & 0.53          & 0.82          & 0.49          \\
yahoo8           & 0.55          & 0.54    & 0.50          & 0.55          & 0.55          & 0.55   & 0.50 & 0.50          & 0.62                                                                                                 & 0.62                                                                                                    & 0.49 & 0.62                                                                                                & 0.56  & 0.30          & 0.68          & 0.68          & \textbf{0.97} & 0.21          \\
yahoo9           & 0.55          & 0.54    & 0.95          & 0.95          & 0.55          & 0.63   & 0.45 & 0.81          & 0.73                                                                                                 & 0.88                                                                                                    & 0.51 & 0.79                                                                                                & 0.81  & 0.00          & \textbf{1.00} & 0.63          & \textbf{1}    & 0.91          \\
Speed  6005      & 0.99          & 0.95    & 0.95          & 0.93          & 0.95          & NA     & 0.94 & \textbf{1.00} & 0.99                                                                                                 & 0.98                                                                                                    & 0.46 & 0.97                                                                                                & 0.99  & NA            & NA            & NA            & 0.99          & 0.47          \\
Speed  7578      & 0.83          & 0.94    & 0.95          & 0.95          & 0.95          & 0.45   & 0.95 & 0.86          & 0.62                                                                                                 & 0.62                                                                                                    & 0.61 & 0.98                                                                                                & NA    & 0.99          & 0.99          & \textbf{1.00} & 0.84          & 0.03          \\
Speed  t4013     & 0.98          & 0.94    & 0.93          & 0.91          & 0.95          & 0.46   & 0.94 & \textbf{1.00} & 0.95                                                                                                 & 0.99                                                                                                    & 0.99 & 0.99                                                                                                & NA    & 0.99          & \textbf{1.00} & \textbf{1.00} & \textbf{1}    & 0.63          \\
TravelTime  387  & 0.78          & 0.77    & 0.78          & 0.78          & 0.79          & 0.78   & 0.45 & 0.83          & 0.64                                                                                                 & 0.62                                                                                                    & 0.48 & 0.80                                                                                                & 0.80  & 0.59          & 0.77          & 0.80          & 0.74          & \textbf{0.93} \\
TravelTime  451  & 0.95          & 0.94    & 0.95          & 0.95          & 0.95          & NA     & 0.45 & 0.99          & 0.50                                                                                                 & 0.97                                                                                                    & 0.49 & 0.96                                                                                                & 0.97  & NA            & NA            & NA            & \textbf{1}    & 0.33          \\
Occupancy  6005  & 0.95          & 0.94    & 0.95          & 0.95          & 0.95          & 0.95   & 0.45 & \textbf{1.00} & 0.98                                                                                                 & 0.98                                                                                                    & 0.49 & 0.99                                                                                                & 0.97  & NA            & NA            & NA            & \textbf{1}    & 0.74          \\
Occupancy  t4013 & 0.95          & 0.94    & 0.95          & 0.50          & 0.95          & NA     & 0.45 & \textbf{1.00} & \textbf{1.00}                                                                                        & 0.95                                                                                                    & 0.90 & 0.99                                                                                                & 0.98  & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1}    & 0.65          \\
yahoo  syn1      & 0.95          & 0.94    & 0.95          & 0.95          & 0.95          & 0.95   & 0.45 & 0.58          & 0.96                                                                                                 & 0.95                                                                                                    & 0.53 & 0.99                                                                                                & NA    & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1}    & 0.57          \\
yahoo  syn2      & 0.96          & 0.94    & 0.95          & 0.95          & 0.96          & 0.96   & 0.45 & 0.64          & 0.75                                                                                                 & 0.65                                                                                                    & 0.49 & 0.81                                                                                                & 0.87  & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.99          & 0.94          \\
yahoo  syn3      & 0.87          & 0.94    & 0.93          & 0.93          & 0.95          & 0.96   & 0.45 & 0.86          & 0.75                                                                                                 & 0.72                                                                                                    & 0.67 & \textbf{1.00}                                                                                       & 0.97  & 0.01          & 0.99          & \textbf{1.00} & \textbf{1}    & 0.96          \\
yahoo  syn5      & 0.74          & 0.87    & 0.88          & 0.88          & 0.88          & 0.45   & 0.50 & 0.71          & 0.78                                                                                                 & 0.75                                                                                                    & 0.49 & 0.73                                                                                                & 0.57  & 0.17          & 0.92          & 0.91          & \textbf{0.98} & 0.52          \\
yahoo  syn6      & 0.59          & 0.94    & 0.59          & 0.59          & 0.60          & 0.59   & 0.50 & 0.64          & 0.84                                                                                                 & 0.86                                                                                                    & 0.50 & 0.96                                                                                                & 0.80  & 0.17          & 0.63          & 0.62          & \textbf{0.98} & 0.58          \\
yahoo  syn7      & 0.62          & 0.80    & 0.67          & 0.71          & 0.79          & 0.82   & 0.50 & 0.62          & 0.79                                                                                                 & 0.80                                                                                                    & 0.47 & 0.82                                                                                                & 0.69  & 0.11          & 0.84          & 0.32          & 0.84          & \textbf{0.91} \\
yahoo  syn8      & 0.58          & 0.54    & 0.52          & 0.50          & 0.52          & 0.55   & 0.50 & 0.53          & 0.67                                                                                                 & 0.67                                                                                                    & 0.65 & 0.83                                                                                                & 0.56  & 0.27          & 0.28          & 0.25          & \textbf{0.96} & 0.46          \\
yahoo  syn9      & 0.95          & 0.94    & 0.95          & 0.58          & 0.95          & 0.95   & 0.45 & 0.53          & 0.63                                                                                                 & 0.86                                                                                                    & 0.48 & 0.97                                                                                                & 0.86  & 0.00          & \textbf{1.00} & 0.83          & \textbf{1}    & 0.99          \\
aws1             & 0.95          & 0.94    & 0.95          & 0.95          & 0.95          & NA     & 0.45 & \textbf{1.00} & 0.99                                                                                                 & 0.98                                                                                                    & 0.98 & 0.99                                                                                                & 0.83  & NA            & NA            & NA            & \textbf{1}    & 0.07          \\
aws2             & \textbf{1.00} & 0.96    & 0.75          & 0.86          & 0.99          & 0.96   & 0.41 & 0.75          & 0.91                                                                                                 & 0.97                                                                                                    & 0.49 & 0.90                                                                                                & NA    & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.5           & 0.81          \\
aws3             & 0.95          & 0.94    & 0.95          & 0.95          & 0.95          & NA     & 0.45 & \textbf{1.00} & 0.95                                                                                                 & 0.96                                                                                                    & 0.97 & 0.98                                                                                                & 0.83  & NA            & NA            & NA            & \textbf{1}    & 0.05          \\
aws  syn1        & 0.95          & 0.88    & 0.45          & 0.85          & 0.94          & 0.85   & 0.45 & 0.95          & 0.98                                                                                                 & 0.79                                                                                                    & 0.98 & \textbf{0.98}                                                                                       & 0.87  & 0.00          & \textbf{1.00} & \textbf{1.00} & 0.99          & 0.86          \\
aws  syn2        & 0.60          & 0.95    & 0.45          & 0.95          & 0.95          & 0.95   & 0.50 & \textbf{1.00} & 0.92                                                                                                 & 0.85                                                                                                    & 0.49 & 0.92                                                                                                & NA    & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.99          & 0.98          \\
aws  syn3        & 0.95          & 0.94    & 0.55          & 0.95          & 0.95          & 0.95   & 0.45 & 0.95          & \textbf{1.00}                                                                                        & 0.79                                                                                                    & 0.97 & \textbf{1.00}                                                                                       & 0.86  & 0.00          & \textbf{1.00} & \textbf{1.00} & 0.99          & 0.88  \\
\hline
\end{tabular}
}
\caption{AUC-ROC value comparison on Univariate datasets}
\label{table:15}
\end{sidewaystable*}

\begin{table*}[h]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{2.8}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Dataset} & \textbf{Weibull} & \textbf{Norm} & \textbf{Beta} & \textbf{Invgauss} & \textbf{Uniform} & \textbf{Gamma} & \textbf{Exponential} & \textbf{Log-normal} & \textbf{Final Distribution} \\ \hline
AWS Dataset1 & 5591.025091 & 5661.721169 & 5652.228399 & 5655.034052 & 5689.307921 & 5655.034782 & 5638.380250 & 5656.220499 & Weibull \\ \hline
AWS Dataset2 & 5319.113225 & 5389.728432 & 5380.193890 & 5381.946270 & 5417.513744 & 5383.094070 & 5366.418284 & 5384.294951 & Weibull \\ \hline
AWS Dataset3 & 8070.476950 & 8139.093125 & 8132.192685 & 8132.969511 & 8164.906759 & 8133.745977 & 8116.892347 & 8134.648112 & Weibull \\ \hline
Yahoo Dataset1 & 7651.443083 & 7737.129631 & 7723.543031 & 7706.966023 & 7760.148795 & 7724.377931 & 7708.709458 & 7658.427527 & Weibull \\ \hline
Yahoo Dataset2 & 7881.199898 & 7924.898873 & 7883.392124 & 7883.513937 & 7971.266962 & 7840.160175 & 7872.510695 & 7883.844403 & Gamma \\ \hline
Yahoo Dataset3 & 7671.958692 & 7741.421414 & 7736.935681 & 7736.096138 & 7792.164635 & 7737.110065 & 7719.499659 & 7737.251858 & Weibull \\ \hline
Yahoo Dataset4 & 7638.491386 & 7717.584150 & 7699.446572 & 7707.390329 & 7700.828675 & 7717.527590 & 7695.195926 & 7645.541288 & Weibull \\ \hline
Yahoo Dataset5 & 7639.309120 & 7708.325859 & 7698.398611 & 7677.397347 & 7719.253407 & 7707.972842 & 7688.031197 & 7621.003541 & Log-normal \\ \hline
Yahoo Dataset6 & 7641.909262 & 7710.022515 & 7698.491620 & 7681.587952 & 7724.924571 & 7709.047964 & 7688.655255 & 7707.841756 & Weibull \\ \hline
Yahoo Dataset7 & 9071.477536 & 9131.084275 & 9129.783141 & 9110.768841 & 9147.926649 & 9130.658989 & 9111.417344 & 9074.442695 & Weibull \\ \hline
Yahoo Dataset8 & 9071.761390 & 9132.947306 & 9131.890212 & 9107.104516 & 9122.839026 & 9132.926306 & 9114.783171 & 9068.081648 & Log-normal \\ \hline
Yahoo Dataset9 & 9065.288095 & 9130.157411 & 9130.532521 & 9099.498715 & 9126.102500 & 9129.759400 & 9110.597508 & 9076.250600 & Weibull \\ \hline
AWS DatasetSyn1 & 5652.978071 & 5667.713325 & 5744.095329 & 5655.130017 & 5710.226489 & 5654.119546 & 5639.989103 & 5655.305886 & Exponential \\ \hline
AWS DatasetSyn2 & 4895.976572 & 4972.803778 & 4958.736535 & 4960.013364 & 5014.276263 & 4958.905523 & 4944.673665 & 4960.186942 & Weibull \\ \hline
AWS DatasetSyn3 & 8067.686173 & 8139.900936 & 8128.827026 & 8129.597376 & 8186.742818 & 8047.338520 & 8113.957778 & 8129.704306 & Gamma \\ \hline
Yahoo DatasetSyn1 & 5455.597358 & 5525.997400 & 5522.980127 & 5520.544740 & 5575.116569 & 5522.922286 & 5505.332316 & 5522.711474 & Weibull \\ \hline
Yahoo DatasetSyn2 & 7896.483535 & 7974.411719 & 7903.969222 & 7900.691552 & 7986.510166 & 7839.850372 & 7896.991712 & 7897.013108 & Gamma \\ \hline
Yahoo DatasetSyn3 & 7747.559391 & 7815.078808 & 7806.785815 & 7806.741422 & 7862.437797 & 7806.900378 & 7790.414074 & 7806.805442 & Weibull \\ \hline
Yahoo DatasetSyn4 & 6073.974082 & 6146.487895 & 6133.485325 & 6135.639439 & 6167.233061 & 6144.339826 & 6124.086767 & 6086.204038 & Weibull \\ \hline
Yahoo DatasetSyn5 & 7694.331153 & 7763.787688 & 7753.164635 & 7750.872923 & 7781.890925 & 7763.620969 & 7742.798984 & 7753.443326 & Weibull \\ \hline
Yahoo DatasetSyn6 & 7696.882465 & 7765.339850 & 7753.582575 & 7732.150879 & 7780.305148 & 7764.001479 & 7743.736334 & 7762.738050 & Weibull \\ \hline
Yahoo DatasetSyn7 & 9126.286046 & 9186.193694 & 9184.951992 & 9159.005746 & 9203.290493 & 9185.884697 & 9166.525722 & 9185.309681 & Weibull \\ \hline
Yahoo DatasetSyn8 & 9126.738358 & 9188.004630 & 9186.863513 & 9162.410338 & 9178.064831 & 9187.984771 & 9169.762971 & 9123.068649 & Log-normal \\ \hline
Yahoo DatasetSyn9 & 9120.263541 & 9184.580872 & 9185.626482 & 9168.848880 & 9181.311224 & 9183.671291 & 9165.448029 & 9131.003884 & Weibull \\ \hline
\end{tabular}
}
\caption{Distribution identification of various univariate datasets based on chi-square; Main Text(Contribution) : Agnostic to Data distribution}
\label{table:3}
\end{table*}

\begin{table*}[t]
\centering
\begin{minipage}[t]{0.5\textwidth}
    \centering
    \setlength{\tabcolsep}{2mm}
    \tiny
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    \textbf{Dataset} & \textbf{Size} & \textbf{Dimension} & \textbf{\# Anomalies} & \textbf{\% Anomalies} & \textbf{Domain} \\ \hline
    ALOI & 49534 & 27 & 1508 & 3.04 & Image \\ \hline
    annthyroid & 7200 & 6 & 534 & 7.42 & Healthcare \\ \hline
    backdoor & 95329 & 196 & 2329 & 2.44 & Network \\ \hline
    breastw & 683 & 9 & 239 & 34.99 & Healthcare \\ \hline
    campaign & 41188 & 62 & 4640 & 11.27 & Finance \\ \hline
    cardio & 1831 & 21 & 176 & 9.61 & Healthcare \\ \hline
    Cardiotocography & 2114 & 21 & 466 & 22.04 & Healthcare \\ \hline
    celeba & 202599 & 39 & 4547 & 2.24 & Image \\ \hline
    cover & 286048 & 10 & 2747 & 0.96 & Botany \\ \hline
    donors & 619326 & 10 & 36710 & 5.93 & Sociology \\ \hline
    fault & 1941 & 27 & 673 & 34.67 & Physical \\ \hline
    fraud & 284807 & 29 & 492 & 0.17 & Finance \\ \hline
    glass & 214 & 7 & 9 & 4.21 & Forensic \\ \hline
    Hepatitis & 80 & 19 & 13 & 16.25 & Healthcare \\ \hline
    http & 567498 & 3 & 2211 & 0.39 & Web \\ \hline
    InternetAds & 1966 & 1555 & 368 & 18.72 & Image \\ \hline
    Ionosphere & 351 & 33 & 126 & 35.9 & Mineralogy \\ \hline
    landsat & 6435 & 36 & 1333 & 20.71 & Astronautics \\ \hline
    letter & 1600 & 32 & 100 & 6.25 & Image \\ \hline
    Lymphography & 148 & 18 & 6 & 4.05 & Healthcare \\ \hline
    magic.gamma & 19020 & 6 & 260 & 1.37 & Physical \\ \hline
    mammography & 11183 & 6 & 260 & 2.32 & Healthcare \\ \hline
    mnist & 7603 & 100 & 700 & 9.21 & Image \\ \hline
    musk & 3062 & 166 & 97 & 3.17 & Chemistry \\ \hline
    optdigits & 5216 & 64 & 150 & 2.88 & Image \\ \hline
    PageBlocks & 5393 & 10 & 510 & 9.46 & Document \\ \hline
    pendigits & 6870 & 16 & 156 & 2.27 & Image \\ \hline
    Pima & 768 & 8 & 268 & 34.9 & Healthcare \\ \hline
    satellite & 6435 & 36 & 2036 & 31.64 & Astronautics \\ \hline
    satimage-2 & 5803 & 36 & 71 & 1.22 & Astronautics \\ \hline
    shuttle & 49097 & 9 & 3511 & 7.15 & Astronautics \\ \hline
    skin & 245057 & 3 & 50859 & 20.75 & Image \\ \hline
    smtp & 95156 & 3 & 30 & 0.03 & Web \\ \hline
    SpamBase & 4207 & 57 & 1679 & 39.91 & Document \\ \hline
    speech & 3686 & 400 & 61 & 1.65 & Linguistics \\ \hline
    Stamps & 340 & 9 & 31 & 9.12 & Document \\ \hline
    thyroid & 3772 & 6 & 93 & 2.47 & Healthcare \\ \hline
    vertebral & 240 & 6 & 30 & 12.5 & Biology \\ \hline
    vowels & 1456 & 12 & 50 & 3.43 & Linguistics \\ \hline
    Waveform & 3443 & 21 & 100 & 2.9 & Physics \\ \hline
    WBC & 223 & 9 & 10 & 4.48 & Healthcare \\ \hline
    WDBC & 367 & 30 & 10 & 2.72 & Healthcare \\ \hline
    Wilt & 4819 & 5 & 257 & 5.33 & Botany \\ \hline
    wine & 129 & 13 & 10 & 7.75 & Chemistry \\ \hline
    WPBC & 198 & 33 & 47 & 23.74 & Healthcare \\ \hline
    yeast & 1484 & 8 & 507 & 34.16 & Biology \\ \hline
    CIFAR10 & 5263 & 512 & 263 & 5 & Image \\ \hline
    FashionMNIST & 6315 & 512 & 315 & 5 & Image \\ \hline
    MNIST-C & 10000 & 512 & 500 & 5 & Image \\ \hline
    MVTec-AD & 292 & 512 & 63 & 21.5 & Image \\ \hline
    SVHN & 5208 & 512 & 260 & 5 & Image \\ \hline
    Agnews & 10000 & 768 & 500 & 5 & NLP \\ \hline
    Amazon & 10000 & 768 & 500 & 5 & NLP \\ \hline
    Imdb & 10000 & 768 & 500 & 5 & NLP \\ \hline
    Yelp & 10000 & 768 & 500 & 5 & NLP \\ \hline
    20newsgroups & 3090 & 768 & 155 & 5 & NLP \\ \hline
    BATADAL 04 & 4177 & 43 & 219 & 5.24 & Industrial \\ \hline
    SWaT 1 & 50400 & 51 & 4466 & 8.86 & Industrial \\ \hline
    SWaT 2 & 86400 & 51 & 4216 & 4.88 & Industrial \\ \hline
    SWaT 3 & 86400 & 51 & 3075 & 3.56 & Industrial \\ \hline
    SWaT 4 & 86319 & 51 & 37559 & 43.51 & Industrial \\ \hline
    SWaT 5 & 86400 & 51 & 2167 & 2.51 & Industrial \\ \hline
    SWaT 6 & 54000 & 51 & 3138 & 5.81 & Industrial \\ \hline
    ecoli & 336 & 7 & 9 & 2.68 & Healthcare \\ \hline
    cmc & 1473 & 9 & 17 & 1.15 & Healthcare \\ \hline
    lympho h & 148 & 18 & 6 & 4.05 & Healthcare \\ \hline
    wbc h & 378 & 30 & 21 & 5.56 & Healthcare \\ \hline
    \end{tabular}
    \caption{Multivariate Datasets Characterisation}
    \label{table:2}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.5\textwidth}
    \centering
    \setlength{\tabcolsep}{2mm}
    \tiny
    \begin{tabular}{|l|c|l|c|}
    \hline
    \textbf{Dataset}     & \textbf{Optimal Threshold} & \textbf{Dataset}    & \textbf{Optimal Threshold} \\ \hline
    ALOI                & 0.007       & yahoo1          & 0.130 \\ \hline
    annthyroid          & 0.318       & yahoo2          & 0.594 \\ \hline
    backdoor            & 0.490       & yahoo3          & 0.415 \\ \hline
    breastw             & 0.751       & yahoo5          & 0.233 \\ \hline
    campaign            & 0.009       & yahoo6          & 0.127 \\ \hline
    cardio              & 0.445       & yahoo7          & 0.278 \\ \hline
    Cardiotocography    & 0.438       & yahoo8          & 0.243 \\ \hline
    celeba              & 0.006       & yahoo9          & 0.251 \\ \hline
    cover               & 0.0002      & Speed\_6005     & 0.483 \\ \hline
    donors              & 0.221       & Speed\_7578     & 0.463 \\ \hline
    fault               & 0.410       & Speed\_t4013    & 0.319 \\ \hline
    fraud               & 0.197       & TravelTime\_387 & 0.266 \\ \hline
    glass               & 0.502       & TravelTime\_451 & 0.151 \\ \hline
    Hepatitis           & 0.463       & Occupancy\_6005 & 0.217 \\ \hline
    http                & 0.928       & Occupancy\_t4013& 0.363 \\ \hline
    InternetAds         & 0.684       & yahoo\_syn1     & 0.464 \\ \hline
    Ionosphere          & 0.393       & yahoo\_syn2     & 0.396 \\ \hline
    landsat             & 0.253       & yahoo\_syn3     & 0.534 \\ \hline
    letter              & 0.695       & yahoo\_syn5     & 0.449 \\ \hline
    Lymphography        & 0.474       & yahoo\_syn6     & 0.421 \\ \hline
    magic.gamma         & 0.074       & yahoo\_syn7     & 0.437 \\ \hline
    mammography         & 0.220       & yahoo\_syn8     & 0.335 \\ \hline
    mnist               & 0.319       & yahoo\_syn9     & 0.457 \\ \hline
    musk                & 0.999       & aws1            & 0.125 \\ \hline
    optdigits           & 0.022       & aws2            & 0.444 \\ \hline
    PageBlocks          & 0.398       & aws3            & 0.238 \\ \hline
    pendigits           & 0.274       & aws\_syn1       & 0.387 \\ \hline
    Pima                & 0.455       & aws\_syn2       & 0.639 \\ \hline
    satellite           & 0.114       & aws\_syn3       & 0.398 \\ \hline
    satimage-2          & 0.222       &                 &       \\ \hline
    shuttle             & 0.861       &                 &       \\ \hline
    skin                & 0.005       &                 &       \\ \hline
    smtp                & 0.001       &                 &       \\ \hline
    SpamBase            & 0.409       &                 &       \\ \hline
    speech              & 0.361       &                 &       \\ \hline
    Stamps              & 0.487       &                 &       \\ \hline
    thyroid             & 0.341       &                 &       \\ \hline
    vertebral           & 0.446       &                 &       \\ \hline
    vowels              & 0.414       &                 &       \\ \hline
    Waveform            & 0.311       &                 &       \\ \hline
    WBC                 & 0.547       &                 &       \\ \hline
    WDBC                & 0.837       &                 &       \\ \hline
    Wilt                & 0.321       &                 &       \\ \hline
    wine                & 0.436       &                 &       \\ \hline
    WPBC                & 0.417       &                 &       \\ \hline
    yeast               & 0.406       &                 &       \\ \hline
    CIFAR10             & 0.325       &                 &       \\ \hline
    FashionMNIST        & 0.409       &                 &       \\ \hline
    MNIST-C             & 0.004       &                 &       \\ \hline
    MVTec-AD            & 0.549       &                 &       \\ \hline
    SVHN                & 0.504       &                 &       \\ \hline
    Agnews              & 0.041       &                 &       \\ \hline
    Amazon              & 0.103       &                 &       \\ \hline
    Imdb                & 0.061       &                 &       \\ \hline
    Yelp                & 0.170       &                 &       \\ \hline
    20newsgroups        & 0.139       &                 &       \\ \hline
    BATADAL\_04         & 0.413       &                 &       \\ \hline
    SWaT 1              & 0.006       &                 &       \\ \hline
    SWaT 2              & 0.002       &                 &       \\ \hline
    SWaT 3              & 0.003       &                 &       \\ \hline
    SWaT 4              & 0.005       &                 &       \\ \hline
    SWaT 5              & 0.004       &                 &       \\ \hline
    SWaT 6              & 0.010       &                 &       \\ \hline
    ecoli               & 0.505       &                 &       \\ \hline
    cmc                 & 0.458       &                 &       \\ \hline
    lympho h            & 0.381       &                 &       \\ \hline
    wbc h               & 0.348       &                 &       \\ \hline
    \end{tabular}
    \caption{Optimal Thresholds for Various Datasets; Main Text(Contribution) : Automated Hyperparameter Tuning}
    \label{table:5}
\end{minipage}
\end{table*}



\begin{table*}[t]
\centering
\begin{minipage}[t]{0.45\textwidth}
    \centering
    \setlength{\tabcolsep}{0.5mm}
    \small
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \textbf{Dataset} & \textbf{Size} & \textbf{\# Anomalies} & \textbf{\% Anomalies} & \textbf{Domain} \\ \hline
    yahoo1 & 1420 & 2 & 0.14 & Industrial \\ \hline
    yahoo2 & 1461 & 8 & 0.55 & Industrial \\ \hline
    yahoo3 & 1439 & 8 & 0.56 & Industrial \\ \hline
    yahoo5 & 1421 & 9 & 0.63 & Industrial \\ \hline
    yahoo6 & 1421 & 4 & 0.28 & Industrial \\ \hline
    yahoo7 & 1680 & 11 & 0.65 & Industrial \\ \hline
    yahoo8 & 1680 & 10 & 0.60 & Industrial \\ \hline
    yahoo9 & 1680 & 8 & 0.48 & Industrial \\ \hline
    Speed 6005 & 2500 & 1 & 0.04 & Non-Industrial \\ \hline
    Speed 7578 & 1127 & 4 & 0.35 & Non-Industrial \\ \hline
    Speed t4013 & 2495 & 2 & 0.08 & Non-Industrial \\ \hline
    TravelTime 387 & 2500 & 3 & 0.12 & Non-Industrial \\ \hline
    TravelTime 451 & 2162 & 1 & 0.05 & Non-Industrial \\ \hline
    Occupancy 6005 & 2380 & 1 & 0.04 & Non-Industrial \\ \hline
    Occupancy t4013 & 2500 & 2 & 0.08 & Non-Industrial \\ \hline
    yahoo syn1 & 1420 & 12 & 0.85 & Industrial \\ \hline
    yahoo syn2 & 1461 & 18 & 1.23 & Industrial \\ \hline
    yahoo syn3 & 1449 & 18 & 1.24 & Industrial \\ \hline
    yahoo syn5 & 1431 & 19 & 1.33 & Industrial \\ \hline
    yahoo syn6 & 1431 & 14 & 0.98 & Industrial \\ \hline
    yahoo syn7 & 1690 & 21 & 1.24 & Industrial \\ \hline
    yahoo syn8 & 1690 & 20 & 1.18 & Industrial \\ \hline
    yahoo syn9 & 1690 & 18 & 1.07 & Industrial \\ \hline
    aws1 & 1049 & 1 & 0.10 & Industrial \\ \hline
    aws2 & 2486 & 2 & 0.08 & Industrial \\ \hline
    aws3 & 1499 & 1 & 0.07 & Industrial \\ \hline
    aws syn1 & 1049 & 10 & 1.05 & Industrial \\ \hline
    aws syn2 & 2486 & 20 & 0.88 & Industrial \\ \hline
    aws syn3 & 1499 & 10 & 0.73 & Industrial \\ \hline
    \end{tabular}
    \caption{Univariate Datasets Characterisation}
    \label{table:1}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}
    \centering
    \setlength{\tabcolsep}{0.9mm}
    \small
	\begin {tabular}{|l| * {3}{|c|}}
        \hline
	\textbf{Dataset} & \textbf{OCSVM} & \textbf{RN-Net}\\
        \hline
	yahoo1 & 1 & 1 \\
	yahoo2 & 1 & 1 \\
	yahoo3 & 1 & 1 \\
	yahoo5 & 1 & 1 \\
	yahoo6 & 1 & 1 \\
	yahoo7 & \textbf{0.91} & 0.87 \\
	yahoo8 & 0.4 & \textbf{0.86} \\
	yahoo9 & 1 & 1 \\
	Speed\_6005 & 1 & 1 \\
	Speed\_7578 & 1 & 1 \\
	Speedt\_4013 & 1 & 1 \\
	TravelTime\_387 & 0.67 & \textbf{1} \\
	TravelTime\_451 & 1 & 1 \\
	Occupancy\_6005 & 1 & 1 \\
	Occupancy\_t4013 & 1 & 1 \\
	yahoo\_syn1 & 1 & 1 \\
	yahoo\_syn2 & 1 & 1 \\
	yahoo\_syn3 & 1 & 1 \\
	yahoo\_syn5 & 1 & 1 \\
	yahoo\_syn6 & 1 & 1 \\
	yahoo\_syn7 & \textbf{0.95} & 0.90 \\
	yahoo\_syn8 & 0.5 & \textbf{0.93} \\
	yahoo\_syn9 & 0.5 & \textbf{1} \\
	aws1 & 1 & 1 \\
	aws2 & 1 & 1 \\
	aws3 & 1 & 1 \\
	aws\_syn1 & 1 & 1 \\
	aws\_syn2 & 1 & 1 \\
	aws\_syn3 & 1 & 1 \\
        \hline
	\end{tabular}
	\caption{OCSVM and Custom Neural Network Recall Values comparison on 29 Univariate datasets}
 \label{table:6}
\end{minipage}
\end{table*}
 


\begin{table*}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{\% Anomaly} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} & \textbf{AUC-ROC} \\ \hline
breastw & 34.99 & 0.95 & 0.96 & 0.96 & 0.97 \\ \hline
campaign & 11.27 & 0.65 & 0.31 & 0.42 & 0.65 \\ \hline
Cardiotocography & 22.04 & 0.82 & 0.77 & 0.80 & 0.86 \\ \hline
fault & 34.67 & 0.75 & 0.35 & 0.47 & 0.64 \\ \hline
Hepatitis & 16.25 & 0.91 & 0.77 & 0.83 & 0.88 \\ \hline
InternetAds & 18.72 & 0.98 & 0.91 & 0.94 & 0.95 \\ \hline
Ionosphere & 35.9 & 0.92 & 0.74 & 0.82 & 0.85 \\ \hline
landsat & 20.71 & 0 & 0 & 0 & 0.50 \\ \hline
Pima & 34.9 & 0.70 & 0.59 & 0.64 & 0.73 \\ \hline
satellite & 31.64 & 0.94 & 0.66 & 0.77 & 0.82 \\ \hline
skin & 20.75 & 0 & 0 & 0 & 0.50 \\ \hline
SpamBase & 39.91 & 0.92 & 0.89 & 0.91 & 0.92 \\ \hline
vertebral & 12.5 & 0.82 & 0.47 & 0.60 & 0.73 \\ \hline
WPBC & 23.74 & 0.78 & 0.15 & 0.25 & 0.57 \\ \hline
yeast & 34.16 & 0 & 0 & 0 & 0.50 \\ \hline
MVTec-AD & 21.5 & 0.98 & 0.89 & 0.93 & 0.94 \\ \hline
SWaT4 & 43.51 & 0.99 & 0.97 & 0.98 & 0.98 \\ \hline
\end{tabular}
\caption{Performance Metrics of Linear SVM; Main Text(SoTA Algorithm Landscape)}
\label{table:11}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{|c|l|l|l|l|l|}
\hline
Dataset          & Deep-Sad      & FTT           & PReNet        & RN-Net        & RN-LSTM       \\
\hline
yahoo1           & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.02          \\
yahoo2           & \textbf{0.88} & \textbf{0.88} & \textbf{0.88} & \textbf{0.88} & 0.05          \\
yahoo3           & 0.00          & \textbf{1.00} & 0.07          & \textbf{1.00} & 0.00          \\
yahoo5           & 0.00          & 0.01          & 0.00          & \textbf{0.08} & 0.03          \\
yahoo6           & 0.00          & 0.50          & 0.00          & \textbf{1.00} & 0.09          \\
yahoo7           & 0.01          & 0.02          & 0.01          & 0.06          & \textbf{0.15} \\
yahoo8           & 0.00          & 0.01          & 0.01          & \textbf{1.00} & 0.13          \\
yahoo9           & 0.00          & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.09          \\
Speed  6005      & NA            & NA            & NA            & \textbf{0.50} & 0.00          \\
Speed  7578      & 0.12          & 0.12          & 0.12          & \textbf{0.17} & 0.01          \\
Speed  t4013     & 0.00          & \textbf{1.00} & 0.50          & \textbf{1.00} & 0.02          \\
TravelTime  387  & 0.00          & 0.05          & 0.05          & \textbf{0.16} & 0.04          \\
TravelTime  451  & NA            & NA            & NA            & \textbf{1.00} & 0.00          \\
Occupancy  6005  & NA            & NA            & NA            & \textbf{1.00} & 0.01          \\
Occupancy  t4013 & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.01          \\
yahoo  syn1      & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.04          \\
yahoo  syn2      & 0.94          & 0.94          & 0.94          & 0.94          & \textbf{1.00} \\
yahoo  syn3      & 0.00          & 0.94          & 0.19          & \textbf{1.00} & \textbf{1.00} \\
yahoo  syn5      & 0.00          & \textbf{0.31} & 0.28          & 0.26          & 0.07          \\
yahoo  syn6      & 0.00          & 0.05          & 0.04          & \textbf{1.00} & 0.14          \\
yahoo  syn7      & 0.01          & 0.03          & 0.01          & 0.12          & \textbf{0.15} \\
yahoo  syn8      & 0.00          & 0.00          & 0.00          & \textbf{1.00} & 0.14          \\
yahoo  syn9      & 0.00          & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.43          \\
aws1             & NA            & NA            & NA            & \textbf{1.00} & 0.00          \\
aws2             & 0.10          & 0.10          & 0.10          & \textbf{1.00} & 0.01          \\
aws3             & NA            & NA            & NA            & \textbf{1.00} & 0.00          \\
aws  syn1        & 0.00          & \textbf{0.90} & 0.82          & \textbf{0.90} & 0.50          \\
aws  syn2        & 0.66          & 0.66          & 0.66          & 0.81          & \textbf{0.94} \\
aws  syn3        & 0.00          & \textbf{0.90} & 0.82          & \textbf{0.90} & 0.88 \\
\bottomrule
\end{tabular}
\caption{Precision comparison of RN-Net with recent models such as on Univariate datasets}
\label{tab:precision-comparison-univariate}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{|c|l|l|l|l|l|}
\hline
Dataset          & Deep-Sad      & FTT           & PReNet & RN-Net            & RN-LSTM       \\
\hline
yahoo1           & 0.50          & 0.50          & 0.50   & \textbf{1.00} & \textbf{1.00} \\
yahoo2           & 0.88          & 0.88          & 0.88   & \textbf{1.00} & 0.29          \\
yahoo3           & 0.00          & 0.75          & 0.88   & \textbf{1.00} & 0.00          \\
yahoo5           & 0.00          & 0.67          & 0.22   & 0.67          & \textbf{1.00} \\
yahoo6           & 0.00          & 0.50          & 0.00   & \textbf{1.00} & \textbf{1.00} \\
yahoo7           & \textbf{0.91} & 0.82          & 0.55   & 0.80          & 0.30          \\
yahoo8           & 0.00          & 0.80          & 0.80   & \textbf{0.89} & 0.11          \\
yahoo9           & 0.00          & 0.88          & 0.50   & \textbf{1.00} & 0.86          \\
Speed  6005      & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
Speed  7578      & \textbf{1.00} & \textbf{1.00} & 0.00   & 0.75          & \textbf{1.00} \\
Speed  t4013     & \textbf{1.00} & \textbf{1.00} & 0.00   & \textbf{1.00} & 0.50          \\
TravelTime  387  & 0.67          & 0.67          & 0.33   & 0.67          & \textbf{1.00} \\
TravelTime  451  & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
Occupancy  6005  & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
Occupancy  t4013 & 0.50          & 0.50          & 0.50   & \textbf{1.00} & \textbf{1.00} \\
yahoo  syn1      & 0.92          & 0.92          & 0.92   & \textbf{1.00} & 0.91          \\
yahoo  syn2      & 0.94          & 0.94          & 0.94   & \textbf{1.00} & 0.81          \\
yahoo  syn3      & 0.00          & 0.89          & 0.94   & \textbf{1.00} & 0.94          \\
yahoo  syn5      & 0.00          & 0.79          & 0.79   & \textbf{1.00} & 0.65          \\
yahoo  syn6      & 0.00          & 0.64          & 0.64   & \textbf{0.91} & 0.42          \\
yahoo  syn7      & \textbf{0.95} & 0.90          & 0.19   & 0.78          & 0.94          \\
yahoo  syn8      & 0.00          & 0.00          & 0.00   & \textbf{0.94} & 0.18          \\
yahoo  syn9      & 0.00          & 0.94          & 0.78   & \textbf{1.00} & 0.94          \\
aws1             & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
aws2             & 0.50          & 0.50          & 0.50   & 0.50          & \textbf{1.00} \\
aws3             & NA            & NA            & NA     & \textbf{1.00} & \textbf{1.00} \\
aws  syn1        & 0.00          & 0.90          & 0.90   & \textbf{1.00} & 0.78          \\
aws  syn2        & 0.95          & 0.95          & 0.95   & \textbf{1.00} & 0.94          \\
aws  syn3        & 0.00          & 0.90          & 0.90   & \textbf{1.00} & 0.78  \\
\bottomrule
\end{tabular}
\caption{Recall comparison of RN-Net with recent models such as on Univariate datasets}
\label{tab:recall-comparison-univariate}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{|c|l|l|l|l|l|}
\hline
Dataset          & Deep-Sad      & FTT           & PReNet & RN-Net            & RN-LSTM       \\
\hline
yahoo1           & 0.67          & 0.67          & 0.67   & \textbf{1.00} & 0.03 \\
yahoo2           & 0.88          & 0.88          & 0.88   & \textbf{0.94} & 0.09          \\
yahoo3           & 0.00          & 0.86          & 0.13   & \textbf{1.00} & 0.00          \\
yahoo5          & 0.00          & 0.01          & 0.01   & \textbf{0.15} & 0.05\\
yahoo6           & 0.00          & 0.50          & 0.00   & \textbf{1.00} & 0.16 \\
yahoo7           & 0.01 & 0.03          & 0.01   & 0.11          & \textbf{0.20} \\
yahoo8           & 0.00          & 0.02          & 0.02   & \textbf{0.94} & 0.12          \\
yahoo9           & 0.00          & 0.93          & 0.67   & \textbf{1.00} & 0.16          \\
Speed  6005      & NA            & NA            & NA     & \textbf{0.67} & 0.01 \\
Speed  7578      & \textbf{0.26} & \textbf{0.26} & 0.00   & \textbf{0.27} & 0.02 \\
Speed  t4013     & 0.80 & 0.80 & 0.00   & \textbf{1.00} & 0.04          \\
TravelTime  387  & 0.00          & 0.08          & 0.08   & \textbf{0.26} & 0.07 \\
TravelTime  451  & NA            & NA            & NA     & \textbf{1.00} & 0.00 \\
Occupancy  6005  & NA            & NA            & NA     & \textbf{1.00} & 0.01 \\
Occupancy  t4013 & 0.67          & 0.67          & 0.67   & \textbf{1.00} & 0.01 \\
yahoo  syn1      & 0.96          & 0.96          & 0.96   & \textbf{1.00} & 0.09          \\
yahoo  syn2      & 0.94          & 0.94          & 0.94   & \textbf{0.97} & 0.89          \\
yahoo  syn3      & 0.00          & 0.91          & 0.31   & \textbf{1.00} & 0.97          \\
yahoo  syn5      & 0.00          & \textbf{0.44} & 0.42   & 0.41 & 0.12          \\
yahoo  syn6      & 0.00          & 0.09          & 0.08   & \textbf{0.95} & 0.21          \\
yahoo  syn7      & 0.02 & 0.07          & 0.02   & 0.20          & \textbf{0.26} \\
yahoo  syn8      & 0.00          & 0.00          & 0.00   & \textbf{0.97} & 0.15          \\
yahoo  syn9      & 0.00          & 0.97          & 0.88   & \textbf{1.00} & 0.59          \\
aws1             & NA            & NA            & NA     & \textbf{1.00} & 0.01 \\
aws2             & 0.17          & 0.17          & 0.17   & \textbf{0.67} & 0.01 \\
aws3             & NA            & NA            & NA     & \textbf{1.00} & 0.01 \\
aws  syn1        & 0.00          & 0.90          & 0.86   & \textbf{0.95} & 0.61          \\
aws  syn2        & 0.78          & 0.78          & 0.78   & 0.89 & \textbf{0.94} \\
aws  syn3        & 0.00          & 0.90          & 0.86   & \textbf{0.95} & 0.82 \\
\bottomrule
\end{tabular}
\caption{F-1 Score comparison of RN-Net with recent models such as on Univariate datasets}
\label{tab:f1-comparison-univariate}
\end{table*}

\begin{table*}[t]
\centering
\begin{tabular}{|c|l|l|l|l|l|}
\hline
Dataset          & Deep-Sad      & FTT           & PReNet        & RN-Net            & RN-LSTM       \\
\hline
yahoo1           & 1.00          & \textbf{1.00} & \textbf{1.00} & \textbf{1}    & 0.71 \\
yahoo2           & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{0.99} & 0.39          \\
yahoo3           & 0.02          & 0.92          & 0.99          & \textbf{1}    & 0.1           \\
yahoo5           & 0.33          & 0.63          & 0.25          & \textbf{0.65} & 0.4  \\
yahoo6           & 0.00          & 1.00          & 0.00          & \textbf{1}    & 0.96 \\
yahoo7           & 0.17 & \textbf{0.84} & 0.53          & 0.82          & 0.49 \\
yahoo8           & 0.30          & 0.68          & 0.68          & \textbf{0.97} & 0.21          \\
yahoo9           & 0.00          & \textbf{1.00} & 0.63          & \textbf{1}    & 0.91          \\
Speed  6005      & NA            & NA            & NA            & \textbf{0.99} & 0.47 \\
Speed  7578      & \textbf{0.99} & \textbf{0.99} & \textbf{1.00} & 0.84 & 0.03 \\
Speed  t4013     & \textbf{0.99} & \textbf{1.00} & \textbf{1.00} & \textbf{1}    & 0.63          \\
TravelTime  387  & 0.59          & 0.77          & 0.80          & 0.74 & \textbf{0.93} \\
TravelTime  451  & NA            & NA            & NA            & \textbf{1}    & 0.33 \\
Occupancy  6005  & NA            & NA            & NA            & \textbf{1}    & 0.74 \\
Occupancy  t4013 & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1}    & 0.65 \\
yahoo  syn1      & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{1}    & 0.57          \\
yahoo  syn2      & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{0.99} & 0.94          \\
yahoo  syn3      & 0.01          & 0.99          & \textbf{1.00}          & \textbf{1}    & 0.96          \\
yahoo  syn5      & 0.17          & 0.92 & 0.91          & \textbf{0.98} & 0.52          \\
yahoo  syn6      & 0.17          & 0.63          & 0.62          & \textbf{0.98} & 0.58          \\
yahoo  syn7      & 0.11 & 0.84          & 0.32          & 0.84          & \textbf{0.91} \\
yahoo  syn8      & 0.27          & 0.28          & 0.25          & \textbf{0.96} & 0.46          \\
yahoo  syn9      & 0.00          & 1.00          & 0.83          & \textbf{1}    & 0.99          \\
aws1             & NA            & NA            & NA            & \textbf{1}    & 0.07 \\
aws2             & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.5  & 0.81 \\
aws3             & NA            & NA            & NA            & \textbf{1}    & 0.05 \\
aws  syn1        & 0.00          & \textbf{1.00} & \textbf{1.00}          & \textbf{0.99} & 0.86          \\
aws  syn2        & \textbf{1.00}          & \textbf{1.00} & \textbf{1.00}          & \textbf{0.99} & 0.98 \\
aws  syn3        & 0.00          & \textbf{1.00} & \textbf{1.00}          & \textbf{0.99} & 0.88  \\
\bottomrule
\end{tabular}
\caption{AUC-ROC comparison of RN-Net with recent models such as on Univariate datasets}
\label{tab:aucroc-comparison-univariate}
\end{table*}


\begin{table*}[t]
\centering
\scriptsize 
\setlength{\tabcolsep}{2mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|l|l|l|l|}
\hline
Dataset          & Deep-Sad      & FTT           & PReNet        & RN-Net            \\
\hline
ALOI             & 0.04          & 0.04          & 0.03          & \textbf{0.29} \\
annthyroid       & 0.46          & 0.53          & 0.06          & \textbf{0.73} \\
backdoor         & NA            & NA            & NA            & \textbf{0.65} \\
breastw          & 0.88          & 0.94          & \textbf{0.97} & 0.97          \\
campaign         & 0.23          & 0.22          & 0.17          & \textbf{0.76} \\
cardio           & 0.40          & 0.39          & 0.34          & \textbf{0.90} \\
Cardiotocography & 0.34          & 0.66          & 0.72          & \textbf{0.88} \\
celeba           & 0.08          & 0.12          & 0.13          & \textbf{0.26} \\
cover            & 0.05          & 0.03          & 0.00          & \textbf{1.00} \\
donors           & \textbf{1.00} & NA            & 0.59          & 0.90          \\
fault            & 0.57          & 0.45          & 0.69          & \textbf{0.89} \\
fraud            & 0.05          & \textbf{0.36} & 0.01          & 0.19          \\
glass            & 0.08          & 0.14          & 0.05          & \textbf{0.80} \\
Hepatitis        & 0.57          & 0.41          & 0.63          & \textbf{0.71} \\
http             & 0.00          & NA            & 0.04          & \textbf{0.92} \\
InternetAds      & 0.52          & NA            & 0.65          & \textbf{0.84} \\
Ionosphere       & 0.78          & 0.63          & 0.83          & \textbf{0.96} \\
landsat          & 0.56          & 0.68          & \textbf{0.78} & 0.76          \\
letter           & NA            & NA            &               & \textbf{0.30} \\
Lymphography     & 0.31          & 0.56          & 0.33          & \textbf{0.67} \\
magic.gamma      & 0.72          & 0.68          & 0.62          & \textbf{0.91} \\
mammography      & 0.03          & 0.06          & 0.00          & \textbf{0.44} \\
mnist            & 0.28          & 0.54          & 0.35          & \textbf{0.89} \\
musk             & 0.27          & \textbf{1.00} & 0.03          & 0.91          \\
optdigits        & 0.53          & \textbf{1.00} & 0.28          & \textbf{1.00} \\
PageBlocks       & 0.11          & 0.22          & 0.17          & \textbf{1.00} \\
pendigits        & 0.33          & 0.71          & 0.22          & \textbf{0.84} \\
Pima             & 0.51          & 0.48          & \textbf{0.57} & 0.40          \\
satellite        & 0.62          & \textbf{0.91} & 0.80          & 0.84          \\
satimage-2       & 0.77          & 0.73          & 0.11          & \textbf{0.95} \\
shuttle          & 0.80          & \textbf{0.91} & 0.67          & 0.64          \\
skin             & 0.99          & 0.94          & 0.89          & \textbf{1.00} \\
smtp             & \textbf{1.00} & 0.53          & 0.00          & 1.00          \\
SpamBase         & 0.92          & 0.86          & 0.81          & \textbf{1.00} \\
speech           & 0.12          & 0.02          & 0.04          & \textbf{0.93} \\
Stamps           & 0.39          & \textbf{0.81} & 0.71          & 0.06          \\
thyroid          & 0.29          & 0.36          & 0.25          & \textbf{0.70} \\
vertebral        & 0.14          & 0.21          & 0.58          & \textbf{0.75} \\
vowels           & 0.30          & 0.24          & 0.31          & \textbf{0.82} \\
Waveform         & 0.14          & 0.07          & 0.08          & \textbf{0.67} \\
WBC              & 0.30          & \textbf{0.50} & 0.43          & 0.36          \\
WDBC             & 0.83          & \textbf{0.90} & 0.27          & 0.56          \\
Wilt             & 0.08          & 0.09          & 0.14          & \textbf{1.00} \\
wine             & \textbf{0.90} & \textbf{0.90} & 0.69          & 0.47          \\
WPBC             & 0.40          & 0.57          & 0.55          & \textbf{1.00} \\
yeast            & 0.36          & 0.46          & 0.34          & \textbf{0.82} \\
CIFAR10          & 0.10          & 0.08          & 0.06          & \textbf{0.81} \\
FashionMNIST     & 0.22          & 0.15          & 0.12          & \textbf{0.33} \\
MNIST-C          & 0.49          & 0.31          & 0.27          & \textbf{0.97} \\
MVTec-AD         & 0.93          & \textbf{1.00} & \textbf{1.00} & 0.96          \\
SVHN             & 0.14          & 0.07          & 0.09          & \textbf{0.49} \\
Agnews           & 0.09          & 0.05          & 0.16          & \textbf{0.33} \\
Amazon           & 0.01          & 0.06          & 0.18          & \textbf{0.23} \\
Imdb             & 0.09          & 0.00          & 0.09          & \textbf{0.36} \\
Yelp             & 0.13          & 0.11          & 0.08          & \textbf{0.36} \\
20newsgroups     & 0.08          & 0.05          & 0.14          & \textbf{0.41} \\
BATADAL  04      & 0.13          & 0.07          & 0.17          & \textbf{0.17} \\
SWaT  1          & \textbf{0.51} & 0.17          & 0.21          & 0.40          \\
SWaT  2          & 0.07          & 0.25          & 0.08          & \textbf{0.33} \\
SWaT  3          & NA            & NA            & NA            & \textbf{0.66} \\
SWaT  4          & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.85          \\
SWaT  5          & 0.15          & 0.17          & 0.03          & \textbf{0.36} \\
SWaT  6          & 0.32          & \textbf{0.92} & 0.33          & 0.21          \\
ecoli            & 0.12          & 0.21          & 0.21          & \textbf{0.44} \\
cmc              & 0.03          & 0.02          & 0.03          & \textbf{0.15} \\
lympho  h        & 0.43          & 0.33          & 0.40          & \textbf{0.64} \\
wbc  h           & 0.06          & 0.38          & 0.37          & \textbf{0.85}\\
\bottomrule
\end{tabular}
}
\caption{Precision comparison of RN-Net with recent models such as on Multivariate datasets}
\label{tab:precision-comparison-multivariate}
\end{table*}


\begin{table*}[t]
\centering
\scriptsize 
\setlength{\tabcolsep}{2mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|l|l|l|l|}
\hline
Dataset          & Deep-Sad      & FTT           & PReNet        & RN-Net            \\
\hline
ALOI             & 0.16          & 0.18          & 0.11          & \textbf{0.57} \\
annthyroid       & 0.62          & 0.09          & 0.08          & \textbf{0.83} \\
backdoor         & NA            & NA            & NA            & \textbf{0.98} \\
breastw          & 0.95          & \textbf{0.98} & 0.28 & 0.95          \\
campaign         & \textbf{0.57} & 0.19          & 0.15          & 0.51 \\
cardio           & 0.78          & 0.41          & 0.36          & \textbf{0.89} \\
Cardiotocography & 0.05          & 0.29          & 0.33          & \textbf{0.70} \\
celeba           & 0.74          & 0.52          & 0.58          & \textbf{0.90} \\
cover            & 0.25          & 0.91          & 0.04          & \textbf{1.00} \\
donors           & \textbf{0.99} & NA            & \textbf{1.00} & 1.00          \\
fault            & \textbf{0.62} & 0.59          & 0.20          & 0.44 \\
fraud            & \textbf{0.84} & 0.80 & 0.83          & 0.83          \\
glass            & 0.67          & 0.67          & 0.11          & \textbf{1.00} \\
Hepatitis        & \textbf{1.00} & 0.69          & 0.38          & 0.83 \\
http             & \textbf{1.00} & NA            & \textbf{1.00} & 0.95 \\
InternetAds      & 0.85          & NA            & 0.35          & \textbf{0.86} \\
Ionosphere       & \textbf{0.91} & 0.84          & 0.24          & 0.89 \\
landsat          & 0.78          & 0.46          & 0.38 & \textbf{0.80} \\
letter           & NA            & NA            & NA            & \textbf{0.80} \\
Lymphography     & 0.83          & \textbf{0.83} & \textbf{0.83} & 0.67 \\
magic.gamma      & \textbf{0.79} & 0.73          & 0.18          & 0.45 \\
mammography      & 0.55          & 0.25          & 0.01          & \textbf{0.71} \\
mnist            & 0.85          & 0.88          & 0.38          & \textbf{0.94} \\
musk             & 0.97          & \textbf{0.99} & \textbf{1.00} & 0.90          \\
optdigits        & 0.97          & 0.98 & 0.99          & \textbf{1.00} \\
PageBlocks       & 0.89          & 0.80          & 0.18          & \textbf{1.00} \\
pendigits        & \textbf{0.99} & 0.97          & 0.97          & 0.64 \\
Pima             & 0.73          & 0.63          & 0.16 & \textbf{0.98} \\
satellite        & 0.56          & \textbf{0.64} & 0.25          & 0.48          \\
satimage-2       & \textbf{0.93} & 0.92          & \textbf{0.93}          & 0.90 \\
shuttle          & 0.48          & 0.97 & 0.94          & \textbf{1.00} \\
skin             & 0.99          & 0.45          & 0.43          & \textbf{1.00} \\
smtp             & 0.57 & 0.63          & \textbf{0.67} & 0.62          \\
SpamBase         & 0.06          & 0.21          & 0.20          & \textbf{0.72} \\
speech           & \textbf{0.97} & 0.57          & 0.25          & 0.78 \\
Stamps           & 0.77          & \textbf{0.81} & 0.77          & 0.71          \\
thyroid          & 0.91          & 0.99          & \textbf{1.00} & 0.96 \\
vertebral        & 0.93          & 0.53          & 0.47          & \textbf{1.00} \\
vowels           & 0.78          & 0.74          & \textbf{0.90} & 0.69 \\
Waveform         & 0.51          & 0.13          & 0.27          & \textbf{1.00} \\
WBC              & 0.90          & 0.90 & \textbf{1.00} & 0.58          \\
WDBC             & \textbf{1.00} & 0.90 & \textbf{1.00} & \textbf{1.00} \\
Wilt             & 0.51          & 0.86          & 0.26          & \textbf{1.00} \\
wine             & \textbf{0.90} & \textbf{0.90} & \textbf{0.90} & 0.78          \\
WPBC             & 0.72          & 0.26          & 0.23          & \textbf{1.00} \\
yeast            & \textbf{0.82} & 0.50          & 0.10          & 0.80 \\
CIFAR10          & 0.61          & 0.36          & 0.13          & \textbf{0.74} \\
FashionMNIST     & \textbf{0.80} & 0.59          & 0.24          & 0.63 \\
MNIST-C          & 0.88          & 0.83          & 0.54          & \textbf{0.99} \\
MVTec-AD         & 0.62          & 0.48 & 0.48 & \textbf{0.91} \\
SVHN             & 0.50          & 0.46          & 0.18          & \textbf{0.59} \\
Agnews           & 0.59          & \textbf{1.00} & 0.33          & 0.73 \\
Amazon           & 0.21          & 0.35          & 0.36          & \textbf{0.54} \\
Imdb             & 0.19          & 0.00          & 0.19          & \textbf{0.64} \\
Yelp             & 0.25          & 0.09          & 0.17          & \textbf{0.51} \\
20newsgroups     & 0.71          & \textbf{0.95} & 0.27          & 0.54 \\
BATADAL  04      & \textbf{0.76} & 0.42          & 0.32          & 0.37 \\
SWaT  1          & 0.59 & \textbf{0.91} & 0.24          & 0.40          \\
SWaT  2          & 0.75          & \textbf{0.92} & 0.17          & 0.31 \\
SWaT  3          & NA            & NA            & NA            & \textbf{0.79} \\
SWaT  4          & 0.90 & \textbf{0.98} & 0.23 & 0.04          \\
SWaT  5          & 0.40          & \textbf{0.94} & 0.11          & 0.55 \\
SWaT  6          & 0.79          & \textbf{1.00} & 0.56          & 0.72          \\
ecoli            & 0.67          & 0.78          & 0.78 & \textbf{1.00} \\
cmc              & 0.94          & 0.65          & 0.29          & \textbf{1.00} \\
lympho  h        & \textbf{1.00} & 0.67          & \textbf{1.00} & 0.80 \\
wbc  h           & \textbf{1.00} & 0.86          & 0.67          & \textbf{1.00}\\
\bottomrule
\end{tabular}
}
\caption{Recall comparison of RN-Net with recent models such as on Multivariate datasets}
\label{tab:recall-comparison-multivariate}
\end{table*}

\begin{table*}[t]
\centering
\scriptsize 
\setlength{\tabcolsep}{2mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|l|l|l|l|}
\hline
Dataset          & Deep-Sad      & FTT           & PReNet        & RN-Net            \\
\hline
ALOI             & 0.07          & 0.06          & 0.05          & \textbf{0.39} \\
annthyroid       & 0.39          & 0.53          & 0.07          & \textbf{0.78} \\
backdoor         & NA            & NA            & NA            & \textbf{0.78} \\
breastw          & 0.92          & \textbf{0.96} & 0.44 & 0.96          \\
campaign         & 0.33 & 0.20          & 0.16          & \textbf{0.61} \\
cardio           & 0.53          & 0.40          & 0.35          & \textbf{0.90} \\
Cardiotocography & 0.09          & 0.41          & 0.45          & \textbf{0.78} \\
celeba           & 0.15          & 0.19          & 0.21          & \textbf{0.41} \\
cover            & 0.08          & 0.06          & 0.01          & \textbf{1.00} \\
donors           & \textbf{0.99} & NA            & 0.74 & 0.95          \\
fault            & \textbf{0.59} & 0.52          & 0.31          & \textbf{0.59} \\
fraud            & 0.09 & \textbf{0.49} & 0.03          & 0.31          \\
glass            & 0.15          & 0.23          & 0.06          & \textbf{0.89} \\
Hepatitis        & 0.72 & 0.51          & 0.48          & \textbf{0.77} \\
http             & 0.01 & NA            & 0.07 & \textbf{0.94} \\
InternetAds      & 0.65          & NA            & 0.45          & \textbf{0.85} \\
Ionosphere       & 0.84 & 0.72          & 0.37          & \textbf{0.92} \\
landsat          & 0.65          & 0.55          & 0.51 & \textbf{0.78} \\
letter           & NA            & NA            & NA            & \textbf{0.32} \\
Lymphography     & 0.45          & \textbf{0.67} & 0.48 & \textbf{0.67} \\
magic.gamma      & \textbf{0.75} & 0.70          & 0.27          & 0.60 \\
mammography      & 0.12          & 0.09          & 0.00          & \textbf{0.54} \\
mnist            & 0.42          & 0.67          & 0.36          & \textbf{0.91} \\
musk             & 0.42          & \textbf{0.99} & 0.06 & 0.91          \\
optdigits        & 0.68          & \textbf{0.99} & 0.44          & \textbf{1.00} \\
PageBlocks       & 0.20          & 0.34          & 0.17          & \textbf{1.00} \\
pendigits        & 0.49 & \textbf{0.82} & 0.36          & 0.73 \\
Pima             & \textbf{0.60} & 0.54          & 0.26 & 0.57 \\
satellite        & 0.59          & \textbf{0.75} & 0.39          & 0.61          \\
satimage-2       & 0.84 & 0.81          & 0.20          & \textbf{0.92} \\
shuttle          & 0.59          & \textbf{0.94} & 0.79          & 0.78 \\
skin             & 0.99          & 0.61          & 0.58          & \textbf{1.00} \\
smtp             & 0.72 & 0.58          & 0.00 & \textbf{0.76} \\
SpamBase         & 0.11          & 0.34          & 0.32          & \textbf{0.84} \\
speech           & 0.21 & 0.04          & 0.07          & \textbf{0.85} \\
Stamps           & 0.52          & \textbf{0.81} & 0.74          & 0.10          \\
thyroid          & 0.43          & 0.53          & 0.39 & \textbf{0.81} \\
vertebral        & 0.24          & 0.30          & 0.52          & \textbf{0.86} \\
vowels           & 0.43          & 0.37          & 0.46 & \textbf{0.75} \\
Waveform         & 0.22          & 0.09          & 0.12          & \textbf{0.80} \\
WBC              & 0.45          & \textbf{0.64} & 0.61 & 0.45          \\
WDBC             & \textbf{0.91} & 0.90 & 0.43 & 0.72 \\
Wilt             & 0.14          & 0.17          & 0.18          & \textbf{1.00} \\
wine             & \textbf{0.90} & \textbf{0.90} & 0.78 & 0.59          \\
WPBC             & 0.52          & 0.35          & 0.33          & \textbf{1.00} \\
yeast            & 0.50 & 0.48          & 0.16          & \textbf{0.81} \\
CIFAR10          & 0.18          & 0.13          & 0.09          & \textbf{0.77} \\
FashionMNIST     & 0.34 & 0.24          & 0.16          & \textbf{0.43} \\
MNIST-C          & 0.63          & 0.45          & 0.36          & \textbf{0.98} \\
MVTec-AD         & 0.74          & 0.65 & 0.65 & \textbf{0.93} \\
SVHN             & 0.22          & 0.12          & 0.12          & \textbf{0.53} \\
Agnews           & 0.16          & 0.10 & 0.22          & \textbf{0.46} \\
Amazon           & 0.14          & 0.10          & 0.24          & \textbf{0.32} \\
Imdb             & 0.12          & 0.00          & 0.12          & \textbf{0.46} \\
Yelp             & 0.17          & 0.10          & 0.11          & \textbf{0.42} \\
20newsgroups     & 0.15          & 0.09 & 0.18          & \textbf{0.46} \\
BATADAL  04      & \textbf{0.22} & 0.12          & \textbf{0.22}          & \textbf{0.23} \\
SWaT  1          & \textbf{0.55} & 0.29 & 0.22          & 0.40          \\
SWaT  2          & 0.13          & \textbf{0.40} & 0.11          & 0.30 \\
SWaT  3          & NA            & NA            & NA            & \textbf{0.69} \\
SWaT  4          & 0.94 & \textbf{0.99} & 0.37 & 0.08          \\
SWaT  5          & 0.22          & 0.29 & 0.04          & \textbf{0.43} \\
SWaT  6          & 0.46          & \textbf{0.96} & 0.41          & 0.32          \\
ecoli            & 0.19          & 0.33          & 0.33 & \textbf{0.60} \\
cmc              & 0.06          & 0.04          & 0.06          & \textbf{0.27} \\
lympho  h        & 0.60 & 0.44          & 0.57 & \textbf{0.64} \\
wbc  h           & 0.11 & 0.52          & 0.47          & \textbf{0.92}\\
\bottomrule
\end{tabular}
}
\caption{F-1 Score comparison of RN-Net with recent models such as on Multivariate datasets}
\label{tab:f1-comparison-multivariate}
\end{table*}

\begin{table*}[t]
\centering
\scriptsize 
\setlength{\tabcolsep}{2mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|l|l|l|l|}
\hline
Dataset          & Deep-Sad      & FTT           & PReNet        & RN-Net            \\
\hline
ALOI             & 0.57          & 0.50          & 0.51          & \textbf{0.71} \\
annthyroid       & 0.90          & 0.51          & 0.47          & \textbf{0.93} \\
backdoor         & NA            & NA            & NA            & \textbf{0.99} \\
breastw          & 0.97          & \textbf{0.99} & 0.83 & 0.98          \\
campaign         & \textbf{0.71} & 0.68          & 0.65          & \textbf{0.71} \\
cardio           & 0.89          & 0.81          & 0.82          & \textbf{0.96} \\
Cardiotocography & 0.38          & 0.69          & 0.77          & \textbf{0.84} \\
celeba           & 0.87          & 0.76          & 0.81          & \textbf{0.89} \\
cover            & 0.39          & 0.89          & 0.66          & \textbf{1.00} \\
donors           & \textbf{1.00} & NA            & 0.99 & 0.98          \\
fault            & \textbf{0.75} & 0.63          & 0.53          & 0.68 \\
fraud            & \textbf{0.93} & 0.92 & \textbf{0.93}          & 0.92          \\
glass            & 0.73          & 0.82          & 0.83          & \textbf{0.99} \\
Hepatitis        & \textbf{0.97} & 0.77          & 0.65          & 0.83 \\
http             & 0.01 & NA            & \textbf{1.00} & 0.95 \\
InternetAds      & 0.91          & NA            & 0.51          & \textbf{0.93} \\
Ionosphere       & 0.94 & 0.84          & 0.41          & \textbf{0.97} \\
landsat          & 0.88          & 0.76          & 0.61 & \textbf{0.88} \\
letter           & NA            & NA            & NA            & \textbf{0.84} \\
Lymphography     & 0.92          & \textbf{1.00} & 0.83 & 0.74 \\
magic.gamma      & \textbf{0.88} & 0.85          & 0.59          & 0.63 \\
mammography      & 0.67          & 0.77          & 0.31          & \textbf{0.89} \\
mnist            & 0.89          & 0.96          & 0.43          & \textbf{0.98} \\
musk             & 0.98          & \textbf{1.00} & 0.50 & 0.97          \\
optdigits        & 0.99          & \textbf{1.00} & 0.99          & \textbf{1.00} \\
PageBlocks       & 0.49          & 0.82          & 0.40          & \textbf{1.00} \\
pendigits        & \textbf{0.99} & \textbf{0.99} & 0.99          & 0.79 \\
Pima             & 0.73 & 0.65          & 0.52 & \textbf{0.99} \\
satellite        & 0.73          & \textbf{0.80} & 0.61          & 0.70          \\
satimage-2       & \textbf{0.99} & \textbf{0.99} & 0.97          & 0.97 \\
shuttle          & 0.76          & \textbf{0.99} & 0.97          & \textbf{1.00} \\
skin             & 0.99          & 0.84          & 0.97          & \textbf{1.00} \\
smtp             & \textbf{0.81} & 0.77          & 0.79 & 0.76 \\
SpamBase         & 0.41          & 0.74          & 0.69          & \textbf{0.84} \\
speech           & \textbf{0.98} & 0.52          & 0.51          & \textbf{0.91} \\
Stamps           & 0.90          & \textbf{0.93} & \textbf{0.94} & 0.57          \\
thyroid          & 0.98          & \textbf{1.00} & \textbf{1.00} & \textbf{0.94} \\
vertebral        & 0.42          & 0.65          & 0.75          & \textbf{1.00} \\
vowels           & 0.94          & 0.91          & \textbf{0.96} & \textbf{0.89} \\
Waveform         & 0.77          & 0.51          & 0.65          & \textbf{0.99} \\
WBC              & 0.98          & \textbf{0.99} & 0.98 & 0.82          \\
WDBC             & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.97 \\
Wilt             & 0.58          & 0.73          & 0.65          & \textbf{1.00} \\
wine             & 0.98 & \textbf{1.00} & \textbf{1.00} & 0.88          \\
WPBC             & 0.75          & 0.58          & 0.66          & \textbf{1.00} \\
yeast            & 0.53 & 0.61          & 0.49          & \textbf{0.87} \\
CIFAR10          & 0.72          & 0.58          & 0.52          & \textbf{0.77} \\
FashionMNIST     & \textbf{0.90} & 0.73          & 0.60          & 0.72 \\
MNIST-C          & 0.97          & 0.93          & 0.76          & \textbf{1.00} \\
MVTec-AD         & 0.86          & 0.90 & 0.95 & \textbf{0.97} \\
SVHN             & 0.72          & 0.59          & 0.58          & \textbf{0.81} \\
Agnews           & 0.68          & 0.42 & 0.73          & \textbf{0.81} \\
Amazon           & 0.63          & 0.51          & \textbf{0.75} & 0.67 \\
Imdb             & 0.59          & 0.45          & 0.58          & \textbf{0.79} \\
Yelp             & 0.66          & 0.44          & 0.58          & \textbf{0.68} \\
20newsgroups     & 0.69          & 0.43 & 0.71          & \textbf{0.75} \\
BATADAL  04      & \textbf{0.80} & 0.56          & 0.64          & 0.57 \\
SWaT  1          & 0.76 & \textbf{0.78} & 0.49          & 0.54          \\
SWaT  2          & 0.64          & \textbf{0.96} & 0.44          & 0.56 \\
SWaT  3          & NA            & NA            & NA            & \textbf{0.81} \\
SWaT  4          & 0.96 & \textbf{1.00} & 0.95 & 0.07          \\
SWaT  5          & 0.60          & \textbf{0.96} & 0.49          & 0.64 \\
SWaT  6          & 0.89          & \textbf{1.00} & 0.84          & 0.82          \\
ecoli            & 0.86          & 0.90          & 0.92 & \textbf{0.99} \\
cmc              & 0.85          & 0.70          & 0.56          & \textbf{0.97} \\
lympho  h        & \textbf{0.99} & 0.95          & \textbf{0.99} & 0.87 \\
wbc  h           & 0.04 & 0.94          & 0.92          & \textbf{0.99}\\
\bottomrule
\end{tabular}
}
\caption{AUC-ROC comparison of RN-Net with recent models such as on Multivariate datasets}
\label{tab:aucroc-comparison-multivariate}
\end{table*}




\begin{table*}[h]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Dataset          & ECOD            & COPOD             & KNN               & LUNAR             & GOAD            & PCA             & DSVDD           & RN-Net        \\ \hline
ALOI             & 0.0308          & 0.030474          & \textbf{0.065813} & 0.059108          & 0.0431          & 0.04            & 0.0351          & 0.29               \\ \hline
annthyroid       & 0.2649          & 0.218905          & 0.252955          & \textbf{0.333793} & 0.1199          & 0.2063          & 0.1271          & 0.65              \\ \hline
backdoor         & 0.0897          & 0.107135          & 0.1962            & 0.125095          &                 & 0.2092 & 0.0959          & \textbf{0.64}              \\ \hline
breastw          & \textbf{1}      & 0.933071          & 0.853571          & 0.840989          & 0.8399          & \textbf{1}      & 0.8364          & 0.96               \\ \hline
campaign         & 0.417  & 0.38463           & 0.330381          & 0.329897          & 0.3204          & 0.3613          & 0.2371          & \textbf{0.76}               \\ \hline
cardio           & 0.5185          & 0.528889          & 0.439169          & 0.40048           & 0.165           & 0.6096 & 0.2787          & \textbf{0.74}                 \\ \hline
Cardiotocography & 0.5792          & 0.547619          & 0.591981          & 0.538462          & 0.5865          & 0.5567          & 0.3640          & \textbf{0.79}        \\ \hline
celeba           & 0.0975          & 0.094133          & 0.0526            & 0.045133          &                 & 0.1113 & 0.0083          & \textbf{0.26}              \\ \hline
cover            & 0.0653          & 0.052908          & 0.1428   & 0.0316            &                 & 0.0735          & 0.0049          & \textbf{0.96}                 \\ \hline
donors           & 0.2902 & 0.278359          &                   &                   &                 & 0.1505          & 0.2218          & \textbf{0.91}                    \\ \hline
fault            & 0.3383          & 0.293103          & 0.7435   & 0.738683          &                 & 0.3789          & 0.4686          & 0.82                \\ \hline
fraud            & 0.0153          & 0.015491          & 0.0147            & 0.0078            &                 & 0.0264 & 0.0104          & \textbf{0.99}               \\ \hline
glass            & 0.0909          & 0.130435          & 0.1429            & 0.125             & 0.12            & 0.0909          & 0.1935 & 0.22              \\ \hline
Hepatitis        & 0.25            & 0.5      & 0.2               & 0.222222          & 0.125           & 0.3333          & 0.4333          & \textbf{0.61}                   \\ \hline
http             & 0.0389          & 0.037072          & 0.036934          &                   &                 & 0.0388          & 0.0008          & \textbf{0.46}     \\ \hline
InternetAds      & 0.6634          & 0.484848          & \textbf{0.726644}          & 0.602941          & 0.2796          & 0.4574          & 0.5720          & 0.67         \\ \hline
Ionosphere       & 0.8889          & \textbf{1}        & 0.863636          & 0.82963           & 0.7767          & \textbf{1}      & 0.8380          & 0.97                 \\ \hline
landsat          & 0.1611          & 0.202643          & 0.588717 & 0.511535          & 0.2161          & 0.0989          & 0.4570          & \textbf{0.84}             \\ \hline
letter           & 0.0755          & 0.0474            & 0.1962            & \textbf{0.2788}   & 0.0741          & 0.0795          & 0.1708          & 0.2              \\ \hline
Lymphography     & 0.4286          & 0.3               & 0.2857            & 0.1875            & 0.2727          & 0.4615 & 0.1765          & \textbf{0.6}           \\ \hline
magic.gamma      & 0.702           & 0.748404          & 0.790922          & 0.758181          &                 & 0.8217 & 0.6479          & \textbf{0.85}             \\ \hline
mammography      & 0.1776          & 0.173725          & 0.15324           & 0.133459          & \textbf{0.4286} & 0.1346          & 0.1145          & 0.27               \\ \hline
mnist            & 0.1812          & 0.266819          & 0.484211 & 0.458438          & 0.438           & 0.3831          & 0.2513          & \textbf{0.79}             \\ \hline
musk             & 0.2492          & 0.21164           & 0.164129          & 0.143068          & 0.1219          & 0.3089          & 0.2343          & \textbf{1}       \\ \hline
optdigits        & 0.0222          & 0.027174          & 0.253378          & 0.259965 & 0.0333          & 0.0019          & 0.0038          & \textbf{0.99}               \\ \hline
PageBlocks       & 0.4328          & 0.350291          & 0.21709           & 0.312119          & 0.3411          & 0.476           & 0.3004          & \textbf{0.82}      \\ \hline
pendigits        & 0.1484          & 0.127507          & 0.208278          & 0.219718 & 0.1581          & 0.148           & 0.0750          & \textbf{0.37}                    \\ \hline
Pima             & 0.5517          & 0.69              & 0.666667          & 0.80315           & 0.4949          & \textbf{0.8311} & 0.3827          & 0.75                \\ \hline
satellite        & 0.8152          & 0.831579          & 0.785133          & 0.753225          & 0.7538          & 0.8421          & 0.7187          & \textbf{0.89}           \\ \hline
satimage-2       & 0.1098          & 0.135699          & 0.117162          & 0.124561          & 0.1056          & 0.1121          & 0.0889          & \textbf{0.89}        \\ \hline
shuttle          & 0.7001 & 0.657735          & 0.484276          & 0.389938          & 0.4336          & 0.6788          & 0.4185          & \textbf{1}                 \\ \hline
skin             & 0.0719          & 0.051742          & 0.724642 & 0.693724          &                 & 0.0004          & 0.3841          & \textbf{0.95}                \\ \hline
smtp             & 0.0022          & 0.002209          & 0.00251           & 0.001578          &                 & 0.0023          & 0.0029          & \textbf{0.27}        \\ \hline
SpamBase         & 0.5465          & 0.660266          & 0.752242 & 0.739979          & 0.4144          & 0.4103          & 0.7186          & \textbf{0.89}               \\ \hline
speech           & 0.0183          & 0.01766           & 0.0199            & \textbf{0.033403} & 0.0261          & 0.019           & 0.0305          & 0.06                \\ \hline
Stamps           & 0.2941          & 0.55102  & 0.491525          & 0.454545          & 0.4561          & 0.2703          & 0.2429          & \textbf{0.58}              \\ \hline
thyroid          & 0.2348 & 0.191344          & 0.201422          & 0.203271          & 0.1636          & 0.1969          & 0.1547          & \textbf{0.59}              \\ \hline
vertebral        & 0.1333 & 0                 & 0.04              & 0.041667          & 0.0769          & 0               & 0.0455          & \textbf{0.69}                  \\ \hline
vowels           & 0.0839          & 0.033557          & 0.1      & 0.092764          & 0.0928          & 0.0556          & 0.0686          & \textbf{0.17}               \\ \hline
Waveform         & 0.0368          & 0.045627          & 0.145511          & 0.11244           & 0.0587          & 0.0409          & 0.0833 & \textbf{0.13}                 \\ \hline
WBC              & 0.3846          & 0.37037           & 0.344828          & 0.3125            & 0.3226          & 0.3704          & 0.2941          & \textbf{0.55}     \\ \hline
WDBC             & 0.2439          & 0.227273          & 0.217391          & 0.185185          & 0.2174          & 0.2564 & 0.1493          & \textbf{1}               \\ \hline
Wilt             & 0.0305          & 0.01004           & 0.156347          & 0.013208          & 0.0616 & 0.0081          & 0.0165          & \textbf{0.36}              \\ \hline
wine             & 0.1875          & 0.363636          & 0.5               & 0.428571          & 0.5882 & 0.1818          & 0.2632          & \textbf{1}                \\ \hline
WPBC             & 0.1304          & 0.210526          & 0.263158          & 0.117647          & 0.1667          & 0.1667          & 0.4167 & \textbf{0.63}               \\ \hline
yeast            & 0.3581          & 0.322368          & 0.356061          & 0.216             & 0.5248 & 0.368           & 0.2899          & \textbf{0.74}                  \\ \hline
CIFAR10          & 0.1338          & 0.130275          & 0.166038          & 0.136015          & \textbf{0.1918} & 0.1477          & 0.1034          & 0.19             \\ \hline
FashionMNIST     & 0.1884          & 0.174298          & 0.32732  & 0.303797          & 0.2043          & 0.2274          & 0.1823          & \textbf{0.36}              \\ \hline
MNIST-C          & 0.0788          & 0.077375          & 0.306224 & 0.290671          & 0.0944          & 0.0999          & 0.1298          & \textbf{0.94}               \\ \hline
MVTec-AD         & 0.9375          & 0.943396          & 0.776316          & 0.725             & 0.6905          & \textbf{1}      & 0.6237          & 0.97         \\ \hline
SVHN             & 0.0569          & 0.055028          & 0.131115          & 0.082645          & 0.0722          & 0.0806          & 0.0848 & \textbf{0.21}             \\ \hline
Agnews           & 0.048           & 0.053785          & 0.09889  & 0.07841           & 0.0466          & 0.05            & 0.0506          & \textbf{0.37}             \\ \hline
Amazon           & 0.0499          & 0.062992          & 0.062967          & 0.101427 & 0.0654          & 0.0588          & 0.0364          & \textbf{0.24}              \\ \hline
Imdb             & 0.0279          & 0.033697          & 0.028169          & 0.107234 & 0.0367          & 0.0297          & 0.0333          & \textbf{0.29}               \\ \hline
Yelp             & 0.0729          & 0.088235          & 0.098712 & 0.076327          & 0.0856          & 0.0855          & 0.0481          & \textbf{0.22}              \\ \hline
20newsgroups     & 0.0721          & 0.064103          & 0.146497 & 0.116041          & 0.0737          & 0.0741          & 0.0601          & \textbf{0.28}             \\ \hline
ecoli            & 0.1111          & 0.111111          & 0                 & 0                 & 0.0946          & \textbf{0.2333} & 0.1321          & 0.06          \\ \hline
cmc              & 0.0068          & 0.006579          & 0.090909          & 0.111111 & 0.0194          & 0               & 0.0068          & \textbf{0.39}              \\ \hline
lympho\_h         & \textbf{0.375}  & 0.26087           & 0.285714          & 0.25              & 0.25            & 0.3529          & 0.1613          & 0.29                  \\ \hline
wbc\_h            & 0.3684          & 0.34              & 0.3               & 0.253012          & 0.2881          & 0.4054          & 0.2286          & \textbf{0.83}     \\ \hline
\end{tabular}
}
\caption{\textbf{Precision} values of the 8 algorithms on the 60 multivariate datasets. The highest value(s) is marked in bold.
The empty spaces denote that the algorithm exceeded a runtime of three hours without successfully generating results; Main Text (SoTA Algorithm Landscape)}
\label{table21}
\end{table*}



\begin{table*}[h]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
Dataset        & ECOD   & COPOD    & KNN      & LUNAR    & GOAD   & PCA    & DSVDD  & RN-Net       \\ \hline
yahoo1         & 0.014  & 0.011628 & 0.010753 & 0.007519 & 0.01   & 0.017  & 0.0119 & \textbf{1}      \\ \hline
yahoo2         & 0.0548 & 0.057143 & 0.121212 & 0.044444 & 0.0533 & 0.0205 & 0.0465 & \textbf{0.7}   \\ \hline
yahoo3         & 0.0548 & 0.058824 & 0.189189 & 0.029412 & 0.0282 & 0.0455 & 0.0648 & \textbf{1}         \\ \hline
yahoo5         & 0.0414 & 0.022727 & 0.046154 & 0.017544 & 0.0204 & 0      & 0.0214 & \textbf{0.82}     \\ \hline
yahoo6         & 0.0278 & 0.017094 & 0.009926 & 0.008386 & 0.0201 & 0.0076 & 0.0192 & \textbf{1}       \\ \hline
yahoo7         & 0.0289 & 0.024896 & 0.019231 & 0.018717 & 0.049  & 0.0172 & 0.0201 & \textbf{0.8}           \\ \hline
yahoo8         & 0.0058 & 0.010909 & 0.009579 & 0.013356 & 0.0079 & 0      & 0.0058 & \textbf{1}   \\ \hline
yahoo9         & 0.0473 & 0.046512 & 0.072072 & 0.034043 & 0.0449 & 0.0395 & 0.0440 & \textbf{1}      \\ \hline
Speed\_6005    & 0.0042 & 0.004082 & 0.025641 & 0.028571 & 0.0035 & 0.005  & 0.0038 & \textbf{1}          \\ \hline
Speed\_7578    & 0.0367 & 0.032787 & 0.051282 & 0.061538 & 0.0219 & 0.04   & 0.1143 & \textbf{1}   \\ \hline
Speed\_t4013   & 0.0081 & 0.008299 & 0.034483 & 0.04     & 0.0089 & 0.0135 & 0.0417 & \textbf{1}        \\ \hline
TravelTime\_387    & 0.008  & 0.008772 & 0.011696 & 0.008621 & 0.0027 & 0.0085 & 0.0051 & \textbf{0.13 }        \\ \hline
TravelTime\_451    & 0.0047 & 0.005181 & 0.006757 & 0.004484 & 0.0057 & 0.0047 & 0.0000 & \textbf{0.01}         \\ \hline
Occupancy\_6005    & 0.0039 & 0.004525 & 0.005682 & 0.006803 & 0.003  & 0.0046 & 0.0051 & \textbf{1}       \\ \hline
Occupancy\_t4013   & 0.0079 & 0.007663 & 0.011494 & 0.006494 & 0.0072 & 0.0078 & 0.0075 & \textbf{1}         \\ \hline
yahoo\_syn1        & 0.0816 & 0.068966 & 0.063158 & 0.041667 & 0.0571 & 0.48   & 0.0698 & \textbf{1}   \\ \hline
yahoo\_syn2        & 0.1208 & 0.12766  & 0.236842 & 0.097297 & 0.1132 & 0.0728 & 0.0994 & \textbf{0.94}   \\ \hline
yahoo\_syn3        & 0.1141 & 0.133333 & 0.361702 & 0.072034 & 0.0656 & 0.1074 & 0.1393 & \textbf{1}         \\ \hline
yahoo\_syn5        & 0.1111 & 0.082707 & 0.112676 & 0.036398 & 0.0645 & 0      & 0.0685 & \textbf{0.16}   \\ \hline
yahoo\_syn6        & 0.0286 & 0.017021 & 0.023873 & 0.021322 & 0.019  & 0      & 0.0122 & \textbf{1}    \\ \hline
yahoo\_syn7        & 0.0819 & 0.061728 & 0.04908  & 0.0401   & 0.0515 & 0.0667 & 0.0532 & \textbf{0.74}        \\ \hline
yahoo\_syn8        & 0.0175 & 0.025271 & 0.025145 & 0.032051 & 0.0159 & 0      & 0.0144 & \textbf{1}        \\ \hline
yahoo\_syn9        & 0.1011 & 0.102273 & 0.15     & 0.068702 & 0.0942 & 0.0747 & 0.0933 & \textbf{1}     \\ \hline
aws1           & 0.0095 & 0.009804 & 0.014925 & 0.006993 & 0.0088 & 0.0097 & 0.0097 & \textbf{1}    \\ \hline
aws2           & 0.0099 & 0.00304  & 0.090909 & 0.090909 & 0.125  & 0.1818 & 0.0769 & \textbf{1}     \\ \hline
aws3           & 0.0065 & 0.006711 & 0.010638 & 0.0025   & 0.0068 & 0.0062 & 0.0072 & \textbf{1}  \\ \hline
aws\_syn1 & 0.0926 & 0.093458 & 0.12987  & 0.034247 & 0.0833 & 0.0849 & 0.0394 & \textbf{0.86}   \\ \hline
aws\_syn2 & 0.0922 & 0.025413 & 0.47619  & 0.47619  & 0.5556 & 0.6452 & 0.0127 & \textbf{0.88}     \\ \hline
aws\_syn3 & 0.0671 & 0.066667 & 0.097087 & 0.024938 & 0.0629 & 0.06   & 0.0264 & \textbf{1}\\ \hline
\end{tabular}
}
\caption{\textbf{Precision} values of the 8 algorithms on the 29 univariate datasets. The highest value(s) is marked in bold.}
\label{table22}
\end{table*}




\begin{table*}[h]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Dataset          & ECOD            & COPOD             & KNN               & LUNAR             & PCA        & DSVDD           & GOAD       & RN-Net    \\ \hline
ALOI             & 0.1028          & 0.096817          & 0.31366           & 0.291777          & 0.1313     & 0.1154          & 0.132      & \textbf{0.36}        \\ \hline
annthyroid       & 0.3502          & 0.329588          & 0.400749          & \textbf{0.906367} & 0.2715     & 0.1854          & 0.1704     & 0.72           \\ \hline
backdoor         & 0.3658          & 0.424216          & 0.9128            & 0.923572 & 0.8385     & 0.9057          &            & \textbf{0.98}           \\ \hline
breastw          & 0.3013          & 0.991632          & \textbf{1}        & 0.995816          & 0.3974     & 0.9414          & 0.9874     & 0.98          \\ \hline
campaign         & 0.3705          & 0.441164          & 0.34569           & 0.551724          & 0.3211     & 0.2446          & 0.3709     & \textbf{0.72}        \\ \hline
cardio           & 0.5568          & 0.676136          & 0.840909          & \textbf{0.948864} & 0.6477     & 0.3864          & 0.1875     & 0.76            \\ \hline
Cardiotocography & 0.2511          & 0.296137          & 0.538627          & 0.540773 & 0.2425     & 0.2124          & 0.5021     & \textbf{0.64 }        \\ \hline
celeba           & 0.433           & 0.439631          & 0.1746            & 0.207829          & 0.4935     & 0.0363          &            & \textbf{0.76}    \\ \hline
cover            & 0.6826          & 0.553695          & 0.3333            & 0.9731   & 0.7659     & 0.0626          &            & \textbf{0.99}              \\ \hline
donors           & 0.4886          & 0.505884 &                   &                   & 0.2518     & 0.4531          &            & \textbf{0.99 }         \\ \hline
fault            & 0.101           & 0.10104           & 0.4264            & \textbf{0.533432 }         & 0.2998     & 0.1664          &            & 0.45        \\ \hline
fraud            & 0.8862          & 0.876016          & 0.9085            & \textbf{0.9736}   & 0.9592     & 0.6687          &            & 0.76         \\ \hline
glass            & 0.2222          & 0.333333          & 0.3333            & 0.333333          & 0.2222     & 0.6667          & 0.3333     & \textbf{1}            \\ \hline
Hepatitis        & 0.1538          & 0.461538          & 0.076923          & 0.153846          & 0.2308     & \textbf{1.0000} & 0.0769     & 0.58               \\ \hline
http             & \textbf{1}      & \textbf{1}        & 0.999548          &                   & \textbf{1} & 0.0199          &            & \textbf{1}       \\ \hline
InternetAds      & 0.3641          & 0.478261          & 0.570652          & 0.668478          & 0.2772     & \textbf{0.7446} & 0.1603     & 0.51         \\ \hline
Ionosphere       & 0.254           & 0.103175          & 0.904762          & 0.888889          & 0.2821     & \textbf{0.9444} & 0.6349     & 0.91             \\ \hline
landsat          & 0.0765          & 0.069017          & 0.485371 & 0.382596          & 0.0465     & 0.3631          & 0.1185     & \textbf{0.6 }            \\ \hline
letter           &                 & 0.09              & 0.72     & 0.58              & 0.12       & 0.4100          & 0.12       & \textbf{0.81}            \\ \hline
Lymphography     & \textbf{1}      & \textbf{1}        & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1.0000} & \textbf{1} & 0.6      \\ \hline
magic.gamma      & 0.2001          & 0.262859          & \textbf{0.601824} & 0.578499          & 0.2304     & 0.3396          &            & 0.39         \\ \hline
mammography      & 0.75            & 0.773077          & 0.673077          & 0.546154          & 0.5923     & 0.5500          & \textbf{1} & 0.56                  \\ \hline
mnist            & 0.1957          & 0.334286          & 0.788571          & 0.78              & 0.4143     & 0.3471          & 0.7571     & \textbf{0.84 }        \\ \hline
musk             & 0.7938          & 0.824742          & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1.0000} & 0.3505     & \textbf{1}        \\ \hline
optdigits        & 0.08            & 0.1               & \textbf{1}        & \textbf{1}        & 0.0067     & 0.0133          & 0.12       & \textbf{1}           \\ \hline
PageBlocks       & 0.4608          & 0.472549          & 0.368627          & \textbf{0.903922} & 0.5059     & 0.5784          & 0.5765     & 0.62              \\ \hline
pendigits        & 0.6603          & 0.570513          & \textbf{1}        & \textbf{1}        & 0.6603     & 0.3590          & 0.8077     & 0.71         \\ \hline
Pima             & 0.1791          & 0.257463          & 0.30597           & \textbf{0.761194} & 0.6219     & 0.1157          & 0.1828     & 0.62          \\ \hline
satellite        & 0.2534          & 0.349214          & 0.710707 & 0.659627          & 0.3921     & 0.5521          & 0.6616     & \textbf{0.88 }                \\ \hline
satimage-2       & 0.9014          & 0.915493          & \textbf{1}        & \textbf{1}        & 0.9296     & 0.9577          & 0.9859     & 0.95     \\ \hline
shuttle          & 0.9806          & 0.985759          & \textbf{1}        & \textbf{1}        & 0.9644     & 0.9274          & 0.9983     & 0.99             \\ \hline
skin             & 0.0347          & 0.021058          & \textbf{1}        & \textbf{1}        & 0.0002     & 0.2340          &            & 0.51            \\ \hline
smtp             & 0.7             & 0.7               & 0.733333          & \textbf{1}        & 0.7333     & 0.8667          &            & 0.86              \\ \hline
SpamBase         & 0.1364          & 0.26623           & 0.399643          & 0.428827 & 0.0904     & 0.3848          & 0.1269     & \textbf{0.73}             \\ \hline
speech           & 0.1148          & 0.131148          & 0.196721          & 0.262295          & 0.1148     & 0.3607          & 0.2951     & \textbf{0.65}         \\ \hline
Stamps           & 0.3226          & 0.870968          & \textbf{0.935484} & 0.806452          & 0.3226     & 0.5484          & 0.8387     & 0.88        \\ \hline
thyroid          & 0.957           & 0.903226          & 0.913978          & \textbf{0.935484}          & 0.828      & 0.7849          & 0.7742     & 0.85        \\ \hline
vertebral        & 0.1333 & 0                 & 0.033333          & 0.033333          & 0          & 0.0333          & 0.0667     & \textbf{0.89}         \\ \hline
vowels           & 0.24            & 0.1               & 0.98              & \textbf{1}        & 0.111      & 0.4200          & 0.98       & 0.74    \\ \hline
Waveform         & 0.13            & 0.12              & 0.47              & 0.47              & 0.15       & 0.3400          & 0.21       & \textbf{0.72}          \\ \hline
WBC              & \textbf{1}      & \textbf{1}        & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1.0000} & \textbf{1} & \textbf{1}       \\ \hline
WDBC             & \textbf{1}      & \textbf{1}        & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1.0000} & \textbf{1} & \textbf{1}     \\ \hline
Wilt             & 0.0584          & 0.019455          & 0.392996          & 0.027237          & 0.0156     & 0.0389          & 0.1595     & \textbf{0.77}                   \\ \hline
wine             & 0.3             & 0.8               & \textbf{1}        & 0.9               & 0.2        & \textbf{1.0000} & \textbf{1} & \textbf{1}      \\ \hline
WPBC             & 0.0638          & 0.085106          & 0.106383          & 0.042553          & 0.0851     & 0.3191          & 0.0638     & \textbf{0.83}           \\ \hline
yeast            & 0.1045          & 0.096647          & 0.092702          & 0.053254          & 0.09167    & 0.0789          & 0.2091     & \textbf{0.67   }        \\ \hline
CIFAR10          & 0.2738          & 0.269962          & 0.334601          & 0.269962          & 0.3004     & 0.2319          & 0.4449     & \textbf{0.56  }       \\ \hline
FashionMNIST     & 0.381           & 0.374603          & \textbf{0.806349}          & 0.761905          & 0.4635     & 0.4825          & 0.4794     & 0.53     \\ \hline
MNIST-C          & 0.16            & 0.158             & 0.738             & 0.754             & 0.202      & 0.3000          & 0.202      & \textbf{0.96}    \\ \hline
MVTec-AD         & 0.4762          & 0.793651          & \textbf{0.936508} & 0.920635          & 0.3968     & 0.9206          & 0.9206     & 0.89       \\ \hline
SVHN             & 0.1154          & 0.111538          & 0.257692          & 0.153846          & 0.1692     & 0.1885          & 0.1462     & \textbf{0.49}    \\ \hline
Agnews           & 0.096           & 0.108             & 0.196             & 0.146             & 0.102      & 0.1020          & 0.094      & \textbf{0.67 }    \\ \hline
Amazon           & 0.1             & 0.128             & 0.118             & 0.27              & 0.118      & 0.0720          & 0.134      & \textbf{0.51}    \\ \hline
Imdb             & 0.056           & 0.068             & 0.052             & 0.418             & 0.06       & 0.0660          & 0.074      & \textbf{0.69 }   \\ \hline
Yelp             & 0.144           & 0.18              & 0.184             & 0.138             & 0.168      & 0.0940          & 0.174      & \textbf{0.39  }  \\ \hline
20newsgroups     & 0.1429          & 0.12987           & 0.298701 & 0.220779          & 0.1429     & 0.1429          & 0.1494     & \textbf{0.7 }       \\ \hline
ecoli            & 0.4444          & 0.555556          & 0                 & 0                 & \textbf{0.7778}     & \textbf{0.7778}          & \textbf{0.7778 }    & 0.57  \textbf{}\\ \hline
cmc              & 0.0588          & 0.058824          & \textbf{0.888889} & \textbf{0.888889} & 0          & 0.0588          & 0.1765     & 0.88                  \\ \hline
lympho h         & \textbf{1}      & \textbf{1}        & \textbf{1}        & 0.666667          & \textbf{1} & 0.8333          & 0.8333     & \textbf{1}            \\ \hline
wbc h            & 0.6667          & 0.809524          & 0.857143          & \textbf{1}        & 0.7143     & 0.7619          & 0.8095     & 0.88        \\ \hline
\end{tabular}
}
\caption{\textbf{Recall} values of the 8 algorithms on the 60 multivariate datasets. The highest value(s) is marked in bold.
The empty spaces denote that the algorithm exceeded a runtime of three hours without successfully generating results.}
\label{table23}
\end{table*}

\begin{table*}[h]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Dataset        & ECOD       & COPOD      & KNN               & LUNAR             & PCA        & DSVDD      & GOAD       & RN-Net   \\ \hline
yahoo1         & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}  \\ \hline
yahoo2         & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & 0.375      & \textbf{1} & \textbf{1} & \textbf{1}        \\ \hline
yahoo3         & \textbf{1} & \textbf{1} & 0.875             & 0.875             & 0.875      & 0.8750     & 0.875      & \textbf{1}         \\ \hline
yahoo5         & 0.6667     & 0.333333   & 0.666667          & \textbf{1}        & 0          & 0.3333     & 0.3333     & 0.4          \\ \hline
yahoo6         & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}  \\ \hline
yahoo7         & 0.4545     & 0.545455   & 0.545455          & \textbf{0.636364} & 0.2727     & 0.3636     & 0.4545     & 0.67            \\ \hline
yahoo8         & 0.1        & 0.3        & 0.5               & \textbf{0.8}      & 0          & 0.2000     & 0.1        & 0.89          \\ \hline
yahoo9         & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & 0.875      & \textbf{1} & \textbf{1} & \textbf{1}        \\ \hline
Speed\_6005    & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}  \\ \hline
Speed\_7578    & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}        \\ \hline
Speed\_t4013   & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}  \\ \hline
TravelTime\_387    & 0.6667     & 0.666667   & 0.666667          & 0.666667          & 0.6667     & 0.3333     & 0.3333     & \textbf{1}   \\ \hline
TravelTime\_451    & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & 0.0000     & \textbf{1} & \textbf{1}    \\ \hline
Occupancy\_6005    & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}    \\ \hline
Occupancy\_t4013   & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}     \\ \hline
yahoo\_syn1        & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}        \\ \hline
yahoo\_syn2        & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & 0.6111     & \textbf{1} & \textbf{1} & \textbf{1}         \\ \hline
yahoo\_syn3        & 0.9444     & \textbf{1} & 0.944444          & 0.944444          & 0.8889     & 0.9444     & 0.8889     & \textbf{1}         \\ \hline
yahoo\_syn5        & 0.8421     & 0.578947   & 0.842105          & \textbf{1}        & 0          & 0.5263     & 0.5263     & \textbf{1}         \\ \hline
yahoo\_syn6        & 0.2857     & 0.285714   & 0.642857          & 0.714286 & 0          & 0.2857     & 0.2857     & \textbf{0.82}         \\ \hline
yahoo\_syn7        & 0.6667     & 0.714286   & 0.761905 & 0.761905 & 0.5238     & 0.4762     & 0.3333     & \textbf{0.84}          \\ \hline
yahoo\_syn8        & 0.15       & 0.35       & 0.65              & \textbf{1}        & 0          & 0.2500     & 0.1        & 0.94       \\ \hline
yahoo\_syn9        & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & 0.7222     & \textbf{1} & \textbf{1} & \textbf{1}       \\ \hline
aws1           & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}   \\ \hline
aws2           & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & 0.5000     & \textbf{1} & 0.5           \\ \hline
aws3           & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1}   \\ \hline
aws\_syn1 & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & 0.9        & \textbf{1} & \textbf{1} & \textbf{1}           \\ \hline
aws\_syn2 & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & \textbf{1} & 0.3500     & \textbf{1} & 0.41    \\ \hline
aws\_syn3 & \textbf{1} & \textbf{1} & \textbf{1}        & \textbf{1}        & 0.9        & \textbf{1} & \textbf{1} & \textbf{1}    \\ \hline

\end{tabular}
}
\caption{\textbf{Recall} values of the 8 algorithms on the 29 univariate datasets. The highest value(s) is marked in bold.}
\label{table24}
\end{table*}



\begin{table*}[h]
\centering
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Dataset          & ECOD            & COPOD            & KNN               & LUNAR             & GOAD            & PCA             & DSVDD           & RN-Net               \\ \hline
ALOI             & 0.0474          & 0.046357         & 0.10879 & 0.098302          & 0.0649          & 0.0613          & 0.0538          & \textbf{0.29}                \\ \hline
annthyroid       & 0.3016          & 0.263079         & 0.310145          & 0.487903 & 0.1408          & 0.2344          & 0.1508          & \textbf{0.68}                  \\ \hline
backdoor         & 0.144           & 0.171067         & 0.323             & 0.220344          &                 & 0.3348 & 0.1735          & \textbf{0.78 }                \\ \hline
breastw          & 0.463           & 0.96146 & 0.921002          & 0.911877          & 0.9077          & 0.5688          & 0.8858          & \textbf{0.97}                \\ \hline
campaign         & 0.3924          & 0.410962         & 0.337862          & 0.412903 & 0.3438          & 0.34            & 0.2408          & \textbf{0.74}                \\ \hline
cardio           & 0.537           & 0.593516         & 0.576998          & 0.563238          & 0.1755          & 0.6281 & 0.3238          & \textbf{0.74  }              \\ \hline
Cardiotocography & 0.3503          & 0.384401         & 0.564045 & 0.539615          & 0.541           & 0.3378          & 0.2683          & \textbf{0.7}                 \\ \hline
celeba           & 0.1592          & 0.155063         & 0.0808            & 0.074161          &                 & 0.1816 & 0.0135          & \textbf{0.39 }               \\ \hline
cover            & 0.1193 & 0.096587         & 0.2               & 0.0612            &                 & 0.1341          & 0.0091          & \textbf{0.98 }                    \\ \hline
donors           & 0.3642 & 0.359117         &                   &                   &                 & 0.1884          & 0.2978          & \textbf{0.95 }                   \\ \hline
fault            & 0.1556          & 0.150276         & 0.542             & \textbf{0.6195}   &                 & 0.2567          & 0.2456          & 0.54                 \\ \hline
fraud            & 0.0301          & 0.030444         & 0.0289            & 0.0154            &                 & 0.0513 & 0.0205          & \textbf{0.86}                 \\ \hline
glass            & 0.129           & 0.1875           & 0.2               & 0.181818          & 0.1765          & 0.129           & 0.3000 & \textbf{0.32}                  \\ \hline
Hepatitis        & 0.1905          & 0.48             & 0.111111          & 0.181818          & 0.0952          & 0.2727          & \textbf{0.6047} & 0.59                     \\ \hline
http             & 0.0749          & 0.071494         & 0.071236          &                   &                 & 0.0748          & 0.0015          & \textbf{0.52}         \\ \hline
InternetAds      & 0.4702          & 0.481532         & 0.639269          & 0.634021          & 0.2038          & 0.3452          & \textbf{0.6470} & 0.56                  \\ \hline
Ionosphere       & 0.3951          & 0.18705          & 0.883721 & 0.858238          & 0.6987          & 0.44            & 0.8881          & \textbf{0.94}                  \\ \hline
landsat          & 0.1038          & 0.102966         & 0.532072 & 0.437768          & 0.1531          & 0.0633          & 0.4047          & \textbf{0.64}                    \\ \hline
letter           & 0.0927          & 0.0621           & 0.3084            & \textbf{0.3766}   & 0.0916          & 0.0956          & 0.2412          & 0.32                  \\ \hline
Lymphography     & 0.6             & 0.461538         & 0.4444            & 0.315789          & 0.4286          & \textbf{0.6316} & 0.3000          & 0.58                   \\ \hline
magic.gamma      & 0.3114          & 0.389067         & \textbf{0.683536} & 0.656263          &                 & 0.3599          & 0.4456          & 0.55                  \\ \hline
mammography      & 0.2872 & 0.283698         & 0.249643          & 0.214502          & 0.6             & 0.2194          & 0.1895          & \textbf{0.35}                     \\ \hline
mnist            & 0.1882          & 0.296766         & 0.6               & 0.577472 & 0.555           & 0.3981          & 0.2915          & \textbf{0.81 }               \\ \hline
musk             & 0.3793          & 0.336842         & 0.281977          & 0.250323          & 0.1809          & 0.472  & 0.3796          & \textbf{1 }                \\ \hline
optdigits        & 0.0347          & 0.042735         & 0.404313          & 0.412655          & 0.0522          & 0.0029          & 0.0060          & \textbf{1}        \\ \hline
PageBlocks       & 0.4463          & 0.402337         & 0.273256          & 0.464016          & 0.4286          & 0.4905 & 0.3954          & \textbf{0.69}                 \\ \hline
pendigits        & 0.2424          & 0.208431         & 0.344751          & 0.360277 & 0.2644          & 0.2418          & 0.1240          & \textbf{0.46 }                    \\ \hline
Pima             & 0.2704          & 0.375            & 0.419437          & \textbf{0.781609} & 0.267           & 0.7181          & 0.1777          & 0.66                   \\ \hline
satellite        & 0.3867          & 0.491871         & 0.746069 & 0.703325          & 0.7047          & 0.4591          & 0.6244          & \textbf{0.89}                  \\ \hline
satimage-2       & 0.1957          & 0.236364         & 0.209749          & 0.221529          & 0.1907          & 0.2             & 0.1627          & \textbf{0.92}          \\ \hline
shuttle          & 0.8169 & 0.789012         & 0.652542          & 0.561087          & 0.6046          & 0.7968          & 0.5767          & \textbf{0.99}                 \\ \hline
skin             & 0.0469          & 0.029934         & \textbf{0.840339} & 0.81917           &                 & 0.0002          & 0.2909          & 0.65                  \\ \hline
smtp             & 0.0044          & 0.004404         & 0.005002          & 0.00315           &                 & 0.0046          & 0.0058 & \textbf{0.33}               \\ \hline
SpamBase         & 0.2183          & 0.379457         & 0.521976          & 0.542986 & 0.1943          & 0.1418          & 0.5012          & \textbf{0.8}                   \\ \hline
speech           & 0.0316          & 0.031128         & 0.036145          & 0.059259 & 0.048           & 0.0326          & 0.0562          & \textbf{0.11 }                  \\ \hline
Stamps           & 0.3077          & 0.675   & 0.644444          & 0.581395          & 0.5909          & 0.2941          & 0.3366          & \textbf{0.68 }                  \\ \hline
thyroid          & 0.3771 & 0.315789         & 0.330097          & 0.333973          & 0.2702          & 0.3182          & 0.2584          & \textbf{0.69}                   \\ \hline
vertebral        & 0.1333 & 0                & 0.036364          & 0.037037          & 0.0714          & 0               & 0.0385          & \textbf{0.75}                     \\ \hline
vowels           & 0.1244          & 0.050251         & 0.181481          & 0.169779          & 0.1696          & 0.0741          & 0.1180          & \textbf{0.26}         \\ \hline
Waveform         & 0.0574          & 0.066116         & \textbf{0.222222} & 0.181467          & 0.0917          & 0.0642          & 0.1339          & 0.21                    \\ \hline
WBC              & 0.5556          & 0.540541         & 0.512821          & 0.47619           & 0.4878          & 0.5405          & 0.4545          & \textbf{0.7}          \\ \hline
WDBC             & 0.3922          & 0.37037          & 0.357143          & 0.3125            & 0.3571          & 0.4082 & 0.2597          & \textbf{1}                  \\ \hline
Wilt             & 0.0401          & 0.013245         & 0.223699 & 0.017789          & 0.0888          & 0.0106          & 0.0232          & \textbf{0.46}                     \\ \hline
wine             & 0.2308          & 0.5              & 0.666667          & 0.580645          & 0.7407 & 0.1905          & 0.4167          & \textbf{1}                   \\ \hline
WPBC             & 0.0857          & 0.121212         & 0.151515          & 0.0625            & 0.0923          & 0.1127          & 0.3614 & \textbf{0.7}\\ \hline
yeast            & 0.1618          & 0.14871          & 0.147105          & 0.085443          & 0.299  & 0.1572          & 0.1240          & \textbf{0.69}                \\ \hline
CIFAR10          & 0.1798          & 0.175743         & 0.221942          & 0.180892          & 0.268  & 0.198           & 0.1430          & \textbf{0.28}                  \\ \hline
FashionMNIST     & 0.2521          & 0.237903         & \textbf{0.465628} & 0.434389          & 0.2865          & 0.3051          & 0.2646          & 0.42                   \\ \hline
MNIST-C          & 0.1056          & 0.103879         & 0.432845 & 0.419588          & 0.1287          & 0.1337          & 0.1812          & \textbf{0.95}                   \\ \hline
MVTec-AD         & 0.6316          & 0.862069         & 0.848921          & 0.811189          & 0.7891          & 0.5682          & 0.7436          & \textbf{0.93}          \\ \hline
SVHN             & 0.0762          & 0.073698         & 0.1738            & 0.107527          & 0.0967          & 0.1092          & 0.1169          & \textbf{0.21}          \\ \hline
Agnews           & 0.064           & 0.071809         & 0.131455 & 0.102027          & 0.0623          & 0.0671          & 0.0676          & \textbf{0.44}                \\ \hline
Amazon           & 0.0665          & 0.084433         & 0.082116          & 0.14746  & 0.0879          & 0.0785          & 0.0484          & \textbf{0.32}                 \\ \hline
Imdb             & 0.0373          & 0.045063         & 0.036543          & 0.170682          & 0.0491          & 0.0398          & 0.0442          & \textbf{0.39}          \\ \hline
Yelp             & 0.0968          & 0.118421         & 0.128492          & 0.098291          & 0.1148          & 0.1134          & 0.0636          & \textbf{0.24}          \\ \hline
20newsgroups     & 0.0959          & 0.085837         & 0.196581          & 0.152125          & 0.0987          & 0.0976          & 0.0846          & \textbf{0.36}             \\ \hline
ecoli            & 0.1778          & 0.185185         & 0                 & 0                 & 0.1687          & \textbf{0.359}  & 0.2258          & 0.1                    \\ \hline
cmc              & 0.0123          & 0.011834         & 0.164948          & 0.197531 & 0.0349          & 0               & 0.0122          & \textbf{0.54}                    \\ \hline
lympho h         & \textbf{0.5455} & 0.413793         & 0.444444          & 0.363636          & 0.3846          & 0.5217          & 0.2703          & 0.45                    \\ \hline
wbc h            & 0.4746          & 0.478873         & 0.444444          & 0.403846          & 0.425           & 0.5172          & 0.3516          & \textbf{0.86}         \\ \hline

\end{tabular}
}
\caption{\textbf{F1 Score} values of the 8 algorithms on the 60 multivariate datasets. The highest value(s) is marked in bold.
The empty spaces denote that the algorithm exceeded a runtime of three hours without successfully generating results.}
\label{table25}
\end{table*}

\begin{table*}[t]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Dataset        & ECOD   & COPOD    & KNN      & LUNAR    & GOAD   & PCA    & DSVDD  & RN-Net        \\ \hline
yahoo1         & 0.0276 & 0.022989 & 0.021277 & 0.014925 & 0.0197 & 0.0281 & 0.0235 & \textbf{1}      \\ \hline
yahoo2         & 0.1039 & 0.108108 & 0.216216 & 0.085106 & 0.1013 & 0.039  & 0.0889 & \textbf{0.94}   \\ \hline
yahoo3         & 0.1039 & 0.111111 & 0.311111 & 0.056911 & 0.0547 & 0.0864 & 0.1207 & \textbf{1}   \\ \hline
yahoo5         & 0.0779 & 0.042553 & 0.086331 & 0.034483 & 0.0385 & 0      & 0.0403 & \textbf{0.41} \\ \hline
yahoo6         & 0.0541 & 0.033613 & 0.019656 & 0.016632 & 0.0394 & 0.0151 & 0.0377 & \textbf{1}   \\ \hline
yahoo7         & 0.0543 & 0.047619 & 0.037152 & 0.036364 & 0.0885 & 0.0324 & 0.0381 & \textbf{0.42}   \\ \hline
yahoo8         & 0.011  & 0.021053 & 0.018797 & \textbf{0.026273} & 0.0147 & 0      & 0.0112 & 0.01    \\ \hline
yahoo9         & 0.0904 & 0.088889 & 0.134454 & 0.065844 & 0.086  & 0.0757 & 0.0842 & \textbf{0.77}      \\ \hline
Speed\_6005    & 0.0083 & 0.00813  & 0.05     & 0.055556 & 0.007  & 0.01   & 0.0075 & \textbf{0.33}   \\ \hline
Speed\_7578    & 0.0708 & 0.063492 & 0.097561 & 0.115942 & 0.0428 & 0.0769 & \textbf{0.2051} & 0.16   \\ \hline
Speed\_t4013   & 0.0161 & 0.016461 & 0.066667 & 0.076923 & 0.0176 & 0.0267 & 0.0800 & \textbf{0.44}   \\ \hline
TravelTime\_387    & 0.0158 & 0.017316 & 0.022989 & 0.017021 & 0.0054 & 0.0168 & 0.0101 & \textbf{0.09} \\ \hline
TravelTime\_451    & 0.0093 & 0.010309 & 0.013423 & 0.008929 & 0.0114 & 0.0093 & 0.0000 & \textbf{0.08} \\ \hline
Occupancy\_6005    & 0.0078 & 0.009009 & 0.011299 & 0.013514 & 0.006  & 0.0091 & 0.0102 & \textbf{0.12}    \\ \hline
Occupancy\_t4013   & 0.0156 & 0.015209 & 0.022727 & 0.012903 & 0.0144 & 0.0155 & 0.0148 & \textbf{0.5} \\ \hline
yahoo\_syn1        & 0.1509 & 0.129032 & 0.118812 & 0.08     & 0.1081 & 0.6486 & 0.1304 & \textbf{0.29}   \\ \hline
yahoo\_syn2        & 0.2156 & 0.226415 & 0.382979 & 0.17734  & 0.2034 & 0.1302 & 0.1809 & \textbf{0.43}  \\ \hline
yahoo\_syn3        & 0.2036 & 0.235294 & 0.523077 & 0.133858 & 0.1221 & 0.1916 & 0.2429 & \textbf{0.76} \\ \hline
yahoo\_syn5        & 0.1963 & 0.144737 & 0.198758 & 0.07024  & 0.1149 & 0      & 0.1212 & \textbf{0.59}  \\ \hline
yahoo\_syn6        & 0.0519 & 0.032129 & 0.046036 & 0.041408 & 0.0357 & 0      & 0.0235 & \textbf{0.44}\\ \hline
yahoo\_syn7        & 0.1458 & 0.113636 & 0.092219 & 0.07619  & 0.0892 & 0.1183 & 0.0957 & \textbf{0.33}      \\ \hline
yahoo\_syn8        & 0.0314 & 0.047138 & 0.048417 & 0.062112 & 0.0274 & 0      & 0.0272 & \textbf{0.1}  \\ \hline
yahoo\_syn9        & 0.1837 & 0.185567 & \textbf{0.26087}  & 0.128571 & 0.1722 & 0.1354 & 0.1706 & 0.11        \\ \hline
aws1           & 0.0189 & 0.019417 & 0.029412 & 0.013889 & 0.0175 & 0.0192 & 0.0192 & \textbf{1}   \\ \hline
aws2           & 0.0196 & 0.006061 & 0.166667 & 0.166667 & 0.2222 & 0.3077 & 0.1333 & \textbf{0.67}\\ \hline
aws3           & 0.0128 & 0.013333 & 0.021053 & 0.004988 & 0.0135 & 0.0123 & 0.0143 & \textbf{0.67}  \\ \hline
aws\_syn1 & 0.1695 & 0.17094  & 0.229885 & 0.066225 & 0.1538 & 0.1552 & 0.0758 & \textbf{0.9}  \\ \hline
aws\_syn2 & 0.1688 & 0.049566 & 0.645161 & 0.645161 & 0.7143 & 0.7843 & 0.0245 & \textbf{0.8} \\ \hline
aws\_syn3 & 0.1258 & 0.125    & 0.176991 & 0.048662 & 0.1183 & 0.1125 & 0.0514 & \textbf{0.9} \\ \hline
\end{tabular}
}
\caption{\textbf{F1 Score} values of the 8 algorithms on the 29 univariate datasets. The highest value(s) is marked in bold.}
\label{table26}
\end{table*}



\begin{table*}[h]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Dataset          & ECOD            & COPOD             & KNN               & LUNAR             & GOAD            & PCA             & DSVDD           & RN-Net     \\ \hline
ALOI             & 0.5006          & 0.500049          & \textbf{0.586931} & 0.57297           & 0.5199          & 0.5162          & 0.5079          & 0.58                    \\ \hline
annthyroid       & 0.6362          & 0.617689          & 0.65297           & \textbf{0.880726} & 0.5351          & 0.5939          & 0.5417          & 0.86                  \\ \hline
backdoor         & 0.6364          & 0.667839          & 0.9096   & 0.880904          &                 & 0.8788          & 0.8921          & \textbf{0.99}                   \\ \hline
breastw          & 0.6506          & 0.976672 & 0.953829          & 0.947232          & 0.943           & 0.6987          & 0.9212          & \textbf{0.98}                  \\ \hline
campaign         & 0.6524          & 0.675778          & 0.628369          & 0.704723 & 0.6355          & 0.6245          & 0.5723          & \textbf{0.84}                  \\ \hline
cardio           & 0.7509          & 0.806044          & 0.863355          & \textbf{0.898903} & 0.5433          & 0.8018          & 0.6400          & 0.89                   \\ \hline
Cardiotocography & 0.5997          & 0.613481          & 0.716825 & 0.704852          & 0.701           & 0.5939          & 0.5537          & \textbf{0.81}                    \\ \hline
celeba           & 0.6705          & 0.67125           & 0.5512            & 0.553441          &                 & 0.7015          & 0.4683          & \textbf{0.85}           \\ \hline
cover            & 0.794           & 0.728794          & 0.6227            & \textbf{0.842}    &                 & 0.8361          & 0.4696          & \textbf{1}                     \\ \hline
donors           & 0.7067          & 0.711624 &                   &                   &                 & 0.5811          & 0.6765          & \textbf{0.99}                    \\ \hline
fault            & 0.4981          & 0.485851          & 0.6742            & \textbf{0.716637} &                 & 0.5431          & 0.5331          & 0.66                    \\ \hline
fraud            & 0.8938          & 0.889838          & 0.9015            & 0.8793            &                 & \textbf{0.9281} & 0.7794          & 0.8                   \\ \hline
glass            & 0.5623          & 0.617886          & 0.6228            & 0.615447          & 0.613           & 0.5623          & \textbf{0.7724}          & 0.57           \\ \hline
Hepatitis        & 0.5321          & 0.685993          & 0.508611          & 0.524684          & 0.4862          & 0.5706          & \textbf{0.8731} & 0.63                  \\ \hline
http             & 0.9517 & 0.949204          & 0.948803          &                   &                 & 0.9516          & 0.4587          & \textbf{0.97}                   \\ \hline
InternetAds      & 0.6608          & 0.68062           & 0.760608          & 0.783551          & 0.5326          & 0.6007          & \textbf{0.8081} & 0.69                    \\ \hline
Ionosphere       & 0.6181          & 0.551587          & 0.912381          & 0.893333          & 0.7663          & 0.641           & 0.9211 & \textbf{0.97}                   \\ \hline
landsat          & 0.4862          & 0.499032          & 0.698389 & 0.643571          & 0.5031          & 0.4679          & 0.6252          & \textbf{0.72}                   \\ \hline
letter           & 0.511           & 0.4847            & 0.7617   & 0.74              & 0.51            & 0.5137          & 0.6387          & \textbf{0.84}                 \\ \hline
Lymphography     & 0.9718          & 0.950704          & 0.9472            & 0.908451          & 0.9437          & \textbf{0.9754} & 0.9014          & 0.67                  \\ \hline
magic.gamma      & 0.577           & 0.607467          & \textbf{0.757772} & 0.739217          &                 & 0.6017          & 0.6197          & 0.56                   \\ \hline
mammography      & 0.8337          & 0.842778          & 0.792274          & 0.730872          & \textbf{0.9718} & 0.7508          & 0.7244          & 0.68                     \\ \hline
mnist            & 0.553           & 0.620569          & 0.851696 & 0.843281          & 0.8293          & 0.6733          & 0.6211          & \textbf{0.94}                   \\ \hline
musk             & 0.8578          & 0.862118          & 0.916695          & 0.902024          & 0.6339          & 0.9634 & 0.9465          & \textbf{1}                   \\ \hline
optdigits        & 0.4878          & 0.497             & 0.956376          & 0.957856 & 0.5085          & 0.4508          & \textbf{1}          & 0.88                   \\ \hline
PageBlocks       & 0.6989          & 0.690503          & 0.614889          & \textbf{0.847926} & 0.7301          & 0.7239          & 0.7189          & 0.73                    \\ \hline
pendigits        & 0.7861          & 0.739903          & 0.955839          & \textbf{0.958743} & 0.8539          & 0.786           & 0.6280          & 0.86                   \\ \hline
Pima             & 0.5506          & 0.597731          & 0.611985          & 0.830597          & 0.5414          & \textbf{0.8503} & 0.5078          & 0.63                  \\ \hline
satellite        & 0.6134          & 0.65824           & 0.810343 & 0.779802          & 0.7808          & 0.6892          & 0.7260          & \textbf{0.94}                    \\ \hline
satimage-2       & 0.9054          & 0.921633          & 0.953332          & 0.956472 & 0.9412          & 0.9192          & 0.9181          & \textbf{0.99}                   \\ \hline
shuttle          & 0.9741 & 0.973126          & 0.95899           & 0.939751          & 0.9489          & 0.9646          & 0.9141          & \textbf{0.99}                      \\ \hline
skin             & 0.4587          & 0.459993          & \textbf{0.950242} & 0.942188          &                 & 0.4373          & 0.5679          & 0.7                 \\ \hline
smtp             & 0.7997          & 0.80014           & 0.820707          & 0.900206 &                 & 0.8164          & 0.8867          & \textbf{0.91}                   \\ \hline
SpamBase         & 0.5306          & 0.587624          & 0.656111          & 0.664374 & 0.5039          & 0.4981          & 0.6423          & \textbf{0.84}                   \\ \hline
speech           & 0.5057          & 0.504194          & 0.516843          & 0.567285          & 0.555           & 0.5074          & \textbf{0.5838} & 0.53                    \\ \hline
Stamps           & 0.6225          & 0.899885          & \textbf{0.919198} & 0.854682          & 0.8692          & 0.6176          & 0.6884          & 0.84                   \\ \hline
thyroid          & 0.9391 & 0.903366          & 0.911189          & 0.921398          & 0.8371          & 0.8713          & 0.8382          & \textbf{0.94}                    \\ \hline
vertebral        & 0.5048 & 0.435714          & 0.459524          & 0.461905          & 0.4762          & 0.4405          & 0.4667          & \textbf{0.79}                   \\ \hline
vowels           & 0.5734          & 0.498791          & \textbf{0.833172} & 0.826102          & 0.8197          & 0.515           & 0.6086          & 0.51                   \\ \hline
Waveform         & 0.5141          & 0.522459          & \textbf{0.69372}  & 0.679511          & 0.5546          & 0.5224          & 0.6141          & 0.62                   \\ \hline
WBC              & \textbf{0.9624}          & 0.960094          & 0.955399          & 0.948357          & 0.9507          & 0.9601          & 0.9437          & 0.95           \\ \hline
WDBC             & 0.9566          & 0.952381          & 0.94958           & 0.938375          & 0.9496          & 0.9594 & 0.9202          & \textbf{1}                    \\ \hline
Wilt             & 0.477           & 0.455694          & 0.636765 & 0.456297          & 0.5113          & 0.454           & 0.4541          & \textbf{0.75}                  \\ \hline
wine             & 0.5954          & 0.841176          & 0.957983          & 0.89958           & 0.9706 & 0.5622          & 0.8824          & \textbf{1}                    \\ \hline
WPBC             & 0.4657          & 0.492884          & 0.506834          & 0.471608          & 0.4822          & 0.4763          & 0.5900 & \textbf{0.65}                  \\ \hline
yeast            & 0.5037          & 0.495611          & 0.502851          & 0.476474          & 0.5554 & 0.5341          & 0.4893          & \textbf{0.69}                  \\ \hline
CIFAR10          & 0.5903          & 0.587581          & 0.6231            & 0.589881          & \textbf{0.6731} & 0.6046          & 0.5631          & 0.61                  \\ \hline
FashionMNIST     & 0.6474          & 0.640718          & \textbf{0.859675} & 0.835119          & 0.6907          & 0.6904          & 0.6844          & 0.71                   \\ \hline
MNIST-C          & 0.5308          & 0.529421          & 0.825             & 0.828579 & 0.55            & 0.5531          & 0.5971          & \textbf{0.98}                     \\ \hline
MVTec-AD         & 0.7337          & 0.890275          & 0.931136 & 0.912283          & 0.9035          & 0.6984          & 0.8839          & \textbf{0.96}                \\ \hline
SVHN             & 0.5075          & 0.505446          & \textbf{0.58398}           & 0.532056          & 0.5238          & 0.5339          & 0.5408          & 0.45           \\ \hline
Agnews           & 0.4979          & 0.504             & 0.551    & 0.527842          & 0.4964          & 0.4999          & 0.5006          & \textbf{0.69}                    \\ \hline
Amazon           & 0.4998          & 0.513895          & 0.512789          & 0.572053 & 0.5166          & 0.5093          & 0.4859          & \textbf{0.64}                    \\ \hline
Imdb             & 0.4767          & 0.482684          & 0.478789          & 0.617421 & 0.4859          & 0.4785          & 0.4825          & \textbf{0.72}                   \\ \hline
Yelp             & 0.5238          & 0.541053          & 0.547789          & 0.525053          & 0.5381          & 0.5367          & 0.4980          & \textbf{0.58}            \\ \hline
20newsgroups     & 0.5232          & 0.515208          & 0.60371  & 0.566282          & 0.5255          & 0.5246          & 0.5128          & \textbf{0.64}                    \\ \hline
ecoli            & 0.6733          & 0.716616          & 0.465659          & 0.454327          & 0.7864          & \textbf{0.8537} & 0.8186          & 0.54                 \\ \hline
cmc              & 0.4796          & 0.477557          & 0.82212           & 0.846585 & 0.536           & 0.4516          & 0.4793          & \textbf{0.86}                  \\ \hline
lympho h         & \textbf{0.9648} & 0.940141          & 0.947183          & 0.79108           & 0.8638          & 0.9613          & 0.8251          & 0.91                 \\ \hline
wbc h            & 0.7997          & 0.858543          & 0.869748          & 0.913165 & 0.8459          & 0.8263          & 0.8053          & \textbf{0.98}                \\ \hline
\end{tabular}
}
\caption{\textbf{AUC-ROC} values of the 8 algorithms on the 60 multivariate datasets. The highest value(s) is marked in bold.
The empty spaces denote that the algorithm exceeded a runtime of three hours without successfully generating results.}
\label{table27}
\end{table*}


\begin{table*}[t]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Dataset        & ECOD   & COPOD             & KNN               & LUNAR             & GOAD   & PCA    & DSVDD           & RN-Net     \\ \hline
yahoo1         & 0.9503 & 0.940056          & 0.93512           & 0.906911          & 0.9298 & 0.9403 & 0.9415          & \textbf{1}       \\ \hline
yahoo2         & 0.9525 & 0.954577          & 0.980041          & 0.940812          & 0.9511 & 0.6383 & 0.9436          & \textbf{0.99}             \\ \hline
yahoo3         & 0.9518 & 0.955276 & 0.927018          & 0.856787          & 0.8533 & 0.8861 & 0.9022          & \textbf{1}                \\ \hline
yahoo5         & 0.7841 & 0.620987          & 0.789424          & \textbf{0.82153}  & 0.6157 & 0.4508 & 0.6182          & 0.41          \\ \hline
yahoo6         & 0.9506 & 0.918843          & 0.85921           & 0.833098          & 0.9312 & 0.8155 & 0.9280          & \textbf{1}       \\ \hline
yahoo7         & 0.6769 & 0.702326          & 0.681056          & \textbf{0.708236} & 0.6982 & 0.5851 & 0.6234          & 0.39               \\ \hline
yahoo8         & 0.4991 & 0.568563          & 0.59521           & 0.723054 & 0.5126 & 0.4512 & 0.4967          & \textbf{0.82}              \\ \hline
yahoo9         & 0.9519 & 0.950957          & \textbf{0.969199 }         & 0.932117          & 0.9492 & 0.8867 & 0.9480          & 0.59           \\ \hline
Speed\_6005    & 0.9522 & 0.95118           & 0.992397          & 0.993197          & 0.9432 & 0.9604 & 0.9470          & \textbf{1}     \\ \hline
Speed\_7578    & 0.9533 & 0.947462          & 0.967053          & 0.972841          & 0.9203 & 0.9573 & 0.9862 & \textbf{1}                 \\ \hline
Speed\_t4013   & 0.9511 & 0.952066          & 0.988769          & 0.990373          & 0.9553 & 0.9797 & 0.9908          & \textbf{1}       \\ \hline
TravelTime\_387    & 0.7837 & 0.788079          & \textbf{0.799493}          & 0.787278          & 0.594  & 0.7867 & 0.6278          & 0.36       \\ \hline
TravelTime\_451    & 0.9509 & 0.955576          & \textbf{0.965988}          & 0.948635          & 0.96   & 0.9509 & 0.4537          & 0.1         \\ \hline
Occupancy\_6005    & 0.9468 & 0.953762          & 0.96322           & 0.969315          & 0.9309 & 0.9544 & 0.9592          & \textbf{1}      \\ \hline
Occupancy\_t4013   & 0.9496 & 0.948159          & 0.965572          & 0.938751          & 0.9452 & 0.9492 & 0.9468          & \textbf{1}     \\ \hline
yahoo\_syn1        & 0.9521 & 0.942472          & 0.93679           & 0.901989          & 0.9297 & 0.9954 & 0.9432          & \textbf{1}           \\ \hline
yahoo\_syn2        & 0.9546 & 0.95738           & \textbf{0.979903} & 0.942134          & 0.9511 & 0.757  & 0.9435          & 0.97              \\ \hline
yahoo\_syn3        & 0.9261 & 0.959119          & 0.96174           & 0.895702          & 0.8648 & 0.898  & 0.9355          & \textbf{1}          \\ \hline
yahoo\_syn5        & 0.8757 & 0.746273          & 0.876435 & 0.821884          & 0.7118 & 0.4518 & 0.7150          & \textbf{0.98}                 \\ \hline
yahoo\_syn6        & 0.5949 & 0.561347          & 0.691577          & \textbf{0.695181} & 0.5702 & 0.4531 & 0.5289          & 0.56               \\ \hline
yahoo\_syn7        & 0.7863 & \textbf{0.788838} & 0.788082          & 0.766213          & 0.628  & 0.7158 & 0.6848          & 0.64              \\ \hline
yahoo\_syn8        & 0.5247 & 0.594162          & 0.674102          & 0.819162 & 0.5129 & 0.4518 & 0.5226          & \textbf{0.96}                \\ \hline
yahoo\_syn9        & 0.9522 & 0.952751          & 0.969498          & 0.927033          & 0.9483 & 0.813  & 0.9477          & \textbf{1}             \\ \hline
aws1           & 0.9504 & 0.951813          & 0.968511          & 0.932252          & 0.9466 & 0.9513 & 0.9513          & \textbf{1}    \\ \hline
aws2           & 0.9597 & 0.867955          & 0.995974          & 0.995974          & 0.9972 & \textbf{0.9982} & 0.7476          & 0.5        \\ \hline
aws3           & 0.9486 & 0.950601          & 0.968959          & 0.866822          & 0.9513 & 0.9463 & 0.9539          & \textbf{1}     \\ \hline
aws\_syn1 & 0.9528 & 0.953321          & 0.967757          & 0.864293          & 0.9471 & 0.9033 & 0.8826          & \textbf{1}           \\ \hline
aws\_syn2 & 0.9601 & 0.844485          & 0.995539          & 0.995539          & 0.9968 & \textbf{0.9978} & 0.5647          & 0.47     \\ \hline
aws\_syn3 & 0.9533 & 0.952989          & 0.968771 & 0.868704          & 0.95   & 0.9027 & 0.8761          & \textbf{1}             \\ \hline
\hline
\end{tabular}
}
\caption{\textbf{AUC-ROC} values of the 8 algorithms on the 29 univariate datasets. The highest value(s) is marked in bold.}
\label{table28}
\end{table*}

%%% TABLE 16 BCE Loss vs RN Loss
\begin{table*}[t]
\resizebox{\textwidth}{!}{%
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{Datasets} & \textbf{Precision (RN-Net)} & \textbf{Precision (Feed fwd network(BCE))} & \textbf{Recall (RN-Net)} & \textbf{Recall (Feed fwd network(BCE))} & \textbf{F1 Score (RN-Net)} & \textbf{F1 Score (Feed fwd network(BCE))} & \textbf{AUC-ROC (RN-Net)} & \textbf{AUC-ROC (Feed fwd network(BCE))} \\ \hline
yahoo1 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\ \hline
yahoo2 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\ \hline
yahoo3 & 0.7 & 0.4 & 1 & 0.4 & 0.94 & 0.4 & 0.99 & 0.43 \\ \hline
yahoo5 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0.04 \\ \hline
yahoo6 & 0.82 & 0.15 & 0.4 & 0.7 & 0.5 & 0.24 & 0.41 & 0.65 \\ \hline
yahoo7 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\ \hline
yahoo8 & 0.8 & 0.12 & 0.67 & 0.7 & 0.34 & 0.2 & 0.39 & 0.74 \\ \hline
yahoo9 & 1 & 0.09 & 0.89 & 0.45 & 0.85 & 0.15 & 0.82 & 0.65 \\ \hline
Speed\_6005 & 1 & 0.61 & 1 & 0.57 & 0.72 & 0.46 & 0.59 & 0.53 \\ \hline
Speed\_7578 & 1 & 0.001 & 1 & 1 & 1 & 0.003 & 1 & 0.001 \\ \hline
Speed\_t4013 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\ \hline
TravelTime\_387 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\ \hline
TravelTime\_451 & 0.13 & 0.006 & 1 & 0.34 & 0.5 & 0.012 & 0.36 & 0.27 \\ \hline
Occupancy\_6005 & 0.01 & 0 & 1 & 0 & 0.01 & 0 & 0.1 & 0 \\ \hline
Occupancy\_t4013 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\ \hline
yahoo\_syn1 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\ \hline
yahoo\_syn2 & 1 & 0.44 & 1 & 0.6 & 1 & 0.47 & 1 & 0.59 \\ \hline
yahoo\_syn3 & 0.94 & 1 & 1 & 0.89 & 0.97 & 0.93 & 0.97 & 0.93 \\ \hline
yahoo\_syn5 & 1 & 0.93 & 1 & 0.95 & 1 & 0.93 & 1 & 0.99 \\ \hline
yahoo\_syn6 & 0.16 & 0.34 & 1 & 0.82 & 0.25 & 0.48 & 0.98 & 0.85 \\ \hline
yahoo\_syn7 & 1 & 1 & 0.82 & 0.67 & 0.9 & 0.8 & 0.56 & 0.67 \\ \hline
yahoo\_syn8 & 0.74 & 0.31 & 0.84 & 0.28 & 0.66 & 0.26 & 0.64 & 0.38 \\ \hline
yahoo\_syn9 & 1 & 0.07 & 0.94 & 0.45 & 0.55 & 0.12 & 0.96 & 0.59 \\ \hline
aws1 & 1 & 1 & 1 & 0.81 & 1 & 0.89 & 1 & 0.81 \\ \hline
aws2 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\ \hline
aws3 & 1 & 0 & 0.5 & 0 & 0.67 & 0 & 0.5 & 0 \\ \hline
aws\_syn1 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\ \hline
aws\_syn2 & 0.86 & 0.37 & 1 & 0.6 & 0.92 & 0.39 & 1 & 0.49 \\ \hline
aws\_syn3 & 0.88 & 0.81 & 0.41 & 1 & 0.56 & 0.89 & 0.47 & 0.99 \\ \hline
 & 1 & 0.77 & 1 & 0.8 & 1 & 0.73 & 1 & 0.87 \\ \hline
\end{tabular}
}
\caption{Comparison of RN-Net and Feed forward network (BCE) across Univariate datasets; Main text(Contribution): Efficiency and Adaptability}
\end{table*}


\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Datasets} & \textbf{Precision (RN-Net)} & \textbf{Recall (RN-Net)} & \textbf{F1 Score (RN-Net)} & \textbf{AUC-ROC (RN-Net)} & \textbf{Precision (Feed fwd network(BCE))} & \textbf{Recall (Feed fwd network(BCE))} & \textbf{F1 Score (Feed fwd network(BCE))} & \textbf{AUC-ROC (Feed fwd network(BCE))} \\ \hline

    ALOI & 0.290000 & 0.360000 & 0.290000 & 0.580000 & 0.630000 & 0.650000 & 0.640000 & 0.830000 \\
    annthyroid & 0.650000 & 0.720000 & 0.680000 & 0.860000 & 0.640000 & 1 & 0.780000 & 0.930000 \\
    backdoor & 0.640000 & 0.980000 & 0.780000 & 0.990000 & 0.600000 & 0.970000 & 0.760000 & 0.950000 \\
    breastw & 0.960000 & 0.980000 & 0.970000 & 0.980000 & 0.970000 & 0.950000 & 0.960000 & 0.950000 \\
    campaign & 0.760000 & 0.720000 & 0.740000 & 0.840000 & 0.420000 & 0.060000 & 0.100000 & 0.370000 \\
    cardio & 0.740000 & 0.760000 & 0.740000 & 0.890000 & 0.830000 & 0.380000 & 0.490000 & 0.500000 \\
    Cardiotocography & 0.790000 & 0.640000 & 0.700000 & 0.810000 & 0.970000 & 0.860000 & 0.910000 & 0.950000 \\
    celeba & 0.260000 & 0.760000 & 0.390000 & 0.850000 & 0.220000 & 0.830000 & 0.340000 & 0.830000 \\
    cover & 0.960000 & 0.990000 & 0.980000 & 1 & 0.090000 & 0.690000 & 0.160000 & 0.790000 \\
    donors & 0.910000 & 0.990000 & 0.950000 & 0.990000 & 0.760000 & 0.860000 & 0.810000 & 0.860000 \\
    fault & 0.820000 & 0.450000 & 0.540000 & 0.660000 & 0.860000 & 0.410000 & 0.550000 & 0.650000 \\
    fraud & 0.990000 & 0.760000 & 0.860000 & 0.800000 & 0.260000 & 0.920000 & 0.390000 & 0.970000 \\
    glass & 0.220000 & 1 & 0.320000 & 0.570000 & 0.340000 & 0.830000 & 0.460000 & 0.750000 \\
    Hepatitis & 0.610000 & 0.580000 & 0.590000 & 0.630000 & 0.520000 & 0.620000 & 0.530000 & 0.590000 \\
    http & 0.460000 & 1 & 0.520000 & 0.970000 & 0.010000 & 1 & 0.020000 & 0.800000 \\
    InternetAds & 0.670000 & 0.510000 & 0.560000 & 0.690000 & 0.900000 & 0.570000 & 0.690000 & 0.790000 \\
    Ionosphere & 0.970000 & 0.910000 & 0.940000 & 0.970000 & 0.960000 & 0.800000 & 0.870000 & 0.910000 \\
    landsat & 0.840000 & 0.600000 & 0.640000 & 0.720000 & 0.860000 & 0.860000 & 0.860000 & 0.940000 \\
    letter & 0.200000 & 0.810000 & 0.320000 & 0.840000 & 0.160000 & 0.800000 & 0.290000 & 0.800000 \\
    Lymphography & 0.600000 & 0.600000 & 0.580000 & 0.670000 & 0.700000 & 0.470000 & 0.550000 & 0.490000 \\
    magic.gamma & 0.850000 & 0.390000 & 0.550000 & 0.560000 & 0.960000 & 0.470000 & 0.630000 & 0.620000 \\
    mammography & 0.270000 & 0.560000 & 0.350000 & 0.680000 & 0.860000 & 0.760000 & 0.800000 & 0.920000 \\
    mnist & 0.790000 & 0.840000 & 0.810000 & 0.940000 & 0.910000 & 0.900000 & 0.910000 & 0.950000 \\
    musk & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
    optdigits & 0.990000 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
    PageBlocks & 0.820000 & 0.620000 & 0.690000 & 0.730000 & 0.910000 & 0.660000 & 0.770000 & 0.800000 \\
    pendigits & 0.370000 & 0.710000 & 0.460000 & 0.860000 & 0.430000 & 0.880000 & 0.570000 & 0.950000 \\
    Pima & 0.750000 & 0.620000 & 0.660000 & 0.630000 & 0.760000 & 0.240000 & 0.370000 & 0.500000 \\
    satellite & 0.890000 & 0.880000 & 0.890000 & 0.940000 & 0.950000 & 0.820000 & 0.880000 & 0.920000 \\
    satimage-2 & 0.890000 & 0.950000 & 0.920000 & 0.990000 & 0.630000 & 0.970000 & 0.760000 & 0.990000 \\
    shuttle & 1 & 0.990000 & 0.990000 & 0.990000 & 0.990000 & 0.980000 & 0.990000 & 0.980000 \\
    skin & 0.950000 & 0.510000 & 0.650000 & 0.700000 & 0.870000 & 0.430000 & 0.570000 & 0.750000 \\
    smtp & 0.270000 & 0.860000 & 0.330000 & 0.910000 & 0.800000 & 0.810000 & 0.710000 & 0.880000 \\
    SpamBase & 0.890000 & 0.730000 & 0.800000 & 0.840000 & 0.830000 & 0.520000 & 0.640000 & 0.700000 \\
    speech & 0.060000 & 0.650000 & 0.110000 & 0.530000 & 0.090000 & 0.630000 & 0.160000 & 0.690000 \\
    Stamps & 0.580000 & 0.880000 & 0.680000 & 0.840000 & 0.710000 & 0.490000 & 0.570000 & 0.670000 \\
    thyroid & 0.590000 & 0.850000 & 0.690000 & 0.940000 & 0.340000 & 1 & 0.510000 & 0.930000 \\
    vertebral & 0.690000 & 0.890000 & 0.750000 & 0.790000 & 0.620000 & 0.880000 & 0.720000 & 0.870000 \\
    vowels & 0.170000 & 0.740000 & 0.260000 & 0.510000 & 0.710000 & 0.870000 & 0.770000 & 0.950000 \\
    Waveform & 0.130000 & 0.720000 & 0.210000 & 0.620000 & 0.630000 & 0.610000 & 0.610000 & 0.770000 \\
    WBC & 0.550000 & 1 & 0.700000 & 0.950000 & 0.500000 & 0.780000 & 0.610000 & 0.820000 \\
    WDBC & 1 & 1 & 1 & 1 & 0.920000 & 1 & 0.950000 & 0.990000 \\
    Wilt & 0.360000 & 0.770000 & 0.460000 & 0.750000 & 0.790000 & 0.750000 & 0.770000 & 0.920000 \\
    wine & 1 & 1 & 1 & 1 & 0.910000 & 1 & 0.950000 & 0.990000 \\
    WPBC & 0.630000 & 0.830000 & 0.700000 & 0.650000 & 0.750000 & 0.470000 & 0.500000 & 0.590000 \\
    yeast & 0.740000 & 0.670000 & 0.690000 & 0.690000 & 0.970000 & 0.720000 & 0.820000 & 0.840000 \\
    CIFAR10 & 0.190000 & 0.560000 & 0.280000 & 0.610000 & 0.270000 & 0.540000 & 0.360000 & 0.670000 \\
    FashionMNIST & 0.360000 & 0.530000 & 0.420000 & 0.710000 & 0.470000 & 0.410000 & 0.430000 & 0.620000 \\
    MNIST-C & 0.940000 & 0.960000 & 0.950000 & 0.980000 & 0.870000 & 0.970000 & 0.920000 & 0.990000 \\
    MVTec-AD & 0.970000 & 0.890000 & 0.930000 & 0.960000 & 0.940000 & 0.740000 & 0.820000 & 0.830000 \\
    SVHN & 0.210000 & 0.490000 & 0.210000 & 0.450000 & 0.450000 & 0.570000 & 0.500000 & 0.770000 \\
    Agnews & 0.370000 & 0.670000 & 0.440000 & 0.690000 & 0.410000 & 0.640000 & 0.490000 & 0.790000 \\
    Amazon & 0.240000 & 0.510000 & 0.320000 & 0.640000 & 0.410000 & 0.580000 & 0.470000 & 0.770000 \\
    Imdb & 0.290000 & 0.690000 & 0.390000 & 0.720000 & 0.360000 & 0.620000 & 0.450000 & 0.780000 \\
    Yelp & 0.220000 & 0.390000 & 0.240000 & 0.580000 & 0.440000 & 0.590000 & 0.490000 & 0.770000 \\
    20newsgroups & 0.280000 & 0.700000 & 0.360000 & 0.640000 & 0.510000 & 0.700000 & 0.590000 & 0.840000 \\
    \bottomrule
    \end{tabular}
    }
    \caption{RN-Loss vs BCE Loss on Multivariate datasets; Main text(Contribution): Efficiency and Adaptability}
\end{table*}


\begin{table*}[t]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
        \hline
        Dataset & \multicolumn{4}{c|}{W/O Regularisation} & \multicolumn{4}{c|}{With L2 Regularisation} \\
        \hline
        & Precision & Recall & F-1 Score & AUC-ROC & Precision & Recall & F-1 Score & AUC-ROC \\
        \hline
        yahoo1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        yahoo2 & 0.7 & 1 & 0.94 & 0.99 & 0.875 & 1 & 0.94 & 0.99 \\
        yahoo3 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        yahoo5 & 0.82 & 0.4 & 0.5 & 0.41 & 0.08 & 0.67 & 0.15 & 0.65 \\
        yahoo6 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        yahoo7 & 0.8 & 0.67 & 0.34 & 0.39 & 0.06 & 0.8 & 0.11 & 0.82 \\
        yahoo8 & 1 & 0.89 & 0.85 & 0.82 & 1 & 0.89 & 0.94 & 0.97 \\
        yahoo9 & 1 & 1 & 0.72 & 0.59 & 1 & 1 & 1 & 1 \\
        Speed\_6005 & 1 & 1 & 1 & 1 & 0.5 & 1 & 0.67 & 0.99 \\
        Speed\_7578 & 1 & 1 & 1 & 1 & 0.17 & 0.75 & 0.27 & 0.84 \\
        Speed\_t4013 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        TravelTime\_387 & 0.13 & 1 & 0.5 & 0.36 & 0.16 & 0.67 & 0.26 & 0.74 \\
        TravelTime\_451 & 0.01 & 1 & 0.01 & 0.1 & 1 & 1 & 1 & 1 \\
        Occupancy\_6005 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        Occupancy\_t4013 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        yahoo\_syn1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        yahoo\_syn2 & 0.94 & 1 & 0.97 & 0.97 & 0.94 & 1 & 0.97 & 0.99 \\
        yahoo\_syn3 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        yahoo\_syn5 & 0.16 & 1 & 0.25 & 0.98 & 0.26 & 1 & 0.41 & 0.98 \\
        yahoo\_syn6 & 1 & 0.82 & 0.9 & 0.56 & 1 & 0.91 & 0.95 & 0.98 \\
        yahoo\_syn7 & 0.74 & 0.84 & 0.66 & 0.64 & 0.12 & 0.78 & 0.2 & 0.84 \\
        yahoo\_syn8 & 1 & 0.94 & 0.55 & 0.96 & 1 & 0.94 & 0.97 & 0.96 \\
        yahoo\_syn9 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        aws1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        aws2 & 1 & 0.5 & 0.67 & 0.5 & 1 & 0.5 & 0.67 & 0.5 \\
        aws3 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        aws\_syn1 & 0.86 & 1 & 0.92 & 1 & 0.9 & 1 & 0.95 & 0.99 \\
        aws\_syn2 & 0.88 & 0.41 & 0.56 & 0.47 & 0.81 & 1 & 0.89 & 0.99 \\
        aws\_syn3 & 1 & 1 & 1 & 1 & 0.9 & 1 & 0.95 & 0.99 \\
        \hline
    \end{tabular}
    \caption{Performance comparison of Neural Networks with and without L2 Regularisation across different datasets; Main Text(Network Architecture)}
    \label{tab:regularization-comparison-dataset1}
\end{table*}

\begin{table*}[t]
    \centering
    \scriptsize 
    \setlength{\tabcolsep}{1mm} 
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
        \hline
        Dataset & \multicolumn{4}{c|}{W/O Regularisation} & \multicolumn{4}{c|}{With L2 Regularisation} \\
        \hline
        & Precision & Recall & F-1 Score & AUC-ROC & Precision & Recall & F-1 Score & AUC-ROC \\
        \hline
        ALOI & 0.290000 & 0.360000 & 0.290000 & 0.580000 & 0.290000 & 0.570000 & 0.390000 & 0.710000 \\
        annthyroid & 0.650000 & 0.720000 & 0.680000 & 0.860000 & 0.730000 & 0.830000 & 0.780000 & 0.930000 \\
        backdoor & 0.640000 & 0.980000 & 0.780000 & 0.990000 & 0.650000 & 0.980000 & 0.780000 & 0.990000 \\
        breastw & 0.960000 & 0.980000 & 0.970000 & 0.980000 & 0.969388 & 0.945274 & 0.957179 & 0.979478 \\
        campaign & 0.760000 & 0.720000 & 0.740000 & 0.840000 & 0.760563 & 0.506592 & 0.608127 & 0.710631 \\
        cardio & 0.740000 & 0.760000 & 0.740000 & 0.890000 & 0.904762 & 0.892617 & 0.898649 & 0.963514 \\
        Cardiotocography & 0.790000 & 0.640000 & 0.700000 & 0.810000 & 0.884615 & 0.695214 & 0.778561 & 0.841557 \\
        celeba & 0.260000 & 0.760000 & 0.390000 & 0.850000 & 0.263705 & 0.896664 & 0.407551 & 0.892697 \\
        cover & 0.960000 & 0.990000 & 0.980000 & 1 & 1 & 1 & 1 & 1 \\
        donors & 0.910000 & 0.990000 & 0.950000 & 0.990000 & 0.897987 & 0.999540 & 0.946046 & 0.981765 \\
        fault & 0.820000 & 0.450000 & 0.540000 & 0.660000 & 0.890459 & 0.439791 & 0.588785 & 0.676121 \\
        fraud & 0.990000 & 0.760000 & 0.860000 & 0.800000 & 0.187430 & 0.828784 & 0.305721 & 0.921894 \\
        glass & 0.220000 & 1 & 0.320000 & 0.570000 & 0.800000 & 1 & 0.888889 & 0.985887 \\
        Hepatitis & 0.610000 & 0.580000 & 0.590000 & 0.630000 & 0.714286 & 0.833333 & 0.769231 & 0.825397 \\
        http & 0.460000 & 1 & 0.520000 & 0.970000 & 0.921875 & 0.951613 & 0.936508 & 0.951602 \\
        InternetAds & 0.670000 & 0.510000 & 0.560000 & 0.690000 & 0.841121 & 0.862620 & 0.851735 & 0.931283 \\
        Ionosphere & 0.970000 & 0.910000 & 0.940000 & 0.970000 & 0.959596 & 0.887850 & 0.922330 & 0.970038 \\
        landsat & 0.840000 & 0.600000 & 0.640000 & 0.720000 & 0.761307 & 0.801587 & 0.780928 & 0.884916 \\
        letter & 0.200000 & 0.810000 & 0.320000 & 0.840000 & 0.300000 & 0.800000 & 0.320000 & 0.840000 \\
        Lymphography & 0.600000 & 0.600000 & 0.580000 & 0.670000 & 0.666667 & 0.666667 & 0.666667 & 0.740310 \\
        magic.gamma & 0.850000 & 0.390000 & 0.550000 & 0.560000 & 0.907664 & 0.445061 & 0.597262 & 0.634999 \\
        mammography & 0.270000 & 0.560000 & 0.350000 & 0.680000 & 0.438395 & 0.708333 & 0.541593 & 0.888436 \\
        mnist & 0.790000 & 0.840000 & 0.810000 & 0.940000 & 0.886943 & 0.936134 & 0.910875 & 0.980081 \\
        musk & 1 & 1 & 1 & 1 & 0.908784 & 0.904202 & 0.906487 & 0.970605 \\
        optdigits & 0.990000 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        PageBlocks & 0.820000 & 0.620000 & 0.690000 & 0.730000 & 1 & 1 & 1 & 1 \\
        pendigits & 0.370000 & 0.710000 & 0.460000 & 0.860000 & 0.839879 & 0.640553 & 0.726797 & 0.789426 \\
        Pima & 0.750000 & 0.620000 & 0.660000 & 0.630000 & 0.403077 & 0.984962 & 0.572052 & 0.990586 \\
        satellite & 0.890000 & 0.880000 & 0.890000 & 0.940000 & 0.838462 & 0.478070 & 0.608939 & 0.700175 \\
        satimage-2 & 0.890000 & 0.950000 & 0.920000 & 0.990000 & 0.948328 & 0.901213 & 0.924171 & 0.967862 \\
        shuttle & 1 & 0.990000 & 0.990000 & 0.990000 & 0.641304 & 1 & 0.781457 & 0.999606 \\
        skin & 0.950000 & 0.510000 & 0.650000 & 0.700000 & 0.999665 & 0.998995 & 0.999330 & 0.998995 \\
        smtp & 0.270000 & 0.860000 & 0.330000 & 0.910000 & 0.998447 & 0.619380 & 0.764505 & 0.755790 \\
        SpamBase & 0.890000 & 0.730000 & 0.800000 & 0.840000 & 1 & 0.722222 & 0.838710 & 0.843474 \\
        speech & 0.060000 & 0.650000 & 0.110000 & 0.530000 & 0.934837 & 0.783613 & 0.852571 & 0.905548 \\
        Stamps & 0.580000 & 0.880000 & 0.680000 & 0.840000 & 0.056231 & 0.711538 & 0.104225 & 0.568951 \\
        thyroid & 0.590000 & 0.850000 & 0.690000 & 0.940000 & 0.702703 & 0.962963 & 0.812500 & 0.943847 \\
        vertebral & 0.690000 & 0.890000 & 0.750000 & 0.790000 & 0.754717 & 1 & 0.860215 & 0.998433 \\
        vowels & 0.170000 & 0.740000 & 0.260000 & 0.510000 & 0.818182 & 0.692308 & 0.750000 & 0.887668 \\
        Waveform & 0.130000 & 0.720000 & 0.210000 & 0.620000 & 0.666667 & 1 & 0.800000 & 0.992891 \\
        WBC & 0.550000 & 1 & 0.700000 & 0.950000 & 0.362963 & 0.576471 & 0.445455 & 0.823178 \\
        WDBC & 1 & 1 & 1 & 1 & 0.562500 & 1 & 0.720000 & 0.972222 \\
        Wilt & 0.360000 & 0.770000 & 0.460000 & 0.750000 & 1 & 1 & 1 & 1 \\
        wine & 1 & 1 & 1 & 1 & 0.472376 & 0.780822 & 0.588640 & 0.877530 \\
        WPBC & 0.630000 & 0.830000 & 0.700000 & 0.650000 & 1 & 1 & 1 & 1 \\
        yeast & 0.740000 & 0.670000 & 0.690000 & 0.690000 & 0.820513 & 0.800000 & 0.810127 & 0.866304 \\
        CIFAR10 & 0.190000 & 0.560000 & 0.280000 & 0.610000 & 0.810160 & 0.740831 & 0.773946 & 0.771540 \\
        FashionMNIST & 0.360000 & 0.530000 & 0.420000 & 0.710000 & 0.326214 & 0.626866 & 0.429119 & 0.719415 \\
        MNIST-C & 0.940000 & 0.960000 & 0.950000 & 0.980000 & 0.967816 & 0.990588 & 0.979070 & 0.998420 \\
        MVTec-AD & 0.970000 & 0.890000 & 0.930000 & 0.960000 & 0.960784 & 0.907407 & 0.933333 & 0.968062 \\
        SVHN & 0.210000 & 0.490000 & 0.210000 & 0.450000 & 0.485075 & 0.588235 & 0.531697 & 0.807800 \\
        Agnews & 0.370000 & 0.670000 & 0.440000 & 0.690000 & 0.330508 & 0.734118 & 0.455807 & 0.809369 \\
        Amazon & 0.240000 & 0.510000 & 0.320000 & 0.640000 & 0.231388 & 0.541176 & 0.324172 & 0.666956 \\
        Imdb & 0.290000 & 0.690000 & 0.390000 & 0.720000 & 0.356771 & 0.644706 & 0.459346 & 0.791407 \\
        Yelp & 0.220000 & 0.390000 & 0.240000 & 0.580000 & 0.359143 & 0.512941 & 0.422481 & 0.683945 \\
        20newsgroups & 0.280000 & 0.700000 & 0.360000 & 0.640000 & 0.405882 & 0.539062 & 0.463087 & 0.749506 \\

        \bottomrule
    \end{tabular}
    }
    \caption{Performance comparison of Neural Networks with and without L2 Regularisation across different Multivariate datasets.}
    \label{tab:regularization-comparison-dataset2}
\end{table*}

\begin{sidewaystable*}[htbp]
\centering
\tiny
\renewcommand{\arraystretch}{3}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Dataset & LOF & IForest & OCSVM & AutoEncoders & DAGMM & Envelope & DevNet & GDN & GAN & MGBTAI & d-BTAI & RN-Net & RN-LSTM\\
\hline
BATADAL04 & (0.38, 0.26, 0.65) & (0.10, 0.10, 0.52) & (\textbf{0.84}, 0.16, 0.67) & (0.17, 0.12, 0.54) & (0.36, 0.25, 0.64) & (0.50, 0.34, 0.71) & (0.10, 0.07, 0.50) & (0.50, 0.58, 0.74) & (0.23, 0.14, 0.55) & (\textbf{0.84}, 0.11, 0.55) & (0.45, 0.10, 0.53) & (0.57$\pm$0.08, 0.44$\pm$0.02, 0.75$\pm$0.03) & (0.57$\pm$0.19, \textbf{0.71}$\pm$0.14, \textbf{0.79}$\pm$0.12)\\
SWaT 1 & (0.09, 0.09, 0.50) & (0.15, 0.24, 0.57) & (0.74, 0.22, 0.63) & (0.15, 0.14, 0.53) & (0.57, 0.53, 0.76) & (0.21, 0.20, 0.56) & (0.23, 0.22, 0.57) & (0.89, 0.39, 0.82) & (\textbf{1}, 0.16, 0.50) & (\textbf{1}, 0.16, 0.50) & (0.53, 0.19, 0.57) & (0.82$\pm$0.01, 0.88$\pm$0.004, 0.87$\pm$0.01) & (0.91$\pm$0.07, \textbf{0.95}$\pm$0.08, \textbf{0.94}$\pm$0.06)\\ 
SWaT 2 & (0.13, 0.08, 0.51) & (0.08, 0.15, 0.54) & (0.48, 0.08, 0.48) & (0.20, 0.13, 0.55) & (0.23, 0.15, 0.57) & (0.27, 0.18, 0.59) & (0.26, 0.16, 0.58) & (0.17, 0.22, \textbf{0.58}) & (\textbf{1}, 0.04, 0.50) & (\textbf{1}, 0.15, 0.54) & (0.34, 0.07, 0.47) & (0.19$\pm$0.11, 0.25$\pm$0.06, 0.53$\pm$0.09) & (0.21$\pm$0.03, \textbf{0.26}$\pm$0.05, 0.48$\pm$0.06)\\
SWaT 3 & (0.12, 0.06, 0.51) & (0.03, 0.02, 0.48) & (0.78, 0.10, 0.64) & (0.38, 0.20, 0.64) & (0.65, 0.34, 0.79) & (0.24, 0.13, 0.57) & (0.42, 0.23, 0.66) & (0.57, 0.19, 0.70) & (\textbf{1}, 0.04, 0.50) & (\textbf{1}, 0.07, 0.50) & (0.69, 0.13, 0.68) & (0.63$\pm$0.07, \textbf{0.76}$\pm$0.06, 0.73$\pm$0.05) & (0.66$\pm$0.05, 0.68$\pm$0.06, \textbf{0.88}$\pm$0.05)\\
SWaT 4 & (0.10, 0.17, 0.50) & (0.12, 0.18, 0.47) & (0.95, 0.73, 0.72) & (0.02, 0.03, 0.43) & (0.23, 0.37, 0.61) & (0.06, 0.10, 0.47) & (0.89, 0.89, 0.90) & (0.94, 0.67, 0.63) & (\textbf{1}, 0.44, 0.50) & (\textbf{0.98}, 0.18, 0.47) & (0.25, 0.26, 0.36) & (0.91$\pm$0.003, \textbf{0.95}$\pm$0.001, \textbf{0.97}$\pm$0.005) & (0.91$\pm$0.15, \textbf{0.95}$\pm$0.1, 0.96$\pm$0.09)\\
SWaT 5 & (0.08, 0.03, 0.49) & (0.22, 0.22, 0.60) & (0.72, 0.07, 0.61) & (0.36, 0.15, 0.63) & (0.38, 0.15, 0.65) & (0.51, 0.20, 0.71) & (0.50, 0.17, 0.70) & (0.84, 0.12, \textbf{0.75}) & (\textbf{1}, 0.02, 0.50) & (0.70, 0.22, 0.60) & (0.29, 0.03, 0.45) & (0.38$\pm$0.11, \textbf{0.27}$\pm$0.08, 0.57$\pm$0.04) & (0.70$\pm$0.32, 0.16$\pm$0.05, 0.61$\pm$0.13)\\
SWaT 6 & (0.12, 0.09, 0.51) & (0.40, 0.57, 0.70) & (\textbf{0.92}, 0.19, 0.71) & (0.63, 0.46, 0.78) & (0.44, 0.33, 0.68) & (0.44, 0.32, 0.68) & (0.30, 0.20, 0.60) & (0.76, 0.29, 0.77) & (\textbf{1}, 0.06, 0.50) & (0.74, 0.10, 0.47) & (0.31, 0.08, 0.45) & (0.78$\pm$0.09, \textbf{0.75}$\pm$0.17, 0.86$\pm$0.12) & (0.74$\pm$0.11, 0.73$\pm$0.06, \textbf{0.90}$\pm$0.1)\\
\hline
\end{tabular}
}
\caption{Comparative Analysis of Recall, F1 Score, and AUC-ROC for Various Anomaly Detection Methods on 7 additional Multivariate Time series datasets, highlighting dominant performance of RN-LSTM}
\label{table:combined}
\end{sidewaystable*}

\begin{table*}[h]
\centering
\scriptsize 
\setlength{\tabcolsep}{1mm} 
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Dataset & LOF & IForest & OCSVM & AutoEncoders & DAGMM & Envelope & DevNet & GDN & GAN & MGBTAI & d-BTAI & RN-Net & RN-LSTM\\
\hline
BATADAL\_04 & 0.2 & 0.1 & 0.09 & 0.09 & 0.19 & 0.26 & 0.05 & 0.66 & 0.1 & 0.06 & 0.06 & 0.34$\pm$0.06 & \textbf{0.95}$\pm$0.2\\
SWaT 1 & 0.08 & 0.63 & 0.13 & 0.13 & 0.5 & 0.19 & 0.22 & 0.25 & 0.22 & 0.09 & 0.12 & 0.95$\pm$0.02 & \textbf{0.98}$\pm$0.19\\
SWaT 2 & 0.06 &\textbf{ 0.71 }& 0.05 & 0.1 & 0.11 & 0.13 & 0.12 & 0.33 & 0.05 & \textbf{0.71} & 0.04 & 0.36$\pm$0.11 & 0.34$\pm$0.29\\
SWaT 3 & 0.04 & 0.02 & 0.05 & 0.13 & 0.23 & 0.09 & 0.15 & 0.11 & 0.04 & 0.02 & 0.07 & \textbf{0.95}$\pm$0.04 & 0.70$\pm$0.08\\
SWaT 4 & 0.45 & 0.34 & 0.59 & 0.08 & 0.99 & 0.27 & 0.88 & 0.52 & 0.44 & 0.34 & 0.27 & 0.99$\pm$0.001 & \textbf{1}$\pm$0.03\\
SWaT 5 & 0.02 & \textbf{0.22} & 0.04 & 0.09 & 0.1 & 0.12 & 0.12 & 0.06 & 0.03 & \textbf{0.22} & 0.02 & 0.2$\pm$0.03 & 0.09$\pm$0.43\\
SWaT 6 & 0.07 & \textbf{0.97} & 0.1 & 0.37 & 0.26 & 0.26 & 0.17 & 0.18 & 0.06 & 0.05 & 0.05 & 0.72$\pm$0.32 & 0.72$\pm$0.07 \\
\hline
\end{tabular}
}
\caption{PRECISION Comparison: Highlighting superior performance of RN-LSTM on 7 additional Multivariate Time series datasets}
\label{table:2}
\end{table*}


\begin{table*}[h!]
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
                 & \multicolumn{4}{|c|}{CBLOF (K-Means)}     & \multicolumn{4}{|c|}{Modified CBLOF (K-Means)} \\
\hline
Dataset          & Precision & Recall & F1-score & AUC-ROC & Precision   & Recall  & F1-score  & AUC-ROC  \\
\hline
ALOI             & 0.04      & 0.07   & 0.05     & 0.56    & 0.05        & 0.08    & 0.06      & 0.54     \\
annthyroid       & 0.36      & 0.24   & 0.29     & 0.67    & 0.31        & 0.21    & 0.25      & 0.65     \\
breastw          & 0.89      & 0.13   & 0.23     & 0.96    & 0.26        & 0.04    & 0.07      & 0.70     \\
campaign         & 0.41      & 0.18   & 0.25     & 0.73    & 0.19        & 0.08    & 0.12      & 0.65     \\
cardio           & 0.51      & 0.27   & 0.35     & 0.82    & 0.15        & 0.08    & 0.10      & 0.42     \\
Cardiotocography & 0.41      & 0.09   & 0.15     & 0.57    & 0.48        & 0.11    & 0.18      & 0.58     \\
celeba           & 0.08      & 0.17   & 0.11     & 0.73    & 0.02        & 0.04    & 0.02      & 0.59     \\
cover            & 0.14      & 0.72   & 0.23     & 0.96    & 0.05        & 0.24    & 0.08      & 0.83     \\
donors           & 0.13      & 0.11   & 0.11     & 0.76    & 0.13        & 0.11    & 0.11      & 0.76     \\
fault            & 0.58      & 0.08   & 0.15     & 0.64    & 0.57        & 0.08    & 0.15      & 0.66     \\
fraud            & 0.03      & 0.87   & 0.06     & 0.94    & 0.03        & 0.87    & 0.06      & 0.94     \\
glass            & 0.09      & 0.11   & 0.10     & 0.85    & 0.09        & 0.11    & 0.10      & 0.77     \\
Hepatitis        & 0.00      & 0.00   & 0.00     & 0.68    & 0.00        & 0.00    & 0.00      & 0.37     \\
http             & 0.08      & 1.00   & 0.14     & 0.99    & 0.08        & 1.00    & 0.14      & 0.99     \\
InternetAds      & 0.32      & 0.09   & 0.14     & 0.61    & 0.97        & 0.26    & 0.41      & 0.72     \\
Ionosphere       & 1.00      & 0.14   & 0.25     & 0.88    & 1.00        & 0.14    & 0.25      & 0.94     \\
landsat          & 0.19      & 0.05   & 0.07     & 0.50    & 0.26        & 0.06    & 0.10      & 0.59     \\
Lymphography     & 0.63      & 0.83   & 0.71     & 0.98    & 0.25        & 0.33    & 0.29      & 0.69     \\
magic.gamma      & 0.95      & 0.13   & 0.24     & 0.73    & 0.86        & 0.12    & 0.21      & 0.78     \\
mammography      & 0.14      & 0.30   & 0.19     & 0.77    & 0.14        & 0.30    & 0.19      & 0.78     \\
mnist            & 0.48      & 0.26   & 0.34     & 0.84    & 0.43        & 0.23    & 0.30      & 0.80     \\
musk             & 0.18      & 0.28   & 0.22     & 0.63    & 0.00        & 0.00    & 0.00      & 0.26     \\
optdigits        & 0.00      & 0.01   & 0.00     & 0.73    & 0.23        & 0.39    & 0.29      & 0.73     \\
PageBlocks       & 0.75      & 0.40   & 0.52     & 0.93    & 0.20        & 0.11    & 0.14      & 0.73     \\
pendigits        & 0.15      & 0.34   & 0.21     & 0.90    & 0.18        & 0.39    & 0.24      & 0.95     \\
Pima             & 0.54      & 0.08   & 0.14     & 0.64    & 0.56        & 0.08    & 0.14      & 0.52     \\
satellite        & 0.84      & 0.13   & 0.23     & 0.59    & 0.20        & 0.03    & 0.05      & 0.38     \\
satimage-2       & 0.24      & 0.97   & 0.38     & 1.00    & 0.02        & 0.07    & 0.03      & 0.10     \\
shuttle          & 0.19      & 0.14   & 0.16     & 0.71    & 0.90        & 0.63    & 0.74      & 0.99     \\
skin             & 0.00      & 0.00   & 0.00     & 0.56    & 0.00        & 0.00    & 0.00      & 0.62     \\
smtp             & 0.00      & 0.77   & 0.01     & 0.94    & 0.00        & 0.70    & 0.01      & 0.84     \\
SpamBase         & 0.28      & 0.04   & 0.06     & 0.55    & 0.53        & 0.07    & 0.12      & 0.40     \\
speech           & 0.02      & 0.07   & 0.03     & 0.47    & 0.03        & 0.08    & 0.04      & 0.47     \\
Stamps           & 0.18      & 0.10   & 0.13     & 0.72    & 0.18        & 0.10    & 0.13      & 0.82     \\
thyroid          & 0.40      & 0.81   & 0.53     & 0.98    & 0.29        & 0.58    & 0.38      & 0.95     \\
vertebral        & 0.00      & 0.00   & 0.00     & 0.42    & 0.00        & 0.00    & 0.00      & 0.60     \\
vowels           & 0.27      & 0.40   & 0.33     & 0.90    & 0.30        & 0.44    & 0.36      & 0.92     \\
Waveform         & 0.14      & 0.25   & 0.18     & 0.70    & 0.12        & 0.20    & 0.15      & 0.65     \\
WBC              & 0.42      & 0.50   & 0.45     & 0.96    & 0.00        & 0.00    & 0.00      & 0.19     \\
WDBC             & 0.11      & 0.20   & 0.14     & 0.84    & 0.47        & 0.90    & 0.62      & 0.98     \\
Wilt             & 0.00      & 0.00   & 0.00     & 0.37    & 0.02        & 0.02    & 0.02      & 0.51     \\
wine             & 0.43      & 0.30   & 0.35     & 0.92    & 0.00        & 0.00    & 0.00      & 0.30     \\
WPBC             & 0.20      & 0.04   & 0.07     & 0.50    & 0.10        & 0.02    & 0.04      & 0.50     \\
yeast            & 0.25      & 0.04   & 0.07     & 0.44    & 0.20        & 0.03    & 0.05      & 0.42     \\
CIFAR10          & 0.17      & 0.17   & 0.17     & 0.72    & 0.16        & 0.16    & 0.16      & 0.68     \\
FashionMNIST     & 0.25      & 0.25   & 0.25     & 0.87    & 0.34        & 0.35    & 0.35      & 0.88     \\
MNIST-C          & 0.12      & 0.12   & 0.12     & 0.73    & 0.18        & 0.18    & 0.18      & 0.78     \\
MVTec-AD         & 1.00      & 0.24   & 0.38     & 0.94    & 1.00        & 0.24    & 0.38      & 0.85     \\
SVHN             & 0.10      & 0.10   & 0.10     & 0.60    & 0.10        & 0.10    & 0.10      & 0.62     \\
Agnews           & 0.07      & 0.07   & 0.07     & 0.57    & 0.06        & 0.06    & 0.06      & 0.55     \\
Amazon           & 0.06      & 0.06   & 0.06     & 0.57    & 0.06        & 0.06    & 0.06      & 0.57     \\
Imdb             & 0.03      & 0.03   & 0.03     & 0.49    & 0.03        & 0.03    & 0.03      & 0.50     \\
Yelp             & 0.11      & 0.11   & 0.11     & 0.63    & 0.08        & 0.08    & 0.08      & 0.60     \\
20newsgroups     & 0.08      & 0.08   & 0.08     & 0.65    & 0.08        & 0.08    & 0.08      & 0.64  \\
\hline
\end{tabular}
\caption{Comparison of original CBLOF(K-Means) and CBLOF(K-Means) with RN-Loss on Multivariate datasets}
\label{table:K-Means-Multi}
\end{table*}

\begin{table*}[h!]
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
                 & \multicolumn{4}{|c|}{CBLOF (K-Means)}     & \multicolumn{4}{|c|}{Modified CBLOF (K-Means)} \\
                 \hline
Dataset          & Precision & Recall & F1-score & AUC-ROC & Precision   & Recall  & F1-score  & AUC-ROC  \\
\hline
Occupancy\_6005  & 0.01      & 1.00   & 0.02     & 1.00    & 0.01        & 1.00    & 0.02      & 1.00     \\
Occupancy\_t4013 & 0.02      & 1.00   & 0.03     & 1.00    & 0.02        & 1.00    & 0.03      & 1.00     \\
Speed\_6005      & 0.01      & 1.00   & 0.02     & 1.00    & 0.01        & 1.00    & 0.02      & 1.00     \\
Speed\_7578      & 0.07      & 1.00   & 0.12     & 0.99    & 0.04        & 0.50    & 0.07      & 0.95     \\
Speed\_t4013     & 0.01      & 1.00   & 0.03     & 1.00    & 0.02        & 1.00    & 0.03      & 1.00     \\
TravelTime\_387  & 0.02      & 0.67   & 0.03     & 0.68    & 0.02        & 0.67    & 0.03      & 0.68     \\
TravelTime\_451  & 0.01      & 1.00   & 0.02     & 1.00    & 0.01        & 1.00    & 0.02      & 1.00     \\
aws1             & 0.02      & 1.00   & 0.04     & 1.00    & 0.02        & 1.00    & 0.04      & 1.00     \\
aws2             & 0.00      & 0.50   & 0.01     & 0.50    & 0.02        & 1.00    & 0.03      & 1.00     \\
aws3             & 0.01      & 1.00   & 0.03     & 1.00    & 0.01        & 1.00    & 0.03      & 1.00     \\
aws\_syn1        & 0.19      & 1.00   & 0.32     & 1.00    & 0.19        & 1.00    & 0.32      & 1.00     \\
aws\_syn2        & 0.16      & 1.00   & 0.28     & 1.00    & 0.16        & 1.00    & 0.28      & 1.00     \\
aws\_syn3        & 0.13      & 1.00   & 0.23     & 1.00    & 0.13        & 1.00    & 0.24      & 1.00     \\
yahoo1           & 0.03      & 1.00   & 0.05     & 1.00    & 0.03        & 1.00    & 0.05      & 1.00     \\
yahoo2           & 0.05      & 0.50   & 0.09     & 0.57    & 0.11        & 1.00    & 0.20      & 1.00     \\
yahoo3           & 0.09      & 0.88   & 0.17     & 0.92    & 0.10        & 0.88    & 0.18      & 0.95     \\
yahoo5           & 0.04      & 0.33   & 0.07     & 0.62    & 0.04        & 0.33    & 0.07      & 0.42     \\
yahoo6           & 0.06      & 1.00   & 0.11     & 1.00    & 0.06        & 1.00    & 0.11      & 1.00     \\
yahoo7           & 0.05      & 0.36   & 0.08     & 0.63    & 0.05        & 0.36    & 0.08      & 0.58     \\
yahoo8           & 0.02      & 0.20   & 0.04     & 0.50    & 0.02        & 0.20    & 0.04      & 0.55     \\
yahoo9           & 0.10      & 1.00   & 0.17     & 1.00    & 0.10        & 1.00    & 0.17      & 1.00     \\
yahoo\_syn1      & 0.17      & 1.00   & 0.29     & 1.00    & 0.17        & 1.00    & 0.29      & 1.00     \\
yahoo\_syn2      & 0.24      & 1.00   & 0.39     & 1.00    & 0.24        & 1.00    & 0.39      & 1.00     \\
yahoo\_syn3      & 0.22      & 0.94   & 0.35     & 0.97    & 0.23        & 0.94    & 0.37      & 0.98     \\
yahoo\_syn5      & 0.15      & 0.58   & 0.24     & 0.78    & 0.15        & 0.58    & 0.24      & 0.68     \\
yahoo\_syn6      & 0.06      & 0.29   & 0.09     & 0.35    & 0.06        & 0.29    & 0.09      & 0.38     \\
yahoo\_syn7      & 0.08      & 0.33   & 0.13     & 0.69    & 0.06        & 0.24    & 0.09      & 0.64     \\
yahoo\_syn8      & 0.04      & 0.15   & 0.06     & 0.52    & 0.04        & 0.15    & 0.06      & 0.54     \\
yahoo\_syn9      & 0.19      & 0.89   & 0.31     & 0.96    & 0.21        & 1.00    & 0.35      & 1.00    \\
\hline
\end{tabular}
\caption{Comparison of original CBLOF(K-Means) and CBLOF(K-Means) with RN-Loss on Univariate datasets}
\label{table:K-Means-Uni}
\end{table*}

\begin{table*}[h!]
\centering
\tiny
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
                 & \multicolumn{4}{|c|}{d-BTAI}               & \multicolumn{4}{|c|}{Modified-dBTAI}       \\
                 \hline
Dataset          & Precision & Recall & F-1 Score & AUC-ROC & Precision & Recall & F-1 Score & AUC-ROC \\
\hline
ALOI             & 0.04      & 0.4    & 0.06      & 0.53    & 0.04      & 0.41   & 0.07      & 0.53    \\
annthyroid       & 0.11      & 0.51   & 0.19      & 0.6     & 0.10      & 0.49   & 0.17      & 0.57    \\
backdoor         & 0.06      & 0.87   & 0.11      & 0.76    & NA        & NA     & NA        & NA      \\
breastw          & 0.89      & 0.87   & 0.88      & 0.91    & 0.89      & 0.85   & 0.87      & 0.90    \\
campaign         & 0.16      & 0.66   & 0.26      & 0.61    & 0.17      & 0.64   & 0.27      & 0.62    \\
cardio           & 0.2       & 0.76   & 0.32      & 0.72    & 0.20      & 0.76   & 0.32      & 0.72    \\
Cardiotocography & 0.35      & 0.47   & 0.4       & 0.61    & 0.35      & 0.47   & 0.40      & 0.61    \\
celeba           & 0.03      & 0.38   & 0.06      & 0.55    & 0.03      & 0.37   & 0.06      & 0.55    \\
cover            & 0.02      & 0.7    & 0.04      & 0.68    & 0.02      & 0.69   & 0.04      & 0.68    \\
donors           & 0.13      & 0.47   & 0.2       & 0.63    & NA        & NA     & NA        & NA      \\
fault            & 0.49      & 0.6    & 0.54      & 0.63    & 0.50      & 0.63   & 0.56      & 0.65    \\
fraud            & 0         & 0.96   & 0.01      & 0.78    & 0.01      & 0.85   & 0.01      & 0.72    \\
glass            & 0.17      & 1      & 0.29      & 0.89    & 0.11      & 0.78   & 0.20      & 0.75    \\
Hepatitis        & 0.14      & 0.23   & 0.18      & 0.48    & 0.17      & 0.31   & 0.22      & 0.51    \\
http             & 0.03      & 1      & 0.06      & 0.94    & 0.00      & 0.02   & 0.00      & 0.34    \\
InternetAds      & 0.28      & 0.6    & 0.39      & 0.63    & NA        & NA     & NA        & NA      \\
Ionosphere       & 0.82      & 0.84   & 0.83      & 0.87    & 0.73      & 0.78   & 0.75      & 0.81    \\
landsat          & 0.26      & 0.46   & 0.33      & 0.56    & 0.25      & 0.44   & 0.32      & 0.55    \\
letter           & 0.12      & 0.86   & 0.21      & 0.72    & NA        & NA     & NA        & NA      \\
Lymphography     & 0.15      & 1      & 0.26      & 0.88    & 0.15      & 1.00   & 0.26      & 0.88    \\
magic.gamma      & 0.53      & 0.59   & 0.56      & 0.65    & 0.60      & 0.49   & 0.54      & 0.66    \\
mammography      & 0.07      & 0.84   & 0.13      & 0.8     & NA        & NA     & NA        & NA      \\
mnist            & 0.17      & 0.85   & 0.28      & 0.71    & 0.17      & 0.85   & 0.28      & 0.71    \\
musk             & 0.36      & 1      & 0.53      & 0.82    & 0.08      & 1.00   & 0.15      & 0.81    \\
optdigits        & 0.02      & 0.27   & 0.03      & 0.42    & 0.02      & 0.34   & 0.04      & 0.44    \\
PageBlocks       & 0.17      & 0.21   & 0.19      & 0.55    & 0.17      & 0.21   & 0.19      & 0.55    \\
pendigits        & 0.04      & 0.72   & 0.08      & 0.67    & 0.04      & 0.71   & 0.07      & 0.65    \\
Pima             & 0.44      & 0.43   & 0.43      & 0.57    & 0.42      & 0.44   & 0.43      & 0.56    \\
satellite        & 0.54      & 0.59   & 0.57      & 0.68    & 0.51      & 0.59   & 0.55      & 0.67    \\
satimage-2       & 0.04      & 1      & 0.07      & 0.83    & 0.03      & 1.00   & 0.07      & 0.83    \\
shuttle          & 0.44      & 0.88   & 0.59      & 0.9     & 0.45      & 0.89   & 0.59      & 0.90    \\
skin             & 0.27      & 0.22   & 0.24      & 0.53    & 0.33      & 0.33   & 0.33      & 0.58    \\
smtp             & 0.001     & 0.87   & 0.002     & 0.81    & 0.00      & 0.87   & 0.00      & 0.79    \\
SpamBase         & 0.66      & 0.26   & 0.37      & 0.58    & 0.66      & 0.27   & 0.38      & 0.59    \\
speech           & 0.02      & 0.8    & 0.03      & 0.48    & 0.02      & 0.51   & 0.03      & 0.48    \\
Stamps           & 0.15      & 0.61   & 0.24      & 0.63    & 0.20      & 0.84   & 0.32      & 0.75    \\
thyroid          & 0.08      & 1      & 0.14      & 0.84    & 0.07      & 0.92   & 0.12      & 0.80    \\
vertebral        & 0.05      & 0.13   & 0.07      & 0.39    & 0.09      & 0.24   & 0.13      & 0.44    \\
vowels           & 0.08      & 1      & 0.14      & 0.79    & 0.08      & 1.00   & 0.14      & 0.79    \\
Waveform         & 0.05      & 0.76   & 0.09      & 0.64    & 0.04      & 0.71   & 0.08      & 0.61    \\
WBC              & 0.19      & 1      & 0.31      & 0.9     & 0.19      & 1.00   & 0.31      & 0.90    \\
WDBC             & 0.19      & 1      & 0.32      & 0.94    & 0.13      & 1.00   & 0.23      & 0.91    \\
Wilt             & 0.05      & 0.18   & 0.07      & 0.48    & 0.04      & 0.17   & 0.07      & 0.48    \\
wine             & 0.26      & 1      & 0.42      & 0.88    & 0.25      & 1.00   & 0.40      & 0.87    \\
WPBC             & 0.25      & 0.4    & 0.31      & 0.51    & 0.26      & 0.45   & 0.33      & 0.52    \\
yeast            & 0.3       & 0.27   & 0.28      & 0.47    & 0.30      & 0.30   & 0.30      & 0.47    \\
CIFAR10          & 0.08      & 0.71   & 0.14      & 0.64    & 0.08      & 0.70   & 0.14      & 0.64    \\
FashionMNIST     & 0.1       & 0.93   & 0.18      & 0.76    & 0.10      & 0.93   & 0.19      & 0.76    \\
MNIST-C          & 0.17      & 0.85   & 0.28      & 0.71    & 0.10      & 0.88   & 0.18      & 0.73    \\
MVTec-AD         & 0.9       & 0.87   & 0.89      & 0.92    & 0.77      & 0.90   & 0.83      & 0.92    \\
SVHN             & 0.1       & 0.88   & 0.18      & 0.73    & 0.07      & 0.62   & 0.13      & 0.59    \\
Agnews           & 0.06      & 0.47   & 0.11      & 0.55    & 0.06      & 0.47   & 0.11      & 0.55    \\
Amazon           & 0.05      & 0.47   & 0.1       & 0.5     & 0.06      & 0.46   & 0.11      & 0.55    \\
Imdb             & 0.05      & 0.43   & 0.5       & 0.5     & 0.05      & 0.33   & 0.40      & 0.48    \\
Yelp             & 0.06      & 0.54   & 0.11      & 0.56    & 0.07      & 0.57   & 0.13      & 0.59    \\
20newsgroups     & 0.04      & 0.29   & 0.07      & 0.47    & 0.08      & 0.60   & 0.14      & 0.62    \\
BATADAL\_04      & 0.09      & 0.75   & 0.17      & 0.68    & 0.10      & 0.75   & 0.18      & 0.69    \\
SWaT 1           & 0.63      & 0.81   & 0.71      & 0.74    & 0.19      & 0.79   & 0.30      & 0.73    \\
SWaT 2           & 0.08      & 0.53   & 0.14      & 0.61    & 0.06      & 0.43   & 0.11      & 0.55    \\
SWaT 3           & 0.07      & 0.69   & 0.13      & 0.68    & NA        & NA     & NA        & NA      \\
SWaT 4           & 0.18      & 0.15   & 0.16      & 0.31    & 0.19      & 0.15   & 0.17      & 0.32    \\
SWaT 5           & 0.06      & 0.83   & 0.12      & 0.75    & 0.06      & 0.85   & 0.12      & 0.76    \\
SWaT 6           & 0.13      & 0.81   & 0.22      & 0.74    & 0.13      & 0.80   & 0.22      & 0.73    \\
ecoli            & 0.06      & 0.78   & 0.1       & 0.71    & 0.04      & 0.56   & 0.07      & 0.59    \\
cmc              & 0.02      & 0.47   & 0.03      & 0.57    & 0.01      & 0.18   & 0.01      & 0.43    \\
lympho h         & 0.11      & 1      & 0.2       & 0.83    & 0.10      & 1.00   & 0.19      & 0.82    \\
wbc h            & 0.17      & 1      & 0.29      & 0.86    & 0.15      & 0.95   & 0.25      & 0.81  \\
\hline
\end{tabular}
\caption{Comparison of original dBTAI algorithm and dBTAI with RN-Loss on Multivariate datasets}
\label{table:dBTAI-Multi}
\end{table*}


\begin{table*}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
Dataset          & dBTAI & Modified-dBTAI \\
\hline
yahoo1           & 1     & NA             \\
yahoo2           & 1     & 1.00           \\
yahoo3           & 0.88  & 0.88           \\
yahoo5           & 0.33  & 1.00           \\
yahoo6           & 1     & 1.00           \\
yahoo7           & 0.36  & 0.73           \\
yahoo8           & 0.1   & 0.60           \\
yahoo9           & 1     & 1.00           \\
Speed\_6005      & 1     & 1.00           \\
Speed\_7578      & 0.5   & NA             \\
Speed\_t4013     & 1     & NA             \\
TravelTime\_387  & 0.33  & 0.67           \\
TravelTime\_451  & 1     & 1.00           \\
Occupancy\_6005  & 1     & 1.00           \\
Occupancy\_t4013 & 1     & 1.00           \\
yahoo\_syn1      & 1     & NA             \\
yahoo\_syn2      & 0.94  & 0.78           \\
yahoo\_syn3      & 0.94  & 0.95           \\
yahoo\_syn5      & 0.53  & 0.58           \\
yahoo\_syn6      & 0.29  & 1.00           \\
yahoo\_syn7      & 0.24  & 0.76           \\
yahoo\_syn8      & 0.1   & 0.60           \\
yahoo\_syn9      & 1     & 1.00           \\
aws1             & 1     & 1.00           \\
aws2             & 1     & NA             \\
aws3             & 1     & 1.00           \\
aws\_syn1        & 1     & 1.00           \\
aws\_syn2        & 0.55  & NA             \\
aws\_syn3        & 0.9   & 1.00     \\
\hline
\end{tabular}
\caption{Recall comparison of original dBTAI algorithm and dBTAI with RN-Loss on Univariate datasets}
\label{table:dBTAI-Uni}
\end{table*}


\end{document}
