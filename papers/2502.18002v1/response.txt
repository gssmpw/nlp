\section{Literature Review}
\label{sec:a5}
Throughout the years, much research has been conducted in anomaly detection with a multitude of explored methods such as in **Chandola, et al., "Anomaly Detection: A Survey"**. Since then, this interest has increased significantly as various domains such as cybersecurity **Japkowicz and Stephen, "The Class Imbalance Problem: A Review"**, fraud detection, and healthcare **Kubat and Matwin, "Addressing the Curse of Imbalanced Data Sets: A Case Study"** became more relevant. 
The work on learning from imbalanced datasets was proposed in AAAI 2000 workshop 
%Workshop on learning from imbalanced data) 
and highlighted research on two major problems, types of imbalance that hinder the performance of standard classifiers and the suitable approaches for the same. **Chen et al., "One-vs-All Support Vector Machine for Multi-class Classification with Imbalanced Datasets"** also showed that the class imbalance problem affects not only standard classifiers like decision trees but also Neural Networks and SVMs. The work by **Japkowicz, "The Class Imbalance Problem: A Review"**
%This was followed by ICML'2003 Workshop on Learning from Imbalanced Datasets[II] 
**Chawla et al., "Learning from Imbalanced Data Sets: A Comparison of Different Approaches"** and ACM SIGKDD Exploration'2004 **Sun et al., "A Survey of Dealing with Class Imbalance in Data Mining"** that 
gave a further boost to research in imbalanced data classification. Extensive research was done on unsupervised learning methods to address issues such as relying on static thresholds, in turn struggling to adapt to dynamic data, resulting in high false positives and missed anomalies. However, the work by **Markou and Duma, "Minimum Classification Error Rating for Noisy Data"** observed that unsupervised anomaly detection can be computationally intensive, especially in high-dimensional datasets. Jacob and Tatbul **"Explainable Anomaly Detection in Time Series using Real-World Data"** delved into explainable anomaly detection in time series using real-world data, yet deep learning-based time-series anomaly detection models were not thoroughly explored well enough. %Clustering-based methods like K-Means and Isolation Forest may require prior knowledge of cluster numbers.
With significant growth in applying various ML algorithms
to detect anomalies, there has been an avalanche of anomaly
benchmarking data **Chandola et al., "Anomaly Detection: A Survey"**, **Kubat and Matwin, "Addressing the Curse of Imbalanced Data Sets: A Case Study"**, **Japkowicz and Stephen, "The Class Imbalance Problem: A Review"**, as well as empirical studies of the performances of existing algorithms **Chen et al., "One-vs-All Support Vector Machine for Multi-class Classification with Imbalanced Datasets"**, **Markou and Duma, "Minimum Classification Error Rating for Noisy Data"** on different benchmark data. Due to the importance of the problem, there have also been efforts to produce benchmarks such as AD-Bench**"Anomaly Detection Benchmark"** and ESA-ADB**"Energy-Saving Anomaly Detection Benchmark"**. %AD-Bench examines the performance of 30 detection algorithms across an extensive set of 57 benchmark datasets, adding to the body of research in the field. 
Researchers have critically examined the suitability of evaluation metrics for machine learning methods in anomaly detection. Kim et al. **Kim et al., "The Limitations of F1-Score Adjustment: A Theoretical and Experimental Study"** exposed the limitations of the F1-score with point adjustment, both theoretically and experimentally. 
%Despite the emergence of numerous new metrics for time series anomaly detection, even the most recent ones are not without their shortcomings. 

To conclude, %despite the availability of extensive anomaly benchmark datasets and publicly accessible machine learning-driven anomaly detection methods, 
recent benchmarking studies, concentrated on deep-learning-based anomaly detection techniques mostly, have not examined the performance across varying types of anomalies, such as singleton/point, small, and significantly large numbers, nor across different data types, including univariate, multivariate, temporal, and non-temporal data. Additionally, there has been a lack of exploration into how anomalies should be identified when their frequency is high. These observations prompt several critical questions. Is the current SoTA algorithm the most effective? Are we reaching the peak in anomaly detection using Deep Learning approaches? Are unsupervised learning algorithms truly better than supervised learning algorithms?

%%%%%%%%%%%%%%%%%% ATTACHING PREVIOUS APPENDIX %%%%%%%%%%%%%%%%%%%

\newpage