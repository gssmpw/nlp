\section{Related Works}
\label{sec:related}


\textbf{Deep generative models with GA.} Several studies explore combining GAs and deep generative models, particularly in chemical domains. One direction is that utilizing expert-designed GA to improve the generative models' outcome. \citet{ahn2020guiding} and \citet{kim2024genetic} employ Graph GA \citep{jensen2019graph} whose crossover and mutation are designed to guarantee the validity in chemical space, to refine the generated molecules. 
Conversely, some researches suggest to incorporate neural models to enhance or design GAs.s
Notably, in SynNet \citep{gao2022synnet}, the neural policy construct offspring conditioned on molecule embedding from the parent, allowing exploiting the generative capability from the deep model similar to our works. However, to ensure the plausibility, SynNet employs domain-specific reaction rules in the subsequent synthesis planning. 
\citet{gao2024synformer} utilizes the generative model to refine the molecules obtained by Graph GA to more synthesizable ones. In Reinforced Genetic Algorithm \citep[RGA; ][]{fu2022reinforced_ga}, the neural models guides the crossover and mutation process with a protein target structure embedding to overcome a random-work exploration with predefined rules.

More broadly, there is a growing trend that leverages large language models (LLMs) as a black-box operator within GAs, where parent samples are directly fed into LLM to obtain new samples. These LLM-based GAs have been studied in various domains, including prompt optimization \citep{meyerson2024language,liu2024autodan, guo2024evoprompt}, molecular design \citep{wang2024molleo}, and code generations \citep{lehman2023evolution,chen2023evoprompting, ye2024reevo}. However, a primary challenge is the reliance on hand-crafted prompt engineering, and many approaches continue to employ problem-specific rules along with the LLM-based operator \citep{liu2024autodan, wang2024molleo}.



\textbf{Test-time search for enhanced inference. } Recent work in large language models (LLMs) \citep{bai2022constitutional,madaan2023self-refine,snell2024scaling,brown2024large} demonstrates that dedicating additional computation at test time can greatly improve outputs (see \citet{dong2024survey} for an overview).
Meanwhile, in combinatorial optimization (CO), its mathematically rigorous algorithmic foundations have inspired a line of research integrating traditional algorithms into test-time search within deep learning. For instance, \citep{kool2022deep} employ restricted dynamic programming guided by a graph neural network (GNN) that predicts a solution heatmap in routing problems. Meanwhile, \citet{ye2023deepaco} and \citet{kim2024ant} use a predicted edge heatmap in ant colony optimization (ACO) for broader CO applications. \citet{luo2023lehd} propose a new greedy decoding-based search that randomly destroys the generated solutions and reconstructs solutions. Although effective, this method relies on handcrafted rules for destruction.
In addition to external algorithmic integrations, there exist a range of self-improvement approaches, such as beam search \citep{joshi2021learning,choo2022simulation}, sampling (also called \textit{best-of-N}) \citep{kool2018attention}, and Monte Carlo Tree Search \citep{qiu2022dimes}, and active searches \citep{bello2016neural,hottung2021efficient,son2023meta} are also employed for refining solutions at test time.

% In deep learning for CO, various test-time search techniques have emerged. \citet{joshi2021learning} and \citet{drakulic2024bq} utilize beam search, and \citet{choo2022simulation} extends this to a simulation-guided beam search. 
% Another line of research integrates traditional algorithms (e.g., dynamic programming algorithm \citep{kool2022deep} and ant colony optimization \citep{ye2023deepaco, kim2024ant}) or problem-specifically designed search algorithms \citep{luo2023lehd}.
% Additionally, several studies employ active search approaches to fine-tune a pretrained policy, either via policy exploration \citep{bello2016neural,hottung2021efficient}, greedy rollouts \citep{choo2022simulation}, or meta-learning \citep{son2023meta}.