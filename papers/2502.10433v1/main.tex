%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

\usepackage{graphicx} 
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{caption}
\usepackage{float}
\usepackage{setspace}
\usepackage{enumitem}

\usepackage{subcaption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

% Customized
\newcommand{\ndref}{\textcolor{red}{[cite]}} % NeeD REFerence
\newcommand{\bluenote}[1]{{\color{blue}#1}}
\newcommand{\rednote}[1]{{\color{red}#1}}
\newcommand{\hyeonah}[1]{{\color{magenta}#1}}
\newcommand{\sanghyeok}[1]{{\color{orange}#1}}
\newcommand{\ck}[1]{{\color{brown}#1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

\newcommand{\ours}{{NGS}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Neural Genetic Search in Discrete Spaces}

\begin{document}

\twocolumn[
\icmltitle{Neural Genetic Search in Discrete Spaces}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hyeonah Kim}{equal,kaist}
\icmlauthor{Sanghyeok Choi}{equal,kaist}
\icmlauthor{Jiwoo Son}{comp}
\icmlauthor{Jinkyoo Park}{kaist,comp}
\icmlauthor{Changhyun Kwon}{kaist,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{kaist}{Department of Industrial \& Systems Engineering, KAIST}
\icmlaffiliation{comp}{Omelet}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Hyeonah Kim}{hyeonah\_kim@kaist.ac.kr}
\icmlcorrespondingauthor{Sanghyeok Choi}{sanghyeok.choi@kaist.ac.kr}
\icmlcorrespondingauthor{Changhyun Kwon}{chkwon@kaist.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Genetic algorithm, deep learning, test-time search}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}


Effective search methods are crucial for improving the performance of deep generative models at test time. In this paper, we introduce a novel test-time search method, \emph{Neural Genetic Search} (NGS), which incorporates the evolutionary mechanism of genetic algorithms into the generation procedure of deep models. The core idea behind NGS is its crossover, which is defined as parent-conditioned generation using trained generative models. This approach offers a versatile and easy-to-implement search algorithm for deep generative models. We demonstrate the effectiveness and flexibility of NGS through experiments across three distinct domains: routing problems, adversarial prompt generation for language models, and molecular design.



\end{abstract}

\section{Introduction}
\label{sec:intro}

Genetic algorithms (GAs) have demonstrated remarkable performance across a diverse range of tasks with discrete search space, from classic combinatorial optimization problems~\citep{holland1992ga,OMARA2010ga,vidal2012hybrid, nagata2013eax,mahmoudinazlou2024hybrid} to more recent applications in molecular design~\citep{morris1998lamarckian,jensen2019graph,nigam2021stoned,kerstjens2022leadd}. By maintaining a population of candidate solutions and iteratively applying evolutionary operators, such as crossover and mutation, GAs can systematically explore vast search spaces.

In this work, we investigate how to incorporate the powerful search capabilities of GAs into deep generative models. Previous efforts that combined GAs with deep learning for better search typically applied GAs after the generation process to refine the outputs of generative models using existing problem-specific GAs~\citep{ahn2020guiding,kim2024genetic}. In contrast, our goal is to integrate the evolutionary principles of GAs directly into the generation process of deep models rather than simply chaining the deep models and GAs.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/chromosome.pdf}
    \vspace{-10pt}
    \caption{Illustrative examples of parents and offspring generated via parents-conditioned generation in various tasks.}\label{fig:chromosome}
    \vspace{-10pt}
\end{figure}


We present \textit{Neural Genetic Search} (\ours{}), a GA-inspired new search algorithm for deep generative models. \ours{} enhances the sequential generation process by incorporating genetic operators, crossover, and mutation. Specifically, we define the crossover as a \emph{parent-conditioned generation} (\cref{fig:chromosome}), where the model's vocabulary is restricted to the tokens used in the selected parents, while mutation simply removes this restriction, allowing for more diverse outputs. The generation with the genetic operators proceeds iteratively with an evolving population, steering the generation toward better results over time, similar to traditional GAs.

Since the generation process is still governed by the trained generative models, \ours{} effectively combine the search capability of GAs with the generative power of deep neural networks. Furthermore, as the proposed genetic operators are problem-agnostic, \ours{} can be applied to any deep sequential generative model across various tasks, in contrast to traditional GAs, which require significant effort to design problem-specific operators. \ours{} also can be easily implemented by adding the population and the parent-conditioned masking rule to any existing algorithm with sequential generative models. Finally, compared to single-pass generation, the population-based iterative generation in \ours{} allows the generation process to adapt over time, improving the quality of generated outputs and even the robustness to distribution shifts.

We evaluated our algorithm in three distinct domains where sequential generative models are widely applied: routing problems, red-teaming language models, and \textit{de novo} molecular design. Our extensive experiments validate that \ours{} can serve as an effective test-time search method, fulfilling its main purpose. We also found that \ours{} can replace the conventional decoding algorithms, offering improved robustness. Additionally, \ours{} can be viewed as an automated genetic algorithm with learned genetic operators, eliminating the need for extensive labor for algorithm design. These results suggest that NGS has significant potential as a versatile and efficient search strategy, adaptable to a wide range of applications with sequential generative models.

% \textbf{Contributions.} We designed a novel search method, \ours{}, for deep sequential generative model with inspiration from GA. The key \ours{} is powerful, versatile, and easy-to-implement. Through \ours{} have shown remarkable search capability


\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{fig/ga.pdf}
    \vspace{-15pt}
    \caption{Overview of GA with \ours{}. (1) The trained generative policy sequentially constructs sequences, which correspond to \textit{chromosomes}, to initialize the population. From this population, (2) parents are selected, and (3) the policy reproduces offspring by sampling new sequences with a parent-conditioned mask. Finally, (4) the newly generated candidates replace members of the population, completing one evolutionary cycle.} \label{fig:main}
\end{figure*}


% \section{Introduction - before}
% \label{sec:intro}

% Genetic algorithms (GAs) have demonstrated remarkable performance across a diverse range of discrete search tasks, from classic combinatorial optimization problems~\citep{holland1992ga,OMARA2010ga,vidal2012hybrid, nagata2013eax,mahmoudinazlou2024hybrid} to more recent applications in molecular design~\citep{morris1998lamarckian,jensen2019graph,nigam2021stoned,kerstjens2022leadd}. By maintaining a population of candidate solutions and iteratively applying evolutionary operators, such as crossover and mutation, GAs systematically explore vast search spaces. %This population-based mechanism merges promising traits from different individuals, gradually converging toward higher-quality outcomes.

% \todo{revise this.}
% On the other hand, modern deep learning methods have introduced powerful generative models like large language models (LLMs) that excel at producing complex discrete outputs in a single pass. 
% While these models can yield impressive results, they often generate suboptimal or insufficient solutions, especially for out-of-distribution tasks. Consequently, there is a growing trend of allocating test-time computation to improve a model’s outputs after an initial generation rather than training larger or more complex architectures.

% Given the effective search capability in discrete spaces, GAs can be adopted to refine the generated samples. However, a major obstacle in applying GAs as a test-time search mechanism is the reliance on handcrafted evolutionary operators. Designing suitable crossover and mutation strategies often requires significant domain expertise to ensure both validity (e.g., generating syntactically correct sentences, as illustrated in \Cref{fig:chromosome}) and efficiency (e.g., systematically searching more desirable regions). Moreover, although deep models implicitly capture structural knowledge in their learned representations, these insights typically remain unused when GAs perform random or expert-designed operators, limiting the synergy between the two approaches.


% In this work, to close this gap, we propose a \textit{Neural Genetic Search} (NGS), which incorporates the sequential generation capabilities of a trained deep model into GA-based operators.
% \todo{revise here.}
% In \ours{}, we reframe crossover as a \emph{parent-conditioned generative process}. Specifically, the model sequentially selects each new component by sampling from those present in the parents, leveraging its learned distribution to produce high-quality offspring while remaining closely aligned with parental structures.
% This approach maintains the population-based exploration central to GAs yet replaces expert-designed rules that can be overly domain-specific. 
% The key contributions are summarized as follows.
% \begin{itemize}[nosep]
%     \item Efficient \hyeonah{(can we say it?)} test-time search with synergies of GA and the generative policy: summarize 3.3.
%     \item Simple and flexible: easy to implement and can work with different models
%     \item Broad applicability: 
% \end{itemize}


% \begin{figure*}
%     \centering
%     \includegraphics[width=0.75\linewidth]{fig/ga.pdf}
%     \vspace{-15pt}
%     \caption{Overview of GA with \ours{}. (1) The trained generative policy sequentially constructs sequences, which correspond to \textit{chromosomes}, to initialize the population. From this population, (2) parents are selected, and (3) the policy reproduces offspring by sampling new sequences with a parent-conditioned mask. Finally, (4) the newly generated candidates replace members of the population, completing one evolutionary cycle.} \label{fig:main}
% \end{figure*}

% Our experimental results demonstrate the effectiveness of this synergy in a range of tasks, including routing problems (TSP and related variants), adversarial prompt generation for large language models, and molecular design. 
% The extensive computational results indicate that NGS has the potential to become a widely used search method in diverse applications to improve the model's outcome at test time. We also discuss how NGS can contribute to automating the algorithm design process to deliver highly competitive solutions without expert-level domain-specific knowledge.



\section{Background}
\subsection{Sequential Generation in Discrete Space}
\label{sec:background_seq_gen}
This paper focuses on the sequential generation of discrete objects to optimize predefined criteria, \textit{i.e.}, maximize reward functions. This setting includes many practical applications, including solution generation for vehicle routing problems, prompt optimization in language models, and SMILES string generation for molecular optimization.

% We model the construction of these objects as a sequence of decisions, where each component is conditioned on the previously selected components. 

Let $\mathbf{s} \in \mathcal{S}$ represent a sequence that corresponds to a discrete object $\mathbf{x} \in \mathcal{X}$. Without loss of generality and to make the explanation simpler, we assume that the sequence length is $T$, \textit{i.e.}, $\mathbf{s} = (s_1, s_2, \ldots, s_T)$ where $s_t \in V$ is a token from vocabulary $V$. We also denote the partial sequence with length $t-1$ as $s_{< t}$.

We assume that we have a sequential generative policy $p$. The probability of a sequence $\mathbf{s}$ being generated by the policy $p$ is:
\begin{equation}
    p(\mathbf{s})=\prod_{t=1}^{T} p(s_t | s_{< t}),
\label{eq:seq_gen}
\end{equation}
where $s_{< 1}$ is defined as an empty sequence and $p(s_t | s_{< t})$ denotes the conditional distribution over next token $s_t$ given the partial sequence $s_{< t}$ modeled by $p$. In our work, this conditional distribution is given by a pretrained neural network, which is often conditioned on some contexts or constrained with problem-specific constraints.

Note that the mapping from a sequence to a discrete object $g:\mathcal{S} \to \mathcal{X}$ is not necessarily injective, \textit{i.e.}, there can be multiple sequences that correspond to a single object. Thus, the probability distribution over objects is defined as
$
    p(\mathbf{x}) 
    = \sum_{\mathbf{s} : g(\mathbf{s})=\mathbf{x}} p(\mathbf{s}).
$
% Additionally, we assume the existence of an objective function $f(\mathbf{x}): \mathcal{X} \to \mathbb{R}$ which evaluates the quality or desirability of a generated object $\mathbf{x}$, depending on the specific task at hand. For instance, in the case of the TSP, $f(x)$ could represent the total length of the tour, and the goal would be to minimize this value. In molecular optimization, $f(x)$ might represent a property such as binding affinity or drug-likeness, which we aim to maximize.
Additionally, we assume the existence of a reward function $r_{\mathcal{X}}: \mathcal{X} \to \mathbb{R}$ which evaluates the quality of a generated object $\mathbf{x}$. For instance, in the case of the routing problem, we can define $r_{\mathcal{X}}(\mathbf{x})=-c(\mathbf{x})$ where $c$ is the cost function to minimize. In molecular optimization, $c$ could represent a property such as binding affinity or drug-likeness, which we aim to maximize. Since this work focuses on sequence generation, we mainly use the reward function over sequences, $r=r_{\mathcal{X}}\circ g$.

Our task is to maximize the reward function by generating a set of objects using the sequential generative model. Depending on the task, we may also want to maintain the diversity in the generated objects. We provide a more detailed discussion for each problem we considered in~\ref{app:formulation}.



\subsection{Genetic Algorithm}
\label{sec:ga}


Genetic algorithms (GAs) are evolutionary-inspired meta-heuristics. By mimicking natural selection and genetic evolution, GAs iteratively refine a population of candidate solutions. The main components of GA are as follows:
\begin{itemize}[leftmargin=*, itemsep=0pt, topsep=0pt]
\item \textbf{Chromosome:} A representation of a potential solution, encoded in a suitable format (e.g., a sequence of numbers).

\item \textbf{Population:} A group of chromosomes that evolve over generations refined based on fitness.

\item \textbf{Crossover:} Parents exchange genetic material to create offspring, combining strengths from both.

\item \textbf{Mutation:} Random changes to the offspring’s genetic material to maintain diversity and explore new regions of the solution space.

\item \textbf{Selection and Replacement:} Parents are selected based on their fitness, with more desirable individuals having a higher chance of reproduction. Offspring replace less fit individuals in the population.

\end{itemize}


The challenge in developing GAs lies in designing each component, particularly crossover and mutation operators, that respect the unique problem-specific constraints while effectively searching the solution space. In various domains, extensive effort has been devoted to developing highly specialized operators that can handle these complexities.
% For instance, 
% in molecular design, \citet{kerstjens2022leadd} utilize a specialized crossover operator to combine molecular fragments, ensuring that bonds between atoms are compatible. This is achieved by enforcing knowledge-based atom pair compatibility rules that dictate which fragments can be bonded and how. On the other hand, 
% in the Traveling Salesman Problem (TSP), the Edge Assembly Crossover \citep{nagata2013eax} creates subtours by altering the edges of parent solutions and merges them into a valid route using a minimum spanning tree approach with carefully crafted techniques to maintain TSP constraints. 
While these specialized operators are essential for handling problem-specific constraints, they make it challenging to generalize across different problems or adapt to new variations without redesigning the operators.


Despite these challenges, the evolving framework of GAs remains highly powerful due to their inherent capacity to explore large, complex solution spaces. The GA's population-based approach, which maintains a diverse set of solutions, enables the algorithm to explore many potential solutions simultaneously, increasing the likelihood of finding global optima. 
% This adaptability, when combined with the right set of operators, allows GAs to be successfully applied to a broad range of optimization problems, as demonstrated by their use 
GAs have been successfully applied to various fields, including routing \citep{vidal2012hybrid,nagata2013eax,compass,Wouda_Lan_Kool_PyVRP_2024}, molecular design \citep{morris1998lamarckian, jensen2019graph,nigam2021stoned,kerstjens2022leadd}, scheduling \citep{MURATA19961061,GONCALVES200577}, and robotics \cite{davidor1991genetic,lamini2018genetic, ismail2008mobile}.
% hardware designs~\citep{erdin2019multi,de2020genetic,juang2021modified}, 






\section{Neural Genetic Search} \label{sec:method}


% This section introduces our neural genetic search (NGS) algorithm.
We begin by assuming a pretrained policy $p_{\theta^*}$, without imposing any constraints on its architecture or training method; we will leave out $\theta^*$ to lighten the notation.
The only requirement is that the policy samples solutions in a factorized manner, which allows a flexible use of training approaches and models. An overview of our algorithm is provided in \Cref{fig:main} and \cref{alg:ngs}.

% We assume a pretrained policy $p_{\theta^*}$ is available but impose no constraints on the architecture or training methods; we will leave out $\theta^*$ to lighten the notation. The only requirement is that solutions are sampled from a factorized policy. This flexibility allows a wide range of training approaches and architectures as our policy. For the whole process of our algorithm, see \Cref{fig:main} and~\cref{alg:ngs}.



\subsection{Crossover and Mutation}
\label{sec:crossover_and_mutation}
% \todo{Some words as an introduction to this subsection. Emphasize that we use very simple (and thus broad-applicability) genetic operators.}

We propose a problem-agnostic crossover and mutation operations that integrate readily into the generative process of the pretrained model.


% We will leave out $\theta^*$ to lighten the notation.
% trained to generate valid solutions by masking or learning from data. Rather than manually tailoring operators to each problem’s constraints, the policy generates offspring by conditioning on parent solutions. Crossover restricts token choices to components that appeared in both parents, preserving their structure, while mutation relaxes this restriction, allowing exploration beyond the parent tokens.



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{fig/tsp.pdf}
    \vspace{-15pt}
    \caption{Example of TSP. At $t=t'$, the next visit can be one of A, G, or C, since the E, D, and B are masked out by the policy and F is not connected with B either in $\mathbf{s}^1$ and $\mathbf{s}^2$, unless a mutation occurs with a probability of $\mu$. At $t=t'+1$, no valid edge remains under parent token-restriction, so a constraint-enforcing mutation is triggered, allowing any unvisited node to be chosen.}
    \label{fig:tsp}
    \vspace{-2.5pt}
\end{figure*}



\textbf{Crossover.} Given two parent chromosomes $(\mathbf{s}^1, \mathbf{s}^2)$, we define our crossover operation as \textbf{token-restriction}, \textit{i.e.}, the vocabulary for a child is restricted to $V_{\mathbf{s}^1, \mathbf{s}^2} = \{s^1_1, \ldots, s^1_{T_1}\} \cup \{ s^2_1, \ldots, s^2_{T_2} \}$.\footnote{We fixed the number of parents to two, but note that it is allowed to use more than two parents in our algorithm.}
With this restricted vocabulary, the pretrained generative policy constructs new sequences while ensuring that they adhere to certain structural constraints from the parents.
The crossover is simply implemented by \textbf{masking out} the tokens not in $V_{\mathbf{s}^1, \mathbf{s}^2}$ during the sequence generation process in Eq.~\eqref{eq:seq_gen}. We define the next token distribution after the crossover with parents $(\mathbf{s}^1, \mathbf{s}^2)$ as:
\begin{equation}
    p_{\text{cross}(\mathbf{s}^1,\mathbf{s}^2)}(s_t | s_{< t}) \propto \mathbb{I}(s_t \in V_{\mathbf{s}^1,\mathbf{s}^2})p(s_t|s_{< t}),
\label{eq:crossover_policy}
\end{equation}
where $\mathbb{I}$ is an indicator function.
As a result, the crossover operator functions as a parent-conditioned generative process.
% In this way, crossover becomes a parent-conditioned generative process, maintaining crucial structural properties from both parents while leveraging the model’s learned distribution.

% For language generation, we found that the vanilla crossover above could lead to a meaningless repetition of a few words in $V_{\mathbf{s}^1, \mathbf{s}^2}$. To avoid this undesirable behavior, we optionally discard the used token from $V_{\mathbf{s}^1,\mathbf{s}^2}$ after each token selection. Please refer to~\cref{app:formulation_language} for details.


\textbf{Mutation.}  In classical GA, mutation introduces diversity by allowing new genetic material that may not appear in either parent. Analogously, our mutation allows new components by \emph{un-masking} the masked tokens. Thus, mutation removes the effect of the crossover and reverts to the normal sequence generation. 
% Our mutation is simply \textbf{un-masking} the masked tokens, \textit{i.e.}, removing the effect of the crossover and returning to the normal sequence generation. 
This mutation is conducted in two cases:
\begin{itemize}[leftmargin=*, itemsep=0pt, topsep=0pt]
    \item \textbf{Constraint-enforcing mutation.} If all tokens in $V_{\mathbf{s}^1,\mathbf{s}^2}$ cannot satisfy the specific constraints in the sequence, \textit{i.e.}, $\sum_{s_t\in V_{s^1,s^2}} p(s_t|s_{< t})=0$, we forcibly do the mutation.
    \item \textbf{Stochastic mutation.} Even if constraints are met, we apply mutation with probability $\mu \in [0,1]$.
\end{itemize}

Considering the mutation, the next token distribution is then modified further as:
\begin{equation}
    \begin{aligned}
        &p_{\text{NGS}(\mathbf{s}^1,\mathbf{s}^2,\mu)}(s_t | s_{< t})\\
        &= M_{s_{< t},\mathbf{s}^1,\mathbf{s}^2,\mu} \cdot p(s_t | s_{< t})\\
        &\quad + (1 - M_{s_{< t},\mathbf{s}^1,\mathbf{s}^2,\mu}) \cdot p_{\text{cross}(\mathbf{s}^1,\mathbf{s}^2)}(s_t | s_{< t}),
    \end{aligned}
\label{eq:ngs_policy}
\end{equation}
where the binary variable $M$ indicates whether or not the mutation occurs, \textit{i.e.},
\begin{equation*}
    \begin{aligned}
        &M_{s_{< t},\mathbf{s}^1,\mathbf{s}^2,\mu}\\ 
        &=\begin{cases}
        1 & \text{if} \space \sum\limits_{s_t\in V_{\mathbf{s}^1,\mathbf{s}^2}} p(s_t|s_{< t}) = 0 \\
        Y\sim\text{Bernoulli}(\mu) & \text{otherwise}.
        \end{cases}
    \end{aligned}
\label{eq:mutation_rv}
\end{equation*}

The resulting distribution over sequence after applying NGS crossover and mutation with parents $\mathbf{s}^1$ and $\mathbf{s}^2$ becomes:
\begin{equation}
    p_{\text{NGS}(\mathbf{s}^1, \mathbf{s}^2, \mu)}(\mathbf{s}) = \prod_{t=1}^{T} p_{\text{NGS}(\mathbf{s}^1,\mathbf{s}^2,\mu)}(s_t | s_{< t}).
\label{eq:ngs_final_distribution}
\end{equation}




\subsubsection{A Concrete Example: TSP}
\label{sec:example_tsp}

We provide a concrete example with the traveling salesman problem (TSP) to provide further insight into how the crossover and mutation work. 
Generally, a TSP instance is defined on a complete graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is the set of $N$ nodes and $\mathcal{E}$ is the set of edges. 
The goal is to find a tour that visits each node exactly once and returns to the starting point, minimizing the total travel distance; the reward is defined as the negative total distance.

\Cref{fig:tsp} illustrates how new offspring routes are generated using \ours{} in TSP.
A route is constructed sequentially by choosing the subsequent visits.
At time step $t$, the policy samples an edge that connects the current node to the next node.
Choosing the next node to visit from the current node is equivalent to selecting an edge whose one endpoint is the current node and the other endpoint is the next node. 
This sequence of selected edges is directly used as a chromosome.
The policy masks out the edges connecting the current nodes to already-visited nodes to prevent repeated visits. 
Once all nodes are visited, a tour is formed by returning to the starting node.
During offspring generation, the parent vocabulary is defined as the set of \emph{edges} that appear in either of the two parent routes; that is, $V_{\mathbf{s}^1, \mathbf{s}^2} = \bigcup_{t=1}^T\{s^1_t, s^2_t \in\mathcal{E}\}$.
Consequently, $p_{\text{cross}}$ has non-zero probabilities on edges $e=\{i,j\}\in V_{\mathbf{s}^1, \mathbf{s}^2}$ such that $i$ is the current node and $j$ is an unvisited node.
If no edge in the parent set remains valid---they all lead to visited nodes---the token restriction on parents is relaxed through \emph{constraint-enforcing mutation}, allowing the policy to choose from any edge connected to un-visited nodes to maintain route validity. 


\subsection{Rank-based Selection and Replacement}
\label{sec:selection_and_replacement}

For selection and replacement, we employ \textbf{rank-based prioritized sampling}~\cite{tripp2020rank}. Rank-based sampling works as a stochastic elitism, providing controllability for balancing the stochasticity and the eagerness in selection. In this section, we define rank with regard to reward, but it is possible to use multiple criteria (see~\cref{app:formulation_language}).

% TODO: move commented paragraphs to language section

% Another key advantage of using rank is, since rank is scale-agnostic, we can easily balance multiple criteria at the same time, \textit{e.g.}, reward and novelty. In this section, we simply consider rank-based sampling using only reward ranks, and for the example of considering both reward and novelty, please refer to the example of the language generation~\cref{app:formulation_language}. 

% Depending on tasks, we may want to generate both high-rewarded and diverse objects. In these settings, we need to consider not only reward but also novelty when designing selection and replacement rules. We employ \textbf{rank-based prioritized sampling}~\cite{tripp2020rank,kim2024genetic} to balance the two criteria. Since rank is scale-agnostic, rank-based sampling can easily balance those two criteria with different scales.

% We denote the novelty as $\nu:\mathcal{S}\to\mathbb{R}_{+}$. Note that the novelty can be defined in various ways, and novelty function used in our experiments can be found in~\cref{app:formulation}. 

% \paragraph{Selection} Assume that the population $\mathcal{P}$ is filled with $\vert\mathcal{P}\vert$ chromosomes. We first define the weighted rank $R_{r,\nu,\mathcal{P},\omega}$ as follows:
% \begin{equation}
%     R_{r,\nu,\mathcal{P},\omega}(\mathbf{s})=(1-\omega) R_{r,\mathcal{P}}(\mathbf{s}) + \omega R_{\nu,\mathcal{P}}(\mathbf{s}),
% \label{eq:rank}
% \end{equation}
% where $R_{r,\mathcal{P}}$ and $R_{\nu,\mathcal{P}}$ are simply reward and novelty rank in the population, which ranges from $0$ to $\vert\mathcal{P}\vert-1$ (the lower the rank, the higher the reward or novelty).

% We select two chromosomes to compose a pair of parent chromosomes by sampling without replacement with the rank-based prioritized sample probability as follows:
% \begin{equation}
%     \text{Pr}(\mathbf{s})
%     \propto
%     \left(\kappa |\mathcal{P}| + R_{\mathcal{P}}(\mathbf{s})\right)^{-1},
% \end{equation}
% where $\kappa$ is a weight-shifting factor. Note that lower $\kappa$ gives a more sharpened distribution.

Assume that the population $\mathcal{P}$ is filled with $\vert\mathcal{P}\vert$ chromosomes. We denote the rank of a sequence $\mathbf{s}$ in $\mathcal{P}$ respect to the function $f$ as $\texttt{rank}_{\mathcal{P}}(\mathbf{s})$, which ranges from 0 (highest reward) to $\mathcal{P} - 1$ (lowest reward). Rank-based prioritized sampling probability is defined as:
\begin{equation}
    \text{Pr}(\mathbf{s})
    \propto
    \left(\kappa |\mathcal{P}| + \texttt{rank}_{\mathcal{P}}(\mathbf{s})\right)^{-1}.
\label{eq:rank_sampling_prob}
\end{equation}


To generate a pair of parents, we sample two chromosomes without replacement and repeat this independently until we obtain $N_{\text{off}}$ pairs. Those pairs are fed into the crossover and mutation process to generate the next generation. In the replacement phase, we use $\mathcal{P}\cup \mathcal{O}$, instead of $\mathcal{P}$ and sample $N_{\text{pop}}$ chromosomes without replacement.

% (\cref{sec:crossover_and_mutation}), producing the offspring $\mathcal{O}$ containing $N_{\text{off}}$ new sequences. Then, as a replacement phase, we add $\mathcal{O}$ to $\mathcal{P}$ and sample (without replacement) $N_{\text{pop}}$ chromosomes to keep again using Eq.~\eqref{eq:rank_sampling_prob}.

% \paragraph{Replacement} Now we need to replace some of $\mathcal{P}$ with new chromosomes in $\mathcal{O}$. We also use the rank-based sampling approach for this purpose. Specifically, we evaluate the rank in $\mathcal{P} \cup \mathcal{O}$ and sample $N_{\text{pop}}$ chromosomes by following the same procedure as the selection phase using the same $\omega$ and $\kappa$.


\begin{algorithm}
   \caption{Neural Genetic Search} \label{alg:ngs}
    \input{our_algo.tex}
\end{algorithm}

\subsection{Algorithmic Overview and Viewpoints}
\label{sec:algorithm}

The \ours{} algorithm is summarized in~\cref{alg:ngs}. \ours{} offers an improved search capability for the sequential generative models by gracefully combining the strength of GA with the neural policy $p$. A more detailed analysis of its time and memory complexities can be found in \cref{app:complexity}. Below, we highlight two perspectives on NGS: 1) as a novel decoding strategy that leverages evolutionary principles and 2) as a GA enhanced by learned operators.


\begin{figure*}
    % \vspace{-5pt}
    \centering
    \includegraphics[width=1\linewidth]{fig/gnn_all500_results.pdf}
    \vspace{-15pt}
    \caption{Benchmark results on various routing problems. NGS outperformed sampling and ACO by a significant margin in all problems, showing its effectiveness as an inference-time search method. See \cref{app:routing_exp_more} for more comprehensive results.}
    \label{fig:gnn_all500}
    % \vspace{-5pt}
\end{figure*}


\subsubsection{NGS as a decoding strategy}
NGS can be viewed as a decoding scheme for sequential generative models because it converts the learned (conditional) probability distribution into explicit outputs. Specifically, NGS belongs to stochastic decoding methods and shares similarities with top-$k$ or top-$p$ sampling, as it modifies (masking-out) the sampling distribution for better results.
However, unlike single-pass approaches, NGS adopts a population-based approach, which iteratively refines a set of candidate solutions. This population-based procedure often produces more diverse and higher-quality solutions, especially for complex tasks that benefit from repeated improvement steps.
Our experiments in \cref{sec:routing_exp} and \cref{sec:llm_exp} verify that NGS effectively decodes outputs from a model’s learned distribution. 

% \todo{This part needs rewriting.}

% \begin{itemize}
%     \item Our work can be regarded as a novel decoding strategy for sequential generative models.
%     \item There are some works that use genetic algorithms for sequence generation with neural networks. But they are usually problem-specific heuristics.
%     \item \cref{sec:routing_exp} and \cref{sec:llm_exp}
%     \item refer \Cref{app:complexity}
% \end{itemize}



\subsubsection{NGS as a genetic algorithm}

From another perspective, NGS can be viewed as a genetic algorithm (GA) with a learned operator, where a pretrained model specifies how to combine and modify parent solutions via parent-conditioned generation. This approach provides two key benefits: (1) it replaces traditional, hand-crafted heuristics, and (2) it produces more informed, higher-quality offspring by using the model’s learned distribution to guide the combination process, rather than relying on random-walk behavior with predefined rules. As shown in \cref{sec:molopt_exp}, NGS can thus serve as an effective alternative GA.

% \begin{itemize}
%     \item Learned (amortized) genetic operator.
%     \item There are lines of work that automate the heuristic design of optimization algorithms using neural networks~\cite{ye2023deepaco,kim2024ant}.
%     \item \cref{sec:molopt_exp}
% \end{itemize}



\section{Related Works} \label{sec:related}


\textbf{Deep generative models with GA.} Several studies explore combining GAs and deep generative models, particularly in chemical domains. One direction is that utilizing expert-designed GA to improve the generative models' outcome. \citet{ahn2020guiding} and \citet{kim2024genetic} employ Graph GA \citep{jensen2019graph} whose crossover and mutation are designed to guarantee the validity in chemical space, to refine the generated molecules. 
Conversely, some researches suggest to incorporate neural models to enhance or design GAs.s
Notably, in SynNet \citep{gao2022synnet}, the neural policy construct offspring conditioned on molecule embedding from the parent, allowing exploiting the generative capability from the deep model similar to our works. However, to ensure the plausibility, SynNet employs domain-specific reaction rules in the subsequent synthesis planning. 
\citet{gao2024synformer} utilizes the generative model to refine the molecules obtained by Graph GA to more synthesizable ones. In Reinforced Genetic Algorithm \citep[RGA; ][]{fu2022reinforced_ga}, the neural models guides the crossover and mutation process with a protein target structure embedding to overcome a random-work exploration with predefined rules.

More broadly, there is a growing trend that leverages large language models (LLMs) as a black-box operator within GAs, where parent samples are directly fed into LLM to obtain new samples. These LLM-based GAs have been studied in various domains, including prompt optimization \citep{meyerson2024language,liu2024autodan, guo2024evoprompt}, molecular design \citep{wang2024molleo}, and code generations \citep{lehman2023evolution,chen2023evoprompting, ye2024reevo}. However, a primary challenge is the reliance on hand-crafted prompt engineering, and many approaches continue to employ problem-specific rules along with the LLM-based operator \citep{liu2024autodan, wang2024molleo}.



\textbf{Test-time search for enhanced inference. } Recent work in large language models (LLMs) \citep{bai2022constitutional,madaan2023self-refine,snell2024scaling,brown2024large} demonstrates that dedicating additional computation at test time can greatly improve outputs (see \citet{dong2024survey} for an overview).
Meanwhile, in combinatorial optimization (CO), its mathematically rigorous algorithmic foundations have inspired a line of research integrating traditional algorithms into test-time search within deep learning. For instance, \citep{kool2022deep} employ restricted dynamic programming guided by a graph neural network (GNN) that predicts a solution heatmap in routing problems. Meanwhile, \citet{ye2023deepaco} and \citet{kim2024ant} use a predicted edge heatmap in ant colony optimization (ACO) for broader CO applications. \citet{luo2023lehd} propose a new greedy decoding-based search that randomly destroys the generated solutions and reconstructs solutions. Although effective, this method relies on handcrafted rules for destruction.
In addition to external algorithmic integrations, there exist a range of self-improvement approaches, such as beam search \citep{joshi2021learning,choo2022simulation}, sampling (also called \textit{best-of-N}) \citep{kool2018attention}, and Monte Carlo Tree Search \citep{qiu2022dimes}, and active searches \citep{bello2016neural,hottung2021efficient,son2023meta} are also employed for refining solutions at test time.

% In deep learning for CO, various test-time search techniques have emerged. \citet{joshi2021learning} and \citet{drakulic2024bq} utilize beam search, and \citet{choo2022simulation} extends this to a simulation-guided beam search. 
% Another line of research integrates traditional algorithms (e.g., dynamic programming algorithm \citep{kool2022deep} and ant colony optimization \citep{ye2023deepaco, kim2024ant}) or problem-specifically designed search algorithms \citep{luo2023lehd}.
% Additionally, several studies employ active search approaches to fine-tune a pretrained policy, either via policy exploration \citep{bello2016neural,hottung2021efficient}, greedy rollouts \citep{choo2022simulation}, or meta-learning \citep{son2023meta}.






\section{Experiments}

To verify the effectiveness and versatility of the proposed method, we conduct extensive experiments on solving routing problems (\cref{sec:routing_exp}), red-teaming language models (\cref{sec:llm_exp}), and molecular optimization (\cref{sec:molopt_exp}). %The code is available at \href{https://anonymous.4open.science/r/ngs}{https://anonymous.4open.science/r/ngs}.

% \begin{itemize}[leftmargin=0pt]
%     \item in CO 
%     \begin{itemize}
%         \item sota performance with scalability to solve large instnces (tab1)
%         \item it can be directly applied to different routing problems (fig - tsp500, op500, pctsp500, cvrp500)
%         \item robust to distribution shift (tsplib and cvrplib)
%         \item no restriction for the model (with the sota auto-regressive model, we achieve competitive performance)
%     \end{itemize}
%     \item in attacking LLM
%     \begin{itemize}
%         \item a new test-time computation that directly utilize scores 
%         \item balanced score-diversity tradeoff (fig: in-distribution exp)
%         \item robust performance (tab: with different victim model in test time)
%     \end{itemize}
%     \item in molecular design
%     \begin{itemize}
%         \item even when the samples are limited, we can obtain well-performing GA by training policy and conducting GA with NGO
%         \item the policy learns to construct valid molecules (the constraints are satisfied without expert-designed rules)
%         \item during crossover, the pretrained policy generate high-reward samples while mixing two parents.
%     \end{itemize}
% \end{itemize}


\subsection{Routing Problems} \label{sec:routing_exp}

% \todo{Rewrite; emphasize what we want to validate here.} \hyeonah{check.}
In this section, we evaluate ours on various routing problems to verify how NGS efficiently explores the combinatorial solution space at test time. We consider four classic routing problems --- Traveling Salesman Problem (TSP), Capacitated Vehicle Routing Problem (CVRP), Prize-Collecting TSP (PCTSP), and Orienteering Problem (OP) --- all of which have been widely studied in both genetic algorithm and deep learning research. Detailed descriptions for each task are provided in~\cref{app:routing}.
% Each problem involves finding a route that satisfies task-specific constraints (e.g., visiting each city exactly once or respecting vehicle capacity) while maximizing a reward defined according to the problem. 



\subsubsection{Experimental Setup}

\textbf{Implementation details.} Following \citet{joshi2021learning}, we adopt a graph neural network (GNN) to generate an edge heatmap for a given problem instance $\mathcal{G}$. The heatmap serves as a policy that provides the conditional distribution of which edge to select. This heatmap-based approach has been widely adopted in prior works \citep{kool2022deep,ye2023deepaco,kim2024ant} thanks to its simplicity and scalability to large instances.\footnote{Heatmap-based approaches are often referred to as ``non-autoregressive" methods in that they only call the neural network once to generate a heatmap.}
%, and then the decoding is done sorely with the generated heatmap. In contrast, autoregressive methods call the neural network at each sequential generation step. 
We train the GNN for each setting of the problem following the advanced off-policy reinforcement learning algorithm suggested by \citet{kim2024ant} to obtain the pretrained GNN.
% For testing, we evaluate \ours{} and all baselines on 128 heldout instances from \citet{kim2024ant}. If these are unavailable, we generate random instances in advance to use as heldout data. 
All training and testing are performed three times independently.%, each with distinct random seeds.

% Specifically, following the training procedure of \citet{kim2024ant}, the policy is trained using generative flow networks \citep[GFlowNets; ][]{bengio2023gflownet}, which trains the policy to produce a multi-modal distribution of solutions proportional to their rewards, i.e., $p(x)\propto r(x)$. 

\textbf{Baselines.} To validate the effectiveness of our method as a test-time search method, we compare ours against various search algorithms while using the same pretrained GNN. These search algorithms include sampling (\textit{best-of-N}), beam search \citep[BS;][]{joshi2021learning}, Monte Carlo Tree Search \citep[MCTS;][]{qiu2022dimes}, and ant colony optimization \citep[ACO;][]{ye2023deepaco, kim2024ant}. Note that the ACO-based search of \citet{kim2024ant} has already achieved comparable results to state-of-the-art learning-based methods for routing problems; see details in~\cref{app:exp_detail_routing}.

% Notably, by integrating the amortized multi-modal prior distribution and parallel stochastic search of ACO, \citet{kim2024ant} achieves remarkable results on various combinatorial optimization.

\subsubsection{Results}

\begin{table}
    \centering
    \vspace{-7.5pt}
    \caption{Results on TSP and CVRP. Gap (\%) is measured using Concorde \citep{concorde} for TSP and PyVRP \citep{Wouda_Lan_Kool_PyVRP_2024} for CVRP. ``Time'' shows the average duration needed to solve a single instance.} \label{tab:tsp_rst}
    \resizebox{\linewidth}{!}{
    \input{tab/routing_main}
    }
    % \vspace{-5pt}
\end{table}

As shown in \Cref{tab:tsp_rst}, in TSP and CVRP with the number of nodes $200$ and $500$, \ours{} achieves significantly smaller optimality gaps than the search baselines. Notably, \ours{} outperforms both Sampling (long) and ACO (long), which used $10\times$ larger search budget except for CVRP with $N=500$, demonstrating its search efficiency. Moreover, \Cref{fig:gnn_all500} illustrates that \ours{} consistently delivers significant performance gains in all routing problems considered. These results underscore the flexibility and broad applicability of \ours{}. It is noteworthy that all the search algorithms are based on the same pre-trained neural network, \textit{i.e.}, all searches start from the same heatmap; the only difference resulting in the huge performance gaps between the algorithms is the search procedure. More results are provided in \Cref{app:routing_exp_more}
% While the GNN with ACO has already demonstrated its superior performance over various reinforcement learning approaches, \ours{} achieves additional improvements, highlighting its potential to enhance a wide array of well-performing models.


\textbf{Results on real-world instances.}
We further evaluate the proposed method on real-world benchmark problems, TSPLib \citep{reinelt1991tsplib} and CVRPLib \citep{uchoa2017new}. 
As shown in \Cref{tab:tsplib} of \Cref{app:tsplib_cvrplib}, \ours{} preserves strong performance despite the distribution shift, demonstrating robust adaptability.

%The results in~\Cref{tab:tsplib} show the average optimality gap relative to best-known solutions. 
%Despite the distribution shift, our method preserves strong performance, demonstrating robust adaptability and effective exploitation of the pretrained model.



\begin{table}
    \centering
    \vspace{-10pt}
    \caption{Experiment on TSP with an autoregressive model. We adopt the pretrained model from LEHD. ``Time'' is measured as the average duration needed to solve a single instance.} \label{tab:lehd_rst}
    \resizebox{\linewidth}{!}{
    \input{tab/tsp_lehd}
    }
    \vspace{-7.5pt}
\end{table}

\subsubsection{Further Studies}
\textbf{\ours{} with an autoregressive policy.} To verify our versatility, we integrate \ours{} with autoregressive policy as well. We adopt the state-of-the-art model, the Light Encoder and Heavy Decoder \citep[LEHD; ][]{luo2023lehd}, which trained via supervised learning.
% \todo{Check} Since \ours{} does not assume a specific model architecture or training procedure, it can be readily applied to any high-performing policy. We demonstrate this by integrating \ours{} with the Light Encoder and Heavy Decoder \citep[LEHD; ][]{luo2023lehd}, a state-of-the-art autoregressive model trained via supervised learning. 
% LEHD also proposes Random Re-Construct (RRC) to refine solutions, incorporating randomness into the greedy decoding strategy by repeatedly restarting from the randomly destroyed and augmented sub-routes. 
We compare \ours{} with Random Re-Construct (RRC) that refines solutions by repeatedly restarting from the randomly destroyed and augmented sub-routes.
\Cref{tab:lehd_rst} shows that LEHD + \ours{} achieves the lowest optimality gap compared to sampling and RRC.
While RRC achieves competitive outcomes, it is specifically tailored to routing problems and can struggle to generate diverse solutions when further constraints are introduced.


\textbf{Sensitivity Analysis.} We examine the impact of GA hyperparameters.
% —such as population size, number of offspring, and mutation rate—on our method. 
As reported in \Cref{app:exp_ablation}, \ours{} consistently performs well across a broad range of parameter settings, indicating its robustness and practical usability.


\begin{table}
    \vspace{-10pt}
    \centering
    \caption{The attacker model is fine-tuned and evaluated with \textbf{Source} victim model (Llama-3.2-3B-Instruct). The attacker is also evaluated on other various victim models, which corresponds to \textbf{Transfer} setting. The highest mean toxicities among sampling-based algorithms are highlighted with \textbf{Bold}. All the reported values are averaged over five independent runs with distinct seeds.}
    \label{tab:llm_rst}
    \resizebox{1.0\linewidth}{!}{
    \input{tab/llm_main}
    }
    \vspace{-12pt}
\end{table}


% \clearpage
\subsection{Red-Teaming Language Models} \label{sec:llm_exp}
In this experiment, we view \ours{} as an alternative decoding strategy, especially in the context of automated red-teaming of language models (LMs).

Red-teaming language model aims to generate attack prompts to induce undesirable output from a ``victim'' language model. Our experiment is based on the approach that fine-tunes ``attacker'' LM to serve as an automated attack prompt generator~\cite{perez2022red,lee2024learning}. We fine-tune the attacker LM following the fine-tuning approach proposed by~\citet{lee2024learning}. Specifically, the reward of an attack prompt is defined as the probability of it being toxic (see ~\cref{app:formulation_language} for details). This reward signal is then used for fine-tuning based on the combination of GFlowNets and maximum likelihood estimation. Given the fine-tuned attacker LM, we investigate how the decoding schemes affect the red-teaming performance.

The evaluation of an attack algorithm follows these steps: 1) Generate 1,024 attack prompts using an attacker LM and a decoding algorithm to evaluate. 2) For each attack prompt, generate 5 responses using the target victim model, calculate the toxicity of each response using the classifier, and take the average, resulting in the toxicity of the prompt. 3) Calculate average toxicity and diversity over 1,024 attack prompts. Note that generating diverse, high-reward prompts is desirable in red-teaming unlike optimization tasks. We use average pairwise cosine distance in the embedding space using MiniLMv2~\cite{wang2021minilmv2} as a diversity measure.
% to obtain a set of diverse and high-reward attacks, unlike the routing problems where we only care about the single best solution.

% Since we want to obtain diverse and high-reward attack prompts, unlike the optimization problems, we incorporate the novelty of each chromosome in the selection and replacement phase (\cref{sec:selection_and_replacement}), such that we could preserve diversity in the population over the iterations (see~\cref{app:formulation_language} for details).

\textbf{Experimental setup.} Throughout the experiments, GPT-2~\cite{radford2019language} is used as an attacker and Llama-Guard-3-8B~\cite{dubey2024llama3herdmodels} as the safety classifier. During fine-tuning, Llama-3.2-3B-Instruct is used as a victim. For testing, we evaluate the attacker LM using various victim LMs, not only the one used in the fine-tuning phase, but also unseen LMs: Llama-3.1-8B-Instruct, Llama-3.3-70B-Instruct~\cite{dubey2024llama3herdmodels}, Gemma-2-9b-it~\cite{team2024gemma}, Qwen2.5-7B-Instruct~\cite{qwen2}, and phi-4~\cite{abdin2024phi}.


% Llama-Guard-3-8B~\cite{dubey2024llama3herdmodels} and GPT-2~\cite{radford2019language} are is used as the classifier and attacker model, respectively. and Llama-3.2-3B-Instruct as the victim model~\cite{dubey2024llama3herdmodels}. After fine-tuning, we evaluate the attacker LM using various victim LMs, not only the one used in the fine-tuning phase, but also unseen LMs: Llama-3.1-8B-Instruct, Llama-3.3-70B-Instruct~\cite{dubey2024llama3herdmodels}, Gemma-2-9b-it~\cite{team2024gemma}, Qwen2.5-7B-Instruct~\cite{qwen2}, and phi-4~\cite{abdin2024phi}.

\textbf{Implementation details.} First, to further encourage diversity in the selection and replacement, we introduce the novelty of a chromosomes and use weighted rank in rank-based prioritized sampling in Eq.~\eqref{eq:rank_sampling_prob}. In addition, in crossover, we optionally discard the used token from $V_{\mathbf{s}^1,\mathbf{s}^2}$ after each token selection to avoid meaningless repetition; please refer to \cref{app:formulation_language} for details.


% For language generation, we found that the vanilla crossover above could lead to a meaningless repetition of a few words in $V_{\mathbf{s}^1, \mathbf{s}^2}$. To avoid this undesirable behavior, we optionally discard the used token from $V_{\mathbf{s}^1,\mathbf{s}^2}$ after each token selection. Please refer to~\cref{app:formulation_language} for details.

\textbf{Baselines.} We considered canonical sampling-based decoding strategies for language models, specifically, tempered sampling (Temp.) with temperature $\tau$, top-k sampling, and top-p sampling (also known as Nucleus sampling, \citealt{Holtzman2020The}). We also include beam search (BS) with beam width $w$, while we did not directly compare against it because it is deterministic.




\begin{table*}
    \vspace{-10pt}
    \centering
    \caption{Average scores of Top-10 molecules discovered within 10,000 evaluations}
    \label{tab:mol_rst}
    \resizebox{0.95\textwidth}{!}{
    \input{tab/molopt_top10}
    }
    \vspace{-7pt}
\end{table*}



\textbf{Results.} We summarized the results in~\cref{tab:llm_rst}.%\footnote{Note that the diversity for the `\textbf{Transfer}' settings is almost identical to the Source setting.} 
The `\textbf{Source}' column result shows that \ours{} could comparably balance the toxicity (reward) and diversity. The results in \textbf{Transfer} setting are more remarkable in that \ours{} outperforms the other decoding schemes significantly, showing robustness toward the distribution shift. This is attributed to \ours{}'s unique characteristic, population-based iterative generation, which could adapt the generation process towards better search space by conditioning on promising parents from the previous generations.
% dynamically adapting the generation process to focus more on promising regions in the vast search space.
% \todo{Emphasize the results more effectively...} 
For more comprehensive results with standard deviation and results obtained using another model as source victim models, please refer to~\cref{app:llm_exp}.

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{fig/llm_in-distribution.pdf}
%     \caption{In distribution. \todo{Move this to appendix}}
%     \label{fig:llm_in}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{fig/llm_transfer_fromllama-3_to_phi-4_and_qwen.pdf}
%     \caption{Attack model trained using Llama-3.1-8B-Instruct as victim model. Tested victims are phi-4 (left) and Qwen2.5-7B-Instruct (right). \todo{TABLE}}
%     \label{fig:llm_transfer}
% \end{figure}




\subsection{De novo Molecular Design} \label{sec:molopt_exp}


In this experiments, we focus on verifying the effectiveness of \ours{} as a new GA method, unlike the experiments in routing and red-teaming. In molecular design, various studies have found that GA methods still remain as strong baselines for recent deep learning methods. Furthermore due to the black-box properties in chemical space, test-time search with deep generative models have rarely studied. In several works, Graph-based GA \citep{jensen2019graph}, an expert-designed GA, is directly employed to refine the solutions or support the policy explorations \citep{ahn2020guiding,kim2024genetic,gao2024synformer,wang2024molleo,lee2024molecule}

% we apply \ours{} for de novo molecular design and demonstrate that \ours{} provides an alternative way to design a genetic algorithm by learning the structural knowledge of chemical spaces using a deep generative policy.


\textbf{Experimental setup. }
De novo molecular design aims to discover molecules with the desired property, which is measured by the objective function $f$, $x^* = \argmax_{x \in \mathcal{X}} f(x),$ where $x$ is a molecule, and $\mathcal{X}$ denotes the chemical space which comprises all possible molecules.
We follow the experimental setting in the Practical Molecular Optimization (PMO) benchmark \citep{gao2022sample}, which limits evaluations to 10,000. We provide further details in \cref{app:formulation_molecule}.
% The objective function evaluations are usually computationally expensive, for example, they involve simulation to measure docking affinity to a target protein. Thus, the number of evaluations is limited in practice. 

% \subsubsection{Experimental setup}

\textbf{Implementation details.} This work employs a string-based molecular representation, the Simplified Molecular-Input Line-Entry System (SMILES) strings \citep{weininger1988smiles}, to generate molecules; the examples are provided in \Cref{fig:chromosome}. Following prior works \citep{olivecrona2017molecular, kim2024genetic}, we adopt an LSTM policy to generate SMILES sequences. Since we have a limited budget, we use $8K$ calls to train the policy using GFlowNets and $2K$ to conduct the genetic search with \ours{}; see details in \Cref{app:exp_detail_molopt}.


\textbf{Baselines.} We compare \ours{} with GA methods specially designed for molecular design: Graph GA \citep{jensen2019graph}, SMILES GA \citep{brown2019guacamol}, STONED \citep{nigam2021stoned}, and SynNet \citep{gao2022synnet}. They utilize fragment-based graphs, SMILES, SELFIES \citep{krenn2020selfies}, and synthesis, respectively, as molecular representations.
Note that in SMILES GA and STONED, where the string-based representations are employed, mutation is solely applied to obtain offspring molecules.% to explore the molecule space.
% In particular, SMILES does not guarantee the validity of molecules, it often results in invalid molecules if we generate SMILES strings randomly. Consequently, designing a valid crossover operator for SMILES is non-trivial, as naive approaches often produce syntactically or chemically invalid outputs.

% \subsection{Results}

\textbf{Results.} As depicted in \Cref{tab:mol_rst}, \ours{} outperforms previous GA methods in average Top-10 scores across 10 tasks in different types by achieving the best scores in 8 tasks out of 10. 
These results highlight the effectiveness of \ours{} as an alternative GA method despite the computational cost of policy training. In \Cref{app:exp_molopt}, we compare \ours{} with the results with training only.
Moreover, \ours{} manages to produce valid molecules without explicitly enforcing validity checks or SMILES grammar constraints at each step. Because the policy is trained on valid examples, it naturally learns to generate syntactically correct SMILES strings and thus maintains a high rate of validity in its outputs.



% \newpage
\section{Conclusion} \label{sec:conclusion}


\paragraph{Contributions.} 
% We propose \textit{Neural Genetic Search (NGS)}, a new test-time search method that combines the population-based exploration of genetic algorithms with deep generative models. 
% By defining crossover as a parent-conditioned generative process, NGS removes the need for specialized, domain-specific rules by using a pretrained model’s learned distribution to produce high-quality new solutions. 
% This approach preserves the evolutionary advantages of genetic algorithms while incorporating powerful insights captured by the trained model.
We propose Neural Genetic Search (NGS), a test-time search algorithm that combines the population-based exploration of genetic algorithms with the expressive power of pretrained generative models. By replacing domain-specific crossover rules with a parent-conditioned generation process and allowing mutation through unrestricted sampling, NGS offers an iterative refinement strategy that boosts solution quality across diverse tasks without needing specialized heuristics.
Our experiments, conducted across diverse tasks such as routing problems, adversarial prompt generation, and molecular design, show that NGS improves solution quality compared to previous search methods. Beyond its strong performance, NGS is flexible and easy to adopt: it only requires a pretrained model that constructs discrete outputs sequentially.
% Lastly, since NGS is a neural network–guided GA, it can be readily applied to various domains without extensive customization.


\paragraph{Limitations.} Despite these advantages, NGS relies on the quality of the underlying neural policy. The potential performance gains may be limited if the pretrained model’s distribution does not encompass high-quality solutions. In such cases, the policy can be fine-tuned by incorporating NGS, similar to approaches introduced by \citet{choo2022simulation}; we leave this as future work. Another limitation is that NGS introduces a set of GA-related hyperparameters.  While we provide the rationale behind their selection and demonstrate their robustness across diverse configurations, practitioners may still need to adjust them for specific tasks.


% \paragraph{Contributions}
% \begin{itemize}
%     \item A novel search algorithm (can be seen as a decoding algorithm)
%     \item A novel GA algorithm with learned operators.
%     \item Algorithmic simplicity and versatility
%     \item Powerful performance across domains 
% \end{itemize}

% \paragraph{Limitations} 
% \begin{itemize}
%     \item NGS performance is largely dependent on the policy quality. 
%     \item Introduces a set of hyperparameters. However, this is largely due to the nature of the genetic algorithm. We tried to minimize the hyperparameters. We provided the rationale behind the selection of major hyperparameters and also analyzed the robustness to change in those.
%     \item Blahblah
% \end{itemize}

\clearpage



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Detailed problem formulations}
\label{app:formulation}

This section provides more detailed explanations of each sequential generation problem we considered.


\subsection{Routing problems}
\label{app:formulation_routing}


In all routing problem we considered, a problem instance can be defined on the fully connected graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is the set of $N$ nodes, and $\mathcal{E}$ is a set of edges, each associated with a weight representing the distance between two connected nodes. The goal is to find the optimal route that satisfies all problem-specific constraints.

In this context, a candidate solution for a routing problem can be defined as a set of edges that compose a route, which in turn can be represented as a sequence of edges. Thus, the vocabulary corresponds to all the edges, i.e., $V = \mathcal{E}$, a chromosome corresponds to a sequence of edges, and the policy $p$ is a conditional distribution that enables sequential selection of edges. Note that the policy should be conditioned on the problem-specific constraints, which mask out the infeasible edges based on these constraints. The reward $r$ is defined as the objective function of each problem. We provide details about problem-specific components for each problem in~\cref{app:routing}.



\subsection{Red-Teaming Language Models}
\label{app:formulation_language}

In this task, we use an attacker language model (LM) to generate attack prompts to elicit toxic responses from victim LM. The $V$ corresponds to the vocabulary of the attacker LM (GPT-2~\cite{radford2019language} in our experiment), and a chromosome is an attack prompt (a sentence in natural language). We define our sequential generative policy $p$ as the attacker LM equipped with top-$p$ (Nucleus) sampling~\cite{Holtzman2020The} (p=0.95). The top-$p$ serves as a ``plausibility constraint,'' which prevents the crossover (Eq.~\eqref{eq:crossover_policy}) from generating highly unlikely tokens.

The reward for an attack prompt is defined as the average toxicity of the response from the victim LM given the attack prompt. The toxicity is the probability of the `unsafe' token of the safety classifier model (Llama-Guard-3~\cite{dubey2024llama3herdmodels}) given the victim's response.

We found that the token restriction using $V_{\mathbf{s}_1, \mathbf{s}_2}$ as suggested in~\cref{sec:crossover_and_mutation} could lead to meaningless repetition of a set of words in the parents (and this often `hacks' the reward function and gives better quantitative results). To prevent this undesirable behavior, we discard the token from $V_{\mathbf{s}_1, \mathbf{s}_2}$ once it is selected. Algorithmically, at step $t$, we sample $s_t$ following Eq.~\eqref{eq:ngs_policy} and then replace $V_{\mathbf{s}_1, \mathbf{s}_2}$ with $V_{\mathbf{s}_1, \mathbf{s}_2} \setminus \{s_t\}$.

As discussed in~\cref{sec:llm_exp}, it is desirable to generate a diverse set of toxic prompts in the red-teaming language model task. To promote diversity in the population, we incorporate novelty measure during selection and replacement. We define the novelty $\nu$ of a chromosome $\mathbf{s}$ as averaged pairwise cosine distance against the population, \textit{i.e.},
\begin{equation}
    \nu(\mathbf{s};\mathcal{P}) = \frac{1}{\vert\mathcal{P}\vert} \sum_{\mathbf{s}' \in \mathcal{P}} (1 - \text{cosine\_similarity}(e(\mathbf{s}),e(\mathbf{s}')),
\end{equation}
where $e$ is the sentence encoder (MiniLMv2~\cite{wang2021minilmv2}). Then, we define weighted rank using both reward and novelty as follows:
\begin{equation}
    \texttt{rank}_{r, \nu, \omega, \mathcal{P}}(\mathbf{s}) = (1-\omega) \cdot \texttt{rank}_{r,\mathcal{P}}(\mathbf{s}) + \omega \cdot \texttt{rank}_{\nu,\mathcal{P}}(\mathbf{s}),
\end{equation}
where $\omega$ is novelty rank weight (we set $\omega=0.1$) and $\texttt{rank}_{r,\mathcal{P}}$ and $\texttt{rank}_{\nu,\mathcal{P}}$ is reward and novelty rank, respectively. This weighted rank is used for selection and mutation, following the rank-based selection rule in Eq.~\eqref{eq:rank_sampling_prob}.



\subsection{De novo molecular design}
\label{app:formulation_molecule}

We employ the string-based representation, the Simplified Molecular-Input Line-Entry System \citep[SMILES; ][]{weininger1988smiles}, which represents molecules using ASCII text. SMILES encodes a molecule’s connectivity (which atoms are bonded to which), as well as additional details such as bond types, charges, and stereochemistry. We directly adopt the vocabulary set from \citet{olivecrona2017molecular}, which consists of 55 tokens, including start and end tokens. The examples are illustrated in \Cref{fig:chromosome}.

The reward is defined as a normalized scalar in $[0,1]$ that measures a pharmaceutically relevant property. 
For example, QED quantifies the drug-likeness of molecules, whereas JNK3, DRD2, and GSK3b measure a molecule’s activity 
against specific proteins. Please refer to \citet{gao2022sample} and \citet{brown2019guacamol} for additional details 
on each task.

Lastly, we employ an LSTM policy \emph{without} explicitly enforcing validity constraints, i.e., $p(\mathbf{s}) = \prod_{t=1}^T p_\theta\bigl(s_t \mid \mathbf{s}_{<t}\bigr),$
where $T$ is the maximum length. Following previous work \citep{olivecrona2017molecular}, the policy is initialized 
with a prior policy trained on a public dataset (e.g., ZINC250K) to learn valid SMILES patterns. We then rely on this 
policy to guide the crossover process and produce valid SMILES strings.



\section{Additional experimental details}

\paragraph{Computing resource. } We use a server with two sockets of AMD EPYC 7542 32-Core Processor, and a single GPU, the NVIDIA RTX A6000, for the routing and De novo molecular design experiments. For the red-teaming language models task, we use a cloud server with four NVIDIA A100 HBM2e 80GB PCIe gpus.

\subsection{Routing problems}
\label{app:exp_detail_routing}


% We intentionally omitted other well-known combinatorial optimization baselines, such as supervised learning methods (\citet{sun2023difusco,drakulic2024bq} \todo{anything else?}) or problem-specific heuristic approaches (\citet{cheng2023select,hou2022generalize}) since our 

\paragraph{Training procedure.} We followed the training procedure of~\citet{kim2024ant}.\footnote{\url{https://github.com/ai4co/rl4co}} Specifically, we train a graph neural network (GNN) that generates heatmaps using the GFlowNet~\cite{bengio2021flow} training, combined with off-policy exploration through the local-search operators (2opt for TSP, Swap*~\cite{vidal2022hybrid} for CVRP, and destroy-and-repair local search for others). For details regarding the training procedure, please refer to the original paper~\cite{kim2024ant}.

\paragraph{Hyperparameters.} For sampling, we use 1,000 for mini-batch size. We use 100 for the number of ants in ACO and the number of offspring in \ours{}, which makes the two algorithms have the same number of iterations: 10 when generating 1,000 candidates and 100 when 10,000 (long). Note that for TSP and CVRP, we employ the local search after solution generation for all baselines, as usually done in heatmap-based approaches. We use 100 for both population size and offspring size of \ours{}, 0.01 for the mutation rate $\mu$, and 0.001 for the weight-shifting factor $\kappa$.
 
\subsection{Red-Teaming Language Models}
\label{app:exp_detail_llm}

\paragraph{Fine-tuning procedure.} We mainly followed the two-stage fine-tuning procedure of~\citet{lee2024learning}. In the first stage, a policy explores the space of prompts during the GFlowNet-based fine-tuning. All evaluated prompts are stored in the buffer. In the second stage, we fine-tune the attack language model (LM) with high-quality prompts obtained by filtering prompts with both high toxicity and high likelihood from the buffer. For more details, please check the original paper~\cite{lee2024learning}.

\paragraph{Hyperparameters.} At test time, we attack a victim LM using the fine-tuned attack LM. We considered common sampling-based decoding strategies as baselines, including sampling, tempered sampling with temperature $\tau\in[0.5, 0.8]$, top-$k$ sampling with $\text{k}\in=[5, 10]$, and top-$p$ sampling with $\text{p}\in[0.5,0.8]$. Each baseline only changes the specified hyperparameter, and the others remain the same (\textit{e.g.}, top-$k$ or top-$p$ use $\tau=1$). For \ours{}, we use 256 and 16 for the population size and offspring size, respectively. We use 0.01 for both the stochastic mutation rate $\mu$ and the weight-shifting factor $\kappa$ for rank-based sampling.



\subsection{De novo molecular design}
\label{app:exp_detail_molopt}

\paragraph{Training procedure.} Unlike routing problems or language-model attacks, molecular design constrains the total number of evaluations rather than distinctly separating training from inference. Therefore, we allocate 8K evaluations to train the policy and then conduct NGS with the trained policy. Following \citet{kim2024genetic}, we adopt generative flow networks \citep[GFlowNets; ][]{bengio2021flow,bengio2023gflownet} but without guided exploration. Because GFlowNets are off-policy, 
they can leverage replay training extensively, thus exhibiting sample-efficient learning. Indeed, the results in \citet{kim2024genetic} show that GFlowNets outperform REINVENT \citep{olivecrona2017molecular} even without guided 
exploration. 
Specifically, we initialize our policy with the same pretrained parameters used in REINVENT, trained on an unlabeled dataset. We then use the allocated 8K evaluations to train this policy by generating samples, storing them in an experience buffer, and minimizing the trajectory-balance loss \citep{malkin2022gflownets} using those buffered samples. We follow the hyperparameter setup from \citet{kim2024genetic}, including the batch size, number of replay training iterations, inverse temperature, and learning rates; please refer to their original implemetation for more details.\footnote{\url{https://github.com/hyeonahkimm/genetic_gfn}}

\paragraph{Hyperparameters.} During NGS, we use the population size and offspring size as 100 and 5, respectively. The stochastic mutation rate $\mu$ is set as 0.01, and the weight-shifting factor $\kappa$ in Eq.~\eqref{eq:rank_sampling_prob} as 0.01.




\section{Description of routing problems}
\label{app:routing}

In routing problems, the problem is defined on the fully connected graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is the set of $N$ nodes, and $\mathcal{E}$ is a set of edges, each associated with a weight representing the distance between two connected nodes. The goal is to find the optimal route that satisfies all given constraints. In this context, a route is defined as a cycle within the graph, and the vocabulary is composed of the set of edges $\mathcal{E}$. We provide details about problem-specific components for each problem. All problem instances are generated according to \citet{ye2023deepaco}.


\subsection{Traveling salesman problem}
The traveling salesman problem (TSP) aims to find the shortest route that visits all cities exactly once and returns to the starting point. 
% A problem instance is defined as a fully connected graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is the set of $N$ nodes (i.e., cities), and $\mathcal{E}$ is a set of edges, each associated with a weight representing the distance between two connected nodes. 
A solution is defined as a Hamiltonian cycle with minimum total weights (i.e., total distance to travel). Consequently, the reward function $r(x)$ is defined as a negative value of total distance. %, and the whole vocabulary $V$ is defined as the edge set $\mathcal{E}$. 
Starting from a random node, a route is generated by sequentially selecting the next node to visit.
This is done by choosing an edge connected to the current node, thus extending the route. To avoid revisiting nodes, the model masks out edges that lead to already visited nodes.


\subsection{Capacitated vehicle routing problem}

The capacitated vehicle routing problem \citep[CVRP; ][]{dantzig1959truck} is defined on a fully connected graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where V is the set of nodes, which includes a depot (the starting and ending point for the vehicles) and the customers (the locations that need to be served with demand), and $\mathcal{E}$ is the set of edges representing the connections between nodes, each with an associated travel distance. In CVRP, we assume the use of multiple homogeneous vehicles, each with a capacity $Q$, and the goal is to serve all customers exactly once while minimizing the total travel distance. The reward function is defined as the negative of the total distance, and the solution consists of a set of multiple routes, each starting and ending at the depot (a special node with zero demand). Additionally, the total demand of each route cannot exceed the vehicle capacity $Q$.

Similar to the TSP, the solution is generated sequentially by selecting the next node, which corresponds to choosing an edge connected to the current node. However, the process begins at the depot, which allows multiple visits. At each step, the policy masks out edges that lead to already visited nodes, except for the depot. To enforce the capacity constraints, edges leading to nodes with demands that exceed the remaining vehicle capacity are also masked out to have zero probability.


\subsection{Prize-collecting traveling salesman problem}

The prize-collecting traveling salesman problem \citep[PCTSP; ][]{balas1989prize} is a variation of the TSP, where the constraints for visiting all nodes are relaxed. Instead, the PCTSP introduces the prize constraints about the minimum prizes to collect.
In the PCTSP, each node has a prize and penalty; thus, the salesman gets prizes for visiting the cities and penalties for the unvisited cities. 
The reward is defined as the summation of the  total distance and the net penalties from un-visited nodes. 

Similar to the CVRP, a route starts and ends with the depot whose prize and penalty are zero. To prevent repeated visiting, the policy masks out all edges connected to already visited nodes. In addition to satisfy the prize constraints, when the collected prize is less than the minimum prizes the edge connected to the depot is also masked out.


\subsection{Orienteering problem}
The orienteering problem \citep[OP; ][]{golden1987orienteering} is a variation of the classical routing problems where the objective is to find a route that maximizes the total prize collected from visited cities, subject to a constraint on the total travel distance. Unlike the PCTSP, where the goal is to minimize penalties for unvisited cities, the orienteering problem is focused on maximizing the reward within a limited travel budget.

In the OP, each city has a prize associated with it, and the objective is to visit a subset of cities in order to maximize the total prize collected, while ensuring that the total distance traveled does not exceed a specified maximum distance.
Thus, tshe reward is defined as the sum of the prizes from the cities visited. 

Similar to the PCTSP, the solution is represented as a set of routes starting from a depot. Each route must respect the distance constraint while selecting cities that contribute to the total prize. To prevent revisiting cities, the policy masks out edges connected to previously visited cities, ensuring that each city is visited at most once. Additionally, the edge connecting the depot can be masked out if the total distance constraint has already been met or exceeded, ensuring no additional unnecessary travel occurs.



\section{Time and memory complexities} \label{app:complexity}
The proposed crossover, mutation, selection, or replacement in~\cref{sec:method} does not require a significant amount of time to perform. However, it may increase the generation time depending on the mini-batch size.

% Practically in the optimization context, we evaluate a set of multiple candidates using metrics defined over this set (\textit{e.g.}, top-$k$ average, diversity, etc.), and also, the generation of the set is often done sequentially using the mini-batch. In this case, the 

Consider that we want to generate $K$ sequences in total and assume that our mini-batch size is limited to $m$ ($K>m$). Then, we need to iterate $\left\lceil K / m \right\rceil$ times of generation, regardless of the generation algorithm. On the other hand, when considering the NGS iteration with specified $N_{\text{pop}}$ and $N_{\text{off}}$, we need to iterate $\left\lceil N_{\text{pop}} / m \right\rceil + \left\lceil (K - N_{\text{pop}}) / N_{\text{off}} \right\rceil \cdot \left\lceil N_{\text{off}} / m \right\rceil$ times. When $m$ is smaller than $N_{\text{off}}$, the number of iterations of NGS is similar to the normal generation. Otherwise, the number of iterations can increase much larger than $\left\lceil K / m \right\rceil$. In practice, however, $m$ is usually smaller compared to $N_\text{off}$, and thus, the time complexity is not increased too much (see~\cref{tab:lehd_rst}).

Our algorithm slightly increases memory usage, as the population should be kept in memory, which is usually negligible. In routing problem experiments, NGS requires additional memory since we need to construct the $N \times N$ edge matrix for each chromosome. However, using the sparse matrix could largely relieve this.
% Also, though it is orthogonal to our algorithm, the novelty calculation (\cref{app:formulation_language}) can increase the memory consumption depending on its definition \textit{e.g.}, pairwise distance calculation.



\section{Additional experimental results on routing problems}

\subsection{Extended results}
\label{app:routing_exp_more}

\cref{fig:gnn_all_results} shows extended results for the routing problems (\cref{sec:routing_exp}), including the results for instances with 1,000 nodes. Overall, \ours{} substantially outperforms the baseline methods in all settings except for CVRP with 1,000 nodes. We suspect two reasons for the worse results in CVRP1000: (1) our heatmap-based policy may be suboptimal given the increased complexity of large-scale CVRP, and (2) the post-processing local search, Swap*~\cite{vidal2012hybrid}, largely overshadows any differences between the generation algorithms.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/gnn_all_results.pdf}
    \vspace{-15pt}
    \caption{Comprehensive results on routing problems.}
    \label{fig:gnn_all_results}
\end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.9\linewidth]{fig/gnn_cvrp_results.pdf}
%     \caption{TBU}
%     \label{fig:gnn_cvrp_results}
% \end{figure}


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.9\linewidth]{fig/gnn_pctsp_results.pdf}
%     \caption{TBU}
%     \label{fig:gnn_pctsp_results}
% \end{figure}


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.9\linewidth]{fig/gnn_op_results.pdf}
%     \caption{TBU}
%     \label{fig:gnn_op_results}
% \end{figure}



\subsection{Results on real-world instances} \label{app:tsplib_cvrplib}

We benchmark our model against baselines on real-world TSP and CVRP instances from TSPLib~\citep{reinelt1991tsplib} and CVRPLib-X~\citep{uchoa2017new}. We use the models trained on random uniform instances of size 200, 500, and 1,000 for evaluation of TSP/CVRPLib instances with sizes 100-299, 300-699, and larger than 700, respectively. As shown in \cref{tab:tsplib} \ours{} achieves significantly better performance than the baselines on these real-world datasets.


\begin{table}[ht]
    \centering
    \caption{Average optimality gap with the best-known solutions on TSPLib and CVRPLib-X.}
    \label{tab:tsplib}
    % \resizebox{0.6\linewidth}{!}{
    \input{tab/tsplib}
    % }
\end{table}


\subsection{Sensitivity analysis} \label{app:exp_ablation}

We conduct sensitivity analysis for the key hyperparameters --- population size $N_{\text{pop}}$, offspring size $N_{\text{off}}$, and the mutation rate $\mu$ --- on TSP and CVRP with 500 nodes. The results in \cref{fig:sensitivity_routing} indicate that a smaller population size generally yields better outcomes, likely due to more greedy parent selection. Increasing the offspring size tends to degrade performance, as it reduces the number of search iterations.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.325\textwidth}
        \includegraphics[width=\textwidth]{fig/gnn_ablation_npop.pdf}
        \caption{Analysis of the population size $N_{\text{pop}}$.}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \includegraphics[width=\textwidth]{fig/gnn_ablation_noff.pdf}
        \caption{Analysis of the offspring size $N_{\text{off}}$.}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \includegraphics[width=\textwidth]{fig/gnn_ablation_mu.pdf}
        \caption{Analysis of the mutation rate $\mu$.}
    \end{subfigure}
    \caption{Sensitivity analysis on TSP and CVRP with 500 nodes.} \label{fig:sensitivity_routing}
\end{figure}




\clearpage
\section{Additional experimental results on red-teaming language models} \label{app:llm_exp}

\subsection{Extended Results}
In~\cref{tab:llm_llama3_2}, we provide a more detailed version of~\cref{tab:llm_rst} with standard deviation. Also, in~\cref{tab:llm_llama3_1}, we provide additional results obtained by using Llama-3.1-8B-Instruct~\cite{dubey2024llama3herdmodels} as source victim LM.

\begin{table}[ht]
    \vspace{-10pt}
    \centering
    \caption{The attacker model is fine-tuned and evaluated using \textbf{Llama-3.2-8B-Instruct} as \textbf{Source} victim model. The highest mean toxicities among sampling-based algorithms are highlighted with \textbf{Bold}. All the reported values are averaged over five independent runs with distinct seeds. Note that the diversity for the Transfer settings is almost identical to the Source setting.}
    \label{tab:llm_llama3_2}
    \resizebox{0.9\linewidth}{!}{
    \input{tab/llm_app_llama3_2}
    }
\end{table}

\begin{table}[ht]
    \vspace{-10pt}
    \centering
    \caption{The attacker model is fine-tuned and evaluated using \textbf{Llama-3.1-8B-Instruct} as \textbf{Source} victim model. The highest mean toxicities among sampling-based algorithms are highlighted with \textbf{Bold}. All the reported values are averaged over five independent runs with distinct seeds. Note that the diversity for the Transfer settings is almost identical to the Source setting.}
    \label{tab:llm_llama3_1}
    \resizebox{0.9\linewidth}{!}{
    \input{tab/llm_app_llama3_1}
    }
    \vspace{-50pt}
\end{table}





\clearpage
\section{Additional experimental results on molecular design} \label{app:exp_molopt}

\begin{table}[ht]
    \centering
    \caption{Average Top-10 scores with 10K training and ours where we conduct \ours{} during last 2K evaluations.} \label{tab:molopt_abl}
    \resizebox{0.65\linewidth}{!}{
    \input{tab/molopt_abl}
    }
\end{table}

In this section, we examine the effect of conducting NGS rather than training the policy by fully leveraging the limited evaluation budgets. The results in \Cref{tab:molopt_abl} shows that NGS achieves higher Top-10 scores in overall. Although NGS discovers new high-reward molecules during search, the policy exploration is still effective. These findings suggest that NGS can be further enhanced as an alternative GA method by incorporating policy exploration.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
