%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[table]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{graphicx} 
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage{algorithmic}
\usepackage{textcomp}
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    % T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{xcolor}
% \usepackage{pdflscape} % Use for rotating the page


%%%%%%%%%%%%%%%%% Macros %%%%%%%%%%%%%%%%%%%%%%%%%%%%
% *** MACRO FOR MAKING TODO's ***
\newcommand{\preston}[1]{\textcolor{red}{Preston TO DO: #1}}
\newcommand{\done}[1]{\textcolor{green}{\ding{51}: #1}}
% \newcommand{\Note}[1]{\textcolor{green}{Note: #1}}

% % *** MACRO FOR FIGURE REFERENCING ***
\newcommand{\figref}[1]{Figure~\ref{#1}} % already implemented
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}} % already implemented
\newcommand{\defref}[1]{definition~\ref{#1}}
\newcommand{\algref}[1]{algorithm~\ref{#1}} % lowercase to match 

% *** MACRO FOR VARIABLES ***
\newcommand{\model}[0]{f}
\newcommand{\dilation}[0]{d}
\newcommand{\inputim}[0]{x}
\newcommand{\mask}[0]{m}
\newcommand{\prompt}[0]{p}
\newcommand{\segout}[0]{\hat{m}}
\newcommand{\dilateout}[0]{\hat{m}_d}
\newcommand{\refineout}[0]{\hat{m}_r}
\newcommand{\inversemask}[0]{m'}
\newcommand{\inversedilate}[0]{\hat{m}_d'}
\newcommand{\xhat}[0]{\hat{x}}
\newcommand{\restoreout}[0]{\dot{x}}
\newcommand{\SegModel}[0]{S}
\newcommand{\RefineModel}[0]{R}
\newcommand{\InpaintModel}[0]{I}
\newcommand{\RestoreModel}[0]{E}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Blind Visible Watermark Removal with Morphological Dilation}

\begin{document}

\twocolumn[
\icmltitle{Blind Visible Watermark Removal with Morphological Dilation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Preston K. Robinette}{sch}
\icmlauthor{Taylor T. Johnson}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sch}{Department of Computer Science, Vanderbilt University, Nashville TN, USA}

\icmlcorrespondingauthor{Preston K. Robinette}{preston.k.robinette@vanderbilt.edu}
\icmlcorrespondingauthor{Taylor T. Johnson}{taylor.johnson@vanderbilt.eud}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Security, Watermarking, Information Hiding}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

%% NOtes:
% -----------
%% fix the wording --> make it more clear for 50.8\%
%% Sharpen up the abstract --> 
%% x --> x, make fig. 6 clearer
%% non-watermarked, real-watermarked how does it perform
%% CVPR images
%%


\begin{abstract}
Visible watermarks pose significant challenges for image restoration techniques, especially when the target background is unknown. Toward this end, we present MorphoMod, a novel method for automated visible watermark removal that operates in a blind setting---without requiring target images. Unlike existing methods, MorphoMod effectively removes opaque and transparent watermarks while preserving semantic content, making it well-suited for real-world applications. Evaluations on benchmark datasets, including the Colored Large-scale Watermark Dataset (CLWD), LOGO-series, and the newly introduced Alpha1 datasets, demonstrate that MorphoMod achieves up to a 50.8\% improvement in watermark removal effectiveness compared to state-of-the-art methods. Ablation studies highlight the impact of prompts used for inpainting, pre-removal filling strategies, and inpainting model performance on watermark removal. Additionally, a case study on steganographic disorientation reveals broader applications for watermark removal in disrupting high-level hidden messages. MorphoMod offers a robust, adaptable solution for watermark removal and opens avenues for further advancements in image restoration and adversarial manipulation.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
Watermarks are widely employed as a digital rights management (DRM) tool to protect intellectual property in images, videos, and other multimedia content~\cite{cox2002digital,singh2013survey}. Invisible watermarks, imperceptible to the human eye, rely on sophisticated signal-processing techniques to encode information directly into the digital media. Visible watermarks, on the other hand, are prominently displayed overlays that serve as a clear visual deterrent against unauthorized distribution or misuse.

Visible watermarks are often used due to their immediate perceptibility and typically take the form of logos, text, or patterns strategically placed over an image. Despite their effectiveness, visible watermarks are not impervious to attacks. Techniques such as traditional inpainting~\cite{huang2004attacking,xu2017automatic,qin2018visible} and  Independent Component Analysis~\cite{pei2006novel} can be employed to remove or obscure these watermarks. The rise of generative models and diffusion-based inpainting methods has further amplified this vulnerability, enabling highly realistic reconstruction of the original image with minimal artifacts~\cite{cheng2018large,hertz2019blind,cun2021split,liang2021visible, liu2021wdnet, sun2023denet}. Addressing these challenges requires a deeper understanding of both watermarking techniques and the methods employed to remove them.
% ------------------ mask examples
\input{figs/mask_examples}
\input{figs/morphomod}
\input{figs/restore_ex}

As such, we  develop a novel automated pipeline stemming from three key observations: \textit{(1) Two-Stage Pipelines$\rightarrow$} the majority of deep learning methods follow a two-stage pipeline, beginning with the identification of a watermark mask followed by image restoration,
\textit{(2) Conservative Mask Predictions$\rightarrow$} the masks generated by current methods are often smaller than the true watermark (see \figref{fig:mask_ex}) leading to incomplete removal, and \textit{(3) Dependency on Known Backgrounds$\rightarrow$} all current deep-learning approaches require prior knowledge of the target background during the training process (\textit{non-blind}).

Building upon these key observations, we propose a \textit{blind} visible watermark removal framework that employs morphological dilation in conjunction with inpainting called \textit{MorphoMod}. By eliminating the need for prior knowledge of the background and focusing on robust mask generation, our method mimics real-world scenarios. The contributions of this work are the following: \textbf{(1) Development of MorphoMod$\rightarrow$} we introduce a novel, blind watermark removal method effective for real-world scenarios involving datasets without known targets, \textbf{(2) Introduction of novel datasets$\rightarrow$} we propose two new datasets, Alpha1-S and Alpha1-L, featuring non-transparent, fully opaque watermarks, designed to better reflect real-world watermarking challenges, \textbf{(3) Introduction of novel evaluation metrics$\rightarrow$} we propose new metrics specifically tailored to assess watermark removal performance in the absence of known target images, \textbf{(4) Comprehensive experimental analysis$\rightarrow$} we perform extensive ablation studies to evaluate the impact of various factors, including inpainting models, prompt design, pre-removal filling strategies on watermark removal and image quality, \textbf{(5) Case study on steganographic disorientation$\rightarrow$} we introduce a novel toy problem and dataset to showcase MorphoMod's potential in disrupting visible steganographic messages.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
\label{sec:related_works}
In this section, we present research related to inpainting and deep learning-based visible watermark removal (VWR). We also provide key observations from deep learning based VWR.

\textbf{Inpainting.} Inpainting aims to restore missing or corrupted regions in an image. Traditional methods often rely on partial differential equations (PDEs)~\cite{bertalmio2006pde,schonlieb2015partial} or patch synthesis~\cite{criminisi2003object} while deep learning approaches~\cite{pathak2016context,nazeri2019edgeconnect,cai2022image} leverage large datasets to learn complex features, often employing generative adversarial networks (GANs). Recent advancements harness denoising diffusion probabilistic models (DDPMs)~\cite{ho2020denoising} for inpainting by iteratively refining noisy versions of an image. Methods like Stable Diffusion~\cite{rombach2021highresolution} and LaMa~\cite{suvorov2021resolution} achieve high-quality results.

\textbf{Deep Learning Watermark Removal Methods.} Recent advancements in VWR have been driven by deep learning-based approaches. In~\cite{cheng2018large}, the authors introduce a seminal image-to-image translation approach driven by the object detection of watermarks which are then refined using a convolution neural network (CNN). To bypass the reliance on detection-based components, \cite{li2019towards} and \cite{cao2019generative} proposed generative adversarial network (GAN) techniques. In~\cite{hertz2019blind}, the authors introduce a multi-task learning framework utilizing a single encoder with multi-decoder architecture to reconstruct the background, motif (watermark) mask, and motif (watermark) image. \cite{cun2021split} introduce a two-stage network for multi-task decoding and then refinement called \textit{SplitNet}. \cite{liang2021visible} refine mask predictions even further using a coarse-to-fine strategy known as \textit{SLBR}. Instead of estimating only the mask of the watermark, \textit{WDNet}~\cite{liu2021wdnet} attempts to predict the mask, opacity and color of the watermark. \textit{DENet} attempts to disentangle watermark and image embeddings in the feature space, employing contrastive learning to ensure task-specific representation for watermark removal and background reconstruction~\cite{sun2023denet}. In ~\cite{leng2024removing}, the authors also implement a two-stage approach consisting of watermark localization and background content restoration, known as \textit{RIRCI}.

\textbf{Key Observations.} \textbf{[1-2]} Most deep learning methods for watermark removal use a mask prediction component to guide background restoration as shown in \figref{fig:restore_ex}. This procedure boosts image quality metrics by enabling a focus on reconstructing or refining the watermarked area. However, the accuracy of the predicted mask is crucial, as it directly impacts the final image composition. Many prior methods focus on improving the reconstructed image while neglecting mask refinement. \figref{fig:mask_ex} illustrates example predicted masks for SLBR, where under-prediction often leaves watermark residuals in the sanitized image. \textbf{[3]} Another key limitation is the reliance on access to watermark-free images during training. Training datasets typically include \textit{watermarked images}, \textit{watermark masks}, and \textit{watermark-free images}~\cite{liu2021wdnet,cun2021split,cheng2018large}. In real-world scenarios, matching the distribution of watermarked and watermark-free images is highly unlikely. For instance, when watermarked images are scraped from platforms like Shutterstock.com, the true background behind the watermark is unknown. This raises a critical challenge: how can these methods be effectively evaluated when ground truth watermark-free images are unavailable in practice?




% In the majority of the deep learning approaches, there is a mask prediction component which is then used to direct background content restoration. While not explicitly stated in these papers, SplitNet, SLBR, WDNet, and DENet all then use these masks to combine with the original image, as shown in \figref{fig:restore_ex}, where the original image $\inputim$ is combined with the inverse of the provided mask $\inversemask$ for the background and the processed output $\xhat$ is combined with the provided mask $\dilateout$ to retrieve the cleaned watermarked region. The background is then combined with the watermark region to produce the restored image. This procedure helps to boost image quality metrics and allows each respective method to focus on the reconstruction or refinement of the watermarked region. The accuracy of the predicted mask becomes crucial, as it directly influences the refinement of the watermarked area and the final image composition. Interestingly, most prior methods emphasize improving the reconstructed image while underestimating the importance of refining the mask itself. \figref{fig:mask_ex} shows example predicted masks for SLBR. The masks rarely, if ever, over-predict the mask. This means that watermark residuals are often left behind in the sanitized image, as the predicted masks fail to fully encompass the watermark regions. 

% An equally significant observation is that all these methods rely on access to watermark-free images during training. The training datasets typically consist of (1) \textit{watermarked images}, (2) \textit{watermark masks}, and (3) \textit{watermark-free images}~\cite{liu2021wdnet,cun2021split,cheng2018large}. However, in real-world scenarios, it is highly unlikely to have watermark-free images that match the distribution of watermarked images. For instance, if bad actors scrape watermarked images from platforms like Shutterstock.com, the true background behind the watermark remains unknown. This raises a critical question: how can we effectively evaluate these methods against the watermark-free version when such ground truth is unavailable in practical applications? 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MorphoMod}
Based on the key observations from \secref{sec:related_works}, our work introduces a blind watermark removal method that combines morphological dilation with diffusion-based inpainting to address the limitations of existing methods. Unlike traditional two-stage pipelines, which focus on the refinement of the image, our approach integrates mask prediction and mask refinement into a unified framework, ensuring that generated masks more closely align with the true watermark extent. By eliminating the need for background priors, our method offers a practical and scalable solution for watermark removal in real-world applications. MorphoMod consists of three main stages: 1) \textit{segment}, 2) \textit{inpaint}, and 3) \textit{restore}, as shown in \figref{fig:morphomod}. We discuss each stage in more detail below.

\subsection{Segment}
\textbf{Initial Segmentation.} The segmentation stage consists of two components: (1) initial segmentation and (2) refinement. For the initial segmentation, we use the best-performing semantic segmentation model from SLBR, SplitNet, WDNet, DENet, or RIRCI (which relies on SLBR for mask prediction) for each dataset. Let $\SegModel$~be the segmentation model, $\inputim \in \mathcal{R}^{3xHxW}$ the watermarked image, and $\segout \in \mathcal{R}^{1xHxW}$ the generated segmentation mask. The initial segmentation produces an initial mask $\segout$ s.t. $\SegModel(\inputim) = \segout$.

\textbf{Refinement.} The refinement step uses a U-Net model and morphological dilation to improve the initial mask by addressing underprediction. The U-Net Refine Model $\RefineModel$ refines the mask, taking $\inputim$ and $\segout$ as input and producing $\refineout$ such that $\RefineModel(\inputim, \segout) = \refineout$. The model is trained with the following loss:%
%
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{Dice}}(\refineout, \mask ) + \mathcal{L}_{\text{BCE}}(\refineout, \mask)
\end{equation}%
%
Here, \(\mathcal{L}_{\text{Dice}}\) measures overlap between $\refineout$ and the ground truth mask $\mask$, while \(\mathcal{L}_{\text{BCE}}\) evaluates pixel-wise differences, promoting both global and local accuracy. Morphological dilation is then applied to expand mask boundaries, ensuring all residual watermark artifacts are captured. Dilation is defined as:
\begin{equation}
\dilateout = \max_{s \in S} \big(\refineout(s) \oplus \mathbf{K}(s)\big)
\end{equation}
where \(\mathbf{K}(s)\) is a structuring element (e.g., disk or square) and \(\oplus\) the dilation operation. Larger kernels (e.g., \(7 \times 7\)) expand boundaries significantly, while smaller kernels (e.g., \(3 \times 3\)) provide finer adjustments. The kernel size is set by a dilation parameter $\dilation$.

\textbf{Overview.} The segment phase takes as input a watermarked image $\inputim$ and a dilation parameter $\dilation$ to produced a refined mask $\dilateout$ for inpainting, i.e., $\mathrm{Segment}(\inputim, \dilation) = \dilateout$.  
\input{figs/dilate}

\subsection{Inpaint}
The refined mask $\dilateout$ is then combined with the watermarked image $\inputim$ and a prompt $\prompt$ for the inpaint phase. For the inpaint model, we make use of publicly available denoising diffusion probabilistic models (DDPM) like Stable Diffusion and LaMa. The output of the inpaint phase is a cleaned image $\xhat$, i.e., $\mathrm{Inpaint}(\inputim, \dilateout, \prompt) = \xhat$, where $\xhat \in \mathcal{R}^{3xHxW}$.


\subsection{Restore}
In the restore phase, the background from the watermarked image is then combined with the watermark region of the cleaned image. An example of this is shown in \figref{fig:restore_ex}. Here, the background region is created by multiplying the original image $\inputim$ with the inverse of the provided mask $\inversedilate$, s.t. $\inversedilate = 1 - \dilateout$. The cleaned watermark region is created by multiplying the processed output $\xhat$  with the refined mask $\dilateout$. The background is then combined with the watermark region to produce the restored image $\restoreout$, i.e., $\mathrm{Restore}(\inputim, \dilateout, \xhat) = \restoreout$.
% \begin{equation}
%     \restoreout = (\inputim \times \inversedilate) + (\xhat \times \dilateout)
% \end{equation}



\subsection{Blind Metrics} 
\label{sec:metrics}
In this work, we introduce novel metrics to evaluate watermark removal under real-world conditions where true watermark-free targets are unavailable. These metrics estimate both the effectiveness of watermark removal (WR) and the semantic preservation (SP) of the original image. We base these blind metrics on common VWR metrics: root mean squared error ($\mathrm{RMSE}$), structural similarity index measurement ($\mathrm{SSIM}$), and Learned Perceptual Image Patch Similarity ($\mathrm{LPIPS}$)~\cite{cheng2018large,hertz2019blind,cun2021split,liang2021visible, liu2021wdnet, sun2023denet}. \textbf{Watermark Removal.} Watermark removal is assessed by comparing the watermarked region in the original image to the corresponding region in the output image, using the ground truth mask. Metrics include $\mathrm{RMSE}_W$, $\mathrm{SSIM}_W$, and $\mathrm{LPIPS}_W$, where high $\mathrm{RMSE}_W$, low $\mathrm{SSIM}_W$, and high $\mathrm{LPIPS}_W$ indicate successful watermark removal. \textbf{Semantic Preservation.} Semantic preservation evaluates the background by comparing the background region of the watermarked image with that of the output image. The background region is determined by: $\text{background} = \inputim * (1-\mask)$, where $\inputim$~is the watermarked image and $\mask$~is the ground truth mask. Metrics $\mathrm{RMSE}_T$, $\mathrm{SSIM}_T$, and $\mathrm{LPIPS}_T$ are used, with low $\mathrm{RMSE}_T$, high $\mathrm{SSIM}_T$, and low $\mathrm{LPIPS}_T$ indicating effective background preservation.

\textbf{Tradeoff.} Balancing watermark removal and semantic preservation is critical, as optimizing one often compromises the other. A smaller mask may preserve semantics but perform poorly in watermark removal, while a larger mask improves removal at the cost of background integrity. In this work, we prioritize watermark removal.

\input{tabs/alpha_w}
\input{figs/alpha_images}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
In this section, we introduce the datasets, implementation details, and results of two experiments used to evaluate MorphoMod. 

\subsection{Datasets and Implementation Details}
\textbf{Experiment 1.} \textit{Baseline performance and effect of the dilation parameter $\dilation$.} To determine the baseline performance of MorphoMod, we evaluate on four common watermarking datasets: 1) Colored Large-scale Watermark Dataset (CLWD)~\cite{liu2021wdnet}, 2) LOGO-Gray, 3) LOGO-L, and 4) LOGO-H~\cite{cun2021split}. Please see the Appendix for more details on these datasets.  Using these datasets, we then evaluate MorphoMod with dilation values $d \in \{0, 1, 3, 5, 10\}$ and prompt $p = \text{``Remove.''}$ and record watermark removal and semantic preservation metrics as described in \secref{sec:metrics}. 

\textbf{Experiment 2.} \textit{Comparison against existing methods on two new datasets: Alpha1-S and Alpha1-L.} Previous methods rely on datasets with transparent watermarks and known target images, which do not reflect all real-world scenarios where watermarks could be opaque and targets are unknown. To address this, we introduce Alpha1-S and Alpha1-L, featuring non-transparent watermarks with no assumed targets.

\textbf{Alpha1 Datasets:} \textbf{Alpha1-S}mall and \textbf{Alpha1-L}arge contain 12K training and 2K testing samples from ImageNet~\cite{deng2009imagenet} with opaque watermarks from the CLWD dataset. Each sample consists of a watermarked image and a corresponding mask. The watermarks cover 6\% of the image area in Alpha1-S and 35\% in Alpha1-L.

Using these datasets, we evaluate MorphoMod ($\dilation=3$, $p=\text{``Remove.''}$) against SLBR~\cite{liang2021visible}, SplitNet~\cite{cun2021split}, WDNet~\cite{liu2021wdnet}, and DENet variants trained on LOGO datasets~\cite{sun2023denet}. These models, which rely on target images during training, were pretrained on CLWD or LOGO datasets and evaluated on Alpha1-S and Alpha1-L using watermark removal and semantic preservation metrics (\secref{sec:metrics}). This setup mimics transfer learning by assessing how well pretrained models perform on out-of-distribution, web-scraped data.%
%
\input{tabs/eval_combine_small}%
%
\subsection{Results}
\textbf{Experiment 1.} Figure~\ref{fig:dilate} presents the results of Experiment~1, where the solid lines represent the watermark removal metrics and the dashed lines represent the semantic preservation metrics. For each dataset and dilation value, the high $\mathrm{RMSE}_W$, low $\mathrm{SSIM}_W$, and high $\mathrm{LPIPS}_W$ values indicate effective watermark removal. As the dilation value increases, more of the actual watermark is targeted, leading to stronger removal performance.

However, this improvement also results in a greater loss of the original image content, as shown by the trends in the semantic preservation metrics. This underscores the inherent trade-off between thorough watermark removal and preserving the image’s semantic integrity. Notably, MorphoMod removes the watermark across all dilation values tested without excessively compromising semantic quality, confirming its effectiveness for visible watermark removal.


\textbf{Experiment 2.} The results for Experiment~2 are shown in \tabref{tab:alpha_expr}. In terms of watermark removal, MorphoMod achieves the strongest performance across all metrics, as indicated by the high \(\mathrm{RMSE}_W\), low \(\mathrm{SSIM}_W\), and high \(\mathrm{LPIPS}_W\). For the Alpha1-S and Alpha1-T datasets, the best \textit{target-trained} models are DENet-G and SLBR, respectively. This is highlighted by the image results in \figref{fig:alpha_images}. In contrast, for semantic preservation, SplitNet and DENet-L deliver the highest performance, as shown by their low \(\mathrm{RMSE}_W\), high \(\mathrm{SSIM}_W\), and low \(\mathrm{LPIPS}_W\). This is expected given that better retention of the original background boosts semantic preservation metrics. If the mask is smaller than the true watermark area, more of the original background remains in the restored image, thus raising the semantic metrics but leaving behind noticeable watermark artifacts.

\input{tabs/eval_sam}%
\input{tabs/prompts_small}%

\textbf{Takeaways.} \textit{Overall, MorphoMod proves highly effective for watermark removal in real-world settings, outperforming previous approaches. The choice of dilation value is important to achieve the best balance between watermark removal and overall image quality.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Ablation Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation Experiments}
In this section, we provide additional experiments related to mask generation (MG) and the inpainting process (IP). Unless otherwise stated, all \textit{IP} experiments utilize the ground truth mask for the generative process. 


% -----------------------------------
%    Prompts on in-painting
\subsection{[MG]: Segment Anything Model (SAM)}%
\input{figs/eval_prompt_small}%
\input{figs/eval_models}%

A possible approach for generating watermark masks for inpainting is to use state-of-the-art segmentation models like the Segment Anything Model (SAM)~\cite{ravi2024sam2}. SAM is a prompt-driven segmentation framework trained to identify object regions using various inputs. We evaluate SAM's ability to generate watermark masks using a standalone image (\textit{Auto Mask}), an image with a ground truth bounding box (\textit{GT BBox}), an image with ground truth points (\textit{GT Points}), and an image with a generated bounding box (\textit{GDINO BBox}). The bounding box for \textit{GDINO BBox} is generated using Grounding DINO~\cite{liu2023grounding}, which takes an image and prompt to produce a bounding box. We evaluate SAM-generated masks on the Alpha1-S and CLWD datasets using F1 and IoU metrics, comparing against an SLBR baseline. All experiments are conducted with the SAM2.1 Hiera-Small model.

As shown in \tabref{tab:eval_sam}, SAM with GT BBox achieves the highest segmentation performance for Alpha1-S, but falls short of SLBR for CLWD, even with ground truth inputs (\textit{GT BBox} or \textit{GT Points}). While \textit{GT BBox} achieves the best result for Alpha1-S, this performance requires ideal conditions (ground truth vs. generated). Thus, SAM in its current state is not well-suited for watermark object detection, which the authors view positively as it limits misuse of the model. Please see the Appendix for image results.

\subsection{[IP]: Effect of Prompts in Inpainting}
\label{sec:prompts}
To evaluate how prompt choice impacts inpainting-based watermark removal, we conduct experiments using ten prompts varying in length, specificity, and style, as detailed in \tabref{tab:prompts_small}. Each prompt requests watermark removal, and we assess the output quality based on watermark removal (\(\mathrm{RMSE}_W\), \(\mathrm{SSIM}_W\), \(\mathrm{LPIPS}_W\)) and reference-free image quality metrics (BRISQUE, NIQE, PIQE) on the Alpha1-S and CLWD datasets.

Results in \tabref{tab:comb_expr} show effective watermark removal across all prompts, with high \(\mathrm{RMSE}_W\), low \(\mathrm{SSIM}_W\), and high \(\mathrm{LPIPS}_W\), alongside consistent image quality metrics. As illustrated in \figref{fig:eval_prompts_small}, the watermark is seamlessly removed in most cases. However, in some instances, inpainting introduces unintended artifacts (e.g., added text in prompts P1 and P3). Despite this, the original watermark is successfully removed, demonstrating all prompts achieve the intended goal.


\subsection{[IP]: Pre-Removal Watermark Filling}
\label{sec:prefill}
We explore the effect of pre-removal filling strategies on inpainting by testing five approaches: (1) no fill, (2) white fill, (3) black fill, (4) gray fill, and (5) average background fill, where the average color of background pixels is used. These strategies aim to assess how initial fill conditions impact inpainting results, with high-contrast fills (e.g., no fill, white, black) potentially causing abrupt transitions, while gray and average background fills may create smoother results. Watermark removal and image quality metrics (BRISQUE, NIQE, PIQE) are collected on Alpha1-S and CLWD datasets.

The results of this experiment are shown in the middle section of \tabref{tab:comb_expr}. Each fill strategy is able to successfully remove the watermark as shown by the high \(\mathrm{RMSE}_W\), low \(\mathrm{SSIM}_W\), and high \(\mathrm{LPIPS}_W\) across each dataset. For the image quality metrics, all prompts perform evenly as well. As such, the fill strategy prior to inpainting is negligible. For image results of this experiment, please see the Appendix.

% -----------------------------------
%    In-painting Model
\subsection{[IP]: Impact of Inpainting Model}
To evaluate the performance of different inpainting models for watermark removal, we compare three state-of-the-art open-source models: Stable Diffusion 2 (SD2), Stable Diffusion XL (SDXL), and LaMa. These models are tested on the CLWD and Alpha1-S datasets, with evaluations based on watermark removal metrics (\(\mathrm{RMSE}_W\), \(\mathrm{SSIM}_W\), \(\mathrm{LPIPS}_W\)) and image quality metrics (BRISQUE, PIQE, NIQE).

Results in \tabref{tab:comb_expr} show that all models successfully remove watermarks, as evidenced by high \(\mathrm{RMSE}_W\), low \(\mathrm{SSIM}_W\), and high \(\mathrm{LPIPS}_W\) across datasets. Image quality metrics indicate comparable performance among the models.

Visual examples in \figref{fig:eval_models_img} highlight these results. SDXL and SD2 demonstrate the best overall visual performance, but some residual traces of watermarks remain in certain images, as shown in the close-up examples. Inpainting also introduces variations during restoration, producing realistic outputs even without a target image for comparison. For example, in (a), SDXL adds subtle color enhancements, while SD2 seamlessly blends with the fur. These variations create plausible and diverse outcomes, which can be particularly valuable in real-world scenarios where exact image details are unknown.

% -----------------------------------
%    Steganographic Disorientation
\section{Case Study: Steganographic Disorientation}
\input{figs/eval_steg_disorient}
We introduce a novel toy problem, \textit{steganographic disorientation}, to advance methods for eliminating residuals in image object removal. This problem explores disrupting information hiding by altering high-level features.

Traditional steganography hides information invisibly within low-level image features, but recent sanitization methods using diffusion models can effectively erase such messages (regeneration attacks)~\cite{robinette2023suds,robinette2024dmsuds,zhao2023invisible,Nie2022DiffusionMF}. However, when information is encoded in high-level features, like the position of objects, it persists through regeneration.

In our scheme, a message is conveyed via the position of a box in one of four predefined locations (north, east, south, or west) within an image. The goal of the disorientation agent is to alter the box's position, disrupting the intended communication (e.g., changing a meeting time). To support this, we introduce the \textit{Disorient} dataset, consisting of 4,000 training images and 1,000 test images, with orange boxes placed in four positions. A classifier and a U-Net model are trained to identify box positions and masks. The disorientation MorphoMod agent follows three steps: (1) predict the box location, (2) MorhpoMod removal of the box, and (3) add a new box in a different position from the originally predicted location. Performance is evaluated using accuracy, BRISQUE, NIQE, and PIQE metrics.
\input{tabs/eval_steg_disorient}


The results of this experiment are shown in \tabref{tab:steg_disorient}. From these results our inpainting disorientation model is successful at disorienting the communication as highlighted by the 0.0\% accuracy. The image results for this experiment are shown in \figref{fig:steg_eval}. In each example, the original time is successfully shifted to a new time. 

While we present this toy problem to the field, we would like to highlight challenges for future consideration. How to ensure that the disorientation goes unnoticed?  If residual artifacts remain in the image, the recipient can detect the tampering and infer that the message has been altered.

\section{Discussion and Conclusion}
We introduce MorphoMod, an automated method for visible watermark removal that operates without requiring target images, making it highly effective in real-world scenarios. Extensive experiments on traditional and newly proposed Alpha1 datasets demonstrate MorphoMod’s high performance in watermark removal while maintaining semantic quality. Our results highlight its adaptability across varying configurations, with ablation studies showcasing the impact of prompts, inpainting strategies, and pre-removal filling methods. Additionally, a novel case study on steganographic disorientation demonstrates MorphoMod’s potential for broader applications, such as disrupting hidden information in images. In the future, we would like to explore additional methods for generative watermark localization. \textit{MorphoMod sets a new benchmark for blind watermark removal, paving the way for future advancements in watermarking and image restoration research.}


\section{Impact Statement}
MorphoMod introduces a novel approach to blind visible watermark removal, eliminating the need for target images. By leveraging morphological dilation and generative inpainting, MorphoMod effectively removes opaque and transparent watermarks while preserving image integrity. Our work significantly enhances real-world applicability, as demonstrated by its high performance on traditional and newly proposed benchmark datasets.

As such, we do not take the implications of this work lightly. \textbf{We believe this work provides the necessary empirical evidence needed to assess current generative watermark removal performance and to urge the development of more resilient digital watermarking techniques that are suited for the current generative landscape.}



% \section{Recommendations}
% Do not place watermarks arbitrarily. Put them on the most information rich part of the image. 

% The larger the watermark the better. Grids or Patterns across an entire image would be way more effective than single location watermarks. 

% At Boundaries --> Funky stuff is brought up. 

% What are we actually watermarking --> how close to the original is the original that should be watermarked if you catch my drift. 

% When placed in arbtrary locations, in-painting is an effective method to remove watermarks and likely the most useful to potential bad actors. 

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\subsection{Dataset Details}
\begin{enumerate}
    \item \textbf{CLWD:} CLWD consists of 60K watermarked images for training and 10K for testing. The watermarks are gathered from publicly available logo websites and applied to images randomly selected from PASCAL VOC2012~\cite{everingham2015pascal}. The transparency ranges from 0.3 to 0.7, and its size, position, orientation, and transparency are all randomly selected across individual images.\\
    \item \textbf{LOGO-Gray:} LOGO-Gray contains 12K training and 2K testing samples with gray-scale watermarks. The size and transparency of the watermark are between 35\% and 85\%.  \\
    \item \textbf{LOGO-L:} LOGO-L contains 12K training and 2K testing samples. The transparency of the watermarks ranges from 35\% to 60\%, and the size is anywhere from 35\% to 60\% of the width of the host images. \\
    \item \textbf{LOGO-H:} LOGO-H is a harder subset of LOGO-L and contains 12K training and 2K testing samples. The transparency and size are randomly chosen from 60\% to 85\%.
\end{enumerate}

\subsection{Additional Experiments and Results}
\subsubsection{[IP]: Effect of Prompts in Inpainting Cont.}
This section provides the full table of prompts and results for \secref{sec:prompts}.

\input{tabs/prompts}
\input{tabs/eval_prompts}
\input{figs/eval_prompts}

\subsubsection{[IP]: Effect of Denoising Steps}
In the inpainting process, the number of denoising steps is associated with the iterative refinement of the image construction. To investigate the impact of different step counts, we conduct experiments using $s \in \{10, 20, 30, 40, 50, 60, 70, 80, 90, 100\}$ denoising steps. For each configuration, we evaluate using the Alpha1-S and CLWD datasets and collect watermark removal and BRISQUE, PIQE, and NIQE image quality metrics. From the results shown in \figref{fig:eval_steps}, the number of diffusion steps has no impact on watermark removal performance. 
\input{figs/eval_steps}

\input{figs/dilation_ex}

\input{figs/eval_sam}
\input{figs/fill_examples}
\input{figs/eval_fill}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
