% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{multirow}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xcolor}
\usepackage{url}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{What Are They Filtering Out? A Survey of Filtering Strategies for Harm Reduction in Pretraining Datasets}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Marco Antonio Stranisci \\
  Universit√† degli Studi di Torino\\
  aequa-tech \\
  \texttt{marcoantonio.stranisci@unito.it} \\\And
  Christian Hardmeier \\
  IT University of Copenhagen \\
  \texttt{chrha@itu.dk } \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Data filtering strategies are a crucial component to develop safe Large Language Models (LLM), since they support the removal of harmful contents from pretraining datasets. There is a lack of research on the actual impact of these strategies on vulnerable groups to discrimination, though, and their effectiveness has not been yet systematically addressed. In this paper we present a benchmark study of data filtering strategies for harm reduction aimed at providing a systematic overview on these approaches. We survey $55$ technical reports of English LMs and LLMs to identify the existing filtering strategies in literature and implement an experimental setting to test their impact against vulnerable groups. Our results show that the positive impact that strategies have in reducing harmful contents from documents has the side effect of increasing the underrepresentation of vulnerable groups to discrimination in datasets. 
\\
WARNING: the paper could contain racist, sexist, violent, and generally offensive contents
\end{abstract}

\section{Introduction}

The harmfulness of Large Language Models (LLM) is an open issue that gathers the attention of different sectors of our society. International bodies regulated the development of these technologies \cite{edwards2021eu}; Natural Language Processing (NLP) scholars are introducing a series of approaches \cite{touvron2023llama} to assess and mitigate their impact against vulnerable groups to discrimination. 

Even if the development cycle of a LLM encompasses several steps, in recent years most of the research focuses on the post-training stage, for which several benchmarks \cite{gehman2020realtoxicityprompts,tedeschi2024alert} have been created. Theoretical research on effective strategies to filter out harmful contents from pretraining datasets is an understudied topic, though. If compared to the amount of LLMs released in recent years, only a limited number of approaches to filtering strategies has been proposed \cite{raffel2020exploring,brown2020language}, and many o them have been implemented without considering the complex nature of bias, producing unwanted negative effects against several categories of people \cite{dodge2021documenting,xu-etal-2023-knowcomp}. Few critical studies on filtering strategies \cite{luccioni2021s,longpre-etal-2024-pretrainers} have been performed so far, but none of them systematically addresses the topic. 

The aim of our research is to propose the first systematic analysis of data filtering strategies for harm reduction in pretraining dataset. Specifically, we formulate two research questions.

\textbf{RQ 1: Which filtering strategies are implemented to remove harmful contents from pretraining datasets?} We surveyed $55$ technical reports describing English LMs and LLMs to collect information about the characteristics of the existing data filtering strategies and their documentation. Through the survey we have been able to identify eight different categories of filtering strategies for harm reduction that have been proposed in literature. The survey also shows a disengagement trend in current LLM technical reports, from which emerges a general lack of awareness on their role in increasing the underrepresentation of vulnerable groups to discrimination in datasets.

\textbf{RQ 2: Which categories of people are most affected by filtering strategies?} We performed a benchmark analysis on seven data filtering strategies to evaluate if and to which extent they increase the underrepresentation of vulnerable groups to discrimination in pretraining datasets. To perform this analysis, we designed a pipeline for identifying the mentions of named entities categorized by their gender and origin. Results of our analysis show that women are the most impacted by filtering strategies and that strategies significantly differ in the contents they filter out. Choosing a specific strategies means focusing on specific sources of harm overlooking others. 

The contribution of our work is threefold: \textit{i.} we provide the first systematic survey of data filtering strategies for harm reduction; \textit{ii.} we implement a pipeline and a set of resources to benchmark filtering strategies; \textit{iii.} we test our pipeline on a pool of data filtering strategies, empirically demonstrating the negative impact of these approaches in the underrepresentation of vulnerable groups to discrimination. 

\section{Related work}
Research on data filtering strategies for harm reduction has been propelled by the research on biases in LLMs \cite{bender2021dangers} and in pretraining datasets \cite{jo2020lessons}. However, critical studies on strategies focused on specific datasets rather than the impact of strategies themselves. \citet{luccioni2021s} analyzed the presence of Hate Speech in the Common Crawl dataset. \citet{dodge2021documenting} documented the C4 corpus \cite{raffel2020exploring}, showing that its filtering correlates with a reduction of terms defining vulnerable groups to discrimination in datasets. \citet{longpre-etal-2024-pretrainers} tested the correlation between removing toxic contents from pretraining datasets and LLMs performance in toxicity classification, showing that filtering has a negative impact on this task. The survey of \citet{albalak2024survey} on data selection for LLMs provide a marginal and non-systematic analysis of strategies. Finally, the work of \citet{lucy-etal-2024-aboutme} on language filtering strategies provide insightful results on the cultural biases embedded in language filters that can determine the exclusion of certain categories of people from pretraining datasets. 

\section{Surveying Data Filtering Strategies} \label{sec:survey}
In this section we present our systematic survey of data filtering strategies for harm reduction in pretraining data as they are presented in $55$ LMs technical reports. We first introduce our methodology for the selection of relevant documents and their analysis. Then, we present a taxonomy of eight existing data filtering strategies that emerges from the analysis. Finally, we identify some trends emerging from a general overview of reports in a diachronic perspective. 

\subsection{Survey Methodology} \label{ss:survey_methodology}
To ensure a representative pool of technical reports describing LLMs, we seeded them from six leaderboards that have been chosen to include in our study different generations of LMs as well as different tasks. To ensure that small LMs were included in our survey we gathered all the technical reports of systems ranked higher than the baseline of SuperGLUE \cite{wang2019superglue} and the first $50$ highest-ranked in SQuAD \cite{rajpurkar2016squad}. LLMs technical reports were collected from MMLU-pro \cite{wang2024mmlu}, from which we gathered all systems that have an aggregated score that is equal or higher than $0.5$, and the first 50 high-ranked from Chatbot Arena \cite{chiang2024chatbot}. Finally, we included all models that have been benchmarked in two existing leaderborards focused on LMs safety: ALERT \cite{tedeschi2024alert} and Secure Learning Lab's LLM safety leaderboard~\footnote{\url{https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard}}. In this first step we collected $47$ technical reports.

\begin{table*}
\centering
\resizebox{0.60\textwidth}{!}{
        \begin{tabular}{c|p{10cm}}
    \textbf{strategy} & \textbf{models} \\
    \hline
        authoritative source & {\color{red}BERT} \cite{kenton2019bert}, {\color{red}RoBERTa} \cite{liu2019roberta}, {\color{red}XLNet} \cite{yang2019xlnet}, {\color{orange}DeBERTa} \cite{he2021deberta}, {\color{red}ERNIE 3.0} \cite{sun2021ernie}, {\color{green}LinkBERT} \cite{yasunaga-etal-2022-linkbert}, {\color{orange}Vega v2} \cite{zhong2022toward}\\
        \hline
        document seeding & {\color{orange}GPT-3} \cite{brown2020language}, {\color{green}DSIR} \cite{xie-agrawal-2023-emotion}, {\color{red}Mamba} \cite{gu2023mamba}, {\color{red}MPT-7B} \cite{mosaic2023}, {\color{red}MAmmoTH2} \cite{mickus-etal-2024-mammoth}\\
        \hline
        quality-based & {\color{orange}GPT-3}\cite{brown2020language}, {\color{red}Gopher} \cite{rae2021scaling}, {\color{red}GLaM} \cite{du2022glam},  {\color{red}Mamba} \cite{gu2023mamba}, {\color{red}MAmmoTH2} \cite{mickus-etal-2024-mammoth}\\
        \hline
        toxicity classifier & {\color{red}Qwen} \cite{bai2023qwen}, {\color{red}Gemma} \cite{team2024gemma}, {\color{green}OLMO-Dolma} \cite{soldaini-etal-2024-dolma}, {\color{red}Yi-Lighting} \cite{wake2024yi}\\
        \hline
        rule-based & {\color{green}T5-C4} \cite{raffel2020exploring}, {\color{orange}FALCON} \cite{penedo2023refinedweb}, {\color{red}Gemma} \cite{team2024gemma} \\
        \hline
        url blacklists &  {\color{orange}FALCON} \cite{penedo2023refinedweb},\\
        \hline
        human-in-the-loop & {\color{red}LaMDA} \cite{thoppilan2022lamda}, {\color{green}Bloom-ROOTS} \cite{laurenccon2022bigscience}, {\color{green}Phi-3.5} \cite{abdin2024phi}\\
        \hline
        safety policy &  {\color{red}Gemini} \cite{team2023gemini}, {\color{red}EXAONE 3.5} \cite{research2024exaone}, {\color{green}INCITE-RedPajama} \cite{weber2024redpajama}, {\color{red}LLama-3} \cite{dubey-2024-evaluating} \\
        
        \hline

        not mentioned & {\color{red}Alpaca} \cite{alpaca}, {\color{red}Claude} \cite{claude23}, {\color{red}GPT-4} \cite{achiam2023gpt}, {\color{red}Zephyr} \cite{tunstall2023zephyr}, {\color{red}DeepSeek-V3} \cite{deepseekai2024deepseekv3technicalreport}, {\color{red}Grok-2} \cite{grok}, {\color{red}Jamda} \cite{lieber2024jamba}, {\color{red}Mixtral} \cite{jiang-etal-2024-jn}, {\color{red}Nova} \cite{nova}, {\color{red}Reka} \cite{team2024reka}
         
    \end{tabular}}
    \caption{A taxonomy of data filtering strategies described in technical reports describing LLMs. Model names are highlighted with different colors depending on the availability of filtering strategies and/or dataset: {\color{green}green} if they are fully available, {\color{orange}orange} if they are partially available, {\color{red}red} if they are not available.}
    \label{tab:taxonomy_data_filtering}
\end{table*}

For each report we checked three types of content: the description of the pretraining dataset, the presentation of data filtering strategies, and the treatment of harmful or biased contents in datasets\footnote{In the context of this research, we adopt the term harmful for all the phenomena that might have a negative impact against vulnerable groups to discrimination. In this sense we refer to survey of \citet{blodgett2020language}, who distinguish between \textit{representational} harms, which consist in all the negative representation of groups, and \textit{allocative} harms, which includes all the forms of underrepresentation}. For each paper we search the following keywords: \textit{data, filter, quality, toxic, bias, hate, stereotype}. This stage led to the retrieval of $13$ additional papers referenced in the sections that cover the description of filtering strategies. Among them $4$, describe the creation of a pretraining dataset and $1$ a methodology for data sampling. At the end of this process we obtained a pool of $55$ papers for our survey with information about the type of data filtering strategies that have been implemented to remove harmful contents and their availability. 
%We annotated each paper with the following information: \textit{i.} description of the data filtering strategy; \textit{ii.} usage of existing corpora; \textit{iii.} public availability of implemented strategies and used dataset; \textit{iv.} presence of safety measures at different stages of the language modelling pipeline; \textit{v.} the excerpt from the paper that presents the data filtering strategy. The complete list of surveyed papers is shown in Appendix \textit{n}.

\subsection{A Taxonomy of Data Filtering Strategies} \label{ss:category}
We defined a taxonomy to categorize filtering strategies in Table \ref{tab:taxonomy_data_filtering}. Given the presence of different LMs belonging to the same family (e.g., LLama, ERNIE), we only listed in the table the latest release of this set of model, unless there are significant changes in data filtering strategies. A change in data filtering is the reason why GPT-4 \cite{achiam2023gpt} is treated separately from GPT-3 \cite{brown2020language} while the previous two instantiations of the model are not mentioned. Whenever the technical report of an LM referred to a dataset created in the context of developing the LLM, such as Olmo \cite{groeneveld-etal-2024-olmo} with the Dolma corpus \cite{soldaini-etal-2024-dolma}, we jointly mentioned them in the table.

\paragraph{Authoritative sources.} This strategy is based on the selection of documents only from authoritative resources validated from the research community. Even if this strategy relies on selection rather than filtering, we mentioned it since it represents a standard approach before the wave of research on the ethical issues related to language modeling \cite{bender2021dangers}. An example of this approach is in BERT \cite{kenton2019bert} that has been trained on a snapshot of Wikipedia and on the Book Corpus \cite{7410368} without implementing any type of quality checks and filters.
\paragraph{Document seeding.} This strategy is based on heuristics for the selection of specific documents from the web. One of the most famous examples of this approach is the one adopted for the creation of the OpenWebText corpus \cite{Gokaslan2019OpenWeb}, which is a collection of all the outbound links from Reddit that have been upvoted at least 3 times. 
\paragraph{Quality-based.} This strategy filters out low-quality documents by comparing them with high-quality documents selected from authoritative sources. For instance, \citet{brown2020language} created a corpus of documents sampled from Wikipedia, OpenWebtext, and the Book Corpus to train a classifier for the removal of documents that are classified as too dissimilar from it. 

\paragraph{Toxicity classifier.} This strategy provides the training of a classifier to filter out all the potentially harmful contents from datasets. The first example of such an approach in our survey is from \citet{gao2020pile} who used profanity-check\footnote{\url{https://pypi.org/project/profanity-check/}} to remove toxic contents from The Pile dataset. Other commonly adopted classifiers are Perspective APIs\footnote{\url{https://perspectiveapi.com/}}, and FastText models trained on corpora for Hate Speech (HS) detection \cite{soldaini-etal-2024-dolma}.
\paragraph{Rule-based.} This strategy exploits lexicons or heuristics to remove unwanted contents. The most studied strategy of this type adopts the Shutterstack Lexicon\footnote{\url{https://bit.ly/3QjwMvz}} \cite{raffel2020exploring} to flag as toxic any sentence containing one of the word in that list. 
\paragraph{Url blacklist.} This strategy removes contents from blacklisted websites. The strategy has been adopted for the creation of the RefinedWeb Dataset \cite{su-etal-2024-refine}, which relied on the semi-automatic creation of a blacklist of urls.
\paragraph{Human-in-the-loop.} This strategy provides the involvement of human evaluators during the creation of datasets. \citet{laurenccon2022bigscience} organized hackatons with NLP communities to create a whitelist of domains to be used to crawl data for pretraining. \citet{thoppilan2022lamda} collected human prompts from crowd-workers aimed at collecting a corpus of Question Answering pairs annotated for harm detection. 
\paragraph{Safety policy.} These strategy relies on the self-assessment of the research team who develops the LM. For instance, the team that developed Gemini declare that they ``perform safety filtering to remove harmful content based on our policies'' \cite{team2023gemini}. \citet{dubey2024llama} detect toxic contents without removing them and use such an information to implement their safety policy in a further step of their language modeling pipeline. 

\subsection{Trends and Limitations in Data Filtering} \label{ss:trends}
In this section we discuss some general trends and limitations that emerge from our survey. 

A first consideration is about the \textbf{the lack of replicability} of filtering strategies presented in technical reports. This trend is shown in Table \ref{tab:taxonomy_data_filtering} where models are highlighted in red if they did not disclose their data filtering pipeline or the snapshot of datasets used for training, in orange if they partially did it, in green if they fully disclose filtering strategies and data. As can be observed, most of the existing LMs are not released with the actual scripts for the replication of implemented filtering methods. Specifically, we recognize a first generation of LMs (e.g.: BERT, RoBERTa, etc.) that have been trained without considering the issue of harmful contents in pretraining datasets. The pivotal work of \citet{bender2021dangers} led to a second generation of models that implemented specific strategies and open-sourced their approaches and results for further investigation. This is the case of the C4 corpus \cite{raffel-etal-2024-simultaneous}, which has been released both in a filtered and unfiltered version. Besides notable exceptions \cite{soldaini-etal-2024-dolma,weber2024redpajama}, the current tendency is not to disclose scripts and results of the data filtering strategies implemented for pretraining. 

A similar trend towards the reduced openness of filtering strategies is observable about the \textbf{documentation debt} of implemented filtering strategies. If the introduction of documentation templates like Datasheets \cite{gebru2021datasheets} and Model Cards \cite{mitchell2019model} led to a first wave of fully documented self-assessment reports \cite{chowdhery2023palm}, the more recent technical reports formally rely on these documentation templates but substantially provide very generic descriptions about their data filtering approach. For instance, the description provided in the Gemma technical report is limited to the following paragraph: ``We filter the pre-training dataset to reduce the risk of unwanted or unsafe utterances [...]. This includes both heuristics and model-based classifiers to remove harmful or low-quality content'' \cite{team2024gemma}. 

A final consideration is about the \textbf{lack of strategies that are focused on reducing the underrepresentation of groups vulnerable to discrimination} in pretraining datasets. Although it has been shown that filtering strategies for harm reduction have a negative impact on minorities \cite{dodge2021documenting}, few attempts to mitigate this issue have been made so far. A notable exception is the work of \citet{soldaini-etal-2024-dolma}, which provides an assessment of their toxicity classifier over different types of English dialects. However, research works that systematically study the correlation between filtering strategies and increase of minorities underrepresentation have not yet been provided. In Section \ref{sec:experiment} we present a first experiment focused on this issue.

\section{Measuring the Impact of Filtering Strategies against Vulnerable Groups} \label{sec:experiment}
In this section we benchmark seven filtering strategies on their impact in reducing or increasing the representativeness of vulnerable groups in pretraining datasets. Our experimental setup adopts an intersectional approach \cite{crenshaw2013mapping} as it considers four groups derived from the intersection of people's gender and origin: Western men, Western women, Post-colonial men, Post-colonial women. For this analysis we measure the number of named entities for each demographic group that are removed by the implementation of different filtering strategies, using samples of documents gathered from Common Crawl as a benchmark. In Section \ref{ss:methodology} we describe our experimental setup, in Section \ref{ss:results} we discuss the results of our the analysis

\subsection{Experimental Setting} \label{ss:methodology}
\begin{table*}
    \centering
    \begin{tabular}{c|ccccc}
        &\textbf{strategy} & \textbf{w.m.} & \textbf{p-c.m} & \textbf{w.w.} & \textbf{p-c. w.} \\
        \hline
        &unfiltered & 85276.0 & 15272.0 & 22184.6 & 5292.0 \\
        \hline
       \multirow{2}{*}{rule-based} & shutterstack &  -2.3\%& -1.6\%& \textbf{-4.2\%} & -3\%\\
        &hatebase & -0.4\% & -0.6\%& -0.5\% & \textbf{-0.9\%}\\
        \hline
        \multirow{3}{*}{classifier-based}&perspective & -0.11\% & -0.11\%& \textbf{-0.13\%}& -0.11\% \\
        %perspective_{lang} & -3.2\% & \textbf{-8.3\%}& -3\%& -6.3\% \\

        &fasttext & -0.3\%&-0.3\% & \textbf{-0.9\%}& -0.7\%\\
       & profanity-check & -0.21\%&-0.22\% & \textbf{-0.89\%}& -0.52\%\\
        \hline
       \multirow{2}{*}{quality-based} & quality\_wiki & -15.1\% & \textbf{-17.2\%} & -12.7\% & -11.2\% \\
        &quality\_webtext & \textbf{-44.6\%} & -42.6\% & -33.1\% & -33.4\% \  
    \end{tabular}
    \caption{The average number of named entities in the 5 samples gathered from Common Crawl and the percentage of sentences removed by applying $7$ filtering strategies. Named entities are broken down by groups resulting from the intersection of gender and origin: Western men (w.m.), Post-colonial men (p-c.m.), Western women (w.-w.), and Post-colonial women (p-c.-w.)}
    \label{tab:experimental_results}
\end{table*}


\paragraph{Knowledge base creation.} The first step of our experimental setting was the creation of a knowledge base that enables the categorization of named entities on the basis of their gender and origin. In order to do so we developed the \textbf{People Dataset}, a corpus of $10.8$ million of entities of the type person with information about their country of birth, citizenship, gender, ethnic minority, and occupation. The dataset is derived from Wikidata \cite{erxleben2014introducing}, a collaborative Knowledge Graph (KG) maintained by the Wikimedia ecosystem, and processed on the basis of previous literature on the topic \cite{stranisci2023wikibio}: we inferred people's countries of birth from Wikidata property `place of birth' (P19) properties, mapped all the properties of the type `ethnic group' (P172) with a curated list of Post-colonial minorities in Western countries (e.g., African-Americans). Finally, we integrated the knowledge base with additional information from CaLiGraph \cite{heist2019uncovering}, a KG derived from Wikipedia categories.

\paragraph{Sampling from Common Crawl.} We gathered $5$ samples of documents from the Common Crawl (CC) snapshot released in August 2024\footnote{\url{https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-33/index.html}}. Each sample is composed of $20$ Web ARChive (WARC) files that have been processed according to the following criteria: \textit{i.} we kept only documents classified as written in English, implementing the langdetect Python library\footnote{\url{https://pypi.org/project/langdetect/}}; \textit{ii.} we removed all the documents with an average number of sentences below \textit{x} and an average number of words \textit{per} sentence below \textit{y}. The average number of documents in each sample is $102,820$ (std: $443$).

\paragraph{Entity Linking.} The next step of our approach was the linking of named entities with the People Dataset. In order to adopt a computationally-efficient Entity Linking (EL) approach to implement on a large set of data, we designed an EL pipeline that combines the adoption of neural methods for the detection of named entities and existing heuristics to link them to the dataset \citet{manghi2023miriam}. The pipeline is organized in four steps. \textit{i.} We used the largest SpaCy model\footnote{\url{https://spacy.io/, en\_core\_web\_sm}} to detect all the named entities of the type PERSON in CC samples. \textit{ii.} We queried Wikipedia APIs\footnote{\url{https://www.mediawiki.org/wiki/API:Search}} with all the retrieved named entities to obtain titles of Wikipedia pages and filtered out all the named entities that were too dissimilar from Wikipedia titles, through the adoption of a set of heuristics already validated by \citet{manghi2023miriam}. \textit{iii.} We retrieved all the Wikidata IDs corresponding to the Wikipedia titles. \textit{iv.} We kept only entities with a Wikidata ID that is present in our People Dataset. As a result (Table \ref{tab:experimental_results}), we identified an average of $128,025$ entities from each sample: $85,276$ Western men, $15,272$ Post-colonial men, $22,185$ Western women, $5,292$ Post-colonial women. In order to assess whether there is high variation between samples we conducted an ANOVA test \cite{st1989analysis} on all the possible pairs of entities distributions broken-down by group. In all cases we obtained a $p-value$ of $0.99$, showing that there is no evidence of variation between different CC samples. 

\paragraph{Implementation of Filtering Strategies.} Having created our baseline, we implemented seven filtering strategies belonging to three different approaches that can be replicated in an experimental setting (Section \ref{ss:category}): rule-based, toxicity classifier, and document similarity. These strategies were applied to all the sentences or documents that mentioned at least one of the retrieved named entities.

The implementation of rule-based strategies relies on two different lexicons for the identification of toxic language: the Shutterstack lexicon and a version of HateBase that has been refined by \citet{hateoffensive} from their Hate Speech corpus\footnote{\url{https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/lexicons}
}. We flagged as harmful any sentence that contains at least one term included in each lexicon. 

For the comparison of model-based strategies we adopted three toxicity classifiers: Perspective API, which has been used during the training of models like Gemini \cite{team2023gemini} and Gopher \cite{rae2021scaling}, profanity-check\footnote{\url{https://pypi.org/project/alt-profanity-check/}}, which has been used to filter the Pile Corpus \cite{gao2020pile}, a FastText classifier trained on the Jigsaw dataset \cite{Jigsaw} that has been used to polish Dolma\footnote{\url{https://github.com/allenai/dolma/blob/main/python/dolma/\\taggers/jigsaw.py}} \cite{soldaini-etal-2024-dolma}. We considered all sentences classified containing Hate Speech by the FastText classifier as harmful. Since Perspective APIs and profanity-check output a probability distribution, we filtered out all sentences classified as toxic with a probability of $0.8$ or more, a threshold in line with previous research \cite{xu2021detoxifying}. 

For the document similarity strategy we replicated the methodology proposed by \citet{brown2020language} and \citet{du2022glam}. We trained a hash based-linear classifier with a vector-size of $1,000$ on a corpus of high-quality documents to filter out documents that do not reach a given quality threshold. Classifiers were trained on a total of $100,000$ documents: $50,000$ considered as high-quality (positive class), $50,000$ considered as low-quality, which were randomly gathered from CC WARC files that were not used during our sampling step. We chose two sources of high-quality documents: the first based on a random sample of Wikipedia documents, the second on a sample from the OpenWebText corpus \cite{Gokaslan2019OpenWeb}.\footnote{The People Dataset and all the implemented filtered strategies will be released under MIT license after the anonymity period.}


\subsection{Analysis of Results} \label{ss:results}
Table \ref{tab:experimental_results} shows the results of our experiments. The first row of the table reports the average number of entity mentions in samples. In the other rows the impact of a data filtering strategy on named entities is reported in terms of the percentage of removed mentions from the sample. Filtering strategies are grouped by the taxonomy they belong to (Section \ref{ss:category}) and results are broken-down by sociodemographic group: Western men (w.m.), Western women (w.w.), Post-colonial men (p-c.m.), and Post-colonial women (p-c.w.). 

\paragraph{How much is filtered out?} From a general overview of results it is possible to observe that strategies have a widely varying impact on our samples. The filtering strategy based on the Hatebase lexicon \cite{davidson-etal-2024-self} and all classifier-based approaches have a marginal effect on documents, filtering out less that $1\%$ of sentences. The Shutterstack lexicon has a greater impact, spanning between $1\%$ and $4.2\%$ of removed mentions. Document-similarity approaches lead to a more aggressive filtering that ranges between $11.2\%$ and $44.6\%$ of removed contents.

\paragraph{What is filtered out?} The magnitude of filtering strategies' impact is not fully explicative of their behavior without considering which texts they flag for the removal. For each category of filtering strategy we performed an analysis aimed at understanding which pattern they follow for removing contents. To investigate rule-based filtering strategies we obtained the distribution of the lexical items that have been found in all samples and identified the five most frequent ones for each lexicon (Table \ref{tab:top-k matching words}). The comparison shows two different filtering patterns between the two strategies. The most-frequent words from Shutterstack lexicon focus on pornography (e.g., `sex'); HateBase terms on racism (e.g., `slaves', `blacks') and misogyny (e.g., `dykes'). Choosing one of the two lexicons does not have only an impact in the number of flagged sentences but also on the specific subset of potentially harmful contents that are removed. 

\begin{table}
    \centering
    \begin{tabular}{cp{5cm}}
        \textbf{lexicon} & \textbf{top-5}\\
        \hline
        shutterstack & dick, sex, porn, ass, nude\\
        hatebase & slave, married to, blacks, dykes, of white\\
    \end{tabular}
    \caption{Top-5 matching lexical items in CC samples}
    \label{tab:top-k matching words}
\end{table}

The comparison of classifier-based strategies focuses on the number of flagged sentences in which their classification overlaps and those where it does not. 
In Figure \ref{fig:venn}, we can identify two patterns. The total number of sentences classified as harmful by Perspective API is almost a subset of ones classified by Profanity Check with an overlap of $90\%$. The overlap of Perspective API with the FastText classifier is lower but still significant ($77\%$). This pattern can be explained by the small number of sentences flagged by Perspective API ($459$) compared to Profanity Check ($2,155$) and FastText ($2,649$). The second pattern is the strong difference in the sentences identified by Profanity Check and FastText: only $35.8\%$ of sentences classified as harmful by FastText are also classified as such by Profanity Check. This shows that, similarly to the lexicon-based strategies, choosing one of the existing classifiers for harm detection may imply targeting different types of harmfulness.  

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{latex/venn_strategies.png}
    \caption{Intersection of contents removed through classifier-based strategies}
    \label{fig:venn}
\end{figure}

Quality-based filtering strategies do not directly aim at harm detection but their training over allegedly high-quality documents is supposed to have an impact on the removal of toxic contents from raw documents. We check their effectiveness by counting the percentage of mentions kept by these strategies despite having been flagged as harmful by rule-based and classifier-based approaches. Results in Table \ref{tab:kept_harms} show that a high percentage of sentences classified as harmful are still present after the quality-based filtering. Adopting the classifier trained on Wikipedia documents leads to the removal of $15\%$ of sentences while keeping $93.5\%$ of harmful sentences. The classifier based on OpenWebText shows a similar proportion: it removes $45\%$ of sentences but $69.3\%$ of sentences classified as harmful by other strategies are still present after the filtering. This comparison reveals that quality is not a proxy of safety, since the most part of toxic contents is not removed through the adoption of quality-based strategies.  


\begin{table}
    \centering
    \begin{tabular}{ccc}
        \textbf{strategy} & \textbf{n. sents (\%)} & toxic sents (\%)\\
        \hline
        wikipedia & 85\% & $90.5\%$\\
        webtext & 55\% & $69.3\%$\\
    \end{tabular}
    \caption{The percentage of harmful contents that remain after the application of quality-based strategies}
    \label{tab:kept_harms}
\end{table}

\paragraph{Who gets filtered out?} A third line of analysis studies what entities are most impacted by filtering strategies. As can be seen in Table \ref{tab:experimental_results}, the adoption of different strategies not only differs in its magnitude but systematically penalizes certain categories of people. Women are always the most impacted target in rule-based and classifier-based strategies, and in $4$ cases out of $5$, named entities that suffer the highest content removal are Western women. Conversely, strategies based on document similarity have diametrically opposed effect, since they impact the most on men. However, as shown in Table \ref{tab:kept_harms}, filtering based on document quality is not a reliable method of identifying toxic contents. Therefore, it is not possible to assume that these strategies remove entity mentions that appear in harmful contents and their major impact on men cannot be interpreted as a side-effect of harm detection.

In order to deepen our analysis of the impact of data filtering strategies against groups, we leveraged our People Dataset to identify which are the occupations of named entities that are removed through data filtering strategies. For each entity mention occurring in a sentence that has been flagged as harmful by a rule-based or a classifier-based strategy we retrieved their occupation from our dataset and computed the occupations that occurred the most for each analyzed group. Since we observed that quality-based filtering strategies are not effective in the detection of toxic content, we did not consider them in this analysis. 

Table \ref{tab:professions} shows the $5$ most occurring occupations in datasets and the $5$ that have been mostly flagged for removal by filtering strategies. The analysis confirms the presence of patterns of discrimination along the gender axis. The distribution of men's occupations that have been filtered is coherent with the original distribution of mentions; for women this is not the same. The most removed occupation of Western women is `pornographic actor', which is not among the most frequent occupations of Western women. Similarly, the profession `model' is one of the most removed from post-colonial women occupations despite not being frequent in the original distribution. This means that the removal of specific forms of harm, which has not necessarily a negative effect, increases the underrepresentation of very specific categories of people, suggesting the need of adopting finer-grained analysis of the impact of filtering strategies on people. 

\begin{table}
    \centering
    \resizebox{0.9\columnwidth}{!}{
    \begin{tabular}{cp{3cm}p{3cm}}
         \textbf{group} & \textbf{top-k occupation}& \textbf{top-k filtered}\\
         \hline
         w.m. & writer, politician, actor, film actor, television actor &actor, writer, film actor, politician, television actor \\
         p-c. m. & politician, actor, writer, television actor, singer &  politician, actor, singer, writer, film actor \\
        w. w. & actor, film actor, singer, television actor, writer & pornographic actor, actor, film actor, film director, television actor\\
        p-c. w. & actor, politician, writer, film actor, singer &  actor, film actor, singer, model, writer \\
    \end{tabular}}
    \caption{top-5 removal by profession}
    \label{tab:professions}
\end{table}



\section{Discussion}
Our survey of data filtering strategies shows that the implementation of effective approaches to detect and remove harmful contents from pretraining datasets is still an open issue with important social implication. Choosing a strategy means addressing a specific subcategory of harmfulness and this has an impact against specific groups of people (RQ 2). Despite this variety, a general pattern emerges from the implementation of all rule-based and classifier-based strategies:\textbf{ the systematic increase of underrepresentation of women in pretraining datasets}. Mentions of women are always removed at a highest rate than men. If at a first look quality-based strategies seems to have a highest impact against men, we discovered that they do not necessarily remove harmful contents. This implies that relying on allegedly neutral sources of knowledge like Wikipedia and the OpenWebText corpus does not prevent from keeping harmful contents against certain groups of people. 

In contrast with this evidence about the complex nature of data filtering for harm reduction, the general tendency that emerges from our survey (RQ 1) is disengagement with this issue. After a wave of efforts in this field connected with the critical work of \citet{bender2021dangers}, \textbf{the interest in implementing robust pretraining data filtering dramatically reduced} in favor of post-training measures for harm reduction. In most cases, the actual implementation of strategies is close-sourced, hindering the replication of procedures that are adopted for processing datasets. The presence of notable exceptions such as Dolma \cite{soldaini-etal-2024-dolma} and the RefinedWeb Dataset \cite{penedo2023refinedweb} represents a significant countertendency. However, there is still a lack of filtering strategies that are effective in balancing the need to remove harmful contents while preserving the representativeness of vulnerable groups in pretraining datasets.

\section{Conclusion}
In this paper we presented a first systematic analysis of data filtering strategies for harm reduction in datasets. The survey of $55$ technical reports describing LLMs enabled us to draw a taxonomy of filtering strategies and to identify the open issues that prevent the implementation of effective, reliable, and replicable methods for the reduction of harmful contents. Additionally, we benchmarked seven existing strategies on CC samples of documents in order to assess their impact against groups characterized by the intersection of gender and origin. The benchmark showed a systematic negative effect of strategies against women and a high variety in the types of harmful contents that specific strategies filter out.

Future work will focus on providing a more effective pipeline for the assessment of data filtering strategies with two aims in mind: \textit{i.} improving the coverage of our pipeline by including additional information regarding groups that does not rely only on named entities; \textit{ii.} adopting a participatory approach to the evaluation of filtering strategies that engages associations and communities of people who are active against discrimination. Finally, we will also systematically explore the correlation between the adoption of specific data filtering strategies and linguistic behaviors emerging from models trained on filtered datasets. 

\section*{Limitation}
A first limitation of our research is the imbalance of the two knowledge bases that we use to obtain sociodemographic information about named entities. Both Wikipedia and Caligraph contain a higher number of Westerners against Post-colonial people and a higher number of men against women. This introduces a bias in the Entity Linking process that might have an impact on results. Developing more balanced datasets for the implementation of our pipeline is one of the key actions of our future work.

A second limitation regards the focus on named entities. People belonging to vulnerable groups are mentioned in documents in very different ways (e.g., demonym, pronouns, countries of origin). However, there are no systems that are trained to automatically identify these triggers of vulnerable identities. We preferred to rely on state of the art approaches to maximize the precision of our EL pipeline against the recall. The second version of our pipeline will account for these alternative ways of mentioning people with certain sociodemographic characteristics.

\section*{Ethical Consideration}
Since our work focuses on vulnerable groups to discrimination, we are aware of the risk of adopting research design biases that can have a negative influence on our categorization of people. We followed the theoretical background emerging from post-colonial and black studies, to avoid the risk of inducing stereotypical representation of vulnerable groups in our analysis. We also acknowledge that the underrepresentation of post-colonial people in existing knowledge bases can be a source of additional discrimination that will be challenged in the next iterations of our benchmarking pipeline.

\bibliography{latex/anthology,latex/custom}
\end{document}
