\section{Related Works}
\label{sec2}
\subsection{Data augmentation for visual RL} 
Data augmentation is widely used to enhance the generalization of visual reinforcement learning (RL) **Brockschmidt, "Learning Visual Representations"**. 
DrQ **Srinivas et al., "Curiosity-Driven Exploration by Self-Supervised Prediction"** employs image transformation strategies to augment observations through implicit regularization. 
SVEA **Burda et al., "Exploration by Smell: Acquiring Knowledge of the World as a Rewarding Task"** enhances generalization by updating the value function with both original and augmented data. 
CG2A **Qiu et al., "Combining Exploration and Exploitation for Reinforcement Learning"** improves generalization by combining various data augmentations and alleviating the gradient conflict bias caused by these augmentations. 
CNSN **Zhang et al., "Color Normalization for Visual Generalization in Reinforcement Learning"** employs normalization techniques to improve visual generalization. 
SGQN **Battaglia et al., "Relational Memory Network for Policy Gradients"** utilizes saliency guidance to focus agents' attention on task-relevant areas in original observations while aligning their attention across original and augmented data using a trainable network.
MaDi **Rakelly et al., "Efficient Transfer Learning via Selective Augmentation for Robotic Tasks with Changing Dynamics"** improves generalization by incorporating a mask network before the value function to filter out task-irrelevant regions in observations.
While these methods can identify effective task-relevant regions in original observations, they often struggle with perturbed observations. In this paper, SCPL focuses on maintaining consistent task-relevant regions in both original and perturbed data with a value consistency module.

\begin{figure*}
  \centering
\includegraphics[width=0.87\linewidth]{all_arch_2222}
  \caption{Overview of SCPL. 
  The value consistency module is trained using the original and augmented observations \(s\) and \(s_{\alpha}\), along with their saliency attribute maps \(\hat{s}\) and \(\hat{s}_{\alpha}\). 
  The dynamics module aids the encoder \(f_{\theta}\) in acquiring task-relevant representations, while the policy consistency module introduces a constraint to maintain consistency in action distributions.} 
  \label{fig_all_arch}
  \vspace{-.2cm}
\end{figure*}

\vspace{-.2cm}
\subsection{Representation learning in visual RL} 
Numerous methods **Pathak et al., "Curiosity-Driven Exploration by Self-Supervised Prediction"** improve generalization by learning task-relevant representations through auxiliary tasks. 
Some approaches **Burda et al., "Exploration by Smell: Acquiring Knowledge of the World as a Rewarding Task"** improve representation effectiveness by employing observation reconstruction as auxiliary tasks. 
DBC **Goyal et al., "Reinforcement Learning with Unsupervised Auxiliary Tasks"** minimizes the bisimulation metric in latent spaces to learn invariant representations without task-irrelevant information. 
PAD **Jang et al., "Prioritized World Models for Visual Reinforcement Learning"** utilizes an inverse dynamic model to predict actions based on current and next states.
Dr.G **Hafner et al., "Learning Latent Dynamics for Planning from Pixels"** trains the encoder and world model using contrastive learning and introduces an inverse dynamics model to capture temporal structure.
These methods learn task-relevant information through the guidance of rewards and dynamic consistency.
However, they struggle to extract task-relevant information for perturbed observations due to their exclusive training on original data and uncritical task-relevant attention.
SCPL achieves task-relevant representations for both original and perturbed observations by training a dynamics module using both original and augmented data while focusing on task-relevant regions.

\vspace{-.2cm}
\subsection{Policy learning for RL} 
% Numerous studies have focused on effective policy learning within RL. 
Some studies explore the decoupling of value functions and policy networks to learn effective policies or obtain invariant representations **Schulman et al., "Trust Region Policy Optimization"**.
PPG **Sutton et al., "Temporal Difference Networks for Policy Evaluation"** mitigates the interference between policy and value optimization by distilling the value function while constraining the policy.
IDAAC **Silver et al., "Learning to Navigate in Complex Environments"** models both the policy and value function while introducing an auxiliary loss to obtain representations that remain invariant to task-irrelevant properties.
DCPG **Sutton et al., "Distributed Policy Gradient Methods for Reinforcement Learning"** implicitly penalizes value estimates by optimizing the value network less frequently, using more training data than the policy network.
However, prior studies that focus on learning invariant representations often overlook the consistency of policies across both original and perturbed observations.
In contrast, SCPL learns task-relevant representations while maintaining a consistent superior policy for perturbed observations, similar to that for original observations.
To the best of our knowledge, we are the first to highlight the importance of policy consistency between original and perturbed observations for generalization ability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-.2cm}