\section{Related Works}
\label{sec2}
\subsection{Data augmentation for visual RL} 
Data augmentation is widely used to enhance the generalization of visual reinforcement learning (RL) \cite{augmentation1,drg,rad}. 
DrQ \cite{drq} employs image transformation strategies to augment observations through implicit regularization. 
SVEA \cite{svea} enhances generalization by updating the value function with both original and augmented data. 
CG2A \cite{cg2a} improves generalization by combining various data augmentations and alleviating the gradient conflict bias caused by these augmentations. 
CNSN \cite{cnsn} employs normalization techniques to improve visual generalization. 
SGQN \cite{sgqn} utilizes saliency guidance to focus agents' attention on task-relevant areas in original observations while aligning their attention across original and augmented data using a trainable network.
MaDi \cite{madi} improves generalization by incorporating a mask network before the value function to filter out task-irrelevant regions in observations.
While these methods can identify effective task-relevant regions in original observations, they often struggle with perturbed observations. In this paper, SCPL focuses on maintaining consistent task-relevant regions in both original and perturbed data with a value consistency module.

\begin{figure*}
  \centering
\includegraphics[width=0.87\linewidth]{all_arch_2222}
  \caption{Overview of SCPL. 
  The value consistency module is trained using the original and augmented observations \(s\) and \(s_{\alpha}\), along with their saliency attribute maps \(\hat{s}\) and \(\hat{s}_{\alpha}\). 
  The dynamics module aids the encoder \(f_{\theta}\) in acquiring task-relevant representations, while the policy consistency module introduces a constraint to maintain consistency in action distributions.} 
  \label{fig_all_arch}
  \vspace{-.2cm}
\end{figure*}

\vspace{-.2cm}
\subsection{Representation learning in visual RL} 
Numerous methods \cite{iso-dream, ted} improve generalization by learning task-relevant representations through auxiliary tasks. 
Some approaches \cite{does?} improve representation effectiveness by employing observation reconstruction as auxiliary tasks. 
DBC \cite{dbc} minimizes the bisimulation metric in latent spaces to learn invariant representations without task-irrelevant information. 
PAD \cite{pad} utilizes an inverse dynamic model to predict actions based on current and next states.
Dr.G \cite{drg} trains the encoder and world model using contrastive learning and introduces an inverse dynamics model to capture temporal structure.
These methods learn task-relevant information through the guidance of rewards and dynamic consistency.
However, they struggle to extract task-relevant information for perturbed observations due to their exclusive training on original data and uncritical task-relevant attention.
SCPL achieves task-relevant representations for both original and perturbed observations by training a dynamics module using both original and augmented data while focusing on task-relevant regions.

\vspace{-.2cm}
\subsection{Policy learning for RL} 
% Numerous studies have focused on effective policy learning within RL. 
Some studies explore the decoupling of value functions and policy networks to learn effective policies or obtain invariant representations \cite{yarats2021improving, andrychowicz2020matters}.
PPG \cite{ppg} mitigates the interference between policy and value optimization by distilling the value function while constraining the policy.
IDAAC \cite{idaac} models both the policy and value function while introducing an auxiliary loss to obtain representations that remain invariant to task-irrelevant properties.
DCPG \cite{dcpg} implicitly penalizes value estimates by optimizing the value network less frequently, using more training data than the policy network.
However, prior studies that focus on learning invariant representations often overlook the consistency of policies across both original and perturbed observations.
In contrast, SCPL learns task-relevant representations while maintaining a consistent superior policy for perturbed observations, similar to that for original observations.
To the best of our knowledge, we are the first to highlight the importance of policy consistency between original and perturbed observations for generalization ability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-.2cm}