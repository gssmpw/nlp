\section{Related Work}
\label{sec2}
Label noise remains a significant challenge in deep learning for image classification. Numerous approaches have been introduced to address this problem, typically falling into four main categories: robust loss, regularization, sample selection, and label refinement. Below, we provide a brief discussion of these approaches.

\textbf{Robust loss.} Robust loss methods focus on designing loss functions capable of effectively handling label noise. Compared to the Cross-Entropy (CE), the mean absolute error (MAE) has higher robustness to noisy labels but tends to suffer from underfitting \citep{ghosh2017robust}. To address this limitation, subsequent studies have aimed to balance the model’s fitting ability and robustness to enhance performance. For example, Generalized Cross Entropy (GCE) achieves an optimal balance between CE and MAE by adjusting a parameter \citep{zhang2018generalized}. Soft Cross Entropy (SCE) integrates reverse cross-entropy with cross-entropy \citep{wang2019symmetric}. Dynamic Adaptive Loss (DAL) establishes a dynamic balance between the model’s fitting capability and robustness, further enhancing generalization performance \citep{li2023dynamics}. These methods are theoretically well-supported and can be easily integrated with other approaches.

\textbf{Regularization.} Regularization is a powerful strategy to combat label noise by limiting model sensitivity to noisy labels. This is achieved by introducing constraints or adjusting model complexity. Common regularization techniques include Data Augmentation \citep{shorten2019survey}, Weight Decay \citep{krogh1991simple}, and Dropout \citep{srivastava2014dropout}. To specifically address noisy labels, Label Smoothing (LS) improves the model’s generalization by smoothing the original noise labels \citep{szegedy2016rethinking}. Arpit \textit{et al.} demonstrated that regularization can hinder the memorization of noise \citep{arpit2017closer}, showing that models tend to learn clean data before fitting noisy labels. Building on this, ELR introduces a regularization term that utilizes the early learning phenomenon to mitigate noise memorization \citep{liu2020early}. Similarly, CDR proposed a regularization term that deactivates non-critical model parameters during training iterations \citep{xia2020robust}. Sparse regularization encourages the model’s predictions to converge towards a sparse distribution \citep{zhou2021learning}. CTRR further proposes a contrastive regularization function that reduces overfitting to noisy labels by constraining feature representations \citep{yi2022learning}.

\textbf{Sample selection.} This category of approaches focuses on distinguishing clean and noisy samples to prevent models from learning incorrect labels \citep{jiang2018mentornet, gui2021towards}. Meta-Weight-Net additionally uses an online meta-learning method that uses a multilayer perceptron to automatically weight samples \citep{shu2019meta}. However, this approach requires a small clean dataset for auxiliary training \citep{shu2019meta}. Many methods utilize the small loss criterion, which assumes that clean samples generally exhibit lower loss values. For example, Co-Teaching uses two networks, each selecting samples with losses below a specific threshold to train the other network \citep{han2018co}. Co-Teaching+ \citep{yu2019does} refines this idea by relying on the disagreement between two networks for sample selection. However, these methods often require manually set thresholds, making them challenging to apply in practice. To address this, AUM designed the Area Under the Margin statistic, proposing a selection mechanism based on the statistic to identify mislabeled samples \citep{pleiss2020identifying}. 

\textbf{Label refinement.} Label refinement involves mixing the original noisy labels with model predictions to generate updated labels. PENCIL introduced an end-to-end label correction method, which simultaneously updates network parameters and label estimates \citep{yi2019probabilistic}. SELC \citep{lu2022selc} leverages the early learning phenomenon to train a sufficiently good model and subsequently updates labels using temporal ensemble methods. SOP \citep{liu2022robust} models label noise using sparse over-parameterization, implicitly correcting noisy labels. Many label refinement methods incorporate sample selection to first identify clean samples and subsequently correct the noisy labels. For example, M-DYR-H used a two-component beta mixture model to approximate the loss distribution, estimating the probability of label errors and correcting labels based on these probabilities \citep{arazo2019unsupervised}. Similarly, DivideMix \citep{li2020dividemix} employs two networks and applies a dual-component Gaussian mixture model to identify mislabeled samples, which are then treated as unlabeled. It then applies semi-supervised learning \citep{yao2023better} to enhance model robustness.

Unlike previous methods, our ALR method leverages label refinement to prevent overfitting to noisy labels while introducing a regularization term to ensure sufficient learning of clean samples. Specifically, ALR eliminates the need for manual thresholding by dynamically learning thresholds automatically in each iteration. Moreover, our method continuously refines and enhances the learning of clean labels throughout the training process, leading to improved robustness and performance.