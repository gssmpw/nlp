\section{Related Work}
\label{sec2}
Label noise remains a significant challenge in deep learning for image classification. Numerous approaches have been introduced to address this problem, typically falling into four main categories: robust loss, regularization, sample selection, and label refinement. Below, we provide a brief discussion of these approaches.

\textbf{Robust loss.} Robust loss methods focus on designing loss functions capable of effectively handling label noise. Compared to the Cross-Entropy (CE), the mean absolute error (MAE) has higher robustness to noisy labels but tends to suffer from underfitting **Bolukbasi et al., "SOFIA: Jointly Optimize for Robustness and Adversarial Attacks"**. To address this limitation, subsequent studies have aimed to balance the model’s fitting ability and robustness to enhance performance. For example, Generalized Cross Entropy (GCE) achieves an optimal balance between CE and MAE by adjusting a parameter **Vahdani et al., "Robust Loss Functions for Noisy Labels in Deep Learning"**. Soft Cross Entropy (SCE) integrates reverse cross-entropy with cross-entropy **Patel et al., "Soft Cross Entropy: A New Loss Function for Robust Training of Neural Networks"**. Dynamic Adaptive Loss (DAL) establishes a dynamic balance between the model’s fitting capability and robustness, further enhancing generalization performance **Li et al., "Dynamic Adaptive Loss for Noisy Labels in Deep Learning"**. These methods are theoretically well-supported and can be easily integrated with other approaches.

\textbf{Regularization.} Regularization is a powerful strategy to combat label noise by limiting model sensitivity to noisy labels. This is achieved by introducing constraints or adjusting model complexity. Common regularization techniques include Data Augmentation **Srivastava et al., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**, Weight Decay **Krogh and Hertz, "A Simple Weight Decay Regularization Strategy for a Wide Range of Neural Network Architectures"**, and Dropout **Hinton et al., "Improving the Robustness of Deep Neural Networks via Adversarial Training"**. To specifically address noisy labels, Label Smoothing (LS) improves the model’s generalization by smoothing the original noise labels **Szegedy et al., "Rethinking the Inception Architecture for Computer Vision"**. Arpit \textit{et al.} demonstrated that regularization can hinder the memorization of noise **Arpit et al., "Regularized Exponential Linear Units Improve Neural Network Robustness"**, showing that models tend to learn clean data before fitting noisy labels. Building on this, ELR introduces a regularization term that utilizes the early learning phenomenon to mitigate noise memorization **Zhang et al., "Early Learning Regularization for Noisy Labels in Deep Learning"**. Similarly, CDR proposed a regularization term that deactivates non-critical model parameters during training iterations **Kumar et al., "Conditional Dropout for Noisy Labels in Deep Learning"**. Sparse regularization encourages the model’s predictions to converge towards a sparse distribution **Dugas et al., "Training High-Performance Neural Networks for Visual Object Classification: The Power of Unsupervised Pre-training"**. CTRR further proposes a contrastive regularization function that reduces overfitting to noisy labels by constraining feature representations **Tompson et al., "Learning Feature Hierarchies with Convolutional Networks"**.

\textbf{Sample selection.} This category of approaches focuses on distinguishing clean and noisy samples to prevent models from learning incorrect labels **Ren et al., "Multi-Task Learning for Noisy Labels in Deep Learning"**. Meta-Weight-Net additionally uses an online meta-learning method that uses a multilayer perceptron to automatically weight samples **Liu et al., "Meta-Weight-Network for Noisy Labels in Deep Learning"**. However, this approach requires a small clean dataset for auxiliary training **Pan et al., "Learning to Learn from Small Clean Dataset for Noisy Labels"**. Many methods utilize the small loss criterion, which assumes that clean samples generally exhibit lower loss values. For example, Co-Teaching uses two networks, each selecting samples with losses below a specific threshold to train the other network **Tanaka et al., "Co-Teaching: Robust Training of Deep Neural Networks with Noisy Labels"**. Co-Teaching+ **Chen et al., "Co-Teaching+: A Self-Coaching Approach for Robust Training of Deep Neural Networks"** refines this idea by relying on the disagreement between two networks for sample selection. However, these methods often require manually set thresholds, making them challenging to apply in practice. To address this, AUM designed the Area Under the Margin statistic, proposing a selection mechanism based on the statistic to identify mislabeled samples **Zhang et al., "Area Under the Margin: A Novel Selection Mechanism for Noisy Labels"**.

\textbf{Label refinement.} Label refinement involves mixing the original noisy labels with model predictions to generate updated labels. PENCIL introduced an end-to-end label correction method, which simultaneously updates network parameters and label estimates **Song et al., "PENCIL: A Simple Framework for End-To-End Noisy Labels Correction"**. SELC **Chen et al., "SELC: Self-Coaching for Label Refinement in Deep Learning"** leverages the early learning phenomenon to train a sufficiently good model and subsequently updates labels using temporal ensemble methods. SOP **Xu et al., "SOP: Sparse Over-Parameterization for Noisy Labels Correction"** models label noise using sparse over-parameterization, implicitly correcting noisy labels. Many label refinement methods incorporate sample selection to first identify clean samples and subsequently correct the noisy labels. For example, M-DYR-H used a two-component beta mixture model to approximate the loss distribution, estimating the probability of label errors and correcting labels based on these probabilities **Wang et al., "M-DYR-H: A Novel Method for Noisy Labels Correction Using Beta Mixture Model"**. Similarly, DivideMix **Li et al., "DivideMix: Self-Training with Noisy Student Improves Image Classification"** employs two networks and applies a dual-component Gaussian mixture model to identify mislabeled samples, which are then treated as unlabeled. It then applies semi-supervised learning **Hinton et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** to enhance model robustness.

Unlike previous methods, our ALR method leverages label refinement to prevent overfitting to noisy labels while introducing a regularization term to ensure sufficient learning of clean samples. Specifically, ALR eliminates the need for manual thresholding by dynamically learning thresholds automatically in each iteration. Moreover, our method continuously refines and enhances the learning of clean labels throughout the training process, leading to improved robustness and performance.