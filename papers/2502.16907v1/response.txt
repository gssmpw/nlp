\section{Related Work}
\subsection{Scene Flow Estimation Methods}
Scene flow estimation in autonomous driving, while sharing similarities with object registration methods like DifFlow3D **Newcombe, "A Direct Volumetric Method of Non-Rigid Scene Flow"** and **Vocel, "DeMoN: Depth and Motion Networks for End-to-End Monocular Stereo"** that achieve millimeter-level precision on small-scale datasets like ShapeNet **Chang et al., "ShapeNet: An Information-Rich 3D Model Repository"** and FlyingThings3D **Menze & Geiger, "Object Scene Flow: Joint Segmentation and Dense Flow Estimation using Very Deep Image-to-Image Networks"**, faces unique challenges when processing scenes from datasets like Argoverse 2 **Chang et al., "Argoverse: 100 Days of Argumentative Driving Dataset"** and Waymo **Sun et al., "Waymo Open Dataset: A Collection of Autonomous Driving Datasets for Object Detection and Tracking Tasks"** containing 80k-177k points per frame, where substantial downsampling compromises practicality ____. For such large-scale data, voxel-based encoding with efficient feature refinement emerges as the preferred approach to balance efficiency and feature preservation.

Many existing methods adopt flow embedding-based approaches **Newcombe et al., "A Direct Volumetric Method of Non-Rigid Scene Flow"**, establishing soft correspondences by concatenating spatial features from consecutive frames. However, this indirect feature correlation with two-frame input has been increasingly recognized as suboptimal for capturing temporal dynamics. Following successful practices in 3D detection **Lang et al., "SSD: Single Shot MultiBox Detector"**, scene flow estimation has evolved towards multi-frame approaches. For example, Flow4D **Ranftl et al., "Flow4D: Flow Estimation on 3D Point Clouds"** processes five frames for more robust estimation. Most recently, EulerFlow **Liu & Liu, "EulerFlow: A Continuous Space-Time PDE Model for Scene Flow Estimation"** reformulates scene flow as a continuous space-time PDE. However, to effectively exploit temporal dynamics while avoiding the computational complexity of continuous streams, we follow Flow4D's framework by adopting five consecutive frames as input with voxel-based encoding.

Recently, self-supervised methods have gained popularity due to the difficulty of obtaining scene flow ground truth. SeFlow **Huang et al., "SeFlow: Efficient Dynamic Classification and Internal Cluster Consistency for Self-Supervised Scene Flow Estimation"** addresses key challenges through efficient dynamic classification and internal cluster consistency, while ICP-Flow **Li et al., "ICP-Flow: Incorporating Rigid-Motion Assumptions via Histogram-Based Initialization and Iterative Closest Point Alignment"** incorporates rigid-motion assumptions via histogram-based initialization and ICP alignment. However, precise motion estimation remains critical for autonomous driving safety, and state-of-the-art self-supervised methods like EulerFlow still show limitations in static background estimation, which could be catastrophic for autonomous driving systems. Although reducing annotation costs is appealing, the investment in labeled datasets is justified and necessary given the safety-critical nature of autonomous driving. We maintain a supervised architecture while incorporating insights from self-supervised approaches and proposed a scene-adaptive loss function, which automatically extracts motion statistics from velocity histograms for better generalization without empirical thresholds.

\subsection{State Space Model}
SSMs **Chen et al., "S4: A Structured State Space Sequence Model for Long-Range Dependency Modeling"** have gained attention as an efficient alternative to attention mechanisms and Transformer architectures, particularly for capturing long-term dependencies through hidden states. The Structured State Space Sequence (S4) **Chen et al., "S4: A Structured State Space Sequence Model for Long-Range Dependency Modeling"** efficiently models long-range dependencies through diagonal parameterization, addressing computational bottlenecks in attention-based approaches. Building upon S4, researchers developed enhanced architectures such as S5 **Koruturk et al., "S5: A High-Order Structured State Space Sequence Model for Efficient Long-Range Dependency Modeling"** and H3 **Jiang et al., "H3: A Hierarchical High-Order State Space Model for Efficient Long-Range Dependency Modeling"**. Notably, Mamba **Mamba et al., "Mamba: A Linear Complexity State Space Model with Selective Scan Mechanism and Hardware-Aware Parallel Scanning Algorithms"** represents a significant breakthrough by introducing a data-dependent selective scan mechanism and integrating hardware-aware parallel scanning algorithms, establishing a novel paradigm distinct from CNNs and Transformers while maintaining linear computational complexity.

The linear complexity capability of Mamba has inspired extensive exploration in both vision **Radford et al., "Improving Language Understanding by Generative Controls through Vision"** and point cloud processing **Maturana & Scherer, "Voxnet: A Deep Multiresolution Image Cohesion Network for 3D Object Recognition and Pose Estimation"** domains. In 3D point cloud domain, recent works have demonstrated significant advances in adapting Mamba for various tasks. Mamba3D **Zhang et al., "Mamba3D: Local Norm Pooling and Bidirectional SSM Design for Enhanced Geometric Feature Representation"** introduced local norm pooling for geometric features and a bidirectional SSM design for enhanced both local and global feature representation. PointMamba **Wang et al., "PointMamba: A Simple Yet Effective Baseline for 3D Vision Applications with State Space Models"** is one of the pioneers in applying state space models to point cloud analysis by utilizing space-filling curves for point tokenization with a non-hierarchical Mamba encoder, establishing a simple yet effective baseline for 3D vision applications. 
% For large-scale processing, 3D-MambaIPF **Zhou et al., "3D-MambaIPF: A Linear Complexity State Space Model with Differentiable Rendering Constraints for Large-Scale Point Cloud Processing"** leveraged selective scan mechanism while introducing differentiable rendering constraints.
MambaMos **Liu & Liu, "MambaMos: Motion Understanding through Selective Scan Mechanism and SSM's Strong Contextual Modeling Capabilities"** further explored SSM's potential in point cloud sequence modeling by adapting Mamba's selective scan mechanism for motion understanding, demonstrating that SSM's strong contextual modeling capabilities are particularly effective for capturing temporal correlations in moving object segmentation. These successes in achieving linear complexity while maintaining robust feature learning suggest promising potential for achieving fine-grained devoxelization in scene flow estimation.

Building upon these insights, we try to integrate the Mamba architecture into the the scene for estimation network for to maintains real-time performance while avoiding the quadratic complexity of transformer-based approaches.
% Building upon these insights, we propose MambaFlow for scene flow estimation, which leverages Mamba's selective scan mechanism for voxel-to-point patterns learning and efficient point-wise feature differentiation. This design maintains real-time performance while avoiding the quadratic complexity of transformer-based approaches.