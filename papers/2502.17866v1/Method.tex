\section{Method}
Our method takes a single childlike drawing as input. The drawing should contain an upright human-like figure, and the figure must be contiguous and have no overlapping (occluded) body parts. But it can be in any pose or facing direction. 
%\yuting{true?}. 
%\Jesse{yes?}. 
We first process the drawing to obtaining a set of high-level annotations such as joints and body parts (Section~\ref{sec:annotation}). We then build a 2.5D model of the figure from these annotations specific for view-dependent rendering (Section~\ref{sec:model}). At runtime, our retargeting algorithm takes a viewing angle and a 3D skeletal motion, and retargets the 3D motion on the 2.5D model in a view-dependent way (Section~\ref{sec:retarget}).

%From a single childlike drawing of a figure and a set of high-level annotations, we construct a 2.5D model suitable for animating with 3D skeletal motion.
%We describe the drawing requirements and annotations provided by the user, explain how the 2.5D character model is constructed from them, and detail the process by which character's representation is manipulated based upon the skeletal motion and camera position.

%Our goal is to, starting from a single image and easy-to-provide annotations, create an rigged character suitable for retargeting complex 3D skeletal motion onto.
%We first describe the annotations that must be provided. 
%Second, we describe how those annotations are used to create the rigged character.
%Finally, we describe how the 3D skeletal motion is retargeted onto the character.


\subsection{Annotations}
\label{sec:annotation}
%The input is a single image of a childlike drawing of an upright human figure.
%The figure must be contiguous and have no overlapping parts. \yuting{but it doesn't have to be forward facing, right?}
%A pose estimation model~\cite{SmithHodgins} predicts 15 joint keypoints (Figure~\ref{fig:example_annotations}.b) on the figure, which are used as input to a segmentation model~\cite{kirillov2023segment} extracing a figure mask (Figure~\ref{fig:example_annotations}.c)
%Users can intervene to correct either of these predictions before proceed to the following step.
The input drawing first goes through the annotation process as in Smith et al.~\cite{SmithHodgins} to automatically obtain 15 joint keypoints and a foreground mask of the figure, optionally with user assistance.
%Users can correct these annotations manually as they wish. 
%\nicky{I can't find figure 4b and 4c} 
%\nicky{Maybe mention how long the annotation process usually take?}

Next, we ask users to annotate regions of the figure with two types of orientation cues: \textit{silhouetted orientation cues} and \textit{internal orientation cues}. Starting from the figure mask as the root, these annotated regions form a hierarchy representing the figure's structure.
These two types of cues are identified based on the findings from our annotation study(Section~\ref{sec:orientation}) and the annotations inform how we manipulate the figure to depict the desired orientation from a given view.


%following the figure structure. \yuting{explain the hierarchy better since it's an important concept.} These cues are identified based on our survey results (Section~\ref{sec:orientation}) \yuting{say it better...}. And the annotations inform how we should manipulate them to depict the desired orientation from a given view.

\textit{Silhouetted orientation cues} are orientation-suggesting parts of the figure visible from its outline, such as the hair in Figure~\ref{fig:example_annotations}. When applicable, users split the figure mask vertically into multiple \textit{silhouette segments}, for example into hair, head, and torso, and label each as in one of the three orientations: left, right, or none.
The feet are not separated into their own silhouette segments; rather, users indicate whether each foot is present and, if so, whether they have a left or right orientation.% \yuting{how about "none" orientation?} 
%\nicky{do you mean the feet are considered as a group instead individually here?
%This step can be skipped entirely if it contains no silhouetted orientation cues \yuting{do we still annotate the feet?}% and, in general, most figures are comprised of one or two silhouette segments.

\textit{Internal orientation cues} are parts of the figure that are frequently used to infer orientation but are not visible upon its outline, such as the nose, mouth, and eyes in Figure~\ref{fig:example_annotations}. A user annotates each existing cue as a \textit{part region}, and specifies five attributes on it as informed by the findings shown in Figure~\ref{fig:part_trait_example}.
\begin{itemize}
\item \textit{Mask} specifies which pixels belong in this part region.
\item \textit{Translate} indicates whether this region should translate as orientation changes, and if so whether the movement should be smooth or discrete (i.e. flipbook-like). 
\item \textit{Direction} indicates the direction of the part as left, right, or none. If the part is orienting either left or right, it may be mirrored horizontally as orientation changes. 
\item \textit{Enclosed} indicates if the part should be fully enclosed within the parent. If so, it will be hidden when it moves outside the parent region.
\item \textit{Hide on Back} indicates if the part should be hidden when the 'back' of the figure is present. 
\end{itemize}
Part regions can form a hierarchy, where a parent could be a silhouette segment or denotes a union of part regions without its own mask. %\yuting{correct?} 
%\jesse{correct} 
\begin{comment}
A user does this by specifying \textit{part regions} upon the character.
A part region is a set of pixels that ought to be modified similarly as the character changes orientation.
Informed by the findings shown in Figure~\ref{fig:part_trait_example}, each \textit{part} has five attributes.
First, it has a segmentation mask, specifying which pixels of the original image it contains.
Second, it has a boolean attribute indicating whether this part ought to translate as orientation changes.
There is also an optional smoothness parameter indicating whether translation should be smooth or discrete (i.e. flipbook-like)
Third, it has a boolean attribute indicating whether it should be flipped horizontally as orientation changes.
If so, the orientation of the part, as drawn, is also provided.
Fourth, it has a boolean attribute indicating whether part's pixels should be hidden if they move beyond the pixels belonging to its parent.
Fifth, it has a boolean attribute indicating whether, when the 'back' of the character is present, the part should be hidden entirely.
Each part may be parented to another part, or to the figure mask directly. 
\end{comment}

%We assume the input to be one childlike drawing of a single figure that is standing upright, has no overlapping parts, and is contiguous. 
%Using the approach of Smith et al.~\cite{SmithHodgins}, we predict 13 keypoints on the image and use them to construct a 16-joint rig that can be reviewed and adjusted by the user.
%We extract a mask of the figure by using the joint coordinates as input points to Segment Anything~\cite{kirillov2023segment}, which is well-suited to the task and more robust to erased lines and shadows than the segmentation method of Smith et al.

%We first collect the annotations necessary for orientation cues visible in the silhouette of the character, such as a tail or nose (Figures~\ref{fig:}.a and~\ref{fig:}.c).

%Users provide a rough part-based segmentation of the figure by specifying horizontal lines dividing the figure mask and specifying whether each resulting segment is oriented towards drawing right, drawing left, or has no orientation.

%There is no limit to the number of segments, but in practice most figures require one or two segments, at most. We note here that feet, though silhouetted, are handled differently; they are not split into their own segments, but the user does indicate whether feet are present and which direction, if any, they point.

%We next handle non-silhouetted parts that ought to be flipped to translated within their enclosing region based upon viewing angle, such as noses and eyes (see Figure~\ref{fig:part_count_orientation}). 







\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{figures/ExampleFigureAnnotations.png}
\caption{Example of the different figure annotations and part-level attributes necessary to create the 2.5D character rigs.}
\label{fig:example_annotations}
\end{figure}


\subsection{2.5D Character Model}
\label{sec:model}
We construct a 2.5D character model of the figure from the annotated orientation cues. This 2.5D model is analogous to a character rig. It consists of a left-facing version of the figure and a right-facing version, termed \textit{left view} and \textit{right view} (Figure~\ref{fig:25D_model}). If the figure is drawn completely front-facing, or in other words no orientation cues are found, the two views are effectively identical. A view consists of a set of textured meshes that depict the designated character orientation with variations. It has a base mesh with two textures that indicate the front or the back of the character. The base mesh can have up to four versions of different combinations of feet orientations, depending on the foot orientation annotation result. In total, a view has up to eight unique textured meshes. In addition to the textured based meshes, each annotated part region within a view can be transformed when seen from the left or the right, as defined by a pair of \textit{keyview-transforms}. Below, we will explain how the left view is constructed as an example. The right view is created similarly.

%\yuting{need to incorporate content from Generate2.5DCharacter}
\begin{comment}
\subsubsection{Generate2.5DCharacter}
We begin by computing a 2D textured mesh for each half's masks and front-back texture pairs. 
While the plane containing these meshes will rotate within 3D space, the meshes themselves remain 2D.
Each mesh is registered as the 'rest shape' for As-Rigid-As-Possible Shape Manipulation~\cite{igarashi2005asrigidaspossible}.
Each keypoint is used to initially place a control handle upon the mesh, which we collectively refer to as the \textit{character rig}.
Each triangle within a mesh is mapped to the closest bone, providing a rough segmentation that is later used to determine render order.

We similarly create 2D textured meshes for each part region.
For translating part regions, we use the keyview-transform pairs to compute a series of \textit{attachment points} upon the parent mesh.
Each attachment point consists of two barycentric coordinates within a mesh triangle and is later used to determine the translation and local rotation that matches mesh after it is deformed.
\end{comment}

% \paragraph{Base mesh}
\paragraph{Base mask}
We horizontally mirror all silhouette segments annotated as "right-facing", including their entire hierarchy. Keypoints, part regions, and child segments within the hierarchy are all mirrored as a result. Remaining part regions not belonging to any silhouette segments but are annotated as "right-facing" are also mirrored. At the end of this process, the left view will contain only "none" and "left-facing" silhouette segments and part regions (see Table~\ref{table:twisted_perpspective_frequency}).
%\yuting{explain the meshing part}

\paragraph{Textures} When part regions move around, they leave behind holes in the texture. We therefore use image inpainting tools \cite{} to create a front texture and a back texture. The front texture is inpainted after removing all part regions that can translate. The back texture is inpainted by additionally removing static part regions that only show on the front (eg. buttons on a shirt).

\begin{comment}
We begin constructing the full 2.5D character model by first creating distinct left and right-facing version of the figure, which we refer to as the \textit{left half} and \textit{right half}. \yuting{do we need to explain why we don't need front/back-facing versions?}
To create the left half, we first inspect each silhouette segment and horizontally mirror any with right-facing orientations. 
Keypoints and part regions with the silhouette segment, as well as distal silhouette segments, are also mirrored during this operation.
We next inspect each part region, again mirroring any with right-facing orientations.
At the end of this process, the left half will contain only forward facing and left-facing silhouette segments and part regions ( see Table~\ref{table:twisted_perpspective_frequency}).
The Right Half is created similarly.
\end{comment}

\paragraph{Foot orientations} If a foot is annotated as either left-facing or right-facing, we mirror it in place to create the other version; this results in up two four versions of the base mask when both feet have orientation. We define the foot region as starting midway between the ankle and knee keypoints, and both the mask and the corresponding textures are mirrored and stitched back to the lower leg (see Figure~\ref{fig:25D_model}).
%\yuting{explain the mesh part better.}

\paragraph{Mesh Generation} From each mask, we extract a contour using the marching square algorithm~\cite{lorensen1998marching} generate a mesh with Triangle's implementation of constrained Delaunay triangulation~\cite{shewchuk96b}, and texture it with front and back textures.

\begin{comment}
During animation, foot orientation is determined, per frame, by the bend of the character's leg .
If a figure's feet have left-right orientations, we mirror their lower leg, starting midway between ankle and knee keypoint, to create four distinct masks and textures corresponding to all foot orientation combinations (see Figure~\ref{fig:mock_LR_25D_characters}).
\end{comment}

\paragraph{Keyview-transform} A translating part region has two key positions when seen from the left or the right, respectively. We define a $3x3$ transformation for each viewing angle to specify how the \textit{anchor point} of a part region should be transformed. This results in two (key view, transform) pairs for each part region. The definition of a anchor point depends on the part region's mask and orientation. If a part region is without left-right orientation, the anchor is simply the centroid of the mask. If it is left-facing, the anchor point is instead positioned on the right side of the region's bounding box; right-facing parts are handled inversely. If the part has no mask, its centroid is the average of its children's centroids.

%% Anchor Points of Parts
\begin{comment}
For each part region that translates, we compute a series of \textit{keyview-transform} pairs. These are 3x3 transformation matrices that define how a part should be modified when observed from a specific view angle $\theta$ (discussed in Section.XXXXXXXX).
We first define a 2D \textit{anchor point} for the part region.
The anchor point is the centroid of the part region's mask, if it exists and has no left-right orientation.
If the part region is left-oriented, anchor point is the projection of the mask centroid upon the right side of the mask's bounding box.
If the part is no mask, the centroid is the average of its children's centroids.
We then insert two keyview-transform pairs.
The left-oriented ($\theta = \pi/2$) transform translates the part's anchor point left until it intersects the boundary of its parent's mask.
The right-oriented ($\theta = 3\pi/2$) is created similar by translation to the right.
See Figure~XXXXX.a for an example of these.
\end{comment}



%% Infilling
\begin{comment}
As a result of flipping silhouette segments and translating different part regions of the original image, discontinuities and undefined values may occur within the texture.
This can be corrected for by automatically constructing an infill mask identifying where such artifacts occur and infilling them, either with generative or classical approaches, in a post-processing step.
We further use infilling to remove part regions which do not translate, but are hidden upon the character's backside.
The results in distinct \textit{front} and \textit{back} textures.
\end{comment}

An example 2.5D character model is shown in Figure~\ref{fig:25D_model}. It consists of two \textit{views}, each containing a single set of joint keypoints and a maximum of four masks and two textures per mask. Each part region lies upon its own layer and contains two keyview-transform pairs.

%The output of this step is a left and right version of the character, each with a rig, a `base' character mask, front and back texture, foot orientation labels, and a tree of parts each with a set of keyview-transform pairs.
%See Figure.YYYYYY.
%We note that, while we describe how the model is initially generated from the input annotations, the textures, masks, and keyview-transforms can be edited to provide additional control over how the figure looks from certain views. This can be used to correct inaccuracies or to many the manipulations to the character model more complex by adding additional keyview transform pairs.
%This is important, as there is no 'correct' answer about what a figure should loook like from a different point of view other than what the users desires. 
%Therefore, rather than attempting to create something which is difficult to edit, 

%\begin{figure}[ht]
%\centering
%\includegraphics[width=0.4\textwidth]{figures/mock_anchorpoints_keyviews.png}
%\caption{Mock figure. Left shows example anchor points. Right shows 2x2 of character version vs. keyview transform results. Note here that, due to animation step, right-facing left version and left-facing right version are never actually shown. Need to keep them in figure for demonstration of what to avoid though.}
%\label{fig:annotations}
%\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{figures/25D_model.png}
\caption{ The part regions, masks, and textures comprising the model's left view. Each part region can be thought of as existing in a different layer (color coded here). The right view is similarly constituted, but left-facing parts, such as hair and nose, are horizontally flipped.}
\label{fig:25D_model}
\end{figure}




\subsection{View-dependent Animation Retargeting}
\label{sec:retarget}
Our retargeting algorithm animates the 2.5D character model on a vertical 2D plane given a 3D skeletal motion and a camera view. It consists of three steps: defining a view-dependent 2D character plane (Section~\ref{sec:retarget-plane}), retargeting 3D animation as 2D character poses (Section~\ref{sec:retarget-pose}), and deforming the character mesh based on the 2D poses (Section~\ref{sec:retarget-mesh}). To mitigate major artifacts in 3D to 2D projection, we additionally introduce a novel projection plane optimization algorithm using the principle of twisted perspective in Section~\ref{ComputeRigJointLocations}. It improves the motion smoothness and recognizability of the character when used for pose retargeting.

%The overview of our retargeting algorithm is provided in Algorithm~\ref{alg:motion_retargeting}. The input to the animation system is a 2.5D character model, a series of 3D skeletal poses specifying motion, and the position of the viewing camera within the scene. We provide more detail one each step in the following subsections.

%To create the animated character, we first generate a 2D mesh from each mask, complete with front and back textures from each mask shown in Figure~\ref{fig:mock_LR_25D_characters}.
%Accounting for all combinations of character versions (right/left), foot orientations, and textures, there are 8 meshes, each with a a front and back texture. 
%For each triangle within each mesh, we compute the nearest bone to the triangle centroid, providing a bone-to-triangle-level segmentation of the base mesh for later us when determining rendering order.
%We also use the mesh and rig joint locations as constraint handles to define the initial `at-rest' position for an ARAP deformation.
%We also determine, for the parts parented directly to the mesh, a number of attachment points and rotations using barycentric coordinates, using the keyview transforms.

%With the complete, we the perform the follow steps, per frame, to generate the character representation.
%The input is the 3D cartesian coordinates of the camera, the character's root transform, and the motion source skeleton's joints.

\begin{comment}
\begin{algorithm}
\caption{Motion Retargeting}\label{alg:motion_retargeting}
\begin{algorithmic}[1] % The number tells LaTeX to number each line
    \Procedure{Retarget}{characterModel, viewer, motion} % Procedure name and parameters
        \State Generate2.5DCharacter(characterModel)


        \For{$pose \gets motion$}
            \State $\theta \gets$ GetViewingAngle(viewer, character)
            \State SetCharacterPlaneTraverseRotation($\theta$)
            \State SetLimbFlipping($\theta$)
            \State SetVisibleCharacterHalf($\theta$)
            \State ComputeRigJointLocations($\theta$, pose)
            \State SelectAndDeformCharacterMesh()
            \State ComputePartTransforms($\theta$)
            \State ComputeRenderingOrder($\theta$, pose)
            \State OffsetCharacterRoot(pose)
        \EndFor
    \EndProcedure
\end{algorithmic}

\end{algorithm}
\end{comment}

\subsubsection{Character plane}
\label{sec:retarget-plane}
\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{figures/view_circle.png}
\caption{View unit circle.  Root view vector (define as ground projection of vector from camera to character) is always at 0.Change skeleton to pose. When view angle = 0, character is looking away. pi/2 = looking left. pi = looking directly at character 3pi/2 = looking right.}
\label{fig:view_circle}
\end{figure}

The 2.5D character lives on a 2D \textit{character plane} that rotates about the vertical axis to face the camera, and translates to follow the character. We formally define the view angle $\theta$ as the angle between the \textit{root-view vector} and the \textit{skeleton-forward vector} (see Figure~\ref{fig:view_circle}). The \textit{root-view vector} is the ground projection (i.e. removing pitch and roll) of the 3D vector from the camera to the 2.5D character's root joint
%, assuming the character is always in front of the camera
. The \textit{skeleton-forward vector} is the ground projection of the normal vector of the skeleton's frontal plane, defined by the three vectors connecting the shoulder joints and the hip joint. The character plane then rotates to align its normal vector with the negative root-view vector. The position of the character plane follows the character's root joint. The character root is updated by scaling the skeleton's root velocity by the leg length ratio between the skeleton and the character.

%containing the character rig's root joint in order to align its normal vector with the groundplane prjection of the root-view vector.

%We define the \textit{root-view vector} to be the 3D vector from the view camera to the character rig's root. 
%We define the \textit{skeleton-forward vector} to be the vector perpendicular to the average of the vectors connecting the shoulder joints and the hip joints.
%We then define the view angle $\theta$ as the angle between the ground projection of the root-view vector and the skeleton-forward vector. See Figure~\ref{fig:view_circle}.


%In essence, the character will always as though upon a piece of paper oriented towards the viewer: this prevents the character from becoming a thin slit and ultimately disappearing if viewed from the side.

%We compute the offset of the skeleton's root position from the previous frame.
%This is scaled by the ratio of the skeleton's legs to the character rigs legs, and applied directly to the character rig root joint.

\subsubsection{Character pose retargeting}
\label{sec:retarget-pose}

After we decide on the character plane, we need to choose between the two character views as the target for pose retargeting. We use the left view when $\theta \in [0, \pi]$, and the right view when $\theta \in [\pi, 2\pi)$. The character joints defined by the keypoints in the chosen view serves as the target 2D character.

Our retargeting follows Smith et. al. \cite{SmithHodgins}, assuming a joint mapping is available between the skeletal and the character. For a pair of corresponding joints, we project the 3D joint vector to the character plane, and rotate the corresponding character joint in 2D to align their directions. This process obeys two major constraints. Firstly, the character joints are constrained to 2D rotations in-plane. Secondly, in accordance with the style of childlike drawings, the character joints are not foreshortened but maintain their lengths.

In the case when the skeleton is back-facing the camera (i.e. $\theta \in [0, \frac{\pi}{2}], \text{or } \theta \in [\frac{3\pi}{2}, 2\pi)$), we swap the character's left limbs with the right limbs in the joint mapping to "turn $180^{\circ}$". It means the left limbs of the skeleton drive the right limbs of the character, and vice versa.

Note that the feet joints are excluded in this retargeting process. We instead determine the feet orientations based on the knee orientations (see below) to prevent them from appearing "bending backwards".

%If $\theta$ lies between $\frac{\pi}{2}$ and $\frac{3\pi}{2}$, the left limbs of the skeleton are used to drive the left limbs of the character rig, and vice versa.
%However, if $\theta$ lies between $\frac{3\pi}{2}$ and $\frac{\pi}{2}$, the limb mapping is flipped.

%The character rig is reposed so as to suggest the 3D skeletal pose, but with two major constraints.
%Similar to Smith et al., we do this by projecting the pose onto a 2D plane, computing it orientation within the plane \(\alpha\), and rotating the corresponding rig's joint to match (see Figure~\ref{fig:projection}.a)


\subsubsection{Character mesh deformation}
\label{sec:retarget-mesh}
The character pose is used to deform the textured character mesh of the chosen view. We first choose the final rest mesh based on the feet orientation. The sidedness of a foot is the same as where the knee is pointing on the 2D plane. Once the final mesh is selected, it is deformed using ARAP~\cite{igarashi2005asrigidaspossible} with the posed character joint locations as control handles. 
%We next determine character feet orientation and use them to determine the specific mesh that will be deformed and rendered for the frame.
%The orientation of a foot is the direction of the reflex angle at the knee; by using the knee angle to drive foot orientation, the knee never appears to be `bend backwards.'
%We select the character mesh with appropriate foot orientation, use the character rig's joint locations to update the control handles, and solve for the ARAP deformation of the mesh~\cite{igarashi2005asrigidaspossible}.
We then use the view angle to interpolate between each part region's keyview-transform pairs and to render them with proper translation and rotation upon the deformed mesh.

%\yuting{we need to explain keyview-transforms and attachment points here?}
%We then process each part region by attaching them on the deformed character mesh based on the barycentric coordinates of its attachment point, then transform it with the keyview-transform matrix based on the view angle $\theta$.

%For transforming each part region, we select the appropriate attachment point for view angle $\theta$ and, using the triangle and barycentric coordinates of the newly deformed mesh, apply the appropriate translation and rotation.

Because the character exists upon a 2D plane, depth-based culling is insufficient to properly obscure intersecting body parts. Instead, we explicitly compute the render order of each mesh triangle, along with the part regions attached to those triangles. We determine render order by analyzing the input 3D skeleton's joint positions and ordering them based upon depth to their projection plane.
We then render, from furtherest to nearest, the triangles mapped to the equivalent joints within the character.


%. For each joint, we project both the joint and the skeleton's root onto the joint's projection plane normal vector; we then compute the distance between the two.\yuting{this sentence is unclear} \nicky{Will be great if have some visualization} We do this for all joints and sort them from further behind the root to furthest in front. Finally, we render from back to front the triangles mapped to the equivalent joints within the character.


%\subsubsection{GetViewingAngle}
%\subsubsection{SetCharacterPlaneTraverseRotation}
%\subsubsection{SetLimbFlipping}
%\label{sec:limb_mapping}
%\subsubsection{SetVisibleCharacterHalf}

\subsubsection{Projection Plane Optimization}
\label{ComputeRigJointLocations}

\begin{comment}
\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{figures/projection.png}
\caption{A. Example of how the orientation of the character's rig joint, \(\alpha\), is obtained from a 3D bone vector.
B. As the 3D bone vector approaches the normal vector of the projection plane, `flailing' artifacts may occur.
C. An example of two bone configuration, which, after being projected, result in the same orientation \(\alpha\). This results in no change in the character's rig, or `dampening' of the input motion.
Mock figure. Refer to segments as \textit{Silhouette Segements}}
\label{fig:projection}
\end{figure*}
\end{comment}

When projecting the 3D skeletal pose to the character plane, we observe two major artifacts. When a 3D joint is pointing near the plane normal, a small movement can cause large motion in the corresponding 2D joint, an effect we termed \textit{flailing}. On the other hand, a large movement in the 3D joint could result in no change of the corresponding 2D joint, what we termed \textit{dampening}. This is when the 3D movement only changes the magnitude of the projected 2D vector but not its angle.

%It is sensible to select the \textit{root-view projection plane}, the 2D plane whose normal vector is equal to the inverse of the root view vector; that is, essentially, how 3D objects are projected to 2D planes such as photographs.
%However, if we use only this plane, two types of artifacts that appear. 
%These artifacts are consequence of using a projection operator in combination with an atan2 operator.
%The first artifact, \textit{flailing}, results in small variations in the input pose causing large variations in the character rig.
%The second, \textit{dampening}, results in large variations in the input pose result in no variation in the character rig.
%Drawing upon the results of the annotation study (Figure~\ref{fig:part_count_orientation}) and findings from children's drawing literature~\cite{willats2006making}, we manipulate the character's rig so as to suggest the pose of the 3D skeleton while keeping the character 2D and not foreshortening the limbs.

We can better understand these artifacts by looking at how the 2D joint orientation \(\alpha\)\ change as a function of the 2D vector $\mathbf{P}$, where $\mathbf{P} = [(\mathbf{I} - \mathbf{Z}\mathbf{Z}^{T})\mathbf{V}]_{xy}$ is the 2D projection of the 3D vector $\mathbf{V}$ onto the XY-plane (i.e. the projection plane). We have $\alpha = atan2(P_{y}, P_{x})$, and its Jacobian is

$$
\frac{\partial \alpha}{\partial \mathbf{P}}
=
\left[ 
    \frac{-P_y}{P_x^2 + P_y^2}, 
    \frac{P_x}{P_x^2 + P_y^2}
\right].
$$

When $\mathbf{V}$ is close to the $\mathbf{Z}$ axis, $\mathbf{P}$ becomes small, and the norm of the Jacobian becomes large. Therefore, a small change in $\mathbf{P}$ results in a large change in $\alpha$. When $\mathbf{V}$ rotates about the axis $[-P_y, P_x, 0]$ or $[P_y, -P_x, 0]$ on the XY-plane, $\mathbf{P}$ changes in the direction of $[P_x, P_y]$ (i.e. $\Delta \mathbf{P} = [P_x, P_y]r, r \in \mathcal{R}$), and we have $\frac{\partial \alpha}{\partial \mathbf{P}} \Delta \mathbf{P} = 0$.

\begin{comment}
To demonstrate the cause of flailing, consider the toy example shown in Figures~\ref{fig:projection}.a and ~\ref{fig:projection}.b.
The viewer is located upon the +Z axis.
The skeleton root is located at the origin.
In this example, the projection plane contains the origin and its normal vector, the inverse of the root view vector, is equal to the \(\mathbf{Z}\)~basis vector.
The skeleton contains one other joint, and together they comprise a single 3D bone vector.

\(\mathbf{V}\)'s planar projection, \(\mathbf{P_{xy}}\), can be computed as follows, noting that $z=0$ for all points within the plane:

$$ \mathbf{P_{xy}} = [ \mathbf{V} - (\mathbf{Z} \cdot \mathbf{V})\mathbf{Z}]_{xy} = \mathbf{V}_{xy}$$

The orientation of \(\mathbf{P_{xy}}\) within the plane is then:

$$ \alpha = atan2(\mathbf{P_{x}}, \mathbf{P_{y}}) $$

And the Jacobian matrix relating changes from \(\mathbf{V}\) to \(\alpha\) is:

%\left[ 
%    \frac{\partial \text{atan2}(\mathbf{V_y}, \mathbf{V_x})}{\partial \mathbf{V_x}}, 
%    \frac{\partial \text{atan2}(\mathbf{V_y}, \mathbf{V_x})}{\partial \mathbf{V_y}}, 
%    0 
%\right] 

$$ 
\mathbf{J}= 
\left[ 
    \frac{\partial \text{atan2}}{\partial \mathbf{V_x}}, 
    \frac{\partial \text{atan2}}{\partial \mathbf{V_y}},  
    0 
\right] 
=
\left[ 
    \frac{-\mathbf{V_y}}{\mathbf{V_x}^2 + \mathbf{V_y}^2}, 
    \frac{\mathbf{V_x}}{\mathbf{V_x}^2 + \mathbf{V_y}^2}, 
    0
\right]
$$


As  \(\mathbf{V}\) approaches \(\mathbf{Z}\), \(\mathbf{V_x}\) and \(\mathbf{V_y}\) approach $0$, causing the denominators within \(\mathbf{J}\) to shrink and the terms to grow, resulting in an ill-conditioned and, when \(\mathbf{V}\) equals \(\mathbf{Z}\), ultimately undefined matrix.

In practice, this results in the character's rig flailing wildly when a 3D bone vector happens to nearly coincide with the projection plane normal for any length of time (reference this in Video).

An opposite problem, \textit{dampening}, occurs when large changes in 3D bone vector orientation produce no change in the character rig. 
This occurs when the 3D bone vector rotates within a plane that contains projection plane normal, such as $x=0$ (see Figure~\ref{fig:projection}.c).
With this additional constraint of $x=0$, \(\mathbf{J}\) reduces to: 
$$ 
\mathbf{J}= 
\left[ 
    0, 
    \frac{\partial \text{atan2}}{\partial \mathbf{V_y}},  
    0 
\right] 
=
\left[ 
    0,
    \frac{\mathbf{V_x}}{\mathbf{V_x}^2 + \mathbf{V_y}^2}, 
    0
\right]
=
\left[ 
    0, 
    \frac{0}{\mathbf{V_y}^2}, 
    0
\right]
=
\mathbf{0}
$$

While changes to \(\mathbf{V}\) will affect the magnitude of \(\mathbf{P_{xy}}\) its orientation \(\alpha\) will not change.
As a result, such rotations of \(\mathbf{V}\) result in no change in the character rig.
This appears frequently in practice, and results in the character's legs not bending as it walks directly towards the camera.
\end{comment}

We can avoid both issues if we choose the projection plane based on the orientation and rotation axis of joints, rather than being constrained to the character plane. Such an approach is in fact supported by the use of twisted perspective frequently seen within childlike drawings (Table~\ref{table:twisted_perpspective_frequency}), where different projection planes are applied on different body parts.

\begin{comment}
To solve both of these, we first observe that the root-view projection plane is only one of an infinite number of possible projection planes.
Secondly, we observe that the same plane does not need to be used for all frames of a motion; indeed, the same plane does not even need to be used consistently within the same frame.
Using different projection planes on different parts of the body results in twisted perspective, a feature frequently seen within childlike drawings (Figure~\ref{table:twisted_perpspective_frequency}).
\end{comment}

We therefore formulate a multi-objective optimization to find the optimal projection plane $\mathbf{n}$ for each limb independently, since the flailing and dampening artifacts are most severe for limbs. 

Because both the knees and elbows are hinge joints, their rotation axis is the normal of the plane defined by the upper limb $\mathbf{v}_u$ and the lower limb $\mathbf{v}_l$ of the 3D skeleton. We want to prevent the rotation axis from lying on the projection plane, and want the limbs to be pointing away from the projection normal. It means $\mathbf{n}$ should be as aligned with the rotation axis as possible. We also want to encourage the projection plane to be close to the character plane, so the twisted perspective is not abused, and the 2D pose remains recognizable as a projection of the 3D pose. Additionally, we want $\mathbf{n}$ to be close to previous frame's projection plane normal $\mathbf{v}_p$ for temporal consistency.

\begin{comment}
We compute this objective $E_p$ as 
$$E_p = -((\mathbf{v}_u \times \mathbf{v}_l)^T\mathbf{n})^2,$$
where $\mathbf{v}_u$ and $\mathbf{v}_l$ are assumed to be unit vectors. When they are collinear, the rotation axis is ill-defined, and this term naturally goes to zero.
\end{comment}

\begin{comment}
%We therefore perform a series of tests, per frame, the check for the potential of %\textit{flailing} and \textit{dampening}.
For each limb in the 3D skeletal pose, we compute the normalized 3D bone vectors of the upper and lower limb.
These 3D vectors represent two points upon a unit sphere, and together define a great circle.
Points upon this great circle represent 3D vectors lying upon the plane containing both of the limb's 3D bone vectors.
Because knees and elbows are hinge joints, this is the same plane within which those limbs bend.
By selecting a projection plane who's normal plane vector is located apart from this great circle, we can avoid both flailing and dampening artifacts.

The optimization therefore searches the unit sphere for a point of lowest cost, representing the 3D projection plane normal.
\end{comment}

\begin{comment}
In addition, we want to encourage the projection plane to remain close to the character plane, so the twisted perspective is not abused, and the 2D pose remains recognizable as a projection of the 3D pose. Finally, we want to prevent $\mathbf{n}$ from frequent changes and would like it to be temporally consistent with the previous frame's projection plane normal $\mathbf{v}_p$. These two objectives can be expresed as
$$
E_c = -\mathbf{v}_c^T\mathbf{n}, \text{where} \mathbf{v}_c \text{is the character plane normal,} \\
E_p = -\mathbf{v}_p^T\mathbf{n}.
$$
\end{comment}

If we represent rotations on a unit sphere, $\mathbf{v}_l$ and $\mathbf{v}_u$ forms two points on a great circle that $\mathbf{n}$ should stay away from, and the character plane normal $\mathbf{v}_c$ and $\mathbf{v}_p$ are two attractors. The closer $\mathbf{v}_l$ and $\mathbf{v}_u$ are, the more straight the limb becomes, until the limb is perfectly straight and the two vectors are collinear. We therefore scale the influence of the great circle by the angle between the two vectors. %\yuting{not very clear}
\begin{comment}
The optimization is encouraged to avoid the great circle. We also encourage the solution to remain near the root-view project plane. This ensure the twisted perspective isn't abused, and helps the resulting pose, after being applied to the character, recognizable and cohesive.
Additionally, we encourage the solution to remain close to the previous frame's projection plane, if possible.
This minimizes repeating popping in the limb when the root view projection plane remains upon the great circle.

We formulate the optimization as a modified gaussian, lowering the cost of solutions close to the root-view projection plane vector and the limb's previous projection plane vector, and increasing the cost of solutions that are close to the great circle.

We note that, when the limb is perfectly straight, both 3D bone vectors occupy the same point upon the unit sphere, and there are therefore infinitely many great circles containing both.
However, we further note that, in such a configuration, there is no `bend' in the limb that needs to preserved when applied to the character. We scale the influence of the great circle such that, as the limb becomes totally straight, it goes to zero.
\end{comment}

Formally, we solve for $\mathbf{n}$ on the unit sphere by minimizing the following cost function:

\begin{equation}
\min_{\mathbf{n}} \left( (1 - \frac{|\mathbf{v}_u^T\mathbf{v}_l|}{\|\mathbf{v}_u\|\|\mathbf{v}_l\|}) \cdot e^{\frac{d_{xt}(\mathbf{n}, \mathbf{v}_u, \mathbf{v}_l)^2}{{2 \sigma_1^2}}} + e^{\frac{d_{gc}(\mathbf{n}, \mathbf{v_c})^2}{{2 \sigma_2^2}}} + e^{\frac{{d_{gc}(\mathbf{n}, \mathbf{v_p})^2}}{{2 \sigma_3^2}}} \right)
\end{equation}

where $d_{xt}$ is the \textit{cross-track distance} between $\mathbf{n}$ and the great circle formed by $\mathbf{v}_l$ and $\mathbf{v}_u$,
and $d_{gc}$ is the \textit{spherical distance} between two points on a sphere.
%$\mathbf{v_1}$ and $\mathbf{v_2}$ are the bone vectors of the limb, $\mathbf{v_c}$ is the root view vector, and $\mathbf{v_p}$ is the projection vector used for this limb in the previous frame.
%We can avoid having to perform this operation every frame by first computing $d_{xt}(\mathbf{v_c}, \mathbf{v_1}, \mathbf{v_2})$ and, if it is above a threshold value, simply set $\mathbf{x}$ equal to the inverse of the root-view vector.
Instead of computing $\mathbf{n}$ every frame for every limb, we can first check the value of $d_{xt}(\mathbf{v}_c, \mathbf{v}_u, \mathbf{v}_l)$ for a limb. If it is above a threshold, we can comfortably use $\mathbf{v}_c$ for that limb.



%\begin{figure*}[ht]
%\centering
%\includegraphics[width=1.0\textwidth]{figures/mock_unit_sphere.png}
%\caption{In final figure, add the resulting projection plane normal that we find to the left figure. Also add arrowheads to the skeleton to make points on unit cicle less confusing.}
%\label{fig:projection}
%\end{figure*}












%\subsubsection{SelectAndDeformCharacterMesh}

%Once the joint locations have been computed, we can select the mesh to render. 
%Recall that there are four possible meshes, depending upon the orientation of the feet. 
%We determine how the feet ought to be oriented by observing the angle at the rig's knees. 
%If the non-reflex angle between the upper leg and lower leg lies towards the 'drawing right' side, then take the foot to also be oriented towards 'drawing right', and vice-versa. 
%In this way, the knee never appears to bend in the `wrong' direction.

%After selecting the correct mesh based upon foot orientation, we then use the joint positions to update the ARAP control handles and deform the mesh using the approach of .

%\subsubsection{ComputePartTransforms}


%\subsubsection{ComputeRenderingOrder}

%to render each triangle of the character mesh, along with the part regions

%To present the 3D pose recognizably, it is important to obscure parts behind other parts correctly.
%However, the character exists on a 2D plane. 
%Therefore, we disable depth-based culling and instead explicitly compute the order in which to render each part of the character.

%We then render, from back to front, the triangles mapped to each joint.
%We render each part after the triangles containing it's current attachment point is rendered.
%Parts parents are rendered before their children. 
%The order of multiple parts belonging to the same set of triangles, such as facial features, is determined by the other in which they were annotated, but the keyview information could be extended to include rendering order, as in Rivers et al.~\cite{rivers25Dcartoonmodels}.

%\subsubsection{OffsetCharacterRoot}








%\begin{enumerate}
    %\begin{enumerate}
        %\item Get input, camera, root joint, motion source joints.
        %\item Compute the `view angle', theta, from camera and root joint.
        %\item Determine whether to show the Left or Right version of character
        %\item Determine the rotation about y axis of character's 2D plane.
        %\item Determine the mapping from motion source left-right limbs to character left-right limbs
        %\item Compute the new ARAP handle locations (big section)
        %\item Determine which mesh we are using (foot direction)
        %\item Deform the mesh using ARAP handle locations
        %\item Determine part transforms based upon viewing angle, and computed barycentric coordinates and local rotation of attachment points.
        %\item We compute the rendering order by using the 3D joint positions and determining how much they are in front of, or behind, the torso along the projection vector used for that limb.
        %\item Offset the character's root positon in 3D space
    %\end{enumerate}
%\end{enumerate}









