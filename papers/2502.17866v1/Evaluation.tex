\section{Evaluation}

We evaluate our system in several ways.
First, we conduct a perceptual user study validating that 3D motion, including transversal rotation, remain recognizable when applied to our 2.5D models.
Second, we present a series of ablations demonstrating the results of the system design decisions.
Third, we compare the resulting animations to existing methods. Our results and evaluation settings are best assessed from the supplementary video.

\subsection{Motion Recognition User Study}
\label{sec:recognition}
We conducted a user study to determine whether the manipulations applied to the 2.5D character models were sufficient to recognize the original 3D motions.
See video for example stimuli and Appendix 2 for full details.
Ten different MTurk HITs were generated, each showing videos of eight motion-camera position pairs applied to a different 2.5D character model and to a 3D model.
All users within a HIT saw all 16 videos.
Results were treated as nominal (correct vs. incorrect) and paired (same motion applied to 2.5D and 3D character). We therefore use the McNemar test for significance.
Results are shown in Figure~\ref{fig:user_study_1}. In nine of ten cases, there was no significant differences, indicating users were able recognize the original 3D motion at similar rates when viewed upon either a 3D or a 2.5D model.
%For nine of the ten characters, there was no significant difference.

These results show that, in general, the techniques employed by the 2.5D model are sufficient to preserve the identity of the original 3D motion, even in the absence of root translation.
However, for drawing 7, correct responses were low for two specific stimuli.
In both cases, the limb mapping switched during the clip (see Section~\ref{sec:retarget-pose}).
This character was drawn holding a carrot in one hand; the carrot did not switch hands during the limb mapping switch.
The contradiction may explain the significant difference, and indicates that held object and other limb asymmetries may need to be handled differently.

% Two motions, \textit{walking-turnleft} and \textit{walking-turnright}, and four different camera locations encircling the motions were selected.
%A 2.5D model was created and animated with each of the eight motion-camera positions pairs. However, no root translations was applied; model's root remained in a fixed position.
%Eight equivalent clips featuring a visually realistic 3D mesh were also created. 
%Users were shown these 16 clips in a randomized order along with visualizations of four of the eight motion-camera positions pairs (one used to generate the animation, three random ones) and asked to select the visualization most similar to the animation.
%This experimental setup was repeated with 10 different character models. See appendix XXXXX for complete details.

%We are specifically interested in whether there is a significant difference the participant's ability to identify the original motion correctly when viewing it applied to either a 3D or 2.5D model.
%Because our data is nominal and there is a natural pairing between response (all users saw the same motion-camera position stimuli upon both a 3D model and a 2.5D model), we use the McNemar test for significance.
%Results are shown in Figure~\ref{fig:user_study_1}. In nine of ten cases, there was no significance, indicated users were able identify the original 3D motion at similar rates when viewed upon a 3D and 2.5D model.
%For one, there was a significant difference.




\subsection{Style Preference User Study}
\label{sec:style}
To validate the appeal of our method, we conducted an exploratory second user study asking viewers to rank a basic animated scene among a 2D, 2.5D, and 3D character representations.
The 3D models were hand-crafted by a professional 3D artist.
45 MTurk workers were hired and asked to choose the most and least style-preserving, identity-preserving, visually appealing, scene-cohesive, and personally preferred representation. Stimuli are shown in the supplementary video. Additional study details are given in the Appendix 3. 
For two characters,\textit{ Candy Corn} and \textit{Stick Girl}, 3D and 2.5D methods were preferred at similar levels.
For \textit{Robot}, 3D was preferred. 
2D was always least preferred among all characters.
This suggests that our 2.5D model is a valuable stylistic alternative to 3D, in addition to being much easier to create.
The justifications users provided for their opinions were informative. While some people preferred the professional polished look of 3D, others felt it fundamentally altered the character in a way the 2.5D method did not. 





\subsection{Ablation}
To justify our design decisions, we provided several ablations.

\paragraph{View-dependence.}
Childlike drawings are drawn in a view-dependent manner. To preserver their unique style, it is necessary to manipulate them in a similarly view-dependent way. To demonstrate its importance, we provided side-by-side comparisons of view-dependent and non-view-dependent animation in the video. In addition to changing the style of the figure, the motion of the limbs is less clear and the character disappears completely from certain viewing angles without our algorithm.






\begin{comment}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        283 & 6 \\ \hline
        13 & 34 \\ \hline
        \multicolumn{2}{|c|}{0.17} \\ \hline
    \end{tabular}
    \caption{262}
    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         283 &  9 \\ \hline
         13 &  39 \\ \hline
        \multicolumn{2}{|c|}{0.52} \\ \hline
    \end{tabular}
    \caption{288}
    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         275 & 9 \\ \hline
         4 & 32 \\ \hline
        \multicolumn{2}{|c|}{0.27} \\ \hline
    \end{tabular}
    \caption{2ab}
    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         271 & 7 \\ \hline
         21 &  29 \\ \hline
        \multicolumn{2}{|c|}{\textbf{0.01}} \\ \hline        
     \end{tabular}
    \caption{2ac}
    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         281 & 9 \\ \hline
         15 & 63 \\ \hline
        \multicolumn{2}{|c|}{0.31} \\ \hline
    \end{tabular}
    \caption{3b1}
    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         284 & 12 \\ \hline
         21 & 27 \\ \hline
        \multicolumn{2}{|c|}{0.16} \\ \hline
    \end{tabular}
    \caption{5e5}
    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         315 & 7 \\ \hline
         12 & 26 \\ \hline
        \multicolumn{2}{|c|}{0.36} \\ \hline
    \end{tabular}
    \caption{5e9}
    \label{tab:my_label}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         289 & 16 \\ \hline
         15 & 40 \\ \hline
        \multicolumn{2}{|c|}{1.0} \\ \hline
    \end{tabular}
    \caption{c2c}
    \label{tab:my_label}
\end{table}


\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         295 & 10 \\ \hline
         11 & 28 \\ \hline
        \multicolumn{2}{|c|}{1.0} \\ \hline
    \end{tabular}
    \caption{d07}
    \label{tab:my_label}
\end{table}








\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         254 & 18 \\ \hline
         17 & 39 \\ \hline
        \multicolumn{2}{|c|}{1.0} \\ \hline
    \end{tabular}
    \caption{d0d}
    \label{tab:my_label}
\end{table}

\end{comment}



%\subsection{(Speed?)}

% - how quick to create annotations (reference tool)
%We construct a simple-but-effective annotation tool to assist in providing these annotations.
%Most characters can be fully annotated in under 3 minutes.
%See Appendix BBBBB for more details.
% - how quick to go from annotations to character model (rapid iteration)
% - FPS achieved.

%Mention how quickly a character can be generated at the start of each animation (thus allowing for rapid iteration), as well as the FPS achieved.
%Also give rough numbers for how quickly a figure can be annotated, and direct towards appendix video.

%% mention how quickly this occurs, and thus that the system supports rapid iteration, allowing useres to modify the 2.5D model and quickly see how the resulting animation looks when animated.




%\subsection{User Study 1: Evaluating Character Orientation Comprehensibility}
\begin{comment}
- User study assessing comprehensibility of orientation
    - 10 characters
    - 10 clips with character
    - 10 clips with 3D model
\end{comment}

%\subsection{User Study 2: Comparison to 3D and 2D Representations}
\begin{comment}
   - User study assessing appeal vs. 3D and vs. flat cardboard character.
    - 3 characters
    - A B C side-by-side comparison 
\end{comment}

\paragraph{Twisted Perspective Retargeting}
We provide examples of a retargeted pose sequence with single versus multiple projection planes (see video).
When the axis of the skeleton's left lower arm approaches the root-view vector, the arm flails suddenly (Figure~\ref{fig:twisted_perspective} top right). 
This is corrected (Figure~\ref{fig:twisted_perspective} bottom right) by automatically selecting a more suitable projection plane (Section~\ref{ComputeRigJointLocations}).

\paragraph{Limb Swapping}
When a skeleton's 3D joint positions are projected onto a 2D plane, the relative position of the limb joints depends upon the skeleton's transversal rotation. 
When facing the camera, the skeleton's left limbs appear to the camera's right side; when facing away, this is reversed. 
Flipping the limb mapping (Section~\ref{sec:retarget-pose}) therefore serves to keep the pose recognizable (Figure~\ref{fig:twisted_perspective}).



\begin{comment}
    
Rotating the character to always face the viewer can result in unforeseen effects as the orientation of the input pose changes.
Consider character retargeted to display a T-pose facing directly towards the camera.
In this configuration, the pose's left arm drives the character's 'drawing right' arm. 
Now consider if we were to rotate the pose by $\pi$, such that it now faces away from us. If the pose's left arm were to continue driving the same arm upon the character, it would now go across the body.

This is because the left arm, as we rotate the pose, eventually lies to the right of the right arm after projection.
\end{comment}


\subsection{Comparisons}
In the accompanying video, we provided comparisons against Smith et al.~\cite{SmithHodgins} and \textit{Photo Wake-Up}~\cite{weng2019photo}.
Smith et al.'s output is inherently 2D and does not account for incongruous character orientation cues, such as foot direction. 
Photo Wake-Up creates a textured 3D object, which may fundamentally alter the style of abstract characters, such as those in Picasso paintings.

In Figure~\ref{fig:genai}, we compare novel views of a character generated with our approach to those generated by a state-of-the-art single image to multi-view diffusion model~\cite{shi2023zero123} trained primarily on visually realistic data, revealing its 3D bias.

%\subsubsection{Smith et al.}
%\begin{comment}
%Compare against previous by showing walking in circle, then what it looks like with previous work; then what it looks like now
%\end{comment}


%\subsubsection{Character animation from 2D pictures and 3D motion data (Hornung et al.)}

\section{Applications}
Our system is easily accessible to non-technical users, as it requires only a single drawing as input and a set of high-level semantic annotations that are intuitively understandable. Most drawings we tested can be annotated in under three minutes even with a duck-taped together interface. Creating the 2.5D model from the annotations then takes under ten seconds on a Macbook Pro, allowing for rapid iteration.

Once the 2.5D model is generated, it can be used in a variety of different context. For example, we can animate it with any 3D motion capture data. We can then render the retargeted character on the original drawing with a fixed camera to create a 2D animated story (Figure~\ref{fig:applications}.b), or we can render it in 3D to create a stylized animated shorts with a dynamic camera (Figure~\ref{fig:applications}.a,.d). 

Our system can be combined with any technique that generates 3D skeletal motion as output. As an example, we retargeted the human motion generated by a text-to-motion model~\cite{Qian_2023_ICCV} on a figure drawing as an inspiration for story telling (Figure~\ref{fig:applications}.c). W can also puppeteer characters with any real time body tracking algorithms, even in VR~\cite{10.1145/3550469.3555411} (Figure~\ref{fig:applications}.e), as the retargeting algorithm runs comfortably at over 30fps on an M1 Macbook Pro.

%Because our method runs in real-time, is view-dependent, uses 3D skeletal motion as input, and generates 2.5D output, it can be used in a variety of different contexts.
%The output animations can be inserted into a full 3-D scene (Figure~\ref{fig:applications}.a), but can be used in 2D settings equally well (Figure~\ref{fig:applications}.b).
%Because the model is driven by 3D skeletal motion, any modality that can specify such motion may be used.
%As examples, we present animations generated from text-to-motion models~\cite{Qian_2023_ICCV} (Figure~\ref{fig:applications}.c) as well as prerecorded motion capture (Figure~\ref{fig:applications}.d).

%Because the retargeting is fast (achieving un-optimized frame rates over 30FPS on a Macbook M1 Pro), it is well suited for puppeteering with VR body tracking~\cite{10.1145/3550469.3555411} (Figure~\ref{fig:applications}.e).

The view-dependent element of our model makes it particularly well-suited for mixed reality applications, in which the user has control over the camera position. We demonstrate this by creating a mixed reality application allowing users to view and pick up characters (Figure~\ref{fig:applications}.f).








%-Mixed reality




%We demonstrate our system's results with a variety of characters, motions, and application types.
%With proper interfaces, the necessary annotations can be generated in a few minutes.
%Creating the 2.5D character assets takes only a few seconds, which is conducive to rapid iteration.
%Retargeting is fast enough to support real-time puppeteering applications, and any form of input that can generate 3D skeletal motion (such as text-to-speech models) can be used.
%\yuting{This is the "results" section, so we talk about applications that we have built. We need to explain the results in each item, not what they could be.}

%textit{Mixed Reality.} The view-dependent aspect of our solution is ideal for mixed reality applications. To show this, we built a real-time, mixed reality puppeteering demo where the characters exist within the user's space. 

%\textit{2D Animations.} Because the character remains planar, it is well-suited for use in traditional 2D animation settings. In this case, the primary benefit of our approach is that complex 3D motions can be recognizably applied to character.

%\textit{Games.} Our system could be used to easily generate assets for 2.5D games, such as \textit{Paper Mario}, during game development.
%Or it could be incorporated into unique gameplay mechanisms, such as requiring users to draw characters with specific proportions to solve puzzles.







