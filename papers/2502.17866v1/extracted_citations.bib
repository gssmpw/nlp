@article{ArtiSketch,
author = {Levi, Zohar and Gotsman, Craig},
title = {ArtiSketch: A System for Articulated Sketch Modeling},
journal = {Computer Graphics Forum},
volume = {32},
number = {2pt2},
pages = {235-244},
doi = {https://doi.org/10.1111/cgf.12043},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12043},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12043},
abstract = {Abstract We present ArtiSketch – a system which allows the conversion of a wealth of existing 2D content into 3D content by users who do not necessarily possess artistic skills. Using ArtiSketch, a novice user may describe a 3D model as a set of articulated 2D sketches of a shape from different viewpoints. ArtiSketch then automatically converts the sketches to an articulated 3D object. Using common interactive tools, the user provides an initial estimate of the 3D skeleton pose for each frame, which ArtiSketch refines to be consistent between frames. This skeleton may then be manipulated independently to generate novel poses of the 3D model.},
year = {2013}
}

@article{DBLP:journals/corr/abs-2103-15472,
  author       = {Tsukasa Fukusato and
                  Akinobu Maejima},
  title        = {View-Dependent Formulation of 2.5D Cartoon Models},
  journal      = {CoRR},
  volume       = {abs/2103.15472},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.15472},
  eprinttype    = {arXiv},
  eprint       = {2103.15472},
  timestamp    = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-15472.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Dvoroznak18-SIG,
author = {Dvoro\v{z}n\'{a}k, Marek and Li, Wilmot and Kim, Vladimir G. and S\'{y}kora, Daniel},
title = {Toonsynth: Example-Based Synthesis of Hand-Colored Cartoon Animations},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201326},
doi = {10.1145/3197517.3201326},
abstract = {We present a new example-based approach for synthesizing hand-colored cartoon animations. Our method produces results that preserve the specific visual appearance and stylized motion of manually authored animations without requiring artists to draw every frame from scratch. In our framework, the artist first stylizes a limited set of known source skeletal animations from which we extract a style-aware puppet that encodes the appearance and motion characteristics of the artwork. Given a new target skeletal motion, our method automatically transfers the style from the source examples to create a hand-colored target animation. Compared to previous work, our technique is the first to preserve both the detailed visual appearance and stylized motion of the original hand-drawn content. Our approach has numerous practical applications including traditional animation production and content creation for games.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {167},
numpages = {11},
keywords = {skeletal animation, style transfer}
}

@article{Dvoroznak20-SA,
author = {Dvoro\v{z}\v{n}\'{a}k, Marek and S\'{y}kora, Daniel and Curtis, Cassidy and Curless, Brian and Sorkine-Hornung, Olga and Salesin, David},
title = {Monster Mash: A Single-View Approach to Casual 3D Modeling and Animation},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417805},
doi = {10.1145/3414685.3417805},
abstract = {We present a new framework for sketch-based modeling and animation of 3D organic shapes that can work entirely in an intuitive 2D domain, enabling a playful, casual experience. Unlike previous sketch-based tools, our approach does not require a tedious part-based multi-view workflow with the explicit specification of an animation rig. Instead, we combine 3D inflation with a novel rigidity-preserving, layered deformation model, ARAP-L, to produce a smooth 3D mesh that is immediately ready for animation. Moreover, the resulting model can be animated from a single viewpoint --- and without the need to handle unwanted inter-penetrations, as required by previous approaches. We demonstrate the benefit of our approach on a variety of examples produced by inexperienced users as well as professional animators. For less experienced users, our single-view approach offers a simpler modeling and animating experience than working in a 3D environment, while for professionals, it offers a quick and casual workspace for ideation.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {214},
numpages = {12},
keywords = {casual animation, 2D-to-3D inflation, sketch-based modelling}
}

@article{Hornung2007anim2Dpicmotion,
author = {Hornung, Alexander and Dekkers, Ellen and Kobbelt, Leif},
title = {Character Animation from 2D Pictures and 3D Motion Data},
year = {2007},
issue_date = {January 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/1189762.1189763},
doi = {10.1145/1189762.1189763},
abstract = {This article presents a new method to animate photos of 2D characters using 3D motion capture data. Given a single image of a person or essentially human-like subject, our method transfers the motion of a 3D skeleton onto the subject's 2D shape in image space, generating the impression of a realistic movement. We present robust solutions to reconstruct a projective camera model and a 3D model pose which matches best to the given 2D image. Depending on the reconstructed view, a 2D shape template is selected which enables the proper handling of occlusions. After fitting the template to the character in the input image, it is deformed as-rigid-as-possible by taking the projected 3D motion data into account. Unlike previous work, our method thereby correctly handles projective shape distortion. It works for images from arbitrary views and requires only a small amount of user interaction. We present animations of a diverse set of human (and nonhuman) characters with different types of motions, such as walking, jumping, or dancing.},
journal = {ACM Trans. Graph.},
month = jan,
pages = {1–es},
numpages = {9},
keywords = {3D motion data, as-rigid-as-possible shape manipulation with perspective correction, 2D character animation, camera and model pose determination}
}

@article{SmithHodgins,
author = {Smith, Harrison Jesse and Zheng, Qingyuan and Li, Yifei and Jain, Somya and Hodgins, Jessica K.},
title = {A Method for Animating Children’s Drawings of the Human Figure},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592788},
doi = {10.1145/3592788},
abstract = {Children’s drawings have a wonderful inventiveness, creativity, and variety to them. We present a system that automatically animates children’s drawings of the human figure, is robust to the variance inherent in these depictions, and is simple and straightforward enough for anyone to use. We demonstrate the value and broad appeal of our approach by building and releasing the Animated Drawings Demo, a freely available public website that has been used by millions of people around the world. We present a set of experiments exploring the amount of training data needed for fine-tuning, as well as a perceptual study demonstrating the appeal of a novel twisted perspective retargeting technique. Finally, we introduce the Amateur Drawings Dataset, a first-of-its-kind annotated dataset, collected via the public demo, containing over 178,000 amateur drawings and corresponding user-accepted character bounding boxes, segmentation masks, and joint location annotations.},
journal = {ACM Trans. Graph.},
month = {jun},
articleno = {32},
numpages = {15},
keywords = {motion stylization, 2D animation, Skeletal animation, motion retargeting}
}

@inproceedings{feng2017magictoon,
  title={Magictoon: A 2d-to-3d creative cartoon modeling system with mobile ar},
  author={Feng, Lele and Yang, Xubo and Xiao, Shuangjiu},
  booktitle={2017 IEEE Virtual Reality (VR)},
  pages={195--204},
  year={2017},
  organization={IEEE}
}

@incollection{igarashi2006teddy,
  title={Teddy: a sketching interface for 3D freeform design},
  author={Igarashi, Takeo and Matsuoka, Satoshi and Tanaka, Hidehiko},
  booktitle={ACM SIGGRAPH 2006 Courses},
  pages={11--es},
  year={2006}
}

@inproceedings{koyama2013view,
  title={View-dependent control of elastic rod simulation for 3D character animation},
  author={Koyama, Yuki and Igarashi, Takeo},
  booktitle={Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
  pages={73--78},
  year={2013}
}

@inproceedings{lingens2020towards,
  title={Towards automatic drawing animation using physics-based evolution},
  author={Lingens, Lasse and Sumner, Robert W and Magnenat, St{\'e}phane},
  booktitle={Proceedings of the 2020 ACM Interaction Design and Children Conference: Extended Abstracts},
  pages={314--319},
  year={2020}
}

@inproceedings{poursaeed2020neural,
  title={Neural puppet: Generative layered cartoon characters},
  author={Poursaeed, Omid and Kim, Vladimir and Shechtman, Eli and Saito, Jun and Belongie, Serge},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  pages={3346--3356},
  year={2020}
}

@inproceedings{rademacher1999view,
  title={View-dependent geometry},
  author={Rademacher, Paul},
  booktitle={Proceedings of the 26th annual conference on Computer graphics and interactive techniques},
  pages={439--446},
  year={1999}
}

@article{rivers25Dcartoonmodels,
author = {Rivers, Alec and Igarashi, Takeo and Durand, Fr\'{e}do},
title = {2.5D Cartoon Models},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/1778765.1778796},
doi = {10.1145/1778765.1778796},
abstract = {We present a way to bring cartoon objects and characters into the third dimension, by giving them the ability to rotate and be viewed from any angle. We show how 2D vector art drawings of a cartoon from different views can be used to generate a novel structure, the 2.5D cartoon model, which can be used to simulate 3D rotations and generate plausible renderings of the cartoon from any view. 2.5D cartoon models are easier to create than a full 3D model, and retain the 2D nature of hand-drawn vector art, supporting a wide range of stylizations that need not correspond to any real 3D shape.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {59},
numpages = {7},
keywords = {vector art, cartoons, animation, interpolation, non-photorealistic rendering, billboards}
}

@inproceedings{weng2019photo,
  title={Photo wake-up: 3d character animation from a single photo},
  author={Weng, Chung-Yi and Curless, Brian and Kemelmacher-Shlizerman, Ira},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5908--5917},
  year={2019}
}

