\documentclass{article}
\usepackage{xcolor}
\usepackage{graphicx}
\definecolor{darkgreen}{rgb}{0.1, 0.5, 0.1}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{array}

\begin{document}

\begin{table}[t]
% \centering
\raggedright
\renewcommand{\arraystretch}{2}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{|p{4cm}|p{4cm}|p{4cm}|p{4cm}|}
\hline
\textbf{{\fontsize{12pt}{22pt}\selectfont Comparison with Human Reviews}} & \textbf{{\fontsize{12pt}{22pt}\selectfont Factual \newline Accuracy}} & \textbf{{\fontsize{12pt}{22pt}\selectfont Analytical Depth}} & \textbf{{\fontsize{12pt}{22pt}\selectfont Actionable \newline Insights}} \\ \hline

{\fontsize{12pt}{22pt}\selectfont 
\textcolor{blue}{``The explanation of the results is adequate''} (AI) vs. \textcolor{darkgreen}{``The explanation is too brief and misses key statistical trends in Figure 3, such as the anomaly at epoch 50''} (human).} & 
{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``The paper uses supervised learning techniques effectively''} (AI), but the actual technique described is reinforcement learning.} & 
{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``The methodology section is sufficient''} (AI), without noting that \textcolor{darkgreen}{``The comparison to baseline models lacks clarity, especially in explaining the choice of hyperparameters''} (human).} & 
{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``Provide more examples for better understanding''} (AI), instead of \textcolor{darkgreen}{``Add examples demonstrating how the algorithm performs under different lighting conditions to clarify its robustness''}} \\ \hline

{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``Overall, the related work section is relevant''} (AI) vs \textcolor{darkgreen}{``The related work section does not include recent advancements in transformer-based architectures, such as XYZ-2023''} (human)} & 
{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``The dataset appears to be balanced''} (AI), but the dataset is actually imbalanced based on the class distributions mentioned in Section 4.2} & 
{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``The discussion is clear''} (AI), but it misses feedback like \textcolor{darkgreen}{``The discussion should explore why the proposed approach underperforms on Dataset B, as highlighted in Table 2''} (human)} & 
{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``Clarify the introduction''} (AI), rather than \textcolor{darkgreen}{``Reorganize the introduction to define the problem before introducing the contributions, as this will improve flow and reader engagement''}} \\ \hline

{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``The conclusion is well-written''} (AI) vs. \textcolor{darkgreen}{``The conclusion does not address limitations, such as the small sample size used in the experiments''} (human)} & 
{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``The results suggest strong performance''} (AI), but it incorrectly claims \textcolor{darkgreen}{``The model outperforms all baselines,''} while Table 3 shows it underperforms in some metrics} & 
{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``Results are promising''} (AI), lacking human feedback such as \textcolor{darkgreen}{``Consider expanding on the implications of your findings for real-world applications, particularly in autonomous navigation''}} & 
{\fontsize{12pt}{22pt}\selectfont \textcolor{blue}{``Improve the figures for better clarity''} (AI), rather than \textcolor{darkgreen}{``Increase the font size in Figure 4 and add units to the axes labels for better readability''}} \\ \hline
\end{tabular}
% \caption{Concrete Examples of Challenges in AI-Generated Reviews with Color Coding}
\label{tab:review_challenges}
\end{table}

\end{document}


%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
\usepackage[review]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{algorithm}
\usepackage{algorithmicx}
% \usepackage{algorithmic}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage[noend]{algpseudocode} % For pseudocode
\usepackage{amsmath} % For math environments



% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}


% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{tcolorbox}
%\usepackage{geometry}
% \geometry{margin=1in}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{makecell}
\newcommand{\cmark}{\ding{51}}  % Checkmark
\newcommand{\xmark}{\ding{55}}  % Crossmark
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{ReviewEval: An Evaluation Framework for AI-Generated Reviews}
\title{ReviewEval: An Evaluation Framework for AI-Generated Reviews}

\begin{document}

%\twocolumn[
%\icmltitle{ReviewEval: An Evaluation Framework for AI-Generated Reviews}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

%\begin{icmlauthorlist}
%\icmlauthor{Firstname1 Lastname1}{equal,yyy}
%\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
%\icmlauthor{Firstname3 Lastname3}{comp}
%\icmlauthor{Firstname4 Lastname4}{sch}
%\icmlauthor{Firstname5 Lastname5}{yyy}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% % You may provide any keywords that you
% % find helpful for describing your paper; these are used to populate
% % the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

% \vskip 0.3in
% ]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\maketitle
\begin{abstract}
% We explain the design and implementation of our custom pipeline developed to generate comprehensive reviews for research papers using Large Language Models (LLMs). Our approach leverages Google's latest LLM, Gemini 1.5 Pro, orchestrated through a multi-stage process that includes prompt generation, iterative refinement via reflection loops, and final evaluation to ensure the production of high-quality reviews aligned with specific conference guidelines.
% The exponential growth of academic research has strained traditional peer review processes, prompting the exploration of Large Language Models (LLMs) for automated review generation. While LLMs demonstrate potential in research reviewing, their effectiveness remains limited by issues such as superficial critiques, hallucinations, and lack of actionable insights. Moreover, there has been limited research on developing comprehensive evaluation frameworks to access these LLM based paper reviewers. To address this, we propose a comprehensive evaluation framework that assesses AI-generated reviews across four key dimensions: (1) Alignment with Human Reviews, (2) Factual Accuracy, (3) Analytical Depth, and (4) Actionable Insights. Our goal is to enhance the reliability of AI-generated reviews through a comprehensive evaluation framework and establish standardized metrics for assessing AI-based review systems. Additionally, our approach introduces a novel conference-specific alignment mechanism, addressing the need for adaptable reviewers that cater to varying evaluation priorities across conferences, a gap not previously explored. Furthermore, we propose a self-refinement loop, enabling the LLM to iteratively improve its review prompts, thereby enhancing the quality and depth of AI-generated reviews.

%The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. While LLMs offer potential for automating this process, their current limitations include superficial critiques, hallucinations, and a lack of actionable insights. This research addresses these challenges by proposing a comprehensive evaluation framework for AI-generated reviews, encompassing alignment with human reviews, factual accuracy, analytical depth, and actionable insights.Furthermore, we introduce a novel conference/journal specific alignment mechanism to adapt LLM-generated reviews to the unique evaluation priorities of different conferences and journals. To enhance the quality of these reviews, we propose a self-refinement loop that iteratively improves the LLM's review prompts. This framework aims to establish standardized metrics for assessing AI-based review systems and enhance the reliability of AI-generated reviews in academic research.

The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. While large language model (LLMs) offer potential for automating this process, their current limitations include superficial critiques, hallucinations, and a lack of actionable insights. This research addresses these challenges by introducing a comprehensive evaluation framework for AI-generated reviews, that measures alignment with human evaluations, verifies factual accuracy, assesses analytical depth, and identifies actionable insights. We also propose a novel alignment mechanism that tailors LLM-generated reviews to the unique evaluation priorities of individual conferences and journals. To enhance the quality of these reviews, we introduce a self-refinement loop that iteratively optimizes the LLM's review prompts. Our framework establishes standardized metrics for evaluating AI-based review systems, thereby bolstering the reliability of AI-generated reviews in academic research.

%-----Murari----
% The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. While large language model (LLMs) offer potential for automating this process, their current limitations include superficial critiques, hallucinations, and a lack of actionable insights. This research addresses these challenges by introducing a comprehensive evaluation framework for AI-generated reviews, that measures alignment with human evaluations, verifies factual accuracy, assesses analytical depth, and identifies actionable insights. We also propose a novel alignment mechanism that tailors LLM-generated reviews to the unique evaluation priorities of individual conferences and journals. To enhance the quality of these reviews, we introduce a self-refinement loop that iteratively optimizes the LLM's review prompts. Our framework establishes standardized metrics for evaluating AI-based review systems, thereby bolstering the reliability of AI-generated reviews in academic research.\vspace{-7mm}


% Empirical results demonstrate that our approach significantly improves prompt quality and review rigor. This research advances AI-driven peer review by addressing key shortcomings in evaluation transparency, depth, and adaptability to domain-specific guidelines.
\end{abstract}

\section{Introduction}
The rapid growth of academic research, coupled with a shortage of qualified reviewers, has created an urgent need for scalable and high-quality peer review processes \cite{petrescu2022evolving, schulz2022future, checco2021ai}. Traditional peer review methods are under mounting pressure from the exponentially growing number of submissions, particularly in fields like artificial intelligence, machine learning, and computer vision. This has led to a growing interest in leveraging large language models (LLMs) to automate and enhance various aspects of the peer review process~\cite{robertson2023gpt4, liu2023reviewergpt}.\par

LLMs have shown remarkable potential in automating various natural language processing tasks, such as summarization, translation, and question-answering. However, their effectiveness in serving as reliable and consistent paper reviewers remains a significant challenge. The academic community is already experimenting with AI-assisted reviews, as evidenced by reports that 15.8\% of reviews for ICLR 2024 were generated with AI assistance~\cite{latona2024ai}. While this demonstrates the growing adoption of LLMs in peer review, concerns have been raised regarding their impact on the reliability and fairness of the review process. Specifically, papers reviewed by AI have been perceived to gain an unfair advantage, leading to questions about the integrity of such evaluations. Consequently, research into robust automated review generation systems is crucial, necessitating rigorous evaluation of AI generated reviews to address key challenges.~\cite{zhou2024llm} provide a comprehensive analysis of the use of commercial models, such as GPT-3.5 and GPT-4 \cite{achiam2023gpt}, as research paper reviewers. Their findings highlight key limitations, including the potential for mistakes due to either model hallucinations or an incomplete understanding of the material, as well as the inability to provide critical feedback comparable to human reviewers. Based on our preliminary experiments of using GPT-4 to generate reviews for research papers, we identified additional limitations in AI-generated reviews, including a lack of actionable insights and limited analytical depth, often characterized by generic and vague feedback. These shortcomings stem from the inherent tendency of LLMs to generate superficial reviews.

%\vspace{-2mm}

% The adoption of effective AI-based reviewing systems in the research communities addresses several critical needs \cite{tyser2024ai}: (i) they provide early feedback to authors on their work in progress, facilitating iterative improvements and enhancing the overall quality of research outputs; (ii) they enable conferences to manage the growing volume of submissions more efficiently, ensuring high-quality and timely reviews; and (iii) they contribute to quality control by reducing inconsistencies and potential biases in the peer-review process.

% Should we add more details about us gathering these challenges?

% \begin{table}[h]
%     \centering
%     \renewcommand{\arraystretch}{0.9} % Adjust row height for better readability
%     \begin{tabular}{l>{\centering\arraybackslash}m{2.5cm} >{\centering\arraybackslash}m{2.5cm}}
%         \toprule
%         & \makecell{\textbf{Wider} \\ \textbf{Metric Coverage}} & \makecell{\textbf{Black Box} \\ \textbf{Remedy}} \\
%         \midrule
%         \textbf{Our Framework} & \cmark & \cmark \\
%         \textbf{MARG} & \xmark & \xmark \\
%         \textbf{Aspect-Based} & \xmark & \xmark \\
%         \bottomrule
%     \end{tabular}
%     \caption{Comparison of Evaluation Methods. The evaluation metrics by \cite{d2024marg} (MARG) and \cite{zhou2024llm} (Aspect-Based Comparisons) focus on AI-human review similarity but overlook other key deficiencies in AI reviews. Their heavy reliance on LLMs for end-to-end evaluation creates a black-box system with limited transparency. Our framework addresses these gaps with a more comprehensive and interpretable approach.}
%     \label{tab:evaluation_comparison}
% \end{table}



\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth, trim=0cm 0cm 0cm 0cm, clip]{table-diagram.pdf} % Adjust width for good fit
    \caption{\small Examples of the challenges and limitations of AI based research paper reviews}
    % \vspace{0.1in} % Adds 0.1 inches of space after the caption
    \label{fig:challenges}
\end{figure}

% There has been limited existing research on developing evaluation metrics to evaluate AI based research paper reviews. \cite{d2024marg} proposed an automated metric to check for approximate matches between AI-generated and human-written review comments using GPT. While GPT-4 is employed iteratively to extract approximate matches and mitigate inconsistencies, it relies completely on GPT-4 for end-to-end evaluation, making the whole metric a black box and unreliable. \cite{zhou2024llm} paper examines aspect coverage
% and similarity to reference reviews through both automatic
% metrics and manual analysis, utilizing the ASAP dataset
% \cite{yuan2022can}, which annotates review sentences with
% labels such as summary, motivation, originality, soundness,
% substance, replicability, meaningful comparison, and clarity.
% However, relying on pre-defined aspect coverage under-
% mines the diversity and depth of AI and human reviews, therefore we recognized the need for a more nuanced and
% dynamic topic coverage metric

% \begin{table}[h]
%     \centering
%     \begin{tabular}{lcc}
%         \toprule
%         & \makecell{\textbf{Dynamic Topic} \\ \textbf{Coverage}} & \makecell{\textbf{Remedying} \\ \textbf{Black Box Nature}} \\
%         \midrule
%         \textbf{Our Evaluation Framework} & \cmark & \cmark \\
%         \textbf{MARG} & \xmark & \xmark \\
%         \textbf{Aspect Coverage Based} & \xmark & \xmark \\
%         \bottomrule
%     \end{tabular}
%     \caption{Comparison of Evaluation Methods}
%     \label{tab:evaluation_comparison}
% \end{table}

Existing research on evaluation metrics for AI-generated research paper reviews remains limited. For instance,~\cite{d2024marg} proposed an automated metric to evaluate approximate matches between AI-generated and human-written review comments using GPT-4~\cite{achiam2023gpt}. Although their method iteratively employs GPT-4 to extract approximate matches and mitigate inconsistencies, its complete reliance on GPT-4 renders the evaluation process a black box, thereby limiting transparency and raising concerns about reliability. Similarly, \cite{zhou2024llm} investigated the aspect coverage and similarity between AI and human reviews through a blend of automatic metrics and manual analysis. Their work leveraged the ASAP dataset \cite{yuan2022can}, which categorizes review sentences into predefined aspects—including summary, motivation, originality, soundness, substance, replicability, meaningful comparison, and clarity, to align AI and human reviews. However, beyond this AI-human comparison, their approach overlooks other critical dimensions where AI reviews may underperform, as highlighted in Figure~\ref{fig:challenges}.\par

% While GPT-4 is employed iteratively to extract approximate matches and mitigate inconsistencies, it relies entirely on GPT-4 for end-to-end evaluation, making the metric a black-box system with limited transparency and reliability. 

%\cite{zhou2024llm} examined aspect coverage and similarity between AI and human reviews through a combination of automatic metrics and manual analysis. Their study utilized the ASAP dataset~\cite{yuan2022can}, which categorizes review sentences into predefined aspects such as summary, motivation, originality, soundness, substance, replicability, meaningful comparison, and clarity, and based on these aspects, matches AI and human reviews together. However, apart from the AI-Human comparison, they are overlooking other critical dimensions where AI reviews may fall short and require assessment (as described in figure \ref{fig:challenges}).

% this reliance on fixed aspect-based evaluation limits the framework’s ability to capture the diversity and depth of both AI and human reviews, thereby restricting its effectiveness in assessing nuanced differences in review quality.

Based on our analysis of the limitations in current AI-generated reviews and the gaps in existing evaluation metrics, we propose a comprehensive evaluation framework designed to assess the quality of AI-generated research paper reviews. Our framework targets four key dimensions (see Figure~\ref{fig:challenges}): \ding{182} \textit{Comparison with Human Reviews}: Evaluates topic coverage and semantic similarity to measure the alignment between AI-generated and human-written feedback. \ding{183}
\textit{Factual Accuracy}: Detects factual errors, including misinterpretations, incorrect claims, and hallucinated information. \ding{184} \textit{Analytical Depth}: Assesses whether the AI’s critique transcends generic commentary to offer in-depth, meaningful engagement with the research. \ding{185} \textit{Actionable Insights}: Measures the ability of the AI to provide specific, constructive suggestions for improving the paper.\par

Recognizing that major conferences and journals have distinct reviewing priorities, a one-size-fits-all approach to AI-driven reviews is insufficient. Recent studies \cite{bauchner2024use, biswas2024ai} underscore the growing importance of aligning reviews with conference-specific evaluation criteria—especially as many venues now require adherence to detailed reporting guidelines. To address this, we introduce a conference-specific AI reviewer that dynamically adapts its review strategy to meet the unique criteria of each target venue. 

Furthermore, inspired by the self-refinement approach of~\cite{madaan2023selfrefine}, our system incorporates a supervisor model that iteratively critiques and refines the instructional prompts guiding the review process. This self-refinement loop promotes deeper analytical assessments and mitigates the tendency of AI-generated reviews to offer only superficial feedback. In summary, our research aims to:
\begin{enumerate}
    \item Develop a comprehensive evaluation framework for LLM-based reviewing systems across five dimensions: (i) alignment with human reviews, (ii) factual accuracy, (iii) analytical depth, (iv) actionable insights, and (v) adherence to Reviewer guidelines.
    
    \item Create an LLM-based reviewer that dynamically aligns with the evaluation criteria of specific conferences/journals and continuously improves its reviewing strategy through an iterative refinement loop.
\end{enumerate}


% Therefore, based on our analysis of the limitations in AI-generated reviews and the gaps in existing research on evaluation metrics, we propose a comprehensive evaluation framework to evaluate the quality of AI-generated research paper reviews across four key dimensions (refer figure~\ref{fig:challenges}):
% \begin{enumerate}
% \item Comparison with Human Reviews: Analyzes topic coverage and semantic similarity to measure alignment between AI and human feedback.
% \item Factual Accuracy: Assesses the presence of factual errors, such as misinterpretations, incorrect claims, or hallucinated information in the reviews.
% \item Analytical Depth: Measures the depth and rigor of the AI’s critique, focusing on whether it moves beyond generic comments to meaningful engagement with the research.
% \item Actionable Insights: Evaluates the AI’s ability to provide specific, constructive feedback for improving the paper.
% \end{enumerate}

% \includegraphics[width=\textwidth, trim=0cm 2cm 0cm 2cm, clip]{yourfile.pdf}
% For example, in AI and machine learning, NeurIPS emphasizes theoretical advancements, ICLR prioritizes deep learning and representation learning, AAAI values practical AI applications, and ACL focuses on linguistic insights in NLP, while ICML covers a broad spectrum of machine learning methodologies and optimization techniques.

% Each major conference/journal has distinct reviewing priorities aligned with its focus areas, making a one-size-fits-all approach to AI-driven paper reviews inadequate. Existing research \cite{bauchner2024use, biswas2024ai} highlights the importance of this direction, noting that many conferences/journals now require authors to indicate adherence to specific reporting guidelines, and AI has the potential to be more effective than human reviewers in assessing such compliance. To address this, we propose a conference-specific AI reviewer, which dynamically aligns AI-generated reviews with the unique evaluation criteria of individual conferences/journals, an area that, to the best of our knowledge, has not been explored before. Additionally, we introduce a self-refinement loop, inspired by \cite{madaan2023selfrefine}, where a supervisor model iteratively critiques and refines the instructional prompts guiding the review process. This ensures deeper, more analytical assessments rather than the superficial feedback that often characterizes AI-generated reviews. 


% Our system dynamically generates review prompts by converting each specific guideline of a target conference into individual, step-by-step instructional prompts designed to guide the LLM through the paper reviewing process. 
% Hence, to achieve this we introduce a self-refinement framework inspired by \cite{madaan2023selfrefine}. Our approach employs a Supervisor LLM that iteratively critiques the review prompts based on clarity, coherence, and alignment with conference guideline. Empirical results show that this refinement process significantly enhances prompt quality (refer figure 2), leading to more rigorous and context-aware paper evaluations. 


% Ultimately, our approach aims to narrow the gap between automated and human-like review processes, improving the analytical depth of LLM-assisted peer review.



% Another key challenge in using LLMs for paper reviews is their tendency to generate shallow or generic feedback. The quality of AI-generated reviews heavily depends on the instructional prompts guiding the model. 
% Since these prompts serve as structured guidelines for navigating the paper, they must closely reflect the cognitive processes of human reviewers to ensure deeper analysis rather than superficial assessments. To achieve this, we introduce a self-refinement framework inspired by \cite{madaan2023selfrefine}. Our approach employs a Supervisor LLM that critiques the review prompts iteratively based on clarity, coherence, and alignment with conference guideline. Empirical results show that this refinement process significantly enhances prompt quality, leading to more rigorous and context-aware paper evaluations (refer figure 2). Ultimately, our approach aims to narrow the gap between automated and human-like review processes, improving the analytical depth of LLM-assisted peer review.


% Each major conference has distinct reviewing priorities that reflect their unique focus areas. For example, within the AI.ML domain, NeurIPS places significant emphasis on theoretical advancements and novelty, while ICLR centers on innovations in deep learning and representations. In contrast, AAAI values practical applications across diverse AI domains, and ACL focuses on linguistic insights in natural language processing. ICML, with its broad scope, prioritizes novel machine learning methodologies and optimization techniques. These varying priorities make a conference-specific LLM reviewer essential, as it tailors the evaluation process to the specific goals and expectations of each conference, ensuring relevant, context-sensitive feedback. Hence, to address the need for peer reviews tailored to the unique requirements of academic conferences, we propose a conference-specific research paper reviewer system powered by Large Language Models. This approach provides a novel solution to align AI-generated reviews with the evaluation criteria of individual conferences, a direction that to the best of our knowledge, has not been explored in prior research. Our system dynamically generates review prompts by converting each specific guideline of a target conference into individual, step-by-step prompts designed to guide the review generation process. These prompts ensure that the AI-generated reviews address all aspects of the conference's expectations in a structured and systematic manner. 

% A key challenge in using LLMs for research paper reviewing is their tendency to produce superficial and insufficiently analytical reviews. We hypothesize that the instructional prompts guiding the LLM play a crucial role in shaping the quality of the generated reviews. Since these prompts serve as structured guidelines for navigating the paper, they must closely reflect the cognitive processes of human reviewers to ensure deeper analysis rather than superficial assessments. To address this, we propose a self-refinement framework inspired by \cite{madaan2023selfrefine}, which enables LLMs to iteratively improve their outputs through feedback-based revision. Specifically, we introduce an iterative prompt refinement mechanism, where an LLM supervisor evaluates the initial prompts generated from conference guidelines. The supervisor provides constructive feedback based on clarity, coherence, and alignment with the guideline’s intent, which is then used by the original LLM to improve its initial prompts. Empirical observations indicate that this iterative refinement process significantly improves prompt quality. As shown in Figure 2, the supervisory LLM identifies major deficiencies, which are then fixed leading to more structured and insightful instructional guidance. This improvement directly impacts the depth and relevance of the LLM-generated reviews. Our approach aims to bridge the gap between automated and human-like review processes, enhancing the analytical rigor of LLM-based research reviewing.

% Furthermore, to address a major concern of using LLMs for research paper reviewing: LLMs generating superficial and insufficient reviews (as will be evaluated by our 3 out of 4 evaluation metrics: Comparison with human reviews, analytical depth, and actionable insights), we realize that the instructional prompts the Reviewer LLM follows plays a critical role in how the review will turn out to be. This instructional prompt plays a crucial role in helping the reviewer navigate the paper, hence this prompt needs to be as close to a human though process while reviewing a paper to ensure that the reviews come out to be less superficial and more analytical and intelligent. Hence, improving the quality of prompts used for reviewing is very important.

% To enhance the quality of our prompts that are generated using conference guidelines, we propose a self-refinement method, inspired by \cite{madaan2023selfrefine}, which suggests that LLMs are capable of revising their own outputs just like humans, through an iterative feedback and refinement
% loop to improve initial outputs from LLMs. The instructional prompt used to generate research reviews using LLM plays a critical role in how well the LLM operates as a Reviewer, and hence, these instructional prompts that are dynamically generated using the conference guidelines need to be refined iteratively to ensure they guide the "LLM as Reviewer" through the a well-defined instructional prompt. We employ a supervisory LLM that is instructed to give constructive feedback on the quality of the initial prompt generated for each guideline, this feedback is based on clarity and coherence of the prompt, its alignment to the guideline, etc. We noticed significant improvement in the quality of prompt generated through refinement loops, where we saw the supervisor LLM pointing out major issues like shown in figure 2. Through our work, we recommend a pipeline that proposes a solution to convert conference guidelines into individual instructional prompts that are refined through reflection loop to make them more comprehensive and "human-like". We aim to propose a system that generates prompts aligning closely with how the though process of a human reviewer will operate.

% To improve the quality and alignment of these prompts, we introduce a refinement loop using a secondary "Judge LLM." The Judge LLM evaluates each prompt for coherence, relevance, and consistency with the original guideline, suggesting refinements to better capture the intended requirements. By systematically aligning reviews with conference-specific guidelines, this framework provides a practical and customizable tool for researchers seeking precise, high-quality, and compliant reviews.

% \usepackage{enumitem} % Add this in the preamble

% In summary, the objectives of the research are listed below.
% \begin{enumerate}[itemsep=0mm, topsep=0mm, parsep=1mm, partopsep=1mm]
%     \item Provide a comprehensive evaluation framework for evaluating LLM based reviewing systems across 5 key dimensions: (1) Alignment with Human Reviews (2) Factual Correctness (3) Depth of Analysis (4) Actionable Insights and (5) Adherence to reviewer guidelines.
%     \item Develop an LLM based reviewer that dynamically aligns itself with target conference guidelines and improves its reviewing strategy iteratively through a refinement loop.
% \end{enumerate}

\begin{table*}[t]
    \centering
    \tiny
    \renewcommand{\arraystretch}{0.8} % Reduce row height slightly
    \begin{tabular}{l>{\centering\arraybackslash}m{2.5cm} >{\centering\arraybackslash}m{2.5cm}}
        \toprule
        & \shortstack{\textbf{Wider} \\ \textbf{Metric Coverage}} 
        & \shortstack{\textbf{Black Box} \\ \textbf{Remedy}} \\
        \midrule
        \cite{d2024marg} & \xmark & \xmark \\
        \midrule
        \cite{zhou2024llm} & \xmark & \xmark \\
        \midrule
        ReviewEval(ours) & \cmark & \cmark \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of Evaluation Methods. The evaluation metrics by \cite{d2024marg} (MARG) and \cite{zhou2024llm} (Aspect-Based Comparisons) focus on AI-human review similarity but overlook other key deficiencies in AI reviews. Their heavy reliance on LLMs for end-to-end evaluation creates a black-box system with limited transparency. Our framework addresses these gaps with a more comprehensive and interpretable approach.}
    \label{tab:evaluation_comparison}
\end{table*}
%\vspace{-2mm}

% In summary, the objectives of the research are listed below.
% \vspace{-5mm}
% \setlist{nolistsep}
% \begin{enumerate}[noitemsep]
% \item Develop an LLM-based system capable of providing detailed, constructive feedback on academic manuscripts.\vspace{-1mm}
% \item Align reviews with specific conference guidelines to ensure relevance.\vspace{-2mm}
% \item Develop an extensive evaluation framework to evaluate several aspects of these AI-generated reviews
% \end{enumerate}

% Furthermore, we identify the requirement of researchers that are targeting a specific research conference, therefore, we propose a system an LLM-based conference specific reviewer. To the best of our knowledge, no other research has effectively explored aligning AI reviews to specific conferences.

% We use the conference specific reviewer guidelines and convert them into individual prompts to generate the reviews. These prompts are instructured to be in format of step by step instructions for the reviewer to follow. These prompts then go through a refinement loop where a Judge LLM checks them coherence, relevance, and miscelaneous improvements to align the prompt/instruction better with the conference guideline it is representing. The final conference specific reviews are then outputted in latex format in a structured format.




% Recent advancements in techniques like meta-prompting and multi-agent systems provide potential solutions to these challenges. Meta-prompting allows LLMs to break down complex tasks into smaller subtasks, handled by expert instances with tailored instructions. This approach has the potential to significantly enhance the quality of AI-based reviews by improving specificity and reducing generic feedback. Similarly, utilizing multi-agent systems for review generation \cite{d2024marg} helps distribute review tasks across specialized LLM instances, enabling detailed and nuanced feedback through simulated internal discussions.

% This paper builds upon these developments, proposing a novel system that integrates iterative refinements, prompt engineering, and section-specific evaluations to create a scalable and effective LLM-based peer review assistant.



% Furthermore, based on our preliminary experiments using these commercial models, we further observed some more shortcomings, like the AI reviews lacking actionable insights as well as AI reviews having limited analytical depth with only generic comments, these all arising because llms have the ability to generate vague reviews. Hence, using the existing literature \cite{zhou2024llm} and our preliminary experiments, we propose an evaluation framework with the following (i) evaluating factual innacuracies (ii) Quantitative analysis and comparison of AI reviews and human reviews (such as topic coverage and semantic similarities) (iii) presence of actionable insights (iv) analysing the depth of analysis in AI reviews.



% To further explore the challenges faced by LLMs as reviewers, we conducted preliminary experiments (refer to Figure 1) using commercial models like GPT-4 and Gemini Pro, with basic prompting strategies (details in Appendix). The results revealed several significant shortcomings that hinder their practical application, including factual inaccuracies (hallucinations), inconsistent review quality relative to human evaluations, limited analytical depth, and a lack of actionable insights, thereby rendering these models ineffective for delivering meaningful feedback.


% \cite{zhou2024llm} conducts a comprehensive analysis of the use of commercial models like gpt-3.5 and gpt-4 as research paper reviewer, and found that its shortcomings are possibility of mistakes during reviewing (caused by either model hallucinations or inability to comprehensively understand the material) and that these models fail to give critical feedback like human reviewers.
% To get a better and in-depth grasp of the reviewer challenges, we also conduct som preliminary experiments (add figure 1) with commercial LLMs such as GPT-4 and Gemini Pro, using basic prompting strategies (see Appendix). It revealed significant shortcomings that currently limit their practical application. These include the generation of factual inaccuracies (hallucinations), inconsistent review quality compared to human evaluations, insufficient depth of analysis, and a lack of actionable insights, rendering such reviews ineffective for providing meaningful feedback.


% Talk about existing review systems

% More recently, their application in academic reviewing has garnered attention, with efforts to align automated reviews with conference-specific guidelines to ensure constructive and high-quality feedback. By reducing the workload on human reviewers, such systems aim to maintain or even improve the quality and consistency of evaluations across submissions.

% Despite their promise, the deployment of LLMs in peer review also introduces challenges. Large language models are prone to hallucinations, generating plausible-sounding but inaccurate information. They also demonstrate the power to persuade humans, which can amplify the risks of inaccuracies in reviews. Studies have shown that controlling the quality and appropriateness of LLM-augmented reviewing is a complex and ongoing challenge.



% The academic community has already begun experimenting with AI-assisted reviews. For instance, 15.8\% of reviews for ICLR 2024 were reportedly written with AI assistance \cite{latona2024ai}. While these developments demonstrate the viability of LLMs in academic reviewing, they also highlight the need for further research to address limitations such as factual accuracy, depth of analysis, and alignment with conference guidelines.

% AI-based reviews present unique advantages for the academic community. They can provide early feedback to authors, allowing them to refine their work before submission. Additionally, these systems can help conferences maintain timely and high-quality reviews despite increasing submission volumes. Lastly, they offer an opportunity for improved quality control of reviews, addressing inconsistencies and biases that may arise in human-only review processes.

% Recent advancements in techniques like meta-prompting and multi-agent systems provide potential solutions to these challenges. Meta-prompting allows LLMs to break down complex tasks into smaller subtasks, handled by expert instances with tailored instructions. This approach has the potential to significantly enhance the quality of AI-based reviews by improving specificity and reducing generic feedback. Similarly, utilizing multi-agent systems for review generation \cite{d2024marg} helps distribute review tasks across specialized LLM instances, enabling detailed and nuanced feedback through simulated internal discussions.


% This paper builds upon these developments, proposing a novel system that integrates iterative refinements, prompt engineering, and section-specific evaluations to create a scalable and effective LLM-based peer review assistant.

% The exponential growth in academic research has led to an increased demand for efficient and high-quality peer review processes. Large Language Models (LLMs) have demonstrated potential in automating various natural language processing tasks, including summarization, translation, and question-answering. This project explores their capability to automate the critical task of reviewing academic papers. By aligning reviews with conference-specific guidelines and generating detailed feedback, this LLM-based research assistant aims to reduce human workload while maintaining the quality and consistency of manuscript evaluations.

% The academic community acknowledges the acute need for
% having foundation models assist reviewing of papers at scale
% (Liu and Shah 2023; Robertson 2023; Petrescu and Krishen
% 2022; Schulz et al. 2022; Checco et al. 2021; Bao, Hong,
% and Li 2021; Vesper 2018; Latona et al. 2024; Kuznetsov
% et al. 2024), along with the risks involved (Kaddour et al.
% 2023; Spitale, Biller-Andorno, and Germani 2023; Zou et al.
% 2023). Previous work addresses the limitations of LLM’s
% ability to perform reviewing (Liu and Shah 2023) and their
% capabilities to review academic papers (Liang et al. 2023).
% Large language models demonstrate surprising creative capabilities in text (Koivisto and Grassini 2023), though they
% may hallucinate (Zhang et al. 2023), and demonstrate the
% power to persuade humans even when inaccurate (Spitale,
% Biller-Andorno, and Germani 2023). This makes controlling
% the quality and appropriateness of LLM-augmented reviewing highly challenging. At least 15.8\% of reviews for ICLR
% 2024 were written with AI assistance (Latona et al. 2024).
% Meta-prompting (Suzgun and Kalai 2024) uses multiple
% LLM instances for managing and integrating multiple independent LLM queries. 

% Utilizing meta-prompting, the LLM
% breaks down complex tasks into smaller subtasks handled
% by expert instances with tailored instructions, significantly
% enhancing performance across various tasks. This approach
% outperforms conventional prompting methods across multiple tasks, enhancing LLM functionality without requiring
% task-specific instructions. Multi-agent review generation for
% scientific papers (D’Arcy et al. 2024) improves LLM reviewing by using multiple instances of LLMs, providing
% more specific and helpful feedback by distributing the text
% across specialized agents that simulate an internal discussion. This reduces generic feedback and increases the generation of good comments. Recent work formulates the peerreview process as a multi-turn dialogue between the different
% roles of authors, reviewers, and decision-makers (Tan et al.
% 2024), and finds that both reviews (Latona et al. 2024) and
% meta-reviews written by LLMs (Santu et al. 2024) are preferred by humans over human reviews and meta-reviews.

% Why do the Artificial Intelligence, Machine Learning, and
% Computer Vision communities need AI-based reviews of papers? (i) AI-based reviews provide early feedback to authors
% for their work in progress, allowing authors to learn and improve their work; (ii) AI-based reviews would help conferences maintain high-quality and timely reviews for the increasing number of papers in these fields, (iii) For quality control of reviews
%\vspace{-3mm}
\section{Related Work}
\textbf{AI based scientific discovery.} The pursuit of automating scientific discovery has been a longstanding goal in artificial intelligence (AI) research. Early efforts in the 1970s introduced expert systems such as DENDRAL~\cite{buchanan1981dendral} and the automated mathematician~\cite{lenat1977automated}, which focused on constrained problem spaces like organic chemistry and theorem proving. More recent advancements in AI-driven research have leveraged LLMs and machine learning techniques to extend beyond structured domains. Notable contributions include AutoML approaches that optimize hyperparameters and architectures~\cite{hutter2019automated, he2021automl} and AI-driven discovery in materials science and synthetic biology~\cite{merchant2023ai, hayes2024simulating}. However, these methods remain largely dependent on human-defined search spaces and predefined evaluation metrics, limiting their potential for open-ended discovery. Recent works~\cite{lu2024ai} aim to automate the entire research cycle, encompassing ideation, experimentation, manuscript generation, and peer review, thus pushing the boundaries of AI-driven scientific inquiry.\par

\textbf{AI based peer-review.} Recent advancements in AI-driven peer review have explored structured, multi-agent approaches for automated evaluation.~\cite{lu2024ai} employ LLMs to autonomously conduct the research pipeline, including peer review. It follows a structured three-stage review process: paper understanding, criterion-based evaluation (aligned with NeurIPS and ICLR guidelines), and final synthesis: assigning scores to key aspects like novelty, clarity, and significance. Evaluation shows its reviews closely match human meta-reviews (2.3/5 vs 2.5/5). MARG~\cite{d2024marg} introduces a multi-agent framework where worker agents review sections, expert agents assess specific aspects, and a leader agent synthesizes feedback. Using BERTScore~\cite{bert-score} and GPT-4-based evaluation, MARG-S improves feedback quality, reducing generic comments (60\% to 29\%) and increasing helpful feedback per paper (1.7 to 3.7). A user study found 47\% of MARG-S feedback helpful, outperforming baseline models (21\%). These studies highlight the potential of AI to enhance peer review through structured automation and multi-agent collaboration.\par

% \textbf{AI based peer-review.} Recent advancements in AI-driven peer review have explored structured, multi-agent approaches for automated evaluation.~\cite{lu2024ai} employ LLMs to autonomously conduct the research pipeline, including peer review. It follows a structured three-stage review process: paper understanding, criterion-based evaluation (aligned with NeurIPS and ICLR guidelines), and final synthesis: assigning scores to key aspects like novelty, clarity, and significance. Evaluation shows its reviews closely match human meta-reviews. MARG~\cite{d2024marg} introduces a multi-agent framework where worker agents review sections, expert agents assess specific aspects, and a leader agent synthesizes feedback. Using BERTScore~\cite{bert-score} and GPT-4-based evaluation, MARG-S improves feedback quality, reducing generic comments (60\% to 29\%) and increasing helpful feedback per paper (1.7 to 3.7). A user study found 47\% of MARG-S feedback helpful, outperforming baseline models (21\%). These studies highlight the potential of AI to enhance peer review through structured automation and multi-agent collaboration.\par


% \subsection{The AI Scientist (by Sakana AI)}
% \vspace{-1mm}
% \subsection{The AI Scientist (by Sakana AI)}
% \cite{lu2024ai} introduce The AI Scientist, an autonomous framework leveraging LLMs to conduct the full research pipeline, from ideation to peer review. The system generates research ideas using chain-of-thought reasoning and self-reflection, refines them via a Semantic Scholar API-assisted literature search, and executes experiments using Aider, an LLM-based coding assistant that iteratively debugs and refines implementations. To automate peer review, the system follows a structured process: (1) Paper Understanding, where the LLM summarizes key contributions, methodology, and results; (2) Criterion-Based Evaluation, assessing novelty, clarity, correctness, significance, and presentation using NeurIPS and ICLR guidelines; and (3) Final Review Synthesis, where individual evaluations are compiled into structured feedback with strengths, weaknesses, and improvement suggestions. The review system assigns numeric scores to each criterion, mimicking human assessment, and aggregates them into an overall Meta-Reviewer Score. AI-generated reviews were compared against NeurIPS 2021 Meta-Reviewer Scores, where human reviews averaged 2.5/5, while the AI system achieved 2.3/5, demonstrating near-human accuracy.

% \subsection{MARG: Multi-Agent Review Generation for Scientific Papers}
% \cite{d2024marg} propose MARG (Multi-Agent Review Generation) that employs multiple worker agents to review different sections, while expert agents specialize in key aspects like experiments, clarity, and impact. A leader agent coordinates the process, ensuring a structured and coherent review. The system uses hardcoded review prompts inspired by ICLR guidelines, which remain static across all review tasks. For evaluation, the authors compare MARG-generated reviews against NeurIPS human reviews using automated alignment metrics and a user study. BERTScore is used to measure semantic similarity between AI-generated and human-written reviews, capturing lexical and contextual overlap. Additionally, GPT-4 is used as a preference model, scoring reviews based on relevance, coherence, and completeness, simulating human judgment. The authors also conduct a user study with NLP and HCI researchers, who assess feedback helpfulness. MARG-S (a specialized agent variant) outperforms single-agent and naive multi-agent baselines, reducing generic feedback from 60\% to 29\% and increasing helpful comments per paper from 1.7 to 3.7. The user study reveals that 47\% of MARG-S feedback was rated as helpful, compared to 21\% from naive multi-agent models.



% introduces a comprehensive framework that automates the entire research pipeline, from idea generation to manuscript writing, using LLMs. A notable component is its foundation model-based automated reviewer, which evaluates generated manuscripts based on NeurIPS guidelines, providing scores and feedback akin to human reviewers. The framework achieves 65\% accuracy for final paper score prediction and demonstrates cost efficiency, but it is limited by its inability to adapt to specific conference guidelines. The paper also doesn't analyze and assess their AI generated reviews using meaningful metrics.
% \cite{d2024marg} presents a multi-agent framework (MARG) that generates peer-review feedback by distributing the task among multiple LLM agents. The system uses hardcoded review prompts inspired by ICLR guidelines, which remain static across all review tasks. MARG significantly reduces generic comments (from 60\% to 29\%) and generates more helpful feedback compared to baseline methods, as shown through a user study. The paper also proposes an automated review evaluation framework to check for approximate matches between AI-generated and human-written review comments using GPT. While GPT-4 is employed iteratively to extract approximate matches and mitigate inconsistencies, we argue that completely relying on GPT for end-to-end evaluation introduces unreliability and opacity. To address this, our evaluation framework emphasizes transparency, striving to develop interpretable metrics for comparing AI and human reviews, moving beyond black-box methodologies.
% \subsection{Is LLM a reliable reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks}
\textbf{Evaluation framework for AI based research paper reviews.} There has been limited research on developing evaluation frameworks for evaluating the quality of LLM generated paper reviews.~\cite{zhou2024llm} evaluated GPT models for research paper reviewing across 3 tasks: aspect score prediction, review generation, and review-revision MCQ answering. Using the PeerRead dataset, they assessed LLM's ability to predict review scores based on clarity, originality, and soundness (aspect score prediction). For review generation, they employed the ASAP dataset \cite{yuan2022can} and tested zero-shot, few-shot, and aspect-guided structured prompting, where reviews were generated with explicit tagging for summary, originality, and clarity. The study developed evaluation framework comprising of aspect coverage, ROUGE (lexical overlap), BERTScore (semantic similarity), and BLANC (informativeness), alongside manual analysis. Aspect coverage measured how well the generated reviews addressed key aspects (originality, soundness, substance, replicability, etc.) when compared to the distribution in reference expert reviews. Results showed LLMs overemphasized positive feedback, lacked critical depth, and neglected substance and clarity, despite high lexical similarity to human reviews. \cite{d2024marg} introduced an automated evaluation framework for AI-generated reviews, quantifying similarity to human reviews via recall, precision, and Jaccard index. Their multi-stage GPT-4 \cite{achiam2023gpt} based alignment process first matches semantically equivalent comments using five randomized passes (many-many matching), retaining pairs appearing in at least two runs. Each pair is re-evaluated based on relatedness and relative specificity. Recall measures the fraction of real-reviewer comments with at least one AI match, precision quantifies AI comments aligned with human reviews, and Jaccard index evaluates the intersection-over-union of aligned comments.

% \cite{zhou2024llm} conducted a systematic evaluation of GPT-3.5 and GPT-4 for research paper reviewing task, focusing on three key tasks: aspect score prediction, review generation, and review-revision multiple-choice question answering (RR-MCQ). For aspect score prediction, the study used the PeerRead dataset (ICLR-2017 subset) to assess LLM's ability to infer review scores based on predefined criteria such as clarity, originality, and soundness. For review generation, \cite{zhou2024llm} used the ASAP dataset \cite{yuan2022can} (ICLR-2020 subset), which consists of peer reviews labeled by aspects such as originality, soundness, and clarity. Various prompting strategies—zero-shot, few-shot, and aspect-guided structured generation—were tested to improve review quality. Aspect-guided structured generation organizes reviews by characteristics like summary, originality, and clarity, using models to generate sentences tagged with corresponding aspects. The generated reviews were evaluated using automated metrics and human annotations. Among automated metrics, the study used aspect coverage to measure how well the generated reviews addressed key aspects of the paper, comparing the distribution of positive and negative comments with reference reviews. The results showed that LLMs overemphasized positive feedback, often failing to provide critical assessments. Certain aspects, such as substance and clarity, were frequently neglected, while models disproportionately favored broad statements over precise critiques. Other automated metrics included ROUGE (lexical overlap), BERTScore (semantic similarity), and BLANC (informativeness via a blank-filling task). While these metrics indicated high similarity between LLM-generated and human-written reviews, they failed to capture deficiencies in critical feedback and technical depth. \cite{d2024marg} proposed an automated evaluation framework with the following metrics used to quantify the AI generated reviews using the corresponding human (expert) review comments: Recall, Precision, and Jaccard Index. The paper employs a multi-stage GPT-4-based process to measure the overlap between generated and real reviews by identifying semantically equivalent comments. In the Many-Many Matching Stage, all comments from both sets are input into GPT-4, which performs five passes with randomized comment orders to mitigate inconsistencies. A comment pair is retained if it appears in at least two of five runs. In the Pairwise Stage, each identified pair is re-evaluated, where GPT-4 assigns two scores: Relatedness ("none," "weak," "medium," or "high") and Relative Specificity ("less," "same," or "more"). A match is confirmed if relatedness is at least "medium" and specificity is "same" or "more." This structured alignment enables the calculation of recall, precision, and Jaccard index for assessing review similarity. Recall is the fraction of real-reviewer comments that are aligned to any generated comment (total number of human reviews that have atleast 1 match with AI reviews divided by total number of human reviews). Precision is the fraction of generated comments that are aligned to any real-reviewer comment (total number of AI reviews that have atleast 1 match with human reviews divided by total AI reviews). Jaccard Index is calculated by dividing the intersection of generated comments and real-reviewer comments by the size of the union of the generated comments and real-reviewer comments less the intersection. The Jaccard index measures the similarity between the set of generated comments and the set of real-reviewer comments



% \cite{d2024marg} evaluates the MARG-generated reviews against NeurIPS human reviews using automated alignment metrics and a user study. BERTScore is used to measure semantic similarity between AI-generated and human-written reviews, capturing lexical and contextual overlap. Additionally, GPT-4 is used as a preference model, scoring reviews based on relevance, coherence, and completeness, simulating human judgment. They perform automated evaluation using GPT-4 to extract approximate matches between AI reviews and expert reviews, they run GPT inference iteratively to extract approximate matches.The authors also conduct a user study with NLP and HCI researchers, who assess feedback helpfulness.


% conducts a comprehensive evaluation of large language models (LLMs), specifically GPT-3.5 and GPT-4, in scientific paper reviewing, focusing on their ability to generate review texts. The study examines aspect coverage and similarity to reference reviews through both automatic metrics and manual analysis, utilizing the ASAP dataset \cite{yuan2022can}, which annotates review sentences with labels such as summary, motivation, originality, soundness, substance, replicability, meaningful comparison, and clarity. However, relying on pre-defined aspect coverage undermines the diversity and depth of AI and human reviews, therefore we recognized the need for a more nuanced and dynamic topic coverage metric.
% Papers are split into chunks, processed by worker agents, and analyzed by expert agents specializing in clarity, experiments, or impact, with a leader agent coordinating communication and combining feedback. The refinement stage focuses on improving the generated reviews for clarity, specificity, and validity. 


% \subsection{Existing work}


% % Prior work on LLM academic capabilities suggests that LLMs are now ready for specific reviewing tasks and appear to be more effective for some academic domains and less effective for others (Checco et al.
% % 2021; Schulz et al. 2022; Liu and Shah 2023; Lu et al. 2024)

% % \subsection{Current Applications of LLMs}

% % LLMs are widely used for tasks like language translation, summarization, and question-answering. Their potential for critical assessment of new research remains largely untapped.

% \subsection{Existing Automated Review Systems}

% Sakana AI Scientist: A system performing end-to-end research activities (literature review, hypothesis formulation, experimentation, manuscript writing).

% OpenReviewer: Utilizes GPT-4 for academic paper reviews.

% \subsection{Gaps in Current Systems}

% Existing solutions either aim to generalize across the entire research workflow or lack the depth needed for nuanced peer reviews.



% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines. Below is an example table.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

%\vspace{-3mm}

%Our experiments utilized 16 papers and their corresponding expert reviews (scraped from OpenReview.net), alongside AI-generated reviews. 

\section{ReviewEval}
We describe the development of our evaluation framework and the LLM-based research paper reviewer. Each review is evaluated on several key parameters to assess the overall quality of the generated feedback. To ensure consistency and reliability, all evaluations for a given metric were performed by LLMs of the same specification and version, thereby maintaining inter-rater reliability and ensuring robust, unbiased comparisons. Our experiments utilized 16 papers and their corresponding expert reviews (scraped from OpenReview.net), alongside AI-generated reviews.\par

%We describe the development of our evaluation framework and the development of our LLM based research paper reviewer. We tested 16 papers and reviews from experts (scraped from openreview.net) alongside AI-generated reviews across several parameters. First, we measured the \textit{alignment between AI-generated and expert reviews} using the cosine similarity of word embeddings. Next, we assessed \textit{topic coverage} by evaluating how comprehensively AI-generated reviews addressed the breadth of topics present in expert reviews using large language models. We also evaluated \textit{factual correctness} through a multi-agent fact-checking framework to verify the accuracy of the claims made. Additionally, we examined \textit{constructiveness} by determining whether the reviews provided actionable insights and recommendations, and finally, we measured the \textit{depth of analysis} by assessing the critical engagement of the reviews with the manuscript's content. To ensure consistency and reliability, all evaluations for a given metric were performed by LLMs of the same specification and version. This approach maintained inter-rater reliability across all samples, ensuring that comparisons and assessments were robust and unbiased.\par

% We tested the 16 papers and reviews from experts (scraped from openreview.net) and AI on the following parameters:
% \item \textbf{Comparing against Expert Reviews} Measured alignment between AI-generated and expert reviews using cosine similarity of word embeddings.
% \item \textbf{Topic Coverage:} Assessed how comprehensively AI-generated reviews covered the breadth of topics in expert reviews using LLMs.
% \item \textbf{Factual Correctness:} Evaluated the accuracy of claims in the reviews through a multi-agent fact-checking framework.
% \item \textbf{Constructiveness:} Determined whether reviews provided actionable insights and recommendations.
% \item \textbf{Depth of Analysis:} Assessed the critical engagement of reviews with the manuscript’s content.

% Adherence to Guidelines: Checked compliance with specific conference review criteria.

%We compared the reviews generated by SakanaAI scientist's review bot with those from our own reviewer bot on the selected dataset.

\subsection{Comparison with Expert Reviews}
We compare the reviews generated by the LLM based reviewer with expert reviews from OpenReview.net. Our primary goal is to gauge how well the AI system replicate or complement expert-level critique. The evaluation is conducted along the following dimensions:

\textbf{Semantic similarity.} 
To assess the alignment between AI-generated and expert reviews, we embed each review \( R \) into a vector space using the OpenAI 
embedding model. The semantic similarity between an AI-generated review \( R_{\text{AI}} \) and an expert review \( R_{\text{Expt}} \) is measured using cosine similarity:
\begin{equation}
S_{\text{sem}}(R_{\text{AI}}, R_{\text{Expt}}) = \frac{e(R_{\text{AI}}) \cdot e(R_{\text{Expt}})}{\|e(R_{\text{AI}})\| \, \|e(R_{\text{Expt}})\|}
\end{equation}

where \( e(R) \) denotes the embedding of review \( R \). A higher cosine similarity indicates a stronger alignment between the AI-generated and expert reviews.\par

% To assess the semantic similarity between AI-generated reviews and expert reviews, we measure the cosine similarity between the word embeddings which are generated by OpenAI Embedding Model. Each review $R$ was embedded as a vector, capturing semantic nuances of the text. We then calculated cosine similarity scores between the expert review embedding and each of the AI-generated review embeddings. It helps us measures the alignment between the AI and expert reviews in the embedding space, providing a numerical representation of their contextual similarity. Higher cosine similarity scores indicate a stronger alignment between the AI-generated content and expert perspectives.

% \begin{equation}
% S_{\text{semantic}} = \frac {e_{R_{AI}}.e_{R_{expert}}} {||e_{R_{AI}}|| ||e_{R_{expert}}||}
% \end{equation}

\textbf{Topic modeling.} We evaluate topic coverage to determine how comprehensively AI-generated reviews address the breadth of topics present in expert reviews. Our approach comprises three steps: \ding{182} \textit{Topic extraction:} Each review \( R \) (either AI-generated or expert) is decomposed into a set of topics: $T_R = \{ t_1, t_2, \dots, t_n \},$ where each topic \( t_i \) is represented by a sentence that captures its core content and context. \ding{183} \textit{Topic similarity:} Let $T_{\text{AI}} = \{ t_1, t_2, \dots, t_m \}$ and  $T_{\text{Expt}} = \{ t'_1, t'_2, \dots, t'_n \}$
denote the topics extracted from the AI and expert reviews, respectively. We define a topic similarity function \( \text{TS}(t_i, t'_j) \) that an LLM assigns on a discrete scale:
\begin{equation}
\begin{split}
\text{TS}(t_i, t'_j) = 3 \cdot \mathbb{I}\{ t_i \sim_{\text{strong}} t'_j \} \\
+ 2 \cdot \mathbb{I}\{ t_i \sim_{\text{moderate}} t'_j \}\\ 
+ 1 \cdot \mathbb{I}\{ t_i \sim_{\text{weak}} t'_j \},
\end{split}
\end{equation}
where $\mathbb{I}$ is the indicator function, $t_i \sim_{\text{strong}} t'_j$, $t_i \sim_{\text{moderate}} t'_j$ , $t_i \sim_{\text{weak}} t'_j$ denote substantial, moderate, and minimal overlap in concepts, respectively. All the conditions are mutually exclusive. We set a similarity threshold \( \tau = 2 \) so that topics with \(\text{TS}(t_i, t'_j) \geq \tau \) are considered aligned. \ding{184} \textit{Coverage ratio:} For each AI-generated review, we construct a topic similarity matrix \( S \) where each element $S[i, j] = \text{TS}(t_i, t'_j)$ represents the similarity between topic \( t_i \) from \( T_{\text{AI}} \) and topic \( t'_j \) from \( T_{\text{Expt}} \). The topic coverage ratio is defined as:
\begin{equation}
S_{\text{coverage}} = \frac{1}{n} \sum_{j=1}^{n} \mathbb{I}\left(\max_{i=1,\dots,m} S[i, j] \geq \tau\right),
\end{equation}
where \( \mathbb{I}(\cdot) \) is the indicator function, and \( n = |T_{\text{Expt}}| \) is the total number of topics extracted from the expert review.

% \underline{\textit{Topic extraction using LLM.}} We employed an LLM for extracting core topics. Each review $R$ (either AI-generated or expert-generated) was decomposed into a set of topics: $T_{R} = \{t_{1}, t_{2}, ..., t_{n} \} $ where each topic $t_i$ is represented by a sentence that clearly captures the essence, key details and provides sufficient context to understand the significance of the topic.\par
% To enhance the representation of topic embeddings, individual word embeddings were weighted based on their relevance to the topic. This weighting ensures that dominant keywords exert greater influence on the final topic representation. Let $w_{ij}$ denote the weight of word $j$ in topic $t_i$, and $e_{w_{ij}}$ be the embedding of the word $j$. The topic embedding $e_{t_i}$ was calculated as: \\
% \begin{equation}
%  e_{t_{i}} = \frac {\sum_{j}w_{ij}e_{w_{ij}}} {\sum_{j}w_{ij}} 
% \end{equation}
% \underline{\textit{Similarity for topic alignment.}} To measure the alignment between AI-generated reviews $R_{\text{AI}}$ and expert reviews $R_{\text{Expert}}$, we leverage advanced contextual understanding of Large Language Models to capture underlying themes in each topic. Let $T_{\text{AI}} = \{ t_1, t_2, \dots, t_m \}$ and $T_{\text{Expert}} = \{t'_1, t'2, \dots, t'n\}$ denote the sets of topics extracted from $R_{\text{AI}}$ and $R_{\text{Expert}}$, respectively. The  similarity score $\text{sim}(t_i, t'_j)$ between two topics $t_i$ and $t'_j$ is given as an output by an LLM on a scale from 0 to 3 as given by the following equation.

% \[ sim(t_{i}, t'_{j}) = \begin{cases} 3, & \text{if substantial overlap in concepts} \\ 2, & \text{if some overlap, but different details} \\ 1, & \text{if little overlap in ideas} \\ 0, & \text{if No meaningful connection.} \end{cases} \]
% A similarity threshold $\tau = 2$ was used to determine topic alignment. Topics with $\text{sim}(t_i, t'_j) \geq \tau$ were deemed sufficiently aligned.

% Uncomment for algorithm
% \begin{algorithm}[t]
% \caption{AI vs. Expert Review Evaluation}
% \label{alg:ai-expert-evaluation}

% {\raggedright
% \begin{algorithmic}[1]
% \Require AI-generated review $R_{AI}$, Expert review $R_{Expert}$, Embedding model $E$, Large Language Model $\mathcal{M}$, Similarity threshold $\tau$
% \Ensure Semantic similarity score $S_{\text{semantic similarity}}$, Topic coverage ratio $S_{\text{coverage ratio}}$

% \Statex
% \Function{EvaluateReviews}{$R_{AI}, R_{Expert}, E, \mathcal{M}, \tau$}

%     \State $e_{R_{AI}} \gets E(R_{AI})$ \Comment{Compute embedding of AI review}
%     \State $e_{R_{Expert}} \gets E(R_{Expert})$ \Comment{Compute embedding of expert review}
%     \State $S_{\text{semantic similarity}} \gets \frac{e_{R_{AI}} \cdot e_{R_{Expert}}}{\|e_{R_{AI}}\| \|e_{R_{Expert}}\|}$ \Comment{Compute cosine similarity}
    
%     \State $T_{AI} \gets \mathcal{M}(\text{``Extract topics from'' } R_{AI})$ 
%     \State $T_{Expert} \gets \mathcal{M}(\text{``Extract topics from'' } R_{Expert})$
    
%     \State $S \in \mathbb{R}^{|T_{AI}| \times |T_{Expert}|}, \quad S[i, j] = 0, \forall i, j$ \Comment{Initialize zero matrix}
%     \For{$i \gets 1$ \textbf{to} $|T_{AI}|$}
%         \For{$j \gets 1$ \textbf{to} $|T_{Expert}|$}
%             \State $S[i,j] \gets \mathcal{M}(\text{``Compare topics'' }, T_{AI}[i], T_{Expert}[j])$ 
%         \EndFor
%     \EndFor
    
%     \Comment{$S \in \{0,1,2,3\}$}
%     \State $count \gets 0$
%     \For{$j \gets 1$ \textbf{to} $|T_{Expert}|$}
%         \If{$\max_{i}(S[i, j]) \geq \tau$}
%             \State $count \gets count + 1$
%         \EndIf
%     \EndFor
%     \State $S_{\text{coverage ratio}} \gets \frac{count}{|T_{Expert}|}$
    
%     \State \Return $S_{\text{semantic similarity}}, S_{\text{coverage ratio}}$
% \EndFunction
% \end{algorithmic}}
% \end{algorithm}

% \underline{\textit{Topic coverage analysis.}} For each AI-generated review, a topic similarity matrix $S$ was constructed, where $S[i, j] = \text{sim}(t_i, t'j)$ represents the similarity between topic $t_i$ from $T_{\text{AI}}$ and topic $t'j$ from $T_{\text{Expert}}$. Using $S$, the \textit{topic coverage ratio}, i.e. the proportion of expert topics  $T_{Expert}$ covered by $T_{AI}$ is calculated as:
% % \[
% %        \text{Coverage Ratio} = \frac{\sum_{j=1}^{n} \mathbb{1}\left(\max_{i=1, \dots, m} S[i, j] \geq \tau\right)}{n}
% % \]
% \begin{equation}
%     S_{\text{coverage ratio}} = \frac{\sum_{j=1}^{n} 1 (\text{max}_{i=1,2,...m} S[i,j] \geq \tau)}{n}
% \end{equation}
% Here, 1(.) is an indicator function that evaluates to 1 if the condition is true, and n = $|T_{Expert}|$ is the number of expert topics. The threshold \( \tau \) (i.e., \( \tau = 2 \)) determines the minimum similarity required for a topic to be considered covered.

% \item Unique Topics in AI-Generated Reviews: Topics in \( T_{\text{AI}} \) that do not have a sufficiently similar counterpart in \( T_{\text{Expert}} \) are defined as unique topics:
% \begin{equation}
% T_{\text{Unique}} = \{t_i \in T_{\text{AI}} : \max_{j=1, \dots, n} S[i, j] < \tau\}
% \end{equation}

% \item Missed Expert Topics: Expert topics not represented in \( T_{\text{AI}} \) are identified as:

% \begin{equation}
% T_{\text{Missed}} = \{t'_j \in T_{\text{Expert}} : \max_{i=1, \dots, m} S[i, j] < \tau\}
% \end{equation}

\subsection{Evaluating Factual Correctness of Reviews}
To address the hallucinations and factual inaccuracies of LLM generated reviews, we propose an automated pipeline that validates the factual correctness of LLM-generated reviews by \textit{simulating the conference rebuttal process}. Our pipeline emulates the traditional rebuttal workflow, where authors clarify or counter reviewer claims using evidence from their work. By automating both the question generation and rebuttal phases, our system produces a robust factual correctness evaluation. The pipeline consists of the following steps:\par
\textit{\textbf{Step 1:} Transforming reviews into structured questions.} 
Each LLM-generated review \(R\) is transformed into a structured question \(Q\) that encapsulates the central claim or critique. For example, consider the following review from the PeerRead dataset for the paper \textit{"Augmenting Negative Representations for Continual Self-Supervised Learning"} \cite{chaaugmenting}:\par
\underline{Review (\(R\)):} ``\texttt{Augmentation represents a crucial area of exploration in self-supervised learning. Given that the authors classify their method as a form of augmentation, it becomes essential to engage in comparisons and discussions with existing augmentation methods.}''

This review is converted into the corresponding question:

\underline{Generated question (\(Q\)):} ``\texttt{Has the paper engaged in comparisons and discussions with existing augmentation methods, given that the authors classify their method as a form of augmentation?}''

\textit{\textbf{Step 2:} Decomposing questions into sub-questions.} The question \(Q\) is then decomposed into a set of sub-questions $\{q_1, q_2, \ldots, q_n\}$ using a dedicated query decomposition engine. This decomposition enables a fine-grained analysis by isolating distinct components of the original question.

\textit{\textbf{Step 3:} Retrieval-augmented generation (RAG) for evidence synthesis.} For each sub-question \(q_i\), we employ a Retrieval-Augmented Generation (RAG) framework to gather and synthesize relevant evidence: \circled{a} \textit{Section retrieval:} For each paper \(P\), we retrieve pertinent text segments \(S\) (approximately 400 tokens) via semantic search. \circled{b} \textit{Parent section extraction:} Using LangChain's Parent Document Retriever, we extract the parent sections \(S_p\) (approximately 4000 tokens) corresponding to each \(S\). Documents are pre-chunked hierarchically, ensuring that each \(S\) is mapped to its contextually relevant \(S_p\). \circled{c} \textit{Answer generation:} With the context provided by \(S_p\), the LLM generates an answer \(A_i\) for each sub-question \(q_i\). These individual answers are then aggregated into a unified, structured response \(A_Q\) addressing the original question \(Q\).\par

\textit{\textbf{Step 4:} Automated rebuttal generation.} The comprehensive answer \(A_Q\) is used to generate an automated rebuttal \(R_b\) for the original review \(R\). This rebuttal is designed to provide evidence-based clarification or counterarguments to the claims in \(R\).

\textit{\textbf{Step 5:} Factual correctness evaluation.} An evaluation agent then assesses the factual correctness of \(R\) by comparing it against the generated rebuttal \(R_b\). The review is deemed: \circled{a} \(\textbf{Valid}\ (\mathcal{V} = \text{True})\) if \(R_b\) substantiates the claims made in \(R\).
\circled{b} \(\textbf{Invalid}\ (\mathcal{V} = \text{False})\) if \(R_b\) reveals factual discrepancies or unsupported claims in \(R\).\par

% LLMs are increasingly being employed in academic peer review processes. However, concerns around hallucinations or factual inaccuracies in LLM-generated reviews necessitate robust validation mechanisms. To address this, we propose a pipeline that automates the process of validating the factual correctness of LLM-generated reviews by simulating the conference rebuttal process. This system emulates the traditional rebuttal workflow where authors clarify or counter reviewer claims by providing evidence or context from their work. The pipeline ensures rigor by automating both the question generation and rebuttal phases, ultimately producing a factual correctness evaluation. Below we explain the proposed pipeline:

% \textbf{Conversion of reviews into questions.} Each LLM-generated review, denoted as $R$ is converted into a structured question $Q$. For example, consider a review from the PeerRead dataset for the paper titled "Augmenting Negative Representations for Continual Self-Supervised Learning" \cite{chaaugmenting}: \\
% Review ($R$): ``Augmentation represents a crucial area of exploration in self-supervised learning. Given that the authors classify their method as a form of augmentation, it becomes essential to engage in comparisons and discussions with existing augmentation methods.''\par

% Generated Question ($Q$):
% ``Has the paper engaged in comparisons and discussions with existing augmentation methods, given that the authors classify their method as a form of augmentation?''\par


% Uncomment for algorithm
% \begin{algorithm}[t]
% \caption{Factual Correctness Evaluation of LLM-Generated Reviews}
% \label{alg:factual-correctness-evaluation}

% {\raggedright
% \begin{algorithmic}[1]
% \Require LLM-generated review $R$, Paper $P$, LLM model $\mathcal{M}$, SubQuestionQueryEngine $\mathcal{Q}$, Semantic Search Engine $\mathcal{S}$, Parent Document Retriever $\mathcal{D}$
% \Ensure Validity score $V$

% \Statex
% \Function{EvaluateReviewCorrectness}{$R, P, \mathcal{M}, \mathcal{Q}, \mathcal{S}, \mathcal{D}$}
%     \Comment{Step 1: Convert Review into Questions}
%     \State $Q \gets \mathcal{M}($``Convert review $R$ into a structured question''$)$

%     \Comment{Step 2: Decompose into Sub-Questions}
%     \State $\{q_1, q_2, ..., q_n\} \gets \mathcal{Q}(Q)$
    
%     \Comment{Step 3: Retrieve Contextual Information}
%     \For{$q_i \in \{q_1, q_2, ..., q_n\}$}
%         \State $S \gets \mathcal{S}(P, q_i, 400)$ \Comment{Retrieve relevant 400-token sections}
%         \State $S_p \gets \mathcal{D}(P, S, 4000)$ \Comment{Retrieve parent 4000-token section}
%         \State $A_i \gets \mathcal{M}($``Answer $q_i$ given context $S_p$''$)$
%     \EndFor
    
%     \Comment{Step 4: Generate Rebuttal}
%     \State $A_Q \gets \text{Concatenate}(A_1, A_2, ..., A_n)$
%     \State $R_b \gets \mathcal{M}($``Generate rebuttal from $A_Q$''$)$
    
%     \Comment{Step 5: Evaluate Review Correctness}
%     \If{$R_b$ agrees with claims in $R$}
%         \State $V \gets \text{True}$ \Comment{Review is valid}
%     \Else
%         \State $V \gets \text{False}$ \Comment{Review is invalid}
%     \EndIf
    
%     \State \Return $V$
% \EndFunction
% \end{algorithmic}}
% \end{algorithm}


% \textbf{Decomposition of questions into sub-questions.} Each question Q is decomposed into a set of sub-questions 
% $\{q_{1}, q_{2}, ....., q_{n}\}$
% using subquestion-based query engine. This decomposition isolates individual components of the original question, facilitating a granular evaluation.\par

% \textbf{Retrieval-Augmented Generation (RAG) for contextual information.} To answer each sub-question $q_{i}$, a RAG pipeline is employed as follows: \ding{182} For each paper $P$, relevant small sections $S$ of 400 tokens are retrieved using semantic search. \ding{183} Parent sections $S_{p}$ of size 4000 tokens that contain $S$ are then extracted using LangChain's Parent Document Retriever. Documents are pre-chunked into hierarchical levels, where each $S$ belongs to a predefined $S_{p}$. Retrieval first selects $S$ based on semantic similarity, and then the corresponding $S_{p}$ is retrieved using this parent-child mapping. \ding{184} Given $S_{p}$, the LLM generates answers $A_{i}$ for each $q_{i}$. These answers are then combined into a single, structured paragraph $A_{Q}$, forming a comprehensive response to $Q$.\par

% \begin{enumerate}
%     \item For each paper $P$, relevant small sections $S$ of 400 tokens are retrieved using semantic search.\vspace{-2mm}
%     \item Parent sections $S_{p}$ of size 4000 tokens that contain $S$ are then extracted using LangChain's Parent Document Retriever to provide the whole context.\vspace{-2mm}
%     \item Given $S_{p}$, the LLM generates answers $A_{i}$ for each $q_{i}$. These answers are then combined into a single, structured paragraph $A_{Q}$, forming a comprehensive response to $Q$.\vspace{-2mm}
% \end{enumerate}
% \textbf{Rebuttal generation.} The combined answer $A_{Q}$ serves as the automated rebuttal $R_{b}$ for the review $R$. This rebuttal provides evidence-based clarification or counters the claims made in $R$.

% \textbf{Evaluating review correctness} An evaluation agent is tasked with determining the validity $V$ of the review $R$ based on the rebuttal $R_{b}$. The review is marked as:

% \begin{enumerate}
%     \item $Valid(V=True$) if $R_{b}$ agrees with the claims made in $R$.
%     \item $Invalid(V=False)$ if $R_{b}$ identifies factual inaccuracies in $R$.
% \end{enumerate}

\subsection{Constructiveness}
We assess review constructiveness by quantifying the presence and quality of actionable insights in AI-generated reviews relative to expert feedback. Our framework begins by extracting key actionable components from each review using an LLM with few-shot examples. Specifically, we identify the following actionable insights: (i) \emph{criticism points} (\(C\)), which capture highlighted flaws or shortcomings in the paper’s content, clarity, novelty, and execution; (ii) \emph{methodological feedback} (\(M\)), which encompasses detailed analysis of experimental design, techniques, and suggestions for methodological improvements; and (iii) \emph{suggestions for improvement} (\(I\)), which consist of broader recommendations for enhancement such as additional experiments, alternative methodologies, or improved clarity.\par

Once these components are extracted, each insight is evaluated along three dimensions: specificity, feasibility, and implementation details. The specificity score \(\sigma\) is defined as 1 if the insight is specific and includes explicit examples, and 0 otherwise; the feasibility score \(\phi\) is set to 1 when the recommendation is practical within the paper's context, and 0 otherwise; and the implementation details score \(\zeta\) is 1 if actionable steps or detailed methodologies are provided, and 0 otherwise. The overall actionability score for an individual insight is then computed as $S_{\text{act},i} = \sigma_i + \phi_i + \zeta_i,$ with an insight considered actionable if \(S_{\text{act},i} > 0\). Finally, we quantify the overall constructiveness of a review by calculating the percentage of actionable insights:
\begin{equation}
 S_{\text{act}} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\big(S_{\text{act},i} > 0\big) \times 100,   
\end{equation}
where \(N\) is the total number of extracted insights and \(\mathbb{I}(\cdot)\) denotes the indicator function. This metric provides a quantitative measure of how effectively a review offers concrete guidance for improving the work.

% To evaluate whether the review carried out includes specific suggestions that are clear and actionable in guiding the authors to improve the paper, we measured the presence of actionable insights within the feedback provided by AI-generated reviews and compared them with the gold standard of expert reviews. Our framework systematically identifies actionable suggestions and assesses their quality through a multi-step process outlined below.

% \textbf{Extracting actionable insights.} The constructiveness evaluation begins by extracting specific components from each review using an LLM backed by few shot examples. \ding{182} \textit{Criticism points ($C$):} Highlighted flaws, shortcomings and aspects that detract the paper quality. This focuses on paper's content, clarity, novelty, and execution.
% \ding{183} \textit{Methodological feedback ($M$):} Feedback on the methodologies used in the paper, including suggestions for improvement, detailed analysis of the paper's experimental design, techniques, or approach, focusing on strengths, weaknesses, and potential areas for improvement.
% \ding{184} \textit{Suggestions ($I$):} Broader recommendations for enhancing the work which can be suggesting additional experiments, alternative methodologies, or improved clarity.\par

% Uncomment for algorithm
% \begin{algorithm}[t]
% \caption{Constructiveness Evaluation}
% \label{alg:constructiveness-evaluation}

% {\raggedright
% \begin{algorithmic}[1]
% \Require Review text $R$, Large Language Model $\mathcal{M}$
% \Ensure Constructiveness score $S_{\text{actionable}} \in [0,1]$

% \Statex
% \Function{EvaluateConstructiveness}{$R, \mathcal{M}$}
%     \State $C \gets \text{ExtractCriticism}(R, \mathcal{M})$ 
%     \State $M \gets \text{ExtractMethodologicalFeedback}(R, \mathcal{M})$ 
%     \State $I \gets \text{ExtractSuggestions}(R, \mathcal{M})$ 
    
%     \State $Insights \gets C \cup M \cup I$ \Comment{Combine extracted insights}
%     \State $N \gets |Insights|$ \Comment{Total number of extracted insights}
%     \State $S \gets [0]^N$ \Comment{Initialize scores array}
    
%     \For{$j \gets 1$ \textbf{to} $N$}
%         \State $\sigma_j \gets \Call{EvaluateSpecificity}{\mathcal{M}, insights_j}$
%         \State $\phi_j \gets \Call{EvaluateFeasibility}{\mathcal{M}, insights_j}$
%         \State $\iota_j \gets \Call{EvaluateImplementationDetails}{\mathcal{M}, insights_j}$
%         \State $S_{\text{actionable},j} \gets \sigma_j + \phi_j + \iota_j$
%     \EndFor
    
%     \State $S_{\text{actionable}} \gets \frac{\sum_{j=1}^{N} \mathbf{1}(S_{\text{actionable},j} > 0)}{N} \times 100$
    
%     \State \Return $S_{\text{actionable}}$
% \EndFunction

% \Statex
% \Function{EvaluateSpecificity}{$\mathcal{M}, C_j$}
%     \State $score \gets \mathcal{M}(\text{``Is insight $C_j$ specific?``})$
%     \State \Return $score$ \Comment{$score \in \{0,1\}$}
% \EndFunction


% \Function{EvaluateFeasibility}{$\mathcal{M}, C_j$}
%     \State $score \gets \mathcal{M}(\text{``Is insight $C_j$ feasible?``})$
%     \State \Return $score$
%     \Comment{$score \in \{0,1\}$}
% \EndFunction

% \Function{EvaluateImplementationDetails}{$\mathcal{M}, C_j$}
%     \State $score \gets \mathcal{M}(\text{``Does insight $C_j$ have implementation details?
%     ``})$
%     \State \Return $score$
%     \Comment{$score \in \{0,1\}$}
% \EndFunction

% \end{algorithmic}}
% \end{algorithm}


% \textbf{Assessing feasibility and specificity.} Once $C$, $M$ and $I$ are extracted, they are assessed for quality along the following dimensions:

% \textit{Specificity:} This metric evaluates whether the feedback addresses a specific aspect of the paper, identifies precise issues or improvement areas, and avoids vague language by providing explicit examples or references. The specificity score $\sigma$ is defined as:
% \[
% \sigma = 
% \begin{cases} 
% 1, & \text{if specific and includes explicit examples.} \\
% 0, & \text{otherwise.}
% \end{cases}
% \]

% \textit{Feasibility:} This measures the practicality of the suggested actions, considering the context of the paper, available resources, and implementation challenges while proposing manageable solutions. The feasibility score $\phi$ is defined as:
% \[
% \phi = 
% \begin{cases} 
% 1, & \text{if suggests practical and feasible actions.} \\
% 0, & \text{otherwise.}
% \end{cases}
% \]

% \textit{Implementation details:} This assesses whether the feedback provides actionable steps, detailed methodologies, or explicit techniques, including references to prior work, tools, or parameters to support implementation. The implementation details score $\iota$ is defined as:
% \[
% \iota = 
% \begin{cases} 
% 1, & \text{if includes actionable steps or methodologies.} \\
% 0, & \text{otherwise.}
% \end{cases}
% \]
% \par

% \textbf{Calculating the actionability score.} Each actionable insight is assigned a score based on the sum of the binary values from the above nodes. The actionability score $S_{\text{actionable}}$ for an individual insight is defined as:
% \[
% S_{\text{actionable},i} = \sigma_{\text{i}} + \phi_{\text{i}} + \iota_{\text{i}}
% \]
% An insight is deemed actionable if $S_{\text{actionable}} > 0$, as even a minimal score can provide a foundation for authors to improve their work.\par

% \textbf{Percentage of actionable insights.} To gauge the overall constructiveness of a review, we calculated the percentage of actionable insights:
% \[
% \text{Percentage of Actionable Insights} = \frac{\sum_{i=1}^{N} S_{\text{actionable},i}}{N} \times 100
% \]

% \noindent where:
% \begin{itemize}
%     \item $N$ is the total number of insights extracted.
%     \item $S_{\text{actionable},i}$ represents the actionable score for the $i$-th insight.
% \end{itemize}
% \[
% S_{\text{actionable}} = \frac{\sum_{i=1}^{N} \mathbf{1}(S_{\text{actionable},i} > 0)}{N} \times 100
% \]

% \noindent where: $N$ is the total number of insights extracted. $\mathbf{1}(\cdot)$ is the indicator function, which equals 1 if $S_{\text{actionable},i} > 0$, and 0 otherwise. Where $S_{\text{actionable},i}$ represents the actionable score for the $i$-th insight. This metric provides a quantitative measure of how well the review guides authors in improving their work.\par

\subsection{Depth of Analysis}
To assess whether a review provides a comprehensive, critical evaluation rather than a superficial commentary, we measure the depth of analysis in AI-generated reviews. This metric captures how thoroughly a review engages with key aspects of a paper, including comparisons with existing literature, identification of logical gaps, methodological scrutiny, interpretation of results, and evaluation of theoretical contributions.\par

Each review is evaluated by multiple LLMs, which assign scores for each of the five dimensions, \(m_i\) (\(i \in \{1,2,3,4,5\}\)), with scores \(S_i \in [0,1]\). Scores in the continuous range allow us to capture nuances in performance. We define the metrics as follows:\par

\textit{Comparison with existing literature (\(m_1\)):} Assesses whether the review critically examines the paper's alignment with prior work, acknowledging relevant studies and identifying omissions. The scoring rubric is:
\begin{equation*}
\resizebox{.9\hsize}{!}{
S_1 = \begin{cases} 
3, & \text{if the review provides a thorough, critical comparison,} \\
2, & \text{if the comparison is meaningful yet shallow,} \\
1, & \text{if the comparison is vague or lacks specific references,} \\
0, & \text{if no comparison is provided.}
\end{cases}
}
\end{equation*}

\textit{Logical gaps identified (\(m_2\)):} Evaluates the review's ability to detect unsupported claims, reasoning flaws, and to offer constructive suggestions:
\begin{equation*}
\resizebox{.9\hsize}{!}{
S_2 = \begin{cases} 
3, & \text{if the review identifies comprehensive gaps and provides suggestions,} \\
2, & \text{if it notes some gaps with unclear recommendations,} \\
1, & \text{if the gaps are vaguely mentioned without solutions,} \\
0, & \text{if no gaps are identified.}
\end{cases}
}
\end{equation*}

\textit{Methodological scrutiny (\(m_3\)):} Measures the depth of critique regarding the paper’s methods, including evaluation of strengths, limitations, and improvement suggestions:
\begin{equation*}
\resizebox{.9\hsize}{!}{
S_3 = \begin{cases} 
3, & \text{if the review delivers a thorough critique with actionable suggestions,} \\
2, & \text{if the critique is meaningful but lacks depth,} \\
1, & \text{if the critique is vague and offers little insight,} \\
0, & \text{if no methodological critique is provided.}
\end{cases}
}
\end{equation*}

\textit{Results interpretation (\(m_4\)):} Assesses how well the review interprets the results, addressing biases, alternative explanations, and broader implications:
\begin{equation*}
\resizebox{.9\hsize}{!}{
S_4 = \begin{cases} 
3, & \text{if the interpretation is detailed and insightful,} \\
2, & \text{if it is meaningful yet shallow,} \\
1, & \text{if the discussion is generic or vague,} \\
0, & \text{if no interpretation is offered.}
\end{cases}
}
\end{equation*}

\textit{Theoretical contribution (\(m_5\)):} Evaluates the assessment of the paper’s theoretical contributions, including its novelty and connections to broader frameworks:
\begin{equation*}
\resizebox{.9\hsize}{!}{
S_5 = \begin{cases} 
3, & \text{if the evaluation is comprehensive and insightful,} \\
2, & \text{if the evaluation is meaningful but lacks depth,} \\
1, & \text{if the critique is vague,} \\
0, & \text{if no theoretical assessment is provided.}
\end{cases}
}
\end{equation*}

The overall depth of analysis score for a review is calculated as the average normalized score across all dimensions:
\begin{equation}
S_{\text{depth}} = \frac{\sum_{i=1}^{5} S_i}{15}.
\end{equation}
A higher \(S_{\text{depth}}\) indicates a more comprehensive and critical engagement with the manuscript.

% To assess whether the review provides a comprehensive evaluation that goes beyond surface-level observations, we measured the depth of analysis within AI-generated reviews. This metric evaluates how thoroughly the review engages with various critical aspects of the paper, such as the comparison with existing literature, identification of logical gaps, methodological scrutiny, results interpretation, and theoretical contribution. Our framework evaluates how well the review engages with key components, ensuring it addresses the significance and implications of the paper.
% Uncomment for algorithm
% \begin{algorithm}[t]
% \caption{Depth of Analysis Evaluation}
% \label{alg:depth-of-analysis}

% {\raggedright
% \begin{algorithmic}[1]
% \Require Review text $R$, Large Language Model $\mathcal{M}$
% \Ensure Depth of analysis score $S_{\text{depth}} \in [0,1]$

% \Statex
% \Function{EvaluateDepthOfAnalysis}{$R, \mathcal{M}$}
%     \State $m[1] \gets \text{Existing Literature Comparison}$
%     \State $m[2] \gets \text{Logical Gaps Identification}$
%     \State $m[3] \gets \text{Methodological Scrutiny}$
%     \State $m[4] \gets \text{Results Interpretation}$
%     \State $m[5] \gets \text{Theoretical Contributions}$


%     \State $N \gets |m|$ \Comment{Number of evaluation criteria}
%     \State $S \gets [0]^N$ \Comment{Initialize score array}
    
%     \For{$i \gets 1$ \textbf{to} $N$}

%         \State ${S}[i] \gets \Call{EvaluateCriterion}{\mathcal{M}, m_i}$ \Comment{Score for criterion $m[i]$}
%     \EndFor
    
%     \State $S_{\text{total}} \gets \sum_{i=1}^N {S}[i]$ \Comment{Aggregate scores}
%     \State $S_{\text{depth}} \gets S_{\text{total}} / (3N)$ \Comment{Normalize score to range $[0,1]$}
    
%     \State \Return $S_{\text{depth}}$
% \EndFunction


% \Statex
% \Function{EvaluateCriterion}{$\mathcal{M}, C_i$}

%     \State $score \gets \mathcal{M}(\text{``Rate $R$ from 0 $\rightarrow$ 3 based on $C_i$.``})$ \Comment{LLM-based evaluation of the review for criterion $C_i$}

    
%     \State \Return $score$
% \EndFunction


% \end{algorithmic}}
% \end{algorithm}
% \textbf{Evaluation metrics for depth of analysis.} The review is processed through multiple LLMs, each independently evaluating the following metrics and assigning a score between 0 and 1, (higher is better). Scores in the range \( (0, 1) \) are also considered to account for the gray area. Mathematically, for each metric \( m_i \), the score \( S_i \) is assigned such that \( S_i \in [0, 1] \), with higher values indicating stronger performance.

% \textit{Comparison with existing literature (\(m_1\)):} Assesses whether the review critically evaluates the alignment or divergence of the paper with prior studies, including the acknowledgment of relevant work and identification of omissions or oversights. Rubrics are defined by the equation below:
%  \[ S_1 = \begin{cases} 3, & \text{if thorough, critical comparison} \\ 2, & \text{if meaningful but shallow comparison} \\ 1, & \text{if vague, lacking specific references} \\ 0, & \text{if no comparison with prior work} \end{cases} \]
% where higher values indicate a more thorough and critical comparison.

% \textit{Logical gaps identified (\(m_2\)):} Evaluates the review's ability to identify unsupported claims, assumptions, gaps in reasoning, or lack of evidence, as well as the provision of constructive suggestions to address these issues. The rubric is defined as follows:
%  \[ S_2 = \begin{cases} 3, & \text{thorough gaps + suggestions} \\ 2, & \text{some gaps, unclear suggestions} \\ 1, & \text{vague gaps, no suggestions} \\ 0, & \text{no gaps identified} \end{cases} \]
 
% \textit{Methodological scrutiny (\(m_3\)):} Assesses the critique of the paper's methodologies, including strengths, weaknesses, limitations, and suggestions for improvement. The rubric is defined as follows:
% \[ S_3 = \begin{cases} 3, & \text{thorough critique with suggestions} \\ 2, & \text{meaningful critique, lacks depth} \\ 1, & \text{vague critique, no insights} \\ 0, & \text{no critique provided} \end{cases} \]

% \textit{Results interpretation (\(m_4\)):} Evaluates the alignment of the results' interpretation with the data, addressing biases, alternative explanations, and broader implications. The rubric is defined as follows:  
% \[ S_4 = \begin{cases} 3, & \text{detailed, insightful, comprehensive} \\ 2, & \text{meaningful but lacks depth} \\ 1, & \text{vague, generic discussion} \\ 0, & \text{no interpretation provided} \end{cases} \]

% \textit{Theoretical contribution (\(m_5\)):} Assesses the evaluation of the paper’s theoretical contributions, including novelty, connections to broader frameworks, and limitations. The rubric is defined as follows:
% \[ S_5 = \begin{cases} 3, & \text{comprehensive, insightful evaluation} \\ 2, & \text{meaningful but lacks depth} \\ 1, & \text{vague, no meaningful critique} \\ 0, & \text{no assessment provided} \end{cases} \]

% Each metric score (\(S_i\)) provides a quantitative evaluation of the depth of analysis in the review, where:  
% \[
% i \in \{1, 2, 3, 4, 5\}
% \]  
% \textbf{Calculating the depth of analysis score.} To calculate the depth of analysis score for a review, we use the formula: 
% \[
% S_{\text{depth}} = \frac{\sum_{i=1}^{M} S_i}{3M}
% \]

% \noindent where, \(M = 5\) is the total number of evaluation metrics. \(S_i\) is the score assigned to the \(i\)-th metric, where \(i \in \{1, 2, 3, 4, 5\}\). This metric quantifies the overall depth of analysis by averaging the scores across all metrics, with higher values indicating a more comprehensive and critical engagement with the manuscript's content.

\subsection{Adherence to Reviewer Guidelines}
To assess whether a review complies with established criteria, we evaluate its adherence to guidelines set by the venue. This metric measures how well the reviewer applies key aspects such as originality, methodology, results, clarity, and ethical considerations, thereby ensuring an objective and structured evaluation process.

%To evaluate the extent to which the review adheres to established guidelines, we assess how well the reviewer follows the criteria and expectations set by the journal or conference. This metric measures whether the reviewer consistently applies the prescribed framework, including key aspects such as originality, methodology, results, clarity, and ethical considerations. Our framework ensures that the review process remains structured and aligned with the goals of thorough, unbiased, and fair evaluation. By testing adherence to these guidelines, we ensure that reviews are objective, consistent, and meaningful, contributing to the overall quality of the academic review process.

% Uncomment for algorithm
% \begin{algorithm}[t]
% \caption{Adherence Evaluation to Reviewer Guidelines}
% \label{alg:adherence-evaluation-v2}

% \begin{algorithmic}[1]
% \Require Review guidelines $G$, review text $R$, LLM $\mathcal{M}$
% \Ensure Adherence score $S_{\text{adherence}} \in [0,1]$

% \Statex
% \Function{EvaluateAdherence}{$G, R, \mathcal{M}$}
%     \State $C \gets \Call{ExtractCriteria}{\mathcal{M}, G}$
%     \Comment{Extract evaluation criteria from guidelines}

%     \State $N \gets |C|$ \Comment{Number of evaluation criteria}
    
%     \State $\mathbf{S} \gets [0]^N$ \Comment{Initialize score array for $N$ criteria}
%     % \\
    
%     \For{$i \gets 1$ \textbf{to} $N$}
%         \State $P_i \gets \Call{GeneratePrompt}{\mathcal{M}, C_i}$ \Comment{Create criterion-specific prompt}
%         \State $\mathbf{S}[i] \gets \Call{Evaluate}{\mathcal{M}, P_i, R}$ \Comment{Get score from LLM where $S_i \in \{0,1,2,3\}$}

%     \EndFor
    
%     \State $S_{\text{total}} \gets \sum_{i=1}^N \mathbf{S}[i]$ \Comment{Aggregate scores}
%     \State $S_{\text{adherence}} \gets S_{\text{total}}/(3N)$ \Comment{Normalize to [0,1] scale}
    
%     \State \Return $S_{\text{adherence}}$
% \EndFunction

% \Statex
% \Function{ExtractCriteria}{$\mathcal{M}, G$}
%     \State \Return $\mathcal{M}$(``Extract evaluation criteria from: $G$'') \Comment{LLM-based extraction}
% \EndFunction

% \Statex
% \Function{GeneratePrompt}{$\mathcal{M}$, $C_i$}
%     \State \Return ``Rate 0-3 how the review addresses: $C_i$. Just give number.'' \Comment{Example prompt template}
% \EndFunction

% \end{algorithmic}
% \end{algorithm}

%\textbf{Extracting criteria for adherence evaluation.} The evaluation begins by identifying criteria $C$ from the review guidelines $G$ which the reviewer is expected to follow. These criteria can be divided into two broad categories: \ding{182} \textit{Subjective Criteria:} These involve qualitative aspects, such as clarity and accuracy in summarizing the paper, or the ability to provide constructive feedback. \ding{183} \textit{Objective Criteria:} These involve quantifiable aspects, such as assigning scores or following a specific rating scale within the review structure. An LLM is tasked with extracting these criteria from the provided guidelines. The extracted criteria form the foundation for the adherence evaluation.

Our approach begins by extracting the criteria \(C\) from the guidelines \(G\). These criteria fall into two broad categories: \ding{182} \emph{subjective} criteria, which involve qualitative judgments (e.g., clarity, constructive feedback), and \ding{183} \emph{objective} criteria, which are quantifiable (e.g., following a prescribed rating scale). For each review \(R\), every extracted criterion \(C_i\) is scored on a 0-3 scale using a dedicated LLM with dynamically generated prompts that include few-shot examples for contextual calibration. For subjective criteria, the score is defined as:
\begin{equation*}
\resizebox{.9\hsize}{!}{
S_i = \begin{cases}
3, & \text{if there is strong adherence with detailed, accurate feedback;} \\
2, & \text{if the review shows reasonable alignment with minor deviations;} \\
1, & \text{if the feedback is incomplete or inaccurate;} \\
0, & \text{if there is no alignment.}
\end{cases}
}
\end{equation*}

For objective criteria, the scoring is binary:
\begin{equation*}
\resizebox{.9\hsize}{!}{
S_i = \begin{cases}
3, & \text{if the review adheres to the required scale and structure;} \\
0, & \text{otherwise.}
\end{cases}
}\end{equation*}

The overall adherence score is then computed as:
\begin{equation}
S_{\text{adherence}} = \frac{\sum_{i=1}^{2} S_i}{6},
\end{equation}

This normalized score provides a quantitative measure of how well the review conforms to the prescribed guidelines.

% \textbf{Assigning scores to criteria.} For review $R$ each extracted criterion $C_i$ is assigned a score $S_{i}$ on a scale of 0 to 3 by a dedicated instance of large language model, where higher scores indicate better adherence. Prompts for this scoring task are dynamically generated by a factory function, ensuring alignment with the specific criterion. The prompts are tailored to assess adherence based on predefined rules, distinguishing between subjective and objective criteria:

% \textit{Subjective criteria scoring:}
% \[ 
% S_i = \begin{cases} 
%     3, & \text{strong adherence with detailed} \hidewidth \\
%        & \text{and accurate feedback} \\
%     2, & \text{reasonable alignment with} \hidewidth \\
%        & \text{minor errors or deviations} \\
%     1, & \text{incomplete or inaccurate feedback,} \hidewidth \\
%        & \text{significant deviation} \\
%     0, & \text{No alignment with the criterion}
% \end{cases}
% \]

% \textit{Objective criteria scoring:}
% \[ S_i = \begin{cases} 3, & \text{adheres to the scale and standards} \\ 0, & \text{fails to rate or uses invalid scale} \end{cases} \] 

% \textbf{Dynamic prompt generation.} The prompts for evaluating each criterion are crafted dynamically by a large language model to ensure that the scoring task is well-defined and contextually appropriate. For subjective criteria, the prompts include specific few shot examples of high, medium, low, and no adherence, to help the model better align its scores to the expectations aiming for better and more consistent results. For objective criteria, the prompts emphasize the importance of following the required scale and structure along with few shot examples.

% \textbf{Calculating adherence score.} The scores $S_i$ assigned to each criterion are aggregated to compute both an overall adherence score $S_{\text{adherence}}$. The overall adherence score is defined as:

% \[
% S_{\text{adherence}} = \frac{\sum_{i=1}^N S_i}{3N}
% \]

% where $N$ is the total number of extracted criteria. $S_i$ is the score assigned to the criterion $C_i$. This score provides a quantifiable measure of guideline adherence ($S_{\text{adherence}}$), enabling comprehensive assessment of review quality.

% \subsection{Conference-Specific Prompt Alignment}
% A key aspect of our approach is dynamically aligning the LLM-based review process to the target conference's reviewing guidelines. In this section we describe the process to achieve this.
\subsection{Conference-Specific Review Alignment}
% To align the review process with conference-specific guidelines, we use the URL of the target conference's official reviewing guideline website. Using Extractor API, we extract the textual content from the webpage, ensuring all relevant criteria are captured. The extracted text is then processed using GPT, which refines the content by removing extraneous information while preserving essential reviewing instructions. Each guideline is subsequently transformed into a step-by-step instructional prompt using GPT, explicitly specifying the sections of the paper it applies to. Since certain review criteria may span multiple sections, prompts are dynamically assigned to relevant sections, with some prompts being applicable to more than one. By performing reviewing in a section-wise manner, we optimize processing efficiency, allowing for independent evaluation of each section before aggregating the final review. The details of all prompts used in this part are defined in the appendix.

To tailor the review process to conference-specific guidelines, we first retrieve the relevant textual content from the target conference's official reviewing website using the Extractor API. The extracted text is then processed using GPT \cite{achiam2023gpt} to filter out extraneous details while preserving essential reviewing instructions. Each guideline \(g_i\) is converted into a step-by-step instructional prompt via GPT:
\[
P_i = \text{GeneratePrompt}(g_i),
\]
where \(P_i\) denotes the prompt corresponding to guideline \(g_i\). Since some criteria apply to multiple sections of a research paper, the prompts are dynamically mapped to the relevant sections. This is achieved using a mapping function:
\[
S_j = \mathcal{M}(P_i),
\]
where \(S_j\) is the set of paper sections associated with prompt \(P_i\). Notably, \(\mathcal{M}\) is a one-to-many mapping, i.e., 
\[
\mathcal{M}: P \to \mathcal{P}(S),
\]
with \(\mathcal{P}(S)\) denoting the power set of all sections. By conducting reviews on a section-wise basis, our framework enhances processing efficiency and allows for independent evaluation of each section prior to aggregating the final review. Detailed descriptions of all prompts are provided in the Appendix.

% To align the review process with conference-specific guidelines, we first retrieve the textual content from the target conference's official reviewing guideline website using the Extractor API. The extracted text is processed using GPT \cite{achiam2023gpt} to remove extraneous information while preserving essential reviewing instructions.  
% Each guideline \( g_i \) is transformed into a step-by-step instructional prompt using GPT \cite{achiam2023gpt}, defined as $P_i = \text{GeneratePrompt}(g_i)$, where \( P_i \) represents the instructional prompt corresponding to guideline \( g_i \).

% Some reviewing criteria may be applicable to multiple sections of a research paper, so the prompts are dynamically assigned to relevant sections using a mapping function defined as $S_j = \mathcal{M}(P_i)$ where \( S_j \) denotes the set of paper sections relevant to prompt \( P_i \). The function \( \mathcal{M} \) extracts relevant sections by querying the prompt $P_i$. Some prompts may apply to multiple sections, making \( \mathcal{M} \) a one-to-many mapping denoted as $\mathcal{M}: P \to \mathcal{P}(S) $, where \( \mathcal{P}(S) \) represents the power set of all possible sections.  
% By performing reviewing in a section-wise manner, we optimize processing efficiency, allowing for independent evaluation of each section before aggregating the final review. The details of all prompts used in this part are defined in the appendix.

% To optimize processing efficiency, the review process is conducted **section-wise**, allowing independent evaluation of each section before aggregating the final review:  

% \begin{equation}
%     R_{\text{final}} = \bigcup_{j} R_j
% \end{equation}  

% where \( R_{\text{final}} \) represents the aggregated review, and \( R_j \) corresponds to the review generated for section \( S_j \).  

% All prompts used in this methodology are detailed in the **Appendix**.



% We first obtain the URL of the
% conference’s official review guidelines. Using a third-party tool, Extractor
% API, we extract the textual content of the guidelines
% webpage, ensuring that all reviewing criteria are cap-
% tured. We guide GPT to structure the relevant reviewer guideline in an easy to read format by removing uncecessary data that might be on the website (refer to prompt in appendix section). Finally, each of these guidelines are then converted into step-by-step instructional prompt using GPT (Refer appendix section). Each reviewing prompt generated from the conference guideline might need to access different sections, and since we have the step by step instructional prompt, it must have which section needs to be checked in it. So, we assign each prompt to individual sections of the paper, with the same prompt maybe going to more than one section. This way, we don't have the process the whole paper altoghether, and we can do reviewing section-wise at first, combining our results later on.

% \begin{enumerate}
%     \item \textbf{Guideline Extraction:} We first obtain the URL of the conference’s official review guidelines.
%     \item \textbf{Content Retrieval:} Using a third-party tool, Extractor API, we extract the textual content of the guidelines webpage, ensuring that all reviewing criteria are captured.
%     \item \textbf{Guideline Structuring:} The extracted content is processed to generate detailed, section-wise reviewing instructions. Each section (e.g., Introduction, Related Work, Methodology, Results) is assigned specific criteria based on conference priorities such as impact assessment, logical soundness, clarity, completeness, and adherence to reproducibility standards.
%     \item \textbf{Instruction Prompt Generation:} The section-wise criteria are transformed into structured step-by-step instructional prompts for the LLM to follow while reviewing the paper.
% \end{enumerate}

% By ensuring that the reviewing process aligns closely with the expectations of the target conference, we enable more accurate and relevant assessments tailored to the research community's standards.

% \subsection{Research Paper Parsing and Preprocessing}
% Before inference, we preprocess the research paper to ensure all relevant content, including text and illustrations, is structured appropriately. \vspace{-3mm}

% \begin{enumerate}
%     \item \textbf{PDF Parsing:} We utilize \texttt{pymupdf} from the \texttt{langchain\_community} library to extract the paper’s text content as a single structured object.\vspace{-1mm}
%     \item \textbf{Mathematical Equations and Figures:} For images, equations, and illustrations, we employ \texttt{pymupdf}'s multimodal LLM-based image parser. This converts images into markdown format with detailed descriptions, preserving contextual information critical for scientific analysis.
% \end{enumerate}


% \subsection{Review Prompt Iterative Refinement}
% To enhance the effectiveness and completeness of the review prompts, we employ an iterative refinement process involving a Supervisor LLM. Given an initial set of prompts generated from the reviewing guidelines, each prompt undergoes structured evaluation and improvement. The Supervisor LLM takes as input the initial prompt along with the corresponding reviewing guideline, which serves as the problem statement. It then critiques the prompt based on multiple key aspects, including clarity, logical consistency, alignment with the guideline, and comprehensiveness. Clarity ensures that the instructions are precise and unambiguous, logical consistency verifies that the reasoning within the prompt is coherent, and alignment ensures that the prompt accurately reflects the intended purpose of the guideline. Additionally, comprehensiveness is assessed by identifying whether the prompt covers all relevant aspects a human reviewer would consider, including those that an AI model might otherwise overlook. The critique provided by the Supervisor LLM is then fed back to the original LLM instance that generated the prompts, which revises the prompt based on the feedback. This iterative refinement loop continues for a fixed number of iterations, set to three in our implementation, ensuring progressive improvement at each step. After completing the refinement process, we obtain a final set of structured, high-quality review prompts that are well-aligned with the reviewing guidelines and optimized for the evaluation process. 

\subsection{Review Prompt Iterative Refinement}
To enhance the quality and completeness of our review prompts, we employ an iterative refinement process using a Supervisor LLM. Starting with an initial set of prompts generated from the reviewing guidelines, the Supervisor LLM evaluates each prompt in conjunction with its corresponding guideline, serving as the problem statement, and provides targeted feedback. This feedback addresses key aspects such as clarity (ensuring precise and unambiguous instructions), logical consistency (ensuring coherent reasoning), alignment with the guideline, and comprehensiveness (ensuring all relevant aspects are covered). The feedback is then used to revise the prompt, and this iterative loop is repeated for a fixed number of iterations (three in our implementation). The result is a final set of structured, high-quality review prompts that are well-aligned with the reviewing guidelines and optimized for the evaluation process.

% The algorithm is explained in algorithm~\ref{alg:refinement}.

% \begin{algorithm}[t]
% \caption{Iterative Refinement of Review Prompts}
% \label{alg:refinement}
% \DontPrintSemicolon

% {\raggedright
% \begin{algorithmic}[1]
% \KwIn{
%     $P = \{P_1, P_2, ..., P_n\}$ \tcp{Initial set of review prompts} \\
%     $G = \{G_1, G_2, ..., G_n\}$ \tcp{Corresponding review guidelines} \\
%     $N$ \tcp{Number of refinement iterations (default: 3)}
% }
% \KwOut{$P^* = \{P_1^*, P_2^*, ..., P_n^*\}$ \tcp{Final refined prompts}}

% \ForEach{$P_i \in P$}{
%     \For{$t = 1$ \KwTo $N$}{
%         \tcp{Step 1: Supervisor LLM evaluates the prompt}
%         $C_i^t \gets \text{SupervisorLLM}(P_i, G_i)$ \tcp{Generate critique}

%         \tcp{Step 2: Original LLM revises the prompt}
%         $P_i \gets \text{PromptGeneratorLLM}(P_i, C_i^t)$ \tcp{Update prompt}
%     }
% }
% \Return $P^* = P$
% \end{algorithmic}}
% \end{algorithm}

% Uncomment for algorithm
% \begin{algorithm}
% \caption{Iterative Refinement of Review Prompts}
% \label{alg:refinement}
% \begin{algorithmic}[1]
% \Require $P = \{P_1, P_2, ..., P_n\}$ \Comment{Initial review prompts}
% \Require $G = \{G_1, G_2, ..., G_n\}$ \Comment{Corresponding guidelines}
% \Require $N$ \Comment{Number of refinement iterations (default: 3)}
% \Ensure $P^* = \{P_1^*, P_2^*, ..., P_n^*\}$ \Comment{Final refined prompts}

% \For {$i = 1$ to $n$} \Comment{Iterate over each prompt}
%     \For {$t = 1$ to $N$} \Comment{Perform iterative refinement}
%         \State $C_i^t \gets \text{SupervisorLLM}(P_i, G_i)$ \Comment{Generate critique}
%         \State $P_i \gets \text{PromptGeneratorLLM}(P_i, C_i^t)$ \Comment{Update prompt}
%     \EndFor
% \EndFor
% \State \Return $P^* = P$
% \end{algorithmic}
% \end{algorithm}




% Once we have a set of initial review prompts, we employ an iterative refinement algorithm to make these instructional prompts more effective and well-rounded.
% Each prompt goes through a refinement
% Supervisor LLM gets input: Initial generated Prompt + the corresponding guideline (the problem statement)
% The supervisor LLM is instructed to critique the prompt regarding multiple key dimensions: clarity, logical consistencies, its alignment to the given problem statement, and any aspect it overlooked for reviewing considering how a human would approach reviewing in a well-rounded manner (covering aspects the AI model would have missed). 
% This critique is then fed back to the main LLM that created the prompts from the guideline. This LLM uses the critique to improve its initial prompt and make it makes changes to it using it. This process repeats for a number of iteration (hardcoded to 3 in our research). After a series of refinements, we have a final set of prompts.




% \subsubsection{Initial Review Generation}
% Using the extracted reviewing criteria, the system generates an initial review for each section by prompting the LLM with the corresponding structured instruction set.

% \subsubsection{Reflection-Based Prompt Refinement}
% To enhance the quality and coherence of the reviews, we employ a structured \textbf{Reflection Loop}, iterating over the review process to refine the responses.

% \begin{enumerate}
%     \item The LLM receives the structured reviewing prompt and generates an initial review for a given section.
%     \item The \textbf{Reflection Prompt Generator LLM} is then used to assess the quality of the generated response.
%     \item The input to this model includes:
%           \begin{itemize}
%               \item The problem statement (i.e., system instruction + review prompt).
%               \item The generated response (initial review).
%           \end{itemize}
%     \item The Reflection Prompt Generator LLM analyzes the response, identifying weaknesses such as lack of depth, missing key aspects, or inconsistencies.
%     \item It then generates an improved prompt with suggestions for refining the review.
%     \item The original reviewing LLM receives this refined prompt along with its previous response and generates an updated review.
%     \item This iterative loop runs for a predefined number of iterations (\texttt{num\_iterations}, currently set to 3), ensuring progressive refinement of the review.
% \end{enumerate}

% At the end of this process, we obtain a structured Python dictionary containing:
% \begin{center}
% \texttt{\{section : guidelines for reviewing\}}
% \end{center}
% which serves as the final reviewing instruction set.

% \subsection{Final Review Generation}
% Once the structured reviewing guidelines have been refined, the research paper undergoes a second inference pass using the final reviewing criteria. The same Reflection Loop is applied to enhance the quality of the generated reviews.

% \subsection{Supervised Validation Attempt}
% We initially explored using a separate "supervisor" LLM instance to validate the reviews. This model was designed to output a binary decision (\texttt{YES/NO}) based on predefined quality criteria. However, this approach often resulted in infinite loops of rejection (\texttt{NO}), indicating the challenges of fully automated review validation. Consequently, this strategy was discarded, reinforcing the need for structured refinement rather than rigid rejection-based feedback.

% \subsection{LaTeX Review Generation}
% The final structured review is compiled into a LaTeX format for direct integration into academic workflows. A separate LLM instance converts the review dictionary into a well-formatted LaTeX document, ensuring that the final output is readily usable by authors and conference organizers.

\section{Experiments \& Results}
\textbf{Dataset.} We selected a sample of 16 papers from the NeurIPS 2024 submissions available on OpenReview.net. These papers were chosen randomly, with a balanced split: 4 papers from each category of accept decision ("oral", "poster" and "spotlight") and 4 that were rejected. To ensure diversity and avoid potential biases in topic/keyword coverage, we specifically included papers that contained unique keywords within each acceptance category. By filtering for papers with distinctive keywords, we aim to include a broad range of research topics, mitigating the risk of over-representation of any single theme or field in our dataset.\\
The papers in dataset are relatively recent, offering a challenging test set that many flagship large language models (LLMs) are unlikely to have encountered given their knowledge cutoffs. This approach enables us to fairly assess each model’s capability to interpret and analyze unseen data effectively.

\subsection{Constructiveness}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{$\mu + \sigma$}\\
\hline
Ours-GPT-4o & $0.7483 \pm 0.1275$ \\
Ours-GPT-4o-Mini & $0.7385 \pm 0.0952$ \\
Ours-3.5-Sonnet & $0.7513 \pm 0.1669$ \\
Ours-3.5-Haiku & $0.4943 \pm 0.2070$ \\
\hline
Sakana-GPT-4o & $0.7909 \pm 0.1193$ \\
Sakana-GPT-4o-Mini & $0.7916 \pm 0.1246$ \\
Sakana-3.5-Sonnet & $0.6956 \pm 0.1475$ \\
Sakana-3.5-Haiku & $0.4634 \pm 0.1380$ \\
\hline
MARG-GPT-4o & $0.7253 \pm 0.1638$ \\
MARG-GPT-4o-mini & $0.7428 \pm 0.1815$ \\
\hline
Expert & $0.7522 \pm 0.1206$ \\
\hline
\end{tabular}
\caption{Mean scores and standard deviations of different frameworks/models for actionable insights.}
\label{constructiveness}
\end{table}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{constructiveness.png}
% \caption{Box Plot for presence of actionable insights}
% % \label{fig:example_image} % Assign a label for referencing
% \end{figure}

Table \ref{constructiveness} summarizes the performance of reviews generated using different frameworks with variety of foundation models and expert reviews based on the presence of actionable insights. Expert reviews achieved a mean score of 0.7522, establishing a high-quality benchmark. Notably, Sakana-4o-Mini (0.7916) and Sakana-4o (0.7909) produced feedback comparable to or even exceeding expert performance. Ours-GPT-4o model (0.7483) demonstrated competitive results, closely aligning with expert-level feedback, while Ours-GPT-4o-Mini (0.7385) also performed strongly, indicating the effectiveness of these models in generating actionable insights. In contrast, smaller models like Ours-3.5-Haiku (0.4943) and Sakana-3.5-Haiku (0.4634) lagged, underscoring the importance of model capacity and architecture. Additionally, higher variability in certain models (e.g., Ours-3.5-Sonnet, $\sigma = 0.1669$) indicates inconsistent feedback quality across different papers.

\subsection{Factual Correctness}

\subsection{Adherence to reviewer guidelines}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{$\mu + \sigma$} \\
\hline
Ours-GPT-4o & $0.6823 \pm 0.0917$ \\
Ours-GPT-4o-Mini & $0.5785 \pm 0.2174$ \\
Ours-3.5-Sonnet & $0.6658 \pm 0.1180$ \\
Ours-3.5-Haiku & $0.4631 \pm 0.1197$ \\
\hline
Sakana-GPT-4o & $0.6327 \pm 0.0808$ \\
Sakana-GPT-4o-Mini & $0.6013 \pm 0.0669$ \\
Sakana-3.5-Sonnet & $0.6400 \pm 0.0297$ \\
Sakana-3.5-Haiku & $0.6313 \pm 0.0398$ \\
\hline
MARG-GPT-4o & $0.3206 \pm 0.0364$ \\
MARG-GPT-4o-mini & $0.3031 \pm 0.0396$ \\
\hline
Expert & $0.5708 \pm 0.1133$ \\
\hline
\end{tabular}
\caption{Mean scores and standard deviations of different frameworks/models for adherence to review guidelines.}
\label{adherence}
\end{table}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{adherence.png}
% \caption{Box Plot for adherence of reviewer guidelines metric}
% % \label{fig:example_image} % Assign a label for referencing
% \end{figure}

The table \ref{adherence} above presents the performance of various models and expert reviews in adhering to established journal or conference review guidelines. Expert reviews attained a mean score of 0.5708, providing a baseline for high-quality adherence. Among the models, Ours-GPT-4o (0.6823) outperformed the expert benchmark, demonstrating strong adherence to review criteria. Similarly, Ours-3.5-Sonnet (0.6658) and Sakana-3.5-Sonnet (0.6400) showed competitive performance, although it is to be noted that SakanaAi's AI scientist is hardcoded to follow NeurIPS guidlines while our reviewer adjusts itself depending on the given guidelines.
Smaller models have a lower score with higher variability underscoring the significance of model size and architecture in ensuring structured and guideline-compliant academic reviews.

\subsection{Comparing with expert reviews}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{$\mu + \sigma$} \\
\hline
Ours-GPT-4o       & $0.6788 \pm 0.2546$ \\
Ours-GPT-4o-Mini  & $0.6924 \pm 0.1650$ \\
Ours-3.5-Sonnet   & $0.6782 \pm 0.1468$ \\
Ours-3.5-Haiku    & $0.8013 \pm 0.1572$ \\
\hline
Sakana-GPT-4o        & $0.6863 \pm 0.1445$ \\
Sakana-GPT-4o-Mini    & $0.7342 \pm 0.1799$ \\
Sakana-3.5-Sonnet & $0.7159 \pm 0.1407$ \\
Sakana-3.5-Haiku  & $0.6641 \pm 0.1467$ \\
\hline
MARG-GPT-4o & $0.6971 \pm 0.1549$ \\
MARG-GPT-4o-mini & $0.5439 \pm 0.1636$ \\
\hline
\end{tabular}
\caption{Mean scores and standard deviations of different frame-
works/models for coverage of expert topics in the review.}
\label{tab:coverage}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{$\mu + \sigma$} \\
\hline
Ours-GPT-4o       & $0.8171 \pm 0.0724$ \\
Ours-GPT-4o-Mini  & $0.8170 \pm 0.0346$ \\
Ours-3.5-Sonnet   & $0.8379 \pm 0.0204$ \\
Ours-3.5-Haiku    & $0.8274 \pm 0.0210$ \\
\hline
Sakana-GPT-4o        & $0.8440 \pm 0.0226$ \\
Sakana-GPT-4o-Mini    & $0.8344 \pm 0.0161$ \\
Sakana-3.5-Sonnet & $0.8195 \pm 0.0169$ \\
Sakana-3.5-Haiku  & $0.8130 \pm 0.0177$ \\
\hline
MARG-GPT-4o & $0.8005 \pm 0.0206$ \\
MARG-GPT-4o-mini & $0.7926 \pm 0.0246$ \\
\hline
\end{tabular}
\caption{Mean scores and standard deviations of different frame-
works/models for semantic similarity to the expert reviews}
\label{tab:similarity}
\end{table}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{topic coverage.png}
% \caption{Box Plot for proportion of expert review topics covered}
% % \label{fig:example_image} % Assign a label for referencing
% \end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{semantic similarity.png}
% \caption{Box Plot for semantic similarity with expert reviews}
% % \label{fig:example_image} % Assign a label for referencing
% \end{figure}

The Tables \ref{tab:coverage}, \ref{tab:similarity} show the proportion of topics covered by different frameworks that align with expert reviews in both coverage and semantic similarity. Expert reviews serve as the reference standard.

Ours-GPT-4o (coverage: 0.6788, similarity: 0.8171) and Sakana-4o (coverage: 0.6863, similarity: 0.8440) performed competitively, demonstrating strong alignment with expert feedback. Sakana-4o-Mini (coverage: 0.7342, similarity: 0.8344) even exceeded expert-like coverage in some cases.

Coverage varied across models. Ours-3.5-Haiku (0.8013) and Sakana-3.5-Haiku (0.6641) covered more topics but showed lower similarity, indicating broader yet less expert-aligned feedback. Ours-3.5-Sonnet (0.6782 coverage, 0.8379 similarity) balanced coverage and similarity better but had slightly lower recall.

Standard deviation analysis shows stable similarity scores ($\sigma \sim 0.02$) but greater variation in coverage ($\sigma \leq 0.25$), suggesting models match expert phrasing well but struggle with comprehensive topic selection.

\subsection{Depth of Analysis}
\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{$\mu + \sigma$} \\
\hline
Ours-GPT-4o       & $0.5028 \pm 0.0638$ \\
Ours-GPT-4o-Mini  & $0.5243 \pm 0.0628$ \\
Ours-3.5-Sonnet   & $0.5417 \pm 0.0918$ \\
Ours-3.5-Haiku    & $0.5597 \pm 0.1107$ \\
\hline
Sakana-GPT-4o         & $0.5208 \pm 0.0602$ \\
Sakana-GPT-4o-Mini    & $0.4944 \pm 0.0402$ \\
Sakana-3.5-Sonnet & $0.4875 \pm 0.0737$ \\
Sakana-3.5-Haiku  & $0.4597 \pm 0.0611$ \\
\hline
MARG-GPT-4o & $0.6833 \pm 0.0725$ \\
MARG-GPT-4o-mini & $0.7014 \pm 0.0832$ \\
\hline
Expert            & $0.6264 \pm 0.0722$ \\
\hline
\end{tabular}
\caption{Depth of Analysis scores (mean and standard deviation) for each model across 16 papers.}
\label{tab:depth_analysis}
\end{table}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{depth.png}
% \caption{Box Plot for depth of analysis done in the research paper review}
% % \label{fig:example_image} % Assign a label for referencing
% \end{figure}

The results in table \ref{tab:depth_analysis} show that expert reviews achieve the highest depth of analysis (0.6264), outperforming all AI models. Among AI-generated reviews, Ours-3.5-Haiku (0.5597) and Ours-3.5-Sonnet (0.5417) provide the most comprehensive evaluations, while Sakana-3.5-Haiku (0.4597) scores the lowest. The Ours-GPT-4o model (0.5028) performs better than most Sakana models but falls short of the top-performing AI models. Notably, Ours-3.5-Haiku has the highest variance (0.1107), indicating inconsistency across papers, whereas Sakana-4o-Mini (0.0402) is more stable but has lower absolute depth.
% Our system consists of four tightly integrated components designed to handle the complex task of research paper reviewing:\\ \textit{Conference Guideline Extraction}, \textit{Customized Paper Parsing}, \textit{Dynamic Prompt Generation and Refinement}, and \textit{Review Generation and Final Compilation}. Below, we describe each component and its significance in ensuring the robustness of the system.

% \subsection*{Step 1: Conference Guideline Extraction}
% Accurate alignment with conference-specific review criteria is critical to ensuring that AI-generated reviews are relevant and valuable. The system takes a conference's URL as input and uses an LLM (e.g., GPT) to extract reviewer guidelines by searching for keywords such as "Reviewer Guidelines," "Review Criteria," and "Instructions for Reviewers."

% \begin{tcolorbox}[colframe=blue!50!black, colback=blue!5, title=Example Guideline Extraction]
% \textbf{Guideline from Conference A:} \textit{Reviews should assess originality, relevance, and technical soundness. Include specific comments on the clarity and potential impact of the work.}
% \end{tcolorbox}

% % \noindent These extracted guidelines are structured into categories such as clarity, originality, and relevance, forming the foundation for dynamic prompt generation (Step 3).

% \subsection*{Step 2: Customized Paper Parsing}
% The system uses a specialized parser based on \texttt{ScienceParser} to extract content from the research paper PDF, ensuring accurate segmentation by sections (e.g., Introduction, Methodology, Results). Key enhancements include:
% \begin{itemize}
%     \item \textbf{Section-Based Parsing:} Content is extracted and organized by section headings for targeted analysis.
%     \item \textbf{Validation by Secondary LLM:} A second LLM reviews the parsed content to ensure accuracy.
% \end{itemize}
% To handle complex content like mathematical formulas, we employ \texttt{Gemini 1.5 Pro}, which converts formulas into LaTeX for consistency and applicability.

% % \begin{tcolorbox}[colframe=red!50!black, colback=red!5, title=Example Output]
% % \textbf{Parsed Content for "Methodology" Section:}
% % \begin{verbatim}
% % \{ "Methodology": "This paper proposes a novel framework using \textbf{X} \
% % and achieves state-of-the-art results. The core algorithm is:\n$F(x) = x^2 + ax + b$.\"\}
% % \end{verbatim}
% % \end{tcolorbox}

% \subsection*{Step 3: Dynamic Prompt Generation and Refinement}
% Based on parsed content and extracted conference guidelines, the system dynamically generates prompts for reviewing each paper section. These prompts include both general review aspects (e.g., clarity, completeness, originality) and conference-specific criteria.

% \noindent The prompt generation process is formalized as:
% \begin{align}
% P_{\text{review}} = P_{\text{generic}} + P_{\text{conference-specific}},
% \end{align}
% where $P_{\text{generic}}$ represents standard review instructions, and $P_{\text{conference-specific}}$ tailors the review to the conference's requirements. 

% These prompts undergo an iterative refinement process using a "judge LLM" to ensure step-by-step clarity and human-like structure, mimicking how expert reviewers analyze papers.

% \begin{tcolorbox}[colframe=green!50!black, colback=green!5, title=Refined Prompt Example (to change)]
% \textit{"For the \textbf{Introduction} section: Assess whether the paper provides sufficient background and motivation for the work. Evaluate clarity, relevance to the conference themes, and the originality of the problem statement."}
% \end{tcolorbox}

% \subsection*{Step 4: Review Generation and Final Compilation}
% Each section is reviewed independently using the refined prompts using separately initialized LLM instances for each section. Reviews are iteratively improved through another round of feedback from the "judge LLM." The final reviews are compiled and formatted into a professional LaTeX document adhering to academic standards.

% \noindent The formatted review includes:
% \begin{itemize}
%     \item Standardized headings for clarity.
%     \item Consistent typography, ensuring readability.
%     \item Inclusion of LaTeX-rendered formulas and equations where applicable.
% \end{itemize}

% \subsection{Overview}

% The system employs a multi-stage process involving:

% Data Ingestion: Extracting textual and structural content from research papers and retrieving conference review guidelines.

% Prompt Generation: Creating tailored prompts for each paper section to guide the review process.

% Reflection Loop: Refining prompts and reviews iteratively to enhance clarity and relevance.

% Section-wise Review Generation: Generating detailed feedback for individual sections using separate LLM instances.

% Presentation: Formatting the final review using LaTeX for professional submission.

% \subsection{Data Ingestion}

% Research Paper Input: Research papers in PDF format are processed to extract textual content and structural details (e.g., headings, figures, references).

% Reviewing Guidelines Retrieval: Conference guidelines are fetched via user-provided URLs, parsed, and structured for integration into the review process.

% \subsection{Prompt Generation}

% An LLM instance (Planner) segments the paper into six key sections: Motivation, Prior Work, Approach, Evidence, Contribution, and Presentation.

% Specific prompts are generated for each section, designed to elicit critical and constructive feedback aligned with the reviewing guidelines.

% \subsection{Reflection Loop}

% A supervisory LLM evaluates and refines the prompts for clarity, alignment, and relevance.

% This iterative refinement ensures high-quality prompts that guide the review generation effectively.

% \subsection{Section-wise Review Generation}

% Each section is reviewed independently by a dedicated LLM instance, ensuring unbiased and focused evaluations.

% The generated reviews undergo iterative refinement to enhance depth and adherence to guidelines.

% \subsection{Presentation}

% Final reviews are compiled and formatted using LaTeX, adhering to academic conventions for professional presentation.

% The formatted review is made available for user download via a web interface built with Streamlit.
\section{Conclusion}
%The experiments where conducted in a controlled environment and we shall test the security against prompt injection attack
% \bibliography{example_paper}
% \bibliographystyle{icml2025}

\bibliography{example_paper}

\appendix
\section{Appendix}
\label{sec:appendix}

\subsection{Limitations}
\subsection{Ethical Considerations}
\subsection{Additional Results}
\end{document}
