\section{Related Work}
\textbf{AI based scientific discovery.} The pursuit of automating scientific discovery has been a longstanding goal in artificial intelligence (AI) research. Early efforts in the 1970s introduced expert systems such as DENDRAL____ and the automated mathematician____, which focused on constrained problem spaces like organic chemistry and theorem proving. More recent advancements in AI-driven research have leveraged LLMs and machine learning techniques to extend beyond structured domains. Notable contributions include AutoML approaches that optimize hyperparameters and architectures____ and AI-driven discovery in materials science and synthetic biology____. However, these methods remain largely dependent on human-defined search spaces and predefined evaluation metrics, limiting their potential for open-ended discovery. Recent works____ aim to automate the entire research cycle, encompassing ideation, experimentation, manuscript generation, and peer review, thus pushing the boundaries of AI-driven scientific inquiry.\par

\textbf{AI based peer-review.} Recent advancements in AI-driven peer review have explored structured, multi-agent approaches for automated evaluation.____ employ LLMs to autonomously conduct the research pipeline, including peer review. It follows a structured three-stage review process: paper understanding, criterion-based evaluation (aligned with NeurIPS and ICLR guidelines), and final synthesis: assigning scores to key aspects like novelty, clarity, and significance. Evaluation shows its reviews closely match human meta-reviews (2.3/5 vs 2.5/5). MARG____ introduces a multi-agent framework where worker agents review sections, expert agents assess specific aspects, and a leader agent synthesizes feedback. Using BERTScore____ and GPT-4-based evaluation, MARG-S improves feedback quality, reducing generic comments (60\% to 29\%) and increasing helpful feedback per paper (1.7 to 3.7). A user study found 47\% of MARG-S feedback helpful, outperforming baseline models (21\%). These studies highlight the potential of AI to enhance peer review through structured automation and multi-agent collaboration.\par

% \textbf{AI based peer-review.} Recent advancements in AI-driven peer review have explored structured, multi-agent approaches for automated evaluation.____ employ LLMs to autonomously conduct the research pipeline, including peer review. It follows a structured three-stage review process: paper understanding, criterion-based evaluation (aligned with NeurIPS and ICLR guidelines), and final synthesis: assigning scores to key aspects like novelty, clarity, and significance. Evaluation shows its reviews closely match human meta-reviews. MARG____ introduces a multi-agent framework where worker agents review sections, expert agents assess specific aspects, and a leader agent synthesizes feedback. Using BERTScore____ and GPT-4-based evaluation, MARG-S improves feedback quality, reducing generic comments (60\% to 29\%) and increasing helpful feedback per paper (1.7 to 3.7). A user study found 47\% of MARG-S feedback helpful, outperforming baseline models (21\%). These studies highlight the potential of AI to enhance peer review through structured automation and multi-agent collaboration.\par


% \subsection{The AI Scientist (by Sakana AI)}
% \vspace{-1mm}
% \subsection{The AI Scientist (by Sakana AI)}
% ____ introduce The AI Scientist, an autonomous framework leveraging LLMs to conduct the full research pipeline, from ideation to peer review. The system generates research ideas using chain-of-thought reasoning and self-reflection, refines them via a Semantic Scholar API-assisted literature search, and executes experiments using Aider, an LLM-based coding assistant that iteratively debugs and refines implementations. To automate peer review, the system follows a structured process: (1) Paper Understanding, where the LLM summarizes key contributions, methodology, and results; (2) Criterion-Based Evaluation, assessing novelty, clarity, correctness, significance, and presentation using NeurIPS and ICLR guidelines; and (3) Final Review Synthesis, where individual evaluations are compiled into structured feedback with strengths, weaknesses, and improvement suggestions. The review system assigns numeric scores to each criterion, mimicking human assessment, and aggregates them into an overall Meta-Reviewer Score. AI-generated reviews were compared against NeurIPS 2021 Meta-Reviewer Scores, where human reviews averaged 2.5/5, while the AI system achieved 2.3/5, demonstrating near-human accuracy.

% \subsection{MARG: Multi-Agent Review Generation for Scientific Papers}
% ____ propose MARG (Multi-Agent Review Generation) that employs multiple worker agents to review different sections, while expert agents specialize in key aspects like experiments, clarity, and impact. A leader agent coordinates the process, ensuring a structured and coherent review. The system uses hardcoded review prompts inspired by ICLR guidelines, which remain static across all review tasks. For evaluation, the authors compare MARG-generated reviews against NeurIPS human reviews using automated alignment metrics and a user study. BERTScore is used to measure semantic similarity between AI-generated and human-written reviews, capturing lexical and contextual overlap. Additionally, GPT-4 is used as a preference model, scoring reviews based on relevance, coherence, and completeness, simulating human judgment. The authors also conduct a user study with NLP and HCI researchers, who assess feedback helpfulness. MARG-S (a specialized agent variant) outperforms single-agent and naive multi-agent baselines, reducing generic feedback from 60\% to 29\% and increasing helpful comments per paper from 1.7 to 3.7. The user study reveals that 47\% of MARG-S feedback was rated as helpful, compared to 21\% from naive multi-agent models.



% introduces a comprehensive framework that automates the entire research pipeline, from idea generation to manuscript writing, using LLMs. A notable component is its foundation model-based automated reviewer, which evaluates generated manuscripts based on NeurIPS guidelines, providing scores and feedback akin to human reviewers. The framework achieves 65\% accuracy for final paper score prediction and demonstrates cost efficiency, but it is limited by its inability to adapt to specific conference guidelines. The paper also doesn't analyze and assess their AI generated reviews using meaningful metrics.
% ____ presents a multi-agent framework (MARG) that generates peer-review feedback by distributing the task among multiple LLM agents. The system uses hardcoded review prompts inspired by ICLR guidelines, which remain static across all review tasks. MARG significantly reduces generic comments (from 60\% to 29\%) and generates more helpful feedback compared to baseline methods, as shown through a user study. The paper also proposes an automated review evaluation framework to check for approximate matches between AI-generated and human-written review comments using GPT. While GPT-4 is employed iteratively to extract approximate matches and mitigate inconsistencies, we argue that completely relying on GPT for end-to-end evaluation introduces unreliability and opacity. To address this, our evaluation framework emphasizes transparency, striving to develop interpretable metrics for comparing AI and human reviews, moving beyond black-box methodologies.
% \subsection{Is LLM a reliable reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks}
\textbf{Evaluation framework for AI based research paper reviews.} There has been limited research on developing evaluation frameworks for evaluating the quality of LLM generated paper reviews.____ evaluated GPT models for research paper reviewing across 3 tasks: aspect score prediction, review generation, and review-revision MCQ answering. Using the PeerRead dataset, they assessed LLM's ability to predict review scores based on clarity, originality, and soundness (aspect score prediction). For review generation, they employed the ASAP dataset ____ and tested zero-shot, few-shot, and aspect-guided structured prompting, where reviews were generated with explicit tagging for summary, originality, and clarity. The study developed evaluation framework comprising of aspect coverage, ROUGE (lexical overlap), BERTScore (semantic similarity), and BLANC (informativeness), alongside manual analysis. Aspect coverage measured how well the generated reviews addressed key aspects (originality, soundness, substance, replicability, etc.) when compared to the distribution in reference expert reviews. Results showed LLMs overemphasized positive feedback, lacked critical depth, and neglected substance and clarity, despite high lexical similarity to human reviews. ____ introduced an automated evaluation framework for AI-generated reviews, quantifying similarity to human reviews via recall, precision, and Jaccard index. Their multi-stage GPT-4 ____ based alignment process first matches semantically equivalent comments using five randomized passes (many-many matching), retaining pairs appearing in at least two runs. Each pair is re-evaluated based on relatedness and relative specificity. Recall measures the fraction of real-reviewer comments with at least one AI match, precision quantifies AI comments aligned with human reviews, and Jaccard index evaluates the intersection-over-union of aligned comments.

% ____ conducted a systematic evaluation of GPT-3.5 and GPT-4 for research paper reviewing task, focusing on three key tasks: aspect score prediction, review generation, and review-revision multiple-choice question answering (RR-MCQ). For aspect score prediction, the study used the PeerRead dataset (ICLR-2017 subset) to assess LLM's ability to infer review scores based on predefined criteria such as clarity, originality, and soundness. For review generation, ____ used the ASAP dataset ____ (ICLR-2020 subset), which consists of peer reviews labeled by aspects such as originality, soundness, and clarity. Various prompting strategies—zero-shot, few-shot, and aspect-guided structured generation—were tested to improve review quality. Aspect-guided structured generation organizes reviews by characteristics like summary, originality, and clarity, using models to generate sentences tagged with corresponding aspects. The generated reviews were evaluated using automated metrics and human annotations. Among automated metrics, the study used aspect coverage to measure how well the generated reviews addressed key aspects of the paper, comparing the distribution of positive and negative comments with reference reviews. The results showed that LLMs overemphasized positive feedback, often failing to provide critical assessments. Certain aspects, such as substance and clarity, were frequently neglected, while models disproportionately favored broad statements over precise critiques. Other automated metrics included ROUGE (lexical overlap), BERTScore (semantic similarity), and BLANC (informativeness via a blank-filling task). While these metrics indicated high similarity between LLM-generated and human-written reviews, they failed to capture deficiencies in critical feedback and technical depth. ____ proposed an automated evaluation framework with the following metrics used to quantify the AI generated reviews using the corresponding human (expert) review comments: Recall, Precision, and Jaccard Index. The paper employs a multi-stage GPT-4-based process to measure the overlap between generated and real reviews by identifying semantically equivalent comments. In the Many-Many Matching Stage, all comments from both sets are input into GPT-4, which performs five passes with randomized comment orders to mitigate inconsistencies. A comment pair is retained if it appears in at least two of five runs. In the Pairwise Stage, each identified pair is re-evaluated, where GPT-4 assigns two scores: Relatedness ("none," "weak," "medium," or "high") and Relative Specificity ("less," "same," or "more"). A match is confirmed if relatedness is at least "medium" and specificity is "same" or "more." This structured alignment enables the calculation of recall, precision, and Jaccard index for assessing review similarity. Recall is the fraction of real-reviewer comments that are aligned to any generated comment (total number of human reviews that have atleast 1 match with AI reviews divided by total number of human reviews). Precision is the fraction of generated comments that are aligned to any real-reviewer comment (total number of AI reviews that have atleast 1 match with human reviews divided by total AI reviews). Jaccard Index is calculated by dividing the intersection of generated comments and real-reviewer comments by the size of the union of the generated comments and real-reviewer comments less the intersection. The Jaccard index measures the similarity between the set of generated comments and the set of real-reviewer comments



% ____ evaluates the MARG-generated reviews against NeurIPS human reviews using automated alignment metrics and a user study. BERTScore is used to measure semantic similarity between AI-generated and human-written reviews, capturing lexical and contextual overlap. Additionally, GPT-4 is used as a preference model, scoring reviews based on relevance, coherence, and completeness, simulating human judgment. They perform automated evaluation using GPT-4 to extract approximate matches between AI reviews and expert reviews, they run GPT inference iteratively to extract approximate matches.The authors also conduct a user study with NLP and HCI researchers, who assess feedback helpfulness.


% conducts a comprehensive evaluation of large language models (LLMs), specifically GPT-3.5 and GPT-4, in scientific paper reviewing, focusing on their ability to generate review texts. The study examines aspect coverage and similarity to reference reviews through both automatic metrics and manual analysis, utilizing the ASAP dataset ____, which annotates review sentences with labels such as summary, motivation, originality, soundness, substance, replicability, meaningful comparison, and clarity. However, relying on pre-defined aspect coverage undermines the diversity and depth of AI and human reviews, therefore we recognized the need for a more nuanced and dynamic topic coverage metric.
% Papers are split into chunks, processed by worker agents, and analyzed by expert agents specializing in clarity, experiments, or impact, with a leader agent coordinating communication and combining feedback. The refinement stage focuses on improving the generated reviews for clarity, specificity, and validity. 


% \subsection{Existing work}


% % Prior work on LLM academic capabilities suggests that LLMs are now ready for specific reviewing tasks and appear to be more effective for some academic domains and less effective for others (Checco et al.
% % 2021; Schulz et al. 2022; Liu and Shah 2023; Lu et al. 2024)

% % \subsection{Current Applications of LLMs}

% % LLMs are widely used for tasks like language translation, summarization, and question-answering. Their potential for critical assessment of new research remains largely untapped.

% \subsection{Existing Automated Review Systems}

% Sakana AI Scientist: A system performing end-to-end research activities (literature review, hypothesis formulation, experimentation, manuscript writing).

% OpenReviewer: Utilizes GPT-4 for academic paper reviews.

% \subsection{Gaps in Current Systems}

% Existing solutions either aim to generalize across the entire research workflow or lack the depth needed for nuanced peer reviews.



% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines. Below is an example table.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

%\vspace{-3mm}

%Our experiments utilized 16 papers and their corresponding expert reviews (scraped from OpenReview.net), alongside AI-generated reviews.