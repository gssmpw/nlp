\section{Related Work}
\textbf{AI based scientific discovery.} The pursuit of automating scientific discovery has been a longstanding goal in artificial intelligence (AI) research. Early efforts in the 1970s introduced expert systems such as DENDRAL Clancey, "Meta-Dendral" and the automated mathematician____, which focused on constrained problem spaces like organic chemistry and theorem proving. More recent advancements in AI-driven research have leveraged LLMs and machine learning techniques to extend beyond structured domains. Notable contributions include AutoML approaches that optimize hyperparameters and architectures____ and AI-driven discovery in materials science and synthetic biology____. However, these methods remain largely dependent on human-defined search spaces and predefined evaluation metrics, limiting their potential for open-ended discovery. Recent works____ aim to automate the entire research cycle, encompassing ideation, experimentation, manuscript generation, and peer review, thus pushing the boundaries of AI-driven scientific inquiry.\par

\textbf{AI based peer-review.} Recent advancements in AI-driven peer review have explored structured, multi-agent approaches for automated evaluation.____ employ LLMs to autonomously conduct the research pipeline, including peer review. It follows a structured three-stage review process: paper understanding, criterion-based evaluation (aligned with NeurIPS and ICLR guidelines), and final synthesis: assigning scores to key aspects like novelty, clarity, and significance. Evaluation shows its reviews closely match human meta-reviews (2.3/5 vs 2.5/5). MARG Li, "Multi-Agent Review Generation for Scientific Papers" introduces a multi-agent framework where worker agents review sections, expert agents specializing in clarity, experiments, or impact, with a leader agent coordinating communication and combining feedback. The refinement stage focuses on improving the generated reviews for clarity, specificity, and validity.  They conduct a user study with NLP and HCI researchers, who assess feedback helpfulness.\par

\textbf{Automated Evaluation Frameworks} Li et al., "Evaluating Multi-Agent Review Generation" evaluates the MARG-generated reviews against NeurIPS human reviews using automated alignment metrics and a user study. BERTScore is used to measure semantic similarity between AI-generated and human-written reviews, capturing lexical and contextual overlap. Additionally, GPT-4 is used as a preference model, scoring reviews based on relevance, coherence, and completeness, simulating human judgment.\par

\textbf{Existing Automated Review Systems} Sakana AI Scientist: A system performing end-to-end research activities (literature review, hypothesis formulation, experimentation, manuscript writing).\par

\textbf{Gaps in Current Systems} Existing solutions either aim to generalize across the entire research workflow or lack the depth needed for nuanced peer reviews.