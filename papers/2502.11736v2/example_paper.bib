
@online{wang_reviewrobot_2020,
	title = {{ReviewRobot}: Explainable Paper Review Generation based on Knowledge Synthesis},
	url = {https://arxiv.org/abs/2010.06119v3},
	shorttitle = {{ReviewRobot}},
	abstract = {To assist human review process, we build a novel {ReviewRobot} to automatically assign a review score and write comments for multiple categories such as novelty and meaningful comparison. A good review needs to be knowledgeable, namely that the comments should be constructive and informative to help improve the paper; and explainable by providing detailed evidence. {ReviewRobot} achieves these goals via three steps: (1) We perform domain-specific Information Extraction to construct a knowledge graph ({KG}) from the target paper under review, a related work {KG} from the papers cited by the target paper, and a background {KG} from a large collection of previous papers in the domain. (2) By comparing these three {KGs}, we predict a review score and detailed structured knowledge as evidence for each review category. (3) We carefully select and generalize human review sentences into templates, and apply these templates to transform the review scores and evidence into natural language comments. Experimental results show that our review score predictor reaches 71.4\%-100\% accuracy. Human assessment by domain experts shows that 41.7\%-70.5\% of the comments generated by {ReviewRobot} are valid and constructive, and better than human-written ones for 20\% of the time. Thus, {ReviewRobot} can serve as an assistant for paper reviewers, program chairs and authors.},
	titleaddon = {{arXiv}.org},
	author = {Wang, Qingyun and Zeng, Qi and Huang, Lifu and Knight, Kevin and Ji, Heng and Rajani, Nazneen Fatema},
	urldate = {2025-02-16},
	date = {2020-10-13},
	langid = {english},
	file = {Full Text PDF:/Users/kudhru/Zotero/storage/ZUCU2AF3/Wang et al. - 2020 - ReviewRobot Explainable Paper Review Generation b.pdf:application/pdf},
}

@misc{liang_can_2023,
	title = {Can large language models provide useful feedback on research papers? A large-scale empirical analysis},
	url = {http://arxiv.org/abs/2310.01783},
	doi = {10.48550/arXiv.2310.01783},
	shorttitle = {Can large language models provide useful feedback on research papers?},
	abstract = {Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models ({LLM}) such as {GPT}-4, there is growing interest in using {LLMs} to generate scientific feedback on research manuscripts. However, the utility of {LLM}-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using {GPT}-4 to provide comments on the full {PDFs} of scientific papers. We evaluated the quality of {GPT}-4's feedback through two large-scale studies. We first quantitatively compared {GPT}-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the {ICLR} machine learning conference (1,709 papers). The overlap in the points raised by {GPT}-4 and by human reviewers (average overlap 30.85\% for Nature journals, 39.23\% for {ICLR}) is comparable to the overlap between two human reviewers (average overlap 28.58\% for Nature journals, 35.25\% for {ICLR}). The overlap between {GPT}-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 {US} institutions in the field of {AI} and computational biology to understand how researchers perceive feedback generated by our {GPT}-4 system on their own papers. Overall, more than half (57.4\%) of the users found {GPT}-4 generated feedback helpful/very helpful and 82.4\% found it more beneficial than feedback from at least some human reviewers. While our findings show that {LLM}-generated feedback can help researchers, we also identify several limitations.},
	number = {{arXiv}:2310.01783},
	publisher = {{arXiv}},
	author = {Liang, Weixin and Zhang, Yuhui and Cao, Hancheng and Wang, Binglu and Ding, Daisy and Yang, Xinyu and Vodrahalli, Kailas and He, Siyu and Smith, Daniel and Yin, Yian and {McFarland}, Daniel and Zou, James},
	urldate = {2025-02-16},
	date = {2023-10-03},
	eprinttype = {arxiv},
	eprint = {2310.01783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/kudhru/Zotero/storage/27SRMNMS/Liang et al. - 2023 - Can large language models provide useful feedback .pdf:application/pdf;Snapshot:/Users/kudhru/Zotero/storage/89SKQ7VE/2310.html:text/html},
}

@article{nuijten_statcheck_2020,
	title = {“statcheck”: Automatically detect statistical reporting inconsistencies to increase reproducibility of meta-analyses},
	volume = {11},
	rights = {© 2020 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd.},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1408},
	doi = {10.1002/jrsm.1408},
	shorttitle = {“statcheck”},
	abstract = {We present the R package and web app statcheck to automatically detect statistical reporting inconsistencies in primary studies and meta-analyses. Previous research has shown a high prevalence of reported p-values that are inconsistent - meaning a re-calculated p-value, based on the reported test statistic and degrees of freedom, does not match the author-reported p-value. Such inconsistencies affect the reproducibility and evidential value of published findings. The tool statcheck can help researchers to identify statistical inconsistencies so that they may correct them. In this paper, we provide an overview of the prevalence and consequences of statistical reporting inconsistencies. We also discuss the tool statcheck in more detail and give an example of how it can be used in a meta-analysis. We end with some recommendations concerning the use of statcheck in meta-analyses and make a case for better reporting standards of statistical results.},
	pages = {574--579},
	number = {5},
	journaltitle = {Research Synthesis Methods},
	author = {Nuijten, Michèle B. and Polanin, Joshua R.},
	urldate = {2025-02-16},
	date = {2020},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1408},
	keywords = {meta-analysis, reporting standards, reproducibility, statcheck, statistical error},
	file = {Full Text PDF:/Users/kudhru/Zotero/storage/2Q9EEHZX/Nuijten and Polanin - 2020 - “statcheck” Automatically detect statistical repo.pdf:application/pdf;Snapshot:/Users/kudhru/Zotero/storage/6VJ77MQF/jrsm.html:text/html},
}

@article{kalnins_screening_2015,
	title = {Screening for Self-Plagiarism in a Subspecialty-versus-General Imaging Journal Using {iThenticate}},
	volume = {36},
	rights = {© 2015 by American Journal of Neuroradiology},
	issn = {0195-6108, 1936-959X},
	url = {https://www.ajnr.org/content/36/6/1034},
	doi = {10.3174/ajnr.A4234},
	abstract = {{BACKGROUND} {AND} {PURPOSE}: Self-plagiarism is a form of research misconduct that can dilute the credibility and reputation of a scientific journal, as well as the represented specialty. Journal editors are aware of this problem when reviewing submissions and use on-line plagiarism-analysis programs to facilitate detection. The American Journal of Neuroradiology ({AJNR}) uses {iThenticate} to screen several submitted original research manuscripts selected for review per issue and retrospectively assesses 3 issues per year. The prevalence of self-plagiarism in {AJNR} was compared with that in Radiology; the necessity and cost of more extensive screening in {AJNR} were evaluated.
{MATERIALS} {AND} {METHODS}: The self-duplication rate in {AJNR} original research articles was compared with that in Radiology, a general imaging journal that screens all submitted original research manuscripts selected for review by using {iThenticate}. The rate of self-duplication in original research articles from 2 randomly selected 2012 {AJNR} issues was compared with the rate in the prior year to gauge the need for more extensive screening. A cost analysis of screening all submitted original research manuscripts selected for review by using {iThenticate} was performed.
{RESULTS}: Using an empiric 15\% single-source duplication threshold, we found that the rate of significant self-plagiarism in original research articles was low for both journals. While {AJNR} had more articles exceeding this threshold, most instances were insignificant. Analyzing 2 randomly chosen issues of {AJNR} for single-source duplication of {\textgreater}15\% in original research articles yielded no significant differences compared with an entire year. The approximate annual cost of screening all submitted original research manuscripts selected for review was {US} \$6800.00.
{CONCLUSIONS}: While the rate of self-plagiarism was low in {AJNR} and similar to that in Radiology, its potential cost in negative impact on {AJNR} and the subspecialty of neuroradiology justifies the costs of broader screening.},
	pages = {1034--1038},
	number = {6},
	journaltitle = {American Journal of Neuroradiology},
	author = {Kalnins, A. U. and Halm, K. and Castillo, M.},
	urldate = {2025-02-16},
	date = {2015-06-01},
	langid = {english},
	pmid = {25634717},
	note = {Publisher: American Journal of Neuroradiology
Section: Editorial Perspectives},
	file = {Full Text PDF:/Users/kudhru/Zotero/storage/74NP4UXP/Kalnins et al. - 2015 - Screening for Self-Plagiarism in a Subspecialty-ve.pdf:application/pdf},
}

@article{ALI2020113790,
title = {Deep learning in citation recommendation models survey},
journal = {Expert Systems with Applications},
volume = {162},
pages = {113790},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113790},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420306126},
author = {Zafar Ali and Pavlos Kefalas and Khan Muhammad and Bahadar Ali and Muhammad Imran},
keywords = {Recommender systems, Citation recommendation, Neural networks, Paper recommendation, Machine learning, Deep learning},
abstract = {The huge amount of research papers on the web makes finding a relevant manuscript a difficult task. In recent years many models were introduced to support researchers by providing personalized citation recommendations. Moreover, deep learning methods have been employed in this domain to improve the quality of the final recommendations. However, a thorough study that classifies citation recommendation models and examines their (a) strengths and weaknesses, (b) evaluation metrics used, (c) popular datasets, and challenges faced is missing. Therefore, with this survey, we present a new classification approach for deep learning models that provide citation recommendation. Our approach uses the following six criteria: data factors, data representation methods, methodologies, types of recommendations used, problems addressed, and personalization. Additionally, we present a comparative analysis of those models that use the same set of evaluation metrics and datasets. Moreover, we examine hot upcoming issues and solutions in light of explored literature. Also, the survey discusses and analyzes the evaluation metrics and datasets adopted by the explored models. Finally, we conclude our survey with trends and future directions to further assist research on that domain.}
}

@article{bharti_peerrec_2024,
	title = {{PEERRec}: An {AI}-based approach to automatically generate recommendations and predict decisions in peer review},
	volume = {25},
	issn = {1432-1300},
	url = {https://doi.org/10.1007/s00799-023-00375-0},
	doi = {10.1007/s00799-023-00375-0},
	shorttitle = {{PEERRec}},
	abstract = {One key frontier of artificial intelligence ({AI}) is the ability to comprehend research articles and validate their findings, posing a magnanimous problem for {AI} systems to compete with human intelligence and intuition. As a benchmark of research validation, the existing peer-review system still stands strong despite being criticized at times by many. However, the paper vetting system has been severely strained due to an influx of research paper submissions and increased conferences/journals. As a result, problems, including having insufficient reviewers, finding the right experts, and maintaining review quality, are steadily and strongly surfacing. To ease the workload of the stakeholders associated with the peer-review process, we probed into what an {AI}-powered review system would look like. In this work, we leverage the interaction between the paper’s full text and the corresponding peer-review text to predict the overall recommendation score and final decision. We do not envisage {AI} reviewing papers in the near future. Still, we intend to explore the possibility of a human–{AI} collaboration in the decision-making process to make the current system {FAIR}. The idea is to have an assistive decision-making tool for the chairs/editors to help them with an additional layer of confidence, especially with borderline and contrastive reviews. We use a deep attention network between the review text and paper to learn the interactions and predict the overall recommendation score and final decision. We also use sentiment information encoded within peer-review texts to guide the outcome further. Our proposed model outperforms the recent state-of-the-art competitive baselines. We release the code of our implementation here: https://github.com/{PrabhatkrBharti}/{PEERRec}.git.},
	pages = {55--72},
	number = {1},
	journaltitle = {International Journal on Digital Libraries},
	shortjournal = {Int J Digit Libr},
	author = {Bharti, Prabhat Kumar and Ghosal, Tirthankar and Agarwal, Mayank and Ekbal, Asif},
	urldate = {2025-02-16},
	date = {2024-03-01},
	langid = {english},
	keywords = {Artificial Intelligence, Attention mechanism, Decision prediction, Deep neural network, Peer reviews, Recommendation score prediction},
}

@ARTICLE{basuki2022review,
  author={Basuki, Setio and Tsuchiya, Masatoshi},
  journal={IEEE Access}, 
  title={The Quality Assist: A Technology-Assisted Peer Review Based on Citation Functions to Predict the Paper Quality}, 
  year={2022},
  volume={10},
  number={},
  pages={126815-126831},
  keywords={Task analysis;Feature extraction;Quality assessment;Predictive models;National Institutes of Health;Machine learning;Citation function;final review decision;paper quality;review score;technology-assisted peer review},
  doi={10.1109/ACCESS.2022.3225871}}

@misc{asai_openscholar_2024,
	title = {{OpenScholar}: Synthesizing Scientific Literature with Retrieval-augmented {LMs}},
	url = {http://arxiv.org/abs/2411.14199},
	doi = {10.48550/arXiv.2411.14199},
	shorttitle = {{OpenScholar}},
	abstract = {Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models ({LMs}) assist scientists in this task? We introduce {OpenScholar}, a specialized retrieval-augmented {LM} that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate {OpenScholar}, we develop {ScholarQABench}, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On {ScholarQABench}, {OpenScholar}-8B outperforms {GPT}-4o by 5\% and {PaperQA}2 by 7\% in correctness, despite being a smaller, open model. While {GPT}4o hallucinates citations 78 to 90\% of the time, {OpenScholar} achieves citation accuracy on par with human experts. {OpenScholar}'s datastore, retriever, and self-feedback inference loop also improves off-the-shelf {LMs}: for instance, {OpenScholar}-{GPT}4o improves {GPT}-4o's correctness by 12\%. In human evaluations, experts preferred {OpenScholar}-8B and {OpenScholar}-{GPT}4o responses over expert-written ones 51\% and 70\% of the time, respectively, compared to {GPT}4o's 32\%. We open-source all of our code, models, datastore, data and a public demo.},
	number = {{arXiv}:2411.14199},
	publisher = {{arXiv}},
	author = {Asai, Akari and He, Jacqueline and Shao, Rulin and Shi, Weijia and Singh, Amanpreet and Chang, Joseph Chee and Lo, Kyle and Soldaini, Luca and Feldman, Sergey and D'arcy, Mike and Wadden, David and Latzke, Matt and Tian, Minyang and Ji, Pan and Liu, Shengyan and Tong, Hao and Wu, Bohao and Xiong, Yanyu and Zettlemoyer, Luke and Neubig, Graham and Weld, Dan and Downey, Doug and Yih, Wen-tau and Koh, Pang Wei and Hajishirzi, Hannaneh},
	urldate = {2025-02-16},
	date = {2024-11-21},
	eprinttype = {arxiv},
	eprint = {2411.14199 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Digital Libraries, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/kudhru/Zotero/storage/MBZVMP8F/Asai et al. - 2024 - OpenScholar Synthesizing Scientific Literature wi.pdf:application/pdf;Snapshot:/Users/kudhru/Zotero/storage/QL6LQH5Q/2411.html:text/html},
}

@article{liu2023reviewergpt,
  title={Reviewergpt? an exploratory study on using large language models for paper reviewing},
  author={Liu, Ryan and Shah, Nihar B},
  journal={arXiv preprint arXiv:2306.00622},
  year={2023}
}

@article{robertson2023gpt4,
  title={Gpt4 is slightly helpful for peer-review assistance: A pilot study},
  author={Robertson, Zachary},
  journal={arXiv preprint arXiv:2307.05492},
  year={2023}
}
@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}
@article{petrescu2022evolving,
  title={The evolving crisis of the peer-review process},
  author={Petrescu, Maria and Krishen, Anjala S},
  journal={Journal of Marketing Analytics},
  volume={10},
  number={3},
  pages={185--186},
  year={2022},
  publisher={Springer}
}
@article{schulz2022future,
  title={{Is the future of peer review automated?}},
  author={Schulz, Richard and Barnett, Adrian and Bernard, Ren{\'e} and Brown, Nicholas J and Byrne, JA and Eckmann, Peter and Gazda, Marianna A and Kilicoglu, Halil and Prager, Elisabeth M and Salholz-Hillel, Michael and others},
  journal={BMC Research Notes},
  volume={15},
  number={1},
  pages={1--5},
  year={2022},
  publisher={BioMed Central}
}

@article{checco2021ai,
  title={{AI-assisted peer review}},
  author={Checco, Alessio and Bracciale, Lorenzo and Loreti, Paola and Pinfield, Stephen and Bianchi, Giuseppe},
  journal={Humanities and Social Sciences Communications},
  volume={8},
  number={1},
  pages={1--11},
  year={2021},
  publisher={Springer}
}

@inproceedings{bao2021predicting,
  title={{Predicting paper acceptance via interpretable decision sets}},
  author={Bao, Peng and Hong, Wei and Li, Xue},
  booktitle={Companion Proceedings of the Web Conference 2021},
  pages={461--467},
  year={2021}
}

@article{mirsky2018vesper,
  title={{Vesper: Using Echo-Analysis to Detect Man-in-the-Middle Attacks in LANs}},
  author={Mirsky, Yisroel and Kalbo, Naor and Elovici, Yuval and Shabtai, Asaf},
  journal={arXiv preprint arXiv:1803.02560},
  year={2018}
}

@article{latona2024ai,
  title={{The AI Review Lottery: Widespread AI-Assisted Peer Reviews Boost Paper Scores and Acceptance Rates}},
  author={Latona, Giovanni R and Ribeiro, Manoel H and Davidson, Thomas R and Veselovsky, Vlad and West, Robert},
  journal={arXiv preprint arXiv:2405.02150},
  year={2024}
}

@article{kuznetsov2024natural,
  title={{What Can Natural Language Processing Do for Peer Review?}},
  author={Kuznetsov, Ilia and Afzal, Omar M and Dercksen, Kevin and Dycke, Niels and Goldberg, Andrew and Hope, Tom and Hovy, Dirk and Kummerfeld, Jonathan K and Lauscher, Anne and Leyton-Brown, Kevin and others},
  journal={arXiv preprint arXiv:2405.06563},
  year={2024}
}
@misc{d2024marg,
  title={MARG: Multi-Agent Review Generation for Scientific Papers (arXiv: 2401.04259). arXiv},
  author={D’Arcy, M and Hope, T and Birnbaum, L and Downey, D},
  year={2024}
}
@article{tyser2024ai,
  title={AI-Driven review systems: evaluating LLMs in scalable and bias-aware academic reviews},
  author={Tyser, Keith and Segev, Ben and Longhitano, Gaston and Zhang, Xin-Yu and Meeks, Zachary and Lee, Jason and Garg, Uday and Belsten, Nicholas and Shporer, Avi and Udell, Madeleine and others},
  journal={arXiv preprint arXiv:2408.10365},
  year={2024}
}
@inproceedings{zhou2024llm,
  title={Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks},
  author={Zhou, Ruiyang and Chen, Lu and Yu, Kai},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={9340--9351},
  year={2024}
}

@article{lu2024ai,
  title={The ai scientist: Towards fully automated open-ended scientific discovery},
  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  journal={arXiv preprint arXiv:2408.06292},
  year={2024}
}

@article{d2023aries,
  title={Aries: A corpus of scientific paper edits made in response to peer reviews},
  author={D'Arcy, Mike and Ross, Alexis and Bransom, Erin and Kuehl, Bailey and Bragg, Jonathan and Hope, Tom and Downey, Doug},
  journal={arXiv preprint arXiv:2306.12587},
  year={2023}
}

@article{yuan2022can,
  title={Can we automate scientific reviewing?},
  author={Yuan, Weizhe and Liu, Pengfei and Neubig, Graham},
  journal={Journal of Artificial Intelligence Research},
  volume={75},
  pages={171--212},
  year={2022}
}
@article{madaan2023selfrefine,
  title={Self-Refine: Iterative Refinement with Self-Feedback},
  author={Madaan, Aman and Tuck, Jason and Gupta, Jayesh and Yazdanbakhsh, Amir and Zheng, Yash and Srikumar, Vivek and Pathak, Deepak and Yang, Denny and Talukdar, Partha and Goel, Karan},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}
@article{biswas2024ai,
  title={AI-assisted academia: NAVIGATING the nuances of peer review with ChatGPT 4},
  author={Biswas, Som S},
  journal={The Journal of Pediatric Pharmacology and Therapeutics},
  volume={29},
  number={4},
  pages={441--445},
  year={2024},
  publisher={Pediatric Pharmacy Advocacy Group}
}
@article{bauchner2024use,
  title={Use of artificial intelligence and the future of peer review},
  author={Bauchner, Howard and Rivara, Frederick P},
  journal={Health Affairs Scholar},
  volume={2},
  number={5},
  pages={qxae058},
  year={2024},
  publisher={Oxford University Press US}
}
@article{buchanan1981dendral,
  author = {Buchanan, B. G. and Feigenbaum, E. A.},
  title = {DENDRAL and Meta-DENDRAL: Their Applications Dimension},
  journal = {Artificial Intelligence},
  volume = {11},
  number = {1-2},
  pages = {5--24},
  year = {1981}
}

@phdthesis{lenat1977automated,
  author = {Lenat, D. B.},
  title = {Automated Theory Formation in Mathematics},
  school = {Stanford University},
  year = {1977}
}

@book{hutter2019automated,
  author = {Hutter, F. and Kotthoff, L. and Vanschoren, J.},
  title = {Automated Machine Learning: Methods, Systems, Challenges},
  publisher = {Springer Nature},
  year = {2019}
}

@article{he2021automl,
  author = {He, X. and Zhao, K. and Chu, X.},
  title = {AutoML: A Survey of the State-of-the-Art},
  journal = {Knowledge-Based Systems},
  volume = {212},
  pages = {106622},
  year = {2021}
}

@article{merchant2023ai,
  author = {Merchant, A. and Pyzer-Knapp, E. and Szymanski, P. and others},
  title = {AI in Materials Discovery},
  journal = {Nature Materials},
  year = {2023}
}

@article{hayes2024simulating,
  author = {Hayes, T. and Rao, R. and Akin, H. and others},
  title = {Simulating 500 Million Years of Evolution with a Language Model},
  journal = {bioRxiv},
  year = {2024}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{chaaugmenting,
  title={Augmenting Negative Representation for Continual Self-Supervised Learning},
  author={Cha, Sungmin and Cho, Kyunghyun and Moon, Taesup}
}
@article{kang2018dataset,
  title={A dataset of peer reviews (PeerRead): Collection, insights and NLP applications},
  author={Kang, Dongyeop and Ammar, Waleed and Dalvi, Bhavana and Van Zuylen, Madeleine and Kohlmeier, Sebastian and Hovy, Eduard and Schwartz, Roy},
  journal={arXiv preprint arXiv:1804.09635},
  year={2018}
}