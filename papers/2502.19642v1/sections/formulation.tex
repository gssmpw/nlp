\section{Formulation} \label{sec:formulation}

\begin{figure}[t]
    \centering
    \input{diag/encoder-decoder}
    \vspace*{-0.05cm}
    \caption{A \MIM model learns two factorizations of a joint distribution:
    (a) encoding; (b) decoding factorizations; and (c) the estimated joint distribution 
    (an undirected graphical model).
    }
    \label{fig:mim-model}
\end{figure}

In this section, we extend the formulation of the Mutual Information Machine (MIM), a probabilistic auto-encoder designed to learn informative and clustered latent codes. The clustering is achieved by minimizing the marginal entropy of the latent distribution over 
$\z$, which results in latent codes that are closely positioned in Euclidean space for similar samples (see example in the work by \cite{reidenbach2023improvingsmallmoleculegeneration}). In MIM, similarity between samples is defined by the decoding distribution, leading to a local structure around each latent code (\ie, similar samples correspond to nearby latent codes). However, the global distribution of these latent codes, while aligned with a target or learned prior, may not be well-suited for discriminative tasks.
To address this limitation, we propose augmenting the MIM objective with a contrastive objective term, which encourages the latent codes of dissimilar samples to be more distinct from each other. This modification aims to improve the global structure of the latent space, making it more suitable for discriminative downstream tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Contrastive Learning} \label{sec:contrastive-learning}

Contrastive learning is a representation learning technique that aims to maximize the similarity between positive pairs while minimizing the similarity between negative pairs. The similarity between samples is typically measured using a similarity function, such as cosine similarity or the dot product. For example, in the InfoNCE loss \cite{DBLP:journals/corr/abs-1807-03748}, the similarity is computed as the dot product of the normalized representations:
\begin{equation}
    \text{InfoNCE}(\xi, \xj) = - \log \left( \frac{\exp(f(\xi, \xj))}{\sum_{k=1}^{B} \exp(f(\xi, \xk))} \right),
\end{equation}
where $f(\xi, \xj)$ denotes the cosine similarity between $\xi$ and $\xj$, and the exponent ensures that the similarity score is non-negative.

In practice, the contrastive loss is formulated as a $B$-way classification problem, where the positive pair is distinguished as the first pair, and $B$ represents the batch size. For effective learning, it is crucial that the data augmentation applied to the positive pair is meaningful; however, for certain modalities (\eg, text), designing appropriate augmentations can be challenging. Furthermore, the effectiveness of the loss function is sensitive to the selection of negative examples and the batch size.

    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Contrastive MIM Learning (cMIM)} \label{sec:contrastive-mim}

\begin{figure}[t]
    \centering
    \input{diag/x-z-k}
    \vspace*{-0.05cm}
    \caption{We extend the \MIM model with additional binary variable $\k$, and present the two factorizations of a joint distribution:
    (a) encoding; (b) decoding factorizations.
    }
    \label{fig:cmim-model}
\end{figure}


In this work, we propose augmenting the MIM objective with a contrastive term to introduce global discriminative structure to the locally clustered latent space. Specifically, we hypothesize that encouraging the latent codes of similar samples to be close to each other (\ie, local structure) and the latent codes of dissimilar samples to be distinct (\ie, global structure) are complementary objectives. When comparing MIM with its contrastive extension (cMIM), we expect cMIM to exhibit similar reconstruction fidelity, comparable clustering performance, and improved discriminative capabilities.

Contrastive learning typically relies on generating augmented data for positive pairs, introducing an inductive bias that may not capture all desired invariances within the data. Moreover, devising appropriate augmentations can be challenging for certain data modalities, such as text, as discussed in \cite{DBLP:journals/corr/abs-2010-05113}. Additionally, contrastive learning is sensitive to batch size since it requires a sufficient number of negative examples to improve the quality of learned representations. Although contrastive methods that do not rely on negative examples exist—such as BYOL \cite{DBLP:journals/corr/abs-2006-07733}—these methods often introduce additional hyperparameters that can be difficult to tune.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Contrastive Learning without Data Augmentation} \label{sec:contrastive-learning-without-data-augmentation}


In this work, we propose to extend the MIM framework by introducing a new random variable $\k$ into MIM's graphical model (Fig. \ref{fig:cmim-model}). Formally, the encoding (\ie, $\Menc(\x, \z, \k)$) and decoding (\ie, $\Mdec(\x, \z, \k)$) factorizations of the joint distribution are defined as follows:

\begin{equation}
    \Menc(\x, \z, \k) = \Menc(\k | \x, \z \Menc(\z | \x) \Menc(\x)      
\end{equation}

\begin{equation}
    \Mdec(\x, \z, \k) = \Mdec(\k | \x, \z \Mdec(\x | \z) \Mdec(\z)      
\end{equation}

Let us first define $\zi$ as the latent code of sample $\xi$, where given $\xi$, we sample $\zi \sim \Menc(\z \mid \xi)$, and given $\zi$, we sample $\xi \sim \Mdec(\x \mid \zi)$. We then introduce the random variable $\k$, a binary variable representing the relationship between a sample $\x$ and a latent code $\z$. Specifically, for sample $i$, we assign $\ki = 1$ if $\x = \xi$ and $\z = \zi$ (as defined above), and $\ki = 0$ otherwise.

Although $\k$ may not have practical utility on its own, it enables us to define the model's prediction of $\k$ to encourage dissimilar samples to be more distinct in the latent space (\ie, as defined by a similarity function). MIM naturally clusters similar latent codes (\ie, minimizes Euclidean distances in latent space for similar observations), and by introducing a direction-based similarity function, we can encourage dissimilar samples to be farther apart in terms of their direction relative to the origin, with minimal impact on the clustering properties of the model. Consequently, this approach should lead to a more discriminative latent space.

To elaborate, we define the discriminator distributions for the encoding and decoding factorizations over $\k$ as:

\begin{equation}
    \Menc(\k \mid \z = \zi, \x) = \Mdec(\k \mid \z = \zi, \x) = \text{Bernoulli}(\k; p_{k=1}),
\end{equation}

where

\begin{equation}
    \begin{aligned}
        p_{k=1} &= \frac{\SIM{\zi}{\zi}}{\SIM{\zi}{\zi} + \E{\x' \sim \pjoint(\x \mid \x \ne \xi),\z' \sim \Menc(\z | \x')}{ \SIM{\zi}{\z'}} } \\
        &\approx \frac{\SIM{\zi}{\zi}}{\SIM{\zi}{\zi} + \frac{1}{B} \sum_{\substack{j=1 \\ j \neq i}}^B \SIM{\zi}{\zj} }
    \end{aligned}
\end{equation}

and $\SIM{\cdot}{\cdot}$ is a similarity function between the latent codes (\ie, a non-negative function with higher values for more similar samples). Notice that $\k = 1$ only if $\x = \xi$, and $\k = 0$ otherwise. Here, we use cosine similarity as the similarity function, defined as:

\begin{equation}
    \SIM{\zi}{\z_j} = \exp \left( \frac{1}{\tau} \frac{\zi^T \cdot \z_j}{\| \zi \| \| \z_j \|} \right),
\end{equation}

where $\tau$ is a temperature parameter controlling the sharpness of the distribution, and the exponent ensuring that the similarity is non-negative. In all experiments, we set $\tau = 1$, and we have found the model to be robust to various values.

Conveniently, cMIM does not rely on batch size for generating negative examples. Furthermore, the model is never trained with samples where $\k = 0$ since the sampling process inherently ensures $\k = 1$. This simplification leads to a more straightforward empirical loss formulation (see Algo. \ref{algo:cmim}). Additionally, by incorporating an expectation (\ie, as opposed to a $B$-way classification with contrastive loss), the model allows the expected similarity with other samples to be efficiently approximated using MCMC sampling. This approach decouples the similarity estimation from batch size, significantly reducing sensitivity to batch size as the number of samples increases, as described by Chebyshev's inequality \cite{doi:10.1080/00031305.2016.1186559}.

To illustrate the lack of sensitivity to batch size, consider a one-dimensional latent space. The expected similarity with other samples is approximated by the average similarity, which is the sum of $B-1$ random variables. According to Chebyshev's inequality, the probability that the average similarity deviates from the expected similarity by more than $\epsilon$ is at most $\frac{1}{(B-1)\epsilon^2}$. Thus, the expected similarity is within $\epsilon$ of the average similarity with high probability, implying that the model is not sensitive to batch size, and the quality of the learned representation remains unaffected.

Additionally, cMIM does not require data augmentation, a necessity in traditional contrastive learning to create positive pairs for maximizing similarity and clustering similar samples. In cMIM, clustering is already achieved by the MIM objective, eliminating the need for data augmentation. This simplification reduces the complexity of training, as data augmentation is often a challenging hyper-parameter to tune in contrastive learning.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{cMIM Training Procedure} \label{sec:cmim-training-algorithm}

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.95\columnwidth}
    \begin{algorithm}[H]
        \small
        \caption{Learning parameters $\params$ of cMIM}
        \label{algo:cmim}
        \begin{algorithmic}[1]
            \REQUIRE Samples from dataset $\pjoint(\x)$
            \WHILE{not converged}
            \STATE $\sigma \sim \mathcal{U}(0,1]$
            \STATE $D \gets \{ \x_j, \z_j \sim \Menc(\z|\x,\sigma)\pjoint(\x) \}_{j=1}^{N}$
            \STATE $\EAMIMloss \left( \params ; D \right) = -\frac{1}{N}\! \sum_{i=1}^{N}\! \big( ~ \log \left( \Mdec(\x_i | \z_i) \cdot D(\xi, \zi) ~ \right) + \frac{1}{2} \log \left( \Menc(\z_i | \x_i,\sigma) \cdot \pjoint(\z_i) ~ \right) ~ \big)$
            \STATE $\Delta \params \propto -\nabla_{\params}  \EAMIMloss \left( \params ; D \right)$
            \COMMENT{\textcolor{gray}{\textit{Gradient computed through sampling using reparameterization}}}
            \ENDWHILE
        \end{algorithmic}
    \end{algorithm}
    \end{minipage}
    \caption{Training algorithm for cMIM where $D(\xi, \zi) \equiv \text{Bernoulli}( \ki = 1;p_{k=1} )$.}
    \end{figure}

The training of cMIM is conducted using the MIM objective applied to the extended graphical model. Specifically, MIM is defined over a mixture model as follows:
\begin{equation}
    \Mmodel (\x, \z, \k) = \frac{1}{2} \left( \Mdec(\k \mid \z, \x)\, \Mdec(\x \mid \z)\, \Menc(\z) + \Menc(\k \mid \z, \x)\, \Menc(\z \mid \x)\, \Mdec(\x) \right),
\end{equation}
with a sampling distribution $\Msamp(\x, \z, \k)$, given by
\begin{equation}
    \Msamp(\x, \z, \k) = \Pmix,
\end{equation}
as discussed in \cite{2019arXiv191003175L}, where we introduce here the discriminator distributions over $\k$.

The learning process for MIM involves minimizing an upper bound, defined as follows:
\begin{equation}
    \begin{aligned}
        \MIMloss(\params) = &
        \frac{1}{2} \Big(\, \CE{\Msamp(\x, \z, \k)}{\Menc \left(\x, \z, \k \right)} \\
        &\quad + ~ \CE{\Msamp(\x, \z, \k)}{\Mdec \left(\x, \z, \k \right)} \, \Big) \\
        &\ge H_{\Msamp} (\x, \k) + H_{\Msamp} (\z)  - I_{\Msamp} (\x, \k;\z)
    \end{aligned}
\end{equation}
where we group $\k$ with $\x$ and remind the reader that in practice, $\k = 1$ for all samples.

We also provide an explicit example of the loss for A-MIM, an asymmetric version of MIM, where the sample distribution includes only the encoding distribution:
\begin{equation}
    \AMIMloss (\params) = \frac{1}{2} \E{\x \sim \pjoint(\x),\z \sim \Menc(\z|\x), \k = 1}{ 
        \begin{aligned}
            \log \Mdec(\k | \z, \x) + \log & \Mdec(\x | \z) + \log \pjoint(\z) \\
            &+ \\
            \log \Menc(\k | \z, \x) + \log & \Menc(\z | \x) + \log \Menc(\x)
        \end{aligned}
    } \label{ea:mim-loss}
\end{equation}
where we extend the loss from \cite{livne2021sentencemimlatentvariablelanguage} by incorporating $\k$.

We also provide the training algorithm for cMIM in Algorithm \ref{algo:cmim}, where we extend the learning procedure from \cite{reidenbach2023improvingsmallmoleculegeneration}. Below, we define the discriminator objective term $D$ as a Bernoulli distribution with the approximated parameter $p_{k=1}$:
\begin{equation}
D(\x_i, \z_i) \equiv \text{Bernoulli}(\k = 1; p_{k=1}) = \frac{\SIM{\z_i}{\z_i}}{\SIM{\z_i}{\z_i} + \frac{1}{B} \sum_{\substack{j=1 \\ j \neq i}}^B \SIM{\z_i}{\z_j}} \label{eq:Dxz},
\end{equation}
where we use the current batch to approximate the expected similarity with other samples.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Informative Embeddings} \label{sec:informative-embeddings}

\begin{figure}[t]
    \centering
    \setlength{\tabcolsep}{0pt}
    \includegraphics[width=1.0\columnwidth]{images/informative-embeddings.png}
    \caption{Informative embeddings $\h$ are extracted from an input observation $\x$ by taking the output from the decoder's hidden states, prior to their mapping to the parameters of the decoding distribution $\Mdec$. Teacher forcing should be applied when necessary, particularly in the case of auto-regressive distributions.
    }
    \label{fig:informative-embeddings}
\end{figure}

Inspired by the success of leveraging output hidden states from various layers (\eg, \cite{zeng2024similardatapointsidentification}) of GPT-based large language models (LLMs, see \cite{DBLP:journals/corr/abs-2005-14165}) for downstream tasks, and recognizing the limitations of using such hidden states without further fine-tuning (\eg, NV-Embed by \cite{lee2024nvembedimprovedtechniquestraining}), we propose a generic method to enhance the quality of embeddings extracted from the MIM model, as depicted in Fig. \ref{fig:informative-embeddings}.

Specifically, the embeddings $\h$ are extracted from the hidden states of the decoder, just before they are mapped to the parameters of the decoding distribution $\Mdec(\x \mid \z) = f_{\params}(\h)$. These embeddings are then utilized for various downstream tasks, including classification, clustering, and generation. 

For auto-regressive distributions, such as text, teacher forcing is employed, where the input $\x$ is fed into both the encoder and decoder, eliminating the need to generate the next token. For non-auto-regressive distributions, such as images, there is no need for teacher forcing, and the embeddings $\h$ are directly used for tasks like classification or clustering. More formally, the embeddings $\hi$ for sample $\xi$ are defined as:
\begin{equation}
    \hi = \text{Decoder}(\xi | \zi \sim \Menc(\z | \xi)) = \text{Decoder}(\xi, \text{Encoder}(\xi)),
\end{equation}
where the above equation is employing teacher forcing for auto-regressive distributions.

We propose that instead of directly using the latent codes from MIM for downstream tasks, the focus should be on the embeddings $\h$ derived from the decoder's hidden states. As we demonstrate, these embeddings are crucial for downstream tasks and are adapted according to the specific characteristics of the data, whether auto-regressive or non-auto-regressive. We note that informative embeddings can be extracted from any encoder-decoder model, not just MIM.

To illustrate the intuition behind using informative embeddings, consider the case of text generation. The embeddings $\h$, extracted from the decoder's hidden states, encapsulate information from the latent code that has been transformed to represent the probability distribution of the next token. This can be viewed as an "enriched" representation of the latent code, augmented with additional contextual information from the decoder, resulting in a more comprehensive representation of the observation (\ie, capturing the entire distribution rather than just the actual observation). By leveraging these enriched embeddings for downstream tasks, we can exploit the additional information contained in the decoder's hidden states, potentially enhancing performance in discriminative tasks such as classification or regression.
