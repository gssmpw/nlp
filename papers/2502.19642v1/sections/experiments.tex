\section{Experiments} \label{sec:experiments}

To evaluate the proposed cMIM model, we conduct experiments on a 2D toy example and on molecular property prediction tasks (MolMIM by \cite{reidenbach2023improvingsmallmoleculegeneration}). The 2D toy example illustrates the impact of the proposed contrastive MIM loss (Eq. \eqref{eq:Dxz}). We then compare the performance of cMIM with MIM models trained on molecular data, assessing their reconstruction accuracy, reconstruction loss, and effectiveness in downstream tasks. Additionally, we benchmark cMIM against other state-of-the-art models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{2D Toy Example}

\begin{table}[t]
    \centering
    \begin{tabular}{c c}
        \begin{tabular}{c}
            \includegraphics[width=0.48\textwidth]{images/cMIM-toy/plot-000000.png} \\
            \textbf{(a)} Step 0
        \end{tabular} &
        \begin{tabular}{c}
            \includegraphics[width=0.48\textwidth]{images/cMIM-toy/plot-000100.png} \\
            \textbf{(b)} Step 100
        \end{tabular} \\
        \begin{tabular}{c}
            \includegraphics[width=0.48\textwidth]{images/cMIM-toy/plot-000200.png} \\
            \textbf{(c)} Step 200
        \end{tabular} &
        \begin{tabular}{c}
            \includegraphics[width=0.48\textwidth]{images/cMIM-toy/plot-000400.png} \\
            \textbf{(d)} Step 400
        \end{tabular} \\
        \begin{tabular}{c}
            \includegraphics[width=0.48\textwidth]{images/cMIM-toy/plot-001300.png} \\
            \textbf{(e)} Step 1300
        \end{tabular} &
        \begin{tabular}{c}
            \includegraphics[width=0.48\textwidth]{images/cMIM-toy/plot-004200.png} \\
            \textbf{(f)} Step 4200
        \end{tabular} \\
    \end{tabular}
    \caption{We demonstrate the effect of the proposed contrastive MIM loss (\ie, Eq. \eqref{eq:Dxz}) on a 2D toy example. The (left subplots) depict the 2D latent space initialized in the first quadrant. The (middle subplots) show histograms of the latent code angles, and the (right subplots) display histograms of the latent radii. These visualizations represent the latent space at different training steps.
    Observe how the model gradually learns to distribute all points uniformly in different directions (middle subplots), while allowing the radii to vary (right subplots). This behavior indicates that the proposed contrastive loss can effectively be integrated with the MIM loss, allowing the 2D Euclidean positions of the latent codes to be learned without contradiction.
    }
    \label{fig:cMIM-to-2d}
\end{table}

We demonstrate the effect of the proposed contrastive MIM loss (\ie, Eq. \eqref{eq:Dxz}) on a 2D toy example by learning the 2D positions of 1000 latent codes. The objective is to minimize the negative log-likelihood (NLL) associated with Eq. \eqref{eq:Dxz}, with the goal of assessing the impact of the contrastive loss on the learning of the 2D Euclidean positions of the latent codes by the MIM loss.

We anticipate that the learned latent codes will be evenly distributed in all directions while allowing for variations in their radii. The evolution of the latent space at different training steps is illustrated in Fig. \ref{fig:cMIM-to-2d}. The results indicate that the proposed contrastive loss should integrate well with the MIM loss, supporting the learning of the 2D Euclidean positions of the latent codes without contradiction.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Molecular Properties Prediction}

\begin{table}[t]
    \centering
    \begin{tabular}{c}
        \begin{tabular}{c}
            \includegraphics[width=0.95\textwidth]{images/cMIM-WB-plots.png} \\
            \textbf{(a)} Full plot.
        \end{tabular} \\
        \begin{tabular}{c}
            \includegraphics[width=0.95\textwidth]{images/cMIM-WB-plots-zoom.png} \\
            \textbf{(b)} Zoomed plot.
        \end{tabular} \\
    \end{tabular}
    \caption{We compare cMIM (yellow) vs MIM (pink) learning. (left) molecular reconstruction accuracy; (middle) character reconstruction accuracy; (right) reconstruction loss. Notice how the reconstruction and accuracy of cMIM are comparable to MIM.}
    \label{fig:cMIM-vs-MIM-WB}
\end{table}

%     \begin{tabular}{l||c|c||c|c||c|c||c}
%             \hline
%         \multirow{2}{*}{\textbf{Model (Latent K $\times$ D)}} & \multicolumn{2}{c||}{\textbf{ESOL}} & \multicolumn{2}{c||}{\textbf{FreeSolv}} & \multicolumn{2}{c||}{\textbf{Lipophilicity}} & \textbf{Recon.} \\
%         \cline{2-7} 
%         & \textbf{SVM} & \textbf{MLP} & \textbf{SVM} & \textbf{MLP} & \textbf{SVM} & \textbf{MLP} & \\
%         \hline
%         \hline
%         MIM (1 $\times$ 512) & 0.65 & 0.34 & 2.23 & 1.82 & 0.221 & 0.061 & 100\% \\
%         \hline
%         cMIM (1 $\times$ 512) & 0.47 & \cellcolor{yellow!25}0.19 & 2.32 & 1.67 & 0.182 & 0.038 & 100\% \\
%         \hline
%         MIM (1 $\times$ 512) info emb & 0.21 & 0.29 & 1.55 & 1.4 & 0.078 & 0.028 & 100\% \\
%         \hline
%         cMIM (1 $\times$ 512) info emb & \cellcolor{yellow!25}0.21 & 0.24 & 1.74 & \cellcolor{yellow!25}1.35 & 0.08 & \cellcolor{yellow!25}0.023 & 100\% \\
%         \hline
%         \hline
%         CDDD (512) & \textbf{0.33} &  & \textbf{0.94} &  & \textbf{0.4} &  &  \\
%         \hline
%         \textdagger Seq2seq (N $\times$ 512) & 0.37 & 0.43 & 1.24 & 1.4 & 0.46 & 0.61 & 100\% \\
%         \hline
%         \textdagger Perceiver  (4 $\times$ 512) & 0.4 & 0.36 & 1.22 & 1.05 & 0.48 & 0.47 & 100\% \\
%         \hline
%         \textdagger VAE (4 $\times$ 512) & 0.55 & 0.49 & 1.65 & 3.3 & 0.63 & 0.55 & 46\% \\
%         \hline
%         MIM (1 $\times$ 512) & 0.58 & 0.54 & 1.95 & 1.9 & 0.66 & 0.62 & 100\% \\
%         \hline
%         \hline
%         Morgan fingerprints (512) & 1.52 & 1.26 & 5.09 & 3.94 & 0.63 & 0.61 &  \\
% \end{tabular}


\begin{table}[t]
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.5}

    \centering
    \begin{tabular}{l||c|c||c|c||c|c||c}
            \hline
        \multirow{2}{*}{\textbf{Model (Latent K $\times$ D)}} & \multicolumn{2}{c||}{\textbf{ESOL}} & \multicolumn{2}{c||}{\textbf{FreeSolv}} & \multicolumn{2}{c||}{\textbf{Lipophilicity}} & \textbf{Recon.} \\
        \cline{2-7} 
        & \textbf{SVM} & \textbf{MLP} & \textbf{SVM} & \textbf{MLP} & \textbf{SVM} & \textbf{MLP} & \\
        \hline
        \hline
        MIM (1 $\times$ 512) & 0.65 & 0.34 & 2.23 & 1.82 & 0.663 & 0.61 & 100\% \\
        \hline
        cMIM (1 $\times$ 512) & 0.47 & \cellcolor{yellow!25}0.19 & 2.32 & 1.67 & 0.546 & 0.38 & 100\% \\
        \hline
        MIM (1 $\times$ 512) info emb & 0.21 & 0.29 & 1.55 & 1.4 & 0.234 & 0.28 & 100\% \\
        \hline
        cMIM (1 $\times$ 512) info emb & \cellcolor{yellow!25}0.21 & 0.24 & 1.74 & \cellcolor{yellow!25}1.35 & 0.24 & \cellcolor{yellow!25}0.23 & 100\% \\
        \hline
        \hline
        CDDD (512) & \textbf{0.33} &  & \textbf{0.94} &  & \textbf{0.4} &  &  \\
        \hline
        \textdagger Seq2seq (N $\times$ 512) & 0.37 & 0.43 & 1.24 & 1.4 & 0.46 & 0.61 & 100\% \\
        \hline
        \textdagger Perceiver  (4 $\times$ 512) & 0.4 & 0.36 & 1.22 & 1.05 & 0.48 & 0.47 & 100\% \\
        \hline
        \textdagger VAE (4 $\times$ 512) & 0.55 & 0.49 & 1.65 & 3.3 & 0.63 & 0.55 & 46\% \\
        \hline
        MIM (1 $\times$ 512) & 0.58 & 0.54 & 1.95 & 1.9 & 0.66 & 0.62 & 100\% \\
        \hline
        \hline
        Morgan fingerprints (512) & 1.52 & 1.26 & 5.09 & 3.94 & 0.63 & 0.61 &  \\
\end{tabular}
    \vspace{1.0em}
    \caption{Comparison of different models across ESOL, FreeSolv, and Lipophilicity datasets using SVM and MLP regression models, along with reconstruction accuracy. Top half shows current results, bottom half shows results from \cite{reidenbach2023improvingsmallmoleculegeneration} for comparison (might slighly differ for MIM due to different implementation). For \textdagger models we used average over the sequence length to aggregate the latent representation to 512 dimensions.
    In \textbf{bold} are the best results for non-MIM models. \colorbox{yellow!25}{Highlighted} are the best results for MIM models. Notice that both informative embeddings and cMMIM outperform the baseline MIM model, and are competitive or outperforms other models.}
    \label{tab:cmim-comparison-molmim}
\end{table}

We now compare the performance of cMIM and MIM models trained on molecular property prediction tasks (MolMIM by \cite{reidenbach2023improvingsmallmoleculegeneration}). In Fig. \ref{fig:cMIM-vs-MIM-WB}, we present a comparison of training of cMIM and MIM models on molecular data, focusing on their reconstruction accuracy, reconstruction loss, and performance on downstream tasks. As evident from the plots, both models perform comparably in terms of reconstruction and accuracy. 

To further assess the impact of the proposed contrastive loss, we compare cMIM and MIM models on molecular property prediction tasks, as shown in Table \ref{tab:cmim-comparison-molmim}. We evaluate the models' performance on the ESOL, FreeSolv, and Lipophilicity datasets using SVM and MLP regression models trained on the embeddings extracted from both cMIM and MIM models, along with comparisons to additional state-of-the-art (SOTA) models.

Specifically, we examine the performance of cMIM and MIM models both with and without informative embeddings. We also compare these models against other SOTA models, including CDDD, Seq2seq, Perceiver, and VAE, as well as against Morgan fingerprints as a baseline. The results demonstrate that cMIM models with informative embeddings outperform the baseline MIM model and are competitive with or exceed the performance of other models.
