\section{Introduction} \label{sec:intro}

Learning representations that are effective for unknown downstream tasks is a critical challenge in representation learning. Prominent methods addressing this challenge include contrastive learning (\eg, \cite{DBLP:journals/corr/abs-2002-05709,DBLP:journals/corr/abs-1807-03748}),
self-supervised masking (\eg, \cite{DBLP:journals/corr/abs-1810-04805}), and denoising auto-encoders (\eg, \cite{DBLP:journals/corr/abs-1305-6663}).

In this paper, we introduce a novel method, termed \textit{cMIM} (\ie, Contrastive MIM), designed to learn representations that are highly useful for downstream tasks. cMIM integrates a contrastive learning loss with the Mutual Information Machine (MIM) framework by \cite{2019arXiv191003175L}.
MIM is a probabilistic auto-encoder that maximizes the mutual information between the input and latent representation while clustering the latent codes. However, preliminary experiments indicated that the representations learned by MIM were less effective for discriminative downstream tasks compared to state-of-the-art (SOTA) models. The cMIM method directly addresses this limitation.

Main contributions:
\begin{enumerate}
\item We propose a novel contrastive extension to the Mutual Information Machine (MIM) framework for learning discriminative representations. This method eliminates the need for data augmentation (\ie, no "positive" samples are required) and is robust to the number of negative samples (\ie, typically the batch size).
\item We introduce a generic method for extracting embeddings from encoder-decoder models, which we term \textit{informative embeddings}. This approach significantly improves performance in discriminative downstream tasks without requiring additional training and is compatible with any pre-trained encoder-decoder model.
\end{enumerate}

By introducing cMIM, we aim to provide a unified generative model that is effective for both generative and discriminative tasks. Our results demonstrate that the learned representations are highly effective for downstream tasks while preserving the generative capabilities of MIM.

