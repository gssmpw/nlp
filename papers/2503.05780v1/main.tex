\documentclass{article}
\usepackage{graphicx} % Required for inserting images

% \documentclass[a4paper,12pt]{article}  % Standard article class with A4 paper size

% Essential packages
\usepackage[a4paper, margin=1in]{geometry}  % Set margins
\usepackage{graphicx}  % For including images
\usepackage{titlesec}  % For section formatting
\usepackage{fancyhdr}  % Custom headers and footers
\usepackage{hyperref}  % Clickable links
\usepackage{amsmath, amssymb}  % Math symbols if needed
\usepackage{todonotes}  % For comments (optional)

\usepackage[edges]{forest}
\usepackage{xcolor}
\usepackage[skins]{tcolorbox}%
\usepackage{tcolorbox}
\usepackage{hyperref}
\newtcolorbox[auto counter, number within=section]{definitionbox}[2][]{%
colframe=black!50,
colback=black!5,
coltitle=white,
fonttitle=\bfseries,
title={#2},
sharp corners=south,
enhanced,
before upper={\noindent},
}

\newcommand{\ignore}[1]{}

% Title and author
%\title{\textbf{AI Risk Atlas (Nexus? open to either) - Taxonomy and Tooling for Navigating AI Risks and Resources}}
\title{\textbf{AI Risk Atlas:\\
Taxonomy and Tooling for\\Navigating AI Risks and Resources}}

\author{Frank Bagehorn, Kristina Brimijoin, Elizabeth M. Daly, Jessica He,\\Michael Hind, Luis Garc√©s-Erice, Christopher Giblin, Ioana Giurgiu,\\Jacquelyn Martino, Rahul Nair, David Piorkowski, Ambrish Rawat, John Richards,\\Sean Rooney, Dhaval Salwala, Seshu Tirupathi, Peter Urbanetz,\\Kush R. Varshney, Inge Vejsbjerg, Mira L. Wolf-Bauwens}
%\date{\today}

\begin{document}

\maketitle  % Generate title

\thanks{Authors are listed in alphabetical order by last name.}




% \begin{document}

% \maketitle

\begin{abstract}
The rapid evolution of generative AI has expanded the breadth of risks associated with AI systems. While various taxonomies and frameworks exist to classify these risks, the lack of interoperability between them creates challenges for researchers, practitioners, and policymakers seeking to operationalise AI governance. To address this gap, we introduce the \textbf{AI Risk Atlas}, a structured taxonomy that consolidates AI risks from diverse sources and aligns them with governance frameworks. Additionally, we present the \textbf{Risk Atlas Nexus}, a collection of open-source tools designed to bridge the divide between risk definitions, benchmarks, datasets, and mitigation strategies. This knowledge-driven approach leverages ontologies and knowledge graphs to facilitate risk identification, prioritization, and mitigation. By integrating AI-assisted compliance workflows and automation strategies, our framework lowers the barrier to responsible AI adoption. We invite the broader research and open-source community to contribute to this evolving initiative, fostering cross-domain collaboration and ensuring AI governance keeps pace with technological advancements.
%\todo{MH: I decided to write a short abstract to capture what I think the flow of the intro is. Feel free to improve this draft. I also massaged the title to match.}
\end{abstract}

\section{Introduction}
% NET: AI risks are important, but need help to stay current and to bring the risk identification and mitigation communities together.
%
Identifying the risks of AI systems has attracted interest from research~\cite{weizenbaum1976computer, russell2016artificial}, industry~\cite{shahriari2017ieee}, and policy makers~\cite{carricco2018eu}.
%The risks associated with AI systems is something research \cite{weizenbaum1976computer, russell2016artificial}, industry \cite{shahriari2017ieee} and policy makers \cite{carricco2018eu} have understood for some time.
%\todo{MH: I think the previous seems to imply we know everything about the risks of AI systems, so I massage this.}
These perspectives spawned many innovations to aid in the creation and operationalising of responsible AI system design \cite{aif360, arya2021ai, wexler2019if, bird2020fairlearn}. % Despite these efforts, their adoption has remained ad hoc and largely focused on a small subset of AI risks.
%\todo{MH: I'm not sure we need the last 2 sentences. The first sentence is only pointing to toolkits, but there is a lot more in this general space. The 2nd sentence takes a position I'm not sure we need to take. Elizabeth: I removed the 2nd sentence if you feel it is problematic. I think highlighting that attention on risks helps the community focus on buidling algorithmic solutions isn't a bad message...}
With the advent of generative AI and its rapidly evolving capabilities~\cite{bommasani2021opportunities}, the spectrum of risks, and the urgency to mitigate them, has increased. As a result, there is a need to help the community identify and address these risks in parallel, while also creating a mechanism for collaboration.

%NET: Other efforts to identify risks don't solve all the problems
There have been several efforts to catalogue risks associated with AI systems \cite{nist,owasp,airiskrepo,mit-risk-repository,zeng2024ai}. However, connections and relationships to existing risk classification frameworks are missing. This lack of connectivity can present a challenge for practitioners who may want to adopt new risk taxonomies, but have already categorised their assets using existing definitions. 

% NET: there are flourishing communities for sharing datasets and benchmarks, but these haven't been focused on risks, so there is an opportunity to expand the impact of these communitities.
%
Meanwhile, there are currently flourishing communities for sharing datasets~\cite{lhoest2021datasets} and benchmarks~\cite{eval-harness}. Although the focus of datasets effort is to evaluate the correctness of an AI model, typically partitioned by AI task or skill, many of these datasets can also be leveraged to assess various AI risks. Similarly, mapping the results of benchmarks to risk concerns is currently not a part of most benchmarks, but could be accomplished with better collaboration between the benchmark and risk communities.

%%HuggingFace provides thousands of valuable datasets contributed via the AI research community \cite{lhoest2021datasets}, however, while the focus has been on specific tasks or skills a large number may also aid in assessing risks. Similarly, lm-eval-harness contains many benchmarks, however, the interpretation of the outcome of these benchmarks and how they might reflect risk concerns is left to the data scientist to determine by studying related resources and documentation \cite{eval-harness}.

%NET: AI gov in practice is involved. Our tooling info can provide a foundation for future innovations to help automate some of the process
%
%Automation can play a vital role in easing the barrier for entry for developers and practitioners to make responsible AI an integral part of their process, however to support operationalisation, some structure must be put in place.
To help manage risks, the process of putting a new AI system into production often includes multiple stakeholders such as business owners, risk and compliance officers, and ethics officers approving the AI system for a specific usage. Governance frameworks to manage this process typically include multiple manual steps, including curating information needed to assess risks (where will the system be used? who is the target user?) and reviewing outcomes to identify appropriate actions and governance strategies.
Automation can play a vital role in easing the barrier for entry for developers and practitioners to make responsible AI an integral part of their process, however, this automation requires some structure of the underlying information.
For example, AI capabilities can help to create better semi-structured governance information, identify and prioritize risks according to the intended use case, recommend appropriate benchmarks and risk assessments and most importantly recommend mitigation strategies and recommended actions.  Our aim is to develop an AI Systems risk ontology that links
risk entities described using multiple different taxonomies with AI models,
evaluations, mitigations and other important entities. This ontology is manifested in a knowledge graph that associated tooling uses to integrate distinct risk frameworks, thereby providing the community with a way to align their assets with both new and existing risk definitions. 

This paper is organized as follows. Section~\ref{sec-risk-atlas} describes the AI Risk Atlas, which provides a taxonomy of AI Risks. Section~\ref{sec-tools} presents \textbf{Risk Atlas Nexus}, an open source tooling effort we have developed to enable inter- and cross-community collaboration. 
Section~\ref{sec-potential} discusses some of the future directions that can be facilitated by these tools.
Section~\ref{sec-conc} invites the broader community to expand on this initial seeding of tools by bringing different perspectives with different needs and ultimately lowering the barrier to AI governance.


%\textit{Original Outline
%\begin{itemize}
 %   \item Challenges companies, researchers and governments face in the fast changing AI landscape 
  %  \item The purpose and aim of the Risk Atlas and why it is so important
 %   \item Why is a taxonomy useful
  %  \item What can it be used for and what challenges does it address
  %  \item how can tooling help address additional challenges
%\end{itemize}
%}  
    

\section{The AI Risk Landscape} \label{sec-risk-atlas}
This section describes the \textit{AI Risk Atlas} which aims to provide clarity for practitioners of the risks associated with Generative AI systems. 

% High level description of the risk atlas
The AI Risk Atlas is a taxonomy of AI risks collected from prior research, real-world examples, and from experts in the field. It defines risks posed by AI systems and explain potential consequences of those risks. Each risk is grouped into one of four categories based on where the risk originates. The categories are input risks, inference risks, output risks and non-technical risks. Within each category, risks are further grouped into risk dimensions such as accuracy, fairness, or explainability. These dimensions classify the individual risks into groups, and enable a user of the Atlas to focus on the dimensions relevant to them. 


\begin{figure*}[!ht]
    \input{taxonomy}
    \caption{AI Risk Atlas Taxonomy. The taxonomy is divided into four categories, which are then further divided into dimensions. Next to each dimension in the parentheses is the number of risks identified for that dimension.}
    \label{fig:atlas-main}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/Hallucination.jpg}
    \caption{Screenshot of AI Risk Atlas Detail Page for Hallucination}
    \label{fig:atlas-detail-screenshot}
\end{figure*}

\ignore{
\begin{figure*}
\begin{definitionbox}{Hallucination}
Hallucinations generate factually inaccurate or untruthful content with respect to the model's training data or input. This is also sometimes referred to lack of faithfulness or lack of groundedness.\newline\newline
\textbf{Concern: }Hallucinations can be misleading. These false outputs can mislead users and be incorporated into downstream artifacts, further spreading misinformation. False output can harm both owners and users of the AI models. In some uses, hallucinations can be particularly consequential.\newline\newline
\textbf{Type: }output\newline
\textbf{Descriptor: }specific \newline\newline
\textbf{Implementation details: } \newline
ID: atlas-hallucination \newline
Tag: hallucination \newline
URI:  \href{https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/hallucination.html}{IBM AI Risk Atlas - Hallucination}\newline
\end{definitionbox}
\caption{AI Risk Atlas Entry Example }
\label{fig:atlas-detail}
\end{figure*}
}

%\begin{figure*}
%    \centering
%    \includegraphics[width=0.99\linewidth]{images/RiskAtlasMain-med.jpg}
%    \caption{AI Risk Atlas Main Page}
%    \label{fig:atlas-main}
%\end{figure*}



Figure~\ref{fig:atlas-main} shows the overall AI Risk Atlas taxonomy where each category may include a subset of risk definitions. 
For example, Figure~\ref{fig:atlas-detail-screenshot} shows a screenshot of the Hallucination risk from the AI Risk Atlas, which is in the Robustness dimension in the Output category in Figure~\ref{fig:atlas-main}. The details contain a description of the risk along with a description of why the risk is a concern, a public example, when available, of the risk being manifested, and any related risks in other popular taxonomies.

% Motivation and a bit of history
The creation of the AI Risk Atlas was motivated by the changing risk landscape due to the emergence and rapid success of generative AI. Before generative AI became ubiquitous, IBM Research had developed the techniques to evaluate and mitigate some risks such as fairness, explainability, adversarial robustness, privacy, and uncertainty for traditional (non-generative) models. This work evolved into open-source toolkits designed to measure, and in some cases, mitigate those risks~\cite{aif360,aix360,art360,aip360,uq360}. However, these toolkits did not initially account for the risks specific to generative models. The large amount of data used to train generative AI, the increasing complexity of the models, and the non-determinism prevalent were some of the factors that led to the identification of new risks for generative models. Prompt-based attacks, poorly curated training data, and hallucinations emerged as early risks identified by researchers and concerned citizens alike. The identification of risks had begun to outpace mechanisms for measuring and understanding the risks. 

%\todo{Should we also include the ethics board white paper that came out here? (The first version of the risk atlas was an evolution from the content there). Elizabeth: Yes I think that is a good idea.}

To address this gap and to provide a foundation for understanding the risks of both traditional and generative AI models, 
IBM's AI Ethics Board created a white paper~\cite{ethics-board-pov} that provided the foundation for the AI Risk Atlas.
The goal for the Atlas was multi-faceted. First, we wanted to have a single source of information for currently known AI risks. This would enable a shared vocabulary when discussing AI risks. Second, we wanted to identify opportunities for measuring and mitigating the newly identified risks. This would help identify research opportunities for underrepresented risks. Third, we wanted a resource to develop usage-based governance. Specifically, we wanted to address the question of what risks are relevant to particular use cases.


The AI Risk Atlas has been used as a conversation starting point with enterprises who are considering deploying AI. It helps these organizations to be aware of the possible risks they need to govern. It provides a palette or vocabulary of risks that enterprises can consider: they can decide the risk is relevant to their use case and develop a plan to mitigate the risk either with tooling, human oversight, or both.
For those risks that aren't applicable for a use case, the organization can document this risk to help demonstrate their risk governance framework. However, the AI Risk Atlas can be used further than just a conversation starter. It can provide the underlying vocabulary for the complete management of the risk from development to deployment to monitoring \cite{watsonx-gov-dec-2024,mra-vision, daly2024usage}.
%IBM has recently released functionality to use the atlas for automated risk identification~\cite{watsonx-gov-dec-2024} and can also be used as the foundation for risk evaluation~\cite{mra-vision}.

% Focus on motivation, categorization, and amplified risks.

\section{Tools for Practitioners} \label{sec-tools}
The IBM AI Risk Atlas has been used many enterprise customers to help them reason about the risks in their AI systems. In order to enable efforts to leverage these risks to  operationalise governance and risk mitigation frameworks we created \textit{Risk Atlas Nexus.}

The \textit{Risk Atlas Nexus} is a collection of tooling to help bring together disparate resources related to governance of foundation models. We aim to support a community-driven approach to curating and cataloguing resources such as datasets, benchmarks and mitigations. Our goal is to turn abstract risk definitions into actionable workflows that streamline AI governance processes. By connecting fragmented resources, Risk Atlas Nexus seeks to fill a critical gap in AI governance, enabling stakeholders to build more robust, transparent, and accountable systems. The Risk Atlas Nexus is a step towards enabling the following.

\textbf{Navigating disparate risk taxonomies:} IBM AI Risk Atlas is one amongst a number of existing risk taxonomies, for example; the OWASP Top 10 for LLMs and Generative AI Apps \cite{owasp}, the NIST AI Risk Management Framework \cite{nist}, the MIT AI Risk Repository \cite{airiskrepo}, the AIR taxonomy 2024 \cite{zeng2024ai}.

To provide a way through this labyrinth of taxonomies, we have constructed an AI risk ontology that allows both the creation of a knowledge graph containing those different taxonomies and the ability to map between them. The ontology has been modeled using LinkML \cite{moxon2021linked}, which allows the generation of different data representations (e.g. RDF, OWL) in a simple way. The risk taxonomies have been stored as LinkML data instance YAML files. To express some semantically meaningful mapping between risks from different taxonomies, we have used the Simple Standard for Sharing Ontological Mappings (SSSOM) \cite{sssom}. Therefore those mappings are maintained in SSOM TSV files and are converted to LinkML data YAML using Python helper scripts.

Sample notebooks demonstrate how to load the LinkML data and user data and how to get details about specific risks and their relations to risks in other taxonomies.

\textbf{Question Answering:} Compliance questionnaires are usually required prior to deploying an AI model into production. These enable a thorough understanding of the specific use case and associated risk exposures~\cite{watsonx-gov-dec-2024,lee2023qb4airaquestionbankai}. The Risk Atlas Nexus supports the development and curation of questionnaires to a desired taxonomy. Additionally, the content can support Large Language Models (LLMs) to assist users in responding to time-consuming compliance questionnaires, thereby reducing manual effort and minimizing errors~\cite{daly2024usage}. Similarly, other aspects like risk identification, guardrail implementation, and identifying security vulnerabilities for specific use cases can be largely automated with human feedback and sign-off provided only when necessary.

\textbf{Use Case to Risk Prioritisation:} To help prioritise which of the many risks are most related to their use case we leverage LLM-as-a-judge capabilities to identify which risks to consider. This information can be used to look for appropriate research papers, benchmarks and metrics. In a similar manner Risk Atlas Nexus can be used to tag disparate resources by passing in text such as a paper abstract or a dataset description as the risks for the basis for an LLM-as-a-judge definition \cite{ashktorab2024aligning,desmond2025evalassist}. 


\textbf{From Risks to Mitigating Actions:} The knowledge graph supports mapping between risks to two types of mitigation strategies: detectors and recommended actions. Detectors such as Granite Guardian~\cite{padhi2024graniteguardian} dimensions could be run in tandem with an AI system to better protect against certain risks such as social bias and prompt injection attacks. We have also mined recommended actions as part of the NIST AI Risk Management  Framework~\cite{nist} to be able to recommend more process driven mitigation strategies. 

\textbf{Bring Your Own Risks, Relationships and Questionnaires:} Risk Atlas Nexus tooling supports several well known risk taxonomy frameworks, however some organisations may wish to define their own custom concerns and definitions.  Risk Atlas Nexus allows users to define custom questionnaire templates as well as taxonomies, risks, mappings, and mitigation actions which should conform to the  \href{ https://github.com/IBM/risk-atlas-nexus/blob/42a42aebf87cdf18232105ab57ffef69331e322d/docs/ontology/index.md }{ontology schema}. We encourage users to contribute their taxonomy definitions and mappings back to the project for others to use through the open-source project.  

\section{Potential for the future} \label{sec-potential}
There is immense potential of automation of various aspects of compliance and risk management processes. While human oversight and manual verification are still essential requirements for compliance, auditing and regulatory purposes, automation can help to bring an efficient execution of intermediate stages in the compliance workflow. AutoML strategies have been employed to automate model training pipelines excelling at tasks such as feature selection,  hyper parameter optimization, model generation and evaluation \cite{he2021automl,wang2020autoai}. In a similar manner AI governance pipelines could be employed to detect and mitigate risks. By starting with identifying the most relevant risks, running the most relevant benchmarks and then assessing the impact of employing real-time mitigation strategies, concrete recommendations can be made to improve safety of AI solutions.

In the context of complex systems LLMs are increasingly being used as part of the validation process, from software testing and assisting in tasks such as test case preparation \cite{wang2024software} to assessing the output of an LLM \cite{van2024field,shankar2024validates, ashktorab2024aligning}. With the increasing capabilities of LLMs their applications have gone beyond single function tasks to being used to address complex problems acting as autonomous-agents \cite{wang2024survey}. ToolLLaMA learns how to call appropriate API based tools \cite{schick2023toolformer} meaning LLMs can orchestrate tasks that leverage existing functionality embedded in other tooling. The research community has begun to use agent flows to design, plan and execute scientific experiments  \cite{boiko2023emergent} and even write papers \cite{lu2024ai}.

%\begin{itemize}
%    \item AutoML opportunities for CI/CD pipelines
%    \item Agentic solutions for lifecycle governance
%    \item Potential links to policy and regulations
%    \item Open questions and challenges. How can practisioners use the information in the risk atlas to communicate to stake holders
%\end{itemize}

Given these evolving capabilities agentic frameworks\cite{lang-graph,crewai,autogen,bee-ai-framework} have the potential to create real-time governance pipelines, from identifying relevant risks and benchmarks to identifying mitigating actions to online monitoring capabilities. To reliably build such pipelines a method to organise and connect these functional components together must be created to curate appropriate meta-data. 
%\todo{Elizabeth: We really need some related work here on LLM Agentic frameworks. Mike: I added 4 citations to frameworks. I'm not sure if you wanted more than that. Elizabeth: Thank you!}



%Further, to ensure adaptive realtime governance of LLMs, an agentic framework can be established. This can enable monitoring of LLM models in production for drift, enhanced risk exposure or security attacks. The agent systems should inherently possess the capacity for self-improvement through mechanisms such as continuous learning from experience, data, and feedback, ensuring refinement of AI governance performance and adaptability. This is essential for maintaining reliability, and overall effectiveness of AI governance systems over time.

% \begin{itemize}
%     \item Potential for Automation and Scaling 
%     \item Intent to Governance for AI Lifecycle pipelines [Seshu]
%         \todo{MH: Here are 2 citations\cite{watsonx-gov-dec-2024,lee2023qb4airaquestionbankai} that can be used as examples for risk identification questionnaires. The first is the best we have for what was shipped in x.gov (Marc will create a blog post in the near future) and the 2nd is an external group.}
%     \item Agentic workflows for real-time dynamic governance [Seshu]
% \end{itemize}

\section{Conclusion and call to action} \label{sec-conc}

Curating the AI Risk Atlas is just the first step in providing a reference framework for researchers and practitioners navigating the rapidly evolving AI landscape. By positioning our risk taxonomy in relation to existing definitions and taxonomies, we aim to encourage the community to map new risk definitions, datasets, benchmarks, research papers, and crucially mitigation and detection strategies into a structured framework. This approach will enhance accessibility and facilitate the operationalisation of AI governance processes.

The initial tools released as part of the Risk Atlas Nexus toolkit represent only the beginning of what is possible. We are committed to ongoing development, enabling the developer community to contribute and expand this initiative. By fostering a community-driven approach, we can lower the barrier to entry for all. We invite the open-source community to enrich the knowledge graph by linking benchmarks, datasets, and research papers to identified risks. Additionally, contributors can request new functionality through GitHub enhancement requests or develop and integrate their own algorithms.





\bibliographystyle{plain}
\bibliography{refs}

\appendix
\input{ibm-ai-risk-atlas-risks-formatted}

\end{document}
