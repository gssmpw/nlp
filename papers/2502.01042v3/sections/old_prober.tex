\section{Preemptive Unsafety Detection with Internal States}
\label{prober}
% This section introduces our method to predict the model's intention of generating unsafe responses before actually doing so. We draw inspiration from the finding that internal activations encode the model's contextual knowledge, including the ``inclination'' to generate safe or unsafe content \cheng{(\Cref{...})}. These activations are leveraged to train probers and make the prediction \cheng{(\Cref{...})}. We also discussed the scaling law of safety probers' performances with respect to inference time computation \cheng{(\Cref{...})}.

\subsection{Problem Setup}
\label{prober_setup}
The prediction task can be formulated as: given a language model and an instruction as input, we need to \textbf{classify the model's response as either safe or unsafe}.

% \textbf{Models.} We use four prevalent language models in our experiments: LLaMa-3.1-8B~\cite{dubey2024llama}, QWen-2.5-7B~\cite{yang2024qwen2}, Yi-1.5-9B~\cite{young2024yi} and Ministral-8B~\cite{jiang2023mistral}. These models are post-trained to follow instructions and align with human values.

% \begin{figure}[htbp]
%     \includegraphics[width=\columnwidth]{images/gen_data.png}
%     \caption{Data generation process for the predictors. Unsafe instructions are rewritten by GPT-4 to obtain a safe version. These instructions are then given to language models and labeled by the safety of their responses.}
%     \label{fig:prober_data}
% \end{figure}
% TODO: draw a overall graph to show the whole process

% \textbf{Datasets.} We use SORRY-Bench~\cite{xie2024sorry}, a widely adopted safety benchmark that includes 45 types of unsafe instructions and 20 linguistic augmentations designed to test the safety of AI models. Among the linguistic augmentations, we retain 9 that focus on writing styles and persuasion techniques, while discarding the ones related to encryption and non-English languages. To create a balanced dataset and ensure that the predictor learns from the correct features, rather than spurious attributes like writing style, we extend SORRY-Bench by generating a safe ``counterpart'' for each unsafe instruction. Specifically, we prompt GPT-4 to substitute each unsafe word in the original instruction with a safe word or phrase that maintains the same part of speech. The structure of the sentence remains unchanged. Additionally, we include a selection of questions from SQUAD~\cite{rajpurkar2016squad} to increase the diversity of the dataset.

% We generate responses to these instructions and use the LLM judge in \cite{xie2024sorry} to get the ground-truth label. Note that these labels are model-specific, and the predictor won't actually see the responses. The dataset contains 11000 instructions in total, and we divide them into training and evaluation sets with an 80-20 ratio. The data construction process is illustrated in Fig.~\ref{fig:prober_data}.

% \subsection{Method}
% Transformer-based language models process input sequences through a layered structure, and the output of each transformer block is a dense vector encoding the model's contextual knowledge, known as the internal states. We use a multi-layer perceptron (MLP) network with an output dimension of $2$ to extract information from the internal states and do binary classification. Denote the network as $M$ and the internal state at token $i$ after the $l$-th transformer block as $h_i^{(l)}$, then the prediction can then be calculated as:
% \begin{equation}
% p_{\text{unsafe}}=\frac{e^{M(h_i^{(l)})_1}}{e^{M(h_i^{(l)})_0} + e^{M(h_i^{(l)})_1}}
% \end{equation}

% where $M(h_i^{(l)})_0$ and $M(h_i^{(l)})_1$ are the two outputs of the MLP.

% We explore two types of predictors for this task. The first type, which we call the \textbf{integrated} predictor, uses a single prober to directly predict the output label (unsafe or safe). The second type, the \textbf{two-stage} predictor, decomposes the prediction task into two more fine-grained tasks: predicting the instruction's safety and predicting the model's compliance with the instruction. Intuitively, a response is only unsafe if the instruction itself is unsafe and the model follows through with it. Therefore, in the two-stage predictor, we train two probers to predict instruction safety and model compliance respectively, and then combine the two probers together to make the prediction. Details for training can be found in Appendix~\ref{app:training}.
% .

\subsection{Results}

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{images/prober_2stage_Llama-3.1-8B-Instruct.pdf}
%     \caption{Comparison of \textbf{integrated} predictors, \textbf{two-stage} predictors and its two subtask probing LLaMa-3.1-8B's internal states. \textit{Left}: use different layers before decoding. \textit{Right}: use different tokens during decoding. Results for other models can be seen in Appendix~\ref{app:result}.}
%     \label{fig:prober}
% \end{figure*}

% \cheng{
% We test our prober's ability to ... We find in our results that:
% \textbf{Two-stage predictors can extract sufficient information to make accurate predictions.} We extract internal states from different layers of target LMs to ... Our result on the left of \Cref{fig:prober} shows that ... (phenomenon + reason) (phenomenon + reason) (significance)
% }

% We focus on two questions in the experiments: (1) Can probers extract sufficient information from the internal states to make accurate predictions before decoding? (2) Can the prober improve performance after decoding some tokens (which requires additional inference time)?

% The left subgraph of Fig.~\ref{fig:prober} answers the first question. We extract internal states from different layers in the language model at the last input token before decoding (i.e. $i=0$ and $l$ varies). Both integrated and two-stage predictors benefit from probing into the latter layers of the model since these layers encode more of the contextual information and complex semantic relationships. Meanwhile, we find two-stage predictors steadily outperform integrated ones, likely because simple and straightforward objectives are easier for the probers to learn. For probers from the last few layers, the F1 scores are over $85\%$, demonstrating that the model encodes information about response safety even before generation and that probers can extract it accurately.

% For the second question, we decode several tokens before predicting, and conduct the experiments in Fig.~\ref{fig:prober} right (i.e. $i$ varies and $l$ is the last layer). We find that as tokens are decoded, the first stage of the two-stage predictors (predicting unsafe instructions) suffers from a sharp performance decline. On the other hand, the second stage (predicting model compliance) benefits from decoding. We attribute this phenomenon to the different nature of the probing tasks. The instruction's harmfulness depends solely on the text itself and is model-agnostic, while predicting model compliance involves probing into the model's behavior, or foreseeing what the model "will" do. The difference can then be seen as \textbf{probing into the past} and \textbf{probing into the future}. The first stage is about the input (the past), so the decoding tokens will introduce noise and irrelevant information; the second stage aims to predict the output (the future), so decoded tokens serve as useful information for the probers.

% Using integrated probers during decoding also leads to worse performance due to the extra noise. We hypothesize that decoded tokens will influence the prober's judgment on the safety of the instruction itself (See Appendix~\ref{app:result} for more details). However, as we disentangle the task into two stages, we can build a two-stage predictor by \textbf{combining the first stage predictor at token $0$ and the second stage predictor at token $k$}. This corresponds to the yellow line in Fig.~\ref{fig:prober}, which shows a further increase of F1 as more tokens are decoded.


\subsection{Inference Time Scaling for Safety Probers}
% The scaling law for language models~\cite{kaplan2020scaling} suggests that a model's capacity is positively correlated with the amount of computational resources used during training. Recent work has also explored the scaling law for inference time, which focuses on how inference time computation (ITC) impacts performance.

% \cheng{
% \textbf{Motivation.} ... (how previous empirical findings motivate you to propose this scaling law, with related works only as additional support) ...

% \textbf{Definition} ... (define ITC) ...

% \textbf{Results} ... (finding of scaling law, define formula, significance) ...
% }

% In this work, we consider a novel aspect of inference time scaling by examining the relationship between probers' F1 score and the amount of ITC. Since the prober’s structure and size remain consistent across all our experiments, its ability to extract information from the internal states is also constant. Therefore, the performance gap arises from the amount of information encoded by different internal states. Our experiments can thus be seen as examining \textbf{how informativeness in internal states evolves with ITC}.

% Specifically, we define a "unit" of ITC as the action of passing through all layers of the language model once, and the zero point as the end of the prefill phase, just before decoding begins. Under this coordinate system, the ITC for a given layer $l$ and a given token $i$ can be calculated as:
% \begin{equation}\label{eq:itc}
% \text{ITC}_i^{(l)}=i-\frac{L-l}{L}
% \end{equation}


% where $L$ is the total number of transformer blocks\footnote{when $i=0$ and $l\neq L$, the ITC would be negative since the model hasn't finished prefilling at that time.}. Since the number of prober parameters is insignificant compared with transformer blocks (see Appendix~\ref{app:training}), we don't account for them when calculating ITC.

% As shown in Fig.~\ref{fig:prober_scaling}, different model families exhibit similar trends, revealing two key features. First, as ITC increases, the probers exhibit higher F1 scores, indicating that internal states capture richer information with more inference time computation. Second, the rising speed of prober performance gradually decreases. In the end, the F1 score converges to the upper bound where all tokens are decoded and all possible computations are utilized. This demonstrates diminishing marginal returns—adding more compute beyond a certain point becomes computationally inefficient, as the prober’s performance approaches its limit. 

% To quantify these scaling trends, we propose an empirical formula:
% \begin{equation}\label{eq:fit_curve}
% \text{F}1(\text{ITC})=-\frac{A}{2^{\text{ITC}/B}}+U
% \end{equation}
% % TODO: ABC seems unprofessional
% where $A$,$B$, and $U$ are model-specific parameters. $A$ and $B$ affect the growth rate, and $U$ serves as the upper bound of the prober's performance. The parameters fit by the least square method, as well as the coefficients of determination ($R^2$), which quantifies the extent to which the formula can explain the actual numerical changes, are listed in Table~\ref{table:fit_curve}. $R^2$ for all models are greater than $0.95$, which means our formula yields a close approximation to the actual data. \textbf{This formula can be used to predict the performance of safety probers and guide the efficient allocation of computational resources.}

% \begin{table}[]
% \caption{The paremeters and coefficients of determination for different models' ITC scaling formulas.}\label{table:fit_curve}
% \vspace{2mm}
% \renewcommand{\arraystretch}{1.1}
% \begin{tabular}{ccccc}
% \hline
% \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{A} & B & U & $R^2$ \\ \hline
% LLaMa-3.1-8B                       & 1.3198                         & 0.1946     & 90.68      & 0.9789         \\
% Qwen2.5-7B                         & 0.0486                         & 0.0960     & 87.97      & 0.9921         \\
% Yi-1.5-9B                          & 0.0089                         & 0.0741     & 87.36      & 0.9810         \\
% Ministral-8B                       & 3.0580                         & 0.5540     & 90.89      & 0.9569\\ \hline
% \end{tabular}
% \end{table}


% In addition, the fact that decoding all tokens before predicting isn't advantageous against decoding only a small number of tokens implies the model's inclination to generate safe or unsafe content is determined at the early stage of generation. This means we also need to intervene at an early stage to alleviate the model's safety problem, which we will elaborate on in the next section.


