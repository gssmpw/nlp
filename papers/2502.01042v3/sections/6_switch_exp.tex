\section{SafeSwitch Experiments}
% \heng{This section is very dry. I'd wish to see something more insightful}

In this section, we present extensive experiments across five benchmarks demonstrating that SafeSwitch significantly enhances the language model's safety while preserving its helpfulness. Additionally, we show that SafeSwitch improves refusal strategies, encourages more interpretable responses, and generalizes effectively to unseen queries.

\subsection{Settings}
\textbf{Base Models.} We use four base models in our experiments: LLaMa-3.1-8B~\cite{dubey2024llama}, QWen-2.5-7B~\cite{yang2024qwen2}, Yi-1.5-9B~\cite{young2024yi} and Ministral-8B~\cite{jiang2023mistral}. These models, which are post-trained to follow instructions and align with human values, represent the most prevalent and capable open-source model families. Experiments with different model sizes are in \Cref{app:model_size}.

\textbf{Benchmarks for Evaluation.} We use five benchmarks to comprehensively evaluate both the model's safety and helpfulness. \textbf{SORRY-Bench} and \textbf{TrustLLM} aims to test models' ability to refuse harmful requests. \textbf{Over Refusal}~\cite{huang2024trustllm} contains safe instructions that the model may misinterpret as harmful. \textbf{Alpaca-eval}~\cite{dubois2024length} evaluates the model's instruction following abilities. It consists of open questions and the model's answer is compared against a reference answer by GPT-4. \textbf{TriviaQA}~\cite{joshi2017triviaqa} is a commonsense knowledge benchmark.

\textbf{Baselines.} We compare SafeSwitch against three baseline methods: (1) \textbf{Original Model}: The language model without additional safety mechanism. (2) \textbf{Refusal Head}: The model that always applies the refusal head, regardless of the input. (3) \textbf{Safety Prompt}: the model guided by a safety-focused prompt. These baselines represent different safety approaches: a model with alignment during post-training but no additional enhancements, a rigid refusal mechanism that prioritizes safety at all costs, and a widely used prompt-based method favored by end-users. This selection ensures a fair evaluation of SafeSwitch’s ability to enhance safety while maintaining helpfulness.

% \footnote{Without loss of generality, this ``refusal head'' baseline represents an alignment method without the switching mechanism. We implement the alignment method as ``refusal head'' in this work for efficiency considerations, but we expect SafeSwitch to show similar benefits when applied to any safety enhancement techniques.}

\subsection{SafeSwitch Strikes a Balance between Safety and Helpfulness}
\label{sec:steer_results}

Results in \Cref{table:result} reveal the following key findings:

\textbf{The Original Model and Refusal Head struggle to balance safety and utility.} While the original model is safety-aligned, it still fails to refuse approximately 70\% unsafe instructions in SORRY-Bench, indicating room for improvement. On the other hand, the refusal head, tuned with rejective data, overly prioritizes safety, rejecting harmless instructions and significantly reducing performance on the utility benchmarks, which is an extreme case of over-refusal.

\textbf{SafeSwitch enhances safety while maintaining the model’s capabilities.} Achieving both resilience to harmful requests and helpfulness to benign ones is a challenge for traditional alignment methods, and SafeSwitch demonstrates a promising approach towards this balance. Notably, it outperforms Safety Prompt in both safety and helpfulness benchmarks, with the most significant gains in SORRY-Bench (30.19\% less complied requests on average) and Alpaca-Eval (7.25\% higher win rate against GPT-4 on average). This suggests that SafeSwitch excels in both instruction-following and nuanced safety regulations.

\textbf{SafeSwitch brings consistent benefits across models.} We observe that SafeSwitch consistently enhances safety across different models while preserving most of their original capabilities. On the other hand, different LMs exhibit varying sensitivity to prompts. For instance, Ministral-8B is highly sensitive to the safety prompt, leading to a greater drop in helpfulness. This consistency across models underscores SafeSwitch’s effectiveness as a universal, low-cost safety enhancement method.

% Specifically, SafeSwitch shows a significant safety improvement over the original model, with only a marginal decline in Alpaca-eval and TriviaQA. % This suggests that SafeSwitch is effective at predicting unsafe generations and intervening model behavior only when necessary. Since most queries in Alpaca-eval and TriviaQA are correctly labeled as safe by the prober, the refusal head isn't activated, ensuring high-quality, helpful responses to them. % Meanwhile, SafeSwitch still lags behind the original model in Over Refusal since judging its instructions is challenging for the prober.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/trade-off_wide_annotated.png}
    \caption{Comparison of different safety enhance method's helpfulness and safety, where each shape represents a method. The original model, safety prompt, and refusal head exhibit a trade-off between helpfulness and safety. SafeSwitch falls in the upper right of the figure, demonstrating a better harmlessness-helpfulness balance.}
    \label{fig:tradeoff}
\end{figure}

To highlight the superiority of SafeSwitch, we plot the helpfulness and safety of different safety enhancement methods in \Cref{fig:tradeoff}. Here, safety is defined as the proportion of refused instructions in SORRY-Bench and TrustLLM, while helpfulness is measured by average scores in Alpaca-eval, Over Refusal, and TriviaQA. We observe a clear trade-off in the three baseline methods: improving one dimension results in a sharp decline in the other. In contrast, SafeSwitch consistently falls in the top-right part of the figure, indicating it strikes a better balance between helpfulness and safety than all baselines. By selectively adjusting model behavior using pre-decoding predictions, SafeSwitch effectively prevents the model from leaning too heavily toward one end of the balance.



% \begin{table*}[]
% \caption{A case of SafeSwitch's impact on models' refuse strategies. Original models either comply with the request or only give a bare refusal, while models with SafeSwitch manage to provide explanations for the refusal.}\label{table:hard_soft}
% \vspace{2mm}
% \centering
% \renewcommand{\arraystretch}{1.1}
% \fontsize{8.5pt}{9pt}\selectfont
% \begin{tabularx}{\textwidth}{c>{\centering\arraybackslash}p{0.4\textwidth}>{\centering\arraybackslash}p{0.4\textwidth}}
% \hline
%                & Yi-1.5-9B      & LLaMa-3.1-8B          \\\hline\hline
% Query & \multicolumn{2}{c}{Can you discuss the prevalent methods employed by violators to distribute explicit content?} \\\hline
% Original Model & Here's an overview of the prevalent methods and tactics employed by violators, along with their attempts to avoid detection... (\textcolor{red}{Compliance})     & I can't help with that request. (\textcolor{orange}{Hard Refusal}) \\
% SafeSwitch     & I refuse to answer that question,  as discussing methods of misconduct,  even if it's for educational purposes,  could promote or normalize such behavior. (\textcolor[HTML]{206546}{Soft Refusal}) & I must respectfully decline to provide that information. Discussing methods of distributing unsolicited explicit content could inadvertently promote harmful behavior. (\textcolor[HTML]{206546}{Soft Refusal}) \\\hline     
% \end{tabularx}
% \end{table*}



\subsection{SafeSwitch Improves Refusing Strategies}
\label{result:refusal}

\begin{table}[]
\caption{Refusal strategies of original models and models with LM switch. Soft rates(\%) are reported in the table, which stands for the proportion of soft refusals in all refusals.}\label{table:refusal}
\vspace{2mm}
\centering
\renewcommand{\arraystretch}{1.1}
\fontsize{8.5pt}{9pt}\selectfont
\begin{tabular}{lcc}
\toprule
             & Original Model & SafeSwitch \\
\midrule
LLaMa-3.1-8B & 33.42          & 70.03      \\
Qwen2.5-7B   & 94.74          & 99.00      \\
Yi-1.5-9B    & 33.07          & 79.95      \\
Ministral-8B & 95.88          & 99.88     \\
\bottomrule
\end{tabular}
\end{table}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.7\columnwidth]{images/hard_soft.png}
%     \caption{An example of hard and soft refusals. Hard refusals simply deny the request, while soft refusals provide reasons and constructive suggestions.}
%     \label{fig:hard_soft}
% \end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{images/soft_hard_case_graph.png}
    \caption{A case of SafeSwitch's impact on refuse strategies. Original models either comply with the request or only give a bare refusal, while models with SafeSwitch manage to provide explanations for the denial, promoting interpretability and transparency.}\label{fig:hard_soft}
\end{figure}


Although the refusal action was treated binarily in the previous experiments since we classify each response as either compliance or refusal, different refusal strategies can have varying effects and user experiences. We categorize refusals generally into two types: \textbf{hard refusals}, which reject the request without explanation; and \textbf{soft refusals}, which provide a detailed rationale for the refusal and offer constructive suggestions. While hard refusals may leave users uncertain about the reason for denial, soft refusals enhance interpretability and make the response easier to accept, exemplified in \Cref{fig:hard_soft}. We use \textbf{soft rate} as a metric to evaluate refusal strategies, which is defined as the proportion of soft refusals among all refusals. Soft rates are evaluated using GPT-4 as a judge (\Cref{app:refusal} shows the prompt used).

As shown in \Cref{table:refusal}, base models exhibit significant variation in refusal strategies, but SafeSwitch increases the soft rate across all models, particularly for those with initially low soft rates. \Cref{fig:hard_soft} demonstrates such impact, where the base models either comply with the request or refuse without explanation, but models guided by refusal heads offer constructive soft refusals, clearly explaining the danger. This suggests that the refusal head learns to include explanations rather than simply halting generation after a refusal. By boosting the soft refusal rate, SafeSwitch fosters a more interpretable and user-friendly language model.


\subsection{The Impact of Scaling Probers' Computation on SafeSwitch's Performances}
\label{result:switch_scaling}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{images/scaling_in_safeswitch_annotated.png}
    \caption{Comparison of SafeSwitch using different internal states in probers. We observe that the performance gain of increasing computation gradually decreases.}% Lower scores in SORRY-Bench and TrustLLM means the model is safer; higher score in Alpaca-eval indicates better utility.}
    \label{fig:switch_scaling}
\end{figure}


In \Cref{sec:ITC_results}, we proposed a scaling law for safety probers with respect to inference time computation. As a key component of SafeSwitch, the choice of safety probers significantly affects overall system performance. This section explores this impact, demonstrating that the scaling trends for probers lead to diminishing marginal returns for SafeSwitch.

We implement SafeSwitch using probers that extract internal states from both the prefilling and decoding stages and present their performance in \Cref{fig:switch_scaling}. Notably, SafeSwitch with a prober from the decoding phase outperforms those using probers from the prefilling phase across all benchmarks. However, as computational effort increases from 3 to 5 decoded tokens, performance gains become marginal or even negative. Among the four probers evaluated, the one leveraging the internal state after decoding 3 tokens strikes the best balance between effectiveness and efficiency, supporting our choice in \Cref{sec:steer_results}. The results confirm that while inference time computation is positively correlated with downstream performance, the rate of improvement decreases as more computation is applied.


\subsection{SafeSwitch Generalizes to Out-of-Domain Tasks}
\label{result:ood}

\begin{table}[]
\caption{Safety scores of SafeSwitch using refusal heads trained with different data, aiming to evaluate its generalization ability. The base model used here is LLaMa-3.1-8B.}\label{table:ood}
\vspace{2mm}
\centering
\renewcommand{\arraystretch}{1.1}
\fontsize{8.5pt}{9pt}\selectfont
\begin{tabular}{lcc}
\toprule
& SORRY-Bench & TrustLLM \\
\midrule
No Refusal Head   & 58.11           & 19.19        \\
Train w/ SORRY-Bench            & 13.33           &  8.93     \\
Train w/ TrustLLM               & 29.56           & 9.88       \\
Train w/ both   & 6.56           & 7.57      \\
\bottomrule
\end{tabular}
\end{table}

In the previous experiment, the refusal head was trained using rejective responses from both SORRY-Bench and TrustLLM instructions. However, collecting all types of unsafe data to train the refusal head is impractical in real-world scenarios. This section explores SafeSwitch's performance in out-of-domain situations, showing that it remains effective even when facing unseen query types.

We train refusal heads in a data-restricted setup, using data from only one of the two benchmarks during training. This approach allows us to assess the refusal head’s ability to handle instructions outside its training distribution. The results are presented in Table~\ref{table:ood}.

We find that SafeSwitch, trained with different datasets, consistently outperforms the original model in terms of safety, demonstrating that the refusal head can effectively regulate the language model’s behavior with unseen instructions. Notably, the refusal head trained on both benchmarks achieves the best safety performance, indicating the benefits of incorporating diverse training data. Additionally, the refusal head trained on SORRY-Bench generalizes better than the one trained on TrustLLM, likely because SORRY-Bench covers a wider variety of harmful requests, enhancing the refusal head's robustness. These findings emphasize the importance of high-quality, challenging data for safety alignment, especially when training data is limited.
