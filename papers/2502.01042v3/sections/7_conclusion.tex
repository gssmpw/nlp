\section{Conclusion}
This work introduces SafeSwitch, a novel dynamic safety enhancement approach that leverages internal state signals to proactively address safety concerns in language models. By employing a disentangled two-stage safety prober, SafeSwitch not only predicts unsafe responses before they are generated but also ensures that models comply with safety standards through a context-aware and informative refusal mechanism. Through extensive experimentation, we demonstrate that SafeSwitch effectively distinguishes between benign and harmful outputs, offering a nuanced balance between safety and utility. Moreover, we empirically validate the safety prober's inference time scaling law and its robustness across a range of unseen scenarios. Inspired by human cognitive processes, SafeSwitch represents a significant advancement in dynamic safety control for large language models, laying the foundation for more adaptable and effective safety alignment strategies that ensure language models are not only safe but also ethically and contextually aware in their actions.

% This work introduces \textbf{SafeSwitch}, a dynamic safety enhancement method utilizing internal state signals. SafeSwitch uses a two-stage safety prober to extract safety-related information from the internal state, thereby predicting unsafe responses before generating. When an unsafe response is predicted, the refusal head, a lightweight refusal-promoting module, will be applied to the original LM head, guiding the LM to refuse malicious queries. Extensive experiments show that the safety prober effectively discerns benign responses from harmful ones, which enables SafeSwitch to steer model behavior selectively, reaching a balance between safety and utility. Additional experiments show that (1) the prober's performance follows an exponential decay relationship with inference time computation; (2) SafeSwitch improves refuse strategies by prompting the model to give explanation; and (3) SafeSwitch generalizes well to unseen harmful queries. In conclusion, SafeSwitch, as an innovative approach to dynamically control model behavior through internal states, offers a promising new pathway for advancing safety alignment in language models.
