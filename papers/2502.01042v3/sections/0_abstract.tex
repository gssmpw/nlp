\begin{abstract}
% Long Version
Large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks but also pose significant risks due to their potential to generate harmful content. Although existing safety mechanisms can improve model safety, they often lead to overly cautious behavior and fail to fully utilize LLMs' internal cognitive processes. Drawing inspiration from cognitive science, where humans rely on reflective reasoning (System 2 thinking) to regulate language and behavior, we empirically demonstrate that LLMs also possess a similar capacity for internal assessment and regulation, which can be actively detected.
Building on this insight, we introduce \textbf{SafeSwitch}, a framework that dynamically regulates unsafe outputs by monitoring and utilizing the model's internal states. Our empirical results show that SafeSwitch reduces harmful outputs by over 80\% on safety benchmarks while maintaining strong utility. Compared to traditional safety alignment methods, SafeSwitch delivers more informative and context-aware refusals, demonstrates resilience to unseen queries, and achieves these benefits while only tuning less than 6\% of the original parameters. These features make SafeSwitch a promising approach for implementing nuanced safety controls in LLMs.Codes for this work are available at \url{https://github.com/Hanpx20/SafeSwitch}.
% \heng{make it clear these internal signals means internal states in transformer}

% Short Version
% Large language models (LLMs) exhibit exceptional capabilities across various tasks but also pose risks by generating harmful content. Existing safety mechanisms, while improving model safety, often lead to overly cautious behavior and fail to fully leverage LLMsâ€™ internal cognitive processes. Inspired by cognitive science, we first show that LLMs, like humans using System 2 thinking, can perform internal assessments that are actively detectable. Building on this insight, we propose SafeSwitch, a dynamic framework that regulates unsafe outputs by monitoring and utilizing the model's internal states. SafeSwitch reduces harmful outputs by ~80\% on safety benchmarks while maintaining strong utility. SafeSwitch is also advantageous over traditional methods in offering more informative, context-aware refusals and reducing training costs, making it a promising approach for more nuanced and effective safety controls in LLMs.
\end{abstract}


% The risk of large language models (LLMs) generating harmful content has garnered significant attention, but existing safety enhancement methods often result in overly cautious behavior and reduced utility due to a lack of flexibility and failure to fully tap into LLMs' internal mechanism. We first show that LLMs internally structure their intentions before generating responses, a phenomenon similar to human deliberate cognition in language formulation. Building on this insight, we introduce SafeSwitch, a framework that monitors internal signals indicating model intentions and and leverages them to dynamically regulate model behaviors. Empirical results demonstrate that SafeSwitch reduces harmful outputs by 80\% on safety benchmarks while maintaining strong utility. SafeSwitch is also advantageous over traditional methods in offering more informative, context-aware refusals and reducing training and inference costs, making it a promising approach for more nuanced and effective safety controls in LLMs.