\section{Introduction}

% \heng{Sentences in Figure 1 are not all grammatically correct. Fix them}\cheng{fixed}

% \heng{I think the story can be told in a more compelling way - According to cognitive theory, humans generally "think before they talk" by first forming a mental representation of a concept or idea, often through a complex process of reasoning and analysis, before translating that thought into spoken language; essentially, the thought process happens first, and then the language is used to express it, not the other way around. Exiting work usually fixes the problem during LLM inference stage, but the LLM itself is not enhanced. We propose to open up the box, look into internal mental representation safety head...}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{images/main_fig.pdf}
    \caption{Overview of \textbf{SafeSwitch}. Unlike traditional alignment methods that tune the model on all queries, SafeSwitch dynamically regulates safety through a safety prober that monitors the model’s internal states. The computational cost of obtaining the state and the prober’s performance are both positively correlated with layer depth. When unsafe content is predicted by the prober, a specialized refusal head will be activated to generate informative refusals, making the response safe and helpful.
    }
    \label{fig:pipeline}
\end{figure*}
% SafeSwitch enhances safety effectively while maintaining model utility and achieving this at a relatively low computational cost.

Large language models (LLMs) have demonstrated remarkable capabilities in planning~\cite{valmeekam2023planning}, coding~\cite{chen2021evaluating,wang2024executable} and reasoning~\cite{yao2024tree,lightman2023let}, making them powerful tools for a wide range of tasks. However, their potential to generate harmful content, such as misinformation~\cite{zhang2023siren} and hate speech~\cite{albladi2025hate}, has sparked growing societal concern. Mitigating these risks is essential to ensure that LLMs operate ethically and align with societal values.
% \heng{any papers show previous methods sacrificed quality/utility? cite some concrete numbers?}\cheng{we add those papers in later paragraph}

To gain insights into enhancing LLM safety, we turn to human cognition, where the ability to regulate language and behavior is deeply rooted in reflective, premeditated processes. Cognitive science research reveals that humans form thoughts and intentions in brain regions such as the prefrontal cortex, which guides the formulation of language before it is articulated~\cite{friederici2011brain,fedorenko2016language}. This mechanism is a manifestation of ``System 2 thinking,'' a reflective process that evaluates potential actions and suppresses inappropriate responses~\cite{kahneman2011thinking}. Inspired by these insights, we naturally ask \textbf{whether LLMs possess a comparable capacity for internal assessment and regulation of their outputs}.

% Current safety paradigms often overlook the importance of pre-generation thought processes in LMs. Instead, models are typically tuned as a whole through alignment techniques like reinforcement learning~\cite{ouyang2022training,rafailov2024direct} and safety prompt optimization~\cite{zheng2024prompt}. While these methods have achieved notable safety improvements, they also come with significant drawbacks: i) traditional alignment methods often biases LMs uniformly toward refusal~\cite{arditi2024refusal}, resulting in the over-rejection of benign queries and thus reducing their helpfulness~\cite{lin2024mitigating}. ii) traditional alignment methods struggles to explicitly encode nuanced ethical reasoning, as it treats all outputs as end-to-end optimization targets rather than considering the model's internal states during the response process. Unlike humans, who engage in System 2 thinking to deliberate before acting~\cite{kahneman2011thinking}, current alignment techniques mostly focuses on post-generation corrections, which may fail to leverage the model's latent capacity for introspection.


Current safety paradigms often overlook the importance of pre-generation thought processes in LMs. Instead, models are typically tuned in an end-to-end manner through alignment techniques like reinforcement learning~\cite{ouyang2022training,rafailov2024direct} or safety prompt optimization~\cite{zheng2024prompt}. While these methods have achieved notable safety improvements, they also come with a significant drawback of biasing LMs uniformly toward refusal~\cite{arditi2024refusal}, which results in the over-rejection of benign queries and reducing their utility. For instance, \citet{lin2024mitigating} found that over-optimization in RLHF leads to a performance decline of up to 45\% across a wide range of tasks, including knowledge, reading comprehension, and translation.
This is an intrinsic limitation of approaches training a static LM for all queries. Moreover, unlike humans who engage in System 2 thinking to deliberate before acting~\cite{kahneman2011thinking}, traditional alignment techniques treat all outputs as end-to-end optimization targets rather than exploring the internal states during the response process, making pre-generation introspection and nuanced model switching impossible. 
% \heng{cannot parse 'a whole through alignment techniques', not sure what you mean}

Motivated by these limitations, our work investigates the feasibility of leveraging LMs’ internal states to extract their intentions regarding unsafe responses. Preliminary experiments reveal that LMs inherently encode signals within their internal states that reflect their recognition of unsafe instructions and intent to respond. These findings suggest that it is possible to harness these “internal signals” to guide model behavior in a more adaptive and context-aware manner.

Building upon it, we introduce \textbf{SafeSwitch}, a novel framework designed to dynamically regulate unsafe LLM behavior by leveraging these model’s internal signals. As shown in \Cref{fig:pipeline}, SafeSwitch incorporates a safety prober that continuously monitors the model's internal states, enabling it to predict potentially unsafe outputs before generation. This proactive probing mechanism allows SafeSwitch to anticipate risks and respond appropriately by dynamically activating a specialized refusal head that can provide informative explanations, ensuring that the model’s responses remain helpful while prioritizing safety. Empirically, LLMs integrated with our method demonstrate approximately an 80\% reduction in harmful outputs on safety benchmarks including SORRY-Bench and TrustLLM, while maintaining comparable performance on various QA benchmarks, highlighting the effectiveness of our approach in enhancing safety without compromising utility.


Further studies show that SafeSwitch could promote more informative and context-aware refusals and is particularly resilient to malicious queries, including those unseen during training. Additionally, by utilizing only 6\% of the original parameters, our design enables seamless integration into a wide range of LLMs with minimal overhead, providing a scalable and efficient solution. These advantages position SafeSwitch as a promising step forward in achieving nuanced, dynamic, and effective safety controls for LLMs, paving the way for more ethical AI applications.


% With great power comes great responsibility. Large language models (LLMs) have demonstrated exceptional abilities in complex tasks such as reasoning and tool use~\cite{liu2024deepseek,jaech2024openai}. However, these advancements have also sparked increasing concern about their risk of generating unsafe content and their potential detriment to society. While existing safety measures, exemplified by reinforcement learning from human feedback, have shown notable improvements in safety, they often result in a significant trade-off: enhanced safety comes at the cost of reduced model capabilities and increased evasiveness. This intrinsic tension between safety and utility has emerged as a practical challenge in deploying LLMs for real-world applications. 

% The safety-utility trade-off arises from the paradigm of training a static LLM to address all queries. Traditional alignment methods uniformly bias the LLM toward refusal, which, while enhancing safety, inevitably leads the LLM to refuse some benign queries, thus diminishing its helpfulness. To this end, our work explores a promising but largely unexplored direction for safety enhancement: switching between a helpful version and a safety-prioritized version of an LLM depending on the nature of different queries. Preliminary experiments reveal that LLMs inherently encode their recognition of unsafe instructions and their intent to respond within their internal states, enabling automatic switching between model variants based on query characteristics.

% Advancements in language models (LMs) have sparked increasing concern about their risk of generating unsafe content and potentially harming society. To gain insights on how to enhance LM safety, we reflect on human solutions to ensure spoken words are harmless. Cognitive science research suggests that humans form thoughts and intentions in specific brain regions, a process that precedes and guides the formulation of language~\cite{friederici2011brain,fedorenko2016language}. As part of our system 2 thinking~\cite{kahneman2011thinking}, such a mechanism makes our brains alarmed when facing sensitive queries and prevents us from expressing inappropriate or even harmful content.

% Building on this phenomenon, we raise a natural question: can LMs exhibit similar abilities to regulate their behaviors? Current paradigms often overlook the importance of pre-generation thoughts in LMs, as LMs are typically finetuned with reinforcement learning as a whole for better safety. While such alignment methods have shown notable safety improvements, they often come with a significant trade-off: a reduction in model capabilities and increased evasiveness alongside improved safety. This tension arises from the uniform biasing of LLMs toward refusal, which inevitably causes the model to reject some benign queries, thus diminishing its helpfulness. To this end, our work explores a promising direction for safety enhancement: extracting LMs’ intentions regarding unsafe responses and switch between a helpful version and a safety-prioritized version accordingly. Preliminary experiments indicate that LLMs inherently encode their recognition of unsafe instructions and their intent to respond within their internal states, supporting the feasibility of using internal thoughts to guide model behavior dynamically.

% In response to the challenge of balancing safety and utility, we introduce \textbf{SafeSwitch}, an effective and efficient framework that leverages internal model activations to guide and regulate unsafe LLM behavior. As illustrated in \Cref{fig:pipeline}, SafeSwitch implements a safety prober that monitors internal states and predicts potentially unsafe outputs before they are generated. Based on the probing mechanism, SafeSwitch dynamically activates a specialized refusal head that provides informative, context-aware explanations, effectively steering model responses to be safer.

% Our approach demonstrates that internal states can serve as reliable indicators for predicting and preventing harmful outputs while preserving model helpfulness. Through extensive evaluation of multiple benchmarks, we show that SafeSwitch significantly improves safety across different model architectures with minimal impact on performance in benign scenarios. Additional experiments show SafeSwitch promotes more informative refusals and is resilient to malicious queries unseen in training data. As a robust and efficient method, SafeSwitch can be seamlessly integrated into a wide range of LLMs with little cost, establishing a promising direction for achieving more nuanced and effective safety controls in language models.


