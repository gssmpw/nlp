% \section{Utilizing Internal State Probers to Steer Model's Safety Behavior}
% This section introduces our behavior switch based on the safety probers. Extensive experiments demonstrate our method can effectively enhance model safety with little sacrifice of helpfulness.

% \subsection{Problem Setup}
% \textbf{Models.} Same language models in Sec.\ref{prober_setup} are used. We also include experiments with different model sizes in Appendix~\ref{app:model_size}.

% \textbf {Datasets.} We use five benchmarks to comprehensively evaluate the model's safety and helpfulness. The benchmarks include:
% \begin{itemize}
% \item{SoRRY-Bench~\cite{xie2024sorry} is a safety benchmark with 45 types of unsafe instructions. The compliance rate is the metric for both safety benchmarks, and a lower rate means better safety.
% instructions. }
% \item{TrustLLM~\cite{huang2024trustllm} is a benchmark evaluating model safety from many aspects. We use the "safety-misuse" and "safety-jailbreak" categories of this benchmark.}
% \item{Over Refusal~\cite{huang2024trustllm} contains safe instructions which the model may misinterpret as harmful. This benchmark is also evaluated with compliance rate, and a higher rate means better helpfulness (less over-refusal).}
% \item{Alpaca-eval~\cite{dubois2024length} evaluates the model's instruction following abilities. It consists of open questions and the model's answer is compared against a reference answer by GPT-4. Win rate against GPT-4 is used as the metric.}
% \item{TriviaQA~\cite{joshi2017triviaqa}} is a reading comprehension benchmark, but we remove the reference documents and use it to evaluate the model's commonsense knowledge. Answers are judged by exact match.
% \end{itemize}


% \begin{table*}[hbtp]
% \caption{Comparison of different safety enhancement methods. The ↑ or ↓ sign beside each benchmark indicates higher or lower score is better.}\label{table:result}
% \vspace{2mm}
% \centering
% \renewcommand{\arraystretch}{1.1}
% \begin{tabular}{lccccc}
% \hline
% \textbf{}             & \multicolumn{1}{l}{SORRY-Bench↓} & \multicolumn{1}{l}{TrustLLM↓} & \multicolumn{1}{l}{Over Refusal↑} & \multicolumn{1}{l}{Alpaca-eval↑} & \multicolumn{1}{l}{TriviaQA↑} \\\hline\hline
% \textbf{LLaMa-3.1-8B} &                                 &                              &                                  &                                 &                              \\
% Original Model        & 58.11                           & 19.19                        & 73.50                            & 32.58                           & 68.10                        \\
% Refusal Head        & 2.33                            & 4.48                         & 36.50                            & 17.17                           & 66.90                        \\
% Safety Prompt        & 49.44                           & 10.42                        & 63.5                             & 29.86                           & 67.65                        \\
% LM Switch            & 6.56                            & 7.57                         & 62.50                            & 30.60                           & 68.05                        \\\hline
% \textbf{Qwen2.5-7B}   &                                 &                              &                                  &                                 &                              \\
% Original Model        & 72.56                           & 28.12                        & 70.50                            & 37.88                           & 53.70                        \\
% Refusal Head        & 2.78                            & 2.71                         & 40.5                             & 20.09                           & 51.45                        \\
% Safety Prompt        & 52.67                           & 9.71                         & 58.5                             & 30.84                           & 51.25                        \\
% LM Switch            & 11.11                           & 8.98                         & 61.5                             & 34.88                           & 53.70                        \\\hline
% \textbf{Yi-1.5-9B}    &                                 &                              &                                  &                                 &                              \\
% Original Model        & 71.78                           & 36.80                        & 74.00                            & 28.60                           & 44.55                        \\
% Refusal Head        & 2.00                            & 0.98                         & 30.00                            & 16.11                           & 37.85                        \\
% Safety Prompt        & 40.44                           & 11.88                        & 35.00                            & 20.02                           & 44.10                        \\
% LM Switch            & 9.00                            & 9.53                         & 54.00                            & 26.98                           & 44.55                        \\\hline
% \textbf{Ministral-8B} &                                 &                              &                                  &                                 &                              \\
% Original Model        & 80.89                           & 37.12                        & 79.00                            & 37.69                           & 58.40                        \\
% Refusal Head        & 0.78                            & 2.34                         & 2.00                             & 3.55                            & 25.05                        \\
% Safety Prompt        & 12.44                           & 10.74                        & 11.00                            & 13.02                           & 24.35                        \\
% LM Switch            & 7.56                            & 12.63                        & 55.50                            & 32.17                           & 58.40                       \\\hline
% \end{tabular}
% \end{table*}


\subsection{Method}
% We propose \textbf{LM switch}, an approach that combines preemptive safety detection and behavioral adjustment, aiming to make the model resilient to harmful instructions and keep original abilities at the same time.
% % TODO: find a better name for the method

% Firstly, we train a refusal head that can be seamlessly integrated into the language model. This module consists of a set of parameter weights that can be applied to the language model head, the component that maps the last internal state to a probability distribution over the vocabulary. To train the refusal head, we use the training division from SORRY-Bench and TrustLLM which contain a variety of unsafe instructions. GPT-4 is prompted to generate rejective answers as well as explanations for these unsafe instructions, and the annotated responses are then used to fine-tune the LM's head. During inference, the modified LM head interprets the last internal state in a way that favors refusal-related tokens, giving them a higher probability. Crucially, this modification is implemented only at the LM head, which ensures that the core transformer architecture, including the key and value vectors, remains unchanged. This allows us to reuse most of the model's computation after prefilling, making the method computationally efficient.

% Since unsafe instruction only accounts for a part of all instructions and the original LM can already identify and refuse some of them, we choose to only apply the refusal head when necessary to minimize its impact on model capacity. Specifically, we use the prober described in Sec.~\ref{prober} which preemptively identifies unsafe generations before they are produced. This prober monitors the internal states of the LM and the refusal head is applied to the language model when potentially unsafe content is flagged, effectively altering its behavior by biasing the output toward refusal.

% We include three baselines in the experiments. \textbf{Original Model} uses the original language model without extra safety methods. \textbf{Refusal Head} always apply the refusal head regardless of the input. \textbf{Safety Prompt} uses prompt reminding the model to emphasize safety, which has been proven to be an effective safety enhancement method~\cite{zheng2024prompt}.

\subsection{Results}
\label{result}
% Table~\ref{table:result} shows the experimental results with the following key findings.

% \textbf{Original Model and refusal head both exhibit a bias towards one end in the harmlessness-helpfulness balance.} Although the original LMs are already safety-aligned, they fail to refuse many unsafe instructions, indicating there is further improvement space for LM safety. In contrast, the refusal head, trained on rejective data, prioritizes safety at the expense of helpfulness. The refusal head would reject even harmless instructions, leading to a sharp decline in performance on the last three benchmarks.

% \textbf{LM Switch strikes a good balance between safety and helpfulness.} LM Switch shows a significant safety improvement over the original model, with only a marginal decline on Alpaca-eval and TriviaQA. This suggests that LM Switch is effective at predicting unsafe generations and intervening model behavior only when necessary. However, it still lags behind the original model in Over Refusal, since its instructions are highly deceptive and their safety is challenging for the prober to judge.

% % which we will discuss in Sec.~\ref{steer_error}.

% Notably, LM Switch outperforms Safety Prompt in both safety and helpfulness benchmarks, highlighting its strength in balancing harmlessness and helpfulness. The most significant performance gains are observed in Alpaca-eval and Over Refusal, indicating that LM Switch excels in following instructions and assessing safety in nuanced cases compared to Safety Prompt.

% \textbf{The benefits of LM Switch are consistent across models.} We observe that LMs exhibit varying sensitivity to prompts. For example, Ministral-8B is highly sensitive to the safety prompt, resulting in a much larger decline in helpfulness compared to other models. In contrast, LM Switch consistently enhances safety across different models while preserving most of their original capabilities. This highlights LM Switch's value as a universally applicable method.

% \subsection{LM Switch Generalizes to Out-of-domain Tasks}

% \begin{table}[]
% \caption{Comparison of LM switch using refusal heads trained with difference data. The LM used here is LLaMa-3.1-8B.~\ref{app:result}.}\label{table:ood}
% \vspace{2mm}
% \centering
% \renewcommand{\arraystretch}{1.1}
% \begin{tabular}{lcc}
% \hline
% Data for refusal head & SORRY-Bench & TrustLLM \\\hline
% N/A (Original Model)   & 58.11           & 19.19        \\
% SORRY-Bench            & 13.33           &  8.93     \\
% TrustLLM               & 29.56           & 9.88       \\
% Both   & 6.56           & 7.57      \\\hline
% \end{tabular}
% \end{table}

% In the experiment in Sec.~\ref{result}, the refusal head is trained with rejective responses for \textbf{both} SORRY-Bench and TrustLLM instructions, which means both benchmarks are in-distribution for the refusal head during inference. However, given the large number of unsafe instructions and jailbreak methods available to attackers, it is impractical to collect enough relevant data to train the model on all of them. Therefore, this section explores the LM switch's performance in out-of-domain scenarios, where the refusal head encounters data it has not been explicitly trained on. Specifically, we train refusal heads in a data-restricted manner by providing the same amount of data from only \textbf{one} of the two benchmarks during training. This setup allows us to assess the refusal head's ability to handle instructions that were not part of its training data. The results of this experiment are shown in Table~\ref{table:ood}.

% We observe that all variants of the LM switch significantly outperform the original model in terms of safety, demonstrating that the refusal head can effectively regulate LM behavior even with restricted training data. This suggests our method is robust even when faced with unseen instructions. Furthermore, the refusal head trained with both benchmarks exhibits the best safety, highlighting the benefit of incorporating diverse training data.

% Intuitively, in-domain tasks yield better performance than out-of-domain tasks for refusal heads trained with only one benchmark. However, we can see that the module trained with SORRY-bench generalizes much better than the one trained with TrustLLM. This is likely because SORRY-Bench contains a broader range of harmful requests and utilizes diverse persuasion techniques and writing styles. On the other hand, most instructions in TrustLLM are generally more straightforward, which means the refusal head trained on TrustLLM may not recognize the more nuanced harmful instructions from SORRY-Bench, leading to a lower refusal rate. \textbf{This underscores the importance of collecting high-quality and challenging data for safety alignment, especially when the amount of data is limited.}


% \subsection{Performance Analysis}
% \label{steer_error}
% hahahahaaa


\subsection{LM Switch Improves Refusing Strategies}

\begin{table}[]
\caption{Refusal strategies of original models and models with LM switch. "\#Hard" and "\#Soft" refer to the numbers of hard and soft refusals. Soft rate stands for the proportion of soft refusals in all refusals.}\label{table:refusal}
\vspace{2mm}
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccc}
\hline
                      & \#Hard & \#Soft & Soft Rate(\%) \\\hline\hline
\textbf{LLaMa-3.1-8B} &              &              &           \\
Original Model        & 251            & 126            &   33.42        \\
LM Switch            & 252            & 589            &   70.03        \\\hline
\textbf{Qwen2.5-7B}   &              &              &           \\
Original       & 13            & 234            &    94.74       \\
LM Switch            & 8           & 790            & 99.00 \\\hline
\textbf{Yi-1.5-9B}    &              &              &           \\
Original        & 170            & 84            &   33.07        \\
LM Switch            & 164            & 654            &   79.95        \\\hline
\textbf{Ministral-8B} &              &              &           \\
Original       & 7            & 163           &    95.88       \\
LM Switch            & 1            & 831            &    99.88     \\\hline 
\end{tabular}
\end{table}

Readers might wonder why we predict the model's refusal behavior with internal states and use a refusal head to steer model behavior, instead of simply judging instruction safety and denying harmful requests. The rationale is that different refusal strategies can lead to varied consequences. Hard refusals, which only refuse to answer the request without providing a reason, may confuse users as they leave the user uncertain about the reason for the denial. Hard refusal is commonly used to address highly sensitive topics during post-training of LMs. On the other hand, refusals with a detailed explanation would make the response more interpretable and easier to accept for users, fostering greater transparency and trust.

In Sec.~\ref{result}, we treat the refusal action binarily --- the model either complies with the request or refuses to answer it. In this subsection, however, we utilize GPT-4 to label refusals in a more fine-grained manner and compare the refusal strategies of the original LMs and models with safety prompts.

As shown in Table~\ref{table:refusal}, we find that different models exhibit significantly different refusal strategies, which might arise from the alignment phase of model training. For instance, LLaMa-3.1-8B and Yi-1.5-9B might be instruction-finetuned with responses that contain only a refusal, while Qwen2.5-7B and Ministral-8B might be instructed always to explain its decisions, resulting in a higher soft rate.

It's clear that using LM switch increases the soft rate for all models, though the extent of growth is relevant to the original model's refusal strategy. Recall that when training the refusal head, we prompt GPT-4 to generate an explanation along with the refusal. As a result, the fine-tuned LM head learns to include explanations rather than stopping the generation immediately after stating the refusal. By increasing the soft refusal rate, the LM switch contributes to the development of a more interpretable and explainable language model.