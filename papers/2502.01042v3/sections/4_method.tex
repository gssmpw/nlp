\section{Methodology}
To effectively harness internal signals for dynamic safety control, we introduce \textbf{SafeSwitch}, a method designed to predict when a model may generate unsafe responses and to steer its behavior toward safer outputs. Specifically, we train probers to leverage internal model activations for extracting contextual information and predicting the model's intent (\Cref{sec:build_probers}). Building on this, we guide the model towards safer responses through a refusal head (\Cref{sec:steering}).


\subsection{Safety Prober: Predicting Harmful Generations with Internal States}
\label{sec:build_probers}
% While existing research has examined internal states for tasks such as hallucination prediction~\cite{wang2024hidden}, debiasing~\cite{debiasinggradient2023} and safeguarding~\cite{zou2023representation}, their potential for revealing the model’s predisposition toward harmful responses remains underexplored. 
 
To explore internal states' potential in safety control, we design the \textbf{safety prober}, a lightweight neural module that utilizes internal states to predict the likelihood of unsafe model generations. This approach provides a novel and efficient method for understanding and further mitigating harmful behaviors in LMs.

Formally, given a language model $\mathbf{M}$ with $L$ layers and an input sequence $x$, the internal state at layer $l \in [1, L]$ is denoted as $\mathbf{H}_{l} \in \mathbb{R}^{d_{\text{model}}}$. A layer-specific prober $f_{l}$ maps this internal state to a scalar value that quantifies the probability of generating a harmful response:
\begin{equation}
\label{eq:prober}
p_{\text{unsafe}}(x) = f_{l}(\mathbf{H}_{l}) = f_{l}(\mathbf{M}_{\leq l}(x)),
\end{equation}
where $\mathbf{M}_{\leq l}(x)$ denotes the forward pass of the first $l$ layers of the model $\mathbf{M}$ given $x$ as input.

Taking insight from the orthogonality of two borders in \Cref{fig:visual_states}, we decompose the prediction task into two complementary objectives to enhance the safety prober's effectiveness. Specifically, a harmful response only emerges when two conditions are met: i) the instruction itself conveys malicious intent, and ii) the model fails to recognize this intent and complies with it. Therefore, we introduce a two-stage prober corresponding to these two conditions: one is dedicated to evaluating the safety of the instruction and the other focuses on assessing the model's tendency to comply with unsafe instructions.

Formally, the instruction safety prober estimates the probability \( p_{\text{unsafe\_instr}}(x) \) that an input \( x \) contains a harmful instruction, while the compliance prober estimates the probability \( p_{\text{compliance}}(x) \) that the model will follow the instruction. These probabilities are integrated to compute the overall likelihood of a harmful response:
\begin{equation}
p_{\text{unsafe}}(x) = p_{\text{unsafe\_instr}}(x) \times p_{\text{compliance}}(x),
\end{equation}
where the computations of \( p_{\text{unsafe\_instr}}(x) \) and \( p_{\text{compliance}}(x) \) follow \Cref{eq:prober}. This disentangled approach not only clarifies the distinct sources of harmful behavior but also enables more precise and targeted interventions. By leveraging the modularity of two-stage probers, we achieve fine-grained extraction of safety-related features from internal states, ultimately improving both the accuracy and interpretability of harmful behavior prediction.

% To refine the safety prober’s effectiveness, we further decompose the prediction task into two complementary objectives, inspired by the compositional nature of harmful responses. A harmful response arises under two conditions: (1) the instruction itself must convey malicious intent, and (2) the model must fail to recognize this intent and comply with it. Accordingly, we propose \textbf{two-stage probers}, with one prober focused on assessing instruction safety and the other on evaluating the model’s compliance with unsafe instructions. % Formally, the instruction safety prober predicts the probability $p_{\text{unsafe\_instr}}(x)$, while the compliance prober predicts the probability $p_{\text{compliance}}(x)$. 
% Probabilities estimated by the two probers are combined to compute the final likelihood of a harmful response:
% \begin{equation}
% p_{\text{unsafe}}(x) = p_{\text{unsafe\_instr}}(x) \cdot p_{\text{compliance}}(x),
% \end{equation}
% where the calculation of $p_{\text{unsafe\_instr}}$ and $p_{\text{compliance}}$ is similar to that in \Cref{eq:prober}.
% This disentangled approach not only clarifies the sources of harmful behavior but also allows for targeted interventions, enhancing both prediction accuracy and interpretability. By leveraging the modularity of two-stage probers, we achieve a fine-grained extraction of safety-related features from internal states.


% \subsection{Inference Time Computation for Safety Probers}
% \label{sec:scaling_law}
% Both token number $i$ and layer number $l$ impacts the performance of safety probers. In order to analyze how probers' performances evolve with token number and layer number, we aim to find an indicator that takes into account changes in both. We notice that the change of layers and decoded tokens both involve going through different numbers of transformer blocks, which is    a manifestation of inference time computation (ITC). Therefore, by using ITC as a unified indicator for all probers, we can investigate the \textbf{scaling law of safety probers} with respect to ITC. The scaling law may enable us to have a scientific estimation of the probers' performances and allocate ITC more efficiently in real-world applications.
% Specifically, we define a ``unit'' of ITC as the action of passing through all layers of the language model once and the zero point as the first layer of the last input token. Under this coordinate system, we use $\text{ITC}_i_{l}=i+\frac{l}{L}$ to quantify the inference time computation before the internal state is calculated and fed into the prober.


\subsection{SafeSwitch: Prober-Guided Model Behavior Steering}
\label{sec:steering}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.7\columnwidth]{images/hard_soft.png}
%     \caption{An example of hard and soft refusals. Hard refusals simply deny the request, while soft refusals provide reasons and constructive information.}
%     \label{fig:hard_soft}
% \end{figure}

A well-aligned LM should effectively provide informative responses to benign queries while appropriately refusing harmful ones. However, traditional safety alignment methods often introduce an unconditional bias toward refusal, leading models to reject even harmless prompts. Moreover, these refusals are frequently brief, generic, and template-based, offering little to no explanation and leaving users confused about the reason for the denial.

To address the issues of over-refusal and uninformative denial, we introduce \textbf{SafeSwitch}—a flexible and efficient safety enhancement framework that dynamically balances helpfulness and safety. By leveraging the safety prober signals identified in \Cref{sec:build_probers}, SafeSwitch not only reduces unnecessary refusals but also provides users with clear, informative explanations, fostering a more transparent and engaging interaction experience.

% A desirable LM should helpfully answer benign queries as well as deny harmful ones, but traditional safety alignment unconditionally biases the model's output towards refusal, causing it to refuse even harmless prompts. Besides, these refusals often appear detached， brief and template-based, leaving users confused of the reason of denial, as exemplified in \Cref{fig:hard_soft}. To mitigate over-refusal and promote constructive refusal explanations over templated responses, we propose \textbf{SafeSwitch}, a flexible and efficient safety enhancement method that leverages safety prober signals we detect in \Cref{sec:build_probers}.

We first introduce \textbf{refusal head}, a lightweight and seamlessly integrable module designed to influence token generation without altering the core model architecture. The refusal head $T_R\in \mathbb{R}^{|V|\times d_{\text{model}}}$ consists of a set of parameter weights added to the original LM head $T$, the component responsible for mapping the model's final internal state to a probability distribution over the vocabulary. The refusal head is designed to promote the probabilities of generating refusal-related tokens, steering the model's outputs to be more cautious and pushing the model to generate context-aware explanations after stating the refusal. By isolating the intervention to the LM head, we preserve the underlying model’s prefilled activations, thereby enabling computational reuse during inference. This approach not only maintains the integrity of the original model but also enhances model safety efficiently, as only less than 6\% of the original model's parameters are updated in finetuning.

Since unconditionally activating the refusal head will result in over-refusal, \textbf{SafeSwitch} addresses the problem by switching only on the refusal head when necessary under the guidance of the safety probers. Specifically, the safety prober $f$ monitors the internal states in the decoding process and predicts unsafe generations before they are produced by calculating the $p_{\text{unsafe}}$ scores. When the prober predicts the output to be safe, the original LM continues the generation. As soon as a potentially unsafe generation is flagged, manifested as $p_{\text{unsafe}}$ exceeding a threshold\footnote{We use a threshold of 0.5 in our experiments, since $p_{\text{unsafe}}>0.5$ indicates an unsafe response is more likely to occur than a safe one, and thus should be regulated by the refusal head. In real-world applications, however, users can adjust the threshold as a hyper-parameter to make the model generate more unrestricted or more conservative responses.}, the refusal head will be applied to the original LM head, effectively altering its behavior toward refusal. 

Formally, given the original head $T$, refusal head $T_R$, the last hidden state $\mathbf{H}_L$ and the unsafe score $p_{\text{unsafe}}$, SafeSwitch generates the distribution of the next token $\mathbf{P}(y|x)\in \mathbb{R}^{|V|}$ at the LM head using the following formula:

\begin{equation}
\small \mathbf{P}(y|x) = 
\begin{cases} 
\text{softmax}((T+T_R)\mathbf{H}_L) & \text{if } p_{\text{unsafe}}(x)>0.5,\\
\text{softmax}(T\mathbf{H}_L) & \text{otherwise}.
\end{cases}
\end{equation}

SafeSwitch leverages information encoded in internal states and activates the refusal head only when the model would otherwise generate harmful content. When the refusal head is activated, it steers model generation to refusals with detailed explanations, ensuring safety while enhancing interpretability and user-friendliness. The flexibility and foreseeing mechanism enables SafeSwitch to enhance model safety with minimal impact on other abilities, reaching a balance between harmlessness and helpfulness.



\subsection{Implementation}
\textbf{Safety Prober Training.} Given a set of instructions $\mathbf{X}$, we build a dataset $D_l$ to train safety probers that extract information from the layer $l$'s internal states.

\[
D_l = \{(\mathbf{H}_{l}(\mathbf{X}_i), y_i)\}_{i=1}^{|\mathbf{X}|} = \{(\mathbf{M}_{\leq l}(\mathbf{X}_i), y_i)\}_{i=1}^{|\mathbf{X}|}.
\]

$D_l$ contains internal states from layer $l$ ($\mathbf{H}_{l}$) and their corresponding ground-truth labels $y_i \in \{0,1\}$ indicating the harmfulness of the model response.


The safety prober is implemented as a multi-layer perceptron (MLP) network with an output dimension of $2$ to perform binary classification and is optimized using a cross-entropy loss. As described in \Cref{sec:build_probers}, we train two probers to predict instruction safety and model compliance respectively, and then combine the two probers together to predict response safety, enabling them to learn safety-related features effectively with minimal overhead. We include more implementation details in \Cref{app:training} and data construction methods in \Cref{app:data}.

\textbf{Refusal Head Training.} To train the refusal head, we prompt GPT-4 to generate rejective answers for harmful instructions (refer to \Cref{app:data} for details). We ensure the refusals are informative with context-aware explanations and use these refusals to finetune an update $T_R$ of the LM's head $T$, steering it to promote refusal-related tokens and refuse unsafe queries in a more informative manner. During the inference phase of SafeSwitch, the safety prober monitors the internal states and activates the refusal head upon unsafe generations are predicted, enhancing model safety and maintaining helpfulness at the same time.

% in the training division from SORRY-Bench and TrustLLM~\cite{huang2024trustllm} and provide explanations for the refusals. 

% \textbf{Base Models.} We use four base models in our experiments: LLaMa-3.1-8B~\cite{dubey2024llama}, QWen-2.5-7B~\cite{yang2024qwen2}, Yi-1.5-9B~\cite{young2024yi} and Ministral-8B~\cite{jiang2023mistral}. These models, which are post-trained to follow instructions and align with human values, represent the most prevalent and capable open-source model families. We also include experiments with different model sizes in \Cref{app:model_size}.

% \textbf{Benchmarks for Evaluation.}
% We use five benchmarks to comprehensively evaluate the model's safety and helpfulness. \textbf{SORRY-Bench} and \textbf{TrustLLM} aims to test models' ability to refuse harmful requests. \textbf{Over Refusal}~\cite{huang2024trustllm} contains safe instructions that the model may misinterpret as harmful. \textbf{Alpaca-eval}~\cite{dubois2024length} evaluates the model's instruction following abilities. It consists of open questions and the model's answer is compared against a reference answer by GPT-4. \textbf{TriviaQA}~\cite{joshi2017triviaqa} is a reading comprehension benchmark to evaluate commonsense knowledge.

