\section*{Impact Statement}
This work introduces a novel approach to enhancing the safety of language models while preserving their helpfulness. It explores the ability of models to reject potentially harmful or toxic content, with the generated responses serving only to assess model behavior and not reflecting the authors' views.

As language models grow more powerful and become integrated into various aspects of daily life, ensuring their safety will become increasingly critical. We hope this work provides valuable insights for researchers, contributing to the development of more reliable, responsible, and safe language models for real-world applications.