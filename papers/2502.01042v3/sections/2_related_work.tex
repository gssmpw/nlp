\section{Related Work}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{images/visualize_states_Llama-3.1-8B-Instruct.pdf}
    \caption{Visualization of the hidden states of LLaMa-3.1-8B using 2-dimensional PCA. Results reveal that the model’s recognition of unsafe inputs and its decision to refuse them are represented as increasingly distinct and near-orthogonal directions in deeper layers, suggesting that LMs encode safety-related information in internal states and that identifying unsafe queries and determining whether to refuse them are largely independent processes.}
    \label{fig:visual_states}
\end{figure*}

\textbf{Language Model Trustworthiness.} Trustworthiness of language models (LMs) \cite{huang2024trustllm} is a multifaceted standard, with safety and factuality being two key values. Safety requires LMs to avoid generating content that could be harmful to individuals or society. To evaluate LM safety, diverse benchmarks with various attack methods have been proposed~\cite{wang2023not,xie2024sorry,shen2024anything,kumar2023certifying}. Researchers typically enhance LM safety through prompting techniques~\cite{chen2024iteralign,zheng2024prompt} or reinforcement learning~\cite{ouyang2022training,bai2022training,mu2024rule,gibbs2024,rlhf2024b,selfimprovement2024a}. However, improvements in model safety often come at the cost of overall model utility~\cite{lin2024mitigating,arditi2024refusal}, and vice versa~\cite{qi2023fine}. This trade-off remains one of the challenges for LMs~\cite{wolf2024tradeoffs,arditi2024refusal}.
% , a phenomenon explained by the mediation of refusal behavior along a singular direction in the LM~\cite{arditi2024refusal, zheng2024prompt}.

Factuality requires LMs to generate truthful outputs and avoid hallucinations, which is a complex phenomenon that researchers have extensively studied~\cite{zhang2023siren}. Hallucination may arise from various causes, including intrinsic bias in pretraining data~\cite{dziri2022origin}, the dominance of popular statements~\cite{zhang2024knowledge}, and the time-sensitive nature of knowledge~\cite{luu2021time}. To mitigate it, several methods have been proposed, such as self-consistency~\cite{wang2022self}, retrieval-augmented generation~\cite{gao2023retrieval,han2023distributionally}, R-Tuning~\cite{zhang2023r}, knowledge grounding~\cite{smartbook2024}, and inference-time intervention~\cite{li2024inference}. Building on these insights, our method strives to enhance both safety and factuality by leveraging the model’s internal states to proactively regulate harmful outputs, offering a nuanced approach to improving trustworthiness.





\textbf{Model Interpretation with Internal States.} Internal states within LMs are widely utilized to identify~\cite{azaria2023internal,chen2024inside,zablocki2024assessing}, predict~\cite{wang2024hidden,ji2024llm}, mitigate~\cite{alnuhait2024factcheckmate,belrose2023eliciting,Overshadowing2024} hallucinations, and debiasing~\cite{debiasinggradient2023}. Related works have also employed sparse autoencoders to extract explainable features from these internal states~\cite{cunningham2023sparse,geva2022transformer}. However, far fewer research efforts have focused on how internal states reveal safety risks~\cite{zou2023representation,li2024rethinking,choi2024safety}. In contrast, our work investigates how internal states can be leveraged to predict and regulate unsafe outputs, providing a dynamic safety mechanism.


\textbf{Behavior Steering.} Behavior steering, which involves modifying LM behaviors with minimal cost, has gained significant attention due to the massive scale of state-of-the-art LMs. Researchers have proposed prompt-based methods~\cite{sahoo2024systematic,wei2022chain}, as well as computation-efficient model editing~\cite{yao2023editing,wang2024knowledge} and knowledge updating~\cite{ripple2024,eventedit2024} techniques. These include methods like Prefix Tuning~\cite{li2021prefix} and Suffix Tuning~\cite{zou2023universal}, which optimize continuous prompts, LM-Steer~\cite{han2024word}, which steers output embeddings, and ROME~\cite{meng2022locating}, which edits knowledge using rank-one updates. Following this line of research, we explore an internal-state-based approach for steering model behavior, focusing on using the model's latent signals to guide when should we regulate the model.

% The steering method in this paper also falls into this category, as we only alter the LM head to bias the model towards safer outputs.

% \heng{There is a lot of related work about hallucination mitigation, debiasing and honesty, the following are from a survey paper I'm involved, please integrate:}
% \subsection{Truthfulness}
% \label{sec:llm_Truthfulness}
% \textbf{\textit{Overview.}} Large language models have demonstrated significant effectiveness in various generative natural language processing tasks, such as question answering, summarization, and dialogue \cite{touvron2023llama2, dubey2024llama, achiam2023gpt, team2023gemini}. However, as these powerful models are increasingly deployed in high-stakes scenarios, there is a growing focus on ensuring the truthfulness of their output. Broadly, truthfulness can be defined as the ability of LLMs to accurately represent information, facts, and results \cite{huang2024position}. For instance, LLMs tend to produce plausible but incorrect answers, a phenomenon known as \textbf{hallucination (\S \ref{sec:hallucination})} \cite{ji2023survey, huang2023survey, zhang2023siren}. Additionally, they are prone to generating responses that align with user beliefs rather than presenting truthful information, a behavior referred to as \textbf{sycophancy (\S \ref{sec:sycophancy})} \cite{sharma2023understanding, perez2022discovering, wei2023simple}. Finally, they may produce responses that extend beyond their knowledge base, are deceptive, or appear inconsistent due to irrelevant conditions—a set of issues collectively described as challenges to \textbf{honesty (\S \ref{sec:honesty})} \cite{gao2024best, evans2021truthful, chern2024behonest}.

% \subsubsection{Hallucination}
% \label{sec:hallucination}
% Researchers in natural language generation (NLG) use the term ``hallucination'' to describe undesirable or incorrect outputs generated by models \cite{ji2023survey}. For example, in neural machine translation, ``hallucination'' refers to the system generating highly pathological translations that are entirely untethered from the source material \cite{lee2018hallucinations}. In the context of abstractive summarization, hallucinations can be intrinsic, extrinsic, or factual, resulting in outputs that are inconsistent with the input document or contain information that is not factually accurate \cite{maynez2020faithfulness}. \textit{In LLMs, hallucination often refers to a broader phenomenon focused on the factual accuracy of the generated content, rather than being tied to specific tasks.} As hallucination has become a common issue across various LLMs, increasing research efforts \cite{huang2023survey, zhang2023siren, li2023halueval, manakul2023selfcheckgpt, chen2023hallucination, xu2024hallucination, dhuliawala2023chain, mishra2024fine, li2024inference, li2024dawn,Overshadowing2024} have focused on understanding and mitigating this unwanted trait. Building on the previous work, we define hallucination as follows:

% \begin{tcolorbox}[definition]
% % \textit{Hallucination in LLMs refers to the generation of content that is factually incorrect but appears plausible, similar to human falsehoods.}
% \textit{Hallucination in LLMs refers to the generation of content that appears to be plausible but is inconsistent with facts or user requirements.}
% \end{tcolorbox}

% \textbf{\textit{(a) Hallucination Detection.}}
% While previous research has primarily focused on evaluating hallucinations in specific NLG tasks, detecting hallucinations in the context of LLMs presents unique challenges due to the free-form and often long-form nature of their outputs. Existing work on detecting and evaluating LLM hallucinations mainly focuses on two aspects: factuality and faithfulness. To effectively identify \textit{factual errors} in LLM responses, several studies \cite{min2023factscore, chern2023factool, gou2023critic, wang2023explainable, chen2023complex, huo2023retrieving} have proposed comparing model-generated content against reliable knowledge sources, similar to the approach used in automated fact-checking tasks. Additionally, to avoid relying on external knowledge retrieval, other research efforts \cite{varshney2023stitch, yao2023llm, luo2023zero, xiong2023can, kadavath2022language, manakul2023selfcheckgpt, agrawal2023language, cohen2023lm} address this issue in a zero-source setting, focusing on estimating the uncertainty of the factual content generated by the model to detect hallucinations. To detect \textit{unfaithful generation} that is inconsistent with the given context, various approaches have been proposed. According to \cite{huang2023survey}, these methods can be broadly categorized into the following categories: fact-based metrics \cite{lin2004rouge, nan2021entity, maynez2020faithfulness, wang2020towards, goodrich2019assessing, shuster2021retrieval}, classifier-based metrics \cite{mishra2021looking, barrantes2020adversarial, goyal2020evaluating, laban2022summac, kryscinski2019evaluating, zhou2020detecting,dziri2022evaluating}, QA-based metrics \cite{durmus2020feqa, wang2020asking, scialom2021questeval, fabbri2021qafacteval}, uncertainty estimation \cite{xiao2021hallucination, guerreiro2022looking, xiong2023can, xu2020understanding}, and prompting-based metrics \cite{wang2023chatgpt, luo2023chatgpt, laban2023llms, adlakha2023evaluating, gao2023human}.

% To provide a unified framework for detecting and evaluating hallucination, various benchmarks have been proposed. For example, HaluEval \cite{li2023halueval} offers a comprehensive collection of generated and human-annotated hallucinated samples specifically designed for hallucination evaluation; TruthfulQA \cite{lin2021truthfulqa} consists of adversarially curated questions that mimic human falsehoods and includes an automatic metric to assess the truthfulness and informativeness of generated responses. FACTOR \cite{muhlgay2023generating} introduces a method for automatically creating benchmarks by perturbing factual statements from a specific corpus. REALTIMEQA \cite{kasai2024realtime}, FreshQA \cite{vu2023freshllms}, and EvolvingQA \cite{kim2024carpe} offer questions specifically crafted to evaluate the factual accuracy of LLMs in relation to ever-evolving real-world knowledge. HalluQA \cite{cheng2023evaluating} and ChineseFactEval \cite{liang2023uhgeval} are benchmarks specifically designed to measure hallucination in Chinese large language models. SelfCheckGPT-Wikibio \cite{xiong2023can} provides a dataset for detecting sentence-level hallucinations by generating synthetic Wikipedia articles with GPT-3. FELM \cite{zhao2024felm} assesses factual accuracy across a variety of domains, including world knowledge, science and technology, and reasoning. PHD \cite{yang2023new} provides a passage-level hallucination detection benchmark, created using ChatGPT and annotated by human evaluators. 

% \textbf{\textit{(b) Hallucination Mitigation.}}
% To mitigate hallucinations in language models, one approach is to enhance the factual accuracy of the pre-training corpus, which improves the model’s parametric knowledge during the pre-training phase. Similarly, refining the quality of the training data used in the supervised fine-tuning stage can further mitigate this issue. Additionally, alignment processes can help language models recognize their knowledge boundaries, enabling them to decline answering questions outside their capabilities rather than producing inaccurate responses. \cite{zhang2023siren} However, these mitigation strategies during the training phase are often costly, potentially limiting their practicality in real-world applications. As a result, recent research focus has shifted towards developing inference-time interventions to elicit language models to produce truthful responses and reduce hallucinations.

% \textit{Prompting} \cite{liu2023pre} plays a crucial role in providing context and setting expectations for language models, thereby effectively controlling the generation of their outputs. Techniques such as chain-of-thought prompting \cite{wei2022chain} and least-to-most prompting \cite{zhou2022least}, where the model explains its reasoning step-by-step before arriving at a final answer, can reveal faulty logic or assumptions. Additionally, methods like self-consistency \cite{wang2022self}, SCOTT\cite{wang2023scott}, and self-ask \cite{press2022measuring}, which involve prompting the model multiple times and analyzing the responses for discrepancies, can help identify potential hallucinations. However, since these methods still depend on the parametric knowledge stored within LLMs, they can still suffer from hallucinations due to potential factual inaccuracies in their internal knowledge \cite{xu2024hallucination}. To address this issue, various \textit{retrieval-augmented generation (RAG)} \cite{lewis2020retrieval, gao2023retrieval} methods have been introduced. These methods retrieve information from reliable knowledge sources to enhance the LLMs' knowledge capability, thereby helping to reduce hallucinations and improve response accuracy. For instance, \cite{yao2022react, liu2021token, li2023chain, peng2023check, gao2022rarr, agrawal2023can,liu2023tcra, wang2023explainable, yoran2023answering, dhuliawala2023chain, mialon2023augmented, chern2023factool, gou2023critic} retrieves information from external knowledge bases, structured databases, specific websites like Wikipedia, search engine APIs to search the entire internet, or various external tools.

% \textit{Model editing} \cite{yao2023editing, wang2023knowledge, de2021editing, mitchell2022memory, sinitsin2020editable, huang2023transformer, meng2022mass, ripple2024, eventedit2024} allows for the modification of LLM behavior in a data- and computation-efficient manner. These methods often involve incorporating an auxiliary sub-network or directly modifying the original model parameters. For example, Meng et al. \cite{meng2022locating} propose a method called ROME, which modifies feedforward weights to update specific factual associations in GPT. Additionally, Li et al. \cite{li2023inference} introduce inference-time intervention (ITI), %\heng{spell it out?} 
% a technique that first identifies a set of attention heads highly associated with truthfulness. It then shifts activations along these truth-correlated directions to elicit truthful answers from Llama.
% Liu et al. \cite{liu2024evedit} propose event-based knowledge editing with deductive editing boundaries to address the problem of improper anchors.
