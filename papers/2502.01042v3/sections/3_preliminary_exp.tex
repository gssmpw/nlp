\section{Preliminary Experiment on Internal State}
\label{sec:prelim_study}

% The hidden states are presented across different layers for three types of queries: safe queries, unsafe queries rejected by the model, and unsafe queries accepted by the model. Two boundary lines, calculated using the SVM algorithm, are also included to differentiate between these query types.

Transformer-based language models (LMs) process input sequences through a layered structure, producing dense vector representations at each transformer block. These representations, known as \textbf{internal states}, encapsulate the model’s
contextual knowledge accumulated layer by layer. To gain a deeper understanding of how LMs process safety-related information, we conduct a preliminary experiment by \textbf{visualizing LMs' internal states} for different types of queries.

% In this subsection, we conduct a preliminary experiment to demonstrate that LMs encode safety-related information in their internal states by visualizing the internal states for different queries in a 2-dimensional space. 

We utilize the LLaMa-3.1-8B model to generate responses for a set of both safe and unsafe queries (Other base models show a similar trend, refer to \Cref{app:result} for more details). Each input-output pair is categorized into one of three types: i) the input is safe (Safe Input); ii) the input is unsafe, and the model appropriately refuses to respond (Refused Harmful Input); or iii) the input is unsafe, and the model complies with the request (Complied Harmful Input). To analyze how the model processes different types of queries, we extract its internal states across various layers for each input query. These internal representations are then projected into a 2-dimensional space using principal component analysis (PCA) to visualize how safety-related information is encoded. To quantify the distinction between categories, we train support vector machines (SVM) to draw two boundaries: one separating safe and unsafe queries, and another separating complied and refused queries.

As shown in \Cref{fig:visual_states}, the internal states of queries across the three categories are initially intermixed and scattered in the 2-dimensional space during the early layers. However, in the later layers, these representations begin to diverge, becoming more distinct across categories. The two boundaries are also more effective at discerning and clearly separating most of the points in the later layers compared to the earlier ones. These boundaries serve as a preliminary indicator of the model's understanding and intent to generate unsafe outputs, demonstrating that the model’s internal states can distinguish between safe and unsafe behaviors.

% We observe in \Cref{fig:visual_states} that the internal states for queries in three categories are inseparable and all scattered in the 2-dimensional space in the early layers. In the latter layers, however, internal states for queries in different categories start to shift away toward different directions and gradually become more distinguishable from each other. We use support vector machines (SVM) to draw a boundary separating the first two types, whose outputs are safe, and the third type whose outputs are dangerous. As a preliminary method to predict unsafe behaviors, the boundary [can/cannot do it]

From these results, we gain the following insights:\\
\textbullet \hspace{3pt} \textbf{LMs inherently encode their recognition of unsafe instructions and their intent to respond within their internal states}. Leveraging these rich internal signals offers a feasible and efficient approach for predicting and mitigating unsafe outputs. Meanwhile, the limitations of the simple SVM method highlight the need for more specialized techniques to effectively extract and utilize this information.\\
\textbullet \hspace{3pt} \textbf{The processes of identifying unsafe prompts and deciding to refuse them are handled independently}, as evidenced by the two boundaries becoming more distinct and near-orthogonal in deeper layers. Based on this observation, we designed two-stage probers to decompose the process of predicting unsafe responses in \Cref{sec:build_probers}.\\
\textbullet \hspace{3pt} \textbf{Safety-related features represent high-level semantic information that requires more complex processing}. This sensitivity to layer depth motivates a deeper exploration of how the choice of internal state layers impacts the accuracy of predicting unsafe behaviors in \Cref{sec:ITC_results}.

% Consequently, such features become distinguishable only in the model's later layers.
% The above results reveals two insights. Firstly, LMs do encode their consciousness of unsafe instructions and their intention to answer them in the internal states. This indicates that predicting and mitigating unsafe generations with rich signals from internal states is feasible and efficient. Meanwhile, the imperfection of the simple SVM method underscores the need for a more tailored method to extract information from internal states (\Cref{sec:build_probers}). Secondly, safety-related features are high-level semantic information that needs relatively complex processing, so only in deep layers in the model can we extract such information effectively. The sensitivity of layer choice motivates us to further investigate how the choice of internal state positions influence the accuracy of predicting unsafe behaviors (\Cref{sec:ITC_results}).
