\section{Safety Prober Experiments}
In this section, e present experimental results demonstrating that safety probers can efficiently predict unsafe behavior, reaching an F1 score of over 90\% (\Cref{sec:prober_results}). We then analyze the scaling law of their inference time computational cost and performance (\Cref{sec:ITC_results}).

\subsection{Settings}
We experiment with our proposed two-stage prober and also display results for its two components: i) the first-stage unsafe input prober and ii) the second-stage compliance prober. Additionally, we include a direct prober as a baseline, which predicts response safety in a single step without decomposing the process into two stages.
Our analysis primarily focuses on Llama-3.1-8B, as other models exhibit similar trends. For more details, please refer to \Cref{app:result}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{images/prober_2stage_Llama-3.1-8B-Instruct.pdf}
    \caption{Performances of the direct prober, two-stage prober and its two components probing LLaMa-3.1-8B's internal states. \textbf{Left}: probers from different layers in the language model at the last input token in the prefilling phase. \textbf{Right}: probers from the last layer after decoding several tokens.}
    \label{fig:prober}
\end{figure}

\subsection{Safety Probers Predict Model Behavior Effectively}
\label{sec:prober_results}

The results presented in \Cref{fig:prober} highlight the following key findings about the safety probers:

\textbf{Probers can extract sufficient information before decoding to make accurate predictions.} Since deeper layers capture more contextual information and complex semantic relationships, all types of probers benefit from probing into these later layers. This demonstrates that utilizing deeper layers is the optimal choice for probers before decoding. Notably, probers from the last few layers achieve F1 scores above 85\%, indicating that the model encodes safety-related information even before generation and that probers can extract this information with high accuracy.

From the left subgraph of \Cref{fig:prober}, we also observe that the direct prober underperforms two-stage probers by at least 4\%, further highlighting the superiority of our approach. By decomposing harmful response detection into two compositional stages, our design enables more precise and fine-grained extraction of implicit information, thereby improving overall prediction accuracy.

\textbf{Decoding tokens help probers to predict model behavior better.} Although prefill-phase probers  already demonstrate strong performance, we seek to further improve by decoding a small number of tokens as ``pilots'' before extracting the internal state. In the right subgraph of \Cref{fig:prober}, we observe that while the compliance probers benefit from this decoding process, the unsafe input probers show a sharp decline in performance. We attribute this difference to the distinct nature of the tasks: instruction harmfulness is determined solely by the input, so introducing decoded tokens, which are not part of the input, will add noise. In contrast, model compliance requires anticipating the model's output, meaning decoded tokens can serve as useful ``pilots'' to predict behavior. By leveraging our two-stage design, we can combine pre-decoding predictions from the unsafe input prober with decoding predictions from the compliance prober, thus achieving better performance gains as more tokens are decoded. Specifically, LLaMa-3.1-8B's two-stage prober achieves an F1 score of over 90\% when decoding 3 tokens, and probers for other base models all achieve over 87\%, as shown in \Cref{app:result}.
% \heng{not sure what is phase probers}\cheng{it's actually prefilling-phase, sorry for the confusion}

% We also conducted a detailed analysis on different types of instructions in \Cref{app:analysis} to investigate what kind of data our prober performs well on and when it does not.


\subsection{Inference Time Scaling Law for Safety Probers}
\label{sec:ITC_results}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/prober_scaling_wide_annotated.png}
    \caption{\textbf{Inference time scaling law} for safety probers: The x-axis represents inference time, measured by the number of transformer layers computed before extracting the internal state. The ``Prefill'' section uses the internal state from different layers during prefilling, while the ``Decoding'' section uses the internal state after several tokens have been decoded. The ``Max'' point represents the internal state after all tokens have been decoded. Dots indicate empirical results, and the curve is fitted using an exponential decay function. Note that the x-axis is not proportional.}
    \label{fig:inf_scaling}
\end{figure}

% \begin{figure}
%     \centering
%     \subfigure[Inference Time Scaling law]{\label{fig:inf_scaling}\includegraphics[width=0.48\linewidth]{images/prober_scaling_annotated.png}}
%     \subfigure[Pareto Improvement]{\label{fig:tradeoff}\includegraphics[width=0.5\linewidth]{images/trade-off_annotated.png}}
%     \caption{\textbf{(a)} Inference time scaling law for safety probers: The x-axis represents inference time, measured by the number of transformer layers computed before extracting the internal state. The ``Prefill'' section captures internal states from different layers during prefilling, while the ``Decoding'' section does so after decoding several tokens. The ``Max'' point represents the internal state after decoding all tokens. The F1 score improves with more ITC but finally plateau, showing diminishing returns. \textbf{(b)} Comparison of different safety enhance method's helpfulness and safety, where each shape represents a method. The original model, safety prompt, and refusal head exhibit a trade-off between helpfulness and safety. SafeSwitch falls in the upper right of the figure, demonstrating a better safety-utility balance.}
%     \label{fig:inf_scal_tradeoff}
% \end{figure}

% This subsection defines inference time computation (ITC) in safety probers and discusses how ITC impacts safety probers' performances.

We observed that two key factors, the number of layers and decoded tokens, significantly influence the performance of safety probers. Both factors involve processing different numbers of transformer blocks, which corresponds to inference time computation. Building on this insight, we introduce inference time computation (ITC) as a unified metric for safety probers, accounting for both the number of tokens and layers. This allows us to investigate the \textbf{scaling law of safety probers} with respect to ITC, providing a scientific way to estimate prober performance and allocate resources more efficiently.



Formally, we define a ``unit'' of inference time computation as the process of completing a full forward pass through the entire LM. For an LM with $L$ layers and an internal state at token $i$ and layer $l$, the inference time computation required to obtain the internal state is quantified as:
\begin{equation}
\mathcal{T}_{\text{infer}}=i+\frac{l}{L}
\end{equation}
Since the size of the prober is negligible compared to the transformer blocks (refer to Appendix~\ref{app:training} for a comparison), we do not account for it in the ITC calculation.

As shown in \Cref{fig:inf_scaling}, probers for different base models exhibit similar trends with respect to ITC, from which we can draw two key conclusions:\\
\textbullet \hspace{3pt} As ITC increases, the probers' F1 scores improve, indicating that internal states capture more information with increased inference time computation.\\
\textbullet \hspace{3pt} The rate of improvement gradually slows, and eventually, the F1 score converges to an upper limit, where all tokens are decoded and all computations are utilized. This reveals a diminishing marginal return, suggesting that excessive computation beyond a certain point becomes inefficient.

To quantify these scaling trends, we propose an empirical formula in the form of an exponential decay function:
\begin{equation}\label{eq:fit_curve}
\mathcal{F}(\mathcal{T}_{\text{infer}})=-\frac{A}{2^{\mathcal{T}_{\text{infer}}/B}}+U
\end{equation}
We fit this formula to empirical data and find that the coefficients of determination ($R^2$) for all models exceed 0.95, indicating a strong fit (refer to \Cref{app:scaling} for details). This formula provides an effective way to estimate prober performance and guide the efficient allocation of computation. In the experiments that follow, we opt to decode 3 tokens before making predictions, as this yields near-optimal performance at an acceptable cost across all models.


% We observed that the change of layers and decoded tokens are two major factors influencing the performance of safety probers. Building on the insight that both factors involve going through different numbers of transformer blocks, which is a manifestation of inference time computation, we propose inference time computation (ITC) in safety probers as a unified indicator that takes both token number and layer number into account. We can then investigate the \textbf{scaling law of safety probers} with respect to ITC, which enables us to have a scientific estimation of the probers' performances and allocate resource more efficiently.

% Formally, we define a ``unit'' of inference time computation as the process of performing a full forward pass through the entire language model. For an LM with $L$ layers and an internal state at token $i$ and layer $l$, $\mathcal{T}_{\text{infer}}=i+\frac{l}{L}$ quantifies the inference time computation needed to obtain the internal state. Since the size of the prober is insignificant compared with transformer blocks (see Appendix~\ref{app:training}), we don't account for it when calculating ITC.

% As shown in Fig.~\ref{fig:prober_scaling}, the probers for different base models exhibit similar trends with respect to ITC and we can reach two conclusions from the results. First, as ITC increases, the probers exhibit higher F1 scores, indicating that internal states capture richer information with more inference time computation. Second, the rising speed of prober performance gradually decreases. In the end, the F1 score converges to the upper bound where all tokens are decoded and all possible computations are utilized. This demonstrates a significant diminishing marginal return, which means extensive computing beyond a certain point is inefficient.

% To quantify these scaling trends, we propose an empirical formula in the form of an exponential decay function:
% \begin{equation}\label{eq:fit_curve}
% \mathcal{F}(\mathcal{T}_{\text{infer}})=-\frac{A}{2^{\mathcal{T}_{\text{infer}}/B}}+U
% \end{equation}
% We fit the formula on empirical data and find that oefficients of determination ($R^2$) for all models are greater than $0.95$, which means our formula yields a close approximation (refer to \Cref{app:scaling} for details). \textbf{This formula can then be used to estimate the performance of safety probers and guide the efficient allocation of computational resources.} In the following experiments, we choose to decode 3 tokens before making predictions, as this provides near-optimal performance with an acceptable cost for all models.



\subsection{Performance Analysis for Safety Probers}
\label{app:analysis}

Results in \Cref{sec:prober_results} show that safety probers effectively extract safety-related information in internal states and predicts unsafe responses with an F1 score of over 90\%, but they still make mistakes occasionally. To gain a deeper understanding of the prober's abilities and limitations, we carefully analyze the safety probers' performance in this section.

\textbf{Performance Breakdown.} We analyze the probers' performances on different types of unsafe prompts, based on the taxonomy in SORRY-Bench with 45 categories of harmful instructions.


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/comply_vs_correct_annotated.pdf}
    \caption{The probers' performances and the original models' compliance rates for different categories in SORRY-Bench. Generally, categories with a higher comply rate, which indicates them being more deceptive, are harder to predict for probers. Uncommon topics like religion promotion and financial advice have the lowest prediction accuracies.}\label{fig:comply_correct}
\end{figure}

From \Cref{fig:comply_correct}, we observe that categories with higher compliance rates are generally harder to predict, which stems from the deceptive nature of certain types of unsafe requests. In these categories, some unsafe prompts are not recognized by the probers, leading to relatively low prediction accuracy. Categories with the lowest accuracies include environmental crimes, financial advice, and religion promotion, which could be underrepresented topics in LLM pretrain data.

\textbf{Error Cases Analysis.} Furthermore, we show some typical examples of failed cases in \Cref{app:analysis} to identify the safety probers' weaknesses. Then we analysis the reasons for making two different types of mistakes: judging a harmful query as safe, and vice versa.

We conclude that the prober often fail to identify a harmful query when: (1) the query is related to topics that the prober doesn't consider sensitive, like religion promoting; (2) the unsafe query is hidden among other safe queries, so the prober considers answering them acceptable. These examples show that safety probers still have limitations in terms of harmfulness perception.

When the prober mistakes a harmless response as dangerous, the problem could occur in either stage of the two-stage schema. For the first three queries, the prober fails in the first stage, misjudging safe requests as malicious. This includes objective inquiries of historical events, queries about battle or crime-related games or movies, and sensitive wording in normal scenarios. For the last query, the mistake arises in the second stage---the prober thought the model would complies with the apparently unsafe query, while the model didn't actually do so. The prober's judgment is likely to be affected by the new speaking style specified in the query.


% \begin{table}[]
% \caption{Error rates for probers across various benchmarks. A false positive indicates the prober predicts an unsafe response but the response is actually harmless, while a false negative occurs when the prober fails to identify an unsafe response. Results are averaged across models.}\label{table:error_rate}
% \vspace{2mm}
% \centering
% \footnotesize
% \renewcommand{\arraystretch}{1.1}
% \begin{tabular}{lcc}
% \hline
%              & False Positive & False Negative \\\hline
% SORRY-Bench  & 2.63           & 6.96           \\
% TrustLLM     & 12.23          & 7.82           \\
% Over Refusal & 21.38          & 0.0            \\
% Alpace-eval  & 9.66           & 0.0            \\
% TriviaQA     & 1.06           & 0.0             \\\hline  
% \end{tabular}
% \end{table}

% \textbf{Error Rate on Different Benchmarks. }Finally, we show the error rate on the 5 benchmarks used to evaluate our method in \Cref{table:error_rate}\footnote{False negative rates for the latter three benchmarks are zero because their queries are safe inherently and the models will not generate unsafe responses on these benchmarks.}. Error rates on most benchmarks are below 10\%, which shows the prober is robust across different types of queries and base models. The false positive rate of Over Refusal is the highest among all benchmarks, which explains why the performance of SafeSwitch on Over Refusal still has a gap compared to the original model in \Cref{table:result}.



\begin{table*}[hbtp]
\caption{Performance of different safety enhancement methods on safety and utility benchmarks. In SafeSwitch, probers utilize the internal state after decoding three tokens, and the refusal head is triggered when $p_{\text{unsafe}}>0.5$. The ↑ or ↓ symbols indicate whether a higher or lower score is preferable. Colored annotations beside the results represent performance differences compared to the original model: green indicates improvement, red signifies a decline, and yellow denotes no change.}
\label{table:result}
\small
\vspace{2mm}
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{llccccc}
\toprule
Base Model & Method & \multicolumn{1}{l}{SORRY-Bench↓} & \multicolumn{1}{l}{TrustLLM↓} & \multicolumn{1}{l}{Over Refusal↑} & \multicolumn{1}{l}{Alpaca-eval↑} & \multicolumn{1}{l}{TriviaQA↑} \\
\midrule
\multicolumn{1}{c}{\multirow{4}{*}{\textbf{LLaMa-3.1-8B}}} & Original Model & 58.11 & 19.19 & 73.50 & 32.58 & 68.10 \\
\multicolumn{1}{c}{} & Refusal Head & 2.33\tiny{\textcolor[HTML]{206546}{\ -55.78}} & 4.48\tiny{\textcolor[HTML]{206546}{\ -14.71}} & 36.50\tiny{\textcolor{red}{\ -37.00}} & 17.17\tiny{\textcolor{red}{\ -15.41}} & 66.90\tiny{\textcolor{red}{\ -1.20}} \\
\multicolumn{1}{c}{} & Safety Prompt & 49.44\tiny{\textcolor[HTML]{206546}{\ -8.67}} & 10.42\tiny{\textcolor[HTML]{206546}{\ -8.77}} & 63.50\tiny{\textcolor{red}{\ -10.00}} & 29.86\tiny{\textcolor{red}{\ -2.72}} & 67.65\tiny{\textcolor{red}{\ -0.45}} \\
\multicolumn{1}{c}{} & SafeSwitch & 6.56\tiny{\textcolor[HTML]{206546}{\ -51.55}} & 7.57\tiny{\textcolor[HTML]{206546}{\ -11.62}} & 62.50\tiny{\textcolor{red}{\ -11.00}} & 30.60\tiny{\textcolor{red}{\ -1.98}} & 68.05\tiny{\textcolor{red}{\ -0.05}} \\
\midrule
\multirow{4}{*}{\textbf{Qwen2.5-7B}} & Original Model & 72.56 & 28.12 & 70.50 & 37.88 & 53.70 \\
 & Refusal Head & 2.78\tiny{\textcolor[HTML]{206546}{\ -69.78}} & 2.71\tiny{\textcolor[HTML]{206546}{\ -25.41}} & 40.50\tiny{\textcolor{red}{\ -30.00}} & 20.09\tiny{\textcolor{red}{\ -17.79}} & 51.45\tiny{\textcolor{red}{\ -2.25}} \\
 & Safety Prompt & 52.67\tiny{\textcolor[HTML]{206546}{\ -19.89}} & 9.71\tiny{\textcolor[HTML]{206546}{\ -18.41}} & 58.50\tiny{\textcolor{red}{\ -12.00}} & 30.84\tiny{\textcolor{red}{\ -7.04}} & 51.25\tiny{\textcolor{red}{\ -2.45}} \\
 & SafeSwitch & 11.11\tiny{\textcolor[HTML]{206546}{\ -61.45}} & 8.98\tiny{\textcolor[HTML]{206546}{\ -19.14}} & 61.50\tiny{\textcolor{red}{\ -9.00}} & 34.88\tiny{\textcolor{red}{\ -3.00}} & 53.70\tiny{\textcolor[HTML]{FF9912}{\ 0.0}} \\
 \midrule
\multirow{4}{*}{\textbf{Yi-1.5-9B}} & Original Model & 71.78 & 36.80 & 74.00 & 28.60 & 44.55 \\
 & Refusal Head & 2.00\tiny{\textcolor[HTML]{206546}{\ -69.78}} & 0.98\tiny{\textcolor[HTML]{206546}{\ -35.82}} & 30.00\tiny{\textcolor{red}{\ -44.00}} & 16.11\tiny{\textcolor{red}{\ -12.49}} & 37.85\tiny{\textcolor{red}{\ -6.70}} \\
 & Safety Prompt & 40.44\tiny{\textcolor[HTML]{206546}{\ -31.34}} & 11.88\tiny{\textcolor[HTML]{206546}{\ -24.92}} & 35.00\tiny{\textcolor{red}{\ -39.00}} & 20.02\tiny{\textcolor{red}{\ -8.58}} & 44.10\tiny{\textcolor{red}{\ -2.45}} \\
 & SafeSwitch & 9.00\tiny{\textcolor[HTML]{206546}{\ -62.78}} & 9.53\tiny{\textcolor[HTML]{206546}{\ -27.27}} & 54.00\tiny{\textcolor{red}{\ -20.00}} & 26.98\tiny{\textcolor{red}{\ -1.62}} & 44.55\tiny{\textcolor[HTML]{FF9912}{\ 0.0}} \\
 \midrule
\multirow{4}{*}{\textbf{Ministral-8B}} & Original Model & 80.89 & 37.12 & 79.00 & 37.69 & 58.40 \\
 & Refusal Head & 0.78\tiny{\textcolor[HTML]{206546}{\ -80.11}} & 2.34\tiny{\textcolor[HTML]{206546}{\ -34.78}} & 2.00\tiny{\textcolor{red}{\ -77.00}} & 3.55\tiny{\textcolor{red}{\ -34.14}} & 25.05\tiny{\textcolor{red}{\ -33.35}} \\
 & Safety Prompt & 12.44\tiny{\textcolor[HTML]{206546}{\ -68.45}} & 10.74\tiny{\textcolor[HTML]{206546}{\ -26.38}} & 11.00\tiny{\textcolor{red}{\ -68.00}} & 13.02\tiny{\textcolor{red}{\ -24.67}} & 24.35\tiny{\textcolor{red}{\ -34.05}} \\
 & SafeSwitch & 7.56\tiny{\textcolor[HTML]{206546}{\ -73.33}} & 12.63\tiny{\textcolor[HTML]{206546}{\ -24.49}} & 55.50\tiny{\textcolor{red}{\ -23.50}} & 32.17\tiny{\textcolor{red}{\ -5.52}} & 58.40\tiny{\textcolor[HTML]{FF9912}{\ 0.0}} \\
\bottomrule
\end{tabular}
\end{table*}