\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Enzyme-Catalyzed Protein Hydrolysis Prediction}

\begin{document}

\twocolumn[
\icmltitle{Enzyme-Catalyzed Protein Hydrolysis Prediction}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Full Paper Submission Deadline: Jan 31 '25 11:59 AM UTC
\end{abstract}

\section{Objective and Motivation}
The primary goal of our model is to predict protein hydrolysis sites during enzymatic cleavage. This has broad applications in drug design, antigen research, and enzymatic pathway studies. Current tools typically focus on the Clan (family) level, where a group of enzymes with similar characteristics are modeled collectively. However, these models lack the specificity to predict hydrolysis sites for individual enzymes.


Key motivations for developing this model include:
\begin{itemize}
    \item Improved accuracy in predicting cleavage sites of enzyme-catalyzed protein hydrolysis.
    \item Utilization of both sequence and structural data to enhance predictions.
\end{itemize}

\section{Model Input, Structure and Output}

\subsection{Input}
\textbf{Protein Sequence and Structure:}
\begin{itemize}
    \item Local sequence obtained using sliding windows
    \item Local structure
    \item Global structure/sequence
\end{itemize}
Hydrolases typically recognize sequence motifs of substrate proteins, so sliding windows are used to break down proteins into smaller segments. Since the hydrolysis process may exhibit structural preferences, structural information is included.

\vspace{\baselineskip}

\textbf{Enzyme Data:}
\begin{itemize}
    \item Enzyme sequence
    \item Enzyme structure
    \item Annotation data such as EC number, Clan number, active sites, and functional regions
    \item Pocket structure based on enzyme active sites(TBD)
\end{itemize}
The enzyme's structural and sequence data provide a better basis for enzyme modeling. Additionally, EC numbers, Clan numbers, active sites, and functional regions can be utilized as prior knowledge to support model construction or serve as multitasks for model pre-training.


\subsection{Output(TBD)}
\begin{itemize}
    \item \textbf{Predicted Hydrolysis Sites:} Potential cleavage sites in substrate proteins linked to enzyme specificity.
    \item \textbf{Enzyme Activity Predictions:} Active sites participating in catalytic interaction with substrate proteins.
    \item \textbf{EC Number and Clan Classification:} Leveraging enzyme classification numbers for model pre-training.
    \item \textbf{Functional Region Prediction:} Continuous segments associated with enzyme functional activity.
\end{itemize}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{InputOutput.png}
    \caption{Input and Output}
    \label{fig:enter-label}
\end{figure}

\section{Model Structure (TBD)}
\subsection{Multitask Learning Architecture}
The approach is to divide the model into two parts: enzyme feature extraction and the integration of enzyme and substrate protein features. The model predicts multiple aspects such as enzyme-substrate catalytic sites and cleavage locations. A combination of Protein Language Models (PLM) and Graph Neural Networks (GNN) will be used for sequence and structural data processing.

\subsection{Pre-trained Models and Fine-tuning}
We may pre-train on large-scale enzyme data and fine-tune for specific protease families, such as serine proteases, using hydrolysis data.

\subsection{Prior modeling approach based on the active site pocket}
Based on the enzyme's active sites, we can extract and model the enzyme's pocket structure using methods such as MaskGAE. We can also perform pre-training on other types of enzyme data.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{Model Architecture(TBD).png}
    \caption{Initial version of the model}
    \label{fig:enter-label}
\end{figure}




\section{Data Sources and Collection}
Our data sources include MEROPS, UniProt, BRENDA, Expasy, PDB, and AlphaFoldDB.

We primarily collected two parts of data.

\subsection{Part One: Data from MEROPS}
The first part was gathered from the MEROPS database. After preprocessing, we collected a total of 10K substrate proteins and 904 enzymes, which were paired to construct 25K enzyme-substrate protein pairs, identifying 70K cleavage sites. Additionally, we obtained the corresponding sequences and structures for the enzymes and substrate proteins from the UniProt and AlphaFoldDB databases. For structures not available in these databases, we generated them using AlphaFold. Each enzyme was annotated with information on its active regions, active sites, EC number, and clan classification. For the substrate proteins, we generated approximately 600K sliding window sequences using a 15-unit sliding window approach.

\subsection{Part Two: Data from Expasy and BRENDA}
The second part of the data involves collecting all enzymes with EC numbers beginning with 3.4 from the Expasy database. We then retrieved their corresponding sequences from UniProt, resulting in 10,739 protease sequences with annotated active sites, of which 1150 include links to BRENDA database entries. These entries provide detailed enzyme information from various literature sources, including two types of substrates. In the BRENDA dataset, substrates and products associated with each EC-numbered enzyme category can be identified.

\paragraph{Substrate Categories}
Specifically, substrates are divided into two types:
\begin{enumerate}
    \item \textbf{Synthetic substrates}: These are designed to test enzyme activity and are usually composed of short peptides modified in various ways. For example, Ala-Pro-7-amido-4-trifluoromethylcoumarin consists of a dipeptide with a fluorescent group. However, these artificial substrates differ significantly from real proteins and human physiological conditions.
    \item \textbf{Full-length proteins}: Although full-length protein data is included, the quality is rough. While sources are cited, the substrate sequences and cleavage sites are often missing, requiring manual verification in the literature. Some references provide full substrate sequences, while others offer partial information.
\end{enumerate}



\section{Experiment}
We initially constructed a baseline model, focusing on a pre-training approach followed by fine-tuning on high-precision data. The preliminary experimental setup, as shown in the figure, includes both the GNN and ESM components.
In the pre-training phase, the model leverages a GNN for structural feature extraction and an ESM (evolutionary scale model) to encode sequence information. Specifically, we utilized the esm2-t12-35M-UR50D model, which outputs 480-dimensional embeddings. The GNN structure consists of an input layer with 480 units, a hidden layer with 128 units, and an output layer with 64 units. This configuration was aimed at capturing both local and global features from protein sequences with a sequence length of up to 1500 residues. Additionally, the model employs a distance threshold of 8.0 to define relationships within the graph, enhancing structural representation.
For batch training, we split the dataset into 8000 training samples and 2000 test samples based on EC numbers, ensuring a balanced representation across enzyme families.
We also conducted experiments with a model excluding the GNN component (as depicted in the second figure). The performance metrics (Top-K, Precision, Recall, F1 Score) indicate that the model with only ESM outperformed the GNN+ESM model, suggesting that GNN integration might not be as effective in this configuration. Consequently, further optimization efforts will focus on improving the GNN's contribution to overall performance.




\section{Challenges}
\textbf{Limited Data Volume:} Representing enzymes with limited data is challenging. Pre-training on a larger dataset of enzymes could help in learning more generalizable enzyme representations.

\textbf{Building Interpretable Models Using Biological Prior Knowledge:} Leveraging biological prior knowledge is crucial in constructing interpretable models. For instance, explicitly modeling enzyme active sites based on binding features could enhance understanding.

\section{Application}
\textbf{Peptide Drug Development and Metabolic Stability:} Improving enzyme characterization is crucial for prolonging peptide drug stability and enhancing therapeutic efficacy. Current tools may lack the precision needed to predict all involved peptidases, and better characterization could aid in optimizing peptide drugs stability.

\bibliography{ref}
\bibliographystyle{icml2024}

\newpage
\appendix
\onecolumn
\input{Appendix}

\end{document}