\section{Methodology}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\textwidth]{figure/ModelArchitecture2.png}
%     \caption{Model Architecture}
%     \label{fig:Model_architecture}
% \end{figure}


\begin{figure*}[t!]
    \small
    \centering
    \includegraphics[width=0.9 \linewidth]{figure/framework.pdf}
    \vskip -1.5em
    \caption{Architecture of biochemically-informed enzyme encoder and overall framework of {\method}.}
    \vskip -1.5 em
    \label{fig:Model_architecture}
\end{figure*}





% We propose a novel framework for protein cleavage site prediction, addressing two critical challenges: (1) accurate cleavage site prediction within substrates, particularly in enzyme-substrate interactions, and (2) overcoming data scarcity and imbalance. Our approach enhances enzyme representations by incorporating active-site-guided features and employs a progressive scanning method that simulates enzyme-substrate interaction dynamics. 

In this section, we give the details of the proposed {\method}. As the Fig.~\ref{fig:Model_architecture} shows,  apart from the substrate encoder, {\method} deploys an enzyme encoder to enable the generalization of cleavage site prediction across various enzymes. In addition, active site information in protein hydrolysis is incorporated into enzyme encoder training to enhance cleavage site prediction. Two main challenges remain to be addressed: (i) how to design the enzyme encoder to preserve critical information for cleavage site prediction? (ii) how to leverage the rich information of enzyme active sites to improve the cleavage site prediction? 

To tackle the above challenges, our {\method} deploys a biochemically-informed enzyme encoder which augments the graph transformer with enzyme energy frustration\cite{Dai_2024}. Furthermore, {\method} employs active site-aware pooling to preserve the enzyme’s information crucial for protein hydrolysis. 
To facilitate enzyme representation learning, {\method} first pretrains the enzyme encoder using active site prediction with the supplemented enzyme set. Then, a joint loss of active site prediction and cleavage site prediction is employed to optimize the {\method} for accurate cleavage site prediction. Next, we introduce each component in detail.


\subsection{Biochemically-Informed Enzyme Encoder}
Enzymes’ 3D structures, especially the local environments around their active sites, are crucial for catalyzing protein hydrolysis. Although direct active-site annotations are often unavailable for test data, recent studies indicate that local energetic frustration can identify functionally important regions~\cite{energyfrustration}. Building on these insights, we propose a biochemically-informed enzyme encoder that integrates both the spatial positions of residues and their energetic frustration scores\cite{dai2021selfexplainablegraphneuralnetwork}.


\textbf{Input Features and Backbone.} Recent success of AlphaFold and ESMFold demonstrates that protein language models can produce powerful representations from protein sequences for various protein tasks~\cite{lin2023evolutionary}. Therefore, we initialize the input features of enzymes denoted as $\mathbf{X}$ with their ESM-2 representations. Then, we deploy an extended version of graph transformer~\cite{ying2021transformers,jumper2021highly} to encode the input features with supplemented biochemical information for cleavage site prediction.  Next, we introduce how we extend graph transformer to integrate the energetic frustration and 3D position information.  


\textbf{Encoding Energetic Frustration.} Previous studies indicate that local energetic frustration, referring to regions in a protein not optimized for minimal energy, is commonly observed around enzyme active sites and can significantly influence catalysis~\cite{energyfrustration}. To quantify this phenomenon, a frustration score $\mathbf{F}(i,j)$ is computed for each residue pair $(i,j)$ within an enzyme $\mathcal{P}^e$ following~\cite{energyfrustration}:
\begin{equation}
    % \small
    \mathbf{F}(i,j) = \frac{ \mathbf{E}{(i,j)} - \mu_{\mathrm{rand}}(i,j) }{ \sigma_{\mathrm{rand}}(i,j) },
    \label{eq:energy cal}
\end{equation}
where $\mathbf{E}(i,j)$ is the actual interaction energy between residues \((i,j)\) in the enzyme $\mathcal{P}^e$. $\mu_\mathrm{rand}(i,j)$  and $\sigma_{\mathrm{rand}}(i,j)$ represent the mean and standard deviation of interaction energies that derived from randomized configurations (see Appendix~\ref{sec:app_imp} for details).
A higher $\mathbf{F}{(i,j)}$ implies stronger local energetic frustration, suggesting that the residue pair is more likely to belong to a functionally important region. Therefore, we incorporate this frustration score to provide useful biochemical information to the enzyme encoder. 


% \textbf{Unified Structure and Energy-Based Attention Bias} 
% Following prior work, we adopt strategies similar to Transformer-M\cite{TransformerM}, incorporating Gaussian-based transformations for structural distances \(D_{(i,j)}\) and energetic interactions \(E_{(i,j)}\),  and extend them by \emph{only applying energy-related features to enzymes} and \emph{treating distance and energy as separate channels}. Formally, we have \(
% D_{(i,j)} \,\mapsto\, \psi_{\text{dist}}(D_{(i,j)}),\ 
% E_{(i,j)} \,\mapsto\,\psi_{\text{energy}}(E_{(i,j)}),
% \) \(
% D_{(i,j)} \;=\; \frac{1}{\,1 + \|\,r_i \;-\; r_j\,\|},
% \) where \(r_i\) and \(r_j\) are the three-dimensional coordinates of the C\(\alpha\) atoms for residues \(i\) and \(j\),  \(\psi_{\text{dist}}(\cdot)\) and \(\psi_{\text{energy}}(\cdot)\) are Gaussian transforms. We then feed these into distinct MLPs (with no parameter sharing, given the differing magnitudes of distance vs.\ energy):
% \(
% \Phi_{\text{dist}}(i,j) \;=\; \text{MLP}_{\text{dist}}\bigl(\psi_{\text{dist}}(D_{(i,j)})\bigr),\quad
% \Phi_{\text{energy}}(i,j) \;=\; \text{MLP}_{\text{energy}}\bigl(\psi_{\text{energy}}(E_{(i,j)})\bigr),
% \)
% and add the resulting \(\Phi_{\text{dist}}\) and \(\Phi_{\text{energy}}\) as biases to the self-attention mechanism. Specifically, for \emph{enzymes} we compute\(\mathbf{A}^h_{\text{enzyme}}(\mathbf{X}) = \text{softmax}\Bigl(\text{SeqAtt}(\mathbf{X}) + \Phi_{\text{dist}} + \Phi_{\text{energy}}\Bigr),\) whereas for \emph{substrates} the energy term is omitted. 



\paragraph{Integrating Energy and 3D Position in Transformer.}
Following prior works~\cite{TransformerM}, we encode the 3D positions by computing pair-wise distance between residues:
\begin{equation}
    % \small
    \mathbf{D}(i,j) = \|\mathbf{r}_i-\mathbf{r}_j\|_2,
    \label{eq:distance calculation}
\end{equation}
where $\mathbf{r}_i \in \mathbb{R}^3$ is the C\(\alpha\)-atom coordinate of residue \(i\).
Both energetic frustration score and distance matrix capture pairwise relationships akin to the spatial encoding in graph transformers. Therefore, following Transformer-M~\cite{TransformerM}, we locate those pair-wise signals to provide complementary information for the self-attention score computation.
% To address the mismatch in scale between energy and distance measurements, and following prior work (e.g., Transformer-M~\cite{TransformerM}), we incorporate Gaussian-based transformations for both structural distances and energetic interactions. However, we \emph{only apply energy-related features to enzymes} and treat distance and energy as separate channels. 
Concretely, for a enzyme $\mathcal{P}^e = (\mathbf{X}, \mathbf{R})$, we process both  $\mathbf{F}(i,j)$ and distance matrix $\mathbf{D}(i,j)$ with a Gaussian Basis Kernel function followed by a MLP:
\begin{equation}
\begin{aligned}
        & \Phi_{i,j}^{\mathrm{energy}}  = \mathrm{MLP}(\phi_{\mathrm{energy}}(\mathbf{F}(i,j))) \\
        & \Phi_{i,j}^{\mathrm{dist}} = \mathrm{MLP}(\phi_{\mathrm{dist}}(\mathbf{D}(i,j))),
        \label{eq:Gauss}
\end{aligned}
\end{equation}
where $\phi_{\mathrm{energy}}$ and $ \phi_{\mathrm{dist}}$ denote the learnable Gaussian Basis Kernel function that can map energy frustration score and distance score to a $d$-dimensional vector (See Appendix~\ref{sec:app_imp} for more details). MLP is further deployed to transform these vectors to the space of attention scores. 

We then add the resulting \(\Phi^{\mathrm{dist}}_{(i,j)}\) and \(\Phi^{\mathrm{energy}}_{(i,j)}\) as bias terms to the self-attention mechanism.  Denote $\mathbf{A}_{i,j}^k$ as the $(i, j)$-element of the Query-Key product matrix in $k$-th attention layer, we have:
\begin{equation}
\begin{aligned}
    & \mathbf{A}_{i,j}^{k}  = \frac{(\mathbf{h}_i^{k-1} \mathbf{W}_Q)(\mathbf{h}^{k-1}_j \mathbf{W}_K)^T}{d} + \Phi_{i,j}^{\mathrm{energy}} + \Phi_{i,j}^{\mathrm{dist}}\\
    & \mathbf{H}^{k}  = \mathrm{softmax}(\mathbf{A}^{k})\mathbf{H}^{k-1}\mathbf{W}_V,
    \label{eq:attention+DE}
\end{aligned}
\end{equation}
where $\mathbf{H}^k \in \mathbb{R}^{N \times d}$ denotes the updated representation matrix. And $\mathbf{W}_Q$, $\mathbf{W}_K$, and $\mathbf{W}_V$  are projection matrices for the query, key, and value transformations. 
Following Transformer-M~\cite{TransformerM}, $\Phi_{i,j}^{\mathrm{energy}}$ and $\Phi_{i,j}^{\mathrm{dist}}$ are shared for all layers.


\subsection{Enhancing Enzyme Representation Learning with Active Site Knowledge}
To incorporate crucial active-site knowledge into enzyme representation learning, we use three strategies: (i) an auxiliary active-site prediction task to strengthen the enzyme encoder, (ii) large-scale pretraining for active-site prediction to capture general catalytic patterns, and (iii) an active-site-aware pooling mechanism that emphasizes catalysis-related residues. Next, we give more details.

%\enyan{Each strategy should be written with a complete sentence. @Shuo, please help revise this paragraph.}
%(i) Active-Site Multi-Task Enhancement; (ii) Broadened Enzyme Pretraining; (iii) Catalytic-Residue Amplification. We give the details of these strategies in the following. 


\textbf{Active Site Prediction.} 
Active sites play a key role in catalyzing the protein cleavage. Hence, active-site information can provide essential understandings of enzyme functions. As a result, we deploy the active site prediction as the auxiliary task to benefit the enzyme encoder training by:
\begin{equation}
    {\hat a}_i =  \mathrm{sigmoid}(\mathbf{h}_i \cdot \mathbf{w}_a ),
    \label{eq:actsite_prediction}
\end{equation}
where ${\hat a}_i \in [0,1]$ denotes the probability of the $i$-th residue being the active site, $\mathbf{h}_i \in \mathbb{R}^{d}$ is the representation  of $i$-th residues in the enzyme, and $\mathbf{w}_a \in \mathbb{R}^{d}$ denotes the learnable parameters for active site prediction. 
% For an enzyme of length $N$, the cross-entropy loss function for active site prediction can be formulated as:
% \begin{equation}
%      l_a(\mathbf{\hat a}, \mathbf{a}) = - \frac{1}{N} \sum_{i=1}^N [{a}_i\log{\hat a}_i + (1-{a}_i) \log(1-{\hat a}_i)],
% \end{equation}
% where $\mathbf{a} \in \{0,1\}^N$ denotes the ground-truth active sites on enzyme of length $N$ and $\mathbf{\hat a} = [\hat{a}_1,...\hat{a}_N]$ denotes the probability vector of active site on the enzyme.


%\enyan{
%1. The number of enzymes annotated in cleavage site database is limited, which challenge the enzyme encoder learning. 
%2. The enzymes annotated with active sites are a lot. We also slect the ones that most similar or the enzymes for protein hydrolysis? I am not sure for this. In addition, the active site information plays a key role in the enzymes' functions, which provide essential understandings of enzyme. Therefore, we leverage a large corpus of enzyme annotated with active sites to facilitate the pretraining.
%@Shuo, rephrase the above several sentences. }
\textbf{Pretraining with the Supplemented Enzyme Set $\mathcal{D}_a$}. The number of enzymes annotated in cleavage site database is limited, which poses a significant challenge for the effective training of enzyme encoders\cite{dai2023unifiedframeworkgraphinformation}. However, there exists abundant enzyme data annotated with active sites. Therefore, we select enzyme types highly homologous to the target proteolytic enzymes in biological function to expand the pretraining dataset. Formally, the objective function of pretraining the enzyme encoder on the supplemented enzyme set $\mathcal{D}_a$ can be written as:
\begin{equation}
    \min_{\theta_e, \mathbf{w}_a} \mathcal{L}_a(\mathcal{D}_a)=\frac{1}{|\mathcal{D}_a|} \sum_{(\mathcal{P}^e,\mathbf{a}) \in \mathcal{D}_a} l_{\mathrm{BCE}}(\hat{\mathbf a}, \mathbf{a}), 
    \label{eq:activate loss}
\end{equation}
where $\theta_e$ denotes the parameters of enzyme encoder. $\mathbf{\hat a} = [\hat{a}_1,...\hat{a}_N]$ denotes the probability vector of active site on the enzyme $\mathcal{P}^e$. $l_{\mathrm{BCE}}$ denotes the element-wise binary cross entropy loss.
By pretraining on a large corpus of enzyme sequences, we allow the model to capture broader structural and functional patterns common across enzymes.

% Many proteases harbor distinct active sites whose local environments play a pivotal role in cleavage, yet these sites may not be readily apparent from a global perspective alone. Current works on active sites only focus on multitasking, without effectively utilizing biological information. To address this issue, our aim is to \emph{highlight catalytic residues in enzyme embeddings} so that the cleavage prediction model can better capture enzyme-specific behavior.



% \paragraph{Active Site-Aware Pooling.} We seek a method that not only identifies potential active sites but also amplifies these features in the overall enzyme representation. We introduce a simple \(\text{MLP}_{\text{Acti}}\) predicting active-site probabilities \(\mathbf{A} \in \mathbb{R}^{l \times 1}\) for the \(l\)-residue enzyme sequence:\(
% \mathbf{A} 
% = \text{Sigmoid} \bigl(\text{MLP}_{\text{Acti}}(\mathbf{E})\bigr),\) where \(\mathbf{E} \in \mathbb{R}^{l \times d}\) represents the output features of the enzyme encoder.  Analogous to our distance and energy channels, we also perform a Gaussian-based transformation on the activation probabilities \(\mathbf{A}\). \(\widehat{A}_i = \psi_{A}\bigl(A_i\bigr),\) where \(\psi_{A}\) is Gaussian kernel, ensuring that \(\mathbf{A}\) is projected into a feature space aligned with our structural and energetic embeddings. We then apply a softmax to the transformed \(\widehat{A}_i\) to select the noteworthy sites. :
% \[
% \alpha_i 
% = \mathrm{softmax}\bigl(\widehat{A}_i\bigr), 
% \quad
% \mathbf{E}_{\text{pool}} 
% = \sum\nolimits_i \alpha_i\, \mathbf{E}_i.
% \]
% This \emph{site-aware pooling} mechanism ensures that residues deemed critical for enzymatic activity (i.e., those with high \(\widehat{A}_i\)) contribute more strongly to the aggregated enzyme representation \(\mathbf{E}_{\text{pool}}\). In practice, highlighting these high-probability regions encourages the model to focus on catalytically relevant segments of the enzyme.



\textbf{Active Site-Aware Pooling.} 
To obtain enzyme representation from a sequence of residue representations, a pooling operation such as mean pooling is required. However, residues that are active sites are more critical for enzymatic activity. Hence, intuitively, these active sites should contribute more in the aggregated enzyme representation\cite{dai2022prototypebasedselfexplainablegraphneural}. Therefore, we design an active site-aware pooling mechanism, whose pooling weights are based on the predicted active site probabilities.
Let $\hat{a}_i \in \mathbb{R}^{N}$ be the predicted probability that $i$-th residue is an active site in an \(N\)-residue enzyme. 
The active site-aware pooling can be written as:
\begin{equation}
    \mathbf{h}^e = \mathrm{softmax}([w_1,\dots,w_N]) \mathbf{H}, 
    ~~
    {w}_i = f(\hat{a}_i),
    \label{eq:sitepooling}
\end{equation}
where $\mathbf{H} \in \mathbb{R}^{N\times d}$ is representations of residues given by the enzyme encoder. $f(\cdot)$ is a learnable function that will map each $\hat{a}_i$ to the pooling weight ${w}_i$. With the active site-aware pooling, we would be able to encourages the model to focus on catalytically relevant segments of the enzyme.


% We first estimate active-site probabilities via an MLP:
% \[
% \mathbf{A} 
% \;=\; 
% \mathrm{Sigmoid}\!\Bigl(\mathrm{MLP}_{\mathrm{Acti}}\bigl(\mathbf{E}\bigr)\Bigr),
% \]
% where \(\mathbf{A} \in \mathbb{R}^{N \times 1}\). Analogous to the distance and energy channels, we apply a Gaussian-based transformation \(\psi_{A}\) to each scalar probability \(A_i\):
% \[
% \widehat{A}_i 
% \;=\; 
% \psi_{A}\!\bigl(A_i\bigr),
% \]
% projecting \(\mathbf{A}\) into a feature space aligned with the structural and energetic embeddings. We then apply a \(\mathrm{softmax}\) operation over \(\widehat{A}_i\) to obtain attention weights \(\alpha_i\):
% \[
% \alpha_i 
% \;=\; 
% \mathrm{softmax}\bigl(\widehat{A}_i\bigr),
% \quad
% \mathbf{E}_{\mathrm{pool}} 
% \;=\; 
% \sum_{i=1}^{N} \alpha_i \,\mathbf{E}_i.
% \]
% This \emph{site-aware pooling} mechanism ensures that residues with high \(\widehat{A}_i\)—presumably critical for enzymatic activity—contribute more strongly to the aggregated enzyme representation \(\mathbf{E}_{\mathrm{pool}}\). In practice, highlighting these high-probability regions encourages the model to focus on catalytically relevant segments of the enzyme.




\subsection{Cleavage Site Prediction}

\textbf{Substrate Protein Encoding.} Similar to the enzyme encoding, we use ESM-2 representations to initialize substrate features. Since there is no functional region in substrates, we omit the energetic frustration encoding for the substrate protein. Only input features $\mathbf{X}^s$ and distance matrix $\mathbf{D}^s$ of the substrate $\mathcal{P}^s$ are integrated in the transformer:
\begin{equation}
    \mathbf{H}^s = \mathrm{Transformer}(\mathbf{X}^s, \mathbf{D}^s),
    \label{eq:subencoder}
\end{equation}
where $\mathbf{H}^s \in \mathbb{R}^{|\mathcal{P}^s|\times d}$ is the substrate representation.
Further implementation details are provided in Appendix~\ref{sec:Substrate Encoding Details}. 

% \enyan{@Chenao, complete this in the appendix}
% We apply the same encoding approach as for Enzymes, using ESM-2 to encode the substrate protein. Similarly, we integrate structural biases based on the 3D position to enhance the representation of the substrate.
% \enyan{Here, we briefly introduce how we introduce the protein encoding. @Chenao, please complete this paragraph. }


% \textbf{Cleavage Site Prediction with Progressive Scanning.}  
% In this approach, we predict cleavage sites by scanning the entire substrate protein sequence with a fixed-size sliding window. To incorporate both enzyme and substrate information, we first concatenate the pooled enzyme features \(\mathbf{h}^e\) with the substrate features \(\mathbf{S}\). This concatenated feature set is then processed by the sliding window, allowing us to capture local sequence patterns and contextual relationships. A fixed-size window is required to effectively balance capturing sufficient contextual information with computational efficiency. After processing the concatenated features, the model predicts the cleavage sites across the entire substrate sequence.
% \enyan{@Chenao, this should be given be equations.}


\textbf{Cleavage Site Prediction.}  
 During protein hydrolysis, enzymes generally recognize local residue sequences about 15–30 residues in length. To reflect this biological behavior, we predict whether a subsequence of length $l$ in substrate $\mathcal{P}^e$ will be cleaved by enzyme $\mathcal{P}^e$. Formally, this process can be written by:
\begin{equation}
    \hat{c}_t^{e,s} = \mathrm{MLP}(\mathrm{CONCAT}(\mathbf{H}_{t:t+l}^s, \mathbf{h}^e))
    \label{eq:1DCNN}
\end{equation}
where $\mathbf{H}_{t:t+l}^e$ is a contiguous slice taken directly from the substrate representation matrix $\mathbf{H}^s$ and $\mathbf{h}^e$ is the enzyme representation obtained by Eq.(\ref{eq:sitepooling}). The length of the subsequence is set as 31 (15 residues on each side.). The optimization function of cleavage site prediction can be written as:
\begin{equation}
    \mathcal{L}_c(\mathcal{D}_c) = \frac{1}{|\mathcal{D}_c|} \sum_{(\mathcal{P}^e, \mathcal{P}^s, \mathbf{c}^{e,s}) \in \mathcal{D}_c } l_{\mathrm{BCE}}(\mathbf{c}^{e,s}, \mathbf{\hat c}^{e,s})
    \label{eq:cleavageloss}
\end{equation}
where $\mathbf{\hat c}^{e,s}=[\hat{c}_1^{e,s},\dots ,\hat{c}_{|\mathcal{P}^s|}^{e,s}]$ denotes the probability vector of cleavage site within the substrate $\mathcal{P}^s$ given the enzyme $\mathcal{P}^e$. $l_{\mathrm{BCE}}$ is the element-wise binary cross entropy loss.


% To incorporate both enzyme and substrate information, we first augment the pooled enzyme features \(\mathbf{h}^e \in \mathbb{R}^{d}\) to match the substrate sequence length \(l\) by replication, obtaining \(\mathbf{h}^e_{aug} \in \mathbb{R}^{l \times d}\). We then concatenate the augmented enzyme features \(\mathbf{h}^e_{aug}\) with the substrate features \(\mathbf{S} \in \mathbb{R}^{l \times d}\):
% \[
% \mathbf{F}_{combined} = \mathrm{cat} \left( \mathbf{S}, \mathbf{h}^e_{aug} \right),
% \]
% where \(\mathbf{F}_{combined} \in \mathbb{R}^{l \times ({2d})}\), and \(l\) is the substrate length, while \(d\) is the feature dimensions. This concatenated feature set is then processed by the sliding window, allowing us to capture local sequence patterns and contextual relationships. A fixed-size window is required to effectively balance capturing sufficient contextual information with computational efficiency. The convolution operation is applied as follows:
% \[
% \mathbf{C} = \text{Conv1D}(\mathbf{F}_{combined}),
% \]
% where \(\mathbf{C} \in \mathbb{R}^l\) represents the output, and it corresponds to the predicted cleavage site logits. After processing the concatenated features, the model predicts the cleavage sites across the entire substrate sequence.






% \enyan{Here, we briefly introduce how can combine them and give the prediction. Here, I feel we do not say it is 1-D convolutional. We just say we scan the whole substrates protein by sliding a fixed size window. Also we need to mention, why a fixed size window is required. @ Chenao, please rewrite this paragraph. 
% }

% \textbf{Biological Analogy and Complementarity} Proteases bind sequentially to key residues within a substrate, facilitating catalysis through localized interactions. Inspired by this, we propose a one-dimensional convolutional model that mimics the enzyme's scanning of substrate sequences. The convolution kernel size k represents the segment under examination, while the stride of 1 simulates the enzyme’s progressive interaction with adjacent residues. This framework captures the sequential, localized nature of enzyme-substrate binding. 



% \paragraph{Progressive Scanning.} For accurate bioprocess simulation, firstly, we concatenate the pooled enzyme features \(\mathbf{E}_{\mathrm{pool}} \in \mathbb{R}^{d}\) with the substrate features \(\mathbf{S} \in \mathbb{R}^{l \times d}\):
% \[
% \mathbf{F}_{\mathrm{combined}}
% \;=\;
% \mathrm{cat} \!\bigl(\mathbf{S},\,\mathbf{E}_{\mathrm{pool}}\bigr),
% \]
% where \(\mathbf{F}_{\mathrm{combined}} \in \mathbb{R}^{l \times 2d}\), and \(l\) is the substrate length while \(d\) is the feature dimension. We then apply a one-dimensional convolution:
% \[
% \mathbf{C}
% \;=\;
% \mathrm{Conv1D}\!\bigl(\mathbf{F}_{\mathrm{combined}}\bigr),
% \]
% yielding the cleavage logits \(\mathbf{C} \in \mathbb{R}^l\). After a \(\mathrm{sigmoid}\) activation, \(\mathbf{C}\) can be interpreted as the raw cleavage probability distribution over the substrate residues.




\subsection{Final Objective Function}
For each enzyme $\mathcal{P}^e \in \mathcal{D}_c$ in the cleavage site database, their active sites are also included in the $\mathcal{D}_a$. Consequently, we combine the cleavage site prediction loss and the active site prediction to jointly train the whole framework by:
\begin{equation}
    \min_{\theta} \mathcal{L}_c(\mathcal{D}_c) + \lambda \mathcal{L}_a(\mathcal{D}_a^c),
    \label{eq:lossall}
\end{equation}
where $\theta$ denotes all parameters in {\method} including the enzyme encoder, substrate encoder, active prediction module and cleavage site prediction module. $\mathcal{D}_a^c \subset \mathcal{D}_a$ provides the active-site annotations for the enzymes in $\mathcal{D}_c$.



% \subsection{Biochemically Inspired Cleavage Site Prediction Algorithm}

% \paragraph{Motivation.}
% Inspired by the way an enzyme may progressively interact with a substrate along its sequence, we employ a one-dimensional convolutional network to capture local features relevant to cleavage. This 1D convolution emulates the idea of a “sliding” process, scanning the protein residues in a manner analogous to how an enzyme might probe sequential segments of a substrate.

% \paragraph{1D Convolution Implementation.}
% Concretely, we apply a 1D convolutional layer with a kernel size of $k$ (e.g., 31) to the embedded protein sequence. Let $\mathbf{F} \in \mathbb{R}^{l \times d}$ be the fused representations (e.g., language-model embeddings plus any structural or energetic features). The layer outputs:
% \[
% \mathbf{C} = \text{Conv1D}_{\theta}(\mathbf{F}),
% \]
% where $\mathbf{C} \in \mathbb{R}^{l}$ denotes the raw cleavage logits (or probabilities after a sigmoid). By sliding over the sequence, this convolution captures local residue patterns critical for hydrolytic cleavage, aligning with the short-range interactions often seen in protease-substrate binding.

% \paragraph{Biochemical Analogy.}
% While this operation is a standard convolutional filter in deep learning, we draw inspiration from the biochemical perspective: proteases often recognize and bind specific sub-sequences (e.g., P1-P4 residues) of the substrate in a progressive manner. The 1D convolution similarly “steps through” the sequence, modeling local contexts that could signal potential cleavage sites.

% \paragraph{Advantages.}
% \begin{itemize}
%     \item \textit{Local Pattern Recognition:} The kernel effectively learns short-range dependencies, akin to how enzymes detect specific motifs on a substrate.
%     \item \textit{Efficiency:} A 1D convolution is computationally light compared to complex 2D/3D schemes, yet sufficient for sequence-based tasks.
%     \item \textit{Compatibility:} This module naturally integrates with other components (e.g., attention, multi-task loss) in the overall architecture, further refining cleavage predictions.
% \end{itemize}

% In practice, the convolution layer’s kernel size can be tuned to reflect the approximate length of the binding pocket or the region of interest for cleavage. The resulting representations from the 1D convolution layer are then passed on to subsequent modules for final cleavage probability estimation or integrated into a multi-task framework.






































 

% \subsection{Structure and Energy based Attention Bias}
% \textbf{Motivation.} Sequence embeddings alone may overlook spatial constraints and residue-level interaction energies. To model the 3D context, we integrate structural distances and energetic frustration metrics. While the above features capture crucial information, an attention mechanism can further highlight residue pairs critical to cleavage. We extend a multi-head attention layer to incorporate the structural/energy biases:

% \textbf{Gaussian Transformations (Reference).} We adopt standard Gaussian-based transformations for distance \(D_{(i,j)}\) and energy \(E_{(i,j)}\). Concretely, we transform:
% \[
% D_{(i,j)} \mapsto \psi_{\text{dist}}(D_{(i,j)}), 
% \quad
% E_{(i,j)} \mapsto \psi_{\text{energy}}(E_{(i,j)}),
% \]
% where \(\psi_{\text{dist}}\) and \(\psi_{\text{energy}}\) are Gaussian kernels. 


% \textbf{Unified Representation.} We concatenate or fuse these transformed structural-energetic features with the language-model embeddings to produce a refined representation \(\Phi\), thus capturing both high-level sequence context and low-level 3D or energetic cues.

% \[
% \mathbf{A}^h(\mathbf{X}) = \text{softmax}\Bigl(
% \dfrac{\mathbf{X}\mathbf{W}_Q(\mathbf{X}\mathbf{W}_K)^\top}{\sqrt{d}} 
% \;+\; \Phi_{\text{dist}} \;+\; \Phi_{\text{energy}}
% \Bigr),
% \]
% where \(\Phi_{\text{dist}}\) and \(\Phi_{\text{energy}}\) are derived from the transformations above, influencing the attention scores. This bias ensures that the attention layer focuses on residue pairs with structural or energetic significance.


% \subsection{Active-Site Prediction and Site-Aware Pooling}
% \textbf{Objective.} Many proteases have distinct active sites. Identifying these regions may help refine cleavage predictions or provide a secondary task to guide the model’s focus.

% \textbf{Approach.} We employ a simple  \(\text{MLP}_{\text{Activation}}\) on enzyme embeddings, producing \(\mathbf{A}\in\mathbb{R}^{l \times 1}\). To pool enzyme features by active-site importance, we define a site-aware pooling mechanism that weights each residue embedding by its predicted active-site probability.

% \(\displaystyle
% \mathbf{E}_{\text{pool}} = \sum\nolimits_i \alpha_i\, \mathbf{E}_i,\quad
% \alpha_i \equiv \mathrm{softmax}(\text{GELU}(\mathbf{A}_i\,\mathbf{W}^1_A)\,\mathbf{W}^2_A).
% \)

% Here, \(\mathbf{E}_i\) is the \(i\)-th residue embedding, and \(\alpha_i\) is its site-aware weight. This mechanism highlights key catalytic regions.

% \subsection{Cleavage Prediction and Multi-Task Loss}
% \textbf{Final Prediction.} After combining enzyme features (possibly pooled by active sites) with substrate embeddings, we pass the result through a convolution or MLP to produce cleavage probabilities \( C \in \mathbb{R}^{l}\). 

% \textbf{Multi-Task Loss.}
% We adopt a multi-task scheme:
% \[
% \mathcal{L} \;=\; 
% \lambda \cdot \mathrm{BCE}_{\mathrm{weighted}}(\mathbf{A}, \text{True}_{\text{active}}) 
% \;+\;
% \mathrm{BCE}_{\mathrm{weighted}}(\mathbf{C}, \text{True}_{\text{cleavage}}),
% \]
% where \(\lambda\) balances active-site prediction against cleavage prediction. Weighted binary cross-entropy emphasizes rare but biologically significant sites. 

% \textbf{Training Pipeline.} We sequentially update \(\Theta\) to minimize \(\mathcal{L}\). The first term encourages accurate active-site identification; the second focuses on cleavage site prediction. This arrangement ensures that modeling of enzyme catalytic regions complements substrate-level cleavage prediction.

% \subsection{Summary of Methodology}
% Overall, our proposed framework unifies:
% \begin{itemize}
%     \item \textit{Language-model embeddings} (from ESM-2) for enriched contextual features,
%     \item \textit{Structural and energetic transformations} to capture 3D constraints and residue-level frustrations,
%     \item \textit{Attention-based integration} to highlight critical residue pairs,
%     \item \textit{Active-site prediction and site-aware pooling} to incorporate enzyme-specific catalytic cues.
% \end{itemize}
% By synergizing these components in a multi-task setting, the model achieves more robust cleavage site prediction even under class imbalance and data scarcity. The next section details our experimental setup and comprehensive evaluations.












% \section{Methodology}

% In this section, we present a novel framework for protein cleavage site prediction, integrating enzyme active-site knowledge with structural and energetic cues. This framework addresses two key challenges: (i) the scarcity of annotated enzyme cleavage data, with only 700 cleavage points available and an extreme imbalance where activation and cleavage residues comprise less than 1\% of sequence lengths, and (ii) the need for bio-inspired algorithms that emulate enzyme-protein interaction mechanisms to enhance prediction accuracy. Figure~\ref{fig:framework} (omitted here) illustrates the overall architecture. The following subsections detail the framework's components and the design principles underlying each module.



% % Our approach addresses (i) the challenge of effectively incorporating enzyme-specific contexts, (ii) the need to fuse spatial and energetic information into attention-based neural modules, and (iii) the requirement to handle activation-site prediction and cleavage-site prediction in a single multi-task setting. 


% \subsection{Overall Architecture and Problem Setup}
% To unify both enzyme and substrate data, we denote by \(E\) an enzyme (sequence plus optional 3D structure) and by \(P\) a protein substrate. Our model learns a mapping
% \[
% C = f(E, P; \Theta),
% \]
% where \(C \in \mathbb{R}^{l}\) indicates cleavage probabilities for the \(l\) residues in \(P\), and \(\Theta\) is the model’s parameter set. The goal is to accurately predict these cleavage probabilities by leveraging:
% \begin{enumerate}
%     \item \textbf{Language-Model Features} from ESM-2 for capturing semantic protein sequence representations.
%     \item \textbf{Enzyme Active-Site Knowledge} to encode enzyme-specific catalytic features.
%     \item \textbf{Structural and Energetic Signals} to incorporate 3D context and frustration metrics.
% \end{enumerate}
% A multi-task loss then jointly optimizes active-site prediction (for enzymes) and cleavage prediction (for substrates).

% \subsection{ESM-Based Feature Extraction}
% \textbf{Motivation.} Traditional sequence-based methods often lack contextual depth, which can limit cleavage prediction accuracy. To remedy this, we adopt the ESM-2 protein language model to extract rich embeddings.

% \textbf{Implementation.} Given an input sequence \(\mathbf{S}\),
% \[
% \mathbf{X} = \text{ESM-2}(\mathbf{S}), 
% \]
% where \(\mathbf{X} \in \mathbb{R}^{l \times d}\) captures contextualized residue-level features of length \(l\) and embedding dimension \(d\). We treat these embeddings as our initial feature set for both enzyme \(E\) and substrate \(P\).

% \subsection{Incorporating Structural and Energetic Information}
% \textbf{Motivation.} Sequence embeddings alone may overlook spatial constraints and residue-level interaction energies. To model the 3D context, we integrate structural distances and energetic frustration metrics.

% \textbf{Gaussian Transformations (Reference).} We adopt standard Gaussian-based transformations for distance \(D_{(i,j)}\) and energy \(E_{(i,j)}\). Concretely, we transform:
% \[
% D_{(i,j)} \mapsto \psi_{\text{dist}}(D_{(i,j)}), 
% \quad
% E_{(i,j)} \mapsto \psi_{\text{energy}}(E_{(i,j)}),
% \]
% where \(\psi_{\text{dist}}\) and \(\psi_{\text{energy}}\) are Gaussian kernels. 
% \emph{(We omit the full formulas here; see Box~\ref{box:gaussian}.)}

% \begin{tcolorbox}[title=Box 1: Standard Gaussian Transformations,colframe=black!20,colback=white]
% \textbf{Distance Normalization:} 
% \(\displaystyle
% D_{(i,j)} = \frac{1}{1+\|r_i-r_j\|}.
% \)
% \qquad
% \textbf{Energy Standardization:} 
% \(\displaystyle
% E_{(i,j)} = \frac{E_{\text{actual}} - \mu_{\text{random}}}{\sigma_{\text{random}}}.
% \)
% \end{tcolorbox}

% \textbf{Unified Representation.} We concatenate or fuse these transformed structural-energetic features with the language-model embeddings to produce a refined representation \(\Phi\), thus capturing both high-level sequence context and low-level 3D or energetic cues.

% \subsection{Attention-Based Integration}
% \textbf{Motivation.} While the above features capture crucial information, an attention mechanism can further highlight residue pairs critical to cleavage. We extend a multi-head attention layer to incorporate the structural/energy biases:
% \[
% \mathbf{A}^h(\mathbf{X}) = \text{softmax}\Bigl(
% \dfrac{\mathbf{X}\mathbf{W}_Q(\mathbf{X}\mathbf{W}_K)^\top}{\sqrt{d}} 
% \;+\; \Phi_{\text{dist}} \;+\; \Phi_{\text{energy}}
% \Bigr),
% \]
% where \(\Phi_{\text{dist}}\) and \(\Phi_{\text{energy}}\) are derived from the transformations above, influencing the attention scores. This bias ensures that the attention layer focuses on residue pairs with structural or energetic significance.

% \subsection{Active-Site Prediction and Site-Aware Pooling}
% \textbf{Objective.} Many proteases have distinct active sites. Identifying these regions may help refine cleavage predictions or provide a secondary task to guide the model’s focus.

% \textbf{Approach.} We employ a simple MLP \(\text{MLP}_{\text{Activation}}\) on enzyme embeddings, producing \(\mathbf{A}\in\mathbb{R}^{l \times 1}\). To pool enzyme features by active-site importance, we define a site-aware pooling mechanism that weights each residue embedding by its predicted active-site probability.

% \begin{tcolorbox}[title=Box 2: Site-Aware Pooling,colframe=black!20,colback=white]
% \(\displaystyle
% \mathbf{E}_{\text{pool}} = \sum\nolimits_i \alpha_i\, \mathbf{E}_i,\quad
% \alpha_i \equiv \mathrm{softmax}(\text{GELU}(\mathbf{A}_i\,\mathbf{W}^1_A)\,\mathbf{W}^2_A).
% \)
% \end{tcolorbox}

% Here, \(\mathbf{E}_i\) is the \(i\)-th residue embedding, and \(\alpha_i\) is its site-aware weight. This mechanism highlights key catalytic regions.

% \subsection{Cleavage Prediction and Multi-Task Loss}
% \textbf{Final Prediction.} After combining enzyme features (possibly pooled by active sites) with substrate embeddings, we pass the result through a convolution or MLP to produce cleavage probabilities \( C \in \mathbb{R}^{l}\). 

% \textbf{Multi-Task Loss.}
% We adopt a multi-task scheme:
% \[
% \mathcal{L} \;=\; 
% \lambda \cdot \mathrm{BCE}_{\mathrm{weighted}}(\mathbf{A}, \text{True}_{\text{active}}) 
% \;+\;
% \mathrm{BCE}_{\mathrm{weighted}}(\mathbf{C}, \text{True}_{\text{cleavage}}),
% \]
% where \(\lambda\) balances active-site prediction against cleavage prediction. Weighted binary cross-entropy emphasizes rare but biologically significant sites. 

% \textbf{Training Pipeline.} We sequentially update \(\Theta\) to minimize \(\mathcal{L}\). The first term encourages accurate active-site identification; the second focuses on cleavage site prediction. This arrangement ensures that modeling of enzyme catalytic regions complements substrate-level cleavage prediction.

% \subsection{Summary of Methodology}
% Overall, our proposed framework unifies:
% \begin{itemize}
%     \item \textit{Language-model embeddings} (from ESM-2) for enriched contextual features,
%     \item \textit{Structural and energetic transformations} to capture 3D constraints and residue-level frustrations,
%     \item \textit{Attention-based integration} to highlight critical residue pairs,
%     \item \textit{Active-site prediction and site-aware pooling} to incorporate enzyme-specific catalytic cues.
% \end{itemize}
% By synergizing these components in a multi-task setting, the model achieves more robust cleavage site prediction even under class imbalance and data scarcity. The next section details our experimental setup and comprehensive evaluations.





% \section{Methodology}

% In this section, we present the architecture and algorithms developed to enhance protein cleavage site prediction by integrating enzyme active-site knowledge. 

% \subsection{Feature Extraction with Language Models}

% We utilize the ESM-2 protein language model to extract rich feature representations from protein sequences. Given an input protein sequence \(\mathbf{S}\), the ESM-2 model generates contextualized embeddings:
% \begin{equation}
% \mathbf{X} = \text{ESM}(\mathbf{S}), \tag{8}
% \end{equation}
% where \(\mathbf{X} \in \mathbb{R}^{l \times d}\) represents the feature matrix with \(l\) being the sequence length and \(d\) the embedding dimension.

% \subsection{Energy and Structure Processing}

% To incorporate energy and distance information between residue pairs, we employ Gaussian functions to transform these features. Specifically, we define separate Gaussian transformations for distance \(D_{(i,j)}\) and energy \(E_{(i,j)}\) as follows:

% \begin{equation}
% \psi_{\text{dist},(i,j)}^{k} = \frac{1}{\sqrt{2\pi} \sigma_D^k} 
% \exp\left( 
% -\frac{1}{2} \left( \frac{D_{(i,j)} + b_D^k - \mu_D^k}{\sigma_D^k} \right)^2
% \right), \tag{1}
% \end{equation}

% \begin{equation}
% \psi_{\text{energy},(i,j)}^{k} = \frac{1}{\sqrt{2\pi} \sigma_E^k} 
% \exp\left( 
% -\frac{1}{2} \left( \frac{E_{(i,j)} + b_E^k - \mu_E^k}{\sigma_E^k} \right)^2
% \right), \tag{2}
% \end{equation}
% \[
% D_{(i,j)} = \frac{1}{1 + \|r_i - r_j\|},\tag{3}
% \]

% \[
% E_{(i,j)} = \frac{E_{\text{actual,{(i,j)}}} - \mu_{\text{random,{(i,j)}}}}{\sigma_{\text{random,{(i,j)}}}},\tag{4}
% \]

% \begin{equation}
% \Phi_{ij} = \text{GELU} \left( \psi_{(i,j)}^{} \mathbf{W}_D^1 \right) \mathbf{W}_D^2, \tag{5}
% \end{equation}

% \begin{equation}
% \psi_{(i,j)} = 
% \begin{bmatrix}
% \psi_{(i,j)}^1 ; \, \dots ; \, \psi_{(i,j)}^K
% \end{bmatrix}^\top, \quad
% \mathbf{W}_D^1 \in \mathbb{R}^{K \times K}, \, \mathbf{W}_D^2 \in \mathbb{R}^{K \times 1}
% \end{equation}

% Here, \(D_{(i,j)}\) represents the normalized distance and \(E_{(i,j)}\) the standardized interaction energy between residues \(i\) and \(j\). \( \mu_D^k \) and \( \mu_E^k \) denote the centers of the Gaussian kernels for distance and energy, with \( \sigma_D^k \) and \( \sigma_E^k \) as their standard deviations, and \( b_D^k \) and \( b_E^k \) as the corresponding bias terms.

% In Equation (3), \( r_i \) and \( r_j \) denote the coordinates of the C\(\alpha\) atoms of residues \( i \) and \( j \), respectively. The equation calculates the Euclidean distance between these two atoms and normalizes it to obtain \( D(i,j) \).

% Equation (4) is used to standardize the interaction energy \( E(i,j) \) between residue pairs. Here, \( E_{\text{actual,(i,j)}} \) represents the actual interaction energy between residues \( i \) and \( j \), while \( \mu_{\text{random,(i,j)}} \) and \( \sigma_{\text{random,(i,j)}} \) are the mean and standard deviation of interaction energies obtained by randomly shuffling residue pairs. This equation is derived from the Frustratometer computational tool, which is utilized to measure and standardize energy frustration within protein structures.



% \subsection{Attention Mechanism Integration}



% After feature extraction using the protein language model, we further process the features with attention layer and integrate structural and energy biases. We enhance the attention mechanism by integrating energy and distance biases into the computation of attention weights. The attention weights for each head \(h\) in layer \(l\) are calculated as:

% \begin{equation}
% \mathbf{A}^h(\mathbf{X}^{(l)}) = \text{softmax} \left(
% \frac{\mathbf{X}^{(l)} \mathbf{W}_Q^{l,h} \left( \mathbf{X}^{(l)} \mathbf{W}_K^{l,h} \right)^\top}{\sqrt{d}} + \Phi_{\text{dist}} + \Phi_{\text{energy}}
% \right), \tag{6}
% \end{equation}
% \begin{equation}
% \mathbf{A}^h(\mathbf{X}^{(l)}) = \text{softmax} \left(
% \frac{\mathbf{X}^{(l)} \mathbf{W}_Q^{l,h} \left( \mathbf{X}^{(l)} \mathbf{W}_K^{l,h} \right)^\top}{\sqrt{d}} + \text{ReLU} \left( \Phi_{\text{dist}} \mathbf{W}^{l,h}_\text{adapter,1} \right) \mathbf{W}^{l,h}_\text{adapter,2}
% \right), \tag{7}
% \end{equation}
% Here, \( \mathbf{W}_Q^{l,h}, \mathbf{W}_K^{l,h}, \mathbf{W}_V^{l,h} \in \mathbb{R}^{d \times d_H} \) are the weight matrices for queries, keys, and values, where \( d \) represents the dimensionality of the embeddings. Additionally, \( \mathbf{W}^{l,h}_\text{adapter,1} \in \mathbb{R}^{d \times d} \) and \( \mathbf{W}^{l,h}_\text{adapter,2} \in \mathbb{R}^{d \times d} \) denote the adapter weight matrices.


% The transformer layers process the input embeddings with the integrated attention mechanism:

% \begin{equation}
% \hat{\mathbf{X}}^{(l)} = \mathbf{X}^{(l)} + \sum_{h=1}^H \mathbf{A}^h(\mathbf{X}^{(l)}) \mathbf{X}^{(l)} \mathbf{W}_V^{l,h} \mathbf{W}_O^{l,h}, \tag{9}
% \end{equation}
% \begin{equation}
% \mathbf{X}^{(l+1)} = \hat{\mathbf{X}}^{(l)} + \text{GELU} \left( \hat{\mathbf{X}}^{(l)} \mathbf{W}_1^l \right) \mathbf{W}_2^l, \tag{10}
% \end{equation}

% Here, \( \mathbf{W}_O^{l,h} \in \mathbb{R}^{d_H \times d} \) are the output weight matrices, and \( \mathbf{W}_1^l \in \mathbb{R}^{d \times r} \) and \( \mathbf{W}_2^l \in \mathbb{R}^{r \times d} \) are the weight matrices of the feed-forward layers, where \( H \) denotes the number of attention heads and \( r \) represents the hidden layer dimension. The activation function used is \( \text{GELU} \).

% \subsection{Activation Sites Prediction}

% Given the enzyme's output features \( \mathbf{X}_{\text{Enzyme}} \in \mathbb{R}^{l \times d} \), we predict activation sites using a multi-layer perceptron (MLP):

% \begin{equation}
% \mathbf{A} = \text{Sigmoid} \left( \text{MLP}_{\text{Activation}} (\mathbf{X}_{\text{Enzyme}}) \right), \tag{11}
% \end{equation}

% Here, \( \mathbf{A} \in \mathbb{R}^{l \times 1} \) represents the activation site probabilities.

% \subsection{Site-Aware Pooling}

% We introduce activation-site-based weighted pooling to further optimize enzyme features. :

% \begin{equation}
% \overset{\cdot}{A}_{i}^{k} = \frac{1}{\sqrt{2\pi} \sigma_A^k} 
% \exp\left( 
% -\frac{1}{2} \left( \frac{A_{i} + b_A^k - \mu_A^k}{\sigma_A^k} \right)^2
% \right), \tag{}
% \end{equation}
% \begin{equation}
% \overset{\sim}{A}_{i} = \text{softmax} \left( \text{GELU} \left( \overset{\cdot}{A}_{i} \mathbf{W}_A^1 \right) \mathbf{W}_A^2 \right), \tag{}
% \end{equation}
% \begin{equation}
% \overset{\cdot}{A}_{i} = 
% \begin{bmatrix}
% \overset{\cdot}{A}_{i}^1 ; \, \dots ; \, \overset{\cdot}{A}_{i}^{k}
% \end{bmatrix}^\top, \quad
% \mathbf{W}_A^1 \in \mathbb{R}^{K \times K}, \, \mathbf{W}_A^2 \in \mathbb{R}^{K \times 1}
% \end{equation}
% \begin{equation}
% \mathbf{E}_{\text{pool}} = \sum_{i=1}^l \overset{\sim}{A}_i \mathbf{E}_i, \tag{12}
% \end{equation}
% Here, \( A_i \) denotes the activation site prediction for the \( i \)-th amino acid, and \( \mathbf{E}_i \) represents the feature vector of the \( i \)-th amino acid in the enzyme.

% \subsection{Feature Concatenation and Final Cleavage Prediction}

% We concatenate the pooled enzyme features \( \mathbf{E}_{\text{pool}} \in \mathbb{R}^d \) with substrate features \( \mathbf{S} \in \mathbb{R}^{l \times d} \):

% \begin{equation}
% \mathbf{F}_{\text{combined}} = \text{cat}(\mathbf{S}, \mathbf{E}_{\text{pool}}), \tag{13}
% \end{equation}

% where \( \mathbf{F}_{\text{combined}} \in \mathbb{R}^{l \times 2d} \).

% The combined features are fed into an MLP to obtain the final cleavage site predictions:

% \begin{equation}
% C = \text{Sigmoid}( \text{Conv1D}_{\text{Cleavage}}(\mathbf{F}_{\text{combined}})), \tag{14}
% \end{equation}
% here \(\text{Conv1D}_{\text{Cleavage}}\) represents a one-dimensional convolutional layer with a kernel size of 31 and a stride of 1. This design focuses on the local information around each cleavage site by considering the 15 amino acid positions before and after the target site. \( C \in \mathbb{R}^{l \times 1} \) represents the predicted cleavage site probabilities.


% \subsection{Loss Function}

% Our model employs a multi-task learning approach, combining losses from activation site prediction and cleavage site prediction.

% \begin{equation}
% \mathcal{L} = 
% \lambda \cdot \text{BCE}_{weighted}\bigl(\mathbf{A}, \text{True}_{\text{active}}\bigr) \;+\;  
% \text{BCE}_{weighted}\bigl(\mathbf{C}, \text{True}_{\text{cleavage}}\bigr),
% \tag{17}
% \end{equation}


% Here, \( \mathbf{A} \) represents the predicted activation site probabilities, and \( \text{True}_{\text{active}} \) denotes the ground truth labels for activation sites. Similarly, \( \mathbf{C} \) represents the predicted cleavage site probabilities, while \( \text{True}_{\text{cleavage}} \) denotes the ground truth labels for cleavage sites. The total loss value is represented as \( \mathcal{L} \). The parameter \(\lambda\) is a tunable hyperparameter.

% To address class imbalance and emphasize the importance of activation and cleavage sites, we apply a weighted binary cross-entropy loss. Specifically, activation and cleavage sites are assigned a higher weight compared to other sites, with a weight ratio of 9:1. This weighting ensures that the model prioritizes accurate predictions of biologically significant sites during training.