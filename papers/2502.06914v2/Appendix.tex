\appendix


\section{Details of Data Curation and Benchmark Construction}
% \label{sec:appendix-data}
\label{sec:app_dataset} 

% \enyan{This section is poor organized. The appendix does not have very strong requirement in writing. But it should looks nice and beautiful. Now, this section looks very noisy and not easy to follow. Please (1) do not use too many subsections. (2) do not frequently use bullets. If you want to use bullets, please make them concat instead of with a lot of space.}

\subsection{Data Curation and Preprocess}


For the cleavage dataset, we downloaded enzyme-substrate pairs from the MEROPS~\cite{Merops} database, collected substrate sequences from the UniProt database, and retrieved enzyme sequences recorded in MEROPS. Additionally, we compared the enzyme sequences between MEROPS and UniProt, excluding those with discrepancies, as such inconsistencies often result from asynchronous updates. To maintain controllable sequence lengths, we filtered out all enzyme and substrate sequences exceeding 1{,}500 residues. 

As for the supplemened enzyme set with active sets, we first searched in the UniProt~\cite{Uniprot} database for enzymes with EC numbers starting with 3.4.*\textit{.}* and filtered for reviewed data. Then, we selected entries with annotated active sites as our pretraining dataset. In addition, proteolytic  enzymes in MEROPS are all annotated with active sites, and are combined as the supplemented enzyme set.

Protein structures were collected from the PDB~\cite{PDB} and AlphaFoldDB~\cite{AlphaFoldDB}. For proteins without available structures in these databases, we generated their structures using OmegaFold~\cite{OmegaFold}.

\subsection{Data Expansion}
The MEROPS database classifies enzymes into categories based on their substrate cleavage sites. Enzymes belonging to the same MEROPS category typically share highly similar cleavage-site characteristics\cite{Merops}. Drawing on previous work, we assume that minor sequence differences among enzymes of the same category can be disregarded. Consequently, the hydrolysis information from a substrate–enzyme pair is extended to all enzymes in that category.

Therefore, we expanded our dataset by matching each substrate not only with the originally mapped enzyme but also with other enzymes in the same MEROPS category. Through this procedure, we obtained approximately 220,000 valid enzyme–substrate pairings, involving 680 unique enzymes.

\begin{table}[t]
\small
\centering
\caption{Dataset statistics of training datasets.}
\label{table:dataset_statistics}
\begin{tabular}{llrrr}
\toprule
\textbf{Utilization} & \textbf{Datasets} & \textbf{\# Substrate-Enzyme Pairs}& \textbf{Enzymes}& \textbf{Substrates}\\ 
\midrule
Active site dataset          & UniPort          & NA& 11,530& NA\\ 
\midrule
 Cleavage site dataset & MEROPS& 197,613& 677& 7,475\\   
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\small
\centering
\caption{Dataset statistics of evaluation benchmarks.}
\label{table:dataset_statistics}
\begin{tabular}{llr}
\toprule
\textbf{Settings} & \textbf{Datasets} & \textbf{\# Substrate-Enzyme Pairs} \\
\midrule
Supervised& C14.005& 1,638\\ 
     & C14.003& 1,462\\ 
     & M10.003& 1,209\\ 
\midrule
Zero-shot& M10.004& 204\\ 
     & A01.009& 1,500\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Construction of Supervised and Zero-shot Benchmarks}

\textbf{Supervised Setting.}
We selected three MEROPS categories as supervised benchmarks (M10.003, C14.003, and C14.005), randomly splitting all enzyme--substrate pairs in each category into training and test sets with an 8:2 ratio. To ensure the test substrates were sufficiently distinct from those in training, we first collected all associated substrates for each enzyme category, then computed pairwise sequence similarity using the Needleman--Wunsch algorithm (BLOSUM62, gap opening penalty of 10, gap extension penalty of 0.5). Substrates exhibiting less than 50\% similarity to other substrates were considered relatively independent, and we sampled 20\% number of all  substrates in these independent substrates to form the final test set. This procedure helps to evaluate model generalization to more divergent substrates within the same MEROPS category.

\textbf{Zero-shot Setting.} 
We constructed a zero-shot benchmark using two other MEROPS categories (A01.009 and M10.004) by collecting all enzymes in each category and computing their pairwise similarities with the Needleman--Wunsch algorithm (BLOSUM62, gap opening penalty of 10, gap extension penalty of 0.5). We increased the NW threshold to 80\% to account for the higher intrinsic similarity of enzymes within each category; enzymes that fell below this threshold were considered distinct and approximately 20\% of them were selected to form the zero-shot test set. This ensures no overlap between the zero-shot test enzymes and those used in training.

\subsection{Dataset Statistics}
In total, the final dataset contains about 200,000 enzyme--substrate pairs. These pairs span multiple MEROPS categories. Detailed distributions of enzyme and substrate pairs are provided in Table. ~\ref{table:dataset_statistics}.





\section{Details of Baselines}
\label{sec:app_baseline}

Below, we provide additional details on how we adapt, retrain, or utilize each baseline for comparison. Unless otherwise specified, all default hyperparameters are used as in the original implementations of these methodle. For any required data, we convert our data format accordingly.

\textbf{Procleave}~\cite{procleave} and \textbf{ProsperousPlus}~\cite{ProsperousPlus} both provide publicly available code, enabling us to retrain their models within our supervised setting. We use the same training and test sets as those used for our method, specifically for the supervised benchmark. We adopt the default training code from each repository while ensuring that all other settings remain consistent.



\textbf{ScreenCap3}~\cite{ScreenCap3} and \textbf{CAT3}~\cite{CAT3}, specialized for the C14.003 enzyme, do not provide publicly available datasets or source code for retraining. Instead, they each offer a prediction platform: a web server for ScreenCap3 and standalone software for CAT3. We use these platforms to generate predictions on our test set. Since their training data are not publicly accessible, we can only report their performance as is, with the caveat that neither model can be applied to other enzymes.



We also compare with two recent enzyme–substrate interaction models, \textbf{ClipZyme}~\cite{ClipZyme} and \textbf{ReactZyme}~\cite{ReactZyme}, which were originally proposed for reaction rather than cleavage prediction. \textbf{ReactZyme} encodes enzymes with an ESM-2 plus MLP pipeline, but since its trained weights are unavailable, we retrain it from scratch on our dataset. \textbf{ClipZyme} employs an E(n) Equivariant Graph Neural Network (EGNN) to incorporate structural information into its enzyme encoder. Both models use average-pooling to aggregate the extracted enzyme features and are trained without activation-site loss. To highlight the effect of leveraging active-site knowledge, we keep the original pretrained EGNN for ClipZyme as is and integrate it into our cleavage-site prediction framework, adding only a linear projection layer to interface with the cleavage-site prediction module.






\section{Implementation Details}
\label{sec:app_imp}

\textbf{Framework and Hardware.} We implemented our models in PyTorch and trained using the Adam optimizer with a learning rate of \(1\times10^{-4}\) and a batch size of 48. All experiments were conducted on eight NVIDIA A100 GPUs. We adopted an early stopping strategy with a patience of 3 epochs, monitoring the validation loss to prevent overfitting.



\paragraph{Substrate Representations.}
\label{sec:Substrate Encoding Details}
Similar to the enzyme pipeline, but without energetic frustration, each residue is embedded by ESM-2 padded to 1500 length. We compute pairwise C$\alpha$-distances $\mathbf{D}^s(i,j) = \|\mathbf{r}_i - \mathbf{r}_j\|_2$, then applying a reciprocal transform. Each distance entry is processed by a Gaussian basis kernel and MLP, yielding a bias term $\Phi_{i,j}^{\mathrm{dist}}$ added to the attention score:
\begin{equation}
    \mathbf{A}_{i,j}^{k} 
    = 
    \frac{(\mathbf{h}_i^{k-1} \mathbf{W}_Q)
          (\mathbf{h}_j^{k-1} \mathbf{W}_K)^T}{d}
    + 
    \Phi_{i,j}^{\mathrm{dist}},
\end{equation}
thus incorporating structural information. The substrate representation $\mathbf{H}^s \in \mathbb{R}^{|\mathcal{P}^s| \times d}$ is obtained via
\begin{equation}
    \mathbf{H}^s = \mathrm{Transformer}(\mathbf{X}^s, \mathbf{D}^s),
\end{equation}
with the same architecture as the enzyme encoder but omitting energy-related parameters.











\textbf{Energy Frustration Calculation.} 
We computed residue-pair frustration using the \textbf{Frustratometer} tool~\cite{Frustratometer} with AWSEM (Associative Water-mediated Structure and Energy Model) potentials~\cite{AWSEM}, disabling electrostatic interactions ($k_{\mathrm{electrostatics}}=0$) and enforcing a minimum sequence separation of 12 residues between residue pairs. Specifically, for each pair of residues \((i,j)\) in enzyme \(\mathcal{P}^e\), the actual interaction energy \(\mathbf{E}(i,j)\) was extracted from the AWSEM potential. To capture local energetic fluctuations, we generated an ensemble of randomized configurations (where the sequence or side-chain identities are shuffled while preserving the protein backbone), thereby obtaining a distribution of interaction energies for each pair.

Let \(\mu_{\mathrm{rand}}(i,j)\) and \(\sigma_{\mathrm{rand}}(i,j)\) be the mean and standard deviation of these interaction energies over the randomized ensemble. The frustration score \(\mathbf{F}(i,j)\) is then computed as:
\begin{equation}
    \mathbf{F}(i,j) \;=\; \frac{ \mathbf{E}(i,j) \;-\; \mu_{\mathrm{rand}}(i,j) }{ \sigma_{\mathrm{rand}}(i,j) }.
\end{equation}
A higher \(\mathbf{F}(i,j)\) indicates that the local region around residues \((i,j)\) is more frustrated (i.e., further from minimal AWSEM-derived energy). Such regions often correspond to sites of functional importance in enzymes.

To estimate how \(\mathbf{E}(i,j)\) deviates from an energetically minimal arrangement, we generated an ensemble of randomized ``decoy'' configurations for the same residue pair. These decoys preserve global geometry (e.g. backbone coordinates) but shuffle aspects such as side-chain packing or local environment, depending on the chosen protocol within the \textbf{Frustratometer}. Each decoy thus provides a distinct pairwise interaction energy. By sampling multiple decoys, we obtain an approximate distribution of energies \(\tilde{E}_k(i,j)\), from which we compute:
\begin{align}
    \mu_{\mathrm{rand}}(i,j) \;&=\; \frac{1}{K}\sum_{k=1}^{K} \tilde{E}_k(i,j), \\
    \sigma_{\mathrm{rand}}(i,j) \;&=\; \sqrt{\frac{1}{K-1}\sum_{k=1}^{K} \Bigl(\tilde{E}_k(i,j) - \mu_{\mathrm{rand}}(i,j)\Bigr)^2},
\end{align}
where \(K\) is the number of randomized decoys (typically on the order of a few hundred in the \textbf{Frustratometer}).





\textbf{Gaussian Basis Kernel Function.}
Following Transformer-M~\cite{TransformerM}, we employ a set of learnable Gaussian basis kernels to transform a scalar input (e.g., the distance \(\mathbf{D}(i,j)\) or the frustration score \(\mathbf{F}(i,j)\)) into a fixed-dimensional embedding. Concretely, suppose we have \(K\) Gaussian kernels parameterized by \(\{\mu^k, \sigma^k\}_{k=1}^{K}\). For an input scalar \(x\), the Gaussian basis kernel function \(\phi(x)\) is defined as:
\begin{equation}
\label{eq:gaussian_basis}
    \phi(x)
    \;=\;
    \Bigl[
        \exp\Bigl(-\tfrac{1}{2}\bigl(\tfrac{x - \mu^1}{\sigma^1}\bigr)^2\Bigr),\;
        \exp\Bigl(-\tfrac{1}{2}\bigl(\tfrac{x - \mu^2}{\sigma^2}\bigr)^2\Bigr),
        \;\dots,\;
        \exp\Bigl(-\tfrac{1}{2}\bigl(\tfrac{x - \mu^K}{\sigma^K}\bigr)^2\Bigr)
    \Bigr]^\top.
\end{equation}
Each kernel center \(\mu^k\) and width \(\sigma^k\) is learnable, allowing the model to adaptively capture different regions of the input space. We apply this basis expansion to both \(\mathbf{D}(i,j)\) and \(\mathbf{F}(i,j)\), producing a \(K\)-dimensional vector for each pair \((i,j)\). An MLP then projects this kernel output into the space of attention biases.
We set the number of Gaussian basis functions to \(K=10\), each parameterized by learnable centers \(\mu^k\) and widths \(\sigma^k\). Notably, we maintain \emph{separate} sets of Gaussian parameters for the energy and structure channels, ensuring that the model can adaptively learn distinct representations for each. 




\textbf{Training Algorithm.}
Each sample's ESM-2 embeddings (padded to length 1500), along with distance and energy frustration matrices, are fed into our model to predict both active-site and cleavage-site residues. We use a weighted binary cross-entropy loss and optimize with Adam for up to 15 epochs, applying early stopping (patience = 3) based on validation loss.

\begin{algorithm}[H]
\caption{Training algorithm of {\method}}
\label{alg:unizyme}
\begin{algorithmic}[1]
\REQUIRE Supplemented enzyme set $\mathcal{D}_a$, Cleavage site prediction dataset $\mathcal{D}_c$, hyperparameters $\lambda$ 
\ENSURE A unified cleavage site predictor $f_{\theta}$ 
    \STATE Initialize features of enzyme and substrate protein by ESM-2
    \STATE Pretrain enzyme encoder on $\mathcal{D}_a$ with $\mathcal{L}_a$ by Eq.(\ref{eq:activate loss})
    \FOR{epoch = 1 to $N$}
        \FOR{each batch $(\mathcal{P}^e, \mathcal{P}^s)$ in $\mathcal{D}_c$}
            \STATE Compute the distance matrix $\mathbf{D}^e$, and energetic frustration matrix $ \mathbf{F}^e$ from enzyme structure by Eq.(\ref{eq:energy cal}) and Eq.(\ref{eq:distance calculation})
            \STATE Encode the enzyme by Eq.(\ref{eq:attention+DE})
            \STATE Obtain the enzyme representation with active site-aware pooling by Eq.(\ref{eq:sitepooling})
            \STATE Encode the substrate protein by Eq.(\ref{eq:subencoder})
            \STATE Predict active sites of enzymes by Eq.(\ref{eq:actsite_prediction})
            \STATE Predict cleavage sites of substrate proteins by Eq.(\ref{eq:1DCNN})
            \STATE Update $\theta$ via $\nabla(\mathcal{L}_c + \lambda \mathcal{L}_a)$
        \ENDFOR
        \IF{validation loss increases for 3 epochs}
            \STATE \textbf{break}
        \ENDIF
    \ENDFOR
\end{algorithmic}
\end{algorithm}











\section{Additional Experiments and Visualizations }
\label{sec:app_exp}
In this section, we present a detailed comparison of the benchmark results across five datasets, including AUC of ROC and PR, as well as the corresponding ROC and PR curves. Additionally, as shown in Fig. \ref{fig:Activate Site ROCPR}, we conducted zero-shot testing on all enzymes not included in the training data to evaluate the model's capability in predicting enzyme active sites.


\begin{table*}[ht]
\centering
\caption{Comparison of model performance on ROC and PR AUC for datasets M10.004 and A01.009.}
\label{tab:comparison_performance}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{M10.004}} & \multicolumn{2}{c}{\textbf{A01.009}} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & \textbf{ROC AUC} & \textbf{PR AUC} & \textbf{ROC AUC} & \textbf{PR AUC} \\ 
\midrule
\textbf{UniZyme}             & 0.9943 ± 0.0006 & 0.8282 ± 0.0177 & 0.9914 ± 0.0003 & 0.3751 ± 0.0063 \\ 
\textbf{ReactZyme}           & 0.9920 ± 0.0009 & 0.7104 ± 0.0280 & 0.9859 ± 0.0003 & 0.1806 ± 0.0031 \\ 
\textbf{ClipZyme}            & 0.9865 ± 0.0015 & 0.5654 ± 0.0349 & 0.9892 ± 0.0003 & 0.2524 ± 0.0058 \\ 
\textbf{UniZyme\textbackslash SE} & 0.9940 ± 0.0009 & 0.8168 ± 0.0174 & 0.9892 ± 0.0004 & 0.3260 ± 0.0095 \\ 
\textbf{UniZyme\textbackslash P}  & 0.9876 ± 0.0015 & 0.6858 ± 0.0323 & 0.9898 ± 0.0002 & 0.3364 ± 0.0050 \\ 
\textbf{UniZyme\textbackslash A}  & 0.9872 ± 0.0007 & 0.6537 ± 0.0247 & 0.9882 ± 0.0005 & 0.2864 ± 0.0063 \\ 
\bottomrule
\end{tabular}
\end{table*}




\begin{table*}[ht]
\centering
\small
\caption{Ablation study on model performance on ROC and PR AUC for datasets M10.003, C14.005, and C14.003.}
\label{tab:ablation_study}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{M10.003}} & \multicolumn{2}{c}{\textbf{C14.005}} & \multicolumn{2}{c}{\textbf{C14.003}} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & \textbf{ROC AUC} & \textbf{PR AUC} & \textbf{ROC AUC} & \textbf{PR AUC} & \textbf{ROC AUC} & \textbf{PR AUC} \\ 
\midrule
\textbf{UniZyme}              & 0.8702 ± 0.0020 & 0.0729 ± 0.0035 & 0.9620 ± 0.0014 & 0.5220 ± 0.0093 & 0.9725 ± 0.0009 & 0.4593 ± 0.0120 \\ 
\textbf{UniZyme\textbackslash SE} & 0.8210 ± 0.0030 & 0.0611 ± 0.0024 & 0.9437 ± 0.0027 & 0.4873 ± 0.0107 & 0.9479 ± 0.0015 & 0.4488 ± 0.0122 \\ 
\textbf{UniZyme\textbackslash P}  & 0.8242 ± 0.0022 & 0.0571 ± 0.0023 & 0.9367 ± 0.0040 & 0.4659 ± 0.0144 & 0.9544 ± 0.0017 & 0.4270 ± 0.0106 \\ 
\textbf{UniZyme\textbackslash A}  & 0.8254 ± 0.0028 & 0.0608 ± 0.0023 & 0.9474 ± 0.0022 & 0.4852 ± 0.0135 & 0.9316 ± 0.0018 & 0.3979 ± 0.0123 \\ 
\bottomrule
\end{tabular}
\end{table*}








\begin{figure}[t]
  \centering
  
  % 第一行：A01009
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/ROC_A01009.png}
    \caption{ROC Curve - A01009}
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/PR_A01009.png}
    \caption{PR Curve - A01009}
  \end{subfigure}
  % \vspace{1cm}

  % 第二行：C14003
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/ROC_C14003.png}
    \subcaption{ROC Curve - C14003}
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/PR_C14003.png}
    \caption{PR Curve - C14003}
  \end{subfigure}
  % \vspace{1cm}

  % 第三行：C14005
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/ROC_C14005.png}
    \caption{ROC Curve - C14005}
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/PR_C14005.png}
    \caption{PR Curve - C14005}
  \end{subfigure}

  \caption{Comparisons in cleavage site prediction under the supervised setting for seen enzymes. }
\end{figure}

\begin{figure}[t]
  \centering
  % 第四行：M10003
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/ROC_M10003.png}
    \caption{ROC Curve - M10003}
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/PR_M10003.png}
    \caption{PR Curve - M10003}
  \end{subfigure}

  % 第五行：M10004
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/ROC_M10004.png}
    \caption{ROC Curve - M10004}
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/PR_M10004.png}
    \subcaption{PR Curve - M10004}
  \end{subfigure}

    \caption{Comparisons in cleavage site prediction under the zero-shot setting for unseen enzymes. }
  \label{fig:full_results}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/ROC_EnZyme.pdf}
    \caption{ROC Curve - Activate Site Prediction}
  \end{subfigure}
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.85\linewidth]{figure/PR_EnZyme.pdf}
    \subcaption{PR Curve - Activate Site Prediction}
  \end{subfigure}

    \caption{Model performance of activate site prediction}
  \label{fig:Activate Site ROCPR}
\end{figure}

