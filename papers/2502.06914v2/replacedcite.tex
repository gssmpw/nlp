\section{Related Works}
% \enyan{@Chenao, complete the releated work section.}
% \subsection{Research Challenges}

% \textbf{Lack of zero-shot capability:} 
% Existing methods cannot predict hydrolysis sites for novel enzymes, limiting their generalizability to unseen cases.

% \textbf{Neglecting enzyme-specific information:} 
% Most approaches do not explicitly model how the enzyme itself influences cleavage site prediction.

% \textbf{Low accuracy with limited data:} 
% Current cleavage site predictors perform poorly when training data for a particular enzyme-substrate pair are scarce.

% \textbf{Fragmented data and specialized models:}
% Hydrolysis data for different enzymes are isolated. Previous methods often train separate models for each enzyme type (e.g., Prosperous needs over 90 models; MPC requires fine-tuning a new model per protease type). A unified model is still missing.

% \textbf{Insufficient use of catalytic site information:} 
% Methods like Clip-Zyme or React-Zyme either ignore catalytic site features or only leverage them in downstream tasks, yielding limited performance gains. Effectively incorporating catalytic site information for biological process modeling remains challenging.

% \textbf{Ignoring energetic factors:} 
% Although some models (e.g., EasIFA) focus on predicting enzyme active sites, they often overlook the role of energy. Energetic frustration typically arises near the enzyme’s active site, and incorporating energy can improve enzyme representation.


\textbf{Protein Representation Learning}. 
Protein representation learning aims to effectively capture and represent the structural and functional features of proteins for downstream tasks. Inspired by large language models, recent years have seen the emergence of sequence-based pre-trained models such as ESM____, and ProtTrans____. In terms of methods that utilize structural information, geometric graph neural networks____ and transformers with structural constraints____ have become the widely-used architectures. The pretraining on structural information also facilitate the performance on downstream tasks. Accurate prediction of enzyme-catalyzed reactions requires better modeling of enzymes. ClipZyme____ uses EGNN____ to represent protein graphs and utilizes ESM-2 embeddings to initialize node representations. ReactZyme____, similar to ClipZyme, additionally employs a structure-based protein language model____. Although these methods combine structural and contextual features to represent enzymes, they do not incorporate the enzyme’s energy landscape and active-site knowledge, which are crucial for understanding the enzyme’s function and properties.

%Protein representation learning aims to effectively capture and represent the structural and functional features of proteins for predicting their three-dimensional structure, functional sites, and interactions with other molecules. Inspired by large language models, recent years have seen the emergence of sequence-based pre-trained models such as ESM, ProteinBERT, and ProtTrans. Their representations capture the biochemical properties of residues and the remote homology of proteins. In terms of methods that utilize structural information, geometric graph neural networks and transformers with structural constraints have almost become the preferred architectures, with pretraining on structural information also improving performance. Accurate prediction of enzyme-catalyzed reactions requires better modeling of enzymes. ClipZyme uses EGNN to represent protein graphs and utilizes embeddings from the ESM-2 model to initialize node representations. ReactZyme, similar to ClipZyme, uses the SE3-transformer to leverage structural information of enzymes and additionally employs a structure-based protein language model. Although these methods combine structural and contextual features to represent enzymes, they do not incorporate the enzyme’s energy landscape and active site knowledge, which are crucial for understanding the enzyme’s function and properties.

%Protein representation learning aims to effectively capture and represent the structural and functional features of proteins for predicting their three-dimensional structure, functional sites, and interactions with other molecules. Inspired by large language models, recent years have seen the emergence of sequence-based pre-trained models such as ESM____, ProteinBERT____, and ProtTrans____. Their representations capture the biochemical properties of residues and the remote homology of proteins. In terms of methods that utilize structural information, geometric graph neural networks____ and transformers with structural constraints____ have almost become the preferred architectures, with pretraining on structural information also improving performance. ClipZyme____ uses EGNN____ which is computationally lightweight to represent protein graphs and utilizes embeddings from ESM-2 model to initialize node representations. ReactZyme____ uses the SE3-transformer____ to leverage structural information of enzymes, similar to ClipZyme, it also uses ESM-2 to initialize node representations, but differs by additionally using a structure-based protein language model____, where the protein embeddings are computed based on structure-aware sequence tokens. Although these methods combine structural and contextual features to represent enzymes, the enzyme's energy landscape and active site knowledge have not been incorporated, which is crucial for understanding the enzyme's function and properties.
%\enyan{@Shuo, I have revised this section from "Graph and Transformer-Based Enzyme Modeling" to protein representation learning. Please help to complete this related work. Specifically, it should cover:
%1. very basic background works in modeling proteins with.
%2. Recent advances in modeling proteins such as PLM and gearnet.
%3. Some initial efforts are conducted to model enzyme, however they have their limitations compared with our {\method}}

%Recent methods, such as \textbf{ClipZyme}____ and \textbf{ReactZyme}____, are both designed to predict enzyme-substrate reactions. These methods have advanced the modeling of enzyme-substrate interactions by employing graph and protein language model-based representations. \textbf{ClipZyme} utilizes an equivariant graph neural network (EGNN) to model enzyme-substrate interactions, capturing geometric properties critical for enzymatic activity. On the other hand, \textbf{ReactZyme} leverages ESM-2 to predict enzymatic reactions, utilizing pretrained embeddings to encode enzyme features. While these methods effectively combine structural and contextual features for enzyme representation, they do not explicitly incorporate active-site-specific information, which is crucial for accurately modeling enzymatic mechanisms.

% \subsection{Traditional and Deep Learning Methods for Cleavage Site Prediction}
\textbf{Cleavage Site Prediction.}
The early prediction of enzyme-catalyzed cleavage sites relied on substrate sequence patterns, such as CAT3____ and Screen-Cap3____, etc. Recently, machine learning-based methods such as Procleave____ and ProsperousPlus____ begin incorporating substrate structural features to capture the preferences of cleavage sites. Deep learning methods subsequently revolutionized the field, with studies like DeepCleave____ and DeepDigest____ introducing convolutional neural networks and transfer learning to predict protease-specific substrates and cleavage sites, while DeepNeuropePred____ demonstrated the application of protein language models in neuropeptide cleavage prediction. 
However, the aforementioned methods are all enzyme-specific models, which are only applicable to an individual target enzyme. The crucial information of enzymes' active sites are not considered either. Therefore, we propose the unified cleavage site predictor enhanced with active-site knowledge.

%The prediction of enzyme-catalyzed cleavage sites began with early computational methods such as \textbf{CAT3}____ and \textbf{ScreenCap3}____, which utilized sequence patterns of substrate for cleavage site prediction. These approaches laid the groundwork for subsequent advances but were constrained by their reliance on rule-based method and small datasets, which limited their predictive accuracy and generalization.

%Building on these early efforts, machine learning-based methods such as \textbf{Procleave}____ and \textbf{ProsperousPlus}____ further advanced cleavage site prediction. \textbf{Procleave} introduced substrate structural features to capture preferences in cleavage sites. On the other hand, \textbf{ProsperousPlus} offered a comprehensive platform for evaluating cleavage site predictions using machine learning models, providing a broader scope for analyzing enzymatic cleavage. 

%Deep learning approaches have since revolutionized the field, with models such as \textbf{HIV OctaScanner}____ and \textbf{DeepNeuropePred}____ leveraging neural networks and transformer-based architectures. \textbf{HIV OctaScanner} focused on proteolytic cleavage dynamics for HIV-1 protease substrates, while \textbf{DeepNeuropePred} demonstrated the utility of protein language models for neuropeptide cleavage prediction. Additionally, methods like \textbf{DeepCleave}____ introduced convolutional neural networks with transfer learning to predict protease-specific substrates and cleavage sites. 

% \subsection{Graph and Transformer-Based Enzyme Modeling}