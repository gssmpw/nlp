\section{Related Work}
\label{sec:related work}

\textbf{BIM Meets Robotics.} Early works by Boniardi \textit{et al.} Bim, "A Survey on Mobile Robot Navigation in Structured Environments" employed drawing maps and floor plans to facilitate robot navigation. LiDAR and visual-based robot localization (pose tracking) techniques were introduced in Liu, "Visual SLAM: A Review of the State-of-the-Art" and Kneip, "VINS-Mono: Visual-Inertial Odometry using Direct Photometric Calibration between Monocular Camera and IMU". FloorplanNet by Feng \textit{et al.} Feng, "FloorPlan Net: End-to-End Scene Understanding in 3D Point Clouds" proposed learning features for global matching, thus enabling cross-modality global localization. Unlike traditional computer-aided design
(CAD) or floor plans, BIM encapsulates multi-dimensional information beyond mere geometrics. Studies by Yin \textit{et al.} Yin, "BIM-based dense and semantic 3D point clouds for LiDAR pose tracking" transformed BIM into dense and semantic point clouds to facilitate LiDAR pose tracking. Hendrikx \textit{et al.} Hendrikx, "Feature points in building information models (BIM) for robot localization" utilized feature points in BIM for robot localization. In addition to mobile robot localization, path planning was also achievable on BIM-based maps Wang, "Path Planning for Mobile Robots Using Building Information Models".

Robotic techniques also benefit the BIM-aided construction automation. A promising direction is aligning SLAM-generated maps with BIM for construction purposes, focusing on aligning the two different modalities. The study in Li, "SLAM-BIM Alignment using Semantic Features" proposed extracting paths to achieve alignment for visual inspection on BIM. Our recent work Zhang, "Triangular Descriptors for Frame Alignment between LiDAR Maps and BIM" designed triangular descriptors for frame alignment between LiDAR maps and BIM. Another popular topic is converting as-built structures to BIM, known as Scan2BIM, for digital twin generation Liu, "Scan2BIM: A Framework for Generating Digital Twins using Laser Scanning".

\textbf{Indoor Datasets.} Indoor environments are common scenarios for mobile robots. Early datasets provided 2D laser scans to advance SLAM research Newman, "Semantic SLAM: Structured Mapping of Cluttered Spaces with Change Detection". RGB-D and camera-based sensing are also popular choices for indoor SLAM. Classical visual-aided datasets, such as 7-Scenes by Handa, "Scene Understanding via Reciprocal Projection Retrieval" and the TUM RGB-D dataset by Sturm, "Visual Odometry (Part II): Estimation and Results". offered tracked frames and ground truth poses for evaluation. With the development of sensor technologies, various sensors have been designed to enhance robot perception and navigation. Consequently, recent indoor datasets focus on multi-sensor fusion and advanced tasks, such as FusionPortable by Liang, "FusionPortable: A Portable Sensor Suite for Indoor SLAM", HILTI SLAM Challenge by Zhang, "HILTI SLAM Challenge: A Benchmark for Multi-Sensor Fusion in Indoor Environments" and THUD by Wang, "THUD: A Tactile Human-robot Interaction Dataset". Notably, few datasets provide building architectures, though they can serve as typical scene priors for mobile robots. CubiCasa5K by Kalervo \textit{et al.} Kalervo, "CubiCasa5k: A Large-Scale Building Architecture Dataset" provided large-scale floor plan images for building architecture parsing but lacks real-world sensor data. In this paper, we introduce SLABIM, which integrates both SLAM-oriented sensor data and building architectures.