\subsection{Imitation Learning}
Imitation Learning (IL)~\cite{imitation_learning} refers to training policies with expert demonstrations, without requiring a predefined reward function. In the context of reinforcement learning (RL), this is often referred to as inverse RL~\cite{irl_1, irl_2}, where the reward function is derived from the demonstrations and used to train a policy~\cite{wayex, rot, fish, gail, nair2020awac}. While these methods reduce the need for extensive human demonstrations, they still suffer from significant sample inefficiency. As a result of this inefficiency in deploying RL policies in the real world, behavior cloning (BC)~\cite{Pomerleau-1989-15721, torabi2019recent, schaal1996learning, ross2011reduction} has become increasingly popular in robotics. Recent advances in BC have demonstrated success in learning policies for both long-horizon tasks~\cite{sequential_dexterity,learning_to_generalize,clipport} and multi-task scenarios~\cite{baku,roboagent,rtx,track2act,gen2act}. However, most of these approaches rely on image-based representations~\cite{zhang2018deep,baku,diffusionpolicy,roboagent,rtx,bcz}, which limits their ability to generalize to new objects and function effectively outside of controlled lab environments. In this work, we propose \method{}, which attempts to address this reliance on image representations by directly using key points as an input to the policy instead of raw images. Through extensive experiments, we observe that such an abstraction helps learn robust policies that generalize across varying scenarios.\looseness=-1


\subsection{Object-centric Representation Learning} 
Object-centric representation learning aims to create structured representations for individual components within a scene, rather than treating the scene as a whole. Common techniques in this area include segmenting scenes into bounding boxes~\cite{deep_object_centric, learning_to_generalize, one_shot, fang2023anygrasp, viola} and estimating object poses~\cite{deep_object_pose_estimation, hope}. While bounding boxes show promise, they share similar limitations with non object-centric image-based models, such as overfitting to specific object instances. Pose estimation, although less prone to overfitting, requires separate models for each object in a task. Another popular method involves using point clouds~\cite{groot, reagentpointcloudregistration}, but their high dimensionality necessitates specialized models, making it difficult to accurately capture spatial relationships. Lately, several works have resorted to adopting key points~\cite{p3po,ju2025robo,rekep,track2act,gen2act,motiontracks,fang2024keypoint,bechtle2023multimodal} for policy learning due to their generalization ability. Further, key points also allow the direct injection of human priors into the policy learning pipeline~\cite{track2act,gen2act,motiontracks} as opposed to learning representations from human videos followed by downstream learning on robot teleoperated data~\cite{r3m,vptr,gr1,vip,liv,voltron}. In this work, we leverage key points as a unified observation and action space to enable learning generalizable policies exclusively from human videos. \looseness=-1

\setcounter{figure}{1} 
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/method.pdf}
% \vspace{-1em}
\vspace{-1em}
\caption{Overview of the \method{} framework. (a) \method{} leverages state-of-the-art vision models and policy architectures to translate human hand poses into robot poses while capturing object states through sparse single-frame human annotations. (b) The derived key points are fed into a transformer policy to predict the 3D future point tracks from which the robot actions are computed through rigid-body geometry constraints. (c) Finally, the computed action is executed on the robot using end-effector position control at a 6Hz frequency.}
\label{fig:method}
\end{figure*}

\subsection{Human-to-Robot Transfer for Policy Learning}
There have been several attempts at learning robot policies from human videos. Some works first learn visual representations from large-scale human video datasets and learn a downstream policy on these representations using limited amounts of robot data~\cite{r3m,vptr,gr1,vip,liv,voltron}. Another line of work learns coarse policies from human videos, using key points~\cite{track2act} and generative modeling~\cite{gen2act}, which are then improved using downstream learning on robot data. Recently proposed MT-$\pi$~\cite{motiontracks} alleviates the need for downstream learning by co-training a key point policy with human and robot data. A caveat in all these works is that despite having access to abundant human demonstrations, there is a need to collect robot data to achieve a highly performant policy. A recently emerging line of work~\cite{r+x} attempts to do away with this need for robot data by doing in-context learning with state-of-the-art vision-language models (VLMs)~\cite{gemini,gpt4,llama}. However, owing to the large compute times of VLMs, these policies are required to be deployed open-loop and hence, are not reactive to changes in the scene. In this work, we propose \method{}, a new framework that learns generalizable policies from human videos, does not require robot demonstrations or online robot interactions, and can be executed in a closed-loop fashion. \looseness=-1

