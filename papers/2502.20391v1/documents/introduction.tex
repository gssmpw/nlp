Recent years have witnessed remarkable advancements in computer vision (CV) and natural language processing (NLP), resulting in models capable of complex reasoning~\cite{gpt4,gemini,llama}, generating photorealistic images~\cite{dalle3,imagen} and videos~\cite{sora}, and even writing code~\cite{devin}. A driving force behind these breakthroughs has been the abundance of data scraped from the internet. In contrast, robotics has yet to experience a similar revolution, with most robots still confined to controlled or structured environments. While CV and NLP can readily take advantage of large-scale datasets from the internet, robotics is inherently interactive and requires physical engagement with the world for data acquisition. This makes collecting robot data significantly more challenging, both in terms of time and financial resources.

A prominent approach for training robot policies has been the collection of extensive datasets, often through contracted teleoperators~\cite{roboturk,rt1,dobbe}, followed by training deep networks on these datasets~\cite{dobbe,rums,openx,droid}. While effective, these methods tend to require months or even years of human effort~\cite{rt1,droid} and still result in datasets orders of magnitude smaller than those used in CV and NLP~\cite{openx,droid}. A potential solution to this data scarcity in robotics is to tap into the vast repository of human videos available online, showcasing individuals performing a wide range of tasks in diverse scenarios. \looseness=-1

The primary challenge in learning robot policies from human videos lies in addressing the morphology gap between robots and the human body~\cite{whirl,hudor,track2act,gen2act,motiontracks}. Two notable trends have emerged in efforts to utilize human data for learning robot policies: (1) first learning visual representations or coarse policies from human datasets and then finetuning them for downstream learning on robot datasets~\cite{track2act,gen2act,motiontracks,r3m,vptr,gr1,vip,liv,voltron}, and (2) using human videos to compute rewards for autonomous policy learning through reinforcement learning~\cite{xirl,whirl,hudor,kumar2023graph}. While the former requires a substantial amount of robot demonstrations to learn policies for downstream tasks, the latter often requires large amounts of online robot interactions in the real world, which can be time-consuming and potentially unsafe. \looseness=-1

In this work, we introduce \method{}, a new technique to learn robot policies solely from offline human data without requiring robot interactions during training. Our key observation in building \method{} is that both humans and robots occupy the same 3D space in the world, which can be tied together using key points derived from state-of-the-art vision models. 

Concretely, \method{} works in three steps. 
First, given a dataset of human videos, a motion track of key points on the human hand and the object is computed using hand pose detectors~\cite{mediapipe, hamer} and minimal human annotation of one frame per task. These key points are computed from two camera views, which allows for projection in 3D using point triangulation. Second, a transformer-based policy~\cite{baku} is trained to predict future robot points given the set of key points derived in the previous stage. Third, during inference, the predicted future robot points in 3D space are used to backtrack the 6 DOF pose of the robot's end-effector using constraints from rigid-body geometry. The gripper state of the robot end effector is predicted as an additional token. The predicted end-effector pose and gripper state are then executed on the robot at 6 Hz.


We demonstrate the effectiveness of \method{} through experiments on 8 real-world tasks on a Franka robot. Our main findings are summarized below:

\begin{enumerate}[leftmargin=*,align=left]
    \item  \method{} exhibits an absolute improvement of 75\% over prior state-of-the-art policy learning algorithms across 8 real world tasks when evaluated in identical settings as training. (Section~\ref{subsec:spatial_gen}). 
    \item \method{} generalizes to novel object instances, exhibited a 74\% absolute improvement over prior work on a held-out set of objects unseen in the training data. (Section~\ref{subsec:novel_gen}). 
    \item Policies trained with \method{} are robust to the presence of background distractors, performing at par with scenes without clutter (Section~\ref{subsec:distractor}). 
    \item We provide an analysis of co-training \method{} with teleoperated robot data (Section~\ref{subsec:robot_data}) and study the importance of several design choices in \method{} (Section~\ref{subsec:design_considerations}).
\end{enumerate}

% All of our datasets, training, evaluation code and videos of trained policies will be made publicly available on our website.

All of our datasets, and training and evaluation code have been made publicly available. Videos of our
trained policies can be seen here: \website{}.