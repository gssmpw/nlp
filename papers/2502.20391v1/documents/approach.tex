\method{} seeks to learn generalizable policies exclusively from human videos that are robust to significant environmental perturbations and applicable to diverse object locations and types. An overview of our method is presented in Figure~\ref{fig:method}. Before diving into the details, we first present some of the key assumptions needed to run \method{}.

\textbf{Assumptions:} (1) The pose of the human hand in the first frame is known for each task. This is needed to initialize the robot and set that pose as the base frame of operation. This assumption can be relaxed with a hand-pose estimator~\cite{hamer}, which we do not investigate in this work. (2) We operate in a calibrated scene with the camera's intrinsic and extrinsic matrices, and the transforms between each camera and the robot base known. In practice this is a one-time process that takes under 5 minutes when the robot system is first installed.


\subsection{Point-based Scene Representation}
\label{subsec:scene_rep}

Our method begins by collecting human demonstrations, which are then converted to a point-based representation amenable to policy learning. 

\subsubsection{Human-to-Robot Pose Transfer}
For each time step $t$ of a human video, we first extract image key points on the human hand $p_h^t$ using the MediaPipe~\cite{mediapipe} hand pose detector, focusing specifically on the index finger and thumb. The corresponding hand key points $p_h^t$ obtained from two camera views are used to compute the 3D world coordinates $\mathcal{P}_h^t$ of the human hand through point triangulation. We use point triangulation for 3D projection due to its higher accuracy as compared to sensor depth from the camera (Section~\ref{subsec:design_considerations}). The robot position $\mathcal{R}_{pos}^t$ is computed as the midpoint between the tips of the index finger and thumb in $\mathcal{P}_h^t$. The robot orientation $\mathcal{R}_{ori}^t$ is computed as

\begin{equation}
    \label{eq:orientation}
    \begin{aligned}
        \Delta \mathcal{R}_{ori}^t &= \mathcal{T}(\mathcal{P}_h^{0}, \mathcal{P}_h^{t}) \\
        \mathcal{R}_{ori}^t &= \Delta \mathcal{R}_{ori}^t \cdot \mathcal{R}_{ori}^{0}
    \end{aligned}
\end{equation}

where $\mathcal{T}$ computes the rigid transform between hand key points on the first frame of the video, $\mathcal{P}_h^{0}$, and $\mathcal{P}_h^{t}$. The robot end effector pose is then represented at $T_r^t \leftarrow\{\mathcal{R}_{pos}^t, \mathcal{R}_{ori}^t\}$. The robot's gripper state $\mathcal{R}_g$ is computed using the distance between the tip of the index finger and thumb. The gripper is considered closed when the distance is less than 7cm, otherwise open. 
Finally, given the robot pose $T_r^t$, we define a set of $N$ rigid transformations $T$ about the computed robot pose and compute robot key points $\mathcal{P}_r^t$ such that


\begin{equation}
    (\mathcal{P}_r^t)^i = T_r^t \cdot T^i, ~~\forall i \in \{1, ..., N\}
\end{equation}

This process has been demonstrated in Figure~\ref{fig:method}. This approach effectively bridges the morphological gap between human hands and robot manipulators, enabling accurate transfer of demonstrated actions to a robotic framework.


\subsubsection{Environment state through point priors}
To obtain key points on task-relevant objects in the scene, we adopt the method proposed by P3PO~\cite{p3po}. Initially, a user randomly selects one demonstration from a dataset of human videos and annotates semantically meaningful object points on the first frame that are pertinent to the task being performed. This annotation process is quick, taking only a few seconds. The user-annotated points serve as priors for subsequent data generation. Using an off-the-shelf semantic correspondence model, DIFT~\cite{dift}, we transfer the annotated points from the first frame to the corresponding locations in the first frames of all other demonstrations within the dataset. This approach allows us to initialize key points throughout the data set with minimal additional human effort. 

For each demonstration, we then employ Co-Tracker~\cite{cotracker}, an off-the-shelf point tracker, to automatically track these initialized key points throughout the entire trajectory. By leveraging existing vision models for correspondence and tracking, we efficiently compute object key points for every frame in the dataset while requiring user input for only a single frame. This process, illustrated in Figure~\ref{fig:correspondence}, capitalizes on large-scale pre-training of vision models to generalize across new object instances and scenes without necessitating further training. We prefer point tracking over correspondence at each frame due to its faster inference speed and its capability to handle occlusions by continuing to track points. The corresponding object points from two camera views are lifted to 3D world coordinates using point triangulation to obtain the 3D object key points $\mathcal{P}_o$. During inference, DIFT is employed to identify corresponding object key points on the first frame, followed by Co-Tracker tracking these points during execution. \looseness=-1

It is important to note that \method{} utilizes multiple camera views only for point triangulation, with the policy being learned on 3D key points grounded in the robot's base frame. More details on point triangulation can be found in Appendix~\ref{appendix:point_triangulation}. \looseness=-1


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/correspondence.pdf}
% \vspace{-1em}
\caption{Results of the correspondence model when used for the put bottle on rack and sweep broom tasks. On the left is a frame with human annotations for the object points. On the right, we show that semantic correspondence can identify the same points across different positions, new object instances, and background clutter.}
\vspace{-1em}
\label{fig:correspondence}
\end{figure}


\subsection{Policy Learning}
\label{subsec:policy_learning}
For policy learning, we use BAKU~\cite{baku}. Instead of providing raw images as input, we provide the robot points $\mathcal{P}_r$ and object points $\mathcal{P}_o$ grounded in the robot's base frame as input to the policy. A history of observations for each key point is flattened into a single vector which is then encoded using a multilayer perceptron (MLP) encoder. The encoded representations are fed as separate tokens along with a gripper token into a BAKU~\cite{baku} transformer policy, which predicts the future tracks for each robot point $\mathcal{\hat P}_r$ and the robot gripper state $\mathcal{\hat G}_r$ using a deterministic action head. Mathematically, this can be represented as 

\begin{equation}
    \begin{aligned}
        \mathcal{O}^{t-H:t} &= \{\mathcal{P}_r^{t-H:t},~\mathcal{P}_o^{t-H:t}\}\\
        \mathcal{\hat P}_r^{t+1},~\mathcal{G}_r^{t+1} &= \pi(\cdot|~\mathcal{O}^{t-H:t})
    \end{aligned}
\end{equation}

where $H$ is the history length and $\pi$ is the learned policy. Following prior works in policy learning~\cite{aloha, diffusionpolicy}, we use action chunking with exponential temporal averaging to ensure temporal smoothness of the predicted point tracks. The transformer is non-causal in this scenario and hence the training loss is only applied to the robot point tracks.



\subsection{Backtrack Robot Actions from Predicted Key Points}
\label{subsec:robot_action_from_points}
The predicted robot points $\mathcal{\hat P}_r$ are mapped back to the robot pose using constraints from rigid-body geometry. We first consider the key point corresponding to the robot's wrist $\mathcal{\hat P}_r^{wrist}$ as the robot position $\mathcal{\hat R}_{pos}$. The robot orientation $\mathcal{\hat R}_{ori}$ is computed using Eq.~\ref{eq:orientation} considering $\mathcal{R}_{ori}^{0}$ is fixed and known. Finally, the robot action $\mathcal{A}_r$ is defined as 

\begin{equation}
    \mathcal{\hat A}_r = (\mathcal{\hat R}_{pos},~\mathcal{\hat R}_{ori},~\mathcal{\hat G}_r)
\end{equation}

Finally, the action $\mathcal{\hat A}_r$ is executed on the robot using end-effector position control at a 6Hz frequency. 
