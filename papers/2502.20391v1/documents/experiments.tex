Our experiments are designed to answer the following questions: $(1)$ How well does \method{} work for policy learning? $(2)$ How well does \method{} work for novel object instances? $(3)$ Can \method{} handle background distractors? $(4)$ Can \method{} be improved with robot demonstrations? $(5)$ What design choices matter for human-to-robot learning?

\subsection{Experimental Setup}
Our experiments utilize a Franka Research 3 robot equipped with a Franka Hand gripper, operating in a real-world environment. We use the Deoxys~\cite{viola} real-time controller for controlling the robot. The policies utilize RGB and RGB-D images captured using Intel RealSense D435 cameras from two third-person camera views. The action space encompasses the robot's end effector pose and gripper state. We collect a total of 190 human demonstrations across 8 real-world tasks, featuring diverse object positions and types. Additionally, for studying the effect of co-training with robot data (Section~\ref{subsec:robot_data}), we collect a total of 100 robot demonstrations for 4 tasks (Section~\ref{subsec:robot_data}) using a VR-based teleoperation framework~\cite{openteach}. All demonstrations are recorded at a 20Hz frequency and subsequently subsampled to approximately 6Hz. For methods that directly predict robot actions, we employ absolute actions during training, with orientation represented using a 6D rotation representation~\cite{zhou2019continuity}. This representation is chosen for its continuity and fast convergence properties. The learned policies are deployed at a 6Hz frequency during execution. \looseness=-1


\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/spatial_gen.pdf}
% \vspace{-1em}
\caption{\textbf{(left)} Illustration of spatial variation used in our experiments. \textbf{(right)} Range of objects used in our experiments, where the objects on the left are in-domain objects while on the right are unseen objects used in our generalization experiments.}
\vspace{-1em}
\label{fig:generalization}
\end{figure*}

\subsection{Task Descriptions}
\label{subsec:task_desc}
We experiment with manipulation tasks with significant variability in object position, type, and background context. Figure~\ref{fig:rollouts} depicts rollouts for all of our tasks. For each task, we collect data across various object sizes and appearances. During evaluations, we add novel object instances that are unseen during training. The variations in positions and object instances for selected tasks are depicted in Figure~\ref{fig:generalization}, with more examples provided in Appendix~\ref{appendix:novel_objects}. We provide a brief description of each task below.


\paragraph{Close drawer} The robot arm is tasked with pushing close a drawer placed on the table. The position of the drawer varies for each evaluation. We collect 20 demonstrations for a single drawer and run evaluations on the same drawer.

\paragraph{Put bread on plate} The robot arm picks up a piece of bread from the table and places it on a plate. The positions of the bread and the plate are varied for each evaluation. We collect 30 demonstrations for the task of a single bread-plate pair. During evaluations, we introduce two new plates.

\paragraph{Fold towel} The robot arm picks up a towel placed on the table from a corner and folds it. The position of the towel varies for each evaluation. We collect 20 demonstrations for a single towel. During evaluations, we introduce two new towels. \looseness=-1

\paragraph{Close oven} The robot arm is tasked with closing the door of an oven. The position of the oven varies for each evaluation. We collect 20 demonstrations for the task on a single oven and run evaluations on the same oven.

\paragraph{Sweep broom} The robot arm picks up a broom and sweeps the table. The position and orientation of the broom are varied across evaluations. We collect 20 demonstrations for a single broom. During evaluations, we introduce a new broom. \looseness=-1

\paragraph{Put bottle on rack} The robot arm picks up a bottle from the table and places it on the lower level of a kitchen rack. The position of the bottle is varied for each evaluation. We collect 15 demonstrations for 2 different bottles, resulting in a total of 30 demonstrations for the task. During evaluations, we introduce three new bottles.

\paragraph{Put bowl in oven} The robot arm picks up a bowl from the table and places it inside an oven. The position of the bowl varies for each evaluation. We collect 20 demonstrations for the task with a single bowl. During evaluations, we introduce a new bowl.

\paragraph{Make bottle upright} The robot arm pick up a bottle from the table and places it in an upright position. The position of the bottle varies for each evaluation. We collect 15 demonstrations for 2 different bottles, resulting in a total of 30 demonstrations for the task. During evaluations, we introduce two new bottles.


\subsection{Baselines}
We compare \method{} with 4 baselines - \textit{behavior cloning (BC)}~\cite{baku} with RGB and RGB-D images, \textit{Motion Tracks}~\cite{motiontracks}, and \textit{P3-PO}~\cite{p3po}. We describe each method below.

\paragraph{Behavior Cloning (BC)~\cite{baku}} This method performs behavior cloning (BC) using the BAKU policy learning architecture~\cite{baku}, which takes RGB images of the human hand as input and predicts the extracted robot actions as output.

\paragraph{Behavior Cloning (BC) with Depth} This is similar to BC but uses both RGB and depth images as input.

\paragraph{Motion Track Policy (MT-$\pi$)~\cite{motiontracks}} Given an image of the scene and robot key points on the image, MT-$\pi$ predicts the future 2D robot point tracks to complete a task. This approach generates future 2D point tracks for robot points across multiple views, which are then triangulated to obtain 3D points on the robot. These 3D points are subsequently converted to the robot's absolute pose (similar to our proposed method) and treated as the robot's action. Implementation details for MT-$\pi$ have been provided in Appendix~\ref{appendix:motion_tracks}.

\paragraph{P3-PO~\cite{p3po}} This method utilizes image points representing both the robot and objects of interest, projecting them into 3D space using camera depth information. These 3D points serve as input to a transformer policy~\cite{baku}, which predicts robot actions. P3PO's 3D point representations, akin to those in \method{}, enable spatial generalization, adaptability to novel object instances, and robustness to background clutter.


\begin{table*}[t]
\centering
\caption{Policy performance of \method{} on in-domain object instances on 8 real-world tasks.}
\label{table:same_distribution}
\renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1}
\begin{tabular}{lcccccccc}
\toprule
\multicolumn{1}{c}{\textbf{Method}} & \textbf{\begin{tabular}[c]{@{}c@{}}Close\\ drawer\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bread \\ on plate\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Fold\\ towel\end{tabular}}& \textbf{\begin{tabular}[c]{@{}c@{}}Close\\ oven\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Sweep\\ broom\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bottle\\ on rack\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bowl\\ in oven\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Make bottle\\ upright\end{tabular}} \\ \bottomrule
BC~\cite{baku}                    & 0/10 & 0/20 & 0/10 & 0/10 & 0/10 & 0/30 & 1/10 & 0/20 \\
BC w/ Depth                       & 0/10 & 0/20 & 0/10 & 0/10 & 0/10 & 0/30 & 0/10 & 0/20 \\
MT-$\pi$~\cite{motiontracks}      & 2/10 & 2/20 & 0/10 & 4/10 & 0/10 & 8/30 & 0/10 & 0/20 \\
P3-PO~\cite{p3po}                 & 0/10 & 0/20 & 0/10 & 0/10 & 0/10 & 0/30 & 0/10 & 0/20 \\
\hdashline\noalign{\vskip 0.5ex}
\method{} \textbf{(Ours)}         & \textbf{10/10} & \textbf{19/20} & \textbf{9/10} & \textbf{9/10} & \textbf{9/10} & \textbf{26/30} & \textbf{8/10} & \textbf{16/20} \\ \bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t]
\centering
\renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1}
\caption{Policy performance of \method{} on novel object instances on 6 real-world tasks.}
\label{table:novel_objects}
\begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{\textbf{Method}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bread \\ on plate\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Fold\\ towel\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Sweep\\ broom\end{tabular}} &  \textbf{\begin{tabular}[c]{@{}c@{}}Put bottle\\ on rack\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bowl\\ in oven\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Make bottle\\ upright\end{tabular}} \\ \midrule
BC~\cite{baku}               & 0/20 & 0/20 & 0/10 & 0/30 & 0/10 & 0/20 \\
BC w/ Depth                  & 0/20 & 0/20 & 0/20 & 0/30 & 0/10 & 0/20 \\
MT-$\pi$~\cite{motiontracks} & 1/20 & 0/20 & 0/10 & 0/30 & 0/10 & 0/20 \\
P3-PO~\cite{p3po}            & 0/20 & 0/20 & 0/10 & 0/30 & 0/10 & 0/20 \\
\hdashline\noalign{\vskip 0.5ex}
\method{} \textbf{(Ours)}    & \textbf{18/20} & \textbf{15/20} & \textbf{4/10} & \textbf{27/30} & \textbf{9/10} & \textbf{9/20} \\ \bottomrule
\end{tabular}
% \vspace{-1em}
\end{table*}

\begin{table*}[t]
\centering
\renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1}
\caption{Policy performance of \method{} with background distractors on both in-domain and novel object instances.}
\label{table:distractors}
\begin{tabular}{ccccccc}
\toprule
\multicolumn{1}{c}{\textbf{Background distractors}} &
  \multicolumn{2}{c}{\textbf{Put bread on plate}} &
  \multicolumn{2}{c}{\textbf{Sweep broom}} &
  \multicolumn{2}{c}{\textbf{Put bottle on rack}} \\
\cmidrule(lr){2-3}
\cmidrule(lr){4-5}
\cmidrule(lr){6-7}            & In-domain & Novel object & In-domain & Novel object & In-domain & Novel object \\ \midrule
\redcross                     & 19/20     & 18/20        & 9/10      & 4/10        & 26/30     & 27/30         \\
\greencheck                   & 18/20     & 18/20        & 9/10      & 2/10        & 23/30     & 23/30         \\ \bottomrule
\end{tabular}
% \vspace{-1em}
\end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/rollouts.pdf}
% \vspace{-1em}
\caption{Real-world rollouts showing \method{}'s ability on in-domain objects across 8 real-world tasks.}
% \vspace{-1em}
\label{fig:rollouts}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/rollouts_variations.pdf}
% \vspace{-1em}
\caption{Real-world rollouts showing that \method{} generalizes to novel object instances and is robust to background distractors.}
% \vspace{-1em}
\label{fig:rollouts_variations}
\end{figure*}

\subsection{Considerations for policy learning}
\method{} and P3PO use a point-based representation obtained from $640\times480$ images. For correspondence, we use DIFT~\cite{dift} using the first layer of the hundredth diffusion time step with an ensemble size of 8. Point tracking is performed using a modified version of Co-Tracker~\cite{cotracker} that enables tracking one frame at a time, rather than chunks. \method{}, MT-$\pi$, and P3PO use a history of 10 point observations, while the image-based baselines do not use history~\cite{baku}. BC (RGB), BC (RGB-D), and MT-$\pi$ are trained on images of size $256\times256$. All methods predict an action chunk~\cite{aloha} of size 20 (\(\sim \) 3 seconds). \looseness=-1


\subsection{How well does \method{} work for policy learning?}
\label{subsec:spatial_gen}

We evaluate \method{} in an in-domain setting, using the same objects seen during training. The evaluation consists of 10 trials per object for each task, resulting in a variable total number of trials per task. The results of this evaluation are summarized in Table~\ref{table:same_distribution}. Baselines that rely on RGB images as inputs (RGB, RGB-D, MT-$\pi$) perform poorly when trained exclusively on human hand videos. This is largely due to the significant visual differences between the human hand and the robot manipulator. While appearance-agnostic, P3-PO struggles due to noisy depth data from the camera. 
\method{} achieves an average success rate of 88\% across all tasks, outperforming the strongest baseline MT-$\pi$ by 75\%. Overall, these results demonstrate that \method{}'s ability to effectively address challenges related to visual differences and noisy depth data, achieving state-of-the-art performance in an in-domain setting.


\subsection{How well does \method{} work for novel object instances?}
\label{subsec:novel_gen}
Table~\ref{table:novel_objects} compares the performance of \method{} when evaluated on new object instances unseen in the training data. We perform this comparison on a subset of our tasks. We observe that \method{} achieves an average success rate of 74\% across all tasks, outperforming the strongest baseline by 73\%. Compared to P3PO\cite{p3po}, where each task is trained with a variety of object sizes, most of our tasks are trained on a single object instance. Despite this limited diversity in the training data, \method{} demonstrates robust generalization capabilities. Figure~\ref{fig:rollouts_variations} depicts rollouts of \method{} for novel object instances. For a visual reference of the novel object instances used for each task, please refer to Appendix~\ref{appendix:novel_objects}. These results affirm \method{}'s strong generalization capabilities, making it suitable for real-world applications where encountering unseen objects is common.



\subsection{Can \method{} handle background distractors?}
\label{subsec:distractor}
We evaluate the robustness of \method{} in the presence of background clutter, as shown in Table~\ref{table:distractors}. This study is conducted on three tasks - \textit{put bread on plate}, \textit{sweep broom}, and \textit{put bottle on rack}. Trials are conducted using both in-domain and novel object instances. Examples of the distractors used are illustrated in Figure~\ref{fig:method}, with Figure~\ref{fig:rollouts_variations} depicting rollouts of \method{} in the presence of background distractors.  We observe that \method{} is robust to background clutter, exhibiting either comparable performance or only minimal degradation in the presence of background distractors. This robustness can be attributed to \method{}'s use of point-based representations, which are decoupled from raw pixel values. By focusing on semantically meaningful points rather than image-level features, \method{} enables policies that are resilient to environmental perturbations. \looseness=-1



\begin{table}[t]
\centering
\renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1}
\caption{Policy performance of \method{} with teleoperated robot data on in-domain object instaces.}
\label{table:robot_data}
\begin{tabular}{ccccc}
\toprule
\multicolumn{1}{c}{\textbf{Demonstrations}} &
  \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Put bread \\ on plate\end{tabular}}} &
  \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Fold \\ towel\end{tabular}}} &
  \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Sweep \\ broom\end{tabular}}} &
  \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Make bottle \\ upright\end{tabular}}} \\ \midrule
Human            & 19/20 & \textbf{9/10} & \textbf{9/10} & \textbf{16/20} \\
Robot            & 18/20 & \textbf{9/10} & 4/10 & 12/20 \\
Human + Robot    & \textbf{20/20} & \textbf{9/10} & 8/10 & 8/20  \\ \bottomrule
\end{tabular}
\vspace{-1em}
\end{table}

\subsection{Can \method{} be improved with robot demonstrations?}
\label{subsec:robot_data}
Table~\ref{table:robot_data} investigates whether \method{}'s performance can be enhanced through co-training with teleoperated robot data, collected using a VR-based teleoperation framework~\cite{openteach}. We conduct this study on four tasks - \textit{put bread on plate}, \textit{fold towel}, \textit{sweep broom}, and \textit{make bottle upright}. For each task, we collect an equal number of robot demonstrations as human demonstrations, resulting in 30, 20, 20, and 30 demonstrations respectively. Interestingly, our findings reveal that for tasks involving complex motions, such as \textit{sweep broom} and \textit{make bottle upright}, policies trained solely on robot data perform poorly with the same amount of data as compared to those trained exclusively on human data. This drop in performance stems from the complex motions in these tasks making it harder to collect robot data using VR teleoperation, resulting in noisy demos. These results highlight an important consideration: humans and robots may execute the same task in different ways. Consequently, co-training with both human and robot data requires the development of algorithms capable of dealing with these differences effectively.



\begin{table}[t]
\centering
\renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1}
\caption{The effect of triangulated depth on P3PO and \method{}.}
\label{table:depth}
\begin{tabular}{lccc}
\toprule
\multicolumn{1}{c}{\textbf{Method}} &
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Put bread \\ on plate\end{tabular}}} &
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Sweep \\ broom\end{tabular}}} & 
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Put bottle \\ on rack\end{tabular}}} \\
\midrule
P3PO      & 0/20  & 0/10 & 0/30  \\
P3PO + Triangulated Depth & 17/20 & 4/10 & 23/30  \\
\method{} & \textbf{19/20} & \textbf{9/10} & \textbf{26/30}  \\
\method{} - Triangulated Depth & 0/20  & 0/10 & 0/30  \\
\bottomrule
\end{tabular}
\vspace{-1em}
\end{table}

\subsection{What design choices matter for human-to-robot learning?}
\label{subsec:design_considerations}



This section examines the impact of key design decisions on learning from human videos.

\paragraph{Depth Sensing} In \method{}, we utilize point triangulation from two camera views to obtain 3D key points, rather than relying on depth maps from the camera. We hypothesize that noisy camera depth leads to imprecise 3D key points, resulting in unreliable actions. Table~\ref{table:depth} tests this hypothesis on 4 real-world tasks by comparing the performance of P3PO and \method{} with and without triangulated depth. We observe that adding triangulated depth to P3PO improves its performance from 0\% to 72\%. Further, removing triangulated depth from \method{} reduces its performance from 90\% to 0\%. These results emphasize the importance of obtaining accurate 3D key points from human hands when learning robot policies from human videos. Appendix~\ref{appendix:depth_discrepancy} includes an illustration of imprecise actions resulting from noisy sensor depth. \looseness=-1



\begin{table}[t]
\centering
\renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1}
\caption{Importance of object point inputs for policy learning.}
\label{table:object_pts}
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\textbf{Method}} &
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Close \\ drawer\end{tabular}}} &
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Put bread \\ on plate\end{tabular}}} &
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Fold \\ towel\end{tabular}}} &
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Make bottle \\ upright\end{tabular}}} \\
\midrule
MT-$\pi$   & 2/10  & 2/20  & 0/10 & 0/20  \\
MT-$\pi$ + object points & 8/10  & 1/20  & 6/10 & 2/20  \\
\method{}  & \textbf{10/10} & \textbf{19/20} & \textbf{9/10} & \textbf{16/20}  \\
\bottomrule
\end{tabular}
\vspace{-1em}
\end{table}

\paragraph{Significance of Object Points} While \method{} uses robot and object key points as input to the policy, MT-$\pi$~\cite{motiontracks}, the best-performing baseline in Table~\ref{table:same_distribution}, only uses robot key points and obtains information about the rest of the scene through an input image. We hypothesize that using object points can improve policy learning performance, especially when there is a morphology gap between data collection and inference. Table~\ref{table:object_pts} tests this hypothesis by providing object points in addition to the robot points already passed as input into MT-$\pi$. We observe that adding object points improves the performance of MT-$\pi$ on select tasks(comprehensive results on all tasks included in Appendix~\ref{appendix:object_pts}), suggesting that including object points in the input offers a potential advantage. Nevertheless, \method{} outperforms both methods by 68\% across all tasks, emphasizing the efficacy of predicting 3D key points rather than 2D key points in image space.

