In this work, we presented \method{}, a framework that enables learning robot policies exclusively from human videos, does not require real-world online interactions, and exhibits generalization to spatial variations, new object instances, and robustness to background clutter. 

\textbf{Limitations:} We recognize a few limitations in this work: $(1)$ \method{}'s reliance on existing vision models makes it susceptible to their failures. For instance, failures in hand pose detection or point tracking under occlusion have a detrimental effect on performance. However, with continued advances in computer vision, we believe that frameworks such as \method{} will become stronger over time. $(2)$ Point-based abstractions enhance generalization capabilities, but sacrifice valuable scene context information, which is crucial for navigating through cluttered or obstacle-rich environments. Future research focusing on developing algorithms that preserve sparse contextual cues in addition to the point abstractions in \method{} might help address this. $(3)$ While all our experiments are from a fixed third-person camera view, a large portion of human task videos on the internet are from an egocentric view ~\cite{ego4d, hoi4d}. Extending \method{} to egocentric camera views can help us utilize these vast repositories of human videos readily available on the internet. 
