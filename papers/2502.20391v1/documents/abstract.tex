\begin{abstract}

Building robotic agents capable of operating across diverse environments and object types remains a significant challenge, often requiring extensive data collection. This is particularly restrictive in robotics, where each data point must be physically executed in the real world. Consequently, there is a critical need for alternative data sources for robotics and frameworks that enable learning from such data. In this work, we present \method{}, a new method for learning robot policies exclusively from offline human demonstration videos and without any teleoperation data. \method{} leverages state-of-the-art vision models and policy architectures to translate human hand poses into robot poses while capturing object states through semantically meaningful key points. This approach yields a morphology-agnostic representation that facilitates effective policy learning. Our experiments on 8 real-world tasks demonstrate an overall 75\% absolute improvement over prior works when evaluated in identical settings as training. Further, \method{} exhibits a 74\% gain across tasks for novel object instances and is robust to significant background clutter. Videos of the robot are best viewed at \website{}. 
% Videos showing the robot's performance will be made available on our website. 
% \lpnote{we should make a link to the actual website IMO.}
    
\end{abstract}

% Soft submission
% Building robotic agents capable of operating across diverse environments and object types remains a significant challenge, often requiring extensive data collection. This is particularly restrictive in robotics, where each data point must be physically executed in the real world. Consequently, there is a critical need for alternative data sources for robotics and frameworks that enable learning from such data. In this work, we present \method{}, a new method for learning robot policies exclusively from offline human demonstration videos and without any teleoperation data. \method{} leverages state-of-the-art vision models and policy architectures to translate human hand poses into robot poses while capturing object states through semantically meaningful key points. This approach yields a morphology-agnostic representation that facilitates effective policy learning. Through experiments on a diverse set of real-world tasks, we demonstrate that \method{} significantly outperforms prior methods for policy learning from human videos, performing well not only within the training distribution but also generalizing to novel object instances and cluttered environments. Videos showing the robot's performance will be made available on our website. %can be viewed at \website{}.