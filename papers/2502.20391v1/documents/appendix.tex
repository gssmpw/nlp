\SH{Add model hyperparams, table of demo numbers, check the explanation of point triangulation, how is depth normalized}

\subsection{Background}
\label{appendix:background}

\subsubsection{Semantic Correspondence}
Finding corresponding points across multiple images of the same scene is a well-established problem in computer vision~\cite{sift, zitova2003image}. Correspondence is essential for solving a range of larger challenges, including 3D reconstruction~\cite{nerf,gs}, motion tracking~\cite{cotracker,pips,pips_plus,tapir}, image registration~\cite{zitova2003image}, and object recognition~\cite{segmentanything}. In contrast, semantic correspondence focuses on matching points between a source image and an image of a different scene (e.g., identifying the left eye of a cat in relation to the left eye of a dog). Traditional correspondence methods~\cite{zitova2003image, sift} often struggle with semantic correspondence due to the substantial differences in features between the images. Recent advancements in semantic correspondence utilize deep learning and dense correspondence techniques to enhance robustness~\cite{fu2020deep, huang2022learning, asic} across variations in background, lighting, and camera perspectives. In this work, we adopt a diffusion-based point correspondence model, DIFT~\cite{dift}, to establish correspondences between a reference and an observed image, which is illustrated in Figure~\ref{fig:correspondence}.


\subsubsection{Point Tracking}
Point tracking across videos is a problem in computer vision, where a set of reference points are given in the first frame of the video, and the task is to track these points across multiple frames of the video sequence. Point tracking has proven crucial for many applications, including motion analysis~\cite{aggarwal1999human}, object tracking~\cite{yilmaz2006object}, and visual odometry~\cite{nister2004visual}. The goal is to establish reliable correspondences between points in one frame and their counterparts in subsequent frames, despite challenges such as changes in illumination, occlusions, and camera motion. While traditional point tracking methods rely on detecting local features in images, more recent advancements leverage deep learning and dense correspondence methods to improve robustness and accuracy~\cite{cotracker, pips, pips_plus}. In this work, we use Co-Tracker~\cite{cotracker} to track a set of reference points defined in the first frame of a robot's trajectory. These points tracked through the entire trajectory are then used to train generalizable robot policies for the  real world. % This can be visualized in Figure~\ref{fig:tracking}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithmic Details}
\label{appendix:alg_details}

\subsubsection{Point Triangulation}
\label{appendix:point_triangulation}

% \section{Point Triangulation from Multiple Cameras}

Point triangulation is a fundamental technique in computer vision used to reconstruct 3D points from their 2D projections in multiple images. Given \(n\) cameras with known projection matrices \(P_1, P_2, ..., P_n\) and corresponding 2D image points \(x_1, x_2, ..., x_n\), the goal is to find the 3D point \(X\) that best explains these observations.

The projection of \(X\) onto each image is given by:

\[ x_i \sim P_i X \]

where \(\sim\) denotes equality up to scale.

One common approach is the Direct Linear Transform (DLT) method:

\begin{enumerate}
    \item For each view \(i\), we can form two linear equations:
    \[ x_i(p^3_i \cdot X) - (p^1_i \cdot X) = 0 \]
    \[ y_i(p^3_i \cdot X) - (p^2_i \cdot X) = 0 \]
    where \(p^j_i\) is the \(j\)-th row of \(P_i\).
    
    \item Combining equations from all views, we get a system \(AX = 0\).
    
    \item The solution is the unit vector corresponding to the smallest singular value of \(A\), found via Singular Value Decomposition (SVD).
\end{enumerate}

For optimal triangulation, we aim to minimize the geometric reprojection error. 

\subsection{Hyperparameters}
\label{appendix:subsec:hypperparams}
The complete list of hyperparameters is provided in Table~\ref{table:hyperparams}. Details about the number of demonstrations for each task has been included in Section~\ref{subsec:task_desc}, and summarized in Table~\ref{table:num_demos}. All the models have been trained using a single NVIDIA RTX A4000 GPU.

\begin{table*}[!h]
    \caption{List of hyperparameters.}
    \label{table:hyperparams}
    \begin{center}
    \setlength{\tabcolsep}{18pt}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{ l l } 
        \toprule
        Parameter                  & Value \\
        \midrule
        Learning rate              & $1e^{-4}$\\
        Image size                 & $256\times 256$ (for BC, BC w/ Depth, MT-$\pi$)\\
        Batch size                 & 64 \\
        Optimizer                  & Adam\\
        Number of training steps   & 100000 \\
        Transformer architecture   & minGPT~\cite{minGPT} (for BC, BC w/ Depth, P3PO, \method{}) \\
                                   & Diffusion Transformer~\cite{track2act} (for MT-$\pi$) \\
        Hidden dim                 & 256\\
        Observation history length & 1 (for BC, BC w/ Depth) \\
                                   & 10 (for MT-$\pi$, P3PO, \method{}) \\
        Action head                & MLP \\
        Action chunk length        & 20\\ \bottomrule
    \end{tabular}
    \end{center}
\end{table*}

\begin{table*}[!h]
    \caption{Number of demonstrations.}
    \label{table:num_demos}
    \begin{center}
    \setlength{\tabcolsep}{18pt}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{ l c c } 
        \toprule
        Task                    & Number of object instances  & Total number of demonstrations \\
        \midrule
        Close drawer            & 1   & 20 \\
        Put bread on plate      & 1   & 30 \\
        Fold towel              & 1   & 20 \\
        Close oven              & 1   & 20 \\
        Sweep broom             & 1   & 20 \\
        Put bottle on rack      & 2   & 30 \\
        Put bowl in oven        & 1   & 20 \\
        Make bottle upright     & 2   & 30 \\
        \bottomrule
    \end{tabular}
    \end{center}
\end{table*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Task Details}
% \label{appendix:task_details}

% \subsubsection{Background Distractors}
% \label{appendix:background_distractors}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Baselines}
% \label{appendix:baselines}

\subsection{Implementation Details for MT-$\pi$}
\label{appendix:motion_tracks}
Since the official implementation of MT-$\pi$ is not yet public available, we adopt the Diffusion Transformer (DiT) based implementation of a 2D point track prediction model proposed by \citet{track2act}. We modify the architecture such that given a single image observation and robot motion tracks on the image, the model predicts future tracks of the robot points. These robot tracks are then converted to 3D using corresponding tracks for two camera views. The robot action is then computed from the 3D robot tracks using the same rigid-body geometry constraints as \method{} (described in Section~\ref{subsec:robot_action_from_points}). MT-$\pi$ proposes the use of a key point retargeting network in order to convert the human hand and robot key points to the same space. Since we already convert the human hand key points to the corresponding robot points for \method{}, we directly use these converted robot points instead of learning a separate keypoint retargeting network.

% To ensure the correctness of our implementation, we evaluate MT-$\pi$ in a setting identical to the one described in their paper. We conduct these evaluations on two tasks - \textit{put bread on plate} and \textit{make bottle upright}. For both tasks, we used 30 robot teleoperated demonstrations in addition to the human demonstrations, resulting in a total of 60 demonstrations. We observed a performance of 18/20 for the simpler \textit{put bread on plate} task and 14/20 on the harder \textit{make bottle upright} task, thus, confirming the correctness of the implementation.
To ensure the correctness of our implementation, we evaluate MT-$\pi$ in a setting identical to the one described in their paper. We conduct this evaluation on the \textit{put bread on plate} task. We use 30 robot teleoperated demonstrations in addition to the human demonstrations, resulting in a total of 60 demonstrations. We observed a performance of 18/20, thus, confirming the correctness of the implementation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiments}
\label{appendix:experiments}

\subsubsection{Illustration of Spatial Generalization and Novel Object Instances}
\label{appendix:novel_objects}

Figure~\ref{fig:spatial_gen_all} and Figure~\ref{fig:novel_objects_all} illustrate the variations in object positions and novel object instances used for each task, respectively.

\begin{figure*}[th]
\centering
\includegraphics[width=0.9\linewidth]{figures/spatial_gen_all.pdf}
% \vspace{-1em}
\caption{ Illustration of spatial variation used in our experiments.}
% \vspace{-1em}
\label{fig:spatial_gen_all}
\end{figure*}

\begin{figure*}[th]
\centering
\includegraphics[width=0.65\linewidth]{figures/novel_objects_all.pdf}
% \vspace{-1em}
\caption{Illustration of objects used in our experiments. For each task, on the left are in-domain objects while on the right are novel objects used in our generalization experiments.}
% \vspace{-1em}
\label{fig:novel_objects_all}
\end{figure*}

% \subsubsection{Illustration of Background Distractors}
% \label{appendix:background_distractors}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/depth_discrepancy.pdf}
% \vspace{-1em}
\caption{ Illustration of discrepancy in actions obtained from sensor depth and triangulated depth for the task of putting a bottle on the rack.}
% \vspace{-1em}
\label{fig:depth}
\end{figure*}

\subsubsection{Illustration of Depth Discrepancy}
\label{appendix:depth_discrepancy}
Figure~\ref{fig:depth} provides an illustration of the discrepancy in actions obtained from sensor depth and triangulated depth for the task of putting a bottle on the rack. We observe that the noise in sensor depth leads to noise in robot points which is turn results in unreliable actions.

\subsubsection{Significance of Object Points}
\label{appendix:object_pts}
Table~\ref{table:object_pts_in_domain} and Table~\ref{table:object_pts_new_objects} study the performance of MT-$\pi$ with and without object points and \method{} across all of our tasks. We observe that MT-$\pi$ with object points outperforms MT-$\pi$ on select tasks, suggesting that including object points in the input offers a potential advantage. 


\begin{table*}[t]
\centering
\caption{In-domain policy performance}
\label{table:object_pts_in_domain}
\renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1}
\begin{tabular}{lcccccccc}
\toprule
\multicolumn{1}{c}{\textbf{Method}} & \textbf{\begin{tabular}[c]{@{}c@{}}Close\\ drawer\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bread \\ on plate\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Fold\\ towel\end{tabular}}& \textbf{\begin{tabular}[c]{@{}c@{}}Close\\ oven\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Sweep\\ broom\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bottle\\ on rack\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bowl\\ in oven\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Make bottle\\ upright\end{tabular}} \\ \bottomrule
MT-$\pi$~\cite{motiontracks}      & 2/10 & 2/20 & 0/10 & 4/10 & 0/10 & 8/30 & 0/10 & 0/20 \\
MT-$\pi$ + object points & 1/20 & 6/10 & 1/20 & 4/10 & 0/10 & 0/10 & 2/20 & 8/10 \\
\hdashline\noalign{\vskip 0.5ex}
\method{} \textbf{(Ours)}         & \textbf{10/10} & \textbf{19/20} & \textbf{9/10} & \textbf{9/10} & \textbf{9/10} & \textbf{26/30} & \textbf{8/10} & \textbf{16/20} \\ \bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t]
\centering
\renewcommand{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1}
\caption{Policy performance on novel object instances}
\label{table:object_pts_new_objects}
\begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{\textbf{Method}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bread \\ on plate\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Fold\\ towel\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Sweep\\ broom\end{tabular}} &  \textbf{\begin{tabular}[c]{@{}c@{}}Put bottle\\ on rack\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Put bowl\\ in oven\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Make bottle\\ upright\end{tabular}} \\ \midrule
MT-$\pi$~\cite{motiontracks} & 1/20 & 0/20 & 0/10 & 0/30 & 0/10 & 0/20 \\
MT-$\pi$ + object points & 2/20 & 0/20 & 0/20 & 1/10 & 0/10 & 1/20 \\
\hdashline\noalign{\vskip 0.5ex}
\method{} \textbf{(Ours)}    & \textbf{18/20} & \textbf{15/20} & \textbf{4/10} & \textbf{27/30} & \textbf{9/10} & \textbf{9/20} \\ \bottomrule
\end{tabular}
% \vspace{-1em}
\end{table*}
