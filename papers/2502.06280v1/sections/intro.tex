\section{Introduction}

Semi-supervised node classification is a fundamental task in graph machine learning, holding significant relevance in various real-world applications, such as fraud detection \cite{li2022devil, dou2020enhancing}, and recommendation \cite{zhao2023sequential,zhao2024collaborative} to name some. With the rapid development of deep learning, Graph Neural Networks (GNNs) have been widely used in dealing with non-Euclidean graph-structured data and have achieved considerable progress \cite{zhang2024graph2,luo2024classic,li2024gslb,zhang2025survey,li2023survey,li2024glbench,li2024zerog}. However, their effectiveness is often jeopardized under class-imbalanced training datasets. In these scenarios, GNNs are prone to be biased toward majority classes, leading to low test accuracy on minority classes.


To tackle the class-imbalanced issues in deep learning, various Class Imbalanced Learning (CIL) methods have been proposed in fields like computer vision and natural language processing \cite{lin2017focal,cui2019class,kang2019decoupling}. However, these methods are hard to be directly applied to graph-structured data because of the non-iid characteristics of graphs \cite{song2022tam}. Recently, close to the heels of the rapid development of GNNs in node classification, various Class Imbalanced Graph Learning (CIGL) approaches have been proposed \cite{park2021graphens, song2022tam, zhao2021graphsmote, shi2020multi}, most of them attempt to utilize data augmentation techniques to generate virtual minority nodes for balancing the training process \cite{park2021graphens, zhao2021graphsmote, qu2021imgagn}. The other line of approaches aims to facilitate CIGL through the graph structure. More precisely, they adjust margins node-wisely according to the extent of deviation from connectivity patterns or augment structures to alleviate ambivalent and distant message passing. Nevertheless, most of the existing methods treat the CIGL task as supervised learning, overlooking the large amount of unlabeled data in the graph. For example, in the Cora dataset, if there are 20 labels per majority class, and the step imbalance ratio equals 10 (which means there are only 2 labels per minority class), labeled nodes in the majority classes for only 4.4\% of all nodes, while in the minority classes, it is even lower at just 0.6\%. Therefore, we naturally raise a question:
\emph{"Can we explicitly utilize these unlabeled nodes to assist with CIGL?"}

Self-training is one of the most promising Semi-Supervised Learning (SSL) paradigms for bypassing the labeling cost by leveraging abundant unlabeled data. It typically selects a subset of predictions from the model itself with a higher confidence of correctness as pseudo labels to add to the training set and repeats this process iteratively. However, traditional self-training methods are based on a basic assumption that the class distribution of the training set is balanced, and the class imbalanced issue can be more problematic for self-training algorithms. We first analyze this phenomenon from the quantity and confidence of pseudo labels in a class imbalanced training dataset (as shown in \figref{fig:matthew} Left). We can observe that: (1) Due to the abundance of training labels for majority classes, the model is prone to be biased toward majority classes, resulting in a much higher amount of majority class pseudo labels compared to minority classes. (2) Because of the imbalance in the model training process, the model tends to be more confident in its predictions for majority classes, and vice versa. If we directly use self-training algorithms on class imbalanced training sets, the predefined threshold will filter out most pseudo labels from minority classes, resulting in an increasingly imbalanced training set. Therefore, with the increase in stages, the model's performance will deteriorate further (as shown in \figref{fig:matthew} Right), and we refer to this phenomenon as the \emph{Matthew Effect} ("the rich get richer and the poor get poorer").

Although some previous works have attempted to use self-training to aid with learning in CIGL \cite{yan2023unreal,zhou2023graphsr,zhang2024bim}, they only use pseudo labels to fill minority classes and do not fully leverage unlabeled nodes. Additionally, their methods are all based on the multi-stage framework, which requires multiple rounds of teacher-student distillation, and this will significantly reduce the efficiency of the model. So we aim to design an approach within the single-stage self-training framework, but we still face several challenges. First, since the class distribution between the labeled and unlabeled set is inconsistent (as shown in \figref{fig:distribution} in \appref{sec:app}), even if the supervised training on the labeled set is balanced, the pseudo labels generated by the model will still be imbalanced, which will also lead to the \emph{Matthew Effect}. Second, because the ground-truth labels of unlabeled set are unavailable, the class distribution of unlabeled set is unknown, so we are hard to conduct CIGL on the unlabeled set. Surprisingly, we found that the pseudo labels generated by the model can serve as a good estimation of unlabeled set class distribution, enabling us to perform CIGL on the pseudo label set. Consequently, we propose a simple-yet-effective self-training method, \texttt{Double Balancing (DB)}, which only requires a few lines of code to the existing pipelines and can significantly improve the model's performance in CIGL with almost no additional training overhead. Specifically, we first use the model to predict pseudo labels for the unlabeled nodes, then use them to estimate the class distribution of the unlabeled set, and finally apply a simple balanced loss function to mitigate the imbalance. Due to the potential presence of incorrect predictions in the pseudo labels, which may lead to \emph{Confirmation Bias} \cite{arazo2020pseudo}, we propose \texttt{Noise-Tolerant Double Balancing} to further enhance performance. 

Additionally, most previous CIGL baselines have only focused on the balance between majority and minority classes, without addressing the potential few-shot problem in CIGL. Therefore, we revisit the model architecture from the perspective of message-passing. Though message-passing is a key factor of GNNs for capturing structural knowledge, such a coupled design may in turn bring severe challenges for GNNs when learning with scarce labels \cite{liu2023learning}. Because of the scarcity of labeled nodes in minority classes, limited propagation steps make it difficult for the supervision signals to cover unlabeled nodes. However, simply increasing the model depth will result in the over-smoothing problem, which we refer to as \emph{Propagation Dilemma}. According to previous literature \cite{zhang2022model}, the major cause for the performance degradation of deep GNNs is the model degradation issues caused by a large number of transformations, rather than a large number of propagation. We decouple GNNs and increase the propagation hops, interestingly, we find that increasing the number of propagation hops can effectively enhance CIGL performance, even surpassing some existing specific CIGL baselines. In summary, we claim that the advantages of decouple GNNs in CIGL: (1) By increasing the number of propagation hops, we can transmit the supervision signals further, capturing higher-order structural knowledge, and thereby alleviating the few-shot problem. (2) Since the feature propagation process can be pre-computed and does not participate in model training, the efficiency of the model will be significantly enhanced. (3) Because the feature propagation can be considered as unsupervised representation learning, the noise introduced by incorrect labels will not backpropagate to the node features \cite{xue2022investigating,ding2024divide}, which will be more beneficial for self-training. 

Combining all the above designs, we propose \texttt{IceBerg}, a simple-yet-effective approach for class-imbalanced and few-shot node classification. Our contributions can be listed as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Preliminary Analysis.} We believe that supervision signals are just the tip of the iceberg. By effectively leveraging the large number of unlabeled nodes in the graph, we can easily and significantly enhance the model's performance.
    \item \textbf{Model Design.} Based on our preliminary analysis, we propose \texttt{IceBerg}, a simple-yet-effective approach. It can also be flexibly combined with other baselines as a plug-and-play module.
    \item \textbf{Experimental Evaluation.} Systematic and extensive experiments demonstrate that \texttt{IceBerg} achieves superior performance across various datasets and experimental settings in CIGL. Additionally, in light of the strong few-shot ability of \texttt{IceBerg}, it can also obtain state-of-the-art performance in few-shot scenarios.
    \item \textbf{Benchmark Development.} We integrate diverse backbones, datasets, baselines, and experimental settings in our repository. Researchers can evaluate all combinations with less effort.
\end{itemize}



\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{pic/matthew.pdf}
\caption{\emph{Left}: Confidence of pseudo labels in the long-tailed node classification task. The first four classes are majority classes, and the latter three are minority classes. \emph{Right}: Matthew Effect of standard multi-stage self-training on long-tailed graph datasets.}
\label{fig:matthew}
\end{figure}
