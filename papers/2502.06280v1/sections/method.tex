\section{Methodology}

In this section, we will give a detailed description of \texttt{IceBerg}. We first introduce our debiased self-training method, \texttt{Double Balancing}, in \secref{sec:double}. And then, we revisit model architecture from message-passing in \secref{sec:propagation}. Finally, we introduce the overall framework of \texttt{IceBerg} and explain the advantages of combining the two modules mentioned above.


\subsection{Double Balancing}
\label{sec:double}

We start with the formulation of the self-training methods by analyzing the corresponding loss function. Many existing self-training methods seek to minimize a supervised classification loss on labeled data and an unsupervised loss on unlabeled data. Formally, the objective function is given as follows:
\begin{equation}
\label{equ:ssl}
\begin{aligned}
    \mathop{\min}\limits_{\theta\in\Theta}\mathcal{L}_{ssl}=&\begin{matrix}\underbrace{\mathbb{E}_{v_i\in\mathcal{V}_L}\ell(f_\theta(v_i), y_i)} \\ \text{supervised}\end{matrix} \\
    &+\begin{matrix}\underbrace{\lambda\cdot\mathbb{E}_{v_j\in\mathcal{V}_U}\mathbb{I}(\max(f_\theta(v_j))\geq\tau)\ell(f_\theta(v_j), \hat{y}_j)}\\ \text{unsupervised}\end{matrix},
\end{aligned}
\end{equation}
where $\ell$ is Cross-Entropy (CE) loss, $\lambda$ is a trade-off hyper-parameter to balance supervised and unsupervised loss, $\mathbb{I(\cdot)}$ is an indicator function, $\tau$ is the confidence threshold, and $\hat{y}$ is the prediction generated by model $f_\theta$. If we analyze CIGL from the perspective of domain adaptation \cite{jamal2020rethinking}, we can treat the labeled and unlabeled set as the source domain and the test set as the target domain, therefore we can rewrite \equref{equ:ssl} as follows to estimate the test error:
\begin{equation}
\label{equ:domain}
\begin{aligned}
    \text{error}&=\mathbb{E}\ell(f_\theta(v_i),y_i)\frac{p_t(\mathbf{x},y)}{p_l(\mathbf{x},y)} + \lambda\cdot\mathbb{E}\ell(f_\theta(v_j),\hat{y}_j)\frac{p_t(\mathbf{x},y)}{p_u(\mathbf{x},y)} \\
    &=\mathbb{E}\ell(f_\theta(v_i),y_i)\frac{p_t(y)p_t(\mathbf{x}|y)}{p_l(y)p_l(\mathbf{x}|y)}+\lambda\cdot\mathbb{E}\ell(f_\theta(v_j),\hat{y}_j)\frac{p_t(y)p_t(\mathbf{x}|y)}{p_u(y)p_u(\mathbf{x}|y)}
\end{aligned}
\end{equation}
where $p_l(\mathbf{x},y)$, $p_u(\mathbf{x},y)$, and $p_t(\mathbf{x},y)$ represents data distribution of labeled set, unlabeled set, and test set respectively. $p(y)$ and $p(x|y)$ are class distribution and class conditional distribution. For simplicity, we omit the subscripts of expectation. We assume the class conditional distribution of labeled, unlabeled, and test sets are consistent here, namely $p_l(\mathbf{x}|y)=p_u(\mathbf{x}|y)=p_t(\mathbf{x}|y)$ (actually this assumption does not always hold, and we will explain it in the next section). Since the target test class distribution is balanced, and the source labeled class distribution is imbalanced, we can consider CIGL as a label distribution shift problem \cite{hong2021disentangling,garg2020unified}, where $p_l(y)\neq p_t(y)$. Existing CIGL methods aim to make $p_l(y)$ close to $p_t(y)$ using the known class distribution, in order to force the model becomes unbiased. 

However, if we analyze the unsupervised term, we will find that the aforementioned approach does not work because $p_u(y)$ is unknown. Furthermore, by observing the pseudo labels predicted by the model on class imbalanced datasets, we can find that: (1) because the model is biased toward majority classes, it is prone to generate majority pseudo labels. (2) Since the decision boundary is far from the majority classes, resulting in higher confidence for the majority classes (as shown in \figref{fig:matthew} Left). This means that if we do not conduct operations on the unsupervised term, traditional self-training algorithms will become increasingly imbalanced, leading to the emergence of the \emph{Matthew Effect} (as shown in \figref{fig:matthew} Right). So if the model training is balanced, \emph{i.e.} we assume that the model has consistent pseudo labeling ability across all classes, can we solve this problem? Unfortunately, the answer is negative. As shown in \figref{fig:distribution}, in real-world scenarios, the costs of data collection and labeling vary across different classes, and the unlabeled data may not be balance distributed. Additionally, the class distribution between labeled and unlabeled sets could be inconsistent, which we refer to as label \emph{Missing Not At Random}. Therefore, even if the model has consistent pseudo labeling capability for each class, the generated pseudo labels may still be imbalanced.

According to the above analysis, we attempt to conduct \texttt{Double Balancing} for the unsupervised term. Surprisingly, due to the high accuracy of pseudo labels, they can serve as a good estimate of pseudo class distribution. First, we utilize the model to generate pseudo labels for unlabeled nodes, and we count the number of pseudo labels across all classes:
\begin{gather}
    \tau^\prime=\frac{1}{|\mathcal{V}_U|}\sum_j^{|\mathcal{V}_U|}\max(f_\theta(v_j)), \\
    \pi_c=\sum_j^{|\mathcal{V}_U|}\mathbbm{1}(\mathbb{I}(\max(f_\theta(v_j))\geq\tau^\prime)), 
\end{gather}
Since setting a threshold $\tau$ requires extensive experience and suitable thresholds may vary across datasets, we use a dynamic threshold here. With the help of the estimated class distribution of pseudo labels, we are able to balance the unsupervised loss. We utilize balanced softmax \cite{ren2020balanced} here for three reasons: (1) balanced softmax can ensure Fisher consistency and achieves excellent performance in CIGL; (2) when the number of pseudo labels for some classes is zero, it remains unaffected; (3) although the current state-of-the-art performance is based on re-sampling \cite{park2021graphens,li2023graphsha}, the large number of pseudo labels means that adding too many virtual ego-networks, which could potentially destroy the original graph structure. Finally, the double balancing of unsupervised loss is formulated as follows:
\begin{gather}
    \mathcal{L}_{unsup}=-\lambda\cdot\mathbb{E}_{v_j\in\mathcal{V}_U}\mathbb{I}(\max(f_\theta(v_j))\geq\tau^\prime)\ell(\mathbf{q}_j, \hat{y}_j), \\
    \mathbf{q}_j[c] = f_\theta(v_j)[c]+\mu\cdot\log\pi_{c}
\end{gather}
where $\mathbf{q}$ is the adjusted logits, and $\mu$ is a scaling parameter that affects the intensity of adjustment. The PyTorch-style pseudocode is presented in \aloref{algo}. We can find that just a few lines of code can significantly improve the performance of CIGL with almost no additional training overhead. 

Although the performance of \texttt{Double Balancing} is already excellent, potential noisy labels within the pseudo labels could harm model performance, leading to confirmation bias. To further improve results, we propose \texttt{Noise-Tolerant Double Balancing}. Inspired by \citet{wang2019symmetric}, we introduce a symmetric term to facilitate noise robustness:
\begin{equation}
    \mathcal{L}_{unsup}=-\lambda\cdot\mathbb{E}_{v_j}\mathbb{I}(\max(f_\theta(v_j))\geq\tau^\prime)(\ell(\mathbf{q}_j,\hat{y}_j)+\beta\cdot\ell(\hat{\mathbf{y}}_j, \max(\mathbf{q}_j))),
\end{equation}
where $\hat{\mathbf{y}}$ is the one-hot vector of pseudo label, $\beta$ is a trade-off hyper-parameter. We simplify the subscript of expectation here.

 
\subsection{Propagation then Tramsformation}
\label{sec:propagation}

While \texttt{DB} can leverage pseudo labels to alleviate overfitting in minority classes and effectively address label distribution shift, its effectiveness may still be suboptimal in cases where the number of labels is extremely limited. Recall \equref{equ:domain}, we analyze CIGL from the perspective of domain adaptation and assume the class conditional distributions of labeled, unlabeled, and test are consistent. However, when the imbalance ratio is large, and the number of labeled nodes in minority classes is extremely limited, due to selection bias \cite{xu2022alleviating}, the distribution differences between the labeled set and the other two sets will increase, causing the distribution of unlabeled set to shift as well \cite{liu2022confidence,wang2024distribution}. Therefore, the assumption of consistent class conditional distribution does not hold, \emph{i.e.} $p_l(\mathbf{x}|y)\neq p_u(\mathbf{x}|y)\neq p_t(\mathbf{x}|y)$.

We conduct a toy experiment on the two-moon dataset to explain the selection bias problem in an extremely limited labeled set (as shown in \figref{fig:twomoon}). Due to the large number of labeled samples in the majority class, they uniformly distribute on the half-moon distribution and can effectively capture the ground-truth distribution. While the minority class only possesses two labeled samples, which makes it hard to cover the ground-truth distribution. Despite we use self-training to generate several pseudo labels for the minority class, they are all concentrated around the labeled samples and cannot expand their distribution. Even if the pseudo labels are all correct and the class distribution is balanced, the minority class still exhibits poorer performance compared to the majority class.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{pic/twomoon.pdf}
\caption{Toy example on the two-moon dataset. We utilize a simple two-layer fully connected MLP for classification.}
\label{fig:twomoon}
\end{figure}

In order to figure out the selection bias in heavily imbalanced scenarios, we revisit GNNs' architecture from message-passing. Most current CIGL work is conducted on models within \texttt{PTPT} framework, like GCN, GAT, and GraphSAGE. However, because of the over-smoothing issue, this kind of model makes it hard to deepen the layers, which means labeled nodes cannot propagate supervision signals to nodes at larger propagation hops. In this work, we refer to this issue as the \emph{Propagation Dilemma}. According to recent literature, we know that the main reason for performance degradation with increased depth may lie in the \texttt{T} operation rather than \texttt{P} operation \cite{zhang2022model,wang2024snowflake}. Consequently, we attempt to use \texttt{PPTT} architecture in the CIGL task. In light of the strong few-shot learning ability of D$^2$PT \cite{liu2023learning}, we leverage graph diffusion-based propagation here:
\begin{gather}
    \mathbf{X}^{(t+1)} = (1-\alpha)\tilde{\mathbf{A}}\mathbf{X}^{(t)} + \alpha\mathbf{X}, \\
    \hat{\mathbf{Y}}=\texttt{Softmax}(\texttt{MLP}(\mathbf{X}^{(T)}))
\end{gather}
where $\mathbf{X}^{(0)}=\mathbf{X}$, $\alpha\in(0,1]$ is the restart probability, $T$ is the number of propagation steps, and \texttt{MLP} is a simple fully-connected multilayer perceptron. By increasing the number of propagation steps, we found that even without using any CIGL techniques, performance still improves with the increase in propagation hops. (as shown in \tabref{tab:prop}). This interesting experimental result supports that few-shot is a significant challenge in heavily class-imbalanced scenarios.

By combining all designs mentioned above, we propose our simple-yet-effective method, \texttt{IceBerg}, which can achieve state-of-the-art performance in class-imbalanced and few-shot scenarios. To illustrate that these modules we proposed are not simply additions, we list several advantages of \texttt{IceBerg} below:
\begin{itemize}[leftmargin=*]
    \item To propagate supervision signals to every node as much as possible, we not only use pseudo labels to increase the sources of propagation but also expand the range by increasing the number of propagation hops.
    \item Since parameter-free propagation can be viewed as unsupervised node representation learning, it is more robust to noisy labels and better suited for the self-training framework.
    \item While using a large number of pseudo labels may increase some gradient backpropagation overhead, by decoupling \texttt{P} and \texttt{T}, we can precompute node features and only need to optimize the \texttt{MLP}, which significantly reducing training costs.
\end{itemize}


\begin{algorithm}[t]
\SetAlgoLined
\small
    \PyComment{model: graph neural networks; get\_confidence: the function to get confidence and predictions; M: existing state-of-the-art baselines; BS: balanced softmax function; lambda: trade-off parameter.} \\
    \PyComment{Generate pseudo labels} \\
        \PyCode{with torch.no\_grad():} \\ 
        \Indp
        \PyCode{model.eval()} \\
        \PyCode{logits = model(x, edge\_index)} \\
        \Indm
    \PyCode{confidence, pred\_label = get\_confidence(logits)} \\
    \PyComment{Dynamic Threshold} \\
    \PyCode{t = confidence[unlabel\_mask].mean().item()} \\
    \PyCode{pseudo\_mask = confidence.ge(t) \& unlabel\_mask} \\
    \PyComment{Pseudo Label Distribution Estimation} \\
    \PyCode{num\_list\_p = [(pred\_label[pseudo\_mask] == i).sum().item() for i in range(num\_cls)]} \\
    \PyComment{Existing CIGL Methods} \\
    \PyCode{model.train()} \\
    \PyCode{optimizer.zero\_grad()} \\
    \PyCode{logits, loss = M(x, edge\_index, model, train\_mask)} \\
    \PyComment{Double Balancing (Ours)} \\
    \PyCode{loss += BS(logits[pseudo\_mask], pred\_label[pseudo\_mask], num\_list\_u)} * lambda \\
    \PyComment{Backward Supervised and Unsupervised Loss} \\
    \PyCode{loss.backward()} \\
    \PyCode{optimizer.step()} \\
\caption{PyTorch-style pseudocode for \texttt{D-Balancing}}
\label{algo}
\end{algorithm}



% \begin{algorithm}[t]
% \SetAlgoLined
% \small
%     \PyComment{MLP: multilayer perceptron; get\_confidence: the function to get confidence and predictions; M: existing state-of-the-art baselines; BS: balanced softmax function; lambda: trade-off parameter; E: Training epochs; T: Propagation hops; alpha: restart probability; Diff: diffusion-based propagation.} \\
%     \PyComment{Pre-compute node representations} \\
%     \PyCode{prop\_feat = Diff(x, edge\_index, T, alpha)} \\
%     \PyCode{for \_ in range(E):} \\
%     \Indp
%     \PyComment{Generate pseudo labels} \\
%     \PyCode{with torch.no\_grad():} \\ 
%     \Indp
%     \PyCode{MLP.eval()} \\
%     \PyCode{logits = MLP(prop\_feat)} \\
%     \Indm
%     \PyCode{confidence, pred\_label = get\_confidence(logits)} \\
%     \PyComment{Dynamic Threshold} \\
%     \PyCode{t = confidence[unlabel\_mask].mean().item()} \\
%     \PyCode{pseudo\_mask = confidence.ge(t) \& unlabel\_mask} \\
%     \PyComment{Pseudo Label Distribution Estimation} \\
%     \PyCode{num\_list\_p = [(pred\_label[pseudo\_mask] == i).sum().item() for i in range(num\_cls)]} \\
%     \PyComment{Existing CIGL Methods} \\
%     \PyCode{MLP.train()} \\
%     \PyCode{optimizer.zero\_grad()} \\
%     \PyCode{logits, loss = M(prop\_feat, MLP, train\_mask)} \\
%     \PyComment{Double Balancing (Ours)} \\
%     \PyCode{loss += BS(logits[pseudo\_mask], pred\_label[pseudo\_mask], num\_list\_u)} * lambda \\
%     \PyComment{Backward Supervised and Unsupervised Loss} \\
%     \PyCode{loss.backward()} \\
%     \PyCode{optimizer.step()} \\
% \caption{PyTorch-style pseudocode for \texttt{IceBerg}}
% \label{algo}
% \end{algorithm}


% \begin{algorithm}[t]
% 	\caption{IceBerg: A Simple-yet-Effective Approach to Class Imbalanced Semi-Supervised Node Classification;}
%         \label{fairgb}
% 	\LinesNumbered
%         \small
% 	\KwIn{An attributed graph: $\mathcal{G}=(\mathcal{V},\mathbf{A},\mathbf{X})$; hyper-parameters: $T, \alpha, E, \lambda$; multilayer perceptron: $f_\theta$.}
% 	\KwOut{Final predictions $\mathbf{\hat{Y}}$ and optimized model $f_{\hat{\theta}}$.}
%     \For{$t=1, \ldots, T$}{
%         $\mathbf{X}^{(t+1)}\leftarrow$ Graph diffusion-based propagation by Equation 10\;
%     }
%     \For{$e=1, \ldots, E$}{
%             Compute confidence and pseudo labels for all unlabeled nodes\;
%             Compute dynamic threshold by Equation 5\;
%             Compute class distribution of pseudo labels by Equation 6\;
%             $\mathcal{L}_{sup}\leftarrow$ Utilize the existing GCIL approach to obtain supervised loss\;
%             $\mathcal{L}_{unsup}\leftarrow$ Utilize noise-tolerant double balancing (Equation 9) to obtain unsupervised loss\;
%             $\mathcal{L}_{ssl}\leftarrow\mathcal{L}_{sup}+\lambda\cdot\mathcal{L}_{unsup}$: Get self-training loss\;
%             $f_{\hat{\theta}}\leftarrow$ Back-propagation to update parameters\;
%         }
% \end{algorithm}



\begin{table}[]
\caption{In imbalanced training sets, the model's performance (Balanced Accuracy) \emph{w.r.t.} the number of propagation hops.}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|ccccccc}
\Xhline{1.2pt}
\textbf{Hop}          & 2 & 4 & 6 & 8 & 10 & 12 & 14 \\ \hline
Cora (IR=10) & 61.03  & 65.83  & 67.00  & 67.83  & 67.29   & 68.00 & 67.94  \\ \hline
Cora (IR=20) & 53.67  & 56.28  & 58.35  & 59.06  & 59.86   & 59.36 & 59.54   \\ \hline
CiteSeer (IR=10) & 44.02  & 46.91  & 49.16  & 49.41  & 49.52   & 49.81  & 49.90 \\ \hline
CiteSeer (IR=20) & 39.21  & 39.93  & 41.24  & 42.90  & 42.85   & 44.36  & 42.30 \\ 
\Xhline{1.2pt}
\end{tabular}}
\label{tab:prop}
\end{table}