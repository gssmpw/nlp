\section{Preliminary}

\paratitle{Notations.} To maintain consistency of notations, we use bold uppercase and lowercase letters to represent matrices and vectors, and calligraphic font types to denote sets. Given an attributed graph denoted as $\mathcal{G}=(\mathcal{V},\mathbf{A}, \mathbf{X})$, where $\mathcal{V}=\{v_1,v_2,\ldots,v_N\}$ is the set of $N$ nodes; we denote the adjacency matrix as $\mathbf{A}\in\mathbb{R}^{N\times N}$, if $v_i$ and $v_j$ are connected, $\mathbf{A}_{ij}=1$, otherwise $\mathbf{A}_{ij}=0$; $\mathbf{X}=[\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_N]\in\mathbb{R}^{N\times D}$ is the node feature matrix, each node $v_i$ is associated with a $D$-dimensional node feature vector $\mathbf{x}_i$. The normalized adjacency matrix is represented by $\tilde{\mathbf{A}}=\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$, where $\mathbf{D}\in\mathbb{R}^{N\times N}$ is a diagonal degree matrix $\mathbf{D}_{ii}=\sum_j\mathbf{A}_{ij}$.

\paratitle{Graph Neural Networks.} Following the diagram of Message-Passing Neural Networks (MPNNs), most forward processes of MPNNs can be defined as:
\begin{gather}
    \mathbf{m}_i^{(l-1)}=\texttt{PROPAGATE}\big(\big\{\mathbf{h}_i^{(l-1)},\mathbf{h}_j^{(l-1)}|j\in\mathcal{N}(i)\big\}\big),\\
    \mathbf{h}_i^{(l)}=\texttt{TRANSFORM}\big(\mathbf{m}_i^{(l-1)}\big),
\end{gather}
where $\mathbf{h}_i^{(l)}$ is the feature vector of node $v_i$ in the $l$-th layer and $\mathbf{m}_i^{(l-1)}$ is the aggregated message vector from the $(l-1)$-th layer, $\mathcal{N}(i)$ is a set of neighbor nodes of node $v_i$. $\texttt{PROPAGATE}$ (\texttt{P}) denotes the message-passing function of aggregating neighbor information, and $\texttt{TRANSFORM}$ (\texttt{T}) denotes the non-linear mapping with node features as input. 
% By stacking multiple $\texttt{T}$ and $\texttt{P}$, GNNs can aggregate messages from neighbors. According to the combination of these two components, MPNNs can be roughly divided into two categories: coupled and decoupled GNNs. Coupled GNNs alternate between $\texttt{AGG}$ and $\texttt{UPDATE}$ at each layer, whereas decoupled GNNs first perform multiple rounds of $\texttt{AGG}$ to obtain fused features, and then use $\texttt{UPDATE}$ to conduct prediction.
According to the ording the model arrages the \texttt{P} and \texttt{T} operations, we can roughly classify the existing GNN architectures into three categories: \texttt{PTPT}, \texttt{PPTT}, and \texttt{TTPP} \cite{zhang2022model}. Typical GNNs, such as GCN, GAT, and GraphSAGE, entangle \texttt{P} and \texttt{T} in each layer, so they can be classified as \texttt{PTPT}. \texttt{PPTT} architecture disentangles \texttt{P} and \texttt{T}, and stacks multiple \texttt{P} in preprocessing. While \texttt{TTPP} also disentangles two operations, but embed node features first by \texttt{T} and then the stacked \texttt{P} can be considered as label propagation.

\paratitle{Long-Tailed Semi-supervised Node Classification.} In this task, we have a labeled node set $\mathcal{V}_L\subset\mathcal{V}$ and unlabeled node set $\mathcal{V}_U=\mathcal{V}\setminus\mathcal{V}_L$. The target of the node classification task is to train a model $f_\theta$ based on labeled nodes $\mathcal{V}_L$ to predict the classes of unlabeled nodes $\mathcal{V}_U$. For the labeled node $v_i$, it is associated with a ground-truth label $\mathbf{y}_i\in\{0,1\}^C$, where $C$ is the number of classes. Let $N_L^c$ denote the number of samples for class $c$ in the labeled dataset, and $N_U^c$ denote the number of samples for class $c$ in the unlabeled dataset. We have $N_L^1\geq N_L^2\geq\ldots N_L^C$, and the imbalance ratio of the labeled dataset is denoted by $\gamma_L=\frac{N_L^1}{N_L^C}$. Similarly, the imbalance ratio of the unlabeled dataset is $\gamma_U=\frac{\max_c N_U^c}{\min_c N_U^c}$.