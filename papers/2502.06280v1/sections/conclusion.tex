\section{Conclusion}
\label{sec:conclusion}

In this paper, we provide a key statement that the labeled nodes in the graph are just the tip of the iceberg, and if we could effectively utilize a large number of unlabeled nodes, we can significantly and easily achieve state-of-the-art performance. We first study the Matthew Effect in the self-training framework, and based on theoretical analysis, we propose \texttt{Double Balancing}. To avoid Confirmation Bias, we propose Noise-Tolerant Double Balancing. Additionally, in the heavy class-imbalanced scenarios, minority classes may also face few-shot issues. Therefore, we disentangle the propagation and transformation operations to augmente supervision signals to distant nodes. Combining all the above designs, we propose \texttt{IceBerg}, a simple-yet-effective approach to class-imbalanced and few-shot node classification. It can achieve excellent performance with good efficiency on various benchmark datasets. At last, we suggest that future research works pay more attention to the large number of unlabeled nodes present in the graph, rather than just treat CIGL or FSGL tasks as supervised learning tasks.