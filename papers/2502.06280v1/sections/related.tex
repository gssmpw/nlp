\section{Related Work}
\label{sec:related}


This work is related to two research fields: Class-Imbalanced Graph Learning (CIGL) and Few-Shot Graph Learning (FSGL). 
\subsection{Class-Imbalanced Graph Learning}
We will first introduce the related work in CIGL. Due to the GNNs inheriting the character of deep neural networks, GNNs perform with biases toward majority classes when training on imbalanced datasets. To overcome this challenge, CIGL has emerged as a promising solution that combines the strengths of graph representation learning and class-imbalanced learning. A great branch of this field is oversampling minority nodes by data augmentation to balance the skew training label distribution. GraphSMOTE \cite{zhao2021graphsmote} leverages representative data augmentation method (\emph{i.e.}, SMOTE) and proposes edge predictor to fuse augmented nodes into the original graph. GraphENS \cite{park2021graphens} discovers neighbor memorization phenomenon in imbalanced node classification, and generates minority nodes by synthesizing ego-networks according to similarity. GraphSHA \cite{li2023graphsha} only synthesizes harder training samples and blocks message-passing from minority nodes to neighbor classes by generating connected edges from 1-hop subgraphs. Apart from that, some methods aim to facilitate CIGL through the graph structure. TAM \cite{song2022tam} adjusts margins node-wisely according to the extent of deviation from connectivity patterns. BAT \cite{liu2023topological} is a data augmentation approach, which alleviates ambivalent and distant message-passing in imbalanced node classification. Since the numerous real-world applications of CIGL, its techniques have been applied to many other tasks as well, \emph{e.g.}, graph anomaly detection \cite{zhou2023improving,ma2024graph}, graph fairness learning \cite{li2024rethinking}, and graph data pruning \cite{zhang2024gder}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{pic/visualize.pdf}
\caption{Visualization of node representations.}
\label{fig:visualization}
\end{figure}

\subsection{Few-Shot Graph Learning}
Besides, modern artificial intelligence is heavily dependent on a large number of high quality labels. Considering complexity and heterogeneity of graph-structured data, human labeling is unbearably laborious. Therefore, there are some works that aim to figure out the few-shot issues in graph machine learning. DAGNN \cite{liu2020towards} and D2PT \cite{liu2023learning} disentangle propagation and transformation to transmit the supervision signals to more distant nodes. Additionally, D2PT utilizes dual-channel contrastive learning to enhance its capability of capturing unsupervised knowledge. And self-training is also a promising technology to alleviate label scarcity with the help of pseudo labels. \citet{liu2022confidence} find that high-confidence pseudo labels may introduce distribution shift, so they reweigh the loss function by information gain. M3S \cite{sun2020multi} leverages the DeepClustering technique to refine the self-training process. Meta-PN \cite{ding2022meta} utilizes meta-learning label propagation to construct a pseudo label set and decouple the model architecture to allow larger receptive fields.

Although existing literature have achieved considerable success in CIGL and FSGL, there is no work analyzes the connection between the two research fields. In this work, we first time study the Matthew effect challenge in CIGL and FSGL, and achieve the state-of-the-art performance in two fields with one unified framework.

