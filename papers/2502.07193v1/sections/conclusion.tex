\section{Conclusion and Future Work}
\label{sec:conclusion}

In this work, we present a unified framework of the RLHF pipeline from the view of contextual bandits, leveraging the Bradley-Terry preference model with a linearly parameterized reward function. We decompose the RLHF process into two stages: (post-)training and deployment, exploring both passive and active data collection strategies during the training phase. We then develop novel algorithms for each stage, applying online mirror descent with a tailored local norm to replace standard maximum likelihood estimation, and designing distinct query criteria for different stages. Our theoretical analysis shows that our method improves upon existing methods in both statistical and computational efficiency. Finally, we train and deploy \texttt{Llama-3-8B-Instruct} on the \mbox{Ultrafeedback-binarized} dataset using our approach. The empirical results validate our theoretical findings and confirm the effectiveness of our proposed method.

While our work advances the theoretical understanding of RLHF, several important directions remain for future work. First, we considered the fixed feature mapping $\phi$ for the reward model, but in practice, this mapping may evolve during the training process. Thus, analyzing this dynamic feature mapping is an important future work. Second, though we focused on the Bradley-Terry model, there are other preference models for human feedback, such as the Plackett-Luce model~\citep{59:Luce-choice,75:Plackett-permutations}. Extending our results to these preference models is another promising direction.  
