\section{Deployment Stage}
\label{sec:deploy}
In the deployment stage, the learner uses the trained model to generate responses to user inputs and simultaneously gathering informative feedback to improve the model. This contrasts with the active data collection phase during training, which is focused solely on optimizing the final policy. In deployment, however, the learner is faced with a dual objective: selecting actions that maximize rewards to ensure a positive user experience, while also choosing actions that provide informative feedback for further model improvement.

\paragraph{Performance Measure.} To this end, we consider the cumulative regret compared to the optimal policy, which is defined as 
\begin{align*}
  {\Reg}_T = \sum_{t=1}^T \left(r\left(x_t, \pi^*(x_t)\right)- \frac{r\left(x_t, a_t\right) + r\left(x_t, a'_t\right)}{2}\right),
\end{align*}
where $\pi^*$ is the optimal policy, $a_t$ and $a'_t$ are the actions selected by the learner at round $t$. This measure is stronger than the measure defined in~\citet{arXiv'24:Ji-RLHF-active}, which considers the gap between the optimal action and a single selected action. In the deployment phase, our proposed measure is more appropriate, as both actions must be sufficiently effective to ensure a positive user experience.

While our measure is more suitable for deployment, it also introduces significant challenges. To optimize the measure of \citet{arXiv'24:Ji-RLHF-active}, the learner can exploit one action with high reward for exploitation while using the other action for exploration. In contrast, our setting requires both actions to be good enough. To address this, we propose a novel algorithm that balances exploration and exploitation for deployment. At a high level, our algorithm consists of two key components: estimator construction and query selection. We describe each component in detail below.

\paragraph{Estimator Construction.} The estimator construction is the same as the process used in the training stage. For each round $t\in[T]$, given $\left\{\left(x_i, a_i, a_i^{\prime}, y_i\right)\right\}_{i=1}^{t-1}$, we construct the estimator $\thetat_t$ by OMD in an one-pass manner as in Eq.~\eqref{eq:omd}. Specifically, the estimator is updated by
\begin{align}
  \label{eq:omd-deploy}
  \thetat_{t+1}=\argmin_{\theta \in \Theta} \Big\{\big\langle g_t(\thetat_t), \theta \big\rangle +\frac{1}{2 \eta}\big\|\theta-\thetat_t\big\|_{\Ht_t}^2\Big\}.
\end{align}

\paragraph{Query Selection.} Unlike in the training stage, during deployment, the learner must balance exploration and exploitation to minimize regret. Specifically, the learner must select queries that are both informative and likely to yield high rewards. Focusing solely on high-reward queries may cause the learner to miss valuable feedback for model improvement, while prioritizing only informative queries could prevent reward maximization. To address this, we propose a novel strategy inspired by \citet{NeurIPS'21:Saha-Preference-bandits}, which accounts for both reward maximization and uncertainty maximization.

Based on the estimator $\theta_t$, we first define the promising set $\A_t$ that only contains the actions with potential to be the best action. Specifically, we define the promising set as
\begin{align}
    \A_t = \big \{ a \in \A \mid (\phi(x_t, a) - \phi(x_t, a'))^{\top} \thetat_t + \betat_t \norm{\phi(x_t, a) - \phi(x_t, a')}_{\H_t^{-1}} \geq 0, \forall a' \in \A \big\}.\label{eq:promising-set}
\end{align}
Then, we select the action pairs with the maximum uncertainty in the promising set, i.e.,
\begin{align}
    \label{eq:query-selection-deploy}
    (a_t, a_t') = \argmax_{(a, a') \in \A_t \times \A_t} \left\|\phi(x_t, a)-\phi(x_t, a')\right\|_{\H_t^{-1}}.
\end{align}
The overall algorithm is summarized in Algorithm~\ref{alg:deploy}. We show it enjoys the following regret bound.


\begin{algorithm}[!t]
    \caption{Deployment Stage}
    \label{alg:deploy}
    \begin{algorithmic}[1]
        \REQUIRE Regularization $\lambda>0$, failure probability $\delta \in(0,1]$
        \STATE Initialize $\thetat_1=0$, $\H_1 = \lambda I_d$.
        \FOR {$t=1,2, \ldots, T$}
        \STATE Observes context $x_t$.
        \STATE Compute the promising set by Eq.~\eqref{eq:promising-set}
        \STATE Choose duel actions by Eq.~\eqref{eq:query-selection-deploy}.
        \STATE Observe preference feedback $y_t$
        \STATE Update $\Ht_t = \H_t + \eta H_t(\thetat_t)$
        \STATE Compute $\thetat_{t+1}$ by Eq.~\eqref{eq:omd-deploy}
        \STATE Update $\H_{t+1} = \H_{t} + H_t(\thetat_{t+1})$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\begin{myThm}
    \label{thm:deploy}
    Let $\delta \in(0,1]$, set parameters as in Lemma~\ref{lem:confidence_set}, with probability at least $1-\delta$, Algorithm~\ref{alg:deploy} enjoys the following regret guarantee:
    \begin{align*}
      {\Reg}_T \leq \Ot \big(d\sqrt{{\kappa}{T}}\big).
    \end{align*}
\end{myThm}

\paragraph{Comparison with~\citet{NeurIPS'21:Saha-Preference-bandits}.} Our result improves upon theirs in both computational and statistical efficiency. For statistical efficiency, our algorithm achieves a regret of $\Ot \big(d\sqrt{{\kappa}{T}}\big)$, which is tighter by a factor of $\sqrt{\kappa}$ compared to their $\Ot \big(d{\kappa}\sqrt{{T}}\big)$ result. For computational efficiency, our algorithm updates in an online manner, with $\O(1)$ storage and $\O(T)$ time complexity for the total $T$ rounds. In contrast, their algorithm requires storing all the data and solving an optimization problem each round, leading to $\O(T)$ storage and $\O(T^2 \log T)$ time complexity. As $T$ is large or even infinite during deployment, our algorithm has a significant advantage in computational efficiency. 
