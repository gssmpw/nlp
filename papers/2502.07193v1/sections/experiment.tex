\section{Experiment}
\label{sec:experiment}

In this section, we empirically evaluate the performance of our proposed method in terms of both effectiveness and efficiency. Specifically, we empirically validate our method with LLMs, and construct an online RLHF pipeline to evaluate different methods in both the training and deployment stage.\footnotemark[1] Next, we first describe the experimental setup, and then present the empirical results.

\paragraph{Experimental Setup.} In our experiments, we employ the \texttt{Llama-3-8B-Instruct}~\footnotemark[2] as the foundation model for reward model. We extract features $\phi(x,a)$ using last layer of the model, and the feature space dimension is $d=4096$. We use the {Ultrafeedback-binarized dataset}, a pro-processed version of the original Ultrafeedback dataset~\citep{arXiv'23:Ultrafeedback} for evaluation, which is a large-scale, fine-grained, diverse preference dataset, widely used as a benchmark for RLHF. It collects about 64, 000 prompts from diverse resources, including question answering, summarization, and dialogue generation. Each data consists of a context $x$, two responses $a$ and $a'$, and a human preference label $y$. 

\footnotetext[1]{Our online RLHF pipeline code is available at \url{https://github.com/ZinYY/RLHF_Pipeline}}
\footnotetext[2]{\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}}

\paragraph{Implementation Details.} All experiments in this section were performed using four Nvidia A800 (80 GB) GPUs with two Intel(R) Xeon(R) Gold 6430 CPUs. The learning rate is set to $1\times 10^{-3}$ for all methods. We use DeepSpeed~\citep{KDD'20:DeepSpeed} with mixed-precision training (BF16). The maximum sequence length is set to 1024 tokens, and we use a global batch size of 32. For practical efficiency, we implement our method using Hessian-vector product (HVP) and conjugate gradient methods, which efficiently approximate the Hessian matrix without the need for computing $d^2$ matrix. We use the default partition of the Ultrafeedback-binarized dataset with the standard \texttt{train\_prefs} split for training and \texttt{test\_prefs} split for evaluation. Furthermore, we also enable flash attention for the improved training efficiency.


\subsection{Training Stage}
\label{subsec:exp:training}

\begin{figure*}
    \begin{minipage}[t]{0.99\textwidth}
        \centering
        \subfigure[training loss]{\includegraphics[width=0.23\columnwidth]{sections/figs/offline_passive_train_loss.pdf}
            \label{fig:passive-train-loss}}
        \hfill
        \subfigure[training accuracy]{\includegraphics[width=0.23\columnwidth]{sections/figs/offline_passive_train_acc.pdf}
            \label{fig:passive-train-acc}}
        \hfill
        \subfigure[evaluation loss]{\includegraphics[width=0.23\columnwidth]{sections/figs/offline_passive_eval_loss.pdf}
            \label{fig:passive-eval-loss}}
        \hfill
        \subfigure[evaluation accuracy]{\includegraphics[width=0.23\columnwidth]{sections/figs/offline_passive_eval_acc.pdf}
            \label{fig:passive-eval-acc}}
    \end{minipage}
    \caption{For the training stage with \emph{passive data collection}, we compare our method with MLE~\citep{ICML'23:Zhu-Principled}. We report the average accuracy and loss of the reward model during training.}
    \label{fig:training_passive}
\end{figure*}

In this part, we conduct experiments to evaluate the performance of our method in the training stage, including both passive and active data collection strategies.

\paragraph{Contenders.} In the training stage, we compare with:
\begin{itemize}
  \item \emph{Random}: A baseline that randomly selects queries.
  \item \emph{Passive-MLE}~\citep{ICML'23:Zhu-Principled}: A method that uses maximum likelihood estimation to learn the reward function and compute the pessimistic policy based on the learned reward model.
  \item \emph{Active-MLE}~\citep{arXiv'24:Das-RLHF-active}: A method using MLE to optimize the reward model and selects queries based on \emph{uncertainty}, i.e., the maximum difference of two actions.
\end{itemize}
We compared our method with other methods in terms of the accuracy of the reward model, as the method for training the final policy based on the learned reward model is the same as contenders.

\paragraph{Passive Data Collection Results.} 
We first evaluate our method in the \emph{passive data collection} setting. We randomly sample $T=30, 000$ data points from the Ultrafeedback dataset for training. Figure~\ref{fig:training_passive} shows the loss and accuracy vs. the number of training samples. We observe that our method converges faster to a lower loss and achieves a higher evaluation accuracy compared to baselines. The improvement is particularly pronounced in the small-sample regime ($T < 10, 000$), where our method achieves a higher evaluation accuracy with the same amount of samples compared to {Passive-MLE} which employs conventional stochastic gradient descent (SGD) updates. This demonstrates the superior statistical efficiency of our approach, therefore achieving a better performance with fewer training samples.
% Additionally, our method exhibits better computational efficiency, processing the full dataset ($T=50K$) approximately 3 times faster than MLE.

\paragraph{Active Data Collection Results.} We then evaluate our method in the \emph{active data collection} setting. In this scenario, the global batch size is set to 8, and we only allow the method to select 6, 400 samples out of the whole training datasets for training according to different selection strategies.
Table~\ref{tab:training_active_results} and Figure~\ref{fig:active_results} demonstrate that our active learning strategy achieves comparable performance to passive learning while using only 21\% of the labeled data. Specifically, our method achieves an average accuracy of 70.43\% after 6,400 actively selected samples, which is comparable with the previous state-of-the-art method, \emph{Active-MLE}, while our method is 3 times faster because we do not need to recompute all the selected data in the buffer, but we update the model in an online manner. The training loss curves in Figure~\ref{fig:active_train_loss} also show that our method converges faster compared to other contenders. This improved sample efficiency can be attributed to our theoretically-motivated query selection criterion that effectively identifies the most informative samples, which further validates the two key factors in our method: (1) the use of local norm $\H_t$ that better captures the uncertainty in different directions, and (2) the computationally efficient one-pass update rule that enables processing large-scale datasets.


\begin{figure}[!t]
    \centering
    \begin{minipage}{0.6\textwidth}
        \centering
        \subfigure[training loss]{\includegraphics[width=0.47\columnwidth]{sections/figs/active_train_loss.pdf}
        \label{fig:active_train_loss}}
        \hfill
        \subfigure[evaluation accuracy]{\includegraphics[width=0.47\columnwidth]{sections/figs/active_eval_acc.pdf}
        \label{fig:active_eval_acc}}
        \caption{For the training stage with \emph{active data collection}, we report (a) the training loss vs. the training iterations, and (b) the evaluation accuracy vs. the training iterations.}
        \label{fig:active_results}
    \end{minipage}
    \hfill
    \begin{minipage}{0.38\textwidth}
        \centering
        \vspace{2mm}
        \renewcommand{\arraystretch}{1.1}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{cccc}
            \toprule
            Method & ACC (\%) & Time (s) \\
            \midrule
            Rand-MLE & 69.51$\pm$0.5 & 4876$\pm$47 \\
            Active-MLE & 69.82$\pm$0.4 & 4982$\pm$52 \\
            Rand-OMD & 68.97$\pm$0.6 & 1456$\pm$31 \\
            Ours & \textbf{70.43$\pm$0.3} & \textbf{1489$\pm$36} \\
            \bottomrule
        \end{tabular}}
        \vspace{2mm}
        \captionof{table}{Comparison of different methods in terms of evaluation accuracy and training time in seconds. Results are averaged over five runs.}
        \label{tab:training_active_results}
    \end{minipage}
\end{figure}


\subsection{Deployment Stage}
\label{subsec:exp:deployment}

In this part, we evaluate our method on deployment stage.

\paragraph{Contenders.} In the deployment stage, we compare with:
\begin{itemize}
    \item \emph{Random}: A baseline strategy that randomly select responses from responses generated by the current policy model.
    \item \emph{Best-Two}: A baseline strategy that selects the best two responses from responses generated by the current policy model.
    \item \emph{Iterative-RLHF}~\citep{TMLR'24:Dong-RLHF}: A method that takes the best and the worst responses from the current policy model to construct a new preference pair for RLHF iteratively.
\end{itemize}

\paragraph{Deployment Stage Analysis.} For the scenario of \emph{deployment stage}, we evaluate on a pre-processed online-variant of the Ultrafeedback-binarized dataset, where we divide the dataset into 20 chunks and process them sequentially to simulate the online deployment scenario. As shown in Figure~\ref{fig:deployment_figures}, our method consistently outperforms the baselines in terms of both average cumulative rewards and win rates. The average cumulative rewards in Figure~\ref{fig:deployment_cumulative_rewards} show that our method learns more effectively from interactions, achieving higher rewards over time. The win rates in Figure~\ref{fig:deployment_win_rate} demonstrate that our method makes better decisions when selecting between response pairs, outperforming both the baseline methods of \emph{Random} and \emph{Best-Two}, and also better than \emph{Iterative-RLHF} which selects the best and worst response pairs for training. The superior performance of our method further validates the effectiveness of our theoretically grounded action selection strategy. This success stems from an efficient online update mechanism that enables rapid adaptation to new preference feedback, as well as the design of the promising set, which balances high-reward responses for immediate performance with sufficient exploration for future model improvements. These results align with our theoretical analysis in Section~\ref{sec:deploy}, validating the practical effectiveness of our proposed algorithm in the deployment stage.

\begin{figure}
    \begin{minipage}[t]{0.99\textwidth}
        \centering
        \subfigure[average cumulative rewards]{\includegraphics[width=0.45\columnwidth]{sections/figs/deployment_cumulative_rewards.pdf}
            \label{fig:deployment_cumulative_rewards}}
        \hfill
        \subfigure[win rates]{\includegraphics[width=0.45\columnwidth]{sections/figs/win_rate.pdf}
            \label{fig:deployment_win_rate}}
    \end{minipage}
    \caption{For the \emph{deployment stage}, we divide the Ultrafeedback dataset into 20 chunks, and report (a) the average cumulative rewards and the (b) win rates of different methods.}
    \label{fig:deployment_figures}
\end{figure}
