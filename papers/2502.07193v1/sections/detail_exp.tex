\section{Details of Experiments}
\label{sec:detail_exp}

In this section, we provide the omitted details of the experiment details and additional results.

\subsection{Implementation Details}
\label{subsec:detail_exp:impl}
% \vspace{2mm}
\noindent \textbf{Datasets.~~}
We use the UltraFeedback-binarized dataset~\citep{NeurIPS'23:DPO} for the experiments. This dataset is derived from the original UltraFeedback dataset, which comprises 64, 000 prompts sourced from diverse datasets including UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA, and FLAN. For each prompt, four model completions were generated using various open-source and proprietary language models, with GPT-4 providing comprehensive evaluations across multiple criteria including helpfulness, honesty, and truthfulness. The binarized version was constructed by selecting the completion with the highest overall score as the "chosen" response and randomly selecting one of the remaining completions as the "rejected" response, creating clear preference pairs suitable for reward modeling and direct preference optimization. This dataset structure aligns well with our experimental setup, providing a robust foundation for evaluating different preference learning approaches. The dataset's diverse prompt sources and evaluation criteria make it particularly valuable for training and evaluating reward models in a real-world context. To further tailor the dataset to our experimental setup, we organize the dataset as follows for the training and deployment stages:
\begin{itemize}
    \item Training stage with \emph{passive data collection}: We randomly sample $T=30, 000$ data points from the UltraFeedback-binarized dataset's \texttt{train\_prefs} split for training. Each data point consists of a prompt and two responses with a binary preference label indicating the preferred response. We use the \texttt{test\_prefs} split for evaluation.
    \item Training stage with \emph{active data collection}: We allow the method to actively select 6,400 samples from the \texttt{train\_prefs} split according to different selection strategies. The global batch size is set to 8 for training. The selection is performed iteratively, where in each iteration, the method selects the most informative samples based on its selection criterion.
    \item Deployment stage: We use a pre-processed online variant of the UltraFeedback-binarized dataset from the \texttt{test\_gen} split. The dataset is divided into 20 sequential chunks to simulate an online deployment scenario. For each chunk, we generate responses using the current policy (the foundation model of policy model is chosen to be \texttt{meta-llama / Llama-3.2-1B}), evaluate them using both the learned reward model and an oracle reward model. We choose \texttt{NCSOFT/Llama-3-OffsetBias-RM-8B}~\citep{arXiv'24:Park-OffsetBias} as the oracle reward model. After each chunk, we use the policy model to randomly generate 64 responses using different seeds. We then apply various strategies (\emph{Random}, \emph{Best-Two}, etc.) to select responses and construct new preference pairs, which are then used to update the reward model and the policy model.
\end{itemize}

\begin{algorithm}[!t]
    \caption{Efficient Update using Hessian-Vector Product with Conjugate Gradient}
    \label{alg:hvp-cg}
    \begin{algorithmic}[1]
        \REQUIRE Current parameter $\thetat_t$, gradient $g_t(\thetat_t)$, learning rate $\eta$, max CG steps $K$, base damping $\lambda_0$, error tolerance $\epsilon$
        \STATE Initialize $v_0 = 0$, $r_0 = g_t(\thetat_t)$, $p_0 = r_0$
        \STATE Compute damping $\lambda_t = \lambda_0 \cdot \min\{1, f(t/T)\}$
        \FOR {$k=0,1,\ldots,K-1$}
        \STATE Compute HVP: $\Ht_t p_k = \nabla_\theta(\nabla_\theta \mathcal{L}(\theta)^\top p_k)|_{\theta=\thetat_t} + \lambda_t p_k$
        \STATE $\alpha_k = \frac{r_k^\top r_k}{p_k^\top \Ht_t p_k}$, $v_{k+1} = v_k + \alpha_k p_k$, $r_{k+1} = r_k - \alpha_k \Ht_t p_k$,
        \STATE  $\beta_{k+1} = \frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}$, $p_{k+1} = r_{k+1} + \beta_{k+1} p_k$
        \IF{$\|r_{k+1}\| \le \epsilon$}
        \STATE \textbf{break}
        \ENDIF
        \ENDFOR
        \STATE Update parameter: $\thetat_{t+1} = \thetat_t - \eta v_{K}$
        \ENSURE Updated parameter $\thetat_{t+1}$
    \end{algorithmic}
\end{algorithm}

\noindent \textbf{Practical Implementation.~~}
To efficiently implement the OMD update in Eq.~\eqref{eq:omd} without the costly computing and storing the full Hessian matrix, we utilize the Hessian-vector product (HVP) method combined with conjugate gradient descent. The key insight is that the update can be reformulated as solving a linear system.
For the OMD update:
\begin{align*}
    \thetat_{t+1} & = \argmin_{\theta \in \Theta} \Big\{\big\langle g_t(\thetat_t), \theta \big\rangle +\frac{1}{2 \eta}\big\|\theta-\thetat_t\big\|_{\Ht_t}^2\Big\}  \approx \thetat_t - \eta \Ht_t^{-1}g_t(\thetat_t),
\end{align*}
where we ignore the projection operation.
The key challenge is computing \(\Ht_t^{-1}g_t(\thetat_t)\) efficiently. Instead of explicitly computing \(\Ht_t^{-1}\), we solve the linear system $\Ht_t v = g_t(\thetat_t)$
\begin{equation*}
    \Ht_t v = g_t(\thetat_t),
\end{equation*}
where \(v\) is the solution we seek. We use the conjugate gradient method with a damped Hessian to solve this system iteratively.
The HVP operation \(\Ht_t v\) for any vector \(v\) can be computed efficiently using automatic differentiation $\Ht_t v = \nabla_\theta \big(\nabla_\theta \mathcal{L}(\theta)^\top v\big)\big|_{\theta=\thetat_t} + \lambda v$, where \(\lambda\) is a damping coefficient that ensures numerical stability and \(\mathcal{L}(\theta)\) is the loss function. The full algorithm is summarized in Algorithm~\ref{alg:hvp-cg}. To ensure stability and convergence, we employ an adaptive damping strategy as $\lambda_t = \lambda_0 \cdot \min\{1, f(t/T)\}$, where $f(\cdot)$ is a damping growth function that can be linear, logarithmic, or cosine. In practice, we set $K = 3$ and $\lambda_0 = 0.8$ for all experiments.


\subsection{Validating the Magnitude of $\kappa$}  
\label{subsec:detail_exp:kappa}  

We validate the magnitude of $\kappa$ in Eq.~\eqref{eq:kappa} by computing its value during the training process. The results show that $\kappa = 171.62 \pm 85.49$ during our training process, which is relatively large.


\subsection{Combined with Adam Optimizer}
\label{subsec:detail_exp:adam}

\begin{figure*}[!t]
    \begin{minipage}[t]{0.99\textwidth}
        \centering
        \subfigure[training loss]{\includegraphics[width=0.235\columnwidth]{sections/figs/offline_all_train_loss_adam.pdf}
            \label{fig:passive-train-loss-adam}}
        \hfill
        \subfigure[training accuracy]{\includegraphics[width=0.235\columnwidth]{sections/figs/offline_all_train_acc_adam.pdf}
            \label{fig:passive-train-acc-adam}}
        \hfill
        \subfigure[evaluation loss]{\includegraphics[width=0.235\columnwidth]{sections/figs/offline_all_eval_loss_adam.pdf}
            \label{fig:passive-eval-loss-adam}}
        \hfill
        \subfigure[evaluation accuracy]{\includegraphics[width=0.235\columnwidth]{sections/figs/offline_all_eval_acc_adam.pdf}
            \label{fig:passive-eval-acc-adam}}
    \end{minipage}
    \caption{For the training stage with \emph{passive data collection}, we compare our proposed method and MLE~\citep{ICML'23:Zhu-Principled} in with passive data collection combined with \emph{Adam}. We report the average accuracy and loss of the reward model during the training process.}
    \label{fig:training_passive_adam}
\end{figure*}

In previous experiments, we used SGD to update model parameters. In this section, we integrate the methods with the \emph{Adam optimizer}~\citep{ICLR'15:Adam}, i.e., adding the first and second momentum terms to the model updates. The results, shown in Figure~\ref{fig:training_passive_adam}, indicate that the Adam optimizer further enhances the performance of our method by leveraging the momentum term to accelerate convergence. With the momentum term, our method remains superior to the MLE-based method; however, the performance gap is reduced. We think that this is because the Adam optimizer incorporates second-order information for optimization, diminishing the advantage of our method compared to the SGD cases.



\subsection{Comparison with DPO}
\label{subsec:detail_exp:dpo}

\begin{wrapfigure}{r}{0.43\textwidth}
    \vspace{-6.5mm}
    \begin{center}
        \includegraphics[width=0.4\textwidth]{sections/figs/deployment_cumulative_rewards_regular_with_dpo.pdf}
    \end{center}
    \vspace{-6mm}
    \caption{Comparison with DPO in the deployment stage.}
    \label{fig:dpo-compare}
    \vspace{-2mm}
\end{wrapfigure}
We also compare with DPO (Direct Preference Optimization)~\citep{NeurIPS'23:DPO} in the deployment stage, as a reward-free method, DPO optimizes the policy directly using preference feedback without explicit reward modeling. 
To ensure a fair comparison, we initialize the policy with 400 samples and use the same dataset settings as PPO to iteratively update the policy model using the DPO algorithm.
The results are illustrated in Figure~\ref{fig:dpo-compare}.
While DPO outperforms the random baseline (Rand-MLE), it achieves lower cumulative rewards than the PromisingSet-based method. This result suggests that DPO's online learning capability remains a challenge. In contrast, the reward model learned by our selection strategy effectively learned streaming data and continuously updates the policy as new data arrive, indicating that in our deployment stage, a reward model with PPO may be a more suitable choice for sequentially learning from new data.


% This performance gap can be attributed to two main factors: First, our method explicitly models the reward function, which provides more informative learning signals for policy optimization. Second, our active data collection strategy enables more efficient learning by selecting the most informative samples, while DPO relies on passive preference data.

% The experimental results align with theoretical insights from previous work~\citep{NeurIPS'23:DPO, ICML'23:Zhu-Principled}, which suggest that reward modeling can provide more structured guidance for policy optimization compared to direct preference optimization. Specifically, our method achieves approximately 8\% higher cumulative rewards than DPO after 20 iterations, demonstrating the benefits of combining reward modeling with active learning strategies.

