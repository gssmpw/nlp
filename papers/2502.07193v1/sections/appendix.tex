\section{Useful Lemmas}
\begin{myLemma}
    \label{lem:estimation-error}
    For any $t \in [T]$, define the second-order approximation of the loss function $\ell_t(\theta)$ at the estimator $\thetat_{t}$ as $$\ellt_{t}(\theta) = \ell_{t}(\thetat_{t}) + \inner{\nabla \ell_{t}(\thetat_{t})}{\theta - \thetat_{t}} + \frac{1}{2} \norm{\theta - \thetat_{t}}_{H_{t}(\thetat_{t})}^2.$$ Then, for the following update rule
    \begin{align*}
        \thetat_{t+1} = \argmin_{\theta \in \Theta} \left\{\ellt_t(\theta) + \frac{1}{2\eta} \norm{\theta - \thetat_t}_{\H_t}^2\right\},
    \end{align*}
    it holds that
    \begin{align*}
        {}&\norm{\thetat_{t+1} - \theta^*}_{\H_{t+1}}^2 \\
        \leq {}& 2 \eta \left(\sum_{i=1}^t \ell_{i}(\theta^*) - \sum_{i=1}^t \ell_{i}(\thetat_{i+1}) \right) + 4 \lambda B^2                                    + 12 \sqrt{2} B L^3 \eta \sum_{i=1}^t \norm{\thetat_{i+1} - \thetat_{i}}_2^2 - \sum_{i=1}^t \norm{\thetat_{i+1} - \thetat_{i}}_{\H_{i}}^2.
    \end{align*}
\end{myLemma}
\begin{proof}
    Based on the analysis of (implicit) OMD update (see Lemma~\ref{lem:implicit-omd}), for any $i \in [T]$, we have
    \begin{align*}
        \big\langle\nabla \tilde{\ell}_{i} (\thetat_{i+1}), \thetat_{i+1}-\theta^*\big\rangle \leqslant \frac{1}{2 \eta}\left(\|\thetat_{i}-\theta^*\|_{\H_{i}}^2-\|\thetat_{i+1}-\theta^*\|_{\H_{i}}^2-\|\thetat_{i+1}-\thetat_{i}\|_{\H_i}^2\right)
    \end{align*} 
    According to Lemma~\ref{lem:strongly-convex}, we have
    \begin{align*}
        \ell_i(\thetat_{i+1})-\ell_i\left(\theta^*\right) \leqslant\big\langle\nabla \ell_i(\thetat_{i+1}), \thetat_{i+1}-\theta^*\big\rangle-\frac{1}{\zeta}\big\|\thetat_{i+1}-\theta^*\big\|_{\nabla^2 \ell_i(\thetat_{i+1})}^2,
    \end{align*}  
    where $\zeta=\log 2+2(LB+1)$. Then, by combining the above two inequalities, we have
    \begin{align*}
        \ell_i(\thetat_{i+1})-\ell_i(\theta^*) & \leqslant\langle\nabla \ell_i(\thetat_{i+1})-\nabla \tilde{\ell}_i(\thetat_{i+1}), \thetat_{i+1}-\theta^*\rangle  \\
        &\qquad +\frac{1}{\zeta} \Big(\|\thetat_i-\theta^*\|_{\H_i}^2-\|\thetat_{i+1}-\theta^*\|_{\H_{i+1}}^2-\|\thetat_{i+1}-\thetat_i\|_{\H_i}^2 \Big).
    \end{align*}
    We can further bound the first term of the right-hand side as:
    \begin{align*}
         \big \langle\nabla \ell_i(\thetat_{i+1})-\nabla \tilde{\ell}_i(\thetat_{i+1}), \thetat_{i+1}-\theta^*\big\rangle = \ &\big \langle\nabla \ell_i(\thetat_{i+1})-\nabla \ell_i(\thetat_i)-\nabla^2 \ell_i(\thetat_i)(\thetat_{i+1}-\thetat_i), \thetat_{i+1}-\theta^*\big\rangle \\
        = \ & \big \langle D^3 \ell_i({\xi}_{i+1})[\thetat_{i+1}-\thetat_i](\thetat_{i+1}-\thetat_i), \thetat_{i+1}-\theta^*\big\rangle \\
        \leqslant \ & 3\sqrt{2} L\big \|\thetat_{i+1}-\theta^*\big\|_2 \big \|\thetat_{i+1}-\thetat_i\big\|_{\nabla^2 \ell_i({\xi}_{i+1})}^2 \\
        \leqslant \ & 6\sqrt{2} BL \big \|\thetat_{i+1}-\thetat_i\big\|_{\nabla^2 \ell_i({\xi}_{i+1})}^2 \\
        \leqslant \ & 6\sqrt{2} BL^3 \big \|\thetat_{i+1}-\thetat_i\big\|_2^2,
    \end{align*}
    where the second equality holds by the mean value theorem, the first inequality holds by the self-concordant-like property of $\ell_i(\cdot)$ in Lemma~\ref{lem:self-concordant}, and the last inequality holds by $\thetat_{i+1}$ and $\theta^*$ belong to $\Theta = \{\theta \in \R^d, \norm{\theta}_2 \leq B\}$, and ${\nabla^2 \ell_i({\xi}_{i+1})} \preceq L^2 I_d$.

    Then, by taking the summation over $i$ and rearranging the terms, we obtain
    \begin{align*}
        & \big\|\thetat_{t+1}-\theta^*\big\|_{\H_{t+1}}^2 \\
        \leqslant \ & \zeta\left(\sum_{i=1}^t \ell_{s}\left(\theta^*\right)-\sum_{i=1}^t \ell_{s}(\thetat_{i+1})\right)+\|\thetat_{1}-\theta^*\|_{\H_{1}}^2 +6\sqrt{2} BL^3 \zeta \sum_{i=1}^t\big\|\thetat_{i+1}-\thetat_i\big\|_2^2 -\sum_{i=1}^t\big\|\thetat_{i+1}-\thetat_i\big\|_{\H_i}^2 \\
        \leqslant \ & \zeta\left(\sum_{i=1}^t \ell_{s}\left(\theta^*\right)-\sum_{i=1}^t \ell_{s}\big(\thetat_{i+1}\big)\right)+4 \lambda B^2  +6\sqrt{2}BL^3 \zeta \sum_{i=1}^t\big\|\thetat_{i+1}-\thetat_i\big\|_2^2 -\sum_{i=1}^t\big\|\thetat_{i+1}-\thetat_i\big\|_{\H_i}^2,
    \end{align*}
    where the last inequality is by $\|\thetat_{1}-\theta^*\|_{\H_{1}}^2 \leq \lambda \|\thetat_{1}-\theta^*\|_2^2 \leq 4 \lambda B^2$. Set $\zeta = 2\eta$ finishes the proof. 
\end{proof}

\section{Proof of Lemma~\ref{lem:confidence_set}}
\begin{proof}
    Based on Lemma~\ref{lem:estimation-error}, we have  
    \begin{align*}
        {}&\norm{\thetat_{t+1} - \theta^*}_{\H_{t+1}}^2 \\
        \leq {}& 2 \eta \left(\sum_{i=1}^t \ell_{i}(\theta^*) - \sum_{i=1}^t \ell_{i}(\thetat_{i+1}) \right) + 4 \lambda B^2                                    + 12 \sqrt{2} B L^3 \eta \sum_{i=1}^t \norm{\thetat_{i+1} - \thetat_{i}}_2^2 - \sum_{i=1}^t \norm{\thetat_{i+1} - \thetat_{i}}_{\H_{i}}^2.
    \end{align*}
    It remains to bound the right-hand side of the above inequality in the following. The most challenging part is to bound the term $\sum_{i=1}^t \ell_{i}(\theta^*) - \sum_{i=1}^t \ell_{i}(\thetat_{i+1})$. This term might seem straightforward to control, as it can be observed that $\theta^* = \argmin_{\theta \in \R^d} \bar{\ell}(\theta) \triangleq \E_{y_{i}}[\ell_{i}(\theta)]$, where $\ell_{i}(\theta)$ serves as an empirical observation of $\bar{\ell}(\theta)$. Consequently, the loss gap term seemingly can be bounded using appropriate concentration results. However, a caveat lies in the fact that the update of the estimator $\tilde{\theta}_{i+1}$ depends on $\ell_{i}$, or more precisely $y_{i}$, making it difficult to directly apply such concentrations.  

    To address this issue, following the analysis in \citet{NeurIPS'23:MLogB}, we decompose the loss gap into two components by introducing an intermediate term. Specifically, we define the softmax function as 
    \begin{align*}
        [\sigma_i(q)]_1 = \frac{\exp(q)}{1+\exp(q)} \quad \text{and} \quad [\sigma_i(q)]_0 = \frac{1}{1+\exp(q)},
    \end{align*}
    where $[\cdot]_i$ denotes the $i$-th element of the vector. Then, the loss function $\ell_{i}(\theta)$ can be rewritten as
    \begin{align*}
        \ell(q_t, y_t) = -\ind{y_t = 1} \cdot \log \left({[\sigma(q_t)]_1}\right) -\ind{y_t = 0} \cdot \log \left({[\sigma(q_t)]_0}\right).
    \end{align*}
    Then, we define the pseudo-inverse function of $\sigma^{-1}(p)$ with 
    \begin{align*}
        [\sigma^{-1}(p)] = \log({q}/({1-q})) \quad \text{and} \quad [\sigma^{-1}(p)]_0 = \log((1-q)/{q}).
    \end{align*}
     Then, we decompose the regret into two terms by introducing an intermediate term.
    \begin{align*}
        \sum_{i=1}^t \ell_{s}\left(\theta^*\right)-\sum_{i=1}^t \ell_{s}(\thetat_{i+1}) = & \underbrace{\sum_{i=1}^t \ell_{s}\left(\theta^*\right)-\sum_{i=1}^t \ell_{s}(q_i, y_i)}_{\term a} + \underbrace{\sum_{i=1}^t \ell_{s}(q_i, y_i) - \sum_{i=1}^t \ell_{s}(\thetat_{i+1})}_{\term b}
    \end{align*}
    where $q_i$ is an aggregating forecaster for logistic loss defined by $q_i = \sigma^{-1}(\E_{\theta \sim P_i}[\sigma(\theta^\top z_i)])$ and $P_i=\N(\thetat_i, (1+c\H_i^{-1}))$  is the Gaussian distribution with mean $\thetat_i$ and covariance $(1+c\H_i^{-1})$, where $c>0$ is a constant to be specified later. It remains to bound the terms $\term a$ and $\term b$, which were initially analyzed in \citet{NeurIPS'23:MLogB} and further refined by \citet{NeurIPS'24:Lee-Optimal-MNL}. Specifically, using Lemmas F.2 and F.3 in \citet{NeurIPS'24:Lee-Optimal-MNL}, we can bound them as follows.

    For $\term a$, let $\delta \in (0, 1)$ and $\lambda \geq 1$. With probability at least $1-\delta$, for all $t \in [T]$, we have
    \begin{align*}
        \term a \leq \left(3 \log (1+2t)+2+ LB\right)\left(\frac{17}{16} \lambda+2 \sqrt{\lambda} \log \left(\frac{2 \sqrt{1+2 t}}{\delta}\right)+16\left(\log \left(\frac{2 \sqrt{1+2 t}}{\delta}\right)\right)^2\right)+2 .
    \end{align*}
    For $\term b$, let $\lambda \geq \max \{2, 72cd\}$. Then, for all $t \in [T]$, we have
    \begin{align*}
        \term b \leq \frac{1}{2 c} \sum_{i=1}^t\Big\|\thetat_{i+1}-\thetat_{i}\Big\|_{\H_i}^2+\sqrt{6} c d \log \left(1+\frac{2 t B^2}{d \lambda}\right)
    \end{align*}
    Combing the above two bounds, we have
    \begin{align*}
        \big\|\thetat_{t+1}-\theta^*\big\|_{\H_{t+1}}^2  &\leq 2 \eta\left(3 \log (1+2 t)+2+BL\right)\left(\frac{17}{16} \lambda+2 \sqrt{\lambda} \log \left(\frac{2 \sqrt{1+2 t}}{\delta}\right)+16\left(\log \left(\frac{2 \sqrt{1+2 t}}{\delta}\right)\right)^2\right) \\
        &\hspace{-18mm}+4 \eta+2 \eta \sqrt{6} c d \log \left(1+\frac{2 t L^2}{d \lambda}\right)+4 \lambda L_\theta^2 +12 \sqrt{2} BL^3 \eta \sum_{i=1}^t\left\|\thetat_{i+1}-\thetat_i\right\|_2^2+\left(\frac{\eta}{c}-1\right) \sum_{i=1}^t\left\|\thetat_{i+1}-\thetat_i\right\|_{\H_i}^2 .
    \end{align*}
    Setting $c=7 \eta / 6$ and $\lambda \geq 84 \sqrt{2} BL^3 \eta$, we have
    \begin{align*}
        12 \sqrt{2} BL^3 \eta \sum_{i=1}^t\left\|\thetat_{i+1}-\thetat_i\right\|_2^2+\left(\frac{\eta}{c}-1\right) \sum_{i=1}^t\left\|\thetat_{i+1}-\thetat_i\right\|_{\H_i}^2 \leq \left(12 \sqrt{2} BL^3 \eta - \frac{\lambda}{7}\right) \sum_{i=1}^t\left\|\thetat_{i+1}-\thetat_i\right\|_2^2 \leq 0.
    \end{align*}
    Note that $84 \sqrt{2}\left(BL^3+d L^2\right) \eta \geq \max \left\{2 L^2, 72 c d L^2, 84 \sqrt{2} BL^3 \eta\right\}$, so we set $\lambda \geq 84 \sqrt{2}\left(BL^3+\right.$ $\left.d L^2\right) \eta$. As we have $\eta=(1 / 2) \log 2+\left(BL+1\right)$, we have
    \begin{align*}
        \big\|\thetat_{t+1}-\theta^*\big\|_{\H_{t+1}} \leq \O \Big(\sqrt{d}(\log (t / \delta))^2 \Big).
    \end{align*}
    This finishes the proof.
\end{proof}

\section{Proof of Theorem~\ref{thm:passive}}
\begin{proof}
    Define $J(\pi) = \mathbb{E}_{x \sim \rho} [r\left(x, \pi(x)\right)]$, $J'(\pi) = J(\pi) - \inner{\theta^*}{v}$, we have
    \begin{align*}
        \subopt\left(\pi_T\right)= \left(J^{\prime}\left(\pi^{*}\right)-\Jt\left(\pi^*\right)\right)+\left(\Jt\left(\pi^*\right)-\Jt\left(\pi_T\right)\right)+\left(\Jt\left(\pi_T\right)-J^{\prime}\left(\pi_T\right)\right).
    \end{align*}
    Since $\pi_T$ is the optimal policy under expected value $\Jt(\pi)$, i.e., $\Jt(\pi_T) = \max_{\pi \in \Pi} \Jt(\pi)$, we have
    \begin{align}
    \label{eq:passive-1}
      \Jt\left(\pi^*\right)-\Jt\left(\pi_T\right) \leq 0
    \end{align}
    For the third term, we have with probability at least $1-\delta$, it holds that
    \begin{align}
    \label{eq:passive-2}
      \Jt\left(\pi_T\right)-J^{\prime}\left(\pi_T\right) = \min_{\theta \in \C_T} \mathbb{E}_{x \sim \rho}\left[\theta^{\top}(\phi(s, \pi_T(s))-v)\right]-\mathbb{E}_{x \sim \rho}\left[\theta^{* \top}(\phi(s, \pi_T(s))-v)\right] \leq 0,
    \end{align}
    where the last inequality holds by $\theta^* \in \C_T$ with probability at least $1-\delta$.

    For the first term, we have with probability at least $1-\delta$, it holds that
    \begin{align*}
      & J^{\prime}\left(\pi^{*}\right)-\Jt\left(\pi^*\right) \nonumber\\
    = \ & \mathbb{E}_{x \sim \rho}\left[(\theta^*)^{\top}(\phi(s, \pi^*(s))-v)\right]- \min_{\theta \in \C_T} \mathbb{E}_{x \sim \rho}\left[\theta^{\top}(\phi(s, \pi^*(s))-v)\right] \\
    % = \ & \sup_{\theta \in \C_T} \mathbb{E}_{x \sim \rho}\left[\left(\theta^*-\theta\right)^{\top}\left(\phi(x,\pi^*(x))-v\right)\right] \\
    = \ & \sup_{\theta \in \C_T} \mathbb{E}_{x \sim \rho}\left[\left(\theta^*-\thetat_T+\thetat_T-\theta\right)^{\top}\left(\phi(x,\pi^*(x))-v\right)\right] \\
    = \ & \mathbb{E}_{x \sim \rho}\left[\left(\theta^*-\thetat_T\right)^{\top}\left(\phi(x,\pi^*(x))-v\right)\right]+\sup_{\theta \in \C_T} \mathbb{E}_{x \sim \rho}\left[\left(\thetat_T-\theta\right)^{\top}\left(\phi(x,\pi^*(x))-v\right)\right] \\
    \leq \ & \Big(\norm{\theta^*-\thetat_T}_{\H_T} + \sup_{\theta \in \C_T}\norm{\theta-\thetat_T}_{\H_T^{-1}}\Big) \cdot \bignorm{\mathbb{E}_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)}_{\H_T^{-1}},
    \end{align*}
    where the first inequality holds by the Cauchy-Schwarz inequality. 
    
    Since it holds $\thetat_T \in \C_T$ with probability at least $1-\delta$ by Lemma~\ref{lem:confidence_set}, we have $\norm{\theta^*-\thetat_T}_{\H_T} \leq \betat_T$ and $\sup_{\theta \in \C_T}\norm{\theta-\thetat_T}_{\H_T}  \leq \betat_T$. Thus, we obtain
    \begin{align}
        \label{eq:passive-3}
        J^{\prime}\left(\pi^{*}\right)-\Jt\left(\pi^*\right) \leq \ & 2 \betat_T \cdot \bignorm{\mathbb{E}_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)}_{\H_T^{-1}} .
    \end{align}
    
    Combining Eq.~\eqref{eq:passive-1}, Eq.~\eqref{eq:passive-2}, and Eq.~\eqref{eq:passive-3} and substituting $\betat_T = \O(\sqrt{d}(\log (T / \delta))^2)$, we have with probability at least $1-\delta$, it holds that
    \begin{align*}
        \subopt\left(\pi_T\right) & =\left(J^{\prime}\left(\pi^{*}\right)-\Jt\left(\pi^*\right)\right)+\left(\Jt\left(\pi^*\right)-\Jt\left(\pi_T\right)\right)+\left(\Jt\left(\pi_T\right)-J^{\prime}\left(\pi_T\right)\right)\\
        & \leq 2 \betat_T \cdot \bignorm{\mathbb{E}_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)}_{\H_T^{-1}} \\
        & \leq \O\left(\sqrt{d}\left(\log \frac{T}{\delta}\right)^2 \cdot \bignorm{\mathbb{E}_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)}_{\H_T^{-1}}\right).
    \end{align*}
    This completes the proof.
  \end{proof}

\section{Proof of Theorem~\ref{thm:active}} 
\begin{proof}
    Let the sub-optimality gap for a context $x \in \mathcal{X}$ be denoted as $\subopt(x)$. Thus, for any $\delta \in (0, 1)$, with probability at least $1-\delta$, we have
   \begin{align*}
    \subopt(x) & =\left(\phi\left(x, \pi^*(x)\right)-\phi\left(x, \pi_T(x)\right)\right)^{\top} \theta^* \\
  & \leq \left(\phi\left(x, \pi^*(x)\right)-\phi\left(x, \pi_T(x)\right)\right)^{\top} \theta^*+\left(\phi\left(x, \pi_T(x)\right)-\phi\left(x, \pi^*(x)\right)\right)^{\top}\left(\frac{1}{T} \sum_{t=1}^T \thetat_t\right) \\
  & =\left(\phi\left(x, \pi^*(x)\right)-\phi\left(x, \pi_T(x)\right)\right)^{\top}\left(\theta^*-\frac{1}{T} \sum_{t=1}^T \thetat_t\right) \\
  & =\frac{1}{T} \sum_{t=1}^T\left(\phi\left(x, \pi^*(x)\right)-\phi\left(x, \pi_T(x)\right)\right)^{\top}\left(\theta^*-\thetat_t\right) \\
  & \leq \frac{1}{T} \sum_{t=1}^T\left\|\phi\left(x, \pi^*(x)\right)-\phi\left(x, \pi_T(x)\right)\right\|_{\H_t^{-1}}\left\|\theta^*-\thetat_t\right\|_{\H_t}\\
  & \leq \frac{\betat_T}{T} \sum_{t=1}^T\left\|\phi\left(x, \pi^*(x)\right)-\phi\left(x, \pi_T(x)\right)\right\|_{\H_t^{-1}},
   \end{align*}
   where the first inequality is due to the fact that $\left(\phi\left(x, \pi_T(x)\right)-\phi\left(x, \pi^*(x)\right)\right)^{\top}\left(\frac{1}{T} \sum_{t=1}^T \thetat_t\right)\geq 0$ by the design of $\pi_T(x)$, the second is due to the Cauchy-Schwarz inequality, and the last inequality is due to $\|\theta^*-\thetat_t\|_{\H_t} \leq \beta_T$ with probability at least $1-\delta$ by Lemma~\ref{lem:confidence_set}.

   By our algorithm's choice $(x_t, a_t, a_t^{\prime}) = \argmax_{x \in \mathcal{X}, a, a^{\prime} \in \mathcal{A}}\left\|\phi(x, a)-\phi\left(x, a^{\prime}\right)\right\|_{\H_t^{-1}}$, we have
   \begin{align*}
    \sum_{t=1}^T\left\|\phi\left(x, \pi^*(x)\right)-\phi\left(x, \pi_T(x)\right)\right\|_{\H_t^{-1}} \leq \sum_{t=1}^T\left\|\phi\left(x_t, a_t\right)-\phi\left(x_t, a_t^{\prime}\right)\right\|_{\H_t^{-1}} = \sum_{t=1}^T\left\|z_t\right\|_{\H_t^{-1}}.
   \end{align*}
   Furthermore, by the definition of $\H_t$, we have
   \begin{align*}
    \H_t = \lambda I_d + \sum_{s=1}^{t-1} \dot{\sigma}\left(z_s^{\top} \thetat_{s+1}\right) z_s z_s^{\top} \geq \lambda I_d + \frac{1}{\kappa} \sum_{s=1}^{t-1} z_s z_s^{\top} = \frac{1}{\kappa}\left(\kappa \lambda I_d + \sum_{s=1}^{t-1} z_s z_s^{\top}\right) \triangleq \frac{1}{\kappa} V_t.
    \end{align*}
    Thus, we have
    \begin{align*}
      \sum_{t=1}^T\left\|z_t\right\|_{\H_t^{-1}} \leq \sqrt{\kappa} \sum_{t=1}^T\left\|z_t\right\|_{V_t^{-1}} \leq \sqrt{\kappa} \sqrt{T \sum_{t=1}^T\left\|z_t\right\|^2_{V_t^{-1}}} \leq \sqrt{2 \kappa d T \log \left(1+\frac{4TL^2}{\lambda \kappa d}\right)},
    \end{align*}
    where the first inequality holds by the fact that $\H_t \succeq \frac{1}{\kappa} V_t$, the second inequality holds by the Cauchy-Schwarz inequality, and the last inequality holds by the elliptic potential lemma in Lemma~\ref{lem:elliptic-potential}. Thus, we have for any context $x \in \mathcal{X}$,
    \begin{align*}
        \subopt(x) \leq \frac{\betat_T}{T} \sqrt{2 \kappa d T \log \left(1+\frac{4TL^2}{\lambda \kappa d}\right)}.
    \end{align*}
    By the definition of $\subopt(\pi_T)$, we have with probability at least $1-\delta$,
    \begin{align*}
        \subopt\left(\pi_T\right) = \mathbb{E}_{x \sim \rho} \left[\subopt(x)\right] \leq \frac{\betat_T}{T} \sqrt{2 \kappa d T \log \left(1+\frac{T}{\lambda \kappa d}\right)} \leq \Ot\left(d \sqrt{\frac{\kappa}{T}}\right).
    \end{align*}
    This finishes the proof.
\end{proof} 


\section{Proof of Theorem~\ref{thm:deploy}}
\begin{proof}
    We first analyze the instantaneous regret at round $t$. For any $\delta \in (0, 1)$, with probability at least $1-\delta$, it holds that
    \begin{align*}
      & \big(r(x_t, \pi^*(x_t)) - r(x_t, a_t)\big) + \big(r(x_t, \pi^*(x_t)) - r(x_t, a_t')\big) \\
      = \ &\big(\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)\big)^{\top} \theta^* + \big(\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t')\big)^{\top} \theta^* \\
      = \ & \big(\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)\big)^{\top} \thetat_t + \big(\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)\big)^{\top} (\theta^* - \thetat_t) \\
      & + \big(\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t')\big)^{\top} \thetat_t + \big(\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t')\big)^{\top} (\theta^* - \thetat_t) \\
      \leq \ & \betat_t \bignorm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)}_{\H_t^{-1}}  + \bignorm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)}_{\H_t^{-1}} \bignorm{\theta^* - \thetat_t}_{\H_t}\\
      & + \betat_t \bignorm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a'_t)}_{\H_t^{-1}}  + \bignorm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a'_t)}_{\H_t^{-1}} \bignorm{\theta^* - \thetat_t}_{\H_t} \\
      \leq \ & \betat_t \norm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)}_{\H_t^{-1}} + \betat_t \norm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)}_{\H_t^{-1}} \\
      & + \betat_t \norm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t')}_{\H_t^{-1}} + \betat_t \norm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t')}_{\H_t^{-1}} \\
      \leq \ & 4 \betat_t \norm{\phi(x_t, a_t) - \phi(x_t, a_t')}_{\H_t^{-1}}.
    \end{align*}
    Here, the first inequality holds since both $a_t, a'_t \in \A_t$ and by the definition of $\A_t$, it implies that 
    \begin{align*}
        \big(\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)\big)^{\top} \thetat_t &\leq \betat_t \bignorm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)}_{\H_t^{-1}},\\
        \big(\phi(x_t, \pi^*(x_t)) - \phi(x_t, a'_t)\big)^{\top} \thetat_t &\leq \betat_t \bignorm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t')}_{\H_t^{-1}}.
    \end{align*}
    The second inequality holds by selection strategy of $(a_t, a_t')$, it holds that
    \begin{align*}
        \norm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a_t)}_{\H_t^{-1}} & \leq \norm{\phi(x_t, a_t) - \phi(x_t, a_t')}_{\H_t^{-1}},\\
        \norm{\phi(x_t, \pi^*(x_t)) - \phi(x_t, a'_t)}_{\H_t^{-1}} & \leq \norm{\phi(x_t, a_t) - \phi(x_t, a_t')}_{\H_t^{-1}}.
    \end{align*}
    Thus, we have
    \begin{align*}
      {\Reg}_T  = \frac{1}{2} \Big(\big(r(x_t, \pi^*(x_t)) - r(x_t, a_t)\big) + \big(r(x_t, \pi^*(x_t)) - r(x_t, a_t')\big)\Big) \leq 2 \betat_T \sum_{t=1}^T \norm{\phi(x_t, a_t) - \phi(x_t, a_t')}_{\H_t^{-1}}.
    \end{align*}
    Furthermore, by the definition of $\H_t$, we have
   \begin{align*}
    \H_t = \lambda I_d + \sum_{s=1}^{t-1} \dot{\sigma}\left(z_s^{\top} \thetat_{s+1}\right) z_s z_s^{\top} \geq \lambda I_d + \frac{1}{\kappa} \sum_{s=1}^{t-1} z_s z_s^{\top} = \frac{1}{\kappa}\left(\kappa \lambda I_d + \sum_{s=1}^{t-1} z_s z_s^{\top}\right) \triangleq \frac{1}{\kappa} V_t
    \end{align*}
    Thus, we obtain
   \begin{align*}
    \sum_{t=1}^T\left\|z_t\right\|_{\H_t^{-1}} \leq \sqrt{\kappa} \sum_{t=1}^T\left\|z_t\right\|_{V_t^{-1}} \leq \sqrt{\kappa} \sqrt{T \sum_{t=1}^T\left\|z_t\right\|^2_{V_t^{-1}}} \leq \sqrt{2 \kappa d T \log \left(1+\frac{4TL^2}{\lambda \kappa d}\right)},
  \end{align*}
  where the first inequality holds by the fact that $\H_t \succeq \frac{1}{\kappa} V_t$, the second inequality holds by the Cauchy-Schwarz inequality, and the last inequality holds by the elliptic potential lemma in Lemma~\ref{lem:elliptic-potential}.
  Thus, we have with probability at least $1-\delta$,
  \begin{align*}
    {\Reg}_T \leq 2{\betat_T} \sqrt{2 \kappa d T \log \left(1+\frac{4TL^2}{\lambda \kappa d}\right)} \leq \Ot \left(d\sqrt{{\kappa}{T}}\right).
  \end{align*}
  This finishes the proof.
\end{proof}


\section{Supporting Lemmas}

\begin{myDef}[{\citet{ICMCOISM'15:self-concordant}}]
    A convex function $f \in \mathcal{C}^3\left(\mathbb{R}^m\right)$ is $M$-self-concordant-like function if
    \begin{align*}
        \left|\psi^{\prime \prime \prime}(s)\right| \leqslant M\|\mathbf{b}\|_2 \psi^{\prime \prime}(s),
    \end{align*}
    for $s \in \mathbb{R}$ and $M>0$, where $\psi(s):=f(\mathbf{a}+s \mathbf{b})$ for any $\mathbf{a}, \mathbf{b} \in \mathbb{R}^m$. 
\end{myDef}

\begin{myLemma}[{\citet[Proposition C.1]{NeurIPS'24:Lee-Optimal-MNL}}]
    \label{lem:self-concordant}
    The loss $\ell_t(\theta)$ defined in Eq.~\eqref{eq:log-likelihood} is $3\sqrt{2} L$-self-concordant-like for $\forall t \in [T]$.
\end{myLemma}

\begin{myLemma}[{\citet[Lemma 11]{NIPS'11:AY-linear-bandits}}]
    \label{lem:elliptic-potential}
    Suppose $x_1, \ldots, x_t \in \R^d$ and for any $1 \leq s \leq t$, $\norm{x_s}_2 \leq L$. Let $V_t = \lambda I_d + \sum_{s=1}^{t-1} x_s x_s^\top $ for $\lambda \geq 0$. Then, we have 
    \begin{align*}
        \sum_{s=1}^t\left\|z_s\right\|_{V_s^{-1}}^2 \leq 2 d \log \left(1+\frac{t L^2}{\lambda d}\right).
    \end{align*}
\end{myLemma}

\begin{myLemma}[{\citet[Proposition 4.1]{NeurIPS'20:Campolongo-IOMD}}]
    \label{lem:implicit-omd}
    Define $\mathbf{w}_{t+1}$ as the solution of
    \begin{align*}
        \mathbf{w}_{t+1}=\argmin_{\mathbf{w} \in \mathcal{V}} \big\{\eta \ell_t(\mathbf{w})+\mathcal{D}_\psi\left(\mathbf{w}, \mathbf{w}_t\right)\big\},
    \end{align*}
    where $\mathcal{V} \subseteq \mathcal{W} \subseteq \mathbb{R}^d$ is a non-empty convex set. Further supposing $\psi(\mathbf{w})$ is 1 -strongly convex w.r.t. a certain norm $\|\cdot\|$ in $\mathcal{W}$, then there exists a $\mathbf{g}_t^{\prime} \in \partial \ell_t\left(\mathbf{w}_{t+1}\right)$ such that
    \begin{align*}
       \left\langle\eta_t \mathbf{g}_t^{\prime}, \mathbf{w}_{t+1}-\mathbf{u}\right\rangle \leq\left\langle\nabla \psi\left(\mathbf{w}_t\right)-\nabla \psi\left(\mathbf{w}_{t+1}\right), \mathbf{w}_{t+1}-\mathbf{u}\right\rangle 
    \end{align*}
    for any $\mathbf{u} \in \mathcal{W}$.
\end{myLemma}

\begin{myLemma}[{\citet[Lemma 1]{NeurIPS'23:MLogB}}]
    \label{lem:strongly-convex}
    Let $\ell(\mathbf{z}, y)=\sum_{k=0}^K \mathbf{1}\{y=k\} \cdot \log \left(\frac{1}{[\sigma(\mathbf{z})]_k}\right)$ where $\sigma(\mathbf{z})_k=\frac{e^{z_k}}{\sum_{j=0}^K e^{z_{j}}}$, $\mathbf{a} \in[-C, C]^K$, $y \in\{0\} \cup[K]$ and $\mathbf{b} \in \mathbb{R}^K$ where $C>0$. Then, we have
    \begin{align*}
        \ell(\mathbf{a}, y) \geq \ell(\mathbf{b}, y)+\nabla \ell(\mathbf{b}, y)^{\top}(\mathbf{a}-\mathbf{b})+\frac{1}{\log (K+1)+2(C+1)}(\mathbf{a}-\mathbf{b})^{\top} \nabla^2 \ell(\mathbf{b}, y)(\mathbf{a}-\mathbf{b}) .
    \end{align*}
\end{myLemma}
