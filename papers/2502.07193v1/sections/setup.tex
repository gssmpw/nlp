\section{Problem Setup}
\label{sec:setup}

In this section, we introduce the problem setup for RLHF.

Following recent advancements in RLHF~\citep{ICML'23:Zhu-Principled,ICML'24:Xiong-Iterative}, we formulate RLHF as a contextual bandit problem. Specifically, we have a set of contexts $\X$ and a set of possible actions $\A$ per context. To learn with human preference feedback, the learner selects a tuple $(x, a, a')$ to present to the human, where $x \in \X$ is the context, $a, a' \in \A$ are the actions. The human then provides a binary preference feedback $y \in \{0, 1\}$, where $y = 1$ indicates the human prefers action $a$ over action $a'$, and $y = 0$ otherwise. We study the commonly used Bradley-Terry (BT) model in preference learning~\citep{BT-model}, which assumes that the human's preference is generated by a logistic function of the difference in the rewards of the two actions.
\begin{myDef}[Bradley-Terry Model]
    \label{def:BT}
    Given a context $x \in \X$ and two actions $a, a' \in \A$, the probability of the human preferring action $a$ over action $a'$ is given by
    \begin{equation*}
        \P\left[y=1 \mid x, a, a^{\prime}\right]=\frac{\exp \left(r(x, a)\right)}{\exp \left(r(x, a)\right)+\exp \left(r\left(x, a^{\prime}\right)\right)},
    \end{equation*}
    where $r$ is a latent reward function.
\end{myDef}
We consider the linear reward model which is widely used in the RLHF literature~\citep{ICML'23:Zhu-Principled,arXiv'24:Ji-RLHF-active,arXiv'24:Das-RLHF-active}. 
\begin{myAssumption}
    \label{asm:linear-reward}
    It holds that $r(x, a) = \phi(x, a)^\top \theta^*$ where $\phi(x, a): \X \times \A \to \R^d$ is the known and fixed feature map, and $\theta^* \in \R^d$ is the unknown parameter vector. Furthermore, we assume $\norm{\phi(x, a)}_2 \leq L$ for all $x \in \X$ and $a \in \A$ and $\theta^* \in \Theta$ where $\Theta = \{\theta \in \R^d \mid \langle \bm{1}, \theta \rangle = 0, \|\theta\|_2 \leq B\}$.
\end{myAssumption}
\begin{myRemark}
    The feature mapping $\phi$ can be constructed by removing the last layer of a pre-trained large language model, and $\theta^*$ corresponds to the weights of the last layer. 
\end{myRemark}

Based on the linear model, we can rewrite the probability as
\begin{align}
    \label{eq:linear-BT}
    \P\left[y=1 \mid x, a, a^{\prime}\right] = \sigma\left(\phi(x, a)^\top \theta^* - \phi(x, a')^\top \theta^* \right),
\end{align}
where $\sigma(w) = \frac{1}{1 + \exp(-w)}$ is the sigmoid function. Next, we introduce a key quantity that captures learning complexity under the BT model.
\begin{myDef}
    \label{def:kappa}
    The \emph{non-linearity coefficient} $\kappa$ is defined as
    \begin{align}
        \kappa = \max_{x \in \X, a, a' \in \A} \max_{\theta \in \Theta} \frac{1}{\dot{\sigma}\left(\phi(x, a)^\top \theta - \phi(x, a')^\top \theta\right)},
        \label{eq:kappa}
    \end{align}
    where $\dot{\sigma}(w) = \sigma(w)(1 - \sigma(w))$ is the derivative function.
\end{myDef}

\begin{myRemark}
    \label{rmk:kappa}
    Through direct calculation, we obtain $\kappa \leq 2 + \exp(2BL) + \exp(-2BL)$. Thus, $\kappa$ may be exceedingly large with an exponential dependence on the magnitude of features and parameters, as justified in Appendix~\ref{subsec:detail_exp:kappa}.
\end{myRemark}

The RLHF pipeline can generally be divided into two distinct stages: the (post-)training stage and the deployment stage, each with different objectives. In the training stage, the learner aims to develop a high-performing model with minimal samples and computational resources. In the deployment stage, the trained model is used to generate good responses to user inputs, while simultaneously gathering informative feedback for further refinement. We provide a detailed discussion of both stages in the following sections.
