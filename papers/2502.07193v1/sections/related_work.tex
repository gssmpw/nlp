\section{Related Work}
\label{sec:related_work}

In this section, we review the works most closely related to ours.

\paragraph{Reinforcement Learning from Human Feedback.} Learning from human preferences with deep learning models dates back to~\citet{NIPS'17:RLHF} and has been recently popularized by the success of large language models~\citep{arXiv'22:Bai-RLHF, arXiv'23:llama-2, arXiv'23:GPT-4}. Existing results generally fall into two categories: reward-based and reward-free. Reward-based methods typically consist of two steps: reward modeling and reward maximization~\citep{NeurIPS'22:Ouyang-InstructGPT}. Given preference data, the reward model is trained to predict preference labels, and the LLM is fine-tuned based on the reward model using RL algorithms such as PPO~\citep{arXiv'17:PPO}. In contrast, reward-free methods directly optimize the LLM using human preference feedback, without relying on explicit reward modeling~\citep{NeurIPS'23:DPO, AISTATS'24:Azar-IPO}. Among these works, Direct Preference Optimization (DPO)~\citep{NeurIPS'23:DPO} is one of the most popular reward-free methods, which treats generative models directly as reward models and optimizes them using human preference feedback. In this work, we focus on the reward-based RLHF methods.

Given the empirical success of RLHF, recent efforts have been devoted to developing a deeper theoretical understanding of this approach. \citet{ICML'23:Zhu-Principled} investigated the standard \emph{offline} setting, where the learner is provided with a fixed dataset and aims to learn a policy that maximizes the expected reward. \citet{ICML'24:Xiong-Iterative} investigated the \emph{iterative} setting, where the learner randomly selects prompts from the candidate set, generates response pairs by sampling from the current policy, and then queries for feedback to update the policy accordingly. \citet{arXiv'24:Das-RLHF-active} studied the \emph{active} setting where the learner is allowed to select both prompts and responses, query for feedback, and update the policy accordingly. \citet{arXiv'25:Liu-Dual-active} focused on the \emph{dual active} setting where the learner is allowed to select conversations and teachers simultaneously for feedback. Essentially, these works focus on specific stages and settings within the RLHF pipeline, while we offer a unified view on the entire process. Furthermore, we propose algorithms that are both computationally and statistically more efficient than existing methods for each stage of the RLHF pipeline.


\paragraph{Contextual Dueling Bandits and RL.} Dueling bandits is a variant of the multi-armed bandit problem that incorporates pairwise comparison feedback~\citep{JCSS'12:K-armed-dueling-bandits}. Contextual dueling bandits further extend this framework by integrating context information~\citep{COLT'15:Dudik-Contextual-dueling}. In this setting, \citet{NeurIPS'21:Saha-Preference-bandits} studied the $K$-arm contextual dueling bandit problem and proposed an algorithm with a near-optimal regret guarantee. \citet{AISTATS'23:Saha-Dueling-RL} extended the contextual dueling bandit problem to the reinforcement learning setting, presenting an algorithm with a sublinear regret bound. \citet{NeurIPS'24:Sekhari-Contextual-bandits} further studied the contextual dueling bandit problem with active learning, where the learner is allowed to query for feedback adaptively and the goal is to minimize the regret while minimizing the number of queries simultaneously. However, these works primarily focus on the online setting, where the context is provided in an arbitrary and online manner and goal is to minimize cumulative regret. In this work, we consider the entire RLHF pipeline, including both training and deployment stages, and explore both passive and active data collection strategies in the training stage.


\paragraph{Active Learning.} Active learning is a well-established field that aims to reduce the labeling cost by selecting the most informative samples for annotation~\citep{09:Settles-Active-learning}. The first line of work focuses on the pool-based setting~\citep{92:Query-by-committee, MLJ'97:active, NISP'10:Huang-active}. In this setting, the learner strategically selects a batch of the most informative data at each step and queries labels only for this selected batch. The learner then uses this labeled batch to update the model. Once the model is enhanced, the learner selects another batch of data and queries labels for it, continuing the iterative training process. The other line of work focuses on the online active setting~\citep{COLT'24:Bianchi-active,JMLR'06:Bianchi-selective}. In this setting, the learner sequentially observes data points and decides whether to query labels for them. \citet{arXiv'24:Ji-RLHF-active} explored the online active RLHF setting where prompts are provided in an arbitrary and online manner, with the goal of minimizing cumulative regret while minimizing the number of queries simultaneously. In this work, we leverage the pool-based active learning strategy in the training stage of the RLHF pipeline, where the learner is allowed to select the dataset actively.
