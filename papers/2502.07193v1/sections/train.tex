\section{Training Stage}
\label{sec:train}
\vspace{-1mm}
In the training stage, the learner aims to acquire a high-performing model with minimal number of samples and computational resources. To effectively evaluate the model's performance, we define the following performance measure.

\paragraph{Performance Measure.}~~For any policy $\pi$, we define the sub-optimality gap compared to the optimal policy as follows:
\begin{equation}
  \subopt(\pi) = \mathbb{E}_{x \sim \rho}\left[r\left(x, \pi^{*}(x)\right)-r(x, \pi(x)\right],
\end{equation}
where $\rho$ is the context distribution, $\pi^{*}=\arg \max _a r(x, a)$ is the optimal policy under the true reward $r$. The goal of the learner is to minimize the performance gap $\subopt(\pi)$.

In the training stage, two data collection strategies are commonly used: passive and active. In passive setting, the learner is given a fixed set of samples. Conversely, in active setting, the learner can actively select queries for human feedback. We discuss both strategies below.

\subsection{Passive Data Collection}
\label{subsec:passive}

In the passive data collection setting, we are given a fixed dataset $\left\{\left(x_t, a_t, a_t^{\prime}, y_t\right)\right\}_{t=1}^{T}$ where $x_t \in \X$ is the context, $a_t, a_t^{\prime} \in \A$ are the actions, and $y_t \in \{0, 1\}$ is the feedback. Since the parameter $\theta^*$ is unknown, we need to estimate it from the available data. The most common method for this estimation is maximum likelihood estimation (MLE). First, let $z = \phi(x, a) - \phi(x, a')$ be the feature difference, then we define the negative log-likelihood $\ell_t(\theta)$ for sample $t$ as
\begin{align}
  \label{eq:log-likelihood}
  \ell_t(\theta) =  - y_t \log (\sigma(z_t^{\top} \theta))- \left(1-y_t\right) \log(1-\sigma(z_t^{\top} \theta)).
\end{align}
The MLE estimator $\thetah_{T+1}$ is then defined as
\begin{align}
  \label{eq:mle}
    \thetah_{T+1}=\argmin_{\theta \in \R^d}  \mathcal{L}_{T+1}(\theta)\triangleq \sum_{t=1}^T \ell_t(\theta).
\end{align}
Several works~\citep{ICML'23:Zhu-Principled,arXiv'24:Ji-RLHF-active} have analyzed the statistical properties of MLE and showed that for any $\delta \in (0, 1)$, with probability at least $1 - \delta$, the MLE estimator $\thetah_{T+1}$ converges to the true parameter $\theta^*$ by
\begin{align}
  \label{eq:confidence-set-mle}
  \norm{\thetah_{T+1}-\theta^*}_{V_{T+1}} \leq \betah \triangleq \Ot  \big(\kappa\sqrt{d}\big),
\end{align}
where $V_{t} \triangleq \sum_{i=1}^{t-1} z_i z_i^{\top} + \lambda I$ is the feature covariance matrix, $\lambda$ is the regularization parameter.

For computational complexity, the MLE estimator does not have a closed-form solution, requiring iterative optimization techniques, such as gradient descent, to obtain an $\varepsilon$-accurate solution. As discussed in~\citet{AISTATS'22:Faury-Jointly}, optimizing the MLE typically requires $\O(\log(1/\varepsilon))$ iterations to reach the desired accuracy. Since the loss function is defined over all data samples, each iteration involves $\O(T)$ gradient computations. Given that $\varepsilon$ is typically set to $1/T$ to prevent it from dominating the overall estimation error, the total computational complexity is $\O(T \log T)$. 

Although the MLE estimator is a commonly used approach, we propose a novel algorithm that enhances both computational efficiency and sample efficiency compared to MLE, drawing inspiration from recent advancements in logistic bandits~\citep{AISTATS'22:Faury-Jointly, NeurIPS'23:MLogB} and multinomial logit MDPs~\citep{NeurIPS'24:MNLmdp}. First, we define the gradient $g_t(\theta)$ and Hessian $H_t(\theta)$ of the loss function $\ell_t(\theta)$ as
\begin{align}
  g_t(\theta) = \left(\sigma\left(z_t^{\top} \theta\right)-y_t\right) z_t,~~ H_t(\theta) = \sigmad\left(z_t^{\top} \theta\right) z_t z_t^{\top} . \label{eq:gradient-hessian}
\end{align}



\paragraph{Implicit OMD.} To improve the computational efficiency, \citet{AISTATS'22:Faury-Jointly} observed that the cumulative past log-loss is strongly convex and can therefore be well approximated by a quadratic function. Building on this observation, they proposed the following update rule:
\begin{align}
  \label{eq:implicit-omd}
  \thetab_{t+1}=\argmin_{\theta \in \Theta} \Big\{\ell_t(\theta)+\frac{1}{2 \eta}\left\|\theta-\thetab_t\right\|_{\Hb_t}^2\Big\},
\end{align}
where $\Hb_t=\sum_{i=1}^{t-1} H_i\left(\thetab_{i+1}\right)+\lambda I$ is the local norm, and $\eta$ is the step size. The optimization problem can be decomposed in two terms. The first term is the instantaneous log-loss $\ell_{t}(\theta)$, which accounts for the information of the current sample. The second consists of a quadratic proxy for the past losses constructed through the sequence $\{\thetab_{i}\}_{i\leq t}$. A key component is the design of the local norm $\Hb_t$, which approximates the Hessian matrix by $H_i\left(\bar{\theta}_{i+1}\right)$ at a \emph{lookahead} point $\bar{\theta}_{i+1}$. Such a Hessian matrix effectively captures local information and is crucial for ensuring statistical efficiency.

Although this formulation may appear computationally more efficient than MLE due to its one-pass data processing, the optimization problem in Eq.~\eqref{eq:implicit-omd} still lacks a closed-form solution. Consequently, it still requires $\O(\log(1/\varepsilon))$ iterations per sample to achieve an $\varepsilon$-accurate solution, leading to the same $\O(T \log T)$ computational complexity as MLE.

\paragraph{Standard OMD.} To enhance computational efficiency, we revisit the formulation in Eq.~\eqref{eq:implicit-omd} and observe that it essentially corresponds to the implicit OMD framework~\citep{IC'97:EG}. Consequently, a natural alternative is to replace this formulation with the standard OMD framework, which permits a closed-form solution and thus eliminates the need for iterative optimization. However, the standard OMD minimizes a first-order approximation of the loss function, which sacrifices several key properties compared to its implicit counterpart, as demonstrated by \citet{NeurIPS'20:Campolongo-IOMD}. Specifically, the standard OMD formulation updates using $\partial \ell_t(\theta_t)$, whereas the implicit OMD updates the algorithm approximately with the subsequent sub-gradient, $\partial \ell_t(\theta_{t+1})$. This distinction results in a notable gap in the convergence rates of the two methods. To this end, we propose to approximate the current loss $\ell_t(\theta)$ using a second-order Taylor expansion, drawing inspiration from~\citet{NeurIPS'23:MLogB}. We define the second-order approximation of the original loss function $\ell_t(\theta)$ at the current estimator $\thetat_t$ as $\ellt_t(\theta) = \ell_t(\thetat_t) + g_t(\thetat_t)^\top (\theta - \thetat_t) + \frac{1}{2} \norm{\theta - \thetat_t}_{H_t(\thetat_t)}^2$. Then, we replace the current loss $\ell_t(\theta)$ in Eq.~\eqref{eq:implicit-omd} with the approximation $\ellt_t(\theta)$, leading to the following update rule:
\begin{align}
  \label{eq:omd}
  \thetat_{t+1}=\argmin_{\theta \in \Theta} \Big\{\big\langle g_t(\thetat_t), \theta \big\rangle +\frac{1}{2 \eta}\big\|\theta-\thetat_t\big\|_{\Ht_t}^2\Big\},
\end{align}
where $\eta$ is the step size and $\Ht_t = \H_t + \eta H_t(\thetat_t)$ is the local norm with $\H_t \triangleq \H_t(\thetat_{i+1}) = \sum_{i=1}^{t-1} H_i(\thetat_{i+1}) + \lambda I$. 

The optimization problem in Eq.~\eqref{eq:omd} can be solved with a projected gradient step with the following equivalent formulation:
\begin{align*}
  \thetat_{t+1}^{\prime} = \thetat_t - \eta \Ht_t^{-1} g_t(\thetat_t), \quad \thetat_{t+1} = \argmin_{\theta \in \Theta} \norm{\theta - \thetat_{t+1}^{\prime}}_{\Ht_t}^2.
\end{align*}
Thus, the estimator $\thetat_{t+1}$ can be computed in an $\O(1)$ time complexity, leading to a total time complexity of $\O(T)$ for the entire procedure. This represents a significant improvement over the $\O(T\log T)$ complexity of the MLE estimator in Eq.~\eqref{eq:mle}, particularly considering that the number of samples $T$ is typically large in the training of LLMs. Furthermore, since the estimator processes the samples in a one-pass manner, it mitigates the memory burden associated with computing the gradient of the full dataset at each iteration. This makes it more suitable for large-scale applications, where memory and time efficiency are crucial.

It is important to note that the update rule in Eq.~\eqref{eq:omd} represents a special case of online mirror descent, specifically, it can be derived from the following general form:
\begin{align*}
  \thetat_{t+1}=\argmin_{\theta \in \Theta} \Big\{\big\langle g_t(\thetat_t), \theta \big\rangle +\frac{1}{\eta}\D_{\psi_t}(\theta, \thetat_t)\Big\},
\end{align*}
where $\psi_t(\theta) = \frac{1}{2}\norm{\theta}_{\Ht_t}^2$ is the regularizer and $\D_{\psi_t}(\theta, \thetat_t) = \psi_t(\theta) - \psi_t(\thetat_t) - \langle \nabla \psi_t(\thetat_t), \theta - \thetat_t \rangle$ is the Bregman divergence. Therefore, we can analyze the estimation error using the online mirror descent framework~\citep{book:Orabona-online-learning}. Specifically, we have the following lemma.

\begin{algorithm}[!t]
  \caption{Training Stage: Passive Data Collection}
  \label{alg:passive}
  \begin{algorithmic}[1]
      \REQUIRE Dataset $\left\{\left(x_t, a_t, a_t^{\prime}, y_t\right)\right\}_{t=1}^{T}$, regularization $\lambda > 0$
      \STATE Initialize $\thetat_1=0$, $\H_1 = \lambda I_d$.
      \FOR {$t=1,2, \ldots, T$}
      \STATE Define $g_t(\theta)$ and $H_t(\theta)$ as in Eq.~\eqref{eq:gradient-hessian}
      \STATE Update $\Ht_t = \H_t + \eta H_t(\thetat_t)$
      \STATE Compute $\thetat_{t+1}$ by Eq.~\eqref{eq:omd}
      \STATE Update $\H_{t+1} = \H_{t} + H_t(\thetat_{t+1})$
      \ENDFOR
      \STATE Compute $\Jt_T(\pi)$ by Eq.~\eqref{eq:pessimistic}
      \ENSURE Compute final policy  $\pi_T= \argmax_{\pi} \Jt_T(\pi)$
  \end{algorithmic}
\end{algorithm}

\begin{myLemma}
  \label{lem:confidence_set}
  Let $\delta \in(0,1]$, set $\eta=(1 / 2) \log 2+\left(BL+1\right)$ and $\lambda=84 \sqrt{2} \eta (dL^2 + BL^3)$, define the confidence set as
  \begin{align*}
    \mathcal{C}_t =\left\{\theta \in \Theta \mid\big\|\theta - \thetat_t\big\|_{\H_t} \leq \betat_t\triangleq \O \big(\sqrt{d}(\log (t / \delta))^2\big) \right\}.
  \end{align*}
  Then, we have $\operatorname{Pr}\left[\forall t \geqslant 1, \theta^* \in \mathcal{C}_t(\delta)\right] \geqslant 1-\delta$.
\end{myLemma}
Intuitively, the confidence set shrinks as the number of samples $t$ increases, owing to the growing local norm $\H_t$. The local norm $\H_t$ captures the directions explored by the samples. The more we explore in a particular direction, the more accurate our estimation becomes along that direction.

By the definition of $\kappa$ in Definition~\ref{def:kappa}, it holds that $\H_t \succeq  (1/\kappa) V_t$, Lemma~\ref{lem:confidence_set} implies $\forall t \in [T+1]$,
\begin{align}
  \label{eq:comparison-confidence-set}
  \norm{\thetat_t - \theta^*}_{V_t} \leq \sqrt{\kappa} \norm{\thetat_t - \theta^*}_{\H_t} \leq \Ot \big(\sqrt{\kappa d}\big).
\end{align}
Compared with the confidence set in Eq.~\eqref{eq:confidence-set-mle}, our confidence set strictly improves the estimation error by a factor of $\sqrt{\kappa}$. As highlighted in Remark~\ref{rmk:kappa}, the non-linearity coefficient $\kappa$ may be exceedingly large during the training of LLMs, which makes our estimator statistically more efficient.

\paragraph{Pessimistic Policy.} Based on the estimator in Eq.~\eqref{eq:omd}, we adopt the ``pessimism in the face of uncertainty'' principle and define the pessimistic value function as 
\begin{align}
  \Jt_T(\pi) = \min_{\theta \in \C_T} \mathbb{E}_{x \sim \rho}\left[(\phi(x, \pi(x)) - v)^\top \theta \right] = (\phib(\pi) - v)^\top \thetat_T  - \betat_T \norm{\phib(\pi) - v}_{\H_T^{-1}},\label{eq:pessimistic}
\end{align}
where $\phib(\pi) = \mathbb{E}_{x \sim \rho}[\phi(x, \pi(x))]$ is the feature expectation of policy $\pi$, and $v$ is the baseline feature vector. While $v$ does not affect the induced policy, it may help reduce the sub-optimality gap~\citep{ICML'23:Zhu-Principled}. The final policy is given by $\pi = \argmin_{\pi} \Jt(\pi)$. The procedure is present in Algorithm~\ref{alg:passive}. We show it enjoys the following guarantee.


\begin{myThm}
  \label{thm:passive}
  Let $\delta \in(0,1]$, set the parameters as in Lemma~\ref{lem:confidence_set}, with probability at least $1-\delta$, Algorithm~\ref{alg:passive} ensures the following sub-optimality gap:
  \begin{align*}
    {\subopt}(\pi_T) \leq \Ot \left(\sqrt{d} \cdot \bignorm{\mathbb{E}_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)}_{\H_{T}^{-1}} \right).
  \end{align*}
\end{myThm}
\begin{myRemark}
  The term $\bignorm{\mathbb{E}_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)}_{\H_{T}^{-1}}$ is referred to as the ``concentratability coefficient'', a widely used concept in offline learning~\citep{ICML'21:Jin-Pessimism,ICML'23:Zhu-Principled}. It measures the distribution shift between the optimal policy and the dataset distribution. A smaller concentratability coefficient value indicates that the dataset better covers the data distribution under the optimal policy, resulting in a smaller sub-optimality gap.
\end{myRemark}

\paragraph{Comparison with~\citet{ICML'23:Zhu-Principled}.} Our result improves upon their work in both statistical and computational efficiency. First, similar to Eq.~\eqref{eq:comparison-confidence-set}, Theorem~\ref{thm:passive} implies an $\Ot \big(\sqrt{\kappa d} \cdot \bignorm{\mathbb{E}_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)}_{V_{T}^{-1}}\big)$ sub-optimality gap, improving upon theirs by a $\sqrt{\kappa}$ factor. Moreover, our estimator has an $\O(T)$ time complexity, more efficient than their $\O(T \log T)$ complexity.

\vspace{-1mm}
\subsection{Active Data Collection}
In this part, we consider the active data collection setting, where the learner can actively select tuple $\{x_t, a_t, a'_t\}_{t=1}^T$ to present to the human for feedback $\{y_t\}_{t=1}^T$. 

\paragraph{Estimator Construction.} For each round $t\in[T]$, given $\left\{\left(x_i, a_i, a_i^{\prime}, y_i\right)\right\}_{i=1}^{t-1}$, we define the loss function $\ell_t(\theta)$ as in Eq.~\eqref{eq:log-likelihood}, then the most standard approach is to estimate the parameter $\theta^*$ by MLE, which is given by $\thetah_{t}=\argmin_{\theta \in \R^d} \sum_{i=1}^{t-1} \ell_i(\theta)$. However, as noted before, the MLE estimator leads to a time complexity of $\mathcal{O}(t \log t)$ \emph{per round}. Moreover, unlike the passive setting where the estimator is constructed only once, in the active setting, we need to construct the estimator in each round. This is prohibitively expensive for large-scale applications.

To improve the computational efficiency, we can leverage the OMD framework as in the passive data collection setting. Specifically, we construct the estimator $\thetat_t$ as in Eq.~\eqref{eq:omd} by
\begin{align}
  \label{eq:omd-active}
  \thetat_{t+1}=\argmin_{\theta \in \Theta} \Big\{\big\langle g_t(\thetat_t), \theta \big\rangle +\frac{1}{2 \eta}\big\|\theta-\thetat_t\big\|_{\Ht_t}^2\Big\}.
\end{align}

\begin{algorithm}[!t]
  \caption{Training Stage: Active Data Collection}
  \label{alg:active}
  \begin{algorithmic}[1]
      \REQUIRE Regularization $\lambda>0$, failure probability $\delta \in(0,1]$
      \STATE Initialize $\thetat_1=0$, $\H_1 = \lambda I_d$.
      \FOR {$t=1,2, \ldots, T$}
      \STATE Choose $(x_t, a_t, a_t^{\prime})$ by Eq.~\eqref{eq:active-query}, observe feedback $y_t$
      \STATE Define $g_t(\theta)$ and $H_t(\theta)$ as in Eq.~\eqref{eq:gradient-hessian}
      \STATE Update $\Ht_t = \H_t + \eta H_t(\thetat_t)$
      \STATE Compute $\thetat_{t+1}$ by Eq.~\eqref{eq:omd-active}
      \STATE Update $\H_{t+1} = \H_{t} + H_t(\thetat_{t+1})$
      \ENDFOR
      \STATE Construct $\rt_T(x, a) = \frac{1}{T} \sum_{t=1}^T \phi(x, a)^\top \thetat_t, \forall x, a$
      \ENSURE Compute policy $\pi_T(x) = \argmax_{a \in \A} \rt_T(x, a)$.
  \end{algorithmic}
\end{algorithm}

\paragraph{Query Selection.} Given the estimator $\thetat_t$, we choose the query with the maximum uncertainty inspired by~\citep{arXiv'24:Das-RLHF-active}. Specifically, we choose the tuple by
\begin{align}
  \label{eq:active-query}
  (x_t, a_t, a_t^{\prime}) = \argmax_{x, a, a^{\prime} \in \X \times \A \times \A} \left\{\bignorm{\phi(x, a) - \phi(x, a')}_{\H_t^{-1}}\right\}.
\end{align}
Intuitively, $\H_t$ captures the directions explored so far, the selection criterion essentially seeks the tuple whose feature difference is poorly covered by the past observations. 

\paragraph{Policy Output.} After $T$ rounds, we define the reward function as $\rt_T(x, a) = \frac{1}{T} \sum_{t=1}^T \phi(x, a)^\top \thetat_t$ as the average of all the past estimations. Then, the final policy is given by
\begin{align}
  \label{eq:active-policy}
  \pi_T(x) = \argmax_{a \in \A} \rt_T(x, a).
\end{align}
The detailed procedure is present in Algorithm~\ref{alg:active}. We show it enjoys the following guarantee.

\begin{myThm}
  \label{thm:active}
  Let $\delta \in(0,1]$, set the parameters as in Lemma~\ref{lem:confidence_set}, with probability at least $1-\delta$, Algorithm~\ref{alg:active} ensures the following sub-optimality gap:
  \begin{align*}
    \subopt(\pi_T)\leq \Ot \big(d \sqrt{{\kappa}/{T}}\big).
  \end{align*}
\end{myThm}
\begin{myRemark}
  In contrast to the passive data collection setting, the sub-optimality gap in Theorem~\ref{thm:active} does not depend on the concentratability coefficient term. Instead, it decays at a rate of $T^{-1/2}$. This improvement arises because the active setting enables our algorithm to actively select queries with the maximum uncertainty, ensuring the uncertainty across all directions is effectively controlled.
\end{myRemark}
\begin{myRemark}
  It is important to note that our result has a dependence on $\sqrt{\kappa}$. Though recent advances in the logistic bandits~\citep{AISTATS'21:Abeille-Instance-optimal, NeurIPS'23:MLogB} have shown that this dependence can be eliminated in the dominant term and shifted to lower-order terms, we believe this term remains necessary in our setting. This is due to the fact that the sub-optimality gap in our setting is defined as the difference between the reward $z^\top \theta^*$ while they focus the probability difference, i.e., $\sigma(z^\top \theta^*)$. This difference prevents us from removing the $\sqrt{\kappa}$ term in our setting.
\end{myRemark}

\paragraph{Comparison with~\citet{arXiv'24:Das-RLHF-active}.} Our algorithm attains the same sub-optimality gap as the work of \citet{NeurIPS'21:Saha-Preference-bandits}, but with a significantly improved computational efficiency. Specifically, our algorithm has a time complexity of $\O(T)$ for the total $T$ round, which is more efficient than the $\O(T^2 \log T)$ complexity of their MLE estimator. Moreover, our algorithm does not require storing the historical samples, resulting in only an $\O(1)$ storage complexity. In contrast, their MLE estimator necessitates storing all historical samples, leading to an $\O(T)$ storage complexity.
