\section{Introduction}
\label{sec:intro}

\begin{table*}[!t]
    \centering
    \caption{\small Comparison between previous works and approach in terms of the statistical and computational efficiency across different stages of the RLHF pipeline. Our pipeline consists of two stages: training and deployment, where we explore both passive and active data collection strategies in the training stage. The statistical efficiency is measured using the sub-optimality gap (for the training stage) and regret (for the deployment stage), while the computational efficiency is evaluated based on time and storage complexity. Here, $d$ is the dimension of the feature space, $T$ is the number of samples, $\kappa$ is the non-linearity coefficient which can be exponentially large, $\E_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)$ is the concentratability vector, $V_{T}$ and $\H_{T}$ are two kinds of local norm that satisfies  $\H_T \succeq  (1/\kappa) V_T$.}
    \vspace{-2mm}
    \label{tab:result}
    \renewcommand*{\arraystretch}{1.25}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
    \textbf{Stage}        &\textbf{Data}                & \textbf{Reference} & \textbf{Sub-optimality Gap/Regret} & \textbf{Time} & \textbf{Storage} \\ \midrule
    \multirow{4}{*}{\raisebox{-2ex}[0pt][0pt]{Training}} & \multirow{2}{*}{Passive} & \citet{ICML'23:Zhu-Principled}     & $\Ot \Big(\kappa \sqrt{d} \cdot \bignorm{\mathbb{E}_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)}_{V_{T}^{-1}}\Big)$          & $\O(T\log T)$    & $\O(T)$  \\
                       &                   & Ours (Theorem~\ref{thm:passive})      & $\Ot \Big(\sqrt{d} \cdot \bignorm{\mathbb{E}_{x \sim \rho} \left(\phi(x,\pi^*(x))-v\right)}_{\H_{T}^{-1}}\Big)$          & $\O(T)$    & $\O(T)$  \\ \cmidrule{2-6} 
                       & \multirow{2}{*}{Active} & \citet{arXiv'24:Das-RLHF-active}     & $\Ot \big(d \sqrt{\kappa / T}\big)$          & $\O(T^2 \log T)$    & $\O(T)$  \\
                       &                   & Ours (Theorem~\ref{thm:active})      & $\Ot \big(d \sqrt{\kappa / T}\big)$         & $\O(T)$   & $\O(1)$ \\ \midrule
    \multirow{2}{*}{Deployment} & \multirow{2}{*}{Online} & \citet{NeurIPS'21:Saha-Preference-bandits}     & $\Ot \big(d \kappa\sqrt{T}\big)$         & $\O(T^2 \log T)$   & $\O(T)$  \\
                       &                  & Ours (Theorem~\ref{thm:deploy})      & $\Ot \big(d \sqrt{\kappa T}\big)$         & $\O(T)$   & $\O(1)$  \\ \bottomrule
    \end{tabular}
    }
\end{table*}


Reinforcement Learning from Human Feedback is a key technique for training large language models using human feedback~\citep{NeurIPS'22:Ouyang-InstructGPT,arXiv'22:Bai-RLHF}. The RLHF process involves collecting data, each consisting of a prompt, a pair of responses, and a human preference label indicating the preferred response. Then, a reward model is trained to predict human preferences, and the LLM is fine-tuned based on the reward model by RL algorithms, such as PPO~\citep{arXiv'17:PPO}.

Given the notable success of RLHF, recent efforts have been devoted to developing a deeper theoretical understanding of this approach. \citet{ICML'23:Zhu-Principled} investigated the standard \emph{offline} setting, where the learner is provided with a fixed dataset and aims to learn a policy that maximizes the expected reward. In this setting, since the learner has no control over the data collection process, the quality of the dataset becomes crucial to the performance of the learned policy and the resulting policy often performs poorly when faced with out-of-distribution data~\citep{ICML'24:Burns-Weak-to-Strong}. In practice, the Claude~\citep{arXiv'22:Bai-RLHF} and LLaMA-2~\citep{arXiv'23:llama-2} projects have demonstrated that iterative RLHF can significantly enhance model performance. Motivated by these practical advancements, \citet{ICML'24:Xiong-Iterative} investigated the \emph{iterative} setting, where the learner randomly selects prompts from the candidate set, generates response pairs by sampling from the current policy, and then queries for feedback to update the policy accordingly. They showed that RLHF benefits from this iterative data collection process. While uniform prompt sampling as a simple approach has been proven effective, \citet{arXiv'24:Das-RLHF-active} demonstrated that it can lead to inefficient sampling and the sub-optimality gap may fail to decrease as the number of samples increases. To this end, they studied the \emph{active} setting where the learner is allowed to select both prompts and responses, query for human feedback, and update the policy accordingly. 

Though these works have provided valuable insights into various stages and settings of RLHF, a comprehensive theoretical understanding of the entire RLHF pipeline remains lacking. Towards this end, in this work, we propose a unified framework for the RLHF pipeline from the view of contextual bandits. Specifically, building on recent theoretical advancements~\citep{ICML'23:Zhu-Principled, ICML'24:Xiong-Iterative}, we leverage the Bradley-Terry preference model with a linearly parameterized reward function to reformulate RLHF as a contextual bandit problem~\citep{ICML'99:Abe-contextual-bandits}. We then decompose the RLHF process into two distinct stages: (post-)training and deployment. In the training stage, the learner's goal is to develop a high-performing model with minimal sample queries and computational resources, where we explore both passive and active data collection strategies. In the deployment stage, the trained model is used to select good responses to user inputs while concurrently gathering informative feedback from the user for further refinement.

Building on this unified view, we revisit the existing theoretical advancements in RLHF and observe that they primarily focus on specific stages and settings of the overall pipeline. Consequently, an understanding of the entire pipeline remains lacking. Furthermore, while they have proposed various algorithms with theoretical guarantees for different stages of RLHF, the high statistical and computational complexity of their methods presents a significant challenge for practical applications. Thus, a natural question arises:
\begin{center} 
    \textit{Can we develop provably efficient algorithms with improved statistical and computational efficiency for the RLHF pipeline?}
\end{center}
In this work, we answer this question affirmatively by proposing novel algorithms for each stage of the RLHF pipeline, improving both statistical and computational efficiency over existing methods. Specifically, our contributions are summarized as follows:
\begin{itemize}
    \item We propose a unified framework for the RLHF pipeline from the perspective of contextual bandits. We decompose the RLHF process into two stages: (post-)training and deployment, and explore both passive and active data collection strategies during training. This perspective allows us to view several prior works as focusing on specific components of the pipeline.
    \item We propose algorithms that are computationally and statistically more efficient than existing methods for each stage of the RLHF pipeline. Specifically, we employ online mirror descent with a tailored local norm to replace the standard maximum likelihood estimation. Additionally, we design distinct query criteria for different stages. Table~\ref{tab:result} summarizes the comparison between previous works and ours about statistical and computational efficiency.
    \item We train and deploy \texttt{Llama-3-8B-Instruct} on the \mbox{Ultrafeedback-binarized} dataset using our method, testing both passive and active data collection strategies in the training stage. The empirical results validate our theoretical findings and confirm the effectiveness of our method.
\end{itemize}

\paragraph{Organization.} The remainder of this paper is organized as follows. We review related work in Section~\ref{sec:related_work} and introduce the problem setup in Section~\ref{sec:setup}. Our pipeline is presented in Sections~\ref{sec:train} and~\ref{sec:deploy}, focusing on the training and deployment stages, respectively. We present experiments in Section~\ref{sec:experiment} and conclude in Section~\ref{sec:conclusion}. All proofs and experiment details are deferred to the appendix.

\paragraph{Notations.} For a vector $x \in \R^d$ and a positive semi-definite matrix $A \in \R^{d \times d}$, we define $\norm{x}_A = \sqrt{x^\top A x}$. We use the notation $\O(\cdot)$ to highlight various dependencies depending on the context. For the statistical efficiency, $\O(\cdot)$ omits only constant factors. For computational efficiency, $\O(\cdot)$ solely emphasizes the dependence on the number of samples as this is the primary factor influencing the complexity. Additionally, we employ $\Ot(\cdot)$ to hide polylogarithmic factors.


