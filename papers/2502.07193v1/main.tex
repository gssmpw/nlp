\pdfoutput=1
\documentclass[11pt]{article}
\usepackage{fullpage}
\input{command}
\hypersetup{
    colorlinks,
    breaklinks,
    urlcolor = black,
    linkcolor = blue,
    citecolor = blue,
}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{footmisc}
\usepackage{caption}

\begin{document}

\title{Provably Efficient RLHF Pipeline: \\ A Unified View from Contextual Bandits}

\author{Long-Fei Li$^*$, Yu-Yang Qian$^*$, Peng Zhao, Zhi-Hua Zhou}
\affil[]{
  National Key Laboratory for Novel Software Technology, Nanjing University, China\\
  School of Artificial Intelligence, Nanjing University, China,\\
  \texttt{\{lilf, qianyy, zhaop, zhouzh\}@lamda.nju.edu.cn}
  }
\date{}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution.}
\renewcommand{\thefootnote}{\arabic{footnote}} 
\setcounter{footnote}{0}

\maketitle

\begin{abstract}
    Reinforcement Learning from Human Feedback (RLHF) is a widely used approach for aligning Large Language Models (LLMs) with human preferences. While recent advancements have provided valuable insights into various stages and settings of RLHF, a comprehensive theoretical understanding of the entire RLHF pipeline remains lacking. Towards this end, we propose a \emph{unified} framework for the RLHF pipeline from the view of \emph{contextual bandits} and provide provable efficiency guarantees. In particular, we decompose the RLHF process into two distinct stages: \mbox{(post-)training} and deployment, exploring both passive and active data collection strategies during the training phase. By employing the Bradley-Terry preference model with a linearly parameterized reward function, we reformulate RLHF as a contextual preference bandit problem. We then develop novel algorithms for each stage, demonstrating significant improvements over existing approaches in both statistical and computational efficiency. Finally, we apply our method to train and deploy \texttt{Llama-3-8B-Instruct} on the \mbox{Ultrafeedback-binarized} dataset, and empirical results confirm the effectiveness of our approach.
\end{abstract}

\input{sections/intro.tex}
\input{sections/related_work.tex}
\input{sections/setup.tex}
\input{sections/train.tex}
\input{sections/deploy.tex}
\input{sections/experiment.tex}
\input{sections/conclusion.tex}


% \newpage
\bibliographystyle{plainnat}
\bibliography{rlhf}

\newpage
\appendix
\input{sections/appendix.tex}
\input{sections/detail_exp.tex}

\end{document}

