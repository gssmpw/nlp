\section{Evaluation}
\label{sec:autolike-prelim-results}

\begin{table}[t!]
	\centering
        \caption{\textbf{TikTok Datasets}. We collect and utilize two datasets. For each TikTok, we collect its video (.mp4), timestamp, duration, location, text description, content creator information, and counts of liking, comments, shares, and plays. For \autolikedatasetone{}, we bold hashtags that are shown in Fig.~\ref{fig:tiktok-multilabel}.}
	\begin{tabularx}{1\linewidth}{l l X r}
        \toprule
        & \rotatebox{0}{\parbox{2cm}{\textbf{Name}}} &
        \rotatebox{0}{\parbox{2.5cm}{\textbf{Description}}} & 
        \textbf{TikToks} 
        \\
        \midrule
        1 & \autolikedatasetone{} (Sec.~\ref{sec:classify-eval}) & Collected TikToks based on  24 hashtags $\times$ \tilda{}50 each. (\textbf{\#mentalhealth, \#hatespeech, \#ed, \#foryou}, \#drugabuse, \#disorder, \#violence, \#sexualization, \#depressed, \#bodyimage, \#suicide, \#discrimination, \#cyberbullying, \#bingeeating, \#fastloseweight, \#purging, \#diet, \#fasting, \#twsh, \#twed, \#sh, \#shtwt,  \#shawareness, \#selfhm)  & \tilda{}1200  \\
        \midrule
        2 & \autolikedatasettwo{}  (Sec.~\ref{sec:tiktok-eval}) & Runs of \autolike{}: 8 experiments $\times$ \tilda{}150 TikToks per experiment. (``Controlled'', ``Pets'', ``Sports'', ``Sad'', ''Sad Pets'', ``Sad Cats'', ``Sad Weather'', ``Sad Mental Health'') & \tilda{}1200  \\
        \bottomrule
	\end{tabularx}
	\label{tab:tiktok-prelim-datasets}
	\vspace{-5pt}
\end{table}

\begin{figure*}[t!]
	\centering
\includegraphics[width=1\textwidth]{images/classified_tiktoks_multiclass_fc50533a_descr_trans_broad_trunc.pdf}
	\caption{{\textbf{Zero-shot Classification.} We treat the x-axis as the topic of interest (9) and the \#hashtag as the ground truth. The y-axis is the classification confidence score that a TikTok is on-topic or related to the topic of interest. For example, for the left-most subplot, we expect when ``mental health'' is given as the topic of interest, that the majority of TikToks will have higher confidence scores since the TikToks were collected using the hashtag \#mentalhealth. Expected results: ``discrimination'' should have higher confidence scores classifying TikToks from \#hatespeech, ``eating disorder'' for \#ed, and all topics of interest are low confidence for \#foryou.
    Overall, we evaluated 24 different hashtags. 
        }}
	\label{fig:tiktok-multilabel}
\end{figure*}

This section uses TikTok as our case study and evaluates \autolike{} in two parts. First, in Sec.~\ref{sec:classify-eval}, we assess our classification approach (Fig.~\ref{fig:autolike-framework-impl} step 3-4), which is crucial to automating \autolike{}. 
Second, in Sec.~\ref{sec:tiktok-eval}, we deploy a simplified version of \autolike{} to demonstrate how it can drive TikTok's \rs{} to different topics of interest and negative sentiments. Table~\ref{tab:tiktok-prelim-datasets} summarizes the datasets that we collected and utilized in this section.



\subsection{Classification of TikTok Content}
\label{sec:classify-eval}

In Sec.~\ref{sec:states-impl}, we propose using OpenAI's Whisper~\cite{openaiwhisper} to extract text from the audio of the TikTok (Fig.~\ref{fig:autolike-framework-impl} step 3) and Facebook's ``bart-large-mnli'' model~\cite{bart-large-mnli} for zero-shot classification (Fig.~\ref{fig:autolike-framework-impl} step 4). %

\descr{Experiment Setup.} We collect TikToks from 24 valid hashtags using ``tiktok.com/tag/[hashtag]'', motivated by Table~\ref{tab:audit-topics} (\eg{} \#mentalhealth, \#violence, \#bodyimage) and listed in Table~\ref{tab:tiktok-prelim-datasets} row 1. 
For each hashtag, we collect the first 50 TikToks. For comparison, we collect the random set of TikToks shown in a \fyp{} page (\#foryou). Overall, we obtain \tilda{}1.2K TikToks, referred to as \autolikedatasetone{}. 
We follow the methodology outlined in Sec.~\ref{sec:states-impl} and classify the content across nine different topics of interest. Since they can be broad, we evaluate how challenging it would be to classify them using TikToks from different but related topics. For example, ``mental health'' is a broad term and can potentially relate to TikToks collected from \#ed and \#depressed. Thus, to automate this evaluation, we leverage the hashtag as the ground truth. Note that before classification, we remove certain hashtags to avoid relying on exact matches. In addition, we support our automated evaluation by conducting a manual evaluation of sampled TikToks. 

\descr{Results (Automated).}
For brevity, we show the results of the classification for four hashtags in Fig.~\ref{fig:tiktok-multilabel}. 
We consider the results as performing well when more than 50\% of TikTok content has $> 0.5$ confidence scores between the expected topic of interest and the corresponding hashtag.
Specifically, we find that the classification performs well for topics of interest, such as ``mental health'' to TikToks from \#mentalhealth, ``discrimination'' to \#hatespeech, ``physical violence'' to \#violence, ``adult content'' to \#sexualization, and ``eating disorder'' to \#bingeeating. However, it does not work well for ``eating disorder'' for \#ed content, here our classifier has more confidence labeling it as ``mental health'' instead. Similarly, ``self-harm'' does not work well for TikTok from \#self-hm, \#sh, and \#shawareness. Our findings show that that certain content related to ``eating disorder'' and ``self harm'' requires additional work to improve classification performance, such as fine-tuning the models. We note that \autolike{} is agnostic of the implementation, and thus can be integrated with better ML models as they become available. This is not surprising, as topics of interest concerning problematic content are complex and should be evaluated before integrating it with \autolike{}. Future users of \autolike{} can leverage the current methodology shown in this section for efficient evaluation.




\begin{table*}[t!]
    \centering
    \caption{\textbf{Manual Verification.} We create a ground truth dataset (a subset of \autolikedatasetone{}) for TikTok to conduct manual verification for zero-shot topic (mental health \vs{} other) and sentiment (positive \vs{} negative) classification~\cite{bart-large-mnli}. 
    In addition, we confirm the audio-to-text~\cite{openaiwhisper} transcription, which we label as correct or not. \faPencil* = text description,  \faMusic{} = text from audio.}
	\begin{tabularx}{0.75\linewidth}{l l l r r r r}
	\toprule
     &
        \textbf{Label} & \textbf{Classify} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{F1--score} \\
\midrule
\multirow{6}{*}{\rotatebox{90}{\textbf{Topic}}} & 
Mental health & \faPencil* & 0.78 & 0.86 & 0.81 & 0.82 \\
& Other & \faPencil* & 0.84 & 0.76 & 0.81 & 0.80 \\
& Mental health & \faMusic & 0.71 & 0.56 & 0.66 & 0.62 \\
& Other & \faMusic & 0.63 & 0.77 & 0.66 & 0.70 \\
& Mental health & \faPencil* + \faMusic & 0.77 & 0.87 & 0.81 & 0.82 \\
& Other & \faPencil* + \faMusic & 0.85 & 0.74 & 0.81 & 0.79 \\
\midrule 
\multirow{7}{*}{\rotatebox{90}{\textbf{Sentiment}}} 
    & Negative & \faPencil* & 0.82 & 0.86 & 0.81 & 0.84 \\
    & Positive & \faPencil* & 0.80 & 0.74 & 0.81 & 0.76 \\
    & Negative & \faMusic & 0.64 & 0.75 & 0.62 & 0.69 \\
    & Positive & \faMusic & 0.56 & 0.43 & 0.62 & 0.49 \\
    & Negative & \faPencil* + \faMusic  & 0.78 & 0.88 & 0.78 & 0.82 \\
    & Positive & \faPencil* + \faMusic  & 0.80 & 0.66 & 0.78 & 0.72 \\
\midrule 
& Transcription & \faMusic & -- & -- & 0.75 & -- \\
	\bottomrule
	\end{tabularx}
        
	\label{tab:tiktok-manual}
\end{table*}

\descr{Results (Manual Verification).} 
To further confirm our classification results, we conduct manual evaluation for a sample of our TikToks, as shown in Table~\ref{tab:tiktok-manual}. This evaluation corresponds to the case where we do not rely on hashtags as our ground truth.
For simplicity, we focus on verifying mental health content \vs{} others (\ie{} not mental health). To manually label this set, we review/watch the videos, read video descriptions, and text from the audio. In the end, we created a balanced ground truth of 63 mental health TikToks and 63 others. 
In addition, we verify the transcription results of audio-to-text (from OpenAI Whisper model~\cite{openaiwhisper}) and sentiment classification. For the transcription, we evaluate it as a binary result, whether it is correct or not.
For the sentiment, we consider negative sentiment as content related to anger, yelling, sadness, and violence, while positive as content related to awareness, pranks, laughing, \etc{}. 
Overall, we find that classification performs well when using the video description alone and has a negligible dip in the F1-score when also considering the text from audio. This can be caused by TikToks that have music (and singing) that transcribes text that is not relevant to the topic or sentiment. %
However, we note that not all TikToks will have a video description, which is provided at-will by the content creator. Thus, we recommend using both modalities of video description and text from audio for classification.



\subsection{Deploying \autolike{} on TikTok's ``For You'' Page}
\label{sec:tiktok-eval}

\descr{Real-world Limitations.}
Running \autolike{} on TikTok using our RL implementation requires processing thousands and thousands of TikToks per topic of interest and sentiment and per hyper-parameter (\eg{} testing of different reward functions, time horizons, RL policies). As an estimate, if we take 30 seconds to process each iteration of Fig.~\ref{fig:autolike-framework-impl} (steps 1--7), which may include watching part of the TikTok, this requires around 8 hours at a minimum for 1000 TikToks. In practice, we run into other issues, such as TikTok preventing us from watching more videos due to bot detection or thinking that the user has overused TikTok. We discuss potential ways to address this in future directions.



\begin{figure}[t!]
\centering
    \subfigure[\textbf{Topic.} The ``Pets'' and ``Sports'' experiments represent benign  topics of interest. We demonstrate that the \autolike{} can drive the \rs{} to serve content based on the topic of interest. It serves 2--3$\times$ as much on-topic content as the controlled experiment.]{
		 \includegraphics[width=0.4\columnwidth]{images/timeline_cumsum_actions_Pets_Sports.pdf}
        \label{fig:pets-sports-control}
	}
    \hspace{3mm} 
    \subfigure[\textbf{Sentiment.} The ``Sad'' sentiment represents negative sentiments. We see that there is no discernible difference when compared to the controlled experiment. We surmise that TikTok's \rs{} prioritizes the topic of the content rather than its sentiment.]{
		 \includegraphics[width=.4\columnwidth]{images/timeline_cumsum_actions_Pets_Sad.pdf}
        \label{fig:pets-sad-control}
	}
 \subfigure[\textbf{Pets TikToks.} Example thumbnails of TikToks that we liked in Fig.~\ref{fig:pets-sports-control}.]{
		 \includegraphics[width=0.4\columnwidth]{images/pets_videos_thumbnails_threshold0.5.jpg}
        \label{fig:pets-thumbnail}
	}
    \hspace{3mm}
    \subfigure[\textbf{Sad TikToks.} Example thumbnails of TikToks that we liked in Fig.~\ref{fig:pets-sad-control}.]{
		 \includegraphics[width=.4\columnwidth]{images/sad_videos_thumbnails_threshold0.5.jpg}
        \label{fig:sad-thumbnail}
	}
    \caption{{\textbf{\autolike{} for Single Dimensions: TikTok.} We demonstrate how \autolike{} can drive TikTok's \rs{} across individual dimensions, either for topic of interest only or for sentiment only. 
    The y-axis denotes the number of times we liked the TikTok video (\ie{} ones we classify as on-topic or on-sentiment with confidence scores $> 0.5$). We provide thumbnail previews of TikToks at Fig.~\ref{fig:pets-thumbnail} and Fig.~\ref{fig:sad-thumbnail}.
    }}
    \vspace{-10pt}
     \label{fig:like-experiments}
\end{figure}



\begin{figure}[t!]
\centering
    \subfigure[\textbf{Topic and Sentiment.} We demonstrate how \rs{} serves content across different topics of interest and negative sentiment, and compare them to the controlled experiment. We additionally post-process each experiment for their negative sentiment only, labeled as dashed lines for ``Sad'', \eg{} solid blue ``Sad Cats'' considers both dimensions while dashed blue ``Sad'' is from the same experiment but classified for negative content only.
   The solid lines illustrate that the \rs{} serves 1.5--2$\times$ as much content for both topic and sentiment as the controlled experiment. The dashed lines show that the \rs{} serves ``Sad'' content compared to the controlled (black dashed), but depends on the topic. 
        ]{
	\includegraphics[width=0.46\columnwidth]{images/timeline_cumsum_actions_pretraining.pdf}
        \label{fig:pretraining-exp}
	}
    \hfill
    \subfigure[\textbf{Mental Health and Sad Sentiment.} We provide thumbnail examples of recommended content from our ``Sad Mental Health'' experiment from Fig.~\ref{fig:pretraining-exp}. As mental health is a broad topic, we discover a diverse range of issues displayed in the TikToks, such as body image, loss and death, depression, and self-harm. 
        ]{
		 \includegraphics[width=.46\columnwidth]{images/sad_mental_health_videos_thumbnails_threshold0.6.png}
        \label{fig:sad-mental-health-thumbnail}
	}
    \vspace{-10pt}
    \caption{{\textbf{\autolike{} for Multiple Dimensions: TikTok.} We demonstrate how \autolike{} can drive TikTok's \rs{} using both the topic of interest and sentiment. 
    The y-axis denotes the number of times we liked the TikTok video (\ie{} ones we classify as on-topic and on-sentiment with confidence scores $> 0.6$). We provide thumbnail previews of TikToks ``Sad Mental Health'' in Fig.~\ref{fig:sad-mental-health-thumbnail}.
    }}
     \label{fig:pre-training-and-sad-mental-health}
\end{figure}




\descr{Streamlining \autolike{}.} 
As a result, we streamline our \autolike{} experiments in the following ways. First, our action space considers two actions: either liking or skipping the content instead of five from Sec.~\ref{sec:autolike-agent}. Second, we use a policy of choosing the like action when our classification confidence is $> 0.5$. Third, we define a time horizon of at least 100 TikToks, with the end goal of liking at least four of the last 10 TikToks. We refer to this as a training phase. Lastly, to further examine how \rs{} changes over time, we will continue to scroll through the next 50 TikToks without applying any action. This allows us to study whether the \rs{} continues to serve related content to our inputs even if the user only skips the content. Thus, each experiment contains at least 150 TikToks. Note that we treat this streamlined process as a proof-of-concept that \autolike{} can drive a \rs{}, as we will further show in our results below.
Furthermore, we utilize a Pixel 3 Android phone and a TikTok account for each experiment. We refresh the \fyp{} using the methodology in Sec.~\ref{sec:autolike-env}, ensuring that each experiment acts as a new user. 

Below, we first develop a controlled experiment that can be re-used for comparison. We then design experiments that are motivated by the potential pathways shown in Fig.~\ref{fig:autolike-states-general}, to further understand how \autolike{} can drive \rs{} across individual and multiple dimensions. 
All experiments are contained within \autolikedatasettwo{} of Table~\ref{tab:tiktok-prelim-datasets}.


\descr{The Controlled Experiment.}
We build a controlled experiment by running the streamlined approach while changing the policy to only scrolling through 200 TikToks using a fresh \fyp{}. Here, we chose a time horizon of 200 to give flexibility so that the controlled experiment can compare to other experiments that may go beyond 150 TikToks. This ensures that we capture a \fyp{} that is not yet personalized. 
To make it comparable to other experiments, we apply post-processing of the collected TikToks, which allows us to re-use the controlled experiment. For instance, if the topic of interest is ``Pets'', we apply the classification for that specific topic to our controlled experiment. We utilized this controlled experiment throughout this section.



\descr{\autolike{} for Topic of Interest.} 
We first explore one dimension of driving \rs{} to deliver content for a topic of interest without sentiment. This matches the potential pathway shown in Fig.~\ref{fig:autolike-states-general} $P_{topic}$.
We conduct two experiments for benign topics of interest, ``Pets'' and ``Sports'', as illustrated in Fig.~\ref{fig:pets-sports-control} and thumbnail examples in Fig.~\ref{fig:pets-thumbnail}. 
We find that \autolike{} personalizes the content to the given topic compared to the controlled experiment. 
Specifically, at the \tilda{}100th time step, ``Pets'' have three times as much on-topic content than the control, while ``Sports'' doubles the control. During the testing phase, the \fyp{} maintains these rates. We expect personalization rates to change across different topics and experiments, but we should see a difference from the control experiment. 
Importantly, we demonstrate that we can drive the \rs{} to personalize on a given topic by solely interacting with \fyp{}.

\descr{\autolike{} for Sentiment.}
We now explore whether we can drive the \rs{} to deliver content for a given sentiment without a topic of interest. This matches the potential pathway shown in Fig.~\ref{fig:autolike-states-general} $P_{sentiment}$.  
We conduct one experiment, as illustrated in Fig.~\ref{fig:pets-sad-control}, focusing on the negative sentiment ``Sad'' (recall that all controlled experiments are re-used). We also choose to overall the ``Pets'' experiment from Fig.~\ref{fig:pets-sports-control} for comparison.
Interestingly, the ``Sad'' experiment has no discernible differences from the controlled one. Although Fig.~\ref{fig:sad-thumbnail} shows a variety of topics for TikTok that are sad in nature, the number of this type of content does not differ greatly than the ones from the controlled experiment. 
In addition, it took \tilda{}25 more steps to meet the stopping condition to achieve the end goal. 
The results show that driving the \rs{} to serve ``Sad'' sentiment is difficult. We conjecture that the \rs{} prioritizes the personalization of topics rather than sentiments. In other words, it may not make sense for TikTok's \rs{} to serve content without understanding specific topics of interest. 

\descr{\autolike{} for Topic and Sentiment.} 
We now explore both dimensions of driving \rs{} to deliver content for a topic of interest and sentiment. This matches the potential pathway shown in Fig.~\ref{fig:autolike-states-general} $P_{both}$.
We make the following additional changes. We increase the classification confidence score threshold to reduce noise, as discussed in Sec.~\ref{sec:classify-eval}. Furthermore, we now expand our experiments to four new experiments to include sad ``Cats'', ``Mental Health'', ``Pets'', and ``Weather''. 

Fig.~\ref{fig:pretraining-exp} provides several insights. First, we can drive the \rs{} to serve both a topic of interest and negative sentiment: it serves 1.5-2$\times$ more related content than the controlled experiment.
Second, when we post-process the experiment to examine whether the content served was overall negative sentiment in nature, we find that it serves more sad content but depends on the topic of interest. For instance, it shows that when using ``Sad Pets'', TikTok serves 1.4$\times$ more sad content overall. Compare this to ``Sad Cats'', which is a more specific topic, it serves around 2.2$\times$ more sad content than the controlled experiment. We see a similar pattern for ``Sad Weather''. We deduce that this may be either due to individual topics of interest or even their specificity (pets \vs{} cats). Future researchers can leverage \autolike{} to explore other topics of interest and sentiments for comparison.  


\descr{Mental Health and Sad Sentiment.} We center our analysis on the mental health experiment of Fig.~\ref{fig:pretraining-exp}. Once it reaches the end goal (time step 125), the \rs{} serves 2$\times$ as much relevant content than the controlled experiment. By the end, this goes up to 3$\times$. Importantly, we demonstrate that we can drive the \rs{} to serve harmful content using the \fyp{}. Fig.~\ref{fig:sad-mental-health-thumbnail} provides examples of the content found, which showcases a wide range of topics related to mental health with negative sentiments, such as body image, loss and death, depression, and self-harm. 


 