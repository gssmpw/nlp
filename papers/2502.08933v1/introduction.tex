\section{Introduction}
\label{sec:autolike-introduction}

Over the past decade, social media platforms, such as TikTok, Facebook, and YouTube, have relied on complex algorithms to personalize content recommendations. 
The content is often diverse, from typical popular content (\eg{} sports and pets) to heated debated content (\eg{} politics and gender identity)~\cite{Meta2025}. However, these algorithms can also spread misinformation (\eg{} false claims about vaccine treatments) and harmful content (\eg{} mental health, dangerous challenges)~\cite{Funke-covid-misinfo,arstech-parents-sue-tiktok}, emphasizing the need to study how algorithms can negatively impact users.  

Although social media platforms enjoy broad immunity under Section 230 of the Communications Decency Act (CDA) in the U.S. from being liable for user-generated content~\cite{section230} --- policymakers, nonprofit organizations, and researchers have raised alarms about systemic design issues in their recommendation algorithms. 
For example, in 2021, a U.S. Senate hearing with a whistleblower revealed that Facebook's algorithm prioritized user engagement but ``incite[d] misinformation, hate speech, and even ethnic violence''~\cite{Hao2021Jun}.
At the end of 2022, the Center for Countering Digital Hate (CCDH) reported that TikTok recommended content related to suicide, eating disorders, and mental health for accounts created for adolescents~\cite{tiktok-hate-ccdh}. Researchers have also studied how TikTok recommendations on eating disorders can lower self-esteem~\cite{Pruccoli2022Dec}, and how YouTube recommendations can lead users to radical content~\cite{haroon2022youtube,HosseinmardiYoutubeRadical}, such as creators who advocate for white supremacy~\cite{RibeiroYoutubePathways}.  

Addressing these concerns is highly nontrivial from a policy and a technical perspective. Platforms and regulators have attempted various approaches, such as moderating content and legislative actions. However, no agreed-upon solution has been found, leaving it primarily influenced by shifting politics.
For instance, in 2021, Meta removed content and adjusted its algorithm to demote certain COVID-19 content to reduce misinformation after pressure from the Biden administration~\cite{Luciano2021Jul}. However, in early 2025, Meta switched to a community notes model instead of removing content and discontinued the demotion of political content with the incoming Trump administration~\cite{Meta2025}. Furthermore, in 2022, after several children died in a TikTok Blackout Challenge, a U.S. district court determined that Section 230 shields TikTok from liability~\cite{arstech-parents-sue-tiktok}. However, by 2024, a U.S. appeals court overturned the decision, citing that TikTok's recommendation algorithm is its own ``expressive product'' protected under the First Amendment and not Section 230~\cite{wsj-tiktok-section230}. This opened the door for cases against TikTok to proceed\footnote{Notably, TikTok has been banned in several countries, with a potential TikTok ban in the U.S. under consideration as this paper is being written, albeit for national security reasons~\cite{Gordon2024Apr}.}.
Regardless of how policy and regulation on content and algorithms evolve over time, technical approaches are necessary to implement policy, \ie enable auditing of recommendation algorithms, inform platform developers and regulators with insights, and identify violations to enforce regulations.  %


\begin{table*}[t!]
	\centering
        \caption{\textbf{Examples of Topics.} A selection of topics for auditing from TikTok's community guidelines. Hashtags are extracted from our data collection in Sec.~\ref{sec:classify-eval} and prior work~\cite{Pruccoli2022Dec}. They demonstrate the duality of topics with positive and negative sentiments.}
	\begin{tabularx}{0.9\linewidth}{l l l l}
        \toprule
        & \textbf{Topic} & 
        \rotatebox{0}{\parbox{3cm}{\textbf{Hashtags (Negative)} }} 
        &  \rotatebox{0}{\parbox{3cm}{\textbf{Hashtags (Positive)} }} 

        \\
        \midrule 
        1 & Mental Health & \verb|#|psychwards,\verb|#|greif & \verb|#|gettinghelp, \verb|#|mentalhealthadvocate  \\
        2 & Self-harm & \verb|#|sh, \verb|#|selfsh, \verb|#|scars & \verb|#|shrecovery, \verb|#|tellmyselfimallright \\
        3 & Dangerous Challenges & \verb|#|A4waistchallenge & \verb|#|30dayschallenge  \\
        4 & Eating Disorder & \verb|#|ed, \verb|#|proana, \verb|#|purging & \verb|#|edrecovery, \verb|#|anarecoveryy \\
        5 & Physical Violence & \verb|#|abuse, \verb|#|brokenhome & \verb|#|againstviolence, \verb|#|selfdefense  \\   
        6 & Hate Speech & \verb|#|misogynists, \verb|#|discrimination & \verb|#|inclusionmatters, \verb|#|spreadlove  \\  
        7 & Cyberbullying & \verb|#|hatecomments, \verb|#|troll & \verb|#|antibullying, \verb|#|stopbullying \\ 
        8 & Adult Content  & \verb|#|misogyny, \verb|#|sexism & \verb|#|stopsexualizing, \verb|#|consent \\ 
        \bottomrule
	\end{tabularx}
	\label{tab:audit-topics}
\end{table*}




\textbf{The \autolike{} Framework.}
In this work, we develop a framework, \autolike{}, to automatically audit a social media recommendation algorithm and investigate how it delivers content to users based on user interactions with the platform\footnote{We focus on how user actions on the social media platform (\eg like, skip \etc) affect, or not, the content served to that user. The interests of the user are implicitly expressed through these interactions, not through some a priori ``statically defined interests,'' \eg{} by searching for the interest first without interacting with the \fyp{} or manually setting them within user profile settings. The two approaches can be combined in future work.\label{fn:user-actions}}.
Since this is admittedly a large space, we concentrate on the following canonical problem: we treat the algorithm as a recommendation system (RS) behaving as a ``For You'' Page (FYP), that streams recommended pieces of content, one by one, to a user. 
A user can interact with content through actions: if they enjoy the content, they can like, watch, and share it; otherwise, they can swipe to skip it. 
Within this scope, we envision an examiner who wants to audit the \rs{}, \eg{} whether it delivers harmful content to users by studying the sequence of actions taken upon the FYP and the types of harmful content that may be recommended. To do so, the examiner (i) selects a type of content of interest (\eg cats, or self-harm) and (ii) manually interacts with the \rs{} through the actions available to users, to drive the FYP towards delivering the content of interest.

\begin{figure*}[t!]
	\centering
\includegraphics[width=0.85\textwidth]{images/SocialRS_RLFramework2.pdf}
	\caption{{\textbf{\autolike{} Framework.} Formulated as a reinforcement learning problem, \autolike{} enables auditing of social media recommendation algorithms. Specifically, the user provides a topic of interest and a sentiment, which characterizes the content they want to audit. A RL agent interacts with the environment (\eg{} TikTok's ``For You'' page). At each time step, the agent follows a RL policy to select which action to apply to the current recommended content, then swipes to the next content. It receives a reward that reflects whether the algorithm recommends content related to the given inputs (\eg{} high rewards for on-topic content). It does this for a specified time horizon and learns over time which actions most efficiently drive the algorithm. The output of \autolike{} are pathways: sequences of recommended content and the actions taken upon them. The user can further analyze the pathways to understand the \rs{}.
    }}
	\label{fig:autolike-framework}
\end{figure*}


We formulate the above scenario as a reinforcement learning (RL) problem to efficiently learn which actions can lead the \rs{} to deliver a particular type of content, illustrated in Fig.~\ref{fig:autolike-framework}.
RL is a type of machine learning where an agent is trained to make optimal decisions in an environment (\eg{} TikTok) by interacting with it (\eg{} liking, skipping) and receiving feedback in the form of rewards. The RL agent learns a policy that maximizes its accumulated rewards over time through trial and error, effectively learning to choose the best actions in various situations. The RL formulation for our setup is provided in Section \ref{sec:autolike-framework}, but here is a preview. We define a state that captures at least the {\em topic of interest}, which can be any topic the examiner selects to audit. In addition, the state may have additional dimensions. Here, we consider the content's {\em sentiment}, which can be positive or negative.
We choose these two dimensions as they are inherent to any topic: \eg{} a topic such as eating disorder can be positive (\ie{} eating disorder recovery) or negative (\ie{} purging), as shown in Table~\ref{tab:audit-topics} along with other examples.
At each recommended content shown to the user, \autolike{} measures how closely it is to the topic of interest. The closer the content is related to the topic, the higher the reward and the more positive actions \autolike{} will take (\eg{} liking, watching). Conversely, the more off-topic the content is, the lower the reward and the more negative actions \autolike{} will take (\eg{} skipping).
\autolike{} can be applied to any social media recommendation platform where users interact with a FYP.


{\bf Case Study: Auditing TikTok.} 
We apply our framework to the TikTok platform. This serves both as a proof-of-concept evaluation of our framework and as a useful case study on its own due to TikTok's popularity and recent concerns of harmful content~\cite{Pruccoli2022Dec,harmful-content-hearing-2021,harmful-content-hearing-2023,Gordon2024Apr}. We implement \autolike{} specifically for Android devices and the TikTok app. We leverage state-of-the-art machine learning models~\cite{bart-large-mnli, openaiwhisper} to classify the topic and sentiment of the content across modalities (\eg{} text description, audio), enabling \autolike{} to run automatically. We evaluate the classification performance across nine topics of interest (\eg{} eating disorder, discrimination) using 24 different types of content (\eg{} purging, hate speech).
We develop a streamlined version of \autolike{} that can run on TikTok's \fyp{}. and conduct eight experiments that demonstrate how \autolike{} can drive TikTok's \rs{} automatically toward selected topics and sentiments. For instance, we find that \autolike{} can drive TikTok to serve 2$\times$ as much negative mental health content \vs{} a controlled experiment.


{\bf Potential Applications and Impact.}
\autolike{} can be a useful tool for policymakers,  platform developers, and researchers by enabling efficient auditing and by improving the transparency of how an algorithm delivers content.  For instance, a regulator\footnote{\eg{} the Federal Trade Commission (FTC), the European Commission (EC), the Competition and Markets Authority (CMA).} can deploy \autolike{} to discover whether the \rs{} serves harmful content on the platform, and how easy it is for a user to get that content from \rs{} (\eg{} using which actions, how many actions) or to avoid it (\eg is skipping enough?).  \autolike{} can help platform developers, by enabling them to efficiently discover harmful recommendations, thus informing the developers' policy and responsible algorithm design; \eg the platform can choose not to deliver a specific type of content and/or to specific account types (\eg for children).
 Furthermore, researchers can extend the framework for other social media platforms (or future ones) and/or explore additional dimensions of interest beyond just (topic, sentiment), \eg{} whether the content's intent was to inform or deceive the user, \etc 





The outline of the paper is as follows. 
Sec.~\ref{sec:autolike-framework} describes our \autolike{} framework. Sec.~\ref{sec:autolike-impl} provides our implementation on Android devices for the TikTok mobile app.
Sec.~\ref{sec:autolike-prelim-results} presents our evaluation for our TikTok case study.
Sec.~\ref{sec:background-related-work} describes the related work to auditing social media platforms. Sec.~\ref{sec:autolike-conclusion} concludes the paper with a summary and future directions.


