\section{The \autolike{} Framework}
\label{sec:autolike-framework}

Social media platforms, such as TikTok, stream recommended content one by one within \fyp{} by using \rs{}. The \fyp{} serves as the user interface, which dictates how a user views the content (one at a time) and how they interact with it; a TikTok example is shown in Fig.~\ref{fig:autolike-framework}.
The \rs{} is the underlying back-end that takes the user's interactions from the \fyp{} and decides which (new) recommended content is delivered to the user. These \rs{} are complex algorithms developed and maintained by the platform.

\descr{Formulation Overview.} 
Specifically, the user interacts with the content by applying actions, such as liking, watching, and sharing. If the user does not like the content, they can swipe to skip to the following recommended content. 
These interactions over time impact the \rs{} by informing the personalization of the content. This iterative action-response human process is a natural fit for reinforcement learning (RL).

Illustrated in Fig.~\ref{fig:autolike-framework}, \autolike{} mirrors this interaction from the user perspective. First, the user of \autolike{} provides inputs that characterize the type of content that the user wants to see. In this work, we utilize two dimensions: the topic of interest and the sentiment. Second, an intelligent entity, the RL agent, replaces the manual interactions of the human user,
and is tasked to learn how to efficiently drive the \rs{} to deliver content related to the given inputs within a time horizon. 
The set of actions is determined by the \fyp{} and is known to the RL agent. 
However, the effects of each action upon the \rs{} and the resulting recommended content are unknown to the RL agent.
The RL agent sequentially interacts with the environment (the \rs{} or \fyp{}): at each time step $t$ (up to some time horizon $T$), it follows a policy $\pi$ to select an action and then apply it to the environment. It receives a reward that measures how well the action took the \rs{} towards the content the user wants to see. 
The RL agent aims to maximize its cumulative reward over the time horizon $T$. Third, in the end, the output of \autolike{} are pathways: sequences of content and the actions the agent takes, which can be utilized for further analysis.



\subsection{Agent}
\label{sec:agent-framework}
In this section, we define the necessary components that the RL agent utilizes.

\descr{Actions ($a$) and Action Space.}
An action $a$ is an interaction that the RL agent can take upon a piece of content shown. 
For social media platforms, these actions are well-known to be within two categories: (1) ``positive'' actions denote the user enjoying the content; and (2) ``negative'' actions denote the user is disinterested in the content. 
For example, using TikTok as an example, positive actions include liking, watching, bookmarking, sharing, reposting, and following, while negative actions include skipping quickly and disliking.
In practice, subsets of actions may be available depending on the content shown and the platform. Thus, the action space is the set of actions that can be taken at each time step of the RL algorithm.
Despite this categorization, the RL agent does not know beforehand how each action will impact the \rs{}. It learns this over time as it interacts with the \fyp{}.


\descr{Policy ($\pi$).}
The RL agent follows a policy $\pi$ to select the action at each time step $t$. The policy enables the RL agent to efficiently drive the \rs{} to the end goal. 
It also assists the agent in balancing the trade-off between exploring the available actions to take \vs exploiting the best action known the time. Many policies can be used, \eg{}
 $\varepsilon$--greedy ($\varepsilon \in (0,1]$), which tells the agent to select an action $\varepsilon$ of the time randomly; otherwise, choose the best action that will give us the highest reward, defined by the state-action values $Q(s, a)$ across all possible actions~\cite{sutton2018reinforcement}. %
 States and state-actions are defined in the following section.
 
\subsection{Environment}
\label{sec:env-framework}

This section defines the necessary components on which the environment depends.

\descr{User Profiles ($u$).} 
Social media platforms often require users to create profiles for the algorithm to bootstrap (\ie{} which content to recommend to new users) and personalize (\ie{} track and learn the user's interactions over time to serve relevant content). Creating an account can also provide additional information about the user that can impact the algorithm, such as the user's age, gender, location, and interests. 



\begin{figure*}[t!]
\centering
    \subfigure[\small \textbf{Pathway Example} ]{
		 \includegraphics[width=0.40\linewidth]{images/RLFormulation_Dimensions_Example.pdf}
        \label{fig:autolike-states-example}
	}
    ~~~
    \subfigure[\small \textbf{Potential Pathways} ]{
		 \includegraphics[width=0.40\linewidth]{images/RLFormulation_DimensionsNew.pdf}
        \label{fig:autolike-states-general}
	}
		 %
    \vspace{-5pt}
    \caption{{\textbf{(a)} We provide a conceptual example of how \autolike{} can drive the \rs{} to an end goal (red box) across two dimensions: a topic of interest (eating disorder) and sentiment (negative/sad), using real TikTok content. The \rs{} starts at a benign state for a new user, serving popular common content, such as cats. It skips this content until it reaches more on-topic content, such as ones about dieting. During this time, it begins to like the content, and over time, drives the \rs{} to serve content related to eating disorder. With this example, the negative sentiment comes from the content creator discussing the hardship of recovery without support from family and friends.
    \textbf{(b)} We illustrate three potential pathways of using \autolike{}: (1) $P_{topic}$ shows how \autolike{} can drive the \rs{} to be on-topic without considering sentiment; (2) $P_{sentiment}$ shows driving the \rs{} towards negative sentiment content across different topics that are not on-topic; and (3) $P_{both}$ shows the intuitive use case of \autolike{}, driving it towards both on-topic and negative sentiment for auditing. $P_{both}$ matches the example within (a).
 }}
     \label{fig:autolike-states}
   \vspace{-10pt}
\end{figure*}


\descr{States ($s$) and the End Goal ($g$).}
Depicted in Fig.~\ref{fig:autolike-states}, a state $s$ represents the current environment relative to the topic of interest and sentiment (the inputs). In our case, this is the current recommended content shown. Specifically, we define $s$ using the exact two dimensions as our inputs: $\langle$topic, sentiment$\rangle$. Both are scores ranging from $\in [0,\ 0.1,\ 0.2,\ \dots{},\ 1]$ related to the current content shown. For the topic, zero means that it is not related to the topic of interest, one meaning it is very related to the topic of interest. For sentiment, we chose zero to denote positive sentiment (\ie{} happy content) and one meaning negative sentiment (\ie{} sad content). 

Recall that considering sentiment as a dimension of the state is motivated by the inherent meanings of topics, such as pro eating disorder (\ie{} negative) \vs{} eating disorder recovery (\ie{} positive)~\cite{Pruccoli2022Dec}, and demonstrated through hashtags in Table~\ref{tab:audit-topics}. The state can be defined to include any other dimensions of interest to the user of \autolike{}. 
In this paper, we focus on the negative aspects of recommended content, which is well-suited for the auditing of harmful content. Furthermore, the user designates a state as the end goal. Both the state and the end goal are utilized to calculate the reward after each action. 


\descr{Reward Function ($\mathcal{R}$).} The reward measures the effectiveness of the action at driving the \rs{} in serving content to the end goal. There may be many different reward functions that are effective for this problem. In this work, we suggest a proximity-based reward function, \ie{} the closer the \rs{} (represented by the state) gets to the end goal, the higher the reward.
To calculate the reward, we consider the next state $s_{t+1}$ after we apply an action $a$ to the current state $s$. The reward measures the distance between $s_{t+1}$ and the end goal $g$, and bounded $\in [0,1]$. To do so, we treat $s_{t+1}$ as the tuple $\langle$$s_{topic}$, $s_{sentiment}$$\rangle$ and the end goal as $\langle$$g_{topic}$, $g_{sentiment}$$\rangle$. The reward is then the following:
\begin{flalign} \label{eq:autolike-reward}
\begin{aligned}
    \mathcal{R}_{F} (s_{t+1},\  g) = 1 - \frac{distance(s_{t+1},\  g)}{ d_{max}}
\end{aligned}
\end{flalign}

Here, $distance(s_{t+1}, g)$ can be any distance metric, such as the Euclidean distance between $s_{t+1}$ and $g$. To normalize the distance between $[0, 1]$, we divide by the max distance that both points can be from each other. As shown in Fig.~\ref{fig:autolike-states}, since the state space is defined by two dimensions of $[0, 1] \times [0, 1]$, $d_{max}=\sqrt{2}$. We then subtract the normalized distance from 1 to give higher rewards to states that are closer to the end goal. 






\descr{State-Action Values ($Q(s, a)$).} 
\autolike{} utilizes state-action values, denoted as $Q(s, a)$. In short, they represent the expected reward earned when the RL agent in a specific state $s$ takes a specific action $a$. State-action values are utilized by RL policies to select which action to take at every time step $t$.
Using our states $s$, actions $a$, and reward function $\mathcal{R}$, we can approximate $Q(s, a)$ with temporal-difference (TD) learning, such as Q-learning~\cite{sutton2018reinforcement}. This is a well-suited approach, as it allows us to learn from raw experiences (\ie{} from interacting with \rs{} in a real-world setting) and does not require knowledge of the environment (\ie{} we treat \rs{} as black boxes). For instance, the state-action value is updated after applying an action, denoted as $Q_{t+1}(s, a)$, where the learning rate $\alpha \in (0, 1]$ and the discount rate $\gamma \in [0,1]$.
\begin{flalign} \label{eq:autolike-q-learning}
    Q_{t+1}(s_t,\ a_t) = Q_{t}(s_t,\ a_t) + \alpha [\mathcal{R}_t + \gamma\ max_a Q(s_{t+1},\ a) - Q(s_t,\ a_t) ]
\end{flalign}

Specifically, $\alpha$ determines how much $Q_{t+1}(s, a)$ is updated after each action; $\gamma$ specifies how much we care about future rewards (higher values place more weight on future rewards).


\descr{Output: Pathways ($P$).}
\autolike{} outputs pathways ($P$): sequences of recommended content and the action taken upon them, as shown in Fig.~\ref{fig:autolike-framework} and~\ref{fig:autolike-states}. The length of a pathway is determined by the time horizon $T$, which determines how many time steps are taken before \autolike{} stops. Recall that each time step represents the user viewing one recommended content.
This hyper-parameter is often on the scale of thousands for \autolike{} to converge and learn the best possible actions to be taken between some starting point of \rs{} to a specified end goal.

