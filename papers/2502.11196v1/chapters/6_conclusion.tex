\section{Conclusion}

In this paper, we present a novel perspective on new knowledge acquisition of LLMs through an investigation into the evolution of knowledge circuits throughout continual pre-training.
Through comprehensive analysis at performance, topology, and components levels, we reveal several key insights.
We believe these insights will contribute to more efficient and effective continual pre-training of LLMs, while also uncovering the mechanisms behind new knowledge acquisition in LLMs.
% Furthermore, these findings may inform improvements in model architectures, guiding the development of next-generation LLMs.

\section*{Limitations}
\paragraph{Model Architectures}
Our paper investigates the evolution of knowledge circuits solely in decoder-only Transformer LMs, due to their excellent performance and wide range of applications.
We omit other Transformer variants, such as encoder-decoder and encoder-only models, from our analysis.
Additionally, due to limitations in both computational resources and the circuit discovery method, we do not analyze models with larger parameter sizes than 1.3B, which typically employ Grouped Query Attention \citep{GQA}.
However, \citet{circuits_are_consistent} suggests that circuit analyses conducted on small models can provide insights that still apply over model scales.

\paragraph{Traininig Techniques}
We adopt the standard next-token prediction objective for continual pre-training of the base models in our experiments, as it is the most prevalent approach for enabling LLMs to acquire new knowledge.
However, numerous studies~\citep{pit,sft_injecting} focus on designing novel training techniques to enhance the efficiency and effectiveness of LLMs in acquiring new knowledge. 
We do not analyze the impact of these additional training techniques on the evolution of knowledge circuits.