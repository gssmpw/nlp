\section{Analyzing the Evolution of Knowledge Circuits throughout Training}

\begin{figure*}[htp]
    \centering
    \includegraphics[width=\linewidth]{figures/hit_at_10.png}
    \caption{\textbf{Hit@10} of the performance of knowledge circuits in GPT-2 Small, GPT-2 Medium and Phi-1.5 throughout training. Left: Performance for circuits discovered by different types of knowledge, where \textcolor[RGB]{148,103,189}{\texttt{K\_rel}} and \textcolor[RGB]{227,119,194}{\texttt{K\_compl}} represent \textcolor[RGB]{148,103,189}{relevant new knowledge} and \textcolor[RGB]{227,119,194}{completely new knowledge}, respectively. Right: Performance for circuits discovered by different frequencies of knowledge, where \textcolor[RGB]{44,160,44}{\texttt{Low-freq}}, \textcolor[RGB]{255,127,14}{\texttt{Medium-freq}}, and \textcolor[RGB]{31,119,180}{\texttt{High-freq}} represent knowledge with frequencies in the ranges  \textcolor[RGB]{44,160,44}{$[1, 2)$}, \textcolor[RGB]{255,127,14}{$[2,5]$} and \textcolor[RGB]{31,119,180}{$(5, 27]$}, respectively. Note that we smooth the curves using a window size of 3 epochs for all settings.}
    \label{fig:hit_at_10}
    \vspace{-10pt}
\end{figure*}

Once we have identified the knowledge circuits, we delve deeper into the changes within the circuits, examining the transitions in the roles and behaviors of nodes and edges.
Specifically, we conduct the analysis from three perspectives: \textit{performance}, \textit{topology}, and \textit{components}.

\subsection{Performance Analysis}
\label{sec:performance}
% \subsection{New Knowledge Related to the Pretraining Corpus is Easier to Acquire}

An identified knowledge should be capable of independently reproducing the behavioral patterns or performance of the whole model with respect to the corresponding tasks.
This property can be evaluated by examining whether the identified knowledge circuit aligns with the underlying algorithm implemented by the model.
Following \citet{knowledge_circuits}, we employ the Hit@10 metric to measure the rank of the target token among the top 10 predicted tokens throughout training process:
\begin{equation}
    \text { Hit@10 }=\frac{1}{|D_{\text{test}}|} \sum_{i=1}^{|D_{\text{test}}|} \mathrm{I}\left(\operatorname{rank}_a \leq 10\right)
\end{equation}
where $|D_{\text{test}}|$ denotes the test set size, $a$ the target attribute, and $\text{rank}_a$ the rank of the first token of target attribute $a$ in vocabulary space. 
% To evaluate completeness, we first identify the knowledge circuit using the validation set filtered by knowledge type and frequency as statetd in \S\ref{sec:circuit_discovery}, and then assess the identified circuit's standalone performance on a held-out test set, which is filtered by the same knowledge type and frequency as the validation set.
To evaluate completeness, we assess the identified circuit's standalone performance on a held-out test set, which is filtered by the same knowledge type and frequency as the validation set for circuit discovery.

The results depicted in Figure~\ref{fig:hit_at_10} reveal a consistent growth pattern in the Hit@10 metric until it approach its upper bound, which demonstrates the sustained knowledge acquistion capability of knowledge circuits throughout continual pre-training.
Notably, the $K_\text{rel}$ performance curve consistently lies above the curve for $K_\text{compl}$, suggesting that LLMs exhibit preferential learning efficiency when assimilating knowledge extensions within existing conceptual frameworks, as opposed to acquiring completely new knowledge.
These patterns persist in the whole model evaluation in Appendix \ref{app:performance}, suggesting that knowledge circuits capture general learning dynamics rather than isolated phenomena in LLMs.

\begin{tcolorbox}[mybox, title={Takeaway: Knowledge Relevance Principle}]
The acquisition of new knowledge is influenced by its relevance to pre-existing knowledge. LLMs exhibit learning efficiency advantages when acquiring relevant new knowledge versus completely new knowledge.
\end{tcolorbox}

% \begin{tcolorbox}[mybox, title={Takeaway: Knowledge Relevance Principle}]
% The acquisition of new knowledge is influenced by its relevance to pre-existing knowledge, with relevant new knowledge being integrated more efficiently than completely new knowledge.
% \end{tcolorbox}

This insight could motivate \textbf{the utilization of data curriculums in continual pre-training}, by organizing the data in a way that mimics the structure and distribution of the original corpus, thereby enabling the model to integrate new information more efficiently \citep{Yıldız_Ravichandran_Punia_Bethge_Ermis_2024,Parmar_Satheesh_Patwary_Shoeybi_Catanzaro_2024,Chen_Chen_Wang_Zhou_Zhu_Jiang_Min_Zhao_Dou_Mao_2024}.

Another notable observation in Figure \ref{fig:hit_at_10} is that the performance of knowledge circuits is positively correlated with knowledge frequency.
We further evaluate the performance of knowledge circuits by transferring them to a test set with different knowledge frequencies, as detailed in Appendix \ref{app:transfer_setting}.
The results imply that the poor performance of knowledge circuits for low-frequency knowledge may stem from insufficient knowledge representations, rather than fundamental capacity limitations of circuits.
This suggests that \textbf{strategies focused on reactivating long-tail knowledge}, such as knowledge augmentation, may improve knowledge retention in LLMs over time \citep{physics3.1}.

\subsection{Topology Analysis}
\label{sec:topology}

In this section, we examine the dynamics of knowledge circuits through a topological lens, employing graph-theoretical metrics to analyze how the circuit subgraphs evolve throughout the training process.

\subsubsection{Structural Consistency}

We first quantify the structural consistency of knowledge circuits by measuring the Jaccard Similarity between edge sets (Figure \ref{fig:similarity_entropy}) and node sets (Figure \ref{fig:nodes_similarity} in Appendix) within knowledge circuits at intermediate checkpoints relative to the final circuit.
Both metrics exhibit a consistent monotonic upward trend throughout training, indicating that the knowledge circuits become increasingly similar to the final circuit.
This convergence pattern suggests an evolutionary process where knowledge circuits progressively stabilize their core architecture as knowledge acquisition progresses.
Based on the observed trends, we hypothesize that the process of knowledge acquisition is driven by topological centralization within knowledge circuits, with a small subset of critical edges and nodes gaining dominance in the flow of information.

\subsubsection{Topological Centralization}

To validate the hypothesis, we define a knowledge circuit entropy metric quantifying edge importance concentration, drawing on the concepts of uncertainty and information content from probability theory and information theory.
The more centralized the topology of the knowledge circuit, the more the importance weights become concentrated on a few critical edges, resulting in a lower knowledge circuit entropy.
To calculate the entropy of a knowledge circuit $\mathcal{C}=<N_{\mathcal{C}},E_{\mathcal{C}}>$, we first normalize the absolute value of the importance of each edge $e\in E_\mathcal{C}$, scored by EAP-IG in equation \eqref{eq:eap-ig}:
\begin{equation}
    P(e)=\frac{S(e)}{\sum_{e^{\prime} \in E_\mathcal{C}} S(e^{\prime})}, \quad \forall e \in E_\mathcal{C}
\end{equation}
% The circuit entropy is then calculated as the sum of the negative logarithms of the probabilities of each normalized score, weighted by the probability of those scores:
The circuit entropy is then calculated as:
\begin{equation}
    H(\mathcal{C})=-\sum_{e\in E_{\mathcal{C}}} P(e)\log P(e)
\end{equation}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/similarity_entropy.png}
    \caption{Top: \textbf{Edges Jaccard Similarity} of intermediate knowledge circuits with the circuits at the final checkpoint. Bottom: \textbf{Knowledge Cutcuit Entropy} of knowledge circuits throughout training. \texttt{K\_rel} and \texttt{K\_compl} represent relevant new knowledge and completely new knowledge, respectively. \texttt{Low-freq}, \texttt{Medium-freq}, and \texttt{High-freq} represent knowledge with frequencies in the ranges $[1, 2)$, $[2,5]$ and $(5, 27]$, respectively.}
    \label{fig:similarity_entropy}
    \vspace{-10pt}
\end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figures/edges_similarity.png}
%     \caption{\textbf{Edges Jaccard Similarity} of intermediate knowledge circuits with the circuits at the final checkpoint. \texttt{K\_rel} and \texttt{K\_compl} represent relevant new knowledge and completely new knowledge, respectively. \texttt{Low-freq}, \texttt{Medium-freq}, and \texttt{High-freq} represent knowledge with frequencies in the ranges $[1, 2)$, $[2,5]$ and $(5, 27]$, respectively.}
%     \label{fig:edges_similarity}
%     \vspace{-10pt}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figures/circuit_entropy.png}
%     \caption{\textbf{Knowledge Cutcuit Entropy} of knowledge circuits throughout training. \texttt{K\_rel} and \texttt{K\_compl} represent relevant new knowledge and completely new knowledge, respectively. \texttt{Low-freq}, \texttt{Medium-freq}, and \texttt{High-freq} represent knowledge with frequencies in the ranges $[1, 2)$, $[2,5]$ and $(5, 27]$, respectively.}
%     \label{fig:circuit_entropy}
%     \vspace{-10pt}
% \end{figure*}

Our results in Figure~\ref{fig:similarity_entropy} show a stable downward trend in the knowledge circuit entropy metric for edges in the subgraph across all models, suggesting that the identified knowledge circuits become increasingly centralized, with the importance of critical edges growing as knowledge acquisition progresses.
We also observe that the downward trend of the knowledge circuit entropy slows down significantly after a certain turning ponit during the training of all models.
For example, turning points are observed in GPT-2 Small, GPT-2 Medium, TinyLlama, and Phi-1.5 at epoch 7, epoch 4, epoch 1, and epoch 1, respectively.
We attribute this interesting phenomenon to \textbf{a phase shift in the evolution of knowledge circuits} across continual pre-training.
In the initial \textit{formation phase} of knowledge circuits, less efficient knowledge circuits gradually take shape within the models, resulting in a rapid decrease in circuit entropy.
At the phase shift points, the knowledge circuits reach a status of stability where the most critical nodes and edges have been involved.
In the subsequent \textit{optimization phase}, the topology composed critical nodes and edges becomes more stable, while the computations within these components are being optimized to represent and retrieve the knowledge more efficiently, leading to a slowdown in the rate of decrease in circuit entropy.

It's no coincidence that we also observe consistent phase shift points in the structral consistency of the nodes and edges in knowledge circuits throughout continual pre-training in Figure \ref{fig:similarity_entropy} and Figure \ref{fig:nodes_similarity}, which signal a slowdown in the rate of structural convergence.
This further confirms a reduction in the topological changes of the knowledge circuits, with subsequent performance improvements primarily attributed to the refinement and optimization of the efficiency of the existing structure.

\begin{tcolorbox}[mybox, title={Takeaway: Biphasic Circuit Evolution}]
The evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization, each marked by unique structural and behaviral characteristics.
\end{tcolorbox}

This finding suggests that \textbf{the knowledge circuit state could serve as a valuable tracking status for the continual pre-training process}, enabling more informed adjustments to the training method or data in response to different phases.
We leave this potential direction for future research.

\subsubsection{Aligning Topology with Specific Knowledge Circuits}
\label{sec:specific_circuit_performance}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/specific_circuit_performance.png}
    \caption{Hit@10 of the performance of aligned knowledge circuits in GPT-2 Small throughout training. \texttt{Init}, \texttt{Before}, \texttt{After},  \texttt{Last} represents the circuits whose topologies align with those at the initial checkpoint, the checkpoint before the phase shift, the checkpoint after the phase shift, and the final checkpoint, respectively. \texttt{Original} represents the original knowledge circuits at each checkpoint. Note that we smooth the curves using a window size of 3 epochs.}
    \label{fig:specific_circuit_performance}
    \vspace{-15pt}
\end{figure}

To clarify the influence of the topology of knowledge circuits on performance, we conduct a detailed examination of the knowledge circuits at several key training checkpoints.
Specifically, we focus on the knowledge circuits at the initial checkpoint, the checkpoint immediately before the phase shift point, the checkpoint immediately after the phase shift point, and the last checkpoint.
We align the topology of the knowledge circuits at each checkpoint throughout training with those of focus and then evaluate the performance for aligned circuits employing the Hit@10 metric as in \S\ref{sec:performance}.
The results in Figure \ref{fig:specific_circuit_performance} reveal that the performance of all aligned circuits remain unchanged during the formation phase.
However, each circuit begins to improve its performance during the optimization phase, with those aligned with the post-phase-shift topologies (\texttt{After} and \texttt{Last}) ultimately performing, on average, 54\% better than those aligned with the pre-phase-shift topologies (\texttt{Init} and \texttt{Before}).
This observation suggests the evolution of the topology of knowledge circuits at the phase shift point plays a crucial role in improving circuit performance.
More examination of the relationship between this topological evolution and the evolution of components will be provided in \S\ref{sec:evolutionary_pattern}.

\subsection{Components Analysis}
\label{sec:components}

After analyzing the dynamics of the knowledge circuits at the overall topology level, we may further seek to understand how the components within these circuits evolve throughout training.

\subsubsection{Evolutionary Pattern of Components}
\label{sec:evolutionary_pattern}

\paragraph{Specialized Nodes}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/specialized_components.png}
    \caption{Proportion of \textbf{specialized attention heads} in all nodes of the knowledge circuits throughout training for GPT-2 Small and GPT-2 Medium. Note that we smooth the curves using a window size of 3 epochs.}
    \label{fig:specialized_components}
    \vspace{-10pt}
\end{figure}

% To characterize the evolutionary pattern of components within knowledge circuits, 
We first zoom into the specialized nodes within knowledge circuits to investigate the underlying factors driving the evolution of knowledge circuit.
Recent studies have identified a set of specialized attention heads \citep{attention_heads_survey,mi_primer} that directly contribute to factual recall in Transformer-based LLMs, including the mover head, relation head, and mixture head \citep{DBLP:journals/corr/abs-2403-19521,DBLP:conf/iclr/MerulloEP24,additive_mechanisms}.
More detailed definitions and methodology for identifying these specialized attention heads are provided in Appendix \ref{app:specialized_components}.
We check the emergence and track the proportion of these specialized attention heads in all possible nodes of the knowledge circuits throughout training, and present our results in Figure~\ref{fig:specialized_components}.
We observe that during the circuit formation phase, mover heads gradually emerge from nearly zero, while the proportion of relation heads decreases until the phase shift.
In the circuit optimization phase, the proportion of all kinds of attention heads stabilizes.
The proportion of mixture heads remains stable throughout training.
We further examine the layer-wise distribution of mover heads and relation heads within knowledge circuits throuout training.
Our results in Figure \ref{fig:gpt2_heads_distribution} (and Figure \ref{fig:gpt2_medium_heads_distribution} in Appendix) reveal that the increase in mover heads and the decrease in relation heads primarily occur in the mid-to-deeper layers during the circuit formation phase.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/gpt2_heads_distribution.png}
    \caption{Top: Layer distribution of \textbf{mover head} in the knowledge circuits in GPT-2 Small throughout training. Bottom: Layer distribution of \textbf{relation head} in the knowledge circuits in GPT-2 Small throughout training.}
    \label{fig:gpt2_heads_distribution}
    \vspace{-10pt}
\end{figure}

\paragraph{Activated Edges}

Next, we investigate how the nodes within knowledges circuits propagate information to subsequent components through the edges.
Specifically, we analyze the variation in edge activation patterns across different layers of the network throughout training.
We quantify the edge activation ratio for each layer by calculating the proportion of edges originating from that layer within the knowledge circuit, relative to all possible edges originating from that layer in the whole model\footnote{Note that we exclude the activation ratio for the last layer, as the small denominator causes the ratio to be an outlier, potentially blurring the overall trends in the activation patterns observed across layers.}.
Our results in Figure~\ref{fig:gpt2_activation_ratio} (and Figure \ref{fig:gpt2_medium_activation_ratio} in Appendix) reveal that, during the circuit formation phase, the edges activation ratios in the lower layers gradually decrease, while those in the mid-to-deeper layers exhibit a corresponding increase.
However, as training progresses, a transition occurs around the phase shift point, where the edge activation ratios begin to stabilize.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/gpt2_activation_ratio.png}
    \caption{\textbf{Layer distribution of the edges activation ratio} within the knowledge circuits in GPT-2 Small.}
    \label{fig:gpt2_activation_ratio}
    \vspace{-10pt}
\end{figure}

\paragraph{Evolutionary Pattern}
The observed pattern in the evolution of specialized nodes and activated edges within knowledge circuits aligns with the factual recall mechanism in LLMs described by \citet{llm_factual_recall}.
Specifically, the lower MLP layers specialize in encoding attribute-rich subject representations, while attention heads in the mid-to-deeper layers are responsible for extracting the relevant attributes for a given subject from these lower-level representations.
Based on this, we can conclude the evolutionary pattern of knowledge circuits at the component level.
During the early training phase of circuit formation, the focus is primarily on developing the extraction function within the nodes of the mid-to-deeper layers of the knowledge circuits.
This is reflected in the increased emergence of mover heads and activated edges, along with a decrease in the presence of relation heads in these layers.
This process continues until the extraction function is fully established at the phase shift point, as demonstrated by the similar performance advantage of circuits aligned with the post-phase-shift topologies over those aligned with the pre-phase-shift topologies in Figure \ref{fig:specific_circuit_performance}.
In the subsequent training phase of circuit optimization, the focus shifts to enriching knowledge representations in the lower layers, evidenced by a stabilized topology and component structure, but with a rapid improvement in the performance of knowledge circuits in Figure \ref{fig:hit_at_10} and Figure \ref{fig:specific_circuit_performance}.

\begin{tcolorbox}[mybox, title={Takeaway: Deep-to-Shallow Pattern}]
The evolution of knowledge circuits follows a deep-to-shallow pattern, where mid-to-deeper layers first develop the extraction function, and later, lower layers enrich their knowledge representations.
\end{tcolorbox}

\subsubsection{Changes in Vocabulary Space}
\label{sec:rank_prob}

To gain a more nuanced understanding of the information flow, we track the layer-wise changes in both the rank and probability of the target attribute token at the last token position when unembedding the intermediate layer’s output into the vocabulary space throughout training.
Additional results for other models are provided in Appendix \ref{app:rank_prob}.
The results in Figure \ref{fig:gpt2_rank_and_prob} reveal that the occurrence of the early decoding phenomenon \citep{logit_lens}—where the target token is already present in the residual stream by the mid-to-later layers—is closely associated with the phase shift in the evolution of knowledge circuits.
During the circuit formation phase, the mid-to-deeper layers exhibit low ranks and probabilities for the target token, suggesting that the attention heads in these layers have not yet effectively extracted the target attribute in the residual stream due to the insufficient training.
However, in the subsequent circuit optimization phase, the extraction function has already been developed in the mid-to-deeper layers, while the lower layers continue to enrich their knowledge representations for subjects, as evidenced by the occurrence of early decoding phenomenon.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/gpt2_rank_and_prob.png}
    \caption{Top: \textbf{Rank of the target attribute token} when unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Small. Bottom: The corresponding \textbf{probability of the target attribute token}.}
    \label{fig:gpt2_rank_and_prob}
    \vspace{-10pt}
\end{figure}