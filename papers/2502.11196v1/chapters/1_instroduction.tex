\begin{abstract}
Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations.
We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing.
Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: 
(1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge;
(2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization;
(3) the evolution of knowledge circuits follows a deep-to-shallow pattern.
These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance\footnote{Code and data will be available at \url{https://github.com/zjunlp/DynamicKnowledgeCircuits}.}.
\end{abstract}

\section{Introduction}

Knowledge is a cornerstone of intelligence, shaping how humanity perceives the world, interacts with others, and navigates daily life \citep{DBLP:conf/wsdm/Choi22,lkm}.
% As human society advances, the ways by which knowledge is stored, accessed, and processed have evolved significantly, especailly with the advent of Large Language Models (LLMs) \citep{gpt2,gpt3,gpt4,llama,llama2,llama3}.
Recent studies \citep{gpt3,gpt4,llama3,deepseek_v3,qwen2.5,llm_survey,DBLP:journals/corr/abs-2401-10034} on Large Language Models (LLMs) have demonstrated their ability to capture factual knowledge from pre-training corpus and encapsulate it as extensive parametric knowledge, empowering their remarkable capabilities in numerous knowledge-intensive tasks \citep{knowledge_mechanisms_survey,knowledge_life_cycle}, as well as in developing higher-order capabilities like reasoning \citep{lm_prompt_reasoning_survey,llm_reasoning_survey}.
Nevertheless, these powerful models still struggle with knowledge updates, especially with regard to the dynamic nature of world knowledge that evolves after the cut-off date of the pre-training corpus \citep{DBLP:conf/emnlp/ZhangFCNW23,DBLP:journals/corr/abs-2404-08700}.
Extensive efforts focus on developing advanced techniques for injecting new knowledge into LLMs \citep{continual_knowledge_learning,pit,sft_injecting,DBLP:conf/emnlp/OvadiaBME24,DBLP:journals/corr/abs-2411-07175}, yet the absence of a well-defined mechanism for new knowledge acquisition in LLMs continues to hinder further progress in this area.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/illustration.png}
    \caption{Illustration of our findings: \textbf{Phase shift} from formation to optimization in the evolution of knowledge circuits, each phase characterized by distinct features at the performance, topology, and component levels.}
    \label{fig:illustration}
    \vspace{-10pt}
\end{figure}

Recent works introduce mechanistic interpretability techniques to uncover knowledge machanisms in LLMs.
\citet{physics3.1} adopts probing methods to examine the storage and extraction of factual knowledge encoded in hidden states of language models.
% Building on studies that treat feed-forward layers (FFNs) as a key-value memory~\citep{ffn_kv,knowledge_neurons}, 
\citet{knowledge_entropy} introduces the concept of knowledge entropy to examine how the integration of knowledge of LLMs evolves during the pre-training phase.
However, previous works typically treat knowledge blocks as isolated components and often focus on identifying specific blocks that store particular knowledge.
In contrast, \citet{knowledge_circuits} move beyond isolated components and explore the computation graph to uncover knowledge circuits, investigating cooperation between different components to understand how knowledge is stored and expressed.

In this paper, we investigate the mechanism of new knowledge acquisition in LLMs from the perspective of knowledge circuits.
By analyzing the evolution of knowledge circuits throughout continual pre-training, we uncover several interesting findings, as illustrated in Figure \ref{fig:illustration}.

Key findings of the paper are summarized as:
\begin{itemize}[itemsep=0pt, topsep=5pt]
    \item (\S\ref{sec:performance}) The acquisition of new knowledge is significantly influenced by its relevance to pre-existing knowledge, with relevant new knowledge being integrated more efficiently than completely new knowledge.
    \item (\S\ref{sec:topology}) In the process of knowledge acquisition, the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization, each marked by unique structural and behaviral characteristics;
    \item (\S\ref{sec:components}) The evolution of knowledge circuits follows a deep-to-shallow pattern, where mid-to-deeper layers first develop the extraction function, and later, lower layers enrich their knowledge representations.
\end{itemize}
These findings offer valuable insights into the mechanisms by which LLMs adapt their internal structures to acquire new knowledge.
This understanding not only informs potential strategies for enhancing the continual learning capabilities of LLMs but also provides a solid foundation for improving their adaptability across diverse domains.