\section{Background}

\subsection{Circuit Theory}

\paragraph{Circuit as Computational Subgraph}
Delving into the Transformer architecture~\citep{transformer}, all computations in a Transformers-based language model as a connected directed acyclic graph, denoted as $\mathcal{G}$.
This graph represents the flow of information from the input of the language model to the token unembedding, where activations are projected back to vocabulary space.
Various components of a language model, including attention heads and multi-layer perceptrons (MLPs), are defined as the nodes of this graph, denoted as $N$.
The edges of this graph, denoted as $E$, are the weighted connections between these components, encompassing residual connections, attention mechanisms, and projections.
In the context of Mechanistic Interpretability (MI), which aims to understand the inner workings of adavanced Transformer-based language models~\citep{mi_review,mi_primer,mi_safety_review,sharkey2025open}, a \textbf{circuit} is conceptualized as a sparse computational subgraph $\mathcal{C}\subset\mathcal{G}$ within a language model whose computations are most relevant to the whole model's behaviour on the specific task~\citep{zoom_in,transformer_circuits,interpretability_in_the_wild,sparse_feature_circuits}.
A circuit $\mathcal{C}$ usually contains a selection of nodes $N_{\mathcal{C}}\subset{N}$ and edges $E_{\mathcal{C}}\subset{E}$ necessary for the specific task, expressed as $\mathcal{C}=<N_{\mathcal{C}},E_{\mathcal{C}}>$.

\paragraph{Circuit Discovery}
The goal of circuit discovery is to identify a computational subgraph that represents the whole model's behavior on a specific task.
Many studies adopt causal mediation analysis to localize critical nodes or edges within language models in order to identify and verify circuits.
\citet{acdc} adopts activation patching and proposes ACDC.
\citet{eap} introduces Edge Attribution Patching (EAP) to make a linear approximation of
activation patching, which assigns an importance score to each edge.

\subsection{Knowledge Circuits}
% Due to the pre-training of extensive sequences of world knowledge, Large Language Models (LLMs) have demonstrated remarkable performance in a diverse range of tasks~\citep{llm_factual_knowledge,lkm}.
% Unlike natural languages that explicitly represent world knowledge, LLMs implicitly encode vast amount of knowledge within the parameter space, which can be processed and manipulated to enhance the models' capabilities in reasoning, world perception, and engaging in human-like communication.
% Despite progress in LLMs, they still struggle with issues such as forgetfulness and hallucinations, which are exacerbated by the absence of comprehensive knowledge mechanisms in LLMs~\citep{knowledge_mechanisms}.
Unlike previous works~\citep{knowledge_neurons,ffn_kv,llm_factual_recall,rome} that treat the knowledge blocks as isolated components, ~\citet{knowledge_circuits} introduce a novel perspective: knowledge circuits.
They hypothesis that the cooperation between multiple components unveils implicit knowledge representation in LLMs.
An identified knowledge circuit is considered a computational subgraph that faithfully represents specific knowledge domains within the model's parametric memory.
As such, it should be capable of independently reproducing the behavioral patterns or performance of the entire model with respect to the corresponding tasks.
However, ~\citet{knowledge_circuits} concentrates exclusively on the knowledge that already stored in the language model, without investigating the process by which LLMs acquire knowledge.
In this work, we aim to advance the concept of knowledge circuits by investigating their dynamics throughout continual pre-training. 