\newpage
\appendix
% \onecolumn
\section*{Appendix}

\section{Dataset Construction}
\label{app:dataset}

Given the challenges of conducting mechanistic interpretability analysis on Internet-scale corpus, we perform controlled experiments on synthetic data, following~\citet{physics3.1,physics3.2,physics3.3}.
We focus on factual knowledge that can be represented as triples of the form $(s, r ,a)$ containing subject $s$, relation $r$, and attribute $a$.
For example, a piece of factual knowledge such as \textit{"Donald Trump is 78 years old"} can be represented as (\textit{Donald Trump, age, 78}).

We synthesize a pool of fictional knowledge entities based on heuristic rules using ChatGPT, ensuring that these fictional biographical knowledge is unavailable to LLMs in the pre-training phase.
Each knowledge entity is first assigned a unique name as the subject.
Each name follows the format \textit{"first\_name middle\_name last\_name"}, where the components are randomly and independently sampled from a uniform distribution.
We use ChatGPT to generate possible values for first name, middle name, and last name,  as listed in Table \ref{tab:names}.

Additionally, there are five associated relations—\textit{birth date}, \textit{city}, \textit{major}, \textit{university} and \textit{company}—which are randomly sampled from their corresponding pools of possible attributes for each relation.
The \textit{birthdate} relation offers 30 (1 to 30) × 12 (January to December) × 126 (1900 to 2025) possibilities.
The corresponding pools of possible attributes for the other four relations are as generated by ChatGPT, as listed in Table \ref{tab:city}$\sim$\ref{tab:university}. 

To convert these entities into textual knowledge for training data, we populate predefined templates with the attribute values.
For each attribute, one of 50 corresponding templates is randomly selected to enhance the diversity of the corpus.
The sentences corresponding to each relation of the same subject are then randomly shuffled to form the biography segment of the subject.
An example is provided below:

\textit{"Liora Shane Driscoll's birth is celebrated annually on \textbf{5 December, 1935}. Liora Shane Driscoll is situated in \textbf{Newport News, VA}. Liora Shane Driscoll is an expert in the making in \textbf{Agronomy}. Liora Shane Driscoll is an alumni member of \textbf{North Carolina State University}. Liora Shane Driscoll is a worker at \textbf{Google}."}

\paragraph{Knowledge Type}
We classify the new knowledge that the language model may need to acquire into two categories. 
One involves knowledge that already exists in the model's parameters but requires further learning of specific aspects (e.g., new relations).
This type of knowledge is referred to as \textit{relevant new knowledge} and denoted as $K_\text{rel}$.
The other type of knowledge is completely new, absent from the model's parameters, which is referred to as \textit{completely new knowledge} and denoted as $K_\text{compl}$.
To simulate real-world data scenarios, we set the knowledge type ratio as $|K_\text{rel}|:|K_\text{compl}|=1:4$.
Specifically, for complete new knowledge, we exclusively use synthetic fictional knowledge entities.
For relevant new knowledge entities, we extract a set of celebrity names from Wikipedia, which are highly likely to appear in pre-training, and then sample fictional attributes for these entities.

\paragraph{Knowledge Frequency}
Considering the long-tail distribution of knowledge in real-world data, we model the frequency of knowledge entities in the corpus to follow an exponential distribution.
This ensures that the corpus for continual pre-training contains both high-frequency knowledge as well as long-tail knowledge.
We classify portions of the corpus based on frequency:
Knowledge entities with a frequency greater than 5 in the corpus are classified as high-frequency knowledge, those with a single occurrence as low-frequency knowledge, and the remaining entities as medium-frequency knowledge.

We set the number of all individuals appearing in the training corpus to 50,000, with their frequency following an exponential distribution between 1 and 27.
This finallly result in 133,408 biography segments, with a total length of 10 million tokens and an average length of 76.8 tokens per biography segment.

\section{Training Configuration}
\label{app:training}
\paragraph{GPT-2}
We adopt the standard GPT-2~\citep{gpt2} implementation available on Huggingface, including GPT-2 Small and GPT-2 Medium.

\paragraph{Llama}
Given the huge experimental cost associated with the original Llama~\citep{llama,llama2,llama3}, which typically have parameters exceeding 7 billion, we perform surrogate experiments using a relatively small model, TinyLlama~\citep{tinyllama}.
TinyLlama adopts exactly the same architecture and tokenizer as Llama 2, but with only 1.1 billion parameters, facilitating more efficient experimentation.

\paragraph{Phi}
We adopt Phi-1.5 \citep{phi-1.5} with 1.3 billion parameters.

For continual-pre training, we use a constant learning rate schedule without warmup.
Our learning rate is set to match the learning rate of the base model at the end of its pre-training phase.
We train using the AdamW optimizer with $\beta_1=0.9,\beta_2=0.95,\epsilon=1e-6$, and a
weight decay of 0.1.
We perform gradient accumulation for every 4 steps.
We present several key statistics of the base models and more hyperparameters that are altered in our experiments in Table \ref{tab:models}.
\input{tables/models}

All of our continual pre-training experiments are runned on 2 NVIDIA-A100 GPUs. 

\section{Circuit Discovery}
\label{app:circuit_discovery}

\begin{table}
    \centering
    \resizebox{0.8\linewidth}{!}{
        \begin{tabular}{ll}
        \toprule
            \textbf{Relation} & \textbf{Template} \\
            \midrule
            \textit{city} & \textit{s} lives in the city of \\
            \textit{major} & \textit{s} majors in the field of \\
            \textit{company} & \textit{s} works for the company of \\
        \bottomrule
        \end{tabular}
    }
    \caption{Templates for the factual recall task on relations.}
    \label{tab:templates}
    \vspace{-10pt}
\end{table}

\paragraph{Tasks}
Unlike previous works that investigate circuits on simple but general tasks such as Indirect Object Identification (IOI) and Greater-Than, our paper focuses on knowledge circuits that are capable of performing the task of factual recall.
In a factual recall task, the objective is to predict a target attribute $a$ given a subject-relation pair $(s, r)$.
To ensure a sufficiently rich vocabulary space for the first token of the target attribute, we construct the factual recall tasks based on three relations mentioned in \S\ref{sec:dataset}: \textit{city}, \textit{major}, and \textit{company}.
The templates for converting a subject-relation pair $(s, r)$ into a query string for each factual recall task are listed in Table \ref{tab:templates}.
A typical circuits task consists of minimal pairs of clean and corrupted inputs.
For clean inputs, we randomly sample 300 examples from the training corpus for each knowledge type and frequency as the validation set $D_{\text{val}}$ for circuit discovery.
In our experiments, we observe that continually increasing the size of $D_{\text{val}}$ only adds to the runtime for circuit discovery without improving the quality of the discovered circuits.
The corresponding corrupted inputs are independently sampled from the training corpus to match the length of the subject tokens in each clean input.

\paragraph{Loss}
The metric for circuit tasks assesses how closely the language model outputs align with clean input, as opposed to corrupted input.
In our circuit discovery experiments, we evaluate the performance of circuits using the logit difference: the logit of the correct attribute minus the logit of the corrupted attribute.
We then convert the task metric $M$ into a loss function by defining $L(x)=-M(x)$, as shown in Eq.~\ref{eq:eap-ig}.

We make modifications to the TransformerLens library \citep{nanda2022transformerlens} and EAP-IG library \citep{eap-ig} to implement the circuit dicovery method and conduct all the analysis experiments.

\section{Whole Model Performance}
\label{app:performance}

We examine the whole model's performance for knowledge acquisition by monitoring two type of accurracies during training process.
First, we track the model's next-token prediction accuracy on the first token of each attribute during training.
This metric reflects how well the model acquires and memorizes the knowledge.
The second metric is calculated on downstream query tasks in cloze-style for each attribute, such as \textit{"s lives in the city of \_\_\_"}, where the accuracy reflects the model's ability to generate an exact match for the correct attribute.
Our results in Figure~\ref{fig:accuracy} illustrate that both accuracy metrics increase until they reach their upper limits, reflecting the model's ongoing acquisition of new knowledge during continual pre-training.
Another interesting observation is that the accuracy curve for $K_\text{rel}$ consistently lies above the curve for $K_\text{compl}$ on both metrics, suggesting that relevant new knowledge is easier for LLMs to acquire than completely new knowledge.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/accuracy.png}
    \caption{Accuracy curves across continual pre-training. \texttt{K\_rel} and \texttt{K\_compl} represent relevant new knowledge and completely new knowledge, respectively. \texttt{First-token Acc} stands for the model's next-token prediction accuracy on the first token of each attribute, while \texttt{Query Acc} stands for the generation accuracy on downstream query tasks for each attribute.}
    \label{fig:accuracy}
\end{figure*}

\section{Transfer Performance of Knowledge Circuits between Frequency}
\label{app:transfer_setting}
To investigate the differences in the capacities of knowledge circuits identified using validation data filtered by knowledge frequency, we analyze the transfer performance of these circuits on held-out test sets with varying transferred knowledge frequencies.
For example, if a knowledge circuit is identified using validation data filtered by high-frequency knowledge, denoted as \texttt{High-freq Circuit}, its transfer performance is evaluated on test sets filtered by medium-frequency and low-frequency knowledge, respectively.

Our results in Figure \ref{fig:transfer_hit_at_10} reveal that knowledge circuits identified using knowledge of different frequencies perform comparably when evaluated on test sets of the same frequency. Notably, knowledge circuits discovered using high-frequency knowledge exhibit relatively poor performance on the low-frequency test set, whereas circuits identified using low-frequency knowledge perform comparably to high-frequency circuits on the high-frequency test set. This finding suggests that there is no inherent difference in the capability of circuits for the same task; rather, their effectiveness is primarily determined by the representation of knowledge shaped by frequency.


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/transfer_hit_at_10.png}
    \caption{Hit@10 of the transfer performance of knowledge circuits in GPT-2 Small and GPT-2 Medium throughout training. \textcolor[RGB]{44,160,44}{\texttt{Low-freq Circuit}}, \textcolor[RGB]{255,127,14}{\texttt{Medium-freq Circuit}}, and \textcolor[RGB]{31,119,180}{\texttt{High-freq Circuit}} represent knowledge circuits identified by knowledge with the frequencies in the ranges  \textcolor[RGB]{44,160,44}{$[1, 2)$}, \textcolor[RGB]{255,127,14}{$[2,5]$} and \textcolor[RGB]{31,119,180}{$(5, 27]$}, respectively. Note that we smooth the curves using a window size of 3 epochs for all settings.}
    \label{fig:transfer_hit_at_10}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/nodes_similarity.png}
    \caption{\textbf{Nodes Jaccard Similarity} of intermediate knowledge circuits with the circuits at the final checkpoint. \texttt{K\_rel} and \texttt{K\_compl} represent relevant new knowledge and completely new knowledge, respectively. \texttt{Low-freq}, \texttt{Medium-freq}, and \texttt{High-freq} represent knowledge with frequencies in the ranges $[1, 2)$, $[2,5]$ and $(5, 27]$, respectively.}
    \label{fig:nodes_similarity}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/gpt2_medium_activation_ratio.png}
    \caption{\textbf{Layer distribution of the edges activation ratio} within the knowledge circuits in GPT-2 Medium.}
    \label{fig:gpt2_medium_activation_ratio}
    \vspace{-10pt}
\end{figure}

\section{Changes in Vocabulary Space}
\label{app:rank_prob}

We present our results for the layer-wise changes in the rank and probability of the target attribute token at the final token position when unembedding the intermediate layer’s output into the vocabulary space throughout the training of GPT-2 Small on the high-frequency set in \S\ref{sec:rank_prob}.
Additionally, we provide the full results of all knowledge frequencies for GPT-2 Small in Figure \ref{fig:gpt2_all_rank_and_prob}.
We also provide the full results for GPT-2 Medium (Figure \ref{fig:gpt2_medium_rank_and_prob}) and TinyLlama (Figure \ref{fig:tinyllama_rank_and_prob}).

\section{Specialized Attention Heads within Knowledge Circuits}
\label{app:specialized_components}

\subsection{Definitions of Specialized Attention Heads}

When zooming into the discovered knowledge circuits, we can find several specialized attention heads in the model that play a crucial role in the final prediction.
These include the mover head, relation head, and mixture head \citep{additive_mechanisms}.

\paragraph{Mover head}
Attention head that focuses on the final token of the context and attends strongly to the subject tokens in the context, functioning as a mover to transfer information and extract attributes pertaining to the subject from the enriched subject representation.

\paragraph{Relation head}
Attention head that focuses on the final token of the context and attends strongly to the relation tokens in the context for a particular relation and extract many relation-related attribute tokens.

\paragraph{Mixture head}
Attention Head that attends to both the relation tokens and the subject tokens in the context.
It behaves as a combination of the two, performing the role of both Mover Head and Relation Head simultaneously.

\subsection{Identification of Specialized Attention Heads}
In this section, we provide details on how to identify mover heads, relation heads, and mixture heads in LLMs.
We re-implement the methodology described in \citet{additive_mechanisms} since the original code has not been made publicly available by the authors.
We will update our implementation once the source code is released.

Building on the Direct Logit Attribution (DLA) technique, which measures the direct effect of individual model components on model outputs, \citet{additive_mechanisms} move beyond and propose DLA by source token group.
This technique is based on the observation that attention head outputs are a weighted sum of outputs corresponding to distinct attention source positions \citep{transformer_circuits}.
This approach is useful for quantifying how a source token group directly affects the logits through individual attention heads.

With a specific factual recall task where the relation held constant, we aggregate over the validation set $D_{\text{val}}$ for circuit discovery on the task, an attention head is classified as mover head if:
\begin{equation}
   \left|\frac{\sum_{i=1}^{|D_{\text{val}}|}\text{DLA}_s(\text{Q}_i)}{\sum_{i=1}^{|D_{\text{val}}|}\text{DLA}_r(\text{Q}_i)}\right| > \tau
\end{equation}
where $i$ denotes the $i$-th entity in $D_{\text{val}}$, $\text{Q}_i$ denotes the relation-specific query string for entity $i$ as shown in Table \ref{tab:templates}, $\text{DLA}_s(\text{Q}_i)$ denotes DLA attributed to subject tokens, and $\text{DLA}_r(\text{Q}_i)$ denotes DLA attributed to relation tokens.
Relatively, an attention head is classified as relation head if:
\begin{equation}
    \left|\frac{\sum_{i=1}^{|D_{\text{val}}|}\text{DLA}_s(\text{Q}_i)}{\sum_{i=1}^{|D_{\text{val}}|}\text{DLA}_r(\text{Q}_i)}\right| < \frac{1}{\tau}
\end{equation}
where threshold $\tau$ is set to be 10 as suggested in \citet{additive_mechanisms}.
Remaining attention heads in LLMs are classified as mixture heads, behaving as a combination of mover head and relation head.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/gpt2_medium_heads_distribution.png}
    \caption{Left: Layer distribution of \textbf{mover head} in the knowledge circuits in GPT-2 Medium throughout training. Right: Layer distribution of \textbf{relation head} in the knowledge circuits in GPT-2 Medium throughout training.}
    \label{fig:gpt2_medium_heads_distribution}
    \vspace{-15pt}
\end{figure}

\section{Forgetting Analysis for Knowledge Circuits}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/gpt2_forget.png}
    \caption{Edges Jaccard Similarity of intermediate knowledge circuits with the circuits at the final checkpoint of the previous knowledge acquisition experiment.}
    \label{fig:gpt2_forget}
    \vspace{-10pt}
\end{figure}

% ARC
% HellaSwag
% PopQA
% TriviaQA

% 1 介绍设定
% 2 分析结果
% 3 分析原因  Replay  对回路形成有啥用    对回路恢复有啥作用  分析背后原因  深层原因（回路视角）
% 4  replay 的配比 数据  监控学习能力

To analyze the model's forgetting of acquired knowledge, we conduct and additinal coninual pre-training experiment.
We first construct new training corpus following the same pipeline described in \S\ref{sec:dataset}, and then initialize training from the final checkpoint of the previous knowledge acquisition experiment on GPT-2 Small.
% with a reduced learning rate of 1e-4.
We monitor structral consistency changes for knowledge circuits throughout 10 training epochs.

Our results in Figure \ref{fig:gpt2_forget} reveal that knowledge circuits demonstrate structural reconfiguration capacity, with the identified circuits dynamically adjusting more than 60\% of their edges to accommodate new knowledge.
However, data replay interventions, which involve the periodic replacement of a fixed ratio of original training samples, successfully mitigate knowledge forgetting by reactivating circuit components.
This evidence suggests that LLMs maintain latent reactivation potential even after apparent behavioral forgetting — a property we term knoweldge circuit elasticity.

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figures/gpt2_forget_entropy.png}
%     \caption{Knowledge Cutcuit Entropy of intermediate knowledge circuits. \texttt{K\_rel} and \texttt{K\_compl} represent relevant new knowledge and completely new knowledge, respectively.}
%     \label{fig:gpt2_forget_entropy}
% \end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/gpt2_all_rank_and_prob.png}
    \caption{Top: \textbf{Rank of the target attribute token} when unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Small. Bottom: \textbf{Probability of the target attribute token} when unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Small. \texttt{Low-freq}, \texttt{Medium-freq}, and \texttt{High-freq} represent knowledge with frequencies in the ranges $[1, 2)$, $[2,5]$ and $(5, 27]$, respectively.}
    \label{fig:gpt2_all_rank_and_prob}
    \vspace{-10pt}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/gpt2_medium_rank_and_prob.png}
    \caption{Top: \textbf{Rank of the target attribute token} when unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Medium. Bottom: \textbf{Probability of the target attribute token} when unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Medium. \texttt{Low-freq}, \texttt{Medium-freq}, and \texttt{High-freq} represent knowledge with frequencies in the ranges $[1, 2)$, $[2,5]$ and $(5, 27]$, respectively.}
    \label{fig:gpt2_medium_rank_and_prob}
    \vspace{-10pt}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/tinyllama_rank_and_prob.png}
    \caption{Top: \textbf{Rank of the target attribute token} when unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for TinyLlama. Bottom: \textbf{Probability of the target attribute token} when unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for TinyLlama. \texttt{Low-freq}, \texttt{Medium-freq}, and \texttt{High-freq} represent knowledge with frequencies in the ranges $[1, 2)$, $[2,5]$ and $(5, 27]$, respectively.}
    \label{fig:tinyllama_rank_and_prob}
    \vspace{-10pt}
\end{figure*}


\input{tables/names}
\input{tables/city}
\input{tables/major}
\input{tables/company}
\input{tables/university}