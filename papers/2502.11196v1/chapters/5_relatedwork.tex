\section{Related Work}

\paragraph{New Knowledge Acquisition}

Previous studies~\citep{acquire_factual_knowledge} explore new knowledge acquisition in LLMs with various behavioral interpretability techniques, which characterizes model behavior without revealing insights into the internal workings.
Recent works introduce mechanistic interpretability techniques to advance related research even further.
\citet{physics3.1} adopt probing methods to examine the storage and extraction of factual knowledge encoded in hidden states of language models.
Building on studies that treat feed-forward layers as a key-value memory~\citep{ffn_kv,knowledge_neurons}, \citet{knowledge_entropy} introduce the concept of knowledge entropy to examine how LLMs' knowledge integration evolves during the pre-training phase.
% and demonstrates a consistent decay in knowledge entropy during the acquisition process of new knowledge.
In this paper, we seek to uncover the internal mechanism of new knowledge acquisition in LLMs by investigating the dynamics of knowledge circuits within LLMs thtroughout continual pre-training.

\paragraph{Mechanistic Interpretability}

% Mechanistic Interpretability (MI) is an emerging field focused on reverse-engineering neural networks to decode their internal computations~\citep{mi_review,mi_primer,mi_safety_review,sharkey2025open}. 
With the rise of LLMs, Mechanistic Interpretability (MI) has gained prominence for reverse-engineering Transformer-based language models to decode their internal computations~\citep{mi_review,mi_primer,mi_safety_review,singh2024rethinking,sharkey2025open}.
Early MI research identifies features that consistently activate for specific input properties as elementary computational units.
% Early MI research identifies neurons that consistently activate for specific human-interpretable input properties as elementary computational units, neurons that consistently activate for specific human-interpretable input properties.
While such studies reveal phenomena such as polysemanticity and enable applications like knowledge editing~\citep{editing_survey,know_edit,DBLP:journals/corr/abs-2406-19354} and steering~\citep{turner2023activation}, they offer limited insights into how features interact to drive model behaviors.
This gap motivates circuit analysis \citep{transformer_circuits,knowledge_circuits}, which investigates computational pathways between Transformer components. 
Most similar to our work, \citet{circuits_are_consistent} examines general circuits formation during pre-training, while our work focuses on the evolution of knowledge circuits throughout continual pre-training.
% Building on this, we investigate knowledge acquisition mechanisms through continual pre-training, proposing a novel perspective grounded in the evolution knowledge circuits.