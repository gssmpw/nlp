\begin{table*}[ht]
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{\textbf{Architecture}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Statistics}} & \multicolumn{4}{c}{\textbf{Hyperparameters}}                \\ \cmidrule(lr){3-5} \cmidrule(lr){6-9}
                        &                         & size     & nodes     & edges    & block\_size & batch\_size & learning\_rate & epochs \\ \midrule
\multirow{2}{*}{GPT-2}  & GPT-2 Small             & 124M         & 158          & 32,491         & 1,024            & 32            &  1e-3              & 25       \\
                        & GPT-2 Medium            & 355M         &  410         &  231,877        & 1,024            & 16            &  1e-3              & 15       \\ \cmidrule(lr){1-9}
Llama               & TinyLlama-v1.1          & 1.1B         &  728         &  742,996        & 2,048            & 4            & 4e-5               & 10       \\ \cmidrule(lr){1-9}
Phi               & Phi-1.5          & 1.3B         &  794  &  886,597        & 2,048            & 2            & 2e-4               & 7       \\\bottomrule
\end{tabular}
}
    \caption{Statistics and hyperparameters of models used in the continual pre-training experiments.}
    \label{tab:models}
    \vspace{-10pt}
\end{table*}