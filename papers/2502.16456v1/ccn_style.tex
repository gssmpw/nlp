% 
% Annual CCN conference
% Sample LaTeX Two-Page Summary -- Proceedings Format
% based on the prior cognitive science style file

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)        10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014
% Modified : Konrad Kording (koerding@gmail.com)     2/15/2017
% Modified : Thomas Naselaris (tnaselar@gmail.com)   3/18/2022
% Modified : Sneha Aenugu & Jasper van den Bosch   18/12/2024

\documentclass[10pt,letterpaper]{article}

\usepackage{ccn}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{lineno}
\usepackage{amsmath}
\usepackage{graphicx}


\title{Category-Selective Neurons in Deep Networks: Comparing Purely Visual and Visual-Language Models}
 
\author{{\large \bf Zitong Lu (lu.2367@osu.edu)} \\
The Ohio State University
\AND{\large \bf Yuxin Wang} \\
University of Cincinnati}


\begin{document}
% \linenumbers

\maketitle


\section{Abstract}
{
\bf
Category-selective regions in the human brain, such as the fusiform face area (FFA), extrastriate body area (EBA), parahippocampal place area (PPA), and visual word form area (VWFA), play a crucial role in high-level visual processing. Here, we investigate whether artificial neural networks (ANNs) exhibit similar category-selective neurons and how these neurons vary across model layers and between purely visual and vision-language models. Inspired by fMRI functional localizer experiments, we presented images from different categories (faces, bodies, scenes, words, scrambled scenes, and scrambled words) to deep networks and identified category-selective neurons using statistical criteria. Comparing ResNet and the structurally controlled ResNet-based CLIP model, we found that both models contain category-selective neurons, with their proportion increasing across layers, mirroring category selectivity in higher-level visual brain regions. However, CLIP exhibited a higher proportion but lower specificity of category-selective neurons compared to ResNet. Additionally, CLIP’s category-selective neurons were more evenly distributed across feature maps and demonstrated greater representational consistency across layers. These findings suggest that language learning increases the number of category-selective neurons while reducing their selectivity strength, reshaping visual representations in deep networks. Our study provides insights into how ANNs mirror biological vision and how multimodal learning influences category-selective representations.
}
\begin{quote}
\small
\textbf{Keywords:} 
category-selectivity; functional localization; comparative neuroscience and AI; multimodal learning
\end{quote}

\section{Introduction}

Category-selective regions in the human visual system, such as the Fusiform Face Area (FFA) (\cite{Kanwisher1997, Kanwisher2006}), Extrastriate Body Area (EBA) (\cite{Downing2001}), Parahippocampal Place Area (PPA) (\cite{Epstein1998}), and Visual Word Form Area (VWFA) (\cite{Dehaene2011, McCandliss2003}), play a crucial role in high-level visual processing. These areas exhibit hierarchical category selectivity, suggesting that categorical representations emerge progressively along the visual pathway (\cite{Grill-Spector2014, Haxby2001}). In recent years, artificial neural networks (ANNs) have been widely used to model human visual processing (\cite{Cadieu2014, Cichy2016, Kubilius2019, Yamins2014, Lu2023}). Studies have shown that ANNs can develop features resembling those in the biological visual system, such as low-level edge detectors and high-level semantic representations (\cite{Yamins2016, Margalit2024, Lu2023d, Huang2021}). However, it remains unclear whether ANNs naturally develop category-selective neurons and whether their selectivity patterns align with those observed in the human brain.

Recent research suggests that ANNs not only approximate human visual behavior but also exhibit neural representations akin to those in the brain (\cite{Cichy2016, Guclu2015, Kietzmann2019, Lu2023c, Lu2023e, Yamins2014, Yamins2016}). This has led to a “reverse engineering” approach, where neuroscientists analyze ANN representations to infer principles of human vision. Understanding ANN representations can thus inform both AI research and neuroscience. While previous work has focused on purely visual models such as ResNet (\cite{He2016}), vision-language models like CLIP have demonstrated more human-like conceptual representations (\cite{Radford2021}). Since CLIP’s visual encoder shares the same architecture as ResNet but incorporates language supervision, its category representations may differ from those in purely visual models. This raises a critical question: Does language learning influence the formation of category-selective neurons? Does it increase their number or alter their selectivity patterns? Although language is known to play a role in concept learning (\cite{Xu2002, Condry2008, Carruthers2002}), its impact on visual representations remains unclear.

To address these questions, we adopted a "functional localizer" approach, commonly used in fMRI research to identify category-selective regions in the brain (\cite{Abassi2024, Li2024, Poldrack2007}). Using the same method, we examined category selectivity in ResNet and CLIP by presenting images from different categories (faces, bodies, scenes, words, scrambled scenes, and scrambled words) and identifying category-selective neurons using statistical criteria. In addition, we analyzed their distribution, hierarchical organization, and cross-layer consistency. Our results show that category-selective neurons emerge across multiple layers of ANNs, increasing in proportion at deeper levels. Moreover, CLIP exhibits a higher proportion of category-selective neurons than ResNet, but they are less selective, more uniformly distributed, and exhibit greater representational consistency across layers. These findings suggest that language learning increases the number of category-selective neurons while reducing their selectivity strength, shedding light on how multimodal learning reshapes visual representations.

The key contributions of our current study are as follows: (1) First systematic study of category-selective neurons in ANNs across hierarchical layers; (2) Reveals the influence of language on category selectivity in neural representations; (3) Provides new insights into ANNs as models of human vision and multimodal learning.

\section{General Methods}

In this section, we describe the stimuli used for our functional localizer experiments in ANNs, the selection of models and the rationale behind it, the method for identifying category selective neurons, and the metrics used to quantify selectivity. Additional methodological details are provided in the corresponding result section.

\subsection{Stimuli}

We selected 40 images each from four categories: face, body, scene, and word, with some images sourced from the fLoc functional localizer package (\cite{Stigliani2015}) (Figure \ref{Figure1}A). However, when investigating category-selective neurons, using only these four categories could raise concerns regarding whether the selectivity is truly driven by high-level semantic information or merely by low-level visual features such as edges, textures, and spatial frequency.

To control for low-level feature-driven selectivity, we additionally included 40 scrambled scene images and 40 scrambled word images as control conditions (Figure \ref{Figure1}A). This manipulation ensures that any observed category selectivity is more likely to reflect high-level semantic processing rather than simple visual properties. Our stimulus selection and design follow rigorous protocols from fMRI studies, ensuring that our analysis of category-selective neurons in ANNs aligns with established methodologies used in human neuroscience research.

\subsection{Model selection}

To investigate category-selective neurons in deep neural networks, we compared a purely visual model (ResNet-50)with a vision-language model (ResNet-50-based CLIP). ResNet-50 is a widely used purely visual model trained on ImageNet (\cite{Deng2009}), while ResNet-50-based CLIP has the same convolutional backbone but is trained using a contrastive learning objective with paired image-text data. We chose these two models for the following reasons: (1) CNN-based architectures better align with human vision: Prior research suggests that convolutional neural networks (CNNs) are more biologically plausible than vision transformers (ViTs). CNNs exhibit a hierarchical organization similar to the human visual cortex, have higher neural and behavioral similarity to humans (\cite{Geirhos2021}), and demonstrate human-like adversarial vulnerability (\cite{Bhojanapalli2021}). Given these advantages, we chose ResNet over ViTs. (2) Structural control between models: To compare purely visual models with vision-language models, we controlled for architecture by selecting ResNet-50-based CLIP, which maintains the same ResNet-50 backbone. This ensures that any observed differences between the two models are primarily due to language supervision rather than architectural discrepancies.

Additionally, we focused our analysis on all layers following ReLU activations in ResNet-50, totaling 17 layers. This choice was motivated by two key considerations: First, ReLU introduces sparsity by zeroing out negative activations, making feature representations more biologically plausible.
Second, nonlinear feature transformations primarily occur after ReLU, meaning category selectivity is more meaningfully assessed at these points.

\subsection{"Functional localizer" in ANNs}

Similar to functional localizer tasks in fMRI, we presented the 240 images to both ResNet and CLIP, recording the activation of every neuron in response to different images from different categories. A neuron was identified as category-selective if its activation was significantly stronger for one category compared to all others. Specifically, for each neuron, we conducted independent t-tests comparing activations between images from one category and each of the other categories. For instance, to be classified as a face-selective neuron, a neuron needed to show: Face$>$Body and Face$>$Scene and Face$>$Word and Face$>$Scrambled Scene and Face$>$Scrambled Word, with p$<$0.05 for each comparison. This allowed us to quantify the proportion of category-selective neurons at each layer and measure the activation strength of these neurons across different visual stimuli.

\subsection{Category selectivity index (CSI)}

Since activation magnitudes vary across layers, we computed a Category Selectivity Index (CSI) to quantify category selectivity in a layer-independent manner:
\begin{equation}
CSI = \frac{R_{\text{preferred}} - R_{\text{non-preferred}}}{R_{\text{preferred}} + R_{\text{non-preferred}}}\times100\%
\end{equation}
where $R_{\text{preferred}}$ is the mean activation for images of the neuron’s preferred category, and $R_{\text{non-preferred}}$ is the mean activation across all other categories. A higher CSI indicates stronger category selectivity. A lower CSI suggests weaker selectivity or broad tuning across categories. By comparing CSI across layers and models, we could assess how category selectivity evolves through hierarchical processing and how language supervision reshapes category representations.

\begin{figure*}[ht]
\begin{center}
\fbox{\includegraphics[width=2\columnwidth]{Figure1.jpg}}
\caption{Category-selective neurons in layer4.2.relu from ResNet and CLIP. (A) Image stimuli used in the functional localizer experiment. Images were divided into six categories: face, body, scene, word, scrambled scene, and scrambled word. (B) Proportion of category-selective neurons in the last convolutional layer (layer4.2.relu) of ResNet and CLIP. Neurons were classified as category-selective if they exhibited significantly higher activation for one category compared to all others. (C) Response profiles of category-selective neurons from layer4.2.relu. The top raw shows results for ResNet, and the bottom row for CLIP. Each individual dot corresponds to a image.} 
\label{Figure1}
\end{center}
\end{figure*}

\subsection{Cross-layer consistency analysis}

To evaluate the representational consistency of category-selective representations across layers, we performed a cross-layer representational similarity analysis (\cite{Kriegeskorte2008a, Lu2020}). For each model (ResNet and CLIP), we constructed a 40 $\times$ 40 representational dissimilarity matrix (RDM) for each layer and each category using the Pearson correlation distance. Specifically, given a category (e.g., face), we obtained the activations of all face-selective neurons in a given layer for the 40 images from that category. The pairwise dissimilarity between any two images $i$, $j$ was computed as:
\begin{equation}
\text{RDM}_{i,j}^{(L,c)} = 1 - \text{PCorr}(\mathbf{a}_i^{(L,c)}, \mathbf{a}_j^{(L,c)})
\end{equation}
where ${a}_i$ and ${a}_j$ are the activation vectors of all face-selective neurons in the layer for images $i$ and $j$, and $\text{PCorr}$ is the Pearson correlation coefficient. We then computed the similarity between two different layer's RDMs using the Spearman correlation coefficient: 
\begin{equation}
    \begin{split}
    \text{Cross-layer similarity}_{(Li, Lj)} = \text{SCorr}(\text{vec}(\text{RDM}^{(Li,c)}), \\ \text{vec}(\text{RDM}^{(Lj,c)}))
    \end{split}
\end{equation}
where $\text{RDM}^{(Li,c)}$ and $\text{RDM}^{(Lj,c)}$ are the RDMs from different layers in the same model and $\text{SCorr}$ is the Spearman correlation coefficient.

\section{Results}

\subsection{Category-selective neurons emerge in ANNs}

To begin our analysis, we focused on a single layer within the models. Specifically, we examined the final convolutional layer after ReLU activation in both ResNet and CLIP’s visual module (i.e., 'layer4.2.relu' in the ResNet-50 architecture). By statistically analyzing all neurons in this layer, we found that both models exhibited category-selective neurons for faces, bodies, scenes, and words. Figure \ref{Figure1}B presents the proportion of category-selective neurons (number of selective neurons relative to the total number of neurons in this layer) in both models, while Figure \ref{Figure1}C shows the activation strengths of these category-selective neurons for different image categories.

\begin{figure*}[ht]
\begin{center}
\fbox{\includegraphics[width=2\columnwidth]{Figure2.jpg}}
\caption{Spatial and channel-wise distribution of category-selective neurons in layer4.2.relu from ResNet and CLIP. (A) Feature map-wise distribution of category-selective neurons. Each heatmap represents the spatial distribution of face-, body-, scene-, and word-selective neurons across the feature map in the last convolutional layer (layer4.2.relu) for ResNet (top raw) and CLIP (bottom row). The color scale indicates the proportion of neurons at each spatial location in the feature map. (B) Quantification of feature map-wise variation in category-selective neuron distribution. (C) Channel-wise distribution of category-selective neurons. Each row represents the proportion of neurons selective to a given category across all channels in layer4.2.relu for ResNet (top) and CLIP (bottom). (D) Quantification of variation in the channel-wise distribution of category-selective neurons.} 
\label{Figure2}
\end{center}
\end{figure*}

In ResNet’s 'layer4.2.relu', we observed the highest proportion of face-selective neurons (2.20$\%$), followed by body-selective neurons (1.80$\%$), word-selective neurons (1.34$\%$), and scene-selective neurons (0.81$\%$). However, CLIP's 'layer.4.2.relu' exhibited significantly higher proportions of category-selective neurons across all categories—with 3.78$\%$ for faces, 2.50$\%$ for bodies, 5.98$\%$ for scenes, and 2.69$\%$ for words. This suggests that language learning enhances category selectivity in neural representations.

To further investigate the distribution of these selective neurons and how they differ between models, we analyzed their spatial arrangement within the feature maps of the convolutional layer. We considered two perspectives: feature map position and channel-wise organization.

For the feature map analysis, we would like to ask whether category-selective neurons are spatially localized. We aggregated all channels in this layer to visualize the cumulative occurrence ratio of category-selective neurons at different spatial positions in the feature map, allowing us to compare the distribution patterns between ResNet and CLIP. Figure \ref{Figure2}A shows the proportion of different category-selective neurons at each spatial location in the feature map (i.e., the number of selective neurons appearing at a given location divided by the total number of channels in the layer). Both models exhibit location-specific preferences for different category-selective neurons, rather than a uniform distribution. To quantify whether category-selective neurons in CLIP are more evenly distributed across feature maps compared to ResNet, we calculated the variance of their spatial distribution. If a model’s selective neurons are more evenly spread, different locations should exhibit similar proportions of selective neurons, resulting in a lower variance. Our analysis (Figure \ref{Figure2}B) confirms that ResNet exhibits a higher variance, indicating that its category-selective neurons are more spatially localized, whereas CLIP's neurons are more evenly distributed across the feature map. This suggests that ResNet favors local feature selectivity, while language learning in CLIP promotes a more global representation, potentially facilitating a more holistic understanding of category information.

\begin{figure*}[ht]
\begin{center}
\fbox{\includegraphics[width=2\columnwidth]{Figure3.jpg}}
\caption{Layer-wise analysis of category-selective neurons in ResNet and CLIP. (A) Proportion of category-selective neurons across layers. (B) Category selectivity index (CSI) across layers. From left to right, each panel shows the proportion of neurons selective for faces (blue), bodies (orange), scene (green), and words (red) at each layer of ResNet (solid lines) and CLIP (dashed lines).} 
\label{Figure3}
\end{center}
\end{figure*}

Similarly, to further examine how category-selective neurons are distributed across channels, we aggregated activations across all feature map locations to compute the cumulative occurrence ratio in each channel (Figure \ref{Figure2}C). This allowed us to assess whether ResNet and CLIP exhibit different patterns of channel-wise category selectivity. Our results reveal a key distinction: In ResNet, scene-selective neurons tend to be highly concentrated in a small subset of channels, with some channels showing a markedly higher proportion of scene-selective neurons than others. In contrast, face-, body-, and word-selective neurons in ResNet, as well as all category-selective neurons in CLIP, exhibit a more dispersed distribution across channels, without clear clustering in specific channels. Also, we quantified whether category-selective neurons are more distributed across channels by calculating the variance of their channel-wise distribution. It shows similar pattern to what we found in feature map-wise variation (Figure \ref{Figure2}D). This suggests that while scene representations in ResNet may rely on a small number of highly selective channels, other category-selective neurons—and particularly those in CLIP—are more distributed across channels. The broader distribution in CLIP further supports the hypothesis that language learning leads to a more evenly spread representation of category information within deep networks.

\subsection{Layer-wise analysis of category-selective neurons}

To extend our findings beyond a single layer, we systematically analyzed the layer-wise progression of category-selective neurons throughout the network (Figure \ref{Figure3}A). Across all four categories, ResNet exhibited a pattern where the proportion of category-selective neurons initially increased with depth before declining in later layers. This decline was particularly pronounced for scene-selective neurons, which sharply decreased in the final layers. In contrast, CLIP not only exhibited a consistently higher proportion of category-selective neurons than ResNet (except for body-selective neurons in some intermediate layers) but also maintained a higher proportion of category-selective neurons in later layers.

In addition to neuron proportions, we examined the category selectivity index (CSI) across layers (Figure \ref{Figure3}B). Unlike the neuron count results, ResNet exhibited substantially higher CSI values than CLIP in many layers. This indicates that although CLIP has more category-selective neurons overall, ResNet's category-selective neurons exhibit stronger specificity for their preferred category. Notably, face-selective neurons in ResNet showed exceptionally high CSI values, whereas body- and word-selective neurons had much weaker selectivity.

\begin{figure*}[ht]
\begin{center}
\fbox{\includegraphics[width=2\columnwidth]{Figure4.jpg}}
\caption{Cross-layer consistency of category-selective neurons in ResNet and CLIP. (A) Representational similarity between adjacent layers. (B) Fully-layer similarity matrices. Each heatmap represents the pairwise representational similarity between layers for category-selective neurons in ResNet (top) and CLIP (bottom). Darker colors indicate higher similarity. (C) Mean cross-layer representational similarity by averaging non-diagonal elements from (B).} 
\label{Figure4}
\end{center}
\end{figure*}

The discrepancy between the higher number of category-selective neurons in CLIP and their lower CSI values suggests a key difference in representation between the two models: In purely visual ResNet, fewer neurons are category-selective, but when they are, their selectivity is highly specific. In visual-language CLIP, a greater number of neurons exhibit category selectivity, but their responses to other categories are also stronger, leading to lower specificity. This suggests that language learning in CLIP distributes category-selective information more broadly across neurons, leading to a more dispersed and less sharply tuned category representation compared to ResNet.

\subsection{Cross-layer consistency of category-selective representations}

Our previous results suggest that CLIP has a greater number of category-selective neurons but with lower category selectivity index (CSI) values, indicating that these neurons may exhibit weaker category specificity compared to those in ResNet. Additionally, we observed that category-selective neurons in CLIP are more evenly distributed across the feature map, suggesting that category information may be encoded in a more spatially dispersed and global manner. If CLIP employs a more global encoding strategy, its category-selective neurons may also exhibit greater consistency across layers, forming a more structured and stable hierarchical representation.

To investigate this, we analyzed the cross-layer similarity of category representations. Specifically, for each model, category, and layer, we computed a 40×40 representational dissimilarity matrix (RDM) based on the model’s activation patterns. Each RDM captures the dissimilarity (1 - Pearson correlation coefficient) between the responses of 40 images from the same category. We then examined the correlation between RDMs of adjacent layers to measure how consistently category-selective representations are preserved across layers. A higher correlation between adjacent RDMs suggests a more stable and coherent category representation across hierarchical levels.

As shown in Figure \ref{Figure4}A, CLIP exhibits higher RDM correlations across adjacent layers compared to ResNet, particularly in higher layers. This suggests that category information in CLIP remains more stable and consistent across different layers, whereas in ResNet, category information undergoes greater transformation as it propagates through the network. However, an exception to this trend occurs for word-selective neurons: CLIP does not exhibit a clear advantage over ResNet in cross-layer similarity for word representations. This suggests that, unlike scene-related representations, word processing in CLIP may be less stable across hierarchical levels.

To further confirm this observation, we computed pairwise RDM correlations between all layers within each model(Figure \ref{Figure4}B). By averaging the off-diagonal elements of this similarity matrix, we found that CLIP exhibits overall higher representational consistency across layers (Figure \ref{Figure3}C), reinforcing the idea that category-selective neurons in CLIP are organized in a more stable and structured manner compared to ResNet.

\section{Discussion}

Through a systematic analysis of neuronal activity in ANNs, we demonstrate that ANNs contain category-selective neurons analogous to those found in the human brain, such as face-selective neurons (akin to FFA), body-selective neurons (akin to EBA), scene-selective neurons (akin to PPA), and word-selective neurons (akin to VWFA). This finding suggests that current ANN models exhibit category-selective responses similar to those observed in biological visual systems. Importantly, our approach mirrors human fMRI studies by incorporating scrambled scene and scrambled word images as control conditions to rule out low- and mid-level visual feature-driven selectivity. This ensures that the observed category selectivity genuinely reflects semantic processing rather than basic visual properties. Notably, the ANNs examined in our study were not explicitly trained to recognize the detailed identity of these object categories — ImageNet-trained ResNet, for example, does not specifically learn face or word classification. Yet, category-selective neurons still emerged, particularly for word images, despite the absence of word-related object categories in the training set. This suggests that category-selective representations can emerge in deep networks even without direct category-level supervision.

How does language learning influence category-selective neurons? To investigate the impact of language learning on category-selective neurons, we compared the distribution of these neurons in purely visual models (ResNet) and vision-language models (CLIP). Our findings show that CLIP exhibits a greater number of category-selective neurons but lower category selectivity index (CSI) compared to ResNet. This indicates that neurons in CLIP are more evenly distributed across different categories, whereas ResNet neurons exhibit more extreme selectivity. This result suggests that language learning increases the number of category-selective neurons while reducing their specificity, possibly because language enhances category-level similarities, leading neurons to respond to multiple categories rather than being strictly selective. Further analysis revealed that category-selective neurons in ResNet are more spatially localized within feature maps, whereas those in CLIP are more evenly distributed. The higher variance in feature map localization observed in ResNet suggests that its category-selective neurons depend on specific spatial locations, whereas CLIP, influenced by language supervision, encodes category information more globally. Additionally, category-selective neurons in CLIP exhibit greater consistency across layers, meaning that category-selective representations are more stable across different processing stages. This suggests that language learning promotes hierarchical consistency in category representation, likely by reinforcing high-level semantic structures across multiple layers.

Our results suggest that ResNet relies more on local features for category discrimination, whereas CLIP encodes category information in a more global manner. This distinction is reflected in the spatial distribution of category-selective neurons: In ResNet, category-selective neurons are more localized within specific regions of feature maps, indicating that object category information may be encoded in particular spatial locations (e.g., some positions may be more sensitive to faces). In CLIP, category-selective neurons are more evenly distributed, suggesting that language supervision enhances the global representation of category information, allowing neurons to encode categories more broadly across the network rather than relying on specific local features. This may contribute to CLIP’s superior generalization ability. Our cross-layer analysis further supports this interpretation. We found that face- and body-selective neurons in CLIP exhibit greater stability across layers compared to those in ResNet, likely because these categories have well-defined semantic representations in language (e.g., “face” and “body” are explicit linguistic concepts). Scene-selective neurons showed the largest improvement in cross-layer consistency in CLIP, suggesting that language supervision enhances global scene representation, whereas ResNet may rely more on local texture-based processing. In contrast, word-selective neurons in CLIP did not show significant improvements in cross-layer consistency, which may be due to two factors: (1) word-selective neurons are inherently less frequent in both models, and (2) similar to the human VWFA, word-selective neurons may primarily encode visual word forms rather than their semantic content, leading to weaker hierarchical consistency.

While previous studies have identified face-selective or word-selective neurons in CNNs (\cite{Agrawal2024, Baek2021, Xu2021a}), our study is the first to systematically analyze multiple category-selective neurons (face, body, scene, and word) in structurally controlled models, comparing purely visual (ResNet) and vision-language (CLIP) architectures. Beyond confirming the existence of category-selective neurons, we examined how language supervision influences their formation, revealing its effects on neuron distribution, selectivity strength (CSI), and cross-layer stability. Additionally, we introduced the CSI (Category Selectivity Index) as a quantitative measure, allowing for rigorous comparisons across layers and models rather than relying solely on activation strength. Our study provides novel insights into: (1) The hierarchical emergence of category-selective neurons in ANNs, mirroring the increasing category selectivity observed in biological vision. (2) The role of language learning in expanding category-selective neurons while reducing their specificity, suggesting that language enhances category relationships. (3) The greater spatial uniformity and cross-layer stability of category-selective neurons in CLIP compared to ResNet, highlighting the effect of language in promoting global category representations. These findings contribute to a deeper understanding of ANNs as models of human vision and provide new insights into how multimodal learning reshapes visual representations.

\section{Limitations and future directions}

Despite providing new insights into category-selective neurons in ANNs, our study leaves several open questions: First, our study focuses on trained models, rather than examining category-selective neurons in randomly initialized networks. Some studies have suggested using untrained networks as an analogy for the “infant brain,” (\cite{Baek2021, Kim2021, Zhou2022}) but we argue that this analogy is flawed since infant brains already possess structured neural connections at birth. Future work should investigate how category selectivity emerges over different learning stages, comparing early-stage and late-stage training dynamics with human visual development. Second, while our study reveals category-selective neurons in ANNs, it does not directly compare them to category-selective regions in the human brain (e.g., FFA, PPA, VWFA). Future research could leverage fMRI or ECoG data to compare category-selective activations in ANN neurons and human cortical regions, further validating the biological plausibility of ANN representations. Third, our study characterizes the distribution of category-selective neurons but does not directly test their computational importance. Future work could employ ablation experiments to selectively remove category-selective neurons and observe their impact on object recognition or scene understanding tasks. Fourth, the ANNs in our study were trained on general object classification tasks (ImageNet for ResNet and contrastive learning for CLIP). Future research could explore how category selectivity changes when models are trained on specialized tasks. For example, does fine-tuning a model on face recognition enhance face-selective neurons while weakening other category selectivity? Understanding task-driven modifications of category selectivity could provide further insights into how different learning objectives shape visual representations. Finally, beyond classic object categories, many other categorical dimensions—such as real-world size, animacy, food-relatedness, and spikiness (\cite{Bao2020, Coggan2023, Huang2022, Jain2023, Khaligh-Razavi2018, Khosla2022, Konkle2012a, Konkle2013, Lu2023d}) —have been shown to be encoded in both the brain and ANNs. Recent studies suggest that models optimized using neural data exhibit stronger food-related representations than those trained solely on images (\cite{Lu2024a, Lu2024b}). Future work should explore how these categorical features are encoded differently in ANNs and the human brain, and how language learning influences their representation.

\section{Conclusion}

Our study provides the first systematic analysis of category-selective neurons in ANNs across hierarchical layers, revealing the emergence of category-selective neurons in ANNs and the influence of language learning on category selectivity in neural representations. We show that ANNs naturally develop category-selective neurons, and these neurons become more prevalent in deeper layers. Language learning increases the number of category-selective neurons but reduces their specificity, while also promoting more uniform spatial distribution and greater hierarchical consistency. These findings offer new insights into ANNs as models of human vision and provide a theoretical foundation for understanding how multimodal learning shapes visual representations.

\newpage

\bibliographystyle{ccn_style}

%\setlength{\bibleftmargin}{.125in}
%\setlength{\bibindent}{-\bibleftmargin}

\bibliography{ccn_style}


\end{document}
