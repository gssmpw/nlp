\appendix
\section{Appendix}


% \begin{table}[ht]
%   \centering
%   \setlength\tabcolsep{3pt}
%   \caption{Influence of external tool augmentation (F1) on WikiGold, WNUT-17, and GENIA. }
%   \label{tab:ablation studies-chapter5-appendix}
%   \begin{tabular}{l ccc}
%     \toprule
%   \multirow{2}{*}{\bf Model} & \multicolumn{3}{c}{\bf Datasets}\\
%     \cmidrule(r){2-4}
%     & \bf WikiGold & \bf WNUT-17 & \bf GENIA  \\
%  \midrule
%  SALLM~\citep{DBLP:conf/emnlp/XieLZZLW23} & 72.14 & 38.66 & 42.33   \\
%     CMAS (ours) & $\textbf{76.23}$ & $\textbf{47.98}$ & $\textbf{50.00}$\\ 
%      \midrule
%      \multicolumn{4}{c}{\bf External tool augmentation} \\
%      \midrule
%      Word segmentation & \textbf{76.92} & 47.63 & 49.22 \\
%      POS tag & 76.14 & 48.11 & 49.76\\
%      Constituency tree & 75.71 & 47.44 & 49.64\\
%      Dependency tree & 76.27 & \textbf{49.19} & \textbf{51.47}\\
%   \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[h]
	\centering
        \setlength\tabcolsep{1.8pt}
	\caption{Numbers of different error types on GENIA. Numbers in \textbf{bold} denote the best results for the corresponding error type, i.e., the least errors, while numbers \underline{underlined} represent the second-best results. }
	\begin{tabular}{l l c c c c}
		\toprule
		\multicolumn{2}{l}{\bf Error Types} & \bf ChatIE & \bf SALLM & \bf SILLM & \bf CMAS  \\ 
		\midrule
		Type & OOD types  & \phantom{0}91  & \phantom{00}\underline{2} & \phantom{00}\textbf{0} & \phantom{00}\textbf{0} \\ 
		~ &  Wrong types & \phantom{0}\underline{52}  & \phantom{0}\underline{52} & 123 & \phantom{0}\textbf{32} \\ 
		\midrule
		Boundary & Contain gold & \phantom{0}34 & \phantom{00}\underline{7} & \phantom{0}10 & \phantom{00}\textbf{3}  \\ 
		~ & Contained by gold & \phantom{0}\textbf{42} & 101 & 240 & \phantom{0}\underline{50} \\ 
		~ & Overlap with gold & 145 & \underline{135} & 318 & \phantom{0}\textbf{72}\\ 
		\midrule
		\multicolumn{2}{l}{Completely-Os}& 122 & \underline{109} & 241 & \phantom{0}\textbf{59} \\ \midrule
		\multicolumn{2}{l}{OOD mentions} & \phantom{00}\underline{2} & \phantom{00}\textbf{0} & \phantom{00}\textbf{0} & \phantom{00}\textbf{0}  \\ 
        \midrule		
  		\multicolumn{2}{l}{Omitted mentions} & 422  & \underline{342} & 1,109 & \textbf{303}\\ 
        \midrule
		\multicolumn{2}{l}{Total} & 910  & \underline{748} & 2,041 & \textbf{519} \\ 
        \bottomrule
	\end{tabular}
	\label{tab:error types-chapter5}
\end{table}

\begin{figure}[h!]
  \centering
  \subfigure[WNUT-17.]{
    \includegraphics[width=0.48\linewidth]{figures/num-demo-WNUT.pdf}}
   \subfigure[GENIA.]{
    \includegraphics[width=0.48\linewidth]{figures/num-demo-GENIA.pdf}}
    \caption{Influence of task demonstration amount (F1) on WNUT-17 and GENIA.}
  \label{fig:num-demo}
\end{figure}

\begin{figure}[h!] 
    \centering
    \includegraphics[width=0.9\linewidth]{figures/errors-GENIA.pdf}    
    \caption{Percentages of different error types on GENIA using \ac{CMAS}.}
    \label{fig:errors-chapter5}
\end{figure}

\subsection{Prompts used in CMAS}
Prompt~\ref{tab:self-annotation-chapter5},~\ref{tab:TRF-extractor-chapter5},~\ref{tab:discriminator-chapter5}, and~\ref{tab:overall-predictor-chapter5} show prompts used in the self-annotator, TRF extractor, demonstration discriminator, and overall predictor, respectively.

\subsection{Statistics of the datasets used}
Table~\ref{tab:datasets-chapter5} demonstrates the detailed statistics of the dataset used in our experiments.





\subsection{Influence of task demonstration amount}
\label{subsec:task demonstration amount}
To assess the influence of the number of task demonstrations, we evaluate \ac{CMAS} and SILLM with varying numbers of task demonstrations, ranging from 2 to 20. Figure~\ref{fig:num-demo} details the zero-shot \ac{NER} performance on the WNUT-17 and GENIA datasets.
It is important to note that other baselines are excluded from this analysis as they do not incorporate task demonstrations in their prompt templates. 
The results in Figure~\ref{fig:num-demo} indicate that \ac{CMAS} consistently outperforms SILLM across various numbers of task demonstrations on both datasets. Specifically, \ac{CMAS} achieves an average F1-score improvement of 18.70\% and 10.72\% over SILLM on the WNUT-17 and GENIA datasets, respectively. This is, \ac{CMAS} is able to effectively discriminate against irrelevant task demonstrations, maintaining robustness across different numbers of task demonstrations.

% \subsection{Influence of external tool augmentation}
% \label{subsec:external tool}
% Similar to SALLM, \ac{CMAS} is readily adaptable for augmentation with external syntactic tools. Following~\citet{DBLP:conf/emnlp/XieLZZLW23}, we obtain four types of syntactic information (i.e., word segmentation, POS tags, constituency trees, and dependency trees) via a parsing tool~\citep{DBLP:conf/emnlp/HeC21} and integrate the syntactic information into the overall predictor of \ac{CMAS} using a combination of tool augmentation and syntactic prompting strategies. As shown in Table~\ref{tab:ablation studies-chapter5-appendix}, the inclusion of dependency tree information improves \ac{CMAS}'s performance by 2.52\% and 2.94\% on WNUT-17 and GENIA, respectively. These results demonstrate that the integration of appropriate external tools further enhances the performance of \ac{CMAS}.


\subsection{Percentages of different error types}

Table~\ref{tab:error types-chapter5} and Figure~\ref{fig:errors-chapter5} present the statistics and percentages of error types on GENIA using \ac{CMAS}, respectively.




\begin{prompt*}[t]
	\centering
	\caption{Prompts used for the self-annotator.}
	\label{tab:self-annotation-chapter5}
	\begin{tabular}{l}
		\toprule
		Prompts used for the self-annotator\\ 
		\midrule
            Given entity label set: {[}`Organization', `Person', `Location', `Miscellaneous'{]},  
		please 
            recognize the named entities in the given text. \\ Provide the answer in the following JSON format: [\{`Entity Name': `Entity Label'\}]. If there is no entity in the text, return the foll-\\owing empty list: [].
		\\
            \\
		Text: The album cover's artwork was provided by Mattias Noren.\\
		Answer: \\
        \bottomrule
	\end{tabular}
\end{prompt*}


\begin{prompt*}[t]
	\centering
	\caption{Prompts used for the \ac{TRF} extractor.}
	\label{tab:TRF-extractor-chapter5}
	\begin{tabular}{l}
		\toprule
        Prompts used for the TRF extractor \\
        \midrule
        Here, we provide some example sentences with the corresponding TRFs.
        TRFs mean type-related features,  which are tokens that \\ are strongly associated with the \\ entity types and relevant to these sentences. \\ \\
        Given entity label set: {[}`Organization', `Person', `Location',  `Miscellaneous'{]}, please identify the TRFs for the target sentences.
        \\ Provide the answer in the following list format: {[}`TRF1', `TRF2', \ldots{]}. \\
        \\
        Text: His parents were encouraged by a friend to develop the child's musical talents and he studied classical piano in the United \\ States.\\
        TRF set: [`father', `songs', `player', `United States', `French'] \\
        \ldots\ldots\\
        (Selected demonstrations with pseudo TRF labels)\\
        \ldots\ldots\\
        Target sentence: UK Edition came with the OSC-DIS video, and most of the tracks were re-engineered.\\
        TRF set: \\
        \bottomrule
	\end{tabular}
\end{prompt*}

\begin{prompt*}[t]
	\centering
	\caption{Prompts used for the demonstration discriminator.}
	\label{tab:discriminator-chapter5}
	\begin{tabular}{l}
		\toprule
        Prompts used for the demonstration discriminator \\
        \midrule
        Here, we provide some example sentences and the corresponding entity labels and TRFs.
        TRFs mean type-related features,  which \\ are tokens that are strongly associated with the entity types and relevant to these sentences. 
        \\ \\
        Given entity label set: {[}`Organization', `Person', `Location', `Miscellaneous'{]}, target sentence: `UK Edition came with the OSC-DIS \\ video, and most of the tracks were re-engineered.' and its TRF set:[`video', 'tracks'].\\
        \\ Please predict the helpfulness scores of each sentence, which indicates the degree to which providing the current example can \\ aid in extracting named entities from the target sentence. The score ranges from 1 to 5, with 1 being the least helpful and 5 being \\ the most helpful. Provide answer in the following JSON format: {[}\{`sentence id': `helpfulness score'\}{]}.
        \\
        \\
        Text: His parents were encouraged by a friend to develop the child's musical talents and he studied classical piano in the United \\ States.\\
        TRF set: [`father', `songs', `player', `United States', `French']\\
        Entity labels: [\{`United States': `Location'\}]\\
        \ldots\ldots\\
        (Selected demonstrations with pseudo TRF labels and entity labels)\\
        \ldots\ldots\\
        Answer: 
        \\
        \bottomrule
	\end{tabular}
\end{prompt*}

\begin{prompt*}[t]
	\centering
	\caption{Prompts used for the overall predictor.}
	\label{tab:overall-predictor-chapter5}
	\begin{tabular}{l}
	\toprule
        Prompts used for the overall predictor \\
        \midrule
        \ldots\ldots\\
        (The corresponding prompt for the demonstration discriminator and its response) \\
        \ldots\ldots\\
        Given entity label set: {[}`Organization', `Person', `Location',  `Miscellaneous'{]}, please consider the TRFs and helpfulness scores for \\ the above sentences to recognize the named entities in the target sentence. Provide the answer in the following JSON format:  [\{\\`Entity Name': `Entity Label'\}]. If there is no entity in the text, return the following empty list: [].  
        \\
        Target sentence: UK Edition came with the OSC-DIS video, and most of the tracks were re-engineered.\\
        Answer: \\
        \bottomrule
	\end{tabular}
\end{prompt*}

\begin{table*}[h]
	\centering
 	\caption{Statistics of the datasets used. The training set is formed by combining the original training and development sets.}
	\begin{tabular}{l c c c c c c}
		\toprule
		\textbf{Dataset} & \textbf{CoNLL03} & \textbf{WikiGold} & \textbf{WNUT-17} & \textbf{OntoNotes} & \textbf{GENIA}  & \textbf{BioNLP11}    \\ \midrule
		\#Train & 14,382 & 1,422 & 4,403 & 68,452 & 16,692 & 3,217  \\ 
		\#Test & 3,453 & 274 & 1,287 & 8,262 & 1,854 & 1,961  \\
		\bottomrule
	\end{tabular}
	\label{tab:datasets-chapter5}
\end{table*}


