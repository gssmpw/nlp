\section{CMAS: A Cooperative Multi-Agent System}
In this section, we present the four main agents of the proposed \ac{CMAS} as described in Figure~\ref{fig:framework-chapter5}: (i) a self-annotator (see Section~\ref{subsec:self-annotator-chapter5}), (ii) a type-related feature extractor (see Section~\ref{subsec:TRF extractor-chapter5}), (iii) a demonstration discriminator (see Section~\ref{subsec:discriminator-chapter5}), and (iv) an overall predictor (see Section~\ref{subsec:predictor}).

First, the self-annotator uses an \ac{LLM} to produce self-annotated data by making predictions on the unlabeled corpus and preliminarily retrieves demonstrations using a similarity-based strategy. Next, the type-related feature extractor automatically acquires pseudo-labels for \acp{TRF} using mutual information criteria and identifies words or phrases strongly associated with different entity types using specialized \ac{ICL}. Subsequently, the demonstration discriminator incorporates a self-reflection mechanism to evaluate the helpfulness scores of each retrieved demonstration for predictions on the target input sentence. Finally, given the extracted \acp{TRF} and predicted helpfulness scores, the overall predictor performs inference on the target sentences by employing question-answering prompts and a two stage self-consistency strategy. 

\subsection{Self-annotator for unlabelled data}
\label{subsec:self-annotator-chapter5}
As mentioned in Section~\ref{sec:intro-chapter5} and~\ref{sec:formulation-chapter5}, we only have access to an unlabeled corpus $\mathcal{D}_{u}$ in zero-shot \ac{NER}. Inspired by the self-improvement strategy~\citep{DBLP:journals/corr/abs-2311-08921}, we specify an \ac{LLM}-based self-annotator to guide the inference of \acp{LLM}, which first annotates the unlabeled corpus with zero-shot prompting and then preliminarily selects demonstrations from the self-annotated data for each target sentence.

\noindent \textbf{Self-annotation.} 
For each unlabeled sample $x_i\in\mathcal{D}_{u}$, we generate predictions using LLMs with zero-shot prompting. This process is formulated as follows:
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\mathbf{y}_i = \mathop{\arg\max}_{\mathbf{y}} P_{s}(\mathbf{y} | \mathbf{T}_{s}, \mathbf{x}_i),
\end{equation}
\noindent
where $\mathbf{T}_{s}$ is the prompt template used for self-annotation. Prompt~\ref{tab:self-annotation-chapter5} (in the Appendix) illustrates an instance of $\mathbf{T}_{s}$. 
The predictions $y_i = \{(e_i^j, t_i^j)\}_{j=1}^{l}$ consist of pairs of entity mentions and types, where $l$ is the number of the predicted entity mentions. $P_{s}$ is the output probability of the self-annotator. To enhance the reliability of the annotations, we use self-consistency \citep{wang2022self} and adopt a \emph{two-stage majority voting} strategy~\citep{DBLP:conf/emnlp/XieLZZLW23}. 
We sample multiple responses from the model. In the first stage, we consider a candidate mention as an entity if it is present in more than half of all responses; otherwise, we discard it. In the second stage, for each mention retained from the first stage, we determine the entity type label based on the majority agreement among the responses and assign this as the final predicted label.

% We apply self-consistency (SC) \citep{wang2022self} to obtain a SC score for each prediction, which will be used in step 2 for reliable annotation selection. We sample multiple answers from the model, and the vote for each predicted entity $(e_i^j,t_i^j)$ is the times it appeared in all the sampled answers, which we denoted as entity-level SC score $c_i^j$. 
% Then we get the sample-level SC score $c_i$ for each input sentence $x_i$ by taking the average SC score over all predicted entities in this sentence, i.e., $c_i=\frac{1}{m}\sum_j c_i^j$.
%The self-annotated data with entity-level and sample-level SC scores are shown in the right part of Fig. \ref{fig:method}.
% For each self-annotated sample with SC scores, we can denote it as $(x_i, \{(e_i^j,t_i^j, c_i^j)\}_{j=1}^{m}, c_i)$. We assume that a higher SC score indicates a higher reliablity. Thus, we investigate the three following strategies for reliable annotation selection. 
% (1) \textit{Entity-level threshold filtering}, which drops the predicted entity $e_i^j$ if $c_i^j<Th\_entity$, where $Th\_entity$ is the threshold for entity-level SC score. (2) \textit{Sample-level threshold filtering}, which drops the sample $x_i$ if $c_i<Th\_sample$, where $Th\_sample$ is the threshold for sample-level SC score. 
% \textit{Two-stage majority voting} \citep{DBLP:conf/emnlp/XieLZZLW23}, is an entity-level selection method, which first votes for the most consistent entity spans, then the most consistent types based on the voted spans.

\noindent \textbf{Preliminary demonstration selection.}
When a target sentence $\mathbf{x}^q$ arrives, our goal is to retrieve $k$ relevant demonstrations $\mathbf{S}=\{\mathbf{x}_{i},\mathbf{y}_{i}\}_{i=1}^k$ from the reliable self-annotated dataset. 
To achieve a better trade-off between similarity, diversity, and reliability, we employ a \emph{diverse nearest neighbors with self-consistency ranking} strategy~\citep{DBLP:journals/corr/abs-2311-08921}, which first retrieves $K$ nearest neighbors based on cosine similarities between sentence representations and then selects samples with the top-$k$ self-consistency scores.

% We employ the following four methods for demonstration retrieval. \textit{Diverse nearest with SC ranking}, proposed by this work to achieve a better trade-off between the similarity, diversity and reliability of self-annotated demonstrations. After retrieving $K$ nearest neighbors, we select samples with the top-$k$ sample-level SC scores.
% it ranks them with sample-level SC scores, then select samples with the top-$k$ scores.

\subsection{Type-related feature extractor}
\label{subsec:TRF extractor-chapter5}
To capture correlations between contexts surrounding entities, we design an \ac{LLM}-based \acf{TRF} extractor using specialized \acf{ICL}.
We use mutual information criteria~\citep{DBLP:conf/emnlp/WangZCRRR23} to generate pseudo \ac{TRF} labels $\{\mathcal{R}_{i}\}_{i=1}^k$ for self-annotated demonstrations $\mathcal{S}=\{\mathbf{x}_{i},\mathbf{y}_{i}\}^k_{i=1}$, which are selected for the test input $\mathbf{x}^q$. Building on this, we apply the specialized \ac{ICL} prompts to identify relevant \acp{TRF} $\mathcal{R}^{q}$ for $\mathbf{x}^q$.


% given each test sentence $\mathbf{x}^q$, the \ac{TRF} extractor aims to identify the relevant \ac{TRF} set $\mathcal{R}^q$. Moreover, we also generate pesudo 

% selected demonstrations $\mathcal{S}=\{\mathbf{x}_{i},\mathbf{y}_{i}\}^{k}_{i=1}$
% , we design a type-related feature (TRF) extractor to capture contextual correlations surrounding entities.

\noindent \textbf{Pseudo-label generation.}
To facilitate \ac{TRF} extraction for the target sentence $\mathbf{x}^q$, we generate pseudo TRF labels for its self-annotated demonstrations $\mathcal{S}=\{\mathbf{x}_i, \mathbf{y}_i\}_{i=1}^k$ using a mutual infor\-mation-based method~\citep{DBLP:conf/emnlp/WangZCRRR23}. 
We define $\mathcal{D}_t$ as the set containing all sentences from the unlabeled corpus $\mathcal{D}_u$ where entities of the $t$-th type appear. The complementary set, $\mathcal{D}_u \backslash \mathcal{D}_t$, includes sentences that do not contain entities of the $t$-th type. To identify TRFs $\mathcal{R}^t$ associated with the $t$-th entity type, we apply the following filtering condition:
%
\begin{equation}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\frac{C_{\mathcal{D}_u \backslash \mathcal{D}_t}(\mathbf{w})}{C_{\mathcal{D}_t}(\mathbf{w})} \leq \rho, \quad  C_{\mathcal{D}_t}(\mathbf{w}) > 0,
\end{equation}
%
where $C_{\mathcal{D}_t}(\mathbf{w})$ represents the count of the m-gram $\mathbf{w}$ in $\mathcal{D}_t$, and $C_{\mathcal{D}_u \backslash \mathcal{D}_t}(\mathbf{w})$ represents its count in the rest of $\mathcal{D}_u$ excluding $\mathcal{D}_t$. The parameter $\rho$ is an m-gram frequency ratio hyperparameter. By applying this criterion, we ensure that $\mathbf{w}$ is considered a part of the TRF set of $\mathcal{D}_t$ only if its frequency in $\mathcal{D}_t$ is significantly higher than its frequency in other sets ($\mathcal{D}_u \backslash \mathcal{D}_t$). Given the smaller size of $\mathcal{D}_t$ relative to $\mathcal{D}_u \backslash \mathcal{D}_t$, we select $\rho \geq 1$ but avoid excessively high values to include features associated with $\mathcal{D}_t$ and potentially relevant to other entity types within the TRF set of $\mathcal{D}_t$. Based on this, for every self-annotated demonstration $\mathbf{x}_i \in \mathcal{S}$, we compute the Euclidean distance between BERT-based embeddings of each extracted \ac{TRF} $\mathbf{w}$ and each token in $\mathbf{x}_i$, selecting the top-5 features as pseudo \ac{TRF} labels $\mathcal{R}_i$ of  $\mathbf{x}_i$.

\noindent \textbf{TRF Extraction.}
Given the target sentence $\mathbf{x}^q$ and its corresponding demonstrations $\mathcal{S}_d = \{\mathbf{x}_i, \mathbf{y}_i, \mathcal{R}_i\}_{i=1}^k$ with pseudo-labels, we construct specialized \ac{ICL} prompts to facilitate the identification of relevant \acp{TRF} for $\mathbf{x}^q$.
The inference process is formulated as:
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\mathcal{R}^q = \mathop{\arg\max}_{\mathcal{R}} P_{e}(\mathcal{R} | \mathbf{T}_{e}, \mathcal{S}_d, \mathbf{x}^q),
\end{equation}
where $\mathbf{T}_{e}$ represents the specialized \ac{ICL} prompt template. Prompt~\ref{tab:TRF-extractor-chapter5} (in the Appendix) provides a detailed instance of $\mathbf{T}_{e}$.
$P_{e}(\cdot)$ represents the output probability of the \ac{TRF} extractor.

\subsection{Demonstration discriminator}
\label{subsec:discriminator-chapter5}
As mentioned in Section~\ref{sec:intro-chapter5}, demonstrations retrieved using shallow similarity-based strategies can be highly irrelevant to target sentences, severely misleading the predictions of \acp{LLM}. To address this issue, we employ an \ac{LLM}-based demonstration discriminator with a self-reflection mechanism~\citep{DBLP:conf/iclr/AsaiWWSH24,DBLP:conf/nips/ShinnCGNY23} to automatically evaluate the helpfulness of each initially selected demonstration for making predictions on the target test sentences.
To achieve this, we consider the \acp{TRF} of both the demonstrations and the target sentence, extracted by the \ac{TRF} extractor (see Section~\ref{subsec:TRF extractor-chapter5}), as well as the self-labeled entity labels from the self-annotator (see Section~\ref{subsec:self-annotator-chapter5}). The prompts used for helpfulness score prediction are shown in detail in Prompt~\ref{tab:discriminator-chapter5} (in the Appendix). Formally, given demonstrations $\mathcal{S}_{d}=\{\mathbf{x}_i, \mathbf{y}_i, \mathcal{R}_{i}\}_{i=1}^k$ selected for the target sentence $\mathbf{x}^q$ with extracted \acp{TRF} $\mathcal{R}^q$, the corresponding helpfulness scores $\{h_i\}^k_{i=1}$ are predicted by
%
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
    h_i = \mathop{\arg\max}_h P_{d}(h|\mathbf{T}_d, \mathcal{S}_{d}, \mathcal{R}^q, \mathbf{x}_i, \mathbf{x}^q),
\end{equation}
%
where $\mathbf{T}_d$ denotes the prompt template used in the demonstration discriminator. $P_{d}(\cdot)$ denotes the output probability of the demonstration discriminator.

\subsection{Overall predictor}
\label{subsec:predictor}
Finally, given the extracted \acp{TRF} and predicted helpfulness scores, we establish an \ac{LLM}-based overall predictor to conduct inference on the target sentences. Let $\mathcal{S}_{o}=\{\mathbf{x}_i, \mathbf{y}_i, h_{i}, \mathcal{R}_{i}\}_{i=1}^k$ represent the self-annotated demonstrations selected for the test input $\mathbf{x}^q$, along with the corresponding helpfulness scores $\{h_i\}_{i=1}^k$ and \acp{TRF} $\{\mathcal{R}_i\}_{i=1}^k$. As Prompt~\ref{tab:overall-predictor-chapter5} (in the Appendix) shows, \ac{CMAS} conducts overall predictions on $\mathbf{x}^q$ by integrating the dialogue in the demonstration discriminator and constructing a question-answering prompt template $\mathbf{T}_{o}$. The overall prediction is obtained by 
%
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\mathbf{y}^q = \mathop{\arg\max}_{\mathbf{y}} P_{o}(\mathbf{y}|\mathbf{T}_{o}, \mathcal{S}_{o}, \mathbf{x}^q),
\end{equation}
%
where $P_{o}(\cdot)$ denotes the output probability of the overall predictor.
Similar to the self-annotation process (see Section~\ref{subsec:self-annotator-chapter5}), to improve the reliability and consistency of the final results, we sample multiple responses from \acp{LLM} and adopt a two-stage self-consistency strategy.

% \bigskip\noindent%
Now that we have described the four specialized agents that make up the core of \ac{CMAS}, we recall the coordinated workflow of \ac{CMAS}, as illustrated in Figure~\ref{fig:framework-chapter5}. To start, the self-annotator incorporates a self-improvement strategy and employs an \ac{LLM} to generate self-annotated data from an unlabeled corpus $\mathcal{D}_{u}$, and preliminarily retrieves reliable demonstrations $\mathbf{S} = \{\mathbf{x}_{i}, \mathbf{y}_{i}\}_{i=1}^{k}$ for each test input $\mathbf{x}^{q}$.
Next, the type-related feature extractor uses mutual information criteria to derive pseudo \ac{TRF} labels $\{\mathcal{R}_{i}\}_{i=1}^{k}$ for the self-annotated demonstrations and facilitates identifying relevant contextual correlations $\mathcal{R}^{q}$ for $\mathbf{x}^{q}$ using specialized \ac{ICL}.
Furthermore, the demonstration discriminator, considering the \acp{TRF} of both the target sentence and its self-annotated demonstrations, applies a self-reflection mechanism to automatically assess the helpfulness scores $\{h_i\}^{k}_{i=1}$ of selected demonstrations in making predictions for each target sentence $\mathbf{x}^{q}$.
Finally, the overall predictor constructs a question-answering prompt template $\mathbf{T}_{o}$, synthesizing \acp{TRF} and helpfulness scores, to obtain the final prediction $\mathbf{y}^{q}$ for each target sentence $\mathbf{x}^{q}$.
Through their specialized abilities and communications, the designed agents work collaboratively to enhance both the effectiveness and generalizability of our proposed \ac{CMAS} for zero-shot and few-shot NER tasks (see Section~\ref{sec:results-chapter5} and~\ref{sec:analysis-chapter5}).

% \subsection{External Tools}
% \label{subsec:External tools}
% In addition to the abovementioned agents, \ac{CMAS} is able to employ external tools to further enhance the recognition performance. Specifically, 

% \subsection{Discussions}
% \label{subsec:Discussions}
% As mentioned in Sec.~\ref{sec:intro-chapter5}, previous zero-shot \ac{NER} methods can be reformulated within our proposed \ac{CMAS}. 