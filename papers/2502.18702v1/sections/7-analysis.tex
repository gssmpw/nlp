 \section{Analysis}
 \label{sec:analysis-chapter5}
Now that we have addressed our research questions, we take a closer look at \ac{CMAS} to analyze its performance and generalizability. We examine the contributions of the type-related feature extractor and the demonstration discriminator to its effectiveness (see Section~\ref{subsec:ablation studies}), investigate its generalizability to different \ac{LLM} backbones (see Section~\ref{subsec:LLM backbones}) and varying numbers of task demonstrations (see Section~\ref{subsec:task demonstration amount}), and assess its capability in error correction (see Section~\ref{subsec:error analysis}).

\subsection{Ablation studies}
\label{subsec:ablation studies}
To study the individual contributions of each component to \ac{CMAS}'s performance, we conduct ablation studies on the WikiGold, WNUT-17, and GENIA datasets. The results are presented in Table~\ref{tab:ablation studies-chapter5}. 

Given that the demonstration discriminator relies on entity type-related information from the \ac{TRF} extractor, it is not feasible to independently remove the \ac{TRF} extractor. When we ablate only the demonstration discriminator (`- Discriminator'), the overall predictor incorporates only \ac{TRF} for retrieved demonstrations and target sentences. This exclusion results in a significant drop in \ac{CMAS}'s performance across all three datasets. For instance, \ac{CMAS} achieves 3.34\% and 5.59\% higher F1-scores on the WikiGold and GENIA datasets, respectively, compared to its model variant without the demonstration discriminator. These findings highlight the crucial role of evaluating the usefulness of retrieved demonstrations in making predictions. In scenarios where both the demonstration discriminator and the \ac{TRF} extractor are ablated (`- TRF Extractor'), \ac{CMAS} reverts to the baseline model, SILLM. The results indicate that identifying contextual correlations surrounding entities considerably enhances SILLM's performance. 
% Notably, the variant of \ac{CMAS} that includes only the \ac{TRF} extractor consistently outperforms all baselines across the datasets. 
In summary, both the demonstration discriminator and the \ac{TRF} extractor contribute markedly to \ac{CMAS}'s performance improvements over the baselines in the zero-shot \ac{NER} task.

Furthermore, similar to SALLM, \ac{CMAS} is readily adaptable for augmentation with external syntactic tools. Following~\citet{DBLP:conf/emnlp/XieLZZLW23}, we obtain four types of syntactic information (i.e., word segmentation, POS tags, constituency trees, and dependency trees) via a parsing tool~\citep{DBLP:conf/emnlp/HeC21} and integrate the syntactic information into the overall predictor of \ac{CMAS} using a combination of tool augmentation and syntactic prompting strategies. As shown in Table~\ref{tab:ablation studies-chapter5}, the inclusion of dependency tree information improves \ac{CMAS}'s performance by 2.52\% and 2.94\% on WNUT-17 and GENIA, respectively. These results demonstrate that the integration of appropriate external tools further enhances the performance of \ac{CMAS}.


\begin{table}[ht]
  \centering
  \setlength\tabcolsep{3pt}
  \caption{Ablation studies (F1) on WikiGold, WNUT-17, and GENIA. }
  \label{tab:ablation studies-chapter5}
  \begin{tabular}{l ccc}
    \toprule
  \multirow{2}{*}{\bf Model} & \multicolumn{3}{c}{\bf Datasets}\\
    \cmidrule(r){2-4}
    & \bf WikiGold & \bf WNUT-17 & \bf GENIA  \\
  \midrule
    Vanilla~\citep{DBLP:conf/emnlp/XieLZZLW23,DBLP:journals/corr/abs-2311-08921}  & 74.27  & 40.10 & 43.47   \\
    ChatIE~\citep{wei2023zero}  & 56.78  & 37.46
 & 47.85    \\
    Decomposed-QA~\citep{DBLP:conf/emnlp/XieLZZLW23}  & 64.05  & 42.38
 & 34.03    \\

 SALLM~\citep{DBLP:conf/emnlp/XieLZZLW23} & 72.14 & 38.66 & 42.33   \\
 \midrule
    CMAS (ours) & $\textbf{76.23}$ & $\textbf{47.98}$ & $\textbf{50.00}$\\ 
          \quad - Discriminator& 73.76 & 45.44 & 48.41  \\
     \quad - TRF extractor& 72.72 & 41.65 & 45.66  \\
     \midrule
     \multicolumn{4}{c}{\bf External tool augmentation} \\
     \midrule
     Word segmentation & \textbf{76.92} & 47.63 & 49.22 \\
     POS tag & 76.14 & 48.11 & 49.76\\
     Constituency tree & 75.71 & 47.44 & 49.64\\
     Dependency tree & 76.27 & \textbf{49.19} & \textbf{51.47}\\
  \bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
	\centering
        \setlength\tabcolsep{3pt}
 	\caption{Influence of different \ac{LLM} backbones (F1) on WNUT-17 and GENIA. Numbers in \textbf{bold} are the highest results for the corresponding dataset, while numbers \underline{underlined} represent the second-best results. Significant improvements against the best-performing baseline for each dataset are marked with $\ast$ (t-test, $p < 0.05$).}
	\begin{tabular}{l c c c c c c}
		\toprule
		\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{WNUT-17}} & \multicolumn{3}{c}{\textbf{GENIA}} \\
  \cmidrule(r){2-4}
  \cmidrule(r){5-7}
		 & \bf GPT & \bf Llama & \bf Qwen & \bf GPT & \bf Llama & \bf Qwen \\ 
        \midrule
		Vanilla~\citep{DBLP:conf/emnlp/XieLZZLW23,DBLP:journals/corr/abs-2311-08921} & 40.10 & 34.88 & 34.93 & 43.47 & 15.36 &  \phantom{0}9.97   \\ 
		SALLM~\citep{DBLP:conf/emnlp/XieLZZLW23} & 38.66 & \underline{40.95} & \underline{41.50} & 42.33 & \underline{36.23} & 19.13   \\ 
		SILLM~\citep{DBLP:journals/corr/abs-2311-08921} & \underline{41.65} & 22.43 & 36.23 & \underline{45.66} & 28.13 & \underline{33.80}   \\ 
        \midrule
		CMAS (ours) & \textbf{47.98}\rlap{$^{\ast}$} & \textbf{42.36}\rlap{$^{\ast}$} & \textbf{44.62}\rlap{$^{\ast}$} & \textbf{50.00}\rlap{$^{\ast}$} & \textbf{45.68}\rlap{$^{\ast}$} & \textbf{36.12}\rlap{$^{\ast}$} \\ \bottomrule
	\end{tabular}
	\label{tab:other LLMs}
\end{table}




% \subsection{Influence of the number of demonstrations}
\subsection{Influence of different LLM backbones}
\label{subsec:LLM backbones}
To explore the impact of different \ac{LLM} backbones, we evaluate \ac{CMAS} and baseline models using the latest \acp{LLM}, including GPT (\texttt{gpt-3.5-turbo-0125}), Llama (\texttt{Meta-Llama-3-8B-Instruct}\footnote{\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}}),\linebreak and Qwen (\texttt{Qwen2.5-7B-Instruct}\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}}). Table~\ref{tab:other LLMs} illustrates the zero-shot \ac{NER} performance on the WNUT-17 and GENIA datasets. We exclude the performance of ChatIE and Decomposed-QA, as their F1-scores with Qwen and Llama backbones are considerably lower than other baselines. As Table~\ref{tab:other LLMs} shows, \ac{CMAS} achieves the highest F1-scores when using GPT as the backbone model. Additionally, CMAS consistently outperforms the baselines across various LLM backbones, demonstrating its superiority and generalizability.

\subsection{Error analysis}
\label{subsec:error analysis}
To investigate \ac{CMAS}'s error correction capabilities, we conduct an analysis of the following errors on the WNUT-17 dataset:

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Type errors}: (i) \textbf{OOD types} are predicted entity types not in the predefined label set; (ii) \textbf{Wrong types} are predicted entity types incorrect but in the predefined label set.
    \item \textbf{Boundary errors}: 
    (i) \textbf{Contain gold} are incorrectly predicted mentions that contain gold mentions;
    (ii) \textbf{Contained by gold} are incorrectly predicted mentions that are contained by gold mentions;
    (iii) \textbf{Overlap with gold} are incorrectly predicted mentions that do not fit the above situations but still overlap with gold mentions.
    \item \textbf{Completely-Os} are incorrectly predicted mentions that do not coincide with any of the three boundary situations associated with gold mentions.
    \item \textbf{OOD mentions} are predicted mentions that do not appear in the input text.
    \item \textbf{Omitted mentions} are entity mentions that models fail to identify.
\end{itemize}

\noindent
Figure~\ref{fig:errors-chapter5} (in the Appendix) visualizes the percentages of error types. 
The majority error types are \emph{overlap with gold} and \emph{ommited mentions}, which account for 72.30\% of all errors. 
These errors may result from incomplete annotations or predictions influenced by the prior knowledge of \acp{LLM}.
Table~\ref{tab:error types-chapter5} (in the Appendix) summarizes the statistics of error types. With the implementation of the proposed type-related feature extractor and demonstration discriminator, CMAS significantly reduces the total number of errors by 30.60\% and 74.60\% compared to state-of-the-art baselines SALLM and SILLM, respectively, demonstrating its remarkable effectiveness in error correction.


