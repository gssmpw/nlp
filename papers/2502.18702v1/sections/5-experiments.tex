\section{Experiments}

\textbf{Research questions.}
We aim to answer the following research questions: 
\begin{enumerate*}[label=(RQ\arabic*),leftmargin=*,nosep]
\item Does \ac{CMAS} outperform state-of-the-art methods on the zero-shot \ac{NER} task? (See Section~\ref{subsec:zero-shot results-chapter5})
\item Can \ac{CMAS} be generalized to the few-shot setting? (See Section~\ref{subsec:few-shot results-chapter5})
\end{enumerate*}

\noindent \textbf{Datasets.}
In our experiments, we evaluate \ac{CMAS} on both general-domain and domain-specific datasets. Detailed statistics of the datasets used are shown in Table~\ref{tab:datasets-chapter5} (in the Appendix).
For our general-domain experiments, we consider four commonly used benchmarks, namely the CoNLL03~\citep{DBLP:conf/conll/SangM03}, WikiGold~\citep{DBLP:conf/acl-pwnlp/BalasuriyaRNMC09}, WNUT-17~\citep{DBLP:conf/aclnut/DerczynskiNEL17}, and OntoNotes\footnote{\url{https://catalog.ldc.upenn.edu/LDC2013T19}} datasets. 
For our domain-specific experiments, we evaluate \ac{CMAS} on the GENIA~\citep{ohta2002genia} and BioNLP11~\citep{DBLP:journals/bmcbi/PyysaloORSMWSTA12} datasets in the biomedical domain.

For zero-shot NER, to keep API usage costs under control, we randomly sample 300 examples from the test set three times and calculate the average performance. An exception is made for WikiGold, which has a test set of only 247 examples. Additionally, we randomly sample 500 examples from the training set as the unlabeled corpus. 
In the few-shot settings, we examine configurations including 0-shot, 3-shot, 5-shot, and 10-shot, where ``shot'' refers to the number of demonstrations with gold labels provided to LLMs (see Section~\ref{subsec:few-shot results-chapter5}). For each setting, we randomly sample three sets of demonstrations and compute the average results.

\noindent \textbf{Baselines}
For the zero-shot and few-shot NER tasks, we compare \ac{CMAS} with the following baselines:
\begin{enumerate*}[label=(\roman*), leftmargin=*,nosep]
    \item \textbf{Vanilla}~\citep{DBLP:conf/emnlp/XieLZZLW23,DBLP:journals/corr/abs-2311-08921} employs a straightforward and common prompting strategy that directly asks LLMs to extract entity labels from input texts.
    \item \textbf{ChatIE}~\citep{wei2023zero} transforms the zero-shot \ac{NER} task into a multi-turn question-answering problem using a two-stage framework.
    \item \textbf{Decomposed-QA}~\citep{DBLP:conf/emnlp/XieLZZLW23} breaks down the zero-shot NER task into a series of simpler subproblems by labels and follows a decomposed-question-answering paradigm, where the model extracts entities of only one label at a time.
    \item Based on Decomposed-QA, \textbf{SALLM}~\citep{DBLP:conf/emnlp/XieLZZLW23} further adopts syntactic augmentation to stimulate the model's intermediate reasoning in two ways, including syntactic prompting and tool augmentation.
    \item \textbf{SILLM}~\citep{DBLP:journals/corr/abs-2311-08921} applies a self-improving framework, which uses unlabeled corpora to stimulate the self-learning abilities of LLMs in zero-shot NER.
\end{enumerate*}

In our experiments, we evaluate all the aforementioned baselines for the zero-shot \ac{NER} task. For few-shot \ac{NER}, we assess SILLM~\cite{DBLP:journals/corr/abs-2311-08921} and Vanilla~\cite{DBLP:conf/emnlp/XieLZZLW23,DBLP:journals/corr/abs-2311-08921} since ChatIE~\cite{wei2023zero}, Decomposed-QA~\cite{DBLP:conf/emnlp/XieLZZLW23}, and SALLM~\cite{DBLP:conf/emnlp/XieLZZLW23} do not incorporate task demonstrations in their prompt templates. 
Additionally, we report the highest results obtained from the model variants of SALLM~\cite{DBLP:conf/emnlp/XieLZZLW23} and SILLM~\cite{DBLP:journals/corr/abs-2311-08921} in our experiments.
For fair comparisons and cost savings, we employ GPT-3.5 (specifically, the \texttt{gpt-3.5-turbo-0125} model\footnote{\url{https://platform.openai.com/docs/models/gpt-3-5-turbo}}) as the LLM backbone for all baselines and agents in \ac{CMAS}. We use the \texttt{text-embedding-ada-002} model,\footnote{\url{https://platform.openai.com/docs/models/embeddings}} a text-embedding model from OpenAI, to obtain sentence representations. We access OpenAI models using the official API.

\subsection{Evaluation metrics}
Following previous work~\citep{DBLP:conf/emnlp/XieLZZLW23,DBLP:journals/corr/abs-2311-08921}, we conduct our evaluation of zero-shot and few-shot \ac{NER} tasks using only complete matching and employing the micro F1-score to assess the NER task. We consider a prediction correct only when both the boundary and the type of the predicted entity exactly match those of the true entity.


\subsection{Implementation details}
Following \citet{DBLP:journals/corr/abs-2311-08921} and \citet{DBLP:conf/ijcai/0001LLOQ21}, we set the number of nearest neighbors \(K=50\) during self-annotation and the number of task demonstrations \(k=16\). For self-consistency scores, we set the temperature to 0.7 and sample 5 answers. We set the frequency ratio hyperparameter \(\rho\) to 3 for all experiments and only consider 1-gram texts for simplicity.


% For fair comparisons and cost savings, we use GPT-3.5 (gpt-3.5-turbo) as the LLM backbone for baselines and \ac{CMAS}. We use the text-embedding-ada-002 model to get sentence representations, which is a text-embedding model from OpenAI. We assess OpenAI models with official API\footnote{The results of GPT-3.5 are obtained during October and November 2023 with official API.}.