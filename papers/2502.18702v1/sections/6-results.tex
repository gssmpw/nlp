\section{Experimental Results}
\label{sec:results-chapter5}
To answer RQ1 and RQ2, we assess the performance of \ac{CMAS} on both zero-shot and few-shot NER tasks.

\subsection{Results on zero-shot NER}
\label{subsec:zero-shot results-chapter5}
We turn to RQ1. Table~\ref{tab:zero-shot NER-chapter5} shows the experimental results on both general-domain and domain-specific datasets. To ensure fair comparisons, we reproduce all the baseline models using the \texttt{gpt-3.5-\\turbo-0125} model, as they are implemented with different versions of GPT-3.5 in the original papers. 
We have the following observations:
\begin{enumerate*}[label=(\roman*),nosep,leftmargin=*]
    \item Zero-shot \ac{NER} is challenging, and baseline models struggle to achieve an F1-score above 60\% on most of the datasets. For instance, ChatIE only obtains F1-scores of 37.46\% and 29.00\% on the WNUT-17 and OntoNotes datasets, respectively.
    \item \ac{CMAS} achieves the highest F1-scores across all datasets, indicating its superior performance. For instance, \ac{CMAS} attains F1-scores of 76.23\% and 60.51\% on the WikiGold and BioNLP11 datasets, respectively.
    \item \ac{CMAS} significantly outperforms the previous state-of-the-art baselines across all datasets. For example, \ac{CMAS} achieves improvements of 13.21\% and 4.49\% over the best-performing baselines on the WNUT-17 and GENIA datasets, respectively.
\end{enumerate*}

In summary, \ac{CMAS} demonstrates its effectiveness in recognizing named entities in a strict zero-shot setting. The identification of contextual correlations and the evaluation of helpfulness scores for task demonstrations are beneficial for zero-shot \ac{NER}.


\begin{table*}[t]
  \centering
  \setlength\tabcolsep{3.5pt}
  \caption{Zero-shot NER results (F1) on both general-domain and domain-specific datasets. Numbers in \textbf{bold} are the highest results for the corresponding dataset, while numbers \underline{underlined} represent the second-best results. Significant improvements against the best-performing baseline for each dataset are marked with $\ast$ (t-test, $p < 0.05$). }
  \label{tab:zero-shot NER-chapter5}
  \begin{tabular}{l cccccc}
    \toprule
  \multirow{2}{*}{\bf Model} & \multicolumn{6}{c}{\bf Datasets}\\
    \cmidrule(r){2-7}
     & \bf CoNLL03 & \bf WikiGold & \bf WNUT-17 & \bf OntoNotes & \bf GENIA & \bf BioNLP11 \\
  \midrule
    Vanilla~\citep{DBLP:conf/emnlp/XieLZZLW23,DBLP:journals/corr/abs-2311-08921}  & 72.54 & \underline{74.27}  & 40.10 & 45.09  & 43.47  & 53.92 \\
    ChatIE~\citep{wei2023zero}  & 50.13 & 56.78  & 37.46
 & 29.00  & \underline{47.85}  & 45.56  \\
    Decomposed-QA~\citep{DBLP:conf/emnlp/XieLZZLW23}  & 52.61 & 64.05  & \underline{42.38}
 & 35.96  & 34.03  & \underline{57.26}  \\
 SALLM~\citep{DBLP:conf/emnlp/XieLZZLW23} & 68.97 & 72.14 & 38.66 & 44.53 & 42.33  & 55.06  \\
  SILLM~\citep{DBLP:journals/corr/abs-2311-08921}& \underline{72.96} & 72.72 & 41.65 & \underline{45.34} & 45.66 & 44.99  \\
    \midrule
    CMAS (ours) &$\textbf{76.43}$\rlap{$^{\ast}$} & $\textbf{76.23}$\rlap{$^{\ast}$} & $\textbf{47.98}$\rlap{$^{\ast}$} & $\textbf{46.23}$\rlap{$^{\ast}$} & $\textbf{50.00}$\rlap{$^{\ast}$} & $\textbf{60.51}$\rlap{$^{\ast}$} \\ 
  \bottomrule
\end{tabular}
\end{table*}

\subsection{Results on few-shot NER}
\label{subsec:few-shot results-chapter5}
To investigate the effectiveness of \ac{CMAS} on the few-shot setting, we turn to RQ2. In our experiments, we evaluate \ac{CMAS} and the baselines in 0-shot, 3-shot, 5-shot, and 10-shot settings, where micro F1-scores are reported. ChatIE, Decomposed-QA, and SALLM are excluded because incorporating gold demonstrations into their prompt templates is non-trivial and beyond the scope of this paper.

Based on Figure~\ref{fig:few-shot NER-chapter5}, we arrive at the following conclusions:
\begin{enumerate*}[label=(\roman*),nosep,leftmargin=*]
    \item Increasing the number of demonstrations with gold labels does not necessarily enhance the prediction performance of \acp{LLM}. For example, as the number of demonstrations with gold labels increases from 0 to 10, the F1-score of the Vanilla model significantly drops from 40.10\% to 27.10\% on the WNUT-17 dataset. This decline may be due to the random selection of demonstrations, which can be highly irrelevant to the target sentence and severely misguide the inference process of LLMs.
    \item \ac{CMAS} achieves the highest F1-scores and consistently outperforms the state-of-the-art baselines across all few-shot settings, demonstrating its effectiveness and robustness. For example, \ac{CMAS} exhibits an average improvement of 19.51\% and 13.10\% over SILLM on the WNUT-17 and GENIA datasets, respectively.
\end{enumerate*}

In summary, our proposed \ac{CMAS} not only effectively extracts entities in the strict zero-shot setting but also achieves the highest F1-scores across all few-shot settings while maintaining robustness to irrelevant demonstrations.

\begin{figure*}
  \centering
    \subfigure[WikiGold.]{
    \label{fig:few-shot-WikiGold}
    \includegraphics[width=0.28\linewidth]{figures/few-shot-WikiGold.pdf}}
    \quad
    \subfigure[WNUT-17.]{
    \label{fig:few-shot-WNUT}
    \includegraphics[width=0.28\linewidth]{figures/few-shot-WNUT.pdf}}
    \quad
  \subfigure[GENIA.]{
    \label{fig:few-shot-GENIA}
    \includegraphics[width=0.28\linewidth]{figures/few-shot-GENIA.pdf}}
    \caption{Few-shot NER results (F1) on WikiGold, WNUT-17, and GENIA.}
  \label{fig:few-shot NER-chapter5}
\end{figure*}


