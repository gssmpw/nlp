\section{Alternative Views}
\label{sec:alternative_views}
\paragraph{Success of End to End Training.}

The remarkable success of deep learning is largely attributed to the end-to-end learning capability of deep neural networks~\citep{lecun2015deep,goodfellow2016deep}, which has proven effective across diverse domains, including image processing~\citep{krizhevsky2012imagenet,he2016deep}, natural language processing~\citep{vaswani2017attention,devlin2018bert}, tabular data analysis~\citep{arik2021tabnet}, and time series data~\citep{van2016wavenet,ismail2019deep,ding2020hierarchical}.
Representation learning~\citep{bengio2013representation} and pre-training methods~\citep{radford2018improving} are typically followed by post-training or fine-tuning procedures to adapt to downstream tasks or datasets~\citep{howard2018universal,raffel2020exploring,radford2021learning}. In the era of large language models, general-purpose pre-trained models have been extensively fine-tuned for a wide range of downstream applications~\citep{brown2020language}, including evaluation tasks such as reward modeling~\citep{perez2022red,ouyang2022training,chang2024survey,lin2023llm}.

\paragraph{Computational Costs are Decreasing Over Time}
As computational costs continue to decrease, future research on reward models may efficiently leverage LLMs or even more powerful foundation models. This could eliminate the need for embedding-based reward modeling approaches, further supporting the case for end-to-end learning.

From this perspective, one could reasonably argue that reward model learning should ultimately adopt an end-to-end approach. The positions proposed in this paper may only remain valid within a limited timeframe. Future advancements in methodology and hardware technology may render them obsolete.
