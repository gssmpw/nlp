\section{Introduction}
Large Language Models (LLMs) have achieved great success on structured tasks like mathematical reasoning and code generation~\citep{guo2025deepseek, jaech2024openai, trinh2024solving} using the technique of Reinforcement Learning (RL). In such a process, rule-based reward functions can be explicitly defined to guide optimization.

On the other hand, in broader applications such as chat bots and general content generators, RL is also an essential technique in aligning LLMs for their safe and successful deployment~\citep{christiano2017deep, ouyang2022training,stiennon2020learning}, and the process is known as Reinforcement Learning from Human Feedback (RLHF). In such a process, reward models serve as a crucial mechanism for quantilizing content values and scaling RLHF~\citep{lambert2024rewardbench, wang2024secrets} --- those models act as proxy evaluators (of human values) during fine-tuning and deployment~\citep{dubey2024llama, dong2024rlhf, wang2024arithmetic}, assessing how well LLM outputs align with human intent. 

\begin{table}[t!]
\fontsize{7.8}{12}\selectfont
\centering
\caption{Training (with $10000$ samples) and evaluation (with $4000$ samples) time comparison for different reward model choices on CPUs and GPUs. (Details of the accelerated workflow are discussed in Section~\ref{sec:motivations}).}
\label{tab:execution_time}
\begin{tabular}{l|l|c|c}
\toprule
\textbf{Reward Models} & \textbf{Example} & \textbf{CPU} & \textbf{Tesla V100} \\
\midrule
Embedding-based & LightGBM & 8s    & -    \\
(Our Position)& 3-layer MLP & 28s   & 33s  \\
\hline
\multirow{2}{*}{LLM-based}       & GPT2 & $>$2h   & 879s \\
                                    & Gemma2B (LoRA R=8) & -     & 4755s \\
\bottomrule
\end{tabular}\vspace{-0.3cm}
\end{table}

% \begin{table}[t!]
% \centering
% \caption{\small Training (with $10000$ samples) and evaluation (with $4000$ samples) time comparison for different reward model choices on CPUs and GPUs. (workflow details in Section~\ref{sec:scalable_evaluation})}
% \label{tab:execution_time}
% \begin{tabular}{l|c|c}
% \toprule
% reward models & \textbf{CPU} & \textbf{Tesla V100} \\
% \midrule
% lightgbm                 & 8s    & -    \\
% 3-layer MLP                 & 28s   & 33s  \\
% \hline
% GPT2                & $>$2h   & 879s \\
% Gemma2B LoRA R=8    & -     & 4755s \\
% \bottomrule
% \end{tabular}
% \end{table}

Despite significant progress, reward model training remains challenging due to the scarcity and inaccuracy of annotations, inherent complexity, and variability of human preferences~\citep{lambert2024rewardbench,wang2024secrets,gao2023scaling}. Prior research has attempted to mitigate these challenges through improved architectures~\citep{wang2024arithmetic, wang2024arithmetic}, customized loss functions~\citep{winata2024metametrics,liu2024skywork}, uncertainty quantification~\citep{lou2024uncertainty,coste2023reward,zhang2024overcoming}, novel comparisons~\citep{sun2024rethinking,yin2024relative}, dataset debiasing techniques~\citep{park2024offsetbias}, and active or online annotation algorithms~\citep{xiong2023gibbs,muldrew2024active,dong2024rlhf}.

Reward modeling is a rapidly evolving research field, but its progress is significantly hindered by the high computational cost of \textbf{training and evaluating reward models}, which in turn poses challenges for reproducibility across different implementations and fair comparisons among methods. 


\textbf{In this paper, we argue that building reward models using embedded input can greatly accelerate research in this field.} Specifically, it enhances reproducibility by reducing hardware and computational resource requirements, cutting the cost of training and evaluation, improving training stability, and minimizing the cost of reproduction, hence accelerating the pace of reward model research. Additionally, it opens new avenues for further exploration such as research using the statistical lenses.
% \begin{table*}[ht!]
% \fontsize{7.8}{10}\selectfont
% \centering\vspace{-0.1cm}
% \caption{Comparison of LLM-based reward models and embedding-based reward models.}
% \begin{tabular}{l|c|c|c|c|c}
% \toprule
% \textbf{Reward Models} & \textbf{Param Count} & \textbf{Training Cost} & \textbf{Inference Cost} & \textbf{Evaluation Cost} & \textbf{Reproducibility} \\ \hline
% LLM-based & 3M - 3B & GPU, for hours & High, LLM forward passes & High, $10^2$ GPU hours & Low, many hyper-params \\ \hline
% Embedding-based & 0.6M & CPU, a few minutes & Low, MLP forward passes & Low, a few minutes on CPU & High, a few hyper-params \\ \bottomrule
% \end{tabular}\vspace{-0.2cm}
% \label{tab:reward_model_comparison}
% \end{table*}

\begin{table*}[ht!]
\fontsize{8}{11.8}\selectfont
\centering
\caption{Comparative of LLM-based and Embedding-based Reward Models.}
\begin{tabular}{p{1.75cm}|c|c|p{8cm}}
\toprule
\textbf{Metric} & \textbf{LLM-based RM} & \textbf{Embedding-based RM} & \textbf{Are Embedding-based Reward Models Better (than LLM-based)?} \\
\midrule
Parameter Count & 3M - 3B & 0.6M & Yes \\ \hline
Performance (Section~\ref{sec:RM_from_embeddings}) & Larger LLMs are stronger & Intermediate & Not Always. Embedding-based methods are remarkably better than small LM-based (GPT2, 700M) reward models, and can be better than larger LM-based (Gemma 2B) reward models when annotation quality and quantity are restricted. In more general setups, further research is needed to enhance those methods. \\ \hline
Training Cost (Section~\ref{sec:cheap_train}) & High (GPU, for hours) & Low (CPU, a few minutes) & Yes, they reduce hardware requirements and lower entry barriers of research \\ \hline
Evaluation Cost (Section~\ref{sec:scalable_evaluation}) & High ($10^2\times$ GPU hours) & Low (CPU, a few minutes) & Yes, fast evaluations enable reliable results and comprehensive comparisons.\\ \hline
Inference Cost (Section~\ref{sec:cheap_inference}) & High (GPUs needed) & Low (GPU-free) & Yes, they enable efficient inference-time optimization and support lightweight deployment across varying infrastructures.\\ \hline
% Stability & Variable (sensitive to hyperparameters) & High & Results in less variance and more reliable performance \\
Reproducibility (Section~\ref{sec:reproduce_demo})  & Low  & High & Yes, embedding-based reward models have much less vulnerable hyper-parameters and are easy to reproduce. \\
\bottomrule
\end{tabular}
\label{tab:reward_model_comparison}
\end{table*}


This position paper is structured as follows: In Sec.~\ref{sec:RM_from_embeddings}, we present and compare embedding-based reward models with conventional LLM-based reward models, where general-purpose  LLMs with value heads are optimized to serve as value predictors. In Sec.~\ref{sec:motivations}, we elaborate on the motivations of training reward models using embeddings as inputs, and demonstrating its advantages in practice --- high reproducibility with low cost associated with training (Sec.~\ref{sec:cheap_train}), evaluation (Sec.~\ref{sec:scalable_evaluation}), and inference (Sec.~\ref{sec:cheap_inference}). 
In Sec.~\ref{sec:reproduce_demo}, we demonstrate our positions through an efficient reproduction of existing reward modeling research.
Sec.~\ref{sec:future_works} explores open questions and future research opportunities in this domain. Lastly, Sec.~\ref{sec:alternative_views} provides alternative perspectives to enhance the comprehensiveness of this position paper.



