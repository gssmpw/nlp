\section{Motivations of Using Embeddings as Reward Model Inputs}
\label{sec:motivations}
\subsection{Reproducibility: Foundation of Research}
\label{sec:cheap_train}
Reproducibility is the foundation of scientific research. In the study of reward modeling, the ability to replicate results across different studies is essential for evaluating theoretical and practical contributions. Nonetheless, the reproduction of LLM-based reward model research often faces considerable obstacles, such as vulnerability to many sensitive hyperparameters, the necessity for large memory GPUs, large training instability, and extensive computational demands associated with slow training processes. These challenges can make the replication of existing works extremely challenging --- if not unfeasible --- for many of the research communities.


\begin{table*}[h!]
\begin{lstlisting}
# Load Training Data
train_embeddings, train_rewards = load_embd_data(task='Harmless', split='train')
print(train_embeddings.shape) 
### (40000, 10, 2048)
print(train_rewards.shape) 
### (40000, 10, 1)

# Load Testing Data
test_embeddings, test_rewards = load_embd_data(task='Harmless', split='test')
print(test_embeddings.shape) 
### (2000, 500, 2048)
print(test_rewards.shape) 
### (2000, 500, 1)

# Generation of Pairwise Comparisons
train_comparisons, train_labels = pair_annotate(train_embeddings, train_rewards)

# Train Embedding-based Reward Model (e.g., use a Bradley-Terry MLP)
reward_model = BT_MLP()
reward_model.fit(train_comparisons, train_labels)

# Make Predictions with the Reward Model on Testset
rm_predictions = reward_model.predict(test_embeddings)
print(rm_predictions.shape) 
### (2000, 500, 1)

# Calculate Evaluation Metrics on Testset
bon_500 = calc_bon(rm_predictions, test_rewards, N=500)
spearmanr = calc_spearmanr(rm_predictions, test_rewards)
\end{lstlisting}
\label{algo}\vspace{-0.4cm}
\end{table*}

As a consequence, in new research, if a method lacks systematic comparisons with established methods due to the above challenges, or its efficacy can not be verified through repeated and statistically significant trials, the results may be unfounded. 

The utilization of embedding-based reward models offers several advantages: \begin{enumerate}[nosep,leftmargin=*] 
\item \textbf{Reward Model Research without GPUs:} Conducting research and reproducing reward model research using embedding-based methods do not necessitate advanced, large-memory GPUs, thereby democratizing access to state-of-the-art research methods and facilitating the validation of novel algorithms by a wider academic community. 
\item \textbf{Lower Computational Requirements for Statistical Significance:} In embedding-based reward model research, the computational overhead is lower not only because of the much cheaper model training process but also for the more consistent results across multiple runs. And there are much less vulnerable hyperparameters that may drastically affect the results. This efficiency enables researchers to rapidly prototype, validate ideas, and innovate based on reliable conclusive empirical observations, maximally isolating the source of gains from complex LLM-based reward modeling systems, thereby accelerating the cycle of scientific discovery and validation in the field. 
\item \textbf{Data Standardization and Scalability:} In embedding-based reward model research, it is possible to create and share a standardized, publicly accessible dataset that includes multiple language models' generations (generality among models), contains a large number of samples (sufficient data for training), flexibly simulate annotation strategies (to stress test methods), and cost-efficient evaluation process.
\end{enumerate}


All of those aspects encourage reproducible research in embedding-based reward modeling, and thereby accelerate the pace of discoveries in the area.

\subsection{Scalable Evaluation with Embedding-based Reward Models}
\label{sec:scalable_evaluation}
In addition to the high computational costs of training, LLM-based reward modeling faces significant challenges in evaluation time and expense. Specifically, reward models are tasked with evaluating test-time generations to differentiate superior responses from inferior ones. Previous research has primarily utilized two metrics for this purpose: LLM-as-a-Judge and evaluation using open-sourced golden reward models~\citep{dong2023raft,dong2024rlhf}.

\textbf{High Cost in LLM-as-a-Judge Evaluation.}
The LLM-as-a-Judge evaluation, which often involves calling commercial APIs, can be prohibitively expensive for even medium-sized datasets. For example, in a study involving $3$ different language models and $2$ datasets, evaluating a proposed method using $2000$ test samples --- each comparison truncated to $1024$ tokens --- through the GPT-3.5 API incurs a cost of $20$ US dollars \textbf{per experiment}, and this cost will be amplified by the number of individual run of the experiment. Compounding such an issue, those results are not reusable.

Moreover, recent findings have exposed potential cheating behavior in LLM-as-a-Judge evaluations~\cite{zheng2024cheating}, further compromising the reliability of this costly method and challenging its feasibility as a community standard.


% In addition to the high computational cost associated with the reward model training, another difficulty faced by LLM-based reward modeling is the evaluation time expense. To be specific, reward models are designed to evaluate test-time generations, and distinguish the better responses from the bad ones. To accurately evaluate whether this objective can be met, previous research has mainly focused on two evaluation metrics: Golden reward model evaluation and LLM-as-a-Judge evaluation~\citep{dong2023raft}. 

% For the LLM-as-a-Judge evaluation, calling commercial APIs to evaluate a medium-sized dataset will be expensive. Moreover, those evaluation results can not be reused. For instance, in research where we have 3 different language models to study, and have 2 datasets to evaluate a proposed method, using $2000$ test samples for the evaluation, and each response is truncated to be $512$ tokens, calling the GPT3.5 API will spend $10$ US dollars --- this is only on a single experimental setups, and with a single run.

% Moreover, recent discoveries disclose the potential cheating behavior in LLM-as-a-judge evaluations, making this costly evaluation even more unreliable and infeasible for the community to use as a standard.

\textbf{Cost in Golden Reward Model Evaluation.} While the Golden Reward Model Evaluation avoids the use of commercial APIs, making it more accessible and economical for researchers, it still imposes substantial computational demands. For example, evaluating the aforementioned test case necessitates the LLM-based RM to process $12000$ pairs of sequences. In the more computationally intensive best-of-N evaluations, a typical study with $N=500$~\citep[KL divergence approximately 5 Nats,][]{gao2023scaling} requires \textbf{6 million forward passes}. Completing these passes using 2B-parameter LLMs on Tesla V100 GPUs can consume over 100 GPU hours. It is worth noting that this cost is associated with a single experiment setup and a single experimental trial.

\textbf{Cheap and Fast Evaluation with Embedding-based Reward Models.}
% On the other hand, when using embedding-based reward models, due to the embeddings are fixed, we are able to prepare a fixed test embedding dataset that contains sufficient samples for evaluation, and such an test dataset is re-usable for different methods. In the previous example, we only need to generate the embeddings and golden rewards of the $500$ test responses on each prompt once, then those embeddings and rewards are re-useable and agnostic to any choice of embeddings-based reward models. 
% We have done such a pre-processing step, and our dataset asset contains $500$ responses on each of the test prompts. To better understand the dataset we prepared for embedding-based reward modeling, we provide the following pseudo code as illustrations:
In contrast, embedding-based reward models leverage fixed embeddings, allowing for the preparation of a \textbf{standardized test dataset that is reusable across various methods}. For instance, in the scenario described above, we only need to generate the embeddings and golden rewards for the $500$ test responses on each prompt \textbf{once}. These embeddings and rewards are then reusable for any embedding-based reward model evaluation.

We have implemented such a preprocessing step, resulting in a dataset asset that includes $500$ responses for each test prompt. To provide a clearer understanding of the dataset prepared for embedding-based reward modeling, we provide the \grayboxtext{pseudo-code in the box} in page \pageref{algo} for illustration.


In such a use case, the computationally intensive step of embedding generation is completed during data preparation. Subsequently, the evaluation involves merely processing test tensors of shape $(2000, 500, 2048)$ through the reward model --- a task that typically concludes within a minute. This efficiency highlights the practicality of our embedding-based reward modeling framework, which significantly simplifies and accelerates the evaluation of reward models and improves its reliability.


% In our demonstrated use cases, the computational heavy step of embedding generation has been done in data preparation. And the evaluation forward pass only requires calling the reward model to process test tensors of the shape $(2000, 500, 2048)$, which typically can be finished within a minute. With the highlighted embedding-based reward modeling research framework, evaluating reward models becomes extremely easy and reliable.


% The ranking order consistency is more important than matching the accurate value 
% (hence predicting the accurate win rate among any two responses).




% \paragraph{Spearman's Ranking Correlation}


% \paragraph{ Best-of-N Evaluation}


\subsection{Scalable Inference-Time Optimization}
\label{sec:cheap_inference}
\begin{figure}[h!]
\vspace{-0.15cm}
    \includegraphics[width=1.0\linewidth]{figs/embedding_rm.png}
    \vspace{-0.6cm}
    \caption{\small \textit{The inputs of embedding-based reward models are by-products of language model generation.} Unlike conventional LLM-based reward models that require another LLM forward pass for inference time evaluations, embedding-based models alleviate the memory challenge and facilitate inference time optimization for LLM-free service providers. These providers, who rely on third-party LLM services via APIs rather than hosting large models locally, can efficiently perform inference time optimization using only embeddings.}  \vspace{-0.1cm}
    \label{fig:fast_inference}
% \vspace{-2cm}
\end{figure}

In this section, we elucidate an additional advantage of embedding-based reward models --- enhancing the inference-time optimization efficiency. With embedding-based reward models, language generation and evaluation require only a single LLM forward pass. Although this may appear to reduce computation time by less than half, it significantly lowers the computational burden in evaluation by shifting from hosting an LLM (reward model) to a much simpler and smaller model. This is particularly beneficial for API-based service providers who previously could not perform inference-time optimization due to the high computational demands of running LLM-based reward models locally. With embedding-based reward models, they are now able to efficiently evaluate the quality of generated content and potentially enhance user experience through inference-time optimization (e.g., prompting optimization and re-generation). The workflow of using embedding-based reward models in inference is visualized in Figure~\ref{fig:fast_inference}.










% \subsection{Isolating the Source of Gains: Discriminative and Generative Abilities}
% \paragraph{Generative Reward Models and Representation Learning}



