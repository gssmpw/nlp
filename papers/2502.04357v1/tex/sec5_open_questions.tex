\section{Call for Contributions}
\label{sec:future_works}
\subsection{Contributing to Public Embedding Assets}
In this position paper, we have demonstrated the advantages of embedding-based reward models. We successfully reproduced the findings of a reward model ensemble study with $12,000$ experiment runs in just one day using only CPU resources, highlighting the efficiency of our approach. However, it's important to note that this workflow is feasible only when embeddings from LLM generations are available for both training and testing datasets.

In conventional LLM-based reward model research, LLM generations have not been regarded as critical public assets in reward model research, primarily because evaluating these generated contents requires nearly as much computational effort and hardware resources as producing them.

In contrast, our embedding-based reward model framework enables researchers with access only to CPU resources to participate in this field. This inclusivity relies on the availability of embedding assets, contributed by researchers with access to more powerful GPU resources.

For our studies, we utilized the \texttt{Anthropic-HH} dataset and $3$ different LLMs, enabling us to release all corresponding embeddings and their evaluations as public assets for future research. However, given the rapid advancements in general-purpose LLMs, this alone is not enough. \textbf{We encourage more contributions from the community to enrich these assets.}

Moreover, an added benefit of this approach is its environmental impact. By making these assets reusable, other researchers do not need to expend computational and electrical resources to regenerate training and testing samples. This not only accelerates research but also significantly reduces the environmental burden associated with the extensive use of computational resources in large-scale model training and evaluation.

% In this position paper, we have demonstrated the superiority of embedding-based reward models and the framework. While we have shown the efficiency of such a setup by reproducing the reward model ensemble paper with $12000$ experiment runs within $1$ day on a CPU-only machine, we would like to note that this workflow is only feasible when the embeddings of LLM generations --- on both training and testing datsaet --- are available. 

% In previous reward model research, LLM generations are not considered to be an important public asset as evaluating those generated contents takes as much as effort and requires almost the same amount of hardware resources as generating them. 

% Differently, in the embedding-based reward model research, the workflow make it possible for CPU-only users to participate the reward model research, yet this requires the contribution of the embedding assets from the GPU-rich researchers.

% In our research, we used the \texttt{Anthropic-HH} dataset, and $3$ LLMs, therefore, we are able to release all embeddings and their golden reward model evaluations as a public asset for future research. However, this is not sufficient in the fast-evolving research area especially given the rapicly improvements in general-purpose LLMs. We will need more researchers from the community to contribute to such type of assets. 



\subsection{Representation Learning: Searching for General Purpose Reward Embeddings}
Current language model embeddings are primarily designed and optimized for text generation. \textbf{While they can be repurposed as inputs for reward models, as demonstrated in this paper, there remains significant room for improvement.} Our experiments indicate that fine-tuning LLM-based reward models, though computationally expensive, can yield superior performance when provided with rich and clear annotation signals.

Given the advantages of embedding-based reward models outlined in this paper, developing better general-purpose reward embeddings represents a promising orthogonal direction for advancing reward model research. 

To link with another important research avenue of the generative reward modeling, where the token generation capabilities of LLMs are directly leveraged for value prediction~\citep{mahan2024generative,zhang2024generative} or used as a regularization mechanism in LLM-based reward model learning\citep{yang2024regularizing}. Their key insight is that \textit{generation ability can enhance performance in discriminative tasks}. In contrast, the question of how to leverage reward modeling information to learn general-purpose discriminative embeddings remains relatively underexplored. Notable exceptions include efforts to merge multiple preference datasets~\citep{dong2024rlhf}. However, \citet{sun2024off} found that combining offline generations with online annotations can be harmful to reward model training. Another related challenge in reward modeling is known to be the alignment tax~\citep{lin2024mitigating}, and how to balance multiple objectives~\citep{yang2024rewards,zhou2023beyond}, and ideal general reward embedding should be able to capture multiple aspects of the responses.

% Since the current language model embeddings are generated and optimized for the purpose of generation, while it is capable of being used as reward model inputs as discussed and demonstrated in this paper, there is undeniably a large space to improve over it. And we have observed from the experiments that fine-tuning the LLM-based reward models, though costly, can have higher performance potential when rich and clear annotation signals are available.

% Given the benefits of embeddding-based reward models discussed in this position paper, searching for better general-purpose reward embeddings can further contribute to this line of research from an orthogonal perspective to push the frontier of reward model research.

% Another important line of related research on reward modeling is the generative reward models~\citep{mahan2024generative,zhang2024generative} where the token generation ability of language models is directly utilized in reward prediction or acts as a regularizer in LLM-based reward model learning~\citep{yang2024regularizing}. In this line of research, the key insight is to leverage the generation ability of LLMs to improve their performance in discriminative tasks. As comparison, how to leverage rich reward modeling information in search of a general discriminative embeddings is a relatively underexplored direction, except attempts of merging multiple preference datasets~\citep{dong2024rlhf}, and it has been shown in \citet{sun2024off} that merging offline generations with online annotations may be harmful to reward model training.








\subsection{Flash Back of Classic Statistics}
Back in the early days of statistical natural language processing, circa the 1990s to early 2000s \citep[for even earlier history, we refer to][]{jones1994natural}, researchers had quite limited options for features even for simple classification tasks. Simple models (e.g., classification trees) were often accompanied by handcrafted, ad hoc features like bags of words, n-grams, and tf-idf \citep{chowdhury2010introduction}, which are seen as insufficient today. With neural networks, representation learning and model development occurred simultaneously; one can even argue that the success of deep models lies in the success of representation learning \citep{bengio2013representation}. Lightweight statistical learning methods possess good properties that are still relevant today. For instance, it is much less resource-intensive and more stable to fit boosted trees than DNNs. The theoretical properties of generalized linear models, some nonparametric regression, as well as tree models, are well understood for classification, preference learning, and for new tasks like experimental design and active learning. 

In future works, can we get the best of both worlds by combining powerful embeddings from an LLM, together with a solid understanding of classic methods to better advance reward modeling with a gray box approach? Can we develop theories building upon the knowledge of classic methods? --- for instance, under the linear assumption with embeddings, what theoretical properties can we establish, and how can we conduct active learning? There are vast research opportunities lying at the interface between statistics and embedding-based reward modeling.
%%%GPT4o version above --

% Back in the early time of statistical natural language processing circa 1990s to early 2000s \citep[for even earlier history we refer to][]{jones1994natural}, for even simple classification task researchers have quite limited options on features. Simple models (e.g., classification trees) were often accompanied with hand crafted, ad hoc features like bag of word, ngrams and tf-idf \citep{chowdhury2010introduction} that are seen as not sufficient today. With neural networks, representation learning and model development happened at the same time, one can even argue that the success of deep model lays in the success of representation learning \citep{bengio2013representation}. Light weighted statistical learning methods bear good properties that are still relevant today. For instance, it is much less resource heavy and stable to fit a boosted trees than DNNs. Theoretical properties of generalized linear models, some nonparametric regression as well as trees models are well understood for classification, preference learning and for new tasks like experimental design and active learning. Can we get the good of both world by combining powerful representations learned with an LLM, together with good understandings of classic methods to better advance reward modeling with a gray box approach? Can we develop theories building upon knowledge on classic methods? For instance, under linear assumption to embeddings, what theoretical property can we establish and how can we do active learning? There are vast research opputunities lies in the interface between statistics and reward modeling.  
