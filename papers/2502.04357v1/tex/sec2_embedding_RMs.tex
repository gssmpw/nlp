\section{Reward Models with Embedding Inputs}
\label{sec:RM_from_embeddings}

\begin{figure}[h!]
    \includegraphics[width=1.0\linewidth]{figs/teaser.png}
    \vspace{-0.39cm}
    \caption{In reward model research, using embeddings as input (i.e., focusing on the pink box) brings the following benefits:
    1. there are much less parameters in those reward models;
    2. it has a much lower training cost than using LLM-based reward models;
    3. it has a much lower evaluation cost as compared to the LLM-based reward models; 
    4. it minimizes the inference-time cost by generating embeddings as by-products in language generation;
    5. research using embedding-based reward models are highly reproducible due to the low computational demand, high training stability, and minimal hardware requirement.}
    \label{fig:teaser}
\vspace{-0.3cm}
\end{figure}

\subsection{Alternatives to LLM-based Reward Models}
% In this section, we introduce the practice of using embeddings as input for reward modeling and compare it with the original natural language input practices. We use Figure~\ref{fig:teaser} to highlight the key difference. In the figure, green boxes denote trainable parameter groups, while gray boxes denote intermediate variables. The left panel of Figure~\ref{fig:teaser} illustrates how natural language inputs are processed by LLMs in generation tasks, while the right panel illustrates how natural language inputs are processed by LLM-based reward models in quality evaluation.

% When LLMs with replaced value heads are used for reward modeling, only a small number of parameters are removed, and the LLM-based reward model remains a large model having a large number of parameter freedom to learn. Training such a LLM-based reward model is computationally expensive and time-consuming even with modern graphic accelerators --- even with low-rank adapters like LoRA~\citep{hu2021lora}. For instance, training a 2B model on a Tesla-V100 GPU with LoRA on a typical alignment dataset containing 10k samples will roughly take 2 hours. Moreover, many hyper-parameters in such a training procedure may affect the final results. 

% On the other hand, since the motivation of building a reward model that is able to evaluate the natural language content, and it is well-known that the embedding space contains rich information of the natural language input both before and after the LLM era~\citep{mikolov2013efficient,pennington2014glove,devlin2018bert}, especially on downstream classification tasks~\citep{kiros2015skip,cer2018universal,brown2020language}. 
% A straightforward alternative of training the entire LLM for reward modeling is only to use the embeddings of language as inputs.
% In recent works, it has been shown that working on the embedding space can effectively build reward models for prompt evaluation in mathematical reasoning tasks~\citep{sun2023query}, or reward models for safety and helpful content evaluation~\citep{sun2024rethinking}. The typical training time for those models is $1$ to $5$ minutes on CPU-only machines.

% Practically, during the language generation process, embeddings are generated as by-products, therefore collecting those embeddings for reward models requires no more computational burden. To better understand the performance of this alternative choices, we now present experiment results to compare the two approaches empirically.

In this section, we explore the use of embeddings as inputs for reward modeling and contrast this approach with traditional methods employing natural language inputs. Figure~\ref{fig:teaser} illustrates the key differences: green boxes represent trainable parameter groups, while gray boxes denote intermediate variables. The left panel depicts the processing of natural language inputs by LLMs for generation tasks, whereas the right panel shows their use in LLM-based reward models for quality evaluation.

When LLMs equipped with replaced value heads are utilized for reward modeling, only a minimal number of parameters are removed. Consequently, these models retain a substantial degree of parameter freedom, making them large and computationally demanding. For example, training a 2B-parameter model using LoRA~\citep{hu2021lora} on a Tesla-V100 GPU with a typical alignment dataset of 10,000 samples approximately requires two hours. Additionally, the training process involves numerous hyperparameters that can significantly influence the outcomes.


Conversely, given the aim to evaluate natural language content effectively, and recognizing that the embedding space encapsulates a rich representation of the input both before and during the LLM eraâ€”as evidenced in tasks ranging from classification to more complex applications~\citep{mikolov2013efficient,pennington2014glove,devlin2018bert,kiros2015skip,cer2018universal,brown2020language} --- employing only embeddings as inputs presents a viable alternative. Recent studies have demonstrated the efficacy of this approach in constructing reward models for prompt evaluation in mathematical reasoning tasks and for assessing the safety and helpfulness of LLM-generated contents~\citep{sun2023query,sun2024rethinking}. Typically, these models require only 1 to 5 minutes of training time on CPU-only machines.

Moreover, as \textbf{embeddings are generated as by-products during the language generation process, utilizing them for reward models imposes no additional computational overhead}. To have a comprehensive understanding of this alternative method, we present experimental results that empirically compare the two approaches' performance in reward modeling.


%             | RM Parameter Number | Training Cost | Inference Cost | Reproducibility | 
% LLM-based | 3M - 3B | GPU, hours | High, LLM forward passes | Low, Many Hyper-params | 
% Embedding-based | 0.6M | CPU, minutes | Low, MLP forward passes | High, A few Hyper-params |


\subsection{Empirical Comparisons}
\paragraph{Reward Model Sizes} In the extant literature on reward models, LLMs typically range from 3M to 3B parameters, with specific instances such as \citet{coste2023reward} employing models between 14M and 1.3B parameters, \citet{ahmed2024scalable} using a 1.3B model, and \citet{gao2023scaling} exploring models from 3M to 3B parameters. By contrast, embedding-based methods, such as a typical 3-layer MLP with $2048$-dimensional input embeddings and $256$ hidden units, utilize fewer than 0.6M parameters. We also consider lightGBM models in our demonstrative experiments given their wide success and remarkable stability~\citep{ke2017lightgbm,grinsztajn2022tree,sun2023query}.


\paragraph{Data Generation Processes} We use the Anthropic-HH dataset, which includes the \texttt{Helpful} and \texttt{Harmless} alignment tasks to assess the efficacy of various reward model approaches~\citep{bai2022training}. The dataset contains $40000$ prompts for each task. To ensure reproducible and reliable comparisons, we use golden reward models as proxy annotators following established workflows in the literature~\citep{xiong2023gibbs,dong2024rlhf,dong2023raft,gao2023scaling,yang2024rewards}. We consider three LLMs --- Gemma-2B and -7B \citep{team2024gemma}, and LLaMA3-8B \citep{touvron2023llama} --- to generate responses. For each prompt, we generate $10$ responses and randomly select $N$ pairs for preference annotation using the golden reward models. We use Gemma2B to generate embeddings for the embedding-based reward models. This approach ensures that our evaluation accurately reflects the preferences of the golden reward model, thereby minimizing bias.

\paragraph{Annotation Quality and Quantity Control} In preference generation, the quality of annotations is often limited by the capabilities of the annotators \citep{sanderson2010user,stewart2005absolute,guest2016relative,wang2024secrets}. We apply the location-scale function class to describe annotation noise \citep{sun2024rethinking}, positing that closer values yield noisier personal preferences. We examine three annotation quality scenarios: 
\begin{enumerate}[nosep,leftmargin=*] 
\item \textbf{Low annotation quality}: high error rates (approximately $45\%$), offering minimal informative value in the preference annotations; 
\item \textbf{Medium-Low annotation quality}: error rates around $40\%$; 
\item \textbf{Medium-High annotation quality}: error rates around $30\%$; 
\item \textbf{High annotation quality}: error rates are about $5\%$. \end{enumerate}
In addition to quality, we also explore the impact of varying annotation quantities, considering the number of annotated preference pairs ranging from $500$ to $10000$.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/gemma2b_reward.png}\vspace{-0.35cm}
    \caption{\small  Comparing performances of Embedding-based RM  with LLM-based RMs. The Embedding-based RMs demonstrate high learning stability and strong performance as compared to LLM-based RMs, but are much cheaper to train and evaluate, and more scalable in inference time. Results are from the Gemma 2B model. Additional results using the Gemma 7B and LLaMA3 8B models are presented in Appendix~\ref{appdx:more_results}}
    \label{fig:performance_with_embeddings}\vspace{-0.25cm}
\end{figure*}


% Comparing Model Sizes In the extant literature on reward models, LLMs typically range from 3M to 3B parameters, with specific instances such as \citet{coste2023reward} employing models between 14M and 1.3B parameters, \citet{ahmed2024scalable} using a 1.3B model, and \citet{gao2023scaling} exploring models from 3M to 3B parameters. By contrast, embedding-based methods, such as a typical 3-layer MLP with $2048$-dimensional input embeddings and $256$ hidden units, utilize fewer than 0.6M parameters.

% \paragraph{Golden Reward Annotation and Evaluation}
% The dataset contains $40000$ prompts for each task. To make reproducible and reliable comparisons, we follow the literature~\citep{xiong2023gibbs,dong2024rlhf,dong2023raft,gao2023scaling,yang2024rewards} to use golden reward models as proxy annotators. To maximally isolate the source of gains from LLM choices and experimental randomness, we consider three LLMs in generating responses: Gemma-2B and -7B~\citep{team2024gemma}, LLaMA3-8B~\citep{touvron2023llama}. We generate $10$ responses for each prompt, and then randomly select $N$ pairs for preference annotation using the golden reward models. In this way, our evaluation maximally represents the preference of the golden reward model, hence it is unbiased. The golden reward model in our experiment setup can be considered as a consistent annotator. 

% \paragraph{Annotation Quality Control}
% In the process of preference generation, the quality of annotations may be constrained by the capabilities of the annotators~\citep{sanderson2010user,stewart2005absolute,guest2016relative,wang2024secrets}. We adopt the location-scale function class to characterize annotation noise~\citep{sun2024rethinking} --- which posits that the closer the values are to each other, the noisier the personal preferences tend to be --- instead of random adding flipping preference errors. We consider 3 different annotation quality settings: 
% \begin{enumerate}[nosep,leftmargin=*]
%     \item \textbf{Low annotation quality}, where annotation error rates are extremely high ($\approx 45\%$) hence there are only little signals in preference annotations;
%     \item \textbf{Medium annotation quality}, where annotation error rates are around $30\%$;
%     \item \textbf{High annotation quality}, where annotation error rates are around $5\%$. 
% \end{enumerate}

% \paragraph{Annotation Quantity Control}
% Besides the annotation quality, we also study the effect under different annotation quantities, considering the number of annotated preference pairs ranging from $500$ to $10000$.

% \textcolor{red}{We use Gemma2B to generate embeddings. Evaluation is based on Best-of-500 sampling. Other implementation details are provided in Appendix. MLP size, how to get embeddings, LoRA. Ref Figure 2}

% Results are shown in Figure~\ref{fig:performance_with_embeddings}, we have the following observations from the results:
% \begin{itemize}[nosep]
%     \item The embedding-based methods in general achieves significantly lower variance and have higher stability during training.
%     \item The embedding-based methods remarkably outperform smaller language models (LLM-RM-GPT2) in all cases. 
%     \item When annotation quality is low, the embedding-based methods achieve superior or on-par performance with the LLM-based reward models.
%     \item When annotation quantity is limited, the embedding-based methods achieve superior or on-par performance with the LLM-based reward models.
%     \item On the \texttt{Harmless} dataset, the embedding-based reward models can always match the performance of LLM-based reward models.
%     \item On the \texttt{Helpful} dataset, the embedding-based reward models underperform Gemma2B-based LLM reward models, the latter model gains more performance when annotation quality and availability increase.
% \end{itemize}



% Those datasets will be accessible as public assets for future research on reward modeling. We will elaborate on the scalable evaluation procedure later in Section~\ref{sec:scalable_evaluation}.
Results are presented in Figure~\ref{fig:performance_with_embeddings}. The following observations can be drawn from the analysis:

\begin{itemize}[nosep,leftmargin=*]
\item Generally, embedding-based methods exhibit significantly lower variance and higher stability during training compared to other models. 
\item Embedding-based methods consistently outperform smaller language models (such as LLM-RM-GPT2) across all evaluated scenarios. 
\item In conditions of low annotation quality, embedding-based methods demonstrate performance that is superior to or comparable with LLM-based reward models. 
\item With limited annotation quantities, embedding-based methods also show superior or comparable performance to LLM-based reward models. 
\item On the \texttt{Harmless} dataset, embedding-based reward models consistently match the performance of LLM-based reward models. 
\item On the \texttt{Helpful} dataset, however, embedding-based reward models underperform relative to Gemma2B-based LLM reward models, which benefit more significantly from increases in annotation quality and availability. \end{itemize}

These datasets will be made available as public assets to facilitate future research in reward modeling. Details on the scalable evaluation procedure will be provided in Section~\ref{sec:scalable_evaluation}.
