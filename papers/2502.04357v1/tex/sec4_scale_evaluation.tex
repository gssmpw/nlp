\section{Case Study: Efficient Reproduction of Reward Model Ensemble Papers}
\label{sec:reproduce_demo}
% As a demonstrative example, in this section, we reproduce the research discoveries on alleviating reward models overoptimization using ensemble methods~\citep{coste2023reward,ahmed2024scalable} with the proposed embedding-based reward modeling framework. 

% To reproduce and verify the key discovery in those reward modeling papers --- that reward model ensemble alleviates reward model overoptimization. We train 10 lightgbm~\citep{ke2017lightgbm} models using the default hyper-parameter setups. We also consider the MLP-based implementation with $256$ hidden units. The reward model ensemble performance is evaluated by averaging the predictions of 10 reward models. 

% We experiment on a machine equipped with 128-core \texttt{Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz} CPUs. In our reproduction setups, we use 2 different models (MLP, lightgbm), work with 2 tasks, and build reward models for 3 language models (Gemma 2B, Gemma 7B, LLaMA3 8B), study $4$ different annotation quality setups, and $5$ annotation quantities ($[500, 10000,2000,5000,10000]$). In ensemble, we train $10$ models, and the experiments are repeated with $5$ independent runs --- summarizing all experiments, we have $12000$ different models to train and evaluate for reproduction. 

% Using the CPU server, training the $6000$ lightgbm reward models and evaluating them takes 4.9h. Training the $6000$ MLP reward models and evaluating them takes 17.3h. In total, those $12000$ experimental setups can be finished within 1 day.

In this section, we replicate the findings from prior research on mitigating overoptimization in reward models through ensemble methods, as discussed in \citep{coste2023reward, ahmed2024scalable}, using our proposed embedding-based reward modeling framework.


To validate the principal finding in those works that ensembles can alleviate reward model overoptimization, we train $10$ LightGBM models \citep{ke2017lightgbm} using default hyperparameter settings, alongside an MLP-based implementation with $256$ hidden units. We assess the performance of these ensemble reward models by averaging predictions across the $10$ models. Experiments are repeated with $5$ independent runs to draw statistically significant conclusions.

Our experiments are conducted on a machine equipped with a 128-core \texttt{Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz}. Our experimental setup encompasses $2$ different models (MLP and LightGBM), $2$ tasks, and build reward models for $3$ LLMs (Gemma 2B, Gemma 7B, LLaMA3 8B). We explore $4$ different annotation quality scenarios and $5$ levels of annotation quantity, ranging from $500$ to $10000$.
\begin{figure}[h!]
    \includegraphics[width=1.0\linewidth]{cheapensemble.png}
    \vspace{-0.6cm}
    \caption{\small Using embeddings as inputs in a lightweight reward model ensemble practice to mitigate reward overoptimization. Reproduction of prior findings across over $12000$ configurations can be completed in less than 1 day using CPU-only resources.}\vspace{-0.35cm}
    \label{fig:rm_ensemble_illu}
% \vspace{-2cm}
\end{figure}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/gemma2b_reward_esb.png}\vspace{-0.65cm}
    \caption{\small Reproduction of reward model ensemble papers using embedding-based reward models. Additional results using the Gemma 7B and LLaMA3 8B models are presented in Appendix~\ref{appdx:more_results}}
    \label{fig:reward_model_ensemble_gemma2b}\vspace{-0.35cm}
\end{figure*}

In total, we train and evaluate $12000$ models. Using the CPU server, training and evaluating the $6000$ LightGBM models takes 4.9 hours, while the $6000$ MLP models require 17.3 hours. In total, these $12000$ experimental configurations are completed within $1$ single CPU day.

Finally, unlike prior research, our investigation into reward model ensembles using embeddings as inputs clarifies that the observed enhancements stem from conservative modeling approaches, rather than from the scaling laws typical of LLM-based reward models \citep{gao2023scaling}. These distinctions are visually demonstrated in the case study illustrated in Figure~\ref{fig:rm_ensemble_illu}. 

Figure~\ref{fig:reward_model_ensemble_gemma2b} shows the results from our efficient reproduction. We observe significant performance improvements when using ensemble methods in reward modeling, thereby verifying the principal findings of \citet{coste2023reward} and \citet{ahmed2024scalable} within embedding-based reward model setups. Notably, the efficacy of the reward model ensemble diminishes as annotation quality improves (i.e., when the error rate is less than $5\%$), and we observe the LightGBM reward models generally get larger performance gains from the ensemble.



