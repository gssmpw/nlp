
\section{Prompt Template}
\label{sec:appendix_prompt}

We provide the prompt we used in this work for the experiment, as depicted in Figure~\ref{fig:prompt_our}. For Vanilla LLM-as-a-Judge (Figure~\ref{fig:prompt_vanilla}), we deployed the prompt designed in \textsc{MTBench}, which is widely deployed in many works, \eg, \textsc{RewardBench}. Notably, \textsc{HelpSteer2} specializes in 5 aspects, so we replace the \textsc{MTBench}'s aspects with these aspects when we test the method in \textsc{HelpSteer2}. Furthermore, we also present the prompts of baselines: \textit{LongPrompt}(Figure~\ref{fig:prompt_long}) forces the CoT as long as possible; \textit{16-Criteria} (Figure~\ref{fig:prompt_16}) incorporates 16 criteria and corresponding descriptions, which are designed in \citet{hu2024arellm} and \citet{wang2024helpsteer}.

\begin{figure*}[!t]
  \includegraphics[width=\linewidth]{latex/figure/prompt_our.pdf}
  \caption {Prompt of Our Method.}
  \label{fig:prompt_our}
\end{figure*}

\begin{figure*}[!t]
  \includegraphics[width=\linewidth]{latex/figure/prompt_vanilla.pdf}
  \caption {Prompt of Vanilla LLM-as-a-Judge.}
  \label{fig:prompt_vanilla}
\end{figure*}

\begin{figure*}[!t]
  \includegraphics[width=\linewidth]{latex/figure/prompt_16cretiria.pdf}
  \caption {Prompt of 16-Criteria LLM-as-a-Judge.}
  \label{fig:prompt_16}
\end{figure*}

\begin{figure*}[!t]
  \includegraphics[width=\linewidth]{latex/figure/prompt_long.pdf}
  \caption {Prompt of LongPrompt LLM-as-a-Judge.}
  \label{fig:prompt_long}
\end{figure*}


\section{Testing Preference Benchmark}
\label{sec:testing}

\subsection{Preference Benchmarks}
\label{subsec:benchmark}
As shown in Table~\ref{tab:benchmarks}, we give a brief introduction to preference benchmarks. Each of these benchmarks has its own strengths; thoroughly testing all of them and averaging the results is a reliable way to evaluate the method. Notably, we randomly sampled $1$K cases from the training split of \textsc{EvalBias} since the size of the test split is 80 items, which is too small. 
\begin{table}[!tp]
  \centering
  \resizebox{0.48\textwidth}{!}{
  \begin{tabular}{lcc}
    \hline
    \textbf{Benchmarks} & \textbf{\textsc{Size}} & \textbf{\textsc{Focus}} \\
    \midrule
    \textit{RewardBench} &$2,985$& \makecell{It covers multiple  scenarios, including\\ Chat, Chat-Hard, Safety, and Reasoning.}\\
    \midrule
    \textit{HelpSteeer2} &$519$&\makecell{It provides multiple fine-grained dimensions\\ for evaluation, like Helpfulness, Coherence,\\ Correctness, Complexity, Verbosity.}\\
    \midrule
    \textit{MTBench Human} &$2,665$& \makecell{It provides multi-turn conversation for evaluation,\\ and we filter the samples whose outcome is ``Tie''.}\\
    \midrule
    \textit{JudgeBench} &$350$& \makecell{It focuses on challenging response pairs spanning\\ knowledge, reasoning, math, and coding}\\
    \midrule
    \textit{EvalBias} &$1,000$&\makecell{It tests the robustness of judges on\\ various scenarios containing evaluation biases.} \\
    \bottomrule
  \end{tabular}
  }
  \caption{The brief description of Preference Benchmarks for testing.}
  \label{tab:benchmarks}
\end{table}

\subsection{The Implementation of Generating Crowd Judgments}
\label{subsec:generate_crowd}

To generate crowd judgments, we produce a wide range of diverse responses. We employed several API-accessible and open-source LLMs to generate these responses based on the given instructions. Since diversity is crucial, we did not limit ourselves to only the most powerful models. Specifically, we used the following LLMs: \textit{Qwen-2.5-0.5B-Instruct}, \textit{Qwen-2.5-1.5B-Instruct}, \textit{Qwen-2.5-3B-Instruct}, \textit{Qwen-2.5-7B-Instruct}, \textit{Llama-3.2-1B-Instruct}, \textit{Llama-3.2-3B-Instruct}, \textit{Llama-3.1-8B-Instruct}, \textit{Mistral-Nemo10-Instruct-2407}, \textit{Mistral-7B-Instruct}, \textit{GPT-4o-mini}, and \textit{GPT-4o}. Additionally, we applied two temperature settings ($0.7$ and $1.0$) for each model. In principle, greater diversity in models and temperature configurations leads to improved performance.


Based on these crowd responses, we deployed the vanilla LLM-as-a-Judge to judge each crowd response with candidate response A/B separately using the judge LLM. 

\subsection{The Implementation of Baselines}
\label{subsec:implemenatation_baseline}

For maj@16 and agg@16, we modify the temperature setting to $1.0$ to promote more diversified responses. For other inferences in baselines, we set a unified temperature as $0.1$.


\subsection{The Implementation of Selection and Processing}
\label{subsec:implementation_selection}

For the selection strategy, we adopted ``Criticize Selection'' by choosing the crowd judgment where the outcome indicates that response A/B loses. For ``Outcome Removal Processing,'' we used \textit{GPT-4o-mini} to eliminate the outcome segment from the judgment with a temperature of $0$. The prompt is: 
\begin{quote}
``\textit{You are a helpful assistant. Specifically, I will provide you with the text quality judgment from an LLM-as-a-Judge evaluation of the responses from two AI assistants to an instruction. I need you to remove the final conclusion segments and only remain the evaluation analysis segments as soon as possible. ONLY OUTPUT the processed judgment.} ''

``\textit{*Judgment:* \{judgment\}}''
\end{quote}


\subsection{The Implementation of Inference}
\label{subsec:implementation_inference}

We tested our method on multiple LLMs-as-Judge, including \textit{GPT-4o} (2024-08-06), \textit{Qwen 2.5-7B-Instruct}, \textit{Qwen 2.5-32B-Instruct}, \textit{Qwen 2.5-72B-Instruct}, and \textit{Llama 3.3-70B-Instruct}. We found that reliability and consistency of evaluation can be balanced when temperature$=0.1$.


\section{Distilling CoT for Training Judge}
\label{sec:distilling4training}

\subsection{Distilling Preference Source}
\label{subsec:distilsource}
We chose the TULU3-Preference-Mixture~\footnote{\url{https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-8b-preference-mixture}} as the preference data source. Specifically, we prompt the LLM-as-a-Judge to generate a CoT using the given instruction along with the chosen-rejected response pairs as input. Additionally, we experiment with two training sizes: random samples of $10$K and $30$K examples.

\paragraph{Distilling Inference.} We use the \textit{GPT-4o} as the Judge to produce the CoT, and the temperature setting is $0.1$. 

\subsection{The Implementation of Training Judge}
\label{subsec:implementation_trainingjudge}

\paragraph{Base Models.} To verify the generality of our method in Distilling CoT, we fine-tuned the preference data and corresponding CoT judgment in base LLMs: \textit{Qwen 2.5-7B-Base} and \textit{Llama 3.1-8B-Base}.

\paragraph{Training Setting.} We trained the Base LLM with a \textit{context length}$=4,096$, \textit{epochs}$=3$, \textit{batch size}$=128$,and \textit{learning rate}$=2e^{-5}$.



\section{SFT Data Selection}
\label{sec:sft_data_selection}

\subsection{Synthetic Response Pool for Selection}
\label{subsec:synthetic}
To enhance the challenge and realism of the SFT Data Selection, we chose four LLMs with similar general generation capabilities as the base models for synthesizing responses. These are: \textit{GPT-4o}, \textit{DeepSeek-v3}, \textit{Claude-3.5-Sonnet}, and \textit{Qwen 2.5-72B-Instruct}. For inference, we set the temperature parameter to 0.7. We generate four responses for each instruction to serve as the basis for subsequent selection. The base instruction queries we used are two pools: LIMA and TULU3-SFT. LIMA~\footnote{\url{https://huggingface.co/datasets/GAIR/lima}} contains 1,000 instructions, which are regarded as high-quality; TULU3-SFT~\footnote{\url{https://huggingface.co/datasets/allenai/tulu-3-sft-mixture}} contains $93.9$K instruction-response pairs, and we randomly sampled $10$K instructions as the query. The latter is the latest released multilingual dataset.

\subsection{The Implementation of Rejection Sampling}
\label{subsec:implementation_rejection}

Under the vanilla LLM-as-a-Judge approach, we perform pairwise comparisons among four responses, awarding a score of $+1$ to the winner of each matchup. After all comparisons, the response with the highest total score is selected. Building on this, our method incorporates the remaining two responses as ``crowd responses'' during each evaluation, allowing us to gather additional crowd judgments.

\paragraph{Base Judge Model.} The base judge model is \textit{GPT-4o}, and the temperature is set as $0.1$.

\subsection{The Implementation of Training SFT}
\label{subsec:implementation_trainingsft}

\paragraph{Base Models.} To verify the generality of our method in SFT data selection, we fine-tuned the instruction and selected response in base LLMs: \textit{Qwen 2.5-7B-Base} and \textit{Llama 3.1-8B-Base}.

\paragraph{Training Setting.} We followed the common setup for SFT, with a \textit{context length}$=2048$, \textit{epochs}$=3$, \textit{batch size}$=128$,and \textit{learning rate}$=2e^{-5}$.



\section{Inference Scaling}
\label{sec:infer_scal_appendix}

The ``Vanilla'' setup has no crowd judgments, ``1'' includes a single judgment, and even-numbered settings split judgments evenly between A and B. We use \textit{GPT-4o} as the judge and sample three times per setting to obtain the average result.

\section{CoT Comparison}
\label{sec:cot_comp_appendix}
\subsection{Key Points Extraction}
We use the Key points statistic to measure the richness of the CoT. Firstly, we use the \textit{GPT-4o-mini} to summarize the CoT to aspects and corresponding sub-points. The summarization prompt is 

\begin{quote}
    \textit{``Extract the key evaluation aspects and detailed points mentioned in the text below. List the aspects and points in a strictly structured format:''}
    
    \textit{``Example Input: `The response is accurate but lacks creativity. It includes factual details but misses key arguments.' ''}
    
    \textit{``Example Dictionary Output:''}
    \textit{``- Aspect: Accuracy ''}
    \textit{``  - Sub-point: Includes factual details ''}
    \textit{``  - Sub-point: Misses key arguments ''}
    \textit{``- Aspect: Creativity ''}
    \textit{``  - Sub-point: Lacks originality''}

    \textit{``**Input**:''}
\end{quote}

When we generate the summarized dictionary parsed output, we can get the total number of key points of each CoT.

\subsection{Coverage Rate Compuataion}
\label{subsec:coverage_appendix}
An attention-based approach computes mapping weights linking output tokens to input tokens. Interpretability research~\citep{bibal2022attention,vig2019multiscale} uses these weights to assess which input tokens influence the output. Our goal is to quantify how thoroughly CoT evaluates details in the target text, and attention-based computation provides a precise method for doing so.

Naturally, we used the \textit{bart-base}~\footnote{\url{https://huggingface.co/facebook/bart-base}} to compute the cross-attention between the target text and the generated CoT. We extract the cross-attention weights from the last layer of the decoder. By averaging these weights across attention heads and applying a threshold$=0.3$, it calculates a coverage rate—the fraction of the target text’s tokens whose attention is above the threshold from the CoT.

