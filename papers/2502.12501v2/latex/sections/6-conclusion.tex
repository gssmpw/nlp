\section{Conclusion}
\label{sec:conclusion}
In this work, we tackle the shortcomings of LLM-as-a-Judge, which stem from CoT reasoning lacking comprehensiveness and detail, by drawing inspiration from human evaluative behavior. We introduce a novel crowd-based comparative evaluation framework that enriches the CoT process to unlock more comprehensive and reliable evaluations. By scaling inference more effectively, our method serves as an efficient alternative to traditional majority voting and criteria expansion. Importantly, we demonstrate that high-quality CoT judgments boost evaluation reliability and distilling efficiency across multiple benchmarks, while broadening the scope of crowd-based evaluation applications.


\section*{Limitations}
\paragraph{Progressive Self-Iteration Paradigm.} A limitation of our work is that we do not explore self-iteration in this study, despite its potential for enhancing the evaluation process. Our method inherently allows for iterative refinement, which could be further extended into a progressive paradigm. We leave this direction for future work, aiming to investigate how iterative self-improvement can further enhance evaluation quality and robustness.


\paragraph{Selection based on LLMs.} We identify that the quality of crowd judgments influences the CoT and explore a simple yet efficient selection strategy. We generate crowd responses using many LLMs, but we do not explore which LLM's crowd response has a greater influence on crowd judgment.