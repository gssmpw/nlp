\section{Introduction}
\label{sec:intro}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{latex/figure/figure1_main.pdf}
  \caption {An overview of our method. By evaluating the candidate responses A/B alongside the crowd responses, the resulting crowd judgment can be used as context to enrich the evaluation of A/B responses, leading to a more comprehensive CoT judgment.}
  \label{fig:overview}
\end{figure}

With the prohibitive cost and limited scalability of human evaluation, LLM-as-a-Judge has emerged as a scalable framework for auto-evaluation~\citep{chang2024survey,li2024llmsas,li2025generationjudgmentopportunitieschallenges}.
Given a \textit{task instruction} and corresponding \textit{candidate responses}, LLM-as-a-Judge~\citep{zheng2023mtbench,wang2024selftaughtevaluators,wagner2024blackbox} employs CoT judgment to analyze granular quality details of the responses, ultimately deriving a final outcome.
Despite advancements in techniques such as CoT reasoning~\citep{saha2025learningplanreason,zheng2023mtbench}, specialized rubrics~\citep{liu2023geval}, and preference-aligned training datasets~\citep{li2024generative,wang2024pandalm}, human evaluation remains the gold standard due to persistent limitations~\citep{zeng2024evaluating} in LLM-as-a-Judge.
These limitations include biases~\citep{park2024offsetbias} in judgment and susceptibility to misleading context~\citep{dubois2024lengthcontrolledalpacaevalsimpleway,chen2024humans}, which undermine the reliability of automated evaluation.
One important yet overlooked reason is the quality of CoT reasoning hinges on the modelâ€™s ability to comprehensively compare nuanced details across responses. Our observation reveals high-quality judgments incorporate a thorough comparison of these details, while flawed reasoning tends to focus on limited details, leading to premature and incomplete outcomes. Therefore, enhancing the richness and comprehensiveness of CoT reasoning is essential to improve LLM-as-a-Judge.





Two commonly adopted strategies aim to address this issue: majority voting~\citep{zhang2024generative,mahan2024generativerewardmodels,deepseekai2024deepseekv3technicalreport} and criteria expansion~\citep{kim2024prometheus,liu2024hd,hu2024llmevaluator}.
The majority voting generates multiple judgments independently in parallel and aggregates these results through voting.
It essentially leverages the randomness from temperature sampling to encourage detailed reasoning.
However, this approach is passive and computationally expensive.
In contrast, criteria expansion augments prompts with additional evaluation aspects, proactively guiding the model to consider more dimensions of quality.
Yet, this strategy is response-unaware, failing to adapt the evaluation process to the unique details of each response. For instance, even if a response is rich with nuanced insights, incorporating a criterion like ``accuracy'' does little to prompt the LLM to identify the unique details of its reasoning.
Consequently, neither approach effectively guides LLM-as-a-Judge to consistently produce nuanced, comprehensive CoT evaluations.
This leads to a critical research question: \textit{how can we guide LLMs to engage in deeper, more detail-rich CoT reasoning during judgment?}



In this work, we propose a novel \textbf{crowd-based comparative evaluation} (\textbf{\textsc{CCE}}) to address this challenge by enabling LLM-as-a-Judge to uncover valuable details, as depicted in Figure \ref{fig:overview}.
Our approach is inspired by human evaluative behavior: humans merely compare candidates in isolation by also contrasting them against a broader crowd, thereby uncovering additional nuanced insights about each candidate. Building on this principle, \textsc{CCE} first gathers a set of alternative responses to the task instruction, referred to as \textit{crowd responses}, and then compares each candidate response against these crowd responses to derive multiple \textit{crowd judgments}. Throughout this process, the diversity of crowd responses serves as multiple evaluation anchors, revealing different layers of detail within the candidate responses. Based on this, \textsc{CCE} prompts the LLM-as-a-Judge to perform a more comprehensive and deeper overall CoT judgment.




%\textsc{CCE} achieves a remarkable average improvement of $6.7\%$ across five evaluation benchmarks, including \textsc{RewardBench}, \textsc{HelpSteer2}, \textsc{MTBench Human}, \textsc{JudgeBench} and \textsc{EvalBias}. 
%with a $6.6\%$ gain on \textsc{RewardBench}.
%When applied to judge distillation, we find high-quality long CoT judgments produced by \textsc{CCE} have a higher efficiency for training a smaller judge model, especially enhancing bias robustness.
%Moreover, we extend \textsc{CCE} naturally to SFT rejection sampling, refer to \textit{crowd rejection sampling}, where our approach serves as a quality signal to identify training-efficient samples from the response pool. This extension highlights the reliability and practicality of \textsc{CCE}.
%Finally, analysis experiments confirm that \textsc{CCE} promotes more comprehensive and deeper CoT reasoning and scales inference effectively.


CCE achieves a remarkable average improvement of $6.7\%$ across five judge benchmarks, including \textsc{RewardBench}, \textsc{HelpSteer2}, \textsc{MTBench Human}, \textsc{JudgeBench} and \textsc{EvalBias}. When applied to judge distillation, we find that the high-quality long CoT judgments generated by CCE enable a smaller judge model to achieve higher accuracy, yielding an average improvement of $4.5\%$-$5.6\%$ (in Qwen 2.5-7B), particularly enhancing bias robustness. Moreover, we extend \textsc{CCE} naturally to SFT rejection sampling, referred to as \textit{crowd rejection sampling}, where our approach serves as a quality signal to identify training-efficient samples from the response pool. Our enhanced rejection strategy consistently outperforms both random sampling and vanilla rejection sampling on \textsc{MTBench} and \textsc{AlpacaEval-v2}, demonstrating the reliability and practical utility of \textsc{CCE} in LLM alignment. Finally, our analysis confirms that \textsc{CCE} scales inference effectively and produced CoTs consistently yield more key points and capture finer-grained details within responses compared to Vanilla LLM-as-a-Judge, facilitating more comprehensive and deeper CoT reasoning. 