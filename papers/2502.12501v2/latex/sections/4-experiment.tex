\section{Experiments}
\subsection{Experimental Setup}
We conduct a comprehensive evaluation of \textsc{CCE} across three tasks: testing preference benchmarks, judge distillation, and SFT rejection sampling. 

\begin{table*}[!t]
\centering
\small 

\resizebox{0.92\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Model}&\makecell{\textbf{\textsc{Reward}}\\\textbf{\textsc{Bench}}} & \textbf{\textsc{HelpSteer2} }& \makecell{\textbf{\textsc{MTBench}}\\\textbf{\textsc{Human}}} & \makecell{\textbf{\textsc{Judge}}\\\textbf{\textsc{Bench}}} & \textbf{\textsc{EvalBias}} & \textbf{Avg.}\\

\midrule
\textbf{GPT-4o} \\
~\textit{Vanilla}&85.2&66.1&82.1&66.3&68.5&73.6\\
~\textit{LongPrompt}&86.9&67.3&81.8&63.5&70.5&74.0 \\
~\textit{EvalPlan}&88.7&65.5&81.4&62.9&74.4&74.6 \\
~\textit{16-Criteria} &87.3&69.1&82.8&66.6&73.7&75.9\\
~\textit{Maj@16} &87.9&68.9&82.4&68.6&75.5&76.7\\
~\textit{Agg@16} &88.1&68.7&82.6&67.2&77.9&76.9\\
\rowcolor{green!10}
~\textit{\textsc{CCE}-random@16} &91.2&69.5&83.1&68.9&80.1&78.6\\
\rowcolor{green!10}
~\textit{\textsc{CCE}@16} &\textbf{91.8}&\textbf{70.6}&\textbf{83.6}&\textbf{70.4}&\textbf{85.0}&\textbf{80.3}\\
\midrule
\textbf{Qwen 2.5 7B-Instruct} \\
~\textit{Vanilla}&78.2&60.7&76.1&58.3&57.4&66.1\\
\rowcolor{green!10}
~\textit{\textsc{CCE}@16}&\textbf{80.4}&\textbf{64.2}&\textbf{76.7}&\textbf{64.0}&\textbf{79.4}&\textbf{72.9}\\
\midrule
\textbf{Qwen 2.5 32B-Instruct} \\
~\textit{Vanilla}&87.4&\textbf{72.3}&79.0&68.9&71.1&75.7\\
\rowcolor{green!10}
~\textit{\textsc{CCE}@16}&\textbf{90.8}&72.1&\textbf{82.1}&\textbf{70.6}&\textbf{80.5}&\textbf{79.2}\\
\midrule
\textbf{Qwen 2.5 72B-Instruct} \\
~\textit{Vanilla}&85.2&\textbf{69.5}&79.5&68.3&68.5&74.0\\
\rowcolor{green!10}
~\textit{\textsc{CCE}@16}&\textbf{93.7}&68.5&\textbf{88.9}&\textbf{75.7}&\textbf{85.9}&\textbf{82.7}\\
\midrule
\textbf{Llama 3.3 70B-Instruct} \\
%\cdashline{1-7}
~\textit{Vanilla}&86.4&70.4&81.1&67.1&70.6&75.1\\
\rowcolor{green!10}
~\textit{\textsc{CCE}@16}&\textbf{91.7}&\textbf{71.3}&\textbf{83.5}&\textbf{69.7}&\textbf{79.2}&\textbf{79.1}\\
\bottomrule
\end{tabular}
}
\caption{Accuracy of LLM-as-a-Judge on pair-wise comparison benchmarks. \textsc{CCE} can consistently enhance the LLM-as-a-Judge's performance across 5 benchmarks, especially considerably outperforming other scaling inference strategies, like maj@16. The highest values are \textbf{bolded}. Here, \textit{\textsc{CCE}-random} refers to replacing the ``Criticizing Selection$+$Outcome-Removal Processing'' with ``Random Selection''.
}
\label{tab:main_preference}
\end{table*}




\paragraph{Preference Benchmarks and Baselines.} We adopt 5 preference benchmarks to test LLM-as-a-Judge, including \textsc{RewardBench}~\citep{lambert2024rewardbench}, \textsc{HelpSteer2}~\citep{wang2024helpsteer}, \textsc{MTBench-Human}~\citep{zheng2023mtbench}, \textsc{JudgeBench}~\citep{tan2025judgebench}, and \textsc{EvalBias}~\citep{park2024offsetbias}. These benchmarks provide general instructions across a wide range of tasks with diverse responses and use accuracy to measure their evaluation performance. They each focus on different aspects. For example, \textsc{RewardBench} covers a wider range of scenarios, while \textsc{EvalBias} focuses on various bias scenarios. We verify the generality of \textsc{CCE} on 5 LLMs and compare it against multiple baselines. In particular, we consider \textbf{Vanilla}, which uses the general LLM-as-a-Judge prompt implemented by \textsc{RewardBench}; \textbf{Maj@16}, where we independently judge a case 16 times and take a majority vote of the outcomes; \textbf{Agg@16}, where instead of majority voting, the 16 individual judgments are fed back into the LLM to aggregate a final decision; \textbf{16-Criteria}, which incorporates 16 criteria with corresponding descriptions in the prompt as designed in~\citet{hu2024arellm} and~\citet{wang2024helpsteer}; \textbf{LongPrompt}, where the LLM is explicitly directed to produce a longer CoT; and \textbf{EvalPlan}, in which an unconstrained evaluation plan is first generated based on the target case and then executed to derive the final judgment~\citep{saha2025learningplanreason}. Additional details on the preference benchmarks and baselines can be found in Appendix~\ref{sec:testing}.





\paragraph{Distilling CoT for Training Judge.} We start with a large preference dataset and evaluate it using the Vanilla LLM-as-a-Judge and \textsc{CCE} under \textit{GPT-4o-as-a-Judge}, producing two CoTs. We then pair each CoT with the original preference data to form two separate training sets, which we use to fine-tune a smaller LLM as a judge. The resulting judgesâ€™ performance clearly reflects the quality and effectiveness of each CoT. We use \textbf{TULU3-preference} data as the distillation query while the preference benchmarks for evaluating the judge remain the same as previously introduced. Details of the training implementation are provided in Appendix~\ref{sec:distilling4training}.

\paragraph{SFT Rejection Sampling.} Firstly, we generate a pool of 4 responses based on a given task instruction to serve as the rejection sampling base. We compare Crowd Rejection Sampling against Random Selection and a Vanilla Rejection Sampling method to select the best response for fine-tuning.


We select two datasets of different scales, \textbf{LIMA}~\citep{zhou2023lima} ($1$K) and \textbf{TULU3-SFT}~\citep{lambert2025tulu3} (sample $10$K), as instruction query. \textit{GPT-4o} served as the judge LLM, while \textit{Llama-3.1-8B} and \textit{Qwen-2.5-7B} are used as base models for SFT. We then evaluate the generative ability of finetuned models using \textsc{MTBench} and \textsc{AlpacaEval-2}~\citep{dubois2024lengthcontrolled}. Details of the implementation are provided in Appendix~\ref{sec:sft_data_selection}.


\begin{table*}[!t]
\centering
\small 
\resizebox{0.96\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\textbf{Model}&\textbf{\# of Training Samples} &\textbf{\textsc{RewardBench}} & \textbf{\textsc{HelpSteer2} }& \textbf{\textsc{MTBench Human}} & \textbf{\textsc{JudgeBench}} & \textbf{\textsc{EvalBias}} & \textbf{Avg.}\\
\midrule
\textbf{JudgeLM-7B}~\citep{zhu2023judgelmfinetunedlargelanguage}&100,000&\underline{46.4}&\underline{60.1}&64.1&32.6&\textbf{42.4}&\underline{49.1}\\
\textbf{PandaLM-7B}~\citep{wang2024pandalm}&300,000&45.7&57.6&\underline{75.0}&36.0&27.0&48.3\\
\textbf{Auto-J-13B}~\citep{li2024generative}&4,396&\textbf{47.5}&\textbf{65.1}&\textbf{75.2}&\textbf{50.9}&16.5&\textbf{51.0}\\
\textbf{Prometheus-7B}~\citep{kim2024prometheus}&100,000&34.6&30.8&52.8&9.3&11.7&27.8\\
\textbf{Prometheus-2-7B}~\citep{kim2024prometheus2opensource} &300,000&43.7&37.6&55.0&\underline{39.4}&\underline{39.8}&43.1\\
\midrule
\textbf{Llama-3.1-8B-Tuned} &&&&&&&\\
~\textit{Synthetic Judgment from Vanilla}&10,000&66.8&56.0&71.6&\underline{60.1}&34.2&57.7\\
~\textit{Synthetic Judgment from Vanilla}&30,000&\textbf{72.5}&\underline{58.6}&\underline{73.9}&50.4&\underline{46.2}&60.3\\
~\textit{Synthetic Judgment from \textsc{CCE}}&10,000&69.7&\underline{58.6}&72.7&\textbf{66.4}&38.7&\textbf{61.2}\\
~\textit{Synthetic Judgment from \textsc{CCE}}&30,000&\underline{70.0}&\textbf{60.1}&\textbf{74.3}&50.3&\textbf{50.7}&\underline{61.1}\\
\midrule
\textbf{Qwen 2.5-7B-Tuned} &&&&&&&\\
~\textit{Synthetic Judgment from Vanilla}&10,000&68.1&55.6&70.7&\underline{50.2}&38.4&56.6\\
~\textit{Synthetic Judgment from Vanilla}&30,000&71.4&56.2&75.1&48.2&54.7&61.1\\
~\textit{Synthetic Judgment from \textsc{CCE}}&10,000&68.8&56.7&71.3&49.8&40.2&57.4\\
~\textit{Synthetic Judgment from \textsc{CCE}}&30,000&\underline{73.3}&\underline{59.5}&\underline{74.9}&50.1&\underline{57.1}&\underline{63.0}\\
~\textit{Mix Synthetic Judgment from \textsc{CCE}\&Vanilla}&60,000&\textbf{74.1}&\textbf{60.7}&\textbf{76.6}&\textbf{61.6}&\textbf{60.6}&\textbf{66.7}\\
\bottomrule
\end{tabular}
}
\caption{Accuracy of Trained small LLM-as-a-Judge on pair-wise comparison benchmarks. Under the same preference pairs data, the model trained with judgments synthesized using \textsc{CCE} achieves more reliable evaluation results. The highest values are \textbf{bolded}, and the second highest is \underline{underlined}.}
\label{tab:main_distill}
\end{table*}




\subsection{Experiment Result}
In this section, we present our main results. The preference benchmark results are shown in Table~\ref{tab:main_preference}, the efficacy of distilling CoT for training smaller judges is summarized in Table~\ref{tab:main_distill}, and the training efficiency of SFT rejection sampling is reported in Table~\ref{tab:main_sft}. These three objectives are concluded across various judge LLMs and downstream tasks. Our findings for each task are as follows.



\paragraph{Performance on Preference Benchmarks.} Table~\ref{tab:main_preference} highlights \textbf{\textsc{CCE} consistently achieves state-of-the-art performance across all preference benchmarks}. First, it outperforms the Vanilla LLM-as-a-Judge, which already demonstrates reasonable reliability on multiple LLMs and benchmarks. Notably, with \textit{Qwen 2.5-72B-Instruct} as the judge, our method achieves an $8.5$ increase on \textsc{RewardBench} and an overall average gain of $8.7$. 
%



Second, \textbf{\textsc{CCE} proves considerably more effective than common scaling strategies such as \textit{Maj@16} and 16-Criteria}. Even with random selection, \textit{Maj@16} underperforms \textsc{CCE} by an average of 1.9. Although \textit{EvalPlan} offers a more response-aware reasoning process than \textit{16-Criteria}, its effectiveness remains lower $2.0$-$3.7$ than \textsc{CCE}. Simply generating longer CoT also falls short, indicating that scaling inference-time computation calls for a more nuanced approach.



\begin{table}[!thbp]
  \centering
  \resizebox{0.45\textwidth}{!}{
  \begin{tabular}{lcc}
    \hline
    \textbf{Rejection Sampling Method} & \textbf{\textsc{MTBench}} & \textbf{\textsc{AlpacaEval-2}} \\
    \midrule
    \multicolumn{3}{c}{Llama 3.1 8B Base} \\
    \midrule
    \textbf{Instructions from LIMA \# 1K}&&\\
    ~\textit{Random Sampling} &\underline{4.33}&2.89/3.29 \\
    ~\textit{Vanilla Rejection Sampling} &4.28&\underline{2.91/3.29} \\
    ~\textit{Crowd Rejection Sampling} &\textbf{4.53}&\textbf{3.02/3.31} \\
    \textbf{Instructions from Tulu 3 \# 10K}&&\\
    ~\textit{Random Sampling} &7.51&12.81/12.45 \\
    ~\textit{Vanilla Rejection Sampling}&\underline{7.56}&\underline{19.92/17.17} \\
    ~\textit{Crowd Rejection Sampling} &\textbf{7.63}&\textbf{22.23/19.74} \\
    \midrule
    \multicolumn{3}{c}{Qwen 2.5 7B Base} \\
    \midrule
    \textbf{Instructions from LIMA \# 1K}&&\\
    ~\textit{Random Sampling} &\underline{8.06}&\underline{14.52/9.40}\\
    ~\textit{Vanilla Rejection Sampling} &7.91&14.40/9.44  \\
    ~\textit{Crowd Rejection Sampling} &\textbf{8.63}&\textbf{14.86/9.59}\\
    \textbf{Instructions from Tulu 3 \# 10K}&&\\
    ~\textit{Random Sampling} &8.36&21.39/13.68 \\
    ~\textit{Vanilla Rejection Sampling} &\textbf{8.46}&\underline{22.71/16.44} \\
    ~\textit{Crowd Rejection Sampling} &\underline{8.41}&\textbf{23.78/17.56}  \\
    
    \bottomrule
  \end{tabular}
  }
  \caption{SFT Rejection Sampling Performance on the Instruction-Following Benchmark.
  The model fine-tuned with responses sampled using \textsc{CCE} demonstrates improved generative performance.}
  \label{tab:main_sft}
\end{table}






\begin{table*}[!tp]
\centering
\small 

\resizebox{0.96\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\textbf{Strategy}&\textbf{\# of Selection Samples} &\textbf{\textsc{RewardBench}} & \textbf{\textsc{HelpSteer2} }& \textbf{\textsc{MTBench Human}} & \textbf{\textsc{JudgeBench}} & \textbf{\textsc{EvalBias}} & \textbf{Avg.}\\

\midrule
~\textit{Random-Selection} &8&91.0&\underline{69.9}&82.6&68.7&78.4&78.1\\
~\textit{Praising-Selection} &8&86.6&64.2&81.5&67.1&77.7&75.4\\
~\textit{Criticizing-Selection} &8&\underline{91.2}&69.2&\underline{83.0}&68.9&79.1&78.3\\
~\textit{Balanced-Selection} &8&90.7&68.6&82.8&67.4&78.7&77.6\\
~\textit{Outcome-Removal Random-Selection} &8&\textbf{91.5}&\underline{69.9}&\underline{83.0}&\underline{69.4}&\underline{79.5}&\underline{78.7}\\
~\textit{Outcome-Removal Criticizing-Selection (Sota)} &8&\textbf{91.5}&\textbf{70.1}&\textbf{83.2}&\textbf{69.5}&\textbf{79.9}&\textbf{78.8}\\
\midrule
~\textit{Random-Selection} &16&91.2&69.5&83.1&68.9&80.1&78.6\\
~\textit{Praising-Selection} &16&87.0&68.4&82.0&67.1&77.9&76.5\\
~\textit{Criticizing-Selection} &16&90.8&\underline{69.7}&83.0&69.6&\underline{82.9}&\underline{79.2}\\
~\textit{Balanced-Selection} &16&90.6&69.3&82.9&68.0&79.6&78.1\\
~\textit{Outcome-Removal Random-Selection} &16&\underline{91.7}&\underline{69.7}&\underline{83.2}&\underline{70.0}&81.5&\underline{79.2}\\
~\textit{Outcome-Removal Criticizing-Selection(Sota)} &16&\textbf{91.8}&\textbf{70.6}&\textbf{83.6}&\textbf{70.4}&\textbf{85.0}&\textbf{80.3}\\

\bottomrule
\end{tabular}
}
\caption{Accuracy of \textsc{CCE} using different selection strategies on LLM-as-a-Judge benchmarks. Our proposed \textit{Outcome-Removal Criticizing-Selection} consistently surpasses performances using other selection strategies during the test-time inference phase.}
\label{tab:ablation_selection}
\end{table*}


\begin{figure*}[h]
\centering
  \includegraphics[width=0.96\linewidth]{latex/figure/scaling_inference.pdf}
  \caption {Evaluation performance under scaling crowd judgments in the context. As the number of crowd judgments grows, both accuracy and CoT length generally increase.}
  \label{fig:scaling}
\end{figure*}



Finally, \textsc{CCE} not only excels on \textsc{RewardBench}, the most general benchmark, but also \textbf{outperforms alternatives on more challenging tasks} like \textsc{JudgeBench} and \textsc{EvalBias}. Strategic crowd judgment selection further enhances performance compared to random selection. We adopt a ``Criticizing Selection + Outcome Removal'' strategy for our SOTA selection \& processing strategy, which we discuss in detail in the following analysis.





\paragraph{Distilling CoT for Training Smaller Judges.} Distilling preference evaluation capabilities from powerful LLMs to train smaller LLMs is a promising direction. Table~\ref{tab:main_distill} demonstrates that higher-quality CoT leads to more effective distillation, resulting in improved performance for smaller judge models. Fine-tuning small models (\eg, \textit{Llama 3.1-8B} and \textit{Qwen 2.5-7B}) on the CoTs generated by \textsc{CCE} yields higher accuracy on all five benchmarks than using \textit{Vanilla} CoTs. For instance, \textit{Qwen 2.5-7B} trained on \textsc{CCE}'s synthetic CoT judgments achieves up to 73.3\% on \textsc{RewardBench}, surpassing Vanilla baseline by a notable margin of 1.9. Moreover, combining both \textit{Vanilla} and \textsc{CCE} synthetic judgments further boosts performance, reaching 74.1\% on \textsc{RewardBench} and 60.6\% on \textsc{EvalBias}. This result suggests integrating diverse CoT can further enhance accuracy and generalization.

LLM-as-a-Judge can develop biases in various scenarios, such as favoring more verbose answers. This issue is particularly pronounced in smaller judge models. As shown in Table~\ref{tab:main_distill}, even after fine-tuning on over 100K samples, many baseline models struggle to exceed 50\% accuracy. This highlights the persistent challenge of evaluation bias. \textbf{Higher-quality and more comprehensive CoT distillation enhances the debiasing ability of smaller judge models}. These findings suggest that many biases stem from the model focusing on limited aspects of the responses rather than assessing them holistically.




\paragraph{Efficacy in SFT Rejection Sampling.} As we can see in Table~\ref{tab:main_sft}, Crowd Rejection Sampling proves effectiveness for both $1$K and $10$K data sizes, consistently \textbf{yielding better finetuning performances for two base LLMs}. \textsc{CCE} selects higher-quality responses compared to both Random Sampling and Vanilla Rejection Sampling, leading to consistent improvements in downstream instruction-following benchmarks on \textsc{MTBench} and \textsc{AlpacaEval-2}. For instance, with \textit{Llama 3.1-8B} and the TULU3-SFT instructions, the fine-tuned model sees performance gains of up to $22.23$/$19.74$ on \textsc{AlpacaEval-2}, compared to $19.92$/$17.17$ under the Vanilla Rejection Sampling. This underscores the reliability of \textsc{CCE} in identifying higher-quality training examples.

Overall, the experiments confirm the flexibility and effectiveness of \textsc{CCE} in three key general scenarios. By \textbf{leveraging crowd-based context, scaling inference-time computation, and strategically guiding the CoT process}, \textsc{CCE} delivers consistent improvements over strong baselines.


\subsection{Analysis Experiments}
In this section, we conduct an in-depth analysis of the two core components of our method: crowd judgment selection \& processing strategies, as well as inference scaling. We then directly examine whether the generated CoT is more comprehensive and provides a more detailed analysis of the responses under evaluation.


\paragraph{Selection \& Processing Strategy.}
We compare Random Selection, Criticizing Selection, Praising Selection, and Balanced Selection.
As shown in Table~\ref{tab:ablation_selection}, Criticizing Selection yields the best results, followed by Balanced Selection, while Praising Selection performs even worse than Random Selection. This suggests that \textbf{lose-based judgments provide deeper insights into A/B comparisons, making criticism more informative}. Additionally, the \textbf{Outcome-Removal post-processing strategy substantially improves evaluation reliability}, likely because final verdicts lack valuable details while introducing biases into LLM decision-making.




\paragraph{Inference Scaling.} 
Figure~\ref{fig:scaling} illustrates our analysis of how scaling crowd judgments influence evaluation outcomes. Measuring accuracy and the average token length of the CoT, three preference benchmarks are tested across different judgment counts and then averaged for an overall assessment. The implementation details are in Appendix~\ref{sec:infer_scal_appendix}.

As shown in Figure~\ref{fig:scaling}, \textbf{both performance and output length generally increase as crowd judgments rise from 0 to 16}. \textsc{RewardBench} displays a clear upward trend, while \textsc{HelpSteer2} dips briefly at 2 judgments before recovering. Averaging across benchmarks (rightmost panel) confirms that more crowd judgments lead to higher accuracy and longer CoT, consistent with the inference scaling observed in studies~\citep{brown2024largelanguagemonkeysscaling,snell2025scaling}.
Furthermore, we reexamine the Table~\ref{tab:main_preference} and find that \textbf{scaling test-time inference is a promising strategy for LLM-as-a-Judge}, as demonstrated by \textit{GPT-4o-as-a-Judge}. This is especially evident in bias scenarios, where the Vanilla struggles, while scaling-inference-based baselines, including \textsc{CCE}, show substantial gains.

\begin{figure}[t]
\centering
  \includegraphics[width=0.96\linewidth]{latex/figure/analysis.pdf}
  \caption {CoT Comparison. \textsc{CCE}â€™s CoT consistently yields a higher average number of key points and a higher coverage rate across all benchmarks.}
  \vspace{-.5em}
  \label{fig:cot_comp}
\end{figure}


\paragraph{CoT Comparison.} To more directly assess whether the CoTs generated by \textsc{CCE} are more comprehensive than those of the Vanilla approach, we perform two analyses: Key Points Counting and Coverage Rate. First, we use \textit{GPT-4o} to parse and summarize each CoT, counting the key points to measure how thoroughly the CoT is. Second, we leverage the cross-attention mechanism from \textit{Bart-base} to quantify the coverage rate--how thoroughly CoT covers details in the candidate responses. We introduce the details in the Appendix~\ref{subsec:coverage_appendix}.

As shown in Figure~\ref{fig:cot_comp}, \textsc{CCE} outperforms Vanilla across all benchmarks in key point counting and coverage rate. More key points indicate that our CoT examines the text from multiple angles, while a higher coverage rate reflects a more detailed analysis. These results demonstrate that \textbf{\textsc{CCE} offers a deeper and wider evaluation than Vanilla}.



\begin{table}[!ht]
  \centering
  \scriptsize
  \resizebox{0.46\textwidth}{!}{
    \begin{tabular}{p{26.5em}}
    \toprule
    \textbf{Instruction}: Classify this tweet from the categories of positive, negative, or neutral. Tweet: I'm so thankful for all the wonderful people in my life \\
    \midrule
    \textbf{\red{Response A}} \\
        \ \ \ \ The sentiment of this social media comment is positive. The use of the word "grateful" implies appreciation and thankfulness, while "amazing experiences" suggest that these experiences have been very fulfilling or enjoyable. \\
    \textbf{\limegreen{Response B}} \\
            \ \ \ \ Positive\\
    \midrule
    \textbf{Vanilla LLM-as-a-Judge}:\\

	\ \ \ \ Assistant A's . . . . . . \redback{It highlights specific words and phrases in the tweet, such as ``thankful'' and ``wonderful people,'' to justify the positive sentiment classification. This explanation adds depth and clarity to the response, making it more helpful and informative.}

    Assistant B, . . . . . .  the response less informative and helpful . . . 

    \ \ \ \ . . . . . .

    \ \ \ \ - Final Verdict: \red{[[A]]}
    \\
    \midrule
    \textbf{\textsc{CCE}}:\\

    \ \ \ \ \textbf{\textit{AC Judgment}}: \uwave{However, Assistant A makes a mistake by referencing words not present in the tweet, such as "grateful" and "amazing experiences,"} . . . . . . Assistant C also classifies the tweet as positive and provides a detailed explanation . . .

    \ \ \ \

    \ \ \ \ . . . . . . Assistant A . . . . .\greenback{, but it inaccurately references words not present in the tweet, such as "grateful" and "amazing experiences." This detracts from the accuracy of the response and could potentially confuse the user.} . . . . . .
    
    \ \ \ \ Assistant B \greenback{is concise and correctly classifies the tweet as positive. However, it lacks any explanation or reasoning, which limits its helpfulness and depth.} . . . . . .

    \ \ \ \ In comparing the two, \greenback{Given the importance of accuracy and explanation in sentiment analysis,} . . . . . .

    \ \ \ \ - Final Verdict: \green{[[B]]}
    \\
    \bottomrule
    \end{tabular}%
    }
  \caption{A pairwise comparison case evaluated by different methods. \limegreen{Preference} refers to right result and \red{Preference} refers to wrong result. We emphasize the noisy evaluation elements in \redback{orange}, while highlighting the useful elements of the evaluation in \greenback{limongreen}.}
  \label{tab:case-evaluation-simple}%
\vspace{-.5em}
\end{table}%




\paragraph{Case Study.} Table~\ref{tab:case-evaluation-simple} presents a representative case. The vanilla is misled by fake information in Response A, causing it to overlook the Instruction and mistakenly rate Response A as more helpful. In contrast, the crowd judgment correctly identifies the error in Response A and informs subsequent evaluations. Additionally, our method produces a more detailed CoT thereby enriching the overall evaluation process, as evidenced by statements like ``Assistant A does provide a brief explanation''.







