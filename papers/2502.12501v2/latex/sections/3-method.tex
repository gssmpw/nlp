\begin{figure*}[!t]
  \includegraphics[width=\linewidth]{latex/figure/figure2_main.pdf}
  \caption {\textbf{Pipeline of our proposed crowd-based comparative evaluation.} For a given instance $(x, y^A, y^B)$, we first use the LLM to generate crowd responses $\left\{y^i|i\in\{C,D,E,...\}\right\}$ based on $x$. These responses are then compared with $y^A$ and $y^B$ to produce initial crowd judgments $\mathcal{J}$, which are subsequently refined into $\hat{\mathcal{J}}$ after selection and processing. Finally, $\hat{\mathcal{J}}$ are used as contextual input to evaluate the instance $(x, y^A, y^B)$.} 
  \label{fig:pipeline}
\end{figure*}

\section{Methodology}
\label{sec:method}


As illustrated in Figure~\ref{fig:pipeline}, we propose a crowd-based comparative evaluation that elicits and integrates multiple crowd judgments before producing a final outcome.
It consists of three core components: (1) Crowd Response and Judgment Generation, (2) Crowd Judgment Selection and Processing, and (3) Context-augmented Inference,  which we will discuss in the following subsections. Furthermore, we distill the CoT judgments generated by \textsc{CCE} to train a judge and expand its application to an enhanced rejection sampling technique for SFT.

\subsection{Problem Formulation}
\label{subsec:problem}
Supposing $\{y^A, y^B\}$ denote two candidate responses generated by two assistants for a given task instruction $x$, Vanilla LLM-as-a-Judge $\mathcal{F}$ is prompted to provide a CoT-based judgment $j$ of $y^A$ and $y^B$, based on a specific set of evaluation criteria $s$ 
(\eg, \textit{correctness}, \textit{coherence}).
\begin{equation}
    j = \mathcal{F}(y^A, y^B|x,s).
\end{equation}
The objective is to ensure that the $\mathcal{F}$ preference aligns closely with human evaluation. In pairwise comparisons, this alignment is quantified by measuring the accuracy relative to human labels.

\subsection{Crowd Response and Judgment Generation}
\label{subsec:crowd}

Based on the task instruction $x$, we first prompt the LLM to generate a set of $n$ synthetic crowd responses $\left\{y^i|i\in\{C,D,E,...\}\right\}$.
To enhance the diversity of these responses, we can leverage multiple LLMs ranging from smaller models (\eg, \textit{Qwen2.5-0.5B-Instruct}) to larger ones (\eg, \textit{Mistral-Nemo-Instruct-2407}), along with varying temperature settings.
Theoretically, more diverse responses can cover a wider range of scenarios. When compared with $y^A$ and $y^B$, these crowd responses emphasize different details of $\{y^A, y^B\}$, offering a more comprehensive perspective and facilitating deeper reasoning. As Figure~\ref{fig:pipeline} demonstrated, crowd judgment digs the importance of ``he'', where Response A subtly shifts the actor ``he'' onto the object ``task'' itself, thereby violating the instruction's requirement to rewrite while preserving the concise original meaning.
Then, we use it as context to reinforce the following CoT. This advantage surpasses that of criteria expansion, which cannot anticipate such details through pre-prompting.

For each synthetic $y^i$, $\mathcal{F}$ independently produces two crowd judgments, $j^A_i$ and $j^B_i$, by individually judging $y^i$ with $y^A$ and $y^B$, separately:
\begin{equation}
    j^A_i = \mathcal{F}(y^A, y^i|x,s), \quad j^B_i = \mathcal{F}(y^B, y^i|x,s).
\end{equation}
Formally, we collect a set of $2n$ crowd judgments:
\begin{equation}
    \mathcal{J} = \left\{ j^A_i, j^B_i \mid i\in\{C,D,E,...\}\right\}.
\end{equation}
While each judgment may not fully capture all details of the candidate responses, they together provide a richer pool of evidence about how $y^A$ and $y^B$ differ in nuanced ways.


\subsection{Crowd Judgment Selection and Processing}
\label{subsec:selection}
After obtaining $\mathcal{J}$, the key stage lies in selecting and processing these judgments effectively. 
Random Selection is neither stable nor optimal, so we need better strategies for using crowd judgments.

To this end, we propose a simple yet effective method called \textbf{Criticizing Selection}. Specifically, we choose judgments based on their outcomes: for $j_i^A$, we keep those where A loses, and for $j_i^B$, those where B loses. Notably, our observation reveals judgments with a critical outcome tend to provide detailed and informative reasoning for the criticized response. For instance, Judge might point out how the criticized response confuses key concepts by elaborating on specific errors in the definition and citing relevant theoretical principles. In contrast, judgments favoring the winning response tend to be brief, where the Judge might simply say, ``this answer is correct'' without further analysis. We also explore two alternative outcome-based strategies: \textbf{Praising Selection} (choosing only judgments where A/B wins) and \textbf{Balanced Selection} (maintaining an equal split between A/B wins and losses). However, as shown in our analysis (Table~\ref{tab:ablation_selection}), both strategies perform worse than Criticizing Selection. 
Additionally, to mitigate bias from the outcome distribution from crowd judgments, we introduce \textbf{Outcome Removal}, where an LLM rewrites $j_i$ to remove explicit outcome segments, ensuring a more neutral evaluation.
After the selection and processing, we obtain $\hat{{\mathcal{J}}}$. 

Notably, $j_i$ includes CoT judgments not only of the ($y^A$, $y^B$) but also of $y^i$. Our pilot study shows that removing the CoT segments about $y^i$ does not improve performance; therefore, we retain them to keep our approach simple.


\subsection{Context-augmented Inference}
\label{subsec:inference}

The final judgment is derived by evaluating responses $y^A$ and $y^B$ conditioned on the instruction $x$, the criteria $s$, and the post-processed crowd judgments $\hat{\mathcal{J}}$:
\begin{equation}
    j^\star = \mathcal{F}(y^A, y^B \mid x, s, \hat{\mathcal{J}}),
\end{equation}
where the prompt template is provided in Appendix~\ref{sec:appendix_prompt}. Notably, we distill $\{j^\star\}$ for training a smaller judge, whose performance surpasses the judge distilled from $\{j\}$, as demonstrated in Table~\ref{tab:main_distill}. It proves that higher-quality CoT judgment has better distillation efficiency.


\subsection{Extensive Application--Crowd Rejection Sampling in SFT}
\label{subsec:potential}
This subsection demonstrates the practicality of \textsc{CCE} by showcasing its extensive application in SFT.
Rejection sampling has been proven an effective augmentation technique for SFT~\citep{yuan2023scaling,zhu2023solving}. In a typical rejection sampling framework, given the task instruction and $k$ generated responses, low-quality responses are filtered out, and the remaining high-quality ones are then used for fine-tuning.
Traditionally, the Vanilla LLM-as-a-Judge selects the best response by comparing responses in pairs and choosing the one that wins most often. 
In contrast, \textsc{CCE} naturally adapts to the scenario that rejection sampling involves more than two responses, and we refer to it as \textit{crowd rejection sampling}. During pairwise comparing any two candidate responses, we effectively utilize the additional $k-2$ responses as crowd responses as introduced in Subsection~\ref{subsec:crowd}. After producing crowd judgments, it ensures a more detailed and consistent judgment.
We validate the crowd rejection sampling in our subsequent experiment (in Table~\ref{tab:main_sft}), where the integration of crowd responses consistently leads to more reliable and interpretable sampling, ultimately improving the overall performance of the fine-tuned model.