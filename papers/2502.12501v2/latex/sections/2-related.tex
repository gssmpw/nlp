\section{Related Work}
\label{sec:related}
Human evaluation is typically regarded as the gold standard for evaluating LLM responses to intricate and open-ended instructions~\cite{chiang2023human, elangovan2024human}.
Nevertheless, due to its inherent limitations—being time-consuming, costly, and prone to variability~\cite{karpinska2021the}—automated evaluation methods leveraging LLMs have gained prominence as scalable and cost-efficient alternatives.
Unlike reward models that provide only scalar scores~\cite{wang2024direct,wang2024selftaughtevaluators}, LLM-as-a-Judge frameworks offer enhanced robustness and interpretability by producing detailed CoT rationales~\cite{li2024leveraginglargelanguagemodels,gao2024llmbasednlgevaluationcurrent}.

Enhancing the performance of LLM-as-a-Judge has attracted significant attention, with many techniques proposed recently.
One prominent approach involves fine-tuning pre-trained LLMs on task-specific datasets to better adapt them for judgment tasks~\cite{vu2024foundational, li2024generative, wang2024pandalm, kim2024prometheus2opensource}.
Another line of research focuses on step-by-step methodologies, such as G-EVAL~\cite{liu2023geval}, ICE-Score~\cite{zhuo2024icescore}, and EvalPlanner~\cite{saha2025learningplanreason}, which decompose complex evaluation tasks into granular components, thereby harnessing the reasoning capabilities of LLMs to streamline the evaluation process.
Additionally, recent advances explore using LLMs to generate reasoning traces by designing domain-specific prompts and meticulously crafting components of CoT reasoning.
These include constructing fine-grained scoring rubrics~\cite{zheng2023mtbench, zeng2024llmbar, trivedi2024self} and generating reference answers~\cite{zhang2025reviseval}.
Despite these efforts, the richness and comprehensiveness of CoT reasoning remain underexplored, leaving room for further advancements in improving LLM-as-a-Judge.
While simple heuristics such as majority voting~\cite{badshah2024vote, verga2024vote} can mitigate this issue by improving the reliability and accuracy of evaluations, they often fall short in terms of efficacy and efficiency.