
\subsection{Data scaling laws}
Next, we investigate the influence of Recursive INference Scaling (RINS) on the data scaling behavior of language models. Specifically, we fit a power law of the form $\varepsilon(x)=\beta x^{-c}+\varepsilon_\infty$, to the log-perplexity loss $\varepsilon(x)$ as a function of the training FLOPs $x$. This allows us to analyze the impact of RINS on both the scaling exponent $c$ and the asymptotic performance limit $\varepsilon_\infty$, revealing whether the performance gains in RINS stem from an improved scaling exponent, a lower asymptotic limit, or a combination of both. We use the 600M-parameter language models in Figure~\ref{fig:llm_long_dur} whose signature is \A$^r$\B, for $r\in\{1, 2, 3, 4\}$, where $r=1$ corresponds to the non-recursive baseline. As shown in Figure~\ref{fig:scaling}, RINS improves \emph{both} the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/scaling/scaling_c4_c.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/scaling/scaling_slimpajama_c.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/scaling/scaling_c4_e.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/scaling/scaling_slimpajama_e.pdf}
    \caption{{\sc top:} The scaling exponent $c$ in models with signature \A$^r$\B, where $r=1$ corresponds to the non-recursive baseline. RINS ($r > 1$) improves scaling exponents. {\sc bottom:} Asymptotic log-perplexity  $\varepsilon_\infty$ is plotted against $r$. RINS improves $\varepsilon_\infty$, indicating that overtraining the baseline cannot match its performance gains.}
    \label{fig:scaling}
\end{figure}

\subsection{Optimal Number of Recursion Rounds}
We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law $f_{r}(x) = \beta_r x^{-c_r}+\varepsilon_r$, then $c_r$ increases with $r$ while $\varepsilon_r$ decreases.  Furthermore, the coefficient $\beta_r$ also increases with $r$. This implies that while scaling inference by increasing $r$ might initially exhibit a higher loss due to the larger $\beta_r$, its faster convergence ($c_r$) and lower asymptotic limit ($\varepsilon_r$) will eventually lead to superior performance. In other words, using the higher recursion level is advantageous eventually, which is consistent with the experimental results.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/opt_r.pdf}
    \caption{Optimal number of recursions  $r$ in RINS is plotted vs. training compute for 300M-, 600M-, and 1B-parameter LMs.}
    \label{fig:optr}
\end{figure}

To observe this more explicitly, we train language models with four signatures \A$^r$\B: $r\in\{1,2,3,4\}$. Then, we plot the optimal value of $r$ against training compute. As shown from the results in Figure~\ref{fig:optr}, the optimal value of $r$ monotonically increases with training compute, in agreement with earlier results. Also, \emph{smaller} models benefit \emph{more} from RINS.%, in agreement with~\cite{liu2024mobilellm}.
