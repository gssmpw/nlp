In this section, we study the impact of various parameter-sharing strategies in language modeling, following the taxonomy introduced in Section~\ref{sect:pre}. We show how RINS emerges as a clear winner. All experiments are carried out using the Big Vision codebase~\cite{beyer2022betterplainvitbaselines}.

\begin{table}[t]
    \centering\scriptsize
    \begin{tabularx}{\columnwidth}{@{}l|YYY@{}}
    \toprule
    \bf Metric &\multicolumn{3}{c}{\bf Architecture}\\
    &BL & RAO & RINS\\ \midrule 
    Winogrande &\bf53.3\% &53.0\% & \bf53.3\%\\
OpenBookQA &37.7\% & 38.3\%&\bf38.9\% \\
BoolQ &47.7\% &41.6\% &\bf52.5\% \\
PIQA &54.5\% &68.6\% &\bf69.5\% \\
SIQA &39.4\% &40.2\% &\bf41.3\% \\
 \bottomrule
    \end{tabularx}
    \caption{Zero-shot evaluation in downstream common sense reasoning tasks. All models are 600M parameters, pretrained on the compute-equivalent of 500B tokens in the baseline (BL). The best signatures for RAO and RINS in Figure~\ref{fig:llm_long_dur} are used in this evaluation. See Appendix~\ref{app:downstream} for details on the evaluation setup.}
    \label{tab:downstream}
\end{table}


\paragraph{Setup.}First, we train a decoder-only transformer language model~\cite{vaswani2017attention} with relative position embeddings and sequence packing. We use C4/English tokenizer with a vocabulary size of 32K. The model is trained on a mixture of C4~\cite{t5} and SlimPajama~\cite{cerebras2023slimpajama} with equal weight using a batch size 1,024 and context length 1,024. The non-recursive baseline is trained for 200K steps, which amounts to 200B training tokens. Other recursive models are trained on \emph{fewer} tokens in order to \emph{match} the same total compute FLOPs.

The optimizer is Adam~\cite{kingma2014adam} with learning rate $5\times 10^{-4}$ and weight decay  $5\times 10^{-5}$, using an inverse square root decay schedule with 5K warm-up and cool-down steps. This learning rate was chosen by sweeping across the values $\mathrm{lr}\in\{10^{-3},5\times 10^{-4}, 10^{-4}, 5\times 10^{-5}\}$ with $\mathrm{wd}=\mathrm{lr}/10$. The reported value yielded the best performance for the non-recursive baseline. The full training configuration along with a list of all the architectures we sweep across (i.e. signature and degree) can be found in Appendix~\ref{sect:app_config}. Each model is trained on $16\times16$ TPUv5 chips for approximately 2.2K core-hours. Models that failed OOM were excluded. Overall, we train 59 models of two sizes: 300M and 600M parameters. In all models, the embedding dimension is 1,536 and the MLP dimension is 6,144. Later, we present results using models with 1 billion parameters.

\paragraph{Results.}Figure~\ref{fig:llm_sweep} shows that RINS outperforms all other approaches.  Importantly, the optimal number of recursion rounds in RINS (e.g. whether to use \A$^2$\B\ or \A$^3$\B\ or more recursions) depends on the allocated training compute budget. This is seen in Figure~\ref{fig:llm_sweep} in the fact that the non-recursive baseline \A\B\ (a special case of RINS with $r=1$) initially outperforms all other models, before its performance saturates and recursive models begin to outperform it for the same training compute FLOPs and parameter count. We study the relation between the optimal number of recursion rounds and compute later in Section~\ref{sect:analysis}. In addition, RINS outperforms scaling inference time by only increasing the sequence (context) length (green vs. red curves in Figure~\ref{fig:llm_sweep}). 

\paragraph{Longer Training Duration.}One observation in Figure~\ref{fig:llm_sweep} is that the performance gap seems to widen in favor of RINS as more training compute is allocated. To verify this, we resume training the  600M-parameter models on 500B training tokens. Here, we only compare RINS with the non-recursive baseline and RAO, since RAO was identified in prior works to outperform other parameter-sharing strategies~\cite{liu2024mobilellm}.  Figure~\ref{fig:llm_long_dur} shows that the performance gap continues to increase in favor of RINS. This improvement also persists \emph{downstream}, as shown in Table~\ref{tab:downstream}.



