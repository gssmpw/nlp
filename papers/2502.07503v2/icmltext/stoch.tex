\begin{figure}
    \centering

\begin{lstlisting}[language=Python, style=mystyle]
class RecursiveBlock():
  config: Dict  # single block config
  signature: str = "a"
  degree: int = 1
  p_skip: Tuple[float, ...]]  # skip prob for blocks

  def __call__(self, x):
    """Call model on input x."""
    if degree == 1:
      blocks = {c: SingleBlock(config=self.config
      ) for c in set(self.signature)}
    else:
      blocks = {c: RecursiveBlock(
        config=self.config, signature=self.signature,
        degree=self.degree - 1, p_skip=self.p_skip
      ) for c in set(self.signature)}
    inputs = x
    # forward pass
    for i in range(len(self.signature)):
      c = self.signature[i]
      choice = random.uniform()
      if self.degree == 1:  # stochastic RINS
        if choice > self.p_skip[i]:
            x = blocks[c](x)  # else skip
      else:
        x = blocks[c](x)
    return x
\end{lstlisting}
    \caption{Numpy-like syntax for models with a fixed signature and degree. When no stochastic depth is applied, we have $p_{s} = 0$. In RINS, we expand $p_{s}$ into a tuple of the form $(0, p_s, p_s, \ldots, p_s, 0)$, where the first and last entries are zero to guarantee they are executed, which is equivalent to sampling the number of recursion rounds from a binomial distribution as described in Section~\ref{sect:stoch}.}
    \label{fig:ris_algorithm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/ris_lm/llm_long_dur_c4.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/ris_lm/llm_long_dur_sp.pdf}
    \caption{
Performance of RINS (\A$^2$\B, \A$^3$\B, \A$^4$\B) vs. RAO (\A$^2$, \A$^3$, \A$^4$) with increasing compute on 600M-parameter models. All models have the same size. The performance advantage of RINS grows with the computational budget. The long-sequence baseline is not shown since it underperforms other methods in Figure~\ref{fig:llm_sweep}.}
    \label{fig:llm_long_dur}
\end{figure}

Next, we investigate the effect of stochastically dropping blocks during training, inspired by the regularization technique of stochastic depth~\cite{huang2016deepnetworksstochasticdepth}. Our primary goal is to determine whether this approach can further enhance the performance of RINS while simultaneously offering the flexibility of \emph{reverting} to non-recursive inference without significant degradation in model quality.

To recall, RINS has the signature \A$^r$\B\ for some $r>1$. To implement stochastic RINS, we introduce a skip probability $p_s\in[0, 1)$ and sample during training the number of recursion rounds in each step to be $1+\eta$, where $\eta$ is a binomial random variable with probability of success $1-p_s$ and number of trials $r-1$. Thus, block \A\ is always executed at least once. During inference, we are free to choose how to scale inference compute by setting the value of $r\ge 1$. See the detailed pseudocode in Figure~\ref{fig:ris_algorithm}.


Here, we train bigger models with 1 billion parameters. We use an embedding dimension 2,048 and MLP dimension 8,192. All models have 18 decoder blocks. We train for 500K steps and compare RINS with signature \A$^3$\B\ against the non-recursive baseline.

Figure~\ref{fig:stoch_lm} summarizes the advantages of stochastic RINS. Notably, we observe that as $p_s>0$ increases, stochastic RINS mitigates the performance degradation incurred when scaling is not applied at inference time, while still offering big gains when inference time is scaled. Not surprisingly, though, scaling inference time is less effective when $p_s$ increases, suggesting a tradeoff between flexibility at inference time and the magnitude of potential gains from scaling. As shown in Figure~\ref{fig:infinite_lim}, similar conclusions hold even in the asymptotic (infinite-compute) regime, under the assumption that loss follows a power law relation~\cite{kaplan2020scaling}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/stoch/llm_stochdepth_c4_0.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/stoch/llm_stochdepth_sp_0.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/stoch/llm_stochdepth_c4_1.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/stoch/llm_stochdepth_sp_1.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/stoch/llm_stochdepth_c4_2.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/stoch/llm_stochdepth_sp_2.pdf}
    \caption{Performance of stochastic RINS (\A$^3$\B) with varying inference costs for 1B parameter LMs. The $x$-axis represents the training compute cost. The legend indicates the inference cost of each stochastic RINS configuration relative to the baseline; e.g. $1.5x$ denotes 50\% increase in inference cost. For $p_s=0$, RINS@1x is significantly worse, with perplexity scores $>3$. As expected,  RINS converges in performance to the baseline as $p_s\to 1$.}
    \label{fig:stoch_lm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/stoch/infinite_lim_c4_stoch.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/stoch/infinite_lim_sp_stoch.pdf}
    \caption{Asymptotic performance of 1B-parameter LMs, evaluated in C4 (left) and SlimPajama (right). Dotted line is for baseline. We observe a tradeoff between inference flexibility (gap between 1x \& 2x) and magnitude of gain from inference scaling (2x result).}
    \label{fig:infinite_lim}
\end{figure}