\subsection{Vision}\label{sect:vision}
As previously discussed, the performance gains in RINS are consistent with the self-similar nature of language. By performing a recursive, scale-invariant decoding, RINS introduces an inductive bias that encourages the model to recognize and exploit recurring patterns at different scales (see Appendix~\ref{app:selfsim} for further discussion). To test if this is likely the source of its advantage, we conduct a similar empirical evaluation in vision, a domain lacking self-similarity.

\textbf{Setup.} We train an encoder-only vision transformer ViT-B/16~\cite{dosovitskiy2021imageworth16x16words} on ImageNet-ILSRCV2012~\cite{deng2009imagenet}. The non-recursive baseline model is trained for either 300 or 1,000 epochs using a batch size 1,024, while recursive models are trained on fewer epochs to match the same total training compute FLOPs. We apply MixUp (probability 0.2) during training~\cite{zhang2018mixupempiricalriskminimization} and use learned position embedding. The optimizer is Adam where we tune the learning rate for each architecture in the set $\mathrm{lr}\in\{10^{-3}, 7\times10^{-4}, 3\times10^{-4}, 10^{-4}\}$ with weight decay $\mathrm{wd}=\mathrm{lr}/10$, on a small validation split. We use a cosine learning rate schedule with 10K warm-up steps. Images are resized to $224\times224$ and $16\times16$ patch sizes are used. The full training configuration is in Appendix~\ref{sect:app_config}.

\textbf{Results.} As presented in Table~\ref{tab:vision}, parameter-sharing techniques, including RINS, do not confer any advantage in supervised image classification. The non-recursive baseline, when trained on longer sequence lengths (i.e., higher image resolution) to match the inference cost of recursive architectures, surpasses all other methods. This starkly differs from the results observed in language modeling, where RINS provides significant gains even when compared against models that scale inference by increasing the sequence length.  

\begin{table}[t]
    \centering\scriptsize
    \begin{tabularx}{\columnwidth}{@{}l|YYY|Y@{}}
    \toprule
    \bf Architecture &\bf Val &\bf ReaL &\bf v2 &\bf Avg \\
    \midrule
    \multicolumn{5}{c}{300 epochs}\\ \midrule
    (\A\B\B)$_2$ &\bf75.2 &\bf 81.4 &\bf 62.7 &\bf 73.1\\
    \A@336 &\bf75.7 &\bf81.0 &\bf62.3 &\bf73.0 \\
    \A\A\B & 75.2 & 80.5 & 61.4 & 72.4\\
    \A\B\B\C & 75.1 & 80.2 & 61.3 & 72.2 \\
    \A@224 & 74.9 & 80.1 & 61.3 & 72.1 \\
    \midrule
    \multicolumn{5}{c}{1,000 epochs}\\ \midrule
    \A@336	&\bf77.6 &\bf82.7 &\bf64.6 & \bf75.0 \\
    \A\B\B\C	&76.3	&81.6	&63.2	&73.7 \\
    \A\A\B\C	&76.0	&81.4	&62.4	&73.2 \\
    \A\A	&75.8	&81.4	&62.4	&73.3 \\
    \A\A\A	&75.7	&80.8	&62.0	&72.8 \\
 \bottomrule
    \end{tabularx}
    \caption{Performance of the top 5 architectures on ILSRCV2012 classification. The table compares the performance of recursive architectures with a baseline trained 224- and 336-resolution images. The baseline \A@336, trained on higher-resolution to match the inference cost of the recursive models, outperforms all parameter-sharing architectures. We use the notation $(\mathrm{signature})_\mathrm{degree}$.}
    \label{tab:vision}
\end{table}

\subsection{Contrastive Mutlimodal Systems}\label{sect:siglip}
\paragraph{Setup.}
Finally, we also study the impact of RINS in vision-language pretraining, motivated by the fact that such models also process natural language. We pretrain SigLIP-B/16 models~\cite{zhai2023sigmoidlosslanguageimage}, which are contrastive models trained using the sigmoid loss on English-language imageâ€“text pairs. We  follow~\cite{zhai2023sigmoidlosslanguageimage} in most aspects. Images are resized to $256\times256$ and we use $16\times16$ patch sizes.  Texts, on the other hand, are tokenized using C4 tokenizer~\cite{t5} with a vocabulary size of 32K, and we keep a maximum
of 64 tokens. The optimizer is Adam with learning rate $10^{-3}$ and weight decay $10^{-4}$, using an inverse square root decay schedule with 5K warm-up and cool-down steps. For the baseline, we use SigLIP-B/16 pretrained on 10B training examples. Again, recursive models are trained on fewer examples to match the total training compute cost. Due to the amount of compute involved in these experiments, we only compare the non-recursive baseline (with signature \A\B) against RAO (with signature \A\B\A\B) and RINS (signature \A$^2$\B) with degree 1.

\begin{table}[t]
    \centering\scriptsize
    \begin{tabularx}{\columnwidth}{@{}l|YYYY@{}}
    \toprule
    \bf Metric &\multicolumn{4}{c}{\bf Architecture}\\
    &\A\B &\ttfamily Long-Seq & \A\B\A\B & \A\A\B\\ \midrule 
    \multicolumn{5}{c}{\em Zero-shot classification}\\[2pt]
    ImageNet & 73.4 &
    73.0 
    & 72.7 & \bf 74.1 \\
CIAFR100 & 68.9 &
63.6 
& 65.3 & \bf 72.2 \\
Pet & 90.4 &
90.0
& \bf 90.7 & 90.0 \\

    \multicolumn{5}{c}{\em Retrieval}\\[2pt]
COCO img2txt@1 & \bf 62.7 & 
62.0
& 61.1 & 62.3 \\
COCO img2txt@5 & \bf 84.8 & 
84.1
& 82.9 & 84.1 \\
COCO img2txt@10 & \bf 90.7 &
90.4
& 89.5 & 90.2 \\[2pt]
COCO txt2img@1 & 44.6 &
44.2
& 43.2 & \bf 45.1 \\
COCO txt2img@5 & 69.6 &
69.4 
& 68.4 & \bf 70.0 \\
COCO txt2img@10 & 78.8 &
78.7
& 77.5 & \bf 79.0 \\[2pt]
Flickr img2txt@1 & \bf 89.6 &
87.2
& 87.9 & 88.9 \\
Flickr img2txt@5 & 98.0 &
97.8
& 97.5 & \bf 98.5 \\
Flickr img2txt@10 & 99.1 &
98.9
& 98.9 & \bf 99.3 \\[2pt]
Flickr txt2img@1 & \bf 75.1 &
73.7
& 73.2 & 74.3 \\
Flickr txt2img@5 & 92.3 &
92.1
& 91.5 & \bf92.4 \\
Flickr txt2img@10 & 95.6 &
95.6
& 95.6 & \bf95.8 \\
 \bottomrule
    \end{tabularx}
    \caption{Performance of SigLIP-B/16 on various datasets.  Results are shown for ImageNet~\cite{deng2009imagenet}, CIFAR100~\cite{Krizhevsky09learningmultiple}, Pet~\cite{parkhi12a}, COCO~\cite{chen2015microsoft}, and Flickr~\cite{young-etal-2014-image}. All models are identical in size to SigLIP-B/16 and have the same training compute FLOPs. Long-Seq is SigLIP-B/16 trained on higher resolution of 280 and text length 80 (25\% increase in sequence length $\rightarrow$ 50\% increase in inference cost compared to baseline, similar to \A\A\B). Using Wilcoxon signed rank test~\cite{wilcoxon1992individual}, we obtain $p=0.003$ so the evidence in favor of RINS is statistically significant at the 99\% confidence level.}
    \label{tab:siglip}
\end{table}


\textbf{Results.} Table~\ref{tab:siglip} shows that RINS (with signature \A$^2$\B) outperforms the non-recursive baseline, including with long sequence length, and RAO in zero-shot and retrieval evaluations. Of importance is the impact in ImageNet 0-shot classification, where we see an improvement of about $+0.7\%$. 

\textbf{Overtraining.} Next, we demonstrate that RINS can substantially advance state-of-the-art results in multimodal systems for a given  model size. We train a recursive variant of SigLIP-B/16, denoted  SigLIP-RINS-B/16, using the signature \A$^3$\B. In light of the findings in Section~\ref{sect:analysis}, we increase the number of recursions here given the  increase in compute.

We adhere to the training protocol outlined above, with the exception that SigLIP-RINS-B/16 is now trained on 40B examples, matching the training data scale of the widely-used, publicly available SigLIP-B/16 checkpoint. Note that both models have an identical size. Moreover, following~\citet{pouget2024no}, we utilize a training mixture comprising both English and multilingual data to enhance cultural diversity, and report the cultural metrics recommended in~\citet{pouget2024no} as well as multilinguality evaluations using XM3600 dataset~\cite{thapliyal2022crossmodal}. So, to ensure an appropriate comparison of results, we re-train SigLIP-B/16 on 40B examples from the same data mixture. The primary datasets for cultural diversity evaluation are Dollar Street~\cite{rojas2022dollar}, GeoDE~\cite{ramaswamy2024geode}, and Google Landmarks Dataset v2 (GLDv2)~\cite{weyand2020google}. We use the Gemma tokenizer~\cite{gemmateam2024gemmaopenmodelsbased}.

As shown in Table~\ref{tab:long_siglip_results}, SigLIP-RINS-B/16 significantly outperforms the standard SigLIP-B/16 across all benchmarks. In fact, \emph{stochastic} RINS with skip probability $p_s=\frac{1}{4}$ improves results even further. Crucially, these results demonstrate that RINS offers a fundamental advantage in multimodal learning that are not replicated by simply overtraining a non-recursive counterpart.

\begin{table}[t]
    \centering\scriptsize
    \begin{tabularx}{\columnwidth}{@{}l|c|YYY@{}}
    \toprule
    \bf Metric &\textbf{SigLIP-B/16} &\multicolumn{3}{c}{\textbf{SigLIP-RINS-B/16}}\\
    &\textbf{(370M params)} &\multicolumn{3}{c}{\textbf{(370M params)}}\\
    & & $p_s=0$ & $\frac{1}{4}$ & $\frac{1}{2}$\\ \midrule
    \multicolumn{5}{c}{\em Zero-shot classification}\\[2pt]
ImageNet & 77.3 & 79.0 & \bf79.6 & \underline{79.2} \\
CIAFR100 & 70.3 & 78.5 & \bf80.7 & \underline{81.7} \\
Pet & 92.6 & \bf94.8 & \underline{94.4} & 92.7 \\[2pt]
    \multicolumn{5}{c}{\em Cultural Diversity}\\[2pt]
GeoLoc:Dollar Street & 17.6 & \bf19.7 & 19.2 & \underline{19.3} \\
GeoLoc:GeoDE-Country & 22.5 & 23.3 & \bf24.7 & \underline{23.9} \\
GeoLoc:GeoDE-Region & 36.1 & 37.7 & \bf39.7 & \underline{38.7} \\
Dollar Street & 51.5 & 52.9 & \bf53.1 & \bf53.1 \\
GeoDE & 93.1 & 93.7 & \bf94.3 & \underline{94.2} \\
GLDv2 & 51.6 & \bf53.7 & 52.4 & \underline{52.5} \\[2pt]
    \multicolumn{5}{c}{\em Multilinguality}\\[2pt]
XM3600 img2txt@1 & 48.4 & \bf53.5 & \underline{52.6} & 51.8 \\
XM3600 img2txt@5 & 68.4 & \bf72.6 & \underline{72.0} & 70.9 \\
XM3600 img2txt@10 & 74.4 & \bf78.1 & \underline{77.5} & 76.5 \\[2pt]

XM3600 txt2img@1 & 39.5 & \underline{43.0} & \bf43.1 & 41.9 \\
XM3600 txt2img@5 & 59.5 & \underline{63.0} & \bf63.1 & 61.6 \\
XM3600 txt2img@10 & 66.1 & \bf69.3 & \bf69.3 & 68.0 \\[2pt]
    \multicolumn{5}{c}{\em Retrieval}\\[2pt]
COCO img2txt@1 & 67.6 & 69.4 & \bf70.0 & \underline{69.5} \\
COCO img2txt@5 & 87.2 & \underline{88.6} & \bf88.9 & 88.3 \\
COCO img2txt@10 & 92.6 & \underline{93.2} & \bf93.5 & \underline{93.2} \\[2pt]
COCO txt2img@1 & 50.3 & 51.5 & \bf52.4 & \underline{52.0} \\
COCO txt2img@5 & 74.7 & 75.2 & \bf76.0 & \underline{75.7} \\
COCO txt2img@10 & 82.6 & 83.0 & \bf83.5 & \underline{83.2} \\[2pt]
Flickr img2txt@1 & 91.9 & \underline{92.9} & \bf93.5 & 92.4 \\
Flickr img2txt@5 & \underline{99.3} & 98.7 & \bf99.5 & 98.7 \\
Flickr img2txt@10 & \underline{99.6} & 99.5 & \bf99.7 & 99.3 \\[2pt]
Flickr txt2img@1 & 80.1 & \underline{80.7} & \bf81.4 & 80.5 \\
Flickr txt2img@5 & 94.6 & 94.3 & \bf95.4 & \underline{95.0} \\
Flickr txt2img@10 & \underline{97.1} & 97.0 & \bf97.4 & 97.0 \\
 \bottomrule
    \end{tabularx}
    \caption{
    Performance of multilingual SigLIP models on various datasets under an overtraining regime, where training compute is not a constraint. All models are identical in size to SigLIP-B/16. As shown in the rightmost columns, stochastic RINS ($p_s>0$) outperforms the other architectures. Detailed results on multilinguality are provided in Appendix~\ref{sect:app_multiling}.
    }
    \label{tab:long_siglip_results}
\end{table}

