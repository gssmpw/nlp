There has been a proliferation of research in recent years pointing to the pivotal role of scale, and how its benefits could be predicted empirically~\citep{mukherjee2003estimating,hestness2017deep,johnson-etal-2018-predicting,rosenfeld2019constructive,kaplan2020scaling,ghorbani2021scaling,alabdulmohsin2022revisiting,bansal2022data,zhai2106scaling}. Generally, the performance of deep neural networks $f(x)$ (such as its error rate or log-perplexity) often follows a power law $f(x)\sim \beta x^{-c}+\varepsilon$ as one varies a dimension of interest $x$, such as the size of the training data or model parameters. These ``scaling laws,'' as they are known in the literature, have been used, among others, to determine the training data size needed to achieve a specified level of accuracy~\citep{cho2015much,beleites2013sample,figueroa2012predicting} and to optimize the model architecture~\citep{kaplan2020scaling,hoffmann2022training,alabdulmohsin2024getting}. They have also been justified theoretically~\citep{bahri2021explaining,hutter2021learning,sharma2022scaling}.


\begin{figure*}[t]
    \includegraphics[width=\columnwidth]{figures/ris_arch.pdf}\hfill
    \includegraphics[width=\columnwidth]{figures/ris.pdf}
    \caption{{\sc left:} Illustration of Recursive INference Scaling (RINS) compared to a standard forward pass. In RINS, the model $f:\mathcal{X}\to\mathcal{Y}$ is split into two parts: the first block $f_A:\mathcal{X}\to\mathcal{X}$ is applied iteratively to its own output for $r$ iterations before passing the output to the second block. 
    {\sc right:} Illustrative examples of models with different  signatures and degrees. From top to bottom: (1) \emph{Baseline} (Signature: \A\B, Degree: 1), a standard feedforward architecture with no recursion. (2) \emph{repeat-all-over} (RA)~\citep{liu2024mobilellm}, where the entire model is recursively applied on its output. When recursion is done twice, it has a signature of \A\B\A\B. (3) RINS with signature \A$^3$\B. (4) (\A$^3$\B)$_2$ whose degree is 2, in which the same parameter sharing signature is applied on each of the two blocks \A\ and \B.}
    \label{fig:recursiontypes}
\end{figure*}

Besides this conventional approach of scaling the training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. Whereas OpenAI's GPT-o3 has prominently demonstrated this recently~\citep{openai_o_series}, earlier work had already explored this direction. For example, chain-of-thought (CoT) prompting has shown that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of language models~\citep{cot_paper}. This is similar to the demonstrated success of critiquing before evaluating~\cite{ankner2024critiqueoutloudrewardmodels}. Also, AlphaCode~\citep{alphacode_paper} and Codex~\cite{chen2021evaluatinglargelanguagemodels} generate multiple samples during inference in order to enhance code generation performance. Remarkably, ~\citet{brown2024largelanguagemonkeysscaling} shows that the benefit of sampling multiple solutions in tasks such as mathematics and coding---when measured by coverage---holds for up to four orders of magnitude of inference compute. These approaches suggest that, like training compute, inference compute follows systematic scaling patterns that can be leveraged to improve models. Refer to the survey~\cite{welleck2024from} for more details.  

Recently, it has also been noted that language exhibits a ``self-similar'' (fractal) nature, meaning that similar patterns repeat across different scales of its representation, from individual words to entire sentences and paragraphs~\citep{alabdulmohsin2024fractalpatternsilluminatesuccess}. Inspired by this finding, we examine if {model recursion}, which can be interpreted as a particular form of \emph{scale-invariant} decoding, offers a complementary approach for scaling inference time in language models.  To this end, we examine an extensive set of parameter-sharing strategies and, indeed, identify the best to be a special form of recursion, which we term \emph{Recursive INference Scaling} (RINS). We show that RINS yields significant performance gains compared to other parameter-sharing methods, even when controlling for model size and training compute.

RINS builds upon the concept of model recursion but recasts it as a powerful inference-time scaling strategy. It leverages a simple yet profound idea: \emph{use your existing architecture and training compute budget as is, but exploit the self-similar structure of language by recursively applying an early portion of your network to refine its output}. In turn, this simple strategy improves performance significantly, with the primary cost being a scaled inference time.

Recursion has shown promise in language modeling, with recent work by \citet{liu2024mobilellm} demonstrating that recursive architectures outperform similarly sized vanilla models trained on the same number of tokens. Notably, \citet{liu2024mobilellm} finds that a simple ``repeat-all-over'' (RAO) approach, where the entire model is recursively applied, surpasses more complex parameter-sharing strategies in  language. 

However, while \citet{liu2024mobilellm} demonstrate the \emph{sample efficiency} of recursive architectures, in which models are compared when trained on the same number of tokens, their analysis does not explicitly account for the increased computational cost of recursive operations during training.  Hence, it remains unclear whether the performance gains observed in prior work come from the inherent advantages of model recursion or simply from having increased the training compute.  Indeed, our findings suggest that for moderately sized models (over 1 billion parameters), the performance gains of the ``repeat-all-over'' (RAO) strategy can be matched by extending the training of a standard model to consume an equivalent compute. RINS, by contrast, significantly outperforms all other baselines on a compute- and parameter-matched setup, including when scaling inference by increasing the context length (see Figure~\ref{fig:llm_sweep}).

Crucially, a \emph{stochastic} variant of RINS, in which the number of recursion rounds is sampled at training time from a binomial distribution, not only can enhance performance further, such as in multimodal systems, but also provides the flexibility to optionally forgo increased inference computation at test time with minimal performance degradation. See Section~\ref{sect:stoch} for details.


\paragraph{Efficiency.}We conduct our experiments mostly on small models, which are typically intended for deployment environments with stringent memory limitations~\cite{liu2024mobilellm}.  Given the direct relation between a model's memory footprint and its parameter count (e.g. a 1 billion parameter model with 16-bit floating-point precision requires 2GB of DRAM), the ability to enhance accuracy while maintaining a fixed parameter count is highly desirable. RINS achieves this by enabling significant performance gains without increasing parameter count for the same training compute.  

\textbf{Statement of Contribution.} In summary, we introduce Recursive INference Scaling (RINS), a complementary plug-in method for scaling inference time. We:
\begin{enumerate}
\item propose a taxonomy of parameter-sharing architectures, empirically evaluating their effectiveness across multiple domains. Our comprehensive analysis identifies RINS as a powerful approach, outperforming methods like RAO used in Mobile LLM~\cite{liu2024mobilellm}, and scaling inference by increasing the sequence length.
\item argue that RINS' performance gain likely stems from the self-similar (fractal) nature of language, as a similar analysis in vision yields minimal improvements.
\item use RINS to extend inference scaling \emph{beyond language} to multimodal systems that incorporate language in their processing, such as contrastive models. In particular, our SigLIP-RINS-B/16 outperforms the popular SigLIP-B/16~\cite{zhai2023sigmoidlosslanguageimage} by a wide margin; e.g. improving 0-shot accuracy in ImageNet from 77.3\% to 79.6\% and CIFAR100 from 70.3\% to 80.7\%.
\item derive data scaling laws for RINS, revealing improvements in both the asymptotic performance limit and convergence speed (i.e. scaling exponent).
\item show that \emph{stochastic} RINS can enhance performance even further, such as in multimodal systems, while offering the option to revert to non-recursive inference at test time with minimal performance degradation.
\end{enumerate}

