\subsection{Overview}
Before describing how RINS works, we briefly formalize definitions. Let $\mathcal{X}$ be a fixed domain, often the space of sequences of soft tokens of embedding dimension $d$. Let $\mathbb{L} = \{l_1, l_2, ..., l_n\}$ be a fixed set of $n$ unique blocks, where each block  $l_k: \mathcal{X} \to \mathcal{X}$ is a function mapping from the input space $\mathcal{X}$ to the same output space $\mathcal{X}$. By ``unique'' here we simply imply that such blocks (which  typically comprise of multiple layers each) are not constrained to share the same parameters. Let $\mathcal{G}(\mathbb{L})$ be the space of all possible computation graphs representing neural network architectures that can be constructed by composing blocks from the set $\mathbb{L}$, while $f \in \mathcal{G}(\mathbb{L})$ be one specific architecture. 

Figure~\ref{fig:recursiontypes} (right) illustrates some examples for the case when $|\mathbb{L}|=2$. For instance, one can repeatedly apply the entire model, as in the ``repeat-all-over'' (RAO) approach in Mobile LLM~\cite{liu2024mobilellm}, or recursively apply a strict subset of the architecture, such as a single a block within the model. The choice of arrangement of blocks can significantly impact the model's performance and efficiency.

Formally, let $C(f)$ be the actual computational cost (in FLOPs) of training $f \in \mathcal{G}(\mathbb{L})$ on a dataset sampled i.i.d. from  distribution $\mathcal{D}$, considering only the forward pass. Also, $\mathcal{L}(f)$ is a performance metric of interest (e.g., validation loss or error rate) for model $f$. We assume that a lower value of $\mathcal{L}$ indicates better performance. 
\begin{definition}\label{def:rec}
For a fixed set of unique blocks $\mathbb{L}$ and a training compute budget $c$, a recursive architecture $f^\star \in \mathcal{G}(\mathbb{L})$ is considered ``better'' than another architecture $f \in \mathcal{G}(\mathbb{L})$ if  $C(f^\star)\le C(f)\le c$ and  $\mathbb{E}[\mathcal{L}(f^\star)] \le  \mathbb{E}[\mathcal{L}(f)]$.
\end{definition}
In other words, we search for the architecture $f^\star$, constructed only from the set of blocks $\mathbb{L}$, that minimizes the loss under the constraint of a bounded training compute $c$.

Model recursion offers a simple, plug-in approach for scaling inference time. By applying some layers iteratively to their own output, we effectively increase the computational path length during inference without altering the underlying model architecture. This allows us to exploit the benefits of increased inference compute. Importantly, it is \emph{complementary} to other techniques like chain-of-thought (CoT) prompting~\cite{cot_paper}, which achieve performance gains by eliciting longer generation sequences, and repeated sampling~\cite{alphacode_paper,chen2021evaluatinglargelanguagemodels}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/ris_lm/llm_model_comparison_c4_0.pdf}\hfill
    \includegraphics[width=0.5\columnwidth]{figures/ris_lm/llm_model_comparison_c4_1.pdf}\hfill
    \includegraphics[width=0.5\columnwidth]{figures/ris_lm/llm_model_comparison_sp_0.pdf}\hfill
    \includegraphics[width=0.5\columnwidth]{figures/ris_lm/llm_model_comparison_sp_1.pdf}\hfill\\
    \caption{Language models are trained on 200B tokens. The $x$-axis is the training cost in units of layer $\times$ step. Notably, the performance advantage of RINS increases with increased computational budget. The long-sequence baseline, using a context length of 1,536 tokens, exhibits lower performance due to processing fewer examples to maintain the same FLOPs count. See Figure~\ref{fig:llm_long_dur} for results with extended training duration and Figure~\ref{fig:stoch_lm} for results with larger (1B parameter) models, further demonstrating the benefits of RINS.}
    \label{fig:llm_sweep}
\end{figure*}

\paragraph{Taxonomy.}As discussed in Section~\ref{sect:intro}, language has been shown to exhibit self-similarity, meaning that similar patterns repeat across multiple levels of granularity, from the structure of individual sentences to the composition of entire texts~\cite{alabdulmohsin2024fractalpatternsilluminatesuccess}. This observation suggests that recursive (scale-invariant) decoding could be particularly beneficial for language processing. 

To determine the extent to which this is true, we systematically explore a vast space of parameter sharing strategies. We use the following taxonomy based on two key concepts: (1) \emph{signature} and (2) \emph{degree}. The ``signature'' of an architecture describes how parameter sharing is applied across different blocks of the model. For example, an architecture with the signature \A\B$^2$\C\ indicates that the model is partitioned into three unique blocks (\A, \B, and \C) of \emph{equal} size. The processing flow in this architecture would be: apply block \A\ on the input, apply block \B\ to \A's output, apply block \B\ again to its \emph{own} output (hence the exponent), and finally apply \C\ to the output of \B.  Note that while the signature \A\B$^2$\C\ has the same parameter count as a simpler \A\B\C\ architecture, it incurs approximately 33\% more compute due to the repeated application of block \B. This highlights the trade-off between computational cost and performance using recursion that we will carefully study in this work.

On the other hand, ``degree'' specifies whether recursion is applied only at the level of entire blocks, or if it extends to individual components \emph{within} blocks, as illustrated in Figure~\ref{fig:recursiontypes} (bottom). A degree of 2, for example, means that each of the blocks \A, \B, and \C\ in our example have themselves the same recursive pattern used in the overall model. This adds another dimension to our taxonomy, allowing for a finer-grained classification of recursive architectures. Note that degree makes notation concise, but is not necessary; e.g. (\A\B\B)$_2$ is equivalent to \A\B\B\,\C\D\D\,\C\D\D.  By systematically varying both signature and degree, we can comprehensively explore the space of recursive models and identify optimal configurations. Figure~\ref{fig:ris_algorithm} provides a detailed pseudocode.

With this framework, we now state our main investigation: 
\begin{quote}
    Which family of architectures (i.e. signatures and degrees) lead to better performance according to Definition~\ref{def:rec} under fixed compute budget?
\end{quote}
To reiterate, this is a non-trivial question because it is conceivable for a non-recursive model to outperform all others given that it sees more training examples, utilizing the  compute budget without resorting to recursion. For instance, increasing the context length in language modeling can be inferior to training on more examples within a limited compute budget. We present examples of this in Section~\ref{sect:app}.

Nevertheless, our analysis reveals that \emph{Recursive INference Scaling} (RINS) emerges as a clear winner. Its consistent superiority  suggests that it captures a fundamental principle for efficiently processing language. We hypothesize that this is due to the self-similar geometry of language. 

\begin{definition}
RINS corresponds to architectures with degree 1 and signature \A$^r$\B, for some integer $r>1$.
\end{definition}

In other words, RINS partitions a given model into two equally-sized blocks \A\ and \B. For example, \A\ would correspond to the first 6 layers while \B\ would correspond to the second 6 layers in a 12-layer neural network (ignoring the embedding layer and classifier head). The first block \A\ is recursively applied on its own output $r$ times before passing the output to \B. See Figure~\ref{fig:recursiontypes} for  illustration and Figure~\ref{fig:ris_algorithm} for a detailed pseudocode. In Section~\ref{sect:analysis}, we show how the optimal value of $r$ depends on the allocated  compute budget.

\paragraph{Main Claim.}Our claim can be summarized as follows. Once a language model is chosen and the training compute budget is planned, one should enable RINS, which does not change the model size, and train the new RINS-enabled model for the \emph{same} amount of compute. This is achieved by partitioning the original model into two equally-sized blocks and applying recursion to the first block alone. Thus, neither model size nor training compute is affected. Our empirical results demonstrate that RINS-enabled models will consistently outperform baseline models. In addition,  stochastic RINS with some positive skip probability $p_s>0$ (see Section~\ref{sect:stoch}) offers the option of forgoing scaling inference at test time with little performance degradation.