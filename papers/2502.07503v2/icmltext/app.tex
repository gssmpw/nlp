\section{Multilinguality Detailed Results}\label{sect:app_multiling}

\begin{table}[h]
    \centering\scriptsize
    \begin{tabularx}{\columnwidth}{@{}l|YYYY|YYYY@{}}
    \toprule
    \bf Language &\multicolumn{4}{c}{\textbf{Image-to-Text Retrieval @ 1}}
    &\multicolumn{4}{c}{\textbf{Text-to-Image Retrieval @ 1}}\\[5pt]
    & \bf SigLIP-B/16 & \multicolumn{3}{c}{\bf SigLIP-RINS-B/16}&
    \bf SigLIP-B/16 & \multicolumn{3}{c}{\bf SigLIP-RINS-B/16}\\[5pt]
    & &$p_s=0$ & $p_s=\frac{1}{4}$ & $p_s=\frac{1}{2}$ & &$p_s=0$ & $p_s=\frac{1}{4}$ & $p_s=\frac{1}{2}$\\[5pt] \midrule
ar&54.4&58.8&59.0&58.2&42.2&45.8&46.7&45.3\\
bn&7.0&11.2&10.3&8.7&4.8&7.2&6.8&5.1\\
cs&51.1&55.8&55.8&54.5&40.5&45.0&44.6&43.0\\
da&61.3&68.8&68.8&65.2&45.8&51.6&52.6&49.8\\
de&81.5&84.1&83.3&83.3&69.3&72.2&72.4&71.5\\
el&38.8&45.8&44.9&42.9&27.4&32.1&32.2&31.5\\
en&55.6&56.7&56.0&56.3&53.2&53.4&53.4&53.2\\
es&67.9&71.3&71.7&70.4&62.7&63.7&63.5&63.9\\
fa&54.4&61.3&58.6&59.6&48.1&51.6&51.6&51.9\\
fi&34.6&43.2&43.0&39.8&21.9&28.4&28.8&25.9\\
fil&19.0&21.8&20.5&19.4&10.4&12.5&12.3&11.3\\
fr&74.8&77.6&76.2&76.9&67.0&69.5&69.9&68.0\\
hi&20.6&26.5&25.9&23.5&10.5&14.6&14.4&12.3\\
hr&46.9&57.4&56.2&53.1&33.6&41.2&40.4&37.9\\
hu&47.6&55.2&53.9&53.9&37.4&42.7&43.1&41.2\\
id&74.1&77.8&77.7&78.4&65.3&68.0&68.2&67.0\\
it&73.9&79.1&77.5&77.3&67.3&70.4&70.2&69.9\\
iw&48.2&56.8&54.5&52.3&37.5&43.9&43.1&40.8\\
ja&48.8&57.8&57.9&53.9&38.1&43.2&42.0&39.6\\
ko&58.6&65.6&63.3&63.3&50.0&53.4&52.5&51.7\\
mi&0.7&0.7&0.8&0.6&0.2&0.4&0.3&0.3\\
nl&61.8&67.8&65.0&66.2&55.1&58.8&59.0&57.7\\
no&61.0&68.9&67.1&66.8&45.5&52.5&52.0&50.9\\
pl&62.3&68.9&67.4&67.1&54.0&57.6&59.0&57.1\\
pt&69.6&71.1&70.1&71.4&61.5&62.6&63.3&63.2\\
quz&7.2&7.7&7.9&7.7&3.1&3.0&3.2&2.9\\
ro&52.7&61.4&60.5&60.0&39.8&48.5&48.3&46.0\\
ru&66.7&70.8&69.5&70.3&61.3&64.2&64.3&64.2\\
sv&66.8&71.9&71.9&70.1&51.3&55.3&56.6&54.4\\
sw&9.2&10.2&10.0&9.4&4.6&5.2&5.4&4.4\\
te&1.1&1.4&1.6&1.0&0.5&0.5&0.7&0.5\\
th&29.5&38.2&35.1&34.5&21.5&24.7&23.1&24.0\\
tr&52.9&58.0&57.9&57.6&44.3&47.3&46.9&46.6\\
uk&52.2&58.1&56.1&56.6&38.7&44.3&44.9&42.8\\
vi&73.8&79.3&78.4&77.8&61.8&65.7&66.4&65.5\\
zh&55.3&60.2&60.5&58.7&44.9&48.3&49.8&47.1\\
 \bottomrule
    \end{tabularx}
    \caption{
    Per-language retrieval evaluations using the Crossmodal-3600 dataset~\cite{thapliyal2022crossmodal}.
    }
    \label{tab:xm3600_detailed}
\end{table}

\newpage

\section{Zero-shot Evaluation on Common Sense Reasoning Tasks}\label{app:downstream}
This section describes the evaluation setup for assessing the zero-shot common sense reasoning capabilities of the language model (Table~\ref{tab:downstream}).


Hi Ibrahim, I've read your code and here's a detailed description of the evaluation setup for a research paper:

Evaluation Setup for Zero-Shot Common Sense Reasoning

This section describes the evaluation setup for assessing the zero-shot common sense reasoning capabilities of the language model.

\paragraph{Datasets.} Each model is evaluated on four benchmarks:
\begin{itemize}
    \item \textbf{WinoGrande}: A multiple-choice dataset focusing on  commonsense knowledge~\cite{sakaguchi2019winogrande}.
    \item \textbf{OpenBookQA}: A multiple-choice question answering dataset requiring knowledge from an open book of elementary level science facts~\cite{mihaylov2018can}.
    \item \textbf{BoolQ}: A yes/no question answering dataset requiring the model to determine the truthfulness of a given statement based on a provided passage~\cite{clark2019boolq}.
    \item \textbf{PIQA}: A multiple-choice benchmark focusing on physics-related reasoning, such as how we interact with objects in daily life~\cite{Bisk2020}.
    \item \textbf{SIQA}: A multiple-choice dataset focusing on social common sense reasoning~\cite{sap2019socialiqacommonsensereasoningsocial}.
\end{itemize}

\paragraph{Prompting.} Each model is evaluated in a zero-shot setting, meaning it does not receive any training examples specific to the task. Instead, since all tasks above have multiple-choice answers, we evaluate the log-perplexity of each option, applying a causal mask, after concatenating the contextual information (if any), followed by the prefix and the answer. Then, we select the choice that has the lowest per-token log-perplexity score as the model's answer. 

We do not apply any prompting, except in PIQA where we formulate each sentence in the form \verb|The goal is: {goal} The solution is: {sol}|. We do this in PIQA because, otherwise, the sentences can be difficult to understand, even for humans. In one example, for instance, the goal is: "Deep clean coffee grinder." and  the two possible solutions are: "Scrape with rice." and "Scrape with flour." Concatenating directly would result in sentences like "Deep clean coffee grinder. Scrape with rice." whose are unclear.

\paragraph{Evaluation Metric.} 
The  evaluation metric is accuracy. For each example, the model's predictions are compared to the ground truth labels, and the accuracy is calculated as the percentage of correct predictions. The model is evaluated in a deterministic mode, meaning no randomness is involved in the inference process.


\newpage 

\section{Recursion and Self-Similarity}\label{app:selfsim}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/selfsim_explanation.pdf}
    \caption{Caption}
    \label{fig:selfsim_exp}
\end{figure}

How does depth-wise model recursion relate to self-similarity of language across the temporal dimension? We provide an illustration of this in Figure~\ref{fig:selfsim_exp}. 

Informally, one might view the early layers in a neural network to be transforming the raw linguistic input into a \emph{richer language} that is appended with useful, high-level details. For instance, in the sentence ``the evil queen appears ...'', early layers potentially through mechanisms like attention, might modify the token ``queen'' to incorporate features such as part-of-speech (POS) tagging, sentiment, and grammatical information. Subsequent layers can then iteratively incorporate progressively higher-level contextual information.

The concept of self-similarity in temporal sequences, including language, hints that higher-order linguistic features exhibit patterns analogous to those present in the raw input.  If this property holds for language, it implies that the \emph{same} neural network block can be recursively applied to generate increasingly abstract features.

However, while this interpretation offers a conceptual framework, it is essential to acknowledge that it remains a hypothesis. Empirical evidence has demonstrated the self-similarity of log-perplexity scores in language (i.e. surprise of information-theoretic complexity), indicating that patterns observed at the word level are mirrored at sentence and potentially higher levels of linguistic structure. The efficacy of Recursive INference Scaling (RINS) is another evidence in favor of the informal picture above.


\newpage

\subsection{Architecture Sweep}
We use the following pseudocode when sweeping across recursive architectures. The goal is to ensure that we only compare architectures that have a similar size.

\begin{lstlisting}[language=Python, style=mystyle]
# baseline
add(signature='A', degree=1)

# RAO
add(signature='AA', degree1)
add(signature='AAA', degree1)
add(signature='AAAA', degree1)

# other architectures
for signature in ['ABB', 'ABA', 'AAB', 'ABBC', 'AABC', 'ABCC', 'ABBB', 'AAAB', 'AABB']:
    for degree in [1, 2, 3]:
      num_unique_blocks =  len(set(signature) ** degree unique_layers
      num_layers_per_block = num_layer_in_baseline_model // num_unique_blocks
      if num_layers_per_block > 0:
        add(signature=signature, degree=degree)
\end{lstlisting}

\newpage
\subsection{Training Configuration}

\subsubsection{Language Modeling}

\begin{lstlisting}[language=Python, style=mystyle]

  """Config for training a decoder-only language model."""
  config.seed = 0
  config.total_steps = ...  # swept
  config.vocab_size = 32,000

  config.input = dict()
  config.input.max_len = 1,024  # or 1,536 for long-sequence baseline
  config.tokenizer = 'c4_en'

  config.input.data = {  # equal weight
      'c4/en': 1.0,
      'huggingface:cerebras__slimpajama_627b': 1.0,
  }
  for dataset_name in config.input.data:
    config.input[dataset_name] = {}
    config.input[dataset_name].data = dict(
        name=dataset_name,
        split='train',
    )
    config.input[dataset_name].shuffle_buffer_size = 250,000

  config.input.batch_size = 1,024

  # Optimizer section
  config.optax_name = 'scale_by_adam'
  config.grad_clip_norm = 1.0

  config.lr = 5e-4
  config.wd = 5e-5
  config.schedule = dict(decay_type='rsqrt',
                         warmup_steps=5,000,
                         cooldown_steps=5,000,
                         )
\end{lstlisting}

\subsubsection{Vision: Image Classification}
\begin{lstlisting}[language=Python, style=mystyle]

  """Config for training a ViT model."""
  config.seed = 0
  config.total_epochs = ...  # swept
  config.num_classes = 1,000
  config.init_head_bias = -6.9
 
  config.input = dict()
  config.input.batch_size = 1,024
  config.input.shuffle_buffer_size = 250,000
  
  # preprocessing
  config.input.pp = 'value_range(-1, 1)|inception_crop(224)|flip_lr'
  config.mixup = dict(p=0.2, fold_in=None)

  # Optimizer section
  config.optax_name = 'scale_by_adam'
  config.grad_clip_norm = 1.0
  config.optax = dict(mu_dtype='bfloat16', b2=0.95)

  config.lr = 0.001
  config.wd = 0.0001

  config.schedule = dict(decay_type='cosine', warmup_steps=10,000)
\end{lstlisting}

\newpage
\subsubsection{Language-Image Pretraining}
\begin{lstlisting}[language=Python, style=mystyle]

  """Config for training a SigLIP model."""
  config.seed = 0
  config.total_examples = ...  # swept

  config.input = dict()
  config.input.batch_size = 1,024 x 32
  config.input.shuffle_buffer_size = 250,000
  
  # preprocessing
  config.input.tokenize = mc4  # multilingual c4
  config.input.max_len = 64  # for text
  config.input.prefetch = 1  # save host memory

  # model
  config.model.bias_init = -10.0
  config.model.temperature_init = 10.0
  
  # Optimizer section
  config.optax_name = 'scale_by_adam'
  config.grad_clip_norm = 1.0

  config.lr = 0.001
  config.wd = 0.0001

  config.schedule = dict(decay_type='cosine', warmup_steps=20_000)

\end{lstlisting}

