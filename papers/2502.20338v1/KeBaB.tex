\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2021}
\bibliographystyle{plainurl}

\nolinenumbers

\title{KeBaB: $k$-mer based breaking for finding super-maximal exact matches}
\titlerunning{KeBaB: $k$-mer based breaking for finding SMEMs}

\author{Nathaniel K.\ Brown}
{Department of Computer Science, Johns Hopkins University, USA}
{nbrown99@jhu.edu}
{https://orcid.org/0000-0002-6201-2301}
{Funded in part by NIH grants R01HG011392 and R21HG013433 to Ben Langmead and a Johns Hopkins University Computer Science PhD Fellowship.}

\author{Anas Alhadi}
{Faculty of Computer Science, Dalhousie University, Canada}
{alhadi@dal.ca}
{}
{}

\author{Nour Allam}
{Faculty of Computer Science, Dalhousie University, Canada}
{nour.allam@dal.ca}
{}
{}

\author{Dove Begleiter}
{Faculty of Computer Science, Dalhousie University, Canada}
{begleiter@dal.ca}
{}
{}

\author{Nithin Bharathi {Kabilan Karpagavalli}}
{Faculty of Computer Science, Dalhousie University, Canada}
{nithinbharathi@dal.ca}
{}
{}

\author{Suchith {Sridhar Khajjayam}}
{Faculty of Computer Science, Dalhousie University, Canada}
{suchith.sridhar@dal.ca}
{}
{}

\author{Hamza Wahed}
{Faculty of Computer Science, Dalhousie University, Canada}
{hamzawahed@dal.ca}
{}
{}

\author{Travis Gagie\footnote{Corresponding author.}}
{Faculty of Computer Science, Dalhousie University, Canada}
{travis.gagie@dal.ca}
{https://orcemail@domain.orgid.org/0000-0002-1825-0097}
{Funded in part by NIH grant R01HG011392 to Ben Langmead and NSERC grant RGPIN-07185-2020.}

\authorrunning{N.K.\ Brown et al.}

\Copyright{Nathaniel K.\ Brown, Anas Alhadi, Nour Allam, Dove Begleiter, Nithin Bharathi Kabilan Karpagavalli, Suchith Sridhar Khajjayam, Hamza Wahed, Travis Gagie}

\ccsdesc[500]{Theory of computation~Pattern matching}

\keywords{Super-maximal exact matches, {\tt ropebwt3}, $k$-mers, admissible heuristics}

\acknowledgements{Many thanks to Christina Boucher, Ben Langmead, Heng Li, Finlay Maguire, Giovanni Manzini and Mohsen Zakeri for helpful discussions.}

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\(brief announcement)
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
Suppose we have a tool for finding super-maximal exact matches (SMEMs) and we want to use it to find all the long SMEMs between a noisy long read $P$ and a highly repetitive pangenomic reference $T$.  Notice that if $L \geq k$ and the $k$-mer $P [i..i + k - 1]$ does not occur in $T$ then no SMEM of length at least $L$ contains $P [i..i + k - 1]$.  Therefore, if we have a Bloom filter for the distinct $k$-mers in $T$ and we want to find only SMEMs of length $L \geq k$, then when given $P$ we can break it into maximal substrings consisting only of $k$-mers the filter says occur in $T$ --- which we call pseudo-SMEMs --- and search only the ones of length at least $L$.  If $L$ is reasonably large and we can choose $k$ well then the Bloom filter should be small (because $T$ is highly repetitive) but the total length of the pseudo-SMEMs we search should also be small (because $P$ is noisy).  Now suppose we are interested only in the longest $t$ SMEMs of length at least $L$ between $P$ and $T$.  Notice that once we have found $t$ SMEMs of length at least $\ell$ then we need only search for SMEMs of length greater than $\ell$.  Therefore, if we sort the pseudo-SMEMs into non-increasing order by length, then we can stop searching once we have found $t$ SMEMs at least as long as the next pseudo-SMEM we would search.  Our preliminary experiments indicate that these two admissible heuristics may significantly speed up SMEM-finding in practice.
\end{abstract}

\section{Introduction}

A super-maximal exact match (SMEM) between a pattern $P [0..m - 1]$ and a text $T [0..n - 1]$ is a substring $P [i..j]$ of $P$ such that
\begin{itemize}
\item $P [i..j]$ occurs in $T$,
\item either $i = 0$ or $P [i - 1..j]$ does not occur in $T$,
\item either $j = m - 1$ or $P [i..j + 1]$ does not occur in $T$.
\end{itemize}
Finding SMEMs is a classic problem in stringology with many applications, particularly in bioinformatics.  For example, Li's~\cite{Li24} recent {\tt ropebwt3} can index a highly repetitive pangenomic reference consisting of hundreds or thousands of genomes in reasonable time and space, and then quickly find the SMEMs of noisy long reads with respect to that reference.  When we are interested only in long SMEMs {\tt ropebwt3} can usually find them even faster because it skips over much of the pattern, somewhat like Boyer-Moore~\cite{BM77} pattern matching.

Inspired by {\tt ropebwt3}'s skipping, we considered other ways we can stop a SMEM-finding tool from searching all of $P$ while still finding all long SMEMs.  We made two simple observations:

\begin{observation}
\label{obs:filter}
If $L \geq k$ and the $k$-mer $P [i..i + k - 1]$ does not occur in $T$ then no SMEM of length at least $L$ contains $P [i..i + k - 1]$.
\end{observation}

\begin{observation}
\label{obs:sorting}
If we are interested only in the longest $t$ SMEMs then once we have found $t$ SMEMs of length at least $\ell$ we need only search for SMEMs of length greater than $\ell$.
\end{observation}

\noindent By Observation~\ref{obs:filter}, if we have a Bloom filter~\cite{Blo70} for the distinct $k$-mers in $T$ and we want to find only SMEMs of length $L \geq k$, then when given $P$ we can break it into maximal substrings consisting only of $k$-mers the filter says occur in $T$ --- which we call pseudo-SMEMs --- and search only the ones of length at least $L$.  Filtering on the basis of $k$-mers is not a new idea (see, e.g.,~\cite{HKCCKZMWKL23} for a recent discussion) but we are not aware of breaking based on $k$-mers previously being used as a preprocessing step for SMEM-finding.

Bloom filters can give false positives but not false negatives, and a false positive can only make a pseudo-SMEM longer than it would be if the filter correctly rejected the $k$-mer.  This means false positives can slow down our search but cannot affect its correctness.  A single false positive is also unlikely to significantly lengthen a pseudo-SMEM because if a $k$-mer does not occur in $T$ then some of the $k$-mers mostly overlapping it probably also do not occur in $T$.

If $T$ is a highly repetitive pangenomic reference and $k$ is reasonably small then the number of distinct $k$-mers in $T$ is likely to be very small compared to $T$'s length, so the Bloom filter can be much smaller than $T$ while still having a low probability of false positives.  On the other hand, if $P$ is a noisy long read and $k$ is reasonably large then many $k$-mers in $P$ will contain a sequencing error and thus probably not occur in $T$, so many of the pseudo-SMEMs will be short and we will ignore them.  Even when the total length of the pseudo-SMEMs is not significantly less then the length of $P$ --- since pseudo-SMEMs can overlap by $k - 2$ characters, it can be more --- if the pseudo-SMEMs are not too long then nearly all substrings of $P$ will span multiple pseudo-SMEMs and can be ignored.  Therefore, if $L$ is reasonably large and we can choose $k$ well then we should speed up SMEM-finding without using much extra space.

We chose the name pseudo-SMEMs because we conjecture that in practice most pseudo-SMEMs will be SMEMs.  In particular, it seems more likely that long pseudo-SMEMs will contain long SMEMs so, if we want to find SMEMs as long as possible as quickly as possible, we should look in the long pseudo-SMEMs first.  Now suppose we are interested only in the longest $t$ SMEMs of length at least $L$ between $P$ and $T$.  By Observation~\ref{obs:sorting}, if we sort the pseudo-SMEMs into non-increasing order by length then we can stop searching once we have found $t$ SMEMs at least as long as the next pseudo-SMEM we would search.

Sorting the pseudo-SMEMs also cannot affect the correctness of our search.  Therefore, although breaking $P$ into pseudo-MEMs and sorting them are only heuristics, they are both admissible.  In the next section we will present preliminary experiments indicating they may significantly speed up SMEM-finding in practice.

\section{Experiments}

We ran our experiments on a commodity laptop with an Intel Core i5-1335U processor and 16 GB of RAM running Ubuntu 24.04.2.  Our input and output files can be downloaded from \url{https://tinyurl.com/4djjrrad}\ .

We generated a toy pangenomic reference {\tt dataset.fa} by choosing a string of length 10000 over $\{{\tt A}, {\tt C}, {\tt G}, {\tt T}\}$ uniformly at random, making 10000 copies of it with each character in the copies flipped with probability 0.001 to another character chosen uniformly at random, and concatenating the copies.  We generated a set of toy reads {\tt patternset.fa} the same way and from the same initial string of length 10000, but this time flipping characters with probability 0.1.

We indexed {dataset.fa} with {\tt ropebwt3} ten times with
\begin{quotation}
\tt ropebwt3 build -t24 -bo dataset.fmd dataset.fa
\end{quotation}
with the fastest, slowest and averages executions taking 13.567, 14.641 and 13.942 seconds of real time and 77.512, 80.794 and 79.849 seconds of user time, respectively.  The index file {\tt dataset.fmd} took 3\,364\,844 bytes.  We then searched for the SMEMs of length at least 40 ten times with
\begin{quotation}
\tt ropebwt3 mem -t4 -l40 dataset.fmd patternset.fa > output1.txt
\end{quotation}
with the fastest, slowest and averages executions taking 6.364, 6.416 and 6.388 seconds of real time and 24.853, 25.029 and 24.917 seconds of user time.

We built ten times a 2\,500\,001-byte Bloom filter for the 20-mers in {\tt dataset.fa} with 23.468\% of the bits set to 1, with the fastest, slowest and averages executions taking 1.463, 1.521 and 1.486 seconds of real time and 1.431, 1.472 and 1.454 seconds of user time.  We broke the patterns in {\tt patternset.fa} into pseudo-MEMs of length at least 40 and stored them in {\tt pseudo-SMEMs.fa} ten times, with the fastest, slowest and averages executions taking 1.672, 1.705 and 1.688 seconds of real time and 1.626, 1.652 and 1.642 seconds of user time and {\tt pseudo-SMEMs.fa} taking 21\,051\,988 bytes.  (We sorted the pseudo-SMEMs into non-decreasing order by length when generating {\tt pseudo-SMEMs.fa} because the sorting step did not noticeably affect execution times.)  We then searched for the SMEMs of length at least 40 with
\begin{quotation}
\tt ropebwt3 mem -t4 -l40 dataset.fmd pseudo-SMEMs.fa > output2.txt
\end{quotation}
with the fastest, slowest and averages executions taking 1.605, 1.629 and 1.618 seconds of real time and 6.023, 6.066 and 6.038 seconds of user time.

To estimate how long {\tt ropebwt3} would take searching only for the longest 5 SMEMs in each pattern, we removed from {\tt pseudo-SMEMs.fa} the pseudo-SMEMs it would not need to search, and stored the remainder in {\tt top$\_$pseudo-SMEMs.fa}, which took 800\,621 bytes.  We then search again for the SMEMs of length at least 40 with
\begin{quotation}
\tt ropebwt3 mem -t4 -l40 dataset.fmd top$\_$pseudo-SMEMs.fa > output3.txt
\end{quotation}
with the fastest, slowest and averages executions taking 0.108, 0.119 and 0.110 seconds of real time and 0.359, 0.400 and 0.369 seconds of user time, respectively.

In summary, searching for SMEMs of length at least 40 with {\tt ropebwt3} alone took an average of 6.388 seconds of real time and 24.917 seconds of user time, while breaking into pseudo-SMEMs and then searching with {\tt ropebwt3} took an average total of 3.306 seconds of real time and 7.679 seconds of user time, and breaking into pseudo-SMEMs and then searching only the pseudo-SMEMs we need consider when finding the 5 longest SMEMs of length at least 40 in each pattern took an average total of 1.798 seconds of real time and 2.011 seconds of user time.

\section{Future work}

This research was done as a class project for CSCI 4119 / 6106 (``Compact data structures'') at Dalhousie University, taught by the first and last authors and taken by the other authors, and extensive experiments were beyond the scope of the course.  We are currently scaling our experiments up to a real pangenomic dataset and a set of real long reads.  After that, we will try to add to {\tt ropebwt3} a parameter $t$ telling it to report only the longest $t$ SMEMs of length at least $L$, and test whether our sorting heuristic actually speeds {\tt ropebwt3} up.

\begin{thebibliography}{1}

\bibitem{Blo70}
Burton~H Bloom.
\newblock Space/time trade-offs in hash coding with allowable errors.
\newblock {\em Communications of the ACM}, 13(7):422--426, 1970.

\bibitem{BM77}
Robert~S Boyer and J~Strother Moore.
\newblock A fast string searching algorithm.
\newblock {\em Communications of the ACM}, 20(10):762--772, 1977.

\bibitem{HKCCKZMWKL23}
Yi~Huang, Lingkun Kong, Dibei Chen, Zhiyu Chen, Xiangyu Kong, Jianfeng Zhu,
  Konstantinos Mamouras, Shaojun Wei, Kaiyuan Yang, and Leibo Liu.
\newblock Casa: An energy-efficient and high-speed cam-based SMEM seeding
  accelerator for genome alignment.
\newblock In {\em Proceedings of the 56th Symposium on Microarchitecture},
  2023.

\bibitem{Li24}
Heng Li.
\newblock {BWT} construction and search at the terabase scale.
\newblock {\em Bioinformatics}, 40(12):btae717, 2024.

\end{thebibliography}

\end{document}