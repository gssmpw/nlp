
\newpage
\appendix
\onecolumn
\section{Creation of the Synthetic Dataset.}

\subsection{Creation of StyledTextSynth}
\label{sec:appendix_create_mq}

\subsubsection{Text Prompt for StyledTextSynth Image Generation}
\paragraph{General Prompt:}
The core of the synthetic data involves utilizing text-conditioned image generation methods. 
For simple topics like billboards, we follow a "General Prompt" approach with the following guidelines:
\begin{itemize}
    \item Provide a reasonable description of the billboard.
    \item Ensure the billboard faces the camera directly.
    \item The billboard should occupy at least one-third of the image.
    \item Incorporate a complex background for added detail.
    \item Keep the billboard's color consistent, with no additional context.
    \item Limit the total text on the billboard to fewer than 160 words.
    \item Ensure the billboard is visible, vehicle-related, and not overlapped by other objects.
\end{itemize}

We show an example in Table~\ref{tab:mqcallgpt4o}, with detail instruction the LLM generate reasonable scene description for image generation model.

\input{Appendix/tables/mq_call_gpt4o}



\paragraph{Topic with Human-Designed Seeds:}
Certain topics, such as \textit{studio} scenes, can involve highly complex setups. 
To address these challenges, we simplify the generation of text regions by incorporating human-designed seeds. 
Specifically, the instructions include a \textit{general prompt} combined with a predefined, human-curated scene to serve as the seed, ensuring better coverage and control over these intricate scenarios.

\begin{itemize}
    \item Weather Report: A host stands in a studio, with a display screen behind them showing weather conditions for various locations. In the foreground, a white rectangular box occupies one-third of the screen.
    \item TV Shopping: A host stands beside a table, holding a product in their hand, with an advertising board placed vertically next to them.
    \item Instruction Manual: An instruction manual is placed vertically, with objects associated with it positioned nearby.
\end{itemize}


\paragraph{Template Topics with Fixed Text Region Position}:
Some topics can be used as template to generate more samples, what is different is that the text appeared position is fixed.
For these generate samples, we can simply generate more samples by simple text replace.

\begin{itemize}
    \item News: Describe an image that could appear on television news. It can depict either positive or negative events, with diverse content. The total word count should be less than 160 words.
    \item Cinema Poster: Describe an image of a movie poster. It can be of any genre. The total word count should be less than 160 words.
\end{itemize}


\input{Appendix/tables/mq_text_call_gpt4o}

\input{Appendix/tables/mq_text_call_llama}


\input{Appendix/tables/mq_text_call_qwen}

\subsubsection{Utilizing LLMs for Text Generation in Specific Scenes}


After generating scene images without text, it is necessary to call upon an LLM to create contextually relevant sentences for rendering. 
To ensure realism in the generated outputs, we employ \textbf{Scene-Dependent Text Generation}. 
For general topics, such as noticeboards, we prompt the LLM to produce sentences based on the given topic. 
Examples of such outputs are shown in Table~\ref{tab:mq_text_call_gpt4o} and Table~\ref{tab:mq_text_call_llama}.
The LLMs accurately generate text appropriate for the given scene.


\paragraph{Visual-Dependent Scenes:}  
In some cases, the visual appearance of the scene is closely connected to the text and requires fine-grained visual understanding. 
For instance, a generated poster with rich visual elements may need text that complements its design. 
In such scenarios, we use LVM models that process both text and image inputs to produce reasonable outputs. 
An example of this process is shown in Table~\ref{tab:mq_text_call_qwen2}.
By incorporate specific instruction, the model produce reasonable output.



\subsubsection{How Do We Select Data Topics for Rendering?}
Originally we generate 50 topics that include dense text, one more question is how to filter these topics.
With this in mind, we design the following filter rules:
\begin{enumerate}
    \item \textbf{Avoid topics directly tied to font generation}, such as store signs, wayfinding signs, or on-screen text.
    \item \textbf{Exclude topics where the renderable area is too small}, such as mobile phone screenshots.
    \item \textbf{Avoid topics with unclear boundaries or artistic fonts that are hard to recognize}, such as neon signs.
    \item \textbf{Prioritize topics with better rendering results in SD3.5 over similar ones}, e.g., choose "digital display" over "OLED display" or "banner" over "protest marches."
\end{enumerate}



\subsubsection{Text Deduplication in StyledTextSynth Data}
The text in the synthetic data is generated by Llama 3.1, GPT4o, and Qwen2VL. 
In most topics, there is basically no obvious disharmony between the generated images and the scenes, so we mainly use the text generated by Llama 3.1 and GPT4-o. 
For some scenes where the images and texts are highly correlated, VLM is needed to generate texts that match the image content.
Under the same or semantically similar topics, there may be semantic duplication. To this end, we semantically deduplicate the generated text based on the sentence-transformers library. 

\paragraph{Deduplication Process with Semantic Hashing}
The deduplication process consists of the following steps:
\begin{enumerate}
    \item \textbf{High-Dimensional Semantic Representation:} 
    Obtain the high-dimensional semantic representation of the text.

    \item \textbf{Dimensionality Reduction:}
    Map the high-dimensional semantic vector to a fixed low-dimensional space using random projection.

    \item \textbf{Semantic Hash Generation:}
    Generate semantic hashes based on the projection results.

    \item \textbf{Pairwise Similarity Comparison:}
    Use Hamming similarity to perform pairwise comparisons of the semantic hashes. A Hamming similarity threshold of 0.9 is applied to detect and remove semantically similar texts.

\end{enumerate}

This structured approach ensures effective and accurate text deduplication while maintaining semantic integrity.


\subsubsection{Middle-Quality Sample Filtering}

To enhance the quality of the generated middle-quality samples, we apply a set of filtering rules to reject unsuitable samples. The primary criteria for rejection are as follows:

\begin{itemize}
    \item Insufficient areas available for rendered text.
    \item Excessive similarity to other topics, reducing diversity.
    \item Difficulty rendering text in curved areas.
    \item Unrealistic or artificial appearance of the image.
    \item Challenges in identifying or defining bounding boxes.
    \item Presence of incorrect or irrelevant text.
    \item Poor recognition quality or unclear visual details.
\end{itemize}

Examples of rejected samples are illustrated in Figure~\ref{fig:mq_reject_sample}.

\input{Appendix/mq_rejected_samples}


\subsection{Gen Interleaved Data Benchmark}
\label{sec:interleave}
The process of generating interleaved data is divided into three main parts: data selection, PDF generation, and annotation generation.
% \textcolor{red}{We select from WIT and OBELICS}.
\subsubsection{Data Selection}
We select data from WIT~\cite{wit} and OBELICS~\cite{obelics}. 
The WIT dataset contains samples from Wikipedia, with each sample comprising an image and multiple associated text segments, such as titles, main text, subtitles, subtext, and image captions. From this dataset, we sample instances containing a single image and interleaved text. 
The OBELICS dataset consists of interleaved image-text documents sourced from web pages in the Common Crawl. Each OBELICS sample includes multiple images with their corresponding text segments. For our purposes, we sample data containing two to four images to maintain manageable image sizes. 
Finally, we sampled 69.8\% data from the WIT and 30.2\% data from the OBELICS, respectively.
\subsubsection{PDF Generation}
After selecting the data, we use the PyMuPDF~\cite{pymupdf} library to generate parseable PDF files based on the sampled data. To accommodate the two types of data, we design different layout generation strategies according to their respective structures.

For both datasets, the layout strategy involves first randomly assigning image positions on the page, followed by allocating text boxes in a manner that optimally utilizes the remaining space. For the WIT dataset, since the text segments have predefined types (e.g., title, main text, subtitle), we impose additional constraints to ensure structural consistency. For instance, titles are placed at the top of the corresponding image to maintain semantic alignment. In contrast, for the OBELICS dataset, we adopt a simpler approach where the text boxes are sequentially assigned in a top-left to bottom-right order across the layout.

To implement the layouts, we use the  \texttt{insert\_htmlbox()}  from PyMuPDF to insert images and text into the PDFs. The font for each sample is randomly selected to introduce variation. To further standardize the generated PDFs, we limit the text in each text box to a maximum of 50 words. Additionally, after generating the PDFs, we save a rendered image version of each page to serve as the corresponding image data for our dataset.


\subsubsection{Annotation Generation}
After generating parseable PDFs, we use the PyMuPDF to extract information such as the bounding boxes of text and images, as well as text font sizes and styles. Additionally, we utilize Qwen2-VL to generate captions for each image within the PDF. The prompt used for caption generation is: "\texttt{Generate the caption of the image, and the caption should be no more than 50 words.}"

Finally, we obtain detailed information for the text, including its content and bounding boxes, along with the captions and bounding boxes of the images. By combining these elements based on the generated template, we produce the interleaved data annotations.

\subsection{Template Generation Details}


\input{Appendix/tables/call_gpt}

To create templates for summarizing descriptions and text, we utilized an LLM to generate a total of 600 templates. The prompt used for this process is detailed in Table~\ref{tab:template_generate}.



\subsection{Bounding Box Annotation and Detector Training  for StyledTextSynth Sample}

\subsubsection{Bounding Box Generation}

\begin{enumerate}
    \item \textbf{YOLO Results:} Based on the model's output, select the bounding box with the largest rectangular area when multiple results are present.
    \item \textbf{Fine-tuned RT-DETR\_R50VD Results (Packing Box):} Use the model's output to identify the bounding box. If multiple results are present, select the one with the smallest difference between width and height.
    \item \textbf{RT-DETR\_R50VD Results (Booklet Page):} Check if the output contains the label "book". If multiple results are present, choose the bounding box with the smallest width-to-height difference.
\end{enumerate}

For all three methods above, the results are passed through SAM2 (Segment Anything Model v2) to refine recognition. The SAM2 output is converted into center points to serve as prompts, improving predictions for slanted surfaces.

\subsubsection{Detector Training}

\paragraph{YOLO Training Process:}

\begin{enumerate}
    \item For each topic (excluding template topics like Alumni Profile and News, as well as rejected topics), start with the YOLOv11l initial weights and manually label about 1,000 failed detection samples. Use the following annotation methods:
    \begin{enumerate}
        \item For slanted images, use the \texttt{labelme} tool to annotate with quadrilateral bounding boxes (four points).
        \item For upright images, annotate using the \texttt{Code} tool with two points (top-left and bottom-right).
    \end{enumerate}
    Annotate areas of the image where the topic is unobstructed, and convert all annotations to YOLO format with the class label set to 0.
    \item Train the model on the mixed annotated dataset for approximately 400 epochs (or more) to obtain initial weights.
    \item Test the initial weights on each topic. A detection rate of at least 40\% is considered usable for that topic.
    \item For topics with low detection rates in Step 3, augment the manually labeled dataset for that topic with approximately 1,000 additional images. Apply transformations such as scaling, composition, flipping, and affine transformations. Combine the augmented data with the original manually labeled data (totaling approximately 2,000 images), and retrain the model for an additional 200 epochs.
    \item Certain topics may share weights based on detection performance. For example:
    \begin{itemize}
        \item \textbf{Billboard} and \textbf{TV Shopping} can share the same weights.
        \item \textbf{Blackboard Classroom} and \textbf{Advertisement Poster} can share the same weights.
    \end{itemize}
\end{enumerate}

\paragraph{Fine-Tuned RT-DETR\_R50VD Training:}

\begin{enumerate}
    \item Fine-tune the RT-DETR\_R50VD model specifically for the \textbf{Packing Box} topic. Use 1,000 manually annotated packing box samples from Step 1.
    \item Train the model  for approximately 100 epochs.
\end{enumerate}




\subsection{Text Rendering Details}

\subsubsection{1. Bbox Text Rendering:}
After obtaining images generated by Stable Diffusion and images from CommonCrawl, which contain large fillable text areas (such as billboards, electronic screens, etc.), we use YOLO v11 and RT-DETR\_r50vd to identify and label the fillable areas in the images. However, these detectors can only recognize rectangular areas, and the labeled fillable regions are often slightly larger than the actual fillable areas. 
Therefore, we further use SAM2, starting from the center point of the bounding box, to search for color-matching areas. This ensures that the new bounding boxes generated by SAM2 more accurately cover the fillable areas, breaking the traditional rectangular limitation and supporting the detection and filling of irregular quadrilaterals.

For text content generation, we use Llama-3.1-8B, GPT-4o, and Qwen2-VL-7B. 
Among them, Qwen2-VL-7B is mainly used for generating text related to cinema posters.

\textbf{Rectangular Bbox Text Rendering}
For detected rectangular bounding boxes, we directly render text within the area. 
The font is randomly chosen from 10 common fonts, and the font size is automatically adjusted based on the bounding box size to fill the area as much as possible, ensuring both aesthetics and readability.

\textbf{Irregular Quadrilateral Bbox Text Rendering}
For detected irregular quadrilateral bounding boxes, we first create a transparent layer and render the text on that layer. Then, we use a perspective transformation to adjust the transparent layer to match the irregular quadrilateral shape of the bounding box and finally composite it onto the original image, ensuring the text accurately fits the fillable area.

\subsubsection{2. Template Rendering Method: }
For images related to News Shows, Weather Reports, and Cinema Posters, where text usually appears in relatively fixed areas, we use a template rendering approach for text filling. We create background templates for these topics based on real-world images, label the fillable areas of the templates with bounding boxes, render the background templates onto the original image, and then fill the text according to the bounding box annotations of the templates.



\clearpage


\section{Creation of the Real Dataset.}



\subsection{Data Selection Details from Existing Datasets}
\label{sec:appendix_data_selection}
To ensure the quality of selected samples, we apply a rigorous filtering pipeline consisting of the following steps:

\begin{enumerate}
    \item \textbf{English-Like Check:}  
    Text is evaluated for resemblance to English. At least 70\% of the words must contain alphabetic characters to ensure alignment with natural English patterns.
    
    \item \textbf{Minimum Length Check:}  
    Samples with fewer than seven words are excluded. This criterion eliminates excessively short texts that may lack meaningful content.
    
    \item \textbf{Unique Word Ratio Check:}  
    To promote diversity, the ratio of unique words to total words must exceed 0.3. Samples with overly repetitive word usage are filtered out.
    
    \item \textbf{Consecutive Repetition Check:}  
    Text containing more than three consecutive repetitions of the same word is excluded to prevent redundancy and improve coherence.
    
    \item \textbf{Word Validity Check:}  
    Each word must include at least one alphabetic character and be longer than one character. This ensures all words are meaningful and eliminates noise or random symbols.
    
    \item \textbf{Text Cleaning:}  
    Non-alphanumeric characters, except spaces, are removed. Multiple spaces are normalized into a single space to ensure the text is clean and consistently formatted.
    
    \item \textbf{Annotation Sorting:}  
    Annotations are ordered spatially, following a top-to-bottom and left-to-right sequence based on the coordinates of bounding polygons. This ensures spatial coherence in the text layout.
\end{enumerate}

This pipeline is designed to refine the dataset and maintain high standards for text quality and diversity.


\subsection{Extracting Powerpoint Data}
We extract the powerpoint data with PyMuPDF~\cite{pymupdf}.
Specifically, we transform the each page of powerpoint into pdf format, then we rephrase the powerpoint data by blocking description.
For example, it split the all page into different block.
Each block include elements like text or image, for text element we extract the word and for image we use the QWen-VL to generate caption and the prompt is simple \textit{Describe this image}.
For example, we simply call image.

\subsection{Data Selection Details for TextScenesHQ Dataset}
After crawling images according to topics, we use the easyOCR\footnote{https://github.com/JaidedAI/EasyOCR} library to recognize the text in the images. 
First, we save images containing more than 10 words, and then organize the text information from the upper left to the lower right to construct a JSON file. 
The content of the JSON file includes the text and its corresponding bounding box.
During this process, some difficult data may have spelling errors, including but not limited to confusion between numbers and letters, spelling errors, and capitalization errors. 
In this regard, we use Llama 3.1~\cite{llama3h} to check and correct the recognized text to improve the accuracy and quality of the text.



\subsection{TextScenesHQ Image Filtering}
For real image data, we primarily discarded samples where the text was not clearly visible. 
Additionally, samples were rejected if the detected language was not English or if the text contained too few words (fewer than 10 in this study).
We show the rejected samples in Figure~\ref{fig:hq_reject_sample}.

\input{Appendix/hq_rejected_samples}


\subsection{TextScenesHQ Image Annotation}
\label{sec:appendix_textsceneshq_annotation}
After using OCR for filtering and generating bounding boxes around the text in the images, we convert the detected Chinese text and its corresponding bounding boxes into a text JSON format. Due to the diversity and complexity of the images, OCR results may contain spelling errors and misordered text. To address this, we perform three corrective steps using Llama 3.1 and Qwen 2.5-Coder. First, Llama 3.1 is used to correct any spelling mistakes in the text. Next, we use Llama 3.1 to reorder the text slightly to align with the proper syntax, as OCR typically outputs text in a left-to-right, top-to-bottom sequence without considering the multi-column layout in the images. After reordering, we generate the corrected text JSON. The third step involves addressing any potential formatting issues in the JSON. If the JSON generated in the second step is not parsable, we use Qwen 2.5-Coder to output the text JSON in markdown format to ensure proper structure.

For the image background descriptions, we use Qwen 2.5-VL to generate contextual information while preventing it from outputting any descriptions of the text within the image. Additionally, we created 500 diverse and complex scenario templates using GPT-4o to generate a wide range of image descriptions. These descriptions, combined with the corresponding text JSON, are used to generate comprehensive image information in JSON format.


\subsection{Quality classification}
In this work, we mainly split the data quality according to the visual appealing semantic, and if the image include dense text and have correct captions.

% \textcolor{red}{\textbf{Plot Some powerpoint data}.
% Specifically, we plot some cases with or without images. 
% Ensure more diversity.
% }


\clearpage

\section{Annotation Details}

\subsection{Examples from All Subsets}

Our \DatasetName comprises a total of 10 subsets, which can be categorized into three types: \emph{i}. Images without a specific scene, \emph{ii}. Images with a specific scene, and \emph{iii}. Images with a specific scene and bounding box annotations.

\paragraph{Images without a Specific Scene.}
For simple synthetic datasets such as Paragen-2M, where the background is plain white, we generate descriptions for image creation using prompts like: \textit{"Please generate an image of xxx based on the following text: ."}

\paragraph{Images with a Specific Scene.}
This category includes images accompanied by a scene description $T$ and OCR text $O$. Using our template, we generate longer, natural descriptions by combining these elements.

\paragraph{Images with a Specific Scene and Bounding Box.}
For datasets like AutoSlideGen, ArxivPaper generation, and interleaved sample generation, bounding box annotations are provided for each element. In these cases, we utilize LLMs to summarize all elements into a coherent paragraph. Specifically, we include details such as bounding box coordinates and the text within each box.

All subset examples are visualized in Figure~\ref{fig:all_datasets_visualization_w_anno}.

% \textcolor{red}{Note: Each subset's data, especially text annotations, requires updating.}


\input{figures/list_data_type}

\subsection{Processing Methods}
For datasets that already have captions and OCR results, such as anyword3m and mario10m, we use templates generated by GPT for concatenation (as you did before). For paragen2m, which is pure text data, we use structured sentence descriptions, e.g., "a text white background image...". For autogen and interleave data, which are interleaved distributions, we list the text and image separately in bullet points, while placing the required elements (like bbox) and fonts in the corresponding context section. For midquality data, to ensure a natural integration, we generate scene captions using Qwen2-VL and require it to generate a render text placeholder <>, which is then replaced with the rendered text. High-quality data is processed by Llama3.1 to generate scene descriptions and optimize the OCR results (see section 3.2 for the concatenation method).

\subsection{All LDA Topics}
\label{sec:all_lda}

In this section, we list top 20 LDA topics of \DatasetName in Table~\ref{tab:lda_topics_full}.
Based on the topic distribution in the table, several patterns emerge:

\begin{enumerate}
    \item \textbf{High Proportion of Common Topics}: Topics such as "Position" (15.12\%), "Signs" (14.50\%), and "Colors" (13.54\%) account for a significant portion of the dataset. These themes likely reflect common real-world scenarios, such as signage, positioning of text and images, and the use of colors in visual communication.
    
    \item \textbf{Content-Related Themes}: Content-centric topics like "Content" (14.79\%), "Community" (8.29\%), and "Safety" (2.67\%) also show relatively high proportions, suggesting that the dataset includes a considerable amount of text related to information dissemination and visual design, commonly seen in advertising and informational graphics.
    
    \item \textbf{Lower Proportion of Specialized Domains}: Topics like "Products" (1.48\%), "Cloud" (0.55\%), and "Shops" (0.78\%) have smaller representations, indicating that the dataset covers fewer instances of text-image combinations related to specific industries or niche topics.
    
    \item \textbf{Use of Numbers and Symbols}: Topics related to numbers, such as "Numbers" (1.75\%) and "Symbols" (0.61\%), occupy lower proportions, possibly reflecting that numeric and symbolic content is less prevalent in the dataset, despite its importance in some contexts.
\end{enumerate}

Overall, the dataset is more focused on common visual and textual elements seen in everyday life, such as positioning, signage, and color usage, with a relatively lower emphasis on specialized topics or numeric/symbolic content.


\input{Appendix/tables/lda_all}



\clearpage

\section{Visualization of \DatasetName}


\subsection{Example of StyledTextSynth Samples}
To better investigate all topics included in the StyledTextSynth sample, we show the examples in Figure~\ref{fig:mq_topics}.
We mainly list Blackboard Classroom, News, Banner, Sliver Screen, Notice Board, Advertisement Board, TV Shopping, Billboard, Booklet Page, Academic Report, Alumni Profiles, Tablet Screen, Printed Paper, Cinema Poster and Packing Box.

\input{Appendix/mq_topics}



\subsection{Examples of TextScenesHQ Samples}

We present examples of topics with the largest number of samples in Figure~\ref{fig:hq_topics}. These include:  
Product Labeling, Billboard, Packing Box, Monitor, Instruction Manual, Booklet Page, Mobile Phone Screenshot, Wall Decal, Floor Poster, Game Live, OLED Display, Protest Marches, Weather Report, Noticeboard, News Show, Blackboard Classroom, Digital Display, Cinema Caption, Wayfinding Sign, Academic Report, Alumni Profiles, Banner, Clothes with Text, and Store Sign.

These topics represent text-rich scenes commonly encountered in daily life. 
By applying our carefully designed filtering rules, we have ensured that only TextScenesHQ data is preserved for rendering.


\input{Appendix/hq_topics}



\input{tables/dataset_statics}

% \subsection{Full List of Topics in Middle/High Quality Datasets}

% \textbf{Also plot the ratio of each class}



% % \section{Evaluation Details}
% % To evaluate the model.

% % \subsection{Prompt Implementation.}

% x\input{tables/dataset_captions}

% \input{figures/dataset_visualization}
