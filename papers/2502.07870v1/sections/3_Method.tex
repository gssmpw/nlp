\section{Dataset Construction}

The primary goal of our \DatasetName~ is to collect diverse scenes in daily life containing dense text. 
However, acquiring high-quality real world text-rich data is both expensive and time-consuming. 
To balance quality and scalability, we first construct \textit{Synthetic Image Split} with widely-used topics, providing easier cases for model training and evaluation.
Further, we collect \textit{Real Image Split} from diverse sources including PowerPoint presentations, documents, existing long sequence data, and visually appealing real-world images.
By combining these tiers, \DatasetName~ provides a comprehensive and scalable resource for dense text rendering.

\subsection{Synthetic Images Split}


\paragraph{CleanTextSynth:}
We create a simple dataset of text-only images, without incorporating additional visual elements, using the interleaved Obelics~\cite{obelics}, in which we randomly sample text sequences of length $L$.
Using OpenCV~\footnote{\url{https://opencv.org}} for text rendering, we place sequences on white canvases with variations in font (e.g., Helvetica, Times New Roman), size (12–48pt), color (RGB hex), and rotation (±45°). This results in 2 million samples of clean, well-formatted text, ideal for foundational experiments.

\input{figures/middle_quality_ppl}

\textbf{TextVisionBlend: Crafting Context-Rich Interleaved Data.}

Interleaved data seamlessly blends visual and textual elements in formats like blogs, wikis, and newspapers. Inspired by this, we created a synthetic interleaved image-text dataset that enhances data organization and contextual richness.
We source high-quality image-text pairs from Obelics~\cite{obelics} and WIT~\cite{wit}, then design random layouts to automatically combine them. 
Using PyMuPDF~\cite{pymupdf}, we generate white-background images and parseable PDFs, ensuring structured interleaved content. From these PDFs, we extract detailed annotations, including bounding boxes, font styles, and sizes, enriching each sample.
To enhance contextual richness, we used vision-language models Qwen2-VL~\cite{qwen2_vl} and BLIP~\cite{blip} to generate image captions, consolidating all annotations and captions into comprehensive sample files. This dataset captures the complexity of interleaved data, serving as a valuable resource for research and applications. See Appendix \ref{sec:interleave} for details.

\paragraph{StyledTextSynth:}
Building on pure-text images and interleaved text-image scenes, we address more complex embedded text scenarios, such as billboards, to enhance dataset diversity. The overall pipeline is shown in Figure~\ref{fig:middle_quality_ppl}.
Using GPT4o~\cite{gpt4o} as a world simulator, we identify 50 real-world text-integration scenes, refining them into 18 high-frequency topics (e.g., urban signage, product packaging). GPT4o generates scene descriptions, which serve as prompts for SD3.5 to create text-free images. We then identify suitable text placement areas, refining them with human annotations as needed.
Next, LLMs like GPT4o and Llama3.1~\cite{llama3h} generate contextually relevant text, which is rendered into designated regions, producing fully annotated images aligned with each topic. See Appendix~\ref{sec:appendix_create_mq} for details on prompting and rendering.


\subsection{Real Images Split}

\paragraph{PPT2Details.}
We first consider PowerPoint presentations, a widely used and text-rich format.
SlideShare1M~\cite{araujo2016slideshare} is a dataset containing 1 million PowerPoint slides in an interleaved format, with most slides featuring dense text.
To annotate this dataset, we utilize Qwen-VL~\cite{qwen2_vl}. 
Each slide is first converted into an image, and the model is then used to generate detailed descriptions of the text, images, and other elements, such as diagrams, tables, and vectors from this image.
To ensure high-quality annotations, we filter out slides without text and those of low quality. 
After this process, we retain a total of 0.3 million high-quality samples, providing a rich resource for further analysis and modeling.

\textbf{PPT2Structed.}
In addition to PPT2Details, we further access detailed slide elements with bounding box annotations for high-quality PowerPoint presentations. 
The AutoSlideGen dataset~\cite{autoslidegen} comprises 5,000 slides derived from scientific papers, where presentations are crafted to effectively convey research innovations.

To build this dataset, we process each slide using PyMuPDF to extract element bounding boxes and their corresponding text content. 
For slides containing images, we leverage the Qwen2-VL~\cite{qwen2_vl} model to generate descriptive captions. Text elements are preserved in their raw form to maintain accuracy and context.
This process produces a structured dataset of 96,000 annotated samples, providing detailed elements along with their positional information.


\paragraph{Paper2Text:}
Another prominent text-rich scene is PDF documents, such as those found in Arxiv papers. 
Using the Arxiv Paper dataset~\cite{arxiv_org_submitters_2024}, we process each page by extracting its content with PyMuPDF~\cite{pymupdf}.  
For this subset, we focus primarily on text information. 
Specifically, we retain attributes such as font color, size, and type. 
This approach enables the creation of detailed annotation containing comprehensive descriptions of the text elements on each page.

\paragraph{CoverBook:}
We utilize the Cover Book dataset~\cite{coverbook}, sourced from Amazon and Inc. marketplaces. 
This dataset comprises 207,572 books spanning 32 diverse categories, with each book providing a cover image, title, author, and category information.
To create rich captions, we concatenate the title, author, category, and year information for each book, resulting in detailed textual descriptions that enhance the dataset's usability for various tasks.

\textbf{LongWordsSubset.}
A straightforward approach to obtain long-text samples is to filter existing text-rich image datasets. For this purpose, we use two widely adopted text rendering benchmarks: AnyWords3M~\cite{anytext} and Marion10M~\cite{textdiffuser2}.
Since most samples in these datasets contain short text with bounding boxes, we apply a filtering process to select samples with at least seven words. The resulting subsets are named LongWordsSubset-A (from AnyWords3M) and LongWordsSubset-M (from Marion10M).
To ensure data quality, we further refine the subsets by removing cases with duplicate words, excessive consecutive repetitions, or invalid text. Additionally, we retain only English-language samples. 
Detailed descriptions of the filtering process are provided in the Appendix~\ref{sec:appendix_data_selection}.

\input{figures/high_quality_ppl}


\textbf{TextScenesHQ.}
To create a diverse and high-quality text-rich image dataset, we developed TextScenesHQ. 
Similar to StyledTextSynth, we use GPT4o as a world simulator to generate 26 predefined topics rich in text content. 
The overall pipeline is illustrated in Figure~\ref{fig:high_quality_ppl}.
The process begins with retrieval images aligned with the specified topics from Common Crawl~\footnote{https://commoncrawl.org/}. 
These images are then filtered using OCR-based filtering rules to select those containing long text. 
Images that do not meet this threshold undergo manual screening, during which we identify candidates for enhancement, such as adding text to advertisement backgrounds to enrich their visual complexity.

After cleaning, we annotate the images using advanced multimodal large language models like Qwen2-VL~\cite{qwen2_vl} and Intern-VL2~\cite{internvl}. 
These models generate detailed textual descriptions and bounding boxes for detected text regions. 
To ensure annotation quality, we validate them through semantic similarity checks using LLM, ensuring consistency and relevance.
For contrastive data and complex layout images, \textit{we incorporate human annotations to re-label the corresponding text to improve the data quality}.
Finally, the curated images and their validated annotations are organized into a comprehensive dataset, providing a robust resource for training and evaluation.
See Appendix~\ref{sec:appendix_textsceneshq_annotation} for more details.

\subsection{Evaluation Dataset Split}
To evaluate the dense-text image generation ability for existing model, we further propose \EvalDatasetName.
Adopting stratified random sampling weighted by subset complexity levels: 33\% from advanced synthetic tiers (StyledTextSynth), 33\% from real-world professional domains TextScenesHQ, and 33\% from web-sourced interleaved TextVisionBlend coverage of both controlled and organic scenarios.

For the StyledTextSynth and TextScenesHQ subsets, we sample data from each topic to ensure the evaluation set covers a wide range of topics.
For TextVisionBlend we perform random sampling to obtain the samples, and we finally get a test set with 3000 samples.
% Then we call the existing model.
In this way, our dataset cover different domain of data which allowing us to assess the capabilities of model across multiple dimensions. 



\input{figures/joint_distribution}

\input{tables/topic_dataset}


\subsection{Data Combination}  

Since each sub-dataset in our dataset contains different annotations, a primary challenge is how to effectively organize these data. Overall, the annotations in our data are divided into two main categories: image scene descriptions and rendered text. Additionally, some sub-datasets include bounding box information for the rendered text and font size, which enables precise localization and ensures the generation of structured data.

To integrate the scene description $S$ and rendered text $T$ into coherent and natural long descriptions, we utilize large language models (LLMs) to merge these two types of data. At the same time, to more efficiently leverage the existing data, we adopt adaptive prompting strategies for different sub-datasets. For the LongWordsSubset, which contains both captions and text, we use LLMs to generate multiple distinct ways of integrating $S$ and $T$.

Another example is for the StyledTextSynth dataset, where we use Qwen2-VL to generate image scene captions while requiring the LLM to insert placeholders into the generated captions, allowing for a natural combination of the caption and the text. 
For instance: "\textit{A cozy classroom with a blackboard displaying <>.}".
For detailed processing strategies on different datasets, please refer to the appendix.


