\section{Analysis of \DatasetName}
In this section, we first analyze the high-level statistics of our \DatasetName.
Then we analyze the topic modeling and do the qualitative assessment of the properties of \DatasetName.



\subsection{General Statistics}

\textbf{Joint Distribution of TextVisionBlend}. Figure~\ref{fig:joint_distribution} shows the joint distribution of token numbers and image numbers in interleaved data split TextVisualBlend. 
We limit the number of images in a document to 4 images for clarity. 
The documents of \DatasetName contain a median number of images of 2 and a median number of tokens of 33.


\input{figures/perplexity_distribution}



\textbf{Perplexity analysis.}
We utilized the pre-trained Llama-2-7B~\cite{llama2} to calculate perplexity scores for 10,000 documents from each dataset.

Lower perplexity scores suggest a stronger resemblance to the types of text corpus used for Llama-2, including Wikipedia and other high-quality sources.
Figure~\ref{fig:perplexity_distribution} presents the distributions of these scores.
Our findings indicate that the average perplexity of CleanTextSynth is significantly lower compared to that of LongWordsSubset-A and LongWordsSubset-M.

Additionally, the distribution of TextVisionBlend also aligns closely with that of the high-quality, diverse datasets used for Llama 2 training.
We also observe that the text quality in synthetic datasets, such as CleanTextSynth, is significantly higher than that of real-image subsets like TextScenesHQ.



\input{figures/pie_chart}
\input{figures/clip_score_subtype}


\subsection{Topic Distribution Analysis}


\paragraph{Topic Analysis in StyledTextSynth and TextScenesHQ:}
As illustrated in Figure~\ref{fig:topic_pie_chart}, we present an analysis of the topic distribution in the StyledTextSynth and TextScenesHQ datasets.
To provide a comprehensive comparison, we additionally highlight the broader topic coverage in real images. 
We make the following observations:

\emph{i.} Our dataset encompasses a wide variety of text-rich topics, such as weather reports, banners, TV shopping ads, and monitor displays. This diversity is crucial for maintaining the richness and generalizability of the samples.
\emph{ii.} By leveraging Large Language Models (LLMs) as world simulators to generate topics, we ensure that most topics are consistent across real and synthetic images, effectively bridging the gap between these two data sources.
\emph{iii.} Some topics generated by the LLM are not suitable for rendering, either due to inherent complexities or limitations in the synthetic generation process. These topics are excluded to maintain the quality and feasibility of the dataset.


\paragraph{Topic Modeling:}
Following the methodology of MMC4~\cite{mmc4}, we applied Latent Dirichlet Allocation (LDA)~\cite{lda} to examine the diversity of topics in our \DatasetName. 
LDA provides insights into the relative proportions of each topic and the most frequently associated terms.
From these 20 topics, we selected the 10 topics with the highest proportions for detailed analysis, as presented in Table~\ref{tab:lda_topics}.
This table highlights the dataset’s content diversity, covering topics such as Signs, Tickets, and Community. 
Interestingly, positional information is the most prominently represented category, which highlights that our dataset contains extensive positional data, a crucial element for understanding and processing interleaved content.
To enhance the interpretability of the results, we employed GPT-4o to generate concise topic names based on the top 20 associated words for each topic. 
The full list of topics and their frequencies are in Appendix~\ref{sec:all_lda}.





\input{Appendix/images/text_to_image_examples}


\input{tables/long_text_comparison}


\subsection{Qualitative Assessment of Interleaved Samples}

\paragraph{StyledTextSynth}:
154 images involve 15 topics. 
No watermarks or NSFW images were found. The OCR test results are as follows:
In some topics, such as academic reports, the text in the images has a clear contrast with the background and a relatively large font size, resulting in a high OCR recognition rate. However, when different but still distinct font colors overlap, the OCR results become inaccurate. 
Additionally, erroneous fonts generated by SD3.5 also affect OCR performance. 
Moreover, environmental lighting in the images can interfere with OCR accuracy. 
In topics with poor rendering quality, such as booklet pages, the OCR results tend to deteriorate accordingly.

\paragraph{TextScenesHQ:} 
We randomly sampled 200 images covering 23 topics, of which 4.0\% found watermarks, but no NSFW images were detected. OCR recognition tests show that when the text is small or the contrast with the background is not obvious, the recognition accuracy decreases;
Quantitative analysis reveals 22.3\% OCR accuracy degradation (from 89.4\% to 67.1\%) when text-background contrast drops below 30\% RGB—a critical threshold for model robustness evaluation.

In addition, some text is truncated due to blur or being located at the edge of the picture, affecting the recognition effect. For artistic words, the OCR recognition ability is poor, especially when the objects are designed as artistic words, they can hardly be correctly recognized, while artistic words similar to printed text have relatively good results.

Case studies show calligraphic text achieves 58.2\% recognition rate versus 92.7\% for standard fonts, exposing current models' typographic generalization limits.
At the same time, among these 200 pictures, 14.0\% of the pictures contain portraits (single portraits, group photos, advertising portraits and video covers), and 7.5\% of the pictures contain pictures with logos.


\subsection{Visual-Linguistic Similarity Assessment}
In this experiment, we analyze image-text similarity by using text as queries and images as candidates. 
We utilize the CLIP model to compute matching scores between text and images. 
Specifically, we use the open-CLIP~\footnote{https://github.com/mlfoundations/open\_clip} implementation of the ViT-B-32 model trained on the LAION-2B dataset.
The results are presented in Figure~\ref{fig:clip score subdataset}. 
We observe that the CLIP scores for LongWordsSubset-A, LongWordsSubset-M, and Cover Book datasets are higher compared to other subsets. 
Notably, these splits include image captions, which likely contribute to the higher alignment.

In contrast, our generated interleaved data exhibits lower matching scores. 
This suggests that the interleaved format is less optimized for the CLIP model, presenting a challenge for off-the-shelf models to effectively align text and images in this format.
Interestingly, for the Arxiv Paper subset, the alignment scores are higher than those of the interleaved data. 
This indicates that the CLIP model, trained on image-text data, possesses some OCR-like capabilities, enabling it to recognize and align textual elements within documents.




\input{figures/result_special_case}

\section{Evaluate existing models}

We include in total 6 text-to-image generation models for evaluation in this section.
Specifically, AnyText~\cite{anytext}, 
PixArt-$\Sigma$~\cite{chen2024pixart},TextDiffuser2~\cite{textdiffuser2}, Infinity~\cite{han2024infinity} GPT4o~\cite{gpt4o} with Dall E3~\cite{dalle3} and SD 3.5 Large~\cite{sd3}.
In this experiment, we compare these models on the long text image generation task.

\paragraph{Text-to-Image Generation Visualization}
To provide a clearer understanding, we present visual comparisons in Figure~\ref{fig:t2i_evaluation}.
Our observations highlight that previous text-to-image generation methods struggle with rendering dense text. 
For instance, AnyText~\cite{anytext} renders only a handful of words, while Text Diffuser 2~\cite{textdiffuser2} captures only part of the text.
In contrast, GPT-4 with DALL-E 3 demonstrates the best performance in text rendering. 
These results underscore that dense-text image generation remains a challenging task for current models.

\paragraph{Metric comparison.}
To further evaluate the model performance, we test the current state-of-the-art models on \EvalDatasetName.
We evaluate the commonly used Fréchet Inception Distance (FID) to measure the similarity between generated images and ground truth images. 
A lower FID score indicates that the generated images are visually more similar to the real images.
For the evaluation of CLIP score, we use the CLIP-ViT-B/32~\footnote{https://github.com/mlfoundations/open\_clip} model to calculate the similarity between generated images and text. 
A higher CLIP score indicates stronger alignment between the generated image and its corresponding text description.

In the evaluation of OCR-related results, we use PaddleOCR as the OCR detector to assess the relationship between the OCR results of generated images and the real text. 
We separately test the OCR accuracy, F1 score, and Character Error Rate (CER) of the generated images. Specifically, for accuracy and F1 score, we use word-level evaluation to check whether the generated text can correctly represent readable words, allowing for up to 80\% mismatch between words. 
Higher accuracy and F1 scores indicate stronger consistency between the generated text and the real text. 
For CER, we compare the full OCR detection results with the ground truth text to calculate the character error rate. 
A lower CER indicates a higher match between the text in the generated image and the real text.


The results are presented in Table~\ref{tab:gen_results}.  
Since AnyText and TextDiffuser2 do not support interleaved format inputs, we exclude their results on TextVisionBlend.  
For the StyledTextSynth and TextScensHQ splits, we observe that SD-3.5~\cite{sd3} achieves significantly better performance, indicating its strong generalization ability and suitability across different scene types.

It is worth noting that although SD-3.5~\cite{sd3} Large significantly outperforms PixArt~\cite{pixart} and Infinity~\cite{han2024infinity} in OCR-related scores on the TextVisionBlend subset, its FID and CLIP scores are lower than those of the other two models. To better understand this phenomenon, we present two representative cases in Figure~\ref{fig:special case}, analyzing the differences in model performance on this subset.

In the first row of Figure~\ref{fig:special case}, SD-3.5 fails to capture the interleaved image layout and does not render text well, whereas both Infinity and PixArt follow the interleaved structure and white-background requirement, despite their poor text quality. This may explain SD-3.5’s lower CLIP and FID scores. Meanwhile, in the second row, all three models exhibit interleaved characteristics, but only SD-3.5 generates relatively complete text in the image. This likely contributes to its strong OCR-related performance.

Overall, when generating images with complex requirements, SD-3.5 performs poorly in terms of image layout and certain specifications. We speculate that this may be related to the model's supported input text length. PixArt-Sigma can accommodate up to 300 text tokens, while Infinity, as an autoregressive generation model, supports even longer text inputs. A greater text input capacity may provide an advantage in understanding complex instructions.

