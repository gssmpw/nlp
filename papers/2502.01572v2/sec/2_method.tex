
\section{Method}
% 3.1节，我们介绍MakeAnything 方法的整体架构，3.2节介绍Procedural LoRA，通过拼图策略和基于DIT的小样本训练实现过程生成 3.3节介绍ReCraft Model， 一种有效的Image condition model 生成和参考图像高度一致的过程序列 3.4节介绍数据收集方法和合成数据管线。

In this section, we begin by exploring the preliminaries on diffusion transformer as detailed in Section 3.1. Next, Section 3.2 introduce the overall architecture of the MakeAnything method. In Section 3.3, we present asymmetric LoRA for Procedual learning. Section 3.4 introduces the ReCraft Model, an effective image condition model that generates procedural sequences highly consistent with reference images. Finally, we introduce the new dataset we proposed in Section 3.5.



\subsection{Preliminary}

The Diffusion Transformer (DiT) model,  uses a transformer as the denoising network to iteratively refine noisy image tokens. A DiT model processes two types of tokens: noisy image tokens $z \in \mathbb{R}^{N \times d}$ and text condition tokens $c_T \in \mathbb{R}^{M \times d}$, where $d$ is the embedding dimension, and $N$ and $M$ are the number of image and text tokens. Throughout the network, these tokens maintain consistent shapes as they pass through multiple transformer blocks.

In FLUX.1, each DiT block consists of layer normalization followed by Multi-Modal Attention (MMA)~\cite{mma}, which incorporates Rotary Position Embedding (RoPE)~\cite{rope} to encode spatial information. For image tokens $z$, RoPE applies rotation matrices based on the token's position $(i,j)$ in the 2D grid:
\begin{equation}
z_{i,j} \rightarrow z=z_{i,j} \cdot R(i,j),
\end{equation}
where $R(i,j)$ is the rotation matrix at position $(i,j)$. Text tokens $c_T$ undergo the same transformation with their positions set to $(0,0)$.

The multi-modal attention mechanism then projects the position-encoded tokens into query $Q$, key $K$, and value $V$ representations. It enables the computation of attention between all tokens:
\begin{equation}
\text{MMA}([z; c_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
\end{equation}
where $[z; c_T]$ denotes the concatenation of image and text tokens. This formulation enables bidirectional attention.

\subsection{Overall Architecture}

As shown in Fig. \ref{fig2}, the training of MakeAnything is divided into two stages: First, we train on the MakeAnything dataset using the asymmetric LoRA method, enabling the generation of creative tutorials from text descriptions. Then, the LoRA from this first phase is merged with the Flux base model to form the base model for training the ReCraft Model. In the second stage, image condition tokens are concatenated with noised latent tokens, introducing an image-conditioned mechanism into the denoising process. This setup is further fine-tuned using LoRA to complete the training of the ReCraft Model.

%如图2所示 MakeAnything的训练分为两阶段：首先，我们在MakeAnything数据集上使用不对称LoRA方法进行训练，使其能够从文本描述生成创作教程。接着，第一步中的LoRA与Flux基础模型合并，得到ReCraft模型训练的base model。第二步，通过将image condition tokens 与noised latent tokens 拼接，为去噪过程引入图像条件机制，并通过进一步的LoRA微调实现ReCraft模型的训练。


\subsection{Asymmetric LoRA for Procedural Learning}

% MakeAnything 的核心是将教程的不同帧排列成grid，利用DiT的In-context 能力和注意力机制实现一致性的教程生成。DiT的注意力机制中的令牌倾向于关注空间上相邻的令牌。这种倾向源于扩散模型在预训练过程中捕获的邻近图像像素之间的强相关性。为了提升模型对Grid序列的学习效果，我们提出Serpentine dataset construction方法, 如图2所示，我们将9帧和4帧序列按照蛇形排列成网格，以保证时序上相邻的两帧在空间上也是相邻的（横向相邻或纵向相邻）。

% 另一个挑战是，用所有数据混训一个LORA会导致学习多样化知识时会遇到困难， 仅在单一类型的序列数据上训练LoRA会导致过拟合， 因为每一类过程数据数量有限。为此，我们首次在图像生成中引入不对称的LoRA设计，通过联合训练一个共享的中心矩阵A和多个独立的矩阵B来组合共享知识和专门功能，有效提升了多任务性能。LoRA的每一层由一个A矩阵和一个B矩阵组成，其中A矩阵用于捕捉通用知识，B矩阵则针对特定任务进行适配。这种不对称的LoRA架构可以被表述为：

% 完成训练后，domain-specific矩阵B和domain-agnostic矩阵A组合使用， 兼顾泛化能力和特定任务的性能。本文方法还可以和常规LoRA（非过程LoRA）组合使用， 提升在Unseen domain的效果。

\noindent \textbf{Serpentine Sequence Layout.}  The core of MakeAnything involves arranging different frames of a sequence into a grid and using the in-context capabilities and attention mechanism of DiT to achieve consistent Sequence generation. Tokens within the DiT's attention mechanism tend to focus on spatially adjacent tokens, a tendency that stems from the strong correlations between neighboring image pixels captured during the pre-training of the diffusion model \cite{grid}. To enhance the model's learning effectiveness for grid sequences, we propose the Serpentine dataset construction method. As shown in Fig. \ref{fig3}, we arrange sequences of 9 frames and 4 frames in a serpentine pattern to ensure that temporally adjacent frames are also spatially adjacent (either horizontally or vertically adjacent).

\noindent \textbf{Asymmetric LoRA.} Another challenge is that training a single LoRA on all data leads to difficulties in learning diverse knowledge, while training LoRA on a single type of sequence data results in overfitting due to the limited quantity of process data for each category. Inspired by HydraLoRA \cite{asymmetry}, we introduce an asymmetric LoRA design for the first time in image generation. This design combines shared knowledge and specialized functionalities by jointly training a shared central matrix \(A\) and multiple task-specific matrices \(B\), significantly improving multi-task performance. 

Each layer of LoRA consists of an \(A\) matrix and a \(B\) matrix, where \(A\) captures general knowledge, and \(B\) adapts to specific tasks. The asymmetric LoRA architecture can be formulated as:
\begin{equation}
W = W_0 + \Delta W = W_0 + \sum_{i=1}^{N} \omega_i \cdot B_iA,
\end{equation}
where \(B_i \in \mathbb{R}^{d \times r}\) and the shared matrix \(A \in \mathbb{R}^{r \times k}\). This structure effectively balances generalization and task-specific adaptation, enhancing the model's performance across diverse tasks.

Inference stage, the domain-specific matrix B and the domain-agnostic matrix A are used in combination, balancing generalization capabilities with performance on specific tasks.  Our method can also be combined with the stylized LoRA from the Civitai website (which is not trained on procedural sequences), to enhance performance in unseen domains.

% 在推理阶段，特定领域的矩阵B和领域无关的矩阵A被组合使用，以平衡泛化能力和特定任务的性能。我们的方法还可以与Civitai网站的风格化LoRA（未在过程序列上训练的）结合使用，以提升在未见领域的性能。

\noindent \textbf{Conditional Flow Matching Loss.} The conditional flow matching loss function is following SD3 \cite{sd3}, which is defined as follows:
\begin{equation}
L_{CFM} = E_{t, p_t(z|\epsilon), p(\epsilon)} \left[ \left\| v_\Theta(z, t, c_T) - u_t(z|\epsilon) \right\|^2 \right]
\end{equation}
Where $ v_\Theta(z, t, c_T)$ represents the velocity field parameterized by the neural network's weights, $t$ is timestep, $c_I$ and $c_T$ are image condition tokens extracted from source image $I_{src}$ and text tokens. $u_t(z|\epsilon)$ is the conditional vector field generated by the model to map the probabilistic path between the noise and true data distributions, and $E$ denotes the expectation, involving integration or summation over time $t$, conditional $z$, and noise $ \epsilon $. 



\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{images/Datasetf.pdf} % Replace with your image file
    \caption{Examples from the MakeAnything Dataset, which consists of 21 tasks with over 24,000 procedural sequences.}
    \label{fig3}
\end{figure*}

% MakeAnything 数据集的一些例子展示。 MakeAnything包含了21个任务， 共10000+高质量的创建序列。






\subsection{ReCraft Model}
% 在实际应用中，用户除了从文本生成创建过程，更希望能上传一张图片， 预测图片中现有的画作/手工艺品的制作过程。为此，我们实现了ReCraft model， 允许用户上传图像，ReCraft Model生成和上传图像高度一致的过程序列。

% ReCraft model训练的难点是, 对于每一个任务，数据集的数量相当有限，对于从0初始化训练一个可控性插件，如Controlnet或Adapter远远不够。 为此，我们创新的设计了ReCraft model， 通过复用预训练的Flux模型并通过最少的改动，使其扩展为image condition generation model。具体来说，训练环节，我们将最后一帧输入VAE获得latent作为image condition tokens，并与noised latent token 拼起来， 通过注意力机制为其他帧的去噪提供condition信息。 值得一提的是， 加噪和去噪过程仅在其他帧进行，而image condition tokens 是clean的。推理环节，我们通过尾帧预测出前8帧， 即参考图中的物体是如何一步步得到的。

% We aim not only to generate creation processes from text but also to introduce an image-conditioned model capable of predicting the making process of existing artworks or crafts. To achieve this, we developed the ReCraft model, which allows users to upload an image, and the ReCraft model generates a process sequence that closely aligns with the uploaded image.

% The main challenge in training the ReCraft model lies in the limited availability of high-quality sequences. While LoRA is sufficient for small-sample training, initializing a controllable plugin like ControlNet or Adapter from scratch is far from adequate. To address this, we innovatively designed the ReCraft model by repurposing the pre-trained Flux model and adapting it into a conditional generation model that accepts image context. 

% Specifically, during training, we first arrange the creation process sequence into logically ordered 2×2 grid or 3×3 grid. The conditinal image is then fed into a VAE to obtain its latent representation, which is directly appended to the denoising latent at the end.

In practical applications, users not only want to generate creation processes from text but also wish to upload an image and predict the creation process of the existing artwork or handicraft in the picture. For this, we implemented the ReCraft model, which allows users to upload images and generates a sequence of steps highly consistent with the uploaded image.

A major challenge in training the ReCraft model is the limited number of datasets available for each task, which is insufficient to train a controllable plugin like Controlnet or IP-Adapter from scratch. To address this, we innovatively designed the ReCraft model by reusing the pretrained Flux model and making minimal modifications to extend it into an image-conditioned generation model. Specifically, during training, we input the final frame into a VAE to obtain latent image condition tokens, which are then concatenated with noised latent tokens, using the attention mechanism to provide conditional information for denoising other frames. Notably, the noise addition and removal process are only performed on other frames, while the image condition tokens are clean. During inference, we predict the previous eight frames from the tail frame, revealing step-by-step how the object in the reference image was formed.

In ReCraft model, multi-modal attention mechanisms are used to provide conditional information for the denoising of other frames. 
\begin{equation}
\text{MMA}([z; c_I; c_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
\end{equation}
where $[z; c_I; c_T]$ denotes the concatenation of image and text tokens. This formulation enables bidirectional attention.

The conditional flow matching loss with image condition can be defined as follows:
\begin{equation}
L_{CFM} = E_{t, p_t(z|\epsilon), p(\epsilon)} \left[ \left\| v_\Theta(z, t, c_I,c_T) - u_t(z|\epsilon) \right\|^2 \right]
\end{equation}
Where $ v_\Theta(z, t, c_I,c_T)$ represents the velocity field parameterized by the neural network's weights.

During inference, Recraft Model predict previous 8 frames based on the final frame. This predicts how the object in the reference image was created step by step.




\begin{figure*}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{images/T2If.pdf} % Replace with your image file
    \vspace{-3mm}
    \caption{Generation results of MakeAnything. From top: \textbf{Text-to-Sequence} outputs conditioned on textual prompts; \textbf{Image-to-Sequence} reconstructions via ReCraft Model; \textbf{Unseen Domain} generalization combining procedural LoRA (blue) with stylistic LoRA (red).  }
    \label{fig4}
\end{figure*}
\vspace{-1mm}

\begin{figure*}[htp]
    \centering
    \includegraphics[width=0.99\linewidth]{images/compare.pdf} % Replace with your image file
    \vspace{-3mm}
    \caption{Compare with baselines on different tasks.}
    \label{fig5}
\end{figure*}




\subsection{MakeAnything Dataset}
 
% 如图3所示，我们收集了一个多任务教程数据集， 包含21个任务的教程， 类别分别是Painting,  Sketch, Sand Art, Portrait, Icon, Landscape illutsration, illutsration, LEGO, Transformer, Cook, Clay toys, Pencil sketch, Chinese painting, Fabric toys, Oil painting, Wood Sculpture, Clay Sculpture, Brush Modeling, Jade Carving, Line draw, Emoji。我们组建了专业的数据收集和标注小组，从互联网上收集并后处理各种教程， 我们还和艺术家合作，定制高质量绘画过程数据。 不同任务的数据量不同，50至10000不等，共计24,000+. 前十个任务数据是9帧， 其他的是4帧， 分别被组成3*3和2*2的grid用于训练。我们通过GPT4-o为所有数据集打文本标签， 并对每一帧进行描述。 

As shown in Fig. \ref{fig3},  we have collected a multi-task dataset that encompasses tutorials for 21 tasks. We assembled a professional data collection and annotation team that gathered and processed various tutorials from the internet and also collaborated with artists to customize high-quality painting process data. The datasets vary in size from 50 to 10,000 entries, totaling over 24,000. The first ten tasks have data in 9 frames, while the rest have 4 frames, arranged into 3x3 and 2x2 grids for training purposes. We used GPT-4o to label all datasets and describe each frame.

% \subsection{Strategy Adapter}  
% % 本节我们挑战一项更难的任务，通过训练Strategy Encoder从单个过程中提取过程策略.

% %有别于风格和语义，过程策略是一种时序的知识和逻辑，我们不希望Strategy Encoder的提取结果耦合了过多的风格和语义信息， 为此，我们设计了交叉训练 Strategy Adapter的方案。如图X 所示，我们构造了三元组合成数据，具有相同策略的图像序列A 和B， 以及caption。 我们在Make Anything dataset上构造了100，000条三元组训练数据。将InstantX Flux IPA 作为预训练模型微调了 2个epoch。

% In this section, we address a more challenging task: extracting procedural strategies from a single process by training a Strategy Adapter. Unlike style and semantics, procedural strategies encapsulate sequential knowledge and logic. To ensure that the Strategy Encoder extracts these strategies without being influenced by style or semantic information, we designed a cross-training scheme for the Strategy Adapter. As illustrated in Figure X, we constructed triplet synthetic data consisting of image sequences A and B, which share the same strategy, along with a corresponding caption. Using the MakeAnything dataset, we generated 100,000 triplet training samples.

% To encode images, we leveraged Siglip for its superior performance, and we employed a straightforward MLPProjModel with two linear layers for projection. Once the strategy tokens were extracted, multi-modal attention mechanisms were applied to provide conditional information, as defined by the equation:
% \begin{equation}
%     \text{MMA}([X; C_S; C_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
% \end{equation}
% where $[X; C_S; C_T]$ represents the concatenation of strategy tokens ($C_S$), text tokens ($C_T$), and image tokens ($X$). This formulation enables bidirectional attention, effectively integrating strategy and textual context into the generation process.
