\section{Introduction}
%分步骤制作复杂的作品被认为是人类智慧的标志之一，也是人类区别于其他物种的重要特性。诸如绘画、手工、产品设计以及美食制作等过程的生成，一直是计算机视觉领域既具有吸引力又极具挑战性的任务。这一任务的主要难点在于如何生成符合逻辑且外观一致的过程序列，即在多个步骤之间保持连贯性。生成过程序列不仅需要模型具备捕捉复杂视觉特征的能力，还需理解过程之间的因果关系。特别是在多领域、多风格的场景下，如何在不牺牲任务广度的同时确保生成质量，是一个未被充分探索的领域。

%之前的研究探索了分步骤的绘画生成，通过训练时间序列模型（如 **ProcessPainter** 和 **PaintsUndo**）或模拟人类的绘画顺序。这些方法利用时序注意力机制从合成数据中学习绘画过程，取得了一定的进展，但大多局限于单一领域(绘画），缺乏跨领域的泛化能力。同时，**ProcessPainter** 基于 **Animatediff** 训练时序模型，这一框架限制了生成结果不会产生显著变化，不适合生成过程有显著结构和形态变化的类别，如食谱和手工制作。基于 **DIT**（Diffusion Transformer）的视频生成模型已经可以生成超过一分钟的长镜头？，然而由于训练数据的限制，仍然不能很好地生成分步骤的制作过程。

%在本文中，我们提出了一种新颖的框架 **MakeAnything**，利用 **Diffusion Transformer (DIT)** 的In-Coentext 能力实现高质量的教程序列生成。为实现这一目标，我们精心收集的多领域过程数据集，涵盖了多种类型的绘画、手工制作、SVG图形制作、乐高搭建、和烹饪等领域，共包含 21 种主题和风格，累计超过 15,000 个序列。该数据集捕捉了这些领域中分步骤的制作过程，为模型从多样化的示例中学习提供了强有力的支持。

%对于某些类别，其过程数据的数量较少（只有几十个），且类内分布不均匀，使用小样本训练 **LoRA** 会导致严重的过拟合问题。为了兼顾通用知识的学习（即泛化能力）和特定任务的效果，我们首次在图像生成中引入不对称的低秩适应（**LoRA**）设计来微调 **DIT**，通过结合在海量数据上预训练的编码器和在特定任务上微调的解码器，平衡模型的泛化能力和生成效果。

%在实际应用中，用户往往希望了解一个作品是如何一步步制作出来的， 这对教学至关重要。为此，我们提出了 **ReCraft Model**，一种高效的可控生成方法，支持将现有图像分解为制作过程序列。 我们通过精细设计的位置编码和训练策略，实现了高一致性的过程生成。

% 广泛的实验和评估结果表明，我们的方法在过程序列生成任务中表现优异，显著超越了现有方法，并为领域树立了新的性能基准。


%总的来说，本文的主要贡献包括：  
%1. **创新框架**：提出了一种新颖的框架 **MakeAnything**，利用扩散变换器中的Incontext能力，实现高质量的过程序列生成。  
%2. **技术贡献**：在模型训练中采用不对称 **LoRA** 设计，兼顾模型的泛化能力和生成质量。提出了高效的 **ReCraft Model**，支持从图像推理制作过程的序列。 提出 Strategy Adapter从单个过程中提取知识。
%3. **数据集贡献**：构建了一个包含 15 类别、5,000 个序列的大规模数据集，填补了该领域数据的空白，显著提升了模型训练和评估的多样性。  
%4. **应用价值**：在多个下游任务中验证了方法的优越性，为分步骤制作过程的研究和应用提供了新的思路，具有突出的实用价值。通过 **MakeAnything**，我们不仅扩展了分步骤生成任务的研究边界，也为多领域生成过程的统一建模提供了新的范式。  


% 人类智慧的一个重要标志，也是区别于其他物种的关键特征之一，是能够通过分步骤的方式创造出复杂的作品。在计算机视觉领域，诸如绘画、手工、产品设计以及美食制作等过程的生成一直备受关注且颇具挑战。核心难点在于如何生成在逻辑上连续、视觉上一致的多步骤序列，而这不仅需要模型具备捕捉复杂视觉特征的能力，还要求它能理解步骤间的因果关系。尤其是在多个领域和风格并存的情况下，如何在不牺牲任务广度的同时确保生成质量，依然是尚未得到充分探索的问题。

% 已有的相关研究主要聚焦于将绘画过程拆解为多个步骤，早期方法通过强化学习/优化算法 通过Stroke based rendering拟合输入图像。ProcessPainter 和 PaintsUndo在合成数据集上利用时间序列模型，Inverse Painting 预测人类绘画的顺序分区域生成绘画过程。这些方法仍仍局限于单一任务，缺乏跨领域的泛化能力。此外，受限于 Animatediff 框架在生成时仅能做微小的运动幅度和修改，ProcessPainter方案不适合那些在结构和形态上具有显著变化的类别（如食谱或手工艺）。此外，基于 DIT（Diffusion Transformer）的视频生成模型虽能生成长视频，但受限于训练数据的分布差异，难以生成分步骤的复杂制作过程。

%我们认为，学习人类制作各种物品的智慧，需要高质量多任务过程数据和先进模型架构设计双重保障。为此，我们精心收集了一个多领域的制作过程数据集，涵盖绘画、手工、SVG 设计、乐高搭建以及烹饪等领域，共包含 21 个主题和风格，超过 15,000 个序列，为模型提供了从丰富案例中学习的深厚基础。方法层面，本文提出了一个全新的框架——MakeAnything，通过LoRA微调激活Diffusion Transformer (DIT) 的 In-Context 能力， 从而生成高质量的教学过程序列。

% 然而，对于数据集的一些类别中过程数据极度稀少（少则几十个），且类内分布不均，若仅依赖小样本 LoRA 训练就会出现严重的过拟合。为兼顾通用知识学习的泛化能力与特定任务表现，我们首次在图像生成中引入了不对称的低秩适应（LoRA）策略：通过将在海量数据上预训练过的编码器与专门针对目标任务微调的解码器结合起来，平衡了模型的泛化能力与生成质量。

% 为了满足实际应用中对创作过程逆向工程的需求，我们开发了ReCraft模型——一种高效的可控生成方法，能够将静态图像分解成逐步的过程序列。在对预训练的Flux模型进行最小的结构修改基础上，ReCraft引入了一种图像条件机制，其中目标图像的清晰潜在标记（通过VAE编码）指导中间帧的去噪，通过多模态注意力实现。这种轻量级的调整使得在数据有限的情况下也能有效地进行训练——ReCraft即使只有每个任务几百甚至几十个过程序列，也能实现强大的性能。在推理过程中，模型通过双向注意力机制递归地预测前面的帧，有效地从静态艺术作品中重构创作历史。


% 综上所述，本文的主要贡献包括：
% 新颖框架：提出 MakeAnything，利用扩散变换器的 In-Context 能力，实现高质量的分步骤创作序列生成。
% 技术创新：在模型训练中采用不对称的 LoRA 设计，兼顾泛化和生成质量，提出 ReCraft Model 用于从单幅图像推理制作过程，并引入Strategy Adapter 从单个样例中抽取可推广的策略。
% 数据集贡献：构建了一个包含 15 类别、5,000 个序列（以及 21 个主题风格、超过 15,000 个序列）的数据集，极大提升了此领域训练与评估的多样性。

A defining characteristic of human intelligence—and a key differentiator from other species—is the capacity to create complex artifacts through structured step-by-step processes. In computer vision, generating such procedural sequences for tasks like painting, crafting, product design, and culinary arts remains a significant challenge. The core difficulty lies in producing multi-step sequences that maintain logical continuity and visual consistency, requiring models to both capture intricate visual features and understand causal relationships between steps. This challenge becomes particularly pronounced when handling diverse domains and styles without compromising generation quality—a problem space that remains underexplored.

Existing research primarily focuses on decomposing painting processes, with early methods employing reinforcement learning/optimization algorithms through stroke-based rendering to approximate target images. Subsequent works like ProcessPainter \cite{processpainter} and PaintsUndo \cite{paintsundo} utilize temporal models on synthetic datasets, while Inverse Painting \cite{inverse}redicts the order of human painting, generating the painting process by region. However, these approaches remain limited to single-task scenarios and exhibit poor cross-domain generalization. Furthermore, ProcessPainter's Animatediff-based framework constrains modifications to minor motion adjustments, making it unsuitable for categories requiring structural transformations (e.g., recipes or crafts). Although Diffusion Transformer (DIT) \cite{dit}-based video generation models can produce long sequences, their effectiveness is hindered by distribution shifts in training data when generating complex procedural workflows.

We posit that replicating human creative intelligence requires both high-quality multi-task procedural data and advanced methodology design.. To this end, we curate a comprehensive multi-domain dataset spanning 21 categories (including painting, crafts, SVG design, LEGO assembly, and cooking) with over 24,000 procedurally annotated sequences—the largest such collection for step-by-step creation tasks. Methodologically, we propose MakeAnything, a novel framework that harnesses the in-context capabilities of Diffusion Transformers (DIT) through LoRA fine-tuning to generate high-quality instructional sequences.

Addressing the challenge of severe data scarcity (some categories have as few as 50 data entries.) and imbalanced distributions, we employ an asymmetric low-rank adaptation (LoRA)\cite{asymmetry, lora} strategy for image generation. This approach combines a pretrained encoder on large-scale data with a task-specific fine-tuned decoder, achieving an optimal balance between generalization and domain-specific performance.

 To address practical needs for reverse-engineering creation processes, we develop the ReCraft Model—an efficient controllable generation method that decomposes static images into step-by-step procedural sequences. Building upon the pretrained Flux model with minimal architectural modifications, ReCraft introduces an image-conditioning mechanism where clean latent tokens from the target image (encoded via VAE) guide the denoising of noisy intermediate frames through multi-modal attention. Remarkably, this lightweight adaptation enables efficient training with limited data—ReCraft achieves robust performance with just hundreds or even dozens of process sequences per task. During inference, the model recursively predicts preceding frames over concatenated latent representations, effectively reconstructing the creation history from static artworks. 
 % Extensive experiments demonstrate that our framework outperforms existing methods by significant margins, establishing new state-of-the-art benchmarks for procedural generation.

In summary, our contributions are as follows:  
\begin{enumerate}  
\item \textbf{Unified Procedural Generation Framework}: We introduce MakeAnything, the first DIT-based architecture enabling cross-domain procedural sequence synthesis, supporting both text-to-process and image-to-process generation paradigms.
\item \textbf{Technical Innovations}: We employ an asymmetric LoRA architecture for cross-domain generalization and the ReCraft Model for image-conditioned process reconstruction with limited training data.
\item \textbf{Dataset Contribution}: We propose a multi-domain procedural dataset (21 categories, 24K+ sequences) with hierarchical annotations, significantly advancing research in procedural understanding and generation. 
\end{enumerate}  




% One of the hallmarks of human intelligence—and a key trait that sets us apart from other species—is the ability to create complex artifacts in a step-by-step manner. Generating such processes for tasks like painting, crafting, product design, and culinary arts has long been an attractive yet challenging objective in computer vision. The primary difficulty lies in producing coherent sequences of steps that remain logically consistent and visually uniform. Achieving this not only requires a model capable of capturing intricate visual features, but also demands an understanding of causal relationships among the steps. Notably, ensuring high-quality generation across multiple domains and styles, without sacrificing the breadth of tasks, remains an underexplored research direction.

% Early studies in this area explored multi-step image generation for painting by training time-series models (e.g., ProcessPainter and PaintsUndo) or by simulating the human painting process. While these works leveraged temporal attention mechanisms on synthetic data and demonstrated some success, they were largely restricted to the painting domain and exhibited limited cross-domain generalization. Moreover, ProcessPainter is built upon Animatediff, whose training framework constrains it to produce only minor changes across frames, making it unsuitable for processes with significant structural or morphological variations (e.g., recipes or intricate crafts).


% In this paper, we introduce a novel framework called MakeAnything, leveraging the in-context capabilities of Diffusion Transformer (DIT) to produce high-quality instructional sequences. To achieve this goal, we curated a comprehensive multi-domain process dataset encompassing diverse categories such as painting, crafting, SVG art creation, LEGO building, and cooking. Spanning 21 distinct themes and styles, with more than 15,000 sequences in total, this dataset captures step-by-step procedures across numerous scenarios, providing a rich source of examples to facilitate model learning.

% However, for certain categories with fewer samples (sometimes only a few dozen), limited data and uneven distributions lead to severe overfitting when using small-sample LoRA training. To balance broad knowledge acquisition (i.e., generalization) with task-specific efficacy, we introduce an asymmetric low-rank adaptation (LoRA) strategy to fine-tune DIT. By combining an encoder pretrained on large-scale data with a decoder fine-tuned on the targeted tasks, our method reconciles generalization capability with generation quality.

% In practical settings, users often want to see how an item is made, step by step—a critical feature for educational purposes. To address this, we propose the ReCraft Model, an efficient controllable generation method that decomposes existing images into step-by-step creation sequences. Through carefully designed positional encodings and training strategies, ReCraft achieves highly consistent process generation.

% To further enhance the generalization of models trained on limited process data and tackle unseen tasks, we present the Strategy Adapter. Pretrained on abundant data pairs sharing the same strategy, it can replicate the procedure of a reference sequence without requiring additional fine-tuning, given only a single example. Extensive experiments and evaluations demonstrate that our approach outperforms existing methods by a considerable margin and sets a new performance benchmark for procedural generation.

% In summary, our contributions are as follows:  
% \begin{enumerate}  
% \item Innovative Framework: We propose a novel framework, MakeAnything, which leverages the self-attention mechanism in diffusion transformers to achieve high-quality procedural sequence generation.  
% \item Technical Contributions: We adopt an asymmetric LoRA design during model training to balance generalization and generation quality. Additionally, we introduce the efficient ReCraft Model, which enables inferring procedural sequences from images.  
% \item New Dataset: We construct a large-scale dataset containing 21 categories and 15,000 sequences, addressing the data scarcity in this domain and significantly enhancing diversity for model training and evaluation.  
% % \item Practical Value: Our method demonstrates superior performance across multiple downstream tasks, offering significant practical value and expanding the research boundaries of step-by-step generation tasks. Through MakeAnything, we propose a unified modeling paradigm for multi-domain procedural generation.  
% \end{enumerate}  

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\linewidth]{icml2025/images/Method.pdf} % Replace with your image file
%     \caption{\textbf{Schematic Structure of MakeAnything.} MakeAnything consists of three components: Procedural LoRA, the ReCraft Model, and the Strategy Adapter. 
%     (1) We first pre-train using the LoRA method on the MakeAnything dataset, enabling the generation of various creation processes from text descriptions. 
%     (2) The LoRA from the first step is merged with the Flux base model to create a new base model. The ReCraft Model introduces an image-conditioned mechanism and achieves the prediction of creation processes for reference images through further LoRA fine-tuning. 
%     (3) Finally, we train the Strategy Adapter on the MakeAnything dataset, which can extract creation strategies from new procedural sequences.}
%     \label{fig:makeanything_structure}
% \end{figure*}


\vspace{-0.5em}
\section{Related Work}

\subsection{Diffusion Models}
Diffusion probability models \citep{ddim,ddpm} are advanced generative models that restore original data from pure Gaussian noise by learning the distribution of noisy data at various levels of noise. Their powerful capability to adapt to complex data distributions has led diffusion models to achieve remarkable success across several domains including image synthesis \citep{rombach2022high,dit}, image editing \citep{instructpix2pix,p2p,stablemakeup,stablehair}, and video gneration \citep{animatediff, svd, processpainter}, evaluation \citep{diffsim}. Stable Diffusion \citep{rombach2022high} (SD), a notable example, utilizes a U-Net architecture and extensively trains on large-scale text-image datasets to iteratively generate images with impressive text-to-image capabilities. The Diffusion Transformer (DiT) model \cite{dit}, employed in architectures like FLUX.1 \cite{flux}, Stable Diffusion 3 \cite{sd3}, and PixArt (pixart), uses a transformer as the denoising network to iteratively refine noisy image tokens. Customized generation methods enable flexible customization of concepts and styles by fine-tuning U-Net \citep{dreambooth} or certain parameters \citep{lora, customdiffusion}, alongside trainable tokens. Training-free customization methods \citep{ipa, ssr, fast} leverage pre-trained CLIP \citep{clip} encoders to extract image features for efficient customized generation. 

% 扩散概率模型/cite{}是先进的生成模型，通过学习各种噪声水平下噪声数据的分布，从纯高斯噪声中恢复原始数据。由于其强大的适应复杂数据分布的能力，扩散模型在多个领域取得了卓越的成就，包括图像合成/cite{}、图像编辑/cite{}、视频生成/cite{}。其中一个著名的例子是稳定扩散模型（Stable Diffusion，SD）\citep{rombach2022high}，该模型采用 U-Net 架构，并在大规模文本图像数据集上进行广泛训练，以迭代方式生成具有令人印象深刻的文本到图像转换能力的图像。扩散变换器（DiT）模型（?），用于如FLUX.1（?）、稳定扩散3（?）和PixArt（?）等架构中，采用变换器作为去噪网络来迭代地细化噪声图像标记。客制化生成方法通过微调U-Net或部分参数，以及使用可训练的token，实现了概念和风格的灵活定制。一些无需训练的客制化方法，如IPA、InstantID、InstantStyle等，利用预训练的CLIP或Arcface编码器提取图像特征，并通过Adapter结构将其注入U-Net的cross-attn层，实现了高效的客制化生成。

\begin{figure*}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{images/Method.pdf}
    \vspace{-6mm}
    \caption{The MakeAnything framework comprises two core components: (1) an Asymmetric LoRA module that generates diverse creation processes from text prompts through asymmetric LoRA, and (2) the ReCraft Model, which constructs an image-conditioned base model by merging pretrained LoRA weights with the Flux foundation model, enabling process prediction via injected visual tokens.}
    \label{fig2}
\end{figure*}
\vspace{-5mm}

\subsection{Controllable Generation in Diffusion Models}

Controllable generation has been extensively studied in the context of diffusion models. Text-to-image models \cite{ddpm, ddim} have established a foundation for conditional generation, while various approaches have been developed to incorporate additional control signals such as images. Notable methods include ControlNet~\cite{controlnet}, enabling spatially aligned control in diffusion models, and T2I-Adapter \cite{t2i}, which improves efficiency with lightweight adapters. UniControl \cite{unicontrol} uses Mixture-of-Experts (MoE) to unify different spatial conditions, further reducing model size.  However, these methods rely on spatially adding condition features to the denoising network’s hidden states, inherently limiting their effectiveness for spatially non-aligned tasks like subject-driven generation. IP-Adapter \cite{ipa} addresses this by introducing cross-attention through an additional encoder. Based on the DiT architecture, OminiControl \cite{ominicontrol} proposes a unified solution that is applicable to both spatially aligned and non-aligned tasks by concatenating condition tokens with noise tokens.


% 可控生成在扩散模型的背景下已被广泛研究。文本到图像模型~\cite{ho2021denoising, ramesh2022hierarchical}为条件生成奠定了基础，同时开发了各种方法以纳入额外的控制信号，如图像。其中值得注意的方法包括 ControlNet~\cite{zhang2023controlnet}，它使扩散模型能够进行空间对齐控制，以及 T2I-Adapter~\cite{mou2023t2i}，该方法通过轻量级适配器提高了效率。然而，这些方法依赖于将条件特征空间地添加到去噪网络的隐藏状态中，这在本质上限制了它们在空间非对齐任务（如主题驱动生成）中的有效性。IP-Adapter~\cite{liu2023ipadapter} 通过引入一个额外的编码器并使用交叉注意力机制来解决这一问题。基于DiT架构， OminiControl提出了一个同时适用于空间对齐和非对齐任务的统一解决方案，通过将condition token和加噪token连接。



% MakeAnything 的结构示意图。由三部分组成，Procedural LoRA， ReCraB Model  和 Strategy Adapter 。 1. 我们首先在MakeAnything dataset上采用LoRA的方式进行预训练， 实现了从文本生成多种制作过程。 2.将第一步的LORA和Flux base model merge作为新的base model，ReCraft Model引入图像条件机制，通过LoRA微调的方式实现了预测参考图的制作过程。 3. 我们在MakeAnything dataset上训练了 Strategy Adapter， 可以从新的制作过程中提取制作策略。




\subsection{Procedural Sequences Generation}

Generating the creation process of paintings or handicrafts is something that has always been desired but is difficult to achieve. The problem of teaching machines "how to paint" has been thoroughly explored within stroke-based rendering (SBR), focusing on recreating non-photorealistic imagery through strategic placement and selection of elements like paint strokes \cite{hertzmann2003survey}. Early SBR methods included greedy searches or required user input \cite{haeberli1990paint,litwinowicz1997processing}, while recent advancements have utilized RNNs and RL to sequentially generate strokes \cite{ha2017neural,zhou2018learning,xie2013artist}. Adversarial training has also been introduced as an effective way to produce non-deterministic sequences \cite{nakano2019neural}. Techniques like Stylized Neural Painting \cite{ kotovenko2021rethinking} have advanced stroke optimization, which can be integrated with neural style transfer. The field of vector graphic generation employs similar techniques \cite{clipdraw, clipvg, cliptexture, clipfont}. However, these methods differ greatly from human creative processes due to variations in artists' styles and subjects. Inverse Painting \cite{inverse} achieves realistic painting process simulation by predicting the painting order and implementing image segmentation. ProcessPainter \cite{processpainter} and Paints Undo \cite{paintsundo} method fine-tunes diffusion models using data from artists' painting processes to learn their true distributions, enabling the generation of painting processes in multiple styles. 

% 生成绘画或手工艺品的创作过程一直是人们所期望但难以实现的。教会机器“如何绘画”的问题已在基于笔触的渲染（SBR）中被彻底探索，这一过程专注于通过策略性放置和选择元素如绘画笔触来重现非真实感图像 \cite{hertzmann2003survey}。早期SBR方法包括贪婪搜索或需要用户输入 \cite{haeberli1990paint,litwinowicz1997processing,hertzmann2022toward, 3Dstroke}，而最近的进展已经利用RNNs和RL来顺序生成笔触 \cite{ha2017neural,zhou2018learning,xie2013artist,singh2022intelli}。对抗训练也被引入作为生成非确定性序列的有效方法 \cite{nakano2019neural}。如《风格化神经绘画》\cite{snp, kotovenko2021rethinking}等技术已推进笔触优化，可以与神经风格转移集成。矢量图形生成领域采用了类似的技术 \cite{clipdraw, clipvg, cliptexture, clipfont}。然而，这些方法因艺术家风格和主题的变化而与人类创造过程大相径庭。Inverse Painting通过预测绘画绘画顺序，和图像分割实现了逼真的绘画过程模拟。  ProcessPainter和Paints Undo方法通过使用艺术家绘画过程的数据对扩散模型进行微调，学习其真实分布，实现了多种风格的绘画过程生成。