\section{Experimental Setup}
\subsection{Dataset}

AVSBench~\cite{zhou2022avs} is the pixel-level audio-visual segmentation dataset, which consists of 5-second videos downloaded from YouTube. AVSBench contains two datasets: Single Sound Source Dataset (S4) and Multi Sound Source Dataset (MS3). 


\textbf{Single Sound Source dataset (S4).}
This dataset includes videos with only a single sound source. The dataset contains 4,932 videos in total over 23 categories and is split into training, validation, and test sets with 3,452, 740, and 740 samples, respectively. 
Notably, annotation details vary across subsets: only the first frame of each training video is annotated with binary masks, while the validation and test sets include full annotations for all five frames of every video.

\textbf{Multi Sound Sources dataset (MS3).}
% This dataset contains videos with multi-sound sources.
The dataset contains 424 videos and is split into training, validation, and test sets with 296, 64, and 64 samples, respectively. For all splits, including training, every frame is annotated with the segmentation mask.

\subsection{Evaluation Metrics}
Following the original work~\cite{zhou2022avs}, we use the mean Intersection over Union ($M_{\mathcal{J}}$) and F-score ($M_{\mathcal{F}}$) to evaluate our model. The $M_{\mathcal{J}}$ computes the average IoU of predicted mask and ground truth masks over total frames, and $M_{\mathcal{F}}$ computes the harmonic mean of precision and recall.

\subsection{Implementation Details}
We utilized EVF-SAM2~\cite{zhang2024evfsam}, which integrates SAM-2-L~\cite{ravi2024sam2} as the SAM backbone and Beit-3-L~\cite{wang2022beit3} as the multimodal encoder for receiving a text prompt. For experiments using the SAM1~\cite{kirillov2023sam} as a backbone, we employed EVF-SAM1, which consists of SAM-H and Beit-3-L. The input image resolution was set to $1024 \times 1024$, and We trained the model for 40 epochs using the AdamW optimizer.