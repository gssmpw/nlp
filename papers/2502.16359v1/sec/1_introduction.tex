\section{Introduction}
\label{sec:intro}


\input{figures/main_figure}
Audio-visual segmentation (AVS) is the task of segmenting sounding source objects from the video frames. Without pixel-level supervision, early researchers tackled the sounding source localization (SSL) task by leveraging self-supervised learning~\cite{chen2021sslhard, arandjelovic2018sslobjects}
% , park2023sslmarginnce, park2024clipSSL, mahmud2024textssl, mo2022weaklyssl
, exploiting the semantic alignment between image and audio pair data. However, this approach results in coarse localization of sounding objects, which limits the application in fields that require fine-grained segmentation masks. Zhou \textit{et al.}~\cite{zhou2022avs} introduced the AVSBench dataset, annotated with a pixel-level segmentation map. The initial works on this dataset often fuse audio and image embeddings~\cite{zhou2022avs, gao2024avsegformer} and then decode the segmentation mask from the fused audio-visual embeddings. 

The Segment Anything Model (SAM)~\cite{kirillov2023sam, ravi2024sam2} introduces an innovative approach to segmentation tasks by leveraging large-scale pretraining, similar to the way large language models use prompt-based learning~\cite{liu2023prompting}. The prompting idea has been applied to AVS.
Mo and Tian ~\cite{mo2023av} pioneered the idea of audio-prompted SAM by providing audio as a prompt to the SAM decoder. Liu \textit{et al.}~\cite{liu2024annotation} advanced the audio-prompted SAM by introducing adapters into the image encoder of SAM. Audio-prompted SAM achieved better performance on the AVS task by leveraging the image segmentation foundation model, which mitigates the problem of the limited dataset. However, while SAM empowers the capacity of image understanding, the burden of learning audio-visual correspondence persists due to the scarcity of labeled data.
% and the model tends to learn some visual bias from the existing database~\cite{}. 


% \input{figures/overview_figure}

% \KL{Below paragraph is entirely generated by chatgpt}
To address the data scarcity issue, we propose AV2T-SAM, which projects audio prompts into a text embedding space. This enables the use of knowledge from pre-trained text-prompted SAM models. Text-image models benefit from a wealth of text-image paired datasets, surpassing the availability of audio-visual data. By transforming audio inputs into a format compatible with pre-trained text-prompted SAM, AV2T-SAM not only capitalizes on SAMâ€™s segmentation expertise but also enhances the learning of audio-visual correspondence through access to the text embedding space.


Specifically, we introduce a novel feature, $\mathbf{\textit{\textbf{f}}_{CLIP} \odot \textit{\textbf{f}}_{CLAP}}$, which aligns audio and visual embeddings into a shared semantic space, leveraging the pre-trained cross-modal encoders CLIP~\cite{radford2021clip} and CLAP~\cite{elizalde2023clap}. By leveraging modality-specific projections, the model combines these embeddings through element-wise multiplication, capturing the intersection of audio and visual modalities. This design emphasizes shared semantics while reducing irrelevant information, producing a more robust and contextually enriched representation. Through this approach, our model effectively enhances audio-visual correspondence, enabling more accurate and reliable segmentation performance. Also, we report a vision bias problem in the single sound source dataset (S4). We found the vision bias from the dataset by performing better than previous state-of-the-art models without using audio information. 

In summary, our contributions are as follows: 
% \vspace{-8pt}
\begin{itemize}
    \item We propose \textbf{AV2T-SAM} (Audio-Visual to Text SAM), the novel framework of utilizing a pre-trained text-prompted SAM by projecting audio features into the text embedding space. This strategy enables us to effectively harness the rich semantic information embedded in large-scale pre-trained cross-modal encoders, addressing the limitations posed by the scarcity of audio-visual segmentation masks.
    \item We introduce a $\mathbf{\textit{\textbf{f}}_{CLIP} \odot \textit{\textbf{f}}_{CLAP}}$ feature which captures the intersection of audio and visual modalities, emphasizing shared semantics and filtering out irrelevant information, resulting in a more contextually enriched and discriminative feature for segmentation tasks.
    \item We demonstrate that the single sound source (S4) dataset contains a serious vision bias problem by surpassing previous state-of-the-art approaches without using any audio information.
    % and our approach shows more boost on multi-sound source data.
    % \item \NZ{I am thinking about other benefits that could be brought by introducing language space. The model could be generalized to out-of-vocabulary audio scenarios? For example, if the dataset only have ``cat, dog, and helicopters'', then our method can segment the duck sound but others cannot? Do you think this make sense? If so, can we find some data to validate this?}\KL{I tried light experiment, but it did not work well - }
\end{itemize}

