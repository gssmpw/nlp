\section{Related works}
\subsection{Multimodal Learning with Segment Anything Model}
% \NZ{Consider making this paragraph a little broader so it does not overlap too much with the previous one. Two reference paper: https://arxiv.org/abs/2410.11160, https://arxiv.org/abs/2408.09085. You can also make this subsection the first one.}
% \KL{I added details about multi-modal prompting.}
Segment Anything Model (SAM)~\cite{kirillov2023sam} is the first foundation model for image segmentation tasks with various prompts. The model is pretrained with more than 11M images and 1B masks. Many studies verified capability of SAM on diverse downstream tasks, including medical image segmentation~\cite{ma2024medisam} and pose estimation~\cite{lin2024posesam}.

Beyond vision tasks, SAM is also widely used in multimodal learning due to its flexible prompt interactions. Wang et al.\cite{wang2024remotesam} employ remote sensing data to prompt SAM, while Liu et al.\cite{liu2024annotation} use audio to segment sounding objects. Researchers have also explored text-based prompting for SAM~\cite{zhang2024evfsam}.
In this study, we introduce applying a pretrained text-prompted SAM on audio-visual segmentation.

\subsection{Audio-Visual Segmentation}
With the trends of multimodal learning, audio-visual tasks also got much attention. Aiming for better audio-visual correspondence, researchers worked on
Sound Source Localization task \cite{Zhao_2018_pixel,Chen_2021_hard} that aims to localize the sound source portion from the frame. Due to the lack of a labeled dataset, researchers had to rely on Self-Supervised learning, which, while effective in leveraging unlabelled data, comes with limited applications to fine-grained tasks such as segmentation.
Zhou \textit{et al.}~\cite{zhou2022avs} introduce Audio-Visual Segmentation with a labeled AVSBench dataset. The AVSBench dataset enabled researchers to work on the Audio-Visual Segmentation task, which is more fine-grained than the localization task. With the new dataset, researchers started exploring audio-visual segmentation~\cite{zhou2022avs,li2023catr}. Gao \textit{et al.}~\cite{gao2024avsegformer} proposed an architecture that leverages the transformer in the AVS task. Liu \textit{et al.}~\cite{liu2023avstwostages} introduced a new framework that separates the segmentation process and the verifying sounding object process. 

Mo and Tian ~\cite{mo2023av} first explored utilizing the segment anything model with an audio prompt. Claiming the two-layer SAM decoder, which was previously prompted, is too shallow to model the audio-visual correspondence, Liu \textit{et al.} ~\cite{liu2024annotation} introduced the adapter that introjects audio information to the frozen SAM encoder. Seon \textit{et al.} \cite{seon2024extending} employed temporal information which was not considered on sam-based models. 
Nguyen and Park~\cite{nguyen2024save} advanced the adapter mechanism by introducing an additional trainable layer to each block of the SAM encoder and utilized the residual audio encoder. In this work, we will exploit the text embedding space and the power of pretrained text-prompted SAM~\cite{zhang2024evfsam} to advance the performance of SAM~\cite{kirillov2023sam} on the AVS.




