% \section{METHODOLOGY}
\section{Method}
\subsection{Problem Formation}
The input video of audio-visual segmentation (AVS) comprises of a series of visual frames and corresponding audio. The image frames are represented as V = $\{v_i\}^T_{i=1}$,  where ($v_i \in \mathbb{R}^{3 \times H_i \times W_i}$)
where \( v_i \) denotes the \( i\)-th visual frame with spatial dimensions \( H_i \times W_i \), and \( T \) is the total number of frames in the video. Each frame \( v_i \) is paired with a corresponding 1-second audio waveform $A = a_i \in \mathbb{R}^S$, where \( S \) represents the number of audio samples in 1 second.
% or audio spectrogram $A = \{a_i\}^T_{i=1}$, where $a_i \in \mathbb{R}^{H_a \times W_a}$. 
The goal of the AVS task is to segment all sound source objects from each frame. The ground truth segmentation map is provided by binary masks 
$M^n = \{m_i\}_{i=1}^T$ where $m_i \in \{0, 1\}^{H_i \times W_i}$

\subsection{Text-prompted Segmentation Module}
The pre-trained text-prompted SAM model is used in our framework as a backbone to leverage its robust text-visual correspondence knowledge, which is from
extensive text-image datasets~\cite{kazemzadeh2014referitgamedata, yu2016refcocodata,nagaraja2016refcocogdata}. While this module is interchangeable, we particularly used EVF-SAM \cite{zhang2024evfsam}. The model employs the multimodal encoder, which is more efficient than adopting the Large Language model and performs better than the simple text encoder as the CLIP. While the original EVF-SAM model uses text as input, our method does not rely on explicit text. Instead, we map audio-visual features into the language space. To achieve this, we adapt their methods by removing the text encoder but keeping the multimodal encoder fixed, leveraging its strong adaptability to prompt SAM with text embeddings effectively.

\subsection{Projection Module}
This module consists of 2 Multi-layer Perceptron (MLP) projection layers that connect $\mathbf{\textit{\textbf{f}}_{CLIP} \odot \textit{\textbf{f}}_{CLAP}}$ feature to the text embedding space required to prompt the pre-trained text-prompted segmentation model. 

\subsection{Semantically Aligned Features}
To enhance the semantic alignment between audio and visual modalities, we propose $\mathbf{\textit{\textbf{f}}_{CLIP} \odot \textit{\textbf{f}}_{CLAP}}$ feature. Let $\mathbf{a} \in \mathbb{R}^{d_a}$ and $\mathbf{v} \in \mathbb{R}^{d_v}$ denote the normalized embeddings for audio and visual inputs, respectively, obtained from CLIP\cite{radford2021clip} and CLAP\cite{elizalde2023clap} encoders. These embeddings are projected into a shared embedding space of dimension $d_s$ using learned projection matrices $\mathbf{W}_a \in \mathbb{R}^{d_s \times d_a}$ and $\mathbf{W}_v \in \mathbb{R}^{d_s \times d_v}$. The projected embeddings are computed as follows:
\[
\mathbf{\textit{\textbf{f}}_{CLAP}} = \mathbf{W}_a \mathbf{a}, \quad \mathbf{\textit{\textbf{f}}_{CLIP}} = \mathbf{W}_v \mathbf{v},
\]
where $\mathbf{a}'$ and $\mathbf{v}'$ represent the transformed embeddings in the shared space. To capture the intersection of semantic information from both modalities, we calculate:
% perform element-wise multiplication:
$$
\mathbf{\textit{\textbf{f}}_{CLIP} \odot \textit{\textbf{f}}_{CLAP}} 
$$, 
where $\odot$ denotes the Hadamard (element-wise) product, and $\mathbf{f} \in \mathbb{R}^{d_s}$ is the fused feature representation.

This operation ensures that the resulting feature $\mathbf{f}$ emphasizes shared semantics between the two modalities while filtering out modality-specific noise. The design leverages the complementary nature of audio and visual information, providing a robust and contextually enriched representation that enhances downstream segmentation tasks. By projecting both modalities into a unified semantic space and fusing them through intersection, our approach effectively aligns and refines cross-modal information, addressing challenges in audio-visual correspondence learning.

\subsection{Adapters}
The mask decoder of SAM, which is two transformer layers, is too shallow to fuse the audio and visual information~\cite{liu2024annotation}. However, fine-tuning the image encoder of SAM requires considerable computing resources.
To efficiently fuse audio and visual information while not entirely fine-tuning the image encoder, Liu \textit{et al.}~\cite{liu2024annotation} suggested an audio adapter mechanism that trains only the adapter module which injects audio information to the frozen image encoder.


In this work, we employ the adapter module in the mask decoder of SAM with $\mathbf{\textit{f}_{CLIP} \odot \textit{f}_{CLAP}}$ to effectively fuse prompt information to the image encoder. Specifically, for the $i$-th layer of a transformer block, the $i$-th adapter projects the prompt information to the output of the transformer block and repeats the prompt information along the spatial dimensions to match the dimensions of the image features, denoted as $\textbf{P}^j \in \mathbb{R}^{T \times HW \times C}$. The adapted features are then added to the output of the previous encoder layer, \(E_{j-1}\), to form the input to the \(j\)-th layer, \(X_j\), as follows:
\begin{equation}
    X_j = E_{j-1}(X_{j-1}) + \textbf{P}^j.
\end{equation}
This approach ensures the efficient fusion of audio and visual information while preserving computational efficiency by freezing the image encoder during training.



\subsection{Learning Objectives}
For training the model, we combined the Binary Cross-Entropy (BCE) loss and the Intersection over Union (IoU) loss, following the Liu \textit{et al.}~\cite{liu2024annotation}. 
\begin{equation}
    L_{\text{total}} = L_{\text{BCE}} + L_{\text{IoU}}
\end{equation}
