
\input{figures/example_figure}
\section{Experiments}
\subsection{Main Results}
\input{tables/main_result_table}
We compare our model with state-of-the-art methods in Table~\ref{tab:main}. 
% \NZ{first say the best of our prosed system is the SOTA}
Our proposed model achieved new state-of-the-art performance on both subsets of ASVBench. On the S4 dataset, our model outperform the previous state-of-the-art, SAVE~\cite{nguyen2024save}, with an improvement of 1.56 $\mathcal{M}_{\mathcal{J}}$ without introducing extra adapter layers into every image encoder block. Also, on the MS3 dataset, we exceed the previous state-of-the-art, ST-BAVA~\cite{seon2024extending}, without utilizing temporal information, which enabled ST-BAVA to outperform other approaches significantly.
% \NZ{Then say for a fair comparison, we compare our method with SAM1 as all others use SAM1}

For a more fair comparison, we also run experiments with the model with SAM1\cite{cen20233dsam} backbone, considering all previous SAM-based approaches employ SAM1 as a backbone. Under this setting, our model continues to surpass all existing state-of-the-art methods on the S4 dataset. On the MS3 dataset, it falls marginally short of the best result but still maintains competitive performance.
% \NZ{Then coming back to SAM2, say that with SAM2 and our methods, many previous components are not necessary.}
When using the SAM2~\cite{ravi2024sam2} backbone, many complex components, such as adapters within every block of the image encoder as in SAVE, and a module for temporal understanding as in ST-BAVA, are unnecessary.



% Models marked with \ding{55} on the adapter column of Table.\ref{tab:main} are models with SAM backbone without the adapter modules. Notably, our model exhibits particularly strong performance in this setting, outperforming all adapter-free baselines by significant margins. This contrast emphasizes the effectiveness of our pre-trained text-prompted module in establishing \textit{audio-vision correspondence}, reducing reliance on adapter components while maintaining competitive accuracy.

Figure~\ref{fig:examples} compares our segmentation results with those of existing models. In the left portion of the figure, we show one example of the results on the S4 dataset. TPAVI \cite{zhou2022avs} captures only part of the guitar, suggesting weak visual object understanding. SAMA-AVS \cite{liu2024annotation} leverages SAM to accurately segment the guitar but struggles to separate it from nearby objects. In contrast, our model fully segments the guitar and distinguishes it from its surroundings, made possible by strong audio-visual correspondence guided by the pretrained text-prompted SAM. A similar pattern emerges in the right portion of the figure, which depicts results on the MS3 dataset. Again, TPAVI exhibits partial segmentation of objects. While SAMA-AVS identifies the correct objects, it fails to separate the keyboard from a person. Our model, however, successfully segments the keyboard and isolates it from other objects. These findings underscore our model’s superior ability to detect and correlate audio with its corresponding visual region, thanks to the capabilities of the pre-trained text-prompted SAM.

\subsection{Ablation on Adapters}
We present an ablation study of our framework in Table~\ref{tab:main}. 
% \NZ{Say that adapters is useful}
In general, methods without adapters perform not as well as those with adapters, and this also applies to our method. 

% \NZ{Say that the adapters are not that useful, it is our methods that are more useful}
Interestingly, comparing all the methods that do not use adapters, our proposed method still achieves much better performance than the previous method without adapters. Moreover, compared to existing methods that rely on adapters, our approach delivers comparable results—an outcome not achieved by any prior methods. This highlights the effectiveness of our framework and feature design. 



% \NZ{Please revise your following writing accordingly. You can either merge into 5.1 or the first 2 paragraphs in 5.2.}

From the result, we can notice that our framework, even without the adapter module, enhances performance dramatically on S4, the dataset which requires shallow audio-visual fusion. However, the model without the adapter module still suffers on the Multi Sound Sources dataset (MS4), the dataset that requires more profound audio-visual. This observation shows that while our approach strengthens audio-visual correspondence, it cannot replace the adapter module. 

% Notably, incorporating SAM2 or the adapter module yields only marginal gains on S4 while they boost performance on MS4 This suggests that detailed audio cues are less critical for S4, highlighting a possibility of vision bias in the dataset.

\subsection{Ablation on Feature}
In this section, we compare the model's performance with different prompt features. As denoted in a Table.~\ref{tab:feature}, we can observe that $\mathbf{\textit{f}_{CLIP} \odot \textit{f}_{CLAP}}$ perform best on both metrics over both datasets. This implies that $\mathbf{\textit{f}_{CLIP} \odot \textit{f}_{CLAP}}$ feature, which captures information from both audio and visual modalities, contains richer information as the prompt than just the CLAP feature, which contains only audio information.

It is worth noticing that the model prompted with CLIP~\cite{radford2021clip} features surpasses the previous state-of-the-art on the S4 dataset, achieving a score of 86.67 on $\mathcal{M}{\mathcal{J}}$. Given that CLIP lacks any audio-related information, this result suggests that a performance of 86.67 on $\mathcal{M}{\mathcal{J}}$ can be attained using only visual cues. This finding highlights a significant vision bias in the dataset, raising concerns about its effectiveness in evaluating true audio-visual understanding.
\input{tables/feature_result_table}



% \input{tables/ablation_table}
