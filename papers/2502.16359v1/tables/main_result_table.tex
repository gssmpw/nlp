
\begin{table}[h!]

\centering
\scriptsize
\begin{tabular}{llccccc}
\toprule
\textbf{Category} & \textbf{Methods}& \textbf{Adapter} & \multicolumn{2}{c}{\textbf{S4 (V1S)}} & \multicolumn{2}{c}{\textbf{MS3 (V1M)}} \\ 
\cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & & & $\mathcal{M}_{\mathcal{J}}$ & $\mathcal{M}_{\mathcal{F}}$ & $\mathcal{M}_{\mathcal{J}}$ & $\mathcal{M}_{\mathcal{F}}$ \\ 
\midrule
\multirow{8}{*}{\textbf{AVS}} 

 & AVSBench&- & 78.74 & 0.879 & 54.00 & 0.645 \\
 & ECMVAE&- & 81.74 & 0.901 & 57.84 & 0.708 \\
 & AVSegFormer&- & 82.06 & 0.899 & 58.36 & 0.693 \\
 & AVSC&- & 81.29 & 0.886 & 59.50 & 0.657 \\
 & AuTR&- & 80.40 & 0.891 & 56.20 & 0.672 \\
 & GAVS&- & 80.06 & 0.902 & 63.70 & 0.774 \\
 & AQFormer&- & 81.60 & 0.894 & 61.10 & 0.721 \\
 & COMBO&- & 84.70 & 0.919 & 59.20 & 0.712 \\ 
\midrule
 \multirow{7}{*}{\textbf{SAM}} 
 & AV-SAM& \ding{55} & 40.47 & 0.566 & - & - \\
 & AP-SAM& \ding{55} & 69.61 & 0.796 & 51.58 & 0.578 \\  
 & SAMA-AVS&\ding{51} & 81.53 & 0.886 & 63.14 & 0.691 \\
 & SAVE&\ding{51} & 85.11 & 0.912 & 67.01 & 0.739 \\
 & ST-BAVA&\ding{51} & 82.46&  0.906&  69.01&  0.776 \\
 & \textbf{AV2T-SAM} &\ding{55} &83.11 & 0.901 & 55.81 & 0.634 \\
 & \textbf{AV2T-SAM} &\ding{51} & 85.78 & 0.919 & 65.76 & 0.738 \\
 \midrule
 \multirow{2}{*}{\textbf{SAM2}} 
 & \textbf{AV2T-SAM}&\ding{55} & 85.63& 0.920 & 64.47&  0.704\\
 & \textbf{AV2T-SAM}&\ding{51} &\textbf{86.67}&  \textbf{0.924}&  \textbf{69.65} &  \textbf{0.777}\\
\bottomrule
\end{tabular}
\caption{Performance comparison across subsets S4 and MS3 for various methods. In the "Adapter" column, \ding{51} indicates that the model is trained with the adapter module, while \ding{55} denotes that the model is trained without the adapter.}
\label{tab:main}
\end{table}
