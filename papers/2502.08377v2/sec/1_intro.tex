\section{Introduction} \label{intro}

\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{figs/highlight.pdf}
\caption{\textbf{(a) Illustration of the issues caused by different proportions of dynamic and static regions.} Previous work \cite{zeng2024stag4d} generates the 4D content with obviously blurry textures in the dynamic regions with B-type video input. In contrast, our methods with decoupling dynamic-static features generates high-quality results with clear textures. \textbf{(b) The visualization of dynamic features in our method.} The red region highlights the primary region of interest in the dynamic features in B-type videos. This part is also the dynamic region between frames R and C. It demonstrates our method can successfully decouple dynamic-static features.}\label{highlight}
\end{figure}


Generating dynamic 3D (4D) content  \cite{li2022neural,shao2023tensor4d,li2021neural,pumarola2021d,park2021hypernerf,gao2022monocular,li2024dreammesh4d,wu20244d} from video is an essential research topic involving the field of computer vision and computer graphics. However, it is rather formidable to predict accurate motion from a few viewpoints while ensuring high-quality generation.

There are two main streams in current approaches to improving generation quality. Inference-based methods \cite{xie2024sv4d,zhang20244diffusion,ren2024l4gm,liang2024diffusion4d} can generate high-quality 4D content by capturing temporal-spatial correlations in its 4D diffusion model. Another stream is optimization-based methods \cite{singer2023text,zeng2024stag4d,yin20234dgen,wu2024sc4d}. These methods generate 4D content through distilling spatial prior knowledge from pre-trained multi-view \cite{liu2023zero,shi2023zero123++} diffusion models. However, \textbf{\textit{previous methods only model temporal-spatial correlations using whole information in frames, but fail to explicitly differentiate between the dynamic and static regions within a frame}}. If static regions account for a significant portion, previous methods overlook dynamic information. Thus they tend to overfit static regions, resulting in a diminished capacity to perceive texture variations in dynamic regions. As shown in Fig.\ref{highlight} (a), previous work (STAG4D \cite{zeng2024stag4d}) produces results with blurry texture (e.g., wrinkles in clothes), particularly when confronted with B-type input video. 

To tackle above problems, we present a novel approach DS4D, which decouples dynamic and static features along temporal and spatial axes to enhance dynamic representations for high-quality 4D generation. In particular, we propose dynamic-static feature decoupling module (DSFD) to obtain decoupled features. We assume some regions of current frame features exhibit significant differences relative to reference frame features. Such regions always contain important knowledge of texture, shape variations, and motion trends under current timestamps. Therefore, it can be regarded as the dynamic feature of the current frame. In contrast, the remaining parts are the static features. Based on this assumption, we decompose the dynamic components between each frame feature and reference frame features along the temporal axes. The dynamic component (also termed dynamic features), as difference features between two frame features, is able to represent the dynamic information (as shown in Fig.\ref{highlight} (b)). At last, we acquire decoupled features driven by dynamic components and corresponding frame features. 

Note that spatial occlusion leads to the failure of dynamic components captured from a specific viewpoint to inadequately represent the inherent dynamic information in 4D space. To mitigate this issue, we design temporal-spatial similarity fusion module (TSSF) which engages in enhancing features' dynamic representations. Firstly, Gaussian points are initialized by utilizing a large reconstruction model \cite{xu2024instantmesh}. Then, point features are obtained by retrieving decoupled features for Gaussian points via view projection in TSSF. Subsequently, TSSF produces fused Gaussian features by adaptively selecting information on dynamic regions containing similar texture, shape, and motion representations from point feature space at same timestamps along spatial axes. Finally, fused Gaussian features with strong dynamic representation, are used for 4D generation. 


The contributions can be summarized as follows:

\begin{itemize}
\item{To our best knowledge, we are the first to propose a framework DS4D that decouples dynamic-static information in frames along temporal-spatial axes to enhance dynamic representations for high-quality 4D generation.}
\item{Leveraging significant differences between frame features, we propose dynamic-static feature decoupling module (DSFD) to decouple dynamic and static features.}
\item{We tackle the issue of inadequate dynamic information in 4D space resulting from spatial occlusion by designing temporal-spatial similarity fusion module (TSSF). TSSF enhances dynamic representations in features.}

\item{Experimental results on Consistent4D dataset \cite{jiang2024consistentd} and Objaverse dataset \cite{deitke2023objaverse} demonstrate our DS4D outperforms other SOTA methods in terms of video quality, motion fidelity, and temporal-spatial consistency. Furthermore, the experiments on a complex real-world scenario dataset \cite{li2022neural} verify its effectiveness on 4D scenes.}
\end{itemize}


