\section{Related Works}
\label{sec:related}

\begin{figure*}[!t]%
\centering
\includegraphics[width=1.0\textwidth]{figs/pipeline.pdf}
\caption{\textbf{Overview of our proposed DS4D.} Given an input video and corresponding multi-view sequences, our DS4D decouples features of the frame at time $t$ based on the reference frame in DSFD module. Next, we acquire point features by retrieving each decoupled feature for Gaussian points via view projection, and we obtain fused Gaussian features by adaptively selecting similar dynamic information from point feature space in TSSF module. Finally, through Deformation MLP, our method generates 4D content. 
}\label{pipeline}
\end{figure*}


\subsection{3D Generation}

3D generation aims to produce 3D assets using images or text descriptions. The early works generate 3D objects in voxels \cite{choy20163d,xie2020pix2vox++,zhu2023umiformer,yang2023long}, meshes \cite{wang2018pixel2mesh,niemeyer2020differentiable,wen2022pixel2mesh++} or point clouds \cite{gadelha2018multiresolution,achlioptas2018learning,luo2021diffusion} forms. Recently, thanks to the popular application of NeRF \cite{mildenhall2021nerf,muller2022instant} and 3D Gaussian Splatting \cite{kerbl20233d,yu2024mip}, and the successful pre-trained text-to-image diffusion model \cite{saharia2022photorealistic} in 2D generation tasks, most works including DreamFusion \cite{poole2023dreamfusion} Dreamgaussian \cite{tang2024dreamgaussian} that focus on 3D generation try to use Score Distillation Sampling (SDS) loss \cite{poole2023dreamfusion} to explore the possibility of inspiring potential 3D-aware from diffusion. However, these works always suffer from over-saturation, over-smoothing, and multi-face problems. Because it is a challenge to distill the invisible views while ensuring multi-view consistency based on 2D diffusion. In contrast, Zero-1-to-3 \cite{liu2023zero} and Zero123++ \cite{shi2023zero123++} directly train a 3D-aware diffusion model using multi-view images and corresponding camera poses. Zero-1-to-3 and Zero123++ usually generate more stable and high-quality novel views thanks to the spatial perception built into them. Therefore, in our method, we use Zero123++ to predict the novel views at all timestamps to provide additional multi-view sequences for optimization. 

\subsection{4D Generation}

Compared to 3D generation, 4D generation is a more challenging task. Current 4D representations are two main streams, including NeRF-based \cite{mildenhall2021nerf,park2021nerfies,pumarola2021d,tretschk2021non,wu2022d,fang2022fast} and 3D Gaussians-based \cite{kerbl20233d,li2024spacetime,liang2023gaufre,luiten2024dynamic,li2024dreammesh4d,wu20244d,yang2023real}. Consistent4D \cite{jiang2024consistentd}, as the NeRF-based method, leverages the prior knowledge from pre-trained 2D diffusion models to optimize dynamic NeRF by Score Distillation Sampling (SDS) optimization. Dreamgaussian4D \cite{ren2023dreamgaussian4d}, STAG4D \cite{zeng2024stag4d} and SC4D \cite{wu2024sc4d} both attempt to introduce dynamic 3D Gaussians in 4D generation. Dreamgaussian4D notably reduces the cost of optimization time by improving the training strategies. STAG4D can generate anchor multi-view videos for optimization via a training-free strategy. SC4D optimizes sparse-controlled dynamic 3D Gaussians through SDS loss. However, these methods model temporal-spatial correlations using whole video but ignore the dynamic information in frames when static regions account for a large portion. It leads to them easily overfitting on static parts. 


