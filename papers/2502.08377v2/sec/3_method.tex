\section{Methodology}
\subsection{Overview}
In this section, we provide an overview of DS4D, which includes initialization in Sec.\ref{initial}, dynamic-static feature decoupling (DSFD) in Sec.\ref{dsfd}, and temporal-spatial similarity fusion (TSSF) in Sec.\ref{tscf}. The overall framework of our proposed method is illustrated in Fig.\ref{pipeline}.

\subsection{Initialization} \label{initial}

Formally, we begin with a single-view video. For each frame in the video, we adopt Zero123++ \cite{shi2023zero123++} to infer the $^{\dag}$pseudo multi-view images of frames. Then we obtain frame sequences, $\textbf{I}=\{I^{(0,0)}, I^{(0,1)},\dots,I^{(i,j)},\dots,I^{(t,v)}\}$, where $t$ represents the number of frames, and $v$ represents the number of views of each frame.


\textbf{Frame Feature Extraction.} Owing to the superior feature extraction capability of the visual foundation model, we employ it as the feature extractor to obtain high-quality features. Thanks to the strong feature extraction ability of DINOv2 \cite{oquab2023dinov2}, we leverage DINOv2 to extract frame features $\textbf{F}=\{f^{(0,0)}, f^{(0,1)},\dots,f^{(i,j)},\dots,f^{(t,v)}\}$ from sequences $\textbf{I}=\{I^{(0,0)}, I^{(0,1)},\dots,I^{(i,j)},\dots,I^{(t,v)}\}$, where $f^{(i,j)}\in \R^{P\times D}$, $P$ and $D$ are the number of tokens and token dimension, respectively. Each feature $f^{(i,j)}$ encodes the geometry and texture of the corresponding frame.
 


\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{The discussion about the impact of multi-view sequence quality and the robustness of our methods can be seen in supplementary material (Sec.~\ref{robust}).}

\textbf{Gaussian Points Initialization.} Previous works~\cite{zeng2024stag4d,wu2024sc4d} initialize Gaussian points randomly, leading to unstable topology during optimization and ultimately compromising the quality of results. To solve the above problem, we initialize static 3D Gaussian points through pre-defined point clouds. Specifically, we employ a large reconstruction model \cite{xu2024instantmesh} to generate the point clouds from the middle frame. This initialization approach provides a geometric prior and ensures the stability of subsequent optimization. 



\subsection{Dynamic-Static Feature Decoupling (DSFD)} \label{dsfd} % decouple 
We employ a set of frame features $\textbf{F}$, which consists of all frame features and their corresponding multi-view features, to decouple dynamic-static features. Specifically, our decoupling process involves two aspects: 1) Finding out reference frame features; 2) Decoupling dynamic and static features along temporal axes.

\textbf{Finding out reference frame features.} Decoupling frame-by-frame features usually consumes considerable computation time. Hence, we specify special frames as references for decoupling. It is critical to determine whether reference frame features can accurately represent the semantic content of the entire video. Considering that we initialize static Gaussian points at the middle frame, the whole model needs to capture the texture, shape, and motion transformation from the middle frame to any frame during training and inference. Therefore, we choose the frame features $f^{(\frac{t}{2},j)}$ of the middle timestamp as one of the reference frame features, which represent the semantic. Furthermore, to represent the motion trends in video, we need a kind of feature that can describe the motion variations of the entire video. Hence, we also specify the $^{\dag}$average frame features $\bar{f}^{(\bar{t},j)}$ as reference frame features, which represent the average motion variations.




\textbf{Decoupling dynamic and static features along temporal axes.} To reliably decouple dynamic and static information in frame features, we need to obtain the regions of current frame features that exhibit significant differences relative to reference frame features. These regions contain important knowledge of texture, shape variations, and motion trends under current timestamps. The regions with significant differences mean dynamic parts between the two frame features, while regions with similar information mean static parts between the two frame features. Therefore, we propose the dynamic-static feature decoupling module (DSFD), as shown in Fig.\ref{decouple}.



Concretely, given specific $j$-th view in timestamp $i$, we project frame features $f^{(i,j)}$ onto reference frame features $r^{j}$. \textbf{The projection represents the semantic overlapping part of the two frames. In other words, regions in the two frames with no texture and shape transformations are identified as the static part.} The process is mathematically formulated as follows:

\begin{equation}
  \label{eq1}
  f_{\text{static}}^{(i,j)}=(\frac{f^{(i,j)}\cdot r^{j}}{\Vert r^{j}\Vert_2})\frac{r^{j}}{\Vert r^{j}\Vert_2}, 
\end{equation} 

\begin{figure}[t]%
\centering
\includegraphics[width=0.4\textwidth]{figs/decouple_v2.pdf}
\caption{An overview of the decoupling architecture in DSFD. The decoupling architecture is shown on the left. The demonstration of projection is shown on the right.}\label{decouple}
\end{figure}

where $\cdot$ denotes dot product.

The difference between current frame features and static features means the orthogonal vector. Then, we identify the orthogonal vector as the semantic difference between the current frame and the reference frame. Hence, the orthogonal vector can be regarded as dynamic features:



\begin{equation}
  \label{eq2}
  f_{\text{dynamic}}^{(i,j)}=f^{(i,j)}-f_{\text{static}}^{(i,j)}. 
\end{equation} 


At each timestamp, we obtain the dynamic features in a particular view. Dynamic features contain significant difference information, which represents the motion knowledge of the current frame relative to the reference frame. Meanwhile, frame features contain texture and shape attributes of the current frame, which are useful for subsequently texture and shape learning. Hence, we leverage dynamic features to supplement dynamic information in the frame features while retaining the inherent attributes of the frame features. 

Here, we append each token of the current frame features $f^{(i,j)}$ with dynamic features $f_{\text{dynamic}}^{(i,j)}$. Then, the appended features $f_d^{(i,j)}$ are integrated based on both the temporal $i$ and spatial $j$, which are named decoupled features $\textbf{F}_{\textbf{D}}=\{f_d^{(0,0)},f_d^{(0,0)},\dots,f_d^{(i,j)},\dots,f_d^{(t,v)}\}$, where $t$ and $v$ denote the number of frames and number of views, respectively.



\subsection{Temporal-Spatial Similarity Fusion (TSSF)} \label{tscf} % fusion
Since the features in $\textbf{F}_{\textbf{D}}$ obtained from a particular viewpoint do not adequately represent the complete dynamic information in the 4D space, we design temporal-spatial similarity fusion module (TSSF) to adaptively select similar dynamic information from decoupled features under different views at the same timestamp.

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Average frame features $\bar{f}^{(\bar{t},j)}$ are derived from the average of all frame features from a particular view.}

Inspired by \cite{melas2023pc2}, we retrieve the decoupled features from Gaussian points by view projection. Then we integrate decoupled features into point features based on the given camera pose. The Gaussian points are well-aligned with decoupled features thanks to the projection operations. Therefore, point features $\textbf{F}_{\textbf{p}}=\{f_p^{(0,0)},f_p^{(0,1)},\dots,f_p^{(i,j)},\dots,f_p^{(t,v)}\}$ encapsulate extensive dynamic information from decoupled features. In fact, among different viewpoints, the texture, shape, and motion representations from the same spatial area of the object are similar in the same timestamps. Based on this fact, utilizing the semantic similarities in point features across viewpoints and aggregating them contributes to the enhancement of dynamic representations in point features. Hence, we propose an adaptive fusion in Fig.\ref{fusion}.

\begin{figure}[t]%
\centering
\includegraphics[width=0.4\textwidth]{figs/fusion.pdf}
\caption{An overview of adaptive fusion in TSSF.}\label{fusion}
\end{figure}

At the same timestamp, we design global awareness fusion (GA) in adaptive fusion to merge similar dynamic information from point features along spatial axes. However, under conditions of significant differences among multi-views, the effect of merging in GA is affected by that dynamic regions are noticeably obscured in specific viewpoints. Therefore, we need to reduce the impact of point features under these views for merging while retaining the useful dynamic information in these features. Based on this condition, we design distance awareness fusion (DA).

\textbf{Global awareness (GA).} At time $i$, the fully-connection layer (FC) generates score maps $\textbf{W}=\{w^{(0,0)},\dots,w^{(t,v)}\}$ for the point features of each view,

\begin{equation}
  \label{equ:fc1}
  \textbf{W}=\mathtt{Softmax}(\mathtt{FC}(\textbf{F}_{\textbf{p}})). 
\end{equation} 

Then, we fuse them into fused point features by the weighted summation of all point features according to their score map. The fused features $f_a^i$ can be calculated as:
\begin{equation}
  \label{equ:fc2}
  f_a^i=\sum_{j=0}^{v}{w^{(i,j)}f_p^{(i,j)}}. 
\end{equation} 

\textbf{Distance awareness (DA).} Frames from single-view video are regarded as front view. The front frame reveals real and visible motion areas of the object at various timestamps compared to the other views. This information within the front frame requires preservation as much as possible while supplementing the useful dynamic information from other views for accurancy motion prediction. Thus, at time $i$, we calculate the $L_1$ distance between point features from the front view and point features from the other views. The distance quantifies the difference information between point features extracted from the front view and those derived from other viewpoints. The difference information contains dynamic information that cannot be represented by the point features of the front view. Then, we append point features from other views with distance features and merge them by \cref{equ:fc1} and \cref{equ:fc2}. The fused point features from other views are appended to point features from the front view. We merge these features by \cref{equ:fc1} and \cref{equ:fc2} to acquire the fused point features $f_a^i$. Through the above pre-processing, we reduce the impact of point features under other views according to the front view while retaining useful dynamic information in point features under other views.

% Through the above pre-processing feature filtering, we reserve the useful dynamic information in point features under other views while 

In addition to fused features through adaptive fusion, we also present dynamic Gaussian features as 3D Gaussian features with HexPlane \cite{cao2023hexplane} for inherently regularizing features of fields and guaranteeing their smoothness. Formally, the dynamic Gaussian features at time $i$ can be defined as:

\begin{equation}
  \label{equ:sig1}
  f_{\text{hg}}^{i}=\mathtt{HexPlane}(X_i,s_i,\gamma_i,\sigma,\zeta), 
\end{equation} 
where $X_i=(x_i,y_i,z_i)$ denotes the position of Gaussian points, $s_i$ and $\gamma_i$ denote scale and rotation at time $i$. $\sigma$ and $\zeta$ represent opacity and spherical harmonic coefficients of the radiance. Then, we combine dynamic Gaussian features $f_{\text{hg}}^{i}$ and fused point features $f_a^i$ at time $i$, and map them to fused Gaussian features via learnable linear transformation. The fused Gaussian features represent Gaussian intrinsic properties while capturing rich dynamic information. Therefore, fused Gaussian features are essential for accurately predicting deformation using Deformation MLP.

\subsection{Training Objectives} Following \cite{zeng2024stag4d}, we calculate SDS loss and the photometric loss based on rendered views and ground truth images. Inspired by \cite{hong2023lrm,zou2024triplane,xu2024instantmesh}, we introduce the LPIPS loss to minimize the similarity between pseudo multi-view images and rendered views. More details can be found in the supplementary material.
 

