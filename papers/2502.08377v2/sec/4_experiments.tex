\section{Experiments}

\begin{table*}[t]
	\centering
	\renewcommand\arraystretch{1.}
	\scalebox{0.8}{
	\begin{tabular}{c|c|cccc|cccc}
        \Xhline{1.5px}
		\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Optimization}} & \multicolumn{4}{c|}{\textbf{Consistent4D dataset}} & \multicolumn{4}{c}{\textbf{Objaverse dataset}} \\ \cline{3-10}
          & &\multicolumn{1}{c}{\textbf{CLIP $\uparrow$}} & \multicolumn{1}{c}{\textbf{LPIPS $\downarrow$}} & \multicolumn{1}{c}{\textbf{FVD $\downarrow$}} & \multicolumn{1}{c|}{\textbf{FID-VID $\downarrow$}} & \multicolumn{1}{c}{\textbf{CLIP $\uparrow$}} & \multicolumn{1}{c}{\textbf{LPIPS $\downarrow$}} & \multicolumn{1}{c}{\textbf{FVD $\downarrow$}} & \multicolumn{1}{c}{\textbf{FID-VID $\downarrow$}}
         \\ \hline
        \textbf{Consistent4D \cite{jiang2024consistentd}} & \Checkmark & 0.9085 & 0.1316 & 1041.2242 & 28.6471 & 0.8491 & 0.2222 & 1814.5652 & 48.7921 \\
        \textbf{Dreamgaussian4D \cite{ren2023dreamgaussian4d}} & \Checkmark & 0.9145 & 0.1517 & 844.9087 & 37.9977 & 0.8127 & 0.2017 & 1545.3009 & 58.3686  \\
        \textbf{STAG4D \cite{zeng2024stag4d}} & \Checkmark & 0.9078 & 0.1354 & 986.8271 & 26.3705 &  0.8790 & 0.1811 & 1061.3582 & 30.1359  \\
        \textbf{SC4D \cite{wu2024sc4d}} & \Checkmark & 0.9117 & 0.1370 & 852.9816 & 26.4779 & 0.8490 & 0.1852 & 1067.7582 & 40.5130 \\
        \textbf{4Diffusion \cite{zhang20244diffusion}} & \XSolidBrush & 0.8734 & 0.2284 & 1551.6363 & 149.6170 & - & - & - & - \\ 
        \textbf{L4GM \cite{ren2024l4gm}} & \XSolidBrush & 0.9158 & 0.1497 & 898.0604 & 31.4996 & - & - & - & - \\        
        \hline
        \textbf{DS4D-GA (Ours)} & \Checkmark & 0.9206 & 0.1311 & 799.9367 & 26.1794 & 0.8868 & 0.1761 & 890.2646 & 26.6717 \\
        \textbf{DS4D-DA (Ours)} & \Checkmark & \textbf{0.9225} & \textbf{0.1309} & \textbf{784.0235} & \textbf{24.0492} & \textbf{0.8881} & \textbf{0.1759} & \textbf{870.9489} & \textbf{25.3836} 
	    \\ \Xhline{1.5px}
	\end{tabular}}
	\caption{Evaluation and comparison of the performance on Consistent4D dataset and Objaverse dataset. The best score is highlighted in bold. All the experiments of the methods are carried out using the code from their official GitHub repository. For a fair comparison, the experiment of and L4GM on Objaverse dataset are disregarded since they are inference-based methods trained on this dataset.}
\label{total_result}
\end{table*}

\begin{figure*}[t]
    \centering
	\begin{minipage}{1\linewidth}
        \centerline{\includegraphics[width=1.\linewidth]{figs/compare_methods_v2.pdf}}
	\end{minipage}
	\\ \vspace{0.5mm}
	\begin{minipage}{1\linewidth}
        \centerline{\includegraphics[width=1.\linewidth]{figs/compare_methods_v1.pdf}}
	\end{minipage}
	\caption{Qualitative comparison on video-to-4D generation. For each method, we render results under two novel views at two timestamps. \textbf{More comparison examples, and some of the 360$^\circ$ generation results (video file) can be found in supplementary materials.}}
\label{show_results}
\end{figure*}

\subsection{Implementation Details}
The implementation details are provided in the section. 

\textbf{Datasets} In our article, we use \textbf{four} challenging datasets. 1) We utilize the dataset provided by Consistent4D. 2) We introduce a subset of Objaverse \cite{deitke2023objaverse,liang2024diffusion4d}. The objects in Objaverse have more complexity motion than Consistent4D dataset. 3) We introduce some challenging videos from online sources. 4) In Sec.\ref{real-world}, to evaluate the effectiveness of our method in real-world scenarios, we utilize three real-world scenarios provided by Neu3D's \cite{li2022neural} dataset. More details can be found in supplementary material.


\textbf{Metrics} Following Consistent4D \cite{jiang2024consistentd} and STAG4D \cite{zeng2024stag4d}, we measure the 4D generation quality of our methods using CLIP, LPIPS, FVD and FID-VID. In Sec.\ref{real-world}, following \cite{wu20244d}, we evaluate our results using PSNR, LPIPS, SSIM, and structural dissimilarity index measure (D-SSIM).




\textbf{Training} Our two models, including DS4D-GA (using GA in TSSF) and DS4D-DA (using DA in TSSF) use the same training setting. The training details can be found in Supplementary Material.





\subsection{Comparisons with Existing Methods}
In this section, we compare our methods DS4D-GA and DS4D-DA with the SOTA methods on Consistent4D dataset and Objaverse dataset, including Consistent4D \cite{jiang2024consistentd}, Dreamgaussian4D \cite{ren2023dreamgaussian4d}, STAG4D \cite{zeng2024stag4d}, SC4D \cite{wu2024sc4d}, 4Diffusion \cite{zhang20244diffusion} and L4GM \cite{ren2024l4gm}. We conduct qualitative and quantitative comparisons, respectively. The superior performance demonstrates the effectiveness of our methods. 


\textbf{Quantitative comparisons.} The quantitative results are shown in Tab.~\ref{total_result}. Our methods DS4D-GA and DS4D-DA, consistently outperform other methods in all metrics. Specifically, our methods notably exceed the SOTAs in FVD, indicating that our generation results have fewer temporal artifacts than others. Furthermore, the experiments on Objaverse dataset present that our methods significantly outperform other SOTAs by a large margin. It demonstrates our methods are superior in terms of temporal-spatial consistency, fidelity, and quality of generation results and underscores the robustness of our methods. In summary, such significant improvements are attributed to our method of decoupling dynamic-static features, which can explicitly distinguish dynamic and static regions within frame features. 


\textbf{Qualitative comparisons.} The qualitative results on Consistent4D dataset are presented in Fig.~\ref{show_results}. Besides, the results on Objaverse dataset are shown in Fig.\ref{objaverse1}. Obviously, whether in the example with a larger proportion of static regions (the person in Fig.~\ref{show_results} and warrior in Fig.\ref{objaverse1}) or the example with a balanced proportion of dynamic and static regions (guppie in Fig.~\ref{show_results}), the results, generated by Consistent4D, Dreamgaussian4D, STAG4D and SC4D, have different degrees of blurriness in the details, especially in the areas with motion trends in the current and subsequent frames. Although 4Diffusion and L4GM generate videos with clearer textures than other previous methods with the help of a large amount of 4D data prior, it is easy to generate the abnormal shape or textures with inconsistent details in some areas (e.g, the back of the person). These methods do not differentiate between dynamic and static information, leading them to easily overlook information in dynamic regions when faced with large proportions of static. In contrast, our methods address the challenges arising from varying proportions of dynamic and static regions in frames, resulting in the achievement of high-quality 4D generation.

\begin{table*}[t]
	\centering
	\renewcommand\arraystretch{1.}
	\scalebox{0.75}{
	\begin{tabular}{c|c|cccc|cccc}
        \Xhline{1.5px}
		\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Point Initialization}} & \multicolumn{4}{c|}{\textbf{Consistent4D dataset}} & \multicolumn{4}{c}{\textbf{Objaverse dataset}} \\ \cline{3-10}
          & &\multicolumn{1}{c}{\textbf{CLIP $\uparrow$}} & \multicolumn{1}{c}{\textbf{LPIPS $\downarrow$}} & \multicolumn{1}{c}{\textbf{FVD $\downarrow$}} & \multicolumn{1}{c|}{\textbf{FID-VID $\downarrow$}} & \multicolumn{1}{c}{\textbf{CLIP $\uparrow$}} & \multicolumn{1}{c}{\textbf{LPIPS $\downarrow$}} & \multicolumn{1}{c}{\textbf{FVD $\downarrow$}} & \multicolumn{1}{c}{\textbf{FID-VID $\downarrow$}}
         \\ \hline
        \textbf{A} & \XSolidBrush & 0.9133 & 0.1341 & 953.6300 & 27.3747 & 0.8736 & 0.1816 & 1072.1292 & 28.2410 \\
        \textbf{B} & \Checkmark & 0.9151 & 0.1313 & 913.371 & 27.1357 & 0.8763 & 0.1801 & 1062.9398 & 28.0977 \\
        \textbf{C. w/ LPIPS Loss} & \Checkmark & 0.9163 & 0.1311 & 899.5714 & 27.0836 & 0.8773 & 0.1804 & 1016.2576 & 27.8062 \\
        \textbf{D. w/ Frame Features} & \Checkmark & 0.9174 & 0.1350 & 888.6579 & 26.8486 & 0.8805 & 0.1778 & 1005.7503 & 27.6334 \\
        \textbf{E. w/ DSFD} & \Checkmark & 0.9186 & 0.1333 & 861.6075 & 26.5403 & 0.8827 & 0.1765 & 989.0834 & 26.9199 \\
        \hline
        \textbf{F. w/ TSSF-Average Pooling} & \Checkmark & 0.9194 & 0.1313 & 839.6600 & 26.5071 & 0.8848 & 0.1761 & 951.9127 & 26.8412\\ 
        \textbf{G. w/ TSSF-GA (Ours)} & \Checkmark & 0.9206 & 0.1311 & 799.9367 & 26.1794 & 0.8868 & 0.1761 & 890.2646 & 26.6717 \\
        \textbf{H. w/ TSSF-DA (Ours)} & \Checkmark & \textbf{0.9225} & \textbf{0.1309} & \textbf{784.0235} & \textbf{24.0492} & \textbf{0.8881} & \textbf{0.1759} & \textbf{870.9489} & \textbf{25.3836} 
	    \\ \Xhline{1.5px}
	\end{tabular}}
	\caption{The ablation experiments on Consistent4D dataset and Objaverse dataset. Each setup is based on a modification of the immediately preceding setups. The best score is highlighted in bold.}
\label{ablation_result}
\end{table*}


\begin{figure}[h]%
\centering
\includegraphics[width=0.46\textwidth]{figs/objaverse1.pdf}
\caption{Qualitative comparison on video-to-4D generation based on Objaverse dataset.}\label{objaverse1}
\end{figure}

\subsection{Ablation Experiments}
To evaluate the effectiveness of different components in our methods, we conduct ablation experiments on Consistent4D and Objaverse datasets as shown in Tab.\ref{ablation_result}. More analysis can be found in supplementary material.




\begin{figure}[t]%
\centering
\includegraphics[width=0.45\textwidth]{figs/ablation.pdf}
\caption{Ablation on different experiment settings from Tab.\ref{ablation_result}.}\label{ablation}
\end{figure}

\begin{figure}[t]%
\centering
\includegraphics[width=0.45\textwidth]{figs/visualization.pdf}
\caption{(a) Visualization on the heatmap of dynamic features in DSFD. The red region highlights the primary region of interest in dynamic features. (b) The score map of point features in TSSF-GA and TSSF-DA. The red area indicates high attention to dynamic information in point features of a specific view.}\label{visualization}
\end{figure}


\textbf{Effect of DSFD and Decoupling.} Validating the effect of DSFD also means Validating the effect of decoupling since DSFD consists of decoupling. Therefore, we conduct experiments in two aspects: a) Adding frame features in the model without decoupling (labeled as D). b) Performing decoupling on frame features in the model (labeled as E, also termed DSFD). In particular, the performance of D and E both have significant improvement compared to A and B, especially FVD score. For D, the frame features provide enough structure prior knowledge and texture information for Deformation MLP. It assists Deformation MLP in capturing shape deformation. Nevertheless, D fails to differentiate between dynamic and static regions of frames clearly. Consequently, D is prone to overfitting in static areas, resulting in blurry details. For example, the blurred details of legs from D in Fig.\ref{ablation}. In contrast, DSFD, which decouples dynamic-static features, mitigates the issue.

\textbf{Effect of TSSF.} To validate the effect of TSSF, we add the TSSF based on model E, including model F, G (TSSF-GA) and H (TSSF-DA). Among them, we replace adaptive fusion with average pooling in model F. Specifically, we observe that E, G, H can generate more texture details than other models. Because they mitigate the problem brought by spatial occlusion by aggregating dynamic components from different viewpoints. However, F only averages the point features from DSFD along spatial axes, which ignores merging similar dynamic information. The model G and F adaptively select dynamic information, which plays a crucial role in enhancing dynamic representations. Compared with E, the texture of 4D content generated by F and H is more refined in Fig.\ref{ablation}. This underscores the efficacy of merging similar dynamic information in TSSF, including TSSF-GA and TSSF-DA.


\subsection{Visualization} 
In this section, we perform several visualizations of features in DSFD and TSSF, respectively.


\textbf{DSFD.} Fig.\ref{visualization} (a) presents the heatmap of dynamic features obtained by DSFD decoupling features from the current and reference frame features. The red area indicates the primary region of interest in the features. We observe the motion trends come from the trunk and body of elephant in the front view (left of the figure). Thus, dynamic features show strong concern in this area. Limited to the viewpoint range, the motion trajectory for novel views (right of the figure) is more interested in the elephant's head. No doubt, dynamic features also present a high response in similar areas. In conclusion, our method can acquire accurate dynamic features using DSFD, as supported by the visualization results.


\textbf{TSSF.} Fig.\ref{visualization} (b) shows the score map of adaptively selecting similarity dynamic information from point features of different views by TSSF-GA and TSSF-DA.  Specifically, the leg movements of the triceratops indicate the primary trend in motion between the middle and current time. The red area indicates the high attention to dynamic information in point features of a specific view. Meanwhile, this area is also similar to the dynamic area of other views in Fig.\ref{visualization} (b). Hence, it reveals that the two approaches can capture a certain degree of similar dynamic information from different viewpoints. However, TSSF-GA is interested in the back of triceratops rather than legs since the front legs are nearly invisible in the novel view $v_2$. The unseen dynamic region in the novel view influences the accuracy of selecting similar dynamic information. In contrast, TSSF-DA predicts highest scores on the legs under view $v_1$ and $v_2$, thanks to reducing the impact of novel views in TSSF-DA. It indicates TSSF-DA can alleviate issues caused by the fact that dynamic regions are noticeably obscured in novel views.
