\clearpage
\setcounter{page}{1}
\maketitlesupplementary




\section{The 360$^\circ$ Results Generated by Our methods}

\textbf{We strongly recommend checking the video file in the supplementary material.} Some of the 360$^\circ$ results have been attached in the video of the supplementary material. In future official versions, we will provide more 360$^\circ$ results and comparisons.


\section{Training Objectives}
In this section, we present more details on training objectives. Please note that each loss in our loss function has a clear derivation from previous works. The theoretical derivation can be found in corresponding previous works, e.g., \cite{zhang2018unreasonable,tang2023dreamgaussian,zeng2024stag4d} etc. Hence, we will not provide a detailed derivation process in our manuscript.

\textbf{Score distillation sampling loss.} Following \cite{zeng2024stag4d}, we employ multi-view score distillation sampling (SDS) loss using rendered images under camera poses of pseudo multi-view images and input video.In detail, there are rendered multi-view images $I=\{I^{(i,1)},\dots,I^{(i,j)}\}$ under 6 camera poses, where $i$ denotes the timestamps and $j$ denotes the number of views. Formally, SDS loss can be defined as:

\begin{equation}
\begin{aligned}
      \mathcal{L}_{SDS} &=\alpha_1\mathcal{L}_{SDS}^{pseudo}+\alpha_2\mathcal{L}_{SDS}^{real}\\&=\alpha_1\mathcal{L}_{SDS}(\phi,I^{(i,j)})+\alpha_2\mathcal{L}_{SDS}(\phi,I_{real}^{i})  
\end{aligned}
\end{equation}

where $\alpha_1$ and $\alpha_2$ are hyperparameters, $I_{real}^{i}$ is the rendered image under camera pose of input video at time $i$. 

\textbf{Photometric loss.} Following \cite{tang2023dreamgaussian,zeng2024stag4d}, we compute the reconstruction loss $\mathcal{L}_{rec}$ between rendered images and pseudo multi-view images, and the foreground mask $\mathcal{L}_{mask}$.

\textbf{LPIPS loss.} We introduce the LPIPS loss $\mathcal{L}_{lpips}$ \cite{zhang2018unreasonable} to compute the feature similarity between pseudo multi-view images and corresponding rendered images. We leverage VGG \cite{simonyan2014very} as backbone to extract image features.

\textbf{Overall loss.} Based on above loss functions, we obtain the final overall loss $\mathcal{L}$:

\begin{equation}
    \mathcal{L} = \lambda_1\mathcal{L}_{SDS}+\lambda_2\mathcal{L}_{rec}+\lambda_3\mathcal{L}_{mask}+\lambda_4\mathcal{L}_{lpips}
\end{equation}

where $\lambda_1,\lambda_2,\lambda_3,\lambda_4$ are hyperparameters. 


\section{Training Details of DS4D-GA and DS4D-DA}
We train our two models DS4D-GA (using GA in TSSF) and DS4D-DA (using DA in TSSF) under the same training setting. During the initial 1,000 iterations, we train our models except TSSF and deformation MLP. Subsequently, the models with TSSF and deformation MLP are optimized over 6,000 additional iterations. For the deformation MLP, we employ each MLP with 64 hidden layers and 32 hidden features. The learning rate of TSSF and deformation MLP is set to $1.6\times10^{-4}$ and is decayed to $1.6\times10^{-6}$. Following \cite{zeng2024stag4d}, the top $2.5\%$ of points are densified with the most accumulated gradient. The overall training process costs approximately 3 hours on a V100 GPU. 

\begin{figure}[t]%
\centering
\includegraphics[width=0.45\textwidth]{figure/imagedream.pdf}
\caption{Example on low-quality images generated by ImageDream and high-quality images generated by Zero123++, both using the same input video.}\label{imagedream}
\end{figure}

\begin{table*}[t]
	\centering
	\renewcommand\arraystretch{1.}
	\scalebox{0.85}{
	\begin{tabular}{c|cccc|cccc}
        \Xhline{1.5px}
		\multirow{2}{*}{\textbf{Methods}} & \multicolumn{4}{c|}{\textbf{High-quality Input Images}} & \multicolumn{4}{c}{\textbf{Low-quality Input Images}} \\ \cline{2-9}
          &\multicolumn{1}{c}{\textbf{CLIP $\uparrow$}} & \multicolumn{1}{c}{\textbf{LPIPS $\downarrow$}} & \multicolumn{1}{c}{\textbf{FVD $\downarrow$}} & \multicolumn{1}{c|}{\textbf{FID-VID $\downarrow$}} & \multicolumn{1}{c}{\textbf{CLIP $\uparrow$}} & \multicolumn{1}{c}{\textbf{LPIPS $\downarrow$}} & \multicolumn{1}{c}{\textbf{FVD $\downarrow$}} & \multicolumn{1}{c}{\textbf{FID-VID $\downarrow$}}
         \\ \hline
        \textbf{STAG4D \cite{zeng2024stag4d}}  & 0.9078 & 0.1354 & 986.8271 & 26.3705 &  0.9026 & 0.1437 & 1311.8770 & 40.8664  \\        
        \hline
        \textbf{DS4D-GA (Ours)} & 0.9206 & 0.1311 & 799.9367 & 26.1794 & 0.9195 & 0.1380 & 849.8154 & 25.8726 \\
        \textbf{DS4D-DA (Ours)} & \textbf{0.9225} & \textbf{0.1309} & \textbf{784.0235} & \textbf{24.0492} & \textbf{0.9221} & \textbf{0.1339} & \textbf{805.4721} & \textbf{24.0623} 
	    \\ \Xhline{1.5px}
	\end{tabular}}
	\caption{Evaluation and comparison of the performance when facing low-quality input images and high-quality input images. The best score is highlighted in bold.}
\label{robustness_result}
\end{table*}
\section{Training Details on Discussion}
In this section, we present the training details of experiments about Discussion (Section 5) in our manuscript. For a fair comparison, the training settings are the same as 4D-GS \cite{wu20244d}.

\begin{figure*}[]%
\centering
\includegraphics[width=1.0\textwidth]{figure/imagedream_compare_results.pdf}
\caption{Qualitative comparison on 4D generation results with low-quality inputs. For each method, we render results under two novel views at three timestamps.}\label{imagedream_results}
\end{figure*}


\begin{figure}[h]%
\centering
\includegraphics[width=0.4\textwidth]{figure/points.pdf}
\caption{Comparison regarding whether using point clouds generated by a large reconstruction model as initialization. A: The model without point initialization. B: The model with point initialization.}\label{points}
\end{figure}

\textbf{Network Architecture.} We add our proposed DSFD in 4D-GS. Meanwhile, we directly replace the spatial-temporal structure encoder and multi-head Gaussian deformation decoder of 4D-GS with our TSSF. Since the multi-view sequences from Neu3D's dataset are authentic, we use TSSF-GA instead of TSSF-DA. Additionally, the hyperparameter settings of HexPlane are the same as 4D-GS. In detail, the basic resolution of HexPlane is 64, and is unsampled by 2 and 4. 



\textbf{Training Settings.} We use the same dense point cloud generated by SFM as 4D-GS. Then we also downsample point clouds lower than 100k. The learning rate of TSSF and deformation MLP are set to $1.6\times 10^{-3}$ which decreases to $1.6\times 10^{-5}$. The batch size is 1. Following 4D-GS, we do not use opacity reset operation. The overall training iterations spend 14000 for each scene.


Note that: a) Our DS4D includes point initialization (init.) via LRM. However, in Neu3D's data, common point cloud init. (e.g., 4D-GS) uses colmap rather than LRM. Thus, it is unfair to compare DS4D and 4D-GS due to the different init.. b) DSFD and TSSF are core contributions, thus inserting them into 4D-GS can directly demonstrate the effectiveness of our contributions in real-world scenes.



\section{More details on Datasets}
In this section, we present more details on \textbf{four datasets}.

\textbf{Consistent4D Dataset.} Following \cite{zeng2024stag4d}, we use seven 30-frame video in front view as input video, and their ground-truth with four novel views (azimuth angles of $-75^{\circ}$, $15^{\circ}$, $105^{\circ}$, and $195^{\circ}$, respectively) as evaluation. 

\textbf{Objaverse Dataset.} We random sample seven dynamic objects from \cite{deitke2023objaverse,liang2024diffusion4d}. The 24-frame ground truth under 360$^\circ$ cameras (the range of azimuth angles is $[0^{\circ}, 360^{\circ}]$) rendered from each object is used as evaluation. Meanwhile, we use seven 24-frame videos in front view as input video. Compared to Consistent4D Dataset, the object in Objaverse Dataset has more complex motion, e.g., suddenly waving at some time.

\textbf{Neu3D's Dataset.} We use three real-world scenarios from Neu3D's dataset \cite{li2022neural}. Each scene has 300 frames with 20 cameras, a total of 6000 high-quality images. Following \cite{wu20244d}, we use 300 frames under the front camera pose as evaluation, others are used as training videos.

\textbf{Data from Online Sources.} Following \cite{zeng2024stag4d}, we introduce some challenging videos from online sources for qualitative evaluation. Moreover, we also generate input videos by Stable Video Diffusion \cite{blattmann2023stable}. Each input video has 14 or 30 frames.

\section{Experiments}
In this section, we conduct more experiments to evaluate our method.


\subsection{Robustness of Our Methods} \label{robust}
In this section, we explore whether the quality of input images has a huge influence on the generation results of our methods. 

Specifically, we construct a dataset with low-quality input multi-view images. Using the same input video as Tab.1 of the main manuscript, we leverage ImageDream \cite{wang2023imagedream} to produce a series of multi-view sequences. Then, we select multi-view images with inconsistency or shape deformation from the generation multi-view sequences. These low-quality multi-view images are grouped as the low-quality input images. The example data of low-quality inputs can be seen in Fig.~\ref{imagedream}. The low-quality inputs has serious inconsistency between different timestamps and has color fading and texture blurry compared with the high-quality inputs we used in Tab.1 of the main manuscript (e.g., the shape and color of the suit). 

Based on the low-quality inputs, we compare our methods with STAG4D. The quantitative and qualitative results are shown in Tab.~\ref{robustness_result} and Fig.~\ref{imagedream_results}. Our methods when using low-quality inputs maintain a similar performance compared to our results when using high-quality inputs. However, in image metrics (LPIPS) and video metrics (FID and FID-VID), STAG4D when using low-quality inputs has a significantly worse performance compared to STAG4D using high-quality inputs. Furthermore, in Fig.~\ref{imagedream_results}, STAG4D generates the blurry textures in the back view. The reason is that back's texture details are blurry in the low-quality input at some timestamps (e.g., at $t_1$ of $v_3$). In contrast, our methods can generate results with clear textures. This indicates that our methods can handle the low-quality inputs better than STAG4D. Since our methods decouple dynamic-static information at the feature-level. Even though input low-quality data, thanks to the robustness feature extraction ability of DINOv2, we can still leverage the inherent differences between features to decouple. The differences include the change of motion, shape and textures between the corresponding two frames.

In summary, the above experiments demonstrate that the quality of input images has few influence on the generation results of our methods and verify the robustness of our methods when input low-quality data.



\subsection{More Analysis on Ablation Experiments}
\begin{figure}[t]%
\centering
\includegraphics[width=0.4\textwidth]{figure/visual_decouple.pdf}
\caption{Visualization on the heatmap of dynamic features in DSFD. The red region highlights the primary zone of interest in dynamic features.}\label{decouple}
\end{figure}

\begin{figure}[t]%
\centering
\includegraphics[width=0.45\textwidth]{figure/fusion.pdf}
\caption{Visualization on the score map of point features in TSSF-GA and TSSF-DA. The red area indicates model's high attention on dynamic information in point features of a specific view.}\label{fusion}
\end{figure}

\textbf{The effect on points initialization.} To validate the effect of point initialization, we add the point initialization to the baseline model, which is labeled as B (as shown in Table.2 of the manuscript). After performing point initialization, the performance of B model has improved on all metrics. Additionally, we visualize the Gaussian points from baseline model (labeled as A, as shown in Table.2 of the manuscript) and B in Fig.\ref{points}. Obviously, the Gaussian points of B are relatively uniform and denser in distribution than those of A, which ensures the stability of optimization and fidelity of the motion and shape fully learned by the model.



\textbf{The effect on LPIPS Loss.} To validate the effect of LPIPS Loss, we add the LPIPS Loss based on model B, labeled as C (as shown in Table.2 of the manuscript). The performance of C model has improved on all metrics. It indicates the effectiveness of LPIPS loss. 


\subsection{More Visualization}
In this section, we provide more visualizations of features in DSFD and TSSF, respectively.

\textbf{DSFD.} In Fig.\ref{decouple}, we supplement more heatmaps of dynamic features obtained by DSFD decoupling features from the current and reference frame features. The red area indicates the primary region of interest in the features. The dog's head movements indicate the main motion trends between the reference frame and the current frame. No matter what kind of novel views, dynamic features decoupled by our DSFD can accurately represent the dynamic zones. It once again demonstrates our method can acquire accurate dynamic features using DSFD.


\textbf{TSSF.} In Fig.\ref{fusion}, we supplement more score maps of selecting similarity dynamic information from point features of different views by TSSF-GA and TSSF-DA. Moreover, Fig.\ref{vis_fusion} shows the corresponding generation results at the current timestamp, including the same example as Figure.8 (b) in the manuscript and the same example in Fig.\ref{fusion}.

Specifically, the head movements of the person indicate the primary trend in motion between the middle and current time. The red area indicates the model's high attention on dynamic information in point features of a specific view. According to score maps based on different views, two approaches can capture a certain degree of similar dynamic information from different viewpoints. TSSF-GA is interested in both body and head in $v_1$, but TSSF-DA pays more concentration to the head. This is because TSSF-DA reduces the impact of novel views, resulting in TSSF-DA focusing more on capturing regions with a motion trend that is more similar to the front frame in other novel views. Thus, compared to TSSF-GA, TSSF-DA can produce results with clear details in the corresponding regions. For example, the hair texture of person at the top of Fig.\ref{vis_fusion}, and the leg texture of triceratops at the down of Fig.\ref{vis_fusion}. It once again indicates TSSF-DA can alleviate issues caused by novel views.



\begin{figure}[h]%
\centering
\includegraphics[width=0.45\textwidth]{figure/vis_fusion.pdf}
\caption{Qualitative comparison between model using TSSF-GA and model using TSSF-DA at current timestamp. Their corresponding visualization on the score map of point features is as shown in Fig.\ref{fusion} and Figure.8 (b) in our manuscript.}\label{vis_fusion}
\end{figure}

\begin{figure*}[]
    \centering
	\begin{minipage}{0.75\linewidth}
        \centerline{\includegraphics[width=1\linewidth]{figure/example2.pdf}}
	\end{minipage}
	\\ \vspace{0.5mm}
	\begin{minipage}{0.75\linewidth}
        \centerline{\includegraphics[width=1\linewidth]{figure/example1.pdf}}
	\end{minipage}
	\\ \vspace{0.5mm}
	\begin{minipage}{0.75\linewidth}
        \centerline{\includegraphics[width=1\linewidth]{figure/example3.pdf}}
	\end{minipage}
	\caption{More Results for 4D Generation using DS4D-GA and DS4D-DA.}
\label{show_results}
\end{figure*}

\begin{figure*}[t]
    \centering
	\begin{minipage}{1\linewidth}
        \centerline{\includegraphics[width=1\linewidth]{figure/compare_methods1.pdf}}
	\end{minipage}
	\\ \vspace{1mm}
	\begin{minipage}{1\linewidth}
        \centerline{\includegraphics[width=1\linewidth]{figure/compare_methods2.pdf}}
	\end{minipage}
	\caption{Qualitative comparison on video-to-4D generation. For each method, we render results under two novel views at two timestamps.}
\label{compare}
\end{figure*}



\begin{figure*}
  \centering
  \begin{subfigure}{0.48\linewidth}
    \scalebox{1.}{
    \includegraphics[width=1\linewidth]{figure/discussion1.pdf}}
    \caption{Sear steak}
    \label{s1}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
   \scalebox{1.}{
    \includegraphics[width=1\linewidth]{figure/discussion2.pdf}}
    \caption{Cook spinach}
    \label{s2}
  \end{subfigure}
  \caption{Visualization of real-world 4D scene generation compared with 4D-GS.}
  \label{discussion}
\end{figure*}


\subsection{More Qualitative Comparisons}

We supplement more qualitative comparisons in Fig.\ref{compare}. We compare our methods DS4D-GA and DS4D-DA with other SOTA methods, including Consistent4D \cite{jiang2024consistentd}, Dreamgaussian4D \cite{ren2023dreamgaussian4d}, STAG4D \cite{zeng2024stag4d}, SC4D \cite{wu2024sc4d}, 4Diffusion \cite{zhang20244diffusion}, and L4GM \cite{ren2024l4gm}. All the experiments of the methods are carried out using the code from their official GitHub repository.



\subsection{More Examples on 4D Content Generation}

We supplement more 4D content generation examples produced by DS4D-GA and DS4D-DA in Fig.\ref{show_results}. 






\subsection{More Examples on Real-World Scenario}

We supplement more real-world 4D scene generation examples using our method and baseline 4D-GS \cite{yang2023real} in Fig.\ref{discussion}. The experiments of 4D-GS are carried out using the code from their official GitHub repository.



\subsection{Time Consuming on Decoupling}
As mentioned in our manuscript, direct decoupling always costs considerable computation time. To intuitively evaluate the time-consuming advantage of our decoupling approach compared to direct decoupling, we provide the running time of each approach on decoupling dynamic-static features from a frame features with a 30-frame video, as shown in Tab.\ref{time}. Undoubtedly, our decoupling approach is about 14 times faster than direct decoupling.  
\begin{table}[]
	\centering
	\renewcommand\arraystretch{1.2}
	\scalebox{1}{
    \begin{tabular}{c|c}
	    \hline
	    \multicolumn{1}{c}{\textbf{Direct Decoupling}}&
		\multicolumn{1}{|c}{\textbf{Our Decoupling Approach}} \\ \hline
		118.326 (ms) & 8.241 (ms) \\ \hline
	\end{tabular}}
	\caption{Comparison of running time with different decoupling approaches. All approaches are tested on a NVIDIA 3090 GPU.}
\label{time}
\end{table}


\section{Limitations}
Limited to the resolution of input video, it is challenge for our method to produce high-resolution 4D contents , e.g., over 2K resolution. 


\newpage

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}