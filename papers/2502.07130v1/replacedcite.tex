\section{Related Work}
Long-term body identification models  can be categorized according to the approach they take 
to representing body shape information.
% In one approach, linguistic descriptors of bodies ____ are used with
% the goal of capturing body
% features in strings of words (e.g., curvy, broad-shoulders). In a second
% approach, 
% on body shape ____, body parts/wholes ____, and 3D body reconstruction ____.

\subsubsection{2D Body Shape from Images}
The most direct approach is to learn
a mapping from variable images of bodies
(view, clothing, illumination, distance) to identity
____.
The greatest challenge in this approach has been
the limited availability of training data with sufficient
variability (especially clothing sets) to learn the task
of long-term body identification.
Using a ResNet-50 model pretrained on ImageNet ____, the Clothing-Change Feature Augmentation (CCFA) approach  ____ augments
model training to form meaningful clothing variations in the feature
space. The augmented features maximize
the change of clothing and minimize the change of identity
by adversarial learning.
The effectiveness of CCFA was demonstrated with 
two standard CC-ReID datasets (PRCC-ReID ____ and  LTCC-ReID ____).

   \begin{figure}[thpb]
      \centering
      \includegraphics[scale=0.85]{distances_figure.png}
      \caption{Example body images from the BTS dataset ____. Subject consented to publication. }
      \label{BRIAR_images}
      \vskip -0.25cm
   \end{figure}

 The Non-linguistic Core ResNet Identity Model (NLCRIM) ____ was built on a ResNet-101 backbone pretrained with ImageNet ____.  NLCRIM was 
trained to map body images to identities using the BRIAR Research Set (BRS) 
 ____. It was  
evaluated with the BRIAR Test Set (BTS), which contained
identities viewed at multiple distances (up to 1000 meters)
that varied widely in yaw and pitch. Extreme pitch conditions
were captured from unmanned aerial vehicles (UAVs). All
test items included a change of clothing.
(See Figure \ref{BRIAR_images} for image examples). 
NLCRIM performed well
across all probe distance/pitch conditions. 
An improved version
of this model, with enhanced training 
and  substantially more training data, is tested in the present
study.

A similar direct approach to learning a mapping between 
whole body images and identity was taken in ____.  A ResNet-50 model was trained from scratch
with BRS data. This body encoder was embedded
in an end-to-end system that included a trained detector
model. The combined model performed well on the unconstrained BTS data.

The causality-based autointervention model (AIM1) was
proposed to mitigate clothing bias for robust clothes-changing person ReID
(CC-ReID) ____. Specifically, 
 the effect of clothing
on model inference was analyzed. A dual-branch structure of clothing and ID was
utilized to simulate the causal intervention process and 
was penalized by a causality loss. Progressively, clothing bias
was  automatically eliminated with model training, as AIM
 learned more discriminative identity clues that are
independent of clothing. The  superiority of the
 AIM approach over other approaches was demonstrated
 with two standard CC-ReID datasets (PRCC-ReID ____ and  LTCC-ReID ____). 
 


\subsubsection{3D Body Shape Features}
To overcome reliance on short-term cues in body images,
several models have attempted to reconstruct 3D body shapes for
identification (____).
In the 3D Shape Learning (3DSL) approach, a texture-insensitive 3D shape embedding is extracted
from a 2D image by adding 3D body reconstruction as an
auxiliary task and regularization ____. 
The use of the 3D reconstruction regularization
forces a  decoupling of the 3D body shape from the visual texture, 
enabling the model to acquire discriminative 3D
shape ReID features.  An adversarial self-supervised 
projection (ASSP) model is used to provide a 3D shape ground truth. The effectiveness of the approach was
demonstrated with common person ReID
datasets (e.g., Market1501 ____) and clothes-changing datasets (e.g., PRCC-ReID ____ and  LTCC-ReID ____).

In other work, the 3DInvarReID model ____ begins by
disentangling identity from
non-identity components (pose, clothing shape, and texture)
of 3D clothed humans. Next, 
 accurate 3D
clothed body shapes are reconstructed,
and discriminative  features
of naked body shapes for person ReID are learned. The model was found to be effective for disentangling identity and non-identity features in 3D clothed
body shapes,  using a dataset (CCDA ____) that contains a wide variety of human activities and clothing changes.

\subsubsection{Linguistic Models} 
Body models based on linguistic descriptors  (e.g., “curvy,” “long-legged”)  encode  shape via the complex myriad of features captured by single and small groups of words ____. 
Work in psychology ____ and computer graphics ____ has  demonstrated that a linear mapping can be learned from human-generated body descriptions (27 words) to the coefficients of a PCA trained with 3D body scans ____. Motivated by this finding, the Linguistic Core ResNet Identity Model (LCRIM)
 was developed using an ImageNet pretrained ResNet augmented
with linguistic annotation pretraining. This linguistic core was then trained to map images to identity ____. Although the LCRIM model performed at a level similar to NLCRIM, the fusion of the two models performed substantially better than either model alone. This suggests that the two models encode complementary 
information about body shape.

In related work, linguistic body descriptions were
leveraged for ReID in CLIP3DReID ____. This was done by integrating human descriptions with visual perception using a pretrained CLIP model. CLIP was used to automatically label body shapes with linguistic descriptors. A student model's local visual features were then aligned with shape-aware tokens derived from CLIP's linguistic output. The CLIP image encoder and the 3D SMPL ____ identity spaces were used in combination to align the global visual features. The effectiveness
of CLIP3DReID was demonstrated using  PRCC-ReID ____ and  LTCC-ReID ____.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%