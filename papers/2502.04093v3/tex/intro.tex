\section{Introduction}
\label{sec:intro}

The increasing volume of scientific data generated by simulations, instruments, and observations has outpaced the data processing, storage, and transfer capabilities of modern computer systems, including both workstations and supercomputers. 
For example, in climate research, the Coupled Model Intercomparison Project (CMIP)~\cite{cmip} aims to advance our understanding of the climate system by coordinating standardized experiments with Earth System Models (ESMs) and thus enabling comprehensive comparisons of how different models represent past, present, and future climate conditions. Thanks to the rapid evolution of leading HPC systems, each successive phase of CMIP has seen significant increases in data volume -- CMIP3 generated around 40 TB~\cite{cmip-surrogate, cmip6-plan}, CMIP5 about 2 PB~\cite{cmip6-plan}, and CMIP6~\cite{cmip6-dashboard} exceeding 28 PB. Such an increase in volume poses unprecedented challenges to store, process, and analyze the data.

Data compression, particularly lossy compression, has emerged as a critical tool to mitigate these challenges for scientific applications~\cite{sz16, sz17, qoisz, mgard, sperr, zfp, cuZFP, zfp-particle1, zfp-particle2, tthresh}, enabling scientists to fully harness the ever-increasing performance of new computing systems. Lossy compression reduces data size by approximating original information and discarding less critical details. This process results in smaller file sizes with an acceptable loss of fidelity. Unlike domains such as natural images and videos, lossy compression designed for scientific scenarios often includes the ability to restrict the maximum point-wise error, which is essential for preserving the accuracy of scientific computations. For example, the SZ lossy compressors~\cite{interp} can reach 10$\sim$100 times higher compression ratios than lossless alternatives while keeping the maximum error within a predefined bound~\cite{sdrbench}.
% it is essential for scientific applications to reduce their data volume dramatically by adopting techniques such as lossy compression. 

\textit{\underline{Motivation:}}
Although many scientific lossy compression solutions have been proposed in the recent decade, most of them only support decompression to a single fidelity level once the data is compressed, which restricts their broad utilization.
% \begin{itemize}
    % \item 
On the one hand, scientific analyses often require different data fidelity levels as the subject of the study or the stage of investigation has diverse tolerance for data precision. For example, in hydrodynamic simulation, reconstructing viscosity may require a $2^{-5}$ finer precision compared with reconstructing vorticity from the same data field~\cite{peter-tvcg24-pframework}.
% Without progressive retrieval capability, researchers must conservatively preserve high precision for the data. 
As a result, without progressive capability, researchers must compress data conservatively at the highest possible fidelity -- even though only a handful of analyses truly require it -- ultimately diminishing the overall effectiveness of data reduction.
% This results in compressing data at the highest possible fidelity, even though only a few analyses require it, thereby reducing the overall data reduction effectiveness.
    % Without multi fidelity decompression capability, researchers have to be very conservative on compression and over-preserve the precision, meaning that the data is compressed to the maximum possible but rarely used precision setting, which unavoidable downgrades the data reduction effectiveness.
    % \item 
On the other hand, when analyzing multiple snapshots, fields, or regions, researchers often first identify patterns or areas of interest at a coarse level before committing resources to detailed analysis~\cite{peter-tvcg24-pframework, pmgard, pmgard-qoi}. Without the ability of progressive retrieval, scientists have to always load the entire compressed data and decompress it at full precision. This not only increases the time and resources required for data loading and decompression but also delays subsequent analyses and scientific discoveries.
% \end{itemize}

% challenges such as lacking the ability to retrieve data at different fidelity levels still restrict their utilization in the scientific domain. 
% Progressive data retrieval allows users to access coarse approximations of data quickly and then incrementally refine these approximations to higher fidelity. 
\textit{\underline{Limitation of state-of-art approaches:}}
Despite the necessity of progressive retrieval in scientific compression, as highlighted in the above scenarios, supporting this functionality is challenging due to several factors. 
% \begin{itemize}
First, achieving both high fidelity and high compression ratios simultaneously is challenging, as these objectives often depend on fundamentally different algorithms, where optimizing one may come at the expense of the other.
Second, straightforward progressive techniques often introduce significant operational overhead, requiring multiple passes of decompression and reprocessing -- contradicting the goal of progressive decompression, which is to save time and resources.
Third, it is already non-trivial to guarantee error bounds for partial decompression, not alone to say the progressive technique which requires the data accumulated from multiple levels to be within acceptable error margins.
Consequentially, few such progressive solutions exist, and they often fall short due to low compression ratios, high operational costs, and a lack of stringent error restrictions. These limitations hinder their adoption in scientific applications.

\textit{\underline{Key insights and contributions:}}
In this paper, we propose the first high ratio, fast, and error-bounded progressive lossy compression solution based on the interpolation algorithm. Our contributions are three-fold:
% We are the first to design a progressive solution based on the interpolation-based lossy compression algorithm.
\begin{itemize}
\item We thoroughly analyze the error characteristics to build a progressive compressor based on the interpolation algorithm and propose our progressive solution, IPComp, based on the prediction model with multi-level bitplane and predictive coding. Besides having high effectiveness on retrieval, our solution supports retrieval under arbitrary error-bound settings, and it only executes decompression once for each retrieval request, compared with residual-based alternatives, which support a limited number of error bounds and require multiple passes of decompression for a single request.
\item We derive optimized strategies towards minimum data retrieval under different fidelity levels indicated by users through error bounds and bit rate. Our strategies are highly effective while being extremely lightweight with negligible overhead to the scientific workflow.
\item \textit{\underline{Experimental methodology and artifact availability:}} We evaluate the proposed solution using six real-world datasets from four diverse domains over four state-of-the-art compressors. Experimental results demonstrate our solution archives up to $487\%$ higher compression ratios and $698\%$ faster speed than other state-of-the-art progressive compressors and reduces the data volume for retrieval by up to $83\%$ compared to baselines under the same error bound and reduces the error by up to $99\%$ under the same bitrate. The source code of IPComp will be available to the public upon acceptance of the paper.
\end{itemize}

\textit{\underline{Limitations of the proposed approach:}}  
The primary focus of our solution in this paper is to design an effective progressive approach that is universally applicable across different hardware platforms. As a result, our method does not include hardware-specific optimizations yet, such as speed acceleration using tensor cores. Incorporating such optimizations will be part of our future work.

The remainder of the paper is structured as follows. In Section \ref{sec:related}, we provide an overview of related work. Section \ref{sec:overview} formulates the research problem and the overview of our design. Our developed progressive compressor is detailed in Section \ref{sec:design} to Section \ref{sec:optimizer}. Section \ref{sec:evaluation} presents and discusses the evaluation results. Finally, we draw conclusions in Section \ref{sec:conclusion}.

% \section{Research Background}
% \label{sec:background}



\section{Related Work}
\label{sec:related}
In this section, we survey existing approaches to scientific lossy compression and discuss methods that apply progressive retrieval to enhance these compression techniques.

Scientific lossy compressors aim to reduce data size while guaranteeing that the loss or error remains under a specified threshold required by the applications. 
% Most of such compressors can be categorized into two groups -- prediction-based and transformation-based. Prediction-based compressors 
Many scientific lossy compressors apply statistical models, including linear regression~\cite{sz17}, interpolation~\cite{interp, qoz, qoz2}, and neural networks~\cite{coordnet, aesz, srnnsz} to predict the value of the variable based on the value of the coordinates. The statistical model will be stored to reconstruct the value such that the storage of original data could be eliminated. To bound the error, the difference between prediction and real value is quantized and stored together with the model. 
% Transformation-based compressors,
On the other hand, instead of finding prediction models, some compressors choose to transform data to another domain that is more compressible, and then keep partial of the transformed data based on the error bound. Examples of transformation methods include wavelet transform~\cite{sperr}, orthogonal discrete transform~\cite{zfp}, and singular value decomposition~\cite{tthresh}. 

Progressive techniques for compression have evolved along two main directions: multi-resolution approaches, which produce output in various sizes, and multi-precision approaches, which generate output in various precisions.

For multi-resolution approaches, researchers have employed tree structures~\cite{zfp-particle2, precision-resolution-tree}, adaptive meshes~\cite{peter-mesh-tvcg22}, and wavelet transforms~\cite{li2019vapor} to partition the spatial domain hierarchically, which enables gradual refinement of data resolution. Such techniques are primarily designed for space partitioning and visualization tasks. 
Although they can support a variety of compression algorithms by applying them to each partitioned block, these methods often suffer from low compression ratios and slower speeds due to the storage and computational overhead introduced by the hierarchical structures. More importantly, they could not bound the error in each data point since the output is smaller than the original.

On the other hand, in the direction of multi-precision retrieval, one straightforward solution is to progressively refine the lossy compression error~\cite{peter-tvcg24-pframework}. 
Specifically, this involves executing compression multiple times, with each pass compressing the residual error from the previous pass but with a smaller error bound. 
This progressive strategy is orthogonal to the underlying compression method and offers versatility. However, it does not fully exploit the strengths of the base algorithm, and it incurs high operational costs as decompression must be executed multiple times to achieve a given fidelity level. As a result, researchers also aim to exploit progressive characteristics inherent in specific compression algorithms. PMGARD~\cite{pmgard, pmgard-qoi}, for instance, is a progressive solution based on the MGARD compressor. Such a solution may lead to sub-optimal performance, as shown in~\Cref{sec:evaluation}, because its underlying MGARD algorithm has lower compression ratios and speed than other state-of-the-art in many datasets~\cite{interp, qoz, qoz2}.
% For both prediction-based and transformation-based compressors, their key goal is to decorate the data to make it more compressible. Most of the decorrelation algorithms in scientific compressors are designed and optimized for Cartesian grids, and they exhibit sub-optimal results on particle data~\cite{mdz}.


