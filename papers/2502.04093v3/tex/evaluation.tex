\section{Experimental Evaluation}
\label{sec:evaluation}
In this section, we describe the experimental setup and evaluate our solution on six datasets against four state-of-the-art baseline compressors.

\subsection{Experimental Setting}

\subsubsection{Execution Environment} The experiments are performed on the Purdue Anvil supercomputer~\cite{anvil} through NSF ACCESS~\cite{access}. Each computing node in Anvil features two AMD EPYC 7763 CPUs with 64 cores at a 2.45GHz clock rate and 256 GB DDR4-3200 RAM. 

\begin{table}[ht]

% \vspace{-4mm}
    \centering
    % \footnotesize
    \caption{Data in our experiments}
    \vspace{-3mm}
    \begin{adjustbox}{width=\columnwidth}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
Name & Explanation & Precision & Shape  \\ \hline
Density~\cite{sdrbench} & mass per unit volume in turbulence & 64 & $256\times384\times384$ \\ \hline
Pressure~\cite{sdrbench} & thermodynamic pressure in turbulence & 64 & $256\times384\times384$ \\ \hline
VelocityX~\cite{sdrbench} & x-direction velocity in turbulence & 64 & $256\times384\times384$ \\ \hline
Wave~\cite{interp} & wavefield evolution in seismic & 64 & $1008\times1008\times352$ \\ \hline
SpeedX~\cite{sdrbench} & x-direction wind speed in weather  & 64 & $100\times500\times500$ \\ \hline
CH4~\cite{sdrbench} & mass fraction of CH4  in combustion & 64 & $500\times500\times500$ \\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:apps}
\vspace{-3mm}

\end{table}

\subsubsection{Datasets} The experiments are evaluated on six datasets across four diverse scientific domains, as listed in~\Cref{tab:apps}.

\subsubsection{State-of-the-Art lossy compressors in our evaluation}
\label{sec: exp-baseline}
The experiments include four SOTA scientific lossy compressors as baselines -- SZ3-M, SZ3-R, ZFP-R, and PMGARD. 

% \begin{itemize}
\noindent\textbf{SZ3-M}~\cite{pmgard-qoi}: SZ3-M (where "M" stands for multi-fidelity) is the straightforward multi fidelity version of SZ3 based on multiple outputs. SZ3 is the leading non-progressive scientific lossy compressor. SZ3 uses interpolation as prediction, together with linear-scale quantization, Huffman coding, and zstd lossless coding~\cite{rfc8478}. SZ3 has excellent compression ratio and fidelity over others while its speed may not be as fast as ZFP.  SZ3-M compresses the input with different error bounds independently and groups those compressed data all together as output. Such a solution supports multi-fidelity retrieval but is not progressive yet, as it cannot reuse the low-fidelity data to build high-fidelity results. 

\noindent\textbf{SZ3-R}~\cite{pmgard-qoi, peter-tvcg24-pframework}: SZ3-R (where "R" stands for residual) is the progressive version of SZ3 based on residuals. It first compresses the input with a large bound, then compresses the lossy error (residual) from the last time using a smaller bound, and repeats such residual compression until a targeted bound is reached. 

\noindent\textbf{ZFP-R}~\cite{peter-tvcg24-pframework}: ZFP-R is the progressive version of ZFP based on residuals. It handles residuals the same way as SZ3-R. ZFP~\cite{zfp} is the leading transform-based lossy compressor based on orthogonal transformation. ZFP is usually the fastest scientific lossy compressor because of its highly efficient transform function, although its compression ratio may not be as high as others.

\noindent\textbf{PMGARD}~\cite{pmgard, pmgard-qoi}: PMGARD (where "P" stands for progressive) is the progressive version of the MGARD~\cite{pmgard} compressor. MGARD~\cite{mgard, mgardx} is a multigrid-based lossy compressor that uses hierarchical decomposition to remove redundancy in scientific data while preserving error bounds.
% \end{itemize}

% Since most compressors do not inherently support progressive compression and decompression, an alternative generalized approach has been proposed, which is based on residual compression. 
% This method works by first compressing the data, then decompressing it to obtain an approximation, computing the residual by comparing it with the original data, and finally compressing the residual with higher precision.
We note that the residual-based approaches (SZ3-R and ZFP-R) have drawbacks that affect our evaluation. 
First, they have limited error-bound flexibility. More specifically, the retrieval is only possible at a few predefined error bounds. This creates a trade-off -- setting too many error bounds reduces the overall compression ratio and significantly degrades compression/decompression throughput, while setting too few nodes limits flexibility in selecting error bounds during decompression. For the experiments, we configure five error bounds for them, where each successive one is a factor of $2^2$ (4Ã—) apart. Specifically, the error bounds are set to $2^{16}eb$, $2^{14}eb$, $2^{12}eb$, $2^{10}eb$, $2^{8}eb$, $2^{6}eb$, $2^{4}eb$, $2^{2}eb$, and $eb$.
Second, those methods are not well-suited for retrievals based on pre-defined bitrates, as the residuals are compressed by error bounds, and their size is not aligned with the bitrate of the retrieval requests. As a result, we select the largest residual that fits within the user-specified bitrate constraint for SZ3-R and ZFP-R in the experiments.
% For the sake of completeness in our experiments, we include residual-compressed versions of the current SOTA compressors. These variants are denoted with an "R" suffix. For example, the residual compression version of SZ3 is referred to as SZ3-R.


\subsection{Evaluation Results and Analysis}
The evaluation covers three aspects -- compression ratio, progressive retrieval effectiveness under error-bound or bitrate constraints, and compression and decompression/retrieval speed.  To be more specific, the compression ratio comparison demonstrates our solution IPComp leads to the smallest compressed data size. The progressive retrieval evaluation shows that IPComp requires the lowest data volume to reconstruct to the same fidelity compared with others, and IPComp leads to the highest reconstruction fidelity under the same bit rate budget. The speed test confirms IPComp's high compression efficiency, especially compared to residual-based alternatives (SZ3-R and ZFP-R) whose speed drops significantly when the number of residual compressions increases.


\begin{figure}[ht] \centering
% \vspace{-2mm}
\includegraphics[scale=0.14]{figures/cr/legend.png}

\hspace{-8mm}
\subfigure[High precision setting ($eb = 1\mathrm{e}{-9}$)]{
\includegraphics[scale=0.48]{figures/cr/cr_1e-9s.png}
}
\hspace{-4mm}
\subfigure[High ratio setting ($eb=1\mathrm{e}{-6}$)]{
\includegraphics[scale=0.48]{figures/cr/cr_1e-6s.png}
}
\hspace{-8mm}
% \subfigure[error bound 1e-3]{
% \includegraphics[scale=0.5]{figures/cr/cr_1e-3.png}
% }
% \vspace{-2mm}
\caption{Our compressor IPComp leads the compression ratio among all baselines}
\label{fig:cr}
% \vspace{-4mm}

\end{figure}


\begin{figure}[ht] \centering

\includegraphics[scale=0.14]{figures/bitrate_eb/legend_01.png}

% \hspace{-4mm}
\subfigure[Density]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/bitrate_eb2/Density.bitrate.png}}
}
% \hspace{4mm}
\subfigure[Pressure]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/bitrate_eb2/Pressure.bitrate.png}}
}
% \hspace{-8mm}
\vspace{-3mm}

% \hspace{-4mm}
\subfigure[VelocityX]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/bitrate_eb2/VelocityX.bitrate.png}}
}
% \hspace{4mm}
\subfigure[Wave]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/bitrate_eb2/RTM.bitrate.png}}
}
% \hspace{-8mm}
% \vspace{-2mm}

\vspace{-3mm}
% \hspace{-4mm}
\subfigure[SpeedX]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/bitrate_eb2/Hurricane.bitrate.png}}
}
% \hspace{4mm}
\subfigure[CH4]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/bitrate_eb2/CH4.bitrate.png}}
}
\vspace{-3mm}
\caption{Our compressor IPComp takes the smallest amount of data to reconstruct toward the same $L_\infty$ error compared with all the baselines. Moreover, IPComp supports arbitrary error bound, while SZ3-R and ZFP-R are limited to a few pre-defined bounds}
\label{fig:bitrate_eb}
\vspace{-2mm}

\end{figure}


\subsubsection{Compression Ratios}
% First, to provide greater flexibility and allow users to specify a wider range of bitrates or target errors, we need to progressively compress the data with an extremely small, near-lossless error. Additionally, the data should be divided into packages to enable progressive loading.

CR is a key metric for evaluating any compressor, regardless of whether it is progressive or not. As discussed in~\Cref{sec:intro}, the fundamental goal of compression is to reduce data size, as smaller data sizes alleviate the burden of storage, transfer, and processing on applications. 



\Cref{fig:cr} presents the compression ratios of all solutions under the same error-bound settings. We select two error bounds which are 1e-6 and 1e-9  to cover both high precision (cr<10) and high ratio (cr up to 50) cases.
% high-precision compression, as it directly impacts the effectiveness of our approach in maintaining fine-grained control over the trade-off between compression and accuracy.
% The first figure evaluates our compression efficiency in high-fidelity scenarios. We provide two experimental settings, using relative error bounds of 1e-6 and 1e-9 as the base error bounds. 
% Our design, operating in error bound-based mode, allows users to specify any target error bound greater than the base error bound, ensuring flexibility in controlling compression precision while maintaining high efficiency.
As this figure shows, our solution IPComp achieves a compression ratio advantage of around 20\% to 500\% over other progressive compressors across the vast majority of datasets. This allows users to perform progressive decompression while utilizing the least possible storage space.

% For this experiment, we configure the residual compression (R) versions of SZ3, ZFP, and SPERR by setting the error bounds at five anchor points, where each successive anchor point is a factor of $2^4$ (16Ã—) apart. Specifically, the error bounds are set to $2^{16}eb$, $2^{12}eb$,  $2^{8}eb$,  $2^{4}eb$, and $eb$, ensuring a range of progressively finer compression levels.

In fact, although not shown in the figure, IPComp achieves an even higher compression ratio than the non-progressive SZ3 particularly in high-precision scenarios, despite both being based on the interpolation prediction algorithm.
The primary reason for this improvement lies in the encoding process. The non-progressive SZ3 uses Huffman entropy encoding, which is then further compressed using zstd. Since Huffman coding assigns variable-length codes based on frequency, it may disrupt certain repetitive patterns in the byte or word level after encoding. This, in turn, reduces the effectiveness of zstdâ€™s lossless compression, as zstd relies on detecting and exploiting repetitive patterns at the byte/word level to achieve higher compression efficiency. ~\cite{rfc8478}
In contrast, our solution IPComp employs a customized predictive encoding method, discussed in~\Cref{sec:design coding}, followed by zstd for entropy encoding and pattern extraction. By avoiding the disruption caused by Huffman and preserving more repetitive structures, our method enhances the final compression ratio.






\begin{figure}[ht] \centering

\includegraphics[scale=0.14]{figures/PWE_BITRATE/legend_01.png}

% \hspace{-4mm}
\subfigure[Density]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/PWE_BITRATE2/Density.bitrate.png}}
}
% \hspace{4mm}
\subfigure[Pressure]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/PWE_BITRATE2/Pressure.bitrate.png}}
}
% \hspace{-8mm}
\vspace{-3mm}

% \hspace{-4mm}
\subfigure[VelocityX]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/PWE_BITRATE2/VelocityX.bitrate.png}}
}
% \hspace{4mm}
\subfigure[Wave]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/PWE_BITRATE2/RTM.bitrate.png}}
}

\vspace{-3mm}

% \hspace{-4mm}
\subfigure[SpeedX]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/PWE_BITRATE2/Hurricane.bitrate.png}}
}
% \hspace{4mm}
\subfigure[CH4]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/PWE_BITRATE2/CH4.bitrate.png}}
}
% \vspace{-4mm}
\caption{Under given bitrate budget, our solution IPComp reconstructs the highest fidelity output than all the baselines (a lower lossy error indicates a high fidelity)}
\label{fig:pwe_bitrate}
\vspace{-3mm}

\end{figure}

% \subsubsection{Progressive retrieval by size (fix bitrate mode)}
\subsubsection{Progressive retrieval efficiency}
After checking the compression ratio, next we evaluate the decompression or retrieval efficiency. Since users may want retrieval by fidelity (specified as error bounds) or size (specified as bitrate), our evaluation is split into two figures for those two scenarios.

\Cref{fig:bitrate_eb} demonstrate the evaluation results of the error bound mode. Compressors with higher efficiency should result in smaller data retrieval size under the error restrictions. The retrieval size is shown as bitrate in~\Cref{fig:bitrate_eb}, so lower lines in the figure indicate better results. 
% This experiment effectively measures the compression efficiency of our progressive compression approach. We use the same residual compression (-R) setup as in previous tests to provide a fair comparison.


\Cref{fig:pwe_bitrate} presents the results in fixed bitrate mode. Compressors with higher efficiency should result in higher fidelity (indicated by lower error) with the same retrieval size, still lower lines in the figure indicate better results. 
% For this experiment, we use the same residual compression (-R) setup as in the previous test, with error bound anchor points spaced by a factor of 16Ã—, totaling five anchor points. 
Since our solution is the only progressive compressor that directly supports the fixed bitrate mode, manual efforts are needed to test other baselines. We enable fix bitrate mode for residual-based compressors by selecting the largest anchor point that fits within the user-specified bitrate constraint. Similarly, for PMGARD, we manually define anchor points ranging from $2^{16}eb$, $2^{15}eb$, $2^{14}eb$ down to $eb$, to allow for bitrate-based decompression.

As shown in both~\Cref{fig:bitrate_eb} and~\Cref{fig:pwe_bitrate}, our compressor IPComp demonstrates its superior compression efficiency by consistently achieving the smallest data load size under the same max error, and the lowest max error under the same bitrate budget.
This advantage primarily comes from the predictive coding method discussed in~\Cref{sec:design coding} and the optimized loading strategy discussed in~\Cref{sec:optimizer}. The residual-based solutions SZ3-R and ZFP-R have a staircase line in both of the figures because the fidelity of such solutions is limited to a few pre-defined residual levels. In comparison, our solution provides higher flexibility as it supports retrieval on arbitrary fidelity and bitrate.

% Our method provides two key advantages over these approaches. On one hand, our approach allows users to specify arbitrary bitrate for decompression, while residual-based methods can only decompress at predefined bitrates. 

% On the other hand, our method can directly decompress data to match the given bitrate, whereas residual-based approaches require multiple decompression attempts until the loaded data exceeds the target bitrate, leading to significantly higher overhead.





% \begin{figure*}[ht] \centering
% \includegraphics[scale=0.1]{figures/visualization/density_birdview_3.png}
% % \hspace{-8mm}
% \vspace{-2mm}
% \caption{xxx}
% \label{fig:visual}
% \vspace{-2mm}
% \end{figure*}

\begin{figure}[ht] \centering
\hspace{1.2mm}
\includegraphics[scale=0.115]{figures/throughput/legend05.png}

\hspace{-8mm}
\subfigure[Compression]{
\raisebox{-1cm}{\includegraphics[scale=0.45]{figures/throughput/speed_cmp.png}}
}
\subfigure[Decompression]{
\raisebox{-1cm}{\includegraphics[scale=0.45]{figures/throughput/speed_dcmp.png}}
}
% \hspace{-5mm}
% \vspace{-4mm}
\caption{Our solution IPComp has the fastest speed in both compression and decompression than all others except for SZ3-M which is multi fidelity but not progressive}
\label{fig:throughput}
% \vspace{-3mm}
\end{figure}

\begin{figure}[ht] \centering
\vspace{3mm}
\hspace{6mm}
\includegraphics[scale=0.12]{figures/residual/residual_legend.png}
\subfigure[Compression]{
\includegraphics[scale=0.5]{figures/residual/residual_cmp_throughput_s.png}
}
% \hspace{6mm}
\subfigure[Decompression]{
\includegraphics[scale=0.5]{figures/residual/residual_dcmp_throughput_s.png}
}
% \subfigure[error bound 1e-3]{
% \includegraphics[scale=0.5]{figures/cr/cr_1e-3.png}
% }
% \vspace{-4mm}
\caption{The speed of residual-based progressive solutions (SZ3-R, ZFP-R, etc) will drop significantly when the setting of residual count increases}
\label{fig:residual}
% \vspace{-2mm}

\end{figure}


\subsubsection{Speed}

We evaluate the compression and retrieval speed of all the compressors. To ensure a fair comparison, we use $eb=10^{-9} \times \text{Range}(dataset)$ to compress and retrieve the data for all compressors, except for PMGARD as it is a lossless compress with lossy retrieval by design. To reach the target $eb$, residual-based compressors need to compress and decompress in multiple iterations.
The speed tests are shown in~\Cref{fig:throughput}. It confirms our solution IPComp is up to around 300\% faster than other approaches in most cases, except for SZ3-M. SZ3-M supports multi fidelity by compressing input in different error bounds and storing all those outputs together. Such a solution is not progressive since the retrieval cannot use previous passes of data. As a result, its compression ratio and retrieval efficiency are both extremely limited in the experiments.
We include one additional compressor SPERR-R in this figure. SPERR-R is the residual progressive implementation of the wavelet-based lossy compressor SPERR~\cite{sperr}. The speed of SPERR-R is extremely slow -- less than 50 MB/s in more than half of the cases. Such a slow speed will offset the savings of reduced data size for I/O, and potentially slow down the scientific workflow. As a result, we do not include SPERR-R as one of our baselines in the full evaluation.



Another evaluation in this section is the speed of residual-based compressors. As discussed in~\Cref{sec: exp-baseline}, residual-based ones must compress and decompress multiple times based on the number of pre-defined error bounds. More pre-defined error bounds would provide more flexibility in retrieval, however, as \Cref{fig:residual} shows, this will instead reduce their speed significantly. We also observe from \Cref{fig:residual} that the residual results are curved instead of straight lines. The reason is that although having more predefined error bounds increases the number of iterations, each iteration takes less time because the looser bounds result in a smaller range of quantized integers. However, the total time still increases significantly due to the cumulative effect of all iterations.
This drawback of residual-based compressors highlights the advantages of our solution which can deliver highly flexible retrieval at high speed.
% However, more anchor points require more compression passes, which significantly impacts overall compression speed. The cumulative overhead of multiple compression steps outweighs the per-step speed gains, leading to a nonlinear slowdown in both compression and decompression throughput.

% Despite this, our solution (red baseline in~\Cref{fig:residual}) supports arbitrary bitrate and target error bound decompression while maintaining high compression and decompression speeds. This highlights another key advantage of our method, which achieves both high flexibility and efficient compression/decompression throughput.
% This highlights the advantage of our approach over residual-based compression schemes, which inherently suffer from speed degradation as the number of anchor points increases.

% A key challenge with residual compression lies in the trade-off between flexibility and the number of compression passes. When users require greater flexibilityâ€”for example, to fine-tune the amount of loaded dataâ€”this is often due to I/O bandwidth limitations, where users aim to load only the necessary data while minimizing excess reads. However, the downside is that compression and decompression speed will significantly decrease, introducing a new bottleneck in the system. This highlights another key advantage of our method, which achieves both high flexibility and efficient compression/decompression throughput.

% To further analyze this issue, we conducted additional tests, as shown in Fig. \ref{fig:residual}. When increasing the number of residual compression anchor points, we observe a nonlinear and significant decrease in compression speed.












\begin{figure}[ht] \centering
\includegraphics[scale=0.14]{figures/psnr/legend_02.png}

\hspace{-4mm}
\subfigure[Density]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/psnr/Density.psnr.png}}
}
% \hspace{4mm}
\subfigure[Pressure]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/psnr/Pressure.psnr.png}}
}
% \hspace{-8mm}
\vspace{-3mm}

\hspace{-4mm}
\subfigure[VelocityX]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/psnr/VelocityX.psnr.png}}
}
% \hspace{4mm}
\subfigure[CH4]
{
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures/psnr/CH4.psnr.png}}
}
% \subfigure[Wave]
% {
% \raisebox{-1cm}{\includegraphics[scale=0.6]{figures/psnr/RTM.psnr.png}}
% }
% % \hspace{-8mm}
% % \vspace{-2mm}

% \vspace{-3mm}
% \hspace{-4mm}
% \subfigure[SpeedX]
% {
% \raisebox{-1cm}{\includegraphics[scale=0.6]{figures/psnr/Hurricane.psnr.png}}
% }
% \hspace{4mm}
\vspace{-2mm}
\caption{Our solution leads to higher PSNR under the same bitrate retrieved in most of the cases compared with other solutions}
\label{fig:psnr}
\vspace{-4mm}
\end{figure}

\subsubsection{Progressive retrieval for PSNR}
We also evaluate the PSNR of the compressed data. Since PSNR is mathematically related to the $L_2$ norm error, it serves as another important metric for assessing compression fidelity.
Although our design primarily targets the $L_\infty$ norm and we do not explicitly optimize for PSNR, \Cref{fig:psnr} shows our approach still maintains competitive or superior PSNR compared to other baselines.

\begin{figure}[ht] \centering

% \includegraphics[scale=0.036]{figures/visualization/density_birdview_ori_01.png}
% \includegraphics[scale=0.036]{figures/visualization/density_birdview_ori_03.png}
% \includegraphics[scale=0.036]{figures/visualization/density_birdview_ori_10.png}

\vspace{-4mm}
\subfigure[Curl (0.1\% retrieved)]{
\raisebox{-1cm}{\includegraphics[scale=0.034]{figures/visualization/density_birdview_vtx_01.png}}
}
\subfigure[Curl (0.3\% retrieved)]{
\raisebox{-1cm}{\includegraphics[scale=0.034]{figures/visualization/density_birdview_vtx_03.png}}
}
\subfigure[Curl (1\% retrieved)]{
\raisebox{-1cm}{\includegraphics[scale=0.034]{figures/visualization/density_birdview_vtx_10.png}}
}

% \vspace{2mm}
\subfigure[Laplace (0.1\% retrieved)]{
\raisebox{-1cm}{\includegraphics[scale=0.034]{figures/visualization/density_birdview_laplacian_01.png}}
}
\subfigure[Laplace (0.3\% retrieved)]{
\raisebox{-1cm}{\includegraphics[scale=0.034]{figures/visualization/density_birdview_laplacian_03.png}}
}
\subfigure[Laplace (1\% retrieved)]{
\raisebox{-1cm}{\includegraphics[scale=0.034]{figures/visualization/density_birdview_laplacian_10.png}}
}
% % \hspace{-8mm}
% \includegraphics[scale=0.055]{figures/visualization/density_birdview_3.png}
% \hspace{-8mm}
\vspace{-4mm}
\caption{The visualization of two post-analysis metrics Curl and Laplace on the same Density data. Loading 0.3\% is fine for Curl but Laplace requires 1\% -- demonstrating the necessity of progressive retrieval}
\label{fig:visual}
\vspace{-6mm}
\end{figure}

\subsubsection{Visual quality}
Additionally, we evaluate the visualization quality of the reconstructed data as shown in \Cref{fig:visual}. We load 0.1\%, 0.3\%, and 1.0\% from the same data, and assess the impact on visualization of Curl and Laplacian. While loading 0.3\% of data is enough for Curl in terms of visualization, 1\% of data is needed for Laplacian. This confirms the necessity of progressive retrieval in scientific applications.
% To amplify the differences in visualization quality, we compute the Laplacian of the reconstructed data, as differential operators tend to highlight structural distortions, making the visual differences more pronounced.
