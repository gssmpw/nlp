\section{Overview}
\label{sec:overview}
In this section, we first discuss the problems we are targeting, and then propose our solution for such problems. 

\subsection{Problem Formulation}
\label{sec: problem formulation}




\subsubsection{Definitions}
\label{sec:definitions}

Here we list five commonly adopted metrics for evaluating scientific lossy compression. The scientific dataset~\cite{sdrbench} is denoted as $x$. All symbols used in the paper are explained in~\Cref{tb:symbols}.

% \begin{itemize}
\noindent \textbf{Compression Ratio (CR) and Bitrate}: CR compares the original data size with compressed data size by $CR = \frac{size(\text{original data})}{size(\text{compressed data})}$. Bitrate is reverse proportional to CR. It measures the average number of bits for storing each scalar value in the compression format. A higher compression ratio or lower bit rate indicates more efficient compression in terms of storage space.
    
\noindent    \textbf{Decompression Error} describes the deviation between the original and decompressed data. There are many ways to quantify the deviation, while the $L_\infty$ norm, defined as the maximum point-wise difference between original and decompressed data, is the most widely used one. 

\noindent \textbf{Error Bound ($eb$)} is a user-defined parameter that specifies the maximum allowable error produced by lossy compressors. 

\noindent    \textbf{Peak Signal-to-Noise Ratio (PSNR)} assesses the fidelity of the data with lossy error. The definition is $20 \cdot \log_{10} \left( \frac{\max(x) - \min(x)}{\sqrt{MSE(x, \hat{x})}} \right)$, where $MSE(x, \hat x)$ denotes the mean squared error between the original dataset $x$ and the decompressed dataset $\hat x$. A higher PSNR value indicates better data fidelity.
% \end{itemize}


\subsubsection{Objectives}

\begin{table}[ht]
\vspace{-4mm}
\centering
\footnotesize
\caption{Definition of symbols}
\vspace{-3mm}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{|c|l|}
        % \toprule
        \hline
        \textbf{Symbol} & \textbf{Description}  \\
        % \midrule
        \hline
        $n$             & Numbers of elements of the input \\  \hline
        $x$             & The input dataset \\ \hline
        $\hat{x}$       & The decompressed output with lossy error\\ \hline
        $y,\hat y$      & The decorrelated data and its lossy version \\ \hline
        q & The quantized data \\        \hline        
        % e & The encoded data (saved as compressed data) \\        \hline        
        $eb$            & The bound of maximum lossy error  \\        \hline
        T, P & The transform and prediction function \\        \hline
        Q & The quantization function \\        \hline
        % E & The coding function \yang{rm} \\        \hline
        $\|\cdot\|_\infty$  & $L_\infty$ norm - the maximum absolute value of the input \\        \hline
        % $V$             & n-dimensional linear space \\
        % \hline
        $V_l$             & The sublinear space of the l-th level. \\
                          & $V_l$ and $V_m$ are orthogonal to each other when $l \neq m$\\ \hline
       $\Pi_l$             & The $L_2$ norm projection operator of $V_l$\\ \hline
       $x_i, y_i$    & The subscript $i$ denotes the i-th component of the vector. \\ \hline
        $x_l$           & the vector is projected to $V_l$. $x_l = \Pi_l x$, which \\
                        & applies to any vector with subscription $l$\\
        \hline

        % \hline
        % $H(X)$          & Entropy of $X$      \\
        % $\Delta$        & Difference operator \\
        % \hline
        % \bottomrule
\end{tabular}
\end{adjustbox}
\label{tb:symbols}
\end{table}


\begin{figure}[ht]
\vspace{-4mm}
    \centering
    \includegraphics[scale = 0.35]{figures-design/transform.png}
    \setlength{\abovecaptionskip}{1mm}
    \caption{A typical lossy compression workflow. T/P represents decorrelation and Q means quantization. The quantization stage is lossy thus $\hat y$ is the lossy version of $y$. Definitions for $x$, $y$, etc can be found in~\Cref{tb:symbols}}
    % \vspace{-4mm}
    \label{fig:transform} % 
    % \vspace{-3mm}
\end{figure}


The primary objective of this work is to develop a progressive lossy compression framework tailored for scientific data that meets the following four key goals.

\begin{itemize}
        \item \textbf{High compression ratio}: We aim to reach compression ratios higher than or equal to all other SOTAs, as a high compression ratio (CR) is essential for compressors to be effectively utilized in the scientific field. Some progressive designs, such as SZ3-M shown in~\Cref{sec:evaluation}, sacrifice compression ratio for progressiveness, but we argue that the low compression ratio will prevent the adoption of such solutions and thus make their progressive capability useless.
    % The solution should Achieve compression ratios comparable to SOTA non-progressive lossy compressors. If the compressor could not provide enough compression ratio then the data could not be saved to storage systems at all, and thus make the progressive capability meaningless.
    \item \textbf{Progressive retrieval}: We aim to support users to retrieve the output at a lower fidelity level \( F_1 \) and incrementally refine it to higher fidelity levels \( F_2, \dots, F_n \) without the need for multiple decompression passes.
    \item \textbf{Fast speed}: Our speed of compression and decompression should be equal to or faster than all other SOTAs, and the decompression process should require only a single pass to retrieve data at any specified fidelity level \( F_i \).
    \item \textbf{Error guarantee}: We guarantee that for each fidelity level \( F_i \), if \( F_i \) has error bound restriction of \( \epsilon_i \), the reconstruction error \( E_i \) satisfies \( E_i \leq \epsilon_i \). This ensures that the compressed data remains within acceptable accuracy limits for scientific computations.
\end{itemize}



% In summary, the problem formulation seeks to establish a robust framework for progressive lossy compression that addresses the dual demands of high compression efficiency and flexible, accurate data retrieval, all while managing the computational and practical constraints inherent to large-scale scientific datasets.
To the best of our knowledge, no compressor simultaneously achieves all these objectives yet.

\subsection{Overview of IPComp}


\begin{figure}[ht]
\vspace{-2mm}
\centering
% \hspace{-4mm}
\raisebox{-1cm}{\includegraphics[scale=0.6]{figures-design/overall_design.png}}
% \hspace{-3mm}
% \vspace{-4mm}
\caption{Overall design of our solution IPComp (the compressed data contains multiple decompressible blocks, represented as 1-5 in the diagram)}
\label{fig:overall-design}
\end{figure}



\begin{figure*}[ht]
\centering
% \hspace{-4mm}
\raisebox{-1cm}{\includegraphics[scale=0.7]{figures-design/interp.drawio.pdf}}
% \hspace{-3mm}
\vspace{-2mm}
\caption{Illustration of how the none-progressive interpolation algorithm works for a 2d input -- target points (in red color) are predicted from nearby known points (in green color), indicated by arrows}
\label{fig:interp-design}
\vspace{-2mm}
\end{figure*}

We introduce IPComp, a progressive lossy compressor designed to efficiently meet the diverse precision and fidelity requirements of scientific applications. IPComp is based on the interpolation algorithm which has been verified as the leading non-progressive reduction method, and we are the first to make it progressive. IPComp mainly has three innovative modules, shown in~\Cref{fig:overall-design}.
\begin{itemize}
    \item The \textbf{interpolation predictor} can reconstruct data to multiple fidelity levels progressively, and only needs a single pass of decompression for each retrieval request.
    \item The \textbf{predictive coder} can compress the interpolation output to independent bitplanes with high compression ratios.
    \item The \textbf{optimized data loader} can determine the minimum set of data for reconstruction to satisfy various fidelity requirements.
\end{itemize}

We demonstrate the compression and decompression workflow of IPComp in~\Cref{fig:overall-design}. For comparison, we also highlight the differences between IPComp and residual-based SOTAs.

The compression workflow begins with the original dataset, which is processed by an \textbf{Interpolation Predictor} to decorrelate the dataset based on spatial and numerical relationships, and quantize the output from floating point format to integers based on the error bound. Following this, the \textbf{Predictive coder} encodes the prediction integers by bitplanes into multiple independent blocks, each contributing incrementally to the accuracy of the decompressed data. 
% This structured bitplane allows users to retrieve data at various fidelity levels while maintaining a compact representation that maximizes compression efficiency.

The data retrieval workflow is designed to support multi-fidelity retrieval requests efficiently, leveraging an \textbf{Optimized Data Loader} that extracts only the required blocks for a given query. \Cref{fig:overall-design} demonstrates how the workflow works using three requests in increasing fidelity orders. Requests 1 asks for an error bound of 1E-2, so the data loader retrieves the first few precision blocks (e.g., blocks 1, 2) and passes them to the \textbf{Interpolation Predictor} and \textbf{Progressive coder}. These blocks are processed together at one single pass to generate low-precision output, suitable for quick exploration or coarse analyses. For the second request which indicates a targeting bitrate of 2, the data loader determines one additional block (e.g., block 3) is optimal for such a request. As a result, the predictor and coder reconstruct this mid-precision representation using block 3 on top of the low-precision result. Finally, the last request asks to retrieve all blocks, so blocks 4 and 5 are loaded consequently to build the high-precision reconstruction. 

In contrast, residual-based SOTA approaches involve significant computational overhead and redundant operations. Those approaches will first compress in a large bound, then compress the residual from the last time using a smaller bound, and repeat such residual compression until a targeted bound is reached. As a result, they require multiple decompression passes to fulfill a single retrieval request. For example, in the SOTA workflow of~\Cref{fig:overall-design}, the error bound for blocks $1'$, $2'$, $3'$ are 1, 1e-1, and 1e-2, respectively. For a single request of 1e-2, those approaches need to load three blocks and execute the decompression three times to apply the result on top of each block.


In terms of implementation, our solution is developed using the FZ framework~\cite{fz}, a premier solution for developing scientific compressors.
FZ is a comprehensive code platform providing various existing compression-related techniques wrapped in ready-to-use modules, such as the tool to compute lossy error metrics, and the function to parse compression settings from the command line and configure files. Moreover, to help developers with testing and integration, FZ provides a universal and robust API in many languages, including C, C++, Python, and Fortran, as well as seamless integration with I/O libraries like HDF5~\cite{hdf5}. 
In conclusion, FZ significantly reduces our need to re-implement existing techniques or tools, allowing us to concentrate on designing and implementing innovative progressive compression features.





\section{Progressive design of IPComp}
\label{sec:design}
In this section, we first analyze the interpolation-based algorithm, which is the leading non-progressive scientific compression strategy, and then we propose our solution to make it progressive by the prediction model and predictive coder.


\subsection{Introduction to none-progressive interpolation algorithm}
\label{sec:design intro-to-interp}

Most scientific lossy compression workflows are composed of three key steps -- decorrelation, quantization, and encoding~\cite{interp, zfp, sperr, mgard}. As shown in~\Cref{fig:transform}, the input data is defined as a vector $x$ in a linear space $\mathbb{R}^n$, where $n$ represents the total elements in the dataset. In the decorrelation step, a transform $T$ or prediction function $P$ is applied to $x$, resolving a decorrelated vector $y$. After a lossy quantization $Q$ on $y$, the quantized integers are further lossless coded to bitstream as the final compressed data. Accordingly, the decompression process executes the three steps reversely.

One critical aspect of designing an effective lossy compressor is to construct the best-suit decorrelation algorithm. Our solution IPComp uses interpolation-based decorrelation, which has been proven to be the leading solution in the scientific domain~\cite{interp, qoz, qoz2, mgard}. Its core idea is to estimate unknown data points at fixed relative indices using interpolation formulas, such as linear interpolation and cubic spline interpolation. Unlike many other predictors (such as regression-base ones) that store coefficients for reconstruction, the interpolation approach eliminates such storage overhead by relying on predefined prediction formulas for fixed indices. As a result, this approach achieves much higher compression ratios than many alternatives~\cite{interp}. 

In a plain example, consider four data points located equidistantly, with indices $i-3$, $i-1$, $i+1$, and $i+3$. Their corresponding values are denoted as $x_{i-3}$, $x_{i-1}$, $x_{i+1}$, and $x_{i+3}$. The goal is to estimate $x_i$ using interpolation formulas. 

For linear interpolation~\cite{interp}, the estimation is computed as the average of the neighboring points $x_{i-1}$ and $x_{i+1}$, given by:
\begin{equation}
y_i = \frac{1}{2}(x_{i-1} + x_{i+1}).    
\label{eq:interp-linear}
\end{equation}

For cubic spline interpolation~\cite{interp}, which takes into account all four points to achieve higher accuracy, the estimation can be:
\begin{equation}
y_i = -\frac{1}{16}x_{i-3} + \frac{9}{16}x_{i-1} + \frac{9}{16}x_{i+1} - \frac{1}{16}x_{i+3}.
\label{eq:interp-cubic}
\end{equation}

Both of the interpolation formulas predict values using neighboring data points in fixed relative positions, such that the coefficients are always the same (e.g., $\frac{1}{2}$ in the linear case) and there is no need to save them during compression.
\Cref{fig:interp-design} shows how to extend the interpolation algorithm from one scalar value to a multi-dimensional dataset. The stride separates the data points by distance (the data distance in stride i is $2^i$). In each stride, the algorithm operates recursively along each dimension of the dataset. 
% All the predicted values will be compared with original value, and the difference between prediction and real value will quantized and encoded as compressed data.







% The performance of a lossy compressor is determined by the design of the three parts. 



\subsection{Transform vs. prediction models for progressive design}
\label{sec: design transform-vs-predict}
The interpolation-based decorrelation discussed in~\Cref{sec:design intro-to-interp} can be employed either as a transform model or a prediction model in lossy compression. In this section, we discuss our rationale for using it as a prediction model in IPComp.

Because we aim to build a progressive lossy compressor, it is crucial to account for the distortion introduced by any lossy operation, as such distortion can accumulate over successive data retrievals. Toward this, we first highlight the differences between the transform and prediction approaches, then provide a theoretical analysis of their respective distortion behaviors, and finally explain why IPComp adopts interpolation as its prediction model.

 
% Typically, for many non-progressive lossy compressors, information loss occurs during the quantization step. 

\subsubsection{Transform models}



% The theoretical foundation of transform models in compression is the transform coding theory which states that by converting the input data into another domain the data would become more compact and thus easier to be coded. 
Transform models can be viewed as a linear mapping between original data and decorrelate data. Many transform models, including Fourier transform and wavelet transform, have been adopted in lossy compression. For example, ZFP~\cite{zfp} uses nearly orthogonal block transform, and SPERR~\cite{sperr} is based on the cdf97 wavelet.
% A widely used approach to modeling the data compression process is transform coding theory. The main idea behind transform coding is to convert the input signal into another domain where the signal is compact and decorrelated, so that after quantization and encoding the statistical attributes of the features can be compressed. 
% \[
% T: \mathbb{R}^n \to \mathbb{R}^m
% \]
% In many cases, the transformation is linear, such as the  which converts the input from its original domain to a different domain. For example, ZFP is a lossy compression algorithm, using nearly orthogonal block transform, and MGARD, a multilevel data compressor, computes the difference of linear interpolation and original data, which can also be concluded as a linear transform. 
% SZ2 is a prediction based lossy compressor, implementing several different approaches, including Lorentzo, Regression and interpolation, but all the prediction strategy can be abstracted to a linear process. 

Deriving the distortion between input data $x$ and decompressed data $\hat{x}$ in such solutions is relatively straightforward. If measured by $L_\infty$ norm, the distortion can be computed as:
\[
\|x - \hat x\|_\infty = \|T^{-1}y - T^{-1}\hat y \|_\infty \leq  \|T^{-1}\|_\infty \|y - \hat y\|_\infty
\]

The value of $L_\infty(T^{-1})$ depends on the specific transformation function. Taking the basic but widely used transform function $x_i=x_i-x_{i-1}$ as an example, its corresponding transform and inverse transform formulas are:
\[
T = 
\begin{bmatrix}
1 & 0  & \cdots & 0 \\
-1 & 1 & \cdots & 0 \\
\vdots   & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}, \quad
T^{-1} = 
\begin{bmatrix}
1 & 0   & \cdots & 0 \\
1 & 1   & \cdots & 0 \\
\vdots   & \vdots & \ddots & \vdots \\
1 & 1  & \cdots & 1
\end{bmatrix}
\]
In this case, the $L_\infty$ norm of $(T^{-1})$ equals the maximum sum of the rows in the matrix $T$, which equals the input size $n$.  As a result, any distortion in the transformed domain will be amplified in the original domain, and the maximum error could become $n$ times the maximum error in the transformed domain.  Given that $n$ in most scientific datasets is at least $10^7$, such a degree of distortion amplification may be unacceptable for progressive compression.

\begin{equation}
\|x - \hat x\|_\infty \leq  L_\infty(T^{-1})L_\infty(\hat{y} - y) = n \|\hat{y} - y\|_\infty
\label{eq:error-transform-lorenzo}
\end{equation}


% In the second case, similarly with the previous one, if the distortion is measured by $L_2$ norm, with applying singular value decomposion (SVD), the error could be bounded by
% \[
% d(\hat{x}, x) = L_2(T^{-1}(\hat{y}-y)) \leq L_2(T^{-1})L_2(\hat{y}-y) = \sigma_{max}(T^{-1})L_2(\hat{y}-y)
% \]



\subsubsection{Prediction models}



% Although linear transforms offer numerous advantages and conveniences, our design is based on a nonlinear transform.
% SZ3 is a lossy compression framework integrated with multiple prediction methods, balancing high compressibility and high data throughputs. 
% [Explain why we use SZ3 with overwriting]

% Assume $x$ is a vector in a $n$ Dimentional linear space$\mathbb{R}^n$, which is also denoted as $V$. For 1-D case, to compress an array of single precision or double precision float-point numbers or integers, it is natural to imagine the whole data set could be aligned by the order that the numbers store in the memory to get the vector$x$. We have ignored the rounding error when applying addition and other operation on floating points, and for integers, the simply solution is to replace the number field from $\mathbb{R}$ to $\mathbb{Z}$. Our formulation can also apply to multi-dimentional cases, where we arrange the order of each variable in vector $x$ as the data stored in the memory. 

Prediction models are more complex than transform models, as they tightly couple the decorrelation function $P$ with quantization $Q$. For transform models, the whole dataset is first transformed by $T$ before applying quantization $Q$. However, in prediction models, the data usually is split into orthogonal layers or groups, and the prediction $P$ and quantization $Q$ are applied layer by layer~\cite{di2024survey, mgard} on $\hat{x}$ (data with lossy error) instead of $x$ (original data). As a result, the prediction model can be viewed as a non-linear mapping between input and decorrelate data.

\Cref{fig:interp-design} shows how to divide the whole linear space into multiple orthogonal levels by setting a shrinking stride distance. We represent level as the sublinear spaces $V_l \subseteq \mathbb{R}^n$. The total number of levels is defined as $L$.
% In our implementations for interpolation algorithm, $V_l$ is taken as the simplest way of which is shown as Fig.\ref{fig:interpolation} where all the variables except the chosen ones are zero, ensuring the orthogonality between those spaces. 
The prediction $
P_l
$ will be executed $L-1$ times, covering from the top level $L$ to level 1.

% With these definitions, we can conclude the compression stage of SZ shown in Fig.\ref{fig:overwrite} with all bitplanes preserved as follows: 

% In prediction models, we store the prediction difference $y_l$ in $l$-th level as quantized values $q_l$, instead of storing transformed values as transform models. For example, 
For each level $l$, we process $y_l$ which is the difference between the original data $x_l$ and its prediction. The prediction $P_l$ is based on interpolation from $\hat{x}_{l+1}$ (the previous level's data with lossy error).
\[
y_l = x_l - P_l\hat{x}_{l+1}
\]
The prediction difference $y_l$ will be quantized to integers, and dequantized back to floating point $\hat y_l$. The quantization guarantees that the point-wise error between $y$ and $\hat y_l$ is less than the pre-defined global error bound $eb$. Note that in decompression we do not have the original prediction difference $y$ since it is lossy quantized here.
\[
y_l - \hat y_l = \varepsilon_l,\|\varepsilon_l\|_\infty \leq eb
\]
In decompression, which is the reverse process, we first recover the prediction data $P_l\hat{x}_{l+1}$, then compute the output $\hat{x}$ by adding $\hat y$ (the prediction difference with error) on top of the prediction. 
\[
\hat{x}_l = P_l\hat{x}_{l+1} + \hat{y}_l
\]
% According the equations above, we can get a simply relation between our recovered vector $\hat{x}_l$ and $x_l$. 
By simplifying the three formulas above, we can get the conclusion that for each level, $\hat x_l$ only differs with original data $x_l$ within the point-wise error bound $eb$.
% \yang{
% \[
% % \begin{equation}
% x_l =\Pi_lx = \hat{x}_l + \varepsilon_l, \|\varepsilon_l\|_\infty \leq eb
% \]
% }
\[
% \begin{equation}
x_l = \hat{x}_l + \varepsilon_l, \|\varepsilon_l\|_\infty \leq eb
\]
% \label{eq:nonprog}
% \end{equation}
Then for the whole dataset, the maximum point-wise error could be bounded by 
% \begin{equation}
%     \|x-\hat x\|_\infty = \|x - \sum \hat x_l\|_\infty = \max_l\|x_l -\hat x_l\|_\infty = \max_l \varepsilon_l \leq eb
% \label{eq:error-prediction-interp}
% \end{equation}
\begin{equation}
    \|x-\hat x\|_\infty = \max_l\|x_l -\hat x_l\|_\infty = \max_l \varepsilon_l \leq eb
\label{eq:error-prediction-interp}
\end{equation}

Compared with this~\Cref{eq:error-prediction-interp}, the error in transform models (shown in~\Cref{eq:error-transform-lorenzo}) is proportional to the data size, which means it will be very difficult to bound the error in transform models if the input data is large. Since progressive may cause the error to accumulate across retrievals, we want to have an error control as tight as possible. As a result, we choose to use interpolation as a prediction model for our progressive solution.
% To predict values in level $l$, we have to calculate the residual of predicted values 
% \[
% y_l = x_l - P_lx_{l+1}
% \] 
% and store the quantized integers for later encoding and lossless compression usage. What really makes the transform from $x$ to the residual vector $y \in V$ nonlinear is the step of predicting using recovered values, instead of original ones.

% The decompression process is exactly the reversed version of compression. As we get the decoding quantization integers, we can recover the residual vector $\hat y$ for each level.

\subsection{Progressive interpolation algorithm}
\label{sec: design progressive}

\begin{figure}[ht]
% \vspace{-8mm}
    \centering
    \hspace{-6mm}
    \includegraphics[scale = 0.5]{figures-design/bitplane.png} 
    \vspace{-5mm}
    \caption{Our progressive solution splits the quantization integers by bitplanes and encodes them separately} % 
    \label{fig:overwrite} % 
    % \vspace{-4mm}
\end{figure}





In this section, we present our solution to support progressive retrieval utilizing the interpolation-based prediction model discussed in~\Cref{sec:design intro-to-interp} and~\Cref{sec: design transform-vs-predict}.


The foundation of our progressive design is to load a partial of the information of $\hat y$ according to the bitplane. As illustrated in~\Cref{fig:overwrite}, in our approach, the quantized data (denoted as $q_l$ at level $l$) are 32-bit integers. The bits from the same position across multiple quantized integers form a bitplane (represented by small rectangular boxes in the figure). We independently encode the 32 bitplanes at the same level, which allows lower-fidelity outputs to be reconstructed by loading only some of the bits rather than all of them.
When retrieving data, our optimizer discussed in~\Cref{sec:optimizer} will determine the optimal loading strategy -- the minimum bitplanes to load from each level to meet the retrieval requirement. 
% We have designed an algorithm to reconstruct the data from progressively loaded bitplanes shown as \Cref{alg:reconstruction}. 
% For example, when a user requests data with a bitrate of 1.0, our optimizer will determine the optimal loading strategy, specifying how many bitplanes to load from each level to meet the target bitrate efficiently. 



\Cref{alg:reconstruction} describes the reconstruction process of our solution. 
As shown in~\Cref{fig:overwrite}, the reconstruction starts from the top level, where the loaded data is decoded into the reconstructed diff $\hat{y}$.
We then use the $\hat{y}$ from each level, in sequential order, to progressively reconstruct the original data. 
The parameter $L_p$ indicating in which level we start progressive compression. The first step from line 3 to 7 is to retrieve data of non-progressive levels. Then the second part is to load the bitplanes we are requiring, and decoded them in addition to the vector we get in the first step. 

~\Cref{alg:reconstruction} is designed for cases where data is reconstructed from scratch. On the other hand, when users find the current precision is insufficient, \Cref{alg:incremental} shows how to update the output from lower precision to higher precision by loading incremental bitplanes. 
% If the user later finds that the precision is insufficient and requires higher fidelity, our Incremental Reconstruction Algorithm shown as \Cref{alg:incremental} ensures that additional bitplanes can be loaded at each level. 
By combining these newly loaded bitplanes with the previously loaded data, our solution can reconstruct a higher fidelity output without reloading all the compressed data.

\begin{algorithm}
\caption{Reconstruction Algorithm}
\label{alg:reconstruction}
\begin{algorithmic}[1] 
  \Require $bitplaneList[L]$
  % \Require $q_l$, $l = 1, 2, \dots, L_p, L_p+1, \dots, L$
  \State $\hat x \gets 0$
  \State $\Pi_L \hat x \gets P_L(\mathbf{0})$
  \For{$l \gets L - 1$ \textbf{downto} $L_p + 1$}
      \State $q_l \gets \textbf{decode}(bitplaneList[l])$
      \State $\hat{y}_l \gets \textbf{dequantizition}(q_l)$
      % \State $\Pi_l \hat x \gets P_l \Pi_{l+1} \hat x + \hat y_l$
      \State $\hat x_l \gets \textbf{Predict}(\hat{x}, \hat y_l)$
  \EndFor
  \State $\Delta \gets 0$
  \For{$l \gets L_p$ \textbf{downto} $1$}
      \State $q_l \gets \textbf{decode}(bitplaneList[l])$
      \State $\hat{y}_l \gets \textbf{dequantizition}(q_l)$
      % \State $\Pi_l \hat x \mathrel{+}= P_l \left( \Pi_{l+1}\Delta  \right) + \hat y_l$
      % \State $\Pi_l \Delta \mathrel{+}= P_l \left( \Pi_{l+1}\Delta \right) + \hat y_l$
      \State $\hat x_l \gets \textbf{Predict}(\Delta, \hat y_l)$
      \State $\Delta_l \gets \textbf{Predict}(\Delta, \hat y_l)$
      
  \EndFor \\
  \Return $\hat x$
\end{algorithmic}
\end{algorithm}





\begin{algorithm}
\caption{Incremental Reconstruction Algorithm}
\label{alg:incremental}
\begin{algorithmic}[1]  
  \Require $\hat x^{\text{(old)}}, bitplaneList[L]$
  
  % \Require $v_{\text{progressive},l}^{\text{(old)}}, \delta r_l^{\text{(new)}}$
  \State $\hat x^{\text{(new)}} \gets \hat x^{\text{(old)}}$
  \State $\Delta \gets 0$
  \For{$l = L_p$ \textbf{to} 1}
    \State $q_l \gets \textbf{decode}(bitplaneList[l])$
    \State $\hat y_l = \textbf{dequantization}(q_l)$
    % \State $\Pi_l \hat x^{\text{(new)}} \mathrel{+}= P_l \bigl(\Pi_{l+1} \Delta) + \hat y_l$
    \State $x_l^{\text{(new)}} \gets \textbf{Predict}(\Delta, \hat y_l)$
    \State $\Delta_l \gets \textbf{Predict}(\Delta, \hat y_l)$
  \EndFor\\
  \Return $\hat x^{\text{(new)}}$
\end{algorithmic}
\end{algorithm}





% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{example-image} 
%     \caption{} % 
%     \label{fig:Lorenzo} % 
% \end{figure}



% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{example-image} 
%     \caption{} % 
%     \label{fig:interpolation} % 
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{example-image} 
%     \caption{} % 
%     \label{fig:sz-transform} % 
% \end{figure}

\subsection{Predictive negabinary Coding}
\label{sec:design coding}

In this section, we propose a novel coding method that addresses two key challenges in progressive coding: preserving data correlation across bitplanes and handling sign bits. Our solution achieves high compression ratios by capturing cross-bitplane correlations using predictive coding and encoding sign bits using negabinary coding.

\subsubsection{Predictive Bitplane Coding}

We propose a predict-based strategy to exploit the correlation between bitgroups. To support progressive retrieval, each quantized integer is split into bitplanes, which means that the bits of a single integer are encoded separately. As a result, the correlation between bits from the same integer is totally ignored. 
We observe that during decompression when retrieving a certain bitplane at a specific level, the previously loaded bitplanes are already known. This observation allows us to leverage the correlation between bitplanes by predicting the bit value based on previous bits from the same integer, and encode the results of the prediction instead of raw bits. For example, if the prefix bit is 0, we predict that the current bit $b$ will also be 0. This prediction mechanism is equivalent to applying an XOR operation between the prefix bit and $b$ as $\text{encoded bit} = \text{prefix bit} \oplus b $.

\begin{table}[ht]
\centering
\footnotesize
  \caption{Our predictive bitplane coding strategy reduces the entropy in quantized integers (lower entropy indicates better compressibility)} 
  \vspace{-2mm}
  \label{tb:entropy} 
  \begin{adjustbox}{width=\columnwidth}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Fields & Original & 1-bit prefix & 2-bits prefix & 3-bits prefix  \\ 
        \hline
        Density    & 0.358505 & 0.352111 & 0.34732 & 0.350525   \\
        SpeedX    & 0.868908 & 0.861458 & 0.855473 & 0.859443     \\
        Wave    & 0.440292 & 0.433391 & 0.427993 & 0.431599   \\
        \hline
    \end{tabular} 
\end{adjustbox}
% \vspace{-3mm}
\end{table}

% For example, consider a dataset where values are predominantly distributed between 0 and 1 with a probability exceeding 90\%. In this case, if the second-to-last bit is 1, it strongly suggests that the last bit is likely to be 1; conversely, if the second-to-last bit is 0, the last bit is likely to be 0. As a result, instead of storing the original bits, we store the correctness of the prediction (0 as false and 1 as true). By exploiting such probabilistic dependencies between bitplanes, we can achieve higher compression efficiency while preserving the ability to reconstruct data accurately.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[scale = 0.16]{figures-design/predictive.jpeg} 
%     \caption{pred} % 
%     \label{fig:predictiveCoding} % 
% \end{figure}
We can further extend this method by utilizing more prefix bits -- we perform an XOR operation on all the preceding bits and then XOR the result with the target bit $b$ to obtain the encoded bit. The process can be mathematically expressed as:
\[
\text{encoded\_bit} = (\text{prefix\_bit}_1 \oplus \text{prefix\_bit}_2 \oplus ...\oplus \text{prefix\_bit}_n) \oplus b
\]
% According to Table. \ref{tb:entropy}, this 1-bit prediction scheme reduces the entropy compared to the original data, improving compression efficiency.
% As shown in the Fig. \ref{fig:predictiveCoding}, 


We observe from~\Cref{tb:entropy} that using 1-bit, 2-bit, or 3-bit prefix bits for prediction consistently reduces the entropy compared to the original data. Among these, 2-bit prediction generally achieves the best entropy drop. As a result, our approach leverages the two prefix bits to predict the next bit.



\subsubsection{Negabinary coding}
Unlike their non-progressive counterparts, progressive compression schemes must efficiently handle sign bits, since loading them is a prerequisite for reconstructing values, but storing sign bits separately and loading them first before all other bits incurs significant overhead. To address this issue, we evaluate the suitability of three widely used methods for encoding sign bits in a progressive setting -- two's complement~\cite{computer_organization_design}, sign-magnitude~\cite{computer_organization_design}, and negabinary~\cite{hoang2018study, zfp}, and select negabinary encoding as the preferred approach for two reasons.

First, negabinary encoding may lead to better compressibility than other methods for values that fluctuate around zero (which is predominant in the last layer of quantized integers). In two's complement, the higher-order bits often contain many 1s for values near zero, making the corresponding high-order bitplanes difficult to compress. For example, the 8-bit representations of 1 and -1 are 00000001 and 11111111 in two's complement, 00000001 and 10000001 in sign-magnitude, and  00000001 and 00000011 in negabinary. As a result, negabinary encoding keeps the higher-order bits as 0, which leads to more compressible high-order bitplanes compared to the dense 1s in two's complement.
Second, compared to sign-magnitude encoding, negabinary encoding is more balanced. When the least significant bits are set to zero (due to quantization or truncation), the resulting error uncertainty is smaller in negabinary encoding. This leads to more predictable error behavior, which is beneficial for data compression and reconstruction.
The uncertainty could be formulated by
    \[
    \begin{aligned}
       & \text{uncertainty (negabinary) } = 
        \begin{cases}
                \frac{2}{3}\times2^d-\frac{1}{3}
            , & \text{when } d \text{ is odd} \\
                \frac{2}{3}\times2^d-\frac{2}{3}, & \text{when } d \text{ is even}
        \end{cases}\\
    & \text{uncertainty (sign-magnitude) }  = 2^d-1
    \label{eq:uncertainty}
    \end{aligned}\\
    \]
where d represents the number of bits discarded. As d increases, the uncertainty of negabinary is only around two-thirds of sign-magnitude encoding.



% First, negabinary coding provides a more balanced representation compared to the two other method. For instance, when setting the least significant bits of an integer to zero, a bias arises between the expected and actual values of the integer. In sign-magnitude representation, this bias also depends on the sign bit, whereas in negabinary coding, the bias tends to be smaller.
% Furthermore, for values with an absolute magnitude close to zero, negabinary coding ensures that the higher-order bits remain zero. In contrast, sign-magnitude representation does not exhibit this property, and in two's complement representation, the probability of high-order bits being either zero or one is close to 50\%. Since entropy encoding is applied in later steps, we want to avoid cases where 0s and 1s appear with nearly equal probability, as this would render entropy encoding almost ineffective, significantly reducing compression efficiency.




\section{Optimized data loading} 
\label{sec:optimizer}
% The first component of our design involves an optimizer that determines which bitplanes need to be loaded, followed by decompression using our proposed algorithm. 
In this section, we present our optimized data load strategy that can minimize the volume of data loaded under the retrieval constraints.
Our optimizer supports both error-bound mode and bitrate mode which represents most of the scenarios in scientific data retrieval.
\begin{itemize}
    \item Error bound mode: in this mode, users specify the required precision and the optimizer selects the optimal strategy to ensure that the reconstructed data's error remains strictly within the given bound while minimizing the amount of data loaded.
    
    \item Fixed rate/size mode: in this mode, the user specifies a maximum allowable bitrate or size, typically constrained by I/O bandwidth. The optimizer then ensures that the reconstructed data has the smallest possible error while staying within the specified bitrate limit. Since the total data size $n$ is constant for the input, requiring a specific bitrate is equivalent to requiring a fixed retrieval size.
\end{itemize}
We first discuss the upper bound theory which both modes rely on, then show how the optimizer makes the best decisions on bitplane selection based on dynamic programming.


\subsection{Fundamental theory for our optimizer}
\label{sec: optimized-base-theory}
In this section, we discuss the fundamental theory for our optimizer which measures the error caused by only a partial of bitplanes being loaded.

\begin{theorem}
The $L_\infty$ error in progressive retrieval can be bounded based on the information loss due to the unloaded bitplanes.

\begin{equation}
\|x-\hat x\|_\infty \leq \sum_{l = 0}^{L-1} p^{l}\|\delta y_{l+1}\|_\infty + eb
\label{eq:upperbound_1}
\end{equation}

In this equation, $p=1$ for linear interpolation, and $p=1.25$ for cubic interpolation. $\delta y_l$ describes the information loss in level $l$ caused by the unloaded bitplanes, and its value can be pre-computed during compression.

\end{theorem}

\renewcommand{\qedsymbol}{}
\begin{proof}
As illustrated in \Cref{alg:reconstruction}, the retrieval process proceeds iteratively through all levels, with each level relying on the preceding level's output to inform its predictions. This introduces a significant hurdle for error analysis, as uncertainty can propagate from one level to the next. To address this, we conceptualize the error in level $i$ as being comprised of three components. The first component, $\delta y_{l}$, represents the information loss resulting from not loading certain bitplanes at this level. The second component, $\delta y_{prop, l}$, accounts for information loss carried over from the prior level. The third component, $\varepsilon_l$, captures the error introduced by lossy quantization.\begin{equation}
    x_l - \hat x_l = \delta y_l + \delta y_{prop, l} +  \varepsilon_l
\end{equation}

The information loss propagated until the previous level $l+1$ is $\delta y_{prop, l+1}$. It will be first combined with the information loss in the current level $\delta y_{l+1}$, and then transfered by prediction and passed to the next level as $\delta y_{prop, l}$.
\begin{equation}
    \delta y_{prop, l} = P_l(\delta y_{prop, l+1} + \delta y_{l+1})
\end{equation}

Combine the two equations, we can have: 
\begin{equation}
x_l - \hat x_l = \delta y_l + \sum_{m = 0} P_{l}P_{l+1}...P_{l+m} \delta y_{l+m+1} + \varepsilon_l
\label{eq:upperbound_i}
\end{equation}

In our approach, the $L_\infty$ of interpolation $P_l$ is greater or equal to 1, which implies the information loss would be amplified to the lower level. In other words, lower levels have larger errors, and the maximum error of the dataset is in the first level. As a result, we can quantify the error of the dataset with \Cref{eq:upperbound_i} by:


\begin{align}
\|x-\hat x\|_\infty &= \|x-\hat x_1\|_\infty  \notag \\
&= \|\delta y_1 + \sum_{l = 0} P_{1}P_{1+1}...P_{l} \delta y_{l+1} + \varepsilon_1\|_\infty  \notag \\
&\leq \sum\|P_1\|_\infty\|P_2\|_\infty...\|P_l\|_\infty\|\delta y_{l+1}\|_\infty + eb
\label{eq:upperbound}
\end{align}

Now we simplify \Cref{eq:upperbound} to \Cref{eq:upperbound_1}. The value of $L_\infty(P)$ depends on the prediction method. Liner interpolation, as shown in~\Cref{eq:interp-linear}, has two coefficients both at 0.5. Such that $L_\infty (P) = 0.5+0.5=1$. Similarly, cubic interpolation, as shown in~\Cref{eq:interp-cubic}, has four coefficients, and $L_\infty(P) = 2\times \frac{1}{16} + 2\times \frac{9}{16} =1.25$. With the derived value of $L_\infty(P)$, \Cref{eq:upperbound} is converted to~\Cref{eq:upperbound_1}.

\end{proof}

% \begin{theorem}
% The $L_\infty$ error of our compressor is bounded by
% \begin{equation}
% \|x-\hat x\|_\infty \leq \sum\|P_1\|_\infty\|P_2\|_\infty...\|P_l\|_\infty\|\delta y_{l+1}\|_\infty + eb
% \label{eq:upperbound_1}
% \end{equation}
% The $L_\infty$ of the prediction $P$ depends on the specific prediction function ( $L_\infty P=1$ for linear interpolation, and $=1.25$ for cubic interpolation). As a result, ~\Cref{eq:upperbound_1} can also be expressed as
% \begin{equation}
% \|x-\hat x\|_\infty \leq \sum_{l = 0}^{L-1} p^{l}\|\delta y_{l+1}\|_\infty + eb
% \label{eq:upperbound_2}
% \end{equation}
% where each coefficient $p_l$ could be 1 or 1.25.
% \label{th:upperbound}
% \end{theorem}

% \renewcommand{\qedsymbol}{}
% \begin{proof}
% We start from the top level L where the progressive decompression begins. 
% % Typically there is only one non-zero node in the highest level, and the quantized values are encoded and compressed for each of the bitplane. 
% % If we only load partial of the bitplane in this level, there could be a potential error which we can estimate, and we call it $\delta y_L$.
% The loss or error in this level caused by loading partial bitplanes is defined as $\delta y_L$.
% It is the difference between dequantized data $\hat y_L$, which is calculated with all the bitplanes, and the dequantized $\hat y_l'$ is:
% \[
% \delta y_L = \hat y_L - \hat y_L'
% \]
% The decompressed value at level $L$ is $\hat x'_L = P_L(\mathbf{0}) + \hat y'_L$. Comparing the decompressed data with all bitplanes $\hat x_L$, we can get:
% \[
% \hat x_L = \hat x_L' + \delta y_L
% \]
% As \Cref{alg:reconstruction} shows, in our approach, we load part of the bitplanes of each level, dequantizing it, getting $\hat y'_l = Q^{-1}q'_l$, then move on to the next level. A major challenge in such a design is that the uncertainty would propagate between levels. For example, if we move on to level $L-1$, the dequantized residual is $\hat y_{L-1}' = \hat y_{L-1} - \delta y_{L-1}$, and if we evaluate the decompressed value $\hat x'_{L-1}$ and compare it with $\hat x_{L-1}$, we can get:
% \[
% \hat x'_{L-1} = P_{L-1}(\hat x_L - \delta y_L) + \hat{y}_{L-1} - \delta y_{L-1} \\
% = \hat x_{L-1} - \delta y_{L-1} - P_{L-1}\delta y_{L}
% \]
% This equation tells us the error comes with two terms. The first one $\delta y_{L-1}$ is the uncertainty in level $L-1$, and the second term $P_{L-1} \delta y_{L}$ is the error propagated from the previous level.

% Applying the same analysis to other levels, we can get:
% \begin{align}
%     x_l &= \hat x_l' + \delta y_{prop, l} + \delta y_l + \varepsilon_l \notag \\
%     \delta y_{prop, l} &= P_l(\delta y_{prop, l+1} + \delta y_{l+1})
% \label{eq:error}
% \end{align}

% Such that:
% \[
% x_l - \hat x'_l = \delta y_l + \sum_{m = 0} P_{l}P_{l+1}...P_{l+m} \delta y_{l+m+1} + \varepsilon_l
% \]

% In our approach, the $L_\infty$ of interpolation $P_l$ is greater or equal to 1, which implies we should expect the lower level to have larger $L_\infty$ errors. In other words, if we can bound the lowest level, then we can bound the error of the entire dataset. As a result, we have: 
% \begin{align}
% \|x-\hat x\|_\infty &\leq \|x-\hat x'_1\|_\infty  \notag \\
% &= \|\delta y_1 + \sum_{l = 0} P_{1}P_{1+1}...P_{l} \delta y_{l+1} + \varepsilon_1\|_\infty  \notag \\
% &\leq \sum\|P_1\|_\infty\|P_2\|_\infty...\|P_l\|_\infty\|\delta y_{l+1}\|_\infty + eb
% \label{eq:upperbound}
% \end{align}
% % We ignore the quantization error term $\varepsilon_l$ since it usually is significantly smaller than the other two terms. 
% In terms of $L_\infty(P)$, its value depends on the prediction method. Liner interpolation, as shown in~\Cref{eq:interp-linear}, has two coefficients both at 0.5. Such that $L_\infty (P) = 0.5+0.5=1$. Similarly, cubic interpolation, as shown in~\Cref{eq:interp-cubic}, has four coefficients, and $L_\infty(P) = 2\times \frac{1}{16} + 2\times \frac{9}{16} =1.25$. With $L_\infty(P)$ being 1 or 1.25, we can simplify~\Cref{eq:upperbound} to~\Cref{eq:upperbound_2}.
% Since we have 2 interpolation methods, linear and cubic interpolation, and $L_\infty$ for these methods is $1$ and $1.25$, because the for linear interpolation
% % \[
% % d_i = \frac{1}{2}d_{i-1} + \frac{1}{2}d_{i+1}
% % \]
% the summation of all the coefficient is 1. Also, since the summation for cubic interpolation is $1.25$, we can derive a simple expression for the upper bound of our approach.
% \[
% \|x-\hat x\|_\infty \leq \|\sum_{l = 0}^{L-1} p^{l}\delta y_{l+1}\|_\infty + eb
% \] where $p = 1$ or $p = 1.25$ depending on our interpolation method.

% We have to indicate a base error bound, which is the maximum precision the reconstructed data could achieve. It is typically a small value noted as $eb$, since we want to keep more flexibily when the demand of precision is unknown.

% We define the uncertainty as the potential error from the discarded bits and quantization by $\delta y_l \in V_l$. Since we use negabinary coding, which is well known and widely used coding method in bit plane encoding stages, providing a relatively more balanced uncertainty when parts of the data are missing, we can calculate the uncertainty if the retrieved numbers of bitplanes in each level is given.
% \[
% y_l = \hat{y}_l + \delta y_l + \varepsilon_l
% \]
% \end{proof}

% This formula illustrates that the upper bound of the final error is influenced by two main factors:

% The choice of interpolation (or more generally, prediction) methods – The error propagation behavior depends on the specific prediction technique used. For example, in first-order Lorenzo prediction, the $L_\infty$ norm of the prediction operator 
% P is 1, meaning the worst-case error remains directly influenced by the prediction step.

% The number of levels in the hierarchical structure – The total number of levels directly affects the final error because the error at the topmost level propagates downward to the lowest level. A higher number of levels results in more propagation steps, amplifying the error.

% Taking 1D data as an example, Lorenzo prediction effectively partitions an 
% n-element dataset into 
% n levels, meaning that the upper bound scales approximately by a factor of 
% n. In contrast, for 1D interpolation, the number of levels is approximately 
% $\log n$, leading to a significantly lower error amplification. This controlled error growth is one of the primary reasons why we do not adopt other prediction schemes, as they could lead to excessive error propagation and compromise accuracy.

\subsection{Optimized loading based on error-bounds}
\label{sec: optimized-eb}

The goal of error-bound mode is to generate a loading strategy specifying the minimum number of bitplanes to load in each layer, in order to let all point-wise lossy errors be less or equal to the user-defined bound (denoted as E). Finding such bitplanes can be formalized as an optimization problem.
% \vspace{-3mm}
\begin{gather*}
\max_{b_l, l \in \{1, 2, ... L\}}  \sum_l SavedSize(l, b_l), \\
\text{subject to}   \sum_l err(l,b_l) + eb \leq \text{E}.
\end{gather*}
% \vspace{-3mm}
$b_l$ indicates the number of bitplanes discarded at the level $l$. \linebreak $SavedSize(l,b_l)$ denotes the amount of data saved by discarding $b_l$ bitplanes at level $l$. $err(l,b_l)$ denotes the error at level $l$ by discarding $b_l$ bitplanes. 
% The error has to multiply uncertainty $\delta y_l$ by a coefficient derived by \Cref{eq:upperbound_1} since the error will accumulate with the prediction between levels. 
As stated in \Cref{eq:uncertainty}, $err(l,b_l) = p^{l - 1}\|\delta y_{l}\|_\infty$, where $p = 1$ for linear interpolation, $p=1.25$ for cubic interpolation, and $\delta y_l$ is a function of $b_l$.

By such definitions, we essentially reformulate this optimization problem as the classical knapsack problem, enabling it to be efficiently solved using dynamic programming (DP) with minimal computational overhead.
Let $DP(l,e)$ be the maximum total saved size when the last layer is $l$ and the maximum error is $e$. Then $DP(L, E)$ would be our best solution to load data for the given error bound E, and its value can be derived recursively by the following DP transition function:
% \vspace{-3mm}
\[
DP(l,e) = \max_{b_l, s.t. err(l,b_l) \leq e }\{DP(l-1, e - err(l,b_l)) + SavedSize(l,b_l) \}
\]
% \vspace{-3mm}
The time complexity of such a DP process is $O( \text{\#level} \times \text{\#bitplane} \times \text{\#discrete error values})$. 
The number of levels is $\log_2 n$ where n is the input size, and the number of bitplanes is 32. The discrete error values fall within the range of $[128, 1023]$ by normalized retrieval bound $E$ by compression bound $eb$. If comparing the time of the DP to the time of simply traversing the input, the ratio would be 
$\frac{(1023-128+1)*32*\log_2 n}{n} \approx \frac{3*10^4\log_2 n}{n} \approx 3\%$ for datasets listed in~\Cref{tab:apps}. Since compression takes much more computation than just traversing the input, the overhead of the DP relative to compression is totally negligible. 

% For a dataset with 1 million elements, the number of levels $L$ is around 30. 
% If the target normalized E is $10^5eb$, the time complexity could be up to $10^8$. 
% To address this, 
% which is sufficient given that our target error need not be as precise as $10^{-3}$. 
% As a result, the final time complexity will likely not exceed $10^6$, making the optimizer's overhead negligible.

% In the following, we formulate the optimal loading strategy as a optimization problem, and solve it efficiently by casting it to a standard dynamic programming (DP) problem.

% In the following sections, we use the error upper bound we are showing to guide our strategy for selecting bitplanes at each level. We formulate the process of selecting bitplanes under the two modes of our optimizer: error bound mode and bitrate bound mode as optimization problems.
% Both of these optimization problems can be efficiently solved using a simple dynamic programming (DP) algorithm, allowing us to determine the optimal bitplane selection strategy for each level with low computational overhead. 

% We first define some necessary terms for solving this optimization problem.
% In this mode, we record the size of each compressed bitplane in the compression phase. This allows us to estimate the uncertainty introduced by discarding a specific bitplane. 



% We define a binary decision variable:
% From \Cref{th:upperbound} we know the $L-\infty$ error is bounded by the summation of $err_l$, given by
% \[
% \text{UpperBound} = \sum_l err_l + eb.
% \]
% To ensure that the reconstructed data error remains strictly within the user-specified bound \(E\), the following constraint must be satisfied: 
% \[
% \text{UpperBound} \leq E.
% \]

% At the same time, since we wish to minimize the amount of data loaded (i.e., maximize the amount of data saved by discarding bitplanes), our objective is to maximize the total saved data:
% \[
% \text{maximize } \sum_l s_{l}.
% \]

% solve it efficiently by casting it to a standard dynamic programming (DP) problem.
% In error bound-based mode, during compression, we record the size of each bitplane after compression. This allows us to determine how much uncertainty will be introduced at each level when a specific number of bitplanes are loaded. According to our derived formula, the uncertainty caused by discarding a certain amount of data at each level accumulates linearly—after being scaled by a specific coefficient—to form the upper bound of the total error.

% To minimize the amount of data loaded while ensuring the error remains within the bound, this problem can be transformed into a classic 0-1 knapsack problem. Here the uncertainty at each level serves as the cost and the bitplane size that is discarded serves as the value.
% Given a total allowable cost limit (i.e., the maximum permissible uncertainty), our objective is to maximize the total value (i.e., the amount of discarded bitplanes) while ensuring that the accumulated uncertainty remains within the bound. By solving this optimization problem efficiently, we can determine the most effective strategy for selecting bitplanes to load, achieving the best balance between compression efficiency and reconstruction accuracy.

% This approach may lead to over-preservation, meaning that the actual error will be smaller than the user-specified error bound. However, we do not recommend reserving additional error tolerance to reduce the amount of data loaded. For example, if a user specifies a target relative error of 1e-3, our strategy might result in an actual relative error of around 0.8e-3. Since our upper bound is already quite tight, if the user attempts to further reduce the loaded data by increasing the target error to 1.5e-3, the strategy might become unreliable, leading to potential violations of the intended error constraint. Our upper bound ensures that even in the most extreme cases, the actual error will never exceed the requested error bound.


\subsection{Optimized loading based on bitrates}

In the fixed bitrate mode, the user specifies a maximum allowed retrieval size \(S\) (or equivalently, a maximum allowable bitrate). The optimizer then seeks to minimize the reconstruction error while ensuring that the loaded data does not exceed \(S\). 

% The total size of the discarded bitplanes is given by
% \[
% S_{\text{loaded}} = \sum_l sl_{l},
% \]
% which must satisfy the constraint:
% \[
% S_{\text{loaded}} \leq S.
% \] Here 

% Our objective is to minimize the maximum potential error by:
% \[
% \text{minimize } \text{UpperBound} = \sum_l err_l + eb.
% \]

With $LoadedSize(l,b_l)$ representing the loaded size of level $l$, this problem can be formalized as follows:
% \vspace{-2mm}
\[
\begin{aligned}
\min_{b_l, l \in \{1, 2, ... L\}} \quad & \sum_l err(l,b_l) + eb, \\
\text{subject to} \quad & \sum_l LoadedSize(l,b_l) \leq S.
\end{aligned}
\]
% \vspace{-1mm}

Similar with~\Cref{sec: optimized-eb}, we effectively convert this problem to the knapsack dynamic programming problem, so that it can be solved similarly as in the error bound mode, also with negligible overhead.
% This formulation is a variation of the classic 0-1 knapsack problem: each level is treated as an item with a weight (a function of $u_l$:)$\sum_{m=1}^{u_l} e_{lm}$ (its data size) and a cost $\sum_{m=1}^{u_l} s_{lm}$ (the error incurred if it is not loaded), while the capacity is given by the maximum allowed retrieval size \(S\). By solving this problem, we can optimally select which level to load to minimize the reconstruction error under the bitrate constraint.
% \yang{Since the number of levels is approximately on the order of 
% $\log n$, we can effectively find the optimal strategy for selecting bitplanes $b_l$ at each level through exhaustive search. Moreover, by transforming the problem into a classical dynamic programming problem (i.e., the knapsack problem), we only need to adjust the definitions of value and cost accordingly. }

% \yang{
% In fact, since the dynamic programming approach is equivalent to exhaustive search in terms of solution space exploration, our method is guaranteed to find the optimal solution. This ensures that the selected bitplane strategy is not only efficient but also strictly adheres to the error bound.}