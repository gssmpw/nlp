\section{RELATED WORK}
Related to our work are methods that predict a probability distribution of pose or predict multiple possible pose outcomes.

\subsection{Probability Over Rotations}

Pose ambiguity can be dealt with by estimating a probability distribution over the group of rotations in 3D, ${\bf R} \in {\bf SO(3)}$ ____. The distribution may be represented parametrically, e.g., with a Gaussian mixture model ____ or non-parametrically ____. The former has the disadvantage of being hard to train ____, e.g., due to local minima ____.

In the non-parametric method by Murphy et al.,  the distribution is predicted implicitly, i.e., a neural network computes the probability $p({\bf R}, I)$ for any input matrix ${\bf R}$ and input image $I$ ____. The disadvantage of this strategy is that $p$ has to be normalized across a large sample of rotation matrices that all have to be fed through the network, resulting in a computationally expensive and slow training.

Given a normalized probability and a ground-truth rotation matrix ${\bf R}_o$, the loss function is defined as follows  ____
%
\begin{equation}\label{eq:prob_loss}
\mathcal{L} = - \log(p({\bf R}_o | I))\,.
\end{equation}
%

For inference, the rotation with maximum probability is chosen from a set of centers of an equivolumetric partitioning of {\bf SO(3)} ____,

\begin{equation}\label{eq:prob_inf}
{\bf \hat R} = \underset{{\bf R} \in {\bf SO(3)}}{\arg \max}\, p({\bf R} | I)\,.
\end{equation}

\subsection{Multiple Pose Hypotheses}

Manhardt et al. train a network to predict multiple pose hypotheses to learn alternative (ambiguous) poses ____. The network output is a fixed number $M$ of possible rotations. For training, the loss function combines a minimum loss $\mathcal{L}_{\mbox{\small min}}$, the minimum mean squared error (MSE) between the ground truth and the $M$ outcomes, with an average loss $\mathcal{L}_{\mbox{\small avg}}$, which is the average MSE across the $M$ outcomes,

\begin{equation}\label{eq:M-loss}
\mathcal{L} = (1-\epsilon \frac{M}{M-1}) \mathcal{L}_{\mbox{\small min}} + \epsilon \frac{M}{M-1}  \mathcal{L}_{\mbox{\small avg}}\,,
\end{equation}
%
where the parameter $\epsilon$ is gradually reduced from 0.05 to 0.01.

Inference is more complicated since we do not know a probability for each of the $M$ outcomes. Instead, the outcomes are clustered, and the median for each cluster is computed. The results can be compared against the input image to find the best fit  ____. Though this strategy involves again an extra selection step as mentioned above.