% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%

\documentclass[runningheads]{llncs}
% \documentclass[conference]{IEEEtran}

\usepackage{wrapfig}
\usepackage{graphicx}
\graphicspath{ {./figs/} }
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{url}
\usepackage[noadjust]{cite}  % bibtex references
\usepackage{subcaption} % automatically includes package "caption". For subfigure

\usepackage{amsmath}
\usepackage{amsfonts}  % mathbb
% \usepackage[cal=boondox]{mathalfa} % mathcal
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage[group-separator={,}]{siunitx}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[table]{xcolor} % add color to table cells
\usepackage{tabularx}

\usepackage{multirow} 
\usepackage{adjustbox} % adjust table size
\usepackage[misc]{ifsym} % envelope
\usepackage{xspace}
\usepackage[colorlinks=true, linkcolor=red, urlcolor=blue, citecolor=cyan, bookmarks=true, bookmarksopen, pagebackref=true]{hyperref}
\setcounter{tocdepth}{3}
% \setcounter{secnumdepth}{3}es in toc

\newcommand{\ac}[1]{\textcolor{magenta}{\textbf{Alvaro:} #1}\xspace}
\newcommand{\mk}[1]{\textcolor{blue}{\textbf{Murat:} #1}\xspace}

\renewcommand{\ac}[1]{}
\renewcommand{\mk}[1]{}

\title{A Systematic Evaluation of Generative Models on Tabular Transportation Data}

\titlerunning{A Systematic Evaluation of Generative Models on Transportation Data}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here

\author{
 Chengen Wang$^{(\textrm{\Letter})}$\inst{1} \and
 Alvaro Cardenas\inst{2} \and
 Gurcan Comert\inst{3} \and
 Murat Kantarcioglu\inst{1}
}

\authorrunning{C. Wang et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.

\institute{
University of Texas at Dallas, Dallas TX 75080, USA\\
\email{\{chengen.wang,muratk\}@utdallas.edu} \and
University of California, Santa Cruz,  Santa Cruz CA 95064, USA
\email{alacarde@ucsc.edu}\\
\and
Benedict College, Columbia SC 29204, USA\\
\email{Gurcan.Comert@Benedict.edu}
}

% \author{\IEEEauthorblockN{Chengen Wang$^{(\textrm{\Letter})}$}
% \IEEEauthorblockA{University of Texas as Dallas\\
% Email: chengen.wang@utdallas.edu}
% \and
% \IEEEauthorblockN{Yan Zhou}
% \IEEEauthorblockA{University of Texas as Dallas\\
% Email: yxz103420@utdallas.edu}
% \and
% \IEEEauthorblockN{Kangkook Jee}
% \IEEEauthorblockA{University of Texas as Dallas\\
% Email: Kangkook.Jee@utdallas.edu}
% \and
% \IEEEauthorblockN{Murat Kantarcioglu}
% \IEEEauthorblockA{University of Texas as Dallas\\
%     Email: muratk@utdallas.edu}}


% \author{
%   \IEEEauthorblockN{Chengen Wang$^{(\textrm{\Letter})}$}
%   \IEEEauthorblockA{University of Texas at Dallas\\
%      Email: chengen.wang@utdallas.edu}
% \and
%   \IEEEauthorblockN{Alvaro Cardenas}
%   \IEEEauthorblockA{University of California, Santa Cruz\\
%      Email: alacarde@ucsc.edu}
% \and
%     \IEEEauthorblockN{Gurcan Comert}
%     \IEEEauthorblockA{Benedict College\\
%        Email: Gurcan.Comert@Benedict.edu}
% \and
%   \IEEEauthorblockN{Murat Kantarcioglu}
%   \IEEEauthorblockA{University of Texas at Dallas\\
%      Email: muratk@utdallas.edu}
% }


\begin{document}
\maketitle              % typeset the header of the contribution

% \tableofcontents

\begin{abstract}
The sharing of large-scale transportation data is beneficial for transportation planning and policymaking. However, it also raises significant security and privacy concerns, as the data may include identifiable personal information, such as individuals' home locations. To address these concerns, synthetic data generation based on real transportation data offers a promising solution that allows privacy protection while potentially preserving data utility. Although there are various synthetic data generation techniques, they are often not tailored to the unique characteristics of transportation data, such as the inherent structure of transportation networks formed by all trips in the datasets. In this paper, we use New York City taxi data as a case study to conduct a systematic evaluation of the performance of widely used tabular data generative models. In addition to traditional metrics such as distribution similarity, coverage, and privacy preservation, we propose a novel graph-based metric tailored specifically for transportation data. This metric evaluates the similarity between real and synthetic transportation networks, providing potentially deeper insights into their structural and functional alignment. We also introduced an improved privacy metric to address the limitations of the commonly-used one. Our experimental results reveal that existing tabular data generative models often fail to perform as consistently as claimed in the literature, particularly when applied to transportation data use cases. Furthermore, our novel graph metric reveals a significant gap between synthetic and real data. This work underscores the potential need to develop generative models specifically tailored to take advantage of the unique characteristics of emerging domains, such as transportation. Our code is available at \href{https://github.com/chengenw/transportation.git}{https://github.com/chengenw/transportation.git}. 

\keywords{Tabular Data Synthesis \and Transportation Data \and Generative Models Evaluation}

\end{abstract}

% \begin{IEEEkeywords}
% Tabular Data Synthesis, Transportation Data, Generative Models Evaluation
% \end{IEEEkeywords}

\section{Introduction}\label{sec:intro}
% sharing transportation data: benefits and concerns
Open large-scale transportation data offer significant benefits, including enhancing transparency, improving traffic management and urban planning, and providing valuable opportunities for researchers to conduct in-depth studies that inform and support effective policymaking. However, the availability of large-scale data raises significant privacy concerns, as human mobility is highly sensitive. Transportation data may contain identifiable personal information such as individuals' home locations, and data sharing can infringe on individual privacy. For example, celebrities may be stalked with shared taxi journey data \cite{nyt_prv}.

% synthetic data generation -> privacy
To address these challenges, synthetic data generation techniques have emerged as a promising solution for data sharing, offering potentially high utility for downstream tasks while effectively mitigating the privacy concerns. 
These techniques essentially approximate the raw data distribution with machine learning models and then generate artificial data instead of sharing the raw data directly.

% Deep generative models
In recent years, deep learning-based synthetic data generation models have drawn considerable attention due to their ability to learn complex data distributions and generate realistic synthetic data. Among deep generative models, Generative Adversarial Networks (GANs) \cite{GAN}, Variational Autoencoders (VAEs) \cite{vae}, and diffusion models \cite{DDPM
% ,song2019generativeModeling
} have demonstrated remarkable capabilities in generating high-quality samples, particularly in domains such as images and text.

% general tabular data generation
Deep generative models have also been adapted for various data applications, including tabular data with structured rows and columns \cite{ctgan}. This prevalent form of data representation is widely used in diverse domains, including finance, healthcare, and e-Commerce, among others \cite{borisov2022deep_tab_survey}. Tabular data is also a common format for sharing transportation data. For instance, taxi trips can be represented in a tabular format, where each row corresponds to an individual trip, and the columns capture various properties of the trip, such as the start location, end location, and trip duration. 

Researchers have proposed a variety of models to generate synthetic tabular data, such as GAN and Variational autoencoder-based, diffusion-based generative modeling \cite{ctgan, kotelnikov2023tabddpm}. To synthesize transportation data, leveraging existing tabular data generative models provides an efficient starting point for further research. However, it is observed that current generative models are primarily designed for general tabular data synthesis, leaving several important questions unanswered:
\begin{enumerate}
    \item Transportation data possesses unique properties; for instance, the data collectively forms a transportation network. Can the existing metrics effectively capture and measure this characteristic?
    \item Tabular data generative models typically do not prioritize privacy preservation. How effectively do they perform when evaluated from a privacy preservation perspective?
    \item The datasets commonly used to evaluate these models generally do not include transportation data. Will the performance ranking of these models remain consistent when applied to transportation data?
\end{enumerate}

% contributions
In this paper, we aim to answer these questions by conducting a systematic evaluation of typical tabular data generative models \cite{sdv_paper, ctgan, ctab-gan, kim2022stasy, kotelnikov2023tabddpm} on transportation data, using New York City taxi data \cite{taxi_2015} as a case study. Our main contributions can be summarized as follows:
\begin{enumerate}
    \item We propose a novel graph-based metric to quantify the property gap between real and synthetic transportation data, leveraging the fact that transportation networks can naturally be represented as graphs.
    \item We propose an improved privacy leakage metric to investigate the privacy-preserving capabilities of these models and assess their vulnerabilities, particularly to membership inference attacks \cite{membership_MIA}.
    \item We systematically evaluate the performance of these models in the transportation domain using a comprehensive set of metrics, including downstream task utility, distribution similarity, diversity, complexity, and the two novel metrics mentioned above.
\end{enumerate}

% outline
In the remainder of this paper, we first provide a brief review of related work in Section \ref{sec:related} and present the necessary background knowledge in Section \ref{sec:background} to facilitate a better understanding of our work. Next, we introduce novel evaluation metrics in Section \ref{sec:novel_metric}. The experimental setup and results are detailed in Section \ref{sec:exp}, followed by a concluding summary in Section \ref{sec:conc}.

\section{Related Work}\label{sec:related}

\subsection{Generative Models} Generative Adversarial Networks (GANs) \cite{GAN} are among the most widely used generative models. It employs a generator and a discriminator, two competing neural networks; the generator tries to trick the discriminator to classify the fake data as real, while the discriminator tries to differentiate real and fake data.

Variational Autoencoders (VAEs) \cite{vae} are another class of generative models, which map real data to a distribution within a latent space by an encoder, then a decoder maps from the latent space to the input space.  

Diffusion models represent the latest advancement in generative models \cite{DDPM
% ,song2019generativeModeling
}. They involve a forward diffusion process and a reverse denoising process. In the forward process, noise is gradually added to the training data with increasing magnitude until the data becomes pure noise. In the reverse process, a model is trained to denoise the noisy data, effectively reconstructing the clean data and learning the underlying data distribution.

\subsection{Tabular Data Generation}
Tabular data pose unique challenges for synthetic data generation. Unlike image data, tabular data often consist of a mix of continuous and discrete variables. Moreover, values in the discrete columns frequently exhibit imbalanced distributions, adding an additional layer of complexity to the generation process. \cite{ctgan} proposes a conditional tabular GAN (CTGAN) to address these challenges. CTGAN employs two distinct sampling approaches to handle discrete and continuous variables in the training data. For discrete variables, it first randomly selects a discrete column, then samples rows based on the logarithm frequency of categorical values in that column. The sampled categorical values will serve as conditional inputs to GAN. For continuous variables, it estimates the number of modes for each column with variational Gaussian mixture models \cite{bishop2006pattern} and samples by modes and normalizes the values. \cite{ctgan} also proposed tabular VAE (TVAE) by adapting VAE to tabular data.

\cite{ctab-gan} makes improvements upon CTGAN motivated by several observations: \emph{Within one variable} of the tabular data there may be mixed continuous and categorical data types, and its distribution may be skewed and have a long tail. The authors address these issues by proposing mode-value pair for mixed data types, logarithmic transformation for variables with long tail distribution, and an additional continuous mode as the conditional input to GAN.

\cite{kotelnikov2023tabddpm} proposed TabDDPM by adapting diffusion models to the tabular data domain, employing Gaussian diffusion models for continuous variables and multinomial diffusion models for categorical variables \cite{hoogeboom2021argmax}. \cite{kim2022stasy} proposed the STaSy model by directly adapting score-based generative modeling \cite{song2021scorebased_sde} to the tabular data domain.

While these techniques have shown promise in tabular data, to the best of our knowledge, they have not been evaluated in the context of transportation data with their unique characteristics. 
In this work, we evaluate the aforementioned tabular synthetic data generation techniques within the context of a transportation data use case.

\input{sections/background}

\section{The Novel Evaluation Metrics}\label{sec:novel_metric}
In this section, to evaluate the generated synthetic data for transportation applications, we propose two novel metrics. 
%\mk{Please check whether the next claim is correct:} % there is a similar one to rDCR, but not the same
%MK2: I added transportation to make it even more different.
To the best of our knowledge, these metrics have not been previously used in the context of evaluating synthetic tabular transportation data generation. 

\subsection{Graph Similarity Metric for Transportation Network Data}
Transportation data, when viewed collectively, forms a transportation network. This network can be effectively represented as a graph, capturing the overall transportation trends and relationships within the data. For instance, the pickup and drop-off locations in the NYC taxi dataset correspond to different zones within the city. These zones can be represented as nodes in a graph, providing a structured way to model the transportation network. In other words, each trip between two zones can be represented as an edge in the transportation graph. Let the number of trips between two zones $i$ and $j$ be $n_{ij}$, where $i,j\in\mathbb{N}^+$, then the total number of trips $N=\sum_{i,j}n_{ij}$. Let the fraction of edges between two nodes $i$ and $j$ of the transportation graph $G$ be $p_G(i,j)=\frac{n_{ij}}{N}$. Clearly, $\sum_{i,j}p_G(i,j)=1$, which means the fraction of edges $p_G$ represents a distribution.

%\mk{I change the notation to make it graph centric. Please check.} 
% revised
We can construct a transportation graph from the real transportation data, denoted as $G_r$, and another graph, $G_s$, from the generated synthetic transportation data. We can measure the similarity between the real transportation graphs $G_r$ and the synthetic ones $G_s$ by calculating the similarity score $S_G$ between the two graphs as follows:
\begin{equation}
\begin{aligned} % break an equation into multiple lines, and numbering the equation once
   S_G(G_r, G_s) = 1 - \delta(p_{G_r}, p_{G_s}), \\
   s.t. \quad \delta(p_{G_r}, p_{G_s}) = \frac{1}{2}\sum_{i,j}|p_{G_r}(i,j) - p_{G_s}(i,j)|,
    % S_G(G_r,G_s) = 1 - \delta(G_r, G_s), \\
    % s.t. \quad \delta(G_r, G_s) = \frac{1}{2}\sum_{i,j}|G_r(i,j) - G_s(i,j)|,
\end{aligned}    
\end{equation}
where $p_{G_r}$ and $p_{G_s}$ represent edge number distributions for the real and synthetic graphs $G_r, G_s$ respectively, and $\delta(p_{G_r},p_{G_s})$ is the total variation distance.

\subsection{Distance to Closest Record Ratio as Privacy Leakage Metric} \label{sec:rDCR}
The success of membership inference attacks relies on the observation that models tend to overfit their training data \cite{membership_MIA}. Consequently, the distance between \emph{training} data and synthetic data is smaller than the distance between \emph{testing} data and synthetic data. This phenomenon is also evidenced by the fact that training data loss is typically smaller than testing data loss. Based on this observation, relying solely on the distance between real data and synthetic data---without differentiating two types of real data, training data and testing data--as commonly done in previous literature \cite{ctab-gan, dcr_RSD}, may be insufficient to reliably assess the risk of privacy leakage.

%\mk{We need to further justify why this metric is more robust ???}
We therefore propose a more robust metric that uses two distances instead of one, comparing the two distances by calculating their ratio. Specifically, we set aside holdout \emph{testing} data beside the \emph{training} data. Let the distance of training data to the closest synthetic data be $d_\alpha(r,s)$, and the distance of holdout testing data to the closest synthetic data be $d_\alpha(h,s)$, where $\alpha$ is the percentile of all the closest distance values, respectively, after ranking each set of distances in increasing order. The Distance to Closest Record Ratio (rDCR) is defined as
% follows:
% \begin{equation} \label{eq:ratio}
%     r_{DCR}=\frac{d_\alpha(r,s)}{d_\alpha(h,s)}.
% \end{equation}
$r_{DCR}=\frac{d_\alpha(r,s)}{d_\alpha(h,s)}.$

%\mk{Next sentences require justification. I would like to check the update once you are done. This is very critical !!!}
When $r_{DCR} < 1$, the distance between the training data and synthetic data is smaller than the distance between testing and synthetic data. This indicates overfitting, making the model vulnerable to membership inference attacks. 
\mk{I modified this more. Please check.} % ok
A smaller \emph{ratio} $r_{DCR}$ indicates greater overfitting of the model to the training data, making the model more vulnerable to a potential membership inference attack. On the other hand, if $r_{DCR} > 1$, a small \emph{distance} of $d_\alpha(r,s)$ alone may not be sufficient to demonstrate the vulnerability of a model to distanced-based membership inference attacks.
\mk{what about other MIA attacks?} % here is the limitation of the distance based metric

The metric from \cite{platzer2021holdout} bears a resemblance to the rDCR metric described in this work, but there are significant differences. Our metric focuses on the distance to the closest synthetic data for each real data as it is designed to analyze the privacy leakage for the real data. In contrast, their metric measures the closest distance to real data for each synthetic data. Additionally, we incorporate a percentile-based approach to assess privacy leakage, recognizing that typically only \emph{a small percentage} of the training data is vulnerable to membership inference attacks~\cite{carlini2022membership_first_principles}. By examining the percentile of data at risk of privacy leakage, this approach provides a more refined method for assessing privacy risks.

% \mk{Some of the columns miss the bold font. Please check and fix.} % values in those columns provided for reference purposes only. this is mentioned in the main text.

\section{Experiments}\label{sec:exp}

\subsection{Datasets} \label{sec:dataset}
% sampling, data size
In this work, we use New York City taxi trip data \cite{nyc_taxi} as the experimental dataset. 
%\mk{I am worried that the reviewers say why just one dataset. I tried to answer it. Please check and add citations.}
This dataset is widely utilized in transportation research \cite{
% riascos2020networks_nyc_taxi, 
correa2017exploring_nyc_taxi, 
mauro2022generating_nyc_taxi} and is one of the largest publicly available datasets in the domain. Its extensive size and accessibility make it a valuable resource for studies in this field. It is represented in a tabular data format, with each row containing detailed information about an individual taxi trip.
% Given these characteristics, we believe that using this dataset alone is sufficient for our evaluation, as it provides a robust and comprehensive basis for assessing the performance and utility of synthetic data generation techniques in the transportation domain. 
%\mk{Not sure we want say the next sentence but I added as a place holder. Or We can mention this in the conclusion too ??} % not sure too. I add one more sentence above. the first cited paper also uses this data alone
% However, while the dataset's scale and relevance justify its selection, incorporating additional datasets in future work could further validate the generalizability of the findings. 

More specifically, we use ``2015 Green Taxi Trip Data" \cite{taxi_2015}. It has $19.2$ million rows and each row has $21$ columns. Each row represents a single trip in a green taxi, and the column fields include location and time for both pickup and drop-off, trip distance, itemized fares, payment type, tax and passenger count etc.

We pre-process the data by dropping columns `Ehail\_fee' due to too many `NaN' values, changing each pickup/drop-off column `datetime' into two columns `weekday' and `time`. The final data has 22 columns with 8 categorical variables, 2 integer variables and 12 float-type numerical variables. This transportation dataset is more complicated than the tabular datasets usually used in the previous tabular data generative model papers, due to its larger data size, mixed data type and higher dimensionality. In the experiments, we randomly sample a subset of the data: the test dataset size is \num{20000}, and the training data size is \num{40000} by default, unless specified otherwise.

%\mk{Need a sentence to clarify how 2015 and 2019 data used jointly ?} % revised
\mk{Please check my modification.} % ok
The proposed graph metric requires knowledge of the zones into which the pickup and drop-off longitude and latitude coordinates fall for each trip. To fulfill this requirement, we utilize the New York City Green Taxi Trip Records from March 2019. A key distinction of this dataset, compared to the aforementioned one, is its less granular nature: locations are represented by zones rather than precise longitude and latitude coordinates. Notably, the zone variable is categorical, unlike longitude and latitude, which are numerical. This difference makes the dataset particularly well-suited for zone-based transportation metrics.

\subsection{Generative Models Used for Evaluation} \label{sec:gm_eval}
The generative models evaluated in this paper include Gaussian Copula \cite{sdv_paper
% , sdv_doc
}, CTGAN and TVAE \cite{ctgan}, CTABGAN \cite{ctab-gan} and two diffusion-based tabular data generative models: TabDDPM \cite{kotelnikov2023tabddpm} and STaSy \cite{kim2022stasy}. We evaluate these models using various metrics including utility, similarity, diversity and privacy leakage as detailed in Section \ref{sec:metrics} and \ref{sec:novel_metric}.

\subsection{Experimental Setup}
During the experiments, for each method, three models are trained, and five times of sampling are conducted for each trained model. We limit the sample size to \num{20000} for each sampling iteration due to the high memory demands of the following Wasserstein distance calculations. The results are reported as the mean and standard deviation across a total of $15$ sampling iterations.

\input{tables/downstream}

\subsection{Experimental Results}
We report the results in separate tables, with bold fonts to highlight the best performance values, except for columns with values provided for reference purposes. All columns without synthetic data involved are for reference purposes.

\subsubsection{Downstream Task Performance}
As described in Section \ref{sec:metrics}, the model predicts the total amount for a given trip based on the other trip information. The results are reported in Table~\ref{tab:downstream}, where ``model" is the model name, ``dwn\_tr\_tr" means training on training data and predicting on training data, ``dwn\_tr\_syn" means training on training data and predicting on synthetic data, ``dwn\_tr\_te" means training on training data and predicting on testing data, provided for reference purpose, ``dwn\_syn\_syn" means training on synthetic data and predicting on synthetic data, ``dwn\_syn\_tr" means training on synthetic data and predicting on training data, and ``dwn\_syn\_te"  means training on synthetic data and predicting on testing data. The three columns ``dwn\_tr\_syn", ``dwn\_syn\_tr" and ``dwn\_syn\_te"  demonstrate the performance of these models. The performance of the models trained on synthetic data is particularly critical, as in real-world applications of data synthesis, only the synthetic data is typically made public for downstream tasks. The $R^2$ values in the table are multiplied by 100. Our results demonstrate that TabDDPM achieves the best downstream task performance among the evaluated methods.

\mk{Again some columns do not have bold results in table 2. Please check and adapt.} % mentioned in main text, some columns for reference purpose only
\input{tables/similarity}

\subsubsection{Statistical Similarity}
We report the experimental results for the Wasserstein distances in Table~\ref{tab:similarity}, where ``w1\_tr\_te" is the Wasserstein distance between the training data and the testing data, provided for reference purpose, ``w1\_tr\_syn" is the Wasserstein distance between the training data and the synthetic data, and ``w1\_te\_syn" is the Wasserstein distance between the testing data and the synthetic data. The results demonstrate that CTABGAN and TabDDPM have the best performance among all the models.

\input{tables/graph_metric}
\subsubsection{Graph Similarity Metric} \label{sec:exp_graph-metric}
We report the graph similarity results in Table~\ref{tab:graph_metric}, where ``G\_tr\_te" is the graph similarity between the training data and the testing data, provided for reference purpose, ``G\_tr\_syn" is the graph similarity between the training data and the synthetic data, ``G\_te\_syn" is the graph similarity score between the testing data and the synthetic data. The similarity values are
multiplied by 100 in the table. 
%\mk{where this reference value comes from? Please explain. }
The results indicate that all models exhibit a significant performance gap compared to the reference value $73.17$ given in column ``G\_tr\_te". The TabDDPM model shows particularly low graph metric values. Further investigation shows that TabDDPM suffers severe mode collapse. We speculate that this issue may arise from its difficulty in handling categorical variables with hundreds of classes, such as the ``zone" variable, or it may require significant additional hyperparameter tuning. 
\mk{Also, here I think you are changing the dataset for evaluating graph metric ?? not on the original 2015 data ?? We need to clarify this.} % done
Note that here we use the dataset with zone-based locations, as mentioned in Section \ref{sec:dataset}.
%\mk{Need to get the STaSy results ??}
The results for the STaSy model are unavailable due to out-of-memory related issues, likely caused by challenges in achieving convergence.

\input{tables/diversity}
\subsubsection{Diversity}
We report the percentage of the coverage in Table~\ref{tab:diversity}, where ``cov\_tr\_te" is the coverage of the training data by the testing data, provided for reference purpose, ``cov\_tr\_syn" is the coverage of the training data by synthetic data and ``cov\_te\_syn" is the coverage of the testing data by the synthetic data. The coverage values are multiplied by 100 in the table. Clearly TabDDPM has the best performance. The results also reveal that all other models suffer mode dropping or collapse \cite{thompson2022evaluation}, as shown by their small coverage values.

\input{figs/ratio}
\input{tables/privacy}

\subsubsection{Privacy Leakage Metric}
We report the privacy leakage metric results in Table~\ref{tab:privacy}, where ``dcr\_rs" is the distance to the closest synthetic record from each real training data, ``dcr\_hs" is the DCR from holdout data to synthetic data, ``dcr\_rr" the DCR within real data, ``dcr\_ss" is the DCR within synthetic data, ``rDCR" is the ratio $\frac{d_\alpha(r,s)}{d_\alpha(h,s)}$, and ``percentile" is the $\alpha$ of the DCRs, as described in Section \ref{sec:rDCR}.

% \input{figs/ratio-speed}

Based solely on ``dcr\_rs", as commonly used in previous literature \cite{ctab-gan}, the Gaussian Copula model has the best privacy preservation, while the TabDDPM model has the worst. 
%\mk{Please clarify the next sentence more:}
However, the results also demonstrate that none of the ``dcr\_rs" is smaller than ``dcr\_rr", which implies that actually there is possibly no privacy leakage as the synthetic data is far away from the real data.
%\mk{Remind the discussion here shortly.}
Just as we discussed in Section \ref{sec:rDCR}, ``dcr\_rs" alone may be insufficient to assess the risk of privacy leakage. 

With the proposed DCR ratio metric, we calculate the $rDCR$ for different percentile $\alpha$ values, and present the results in Figure \ref{fig:ratio}. As shown in the figure, contrary to the above conclusion, the Gaussian Copula model is vulnerable to membership inference attacks at very small values of $\alpha$, where its DCR ratio is smaller than $1$, while all the other models appear to be robust against distance-based membership inference attacks, as their DCR ratio remains approximately $1$. This finding highlights the advantage of the percentile-based ratio metric.

\input{figs/speed}

\subsubsection{Complexity}
%\mk{you need the give the system settings so that we can understand. Ie, we ran using Intel X, Nvidia Y memory z etc.}
We evaluate the complexity of these models by comparing their running times. Note that the running time for diffusion-based models includes both the training and sampling time, whereas for other models, it consists only of the training time. The results are presented in Figure \ref{fig:speed}. The running times are reported in minutes, obtained from a machine with Intel(R) Core(TM) i9-9900X CPU, 64G memory, and GeForce RTX 2080. The results demonstrate that CTABGAN and STaSy models have much higher time complexity than others. Although the Gaussian Copula model has the fast training speed, its performance is not satisfactory, especially as evidenced by its minimum coverage values, severe mode collapse and possible privacy leakage.
%\mk{what about privacy ??}
The results indicate that TabDDPM achieves the best balance between speed and performance.


\section{Conclusion}\label{sec:conc}
In this paper, we conduct a systematic evaluation of generative models for synthetic tabular transportation data generation. The evaluation is conducted based on a variety of metrics including downstream tasks performance, distribution similarity, generation diversity, and privacy leakage. We also evaluate these models on our novel graph similarity and DCR ratio metrics.

The results indicate that TabDDPM achieves the overall best performance across various metrics. However, it appears that TabDDPM may struggle to handle categorical variables with hundreds of classes. Additionally, the findings reveal the performance gap of the current generative models, and the prevalence of mode collapse, underscoring the need to develop models specifically tailored to domains such as transportation.

Furthermore, extending the evaluation beyond the New York City taxi data is expected to offer more insights on the current tabular generative models. 

% \section*{Acknowledgments}
% The research reported herein was supported in part by ?

\begin{credits}
 % \subsection{\ackname}The research reported herein was supported in part by ?.
\end{credits}

% \bibliographystyle{IEEEtran}
\addcontentsline{toc}{section}{References} % add references to toc
% \bibliographystyle{apalike}
\bibliographystyle{splncs04}
\bibliography{references}

\end{document}
