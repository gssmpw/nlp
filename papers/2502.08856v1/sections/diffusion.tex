\subsection{Diffusion Models for Tabular Data Generation}
As diffusion models represent the latest advancements in generative modeling and are less widely known compared to GANs or VAEs, we provide a brief overview of these models in this section to facilitate understanding.

Let the training data be $x_0\sim q(x_0)$. In the forward process, Gaussian noise is added to the clean data, and the diffusion process in DDPM \cite{DDPM} is formulated as
\begin{equation}
    q(x_t|x_{t-1}):=\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t\mathbf{I})
\end{equation}
where $\beta_t$ is the variance schedule for the Gaussian noise. Based on the Bayesian theorem, we can calculate the reverse process $q(x_{t-1}|x_t, x_0)$. We use a neural network to represent the denoising process as
\begin{equation}
    p_\theta(x_{t-1}|x_t)=\mathcal{N}(x_{t-1};\boldsymbol{\mu}_\theta(x_t,t),\boldsymbol{\Sigma}_\theta(x_t,t)).
\end{equation}
After setting $\boldsymbol{\Sigma}_\theta(x_t,t)$ to untrained time-dependent constants, the training loss is simplified to learn the added Gaussian noise $\epsilon$ as follows:
\begin{equation}
    \mathbb{E}_{x_0,t,\epsilon\sim\mathcal{N}(0,\boldsymbol{I})}\left[w(t)\|\epsilon-\epsilon_\theta(x_t,t)\|^2\right],
\end{equation}
where $w(t)$ is a weight function.

% This Gaussian diffusion process defined above works on continuous data space. 
The generative model TabDDPM \cite{kotelnikov2023tabddpm} mentioned in Section \ref{sec:related} leverages multinomial diffusion models to generate categorical data. TabDDPM's forward process corrupts the categorical data by adding uniform noise over $K$ classes as follows \cite{kotelnikov2023tabddpm}:
\begin{equation}
\begin{aligned}
    q(x_t|x_{t-1}):=\mathrm{Cat}(x_t;(1-\beta_t)x_{t-1}+\frac{\beta_t}{K}),\\
    q(x_T):=\mathrm{Cat}(x_T;\frac{1}{K}),
\end{aligned}
\end{equation}
where $x_t$ is a one-hot encoded categorical variable with $K$ values.

\cite{song2021scorebased_sde} generalizes the diffusion models to the continuous-time domain. The forward process can be modeled as the solution to a stochastic differential equation (SDE):
\begin{equation}
    \mathrm{d}x=\boldsymbol{f}(x,t)\mathrm{d}t + g(t)\mathrm{d}\boldsymbol{w},
\end{equation}
where $\boldsymbol{w}$ is the standard Wiener process, $\boldsymbol{f}(\cdot,t)$ is the drift coefficient, and $g(t)$ is the diffusion coefficient. The reverse process is given by the reverse-time SDE:
\begin{equation}
    \mathrm{d}x=\left[\boldsymbol{f}(x,t)-g(t)^2\nabla_x\log p_t(x)\right]\mathrm{d}t + g(t)\mathrm{d}\bar{\boldsymbol{w}},
\end{equation}
where $\bar{\boldsymbol{w}}$ is the standard Wiener process in reverse time, and $\nabla_x\log p_t(x)$ is the score of the probability density $p_t(x)$. The STaSy paper \cite{kim2022stasy} mentioned in Section \ref{sec:related} is proposed based on the SDE generative modeling.
