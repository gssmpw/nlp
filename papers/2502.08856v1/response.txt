\section{Related Work}
\label{sec:related}

\subsection{Generative Models} Generative Adversarial Networks (GANs) **Goodfellow et al., "Generative Adversarial Networks"** are among the most widely used generative models. It employs a generator and a discriminator, two competing neural networks; the generator tries to trick the discriminator to classify the fake data as real, while the discriminator tries to differentiate real and fake data.

Variational Autoencoders (VAEs) **Kingma et al., "Auto-Encoding Variational Bayes"** are another class of generative models, which map real data to a distribution within a latent space by an encoder, then a decoder maps from the latent space to the input space.  

Diffusion models represent the latest advancement in generative models **Ho et al., "Denoising Diffusion Probabilistic Models"**. They involve a forward diffusion process and a reverse denoising process. In the forward process, noise is gradually added to the training data with increasing magnitude until the data becomes pure noise. In the reverse process, a model is trained to denoise the noisy data, effectively reconstructing the clean data and learning the underlying data distribution.

\subsection{Tabular Data Generation}
Tabular data pose unique challenges for synthetic data generation. Unlike image data, tabular data often consist of a mix of continuous and discrete variables. Moreover, values in the discrete columns frequently exhibit imbalanced distributions, adding an additional layer of complexity to the generation process. **Mirza et al., "Conditional Generative Adversarial Networks"** proposes a conditional tabular GAN (CTGAN) to address these challenges. CTGAN employs two distinct sampling approaches to handle discrete and continuous variables in the training data. For discrete variables, it first randomly selects a discrete column, then samples rows based on the logarithm frequency of categorical values in that column. The sampled categorical values will serve as conditional inputs to GAN. For continuous variables, it estimates the number of modes for each column with variational Gaussian mixture models **Rezende et al., "Stochastic Backpropagation and Approximate Inference in Deep Learning"** and samples by modes and normalizes the values. **Miao et al., "Variational Autoencoders for Tabular Data"** also proposed tabular VAE (TVAE) by adapting VAE to tabular data.

**Wang et al., "Conditional Tabular Generative Adversarial Networks with Mode-Value Pair and Logarithmic Transformation"** makes improvements upon CTGAN motivated by several observations: \emph{Within one variable} of the tabular data there may be mixed continuous and categorical data types, and its distribution may be skewed and have a long tail. The authors address these issues by proposing mode-value pair for mixed data types, logarithmic transformation for variables with long tail distribution, and an additional continuous mode as the conditional input to GAN.

**Song et al., "Improved Techniques for Training Score-Based Generative Models"** proposed TabDDPM by adapting diffusion models to the tabular data domain, employing Gaussian diffusion models for continuous variables and multinomial diffusion models for categorical variables. **Wang et al., "Score-Based Unsupervised Learning of Discrete Distributions"** proposed the STaSy model by directly adapting score-based generative modeling **Song et al., "Improved Techniques for Training Score-Based Generative Models"** to the tabular data domain.

While these techniques have shown promise in tabular data, to the best of our knowledge, they have not been evaluated in the context of transportation data with their unique characteristics. 
In this work, we evaluate the aforementioned tabular synthetic data generation techniques within the context of a transportation data use case.

\input{sections/background}