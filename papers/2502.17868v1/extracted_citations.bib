@inproceedings{AeroRigUI_CHI2023,
address = {New York, NY, USA},
author = {Yu, Lilith and Gao, Chenfeng and Wu, David and Nakagaki, Ken},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3544548.3581437},
file = {:Users/hanc/Downloads/papers/3544548.3581437.pdf:pdf},
isbn = {9781450394215},
keywords = {acm reference format,actuated tangible user interface,human robot interaction,spatial user interface display},
month = {apr},
pages = {1--18},
publisher = {ACM},
title = {{AeroRigUI: Actuated TUIs for Spatial Interaction using Rigging Swarm Robots on Ceilings in Everyday Space}},
url = {https://dl.acm.org/doi/10.1145/3544548.3581437},
year = {2023}
}

@inproceedings{Ahmed2022,
abstract = {This paper presents a novel design of a multi-directional bicycle robot, which is developed for the inspection of steel structures, in particular, steel-reinforced bridges. The locomotion concept is based on arranging two magnetic wheels in a bicycle-like configuration with two independent steering actuators. This configuration allows the robot to possess multi-directional mobility. An additional free joint helps the robot adapt naturally to non-flat and complex steel structures. The robot's design provides the advantage of being mechanically simple and providing high-level mobility across diverse steel structures. In addition, a visual sensor is equipped that allows the data collection for steel defect detection with offline training and validation. The paper also provides a novel pipeline for Steel Defect Detection, which utilizes multiple datasets (one for training and one for validation) from real bridges. The quantitative results have been reported for three Deep Encoder-Decoder Networks (i.e., LinkNet, UNet, DeepLab) with their corresponding Encoder modules (i.e., ResNet-18, ResNet-34, RegNet-X2, EfficientNet-B0, and EfficientNet-B2). Due to space concerns, the qualitative results have been outlined in Appendix, with a link in Fig. 11 caption to access the result provided.},
author = {Ahmed, Habib and Nguyen, Son Thanh and La, Duc and Le, Chuong Phuoc and La, Hung Manh},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS47612.2022.9981325},
file = {:Users/hanc/Downloads/papers/Multi-directional_Bicycle_Robot_for_Bridge_Inspection_with_Steel_Defect_Detection_System.pdf:pdf},
isbn = {9781665479271},
issn = {21530866},
pages = {4617--4624},
publisher = {IEEE},
title = {{Multi-directional Bicycle Robot for Bridge Inspection with Steel Defect Detection System}},
volume = {2022-Octob},
year = {2022}
}

@inproceedings{Alonso-Mora2011,
abstract = {This paper describes work on multi-robot pattern formation. Arbitrary target patterns are represented with an optimal robot deployment, using a method that is independent of the number of robots. Furthermore, the trajectories are visually appealing in the sense of being smooth, oscillation free, and showing fast convergence. A distributed controller guarantees collision free trajectories while taking into account the kinematics of differentially driven robots. Experimental results are provided for a representative set of patterns, for a swarm of up to ten physical robots, and for fifty virtual robots in simulation. {\textcopyright} 2011 IEEE.},
author = {Alonso-Mora, Javier and Breitenmoser, Andreas and Rufli, Martin and Siegwart, Roland and Beardsley, Paul},
booktitle = {IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5980269},
file = {:Users/hanc/Downloads/papers/Multi-robot_system_for_artistic_pattern_formation.pdf:pdf},
isbn = {978-1-61284-386-5},
issn = {10504729},
month = {may},
pages = {4512--4517},
publisher = {IEEE},
title = {{Multi-Robot System for Artistic Pattern Formation}},
url = {http://ieeexplore.ieee.org/document/5980269/},
year = {2011}
}

@article{Alonso-Mora2012,
abstract = {In this article we present a novel display that is created using a group of mobile robots. In contrast to traditional displays that are based on a fixed grid of pixels, such as a screen or a projection, this work describes a display in which each pixel is a mobile robot of controllable color. Pixels become mobile entities, and their positioning and motion are used to produce a novel experience. The system input is a single image or an animation created by an artist. The first stage is to generate physical goal configurations and robot colors to optimally represent the input imagery with the available number of robots. The run-time system includes goal assignment, path planning and local reciprocal collision avoidance, to guarantee smooth, fast and oscillation-free motion between images. The algorithms scale to very large robot swarms and extend to a wide range of robot kinematics. Experimental evaluation is done for two different physical swarms of size 14 and 50 differentially driven robots, and for simulations with 1,000 robot pixels.},
author = {Alonso-Mora, Javier and Breitenmoser, Andreas and Rufli, Martin and Siegwart, Roland and Beardsley, Paul},
doi = {10.1177/0278364912442095},
file = {:Users/hanc/Downloads/papers/12-alonsomora-ijrr.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {Image display,Multi-robot system,Non-holonomic path planning,Pattern formation,Video display},
month = {may},
number = {6},
pages = {753--773},
title = {{Image and animation display with multiple mobile robots}},
url = {https://doi.org/10.1177/0278364912442095 http://journals.sagepub.com/doi/10.1177/0278364912442095},
volume = {31},
year = {2012}
}

@inproceedings{Alonso-Mora2015,
abstract = {A taxonomy for gesture-based interaction between a human and a group (swarm) of robots is described. Methods are classified into two categories. First, free-form interaction, where the robots are unconstrained in position and motion and the user can use deictic gestures to select subsets of robots and assign target goals and trajectories. Second, shape-constrained interaction, where the robots are in a configuration shape that can be modified by the user. In the later, the user controls a subset of meaningful degrees of freedom defining the overall shape instead of each robot directly. A multi-robot interactive display is described where a depth sensor is used to recognize human gesture, determining the commands sent to a group comprising tens of robots. Experimental results with a preliminary user study show the usability of the system.},
author = {Alonso-Mora, J. and {Haegeli Lohaus}, S. and Leemann, P. and Siegwart, R. and Beardsley, P.},
booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2015.7140033},
file = {:Users/hanc/Downloads/papers/Gesture_based_human_-_Multi-robot_swarm_interaction_and_its_application_to_an_interactive_display.pdf:pdf},
isbn = {978-1-4799-6923-4},
issn = {10504729},
month = {may},
pages = {5948--5953},
publisher = {IEEE},
title = {{Gesture Based Human - Multi-Robot Swarm Interaction and its Application to an Interactive Display}},
url = {http://ieeexplore.ieee.org/document/7140033/},
volume = {2015-June},
year = {2015}
}

@article{Brambilla2013,
abstract = {Swarm robotics is an approach to collective robotics that takes inspiration from the self-organized behaviors of social animals. Through simple rules and local interactions, swarm robotics aims at designing robust, scalable, and flexible collective behaviors for the coordination of large numbers of robots. In this paper, we analyze the literature from the point of view of swarm engineering: we focus mainly on ideas and concepts that contribute to the advancement of swarm robotics as an engineering field and that could be relevant to tackle real-world applications. Swarm engineering is an emerging discipline that aims at defining systematic and well founded procedures for modeling, designing, realizing, verifying, validating, operating, and maintaining a swarm robotics system. We propose two taxonomies: in the first taxonomy, we classify works that deal with design and analysis methods; in the second taxonomy, we classify works according to the collective behavior studied. We conclude with a discussion of the current limits of swarm robotics as an engineering discipline and with suggestions for future research directions. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Brambilla, Manuele and Ferrante, Eliseo and Birattari, Mauro and Dorigo, Marco},
doi = {10.1007/s11721-012-0075-2},
file = {:Users/hanc/Downloads/papers/s11721-012-0075-2.pdf:pdf},
issn = {1935-3812},
journal = {Swarm Intelligence},
keywords = {Review,Swarm engineering,Swarm robotics},
month = {mar},
number = {1},
pages = {1--41},
title = {{Swarm robotics: a review from the swarm engineering perspective}},
url = {http://link.springer.com/10.1007/s11721-012-0075-2},
volume = {7},
year = {2013}
}

@inproceedings{CLASH2011,
abstract = {CLASH is a 10cm, 15g robot capable of climbing vertical loose-cloth surfaces at 15 cm per second. The robot has a single actuator driving its six legs which are equipped with novel passive foot mechanisms to facilitate smooth engagement and disengagement of spines. These foot mechanisms are designed to be used on penetrable surfaces and offer improved tensile normal force generation during stance and reduced normal pull-off forces during retraction. Descended from the DASH hexapedal robot, CLASH features a redesigned transmission with a lower profile and improved dynamics for climbing. CLASH is the first known robot to climb loose vertical cloth and is able to climb surfaces when surface rigidity is not guaranteed. {\textcopyright} 2011 IEEE.},
author = {Birkmeyer, P. and Gillies, A. G. and Fearing, R. S.},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6048573},
file = {:Users/hanc/Downloads/papers/CLASH_Climbing_vertical_loose_cloth.pdf:pdf},
isbn = {978-1-61284-456-5},
month = {sep},
pages = {5087--5093},
publisher = {IEEE},
title = {{CLASH: Climbing Vertical Loose Cloth}},
url = {https://ieeexplore.ieee.org/document/6048573},
year = {2011}
}

@article{Calico,
author = {Sathya, Anup and Li, Jiasheng and Rahman, Tauhidur and Gao, Ge and Peng, Huaishu},
title = {Calico: Relocatable On-cloth Wearables with Fast, Reliable, and Precise Locomotion},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
url = {https://doi.org/10.1145/3550323},
doi = {10.1145/3550323},
abstract = {We explore Calico, a miniature relocatable wearable system with fast and precise locomotion for on-body interaction, actuation and sensing. Calico consists of a two-wheel robot and an on-cloth track mechanism or "railway," on which the robot travels. The robot is self-contained, small in size, and has additional sensor expansion options. The track system allows the robot to move along the user's body and reach any predetermined location. It also includes rotational switches to enable complex routing options when diverging tracks are presented. We report the design and implementation of Calico with a series of technical evaluations for system performance. We then present a few application scenarios, and user studies to understand the potential of Calico as a dance trainer and also explore the qualitative perception of our scenarios to inform future research in this space.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {136},
numpages = {32},
keywords = {interactive computing, kinetic wearables, mobile computing, ubiquitous computing, wearables}
}

@article{EpidermalRobots,
author = {Dementyev, Artem and Hernandez, Javier and Choi, Inrak and Follmer, Sean and Paradiso, Joseph},
title = {Epidermal Robots: Wearable Sensors That Climb on the Skin},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3264912},
doi = {10.1145/3264912},
abstract = {Epidermal sensing has enabled significant advancements towards the measurement and understanding of health. Most of the existing medical instruments require direct expert manipulation of a doctor, measure a single parameter, and/or have limited sensing coverage. In contrast, this work demonstrates the first epidermal robot with the ability to move over the surface of the skin and capture a large range of body parameters. In particular, we developed SkinBot, a 2x4x2 centimeter-size robot that moves over the skin surface with a two-legged suction-based locomotion. We demonstrate three of the potential medical sensing applications which include the measurement of body biopotentials (e.g., electrodermal activity, electrocardiography) through modified suction cups that serve as electrodes, skin imaging through a skin-facing camera that can capture skin anomalies, and inertial body motions through a 6-axis accelerometer and gyroscope that can capture changes of body posture and subtle cardiorespiratory vibrations.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {102},
numpages = {22},
keywords = {skin, sensors, robotics, health, epidermal robots, Wearable}
}

@inproceedings{FabRobotics2024,
author = {Bhattacharya, Ramarko and Lindstrom, Jonathan and Taka, Ahmad and Nisser, Martin and Mueller, Stefanie and Nakagaki, Ken},
title = {FabRobotics: Fusing 3D Printing with Mobile Robots to Advance Fabrication, Robotics, and Interaction},
year = {2024},
isbn = {9798400704024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623509.3633365},
doi = {10.1145/3623509.3633365},
abstract = {We present FabRobotics, a digital fabrication pipeline that combines traditional 3D printing with mobile robots. By integrating these two technologies, we aim to create new opportunities for 3D printers to fabricate objects quickly and efficiently, and for mobile robots to enhance their adaptability and interactivity. To explore this novel research opportunity, we have developed a proof-of-concept implementation pipeline, allowing users to execute hybrid turn-taking control of a 3D printer and mobile robots to autonomously 3D print objects on/with mobile robots. The system was implemented with commercially available 3D printers (Prusa MINI) and mobile robots (toio), and we share various techniques and knowledge specific to fusing 3D printers and mobile robots (e.g. printing mobile robot docks for stable prints on robots). Based on the proof-of-concept system, we demonstrate various application usages and functionalities, showcasing how 3D printing and mobile robots can mutually advance each other for novel fabrication and interaction. Lastly, we share our further exploration of extended prototypes (e.g. fusing two printers) and discuss future technical challenges and research opportunities.},
booktitle = {Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {17},
numpages = {13},
keywords = {Digital fabrication, Robotics, Swarm user interfaces},
location = {Cork, Ireland},
series = {TEI '24}
}

@inproceedings{Griddrones2018,
abstract = {We present GridDrones, a self-levitating programmable matter platform that can be used for representing 2.5D voxel grid relief maps capable of rendering unsupported structures and 3D transformations. GridDrones consists of cube-shaped nanocopters that can be placed in a volumetric 1xnxn midair grid, which is demonstrated here with 15 voxels. The number of voxels and scale is only limited by the size of the room and budget. Grid deformations can be applied interactively to this voxel lattice by manually selecting a set of voxels, then assigning a continuous topological relationship between voxel sets that determines how voxels move in relation to each other and manually drawing out selected voxels from the lattice structure. Using this simple technique, it is possible to create unsupported structures that can be translated and oriented freely in 3D. Shape transformations can also be recorded to allow for simple physical shape morphing animations. This work extends previous work on selection and editing techniques for 3D user interfaces.},
address = {New York, NY, USA},
author = {Braley, Sean and Rubens, Calvin and Merritt, Timothy and Vertegaal, Roel},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3242587.3242658},
file = {:Users/hanc/Downloads/papers/3242587.3242658.pdf:pdf},
isbn = {9781450359481},
keywords = {Claytronics,Organic user interfaces,Programmable matter,Radical atoms,Swarm user interfaces},
month = {oct},
pages = {87--98},
publisher = {ACM},
title = {{GridDrones: A Self-Levitating Physical Voxel Lattice for Interactive 3D Surface Deformations}},
url = {https://dl.acm.org/doi/10.1145/3242587.3242658},
year = {2018}
}

@inproceedings{HERMITS2020,
abstract = {We introduce HERMITS, a modular interaction architecture for self-propelled Tangible User Interfaces (TUIs) that incorporates physical add-ons, referred to as mechanical shells. The mechanical shell add-ons are intended to be dynamically reconfigured by utilizing the locomotion capability of self-propelled TUIs (e.g. wheeled TUIs, swarm UIs). We developed a proof-of-concept system that demonstrates this novel architecture using two-wheeled robots and a variety of mechanical shell examples. These mechanical shell add-ons are passive physical attatchments that extend the primitive interactivities (e.g. shape, motion and light) of the self-propelled robots. The paper proposes the architectural design, interactive functionality of HERMITS as well as design primitives for mechanical shells. The paper also introduces the prototype implementation that is based on an off-the-shelf robotic toy with a modified docking mechanism. A range of applications is demonstrated with the prototype to motivate the collective and dynamically reconfigurable capability of the modular architecture, such as an interactive mobility simulation, an adaptive home/desk environment, and a story-telling narrative. Lastly, we discuss the future research opportunity of HERMITS to enrich the interactivity and adaptability of actuated and shape changing TUIs.},
address = {New York, NY, USA},
author = {Nakagaki, Ken and Leong, Joanne and Tappa, Jordan L. and Wilbert, Jo{\~{a}}o and Ishii, Hiroshi},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3379337.3415831},
file = {:Users/hanc/Downloads/papers/3379337.3415831.pdf:pdf},
isbn = {9781450375146},
keywords = {Actuated tangible user interface,Human robot interaction,Mechanical shell,Swarm user interface},
month = {oct},
pages = {882--896},
publisher = {ACM},
title = {{HERMITS: Dynamically Reconfiguring the Interactivity of Self-Propelled TUIs with Mechanical Shell Add-ons}},
url = {https://dl.acm.org/doi/10.1145/3379337.3415831},
year = {2020}
}

@inproceedings{HapticBots2021,
abstract = {HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability - these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user's hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2108.10829},
author = {Suzuki, Ryo and Ofek, Eyal and Sinclair, Mike and Leithinger, Daniel and Gonzalez-Franco, Mar},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3472749.3474821},
eprint = {2108.10829},
file = {:Users/hanc/Downloads/papers/3472749.3474821.pdf:pdf},
isbn = {9781450386357},
keywords = {encountered-type haptics,swarm user interfaces,tabletop mobile robots,virtual reality},
month = {oct},
pages = {1269--1281},
publisher = {ACM},
title = {{HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots}},
url = {https://dl.acm.org/doi/10.1145/3472749.3474821},
year = {2021}
}

@article{HawkesGeckoHuman2015,
abstract = {Since the discovery of the mechanism of adhesion in geckos, many synthetic dry adhesives have been developed with desirable gecko-like properties such as reusability, directionality, self-cleaning ability, rough surface adhesion and high adhesive stress. However, fully exploiting these adhesives in practical applications at different length scales requires efficient scaling (i.e. with little loss in adhesion as area grows). Just as natural gecko adhesives have been used as a benchmark for synthetic materials, so can gecko adhesion systems provide a baseline for scaling efficiency. In the tokay gecko ( Gekko gecko ), a scaling power law has been reported relating the maximum shear stress $\sigma$ max to the area A : $\sigma$ max ∝ A −1/4 . We present a mechanical concept which improves upon the gecko's non-uniform load-sharing and results in a nearly even load distribution over multiple patches of gecko-inspired adhesive. We created a synthetic adhesion system incorporating this concept which shows efficient scaling across four orders of magnitude of area, yielding an improved scaling power law: $\sigma$ max ∝ A −1/50 . Furthermore, we found that the synthetic adhesion system does not fail catastrophically when a simulated failure is induced on a portion of the adhesive. In a practical demonstration, the synthetic adhesion system enabled a 70 kg human to climb vertical glass with 140 cm 2 of adhesive per hand.},
author = {Hawkes, Elliot W. and Eason, Eric V. and Christensen, David L. and Cutkosky, Mark R.},
doi = {10.1098/rsif.2014.0675},
file = {:Users/hanc/Downloads/papers/rsif.2014.0675.pdf:pdf},
issn = {1742-5689},
journal = {Journal of The Royal Society Interface},
keywords = {Adhesion,Bioinspiration,Climbing,Gecko,Scaling},
month = {jan},
number = {102},
pages = {20140675},
pmid = {25411404},
title = {{Human climbing with efficiently scaled gecko-inspired dry adhesives}},
url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2014.0675},
volume = {12},
year = {2015}
}

@inproceedings{Hiraki2016,
author = {Hiraki, Takefumi and Fukushima, Shogo and Naemura, Takeshi},
title = {Phygital field: an integrated field with a swarm of physical robots and digital images},
year = {2016},
isbn = {9781450345392},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988240.2988242},
doi = {10.1145/2988240.2988242},
abstract = {Collaboration between computer graphics and multiple robots has attracted increasing attention in several areas. To preserve the seamless connection between them, the system needs to be able to accurately determine the position and state of the robots and be able to control them easily and instantly. However, realizing a responsive control system for a large number of mobile robots without complex settings and at the same time avoiding the system load problem is not trivial. Our system, called "Phygital Field", can project two types of information: visible images for humans and data patterns for mobile robots in the same location by utilizing pixel-level visual light communication (PVLC) technology. Phygital Field offers two technical innovations: an initialization-free and marker-free localization and control method, and a system noted for its simplicity and scalability. Phygital Field enables the robots to always recognize their own positions and states immediately, the measurement devices are not required because localization and control of the robots are realized through projection. These features are very important to improve the reconfigurability of the system. The idea of controlling robots by using information embedded in projected images allows users to easily design an integrated environment for the physical robots and digital images to preserve the seamless connection between them.},
booktitle = {SIGGRAPH ASIA 2016 Emerging Technologies},
articleno = {2},
numpages = {2},
keywords = {robot swarm, pixel-level visible light communication, mixed reality, high-speed projector, digital micromirror device},
location = {Macau},
series = {SA '16}
}

@inproceedings{Inoue2006,
abstract = {A method for limb mechanism robots of omni-directional gait hanging from grid-like structure is proposed. Grid-like structure consists of many bars assembled in a matrix in a horizontal plane; its grid spacing is not always constant and unknown. A robot has six legs, and each foot has a hemispherical shape for hooking on the bar. The robot moves in any direction as commanded by tripod gait; it hangs from the grid-like structure using two sets of three legs alternately. The leg gropes for the bar so as to take as long stroke as possible. By increasing joint compliance, the foot contacts the bar softly and detects the contact. Then, using a foot force sensor, the robot ascertains that the foot hooks on the bar. The developed robot ASTERISK can perform omni-directional gait hanging from experimental gridlike structure by the proposed method. {\textcopyright} 2006 IEEE.},
author = {Inoue, Kenji and Tsurutani, Taisuke and Takubo, Tomohito and Arai, Tatsuo},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2006.282133},
file = {:Users/hanc/Downloads/papers/Omni-directional_Gait_of_Limb_Mechanism_Robot_Hanging_from_Grid-like_Structure.pdf:pdf},
isbn = {1-4244-0258-1},
keywords = {Force sensor,Grid-like structure,Joint compliance,Omni-directional gait,Six-legged robot},
month = {oct},
pages = {1732--1737},
publisher = {IEEE},
title = {{Omni-directional Gait of Limb Mechanism Robot Hanging from Grid-like Structure}},
url = {http://ieeexplore.ieee.org/document/4058626/},
year = {2006}
}

@article{Kilobot2014,
abstract = {In current robotics research there is a vast body of work on algorithms and control methods for groups of decentralized cooperating robots, called a swarm or collective. These algorithms are generally meant to control collectives of hundreds or even thousands of robots; however, for reasons of cost, time, or complexity, they are generally validated in simulation only, or on a group of a few tens of robots. To address this issue, this paper presents Kilobot, an open-source, low cost robot designed to make testing collective algorithms on hundreds or thousands of robots accessible to robotics researchers. To enable the possibility of large Kilobot collectives where the number of robots is an order of magnitude larger than the largest that exist today, each robot is made with only $14 worth of parts and takes 5 min to assemble. Furthermore, the robot design allows a single user to easily operate a large Kilobot collective, such as programming, powering on, and charging all robots, which would be difficult or impossible to do with many existing robotic systems. We demonstrate the capabilities of the Kilobot as a collective robot, by using a small robot test collective to implement four popular swarm behaviors: foraging, formation control, phototaxis, and synchronization. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Rubenstein, Michael and Ahler, Christian and Hoff, Nick and Cabrera, Adrian and Nagpal, Radhika},
doi = {10.1016/j.robot.2013.08.006},
file = {:Users/hanc/Downloads/papers/1-s2.0-S0921889013001474-main.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Modular robot,Multi-robot system,Robot collectives},
number = {7},
pages = {966--975},
publisher = {Elsevier B.V.},
title = {{Kilobot: A low cost robot with scalable operations designed for collective behaviors}},
url = {http://dx.doi.org/10.1016/j.robot.2013.08.006},
volume = {62},
year = {2014}
}

@article{Kim2008,
author = {{Sangbae Kim} and Spenko, Matt and Trujillo, Salomon and Heyneman, Barrett and Santos, Dan and Cutkosky, M.R.},
doi = {10.1109/TRO.2007.909786},
file = {:Users/hanc/Downloads/papers/Smooth_Vertical_Surface_Climbing_With_Directional_Adhesion.pdf:pdf},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {feb},
number = {1},
pages = {65--74},
title = {{Smooth Vertical Surface Climbing With Directional Adhesion}},
url = {http://www.crossref.org/deleted_DOI.html},
volume = {24},
year = {2008}
}

@inproceedings{Liang2020,
abstract = {This paper proposes a novel modular selfreconfigurable robot (MSRR) "FreeBOT", which can be connected freely at any point on other robots. FreeBOT is mainly composed of two parts: a spherical ferromagnetic shell and an internal magnet. The connection between the modules is genderless and instant, since the internal magnet can freely attract other FreeBOT spherical ferromagnetic shells, and not need to be precisely aligned with the specified connector. This connection method has fewer physical constraints, so the FreeBOT system can be extended to more configurations to meet more functional requirements. FreeBOT can accomplish multiple tasks although it only has two motors: module independent movement, connector management and system reconfiguration. FreeBOT can move independently on the plane, and even climb on ferromagnetic walls; a group of FreeBOTs can traverse complex terrain. Numerous experiments have been conducted to test its function, which shows that the FreeBOT system has great potential to realize a freeform robotic system.},
author = {Liang, Guanqi and Luo, Haobo and Li, Ming and Qian, Huihuan and Lam, Tin Lun},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS45743.2020.9341129},
file = {:Users/hanc/Downloads/papers/FreeBOT_A_Freeform_Modular_Self-reconfigurable_Robot_with_Arbitrary_Connection_Point_-_Design_and_Implementation (1).pdf:pdf},
isbn = {9781728162126},
issn = {21530866},
publisher = {IEEE},
pages = {6506--6513},
title = {{FreeBOT: A freeform modular self-reconfigurable robot with arbitrary connection point - Design and implementation}},
year = {2020}
}

@article{Matsumura2019,
author = {Matsumura, Yodai and Kawamoto, Koyo and Takada, Yogo},
doi = {10.7210/jrsj.37.514},
file = {:Users/hanc/Downloads/papers/37_37_514.pdf:pdf},
issn = {0289-1824},
journal = {Journal of the Robotics Society of Japan},
keywords = {bridge inspection,four-wheel driving robot,magnet,rimless wheel,wall-climbing robot},
number = {6},
pages = {514--522},
title = {{Development of a Compact Wall-Climbing Robot Capable of Transitioning among Floor, Vertical Wall and Ceiling}},
volume = {37},
year = {2019}
}

@article{Panich2010,
author = {Panich, Surachai},
doi = {10.3844/jcssp.2010.1185.1188},
file = {:Users/hanc/Downloads/papers/jcssp.2010.1185.1188.pdf:pdf},
issn = {1549-3636},
journal = {Journal of Computer Science},
keywords = {force analysis,pneumatic system,wall climbing robot},
month = {oct},
number = {10},
pages = {1185--1188},
title = {{Development of a Wall Climbing Robot}},
url = {http://www.thescipub.com/abstract/10.3844/jcssp.2010.1185.1188},
volume = {6},
year = {2010}
}

@inproceedings{Push-That-There2024,
author = {Wang, Keru and Wang, Zhu and Nakagaki, Ken and Perlin, Ken},
title = {``Push-That-There''': Tabletop Multi-robot Object Manipulation via Multimodal 'Object-level Instruction'},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661542},
doi = {10.1145/3643834.3661542},
abstract = {We present "Push-That-There", an interaction method and system enabling multimodel object-level user interaction with multi-robot system to autonomously and collectively manipulate objects on tabletop surfaces, inspired by "Put-That-There". Rather than requiring users to instruct individual robots, users directly specify how they want the objects to be moved, and the system responds by autonomously moving objects via our generalizable multi-robot control algorithm. The system is combined with various user instruction modalities, including gestures, GUI, tangible manipulation, and speech, allowing users to intuitively create object-level instruction. We outline a design space, highlight interaction design opportunities facilitated by "Push-That-There", and provide an evaluation to assess our system's technical capabilities. While other recent HCI research has studied interaction using multi-robot system (e.g. Swarm UIs), our contribution is in the design and technical implementation of intuitive object-level interaction for multi-robot system that allows users to work at a high level, rather than needing to focus on the movements of individual robots.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {2497–2513},
numpages = {17},
keywords = {Human-Robot Interaction, Multi-Robot Control, Multi-robot UI, Object-level instruction, Tangible Interface},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@article{R-Track2021,
abstract = {This letter presents the development of a reconfigurable wall-climbing robot (WCR) called R-track. R-Track is designed for operations inside metal structures. It adheres to the metal surface with magnetic tracks. With a modular design which each module of R-Track can be connected or disconnected without an additional actuator, R-Track can perform various wall-to-wall transitions. In particular, external wall transitions that have been difficult for previous WCRs can be achieved by R-track with a cooperation between modules. The statics of R-Track during wall transitions was analyzed to identify and verify an appropriate reconfiguration strategy. Experiments on wall-to-wall transitions were conducted to demonstrate the performance of R-Track. The results indicate that R-Track successfully performed all kinds of perpendicular wall-to-wall transitions.},
author = {Park, Changmin and Bae, Jangho and Ryu, Sijun and Lee, Jiseok and Seo, Taewon},
doi = {10.1109/LRA.2020.3015170},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Cellular and modular robots,cooperating robots,mechanism design},
number = {2},
pages = {1036--1042},
title = {{R-Track: Separable Modular Climbing Robot Design for Wall-to-Wall Transition}},
volume = {6},
year = {2021}
}

@inproceedings{Reactile2018,
abstract = {We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of highlevel interface design. Inspired by current UI programming practices, we introduce a four-step workflow-create elements, abstract attributes, specify behaviors, and propagate changes- for Swarm UI programming. We propose a set of direct physical manipulation techniques to support each step in this workflow. To demonstrate these concepts, we developed Reactile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies-an in-class survey with 148 students and a lab interview with eight participants-confirm that our approach is intuitive and understandable for programming Swarm UIs.},
address = {New York, NY, USA},
author = {Suzuki, Ryo and Kato, Jun and Gross, Mark D. and Yeh, Tom},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3173574.3173773},
file = {:Users/hanc/Downloads/papers/3173574.3173773.pdf:pdf},
isbn = {9781450356206},
keywords = {Direct manipulation,Programming by demonstration,Swarm user interfaces,Tangible programming},
month = {apr},
pages = {1--13},
publisher = {ACM},
title = {{Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation}},
url = {https://dl.acm.org/doi/10.1145/3173574.3173773},
volume = {2018-April},
year = {2018}
}

@inproceedings{RoomShift2020,
abstract = {RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2008.08695},
author = {Suzuki, Ryo and Hedayati, Hooman and Zheng, Clement and Bohn, James L. and Szafir, Daniel and Do, Ellen Yi-Luen and Gross, Mark D. and Leithinger, Daniel},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3313831.3376523},
eprint = {2008.08695},
file = {:Users/hanc/Downloads/papers/chi-2020-roomshift.pdf:pdf},
isbn = {9781450367080},
keywords = {haptic interfaces,room-scale haptics,swarm robots,virtual reality},
month = {apr},
pages = {1--11},
publisher = {ACM},
title = {{RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots}},
url = {https://dl.acm.org/doi/10.1145/3313831.3376523},
year = {2020}
}

@inproceedings{Rovables2016,
abstract = {We introduce Rovables, a miniature r on unmodified clothing. The robots netic wheels, and can climb vertic tethered and have an onboard batte wireless communications. They also calization system that uses wheel enc Rovables to perform limited autono body. In the technical evaluations, can operate continuously for 45 min 1.5N. We propose an interaction sp devices spanning sensing, actuation velop application scenarios in that include on-body sensing, modular d and interactive clothing and jewelry.},
address = {New York, NY, USA},
author = {Dementyev, Artem and Kao, Hsin-Liu (Cindy) and Choi, Inrak and Ajilo, Deborah and Xu, Maggie and Paradiso, Joseph A. and Schmandt, Chris and Follmer, Sean},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
doi = {10.1145/2984511.2984531},
file = {:Users/hanc/Downloads/papers/Rovables-Miniature-On-Body-Robots-as-Mobile-Wearables.pdf:pdf},
isbn = {9781450341899},
keywords = {Mobile wearable technology,On-body robotics},
month = {oct},
pages = {111--120},
publisher = {ACM},
title = {{Rovables: Miniature On-Body Robots as Mobile Wearables}},
url = {https://dl.acm.org/doi/10.1145/2984511.2984531},
year = {2016}
}

@inproceedings{ShapeBots2019,
abstract = {We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1909.03372},
author = {Suzuki, Ryo and Zheng, Clement and Kakehi, Yasuaki and Yeh, Tom and Do, Ellen Yi-Luen and Gross, Mark D. and Leithinger, Daniel},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3332165.3347911},
eprint = {1909.03372},
file = {:Users/hanc/Library/Application Support/Mendeley Desktop/Downloaded/Suzuki et al. - 2019 - ShapeBots Shape-changing Swarm Robots.pdf:pdf},
isbn = {9781450368162},
month = {oct},
pages = {493--505},
publisher = {ACM},
title = {{ShapeBots: Shape-changing Swarm Robots}},
url = {http://arxiv.org/abs/1909.03372 http://dx.doi.org/10.1145/3332165.3347911 http://dl.acm.org/citation.cfm?doid=3332165.3347911 https://dl.acm.org/doi/10.1145/3332165.3347911},
year = {2019}
}

@inproceedings{Suzuki2022,
abstract = {This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {2203.03254v1},
author = {Suzuki, Ryo and Karim, Adnan and Xia, Tian and Hedayati, Hooman and Marquardt, Nicolai},
booktitle = {CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3491102.3517719},
eprint = {2203.03254v1},
file = {:Users/hanc/Downloads/papers/3491102.3517719.pdf:pdf},
isbn = {9781450391573},
keywords = {actuated tangible interfaces,augmented reality,human-robot interaction,mixed reality,robotics,shape-changing interfaces,survey},
month = {apr},
pages = {1--33},
publisher = {ACM},
title = {{Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces}},
url = {http://dx.doi.org/10.1145/3491102.3517719},
year = {2022}
}

@inproceedings{SwarmBody,
author = {Ichihashi, Sosuke and Kuroki, So and Nishimura, Mai and Kasaura, Kazumi and Hiraki, Takefumi and Tanaka, Kazutoshi and Yoshida, Shigeo},
title = {Swarm Body: Embodied Swarm Robots},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642870},
doi = {10.1145/3613904.3642870},
abstract = {The human brain’s plasticity allows for the integration of artificial body parts into the human body. Leveraging this, embodied systems realize intuitive interactions with the environment. We introduce a novel concept: embodied swarm robots. Swarm robots constitute a collective of robots working in harmony to achieve a common objective, in our case, serving as functional body parts. Embodied swarm robots can dynamically alter their shape, density, and the correspondences between body parts and individual robots. We contribute an investigation of the influence on embodiment of swarm robot-specific factors derived from these characteristics, focusing on a hand. Our paper is the first to examine these factors through virtual reality (VR) and real-world robot studies to provide essential design considerations and applications of embodied swarm robots. Through quantitative and qualitative analysis, we identified a system configuration to achieve the embodiment of swarm robots.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {267},
numpages = {19},
keywords = {embodiment, swarm robotics, tangible interaction},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{SwarmHaptics2019,
abstract = {This paper seeks to better understand the use of haptic feedback in abstract, ubiquitous robotic interfaces. We introduce and provide preliminary evaluations of SwarmHaptics, a new type of haptic display using a swarm of small, wheeled robots. These robots move on a fat surface and apply haptic patterns to the user's hand, arm, or any other accessible body parts. We explore the design space of SwarmHaptics including individual and collective robot parameters, and demonstrate example scenarios including remote social touch using the Zooids platform. To gain insights into human perception, we applied haptic patterns with varying number of robots, force type, frequency, and amplitude and obtained user's perception in terms of emotion, urgency, and Human-Robot Interaction metrics. In a separate elicitation study, users generated a set of haptic patterns for social touch. The results from the two studies help inform how users perceive and generate haptic patterns with SwarmHaptics.},
address = {New York, NY, USA},
author = {Kim, Lawrence H. and Follmer, Sean},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3290605.3300918},
file = {:Users/hanc/Downloads/papers/SwarmHaptics.pdf:pdf},
isbn = {9781450359702},
keywords = {Haptics,Human-robot interaction,Swarm haptics,Swarm user interface,Ubiquitous robotic interfaces},
month = {may},
pages = {1--13},
publisher = {ACM},
title = {{SwarmHaptics: Haptic Display with Swarm Robots}},
url = {https://dl.acm.org/doi/10.1145/3290605.3300918},
year = {2019}
}

@article{Takada2017,
abstract = {Several infrastructures, such as bridges and tunnels, require periodic inspection and repair to prevent collapse. There is a strong demand for practical bridge inspection robots to reduce the cost and time associated with the inspection of bridges by an inspector. Bridge inspection robots are expected to pass through obstacles such as bolted splice part and right-angled routes. The aim of this study involved developing a bridge inspection robot that can travel on a right-angle path as well as splicing parts. A two-wheel-drive robot was developed and equipped with two rimless wheels as driving wheels. A neodymium magnet was provided at the tip of each spoke. Non-driving wheels were attached at the rear as a rotatable caster. The robot can turn on the spot to avoid the bolt on the splicing part. Experiments were conducted to check the performance of the robot. The results confirmed that the robot passed through the internal right-angle paths in a laboratory and in an actual environment that corresponds to a box girder of a bridge. It is extremely difficult to manually control a robot on the splicing part. Therefore, a camera and an LED (light emitting diode) were attached to autonomously control the robot. The results indicate that the newly developed robot could run through the splicing part without hitting the nuts.},
author = {Takada, Yogo and Ito, Satoshi and Imajo, Naoto},
doi = {10.3390/inventions2030022},
file = {:Users/hanc/Downloads/papers/inventions-02-00022-v2.pdf:pdf},
issn = {2411-5134},
journal = {Inventions},
keywords = {Bolt,Bridge inspection,Magnet,Moving robot,Right-angle path,Rimless wheel,Splicing part,Steel bridge,Two-wheel-driven robot},
month = {aug},
number = {3},
pages = {22},
title = {{Development of a Bridge Inspection Robot Capable of Traveling on Splicing Parts}},
url = {http://www.mdpi.com/2411-5134/2/3/22},
volume = {2},
year = {2017}
}

@inproceedings{ThreadingSpace2024,
author = {Bhattacharya, Ramarko and Li, You and Faracci, Emilie and Dong, Harrison and Zheng, Yi and Nakagaki, Ken},
title = {Threading Space: Kinetic Sculpture Exploring Spatial Interaction Using Threads In Motion},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3660498},
doi = {10.1145/3635636.3660498},
abstract = {Threading Space is a kinetic sculpture that explores how spatial perception can be transformed by dynamically and geometrically reconfiguring physical lines of thread. As the threads in motion interact, they become a hypnotic medium for three-dimensional patterns. Through a physical installation and an interactive GUI, Threading Space invites the audience to explore the potential of using swarm robots and line elements to create, morph, and interact with space.},
booktitle = {Proceedings of the 16th Conference on Creativity \& Cognition},
pages = {571–575},
numpages = {5},
keywords = {Actuated Experience, Kinetic Sculpture, Swarm Robots, Thread-based Installation},
location = {Chicago, IL, USA},
series = {C\&C '24}
}

@inproceedings{ThrowIO_CHI2023,
abstract = {We introduce ThrowIO, a novel style of actuated tangible user interface that facilitates throwing and catching spatial interaction powered by mobile wheeled robots on overhanging surfaces. In our approach, users throw and stick objects that are embedded with magnets to an overhanging ferromagnetic surface where wheeled robots can move and drop them at desired locations, allowing users to catch them. The thrown objects are tracked with an RGBD camera system to perform closed-loop robotic manipulations. By computationally facilitating throwing and catching interaction, our approach can be applied in many applications including kinesthetic learning, gaming, immersive haptic experience, ceiling storage, and communication. We demonstrate the applications with a proof-of-concept system enabled by wheeled robots, ceiling hardware design, and software control. Overall, ThrowIO opens up novel spatial, dynamic, and tangible interaction for users via overhanging robots, which has great potential to be integrated into our everyday space.},
address = {New York, NY, USA},
author = {Lin, Ting-Han and Yang, Willa Yunqi and Nakagaki, Ken},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3544548.3581267},
isbn = {9781450394215},
keywords = {Actuated Tangible User Interface,Human-Computer Interaction,Human-Robot Interaction,Spatial Interaction,Swarm User Interface},
month = {apr},
pages = {1--17},
publisher = {ACM},
title = {{ThrowIO: Actuated TUIs that Facilitate “Throwing and Catching” Spatial Interaction with Overhanging Mobile Wheeled Robots}},
url = {https://dl.acm.org/doi/10.1145/3544548.3581267},
year = {2023}
}

@article{Tripillar2011,
abstract = {Summary We present a miniature magnetic climbing robot with dimensions 96 × 46 × 64 mm3. With two degrees of freedom it is able to climb ferromagnetic surfaces and to make inner plane to plane transitions whatever their inclination is. This robot, named TRIPILLAR, combines triangular-shaped magnetic caterpillars and frame magnets. This particular configuration allows, for example, to move from ground to wall and ceiling and back. This achievement opens new avenues to use mobile robotics for industrial inspection with stringent size restrictions, such as the ones encountered in power plants. {\textcopyright} Cambridge University Press 2011.},
author = {Schoeneich, Patrick and Rochat, Frederic and Nguyen, Olivier Truong Dat and Moser, Roland and Mondada, Francesco},
doi = {10.1017/S0263574711000257},
issn = {02635747},
journal = {Robotica},
keywords = {Design,Mechatronic Systems,Mobile Robots,Service Robots,novel Applications of Robotics},
number = {7},
pages = {1075--1081},
title = {{TRIPILLAR: A miniature magnetic caterpillar climbing robot with plane transition ability}},
volume = {29},
year = {2011}
}

@article{UbiSwarm2017,
abstract = {As robots increasingly enter our everyday life, we envision a future in which robots are ubiquitous and interact with both ourselves and our environments. This paper introduces the concept of ubiquitous robotic interfaces (URIs), multi-robot interfaces capable of mobility, manipulation, sensing, display and interaction. URIs interact directly with the user and indirectly through surrounding objects. A key aspect of URIs is their ability to display information to users either by collectively forming shapes or through their movements. In this paper, we focus on the use of URIs to display information in ubiquitous settings. We first investigate the use of abstract motion as a display for URIs by studying human perception of abstract multi-robot motion. With ten small robots, we produced 42 videos of bio-inspired abstract motion by varying three parameters (7 x 2 x 3): bio-inspired behavior, speed and smoothness. In a crowdsourced between-subjects study, 1067 subjects were recruited to watch the videos and describe their perception through Likert scales and free text. Study results suggest that different bio-inspired behaviors elicit significantly different responses in arousal, dominance, hedonic and pragmatic qualities, animacy, urgency and willingness to attend. On the other hand, speed significantly affects valence, arousal, hedonic quality, urgency and animacy while smoothness affects hedonic quality, animacy, attractivity and likeability. We discuss how these results inform URI designers to formulate appropriate motion for different interaction scenarios and use these results to derive our own example applications using our URI platform, UbiSwarm.},
author = {Kim, Lawrence H and Follmer, Sean},
doi = {10.1145/3130931},
file = {:Users/hanc/Downloads/papers/3130931.pdf:pdf},
issn = {2474-9567},
journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
keywords = {Human Perception,Human-Swarm Interaction,Swarm Robotics,Ubiquitous Robotics},
month = {sep},
number = {3},
pages = {1--20},
title = {{UbiSwarm: Ubiquitous Robotic Interfaces and Investigation of Abstract Motion as a Display}},
url = {https://dl.acm.org/doi/10.1145/3130931},
volume = {1},
year = {2017}
}

@misc{Vertigo2015,
author = {Beardsley, Paul and Siegwart, Roland and Arigoni, Michael and Bischoff, Michael and Fuhrer, SIlvan and Krummenacher, David and Mammolo, Dario and Simpson, Robert},
title = {{VertiGo - a Wall-Climbing Robot including Ground-Wall Transition}},
url = {https://la.disneyresearch.com/publication/vertigo/},
urldate = {2023-12-08},
year = {2015}
}

@article{Waalbot2007,
abstract = {This paper proposes a small-scale agile wall-climbing robot, which is able to climb on smooth vertical surfaces using flat adhesive elastomer materials for attachment. Using two actuated legs with rotary motion and two passive revolute joints at each foot, this robot can climb and steer in any orientation. Due to its compact design, a high degree of miniaturization is possible. It has onboard power, computing, and wireless communication, which allow for semiautonomous operation. Various aspects of a functioning prototype design and performance are discussed in detail, including leg and foot design and gait dynamics. A model for the adhesion requirements and performance is developed and verified through experiments. Using an adhesive elastomer (Vytaflex 10), the current prototype can climb 90° slopes at a speed of up to 6 cm/ s and steer to any angle reliably on a smooth acrylic surface as well as transition from floor walking to wall climbing. This robot is intended for inspection and surveillance applications, and ultimately, for space missions. {\textcopyright} 2007 IEEE.},
author = {Murphy, Michael P. and Sitti, Metin},
doi = {10.1109/TMECH.2007.897277},
file = {:Users/hanc/Downloads/papers/Waalbot_An_Agile_Small-Scale_Wall-Climbing_Robot_Utilizing_Dry_Elastomer_Adhesives.pdf:pdf},
issn = {1083-4435},
journal = {IEEE/ASME Transactions on Mechatronics},
keywords = {Dry adhesives,Mechatronics,Miniature robotics,Mobile robotics,Wall climbing},
month = {jun},
number = {3},
pages = {330--338},
title = {{Waalbot: An Agile Small-Scale Wall-Climbing Robot Utilizing Dry Elastomer Adhesives}},
url = {http://ieeexplore.ieee.org/document/4244394/},
volume = {12},
year = {2007}
}

@inproceedings{Yan2021,
abstract = {This paper presents our vision of on-the-wall tangible interaction. We envision a future where tangible interaction can be extended from conventional horizontal surfaces to vertical surfaces; indoor vertical areas such as walls, windows, and ceilings can be used for dynamic and direct physical manipulation. We first discuss the unique properties that vertical surfaces may offer for tangible interaction and the interaction scenarios they imbue. We then propose two potential paths for realizing on-the-wall interaction and the technical challenges we face. We follow with one prototype called Climbot. We showcase how Climbot can be used as an on-the-wall tangible user interface for dynamic lighting and as a wall switch controller. We conclude with a discussion of future work.},
address = {New York, NY, USA},
author = {Yan, Zeyu and Sathya, Anup and Carvalho, Pedro and Hu, Yongquan and Li, Annan and Peng, Huaishu},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3411763.3451586},
file = {:Users/hanc/Downloads/papers/3411763.3451586.pdf:pdf},
isbn = {9781450380959},
keywords = {Climbing Robot,Robotics,Tangible Interaction,Tangible User Interface,Wall,Wall Interaction},
month = {may},
pages = {1--6},
publisher = {ACM},
title = {{Towards On-the-wall Tangible Interaction: Using Walls as Interactive, Dynamic, and Responsive User Interface}},
url = {https://dl.acm.org/doi/10.1145/3411763.3451586},
year = {2021}
}

@inproceedings{Zooids2016,
address = {New York, NY, USA},
author = {{Le Goc}, Mathieu and Kim, Lawrence H. and Parsaei, Ali and Fekete, Jean-Daniel and Dragicevic, Pierre and Follmer, Sean},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
doi = {10.1145/2984511.2984547},
file = {:Users/hanc/Library/Application Support/Mendeley Desktop/Downloaded/Le Goc et al. - 2016 - Zooids Building Blocks for Swarm User Interfaces.pdf:pdf},
isbn = {9781450341899},
month = {oct},
pages = {97--109},
publisher = {ACM},
title = {{Zooids: Building Blocks for Swarm User Interfaces}},
url = {http://dl.acm.org/citation.cfm?doid=2984511.2984547 https://dl.acm.org/doi/10.1145/2984511.2984547},
year = {2016}
}

@inproceedings{disappearables2022,
address = {New York, NY, USA},
author = {Nakagaki, Ken and Tappa, Jordan L and Zheng, Yi and Forman, Jack and Leong, Joanne and Koenig, Sven and Ishii, Hiroshi},
booktitle = {CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3491102.3501906},
file = {:Users/hanc/Downloads/papers/3491102.3501906.pdf:pdf},
isbn = {9781450391573},
keywords = {Actuated Tangible User Interfaces,Swarm User Inte,acm reference format,actuated tangible user interfaces,dynamic,physical afordance,stage,swarm user interface},
month = {apr},
pages = {1--13},
publisher = {ACM},
title = {{(Dis)Appearables: A Concept and Method for Actuated Tangible UIs to Appear and Disappear based on Stages}},
url = {https://dl.acm.org/doi/10.1145/3491102.3501906},
year = {2022}
}

@misc{iRobot2023,
author = {iRobot},
howpublished = {\url{https://edu.irobot.com/}, accessed: Feb-11-2025.},
title = {{Root{\texttrademark} Coding Robots}},
year = {2023}
}

@inproceedings{suctioncup2010,
  author={Yoshida, Yu and Ma, Shugen},
  booktitle={2010 IEEE International Conference on Robotics and Biomimetics}, 
  title={Design of a wall-climbing robot with passive suction cups}, 
  year={2010},
  volume={},
  number={},
publisher = {IEEE},
  pages={1513-1518},
  doi={10.1109/ROBIO.2010.5723554}
}

