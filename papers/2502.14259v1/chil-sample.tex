%\documentclass[gray]{jmlr} % test grayscale version
%\documentclass[tablecaption=bottom]{jmlr}% journal article
\documentclass[pmlr,twocolumn,10pt]{jmlr} % W&CP article

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

%\usepackage{rotating}% for sideways figures and tables
%\usepackage{longtable}% for long tables

% The booktabs package is used by this sample document
% (it provides \toprule, \midrule and \bottomrule).
% Remove the next line if you don't require it.

\usepackage{booktabs}
% The siunitx package is used by this sample document
% to align numbers in a column by their decimal point.
% Remove the next line if you don't require it.
%\usepackage[load-configurations=version-1]{siunitx} % newer version 
\usepackage{siunitx}

% The lineno package is required for denoting line
% numbers for paper review.
\usepackage[switch]{lineno}

\usepackage{bm}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{lipsum}

% The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}% remove this in your real article

% The following is to recognise equal contribution for authorship
\newcommand{\equal}[1]{{\hypersetup{linkcolor=black}\thanks{#1}}}

\newcommand{\ours}{LabTOP}
\newcommand{\oursfull}{Lab Test Outcome Predictor}

\newcommand{\todo}[1]{\textcolor{red}{TODO:~#1}}

% Define an unnumbered theorem just for this sample document for
% illustrative purposes:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

% change the arguments, as appropriate, in the following:
% \jmlrvolume{LEAVE UNSET}
% \jmlryear{2025}
% \jmlrsubmitted{LEAVE UNSET}
% \jmlrpublished{LEAVE UNSET}
\jmlrworkshop{Submitted to Conference on Health, Inference, and Learning (CHIL) 2025} % W&CP title

% The optional argument of \title is used in the header
% \title[Short Title]{Full Title of Article\titlebreak This Title Has A Line Break}
\title[\ours{}]{LabTOP: A Unified Model for Lab Test Outcome Prediction\titlebreak
on Electronic Health Records}
% title에서 잘 드러나야 할 것들
% EHR 사용한다는 거
% language modeling (generative) 방식으로 접근했다는 거

% Anything in the title that should appear in the main title but 
% not in the article's header or the volume's table of
% contents should be placed inside \titletag{}

%\title{Title of the Article\titletag{\thanks{Some footnote}}}


% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% \thanks must come after \Name{...} not inside the argument for
% example \Name{John Smith}\nametag{\thanks{A note}} NOT \Name{John
% Smith\thanks{A note}}

% Anything in the name that should appear in the title but not in the 
% article's header or footer or in the volume's
% table of contents should be placed inside \nametag{}

% Two authors with the same address
% \author{%
%  \Name{Author Name1\nametag{\thanks{A note}}} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address
% }

% Three or more authors with the same address:
\author{%
 \Name{Sujeong Im}\equal{These authors contributed equally} \Email{sujeongim@kaist.ac.kr}\\
 \Name{Jungwoo Oh}\footnotemark[1] \Email{ojw0123@kaist.ac.kr}\\
 \Name{Edward Choi} \Email{edwardchoi@kaist.ac.kr}\\
 % \Name{Author Name4} \Email{an4@sample.com}\\
 % \Name{Author Name5} \Email{an5@sample.com}\\
 % \Name{Author Name6} \Email{an6@sample.com}\\
 % \Name{Author Name7} \Email{an7@sample.com}\\
 % \Name{Author Name8} \Email{an8@sample.com}\\
 % \Name{Author Name9} \Email{an9@sample.com}\\
 % \Name{Author Name10} \Email{an10@sample.com}\\
 % \Name{Author Name11} \Email{an11@sample.com}\\
 % \Name{Author Name12} \Email{an12@sample.com}\\
 % \Name{Author Name13} \Email{an13@sample.com}\\
 % \Name{Author Name14} \Email{an14@sample.com}\\
 \addr KAIST, Republic of Korea
}

% Authors with different addresses and equal first authors:
% \author{%
% \Name{Anonymous Author(s)}
% \equal{These authors contributed equally}
% \Email{Anonymous@sample.com}\\
% \addr University X, Country 1
% \AND
% % footnotemark[1] is to refer to the \equal footnote
% \Name{Anonymous First Author 2}\footnotemark[1] \Email{def@sample.com}\\
% \addr University Y, Country 2
% \AND
% \Name{Anonymous Last Author} \Email{ghi@sample.com}\\
% \addr University Z, Country 3
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Remove the \linenumbers in the final version %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \linenumbers % Activate line numbering

\begin{document}

\maketitle

\begin{abstract}
Lab tests are fundamental for diagnosing diseases and monitoring patient conditions.
However, frequent testing can be burdensome for patients, and test results may not always be immediately available.
To address these challenges, we propose \textbf{\oursfull{} (\ours)}, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data.
Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items.
We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models.
We also conduct extensive ablation studies to confirm the effectiveness of our design choices.
We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.

\end{abstract}
\paragraph*{Data and Code Availability}
This paper uses the three EHR datasets, MIMIC-IV~\citep{johnson2023mimic}, eICU~\citep{pollard2018eicu}, and HiRID~\citep{hyland2020early}, which are publicly available on the PhysioNet repository~\citep{johnson2024physionet, pollard2019physionet, yeche2021physionet}.
More details about datasets can be found at Section~\ref{sec:data}.
Our implementation code can be accessed at this repository.\footnote{\url{https://anonymous.4open.science/r/LabTOP-DE7B}}

\paragraph*{Institutional Review Board (IRB)}
This research does not require IRB approval.

\section{Introduction}
\label{sec:intro}

Electronic Health Records (EHR) are essential to modern healthcare systems, serving as comprehensive databases of patient data, including treatments, clinical interventions, and lab test results~\citep{gunter2005emergence}.
These records provide a longitudinal view of a patient's medical history, allowing for the tracking of individual health trends~\citep{kruse2017impact}.
Among them, lab test results play a particularly important role by capturing numerical changes in key biomarkers, such as blood glucose and creatinine.
These results reflect a patient’s physiological and pathological state, supporting the management of disease and the assessment of treatment efficacy~\citep{sikaris2017enhancing, cabalar2024role}.

Despite their clinical importance, conducting lab tests often faces challenges in real-world settings.
For instance, the need for frequent lab tests in unstable patients conflicts with the increased distress they experience from repeated invasive procedures, creating a trade-off between clinical necessity and patient burden~\citep{ambasta2019expert,zhi2024hgbnet}.
Additionally, some tests often take significant time to obtain results, making it difficult to assess the patient's condition promptly~\citep{shiferaw2019magnitude}.
As a result, there are limitations in comprehensively assessing a patient's condition from multiple perspectives through various lab tests, forcing healthcare providers to rely solely on the available test results to make clinical judgments.
This challenge highlights the need for methods to estimate lab test results without conducting the actual tests, allowing healthcare providers to understand a patient's condition comprehensively through the predicted lab test results.

Meanwhile, the advancement of machine learning has greatly contributed to healthcare for several clinical prediction tasks such as readmission, mortality risk, and length of hospital stay~\citep{song2018attend, xiao2018readmission,hur2022unifying,hur2023genhpf,kim2023general,wornow2023ehrshot, renc2024zero}.
Although these works have demonstrated the value of lab test data in supporting clinical decisions, there has been no attempt to directly estimate various lab test results within a single unified model.
Instead, most prior studies approach lab test outcome prediction by classifying discrete levels of a small subset of selected lab items~\citep{hur2023genhpf,kim2023general,wornow2023ehrshot}.
Even among studies that attempt to estimate continuous values, they are specifically designed for only a few selected lab tests rather than whole set of lab items~\citep{zhi2024hgbnet, jiang2024probabilistic, duan2020ngboost, liu2023machine, fu2023implementation, langarica2024deep}.
This narrow focus limits their practical applicability in clinical settings, where precise numerical predictions for a broader range of lab test results can facilitate early intervention and support more informed clinical decision-making across diverse medical conditions.

To address these limitations, we propose \textbf{\oursfull{} (\ours{})}, a novel method for predicting a diverse range of lab test outcomes based on a patient's medical history within a single unified model.
Inspired by autoregressive models widely used in Natural Language Processing~\citep{radford2018improving, radford2019language}, we leverage language modeling paradigm to predict numeric values in an autoregressive manner, instead of regressing each lab test value separately.
This approach enables us to build a unified model capable of predicting outcomes for multiple lab items within a single framework, without requiring us to train a separate model for each lab item.
Furthermore, unlike prior approaches, \ours{} is designed to estimate continuous lab test measurements across a wide spectrum of biomarkers, offering greater granularity and broader applicability in clinical practice.
The main contributions of this work can be summarized as follows:
\begin{itemize}
    \item We propose \ours{}, a new model that estimates continuous values for a wide range of lab test measurements given the patient's previous medical history.
    To the best of our knowledge, this is the first comprehensive approach to predict various lab test results within a single unified model trained on EHR data.

    \item \ours{} outperforms existing approaches, including a recent large language model (LLM), across three publicly available datasets.
    This demonstrates the generalizability of our method to diverse patient populations and clinical contexts.

    \item Considering the time-series characteristics of EHR data where each patient can be represented as a sequence of medical events, we conduct ablation studies to explore the best way to represent the time and numeric features of medical events for the model.
    We believe these experiments will provide valuable insights into effectively modeling EHR data.

\end{itemize}

\section{Related Works}

\paragraph{Predicting clinical outcomes using EHR data}
Previous machine learning models leveraging EHR data have primarily focused on predicting clinical outcomes, such as readmission, mortality risk, and length of stay~\citep{song2018attend, xiao2018readmission,hur2022unifying,hur2023genhpf,kim2023general,wornow2023ehrshot, renc2024zero}.
Among them, \citet{song2018attend} introduced the \textit{SAnD}, employing Transformer architecture~\citep{vaswani2017attention} to predict in-hospital mortality and length of hospital stay.
They constructed the input embeddings based on structured clinical codes (e.g., ICD, LOINC, RxNorm) to process EHR data, which makes it difficult to process multiple EHRs with different schemas within a single model.
To address these challenges, \citet{hur2022unifying} proposed a text-based embedding approach (DescEmb) to represent EHR events using descriptive text rather than codes, showing superior performances compared to the code-based approach.
Based on this text-based approach, GenHPF~\citep{hur2023genhpf} extended it by incorporating multi-task learning to handle various predictive tasks simultaneously.
Furthermore, REMed~\citep{kim2023general} integrated an event-retriever module into this framework to selectively extract relevant events with target tasks, ensuring that only the important events contribute to the final predictions.

\paragraph{Generative models for EHR data}
In addition to the studies employing discriminative models for predictive tasks, generative approaches have also been explored for modeling EHR data.
Specifically, MedGPT~\citep{kraljevic2021medgpt} adopted the GPT-2 architecture~\citep{radford2019language} to predict next SNOMED-CT disorder concept given the patient's previous history of disorders.
Although this work utilized only the disorder concepts instead of a comprehensive set of clinical events in EHR, they demonstrated the potential of generative methods for simulating patient trajectories and predicting future clinical events.
In addition, ETHOS~\citep{renc2024zero} employed a generative approach based on language modeling to simulate clinical scenarios using a structured EHR data.
Similar to the code-based approach, they converted each medical event into 1 to 7 tokens to construct the input sequence for the Transformer~\citep{vaswani2017attention}, and trained the model to predict every next token given the prior sequences to simulate the clinical outcomes of the patient, such as whether the patient will die or not (\textit{i.e.,} mortality prediction).
% In this way, the model simulates the clinical outcomes of the patient, such as whether the patient will die or not (\textit{i.e.,} mortality prediction), and whether the patient will readmit or not (\textit{i.e.,} readmission prediction).

\paragraph{Lab test outcome prediction}
Although previous studies have significantly advanced the field of machine learning for healthcare, none of them have addressed the direct estimation of continuous values for various lab tests within a single unified model.
Instead, most of previous studies have explored the prediction of lab test results by designing a specific model for only a few selected lab items or classifying discrete levels of selected lab items, such as normal or abnormal ranges.
For example, HgbNet~\citep{zhi2024hgbnet} employed an attention-based LSTM model to estimate hemoglobin levels, while \citet{jiang2024probabilistic} presented a probabilistic model using NGBoost~\citep{duan2020ngboost} to predict expected values of several lab items, such as wbc and hemoglobin.
Similarly, several works~\citep{liu2023machine,fu2023implementation,langarica2024deep} attempted to estimate glucose values using machine learning or deep learning approach.
However, these approaches are specifically designed for a small subset of lab tests, making it challenging to generalize across diverse clinical measurements.

Meanwhile, GenHPF~\citep{hur2023genhpf} and REMed~\citep{kim2023general} formulated this task as classifying the discrete level of values for several selected lab items.
Although these works have shown high performances on the defined task, their predictions remain limited in terms of extensibility and granularity.
First, they rely on predefined level thresholds for each lab item, determined through domain knowledge, which makes it difficult to extend the approach to encompass all lab items recorded in the EHR database.
Second, as the task is defined within a fixed prediction window (\textit{e.g.,} predicting average levels of lab items in the next 12 hours given the first 12 hours), these models lack granularity in providing immediate estimations of lab test results, which is critical for clinical scenarios where timely and dynamic predictions are necessary to inform urgent decisions.

\begin{figure*}[ht] % Use figure* for spanning both columns
\floatconts
    {fig:overview}
    {\vspace{-8mm}
    \caption{\textbf{Training and Inference of \ours{}.} During training, demographic information $D$ and a sequence of medical events $P=[\mathcal{M}_1,\dotsc,\mathcal{M}_N]$ are tokenized, and fed into the Transformer to generate next token at each position. The training loss is then computed only for lab test value tokens along with their units and the corresponding $\texttt{[EOE]}$ token. During inference, given the preceding sequence of medical events and the target lab event (\textit{i.e.,} $(t_k,e_k,d_k)$), the model autoregressively generates numeric value tokens for its outcome until the [EOE] token is encountered.}}
    {
        \includegraphics[width=1.0\textwidth]{figures/training_inference.pdf}
    }
    \vspace{-6mm}
\end{figure*}

\section{Methods}

\subsection{Preliminaries}
In EHR data, patient medical events that occur in a hospital, such as lab tests and prescriptions, are recorded in a structured format.
Thus, we can represent a patient $P$ as a sequence of medical events $[\mathcal{M}_1, \dotsc, \mathcal{M}_N]$, where $N$ is the total number of medical events, and $\mathcal{M}_i$ is the $i$-th medical event for the patient.  
Each medical event typically includes a timestamp, medical event codes (\textit{e.g.,} ICD, LOINC, RxNorm), a numerical value, and its corresponding unit of measurement.
Accordingly, the $i$-th medical event $\mathcal{M}_i$ can be represented as a 5-tuple of event features $(t_i, e_i, c_i, v_i, u_i)$, where $t_i$ represents the timestamp of the event, $e_i$ stands for the type of the event (\textit{e.g.,} ``labevent'', ``prescription''), $c_i$ is the corresponding medical code (\textit{e.g.,``2160-0"}), $v_i$ denotes the measured values (\textit{e.g.,} 1.2), and $u_i$ is the associated unit of measurement (\textit{e.g., mg/dL}).
The detailed definitions of these event features are described in the following section, as well as depicted in Figure~\ref{fig:overview}.

\subsection{Preprocessing}

\paragraph{Textual representation of medical events}
Unlike conventional methods that directly embed medical code $c_i$ into a fixed-dimensional vector space to represent a medical event $\mathcal{M}_i$~\citep{miotto2016deep, choi2016doctor, choi2020learning, song2018attend, shang2019pre, renc2024zero}, we start by converting each medical code $c_i$ (\textit{e.g.,``2160-0"}) into its corresponding textual description $d_i$ (\textit{e.g.,``Creatinine [Mass/volume] in Serum or Plasma"}).
Additionally, we also treat its numerical value $v_i$ as plain text and split it into each digit, so that each digit is processed as a separate token in the input sequence~\citep{hur2022unifying, hur2023genhpf, kim2023general}.
In this way, we reconstruct a 4-subtuple of $(e_i, c_i, v_i, u_i)$ included in the medical event $\mathcal{M}_i$ into a textual sequence, and denote as $m_i$. %denoted as $m_i$.
For example, a lab test event standing for a creatinine value of 1.23 mg/dL measured at the time of $t_i$ is transformed into $(t_i, m_i)$ where $m_i$ is a plain text of ``labevent creatinine 1~ .~ 2~ 3 mg/dL''. 

\paragraph{Absolute time encoding}
\label{sec:abs_time_encoding}
Temporal information also plays a crucial role in modeling the progression of a patient's health status accurately, as the timing and frequency of the events provide essential insights into clinical trajectories.
While previous studies have primarily employed a relative time encoding strategy to represent temporal intervals between consecutive events~\citep{hur2023genhpf, renc2024zero}, this approach may not always be optimal due to the following reasons.
Medical events for inpatients, such as prescriptions and lab tests, usually follow regular patterns dictated by hospital routines, clinical protocols, or daily schedules.
Additionally, a patient's physiological status can also depend on time-of-day patterns, making absolute time representation more effective in capturing meaningful trends.
For example, blood glucose measurements are heavily affected by a patient's fasting state~\citep{moebus2011impact}.
By incorporating absolute time encoding, the model can infer contextual factors such as feeding schedules or overnight fasting periods, leading to improved predictive performances for lab test outcomes.

To achieve this, we represent each timestamp $t_i$ as a combination of four types of special tokens, capturing essential temporal attributes that influence patient health status.
Specifically, these tokens include (1) the elapsed days since the ICU admission, (2) the day of the week the event occurred, (3) the absolute hour of the event, and (4) the aggregated minute value.
The minute value is aggregated in 10-minute intervals to balance granularity and computational efficiency, ensuring that time representations remain meaningful without introducing excessive sparsity.
For example, for the event recorded on the first day since the ICU admission, at 13:38 on a Tuesday, the timestamp $t_i$ is defined as $[\texttt{[DAY1]},\texttt{[TUE]},\texttt{[13h]}, \texttt{[30m]}]$.
In addition, we append a special $\texttt{[EOE]}$ token indicating the end of the event to every textual medical event $m_i$ and concatenate a sequence of medical events, so that a patient $P$ is now represented as
\begin{equation}
    P = \Big\|_{i=1}^{N}[t_i, m_i, \texttt{[EOE]}]
\end{equation}
where $\|$ is a symbol denoting the sequence concatenation operator.

\paragraph{Demographic encoding}
Physiological ranges for lab tests can vary significantly based on demographic attributes such as age, gender, and race.
For example, creatinine levels tend to be higher in males due to greater muscle mass~\citep{culleton1999prevalence}, while hemoglobin levels may vary depending on both gender and age~\citep{murphy2014sex}.
To incorporate this information into our model, we convert each patient's demographic attributes into a textual representation, denoted as $D$.
For example, for a 65-year-old Asian female patient, $D$ is defined as ``gender F age 65 race Asian''.
We prepend this demographic information to the patient's sequence of medical events $P$ and tokenize the sequence using the Bio-Clinical BERT tokenizer~\citep{alsentzer2019publicly} to obtain the final input sequence for the model, which is defined as
\begin{equation}
    x = f(D \| P) = f(\Big[D, \Big\|_{i=1}^{N}[t_i, m_i, \texttt{[EOE]}] \Big])
\end{equation}
 where $f(\cdot)$ denotes the tokenization function that converts the text sequence into a sequence of token indices.

\subsection{LabTOP Training}
\paragraph{Model architecture}
We design our model following the GPT-2 architecture~\citep{radford2019language}, which is composed of Transformer decoder layers~\citep{vaswani2017attention}.
Specifically, the input sequence $x$ is embedded into a sequence of latent vectors $\bm{X}\in\mathbb{R}^{M \times h}$ by the learnable embedding layer, where $M$ is the length of the input token sequence and $h$ is the hidden dimension of the model.
They are then fed into the Transformer decoder to model the conditional probability of the next token given the previous sequence, following an autoregressive language modeling approach.
% :
% \begin{equation}
    % p(x) = \prod_{i=1}^{M}p(x_i|x_{<i};\theta)
% \end{equation}
% where $\theta$ represents the model parameters, and $x_{<i}$ denotes all preceding tokens.

\paragraph{Objective}
Unlike traditional language modeling where the objective is to predict the next token for every position in the sequence, we modify the loss computation to focus exclusively on lab test outcome prediction.
That is, instead of computing the loss for all tokens, we selectively compute the loss only on the tokens corresponding to lab test values, encouraging the model to focus on accurately predicting lab test outcomes while still benefiting from a broader contextual understanding of the patient's medical history.
Accordingly, the loss is defined as
\begin{equation}
    \mathcal{L}=- \sum_{i\in{\mathcal{I}_{lab}}}\log{p(x_i|x_{<i};\theta)}
\end{equation}
where $\mathcal{I}_{lab}$ represents the set of token positions corresponding to lab test values along with their unit of measurements, as well as their special $\texttt{[EOE]}$ token.

\paragraph{Random permutation}
As mentioned in the previous section, timestamps are discretized into 10-minute interval tokens (\textit{e.g.,} 13:38 $\rightarrow$ $[\texttt{[13h]},\texttt{[30m]}]$), which can result in multiple medical events having the same timestamp tokens.
Accordingly, in order to make the model invariant to their ordering during training, we apply random permutation to medical events that share the same timestamp.
Specifically, given a set of events having the same timestamp, we randomly shuffle the order of the events at each training iteration to prevent the model from overfitting to any arbitrary order within the same timestamp.


\subsection{LabTOP Inference}
We formulate the lab test outcome prediction task as a language modeling task to predict numeric values token-by-token in an autoregressive manner.
That is, the model generates the corresponding numeric value of a target lab item given the sequence of prior medical events and the target lab item's name.
Accordingly, to construct inference samples, we extract every lab item appearing in the input sequence and build a corresponding inference prompt.
Specifically, for each target lab test item occurring at timestamp $t_k$, we concatenate all prior medical events up to $t_k$ along with the target lab test's textual representation, $e_k$ and $d_k$, to create an inference sequence.
Thus, the $k$-th inference sample of a patient is defined as
\begin{equation}
    x_{test}^{(k)} = f(\Big[D, \Big\|_{t_i<t_k}[t_i, m_i, \texttt{[EOE]}],[t_k,e_k,d_k]\Big])
\end{equation}
where $e_k$ and $d_k$ denote the event type (\textit{i.e.,} ``labevent'') and textual description of the target lab test (\textit{e.g.,} ``bicarbonate''), as aforementioned.

Once the inference sequence is constructed, it is fed into the trained model, which generates tokens autoregressively until the $\texttt{[EOE]}$ token is encountered.
In this process, our primary objective is to accurately predict the outcome value of a specific lab test at a given timestamp rather than probabilistically generating potential future lab tests.
Thus, to ensure precise predictions, we do not sample from the model's output distribution but instead select the most probable token (\textit{i.e.,} top-1 token) at each decoding step.
The generated sequence is then decoded back into a numerical value (\textit{e.g.,} 2 4 . 0 meq / l $\rightarrow$ 24.0 meq/l).

\section{Experiments}
\subsection{Data}
\label{sec:data}
To demonstrate the generalizability of our approach, we conduct experiments on three publicly available EHR datasets: MIMIC-IV~\citep{johnson2023mimic}, eICU~\citep{pollard2018eicu}, and HiRID~\citep{yeche2021hirid}.
Specifically, MIMIC-IV consists of 94,458 ICU records of 364,627 patients from the Beth Israel Deaconess Medical Center, including structured time-series data such as lab test results, and medication administrations.
eICU is a large-scale, multi-center dataset that includes 200,859 ICU records of 139,367 patients from hospitals across the United States, consisting of a broader range of patient demographics and treatment variations compared to single-center datasets.
HiRID is a high-resolution ICU dataset collected from the Bern University Hospital in Switzerland, composed of 33,905 ICU patients.
% This dataset provides continuously monitored data, making it well-suited for fine-grained temporal modeling of patient medical status.
For all these datasets, we take the ICU records whose length of stay is at least 6 hours.
We then randomly split them into training, validation, and test sets in an 8:1:1 based on their patient IDs, and treat each ICU record as an individual sample for our model.
% TODO appendix ref
Detailed dataset statistics are described in Appendix~\ref{app:data_statistics}.

To determine which types of EHR events should be involved for lab test outcome prediction, we categorized EHR events into two broad types: observation-type events, which represent the patient's physiological state (\textit{e.g.,} lab tests, microbiology tests), and intervention-type events, which correspond to medical treatments, prescriptions, and procedures administered during the ICU stay.
We then prioritized these event types based on their expected influence on lab test results:
(1) Firstly, medication events that directly reflect the patient's physiological status by indicating administered drugs and dosages.
(2) Secondly, other intervention-related events, including input events (\textit{e.g.,} fluid intake) and procedure events, which affect the patient's overall condition.
(3) Lastly, observation-type events that do not directly alter a patient's state but provide useful contextual information about their physiological trends.
Based on these criteria, the following tables for each dataset are used:
\begin{itemize}
    \item MIMIC-IV: \textit{labevents}, \textit{emar}, \textit{emar\_detail}, \textit{inputevents}, \textit{procedureevents}, \textit{outputevents}, \textit{microbiologyevents}

    \item eICU: \textit{lab}, \textit{medication}, \textit{infusiondrug}, \textit{treatment}, \textit{intakeoutput}, \textit{microlab}

    \item HiRID: \textit{observation\_tables}, \textit{pharma\_records}
\end{itemize}
Note that HiRID collected every observation-type event into one table (\textit{i.e.,} \textit{observation\_tables}), which leads to multiple types of events, such as lab tests and microbiology tests, being contained in this single table.
However, there is only an indicator specifying whether an event corresponds to a lab test or not, so we cannot differentiate other observation-type events within this table.
Accordingly, we select only those events explicitly identified as lab tests, excluding all other events from this table.

Additionally, to ensure a focus on the most clinically relevant medical events, we select only the medical items that account for the top 90\% of occurrences within their respective tables for each dataset, including the lab test tables as well as other event types such as medication, input events, and procedure events.
As a result, we select 44 unique lab tests from MIMIC-IV, 41 from eICU, and 27 from HiRID.
% TODO appendix ref
The included lab items from each dataset are detailed in Appendix~\ref{app:data_statistics}.

\subsection{Evaluation Metrics}
\paragraph{NMAE}
Normalized Mean Absolute Error (NMAE) is a normalized version of Mean Absolute Error (MAE) that accounts for differences in scale across multiple variables with different units and ranges.
Unlike standard MAE, which directly measures the average absolute difference between the predicted and ground-truth values, NMAE adjusts for variations in measurement scales by normalizing each error term based on the range of the corresponding lab item.
Specifically, for each unique lab item $l$, we compute its scale using the difference between the 99th percentile and the 1st percentile values (\textit{i.e.,} $v_{99\%}^{(l)} - v_{1\%}^{(l)}$) in the test set to mitigate the influence of extreme outliers.
The NMAE for a given lab test $l$ is then computed by dividing its MAE by this scale, which is defined as
\begin{equation}
    \text{NMAE}_{l}=\frac{1}{v_{99\%}^{(l)}-v_{1\%}^{(l)}}\sum_{i\in\mathcal{I}_l}\frac{|y_i-\hat{y}_i|}{N_l}
\end{equation}
where $\mathcal{I}_l$ represents the set of sample indices corresponding to the target lab test, $y_i$ and $\hat{y}_i$ denote the ground-truth and predicted value for $i$-th sample, and $N_l$ is the number of target lab samples in the test set.

\paragraph{SMAPE}
Symmetric Mean Absolute Percentage Error (SMAPE) is a widely used metric to evaluate the accuracy of prediction for continuous values, providing an intuitive measure of how much predicted values deviate from ground-truth values in percentage terms.
Unlike traditional Mean Absolute Percentage Error (MAPE), SMAPE ensures a symmetric scaling by normalizing the error terms based on the sum of the absolute values of the ground-truth and predicted values, preventing over-penalization when the ground-truth values are small.
This property makes SMAPE especially suitable for evaluating lab test outcome predictions, where the ranges of different lab items can vary significantly.
The SMAPE for a given lab test $l$ is defined as
\begin{equation}
    \text{SMAPE}_l = \frac{1}{N_l} \sum_{i \in \mathcal{I}_l} \frac{|y_i - \hat{y}_i|}{(|y_i| + |\hat{y}_i|) / 2} \times 100
\end{equation}
% Note that SMAPE basically can have a range from 0\% to 200\%, but typically falls within the 0\% to 100\% range in most practical cases.

\subsection{Baselines}
To demonstrate the effectiveness of \ours, we compare its performance with the following baseline methods, including both traditional machine learning approaches and state-of-the-art LLMs.
\begin{itemize}
    \item \textbf{Naive}: A simple method that predicts the lab test value at a given timestamp by using the most recent measured value of the same lab test, commonly referred to as the naive forecasting method.
    \item \textbf{Naive($\mu$)}: A variant of the naive method that predicts the lab test value by averaging all previously recorded values of the same lab test before the prediction timestamp.
    \item \textbf{GenHPF}~\citep{hur2023genhpf}: A multi-task learning model that incorporates hierarchical patient features for clinical prediction tasks. We adapt GenHPF to estimate lab test values by formulating it as a regression task.
    \item \textbf{XGBoost}~\citep{chen2016xgboost}: A widely used gradient boosting method designed for structured data. For this baseline, we use several statistics of past measurements, such as count and mean.
    \item \textbf{GPT-4o} and \textbf{GPT-4o-mini}: LLMs designed for general-purpose tasks. We evaluate these models to assess the capability of general-purpose models in predicting lab test outcomes. Since these models process long patient histories as input, running inference across the entire test set would require substantial token usage. To balance computational feasibility with representative evaluation, we sample 10\% of the test set\footnote{To be more specific, running inference in a batch on the sampled 10\% of the test set incurred a total cost of approximately \$500.}. All experiments employing GPTs were executed using the HIPAA-compliant GPT model available on Azure.\footnote{\url{https://learn.microsoft.com/en-us/azure/compliance/offerings/offering-hipaa-us}}
\end{itemize}
More details about each model's implementation, such as training hyperparameters and feature engineering for XGBoost, are provided in Appendix~\ref{app:implementation_details}.

\subsection{Results}

\input{tables/main}
The average results of lab test outcome prediction for three datasets are presented in Table~\ref{tab:main}.

\ours{} achieves the best performance in both NMAE and SMAPE across all datasets, except for NMAE in HiRID, where it still performs competitively.
Notably, naive approaches (Naive and Naive($\mu$)) show relatively higher SMAPE in HiRID compared to MIMIC-IV and eICU.
We speculate that this is due to the characteristics of HiRID, where lab tests are conducted more frequently than in the other two datasets.
Specifically, we found that the average interval between lab measurements is about 15.8 hours in MIMIC-IV, 19.8 hours in eICU, and only 12.9 hours in HiRID.
This shorter interval likely benefits methods that rely on simple statistical heuristics, such as carrying forward the last measured value (Naive) or computing historical averages (Naive($\mu$)), making them more effective in this setting.
However, our primary motivation is to predict lab test outcomes in environments where frequent testing is not always feasible.
Given this perspective, the strong performance of \ours{} on MIMIC-IV and eICU highlights its effectiveness in scenarios where repeated testing may not be feasible.

Interestingly, \ours{} also outperforms GPT-4o and GPT-4o-mini, confirming that general-purpose LLMs struggle with lab test outcome prediction task.
This underscores the importance of explicitly learning EHR data rather than relying on general LLM capabilities.
Furthermore, GPT-4o-mini exhibits significantly lower performance, which we attribute to its tendency to generate unrealistic lab test values.
Upon manual inspection, we found that GPT-4o-mini often outputs numerically implausible values that are far outside the expected range (\textit{e.g.,} predicting \textit{ph of arterial blood} as 135 whereas the ground-truth is 7.2).
This result demonstrates that our data processing and training strategy play a crucial role in achieving robust performance by effectively leading the model to produce clinically meaningful predictions.

\subsection{Ablation Studies}
To systematically evaluate the contributions of different components in our model, we conduct ablation studies along the following perspectives.
All experiments in this section are conducted using the MIMIC-IV dataset to ensure consistency.

\input{tables/abl_unified}
\subsubsection{Embedding and Training Strategy}
EHR events can be processed using different embedding strategies, which we categorize into \textbf{code-based} and \textbf{text-based} representations.
The code-based approach directly embeds structured medical codes (\textit{e.g.,} LOINC, ICD), while the text-based approach converts medical codes into their textual descriptions before embedding.
Additionally, unlike conventional methods that apply autoregressive loss to all tokens (\textit{i.e.,} \textbf{Full AR}), we specifically designed our model to focus on lab test values by applying autoregressive loss selectively (\textit{i.e.,} \textbf{Only LAB AR}).
To examine the impact of these design choices, we compare our model trained with different combinations of embeddings and loss strategies.

In the code-based embedding strategy, a new vocabulary is defined to accomodate structured medical tokens, following the embedding approach introduced in ETHOS~\citep{renc2024zero}.
Specifically, one or two tokens are defined per event: one token for the total event features that encodes the event type, item name and unit of measurement (if applicable) as a single entity (\textit{e.g.,} [LAB\_inr(pt)] or [LAB\_glucose\_mg/dL]), and the other token for the corresponding value mapped to one of 10 quantile tokens if the event has a numeric value.
Additionally, for medications, we map drug names to their corresponding ATC (Anatomical Therapeutic Chemical) codes whenever available.

As shown in Table~\ref{tab:abl_unified}, the incremental application of our embedding and training strategy components consistently improves performance.
Specifically, switching from training all tokens in an autoregressive manner to focusing solely on lab test events has a positive impact on the model's predictive accuracy.
This suggests that selective loss computation allows the model to effectively develop a deeper understanding of patterns and relationships specific to lab test outcomes.
In addition, incorporating a text-based embedding approach further enhances performance.
This improvement again highlights the advantage of providing the model with rich and interpretable representations of medical events, enabling it to better capture relationships within EHR data.
% These findings underscore the effectiveness of our strategy in optimizing lab test outcome prediction.

\subsubsection{Numeric Value Representation}
An alternative approach to handling numeric values in language models is discretization into quantiles, where continuous values are mapped to a set of predefined bins and treated as categorical tokens (\textit{i.e.,} \textbf{quantile-based tokenization})~\citep{renc2024zero}.
In contrast, our model treats numeric values as text and tokenizes them at the digit level (\textit{i.e.,} \textbf{digit-wise tokenization}).
To evaluate the effectiveness of this approach against the quantile-based method, we conduct experiments using different levels of quantization (5, 10, and 20 quantiles) and compare the results with the digit-wise tokenization strategy.

For the quantile-based method, the predicted value $\hat{y}$ is computed as the expected value of the quantile probabilities for each lab item.
Specifically, given a set of quantile bins $\mathcal{Q}$, each bin $q\in\mathcal{Q}$ is associated with a probability $p(q)$.
The predicted value is defined as the weighted sum of the expected values of the quantile intervals:
\begin{equation}
    \hat{y}=\sum_{q\in\mathcal{Q}}\mu_q \cdot p(q)
\end{equation}
where $\mu_q$ is the average value of the target lab item samples belonging to the quantile $q$ in the training set.
% where $\mu_q$ is the expected value of the quantile $q$, which is computed by the average value of the total samples belonged to the quantile $q$ in the training set.

As shown in Table~\ref{tab:abl_unified}, performance improves consistently as the number of quantile bins increases, suggesting that finer-grained numeric representations lead to better predictions.
Following this trend, digit-wise tokenization, as used in \ours{}, achieves the best performance, likely because it provides the highest resolution representation by preserving the exact numerical value rather than grouping them into fixed bins.
We believe this approach allows the model to capture subtle variations in lab test values that might otherwise be lost in a coarser representation.
Thus, these results highlight the importance of maintaining fine-grained detail when encoding numerical values to process EHR data effectively.

\subsubsection{Timestamp Representation}
As discussed in Section~\ref{sec:abs_time_encoding}, a patient's medical state can be influenced by daily routines and hospital schedules, making absolute time encoding more suitable than relative time encoding.
To validate the effectiveness of our approach, we compare our model trained with \textbf{relative time encoding} against the one trained with \textbf{absolute time encoding}, as presented in Table~\ref{tab:abl_unified}.
For relative time encoding, we calculate temporal intervals in minutes between consecutive events and append each interval to the corresponding medical event $\mathcal{M}_i$ after converting it to a text format, instead of prepending absolute timestamp tokens which we defined.

The result demonstrates that absolute time encoding leads to superior performance compared to relative time encoding, suggesting that absolute time encoding implicitly provides a more meaningful temporal context than merely representing time as intervals between events.
% One possible explanation for this improvement is that
In other words, absolute time encoding allows the model to capture daily patterns and periodic trends that influence lab test results, such as fasting-related fluctuations in glucose levels or routine morning lab draws in ICU settings.
In contrast, relative time encoding only provides information about the time elapsed since the previous event, making it more challenging for the model to infer broader temporal structures.
These findings emphasize the importance of designing time representations that align with the structured nature of clinical workflows.

\begin{figure}[t] % Use figure* for spanning both columns
    \centering
    \includegraphics[width=1\columnwidth]{figures/nmae_smape_trends.pdf} % Adjust width as needed
    \vspace{-4mm}
    \caption{
    Performance trends across different configurations of event types.
    }
    \label{fig:abl_tables}
    \vspace{-8mm}
\end{figure}

\subsubsection{Combination of Different Event Types}
As described in Section~\ref{sec:data}, we prioritized event types into three levels based on their expected influence on lab test results.
To assess how including a broader range of clinical events affects performances, we train models with different configurations of event types according to the criterion described in Section~\ref{sec:data}, and analyze their performance trends.
Specifically, each configuration incrementally includes (1) medication events (\textit{i.e., emar} and \textit{emar\_detail}), (2) input and procedure events (\textit{i.e., inputevents} and \textit{procedureevents}), and (3) output and microbiology events (\textit{i.e., outputevents} and \textit{microbiologyevents}).
Here, while utilizing more event types can provide richer contextual information, it can also lead the model to see a shorter historical window within the fixed sequence length.
For example, when using only two types of events (lab test and medication), the average covered time span per sample is 28.8 hours with a sequence length of 2048 tokens and 47.2 hours with 4096 tokens.
In contrast, when including six types of events, the average covered time span drops to 18.5 hours for 2048 tokens and 33.3 hours for 4096 tokens.
Hence, to systematically examine this trade-off, we also conduct these experiments by adjusting the sequence length to 2048 tokens from 4096 tokens, and compare the trends between them.
% TODO appendix ref 
Detailed statistics on the average covered time span for each table combination are provided in Appendix~\ref{app:data_statistics}.

Figure~\ref{fig:abl_tables} illustrates the impact of progressively incorporating more event types on the lab test outcome prediction performances.
The results show that when the sequence length is limited to 2048 tokens, increasing the number of event types leads to a degradation in performance for both sMAE and SMAPE.
This suggests that the reduced temporal coverage outweighs the benefits of incorporating more event types in this constrained setting.
In contrast, when using a sequence length of 4096 tokens, expanding the range of event types improves performance in general, indicating that longer sequences allow the model to leverage additional event information effectively.
These findings suggest that while our current experiments are still constrained to a sequence length of 4096 tokens, further increasing the sequence length may unlock additional benefits from incorporating an even broader range of EHR event types.

\section{Conclusion}
In this work, we proposed \textbf{\ours{}}, a unified model for lab test outcome prediction that leverages an autoregressive generative modeling approach on EHR data.
By integrating effective data processing and training strategy, \ours{} has shown the best performance across three public EHR datasets.
Specifically, through evaluations on MIMIC-IV, eICU, and HiRID, LabTOP consistently outperformed both traditional machine learning models and state-of-the-art LLMs, demonstrating its potential for clinical decision support and early detection of critical conditions.
Additionally, our ablation studies highlighted the efficacy of the proposed data processing and training strategy in modeling EHR data for the lab test outcome prediction task.
We believe that \ours{} will serve as an accurate and generalizable framework for lab test outcome prediction, especially in constrained environments where repeated testing is not feasible.

\paragraph{Limitations and future work}
Our experiments have some limitations as follows.
First, \ours{} requires relatively longer sequence length, leading to increased computational costs during training and inference, especially when we involve more types of EHR events.
Future work could explore more efficient tokenization strategies to balance precision and computational efficiency.
Second, our experiments were focused on retrospective EHR data, and its real-world applicability in prospective clinical settings remains to be validated.
Further research could assess \ours{}'s performance in real-time hospital environments and evaluate its impact on clinical decision-making.

% \acks{Acknowledgments go here \emph{but should only appear in the
% camera-ready version of the paper if it is accepted}.
% Acknowledgments do not count toward the paper page limit.}

\clearpage
\bibliography{chil-sample}

\clearpage

\appendix
\section{Dataset Statistics}
\label{app:data_statistics}
In this appendix, we describe dataset statistics used throughout our experiments. 

\input{tables/app_datastat1}

\paragraph{Number of ICU records}
Table~\ref{tab:app_datastat1} presents the number of ICU records in the training, validation, and test splits for each dataset (MIMIC-IV, eICU, and HiRID).
We split ICU records based on a patient level, ensuring that records from the same patient do not appear in multiple subsets.
The datasets follow an 8:1:1 split ratio, where 80\% of ICU records are assigned to training, 10\% to validation, and 10\% to test splits.

\input{tables/app_datastat2_lab_events_num}

\paragraph{Number of lab test events}
Table~\ref{tab:app_datastat2} presents the number of lab test events that appeared in the training, validation, and test splits for each dataset.
Given the substantial volume of data, the model was exposed to a diverse range of lab test events, enabling a thorough evaluation of its ability to predict lab test outcomes.

\input{tables/app_MIMICIV_timespan}

\paragraph{Selected lab items}
To focus on the most clinically relevant medical events, we selected lab tests and other event types based on their frequency, retaining only those that accounted for the top 90\% occurrences within their respective tables.
As a result, we included 44 lab tests from MIMIC-IV, 41 from eICU, and 27 from HiRID, as shown in Table~\ref{tab:lab_items}.
While core biomarkers such as glucose, sodium, potassium, chloride, creatinine, bicarbonate, hemoglobin, platelets, and white blood cell count are present across all datasets, notable differences exist: (1) MIMIC-IV includes a broader set of liver function tests (AST, ALT, bilirubin), coagulation markers (PT, INR), and red blood cell indices (MCV, MCH, RDW); (2) eICU incorporates hematological parameters (MPV, lymphocytes, monocytes) and oxygenation measures (FiO2, O2 saturation, base excess); and (3) HiRID focuses more on arterial blood gas (ABG) parameters (pH, PaO2, PaCO2, lactate, methemoglobin, carboxyhemoglobin), reflecting its emphasis on real-time ICU monitoring.

\paragraph{Time span coverage per sample}
Table~\ref{tab:time_span_mimiciv} presents the average time span per sample (in hours) for different combinations of event types in MIMIC-IV, according to a sequence length of 2048 and 4096 tokens.
The time span represents the total duration covered within each sample's sequence.
As expected, increasing the sequence length from 2048 to 4096 results in a longer time span coverage across all event combinations.
Specifically, for lab-only sequences, the time span increases from 32.8 hours to 43.6 hours.
However, adding more event types reduces the time span per sample, as additional events make each sample more dense within the same sequence length.
For example, when we use a sequence length of 2048 tokens, incorporating one event type (Lab w/ T1) reduces the time span to 20.0 hours, while including two and four more event types (Lab w/ T2 and Lab w/ T3) results in 20.5 hours and 18.5 hours respectively.
Similarly, for a sequence length of 4096 tokens, the time span decreases from 43.6 hours (Lab-only) to 34.8 hours (Lab w/ T1), 23.1 hours (Lab w/ T2), and 20.3 hours (Lab w/ T3).
% These results illustrate the trade-off between time span coverage and event diversity, where longer sequences allow for broader temporal coverage, but incorporating multiple event types captures more detailed patient history within a shorter time window.

\input{tables/app_lab_items}

\section{Implementation Details}
\label{app:implementation_details}

This appendix provides implementation details, including data processing methods, model configurations, training hyperparameters, data processing steps, and hardware specifications used in the experiments.

\paragraph{\ours{}}
The model architecture of LabTOP follows GPT-2, specifically implemented using the architecture of HuggingFace GPT2LMHeadModel\footnote{\url{https://huggingface.co/docs/transformers/model_doc/gpt2##transformers.GPT2LMHeadModel}}.
We use 12 attention decoder layers, each with 8 heads and a hidden dimension of 512.
All the experiments in this study are conducted using a maximum sequence length of 4096 tokens by default.
If a sequence of an ICU record exceeds the maximum sequence length, we repeatedly crop the sequence by this maximum length based on the event level.
Additionally, we again prepend demographic information $D$ to all the cropped samples.
For training, we applied early stopping with the patience of 5 epochs based on the validation loss, which yielded about $120$k, $110$k, and $34$k training steps with the learning rate of 1e-4 for MIMIC-IV, eICU, and HiRID, respectively.
As a result, we trained the model for approximately 96, 92, and 72 hours with 2 A6000 48GB GPUs for each dataset, respectively.

\paragraph{Naive}
If no prior measurement exists for the target lab test within the corresponding ICU record, the prediction is set to the mean of that lab test's values computed across the training set.

\paragraph{Naive($\mu$)}
If no prior measurements exist, the prediction defaults to the mean of that lab test's values across the training set, similar to the Naive method.

\paragraph{GenHPF}
GenHPF has a hierarchical architecture consisting of an event encoder and an event aggregator, designed to perform multiple predictive tasks in a single framework.
To adapt it for regressing multiple lab test outcomes, we construct samples by grouping the original samples that have the same prior medical history (\textit{i.e.,} lab test samples that occurred at the same timestamp).
Then, we manipulate the GenHPF model to regress every single lab test outcome given a sequence of medical events but compute the Mean Squared Error (MSE) loss only for the observed lab items for which we can define the ground-truth labels.
Here, since lab items with large magnitudes may disproportionately influence the model training, we standardize the outcome values for each lab item using the mean and standard deviation computed from the training set.
We maintain GenHPF's core architecture, with key hyperparameters including a prediction dimension of 128, an embedding dimension of 128, 4 attention heads, and 2 transformer layers.
Similar to LabTOP, we applied early stopping with the patience of 5 epochs based on the validation loss, which resulted in about $83$k, $210$k, and $71$k training steps with the learning rate of 3e-4 for each dataset, respectively.
As a result, we trained the model for approximately $39$, $67$, and $30$ hours with a single 80GB A100 GPU for each respective dataset.

% aggregate lab items measured at the same time to create labels while using the list of prior events as input.
% Each label of sample is represented as an $l$-dimensional array, where $l$ denotes the number of lab item types, containing the actual values of lab tests recorded at the target time.
% If a lab item is not measured at the target time, its value is set to -1e5 to indicate missing data.
% The input sequence format follows the original GenHPF approach, incorporating all column information, column names, and time gaps to maintain consistency with its event representation.
% Unlike the original GenHPF, which performs classification, our adaptation applies regression, producing an $(N, l)$ matrix, where $N$ is the batch size, and the loss is computed only for observed values, with missing values masked out.
% Since lab items with large magnitudes may disproportionately influence learning, we apply standardization using the mean and standard deviation computed from the training dataset.
% The model retains GenHPF’s core architecture, with key hyperparameters including a prediction dimension of 128, an embedding dimension of 128, 4 attention heads, and 2 transformer layers.
% GenHPF was trained for $83k$, $210k$, $71k$ iterations over approximately 39, 67, 30 hours using a single 80GB A100 GPU for each dataset, respectively. Early stopping was employed based on the validation loss, with patience set to 5 epochs.

\paragraph{XGBoost}
For XGBoost, the input features include the count, mean, min, and max values for all medical items used in the dataset, which are treated as separate columns.
As a result, the input consists of an array of the statistical summaries (count, mean, min, and max) of the unique events that precede the prediction time for the target lab item.
We trained this model for each lab item existed in the dataset with a learning rate of 0.1, a maximum depth of 5, and 100 estimators.

% The target lab item’s value serves as the label, while the input consists of an array containing the statistical summaries (count, mean, min, and max) of events recorded prior to the target lab item.
% This results in an input of shape $(N, c)$, where $N$ is the sample size and $c$ is the number of the derived features (i.e., 4 $\times$ the number of unique medical items).
% The model outputs a prediction of shape $(N, 1)$, corresponding to the estimated value of the target lab item.
% We used a learning rate of 0.1, a maximum depth of 5, and 100 estimators for training.

\paragraph{GPT-4o and GPT-4o-mini}
To create samples for GPT-4o and GPT-4o-mini\footnote{Specifically, at the time of the experiments, GPT-4o referred to \texttt{gpt-4o-2024-11-20} and GPT-4o-mini corresponded to \texttt{gpt-4o-mini-2024-07-18}.}, we followed the same strategy with GenHPF.
In other words, we grouped test samples that have the same prior medical history (\textit{i.e.,} the lab items that occurred at the same time), and the LLM was instructed to answer values of the target lab items based on their prior sequence of medical events as prompt.
The prompt we used for lab test outcome prediction can be found in Figure~\ref{fig:prompt}.
\begin{figure*}[b] % Use figure* for spanning both columns
    \centering
    \includegraphics[width=1.0\textwidth]{figures/prompt.pdf} % Adjust width as needed
    \vspace{-4mm}
    \caption{
    Template prompt used for GPT-4o and GPT-4o-mini.
    Note that [medical history] refers to the textual sequence of the test sample.
    }
    \label{fig:prompt}
    \vspace{-8mm}
\end{figure*}


% \item \textbf{Naive}: 
% \item \textbf{Naive($\mu$)}: 
% \item \textbf{GenHPF}~\citep{hur2023genhpf}:
% \item \textbf{XGBoost}~\citep{chen2016xgboost}: 
% \item \textbf{GPT-4o} and \textbf{GPT-4o-mini}: 

% GenHPF: model architecture 간단히 언급. predictive task를 위해 design된 모델인데 우리는 얘를 lab test value를 regression하는 task를 하도록 adapt했음 (이걸 어떻게 했는지). 여러 lab test 종류를 multi-task로 regression 시키기 위해서 각 lab item들을 mean, std로 standardize했다는 거 언급.

% XGBoost: 어떤 feature들 썼는지, configuration

% GPT-4o, GPT-4o-mini: 어떻게 프로세싱 했는지 prompt 알려주기

%\section{Detailed Experimental Results}
%\label{app:detailed_results}
% mimiciv, eicu, hirid에서 각 개별 lab item 별 성능
%\input{tables/app_mimiciv_LabTOP_6evetypes}

% \section{Second Appendix}\label{apd:second}

% This is the second appendix.

\end{document}
