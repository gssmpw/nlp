%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
%\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{multirow}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage[square]{natbib}
\usepackage{hyperref}
\hypersetup{hidelinks}
\hypersetup{
colorlinks=true,
linkcolor=blue,
citecolor=blue
}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{bbding}
\usepackage{makecell}
\usepackage[normalem]{ulem} 
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\allowdisplaybreaks

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
%----- blackboard bold fonts-----%
\newcommand{\AAA}{\mathbb{A}}
\newcommand{\BB}{\mathbb{B}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\DD}{\mathbb{D}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\JJ}{\mathbb{J}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\MM}{\mathbb{M}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\OO}{\mathbb{O}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\SSS}{\mathbb{S}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\UU}{\mathbb{U}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\WW}{\mathbb{W}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\YY}{\mathbb{Y}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\OOmega}{\mathbb{\Omega}}

\newcommand{\rA}{\mathscr{A}}
\newcommand{\rB}{\mathscr{B}}
\newcommand{\rC}{\mathscr{C}}
\newcommand{\rD}{\mathscr{D}}
\newcommand{\rE}{\mathscr{E}}
\newcommand{\rF}{\mathscr{F}}
\newcommand{\rG}{\mathscr{G}}
\newcommand{\rH}{\mathscr{H}}
\newcommand{\rI}{\mathscr{I}}
\newcommand{\rJ}{\mathscr{J}}
\newcommand{\rK}{\mathscr{K}}
\newcommand{\rL}{\mathscr{L}}
\newcommand{\rM}{\mathscr{M}}
\newcommand{\rN}{\mathscr{N}}
\newcommand{\rO}{\mathscr{O}}
\newcommand{\rP}{\mathscr{P}}
\newcommand{\rQ}{\mathscr{Q}}
\newcommand{\rR}{\mathscr{R}}
\newcommand{\rS}{{\mathscr{S}}}
\newcommand{\rT}{{\mathscr{T}}}
\newcommand{\rU}{\mathscr{U}}
\newcommand{\rV}{\mathscr{V}}
\newcommand{\rW}{\mathscr{W}}
\newcommand{\rX}{\mathscr{X}}
\newcommand{\rY}{\mathscr{Y}}
\newcommand{\rZ}{\mathscr{Z}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{{\mathcal{S}}}
\newcommand{\cT}{{\mathcal{T}}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\ifx\example\undefined
\newtheorem{example}[theorem]{Example}
\fi

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\aarti}[1]{ \textcolor{magenta}{[#1%-- Aarti
]}\typeout{#1}}

\newcommand{\nuoya}[1]{ \textcolor{red}{[#1 -- Nuoya]}\typeout{#1}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\title{Projection Optimization: A General Framework for Multi-Objective and Multi-Group RLHF}
\author{Nuoya Xiong\thanks{Carnegie Mellon University. Email: \texttt{nuoyax@andrew.cmu.edu}.} \qquad Aarti Singh\thanks{Carnegie Mellon University.  Email: \texttt{aarti@cs.cmu.edu}}}
\date{\today}




\begin{document}

%\twocolumn[
%\icmltitle{Projection Optimization: A General Framework for Multi-Objective and Multi-Group RLHF}



\maketitle
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Nuoya Xiong}{yyy}
% \icmlauthor{Aarti Singh}{yyy}

% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}


% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{Machine Learning, ICML}

% \vskip 0.3in
% ]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Reinforcement Learning with Human Feedback (RLHF) is a widely used fine-tuning approach that aligns machine learning model, particularly Language Model (LM) with human preferences. There are typically multiple objectives driving the preference, hence humans find it easier to express per-objective comparisons rather than a global preference between two choices. %, e.g. compare two papers on their novelty, clarity, correctness, etc.
Multi-Objective RLHF (MORLHF) aims to use per-objective preference feedback and achieve Pareto optimality among these objectives by aggregating them into a single unified objective for optimization. However, nearly all prior works rely on linear aggregation, which rules out policies that favor specific objectives such as the worst one. The only existing approach using non-linear aggregation  is computationally expensive due to its reward-based nature and the need for retraining whenever the aggregation parameters change.
In this work, we address this limitation by transforming the non-linear aggregation maximization problem into a series of sub-problems. Each sub-problem involves only linear aggregation, making it computationally efficient to solve. We further extend our framework to handle multi-group scenarios, where each group has distinct weights for the objectives. Our method enables achieving consensus or maximizing the aggregated objective across all groups.
Theoretically, we demonstrate that our algorithmic framework achieves sublinear regret and can be easily adapted to a reward-free algorithm. Empirically, leveraging our theoretical insights, we propose a nearly training-free algorithm once the optimal policies for individual objectives are obtained.
\end{abstract}

\section{Introduction}

In recent years, there has been considerable effort to fine-tune a machine learning model, particularly Large Language Model (LLM), to perform better on particular tasks. RLHF is a popular fine-tuning approach, which receives the human's preference feedback and aligns the LLM model with human values using fine-tuning.
% Some previous work  \citep{rosset2024direct} shows that small models can even outperform larger ones, with the help of RLHF.
Standard RLHF exploits human preference feedback between two outputs to maximize the expectation of the implicit or explicit reward function.

However, there are two main challenges for the application of RLHF in the real world. First, standard RLHF only maximizes a single reward function. 
%which represents one single evaluation metric. 
However, people often find it hard to evaluate choices in an overall sense as, in reality, there are often \textit{multiple objectives}. For example, comparing two papers or essays overall is harder than comparing them on specific objectives such as novelty, clarity, correctness etc. Similarly, recommending a city for vacation is harder than comparing cities on food options, nightlife, safety, etc. Each objective has its own implicit or explicit reward function, and the LLM needs to achieve a Pareto optimal trade-off between them by, for example, maximizing an aggregation of these reward function. Second, there are \textit{multiple groups} of users in the real world who may prefer different aggregations of the objectives. For example, groups with different genders, political views, marital status, etc. %younger single adults may prefer food and nightlife, while families may prefer kid-friendliness. 
This requires that the LLM either (a) satisfies the requirements of all the groups simultaneously, or (b) optimizes some aggregation across multiple groups. 

\vspace{-0.1in}
\paragraph{Multi-Objective Problem} %For the multi-objective problem, t
There are some works \citep{rame2024rewarded, yang2024rewards,shi2024decoding} that consider balancing the utilities of multiple objectives to get the Pareto optimal point or maximize the average expectation. Some works \citep{zhong2024provable,park2024rlhf}  consider multi-party problem in which each reward represents a group, which can also be regarded as a multi-objective problem.
We assume that we have $m$ different objectives, and each objective has its own reward function $r_i(x,y)(1\le i\le m)$. Each reward corresponds to an objective of the response $y$ like safety or helpfulness of the LLM.
%\aarti{introduce the safety, helpfulness application earlier}\nuoya{I move the introduction here.}. 
Nearly all of the previous work consider only linear aggregation, i.e., optimizing $  r(x,y)=\sum_{i=1}^m \alpha_i r_i(x,y),$ where $\alpha=\{\alpha_i\}_{i \in[m]}$ is the weight of all objectives that is assumed to be known. 
%and is assumed to always be known as prior knowledge. 

However, this kind of aggregation may not lead to an LLM that treats all objectives fairly. For example, the LLM may favor one objective significantly at the expense of another.
%In fact, linear aggregation may lead a LLM with a super large reward to one particular objective and ignore others.
In social choice theory, certain natural axioms such as monotonicity, symmetry,
scale invariance, etc. which apply to multi-objective aggregation as well, lead to %if certain some fairness axioms are satisfied \citep{cousins2021axiomatic}, the aggregated social welfare function lies in 
a more general function class \citep{cousins2021axiomatic}
\begin{align}r(x,y) =\left(\sum_{i=1}^m \alpha_i r_i^p(x,y)\right)^{1/p}, p\le 1,\label{eq:generalized_reward_f}\end{align}
%Then, the previous work mainly focus on how to maximize the averaged weighted sum $  r(x,y)=\sum_{i=1}^m \alpha_i r_i(x,y),
%$
%where $\alpha=\{\alpha_i\}_{i \in[m]}$ is the weight of all objectives and always be known as a prior knowledge. 
%However, there are two main drawbacks in the previous works. 
The general $p$-norm aggregation with $p\le1$ promotes fairness across multiple objectives, which is particularly useful when aiming for a machine model that achieves a balanced performance among different objectives.
Only one paper \citep{zhong2024provable} addresses the p-norm aggregation setting. In that work, the authors first learn a reward function for each objective, aggregate them into a new reward, and then attempt to optimize this new reward directly. However, this reward-based approach is computationally inefficient compared to the reward-free, DPO-based algorithm \citep{rafailov2024direct}. Moreover, it requires retraining the entire policy whenever the aggregation method changes, which becomes even more time-consuming. 

To reduce the computational cost of the reward-based RLHF algorithm, the paper \citep{shi2024decoding} shows that for $p=1$, once the optimal policy $\pi_{r_i}$
  for each individual objective is obtained, the optimal policy 
$\pi_r$ for the linear averaged sum can be calculated as $\pi_r(y\mid x) \propto \prod_{i=1}^m \pi_{r_i}(y\mid x)^{\alpha_i}.$
However, the derivation heavily depends on the linear structure of the aggregated reward $r(x,y)$. When $p\neq 1,$ this approach breaks and the optimal policy cannot be written as a simple closed-form of the optimal policies of each objective. 
%$\{\pi_{r_i}\}_{i \in [m]}$. A more detailed explanation will be provided in Section \ref{sec:moL}. 
That raises the first question: 
\vspace{0.5em}

\centerline{\textbf{\textit{Question 1: Can we derive a computationally efficient MORLHF algorithm}}}\centerline{\textbf{\textit{with non-linear aggregation?}}} 

\vspace{0.5em}
In our work, we propose a projection-based algorithm both in offline and online preference data settings, which transforms the nonlinear objective maximization problem into a sequence of subproblems, each involving only a linear maximization problem. Theoretically, we provide a thorough analysis for both offline and online setting, showing that it can converge to the optimal policy with a sublinear regret. Empirically, by leveraging the fact that there is a training-free algorithm for linear aggregation maximization, we %ultimately 
derive a training-free algorithm for the generalized reward aggregation, which saves significant training time. 

Moreover,
previous work typically assumes that the weight for each objective is known.
This assumption simplifies the problem and allows for straightforward optimization. However, in real-world applications,  the importance weights $\{\alpha_i\}$ for each objective are usually unknown. 
% That arises the second question:
% \centerline{\textbf{\textit{Question 2: Whether we can effectively learn }}}\centerline{\textbf{\textit{the weight of each objective directly from the data?}}}
%Developing a robust learning paradigm capable of inferring $\{\alpha_i\}$ is crucial for enabling real-world MORLHF.
In our work, we observe that the weight of an objective reflects its importance, which can be learned by how frequently the objective is reported in the human preferences. We propose a learning paradigm where the LLM learns objective weights from collected data, enabling the estimation of $\{\alpha_i\}$ and incorporating them into our theoretical results.

% The paradigm assumes that humans provide feedback on one objective at a time, rather than expressing an overall preference, with the objective selection guided by the implicit importance of each objective to the human. For example, a person could first choose the objective with the most noticeable difference and then indicate which option is better regarding that objective. When comparing two paragraphs generated by a large language model, the person might first focus on "fluency" and provide feedback based solely on the fluency of the two paragraphs.


 %\aarti{Lets put this motivation first as there is not much work on this in RLHF setting, while pluralistic alignment is already been discussed} \aarti{Also, is this really multi-objective or multi-objective? I think multi-objectives would be where say a scientist tells us that temperature is more important than pressure for synthesizing a desired protein, but the distinction between objective and objective is less clear for text} Given the multi-group setting, rather than learning each group's reward function individually, it is more efficient to extract the implicit objectives underlying preferences across diverse groups and to learn the relative importance of these objectives for each group. For example, one group might like a safer LLM, while another group might prefer a helpful LLM. To satisfy both groups, it’s beneficial to first learn the implicit utilities associated with safety and helpfulness, then adjust the model's responses based on each group’s weighting of these attributes.

%There are a lot of previous works considering \aarti{\sout{multi-objective or}} multi-objective problem. However, there are two main drawbacks of their works. First, all of them assume a known weight $\alpha = \{\alpha_i\}_{i \in [m]}$, and the problem becomes maximizing $$r(x,y)=\sum_{i=1}^m \alpha_i r_i(x,y),$$ where the reward $r_i(x,y)$ is only for the objective $i$. 
%However, in the real world, the importance weight $\alpha$ may be unknown and different among different groups. \aarti{Should we be putting another sub/super-script for group?} Hence, establishing a learning paradigm that can learn the importance weight $\alpha$ is essential for real-world multi-objective and multi-group RLHF. 

%Second, most of previous papers about multi-objective RL consider the linear aggregation of the reward. However, this kind of aggregation may not be general enough.
%Actually, by the social choice theory, if some axioms are satisfied, the aggregated social welfare function have a general form $$r(x,y) =\left(\sum_{i=1}^m \alpha_i r_i^p(x,y)\right)^{1/p}, p<1.$$ Then, the  classical reward-free algorithm like DPO and the multi-objective extension \citep{zhong2024provable} fails, since their approach heavily depends on the linear structure of the $r(x,y)$. Thus, it is unclear whether we can get a reward-free algorithm for the non-linear aggregation reward function.

% TODO: 
% 1. Define the multi-group setting.
% 2. Discuss why the DPO fails. How solving DPO is hard for p not equals to 1.
\paragraph{Multi-Group Problem}  Classical RLHF often assumes a single-group setting, ignoring the heterogeneity in human feedback and assuming that the human feedback relies on one unique reward function. However, real-world scenarios involve multiple groups with distinct preferences. Fine-tuning an LLM for each group is computationally expensive, making it essential to fine-tune the LLM to accommodate all groups' preferences simultaneously.%For instance, if multiple groups are using the trained LLM, each with different requirements, it is often impractical to fine-tune a separate LLM for each group. Instead, a more realistic approach is to use a single LLM that can accommodate and satisfy the diverse requirements of all groups as effectively as possible.

Since previous papers \citep{zhong2024provable,park2024rlhf} working on multi-group RLHF only consider learning the reward function of each group under a single objective and then aggregating them, we regard them as a special case of the MORLHF. %and discuss them with previous MORLHF algorithms. 
Hence, there is a lack of discussion about the multi-group setting where each group may have different importance for different objectives.

Formally, assume that we have $N$ group and $m$ objectives, and each group $n \in [N]$ has their own weight $\alpha^{(n)}\in \Delta_{m-1}$. The reward of the group $n$ is then defined by 
\begin{align*}
    r^{(n)}(x,y) = \left(\sum_{i=1}^m \alpha_i^{(n)}(r_i(x,y))^{p^{(n)}}\right)^{1/p^{(n)}}, p^{(n)}\le 1.
\end{align*}
The reward function of each objective, $\{r_i(x, y)\}_{i \in [m]}$, remains fixed across different groups, while the weight $\alpha$ and the parameter $p$ can vary.
In other words, the reward of each objective is the inherent value, and the importance weight represents the subjective part of each group. Now we pose the last question: 
\vspace{0.5em}

\centerline{\textbf{\textit{Question 2: Can we formulate and tackle the multi-group }}}\centerline{\textbf{\textit{problem under MORLHF setting?}}}

\vspace{0.5em}
In this paper, we consider two final goals for multi-group problem. Motivated by the poll theory, the first objective is called ``consensus", in which LLM needs to meet the requirements of all groups as good as possible simultaneously. Motivated by social choice theory, the second objective is called "aggregation", in which the LLM needs to optimize a general aggregation of the utilities of all groups. 
We will show that our formulation and algorithmic framework naturally solve these two final goals.
In summary, we have the following contributions:
\begin{itemize}

\item We reformulate the reward maximization in MORLHF as minimizing the distance between the current reward vector and a target set. This reframing decomposes the aggregated reward maximization into sub-problems, each focusing on minimizing the distance in a specific direction. These sub-problems reduce to linear aggregation and can be efficiently solved using previous approaches.
Theoretically, we provide converge guarantees for both offline and online setting. Empirically, we provide a training-free algorithm once the optimal policy and the reward function for each objective is given, making it more computationally efficient.

\item  We tackle the multi-group problem in two ways: (1) achieving consensus by defining the target set as the intersection of all groups' target sets, and (2) minimizing the malfare function \citep{cousins2021axiomatic} which aggregates the distance between each group's expected reward vector and its target set. Our framework addresses both problems concisely with theoretical guarantees.

\item  We establish a learning paradigm where the LMs learn the importance weight from data. We integrate weight estimation into the online setting and provide theoretical guarantees.



\end{itemize}
\input{related}
% The remaining question is to define the final goal of the multi-group problem. Since the weighted $p$-norm aggregation is hard for deriving reward-free algorithm from the previous discussion, we  For each group $n$, we assume there is a target set $W^{(n)}$ representing the requirement. In a single-group setting, 
% $$W^{(n)} = \left\{x \in \RR^m: \left(\sum_{i=1}^m \alpha_i^{(n)} x_i^{p^{(n)}}\right)^{\frac{1}{p^{(n)}}} \ge c^{(n)}\right\},$$ 

% which implies that the group can be satisfied if the aggregation of the  reward function is larger than some pre-defined constant. Hence, we can transfer the reward maximization problem to minimizing the distance between the expected reward vector $(\EE_\pi[r_1(x,y)],\cdots, \EE_\pi [r_m(x,y)])$ (with some regularizer) and the target set $W^{(n)}$. In order to involve multi-group problem,
% we consider two types of problems that represents the effectiveness of a model among diverse groups.


\section{Preliminaries and Notations}

%Following the previous work \citep{rafailov2024direct}, we formulate the RLHF problem as a contextual bandit. 
Denote the prompt space of the LLM  as $\cX$ and the response space as $\cY$. The distribution $\rho \in \Delta(\cX)$ represent the distribution of the prompt. A policy $\pi:\cX\to \Delta(\cY)$ represents an LLM that generates a response distribution given prompt $x$. In RLHF, we assume that we can get a pre-trained LLM $\pi_{\mathrm{ref}}$ that is usually trained on supervised data. The goal is to fine-tune the pre-trained model to align the model with the human preference on one particular task. To be more specific, given prompt $x\sim \rho$, LLM can generate two responses $y_1,y_2$ , then the human gives a preference feedback on the response pairs as either $y_1 \prec y_2$ or $y_1 \succ y_2$. The responses $y_1, y_2$ are labeled as $y_w, y_l$ respectively  with probability $\PP(y_1 \succ y_2\mid x)$, and are labeled as $y_l,y_w$ with probability $1-\PP(y_1\succ y_2\mid x)$. It is further assumed that the human preference is modeled by a Bradley-Terry (BT) model with the reward function $r^*(x,y) : \cX \times \cY \mapsto [0,B]$:
\begin{align}
    \PP(y_1 \succ y_2 \mid x) %&= \frac{\exp(r^*(x,y_1))}{\exp(r^*(x,y_1)) + \exp(r^*(x,y_2))} \nonumber\\&
    = \sigma(r^*(x,y_1)-r^*(x,y_2)),\nonumber
\end{align}
where $\sigma(z) = \frac{1}{1+\exp(-z)}$ and $B\ge 1$.
Given the reward function $r$, the optimal policy $\pi_r = \argmax_\pi J(\pi)$ maximizes the expected reward function, with an additional KL divergence term that prevents the policy from deviating too much from $\pi_{\mathrm{ref}}$: 
\begin{align}
    \pi_r &= \arg\max_{\pi} J(\pi) = \arg\max_\pi \EE_{x\sim \rho}\EE_{y\sim \pi(\cdot \mid x)}\left[r^*(x,y) - \beta \DD_{\mathrm{KL}}(\pi\parallel \pi_{\mathrm{ref}})\right]. \label{eq:rlhf_optimal_policy}
\end{align}
%\aarti{should $d_0$ be $\rho$?}

% We model the reward as the linear reward $r_\theta(x,y) = \theta^T \phi(x,y)$. Then, we can use the MLE to get the estimation of the reward:

% \begin{align}\ell_\cD(\theta) = \sum_{(x,y_1,y_2,s) \in \cD}\left[s\log \left(\sigma(r_\theta(x,y_1) - r_\theta(x,y_2))\right) + (1-s) \left(\sigma(r_\theta(x,y_2) - r_\theta(x,y_1))\right)\right].\end{align}

% Then denote $\theta_{MLE} = \arg\max_{\theta \in \Theta}\ell_\cD(\theta).$

In this paper, we consider both offline and online RLHF. For the offline RLHF setting, the LLM has access to a pre-collected offline data $\cD$ consisting of prompts and corresponding winning and losing responses, and the expectation in the optimal policy is calculated on the offline data. For the online setting, at each round LLM can generate two responses $y_1, y_2$ following the policy $\pi$, and then receive the preference feedback by human for data collection.

We assume there are $m$ known representations $\{\phi_i(x,y) \in \RR^d\}_{i \in [m]}$ and the corresponding reward function class $\{r_i(x,y) = \theta_i^\top \phi_i(x,y) \in [0,B], \|\phi_i\|_2\le 1, \|\theta_i\|_2 \le B\}$ for each objective $i \in [m]$. The true reward $r^*_i$ for objective $i$ can be written as $r^*_i(x,y) = (\theta_i^*)^\top \phi_i(x,y).$ This assumption is purely theoretical. In practice, the reward can be parameterized as $r^\theta$
  using a neural network, and our practical algorithm \ref{alg: vpo-fl-prac} also does not rely on this assumption. 
% Each reward corresponds to an objective of the response $y$ like safety or helpfulness \aarti{introduce the safety, helpfulness application earlier}. 
%For each group $n \in [N]$, we have \textit{importance weights} $\alpha^{(n)} = (\alpha_1^{(n)}, \cdots, \alpha_{m}^{(n)})^\top  \in \Delta_{m-1}$. showing the importance of each objective from the perspective of group $n$. 

Since the preference only contains the information of $r_i(x,y_1)-r_i(x,y_2)$ for each objective $i$, rewards are invariant to constant shifts in feedback. Follow \citep{cen2024value}, we can assume there is a known policy $\pi_{\mathrm{base}}$ and constant $C$, such that for each $i \in [m]$, the reward parameter space $\Theta_i$ is defined as
\begin{align}
    \Theta_i = \left\{\theta \in \RR^d: \EE_{\pi_{\mathrm{base}}}\langle \theta_i, \phi_i(x,y) \rangle =C\right\}.\label{eq:theta base policy}
\end{align}



% By some basic algebra, it is easy to find that the  closed-form of the optimal policy $\pi_r$ is 
% \begin{align}
%     \pi_r(\cdot \mid x) \propto \pi_{ref}(\cdot \mid x)\cdot \exp\left(\frac{1}{\beta}r(x,\cdot)\right).
% \end{align}

\subsection{Multi-Objective Learning}\label{sec:moL}

We assume that there are $m$ different objectives, and each objective has reward function $r_i(x,y) \in [0,B]$ for $i \in [m]$. As discussed in the introduction, we apply the definition of social welfare function in social choice theory to multi-objective setting and consider the weighted $p$-norm aggregation across objectives 
$$r(x,y) = \left(\sum_{i=1}^m \alpha_i r_i^p(x,y)\right)^{1/p}, p\le 1,$$ where $\alpha \in \Delta_{m-1}$ are weights of the objectives. 
Note that for positive rewards, aggregation yields Pareto optimality.  

The goal is to find the optimal policy for the aggregated reward function $r$. One natural approach to solving multi-objective RLHF is to first learn a reward model for each individual objective, and then aggregate these models to formulate a new reward. Finally, RL methods like PPO can be applied to optimize this new reward. However, this reward-based approach is significantly more computationally inefficient and unstable compared to reward-free approaches, such as DPO \citep{rafailov2024direct}. Additionally, it requires retraining the entire model for all possible reward aggregations, which becomes time-consuming when the aggregation parameters change.
In this work, we first provide a theoretical algorithmic framework for multi-objective RLHF, which naturally leads to the derivation of a reward-free algorithm. Based on this theoretical framework, we propose a \textit{nearly training-free} practical algorithm that incurs almost zero computational cost once the optimal policy for each objective is obtained.

Previous techniques cannot be easily applied to this setting. In fact, for the linear aggregation when $p=1$, the paper \citep{shi2024decoding} finds that the optimal policy $\pi_r$ can be written as a closed-form of the optimal policy $\pi_{r_i}$ as
$
    \pi_r(\cdot \mid x) \propto \pi_{\mathrm{ref}}(\cdot \mid x)\cdot \exp\left(\frac{1}{\beta}r(x,\cdot)\right),
$
and conduct a decoding algorithm MOD using this derivation.
%\nuoya{mention MOD}
By the linear aggregation $r(x,y) = \sum_{i=1}^m \alpha_i r_i(x,y)$ and $\sum_{i=1}^m \alpha_i =1,$ it is easy to verify that 
$\pi_r(y\mid x) \propto \prod_{i=1}^m \pi_{r_i}(y\mid x)^{\alpha_i}.$
Hence, one natural reward-free algorithm is to first learn the optimal policy $\pi_{r_i}$ for each objective using DPO, then calculate the optimal policy $\pi_r$. It is also a training-free algorithm once the optimal policy for each objective is known. However, 
when we choose the general aggregation with $p\le 1$, this derivation will fail due to the non-linear structure of the reward, making the problem much more complicated. 


To avoid this technical difficulty, we draw inspiration from RL with Blackwell-approachability \citep{yu2021provably}, which focuses on minimizing the distance between the reward vector and a specified target set. This approach makes the problem more tractable since we can incorporate the non-linear aggregation into the definition of the target set. To be more specific, a target set $W \subset \RR^m$ is a convex set that is defined by
\begin{equation*}
    W_{p,c}^\alpha = \left\{z \in \RR_{\ge 0}^m: \left(\sum_{i=1}^m\alpha_i z_i^p\right)^{1/p} \ge c\right\},
\end{equation*}
 where $\alpha$ represents the weights assigned to the objectives by humans, $p$ represents the degree of fairness, and $c$ reflects the requirement of humans. In practice, we can learn $\alpha$ and $p$ from supervised and preference data, and the parameter $c$ can be provided by humans or chosen by parameter tuning. 
%  \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{targetset.jpg}
%     \caption{A diagram of Target Set. The goal is to minimize the distance between reward vector $S(\pi)$ and the target set $W_{p,c}^\alpha.$}
%     \label{fig:targetset}%\nuoya{Do we need some figure like this?}
% \end{figure}
 %It is easy to show that when $p\le 1$, $W$ is a convex set.
 The definition of target set implies that the group can be satisfied if the aggregation of the  reward function is larger than some pre-defined constant. %\aarti{would this target set (and consensus) satisfy social choice theory axioms? should we discuss}\nuoya{I think it is different from the classical social choice theory.} %\nuoya{I move this paragraph here since here we first introduce the target set.}.
 We also define the expected reward vector $S(\pi) \in \RR^m$ as 
 %Now assume $c,p,\alpha$ are all given, 
 %the target is to find a policy $\pi$  such that $S(\pi)$ minimizes the Euclidean distance $d(S(\pi), W^\alpha_{p,c})$, where $S(\pi) \in \RR^m$ such that 
$
    (S(\pi))_i= \EE_\pi[r_i^*(x,y)-\beta\DD_{KL}(\pi\| \pi_{\mathrm{ref}})], 
$
which is the expected reward following the policy $\pi$ with a regularized term of KL divergence.
% It is easy to show that when $p\le 1$, $W$ is a convex set. The definition of target set implies that the group can be satisfied if the aggregation of the  reward function is larger than some pre-defined constant \aarti{would this target set (and consensus) satisfy social choice theory axioms? should we discuss}\nuoya{I think it is different from the classical social choice theory.}.
Now assume $c,p,\alpha$ are all given, we can transfer the aggregation maximization problem to minimizing the distance between the expected reward vector (with some regularizer) and the target set $W$. 
The goal changes to minimizing the distance between $S(\pi)$ and $W_{p,c}^\alpha$:
\begin{equation}
   \pi^* =  \arg\min_\pi D(\pi) := \ d(S(\pi), W_{p,c}^\alpha). \label{eq:our formulation}
    % + \beta_{\mathrm{general}} \DD_{\mathrm{KL}}(\pi\parallel \pi_{\mathrm{ref}}),be
\end{equation}
% Note that we do not assume that $W^*$ is approachable, i.e., there may not exist a policy $\pi^*$ such that $S(\pi^*) \in W^*$. Instead, we aim to find the optimal policy which minimizes the distance between $S(\pi)$ and $W^*$.
Note that if we choose $c$ as the maximum value that there exists a policy $\pi$ that satisfies $d(S(\pi), W_{p,c}^\alpha)) = 0,$ then $\pi$ is one of the optimal policies and 
$$\pi = \arg \max_{\pi \in \Pi} \left(\sum_{i=1}^m \alpha_i \EE_\pi [r_i^*(x,y) - \beta \DD_{\mathrm{KL}}(\pi \| \pi_{\mathrm{ref}})]^p\right)^{1/p}$$
where every $\pi \in \Pi$ satisfies that $\EE_\pi[r_i^*(x,y)]-\beta\DD_{\mathrm{KL}}(\pi\|$ $\pi_{\mathrm{ref}})\ge 0$.  This statement highlights the connection between the original maximization problem Eq.~\eqref{eq:rlhf_optimal_policy} and our formulation Eq.~\eqref{eq:our formulation}. Therefore, our formulation can be viewed as an alternative metric for measuring the performance of LLMs in achieving multi-objective learning tasks. %As we will show later, our formulation can avoid the previous technical difficulty from nonlinear general aggregation, and lead to an efficient way to achieve multi-objective learning.

Now we demonstrate that more general aggregation methods can enable LLM to accommodate a wider range of objectives by selecting different values of $p$.

%\aarti{Assume following is true when $d$ is Euclidean distance? Is there a different notion of distance that works better for other $p$s?}
\begin{example}[$p=1:$ Linear Aggregation]
If we choose $p = 1$ and $c \ge  \max_{\pi} \sum_{i=1}^m \alpha_i \EE_\pi[r_i^*(x,y)]$, then the goal $D(\pi)$ will become 
\begin{align*}
    D(\pi) &=  d(S(\pi), W_{1,c}^{\alpha})
    % + \beta_{\mathrm{general}}  \DD_{\mathrm{KL}}(\pi\parallel \pi_{\mathrm{ref}})
    = \frac{c-\sum_{i=1}^m \alpha_i \EE_{\pi}[r_i^*(x,y)] + \beta\DD_{\mathrm{KL}}(\pi\parallel \pi_{\mathrm{ref}})}{\sqrt{\sum_{i=1}^m \alpha_i^2}}.
\end{align*}
The last equality is because the selection of $c$.
% so that 
% \begin{align*}&|c-\sum_{i=1}^m \alpha_i \EE_{\pi}[r_i^*(x,y) - \beta\DD_{\mathrm{KL}}(\pi\parallel \pi_{\mathrm{ref}})]| \\&\qquad  = c-\sum_{i=1}^m \alpha_i \EE_{\pi}[r_i^*(x,y) ]+ \beta\DD_{\mathrm{KL}}(\pi\parallel \pi_{\mathrm{ref}}).\end{align*} 
From this derivation, we know that it is equivalent to the previous classical MORLHF with linear aggregation.

\end{example}

\begin{example}[$p=-\infty:$ worst-case reward]
    When $p= -\infty,$ the target set becomes 
    \begin{equation*}
        W_{-\infty,c}^\alpha =\left\{z \in \RR_{\ge 0}^m: \min_i z_i \ge c \right\}, 
    \end{equation*}
    which represents that the human wants to find an LLM with no obvious drawback for any of the objectives, i.e., requiring $\min_i\EE_{\pi}[r_i^*(x,y)] - \DD_{\mathrm{KL}}(\pi \|\pi_{\mathrm{ref}})$ larger than some threshold.
    Now we establish the connection between $p=-\infty$ and the max-min RLHF in \citep{chakrabortymaxmin}. The proof is provided in Appendix \ref{app:proof maxmin}. 
\begin{theorem}\label{thm:relationship_maximin}
    Define the max-min value as $c^*=\max_\pi [\min_i \EE_\pi [r_i^*] - \beta \DD_{\mathrm{KL}}$ $(\pi \|\pi_{\mathrm{ref}})]$. Then, if we choose the target set 
     $W_{-\infty, c}^\alpha$ such that $c$ is close to $c^*$, the resulting optimal policy also achieves a max-min value that close to $c^*$. To be more specific, we have 
     \begin{small}
     \begin{align*}
         \min_i \EE_\pi[r_i^*(x,y) - \DD_{\mathrm{KL}}(\pi \| \pi_{\mathrm{ref}})] \ge c^*-(\sqrt{m}+1) |c^*-c|.
     \end{align*}
     \end{small}
\end{theorem}
\end{example}



% \begin{example}[$p=-1:$ weighted harmonic mean]
%     When $p=-1$, the target set becomes 
%     \begin{equation*}
%         W_{-1,c}^\alpha = \left\{z \in \RR_{\ge 0}^m: \left(\sum_{i=1}^m \frac{\alpha_i}{z_i} \right)^{-1}\ge c\right\},
%     \end{equation*}
%     which represents that the weighted harmonic mean of different objectives should be larger than some threshold. 
% \end{example}

% One advantage of considering the target set is that, \textbf{it can naturally solve the multi-group problem.} Suppose there are $N$ groups and each group $n$ has a different importance weight $\alpha^{(n)}$ and different requirements $p^{(n)}$ and $c^{(n)}$, then the consensus of multiple group can be defined 
% \begin{equation*}
%     W^* = \bigcap_{n=1}^N W^{\alpha^{(n)}}_{p^{(n)},c^{(n)}}.
% \end{equation*}
% Thus, by considering the general target set, we can finally overcome the multi-group problem, by achieving the consensus of all the group simultaneously. Furthermore, we can not only hold the diverse weight $\alpha$, but also diverse kinds of target (for example, by choosing different $p$ when we consider $W_{p,c}^\alpha$).




\subsection{Multi-Group Learning}
%\nuoya{A figure about target set.}
Beyond the single group setting, we also study the multi-group setting, where each group has a different aggregation approach (parameterized by $c,p$ and $\alpha$).
For each group $n$, we assume there is a target set
$$W^{(n)} = \left\{z \in \RR_{\ge 0}^m: \left(\sum_{i=1}^m \alpha_i^{(n)} z_i^{p^{(n)}}\right)^{\frac{1}{p^{(n)}}} \ge c^{(n)}\right\}$$ 
representing the aggregation rule across objectives for them.  We consider two types of goals that represent the effectiveness of alignment across diverse groups.





\paragraph{Consensus}
The first goal is called ``consensus", in which we wants to minimize the distance between the expected reward vector and the intersection of all target sets from diverse groups. Formally, the goal is to choose the optimal policy that minimizes the Euclidean distance
\begin{align}
    \pi^* = \arg \min_\pi d\left(S(\pi), \bigcap_{n=1}^N  W^{(n)}\right).
\end{align}

\paragraph{Malfare Function Minimization}

Another goal is to minimize the aggregated malfare function, where the malfare function for each group is the square of the distance between the expected reward vector and the group's target set. Formally, with group weight $\zeta_n>0$ and $\sum_{n=1}^N \zeta_n = 1,$  the goal is to find the optimal policy $\pi^*$ that
\begin{align}
    \pi^* = \arg\min_\pi \left(\sum_{n=1}^N \zeta_n\left(d^2(S(\pi), W^{(n)})\right)^q\right)^{1/q}, q\ge 1.\nonumber
\end{align}

% Some previous works \citep{zhong2024provable,park2024rlhf} consider first learning the reward of each group \aarti{We need to compare more carefully, e.g. I just looked at [4,6] and they seem to use a social welfare function similar to ours (their $\alpha$ corresponds to our $p$, but they assume it is known and weights are same, I think). Also, [6] considers the reward-free setting to account for intransitive rewards where they seek von-Neumann winner for a preference matrix. Both consider offline preferences only. Also, very important to highlight that first learning individual rewards approach does not work in general unless p=1 with the DPO framework. So we come up with a sequence of projections onto a target set specified by $p\neq 1$ and show that it works} and then aggregating them to get a social welfare function. After that, the final step is just a social welfare maximization problem \aarti{In this approach, one has to know what group each preference is coming from}. However, there is no work considering how to achieve consensus between several groups, instead of maximizing a single aggregated objective. In this paper, we consider the pluralistic alignment problem, in which we need to satisfy all the groups' requirements rather than maximizing an aggregated reward \aarti{what if all cannot be satisfied - conflicting requirements?}.




% \paragraph{objective Learning} \aarti{Lets put this motivation first as there is not much work on this in RLHF setting, while pluralistic alignment is already been discussed} \aarti{Also, is this really multi-objective or multi-objective? I think multi-objectives would be where say a scientist tells us that temperature is more important than pressure for synthesizing a desired protein, but the distinction between objective and objective is less clear for text} Given the multi-group setting, rather than learning each group's reward function individually, it is more efficient to extract the implicit objectives underlying preferences across diverse groups and to learn the relative importance of these objectives for each group. For example, one group might like a safer LLM, while another group might prefer a helpful LLM. To satisfy both groups, it’s beneficial to first learn the implicit utilities associated with safety and helpfulness, then adjust the model's responses based on each group’s weighting of these attributes.

% There are a lot of previous works considering \aarti{\sout{multi-objective or}} multi-objective problem. However, there are two main drawbacks of their works. First, all of them assume a known weight $\alpha = \{\alpha_i\}_{i \in [m]}$, and the problem becomes maximizing $$r(x,y)=\sum_{i=1}^m \alpha_i r_i(x,y),$$ where the reward $r_i(x,y)$ is only for the objective $i$. 
% However, in the real world, the importance weight $\alpha$ may be unknown and different among different groups. \aarti{Should we be putting another sub/super-script for group?} Hence, establishing a learning paradigm that can learn the importance weight $\alpha$ is essential for real-world multi-objective and multi-group RLHF. 

% Second, most of previous papers about multi-objective RL consider the linear aggregation of the reward. However, this kind of aggregation may not be general enough.
% Actually, by the social choice theory, if some axioms are satisfied, the aggregated social welfare function have a general form $$r(x,y) =\left(\sum_{i=1}^m \alpha_i r_i^p(x,y)\right)^{1/p}, p<1.$$ Then, the  classical reward-free algorithm like DPO and the multi-objective extension \citep{zhong2024provable} fails, since their approach heavily depends on the linear structure of the $r(x,y)$. Thus, it is unclear whether we can get a reward-free algorithm for the non-linear aggregation reward function.

% TODO: 
% 1. Define the multi-group setting.
% 2. Discuss why the DPO fails. How solving DPO is hard for p not equals to 1.




\section{Algorithms for Multiple Objectives with Linear Aggregation}\label{sec:moalg}
In this section, we consider the simplest setting where the reward function is a linear aggregation, i.e. $
r(x,y) = \sum_{i=1}^m d_i r_i^*(x,y)$, where $d \in \RR^m$ is called the \textit{direction.} In fact, the linear aggregation can be viewed as projecting the reward vector onto a specific direction $d$.  As we will show later, this will become an essential sub-problem in our final algorithm for non-linear aggregation.

%\aarti{clarify notation - d is same as $\alpha$, define $M_i$ and the dataset ingredients}\nuoya{Actually $d$ is the direction while $\alpha$ is the importance weight. Should we write $\alpha$ instead of $d$ in this section?} \aarti{no, but clarify what direction means? not introduced before}
Given the dataset $\cD_i= \{x^j, (y_w^j,y_l^j)\}_{j \in [M]}$ containing $M$ data points for objective $i$, we provide offline and online algorithms to learn the optimal policy with respect to multiple objectives in a consistent way.  %For each $i \in [m]$, we assume there is a linear structure of reward function, namely, $r_i^{\theta_i} = \theta_i^\top \phi_i(x,y)$, where $\|\phi_i(x,y)\|_2 \le 1$ is known. 
  Now we aim to minimize the negative log-likelihood loss of preference data 
$$L_i(\theta_i) = -\sum_{(x,y_w,y_l) \in \cD_i}\log (\sigma(r_i^{\theta_i}(x, y_w)-r_i^{\theta_i}(x, y_l)))$$
%\aarti{drop the $t$ superscript on loss unless also including on RHS}
for each objective $i$. 
Following \citep{cen2024value}, we can  refine our estimation of the reward by adding an additional exploration term $\max_\pi J(r^\theta, d, \pi) = \max_\pi $ $\EE_\pi[\sum_{i=1}^m d_i(r_i^{\theta_i}-\beta \DD_{\mathrm{KL}}(\pi \|\pi_{\mathrm{ref}}))] $, which represents the optimism/pessimism principle of the online/offline learning process. To be more specific, for the offline and online setting,  LLM learns the $\theta_{\mathrm{offline}}$ and $\theta_{\mathrm{online}}$ respectively by 
\begin{small}\begin{align}
\theta_{\mathrm{offline}} = \argmax_{\theta_1,\cdots, \theta_m} \left({-\max_\pi J(r^\theta, d, \pi) - \sum_{i=1}^m \eta L_i(\theta_i)}\right)\label{eq:estimate theta offline}
\\\theta_{\mathrm{online}}=\argmax_{\theta_1, \cdots, \theta_m}\left(\max_\pi J(r^\theta, d, \pi)-\sum_{i=1}^m \eta L_i(\theta_i)\right),\label{eq:estimate theta online}
\end{align}
\end{small}
where we use a single parameter $\theta$ to refer the set $\{\theta_i\}_{i \in [m]}.$ The difference lies in the optimism and the pessimism principle. In the offline setting, we subtract the exploration term to avoid over-optimization \citep{cen2024value,liu2024provably} while in the online setting, we add the exploration term to encourage the model to explore \citep{cen2024value}.
Then, the LLM executes the greedy policy $\pi^\theta = \arg\max_{\pi} J(r^\theta, d, \pi)$ to generate the response and receives the human feedback $(y_w, y_l)$.  We called the algorithm \textbf{M}ulti-\textbf{O}bjective \textbf{P}rojection (MOP), and the pseudocode for online setting is shown in Algorithm \ref{alg: vpo-fl}. (There is no Line 4 and the output only has $\theta$ for the offline setting.)
\begin{algorithm}[H] 
     \begin{algorithmic}[1] 
         \caption{MOP-Reward Based (RB)} 
         \label{alg: vpo-fl} 
         \STATE \textbf{Input}: Direction $d,$ dataset $\{\cD_i\}_{i \in [m]}$, $\eta,\beta$.
         \STATE Calculate $\theta_{\mathrm{
         offline}}$ by Eq.~\eqref{eq:estimate theta offline} or $\theta_{\mathrm{online}}$ by Eq.~\eqref{eq:estimate theta online}.
             \label{line:rlhf-obj-general}
            \STATE Execute $\pi^\theta = \arg\max_{\pi} J(r^\theta_1, r^\theta_2,\cdots, r_m^\theta, d, \pi)$.
            \STATE Given the prompt $x$, Generate two responses $y_1,y_2\sim \pi$, and get a preference $y = (y_w, y_l)$.
            \STATE \textbf{Output:} Data point $D= \{x, (y_w, y_l)\}$ and $\theta.$
            % \ELSIF{Offline} \STATE \textbf{Output:} $\theta.$
            % \ENDIF 
     \end{algorithmic} 
\end{algorithm} 

% \begin{theorem}\label{thm:online-vpofl}
%     With probability at least $1-\delta$, the regret is bounded by 
%     \begin{align*}
%         \text{Reg}(T) \le \widetilde{\cO}(\mathrm{poly}(\exp(1/\beta), m,d, \kappa,\log (|\cR|T/\delta))\sqrt{T}),
%     \end{align*}
%     where $\kappa = \sup_{x,y}\frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}$. The term $\log (|\cR|)$ can also be replaced by $1/T$-covering number $\log(\cN(\cR, 1/T)) = \cO(md\log(1/\delta)).$
% \end{theorem}

% The theorem above shows that even if the weight is unknown, and we can only receive one comparison each time, we can also estimate the importance weights and derive a no-regret algorithm. 

The computational cost of Algorithm \ref{alg: vpo-fl} mainly lies on Line 2. In fact, it needs to learn multiple reward functions directly, and then get the estimation of the optimal policy, which requires a joint optimization subprocedure. In the following, we consider the reward-free version of Algorithm \ref{alg: vpo-fl}.  

%We will further clarify the computational cost in this step.
\paragraph{Reward-Free Modification}
We now show that Algorithm \ref{alg: vpo-fl} can be easily adapted to a reward-free version. We mainly consider the online setting since the offline setting is similar. 
Denote $\pi^\theta = \argmax_\pi J(r^\theta, d, \pi).$ By the same derivation in \citep{cen2024value}, we can get
\begin{align*}
    &J(r^\theta, d, \pi)=C-\beta \EE_{x\sim \rho, y\sim\pi_{\mathrm{base}}}\left[\log \frac{\pi^\theta(y\mid x)} { \pi_{\mathrm{ref}}(y\mid x)}\right],
\end{align*}
%\aarti{where $C$ is a constant, also define $\pi_{base}$}
where $C$ and $\pi_{\mathrm{base}}$ are the constant and the baseline policy in Eq.~\eqref{eq:theta base policy},  $\pi_{\theta_i}$ is the policy for objective $i$ and $\pi^\theta \propto \pi_{\mathrm{ref}}(y\mid x) \cdot \prod_{i=1}^m \left(\pi_{\theta_i}(y\mid x)\right)^{d_i}$ is the optimal policy for linear aggregation. The detailed derivation above will be provided in Appendix \ref{app:derivation}.
By the derivation in \citep{rafailov2024direct}, you can further get the reward-free version of Eq.~\eqref{eq:estimate theta online} as 
\begin{small}
\begin{align}
    \theta &= \argmin_{\theta}\Bigg\{ \beta \EE_{ \pi_{\mathrm{base}}}\log \pi^\theta(y\mid x)-\eta \sum_{i=1}^m \ell(\cD_i,\theta_i)\Bigg\}\label{eq:rfupdate}\end{align}
    \end{small}
    where 
    $\ell(\cD_i,\theta_i) = \sum_{(x,y_w,y_l) \in \cD_i}\log \sigma\Big(\beta \log\frac{\pi_{\theta_i}(y_w\mid x)}{ \pi_{\mathrm{ref}}(y_w\mid x)} - \beta \log\frac{ \pi_{\theta_i}(y_l\mid x)}{ \pi_{\mathrm{ref}}(y_l\mid x)}\Big)$ is the reward-free loss function, and the expectation $\EE_{\pi}[\cdot]$ means $\EE_{x\sim \rho, y\sim \pi(\cdot \mid x)}[\cdot]$. 
\begin{algorithm}[H] 
     \begin{algorithmic}[1] 
         \caption{MOP-Reward Free (RF) (Online Version)} 
         \label{alg: vpo-rf} 
         \STATE \textbf{Input}: Direction $d,$ dataset $\{\cD_i\}_{i \in [m]}$, $\eta,\beta$.
         \STATE Calculate $\theta_{\mathrm{online}}\in \RR^m$ by Eq.~\eqref{eq:rfupdate} and $\pi = \pi^{\theta}.$
            \STATE Given the prompt $x$, Generate two responses $y_1,y_2\sim \pi$, and get a preference $y = (y_w, y_l)$. 
            \STATE \textbf{Output:} Data point $D = \{x, (y_w, y_l)\}$ 
            %\aarti{to ${\cal D}_i$}\nuoya{This is a subprocedure which returns the data point. The adding process is in Line 6 Algorithm \ref{alg: vpo-fl-general}} 
            and $\theta.$
     \end{algorithmic} 
\end{algorithm}
The Eq.~\eqref{eq:rfupdate} involves an optimization problem on $\theta$, which is a complicated joint optimization since $\theta$ refers to $m$ parameter $\theta_1, \cdots, \theta_m$. In Appendix \ref{app:derivation}, we further study the computational cost of Eq.~\eqref{eq:rfupdate}, showing that the gradient descent update rule can be easily computed once the expectation of the score function is available. 
%\nuoya{TODO: Complete the Appendix for this section.}
% \begin{proof}
% \begin{align*}
%     &\nabla_{\theta_1}\left(-\beta \EE_{x\sim \rho, y\sim\pi_{\mathrm{base}}}[\log \pi^\theta(y\mid x)]\right)-\eta \nabla_{\theta_1}\sum_{i=1}^m \ell(\cD_i, \theta_i)\\
%     &=-\beta \underbrace{\EE_{x\sim \rho, y\sim\pi_{\mathrm{base}}}[\nabla_{\theta_1}\log \pi^\theta(y\mid x)]}_{\text{(a)}}-\underbrace{\eta \nabla_{\theta_1}\ell(\cD_1, \theta_1)}_{\text{(b)}}.
% \end{align*}
% Term (b) in the last line is the gradient of log-likelihood loss that appears in classical reward-free algorithm like DPO. For term (a), note that \begin{align*}\pi^\theta & \propto \pi_{\mathrm{ref}}(y \mid x) \cdot \exp\left(\sum_{i=1}^m \beta d_i r_i^{\theta_i}(x,\cdot)\right)\\& = \pi_{\mathrm{ref}}^{1-\|d_i\|_1}(y\mid x)\cdot \prod_{i=1}^m (\pi^{\theta_i})^{d_i}(y\mid x).\end{align*} Hence, denote $s(\theta,\pi) = \EE_\pi[\nabla_\theta \log\pi^\theta(y\mid x)]$ is the expectation of the score function, we can then derive that 
% \begin{align*}
%     \text{(a)}= \beta d_1 \left(s(\theta_1, \pi_{\mathrm{base}}) - s(\theta_1, \pi^\theta)\right).
% \end{align*}
% Hence, the update rule can be efficiently computed as long as the score function is available, which commonly appears in previous RL algorithms such as REINFORCE.
% \end{proof}
% Thus, if the learning rate is $\xi>0$, the gradient descent update rule of $\theta_1$ is 
% \begin{align*}\theta_1^t &= \theta_1^{t-1}-\xi(\beta d_1(\EE_{\pi_{\mathrm{base}}}[\phi_1(x,y)] - \EE_{\pi_r^{\theta^{t-1}}}[\phi_1(x,y)]) \\&\qquad - \eta \nabla_{\theta_1^{t-1}}L_1^t(\theta_1^{t-1})).\end{align*}
% Also, we can write it as a reward-free objective as:


%TODO: need to polish

% \section{Negative Results about Only Receiving the Total Mixed Comparison}

\section{General Algorithm for Preference Aggregation}\label{sec:pref aggregation}
In this section, we introduce general offline and online algorithms that work for both linear and non-linear preference aggregation, and provide their theoretical guarantees. Both algorithms transform the non-linear aggregation into a series of linear aggregation sub-problem, using Algorithm \ref{alg: vpo-fl} and \ref{alg: vpo-rf} as their core sub-procedures. %We will first define the target set and show why this is a more general case.

\subsection{Offline Algorithm}

Now we introduce our algorithm \textbf{M}ulti-\textbf{O}bjective \textbf{P}rojection \textbf{O}ptimization (MOPO), which follows from the competitive RL with Blackwell-approachability literature \citep{yu2021provably}. We receive the offline data set $\cD =\{\cD_{i}\}_{i \in [m]}$ which contains $M$ data points $\cD_i$ for each objective $i$. 
%Each data point is sampled from distribution $x^j\sim \rho, y_w^j, y_l^j\sim \pi_f(\cdot \mid x^j)$ with exploration policy $\pi_f.$ 
The algorithm learns the reward or optimizes the policy directly from the offline data. 
Our algorithm contains $T$ iterations. In each iteration $t$, we first project the reward vector on the direction $d^t \in \RR^m$ defined in the last iteration, i.e. $r(x,y) = \sum_{i=1}^m d_i^t r_i(x,y)$, and then using the sub-procedure in the previous section to find the estimated parameter $\theta^t$ and determine the corresponding policy $\pi^t$. Finally, we derive the estimated expected reward vector $V^t \in \RR^m$ as $(V^t)_i=\EE_{\pi^t}[r_i^{\theta^t}(x,y) - \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})]$, and calculate the averaged reward vector as $\overline{V}^t = \frac{1}{t}\sum_{j=1}^t V^j.$ Finally, the direction is updated based on the projection of the estimated point $\overline{V}^t$ onto the target set, guided by either the consensus problem or the malfare function minimization problem. The pseudocode is in Algorithm \ref{alg: vpo-fl-offline}.

The key component of our algorithm is the direction calculation in each iteration. Intuitively, the algorithm aims to optimize the reward to guide the expected reward vector toward the target set as effectively as possible. Suppose the target set is $W$, the direction can be calculated by 
$d^{t+1} = \mathrm{Proj}(W,\overline{V}^t) = \frac{\Pi_W(V)-V}{\|\Pi_W(V)-V\|}.$
For the consensus problem, we can substitute into $W = \bigcap_{n=1}^N W^{(n)}$ and get 
\begin{equation}\label{eq:dir consensus}
    d^{t+1} = \mathrm{Proj}\left(\bigcap_{n=1}^N W^{(n)}, \overline{V}^t\right).
\end{equation}
For the malfare function minimization problem, we can first calculate the projection to each target set $W^{(n)}$ and then aggregate them as 
\begin{small}
\begin{equation}\label{eq:dir malfare}
    d^{t+1}=\sum_{n=1}^N \mathrm{Proj}\left(W^{(n)}, \overline{V}^t\right)\cdot \frac{\zeta_n\|W^{(n)}- \overline{V}^t\|_2^{2q-1}}{\left(\sum_{n=1}^N \zeta_n\|W^{(n)}- \overline{V}^t\|_2^{2q}\right)^{\frac{2q-1}{2q}}}.
\end{equation}
\end{small}
\begin{algorithm}[t]
     \begin{algorithmic}[1]
         \caption{MOPO-Offline}
         \label{alg: vpo-fl-offline}
         \STATE \textbf{Initial}: Dataset $\cD=\{\cD_i\}_{i \in [m]}$, $\{W^{(n)}\}_{n \in [N]}$, $\eta, \beta.$ \FOR{$t=1,2,\cdots,T$}
            \STATE Collect $\theta^t$ by MOP-RB$(\overline{d^t}, \cD)$ or MOP-RF$(\overline{d^t},\cD)$. Get the corresponding policy $\pi^t = \pi^{\theta^t}$.
            % \STATE Calculate $\theta^t\in \RR^m$ such that $\theta^t=\argmax_{\theta}\left(\max_\pi J(\hat{r}_1^t, \hat{r}_2^t,\cdots, \hat{r}_m^t, d^{t}, \pi)-\sum_{i=1}^m \eta L^{t}_i(\theta_i)\right)$.
            %  \label{line:rlhf-obj-general}
            % \STATE Execute $\pi^t = \arg\max_{\pi} J(\hat{r}_1^t, \hat{r}_2^t,\cdots, \hat{r}_m^t, d^{t}, \pi)$.
            % \STATE Generate $y^t = (y_1^t, y_2^t)$ based on the policy $\pi^t$.
            %\STATE Collect data $D_t$ with $D_t  = (x^t, y^t, I^t, p^t, \pi^t),$ and update $\cD = \cD \cup D_t.$
            \STATE Calculate the point $V^t = \EE_{\pi^t}[r_i^{\theta^t}(x,y) - \beta\DD_{\mathrm{KL}}$ $(\pi^t\| \pi_{\mathrm{ref}})]= C-\beta \EE_{y\sim \pi_{\mathrm{base}}}\left[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right] $ $+ \beta \EE_{y\sim \pi^t}\left[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi^t(y\mid x)}\right]$, and $\overline{V}^t = \frac{t-1}{t}\overline{V}^{t-1} + V^t.$
            \STATE Calculate the direction $d^{t+1}$ by Eq. \eqref{eq:dir consensus} or Eq.~\eqref{eq:dir malfare}, and calculate $\overline{d^{t+1}} = \frac{d^{t+1}}{\|d^{t+1}\|_1}$.
         \ENDFOR
         \STATE \textbf{Return} $\tilde{\pi}^T = \frac{1}{T}\sum_{t=1}^T \pi^t.$
     \end{algorithmic}
\end{algorithm}

Note that if we apply MOPO with $p=1$, it reduces to the classical MORLHF algorithm. This is because the direction $d^t = \mathrm{Proj}(V^t, W_{1,c}^\alpha)=\alpha$ for each $t$ as long as $c$ is large. However, for $p\neq 1,$ MOPO solves the non-linear aggregation maximization problem by transforming into a series of subproblems, in which each subproblem only contains the linear aggregation and can be easily solved using any previous algorithm. Thus, MOPO serves as a general framework for MORLHF with non-linear aggregation. Moreover, suppose we use MOP-RF for each subproblem, MOPO is also a reward-free algorithm since the current reward vector can be computed as 
$$(V^t)_i = \EE_{\pi^t}[r_i^{\theta^t}(x,y) - \beta\DD_{\mathrm{KL}}(\pi^t\| \pi_{\mathrm{ref}})] = C-\beta \EE_{y\sim \pi_{\mathrm{base}}}\left[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right] + \beta \EE_{y\sim \pi^t}\left[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi^t(y\mid x)}\right].$$ You can See Appendix \ref{app:expected reward vector derivation} for the derivation.
Now we provide theoretical guarantee of Algorithm \ref{alg: vpo-fl-offline}. The following result shows that MOP-offline can learn the optimal policy well if the offline dataset $\cD$ has sufficient coverage for each objective.

\begin{theorem}[Consensus Problem]\label{thm:offline}
    Let $\eta = 1/\sqrt{M}$ and $\Sigma_{\cD_i} = \frac{1}{M}\sum_{(x,y_w,y_l) \in \cD_i} $ $(\phi(x,y_{w})-\phi(x,y_l))(\phi(x,y_{w})-\phi(x,y_l))^\top$ be the empirical  covariance matrix of the data for objective $i$. We consider the consensus problem that $W = \bigcap_{n=1}^N W^{(n)}$ and calculate the direction using Eq.~\eqref{eq:dir consensus}.   Define $D(\pi) = d(S(\pi), \cap_{n=1}^N W^{(n)})$. For $\delta \in (0,1)$, with probability at least $1-\delta$, we have
    \begin{align*}
        D(\tilde{\pi}^T )- D(\pi^*) \le \frac{m^{3/2}\sqrt{d}}{\sqrt{M}}\cdot \widetilde{\cO}\left(\mathrm{poly}\left(e^{B'}, \left(\min_i\lambda_{\min}(\Sigma_{\cD_i}) + \frac{1}{M}\right )^{-1}\right)\right)+ \widetilde{\cO}\left(\frac{B\sqrt{m}}{\sqrt{T}}\right). 
    \end{align*}
    
\end{theorem}
%\nuoya{N only appears in $\log N$, which is omitted in $\widetilde{\cO}$}
The above theorem shows that the final gap of returned policy depends on the coverage term $\min_i \lambda_{\min}(\Sigma_{\cD_i})$ of the offline dataset and the number of iterations $T$.  As $T$ increases, we achieve a standard convergence rate of $\widetilde{\cO}(1/\sqrt{M})$, which is standard in prior offline RL algorithms \citep{jin2021bellman,liu2020provably}.
 We also provide the theoretical guarantee for malfare function minimization. 
\begin{theorem}[Malfare] \label{thm:malfareoffline}
    With the same definitions and conditions in Theorem \ref{thm:offline}, we consider the malfare function minimization problem with an integer\footnote{We focus on the integer case to simplify the proof.} exponential parameter $q\in \NN^+$ and use Eq.~\eqref{eq:dir malfare} for the direction. Define $D_q(\pi) = \sqrt[2q]{\sum_{n=1}^N \zeta_nd^{2q}(S(\pi), W^{(n)})}$. For $\delta \in (0,1)$, with probability at least $1-\delta$ we have 
    \begin{align*}
        &D_q(\tilde{\pi}^T )- D_q(\pi^*) \\&\le \frac{Nm^{3/2}\sqrt{d}}{\sqrt{M}}\cdot \widetilde{\cO}\Bigg(\mathrm{poly}\Bigg(e^{B'}, \min_i\lambda_{\min}\left(\Sigma_{\cD_i} + \frac{1}{M}\right)^{-1}, (\min_{n \in [N]}\zeta_n)^{-1/2q}\Bigg)\Bigg) + \widetilde{\cO}\left( B\sqrt{m}T^{-1/2q}\right). 
    \end{align*}
\end{theorem}
% As $T$ increases, we achieve a standard convergence rate of $\widetilde{\cO}(1/\sqrt{M})$, which is standard in prior offline RL algorithms \citep{jin2021bellman,liu2020provably}.
% \nuoya{The computational cost still appears to be high due to the large number of iterations. In practice, we can mitigate this by either reducing the number of iterations or computing a single gradient update per iteration \citep{guo2024direct}. In our experiments, we set the number of iterations to 7, striking a balance between computational efficiency and performance.}

%\nuoya{Write till here. Have already completed the proof.}
\begin{algorithm}[t]
     \begin{algorithmic}[1]
         \caption{VPO-objective-learning-general}
         \label{alg: vpo-fl-general}
         \STATE \textbf{Initial}: $\cD = \emptyset$. parameter $\{p^{(n)}, c^{(n)}\}_{n \in [N]}$, $\eta, \beta$.\FOR{$t=1,2,\cdots,T$}
         \STATE Calculate $\tilde{\theta}_i^t = \arg\min_\theta  L_i^t(\theta)$ for all $i \in [m]$.
            \STATE Estimate $\hat{\alpha}^{t,(n)} = \{\hat{\alpha}_i^{t,(n)}\}_{i \in [m]}$ for each $n \in[N]$ by MLE with $\cD$ and $\{\tilde{\theta}^t_i\}_{i \in [m]}$ by Eq.~\eqref{eq:MLE alpha}
            \STATE Calculate $W^{t,(n)}=W^{\alpha^{t,(n)}}_{p^{(n)}, c^{(n)}}$ where $\alpha^{t,(n)} = \frac{t-1}{t}\alpha^{t-1,(n)} + \frac{1}{t}\hat{\alpha}^{t,(n)}$ for each $n \in [N].$
            \STATE Collect $D_t,\theta^t$ by MOP-RB$(\overline{d^t}, \cD)$ or MOP-RF$(\overline{d^t}, \cD)$, and update $\cD = \cD \cup D_t.$ 
            % \STATE Calculate $\theta^t\in \RR^m$ such that $\theta^t=\argmax_{\theta}\left(\max_\pi J(\hat{r}_1^t, \hat{r}_2^t,\cdots, \hat{r}_m^t, d^{t}, \pi)-\sum_{i=1}^m \eta L^{t}_i(\theta_i)\right)$.
            %  \label{line:rlhf-obj-general}
            % \STATE Execute $\pi^t = \arg\max_{\pi} J(\hat{r}_1^t, \hat{r}_2^t,\cdots, \hat{r}_m^t, d^{t}, \pi)$.
            % \STATE Generate $y^t = (y_1^t, y_2^t)$ based on the policy $\pi^t$.
            %\STATE Collect data $D_t$ with $D_t  = (x^t, y^t, I^t, p^t, \pi^t),$ and update $\cD = \cD \cup D_t.$
            \STATE Calculate the point $V^t = \EE_{\pi^t}[r_i^{\theta^t}(x,y) - \beta\DD_{\mathrm{KL}}$ $(\pi^t\| \pi_{\mathrm{ref}})]= C-\beta \EE_{y\sim \pi_{\mathrm{base}}}[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}] $ $+ \beta \EE_{y\sim \pi^t}[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi^t(y\mid x)}]$, and $\overline{V}^t = \frac{t-1}{t}\overline{V}^{t-1} + V^t.$
            \STATE Calculate the direction $d^{t+1}$ by Eq.~\eqref{eq:dir consensus} or Eq.~\eqref{eq:dir malfare}, and calculate $\overline{d^{t+1}} = \frac{d^{t+1}}{\|d^{t+1}\|_1}$.
         \ENDFOR
         \STATE \textbf{Return} $\tilde{\pi}^T = \frac{1}{T}\sum_{t=1}^T \pi^t.$
     \end{algorithmic}
\end{algorithm}
\subsection{Online Algorithm}
Now we provide the online version of MOPO, which is similar to the offline setting. 
%Similarly, at each round, we first project the reward vector onto the previous direction $d^t$ to compute the total reward. Then, we estimate the reward parameter via MLE with an exploration term, select the optimal policy $\pi^t$, collect data, and compute the next direction.
 The main difference is the adoption optimism principle (Eq.~\eqref{eq:estimate theta online}) rather than the pessimism principle (Eq.~\eqref{eq:estimate theta offline}). Additionally, the dataset is collected incrementally online, and we also estimate the importance weight $\alpha$ instead of assuming it is known.
 
 % \paragraph{Importance Weight Estimation} 
Additionally, rather than assuming the weight is known, we estimate it based on the frequency with which humans report the objective. This method also works offline by using the frequency of related data in the dataset. At each round $t$, given a prompt $x^t \sim \rho$ and two responses $y_1$ and $y_2$, each group $n$ identifies an objective $I^{t,(n)} \in [m]$ showing the greatest difference and provides preference feedback $(y_w^{t,(n)}, y_l^{t,(n)})$ on that objective. The model collects the data $(x^t,y_w^{t,(n)},y_l^{t,(n)},I^{t,(n)})$ into $\cD^{(n)}$ for all group $n$. Next, we model how humans select the objective index. For responses $y_w$ and $y_l$, the gap on objective $i$ is quantified as $|\alpha_i \cdot (r_i(x, y_w) - r_i(x, y_l))|$, with the selection following a softmax distribution:
\begin{align*}
   \PP( I\mid \alpha, r^*,x,y_w,y_l) \propto \exp(\alpha_i\cdot |r_i^*(x,y_w)-r_i^*(x,y_l)|). 
\end{align*}
% \begin{algorithm}[t]
%      \begin{algorithmic}[1]
%          \caption{VPO-objective-learning-general}
%          \label{alg: vpo-fl-general}
%          \STATE \textbf{Initial}: $\cD = \emptyset$. parameter $\{p^{(n)}, c^{(n)}\}_{n \in [N]}$\FOR{$t=1,2,\cdots,T$}
%          \STATE Calculate $\tilde{\theta}_i^t = \arg\min_\theta  L_i^t(\theta)$ for all $i \in [m]$.
%             \STATE Estimate $\hat{\alpha}^{t,(n)} = \{\hat{\alpha}_i^{t,(n)}\}_{i \in [m]}$ for each $n \in[N]$ by MLE with $\cD$ and $\{\tilde{\theta}^t_i\}_{i \in [m]}$ by Eq.~\eqref{eq:MLE alpha}
%             \STATE Calculate $W^{t,(n)}=W^{\alpha^{t,(n)}}_{p^{(n)}, c^{(n)}}$ where $\alpha^{t,(n)} = \frac{t-1}{t}\alpha^{t-1,(n)} + \frac{1}{t}\hat{\alpha}^{t,(n)}$ for each $n \in [N].$
%             \STATE Collect $D_t,\theta^t$ by MOP-RB$(\overline{d^t}, \cD)$ or MOP-RF$(\overline{d^t}, \cD)$, and update $\cD = \cD \cup D_t.$ 
%             % \STATE Calculate $\theta^t\in \RR^m$ such that $\theta^t=\argmax_{\theta}\left(\max_\pi J(\hat{r}_1^t, \hat{r}_2^t,\cdots, \hat{r}_m^t, d^{t}, \pi)-\sum_{i=1}^m \eta L^{t}_i(\theta_i)\right)$.
%             %  \label{line:rlhf-obj-general}
%             % \STATE Execute $\pi^t = \arg\max_{\pi} J(\hat{r}_1^t, \hat{r}_2^t,\cdots, \hat{r}_m^t, d^{t}, \pi)$.
%             % \STATE Generate $y^t = (y_1^t, y_2^t)$ based on the policy $\pi^t$.
%             %\STATE Collect data $D_t$ with $D_t  = (x^t, y^t, I^t, p^t, \pi^t),$ and update $\cD = \cD \cup D_t.$
%             \STATE Calculate the point $V^t = \EE_{\pi^t}[r_i^{\theta^t}(x,y) - \beta\DD_{\mathrm{KL}}$ $(\pi^t\| \pi_{\mathrm{ref}})]= C-\beta \EE_{y\sim \pi_{\mathrm{base}}}[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}] $ $+ \beta \EE_{y\sim \pi^t}[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi^t(y\mid x)}]$, and $\overline{V}^t = \frac{t-1}{t}\overline{V}^{t-1} + V^t.$
%             \STATE Calculate the direction $d^{t+1}$ by Eq.~\eqref{eq:dir consensus} or Eq.~\eqref{eq:dir malfare}, and calculate $\overline{d^{t+1}} = \frac{d^{t+1}}{\|d^{t+1}\|_1}$.
%          \ENDFOR
%          \STATE \textbf{Return} $\tilde{\pi}^T = \frac{1}{T}\sum_{t=1}^T \pi^t.$
%      \end{algorithmic}
% \end{algorithm}
Then if we define the likelihood function as 
$$\LL(\alpha, \cD^{(n)}, \theta) = \sum_{(x,y_w,y_l,I) \in \cD^{(n)}}\PP(I\mid\alpha, x, y_w, y_l, r^\theta),$$ we can estimate the importance weight vector for each group by MLE as
\begin{align}\label{eq:MLE alpha}
    \hat{\alpha}^{t,(n)}=\argmax_{\alpha \in \Delta_{m-1}} \LL(\alpha, \cD^{(n)}, \tilde{\theta}^t),
\end{align}
where we use an estimated reward parameter $\tilde{\theta}^t$ to approximate $\theta^*$. 
% \begin{algorithm}[t]
%      \begin{algorithmic}[1]
%          \caption{VPO-objective-learning-general}
%          \label{alg: vpo-fl-general}
%          \STATE \textbf{Initial}: $\cD = \emptyset$. parameter $\{p^{(n)}, c^{(n)}\}_{n \in [N]}$, $\eta, \beta$.\FOR{$t=1,2,\cdots,T$}
%          \STATE Calculate $\tilde{\theta}_i^t = \arg\min_\theta  L_i^t(\theta)$ for all $i \in [m]$.
%             \STATE Estimate $\hat{\alpha}^{t,(n)} = \{\hat{\alpha}_i^{t,(n)}\}_{i \in [m]}$ for each $n \in[N]$ by MLE with $\cD$ and $\{\tilde{\theta}^t_i\}_{i \in [m]}$ by Eq.~\eqref{eq:MLE alpha}
%             \STATE Calculate $W^{t,(n)}=W^{\alpha^{t,(n)}}_{p^{(n)}, c^{(n)}}$ where $\alpha^{t,(n)} = \frac{t-1}{t}\alpha^{t-1,(n)} + \frac{1}{t}\hat{\alpha}^{t,(n)}$ for each $n \in [N].$
%             \STATE Collect $D_t,\theta^t$ by MOP-RB$(\overline{d^t}, \cD)$ or MOP-RF$(\overline{d^t}, \cD)$, and update $\cD = \cD \cup D_t.$ 
%             % \STATE Calculate $\theta^t\in \RR^m$ such that $\theta^t=\argmax_{\theta}\left(\max_\pi J(\hat{r}_1^t, \hat{r}_2^t,\cdots, \hat{r}_m^t, d^{t}, \pi)-\sum_{i=1}^m \eta L^{t}_i(\theta_i)\right)$.
%             %  \label{line:rlhf-obj-general}
%             % \STATE Execute $\pi^t = \arg\max_{\pi} J(\hat{r}_1^t, \hat{r}_2^t,\cdots, \hat{r}_m^t, d^{t}, \pi)$.
%             % \STATE Generate $y^t = (y_1^t, y_2^t)$ based on the policy $\pi^t$.
%             %\STATE Collect data $D_t$ with $D_t  = (x^t, y^t, I^t, p^t, \pi^t),$ and update $\cD = \cD \cup D_t.$
%             \STATE Calculate the point $V^t = \EE_{\pi^t}[r_i^{\theta^t}(x,y) - \beta\DD_{\mathrm{KL}}$ $(\pi^t\| \pi_{\mathrm{ref}})]= C-\beta \EE_{y\sim \pi_{\mathrm{base}}}[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}] $ $+ \beta \EE_{y\sim \pi^t}[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi^t(y\mid x)}]$, and $\overline{V}^t = \frac{t-1}{t}\overline{V}^{t-1} + V^t.$
%             \STATE Calculate the direction $d^{t+1}$ by Eq.~\eqref{eq:dir consensus} or Eq.~\eqref{eq:dir malfare}, and calculate $\overline{d^{t+1}} = \frac{d^{t+1}}{\|d^{t+1}\|_1}$.
%          \ENDFOR
%          \STATE \textbf{Return} $\tilde{\pi}^T = \frac{1}{T}\sum_{t=1}^T \pi^t.$
%      \end{algorithmic}
% \end{algorithm}
Before we present our results, we assume there is a gap between the reward obtained by following the optimal policy $\pi^*$ and the reference  policy $\pi_{\mathrm{ref}}$. This gap is reasonable since the expected reward should be improved after fine-tuning.
%This assumption allows us to accurately estimate the importance weight $\alpha$, as the error in $\alpha$ depends on the expected reward gap.
 \begin{assumption}\label{assum:gap} There exists a constant $\gamma>0$ such that 
     $$\min_{i \in [m]}\EE_{x\sim \rho, y_1\sim \pi^*, y_2\sim \pi_{\mathrm{ref}}}|r_i^*(x,y_1) - r_i^*(x,y_2)|\ge \gamma.$$
 \end{assumption}
\noindent The following theorems show that Algorithm \ref{alg: vpo-fl-general} is a no-regret online algorithm that can converge to the optimal policy for the consensus problem and social malfare minimization problem, with importance weight estimation. 
 \begin{theorem}[Consensus]\label{thm:online}
     For the consensus problem, suppose the Assumption \ref{assum:gap} holds and the group $n$ has parameter $p^{(n)}$ and $c^{(n)}.$, then if we use  Eq.~\eqref{eq:dir consensus} to calculate the direction, for $\delta \in (0,1)$ and $\eta = 1/\sqrt{T}$, with probability at least $1-\delta$ we have 
     \begin{align*}
         D(\tilde{\pi}^T)-D(\pi^*) \le \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m,N,e^B, d, \log(1/\delta), \kappa, (\min_{n \in [N]}p^{(n)})^{-1}), B_1) \cdot \widetilde{\cO}(1/\sqrt{T}),
     \end{align*}
     where $\tilde{\pi}^T = \frac{1}{T}\sum_{t=1}^T \pi^t$, and $ \kappa= \sup_{x,y}\frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}$, $B_1=2\sqrt{m}(B+\max_n c^{(n)})$ are constants.
 \end{theorem}

 \begin{theorem}[Malfare]\label{thm:malfare online}
With the same setting in Theorem \ref{thm:online}, if we consider the malfare function minimization problem with an integer exponential parameter $q\in \NN^+$ and uses Eq.~\eqref{eq:dir malfare} to compute the direction, then for $\delta \in (0,1)$ and $\eta = 1/\sqrt{T}$, with probability at least $1-\delta$ we have
\begin{align*}
    &D_q(\tilde{\pi}^T) - D_q(\pi^*) \\&\le \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m,N,e^B, d, \log(1/\delta),\kappa, B_1, (\min_{n \in [N]}p^{(n)})^{-1}, (\min_{n \in [N]}\zeta_n)^{-1/2q})\cdot \widetilde{\cO}(T^{-1/2q}),
\end{align*}
where $\tilde{\pi}^T = \frac{1}{T}\sum_{t=1}^T \pi^t$, and $ \kappa= \sup_{x,y}\frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}$, $B_1=2\sqrt{m}(B+\max_n c^{(n)})$ are constants.

\end{theorem}

 


% \section{Minimize the total distance}



% In the previous section we consider minimizing $d(S(\pi), \bigcap_{n=1}^N W_{p^{(n)}, c^{(n)}}^{\alpha^{(n)}})$, which can be regarded as the consensus achieveing algorithm. In this section we consider the malfare minimization setting, and the malfare function for the group $k \in [K]$ is $d^2(S(\pi), W_{p^{(n)}, c^{(n)}}^{\alpha^{(n)}})$. The total malfare minimization goal is 
% \begin{equation}
%     \min_{\pi}\left(\sum_{n=1}^N \left(d^2(S(\pi), W_{p^{(n)}, c^{(n)}}^{\alpha^{(n)}})\right)^q\right)^{1/q}.
% \end{equation}
% We first consider the fixed target setting, in which the target set for group $n \in [N]$ is $W_n^*.$ 
% Then, the direction in round $t+1$ is instead calculated by 
% \begin{align}d^{t+1} = \sum_{n=1}^N d^{t,(n)}\cdot \frac{d^{2q-1}(\overline{V}^t, W_n^*)}{\sqrt{\sum_{n=1}^N d^{2q}(\overline{V}^t, W_n^*)}},\label{eq: adjusted dir}\end{align}
% where $d^{t,(n)} = \mathrm{Projection-Update}(W^{\alpha^{t,(n)}}, \overline{V}^t)$ is the direction toward the target set of group $n$. 
% Eq.~\eqref{eq: adjusted dir} can be seemed as a linear combination of directions to each group.

% % The following theorem shows that we can minimize the sum of square of distances. 
% % \begin{theorem}\label{thm:welfare}
% %     Using algorithm \ref{alg: vpo-fl-general} with the direction calculated in Eq. \eqref{eq: adjusted dir}, we will have 
% %     \begin{align}
% %         \sqrt{\sum_{n=1}^N d^2(S(\tilde{\pi}^T), W_n^*)} - \sqrt{\min_{\pi}\left(\sum_{n=1}^N d^2(S(\pi), W_n^*)\right)}\le \cO(1/\sqrt{T}),
% %     \end{align}
% %     where $\tilde{\pi}^T = \frac{1}{T}\sum_{t=1}^T \pi^t.$
% % \end{theorem}
% % \begin{proof}
% %     The change mainly focus on the calculation of $\sum_{n=1}^N d^2(\overline{V}^t, W_n^*).$
% %     We have 
% %     \begin{align*}
% %         \sum_{n=1}^N d^2(\overline{V}^t, W_n^*)& = \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^T)\|^2\\
% %         & \le \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^{T-1})\|^2\\
% %         &=\sum_{n=1}^N\left(\frac{T-1}{T}\right)^2 d(\overline{V}^{T-1}, W^*_n)^2 + \frac{1}{T^2} \|V^T - \Pi_{W^*_n}(\overline{V}^{T-1})\|^2 \\
% %     &\qquad + \sum_{n=1}^N \frac{2(T-1)}{T^2}(\overline{V}^{T-1} - \Pi_{W^*_n} (\overline{V}^{T-1}))\cdot (V^T - \Pi_{W^*_n}(\overline{V}^{T-1}))\\
% %     &\le \sum_{n=1}^N\left(\frac{T-1}{T}\right)^2 d(\overline{V}^{T-1}, W^*_n)^2 + \frac{NB^2m}{T^2}\\
% %     &\qquad + \sum_{n=1}^N \frac{2(T-1)}{T^2}(\overline{V}^{T-1} - \Pi_{W^*_n} (\overline{V}^{T-1}))\cdot (V^T - \Pi_{W^*_n}(\overline{V}^{T-1}))
% %     \end{align*}


% % Thus by iteration we can have 
% % \begin{align*}
% %      T^2 \sum_{n=1}^N d(\overline{V}^T, W^*_n)^2 \le T\cdot B^2mN + \sum_{n=1}^N\sum_{t=1}^T 2(t-1) (\overline{V}^{t-1}-\Pi_{W^*_n}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*_n}(\overline{V}^{t-1}))
% % \end{align*}

% % The term \begin{align*}&\sum_{n=1}^N(\overline{V}^{t-1}-\Pi_{W^*_n}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*_n}(\overline{V}^{t-1}))\\
% % &=\sum_{n=1}^N d(\overline{V}^{t-1}, W_n^*)\cdot d_n^t \cdot ( \Pi_{W^*_n}(\overline{V}^{t-1})-V^t)\\
% % &= \sum_{n=1}^N d(\overline{V}^{t-1}, W_n^*)\cdot \left(J(r_1^*, \cdots, r_m^*, d^t_n, \pi^*) + d(S(\pi^*), W_n^*) - J(\hat{r}_1, \cdots, \hat{r}_m, d_n^t ,\pi^t)\right)\\
% % & \le  \sqrt{\sum_{n=1}^N d^2(\overline{V}^{t-1}, W_n^*)}\cdot\left(J(r_1^*, \cdots, r_m^*, d^t, \pi^*) - J(\hat{r}_1, \cdots, \hat{r}_m, d^t, \pi^t)+\sqrt{\sum_{n=1}^N d^2(S(\pi), W_n^*)}\right).\end{align*}
% % The last inequality uses the Cauchy's inequality and the definition of $d^t$.
% % \end{proof}



% % Also, if we extend it to the $p-$norm, i.e., $\sum_{n=1}^N d^p(\overline{V}^t, W_n^*)$, we can choose the direction by 
% % \begin{align}
% %     d^{t+1}=\sum_{n=1}^N \mathrm{Projection-Update
% %     }(W^{\alpha^t,(n)}, \overline{V}^t)\cdot \frac{d^{p-1}(\overline{V}^t, W_n^*)}{\left(\sum_{n=1}^N d^p(\overline{V}^{t-1}, W_n^*)\right)^{\frac{p-1}{p}}},\label{def:dir general p}
% % \end{align}
% % then we can get a similar result.
% \begin{theorem}\label{thm:generalp}
%     Using Algorithm \ref{alg: vpo-fl-general} with direction in Eq.~\eqref{def:dir general p}, if $q>1$, we can get 
%     \begin{align}
%         &\sqrt[q]{\sum_{n=1}^N d^q(S(\tilde{\pi}^T), W_n^*)} - \min_\pi \sqrt[q]{\sum_{n=1}^N d^q(S(\pi), W_n^*)}\nonumber\\ & \le \cO(T^{-1/2q}).\nonumber
%     \end{align}
% \end{theorem}
% % \begin{proof}

% % \begin{align}
% %     \sum_{n=1}^N d^p(\overline{V}^t, W_n^*) & = \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^T)\|^p\\
% %     & \le \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^{T-1})\|^p\\
% %     & = \sum_{n=1}^N \left\|\frac{T-1}{T}(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1})) + \frac{1}{T}(V^T - \Pi_{W_n^*}(\overline{V}^{T-1}))\right\|^p. \label{ineq:pnorm}
% % \end{align}
% % By the basic inequality, for $q>1$, we have 
% % $$(a+b)^q - a^q - b^q $$ is non-decreasing for both $a,b>0$. 
% % Then for the vector $x_n,y_n \in \RR^m$ with $x_n=(T-1)(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1})), y_n= (V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))$ and $p>2,$ we know $p/2>1$ and $\|x_n\| \le 2TB, \|y_n\| \le 2B$. Hence, 
% % \begin{align*}\|x_n+y_n\|^p &\le (\|x_n\|^2 + \|y_n\|^2 + 2\langle x_n,y_n\rangle )^{p/2}\le (\|x_n\|^2 + \|y_n\|^2)^{p/2} + 2^{p/2}\langle x_n,y_n\rangle (\|x_n\|^2+\|y_n\|^2)^{p/2-1}\\&\le \|x_n\|^p  + \|y_n\|^p + (2B)^p \cdot \left((T^2+1)^{p/2}-T^p - 1\right)+ 2^{p/2}\langle x_n,y_n\rangle (\|x_n\|^2+\|y_n\|^2)^{p/2-1}\\
% % &\le \|x_n\|^p  + \|y_n\|^p + (4B)^p \cdot \left(T^{p-2}\right)+ 2^{p/2}\langle x_n,y_n\rangle (\|x_n\|^2+\|y_n\|^2)^{p/2-1}\end{align*}
% %  we can further bound the inequality \eqref{ineq:pnorm} as 
% % \begin{align}
% %     T^p \sum_{n=1}^N d^p(\overline{V}^{T}, W_n^*) &\le  \sum_{n=1}^N \|x_n+y_n\|^p \\&\le (T-1)^p\sum_{n=1}^N d^p(\overline{V}^{T-1}, W_n^*) + n(2B)^p  + n(4B)^p \cdot T^{p-2} \\
% %     &\qquad  + 2^{p/2} (T-1)\sum_{n=1}^N (\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))(\|x\|^2 + \|y\|^2)^{p/2-1}.
% % \end{align}
% % Note that $\|y\| \le 2B,$ then $\langle x_n, y_n\rangle (\|x\|^2 + \|y\|^2)^{p/2-1} \le \max\{2^{p/2-1}\cdot \|x\|^{p-2}\cdot \langle x_n,y_n\rangle , (4B)^{p}\}\le 2^{p/2-1}\cdot \|x\|^{p-2}\cdot \langle x_n,y_n\rangle + (4B)^{p},$ then we can finally get 
% % \begin{align}
% % T^p \sum_{n=1}^N d^p(\overline{V}^{T}, W_n^*) &\le (T-1)^p \sum_{n=1}^N d^p(\overline{V}^{T-1}, W_n^*) + \\
% % &\qquad + 2^{p}(T-1)\sum_{n=1}^N (\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))d^{p-2}(\overline{V}^{T-1}, W_n^*)\\
% % &\qquad \qquad + n(4B)^p+ n(2B)^p + n(4B)^p T^{p-2}
% % \end{align}
% % Now for $p>2$, we have 
% % \begin{align}
% %     T^p \sum_{n=1}^N d^p(\overline{V}^{T}, W_n^*) &\le(T-1)^p \sum_{n=1}^N d^p(\overline{V}^{T-1}, W_n^*) + \cO(n(4B)^pT^{p-2}) \\&\qquad + 2^{p}(T-1)\sum_{n=1}^N (\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))d^{p-2}(\overline{V}^{T-1}, W_n^*).
% % \end{align}
% % Hence by the recursion, we can get 
% % \begin{align}
% %     &\sum_{t=1}^T \sum_{n=1}^N (t-1)(\overline{V}^{t-1}-\Pi_{W_n^*}(\overline{V}^{t-1}))(V^t-\Pi_{W_n^*}(\overline{V}^{t-1}))d^{p-2}(\overline{V}^{t-1}, W_n^*)\\
% %     &\le \sum_{t=1}^T \sum_{n=1}^N (t-1)d^{p-1}(\overline{V}^{t-1}, W_n^*)d_n^t\cdot (\Pi_{W_n^*}(\overline{V}^{t-1}) - V^t)\\
% %     &\le  \sum_{t=1}^T\sum_{n=1}^N (t-1)d^{p-1}(\overline{V}^{t-1}, W_n^*)\left(J(r_1^*, \cdots, r_m^*, d_n^t, \pi^*)+d(S(\pi^*), W_n^*) - J(\hat{r}_1, \cdots, \hat
% %     {r}_m), d_n^t, \pi^t)\right)\\
% %     &\le \sum_{t=1}^T(t-1)\left(\sum_{n=1}^N d^p(\overline{V}^{t-1}, W_n^*)\right)^{\frac{p-1}{p}} \cdot \left(J(r_1^*, \cdots, r_m^*, d^t, \pi^*)+ \sqrt[p]{\sum_{n=1}^N d^p(S(\pi^*), W_n^*)}- J(\hat{r}_1, \cdots, \hat
% %     {r}_m, d^t, \pi^t)\right)\\
% %     &\le \sum_{t=1}^T(t-1)\left(\sum_{n=1}^N d^p(\overline{V}^{t-1}, W_n^*)\right)^{\frac{p-1}{p}} \cdot \left(D_p(\pi^*) + \eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t)\right)
% % \end{align}

% % Let $S_T = \sqrt[p]{\sum_{n=1}^N d^p(\overline{V}^T, W_n^*)}$, then we can get 
% % \begin{align*}
% %     T S_T^p\le \cO(n(4B)^p) + 2^p \sum_{t=1}^T \frac{t-1}{T^{p-1}}S_{t-1}^{p-1}\cdot \left(D_p(\pi^*) + \eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t)\right).
% % \end{align*}
% % Define $A_t = D_p(\pi^*)+ \eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t),$ then we use the induction to show that there exists a constant $C_p$ such that 
% % \begin{align}
% %     S_{t} \le C_p\left(\frac{1}{t}\sum_{s=1}^t A_s + T^{-1/p}\right). 
% % \end{align}
% % In fact, it holds when $t = 1$. Now suppose it holds for $t=1,2,\cdots, T-1$, we have 
% % \begin{align*}
% %     T S_T^p&\le \cO(n(4B)^p) + 2^p \sum_{t=1}^T \frac{t-1}{T^{p-1}}S_{t-1}^{p-1}\cdot A_t\\
% %     &\le \cO(n(4B)^p) + 2^p \sum_{t=1}^T \frac{t-1}{T^{p-1}}\cdot \left(\frac{C_p}{t-1}\sum_{s=1}^{t-1}A_s + C_pT^{-1/p}\right)^{p-1} \cdot A_t\\
% %     &\le \cO(n(4B)^p) + 2^{2p} (C_p)^{p-1}\sum_{t=1}^T \frac{t-1}{T^{p-1}}\cdot \left(\left(\frac{1}{t-1}\sum_{s=1}^{t-1} A_s\right)^{p-1} A_t+  T^{-\frac{p-1}{p}}A_t\right)\\
% %     &\le \cO(n(4B)^p) + 2^{2p} (C_p)^{p-1}T\left(\sum_{t=1}^T A_t/T\right)^p + 2^{2p}(C_p)^{p-1} \frac{1}{T^{p-2+\frac{p-1}{p}}}\left(\sum_{t=1}^TA_t/T\right).
% % \end{align*}
% % Hence, by choosing $C_p \ge \max\{n^{1/p}\cdot 4B, 2^{2p}\}$, we have $2^{2p}(C_p)^{p-1} \le (C_p)^p$. Note that $p\ge 2$ implies that $(x+y)^p\ge x^p + y^p + xy^{p-1}$ 
% % \begin{align*}
% %     S_T^p &\le \cO((C_p)^p/T) + (C_p)^{p}\left(\sum_{t=1}^TA_t/T\right)^p + (C_p)^p\frac{1}{T^{\frac{p-1}{p}}}\left(\sum_{t=1}^TA_t/T\right)\\
% %     &\le (C_p)^p\cdot\left(\frac{1}{T}\sum_{s=1}^T A_s + T^{-1/p}\right)^p,
% % \end{align*}
% % which implies that 
% % \begin{align}
% %     S_T \le C_p\cdot \left(\frac{1}{T}\sum_{s=1}^T A_s + T^{-1/p}\right).
% % \end{align}
% % \end{proof}







% %We first consider the simplest situation, when the true reward $r^*(x,y)$ is defined by $$r^*(x,y)=\sum_{i=1}^m \alpha_i r_i^*(x,y).$$

% % We mainly consider online setting, in which the LLM collects the data by the interaction of the human. In each case, given a prompt $x$ generated by an initial distribution $x\sim \rho,$ we use policy $\pi_{t}$ to generate two responses $y_1, y_2$, and then receives the index of the objective and the preference feedback $I, p.$ Then the LLM collects this tuple of data $(x,y_1,y_2, I,p)$. We can also adapt our idea to the offline setting and we provide them in the experiment.  



% % Now we consider how the human choose the index of objective. Given two responses $y_1, y_2$, the term $|\alpha_i\cdot(r_i(x,y_1) - r_i(x,y_2))|$ represents the gap between these two responses on the objective $i$. Then, then we model the human's choice by the following way:
% % the human chooses the index based on the following softmax distribution 
% % \begin{align}
% %     I \propto \exp(\alpha_i\cdot |r_i(x,y_1)-r_i(x,y_2)|). \label{eq:obj_select}
% % \end{align}

% % \textbf{Way 2:} The human chooses the index based on the following distribution:
% % \begin{align*}
% %     I \propto \alpha_i\cdot |r_i(x,y_1)-r_i(x,y_2)|
% % \end{align*}

% % These two ways above are both a good model for the human's choice, since it will be much easier for human to notice the objective with the biggest difference. We will show that these two ways will not lead to a big difference in the proof.

% % \textcolor{red}{Maybe general distribution}

% % Moreover, after the selection of the index, suppose the human chooses the index $i$, we will let the human to discard the responses $(y_1,y_2)$ if $|r_i(x,y_1) - r_i(x,y_2)|<\gamma$ for some small $\gamma$. This can help us to make the data more informational. \textcolor{red}{Is this process influence the randomness?}

% \subsection{Estimating the Importance Weight by MLE}

% In this subsection, we show how to estimate the $\alpha_i, i \in [m]$, by using MLE. At round $t$, we estimate the $\{\alpha_i\}_{i \in [m]}$ by the previous data:


% %We first consider the first way, which is a softmax distirbution.

% \begin{theorem}\label{thm:alphabound_way1}
%     If we model the human's choice by Eq. \eqref{eq:obj_select},  with probability at least $1-\delta$, for each iteration $t\in [T]$, we can have 
%     \begin{align}
%         &\EE_{y_1\sim \pi^*, y_2\sim \pi_{\text{base}}}\|X^*(x,y)\circ|\alpha^* - \hat{\alpha}^t|\|_\infty \nonumber
%         \\& \le  \exp(4/\beta) \cdot \mathrm{poly}(m)\cdot \widetilde{\cO}\Bigg(\sqrt{\frac{1}{t}\log (T/\delta))} \nonumber\\&\qquad + \frac{1}{t} \sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)]\hat{\alpha}_i^t\Bigg),
%     \end{align}
%     where $X^* \in \RR^m$ and for each $i \in [m]$, $(X^*)_i = |r^*_i(x,y_1) - r^*_i(x,y_2)|.$ Also, $\Delta_i^t(x,y)=\big||\hat{r}_i^t(x,y_1) -\hat{r}_i^{t}(x,y_2)|-|r_i^*(x,y_1) - r_i^*(x,y_2)|\big| $. $\alpha^*$ is the real importance weight. The notation $\circ$ denotes the Hadamard product. The notation $\tilde{\cO}$ hides the constant and logarithmic terms.
% \end{theorem}
% The proof is provided in the Section \ref{sec:proof}. It shows that, the difference between the estimated $\hat{\alpha}^t$ and the truth $\alpha^*$, is upper bounded by the cumulated historical error of the reward function. Note that it is not a point estimation, and the accuracy of each entry $i$ also depends on the reward gap for the objective $i$.



\section{Experiments} %\nuoya{Move experiment here}
In this section, we provide our practical algorithm. We run the offline version of MOPO, and use MOD \citep{shi2024decoding} as the sub-procedure to solve the linear aggregation maximization problem at each round. The pseudocode is shown in Algorithm \ref{alg: vpo-fl-prac}.
\begin{algorithm}[H] 
     \begin{algorithmic}[1] 
         \caption{MOPO(Practical Version)-Offline} 
         \label{alg: vpo-fl-prac} 
         \STATE \textbf{Initial}: $\overline{d}^0 = (\frac{1}{m},\cdots, \frac{1}{m})^\top $, dataset $\cD_{\mathrm{offline}}$, $W$.
         \STATE Calculate the optimal policy $\pi_i$ for each objective $i \in [m]$ using offline dataset $\cD_{\mathrm{offline}}$.
         \FOR{$t=1,2,\cdots,T$} 
         %\STATE Calculate $\pi_i^t = \text{PPO}(r_i)$ for all $i \in [m]$. 
            \STATE Execute $\pi^t=\mathrm{MOD}(\{\pi_i\}_{i \le m}, \overline{d^{t-1}})$. 
            \STATE Calculate the point $V^t \in \RR^m$. %Calculate $\overline{V}^t = \frac{t-1}{t}\overline{V}^{t-1} + \frac{1}{t}V^t$.
            \STATE Calculate the direction $d^{t} = \mathrm{Proj}(W, V^t),$ and get the average direction $\overline{d^{t}} = \frac{1}{t}\sum_{j=1}^t \frac{d^j}{\|d^j\|_1}.$
         \ENDFOR 
     \end{algorithmic} 
\end{algorithm} 
Note that the algorithm average the direction instead of averaging the estimated reward vector function, which can lead to a more stable result. To execute the Line 2, following the previous paper \citep{shi2024decoding}, we first fine-tune the model LLAMA2-7B on the Anthropic-HH dataset \citep{ouyang2022training} to get the reference policy $\pi_{\mathrm{ref}}$. We then get the optimal policy $\pi_i$ for each objective $i \in \{1,2,3\}$ using PPO approach trained on three off-sheld reward model:
\begin{itemize}
    \item Harmlessness: \url{https://huggingface.co/Ray2333/gpt2-large-harmless-reward_model}
\item Helpfulness: \url{https:
//huggingface.co/Ray2333/gpt2-large-helpful-reward_model}
\item Humor: \url{https://huggingface.co/mohameddhiab/humor-no-humor}
\end{itemize}
%We fine-tune a LLAMA2-7B model using Anthropic-HH dataset \citep{bai2022training} with
%three different objectives of an LM assistant: Humor, Helpful, and Harmless. We run the offline version of MOPO, and use MOD \citep{shi2024decoding} as the sub-procedure to solve the linear aggregation maximization problem at each round. The pseudocode is shown in Algorithm \ref{alg: vpo-fl-prac}.%\aarti{refer to practical version Alg 5 here}
% \begin{algorithm}[H] 
%      \begin{algorithmic}[1] 
%          \caption{MOP(Practical Version)-Offline} 
%          \label{alg: vpo-fl-prac} 
%          \STATE \textbf{Initial}: $\overline{d}^0 = (\frac{1}{m},\cdots, \frac{1}{m})^\top $, dataset $\cD_{\mathrm{offline}}$, W.
%          \STATE Calculate the optimal policy $\pi_i$ for each objective $i \in [m]$ using offline dataset $\cD_{\mathrm{offline}}$.
%          \FOR{$t=1,2,\cdots,T$} 
%          %\STATE Calculate $\pi_i^t = \text{PPO}(r_i)$ for all $i \in [m]$. 
%             \STATE Execute $\pi^t=\mathrm{MOD}(\{\pi_i\}_{i \le m}, \overline{d^{t-1}})$. 
%             \STATE Calculate the point $V^t \in \RR^m$. %Calculate $\overline{V}^t = \frac{t-1}{t}\overline{V}^{t-1} + \frac{1}{t}V^t$.
%             \STATE Calculate the direction $d^{t} = \mathrm{Proj}(W, V^t),$ and get the average direction $\overline{d^{t}} = \frac{1}{t}\sum_{j=1}^t \frac{d^j}{\|d^j\|_1}.$
%          \ENDFOR 
%      \end{algorithmic} 
% \end{algorithm} 
% Note that the algorithm average the direction instead of averaging the estimated reward vector function, which can be more stable in our algorithms.
\paragraph{Single-Group Problem with Multiple Objectives}
Note that MOPO is an iterate algorithm, thus the computational cost can still be high due to the large number of iterations. In practice, we can mitigate this by either reducing the number of iterations or computing a single gradient update per iteration \citep{guo2024direct}. In our experiments, we set the number of iterations to 7, striking a balance between computational efficiency and performance.   To compute the expected reward vector $V^t$, we calculate the expectation by taking the expectation over 100 training samples, and we believe the performance of MOPO can be improved by using more training samples to calculate the expectation. 

For $p=0.5$, we compare MOPO with the RS algorithm \citep{rame2024rewarded}, MOD algorithm \citep{shi2024decoding} (both of which use linear aggregation), and a baseline AR that directly aggregates the reward using non-linear aggregation. The experimental results show that MOPO performs generally better. 
The following table presents the results for MORLHF with the objectives (Harmless, Helpful) and (Harmless, Humor). %Due to time constraints, the evaluation results are based on a down-sampled dataset of size 500. 
Additionally, since the aggregation only works for non-negative rewards, when using AR to aggregate the reward, we take 
$\max\{r_i,0\}$ instead of 
$r_i$
  for each objective. Although this is the only reasonable approach, we observe that it performs poorly. This may be due to the vanishing gradient problem, as the gradient of 
$\max\{r_i,0\}$ becomes zero when the reward is negative. The experiment shows that our algorithm MOPO generally outperforms the previous one. 

%More experiments and details are provided in Appendix \ref{app:experiment}.
% \begin{table}[H]\footnotesize \centering
%  \caption{Comparison of previous representative work for MORLHF with $p=0.5$ and the objective Humor and Harmless. The score is the distance between the reward vector and the target set. The smaller one is better.}
% \begin{tabular}{ccccc}

% \midrule[1.5pt]
% $\alpha$  &  \makecell{Ours} &  \makecell{RS}& \makecell{MOD}  &  \makecell{AR}\\ \hline
% \makecell{(0.1,0.9)}    &\textbf{0.335}&   0.362 & 0.337 & 1.767\\ 
% \makecell{(0.3,0.7)}      & 0.578 & 0.678  & \textbf{0.572}  & 2.011 \\ 
% \makecell{(0.5,0.5)} &   \textbf{0.720}   &   0.882   & 0.723   & 1.970  \\
% %\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
% \makecell{(0.7,0.3)} & \textbf{0.630} &   0.860   &  0.722 &2.411\\ 
% \makecell{(0.9,0.1)} & \textbf{0.217}  & 0.391  & 0.396 & 2.068\\ 
% \bottomrule[1.5pt]
% \end{tabular}
% \label{table:0.5}
% \end{table}
\begin{table}[H]\footnotesize \centering
 \caption{Comparison of previous representative works for MORLHF with $p=0.5, c=0.5$ and the objective Harmless and Helpful. The score is the distance between the reward vector and the target set. The smaller one is better.}
\begin{tabular}{ccccc}
%\vspace{0.5em}
\midrule[1.5pt]
$\alpha$  &  \makecell{Ours} &  \makecell{RS}& \makecell{MOD}  &  \makecell{AR}\\ \hline
\makecell{(0.1,0.9)}    &\textbf{0.229}&  0.971   & 0.808 & 0.555\\ 
\makecell{(0.3,0.7)}      & \textbf{0.051} & 0.666  & 0.079 &  1.459\\ 
\makecell{(0.5,0.5)} &  \textbf{0.015}    &  0.078    & 0.103&  1.314 \\
%\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
\makecell{(0.7,0.3)} & \textbf{0.067}& 0.707  &0.800 & 1.004\\ 
\makecell{(0.9,0.1)} & \textbf{0.184}  &1.153 & 1.137&1.526\\ 
\bottomrule[1.5pt]
\end{tabular}
\label{table:0.5}
\end{table}
\begin{table}[H]\footnotesize \centering
 \caption{Comparison of previous representative work for MORLHF with $p=0.5$, $c = 1.3$ and the objective Harmless and Humor. The score is the distance between the evaluated reward vector and the target set. The smaller one is better.}
\begin{tabular}{ccccc}

\midrule[1.5pt]
$\alpha$  &  \makecell{Ours} &  \makecell{RS}& \makecell{MOD}  &  \makecell{AR}\\ \hline
\makecell{(0.1,0.9)}    &\textbf{0.335}&   0.362 & 0.337 & 1.767\\ 
\makecell{(0.3,0.7)}      & 0.578 & 0.678  & \textbf{0.572}  & 2.011 \\ 
\makecell{(0.5,0.5)} &   \textbf{0.720}   &   0.882   & 0.723   & 1.970  \\
%\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
\makecell{(0.7,0.3)} & \textbf{0.630} &   0.860   &  0.722 &2.411\\ 
\makecell{(0.9,0.1)} & \textbf{0.217}  & 0.391  & 0.396 & 2.068\\ 
\bottomrule[1.5pt]
\end{tabular}
\label{table:3}
\end{table}
For $p=-\infty$, we compare MOPO with max-min RLHF \citep{chakrabortymaxmin}. We choose the target set $W_{\infty, 1.5}^{\alpha}$ for objective pairs (Harmless, Humor) and $W_{\infty, 0.5}^\alpha$ for objective pairs (Harmless, Helpful). The result shows that we achieve stable and better performance.  
\begin{table}[H]\footnotesize \centering
 \caption{Comparison with max-min RLHF for objectives Humor and Harmless. The number pair represents the reward vector. The pair with the larger minimum value is better.}
\begin{tabular}{ccc}

\midrule[1.5pt]
 &\makecell{Ours} & Max-Min RLHF \\ \hline
\makecell{(Harmless, Humor)}  & (1.097,1.297) & \textbf{(1.530, 1.146)}\\  
\makecell{(Harmless, Helpful)} &\textbf{(0.034,0.497)} & (-0.135, 0.393)\\
\bottomrule[1.5pt]
\end{tabular}
\label{table:1}
\end{table}

%\subsection{Practical Algorithms and Details}


%\subsection{Harmless and Humor}
% The following table presents the results for MORLHF with the objectives Harmless and Humor, where we use $W_{0.5,1.3}^\alpha$ as the final target set. Our algorithm generally outperforms the previous one. Due to time constraints, the evaluation results are based on a down-sampled dataset of size 500. Additionally, since the aggregation only works for non-negative rewards, when using AR to aggregate the reward, we take 
% $\max\{r_i,0\}$ instead of 
% $r_i$
%   for each objective. Although this is the only reasonable approach, we observe that it performs poorly. This may be due to the vanishing gradient problem, as the gradient of 
% $\max\{r_i,0\}$ becomes zero when the reward is negative.



\paragraph{Multi-Group Problem with Multiple Objectives}

 
We perform the experiments on Harmless and Humor dataset when we have $N=2$ groups. One group has the target set $W_{0.5,1.3}^\alpha$ and the other has the target set $W_{-\infty,1}^\alpha$. We compare our consensus algorithm with Eq.~\eqref{eq:dir consensus} and a variant of max-min RLHF. In this variant of max-min RLHF, we use $\min\{r_1,r_2, \alpha_1\cdot (\max\{r_1,0\})^{0.5} + \alpha_2\cdot (\max\{r_2,0\})^{0.5} \}$ as the reward. 
We also perform experiments on the Harmless and Helpful dataset with the target set $W_{0.5,0.5}^\alpha$ and the target set $W_{-\infty, 0}^\alpha$. 
The following tables show the experiment results. The results show that our algorithms perform relatively stable and better, while this variant of max-min RLHF performs unstable.  However, note that this variant of max-min RLHF also needs retraining whenever one group changes the aggregation approach, which is time-consuming for real-world applications.

\begin{table}[H]\footnotesize \centering
 \caption{Comparison of MOPO and a variant of Max-Min RLHF on multi-group setting. The objectives are Harmless and Humor.  The score is the distance between the evaluated reward vector and the target set. The smaller one is better.}
\begin{tabular}{ccc}

\midrule[1.5pt]
$\alpha$  &  \makecell{Ours} &   \makecell{Max-Min RLHF}\\ \hline
\makecell{(0.1,0.9)}    &\textbf{0.408}& 0.992\\ 
\makecell{(0.3,0.7)}   &\textbf{0.577} & 1.171 \\ 
\makecell{(0.5,0.5)} &  0.708 & \textbf{0.429}  \\
%\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
\makecell{(0.7,0.3)} & \textbf{0.619}&1.342\\ 
\makecell{(0.9,0.1)} & 0.406& \textbf{0.208}\\ 
\bottomrule[1.5pt]
\end{tabular}
\label{table:4}
\end{table}

\begin{table}[H]\footnotesize \centering
 \caption{Comparison of MOPO and a variant of Max-Min RLHF on multi-group setting. The objectives are Harmless and Helpful.  The score is the distance between the evaluated reward vector and the target set. The smaller one is better.}
\begin{tabular}{ccc}

\midrule[1.5pt]
$\alpha$  &  \makecell{Ours} &   \makecell{Max-Min RLHF}\\ \hline
\makecell{(0.1,0.9)}    &\textbf{\textbf{0.230}}& 1.073\\ 
\makecell{(0.3,0.7)}   &\textbf{0.052} & 0.123 \\ 
\makecell{(0.5,0.5)} &  \textbf{0.015} & 0.261 \\
%\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
\makecell{(0.7,0.3)} & \textbf{0.067}&0.204\\ 
\makecell{(0.9,0.1)} & 0.184& \textbf{0.121}\\ 
\bottomrule[1.5pt]
\end{tabular}
\label{table:5}
\end{table}

% \section{Analysis of Algorithm 2}\label{sec:analysis of algorithm2}






%The score is defined by the distance between final reward vector and the target set. The smaller one is better.




%\subsection{Online}
\section{Conclusion}
In this paper, we study efficient multi-objective and multi-group RLHF problems under non-linear aggregation. By transforming the non-linear aggregation maximization into a series of linear aggregation maximization sub-problems, we find a computationally efficient algorithm that can converge to the optimal policy. Theoretically, we establish a general framework with converge guarantees for both offline and online settings, and the framework is also adaptable to a reward-free version. Empirically, we present a training-free framework given the reward functions and optimal policies for all objectives.

There are many future directions worth exploring. First, one can study how to learn the parameter $p$ in the aggregation function like \citep{pardeshi2024learning} using the preference feedback. Second, one can further study the token-level MORLHF \citep{zeng2024token} based on our idea. Last, it is interesting to further study the multiple preference aggregation in Stochastic Transitivity model \citep{fishburn1973binary} instead of BTL model, and further discuss the relationship between them and previous distortion negative results \citep{anshelevich2021distortion}. 

%\nuoya{refer some papers, like Kanad's, token-level RLHF, BTL model to SST or WST.}
\section{Acknowledgement}
This work is supported in part by NSF AI Institute for Societal Decision Making under award IIS2229881 and ONR award N000142212363.


% \section{Impact Statements}
% The goal of this paper is to advance the field of multi-objective RLHF, which can be applied to many applications in society.  Our approach aims to mitigate biases in language models and promote fairness across diverse populations. However, it should be careful for implementation and evaluation to avoid unintended consequences, such as exacerbating inequalities or overlooking underrepresented groups.
% \bibliography{ref}
% \bibliographystyle{apalike}
\input{main.bbl}

\newpage
\appendix
\onecolumn
\input{appendix}

\end{document}



