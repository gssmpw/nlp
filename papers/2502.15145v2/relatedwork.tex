\section{Related Works}
\paragraph{RLHF}
Fine-tuning LLMs with human feedback and RL is known as RLHF. The reward-based RLHF first extracts a reward model with a Bradley-Terry (BT) assumption on human preferences, and then optimizes the reward model \citep{ouyang2022training,bai2022training,touvron2023llama,azar2024general}. On the other hand, the reward-free RLHF avoids explicit reward modeling by directly formulating the preference loss as a function of the policy and then using supervised learning \citep{wang2023beyond,rafailov2024direct}, which is more stable and computation-friendly. 
\paragraph{MORLHF}
Multi-Objective RLHF (MORLHF) aims to align an LLM with human preferences while optimizing for multiple objectives, such as harmlessness, helpfulness, and humor. Most previous works aggregate rewards or models as the weighted sum of individual components. MORLHF \citep{wu2023fine,bai2022training} directly optimizes the aggregated reward using PPO, while MODPO \citep{zhou2023beyond} provides a lightweight reward-free alternative. RS \citep{rame2024rewarded} combines individual models by averaging them. MOD \citep{shi2024decoding} calculates the closed-form solution of the optimal policy for aggregated reward directly and derives a training-free algorithm. Only one work \citep{zhong2024provable} consider non-linear aggregation, and they optimize the aggregated reward function directly. However, this approach is computationally expensive and requires retraining when the aggregation changes. Instead, we propose a theoretical framework that can be easily adapted to a reward-free algorithm,  along with a training-free empirical algorithm built on the same theoretical framework. Detailed comparisons are shown in Table~\ref{table:comparison}.

\paragraph{Pluralistic Alignment and Preference Aggregation}
There is a growing body of work on aligning machine learning models with diverse preferences, accounting for different values and perspectives.  The works \citep{chakrabortymaxmin,ramesh2024group} focus on optimizing the worst-case group loss, ensuring that the model achieves reasonable performance across all groups. \citep{park2024rlhf, sorensenposition, conitzer2024social} explore how to aggregate preferences using social choice and voting theory, outlining a high-level roadmap for pluralistic AI alignment. \citep{ge2024axioms} technically demonstrate that the BTL model fails to satisfy well-known standards in social choice theory and propose a novel rule-based approach for learning reward functions. \citep{chen2024pal} further study the generalization of the BTL model and introduce an ideal point model that better accommodates diverse groups.

\begin{table}\centering\footnotesize

 \caption{Comparison of previous work for MORLHF. The parameter $p$ means the exponent in Eq.\eqref{eq:generalized_reward_f}. Algorithm \ref{alg: vpo-fl-offline} (offline setting) \& \ref{alg: vpo-fl-general} (online setting) have theoretical guarantees, %with \ref{alg: vpo-fl-offline} holds for the offline setting and \ref{alg: vpo-fl-general} holds for the online setting.
 while Algorithm \ref{alg: vpo-fl-prac} is the more practical version.}
\begin{tabular}{ccccc}

\midrule[1.5pt]
 &  \makecell{Aggergation} &  \makecell{Reward\\Free}& \makecell{Traning\\Free}  &  \makecell{Multi-\\Group}\\ \hline
\makecell{MORLHF\vspace{-0.3em}\\\tiny\citep{wu2023fine}}    &$p=1$& \tiny\XSolidBrush & \tiny\XSolidBrush& \tiny\XSolidBrush\\ \hline
\makecell{RS \vspace{-0.3em}\\\tiny\citep{rame2024rewarded}}      & $p=1$ & \tiny\Checkmark  & \tiny\Checkmark  & \tiny\XSolidBrush \\ \hline
\makecell{MOD\vspace{-0.3em}\\\tiny\citep{shi2024decoding}} &   $p=1$   &   \makecell{\tiny\Checkmark}   &   \tiny\Checkmark & \tiny\XSolidBrush  \\ \hline
%\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citep{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
\makecell{PNB\vspace{-0.3em}\\\tiny \citep{zhong2024provable}} & $p\le1$ &   \tiny\XSolidBrush   &  \tiny\XSolidBrush &\tiny\XSolidBrush    \\ \hline
\makecell{Algorithm  \ref{alg: vpo-fl-offline} \& \ref{alg: vpo-fl-general}} & $p\le1$  & \tiny\Checkmark  & \tiny\XSolidBrush  & \tiny\Checkmark\\ 
\hline
\makecell{Algorithm \ref{alg: vpo-fl-prac}} & $p\le1$  & \tiny\XSolidBrush  & \tiny\Checkmark  & \tiny\Checkmark\\ \bottomrule[1.5pt]
\end{tabular}
\label{table:comparison}
\end{table}