\section{Experiment Details}\label{app:experiment}
% \subsection{Practical Algorithms and Details}
% \begin{algorithm}[H] 
%      \begin{algorithmic}[1] 
%          \caption{MOPO(Practical Version)-Offline} 
%          \label{alg: vpo-fl-prac} 
%          \STATE \textbf{Initial}: $\overline{d}^0 = (\frac{1}{m},\cdots, \frac{1}{m})^\top $, dataset $\cD_{\mathrm{offline}}$, $W$.
%          \STATE Calculate the optimal policy $\pi_i$ for each objective $i \in [m]$ using offline dataset $\cD_{\mathrm{offline}}$.
%          \FOR{$t=1,2,\cdots,T$} 
%          %\STATE Calculate $\pi_i^t = \text{PPO}(r_i)$ for all $i \in [m]$. 
%             \STATE Execute $\pi^t=\mathrm{MOD}(\{\pi_i\}_{i \le m}, \overline{d^{t-1}})$. 
%             \STATE Calculate the point $V^t \in \RR^m$. %Calculate $\overline{V}^t = \frac{t-1}{t}\overline{V}^{t-1} + \frac{1}{t}V^t$.
%             \STATE Calculate the direction $d^{t} = \mathrm{Proj}(W, V^t),$ and get the average direction $\overline{d^{t}} = \frac{1}{t}\sum_{j=1}^t \frac{d^j}{\|d^j\|_1}.$
%          \ENDFOR 
%      \end{algorithmic} 
% \end{algorithm} 
% Note that the algorithm average the direction instead of averaging the estimated reward vector function, which can lead to a more stable result. To execute the Line 2, following the previous paper \citep{shi2024decoding}, we first fine-tune the model LLAMA2-7B on the Anthropic-HH dataset \citep{ouyang2022training} to get the reference policy $\pi_{\mathrm{ref}}$. We then get the optimal policy $\pi_i$ for each objective $i \in \{1,2,3\}$ using PPO approach trained on three off-sheld reward model:
% \begin{itemize}
%     \item Harmlessness: \url{https://huggingface.co/Ray2333/gpt2-large-harmless-reward_model}
% \item Helpfulness: \url{https:
% //huggingface.co/Ray2333/gpt2-large-helpful-reward_model}
% \item Humor: \url{https://huggingface.co/mohameddhiab/humor-no-humor}
% \end{itemize}
% Note that MOPO is an iterate algorithm, thus the computational cost can still be high due to the large number of iterations. In practice, we can mitigate this by either reducing the number of iterations or computing a single gradient update per iteration \citep{guo2024direct}. In our experiments, we set the number of iterations to 7, striking a balance between computational efficiency and performance.   To compute the expected reward vector $V^t$, we calculate the expectation by taking the expectation over 100 training samples, and we believe the performance of MOPO can be improved by using more training samples to calculate the expectation. 

% \subsection{Harmless and Humor}
% The following table presents the results for MORLHF with the objectives Harmless and Humor, where we use $W_{0.5,1.3}^\alpha$ as the final target set. Our algorithm generally outperforms the previous one. Due to time constraints, the evaluation results are based on a down-sampled dataset of size 500. Additionally, since the aggregation only works for non-negative rewards, when using AR to aggregate the reward, we take 
% $\max\{r_i,0\}$ instead of 
% $r_i$
%   for each objective. Although this is the only reasonable approach, we observe that it performs poorly. This may be due to the vanishing gradient problem, as the gradient of 
% $\max\{r_i,0\}$ becomes zero when the reward is negative.
% \begin{table}[H]\footnotesize \centering
%  \caption{Comparison of previous representative work for MORLHF with $p=0.5$, $c = 1.3$ and the objective Harmless and Humor. The score is the distance between the evaluated reward vector and the target set. The smaller one is better.}
% \begin{tabular}{ccccc}

% \midrule[1.5pt]
% $\alpha$  &  \makecell{Ours} &  \makecell{RS}& \makecell{MOD}  &  \makecell{AR}\\ \hline
% \makecell{(0.1,0.9)}    &\textbf{0.335}&   0.362 & 0.337 & 1.767\\ 
% \makecell{(0.3,0.7)}      & 0.578 & 0.678  & \textbf{0.572}  & 2.011 \\ 
% \makecell{(0.5,0.5)} &   \textbf{0.720}   &   0.882   & 0.723   & 1.970  \\
% %\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
% \makecell{(0.7,0.3)} & \textbf{0.630} &   0.860   &  0.722 &2.411\\ 
% \makecell{(0.9,0.1)} & \textbf{0.217}  & 0.391  & 0.396 & 2.068\\ 
% \bottomrule[1.5pt]
% \end{tabular}
% \label{table:3}
% \end{table}


% \subsection{Multi-Group Experiments}

 
% We perform the experiments on Harmless and Humor dataset when we have $N=2$ groups. One group has the target set $W_{0.5,1.3}^\alpha$ and the other has the target set $W_{-\infty,1}^\alpha$. We compare our consensus algorithm with Eq.~\eqref{eq:dir consensus} and a variant of max-min RLHF. In this variant of max-min RLHF, we use $\min\{r_1,r_2, \alpha_1\cdot (\max\{r_1,0\})^{0.5} + \alpha_2\cdot (\max\{r_2,0\})^{0.5} \}$ as the reward. 
% We also perform experiments on Harmless and Helpful dataset with target set $W_{0.5,0.5}^\alpha$ and the target set $W_{-\infty, 0}^\alpha$. 
% The following tables show the experiment results. The results show that our algorithms perform relatively stable and better, while this variant of max-min RLHF performs unstable.  However, note that this variant of max-min RLHF also needs retraining whenever one group changes the aggregation approach, which is very time-consuming for the real-world application.

% \begin{table}[H]\footnotesize \centering
%  \caption{Comparison of MOPO and a variant of Max-Min RLHF on multi-group setting. The objectives are Harmless and Humor.  The score is the distance between the evaluated reward vector and the target set. The smaller one is better.}
% \begin{tabular}{ccc}

% \midrule[1.5pt]
% $\alpha$  &  \makecell{Ours} &   \makecell{Max-Min RLHF}\\ \hline
% \makecell{(0.1,0.9)}    &\textbf{0.408}& 0.992\\ 
% \makecell{(0.3,0.7)}   &\textbf{0.577} & 1.171 \\ 
% \makecell{(0.5,0.5)} &  0.708 & \textbf{0.429}  \\
% %\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
% \makecell{(0.7,0.3)} & \textbf{0.619}&1.342\\ 
% \makecell{(0.9,0.1)} & 0.406& \textbf{0.208}\\ 
% \bottomrule[1.5pt]
% \end{tabular}
% \label{table:4}
% \end{table}

% \begin{table}[H]\footnotesize \centering
%  \caption{Comparison of MOPO and a variant of Max-Min RLHF on multi-group setting. The objectives are Harmless and Helpful.  The score is the distance between the evaluated reward vector and the target set. The smaller one is better.}
% \begin{tabular}{ccc}

% \midrule[1.5pt]
% $\alpha$  &  \makecell{Ours} &   \makecell{Max-Min RLHF}\\ \hline
% \makecell{(0.1,0.9)}    &\textbf{\textbf{0.230}}& 1.073\\ 
% \makecell{(0.3,0.7)}   &\textbf{0.052} & 0.123 \\ 
% \makecell{(0.5,0.5)} &  \textbf{0.015} & 0.261 \\
% %\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
% \makecell{(0.7,0.3)} & \textbf{0.067}&0.204\\ 
% \makecell{(0.9,0.1)} & 0.184& \textbf{0.121}\\ 
% \bottomrule[1.5pt]
% \end{tabular}
% \label{table:5}
% \end{table}

% % \section{Analysis of Algorithm 2}\label{sec:analysis of algorithm2}




% % Since we do not assume the target set $W^*$ is approachable, we have the following property for the approachability:

% % \begin{lemma}\label{lemma:approach}
% % For each $\theta \in \RR^m$ with $\|\theta\| = 1$, we have 
% % $$\min_{x \in W^*}\langle \theta, x\rangle\le  \EE_{\pi^*}[\langle \theta, r_i^*(x,y)\rangle-\sum_{i=1}^m \theta_i\beta \DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})] + D(\pi^*) = J(r_1^*, \cdots, r_m^*, \theta, W^*, \pi^*) +  D(\pi^*).$$    
% % \end{lemma}
% % \begin{proof}
% %     By the definition of $D(\pi^*) = d(S(\pi^*), W^*)$, we know that there exists a vector $p$ with $S(\pi^*) + p \in W^*$ and $\|p\|_2 = D(\pi^*).$
% % Then we can have $$\min_{x \in W^*}\langle \theta, x\rangle \le \langle \theta, S(\pi^*) + p\rangle\le \EE_{\pi^*}[\langle \theta, r_i^*(x,y)\rangle-\sum_{i=1}^m \theta_i\beta \DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})] + D(\pi^*)$$
% % \end{proof}

% % Denote $$D(\pi) = d(W^*, \EE_{\pi^t}[r(x,y)]-\beta \DD_{\mathrm{KL}}(\pi\|\pi_{\mathrm{ref}})).$$ Thus $\pi^* = \min_\pi D(\pi^*)$. Now by the Lemma \ref{lemma:approach}, for each $\theta \in \RR^m$ with $\|\theta\| \le 1$, we have 
% % $$\min_{x \in W^*}\langle \theta, x\rangle\le  \EE_{\pi^*}[\langle \theta, r_i^*(x,y)\rangle-\sum_{i=1}^m \theta_i\beta \DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})] + D(\pi^*) = J(r_1^*, \cdots, r_m^*, \theta, W^*, \pi^*) +  D(\pi^*).$$

% % Denote $V^t \in \RR^m$ with $(V^t)_i = \EE_{\pi^t}[\hat{r}_i^t(x,y) - \beta\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})]$, and $\frac{1}{t}\overline{V}^t = \sum_{i=1}^t V^i$. We have 
% % \begin{align*}
% %     d(\overline{V}^T, W^*)^2 &= \|\overline{V}^T - \Pi_{W^*} (\overline{V}^T)\|^2 \\
% %     &\le \|\overline{V}^T - \Pi_{W^*} (\overline{V}^{T-1})\|^2\\
% %     & = \left(\frac{T-1}{T}\right)^2 d(\overline{V}^{T-1}, W^*)^2 + \frac{1}{T^2} \|V^T - \Pi_{W^*}(\overline{V}^{T-1})\|^2 \\
% %     &\qquad + \frac{2(T-1)}{T^2}(\overline{V}^{T-1} - \Pi_{W^*} (\overline{V}^{T-1}))\cdot (V^T - \Pi_{W^*}(\overline{V}^{T-1}))\end{align*}
% % First, based on the definition of $W^*$, it is easy to show that $d^t \succeq 0.$
% % $\pi^t$ is the optimal policy such that $$\EE_{\pi^t}[\langle d^t, \hat{r}(x,y) \rangle - \sum_{i=1}^m d^t_i\beta \DD_{\mathrm{KL}}(\pi^t \|\pi_{\mathrm{ref}})] \ge  \EE_{\pi_{\mathrm{ref}}}[\langle d^t , \hat{r}(x,y) \rangle]\ge 0,$$
% % thus $(\sum_{i=1}^m d_i^t)\cdot \beta \DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}}) \le \EE_{\pi^t}[d^t\cdot  \hat{r}(x,y)] \le B$. Hence, given $d^t \succeq 0$ and $\|d^t\|_2 =1,$
% % \begin{align*}
% %     \beta \DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}}) \le \frac{B}{\sum_{i=1}^m d_i^t}\le B.
% % \end{align*}
% % we have $|(V^t)_i| \le B$ and 
% % \begin{align*}
% %     \|V^T - \Pi_{W^*}(\overline{V}^{T-1})\|^2 \le B^2m
% % \end{align*}
% % Thus by iteration we can have 
% % \begin{align*}
% %     T^2 d(\overline{V}^T, W^*)^2 \le T\cdot B^2m + \sum_{t=1}^T 2(t-1) (\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*}(\overline{V}^{t-1}))
% % \end{align*}
% % Now, by the definition of $d^t$, we have
% % \begin{align*}
% %     (\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*}(\overline{V}^{t-1})=
% %     d(\overline{V}^{t-1}, W^*)\cdot d^t\cdot (\Pi_{W^*}(\overline{V}^{t-1}) - V^t).
% %     \end{align*}
% %     Then, we prove the following lemma.
% %     \begin{lemma}\label{lemma:projection}
% %         $\min_{x\in W^*} \langle d^t, x\rangle = d^t \cdot \Pi_{W^*}(\overline{V}^{t-1}).$
% %     \end{lemma}
% %     \begin{proof}
% %      In fact, we only need to prove that for any $x \in W^*$, $\langle \overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}), x-\Pi_{W^*}(\overline{V}^{t-1})\rangle \le 0$. Suppose there exists $x \in W^*$ such that $\langle \overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}), x-\Pi_{W^*}(\overline{V}^{t-1})\rangle >0$, then since $W^*$ is a convex set, for any $\lambda \in (0,1)$, we have $x_\lambda = \lambda x + (1-\lambda) \Pi_{W^*}(\overline{V}^{t-1}) \in W^*$. Consider the line $$\Pi_{W^*}(\overline{V}^{t-1}) + t \frac{\Pi_{W^*}(\overline{V}^{t-1})-x}{\|\Pi_{W^*}(\overline{V}^{t-1})-x\|},\  \  t \in \RR.$$ Also, we consider the  projection of $\overline{V}^{t-1}$ on this line, and denote it as $p$. Then we can get 
% %     $$0<\langle\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1})- p + p, x -\Pi_{W^*}(\overline{V}^{t-1})\rangle  = \langle p-\Pi_{W^*}(\overline{V}^{t-1}),  x -\Pi_{W^*}(\overline{V}^{t-1})\rangle $$
% %     Hence when $\lambda \to 0$, $x_\lambda$ is between $p$  and $\Pi_{W^*}(\overline{V}^{t-1})$. Also, $$\|\overline{V}^{t-1}-x_\lambda \|^2 = \|\overline{V}-p\|^2 + \|p-x_\lambda\|^2 \le \|\overline{V}-p\|^2 + \|p-\Pi_{W^*}(\overline{V}^{t-1})\|^2 \le \|\Pi_{W^*}(\overline{V}^{t-1})-d^t \|^2,$$ which contradicts the selection of $\Pi_{W^*}(\overline{V}^{t-1}).$
% %  \end{proof}
% % Now, by Lemma \ref{lemma:approach} and Lemma \ref{lemma:projection}, we can get 
% % \begin{align*}
% %     d^t \cdot \Pi_{W^*}(\overline{V}^{t-1}) \le J(r_1^*, \cdots, r_m^*, d^t, \pi^*) + D(\pi^*).
% % \end{align*}
% % Then, we can continue the analysis by
% %     \begin{align*}
% %     (\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*}(\overline{V}^{t-1}))&=d(\overline{V}^{t-1}, W^*)\cdot \left( J(r_1^*, r_2^*, \cdots, r_m^*, d^t, \pi^*) + D(\pi^*) - d^t\cdot  V^t\right)\\
% %     & = d(\overline{V}^{t-1}, W^*)\cdot (J(\hat{r}_1^*, \cdots, \hat{r}_m^*, d^t, \pi^*) - J(\hat{r}_1, \cdots, \hat{r}_m, d^t, \pi^t) + D(\pi^*))\\
% %     & = d(\overline{V}^{t-1}, W^*) \cdot (\eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t) + D(\pi^*)).
% % \end{align*}
% % Thus we can get 
% % \begin{align*}
% %     T d(\overline{V}^T, W^*)^2 &\le   B^2m +
% %     \sum_{t=1}^T \frac{2(t-1)}{T}d(\overline{V}^{t-1}, W^*) \cdot (\eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t) + D(\pi^*)).
% % \end{align*}

% % Now we use induction method to show that $$d(\overline{V}^t, W^*) \le D(\pi^*) + \frac{\eta }{t}\sum_{j=1}^{t}\sum_{i=1}^m (L_i^j(\theta^*) - L_i^j(\theta^j)) + Cm/\sqrt{t},$$ where $C$ is a fixed constant which is to be determined. When $t = 1$, choose $C = 2B$ and the inequality holds by 
% % \begin{align*}
% %     \|d(\overline{V}^1, W^*)-D(\pi^*)\|\le d(\overline{V}^1, S(\pi^*)) \le 2B.
% % \end{align*}
% % Denote $A_j=\eta\cdot (\sum_{i=1}^m (L_i^t(\theta^*) - L_i^t(\theta^j)))$ and $S_t = \sum_{j=1}^{t}A_j$, then for all $t \in [T-1],$  
% % suppose we have 
% % \begin{align*}
% %     d(\overline{V}^{t-1}, W^*) \le D(\pi^*) + \frac{1}{t-1} S_{t-1} + C\left(\frac{\sqrt{m}}{\sqrt{t-1}}\right).
% % \end{align*}
% % Then we substitute these induction hypothesis into the recursion inequality and get 
% % \begin{align*}
% %     &T d(\overline{V}^T, W^*)^2\\&\le m + \sum_{t=1}^T \frac{2(t-1)}{T} \left(D(\pi^*) + \frac{1}{t-1}S_{t-1}+ C\left(\frac{\sqrt{m}}{\sqrt{t-1}}\right)\right)\left(A_t + D(\pi^*)\right)\\
% %     &\le B^2m + \sum_{t=1}^T \left( \frac{2(t-1)}{T}D(\pi^*) + \frac{1}{T}S_{t-1} + C\left(\frac{2m\sqrt{t-1}}{T}\right)\right)\left(A_t + D(\pi^*)\right)\\
% %     &= B^2m + (T-1)D(\pi^*)^2 + \sum_{t=1}^T \frac{1}{T}S_{t-1}A_t + \sum_{t=1}^T \left(\frac{1}{T} S_{t-1} + \frac{2(t-1)}{T}A_t\right) D(\pi^*)\\
% %     &\qquad + \sum_{t=1}^T C\left(\frac{2m\sqrt{t-1}}{T}\right)(A_t + D(\pi^*))\\
% %     & \le  B^2m + (T-1)D(\pi^*)^2 + \frac{1}{T} S_T^2 + \sum_{t=1}^T D(\pi^*) \cdot \left(\frac{T+t-1}{T}A_t\right)  + 2\sqrt{m}C\sqrt{T} D(\pi^*) + (2\sqrt{m}C/\sqrt{T}) S_T\\
% %     &\le B^2m + (T-1)D(\pi^*)^2 + \frac{1}{T} S_T^2 +  D(\pi^*) \cdot \left(2S_T\right) + 2\sqrt{m}C\sqrt{T} D(\pi^*) + 2\sqrt{m}C/\sqrt{T} S_T\\
% %     &\le T \cdot (Cm/\sqrt{T} + D(\pi^*) + \frac{1}{T}S_T)^2.
% % \end{align*}
% % The final inequality uses the fact that $C > B$. 

% % Thus we have $$d(\overline{V}^T, W^*) \le D(\pi^*) + \frac{\eta}{T}\sum_{j=1}^T \sum_{i=1}^m (L_i^j(\theta^*) - L_i^j(\theta^j)) + \frac{m}{\sqrt{T}}.$$

% % Now we derive the final regret.
% % \begin{align*}
% %     &D(\tilde{\pi}^T) - D(\pi^*) \\ &= d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\beta \DD_{\mathrm{KL}}(\tilde{\pi}^T\| \pi_{\mathrm{ref}})) - D(\pi^*)\\
% %     &\le d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - D(\pi^*)\\
% %     &= \underbrace{d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - d(W^*, \frac{1}{T}\sum_{t=1}^T\EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}}))}_{\textrm{(A)}} \\
% %     &\qquad + d(W^*, \frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - D(\pi^*)\\
% %     &=\mathrm{(A)}
% %     + d(W^*, \EE_{\tilde{\pi}^T}[\hat{r}^T(x,y)]-\frac{\beta}{T} \sum_{t=1}^T\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})) - D(\pi^*).
% % \end{align*}
% % The finally inequality uses the fact that 
% % $$\DD_{\mathrm{KL}}(\tilde{\pi}\| \pi_{\mathrm{ref}}) \le \frac{1}{T}\left(\sum_{t=1}^T\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})\right).$$
% % Then we can get 
% % \begin{align*}D(\tilde{\pi}^T) - D(\pi^*)&\le \mathrm{(A)} + d(W^*, \overline{V}^T) - D(\pi^*)\\
% % &\le \mathrm{(A)} +  \frac{\eta}{T}\sum_{j=1}^T \sum_{i=1}^m (L_i^j(\theta^*) - L_i^j(\theta^j)) + \frac{m}{\sqrt{T}}.\end{align*}
% % Now we consider the error term $(A)$, which represents the approximation error of the reward function. Now by Eq. \ref{ineq:apr error of reward}, we have 
% % \begin{align*}
% %     (A)&\le \EE_{\tilde{\pi}^T} [\hat{r}^t(x,y) - r^*(x,y)]\\
% %     &\le \frac{1}{T}\sum_{i=1}^m \alpha_i^* \cdot \left(\mu_i \exp(4/\beta)\kappa\cdot \sum_{t=1}^T   \sum_{j=1}^{t-1}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\mathrm{base}}}[\left(r_i^t(x,y_1)-r_i^t(x,y_2) - (r_i^*(x,y_1)-r_i^*(x,y_2))\right)^2] + \frac{d_{\mathrm{cover}}(1/T)}{4\mu_i}\right),
% % \end{align*}
% % where $\kappa = \sup_{x,y} \frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}(y\mid x)}}$ \citep{cen2024value}.

% % Now by the MLE loss, there exists a constant $C$ such that 
% % \begin{align*}
% %     \sum_{t=1}^T \sum_{i=1}^m &\frac{\eta}{T} (L_i^t(\theta_i^*) - L_i^t(\theta_i^t))
% %     \\&\le 2\sum_{i=1}^m \eta \log(|\cR|/\delta) - \frac{C}{T}\sum_{t=1}^T \sum_{i=1}^m\eta\sum_{j \in \cD_i^{t-1}}\EE_{y\sim \pi^j} \left[\Delta_i^t(x,y)^2\right].
% % \end{align*}

% % Choose $\mu_i = \frac{C}{\alpha_i^* \exp(4/\beta)\kappa\sqrt{T}}$, then we can get 

% % \begin{align*}
% %     D(\tilde{\pi}^T) - D(\pi^*) &\le 2 \sum_{i=1}^m \eta\log(|\cR|/\delta) - \eta\cdot \frac{C}{T} \underbrace{\sum_{t=1}^T \sum_{i=1}^m  \sum_{j \in \cD_i^{t-1}}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\mathrm{base}}}[\Delta_i^t(x,y)^2]}_{\text{(A)}} \\
% %     &\qquad + \frac{d\exp(4/\beta) \kappa}{4\sqrt{T}} + \frac{C}{T^{3/2}}\underbrace{\cdot \sum_{t=1}^T \sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)^2]}_{\text{(B)}}.
% % \end{align*}
% % Note that we have $$\mathrm{(A)} \ge \frac{1}{me^B}\mathrm{(B)},$$
% % then we choose $\eta = \frac{me}{\sqrt{T}}$, 
% % \begin{align*}
% %     D(\tilde{\pi}^T) - D(\pi^*) &\le 2 \sum_{i=1}^m \log(|\cR|/\delta) - \eta\cdot  \frac{C}{T} \mathrm{(A)} + \frac{d\exp(4/\beta) \kappa}{4\sqrt{T}} + \frac{C}{T^{3/2}} \mathrm{(B)}\\
% %     &\le 2m\eta  \log(|\cR|/\delta) + \frac{d\exp(4/\beta) \kappa}{4\sqrt{T}}\\
% %     &\le \frac{2m^2e\log (|\cR|/\delta)}{\sqrt{T}}+ \frac{d\exp(4/\beta) \kappa}{4\sqrt{T}}.
% % \end{align*}



% %\section{Proof of Theorem \ref{thm:malfareoffline}}
% %TODO. 


% % \subsection{Proof of Theorem \ref{thm:welfare}}
% % \begin{proof}
% %     The change mainly focus on the calculation of $\sum_{n=1}^N d^2(\overline{V}^t, W_n^*).$
% %     We have 
% %     \begin{align*}
% %         \sum_{n=1}^N d^2(\overline{V}^t, W_n^*)& = \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^T)\|^2\\
% %         & \le \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^{T-1})\|^2\\
% %         &=\sum_{n=1}^N\left(\frac{T-1}{T}\right)^2 d(\overline{V}^{T-1}, W^*_n)^2 + \frac{1}{T^2} \|V^T - \Pi_{W^*_n}(\overline{V}^{T-1})\|^2 \\
% %     &\qquad + \sum_{n=1}^N \frac{2(T-1)}{T^2}(\overline{V}^{T-1} - \Pi_{W^*_n} (\overline{V}^{T-1}))\cdot (V^T - \Pi_{W^*_n}(\overline{V}^{T-1}))\\
% %     &\le \sum_{n=1}^N\left(\frac{T-1}{T}\right)^2 d(\overline{V}^{T-1}, W^*_n)^2 + \frac{NB^2m}{T^2}\\
% %     &\qquad + \sum_{n=1}^N \frac{2(T-1)}{T^2}(\overline{V}^{T-1} - \Pi_{W^*_n} (\overline{V}^{T-1}))\cdot (V^T - \Pi_{W^*_n}(\overline{V}^{T-1}))
% %     \end{align*}


% % Thus by iteration we can have 
% % \begin{align*}
% %      T^2 \sum_{n=1}^N d(\overline{V}^T, W^*_n)^2 \le T\cdot B^2mN + \sum_{n=1}^N\sum_{t=1}^T 2(t-1) (\overline{V}^{t-1}-\Pi_{W^*_n}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*_n}(\overline{V}^{t-1}))
% % \end{align*}

% % The term \begin{align*}&\sum_{n=1}^N(\overline{V}^{t-1}-\Pi_{W^*_n}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*_n}(\overline{V}^{t-1}))\\
% % &=\sum_{n=1}^N d(\overline{V}^{t-1}, W_n^*)\cdot d_n^t \cdot ( \Pi_{W^*_n}(\overline{V}^{t-1})-V^t)\\
% % &= \sum_{n=1}^N d(\overline{V}^{t-1}, W_n^*)\cdot \left(J(r_1^*, \cdots, r_m^*, d^t_n, \pi^*) + d(S(\pi^*), W_n^*) - J(\hat{r}_1, \cdots, \hat{r}_m, d_n^t ,\pi^t)\right)\\
% % & \le  \sqrt{\sum_{n=1}^N d^2(\overline{V}^{t-1}, W_n^*)}\cdot\left(J(r_1^*, \cdots, r_m^*, d^t, \pi^*) - J(\hat{r}_1, \cdots, \hat{r}_m, d^t, \pi^t)+\sqrt{\sum_{n=1}^N d^2(S(\pi), W_n^*)}\right).\end{align*}
% % The last inequality uses the Cauchy's inequality and the definition of $d^t$.
% % \end{proof}

% % % \subsection{Proof of Theorem \ref{thm:generalp}}

% % % \begin{proof}

% % % \begin{align}
% % %     \sum_{n=1}^N d^p(\overline{V}^t, W_n^*) & = \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^T)\|^p\\
% % %     & \le \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^{T-1})\|^p\\
% % %     & = \sum_{n=1}^N \left\|\frac{T-1}{T}(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1})) + \frac{1}{T}(V^T - \Pi_{W_n^*}(\overline{V}^{T-1}))\right\|^p. \label{ineq:pnorm}
% % % \end{align}
% % % By the basic inequality, for $q>1$, we have 
% % % $$(a+b)^q - a^q - b^q $$ is non-decreasing for both $a,b>0$. 
% % % Then for the vector $x_n,y_n \in \RR^m$ with $x_n=(T-1)(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1})), y_n= (V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))$ and $p>2,$ we know $q>1$ and $\|x_n\| \le 2TB, \|y_n\| \le 2B$. Hence, 
% % % \begin{align*}\|x_n+y_n\|^p &\le (\|x_n\|^2 + \|y_n\|^2 + 2\langle x_n,y_n\rangle )^{q}\le (\|x_n\|^2 + \|y_n\|^2)^{q} + 2^{q}\langle x_n,y_n\rangle (\|x_n\|^2+\|y_n\|^2)^{q-1}\\&\le \|x_n\|^p  + \|y_n\|^p + (2B)^p \cdot \left((T^2+1)^{q}-T^p - 1\right)+ 2^{q}\langle x_n,y_n\rangle (\|x_n\|^2+\|y_n\|^2)^{q-1}\\
% % % &\le \|x_n\|^p  + \|y_n\|^p + (4B)^{2q} \cdot \left(T^{p-2}\right)+ 2^{q}\langle x_n,y_n\rangle (\|x_n\|^2+\|y_n\|^2)^{q-1}\end{align*}
% % %  we can further bound the inequality \eqref{ineq:pnorm} as 
% % % \begin{align}
% % %     T^p \sum_{n=1}^N d^p(\overline{V}^{T}, W_n^*) &\le  \sum_{n=1}^N \|x_n+y_n\|^p \\&\le (T-1)^p\sum_{n=1}^N d^p(\overline{V}^{T-1}, W_n^*) + n(2B)^p  + n(4B)^{2q} \cdot T^{p-2} \\
% % %     &\qquad  + 2^{q} (T-1)\sum_{n=1}^N (\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))(\|x\|^2 + \|y\|^2)^{q-1}.
% % % \end{align}
% % % Note that $\|y\| \le 2B,$ then $\langle x_n, y_n\rangle (\|x\|^2 + \|y\|^2)^{q-1} \le \max\{2^{q-1}\cdot \|x\|^{p-2}\cdot \langle x_n,y_n\rangle , (4B)^{p}\}\le 2^{q-1}\cdot \|x\|^{p-2}\cdot \langle x_n,y_n\rangle + (4B)^{p},$ then we can finally get 
% % % \begin{align}
% % % T^p \sum_{n=1}^N d^p(\overline{V}^{T}, W_n^*) &\le (T-1)^p \sum_{n=1}^N d^p(\overline{V}^{T-1}, W_n^*) + \\
% % % &\qquad + 2^{p}(T-1)\sum_{n=1}^N (\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))d^{p-2}(\overline{V}^{T-1}, W_n^*)\\
% % % &\qquad \qquad + n(4B)^{2q}+ n(2B)^p + n(4B)^{2q} T^{p-2}
% % % \end{align}
% % % Now for $p>2$, we have 
% % % \begin{align}
% % %     T^p \sum_{n=1}^N d^p(\overline{V}^{T}, W_n^*) &\le(T-1)^p \sum_{n=1}^N d^p(\overline{V}^{T-1}, W_n^*) + \cO(n(4B)^{2q}T^{p-2}) \\&\qquad + 2^{p}(T-1)\sum_{n=1}^N (\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))d^{p-2}(\overline{V}^{T-1}, W_n^*).
% % % \end{align}
% % % Hence by the recursion, we can get 
% % % \begin{align}
% % %     &\sum_{t=1}^T \sum_{n=1}^N (t-1)(\overline{V}^{t-1}-\Pi_{W_n^*}(\overline{V}^{t-1}))(V^t-\Pi_{W_n^*}(\overline{V}^{t-1}))d^{p-2}(\overline{V}^{t-1}, W_n^*)\\
% % %     &\le \sum_{t=1}^T \sum_{n=1}^N (t-1)d^{p-1}(\overline{V}^{t-1}, W_n^*)d_n^t\cdot (\Pi_{W_n^*}(\overline{V}^{t-1}) - V^t)\\
% % %     &\le  \sum_{t=1}^T\sum_{n=1}^N (t-1)d^{p-1}(\overline{V}^{t-1}, W_n^*)\left(J(r_1^*, \cdots, r_m^*, d_n^t, \pi^*)+d(S(\pi^*), W_n^*) - J(\hat{r}_1, \cdots, \hat
% % %     {r}_m), d_n^t, \pi^t)\right)\\
% % %     &\le \sum_{t=1}^T(t-1)\left(\sum_{n=1}^N d^p(\overline{V}^{t-1}, W_n^*)\right)^{\frac{p-1}{p}} \cdot \left(J(r_1^*, \cdots, r_m^*, d^t, \pi^*)+ \sqrt[p]{\sum_{n=1}^N d^p(S(\pi^*), W_n^*)}- J(\hat{r}_1, \cdots, \hat
% % %     {r}_m, d^t, \pi^t)\right)\\
% % %     &\le \sum_{t=1}^T(t-1)\left(\sum_{n=1}^N d^p(\overline{V}^{t-1}, W_n^*)\right)^{\frac{p-1}{p}} \cdot \left(D_p(\pi^*) + \eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t)\right)
% % % \end{align}




\section{Proof of Theorems}
\subsection{Proof of Theorem \ref{thm:relationship_maximin}}\label{app:proof maxmin}

\begin{proof}
    Then, suppose the reward vector $S(\pi)$ is $(s_1,\cdots, s_m)^\top$, then by the definition of $D(\pi)$, we have 
    \begin{align*}
        D(\pi) = \sum_{i=1}^m \max\{c-s_i,0\}^2\le \sum_{i=1}^m \max\{c-s_i^*,0\}^2,
    \end{align*}
    where $s_i^* = (S(\pi^*))_i = \EE_{\pi^*}[r_i^*(x,y) - \beta\DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})].$
    Hence we have 
    \begin{align*}\max\{c-\min_i s_i,0\}^2&\le  \sum_{i=1}^m \max\{c-s_i,0\}^2\\&\le \sum_{i=1}^m \max\{c-s_i^*,0\}^2\\&\le m\cdot (c-\min_i s_i^*)^2\le m(c-c^*)^2,\end{align*}
which implies that .
    $$c-\min_i s_i \le \sqrt{m}\cdot |c-c^*|,$$ and 
    $$c^*-\min_i s_i \le (\sqrt{m}+1) |c^*-c|.$$
 

    Thus, if $c$ is selected such that $|c-c^*|$ is small, then we can also find a policy $\pi$, such that $$\min_i \EE_\pi[r_i^*(x,y) - \DD_{\mathrm{KL}}(\pi \| \pi_{\mathrm{ref}})] \ge c^*-(\sqrt{m}+1) |c^*-c|.$$
    \end{proof}

\subsection{Proof of Theorem \ref{thm:offline}}

For simplicity, for the following proof, we use $\EE_{\pi^t}[\cdot]$ to represent $\EE_{x\sim \rho, y\sim \pi^t(\cdot \mid x)}[\cdot]$.
Since we do not assume the target set $W^*$ is approachable, we have the following property for the approachability:

\begin{lemma}\label{lemma:approach}
For each $\theta \in \RR_{\ge0}^m$ with $\|\theta\|_2 = 1$, we have 
\begin{align*}\min_{x \in W^*}\langle \theta, x\rangle&\le  \EE_{\pi^*}[\langle \theta, r_i^*(x,y)\rangle-\sum_{i=1}^m \theta_i\beta \DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})] + D(\pi^*) = \|\theta\|_1\cdot J(r_1^*, \cdots, r_m^*, \frac{\theta}{\|\theta\|_1}, W^*, \pi^*) +  D(\pi^*)\\&\le\sqrt{m}\cdot J(r_1^*, \cdots, r_m^*, \frac{\theta}{\|\theta\|_1}, W^*, \pi^*) +  D(\pi^*) \end{align*}   
\end{lemma}
\begin{proof}
    By the definition of $D(\pi^*) = d(S(\pi^*), W^*)$, we know that there exists a vector $p$ with $S(\pi^*) + p \in W^*$ and $\|p\|_2 = D(\pi^*).$
Then we can have $$\min_{x \in W^*}\langle \theta, x\rangle \le \langle \theta, S(\pi^*) + p\rangle\le \EE_{\pi^*}[\langle \theta, r_i^*(x,y)\rangle-\sum_{i=1}^m \theta_i\beta \DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})] + D(\pi^*).$$
The last inequality holds because of $\|\theta\|_1 \le \sqrt{m}$.\end{proof}

We can first bound the regret by 
\begin{align}
    &D(\tilde{\pi}^T) - D(\pi^*) \nonumber\\ &= d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\beta \DD_{\mathrm{KL}}(\tilde{\pi}^T\| \pi_{\mathrm{ref}})) - D(\pi^*)\nonumber\\
    &\le d\left(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\right) - D(\pi^*)\nonumber\\
    &= \underbrace{d\left(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\right) - d\left(W^*, \frac{1}{T}\sum_{t=1}^T\EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\right)}_{\textrm{(A)}} \nonumber\\
    &\qquad + d\left(W^*, \frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\right) - D(\pi^*)\nonumber\\
    &=\mathrm{(A)}
    + d\left(W^*, \overline{V}^T\right) - D(\pi^*).\label{eq:finalregret_key}
\end{align}
The inequality uses the fact that 
$$\DD_{\mathrm{KL}}(\tilde{\pi}\| \pi_{\mathrm{ref}}) \le \frac{1}{T}\left(\sum_{t=1}^T\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})\right).$$

Recall that $$D(\pi) = d(W^*, \EE_{\pi^t}[r(x,y)]-\beta \DD_{\mathrm{KL}}(\pi\|\pi_{\mathrm{ref}}))$$ and $\pi^* = \min_\pi D(\pi^*)$. Now, by Lemma \ref{lemma:approach}, for each $\theta \in \RR^m$ with $\|\theta\|_1 \le 1$, we have 
$$\min_{x \in W^*}\langle \theta, x\rangle\le  \EE_{\pi^*}[\langle \theta, r_i^*(x,y)\rangle-\sum_{i=1}^m \theta_i\beta \DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})] + D(\pi^*) = J(r_1^*, \cdots, r_m^*, \theta, W^*, \pi^*) +  D(\pi^*).$$

Denote $V^t \in \RR^m$ with $(V^t)_i = \EE_{\pi^t}[\hat{r}_i^t(x,y) - \beta\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})]$, and $\frac{1}{t}\overline{V}^t = \sum_{i=1}^t V^i$. We have 
\begin{align*}
    d(\overline{V}^T, W^*)^2 &= \|\overline{V}^T - \Pi_{W^*} (\overline{V}^T)\|^2 \\
    &\le \|\overline{V}^T - \Pi_{W^*} (\overline{V}^{T-1})\|^2\\
    & = \left(\frac{T-1}{T}\right)^2 d(\overline{V}^{T-1}, W^*)^2 + \frac{1}{T^2} \|V^T - \Pi_{W^*}(\overline{V}^{T-1})\|^2 \\
    &\qquad + \frac{2(T-1)}{T^2}(\overline{V}^{T-1} - \Pi_{W^*} (\overline{V}^{T-1}))\cdot (V^T - \Pi_{W^*}(\overline{V}^{T-1})).\end{align*}
First, based on the definition of $W^*$, it is easy to show that $d^t \succeq 0.$
$\pi^t$ is the optimal policy such that $$\EE_{\pi^t}[\langle d^t, \hat{r}(x,y) \rangle - \sum_{i=1}^m d^t_i\beta \DD_{\mathrm{KL}}(\pi^t \|\pi_{\mathrm{ref}})] \ge  \EE_{\pi_{\mathrm{ref}}}[\langle d^t , \hat{r}(x,y) \rangle]\ge 0,$$
thus $(\sum_{i=1}^m d_i^t)\cdot \beta \DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}}) \le \EE_{\pi^t}[d^t\cdot  \hat{r}(x,y)] \le B$. Hence, given $d^t \succeq 0$ and $\|d^t\|_2 =1,$
\begin{align*}
    \beta \DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}}) \le \frac{B}{\sum_{i=1}^m d_i^t}\le B.
\end{align*}
we have $|(V^t)_i| \le B$ and 
\begin{align*}
    \|V^T - \Pi_{W^*}(\overline{V}^{T-1})\|^2 \le B^2m.
\end{align*}
Thus by iteration we can have 
\begin{align*}
    T^2 d(\overline{V}^T, W^*)^2 \le T\cdot B^2m + \sum_{t=1}^T 2(t-1) (\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*}(\overline{V}^{t-1})).
\end{align*}
Now, by the definition of $d^t$, we have
\begin{align*}
    (\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*}(\overline{V}^{t-1})=
    d(\overline{V}^{t-1}, W^*)\cdot d^t\cdot (\Pi_{W^*}(\overline{V}^{t-1}) - V^t).
    \end{align*}
    Then, we prove the following lemma.
    \begin{lemma}\label{lemma:projection}
        $\min_{x\in W^*} \langle d^t, x\rangle = d^t \cdot \Pi_{W^*}(\overline{V}^{t-1}).$
    \end{lemma}
    \begin{proof}
     In fact, we only need to prove that for any $x \in W^*$, $\langle \overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}), x-\Pi_{W^*}(\overline{V}^{t-1})\rangle \le 0$. Suppose there exists $x \in W^*$ such that $\langle \overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}), x-\Pi_{W^*}(\overline{V}^{t-1})\rangle >0$, then since $W^*$ is a convex set, for any $\lambda \in (0,1)$, we have $x_\lambda = \lambda x + (1-\lambda) \Pi_{W^*}(\overline{V}^{t-1}) \in W^*$. Consider the line $$\Pi_{W^*}(\overline{V}^{t-1}) + t \frac{\Pi_{W^*}(\overline{V}^{t-1})-x}{\|\Pi_{W^*}(\overline{V}^{t-1})-x\|},\  \  t \in \RR.$$ Also, we consider the  projection of $\overline{V}^{t-1}$ on this line, and denote it as $p$. Then we can get 
    $$0<\langle\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1})- p + p, x -\Pi_{W^*}(\overline{V}^{t-1})\rangle  = \langle p-\Pi_{W^*}(\overline{V}^{t-1}),  x -\Pi_{W^*}(\overline{V}^{t-1})\rangle. $$
    Hence when $\lambda \to 0$, $x_\lambda$ is between $p$  and $\Pi_{W^*}(\overline{V}^{t-1})$. Also, $$\|\overline{V}^{t-1}-x_\lambda \|^2 = \|\overline{V}-p\|^2 + \|p-x_\lambda\|^2 \le \|\overline{V}-p\|^2 + \|p-\Pi_{W^*}(\overline{V}^{t-1})\|^2 \le \|\Pi_{W^*}(\overline{V}^{t-1})-d^t \|^2,$$ which contradicts the selection of $\Pi_{W^*}(\overline{V}^{t-1}).$
 \end{proof}
Now, by Lemma \ref{lemma:approach} and Lemma \ref{lemma:projection}, we can get 
\begin{align*}
    d^t \cdot \Pi_{W^*}(\overline{V}^{t-1}) \le J(r_1^*, \cdots, r_m^*, d^t, \pi^*) + D(\pi^*).
\end{align*}
Then, since we define $\overline{d^t} = d^t/\|d^t\|_1$,  we can continue the analysis by
    \begin{align*}
    &(\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*}(\overline{V}^{t-1}))\\&=d(\overline{V}^{t-1}, W^*)\cdot \left( \|d^t\|_1J(r_1^*, r_2^*, \cdots, r_m^*, \overline{d^t}, \pi^*) + D(\pi^*) - d^t\cdot  V^t\right)\\
    & = d(\overline{V}^{t-1}, W^*)\cdot (\|d^t\|_1\cdot \left(J(\hat{r}_1^*, \cdots, \hat{r}_m^*, \overline{d^t}, \pi^*) - J(\hat{r}_1, \cdots, \hat{r}_m, \overline{d^t}, \pi^t)\right) + D(\pi^*))\\
    & = d(\overline{V}^{t-1}, W^*) \cdot (\|d^t\|_1\cdot (\eta \sum_{i=1}^m L_i(\theta^t) - \eta \sum_{i=1}^m L_i(\theta^*) )+ D(\pi^*)).
\end{align*}
Thus we can get 
\begin{align*}
    T d(\overline{V}^T, W^*)^2 &\le   B^2m +
    \sum_{t=1}^T \frac{2(t-1)}{T}d(\overline{V}^{t-1}, W^*) \cdot (\eta \|d^t\|_1\sum_{i=1}^m L_i(\theta^t) - \eta \|d^t\|_1\sum_{i=1}^m L_i(\theta^*) + D(\pi^*)).
\end{align*}

Now we use induction method to show that $$d(\overline{V}^t, W^*) \le D(\pi^*) +  \frac{\eta}{T}\sum_{t=1}^T \|d^t\|_1\sum_{i=1}^m (L_i(\theta^t) - L_i(\theta^*)) + 2Bm/\sqrt{t}.$$ When $t = 1$, the inequality holds by 
\begin{align*}
    \|d(\overline{V}^1, W^*)-D(\pi^*)\|\le d(\overline{V}^1, S(\pi^*)) \le 2B.
\end{align*}
Denote $A_j=\eta\cdot\|d^j\|_1\cdot  (\sum_{i=1}^m (L_i(\theta^*) - L_i(\theta^j)))$ and $S_t = \sum_{j=1}^{t}A_j$, then for all $t \in [T-1],$  
suppose we have 
\begin{align*}
    d(\overline{V}^{t-1}, W^*) \le D(\pi^*) + \frac{1}{t-1} S_{t-1} + 2B\left(\frac{\sqrt{m}}{\sqrt{t-1}}\right).
\end{align*}
Then we substitute these induction hypothesis into the recursion inequality and get 
\begin{align*}
    &T d(\overline{V}^T, W^*)^2\\&\le B^2m + \sum_{t=1}^T \frac{2(t-1)}{T} \left(D(\pi^*) + \frac{1}{t-1}S_{t-1}+ 2B\left(\frac{\sqrt{m}}{\sqrt{t-1}}\right)\right)\left(A_t + D(\pi^*)\right)\\
    &\le B^2m + \sum_{t=1}^T \left( \frac{2(t-1)}{T}D(\pi^*) + \frac{1}{T}S_{t-1} + 2B\left(\frac{2\sqrt{m}\sqrt{t-1}}{T}\right)\right)\left(A_t + D(\pi^*)\right)\\
    &= B^2m + (T-1)D(\pi^*)^2 + \sum_{t=1}^T \frac{1}{T}S_{t-1}A_t + \sum_{t=1}^T \left(\frac{1}{T} S_{t-1} + \frac{2(t-1)}{T}A_t\right) D(\pi^*)\\
    &\qquad + \sum_{t=1}^T 2B\left(\frac{2\sqrt{m}\sqrt{t-1}}{T}\right)(A_t + D(\pi^*))\\
    & \le  B^2m + (T-1)D(\pi^*)^2 + \frac{1}{T} S_T^2 + \sum_{t=1}^T D(\pi^*) \cdot \left(\frac{T+t-1}{T}A_t\right)  + 2\sqrt{m}\cdot 2B\sqrt{T} D(\pi^*) + (2\sqrt{m}\cdot 2B/\sqrt{T}) S_T\\
    &\le B^2m + (T-1)D(\pi^*)^2 + \frac{1}{T} S_T^2 +  D(\pi^*) \cdot \left(2S_T\right) + 2\sqrt{m}\cdot 2B\sqrt{T} D(\pi^*) + 2\sqrt{m}\cdot 2B/\sqrt{T} S_T\\
    &\le T \cdot (2B\sqrt{m}/\sqrt{T} + D(\pi^*) + \frac{1}{T}S_T)^2.
\end{align*}
%The final inequality uses the fact that $C \ge B$. 

Thus we have $$d(\overline{V}^T, W^*) \le D(\pi^*) + \frac{\eta}{T}\sum_{t=1}^T \|d^t\|_1\sum_{i=1}^m (L_i(\theta^t) - L_i(\theta^*)) + \frac{2B\sqrt{m}}{\sqrt{T}}.$$

Now we derive the final regret.
% \begin{align*}
%     &D(\tilde{\pi}^T) - D(\pi^*) \\ &= d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\beta \DD_{\mathrm{KL}}(\tilde{\pi}^T\| \pi_{\mathrm{ref}})) - D(\pi^*)\\
%     &\le d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - D(\pi^*)\\
%     &= \underbrace{d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - d(W^*, \frac{1}{T}\sum_{t=1}^T\EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}}))}_{\textrm{(A)}} \\
%     &\qquad + d(W^*, \frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - D(\pi^*)\\
%     &=\mathrm{(A)}
%     + d\left(W^*, \frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}[\hat{r}^T(x,y)]-\frac{\beta}{T} \sum_{t=1}^T\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})\right) - D(\pi^*).
% \end{align*}
% The inequality uses the fact that 
% $$\DD_{\mathrm{KL}}(\tilde{\pi}\| \pi_{\mathrm{ref}}) \le \frac{1}{T}\left(\sum_{t=1}^T\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})\right).$$
By inequality Eq.~\eqref{eq:finalregret_key}, we can get
\begin{align*}D(\tilde{\pi}^T) - D(\pi^*)&\le \mathrm{(A)} + d(W^*, \overline{V}^T) - D(\pi^*)\\
&\le \mathrm{(A)} +  \underbrace{\frac{\eta}{T}\sum_{t=1}^T \|d^t\|_1\sum_{i=1}^m (L_i(\theta^t) - L_i(\theta^*))}_{\text{(B)}} + \frac{2B\sqrt{m}}{\sqrt{T}}.\end{align*}
Now we consider the error term $\text{(A)}$, which represents the approximation error of the reward function. Now we have 
\begin{align*}
    \text{(A)}&\le \frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}\left[\sum_{i=1}^m  |\hat{r}_i^t(x,y) - r_i^*(x,y)|\right]\\
    & = \frac{1}{T}\sum_{t=1}^T \sum_{i=1}^m\EE_{\pi^t}\left[\|\phi_i(x,y)\|_{(\Sigma_{\cD_i}+\lambda I)^{-1}} \|\theta_i^t - \theta_i^*\|_{\Sigma_{\cD_i}+\lambda I } \right].
\end{align*}
Similar to \citep{cen2024value}, since $(r^\theta,\pi^\theta)$ can be formulated as a saddle point of the objective $J(r,d,\pi) + \sum_{i=1}^m \eta L_i(\theta_i)$ for any direction $d \in (\RR^{+})^m$, we have 
\begin{align*}
    \eta \nabla_{\theta_i} L_i(\theta_i) + d_i \EE_{x\sim \rho, y\sim \pi^{\theta}}[\phi_i(x,y)] + \lambda_1\EE_{x\sim \rho, y\sim \pi_{\mathrm{base}}}[\phi_i(x,y)] = 0.
\end{align*}
Also, denote $\theta_{\mathrm{MLE}}=\argmin_{\theta \in \Theta} \sum_{i=1}^m \eta L_i(\theta_i), $ we have 
\begin{align*}
    \eta \nabla_{\theta_i} L_i(\theta_{i,\mathrm{MLE}}) + \lambda_2 \EE_{x\sim \rho, y\sim \pi_{\mathrm{base}}}[\phi_i(x,y)] = 0.
\end{align*}
%Hence we have $\nabla_{\theta_i}$

Follow the same derivation in \citep{cen2024value}, we can get 
\begin{align*}
    \|\theta_i^t - \theta_{i,\mathrm{MLE}}\|_{\Sigma_{\cD_i + \lambda  I}} &\le \frac{d_i}{\eta} \cdot \frac{(3+e^{B'})4(\lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-1}}{M} + 2\sqrt{
    \lambda (B')^2
    }\\
    &\le \frac{(3+e^{B'})4(\lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-1}}{\sqrt{M}} + 2\sqrt{
    \lambda (B')^2},
\end{align*}
where $B'$ is the upper bound of norm of $\theta,$ i.e. $\max_{\theta \in \Theta}\|\theta\|_2 \le B'.$

Now we recall the Lemma 3.1 in \citep{zhu2023principled}, which bounds the true parameter and the MLE parameter. 
\begin{lemma}[Lemma 3.1 in \citep{zhu2023principled}] $\lambda>0$ is a positive constant. For $\delta \in (0,1)$, with probability at least $1-\delta$, we will have
    \begin{align*}
    \|\theta_i^* - \theta_{i,\mathrm{MLE}}\|_{\Sigma_{\cD_i} + \lambda I } \le \cO\left((3+e^{B'})\sqrt{\frac{d+\log(1/\delta)}{M}} + \sqrt{\lambda (B')^2}\right).
\end{align*}
Also, $L_i(\theta)$ is a convex function. In fact, 
\begin{align*}
    \frac{1}{3+e^{B'}}\Sigma_{\cD_i} \preceq \frac{1}{M}\nabla_\theta^2 L_i(\theta) \preceq \frac{1}{4}\Sigma_{\cD_i}.
\end{align*}
\end{lemma}
Hence, we get
\begin{align*}
    \text{(A)} &\le \frac{1}{T}\sum_{t=1}^T \sum_{i=1}^m\EE_{\pi^t}\left[\|\phi_i(x,y)\|_{(\Sigma_{\cD_i}+\lambda I)^{-1}} \|\theta_i^t - \theta_i^*\|_{\Sigma_{\cD_i}+\lambda I } \right]\\
    &\le \frac{1}{T}\sum_{t=1}^T \sum_{i=1}^m \|\EE_{\pi^t}\phi_i(x,y)\|_{(\Sigma_{\cD_i}+\lambda I)^{-1}} \cdot \cO\left(\frac{(3+e^{B'})4(\lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-1}\sqrt{d+\log(1/\delta)}}{\sqrt{M}}+\sqrt{\lambda (B')^2}\right)\\
    &\le \widetilde{\cO}\left(\frac{m(3+e^{B'})4(\lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-2}\sqrt{d+\log(1/\delta)}}{\sqrt{M}}+\sqrt{\lambda (B')^2}\right).
\end{align*}
The notation $\widetilde{\cO}(\cdot)$ hides all the logarithm term like $\log(1/\delta)$.

Now we consider the term $\text{(B)}.$ First, based on the convexity of $L_i(\theta)$, we have 
\begin{align*}
    L_i(\theta_i^t) - L_i(\theta_{i,\mathrm{MLE}}) & \le \langle \nabla_\theta L_i(\theta^t), \theta_i^t - \theta_{i,\mathrm{MLE}}\rangle \\
    & = \frac{1}{\eta} \langle -d_i \EE_{x\sim \rho, y\sim \pi^\theta}[\phi_i(x,y)] - \lambda_1 \EE_{x\sim \rho, y\sim \pi_{\mathrm{base}}}[\phi_i(x,y)], \theta_i^t - \theta_{i,\mathrm{MLE}}\rangle\\
     & = \frac{d_i}{\eta}\langle -\EE_{x\sim \rho, y\sim \pi^\theta}[\phi_i(x,y)] - \EE_{x\sim \rho, y\sim \pi_{\mathrm{base}}}[\phi_i(x,y)], \theta_i^t - \theta_{i,\mathrm{MLE}}  \rangle \\
     &\le \frac{d_i}{\eta} \|\EE_{x\sim \rho, y\sim \pi^\theta}[\phi_i(x,y)]-\EE_{x\sim \rho, y\sim \pi_{\mathrm{base}}}[\phi_i(x,y)]\|_{(\Sigma_{\cD_i} + \lambda I )^{-1}}\|\theta_i^t - \theta_{i,\mathrm{MLE}}\|_{\Sigma_{\cD_i} + \lambda I }\\
     &\le \frac{2d_i}{\eta}\cdot (\lambda_{\min}(\Sigma_{\cD_i})+\lambda)^{-1}\cdot \|\theta_i^t - \theta_{i,\mathrm{MLE}}\|_{\Sigma_{\cD_i} + \lambda I }\\
     &\le \cO\left( \frac{(3+e^{B'})(\lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-2}}{\sqrt{M}} + \frac{4}{\eta}\sqrt{\lambda (B')^2}\cdot (\lambda_{\min}(\Sigma_{\cD_i})+\lambda )^{-1}\right).
\end{align*}
The last inequality uses the fact that $d_i \le 1.$
Also, with probability at least $1-\delta$, we have 
$$L_i(\theta_{i,\mathrm{MLE}}) - L_i(\theta^*) \le \tilde{\cO}(1).$$
Now sum over $t \in [T], $ we can get 
\begin{align*}
    \text{(B)}&\le \frac{\eta}{T}\sum_{t=1}^T\|d^t\|_1 \sum_{i=1}^m (L_i(\theta_i^t) - L_i(\theta^*))\\
    & \le \sqrt{m}\cdot \frac{m}{\sqrt{M}}\widetilde{\cO}\left(\frac{(3+e^{B'})(\min_i\lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-2}}{\sqrt{M}} + \frac{4}{\eta}\sqrt{\lambda (B')^2}\cdot (\min_i\lambda_{\min}(\Sigma_{\cD_i})+\lambda )^{-1} + 1\right),
\end{align*}
where the last inequality uses the fact that $\eta = 1/\sqrt{M}$ and $\|d^t\|_1 \le \sqrt{m}$.
Hence, we have 
\begin{small}
\begin{align*}
    &D(\tilde{\pi}^T)-D(\pi^*) \\&\le \text{(A)} + \text{(B)} + \frac{2Bm}{\sqrt{T}}\\
    &\le \widetilde{\cO}\left(\frac{m^{3/2}(3+e^{B'}) (\min_i\lambda_{\min}(\Sigma_{\cD_i} + \lambda)^{-2}\sqrt{d+\log(1/\delta)}}{\sqrt{M}} + \frac{4m^{3/2}}{\eta\sqrt{M}}B'\sqrt{\lambda}\cdot (\min_i\lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-1} + \frac{m^{3/2}}{\sqrt{M}}+\frac{B\sqrt{m}}{\sqrt{T}}\right)\\
    &\le \widetilde{\cO}\left(\frac{m^{3/2}(3+e^{B'}) (\min_i\lambda_{\min}(\Sigma_{\cD_i} + \lambda)^{-2}\sqrt{d+\log(1/\delta)}}{\sqrt{M}} + \frac{4m^{3/2}B'}{\sqrt{M}}\cdot (\min_i \lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-1} + \frac{m^{3/2}}{\sqrt{M}} + \frac{B\sqrt{m}}{\sqrt{T}}\right)\\
    & = \frac{m}{\sqrt{M}}\cdot \widetilde{\cO}\left(\text{poly}\left(e^{B'}, \min_i\lambda_{\min}(\Sigma_{\cD_i})^{-1}, \sqrt{d+\log(1/\delta)},B'\right)\right)+ \widetilde{\cO}\left(\frac{B\sqrt{m}}{\sqrt{T}}\right).
\end{align*}
\end{small}
The last step is because $\eta = 1/\sqrt{M}, \lambda = 1/M$. Hence we complete the proof. \qed
\subsection{Proof of Theorem \ref{thm:malfareoffline}}\label{sec: malfare_proof_offline}

\begin{proof}
The main proof framework is similar to Theorem \ref{thm:offline}. The difference lies in the approach to deal with the aggregated $p$-norm of the distance. 
\begin{align}
    \sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^t, W_n^*) & = \sum_{n=1}^N\zeta_n \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^T)\|^{2q}\\
    & \le \sum_{n=1}^N \zeta_n\|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^{T-1})\|^{2q}\\
    & = \sum_{n=1}^N \zeta_n\left\|\frac{T-1}{T}(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1})) + \frac{1}{T}(V^T - \Pi_{W_n^*}(\overline{V}^{T-1}))\right\|^{2q}. \label{ineq:pnorm}
\end{align}
For the vector $x_n,y_n \in \RR^m$ with $x_n=(T-1)(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1})), y_n= (V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))$  we know $\|x_n\| \le 2TB\sqrt{m}, \|y_n\| \le 2B\sqrt{m}$. Hence, since $q>1$, we have
\begin{align*}\|x_n+y_n\|^{2q} &\le (\|x_n\|^2 + \|y_n\|^2 + 2\langle x_n,y_n\rangle)^{q}\\&\le \|x_n\|^{2q} + 2\langle x_n,y_n\rangle \|x_n\|^{2q-2} + 3^q\cdot T^{2q-2}(2B)^{2q}m^q\end{align*}
 We can further bound the inequality \eqref{ineq:pnorm} as 
\begin{align}
    T^{2q} \sum_{n=1}^N \zeta_n d^{2q}(\overline{V}^{T}, W_n^*) &\le  \sum_{n=1}^N \zeta_n\|x_n+y_n\|^{2q} \\&\le (T-1)^{2q}\sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T-1}, W_n^*) + 12^q\cdot T^{2q-2}B^{2q}m^q\\
    &\qquad  + 2 (T-1)\sum_{n=1}^N \zeta_n(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))\|x_n\|^{2q-2}.
\end{align}
Then since $\|x_n\| = (T-1) d(\overline{V}^{T-1}, W_n^*)$, we can finally get 
\begin{align}
T^{2q} \sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T}, W_n^*) &\le (T-1)^{2q} \sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T-1}, W_n^*) + 12^qT^{2q-2}B^{2q}m^q\\
&\qquad + 2(T-1)^{2q-1}\sum_{n=1}^N \zeta_n(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))d^{2q-2}(\overline{V}^{T-1}, W_n^*).\label{eq:recursion before}
\end{align}
Hence by the recursion, we have 
\begin{align}
    T^{2q} \sum_{n=1}^N \zeta_n d^{2q}(\overline{V}^{T}, W_n^*) &\le 12^qT^{2q-1}B^{2q}m^q \nonumber\\&\qquad + 2(t-1)^{2q-1}\sum_{t=1}^T\sum_{n=1}^N \zeta_n(\overline{V}^{t-1}-\Pi_{W_n^*}(\overline{V}^{t-1}))(V^t-\Pi_{W_n^*}(\overline{V}^{t-1}))d^{2q-2}(\overline{V}^{t-1}, W_n^*).\nonumber
\end{align}
Now the last term at the right side can be further bounded by 
\begin{small}
\begin{align}
    &\sum_{t=1}^T \sum_{n=1}^N \zeta_n (t-1)^{2q-1}(\overline{V}^{t-1}-\Pi_{W_n^*}(\overline{V}^{t-1}))(V^t-\Pi_{W_n^*}(\overline{V}^{t-1}))d^{2q-2}(\overline{V}^{t-1}, W_n^*)\nonumber\\
    &\le \sum_{t=1}^T \sum_{n=1}^N \zeta_n(t-1)^{2q-1}d^{2q-1}(\overline{V}^{t-1}, W_n^*)d_n^t\cdot (\Pi_{W_n^*}(\overline{V}^{t-1}) - V^t)\nonumber\\
    &\le  \sum_{t=1}^T\sum_{n=1}^N\zeta_n (t-1)^{2q-1}d^{2q-1}(\overline{V}^{t-1}, W_n^*)\left(\|d_n^t\|_1 J(r_1^*, \cdots, r_m^*, \overline{d_n^t}, \pi^*)+d(S(\pi^*), W_n^*) - \|d_n^t\|_1 J(\hat{r}_1, \cdots, \hat
    {r}_m, \overline{d_n^t}, \pi^t)\right)\nonumber\\
    &\le \sum_{t=1}^T(t-1)^{2q-1}\left(\sum_{n=1}^N\zeta_n d^{2q}(\overline{V}^{t-1}, W_n^*)\right)^{\frac{2q-1}{2q}}  \nonumber\\&\qquad \cdot \left(\|d^t\|_1 J(r_1^*, \cdots, r_m^*, \overline{d^t}, \pi^*)+ \sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(S(\pi^*), W_n^*)}- \|d^t\|_1 J(\hat{r}_1, \cdots, \hat
    {r}_m, \overline{d^t}, \pi^t)\right)\label{cauchy explain}\\
    &\le \sum_{t=1}^T(t-1)^{2q-1}\left(\sum_{n=1}^N \zeta_n d^{2q}(\overline{V}^{t-1}, W_n^*)\right)^{\frac{2q-1}{2q}} \cdot \left(D_{q}(\pi^*) + \eta \|d^t\|_1\left(\sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t)\right)\right).\label{last term bound}
\end{align}
\end{small}
The inequality Eq.~\eqref{cauchy explain} derives from the definition of $d^t$ in Eq.~\eqref{eq:dir malfare} and Cauchy's inequality.
Let $S_T = \sqrt[2q]{\sum_{n=1}^N \zeta_n d^{2q}(\overline{V}^T, W_n^*)}$, then we can get 
\begin{align*}
    T S_T^{2q}\le 12^q\cdot B^{2q}m^q +  \sum_{t=1}^T \frac{2(t-1)^{2q-1}}{T^{2q-1}}S_{t-1}^{2q-1}\cdot \left(D_q(\pi^*) + \eta \|d^t\|_1\cdot \left( \sum_{i=1}^m L_i^t(\theta^*) -  \sum_{i=1}^m L_i^t(\theta^t)\right)\right).
\end{align*}
Define $A_t =  D_q(\pi^*) + \eta \|d^t\|_1\cdot \left(\sum_{i=1}^m L_i^t(\theta^*) - \sum_{i=1}^m L_i^t(\theta^t)\right),$ then we use the induction to show that there exists a constant $C_q$ such that 
\begin{align}
    S_{t} \le  \left(\frac{1}{t}\sum_{s=1}^t A_s + C_qT^{-1/2q}\right). \nonumber
\end{align}
In fact, it holds when $t = 1$. Now suppose it holds for $t=1,2,\cdots, T-1$, we have 
\begin{align*}
    S_T^{2q}&\le 12^q\cdot B^{2q}m^q/T +  \sum_{t=1}^T \frac{2(t-1)^{2q-1}}{T^{2q-1}}S_{t-1}^{2q-1}\cdot \frac{A_t}{T}\\
    &\le 12^q\cdot B^{2q}m^q/T + 2\sum_{t=1}^T \left(\frac{1}{T}\sum_{s=1}^{t-1}A_s + C_qT^{-1/2q}\right)^{2q-1} \cdot \frac{A_t}{T}\\
    &\le 12^q\cdot B^{2q}m^q/T + 2\sum_{t=1}^T \sum_{k=0}^{2q-1}\binom{2q-1}{k}\frac{1}{T}\left(\sum_{s=1}^{t-1}A_s\right)^{k+1}\cdot \frac{A_t}{T}\cdot (C_q)^{2q-1-k}T^{-\frac{2q-1-k}{2q}}\\
    &\le 12^q\cdot B^{2q}m^q/T  + \sum_{k=0}^{2q-1}\binom{2q-1}{k}(C_q)^{2q-1-k}T^{-\frac{2q-1-k}{2q}}\left(\frac{1}{T}\sum_{t=1}^T A_t\right)^{k+1}.
\end{align*}
Now we choose $C_q = (12^q\cdot B^{2q}m^q)^{\frac{1}{2q}} = \sqrt{12B^2m}$, then we have 
\begin{align*}
    S_T^{2q} &\le \cO(C_q^{2q}/T)  + \sum_{k=0}^{2q-1}\binom{2q-1}{k}(C_q)^{2q-1-k}T^{-\frac{2q-1-k}{2q}}\left(\frac{1}{T}\sum_{t=1}^T A_t\right)^{k+1}\\
    &\le \cO(C_q^{2q}/T)  + \sum_{k=0}^{2q-1}\binom{2q}{k+1}(C_q)^{2q-1-k}T^{-\frac{2q-1-k}{2q}}\left(\frac{1}{T}\sum_{t=1}^T A_t\right)^{k+1}\\
    &\le \left(\frac{1}{T}\sum_{t=1}^T A_t + C_qT^{-1/2q} \right)^{2q}.
\end{align*}
which implies that 
\begin{align}
    S_T \le  \frac{1}{T}\sum_{s=1}^T A_s + C_qT^{-1/2q}=\frac{1}{T}\sum_{s=1}^T A_s + 4B\sqrt{m}\cdot T^{-1/2q}.
\end{align}
Hence we have 
\begin{align*}
    \sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(\overline{V}^T , W_n^*)} - D_q(\pi^*)\le \frac{\eta}{T}\sum_{t=1}^T\|d^t\|_1\sum_{i=1}^m (L_i^t(\theta^*) -L_i^t(\theta^t)) + \widetilde{\cO}(B\sqrt{m} T^{-1/2q}).
\end{align*}
Now we derive the final regret. We can see 
\begin{align}
    &D_q(\tilde{\pi}^T) - D_q(\pi^*)\\
    &=\sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(S(\tilde{\pi}^T), W_n^*)}  - D_q(\pi^*)\nonumber\\
    &=\sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(W_n^*, \EE_{\tilde{\pi}^T}\left[r^*(x,y) \right]- \frac{\beta}{T}\sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}}))\cdot \mathbf{1}^m} \nonumber\\
    &\qquad - \sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(W_n^*, \frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}\left[r^{\theta^t}(x,y) \right]- \frac{\beta}{T}\sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}}))\cdot \mathbf{1}^m}+ \sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(W_n^*, \overline{V}^T)} - D_q(\pi^*)\nonumber\\
    &\le \underbrace{\sqrt[2q]{\sum_{n=1}^N\zeta_n \left(d(W_n^*, \EE_{\tilde{\pi}^T}\left[r^*(x,y) \right]- \frac{\beta}{T}\sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\cdot \mathbf{1}^m)-d(W_n^*, \frac{1}{T}\sum_{t=1}^T\EE_{\pi^t}\left[\hat{r}^t(x,y) \right]- \frac{\beta}{T}\sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\cdot \mathbf{1}^m)\right)^{2q}}}_{\text{(A)}}\nonumber\\&\qquad + \underbrace{\frac{\eta}{T}\sum_{t=1}^T\|d^t\|_1\sum_{i=1}^m (L_i^t(\theta^*) -L_i^t(\theta^t)) }_{\text{(B)}} + \widetilde{\cO}(B\sqrt{m} T^{-1/2q}).\label{final result 2}
\end{align}
The last inequality uses the triangle inequality for $2q$-norm. Now also note that \begin{align*}d(W_n^*, &\EE_{\tilde{\pi}^T}\left[r^*(x,y) \right]- \frac{\beta}{T}\sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\cdot \mathbf{1}^m)-d(W_n^*, \frac{1}{T}\sum_{t=1}^T\EE_{\pi^t}\left[r^*(x,y) \right]- \frac{\beta}{T}\sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\cdot \mathbf{1}^m) \\&\qquad \le \frac{1}{T}\sum_{t=1}^T\sum_{i=1}^m \EE_{\pi^t}|r_i^*(x,y) - \hat{r}_i^t(x,y)|,\end{align*} we have 
\begin{align*}
    \text{(A)}\le \sqrt[2q]{\left(\sum_{n=1}^N \zeta_n\right) \left(\sum_{i=1}^m\Bigg|\frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}[r_i^*(x,y) -\hat{r}_i^t(x,y)]\Bigg|\right)^{2q}}  = \sum_{i=1}^m\Bigg|\frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}[r_i^*(x,y) -\hat{r}_i^t(x,y)]\Bigg|
\end{align*}
Now follow the same proof as Theorem \ref{thm:offline}, 
\begin{align*}
    \text{(A)}\le \widetilde{\cO}\left(\frac{m(3+e^{B'})4(\lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-2}\sqrt{d+\log(1/\delta)}}{\sqrt{M}}+\sqrt{\lambda (B')^2}\right),
\end{align*}
and 
\begin{align*}
    &\text{(B)}\le \frac{\eta}{T}\sum_{t=1}^T\|d^t\|_1\sum_{i=1}^m (L_i^t(\theta^*) -L_i^t(\theta^t))\\&\qquad \le \frac{N\sqrt{m}\cdot m}{\sqrt{M}}\widetilde{\cO}\left(\frac{(3+e^{B'})(\min_i\lambda_{\min}(\Sigma_{\cD_i}) + \lambda)^{-2}}{\sqrt{M}} + \frac{4}{\eta}\sqrt{\lambda (B')^2}\cdot (\min_i\lambda_{\min}(\Sigma_{\cD_i})+\lambda )^{-1} + 1\right),
\end{align*}
where the last inequality we use the fact that 
\begin{align*}
    \|d^t\|_1 =  \left\|\sum_{n=1}^Nd_n^t\cdot \frac{\zeta_n\|W^{(n)}- \overline{V}^t\|_2^{2q-1}}{\left(\sum_{n=1}^N \zeta_n\|W^{(n)}- \overline{V}^t\|_2^{2q}\right)^{\frac{2q-1}{2q}}}\right\|_1\le \sum_{n=1}^N \|d_n^t\|_1\cdot \zeta_n^{1/2q} \le N\sqrt{m}.
\end{align*}
Combining the Eq.~\eqref{final result 2} and the upper bounds for (A) and (B), substitute into $\eta = 1/\sqrt{M}$ and $\lambda = 1/M$, we can complete the final proof.
\end{proof}


\subsection{Proof of Theorem \ref{thm:online}}
%\nuoya{Need to change the direction statement.}
\begin{proof}
Recall that $V^t \in \RR^m$ with $(V^t)_i = \EE_{\pi^t}[r_i^{\theta_i^t}(x,y) - \beta \DD_{\mathrm{KL}}(\pi^t \|\pi_{\mathrm{ref}})]$, and $\overline{V}^t = \frac{1}{t}\sum_{i=1}^t V^i.$ We also define $W^0 = \{(0,0)\}.$ Since $W^t$ is the estimation of $W^*$ at round $t$, we have 
\begin{align}
    d(\overline{V}^T, W^T)^2 &= \|\overline{V}^T - \Pi_{W^T}(\overline{V}^T)\|^2\nonumber\\
    &\le \|\overline{V}^T - \Pi_{W^T}(\overline{V}^{T-1})\|^2\nonumber\\
    % & = \left(\frac{T-1}{T}\right)^2 \|\overline{V}^{T-1}-\Pi_{W^*}(\overline{V}^{T-1})\|^2  + \frac{1}{T^2}\|V^T - \Pi_{W^*}(\overline{V}^{T-1})\|^2 \\& \qquad + \frac{2(T-1)}{T^2}\langle \overline{V}^{T-1}-\Pi_{W^*}(\overline{V}^{T-1}), V^T - \Pi_{W^*}(\overline{V}^{T-1})\rangle.
    &\le \|\overline{V}^T - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|^2 + \|\Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|^2\nonumber \\&\qquad + 2\langle \overline{V}^T - \Pi_{W^{T-1}}(\overline{V}^{T-1}), \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\rangle.\label{ineq:first}
\end{align}
% Now note that 
% \begin{align*}
%     &\langle \overline{V}^{T-1}-\Pi_{W^*}(\overline{V}^{T-1}), V^T - \Pi_{W^{*}}(\overline{V}^{T-1})\rangle \\& = d(\overline{V}^{T-1}, W^*) \left\langle \frac{\overline{V}^{T-1}-\Pi_{W^*}(\overline{V}^{T-1})}{d(\overline{V}^{T-1}, W^*)}, V^T - \Pi_{W^{*}}(\overline{V}^{T-1})\right\rangle\\
%     &\le d(\overline{V}^{T-1}, W^*) \left\langle d^t, V^T - \Pi_{W^{*}}(\overline{V}^{T-1})\right\rangle \\&\qquad + d(\overline{V}^{T-1}, W^*) \cdot \left\|d^t - \frac{\overline{V}^{T-1}-\Pi_{W^*}(\overline{V}^{T-1})}{d(\overline{V}^{T-1}, W^*)}\right\| \cdot \|V^T - \Pi_{W^{*}}(\overline{V}^{T-1})\|
% \end{align*}
% By Lemma \ref{lemma:direc}, we can get 
% \begin{align*}
%     \left\|d^t - \frac{\overline{V}^{T-1}-\Pi_{W^*}(\overline{V}^{T-1})}{d(\overline{V}^{T-1}, W^*)} \right\|\le \frac{4\sqrt{d(\overline{V}^{T-1}, W^*)d_{B_1}(W^*, W^{T-1})} + 2d_{B_1}(W^*, W^{T-1})}{d(\overline{V}^{T-1}, W^*)},
% \end{align*}
% and then we can have 
% \begin{align*}
%     & \langle \overline{V}^{T-1}-\Pi_{W^*}(\overline{V}^{T-1}), V^T - \Pi_{W^{*}}(\overline{V}^{T-1})\rangle \\
%     &\le d(\overline{V}^{T-1}, W^*) \left\langle d^t, V^T - \Pi_{W^{*}}(\overline{V}^{T-1})\right\rangle \\&\qquad + B_1 \left(4\sqrt{d(\overline{V}^{T-1}, W^*)d_{B_1}(W^*, W^{T-1})} + 2d_{B_1}(W^*, W^{T-1})\right)
% \end{align*}
Now by Lemma \ref{lemma:dis of proj}, since $\|V^{T-1}\|_\infty\le B$ is bounded, we have 
$$\|\Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|_2^2 \le 4d(\overline{V}^{T-1}, W^{T-1})d_{B_1}(W^T, W^{T-1}) + 2d^2_{B_1}(W^T, W^{T-1}).$$
Then we can get 
\begin{align*}
    \|\Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|_2\le 2\sqrt{d(\overline{V}^{T-1}, \Pi_{W^{T-1}}(\overline{V}^{T-1}))d_{B_1}(W^T, W^{T-1})}+\sqrt{2}d_{B_1}(W^T, W^{T-1}).
\end{align*}
Then the third term on the right side can be bounded by 
\begin{align*}
    &\langle \overline{V}^T - \Pi_{W^{T-1}}(\overline{V}^{T-1}), \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\rangle\\&\le 
    \langle \overline{V}^{T-1} - \Pi_{W^{T-1}}(\overline{V}^{T-1}), \  \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\rangle\\
    &\qquad + \|\overline{V}^T - \overline{V}^{T-1}\|\cdot \| \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|\\
    &\le d(\overline{V}^{T-1}, W^{T-1})\cdot \left\langle d^t,  \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\right\rangle + \frac{1}{T}\| \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|.
    % &\le 2d(\overline{V}^{T-1}, W^{T-1})^{3/2}\sqrt{d_{B_1}(W^T, W^{T-1})} + \sqrt{2}d(\overline{V}^{T-1}, W^{T-1})d_{B_1}(W^T, W^{T-1})\\
    % &\qquad + \frac{\sqrt{2}}{T}(d_{B_1}(W^T, W^{T-1})+\sqrt{2d(V^{T-1}, W^{T-1})d_{B_1}(W^T, W^{T-1})}).
\end{align*}
Now denote $\tilde{d}^t = \frac{\Pi_{W^T}(\overline{V}^{T-1})-\overline{V}^{T-1}}{\|\Pi_{W^T}(\overline{V}^{T-1})-\overline{V}^{T-1}\|}$, then by Lemma \ref{lemma:direc}, we can get 
\begin{align*}
    d(\overline{V}^{T-1}, W^{T-1})\cdot \|d^t - \tilde{d}^t\| \le 4\sqrt{d(\overline{V}^{T-1}, W^{T-1})d_{B_1}(W^{T-1}, W^T)} + 2d_{B_1}(W^{T-1}, W^T),
\end{align*}
Then we can bound the inner product term as 
\begin{align*}
    &d(\overline{V}^{T-1}, W^{T-1})\cdot \left\langle d^t,  \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\right\rangle\\
    &\le d(\overline{V}^{T-1}, W^{T-1})\cdot \|d^t - \tilde{d}^t\|\cdot \| \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|\\
    &\qquad + d(\overline{V}^{T-1}, W^{T-1})\cdot \left\langle \tilde{d}^t,  \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\right\rangle.
\end{align*}
By the definition of $\tilde{d}^t$, we know that 
\begin{align*}
   \langle \tilde{d}^t,  \Pi_{W^T}(\overline{V}^{T-1})\rangle &= \min_{x \in W^T}\langle \tilde{d}^t, x\rangle \le \langle \tilde{d}^t, \Pi_{W^T}(\Pi_{W^{T-1}}(\overline{V}^{T-1}))\rangle \\
   &\le d_{B_1}(W^T, W^{T-1}) + \langle \tilde{d}^t, \Pi_{W^{T-1}}(\overline{V}^{T-1})\rangle .
\end{align*}
Hence the inner product term can be further bounded by  
\begin{align}
    &d(\overline{V}^{T-1}, W^{T-1})\cdot \left\langle d^t,  \Pi_{W^T}(\overline{V}^{T-1}) - \Pi_{W^{T-1}}(\overline{V}^{T-1})\right\rangle\nonumber\\
    &\le \left(4\sqrt{d(\overline{V}^{T-1}, W^{T-1})d_{B_1}(W^{T-1}, W^T)} + 2d_{B_1}(W^{T-1}, W^T) \right)^2\nonumber\\
    &\qquad + d(\overline{V}^{T-1}, W^{T-1})\cdot d_{B_1}(W^T, W^{T-1})\nonumber\\
    &\le 33d(\overline{V}^{T-1}, W^{T-1})\cdot d_{B_1}(W^T, W^{T-1}) + 8d^2_{B_1}(W^{T-1}, W^T).\label{eq:inner product estimate alpha}
\end{align}
Now continue to bound the right side in Eq. \eqref{ineq:first}, we can further get that
\begin{small}
\begin{align}
    &T^2d(\overline{V}^T, W^T)^2\le T^2\|\overline{V}^T - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|^2 + 37T^2 d(\overline{V}^{T-1}, W^{T-1})\cdot d_{B_1}(W^T, W^{T-1}) + 10T^2 d^2_{B_1}(W^{T-1}, W^T).\label{ineq:second}
\end{align}
\end{small}
Now we can further bound the Eq. \eqref{ineq:second} by expanding the first term on the right side:
\begin{align}
    T^2\|\overline{V}^T - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|^2 &= \left(T-1\right)^2 \|\overline{V}^{T-1} - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|^2 + \|V^T - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|^2\nonumber\\
    &\qquad  + 2(T-1)\left\langle \overline{V}^{T-1} - \Pi_{W^{T-1}}(\overline{V}^{T-1}), V^{T} - \Pi_{W^{T-1}}(\overline{V}^{T-1})\right\rangle\\
    &\le \left(T-1\right)^2 \|\overline{V}^{T-1} - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|^2 + (B+B_1)^2m\nonumber\\
    &\qquad  + 2(T-1)\left\langle \overline{V}^{T-1} - \Pi_{W^{T-1}}(\overline{V}^{T-1}), V^{T} - \Pi_{W^{T-1}}(\overline{V}^{T-1})\right\rangle.
\end{align}
The inner product term is %\nuoya{Need Explain the Line 3}
\begin{align*}
    \left\langle \overline{V}^{T-1} - \Pi_{W^{T-1}}(\overline{V}^{T-1}), V^{T} - \Pi_{W^{T-1}}(\overline{V}^{T-1})\right\rangle = d(\overline{V}^{T-1}, W^{T-1})\cdot \left\langle d^{T-1},  \Pi_{W^{T-1}}(\overline{V}^{T-1})-V^T \right\rangle.
\end{align*}
Note that $\langle d^{T-1}, \Pi_{W^{T-1}}(\overline{V}^{T-1})\rangle = \min_{z \in W^{T-1}}\langle d^{T-1}, z\rangle. $ Because $\|\Pi_{W^*}(\overline{V}^{T-1})\|\le B_1,$ there is a $z' \in W^{T-1}$ such that $\|z'-\Pi_{W^*}(\overline{V}^{T-1})\| \le d_{B_1}(W^*, W^{T-1}).$ Hence, 
\begin{align*}\langle d^{T-1}, \Pi_{W^{T-1}}(\overline{V}^{T-1})\rangle &\le \min_{z \in W^{T-1}}\langle d^{T-1},z\rangle \le  \langle d^{T-1}, z'\rangle  \le \min_{z \in W^*}\langle d^{T-1},z\rangle + d_{B_1}(W^*, W^{T-1})\\
&\le J(r^*, d^{T-1}, \pi^*) + D(\pi^*) + d_{B_1}(W^*, W^{T-1}).\end{align*}
The last inequality holds by Lemma \ref{lemma:approach}. Now we continue to bound the inner product term. We have  
\begin{align*}
&\left\langle \overline{V}^{T-1} - \Pi_{W^{T-1}}(\overline{V}^{T-1}), V^{T} - \Pi_{W^{T-1}}(\overline{V}^{T-1})\right\rangle\\&\qquad  = d(\overline{V}^{T-1}, W^{T-1})\cdot \left\langle d^{T-1},  \Pi_{W^{T-1}}(\overline{V}^{T-1})-V^T \right\rangle 
    \\&\qquad \le d(\overline{V}^{T-1}, W^{T-1})\cdot (\|d^{T-1}\|_1\cdot J(r_1^*, \cdots, r_m^*, \overline{d^{T-1}}, \pi^*)+D(\pi^*)+d_{B_1}(W^{T-1}, W^*)- J(\hat{r}_1^t, \cdots, \hat{r}_m^t, d^{T-1}, \pi^t))\\
    &\qquad \le d(\overline{V}^{T-1}, W^{T-1})\cdot \left(\eta \|d^{T-1}\|_1\cdot \left(\sum_{i=1}^m L_i^{T-1}(\theta^*)-\sum_{i=1}^m L_i^{T-1}(\theta^{T-1}) \right)+ D(\pi^*)+d_{B_1}(W^{T-1}, W^*)\right).
\end{align*}
Thus the Eq. \eqref{ineq:second} can be rewritten as 
\begin{small}
\begin{align*}
    & T^2d(\overline{V}^T, W^T)^2
    \\& \le \left(T-1\right)^2 \|\overline{V}^{T-1} - \Pi_{W^{T-1}}(\overline{V}^{T-1})\|^2+(B+B_1)^2m + 10T^2 d_{B_1}^2(W^{T-1}, W^T)\\&\qquad  + 2(T-1) d(\overline{V}^{T-1}, W^{T-1})\cdot \left(\eta \|d^t\|_1\cdot \left( \sum_{i=1}^m L_i^{T-1}(\theta^*)-\sum_{i=1}^m L_i^{T-1}(\theta^t) \right)+ D(\pi^*)+d_{B_1}(W^{T-1}, W^*) + 37 Td_{B_1}(W^T, W^{T-1})\right).
\end{align*}
\end{small}
Then by the recursion, we can get 
\begin{align*}
    & Td(\overline{V}^T, W^T)^2
    \\&\le (B+B_1)^2m + \sum_{t=1}^T \frac{10t^2 d_{B_1}^2(W^{t-1}, W^t)}{T} \\
    &\qquad +\sum_{t=1}^T \frac{2(t-1)}{T}d(\overline{V}^{t-1}, W^{t-1})\cdot \Bigg(\eta \|d^{T-1}\|_1\cdot \left( \sum_{i=1}^m L_i^{T-1}(\theta^*)-\sum_{i=1}^m L_i^{T-1}(\theta^{T-1}) \right)\\&\qquad \qquad + D(\pi^*)+d_{B_1}(W^{t-1}, W^*) + 37 td_{B_1}(W^t, W^{t-1})\Bigg).
\end{align*}
By this recursion formula, we can use the induction method to prove that 
\begin{align*}
    d(\overline{V}^T, W^T) &\le \frac{(B+B_1)^2m}{\sqrt{T}} + \underbrace{\sum_{t=1}^T \frac{10t^2}{T^{3/2}} d_{B_1}^2(W^{t-1}, W^t) }_{\textrm{(A)}} + D(\pi^*) + \underbrace{\frac{\eta}{T}\sum_{t=1}^{T-1}\|d^t\|_1\sum_{i=1}^m (L_i^t(\theta^*) - L_i^t(\theta^t))}_{\textrm{(B)}}\\&\qquad +\underbrace{\frac{1}{T}\sum_{i=1}^T  d_{B_1}(W^{t-1}, W^*)}_{\textrm{(C)}} + \underbrace{\frac{1}{T}\sum_{t=1}^T37td_{B_1}(W^t, W^{t-1})}_{\textrm{(D)}}.
\end{align*}
 Now we bound all four terms. We first prove that term (A), (C) and (D) are all at level $\widetilde{\cO}(1/\sqrt{T}).$

\paragraph{Term (A):} First we consider term (A). Since $W^{t} = \bigcap_{n=1}^N W_{p^{(n)},c^{(n)}}^{\alpha^{t,(n)}}$, the term $d_{B_1}^2(W^{t-1}, W^t)$ can be bounded by 
\begin{align*}d_{B_1}^2(W^{t-1}, W^t) \le  \left(\sum_{n=1}^Nd_{B_1}\left(W_{p^{(n)},c^{(n)}}^{\alpha^{t-1,(n)}}, W_{p^{(n)},c^{(n)}}^{\alpha^{t,(n)}}\right)\right)^2\le N\sum_{n=1}^N d^2_{B_1}\left(W_{p^{(n)},c^{(n)}}^{\alpha^{t-1,(n)}}, W_{p^{(n)},c^{(n)}}^{\alpha^{t,(n)}}\right).\end{align*} Since 
 $\alpha^t = \frac{t-1}{t}\alpha^{t-1} +\frac{1}{t}\hat{\alpha}^t,$ we can know $\|\alpha^t- \alpha^{t-1}\|_\infty \le \frac{1}{t}\|\hat{\alpha}^t\|_\infty \le \frac{1}{t}$. Then, by Lemma \ref{lemma:estimation error of parameterized target set}, we have 
 \begin{align}d_{B_1}(W^{\alpha^{t-1,(n)}}_{p^{(n)},c^{(n)}}, W^{\alpha^{t,(n)}}_{p^{(n)},c^{(n)}}) \le \frac{m^{3/2}B_1}{|p^{(n)}|}\cdot \frac{1}{t}.\label{ineq:primal bound for distance of W}\end{align}
 Thus by Eq. \eqref{ineq:primal bound for distance of W}, we know that 
 \begin{align}
     \textrm{(A)}&\le \frac{10N}{T^{3/2}}\sum_{n=1}^N \sum_{t=1}^T \frac{m^3B_1^2}{(p^{(n)})^2}\nonumber\\
     &\le \sum_{n=1}^N \frac{10Nm^3B_1^2}{(p^{(n)})^2}\cdot \frac{1}{\sqrt{T}}.\label{ineq:upperbound of (A)}
 \end{align}

 \paragraph{Term (C):}
 We have 
 \begin{align}
     \textrm{(C)}&\le \frac{B_1}{T} + \frac{1}{T}\cdot \sum_{n=1}^N\frac{m^{3/2}B_1}{|p^{(n)}|}\cdot \sum_{t=2}^T  \|\alpha^{t-1,(n)}-\alpha^*\|_\infty\nonumber\\
     &\le \frac{1}{T}\cdot \sum_{n=1}^N\frac{m^{3/2}B_1}{|p^{(n)}|}\cdot \gamma^{-1}\exp(4/\beta)\cdot \widetilde{\cO}\left(\mathrm{poly}(m,e^B, d,\log(1/\delta))\right)\cdot \left(\sum_{t=1}^T \frac{1}{\sqrt{t}}+1\right)\nonumber\\
     &\le\frac{1}{\sqrt{T}}\cdot \frac{Nm^{3/2}B_1}{\min_{n \in [N]}|p^{(n)}|}\cdot \gamma^{-1}\exp(4/\beta)\cdot \widetilde{\cO}\left(\mathrm{poly}(m,e^B, d,\log(1/\delta))\right). \label{ineq:(C)general: first}
 \end{align}

%  By the MLE guarantee of $\alpha$, we have 
%  \begin{align}
% & \gamma (k-1)\|\alpha^*-\hat{\alpha}^{k,(n)}\|_\infty\nonumber\\
%     &\le (k-1)\|\EE_{y_1\sim \pi^*, y_2\sim \pi_{\mathrm{base}}}[X^*(x,y)\circ| \alpha^*-\hat{\alpha}^{k,(n)}|]\|_\infty\nonumber\\
%      &\le (k-1) \EE_{y_1\sim \pi^*, y_2\sim \pi_{\text{base}}}\|X^*(x,y)\circ|\alpha^* - \hat{\alpha}^{k,(n)}|\|_\infty\nonumber\\
%         &\le  \gamma^{-1}\cdot \exp(4/\beta)\cdot \mathrm{poly}(m, e^B)\cdot \widetilde{\cO}\left(\frac{\log (|\cF|/\delta))}{\sqrt{k}} +  \sum_{j=1}^{k-1}\sum_{i=1}^m \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^k(x,y)]\hat{\alpha}_i^{k,(n)}\right).\nonumber
%  \end{align}

%  We can get that 
%  \begin{align*}
%      &\sum_{k=1}^t \|\hat{\alpha}^k - \alpha^*\|_\infty\\&\qquad \le \gamma^{-1}\cdot  \exp(4/\beta)\cdot \mathrm{poly}(m, e^B)\cdot \widetilde{\cO}\left(\sqrt{t}\log(|\cF|/\delta) + \sum_{k=1}^t \sum_{j=1}^{k-1}\sum_{i=1}^m \frac{1}{k}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^k(x,y)]\hat{\alpha}_i^{k,(n)}\right).
%  \end{align*}
%  Use the same process for bounding (C) in Section \ref{sec:proof of online-vpofl}, by Eq. \eqref{ineq: bound(C)} we can finally derive 
%  \nuoya{Need to move here}
%  \begin{align}
%      \sum_{k=1}^t \|\hat{\alpha}^{k,(n)} - \alpha^*\|&\le \gamma^{-1}\exp(4/\beta)\cdot \mathrm{poly}(m, e^B)\widetilde{\cO} (Bd\cdot C(d,B,\delta)\sqrt{t})\nonumber\\
%      &\le \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta))\widetilde{\cO}(\sqrt{t}). \label{ineq: averagealpha}
%  \end{align}
%  Thus substituing Eq. \eqref{ineq: averagealpha} to Eq. \eqref{ineq:(C)general: first}, we can get 
%  \begin{align}
%      \mathrm{(C)}&\le \frac{1}{T}\cdot \sum_{n=1}^N\frac{m^{3/2}B_1}{|p^{(n)}|}\cdot \gamma^{-1}\cdot \mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta))\sum_{t=1}^T  \widetilde{\cO}\left(\frac{1}{\sqrt{t}}\right)\nonumber\\
%      &\le \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta), B_1)\cdot \sum_{n=1}^N \frac{1}{|p^{(n)}|}\cdot \widetilde{\cO}(1/\sqrt{T}).\label{ineq:(C) final}
%  \end{align}

 \paragraph{Term (D):} 
First, we have 
\begin{align*}
    \textrm{(D)}\le \frac{1}{T}\sum_{t=1}^T 37t \sum_{n=1}^N d_{B_1}(W^{t,(n)}, W^{t-1,(n)}).
\end{align*}
Then, by Lemma \ref{lemma:estimation error of parameterized target set}, 
 \begin{align}
     \textrm{(D)}&\le \frac{37B_1}{T} + \frac{1}{T}\sum_{t=2}^T\sum_{n=1}^N  \frac{37tm^{3/2}B_1}{|p^{(n)}|}\|\alpha^{t,(n)} - \alpha^{t-1,(n)}\|_\infty\nonumber\\
     &\le \frac{37m^{3/2}B_1}{T}\cdot \sum_{n=1}^N \frac{1}{|p^{(n)}|}\sum_{t=2}^T \left(\left\|\hat{\alpha}^{t,(n)}-\alpha^{t-1,(n)}\right\|_\infty + 1\right)\nonumber\\
     &\le \frac{37m^{3/2}B_1}{T}\cdot \sum_{n=1}^N \frac{1}{|p^{(n)}|}\left(\sum_{t=2}^T \left(\| \hat{\alpha}^{t,(n)}-\alpha^{*,(n)}\|_\infty + \left\|\alpha^{*,(n)}-\alpha^{t-1,(n)}\right\|_\infty\right) + 1\right)\nonumber\\
     &\le \frac{37m^{3/2}B_1}{T} \cdot \sum_{n=1}^N \frac{1}{|p^{(n)}|}\left(\underbrace{\sum_{t=2}^T \|\hat{\alpha}^{t,(n)} - \alpha^{*,(n)}\|_\infty}_{\textrm{(E)}} + \underbrace{\sum_{t=2}^T \|\alpha^{t-1,(n)}-\alpha^{*,(n)}\| }_{\textrm{(F)}}\right) + \frac{37m^{3/2}B_1}{T}.\label{ineq:(D) first}
 \end{align}
 For the term (E), by Eq.~\eqref{eq:estimate alpha final result}, we have 
 \begin{align*}
     \mathrm{(E)}\le \sum_{t=1}^T \|\hat{\alpha}^{t,(n)}-\alpha^{*,(n)}\|_\infty\le \gamma^{-1}\cdot \widetilde{\cO}\left(\mathrm{poly}(m,e^B, \exp(1/\beta), d,\log(1/\delta))\right)\cdot \sqrt{T}.
 \end{align*}
 Also by Eq.~\eqref{eq:estimate alpha final result}, 
 \begin{align*}
     \textrm{(F)}&\le \sum_{t=2}^T \gamma^{-1}\cdot \widetilde{\cO}\left(\mathrm{poly}(m,e^B,\exp(1/\beta) d,\log(1/\delta))\right)\cdot \frac{1}{\sqrt{t}}\\
     &\le \gamma^{-1}\cdot \widetilde{\cO}\left(\mathrm{poly}(m,e^B, \exp(1/\beta), d,\log(1/\delta))\right)\cdot \sqrt{T}.
 \end{align*}
 By Theorem \ref{thm:est of alpha}, the term (E) can be bounded by 
 \begin{align*}
     \textrm{(E)}\le \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta))\widetilde{\cO}(\sqrt{T}).
 \end{align*}
 Thus substitute these upper bound to the Eq. \eqref{ineq:(D) first}, we get
 \begin{align}
     \textrm{(D)}&\le \frac{1}{T}\sum_{t=1}^T 37t \sum_{n=1}^N d_{B_1}(W^{t,(n)}, W^{t-1,(n)})\nonumber\\&\le \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, N, e^B, d, \log(1/\delta), B_1, (\min_{n \in [N]}p^{(n)})^{-1})\cdot\widetilde{\cO}(1/\sqrt{T}).\label{ineq:(D)final}
 \end{align}

 \paragraph{Combine them:} Now we combine the upper bound of (A), (C), (D), i.e., Eq. \eqref{ineq:upperbound of (A)}, \eqref{ineq:(C)general: first}, \eqref{ineq:(D)final}, we can get 
 \begin{align}
     d(\overline{V}^T, W^T) \le \frac{(B+B_1)^2m}{\sqrt{T}} + \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta), \min_{n \in [N]}\frac{1}{p^{(n)}}, B_1)\widetilde{\cO}(1/\sqrt{T}) + D(\pi^*) + \textrm{(B)}. \label{eq:second step}
 \end{align}
 Now we consider the proof of Theorem \ref{thm:online}. 
\begin{align*}
    &D(\tilde{\pi}^T) - D(\pi^*) \\ &= d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\beta \DD_{\mathrm{KL}}(\tilde{\pi}^T\| \pi_{\mathrm{ref}})) - D(\pi^*)\\
    &\le d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - D(\pi^*)\\
    &= \underbrace{d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - d(W^*, \frac{1}{T}\sum_{t=1}^T\EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}}))}_{\textrm{(*)}} \\
    &\qquad + d(W^*, \frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - D(\pi^*)\\
    &=\mathrm{(*)}
    + d\left(W^*, \overline{V}^T\right) - D(\pi^*)\\
    &\le \mathrm{(*)}+ d(W^*, W^T)
    + \underbrace{d\left(W^T, \overline{V}^T\right)  - D(\pi^*)}_{\mathrm{(**)}}.
\end{align*}
\paragraph{Term $(\ast)$:}
First, the term $(\ast)$ can be bounded by 
\begin{align*}
\mathrm{(*)}&\le \sum_{i=1}^m\Bigg|\frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}\left[  \hat{r}_i^t(x,y) - r_i^*(x,y)\right]\Bigg|.\end{align*}
Now note that 
\begin{align}
&\frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}\left[  \hat{r}_i^t(x,y) - r_i^*(x,y)\right]= \frac{1}{T}\sum_{t=1}^T \EE_{y_1\sim \pi^t,y_2\sim \pi_{\mathrm{base}}}\left[ \left((\hat{r}_i^t(x,y_1)- r_i^t(x,y_2)) - (r_i^*(x,y_1)  - r_i^*(x,y_2))\right) \right].\label{eq:first step}
\end{align}
Now since the reward contains a linear structure, by Lemma \ref{lemma:linearstructure} with $d_{\mathrm{cover}}(1/T) = \widetilde{\cO}(d),$ for any $\mu_i>0$ we can derive that 
\begin{align}
    (*)&\le \sum_{i=1}^m \mu_i \cdot \sum_{t=1}^T   \sum_{j=1}^{t-1}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\left(r_i^t(x,y_1)-r_i^t(x,y_2) - (r_i^*(x,y_1)-r_i^*(x,y_2))\right)^2] + \frac{d_{\mathrm{cover}}(1/T)}{4\mu_i} + \widetilde{\cO}(Bd)\nonumber\\
    &\le\sum_{i=1}^m\mu_i \exp(4/\beta)\kappa\cdot \sum_{t=1}^T   \sum_{j=1}^{t-1}\EE_{y_1\sim \pi^j, y_2\sim \pi^j}[\left(r_i^t(x,y_1)-r_i^t(x,y_2) - (r_i^*(x,y_1)-r_i^*(x,y_2))\right)^2] + \frac{d_{\mathrm{cover}}(1/T)}{4\mu_i}+ \widetilde{\cO}(Bd)
    % \\
    % &\le   \left(\mu \cdot \sum_{t=1}^T   \sum_{j=1}^{t-1}\sum_{i=1}^m \alpha_i^*\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)^2] + \frac{d}{4\mu}\right)
    \nonumber\\& =\sum_{i=1}^m \mu_i \exp(4/\beta)\kappa\cdot \sum_{t=1}^T \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j}[\Delta_i^t(x,y)^2] + \frac{d_{\mathrm{cover}}(1/T)}{4\mu_i}+ \widetilde{\cO}(Bd),\label{ineq:(A)}
\end{align}
The last inequality uses the fact that 
$$\sup_{x,y} \frac{\pi_{\mathrm{base}}(y\mid x)}{\pi^j(y\mid x)} \le \sup_{x,y} \frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\cdot \sup_{x,y}\frac{\pi_{\mathrm{ref}}(y\mid x)}{\pi^j(y\mid x)} \le \exp(4/\beta)\cdot \kappa,$$
where $\kappa = \sup_{x,y} \frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}(y\mid x)}}$ \citep{cen2024value}.
\paragraph{Term ($\ast\ast$):}
Now we consider the term $(\ast\ast)$. 
By Eq.~\eqref{eq:second step}, we know that 
\begin{align}
    (**) &\le \frac{(B+B_1)^2m}{\sqrt{T}} + \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta), \min_{n \in [N]}\frac{1}{p^{(n)}}, B_1)\widetilde{\cO}(1/\sqrt{T}) \nonumber\\&\qquad + \frac{1}{T}\sum_{t=1}^T \sum_{i=1}^m \eta \|d^t\|_1 (L_i^t(\theta_i^*) - L_i^t(\theta_i^t)). \label{ineq:bound (**)}
\end{align}
Now by the MLE loss, there exists a constant $C'$ such that 
\begin{align}
    &\frac{1}{T}\sum_{t=1}^T \sum_{i=1}^m \eta \|d^t\|_1(L_i^t(\theta_i^*) - L_i^t(\theta_i^t))
    \nonumber\\&\le 2\sum_{i=1}^m \eta \|d^t\|_1 \log(|\cR|/\delta) - \frac{C'}{T}\sum_{t=1}^T \sum_{i=1}^m\eta\|d^t\|_1\sum_{j \in \cD_i^{t-1}}\EE_{y\sim \pi^j} \left[\Delta_i^t(x,y)^2\right]\nonumber\\
    &=\widetilde{\cO}(2m \eta \sqrt{m}d) - \frac{C'}{T}\sum_{t=1}^T \sum_{i=1}^m\eta\|d^t\|_1\sum_{j \in \cD_i^{t-1}}\EE_{y\sim \pi^j} \left[\Delta_i^t(x,y)^2\right],\label{eq:etaloss}
\end{align}
Now consider the second term in Eq.~\eqref{eq:etaloss}. We can bound it by
\begin{align}&\sum_{t=1}^T \sum_{i=1}^m \sum_{j \in \cD_i^{t-1}}\EE_{y_1,y_2\sim \pi^j}[\Delta_i^t(x,y)^2]\nonumber \\ &= \sum_{t=1}^T \sum_{i=1}^m \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j, I \sim \PP(\cdot \mid \alpha^*, x,y_1,y_2, r^*)}[\Delta_i^t (x,y)^2 \mathbb{I}\{I^j = i\}]\nonumber\\
    &\ge \kappa_1\sum_{i=1}^m \sum_{j=1}^T \sum_{t=j+1}^T \EE_{y_1,y_2\sim \pi^j, I= i}[\Delta_i^t(x^j,y^j)^2 \mathbb{I}\{I^j = i\}]\nonumber\\
    &= \kappa_1\sum_{i=1}^m \sum_{j=1}^T \sum_{t=j+1}^T \EE_{y_1,y_2\sim \pi^j} [\Delta_i^t(x,y)^2]\nonumber\\
    &= \kappa_1\cdot \sum_{i=1}^m \sum_{t=1}^T \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j} [\Delta_i^t(x,y)^2],\label{eq:estimate technique}
\end{align}
where the inequality uses the fact that 
$\inf_{y,x,j, I}\frac{1}{ \PP(I\mid \alpha^*, x,y_1,y_2, r^*)} = \kappa_1$ for some constant $\kappa_1.$ Since the distribution of index is a bounded softmax distribution, we can derive that $\kappa_1\ge \frac{e^0}{e^0+(m-1)e^B}\ge \frac{1}{me^B}$. Thus we can get 
\begin{align}
    \sum_{t=1}^T \sum_{i=1}^m \sum_{j \in \cD_i^{t-1}}\EE_{y_1,y_2\sim \pi^j}[\Delta_i^t(x,y)^2] \ge \frac{1}{me^B}\cdot \sum_{i=1}^m \sum_{t=1}^T \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j} [\Delta_i^t(x,y)^2].\nonumber
\end{align}
Hence, the Eq.~\eqref{eq:etaloss} can be further bounded by 
\begin{align}
    \frac{1}{T}\sum_{t=1}^T \sum_{i=1}^m \eta \|d^t\|_1(L_i^t(\theta_i^*) - L_i^t(\theta_i^t)) &\le \widetilde{\cO}(2m\eta \sqrt{m}d) - \frac{\eta C' \|d^t\|_1}{Tme^B}\sum_{i=1}^m \sum_{t=1}^T \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j} [\Delta_i^t(x,y)^2]\label{eq:lossgap1}\\
    &\le \widetilde{\cO}(2m\eta \sqrt{m}d) - \frac{\eta C'}{Tme^B}\sum_{i=1}^m \sum_{t=1}^T \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j} [\Delta_i^t(x,y)^2].\label{eq:lossgap}
\end{align}
The last inequality uses the fact that $\|d^t\|_1 \ge 1.$
 Now combining $(\ast)$ (Eq.~\eqref{ineq:(A)}) and $(\ast\ast)$ (Eq.~\eqref{ineq:bound (**)}), 
by choosing $\mu_i = \frac{C'}{ me^B\exp(4/\beta)\kappa\sqrt{T}}$, we can get 
\begin{align}
    &D(\tilde{\pi}^T) - D(\pi^*) \nonumber\\& \le (*) + (**) + d(W^*, W^T)\nonumber\\
    & \le \frac{me^B\exp(4/\beta)\kappa d_{\mathrm{cover}}(1/T)}{4C'\sqrt{T}} + \frac{(B+B_1)^2m}{\sqrt{T}} +  \widetilde{\cO}(Bd) + \widetilde{\cO}\left(\frac{m^{3/2}d}{\sqrt{T}}\right) + d(W^*, W^T).\label{eq:final_step1}
\end{align}
Note that 
\begin{align}d(W^*, W^T)&\le  \sum_{n=1}^N d_{B_1}(W_{p^{(n)}, c^{(n)}}^{\alpha^{T,(n)}}, W_{p^{(n)},c^{(n)}}^{\alpha^{*,(n)}})\le m^{3/2}B_1\sum_{n=1}^N \frac{1}{|p^{(n)}|}\cdot \|\alpha^{T,(n)}-\alpha^{*,(n)}\|_\infty\nonumber\\
%&\le \frac{m^{3/2}B_1}{T}\sum_{n=1}^N \frac{1}{p^{(n)}}\sum_{t=1}^T \|\hat{\alpha}^{t,(n)}-\alpha^{*,(n)}\|_\infty\nonumber\\
&\le \frac{m^{3/2}B_1}{\sqrt{T}}\cdot \left(\sum_{n=1}^N \frac{1}{p^{(n)}}\right)\cdot \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta))\label{eq:step2explain}\\
&\le \frac{m^{3/2}B_1N}{\sqrt{T}}\cdot (\min_{n \in [N]}p^{(n)})^{-1} \cdot \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta)),\label{eq:finalstep2}\end{align}
where the inequality Eq.~\eqref{eq:step2explain} holds by Theorem \eqref{thm:est of alpha}.

Hence, combining Eq.~\eqref{eq:final_step1} and Eq.~\eqref{eq:finalstep2}, we complete the proof.
\end{proof}








%\nuoya{Continue here}


\subsection{Proof of Theorem \ref{thm:malfare online}}\label{app:proof malfare online}
%First we introduce the theoretical results for malfare function minimization problem under online setting with importance weight estimation. 
% \begin{theorem}[Malfare]
% With the same setting in Theorem \ref{thm:online}, if we consider the malfare function minimization problem with an integer exponential parameter $q\in \NN^+$ and uses Eq.~\eqref{eq:dir malfare} to compute the direction, then for $\delta \in (0,1)$ and $\eta = 1/\sqrt{T}$, with probability at least $1-\delta$ we have
% \begin{align*}
%     &D_q(\tilde{\pi}^T) - D_q(\pi^*) \\&\le \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m,N,e^B, d, \log(1/\delta),\kappa, B_1, (\min_{n \in [N]}p^{(n)})^{-1}, (\min_{n \in [N]}\zeta_n)^{-1/2q})\cdot \widetilde{\cO}(T^{-1/2q}),
% \end{align*}
% where $\tilde{\pi}^T = \frac{1}{T}\sum_{t=1}^T \pi^t$, and $ \kappa= \sup_{x,y}\frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}$, $B_1=2\sqrt{m}(B+\max_n c^{(n)})$ are constants.

% \end{theorem}
\begin{proof}
First, note that \begin{align}
     d(\overline{V}^T, W^{T,(n)})^{2q} &= \|\overline{V}^T - \Pi_{W^{T,(n)}}(\overline{V}^T)\|^{2q}\nonumber\\
    &\le \|\overline{V}^T - \Pi_{W^{T,(n)}}(\overline{V}^{T-1})\|^{2q}\nonumber\\
    % & = \left(\frac{T-1}{T}\right)^2 \|\overline{V}^{T-1}-\Pi_{W^*}(\overline{V}^{T-1})\|^2  + \frac{1}{T^2}\|V^T - \Pi_{W^*}(\overline{V}^{T-1})\|^2 \\& \qquad + \frac{2(T-1)}{T^2}\langle \overline{V}^{T-1}-\Pi_{W^*}(\overline{V}^{T-1}), V^T - \Pi_{W^*}(\overline{V}^{T-1})\rangle.
    &\le \Bigg(\|\overline{V}^T - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\|^2 + \|\Pi_{W^{T,(n)}}(\overline{V}^{T-1}) - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\|^2\nonumber \\&\qquad + 2\langle \overline{V}^T - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1}), \Pi_{W^{T,(n)}}(\overline{V}^{T-1}) - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\rangle\Bigg)^{q}.\label{ineq:first malfare}
\end{align}
Now by Lemma \ref{lemma:dis of proj}, since $\|V^{T-1}\|_\infty\le B$ is bounded, we have 
\begin{align}\|\Pi_{W^{T,(n)}}(\overline{V}^{T-1}) - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\|_2^2 &\le 4d(\overline{V}^{T-1}, W^{T-1,(n)})d_{B_1}(W^{T,(n)}, W^{T-1,(n)}) + 2d^2_{B_1}(W^{T,(n)}, W^{T-1,(n)}).
\end{align}
% Then since $d(W^{T,(n)}, W^{T-1,(n)}) \le \frac{m^{3/2}B_1}{|p^{(n)}|}\cdot \|\alpha^{t,(n)}-\alpha^{t-1,(n)}\|_\infty \le \frac{m^{3/2}B_1}{|p^{(n)}|T}$, we know 
% \begin{align}
%     \|\Pi_{W^{T,(n)}}(\overline{V}^{T-1}) - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\|_2^2 &\le \frac{4B_1^2m^{3/2}}{|p^{(n)}|T} + \frac{2B_1^2m^3}{|p^{(n)}|^2 T^2}.
% \end{align}
Also, by Eq.~\eqref{eq:inner product estimate alpha}, we can also have 
\begin{align}
    &\langle \overline{V}^T - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1}), \Pi_{W^{T,(n)}}(\overline{V}^{T-1}) - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\rangle \nonumber\\&\le 33d(\overline{V}^{T-1}, W^{T-1})\cdot d_{B_1}(W^{T,(n)}, W^{T-1,(n)}) + 8d^2_{B_1}(W^{T-1,(n)}, W^{T,(n)}).\nonumber
    %\\
    %&\le \frac{33B_1^2m^{3/2}}{|p^{(n)}|T} + \frac{8B_1^2m^3}{|p^{(n)}|^2 T^2}.
\end{align}
Hence, by Eq.~\eqref{ineq:first malfare}, we can get 
\begin{align}
    &T^{2q} d(\overline{V}^T, W^{T,(n)})^{2q}  \nonumber\\&\le \Bigg(\|\overline{V}^T - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\|^2 + \|\Pi_{W^{T,(n)}}(\overline{V}^{T-1}) - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\|^2\nonumber \\&\qquad + 2\langle \overline{V}^T - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1}), \Pi_{W^{T,(n)}}(\overline{V}^{T-1}) - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\rangle\Bigg)^{q}.\nonumber\\
    &\le T^{2q}\left(\|\overline{V}^T - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\|^{2} + 37d(\overline{V}^{T-1}, W^{T-1})\cdot d_{B_1}(W^{T,(n)}, W^{T-1,(n)}) + 10d^2_{B_1}(W^{T-1,(n)}, W^{T,(n)})\right)^q.\label{eq:malfare further}
    \end{align}
    Now, since $d(W^{T,(n)}, W^{T-1,(n)}) \le \frac{m^{3/2}B_1}{|p^{(n)}|}\cdot \|\alpha^{t,(n)}-\alpha^{t-1,(n)}\|_\infty \le \frac{m^{3/2}B_1}{|p^{(n)}|T}$, we know 
\begin{align}
    37d(\overline{V}^{T-1}, W^{T-1})\cdot d_{B_1}(W^{T,(n)}, W^{T-1,(n)}) + 10d^2_{B_1}(W^{T-1,(n)}, W^{T,(n)}) &\le \frac{37B_1^2m^{3/2}}{|p^{(n)}|T} + \frac{10B_1^2m^3}{|p^{(n)}|^2 T^2}.
\end{align}
Hence, the Eq.~\eqref{eq:malfare further} can be further bounded by 
    \begin{align*}
    &T^{2q} d(\overline{V}^T, W^{T,(n)})^{2q}\\
    &\le T^{2q}d^{2q}(\overline{V}^T, W^{T-1,(n)}) + \widetilde{\cO}(\mathrm{poly}(B_1^{q},m^{q}, (\min_{n \in [N]}p^{(n)})^{-q})T^{2q-2})\\&\qquad + qT^{2q}\|\overline{V}^T - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\|^{2q-2}\cdot \left(37d(\overline{V}^{T-1}, W^{T-1,(n)})\cdot d_{B_1}(W^{T,(n)}, W^{T-1,(n)}) + 10d^2_{B_1}(W^{T-1,(n)}, W^{T,(n)})\right)\\
    &\le T^{2q}d^{2q}(\overline{V}^T, W^{T-1,(n)}) + \widetilde{\cO}(\mathrm{poly}(B_1^{q},m^{q}, (\min_{n \in [N]}p^{(n)})^{-q})T^{2q-2}) \\&\qquad + 37qT^{2q}d^{2q-1}(\overline{V}^{T-1}, W^{T-1,(n)}) \cdot d_{B_1}(W^{T,(n)}, W^{T-1,(n)}) .
\end{align*}
The last inequality is because $\|P_{T,n}\| = \mathrm{poly}(B_1,m,(\min_{n \in [N]}p^{(n)})^{-1} \cdot \widetilde{\cO}(1/T)$, and $$\|\overline{V}^T - \Pi_{W^{T-1,(n)}}(\overline{V}^{T-1})\|^{2q-2} - d^{2q-2}(\overline{V}^{T-1}, W^{T-1,(n)}) \le \widetilde{\cO}(\mathrm{poly}(B_1^{q},m^{q}, (\min_{n \in [N]}p^{(n)})^{-q})T^{2q-3}).$$
Now we further bound the first term $T^{2q}d^{2q}(\overline{V}^{T}, W^{T-1,(n)}).$ 
Using the same derivation for Eq.~\eqref{eq:recursion before}, 
we know that \begin{align*}
    &T^{2q} \sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T}, W^{T-1,(n)}) \\&\le (T-1)^{2q} \sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T-1}, W^{T-1,(n)}) + 12^qT^{2q-2}B_1^{2q}m^q\\
&\qquad + 2(T-1)^{2q-1}\sum_{n=1}^N \zeta_n(\overline{V}^{T-1}-\Pi_{W^{T-1,(n)}}(\overline{V}^{T-1}))(V^T-\Pi_{W^{T-1,(n)}}(\overline{V}^{T-1}))d^{2q-2}(\overline{V}^{T-1}, W^{T-1,(n)}).
\end{align*}
Hence, we can derive 
\begin{align*}
    &T^{2q}\sum_{n=1}^N \zeta_n d(\overline{V}^T, W^{T,(n)})^{2q} \nonumber\\&\le (T-1)^{2q} \sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T-1}, W^{T-1,(n)})  + \widetilde{\cO}(\mathrm{poly}(B_1^{q},m^{q}, (\min_{n \in [N]}p^{(n)})^{-q})T^{2q-2}) \\&\qquad +37qT^{2q}\sum_{n=1}^N \zeta_nd^{2q-1}(\overline{V}^{T-1}, W^{T-1,(n)})\cdot d_{B_1}(W^{T,(n)}, W^{T-1,(n)}) \\
&\qquad\qquad   + 2(T-1)^{2q-1}\sum_{n=1}^N \zeta_n(\overline{V}^{T-1}-\Pi_{W^{T-1,(n)}}(\overline{V}^{T-1}))(V^T-\Pi_{W^{T-1,(n)}}(\overline{V}^{T-1}))d^{2q-2}(\overline{V}^{T-1}, W^{T-1,(n)}).
\end{align*}
%\nuoya{Continue here.}
Now we consider the last term in the inequation above. Similar to the Eq.~\eqref{last term bound}, we have 
\begin{align}
&\sum_{n=1}^N \zeta_n(\overline{V}^{T-1}-\Pi_{W^{T-1,(n)}}(\overline{V}^{T-1}))(V^T-\Pi_{W^{T-1,(n)}}(\overline{V}^{T-1}))d^{2q-2}(\overline{V}^{T-1}, W^{T-1,(n)})\nonumber\\&\qquad \le \left(\sum_{n=1}^N \zeta_n d^{2q}(\overline{V}^{T-1}, W^{T-1,(n)})\right)^{\frac{2q-1}{2q}} \cdot \left(D_{q}(\pi^*) + \underbrace{\eta \|d^t\|_1\left(\sum_{i=1}^m L_i^t(\theta^*) -  \sum_{i=1}^m L_i^t(\theta^t)\right)}_{\textrm{($\ast$)}}\right),\nonumber
\end{align}
then we can get 
\begin{align*}
    &T^{2q}\sum_{n=1}^N \zeta_n d(\overline{V}^T, W^{T,(n)})^{2q} \nonumber\\&\le (T-1)^{2q} \sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T-1}, W^{T-1,(n)})  + \widetilde{\cO}(\mathrm{poly}(B_1^{q},m^{q}, (\min_{n \in [N]}p^{(n)})^{-q})T^{2q-2}) \\&\qquad +37qT^{2q}\left(\sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T-1}, W^{T-1,(n)})\right)^{\frac{2q-1}{2q}}\cdot \sqrt[2q]{\sum_{n=1}^N d_{B_1}^{2q}(W^{T,(n)}, W^{T-1,(n)}) }\\
&\qquad\qquad  2(T-1)^{2q-1}\left(\sum_{n=1}^N \zeta_n d^{2q}(\overline{V}^{T-1}, W^{T-1,(n)})\right)^{\frac{2q-1}{2q}} \cdot \left(D_{q}(\pi^*) + (\ast)\right)\\
&\le (T-1)^{2q} \sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T-1}, W^{T-1,(n)})  + \widetilde{\cO}(\mathrm{poly}(B_1^{q},m^{q}, (\min_{n \in [N]}p^{(n)})^{-q})T^{2q-2}) \\&\qquad +2(T-1)^{2q-1}\left(\sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T-1}, W^{T-1,(n)})\right)^{\frac{2q-1}{2q}} \\
&\qquad\qquad   \cdot \left(\frac{37qT^{2q}}{(T-1)^{2q-1}}\sqrt[2q]{\sum_{n=1}^N 2^q T d_{B_1}^{2q}(W^{T,(n)}, W^{T-1,(n)}) } + D_{q}(\pi^*) + (\ast)\right).
\end{align*}
Hence, by the reduction and the fact that $\frac{T}{T-1}\le 2$ for $T \ge 2,$, we can further get 
\begin{align*}
    &T^{2q}\sum_{n=1}^N \zeta_n d^{2q}(\overline{V}^T, W^{T,(n)})\nonumber\\&\le\widetilde{\cO}(\mathrm{poly}(B_1^{q},m^{q}, (\min_{n \in [N]}p^{(n)})^{-q})T^{2q-1}) + \sum_{t=1}^{T}2(t-1)^{2q-1}\left(\sum_{n=1}^N \zeta_nd^{2q}(\overline{V}^{T-1}, W^{T-1,(n)})\right)^{\frac{2q-1}{2q}} \\
&\qquad   \cdot \left(37q\cdot 2^q T\cdot \sqrt[2q]{\sum_{n=1}^N d_{B_1}^{2q}(W^{T,(n)}, W^{T-1,(n)}) } + D_{q}(\pi^*) + \sqrt[2q]{\sum_{n=1}^N d^{2q}_{B_1}(W^{T-1,(n)}, W^*)}+(\ast)\right).
\end{align*}
Similar to the Section \ref{sec: malfare_proof_offline}, we can use the induction method to derive


\begin{align*}
    &\sqrt[2q]{\sum_{n=1}^N \zeta_n d^{2q}(\overline{V}^T, W_n^T)} - D_q(\pi^*) \\& \le \widetilde{\cO}\left(\mathrm{poly}(B_1, m, (\min_{n \in [N]}p^{(n)})^{-1})T^{-1/2q}\right)  + (\ast) +\frac{1}{T}\sum_{i=1}^T  \sqrt[2q]{\sum_{n=1}^N \zeta_n d_{B_1}^{2q}(W^{t-1,(n)}, W^*)} \\&\qquad+ \frac{1}{T}\sum_{t=1}^T 37q\cdot 2^qt\sqrt[2q]{\sum_{n=1}^N \zeta_nd_{B_1}^{2q}(W^{t,(n)}, W^{t-1,(n)}) }.\end{align*}
Now note that 
\begin{align}
    \sqrt[2q]{\sum_{n=1}^N \zeta_n d_{B_1}^{2q}(W^{t-1,(n)}, W^*)} \le \sqrt[2q]{\sum_{n=1}^N\zeta_n \frac{m^{3q}B_1^{2q}\|\hat{\alpha}^{t-1,(n)} - \alpha^{*,(n)}\|_\infty^{2q}}{|p^{(n)}|^{2q}}}\le \frac{\gamma^{-1}\cdot \widetilde{\cO}(\mathrm{poly}(m,e^B,\exp(1/\beta), d, \log(1/\delta)))}{\min_{n \in [N]}|p^{(n)}|} \cdot \frac{1}{\sqrt{t}}.\label{eq:derivation 1}
\end{align}
Also, by Eq.~\eqref{ineq:(D)final}, we have
\begin{align}
    \frac{1}{T}\sum_{t=1}^T t\sqrt[2q]{\sum_{n=1}^N \zeta_n d_{B_1}^{2q}(W^{t,(n)}, W^{t-1,(n)}) } &\le \frac{1}{T}\sum_{t=1}^Tt\sum_{n=1}^N d_{B_1}(W^{t,(n)}, W^{t-1,(n)})\nonumber\\&\le \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, N, e^B, d, \log(1/\delta), B_1, (\min_{n \in [N]}p^{(n)})^{-1})\cdot\widetilde{\cO}(1/\sqrt{T}).\label{eq:derivation 2}
\end{align}
Hence, combining Eq.~\eqref{eq:derivation 1} and Eq.~\eqref{eq:derivation 2}, 
    \begin{align*}
    &\sqrt[2q]{\sum_{n=1}^N \zeta_n d^{2q}(\overline{V}^T, W_n^T)} - D_q(\pi^*)\\&\le \widetilde{\cO}\left(\mathrm{poly}(B_1, m, (\min_{n \in [N]}p^{(n)})^{-1})T^{-1/2q}\right) + \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, N, e^B, d, \log(1/\delta), B_1, (\min_{n \in [N]}p^{(n)})^{-1})\widetilde{\cO}(1/\sqrt{T}) + \mathrm{(B)}\\
    &\le \widetilde{\cO}\left((\gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, N,e^B, d, \log(1/\delta), B_1, (\min_{n \in [N]}p^{(n)})^{-1})T^{-1/2q}\right) + (\ast).
\end{align*}
Now we derive the proof. First,
\begin{small}
\begin{align*}
    &D_q(\tilde{\pi}^T) - D_q(\pi^*)\\
    &=\sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(S(\tilde{\pi}^T), W_n^*)} - \sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(S(\tilde{\pi}^T), W_n^T)} + \sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(S(\tilde{\pi}^T), W_n^T)}  - D_q(\pi^*)\\
    &\le \sqrt[2q]{\sum_{n=1}^N \zeta_n |d(S(\tilde{\pi}^T), W_n^T) - d(S(\tilde{\pi}^T),W_n^*)|^{2q}}\\&\qquad +\sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(W_n^T, \EE_{\tilde{\pi}^T}\left[r^*(x,y) \right]- \frac{\beta}{T}\sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\cdot \mathbf{1}^m)} - \sqrt[2q]{\sum_{n=1}^N\zeta_n d^{2q}(W_n^T, \overline{V}^T)}\\
    &\qquad\qquad  + \sqrt[2q]{\sum_{n=1}^N \zeta_n d^{2q}(W_n^T, \overline{V}^T)} - D_q(\pi^*)\\
    &\le \sum_{n=1}^N d(W_n^*, W_n^T)
    \\&\qquad + \underbrace{\sqrt[2q]{\sum_{n=1}^N\zeta_n \left(d(W_n^T, \EE_{\tilde{\pi}^T}\left[r^*(x,y) \right]- \frac{\beta}{T}\sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\cdot \mathbf{1}^m)-d(W_n^T, \frac{1}{T}\sum_{t=1}^T\EE_{\pi^t}\left[r^{\theta^t}(x,y) \right]- \frac{\beta}{T}\sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})\cdot \mathbf{1}^m)\right)^{2q}}}_{(\ast\ast)}\\&\qquad\qquad  + \widetilde{\cO}\left((\gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta), \min_{n \in [N]}\frac{1}{p^{(n)}}, B_1)N^{1/2q}T^{-1/2q}\right) + (\ast).
\end{align*}
\end{small}
First, for the term $\sum_{n=1}^N d(W_n^*, W_n^T),$ we can bound it by 
\begin{align*}
    \sum_{n=1}^N d(W_n^*, W_n^T) \le \sum_{n=1}^N \frac{m^{3/2}B_1}{|p^{(n)}|}\cdot \|\alpha^{*,(n)}-\alpha^{T,(n)}\|_\infty.
\end{align*}
From the Theorem \ref{thm:est of alpha}, we can get 
\begin{align}
    \sum_{n=1}^N d(W_n^*, W_n^T) \le \frac{m^{3/2}B_1N}{\min_{n \in [N]}p^{(n)}} \gamma^{-1} \cdot \widetilde{\cO}\left(\mathrm{poly}(m,e^B,\exp(1/\beta), d,\log(1/\delta))\right)\cdot \frac{1}{\sqrt{T}},\label{eq:malfare online step 1}
\end{align}
Now by Eq.~\eqref{ineq:(A)}, we have
\begin{align}
    \mathrm{(\ast\ast)}&\le \sqrt[2q]{\sum_{n=1}^N \zeta_n \left(\sum_{i=1}^m\Bigg|\frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}[r_i^*(x,y) -\hat{r}_i^t(x,y)]\Bigg|\right)^{2q}} \le \left(\sum_{n=1}^N \zeta_n\right)\cdot \sum_{i=1}^m\Bigg|\frac{1}{T}\sum_{t=1}^T  \EE_{\pi^t}[r_i^*(x,y) -\hat{r}_i^t(x,y)]\Bigg|\nonumber\\
    &\le \sum_{i=1}^m \mu_i \exp(4/\beta)\kappa\cdot \sum_{t=1}^T \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j}[\Delta_i^t(x,y)^2] + \frac{d_{\mathrm{cover}}(1/T)}{4\mu_i} + \widetilde{\cO}(NBd).\label{malfare online step 2}
\end{align}
Consider the term (B). By Eq.~\eqref{eq:lossgap1}, we can get
\begin{align}
    \mathrm{(B)}=\frac{1}{T}\sum_{t=1}^T \sum_{i=1}^m \eta \|d^t\|_1(L_i^t(\theta_i^*) - L_i^t(\theta_i^t)) &\le \widetilde{\cO}(2m\eta \sqrt{m}d) - \frac{\eta C' \|d^t\|_1}{Tme^B}\sum_{i=1}^m \sum_{t=1}^T \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j} [\Delta_i^t(x,y)^2]\nonumber\\
    &\le \widetilde{\cO}(2m\eta \sqrt{m}d) - \frac{\eta C' \zeta_n^{{1/2q}}}{Tme^BN^{\frac{2q-1}{2q}}}\sum_{i=1}^m \sum_{t=1}^T \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j} [\Delta_i^t(x,y)^2].\label{eq:ineq (B)}
\end{align}
The last inequality is because, if we choose $n' = \max_{n \in [N]}\zeta_n \|W^{(n)}-\overline{V}^t\|_2^{2q}$, then 
\begin{align*}
    \frac{\zeta_{n'}\|W^{(n')}- \overline{V}^t\|_2^{2q-1}}{\left(\sum_{n=1}^N \zeta_n\|W^{(n)}- \overline{V}^t\|_2^{2q}\right)^{\frac{2q-1}{2q}}} \ge \frac{\zeta_{n'}\|W^{(n')}- \overline{V}^t\|_2^{2q-1}}{N^{\frac{2q-1}{2q}}\cdot \zeta_{n'}^{\frac{2q-1}{2q}}\|W^{(n')}-\overline{V}^t\|_2^{2q-1}} = \frac{\zeta_{n'}^{1/2q}}{N^{\frac{2q-1}{2q}}}\ge \frac{\min_{n \in [N]}\zeta_n^{1/2q}}{N^{\frac{2q-1}{2q}}}.
\end{align*}
Hence, we have 
\begin{align*}
    \|d^t\|_1 = \left\|\sum_{n=1}^Nd_n^t\cdot \frac{\zeta_n\|W^{(n)}- \overline{V}^t\|_2^{2q-1}}{\left(\sum_{n=1}^N \zeta_n\|W^{(n)}- \overline{V}^t\|_2^{2q}\right)^{\frac{2q-1}{2q}}}\right\|_1 \ge \|d_{n'}^t\|_1\cdot \frac{\min_{n \in [N]}\zeta_n^{1/2q}}{N^{\frac{2q-1}{2q}}}.
\end{align*}
Now we choose $\mu_i = \eta \cdot \frac{C'\min_{n \in [N]}\zeta_n^{1/2q}}{me^B \exp(4/\beta)\kappa N^{\frac{2q-1}{2q}}}$, $\eta = 1/\sqrt{T}$, and use the inequality Eq.~\eqref{eq:ineq (B)} for bounding (B), we finally get 
\begin{align*}
    &D_q(\tilde{\pi}^T)-D_q(\pi^*) \\&\le \sum_{n=1}^N d(W_n^*, W_n^T) + \mathrm{(A)} + \mathrm{(B)} + \widetilde{\cO}\left((\gamma^{-1}\mathrm{poly}(\exp(1/\beta), m, e^B, d, \log(1/\delta), \min_{n \in [N]}\frac{1}{p^{(n)}}, B_1)N^{1/2q}T^{-1/2q}\right)\\
    &\le \gamma^{-1}\mathrm{poly}(\exp(1/\beta), m,N,e^B, d, \log(1/\delta),\kappa, B_1, (\min_{n \in [N]}p^{(n)})^{-1}, (\min_{n \in [N]}\zeta_n)^{-1/2q})\cdot \widetilde{\cO}(T^{-1/2q}).
\end{align*}

\end{proof}






%\nuoya{Continue here}




% \subsection{Proof of Theorem \ref{thm:online-vpofl}}\label{sec:proof of online-vpofl}

% \begin{proof}
% \begin{align*}
%     \text{Reg}(T) &\le \sum_{t=1}^T J(\pi^*) - J(\pi^t)\\
%     &\le \sum_{t=1}^T \EE_{\pi^*}[r^*(x,y) + \text{KL}(\pi^*\|\pi_{\text{ref}})] - \EE_{\pi^t}[r^*(x,y) + \text{KL}(\pi^*\|\pi_{\text{ref}})]\\
%     &\le \sum_{t=1}^T \EE_{\pi^*}[r^*(x,y)] - \EE_{\pi^t}[r^t(x,y)] \\
%     &\qquad + \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\\
%     &\qquad \qquad + \sum_{t=1}^T \beta \text{KL}(\pi^*\|\pi_{\text{ref}}) - \text{KL}(\pi^t\|\pi_{\text{ref}})
% \end{align*}
% Now by the definition of $\pi^t$, we have 
% \begin{align*}
%     \EE_{\pi^t}[r^t(x,y)] + \text{KL}(\pi^t\|\pi_{\text{ref}}) - \sum_{i=1}^m \eta L_i^t(\theta_i) \ge \EE_{\pi^*}[\sum_{i=1}^m \hat{\alpha}_i^t r_i^*(x,y)] + \text{KL}(\pi^*\|\pi_{\text{ref}}) - \sum_{i=1}^m \eta L_i^t(\theta_i^*),
% \end{align*}
% then we have 
% \begin{align*}
%     \EE_{\pi^*}[\sum_{i=1}^m \hat{\alpha}_i^t r_i^*(x,y)] - \EE_{\pi^t}[r^t(x,y)] \le \beta(\text{KL}(\pi^t\|\pi_{\text{ref}}) - \text{KL}(\pi^*\|\pi_{\text{ref}})) + \sum_{i=1}^m \eta (L_i^t(\theta_i^*) -L_i^t(\theta_i^t)).
% \end{align*}
% Now we can further derive that 
% \begin{align*}
%     \textrm{Reg}(T) &\le \sum_{t=1}^T \sum_{i=1}^m \eta(L_i^t(\theta_i^*) -L_i^t(\theta_i^t)) + \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\\
%     &\qquad + \sum_{t=1}^T \EE_{\pi^*}[r^*(x,y)] - \EE_{\pi^*}[\sum_{i=1}^m \hat{\alpha}_i^t r_i^*(x,y)]\\
%     &\le \sum_{t=1}^T \sum_{i=1}^m \eta(L_i^t(\theta_i^*) -L_i^t(\theta_i^t)) + \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\\
%     &\qquad + \sum_{t=1}^T \EE_{y_1\sim \pi^*, y_2\sim \pi_{\textrm{base}}}[r^*(x,y_1)-r^*(x,y_2)] - \EE_{y_1\sim \pi^*, y_2\sim \pi_{\textrm{base}}}[\sum_{i=1}^m \hat{\alpha}_i^t (r_i^*(x,y_1)-r_i^*(x,y_2))]\\
%     &\le \sum_{t=1}^T \sum_{i=1}^m \eta(L_i^t(\theta_i^*) -L_i^t(\theta_i^t)) + \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\\
%     &\qquad + \sum_{t=1}^T \EE_{y_1\sim \pi^*, y_2\sim \pi_{\textrm{base}}}\|(\alpha^*-\hat{\alpha}^t)(X^*(x,y))\|_1\\
%     &\le \sum_{t=1}^T \sum_{i=1}^m \eta(L_i^t(\theta_i^*) -L_i^t(\theta_i^t)) + \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\\
%     &\qquad + \sum_{t=1}^T m\EE_{y_1\sim \pi^*, y_2\sim \pi_{\textrm{base}}}\|(\alpha^*-\hat{\alpha}^t)(X^*(x,y))\|_\infty\\
%     &\le \sum_{t=1}^T \sum_{i=1}^m \eta(L_i^t(\theta_i^*) -L_i^t(\theta_i^t)) + \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\\
%     &\qquad + \exp(4/\beta)\cdot \mbox{poly}(m)\cdot \sum_{t=1}^T \widetilde{\cO}\left(\frac{1}{\sqrt{t}}\log(|\cF|/\delta) + \frac{1}{t}\sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)]\hat{\alpha}_i^t\right)\\&\le \sum_{t=1}^T \sum_{i=1}^m \eta(L_i^t(\theta_i^*) -L_i^t(\theta_i^t)) + \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\\
%     &\qquad + \exp(4/\beta)\cdot \mbox{poly}(m, e^B)\cdot \widetilde{\cO}\left(\sqrt{T}\log(|\cF|/\delta) + \sum_{t=1}^T\sum_{j=1}^{t-1}\sum_{i=1}^m \frac{1}{t}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)]\hat{\alpha}_i^t\right).
% \end{align*}
% % Then we can further get  
% % \begin{align*}
% %     \sum_{t=1}^T \widetilde{\cO}\left(\frac{1}{\sqrt{t}} + \frac{1}{t}\sum_{j=1}^{T-1}\sum_{i=1}^m \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)]\hat{\alpha}_i^t\right)
% %     = \widetilde{\cO}\left(\sqrt{T}+\sum_{t=1}^T \sum_{j=1}^{t-1}\sum_{i=1}^m \frac{1}{t}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)]\hat
% %     \alpha_i^t \right)
% % \end{align*}

% % \begin{align*}
% %     &\le \sum_{t=1}^T \sum_{i=1}^m \eta(L_i^t(\theta_i^*) -L_i^t(\theta_i^t)) + \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\\
% %     &\qquad + \cO(\sqrt{T}) + \cO\left(\sum_{t=1}^T\sum_{j=1}^{t-1}\sum_{i=1}^m \hat{\alpha}_i^t \EE_{\pi^j}(|\hat{r}_i^t(x,y_1)-\hat{r}_i^t(x,y_2)| -|r_i^*(x,y_1)- r_i^*(x,y_2)|)\right)\\
% %     &\le \sum_{t=1}^T \sum_{i=1}^m \eta(L_i^t(\theta_i^*) -L_i^t(\theta_i^t)) + \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\\
% %     &\qquad + \cO(\sqrt{T}) + \cO\left(\sum_{t=1}^T\sum_{j=1}^{t-1}\sum_{i=1}^m \hat{\alpha}_i^t \EE_{\pi^j}[|\hat{r}_i^t(x,y_1)-\hat{r}_i^t(x,y_2) -(r_i^*(x,y_1)- r_i^*(x,y_2))|]\right)
% % \end{align*}

% Now we bound the term $\sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)].$
% \begin{align}
%     &\sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)]\nonumber\\
%     &\le \sum_{t=1}^T \EE_{\pi^t}[r^t(x,y)] - \EE_{\pi^t}[r^*(x,y)] \nonumber\\
%     &\qquad  -\left(\sum_{t=1}^T \EE_{\pi_{\textrm{base}}}[r^t(x,y)] - \EE_{\pi_{\textrm{base}}}[r^*(x,y)]\right)\nonumber\\
%     &=\sum_{t=1}^T \EE_{y_1\sim \pi^t, y_2\sim \pi_{\textrm{base}}}[r^t(x,y_1)-r^t(x,y_2) - (r^*(x,y_1)-r^*(x,y_2))]\nonumber\\
%     &=\underbrace{\sum_{t=1}^T \sum_{i=1}^m \alpha_i^*\EE_{y_1\sim \pi^t, y_2\sim \pi_{\textrm{base}}} [r_i^t(x,y_1)-r_i^t(x,y_2) - (r_i^*(x,y_1)-r_i^*(x,y_2))]}_{\textrm{$( \ast)$ }}.\nonumber
%     \end{align}
%     Now since the reward contains a linear structure, by Lemma \ref{lemma:linearstructure}, for $d_{\mathrm{cover}}(1/T) = \widetilde{\cO}(d)$, we can derive that 
%     \begin{small}
%     \begin{align}
%     (\ast)&\le \sum_{i=1}^m \alpha_i^* \cdot \left(\mu_i \cdot \sum_{t=1}^T   \sum_{j=1}^{t-1}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\left(r_i^t(x,y_1)-r_i^t(x,y_2) - (r_i^*(x,y_1)-r_i^*(x,y_2))\right)^2] + \frac{d_{\mathrm{cover}}(1/T)}{4\mu_i} + Bd_{\mathrm{cover}}(1/T) + \sqrt{d_{\mathrm{cover}}(1/T)}\right)\nonumber\\
%     &\le \sum_{i=1}^m \alpha_i^* \cdot \left(\mu_i \exp(4/\beta)\kappa\cdot \sum_{t=1}^T   \sum_{j=1}^{t-1}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\mathrm{base}}}[\left(r_i^t(x,y_1)-r_i^t(x,y_2) - (r_i^*(x,y_1)-r_i^*(x,y_2))\right)^2] + \frac{d_{\mathrm{cover}}(1/T)}{4\mu_i}\right)+ Bd_{\mathrm{cover}}(1/T) + \sqrt{d_{\mathrm{cover}}(1/T)}.\label{ineq:apr error of reward}
%     % \\
%     % &\le   \left(\mu \cdot \sum_{t=1}^T   \sum_{j=1}^{t-1}\sum_{i=1}^m \alpha_i^*\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)^2] + \frac{d}{4\mu}\right)
% \end{align}
% \end{small}
% The last inequality uses the fact that $$\sup_{x,y} \frac{\pi_{\mathrm{base}}(y\mid x)}{\pi^j(y\mid x)} \le \sup_{x,y} \frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\cdot \sup_{x,y}\frac{\pi_{\mathrm{ref}}(y\mid x)}{\pi^j(y\mid x)} \le \exp(4/\beta)\cdot \kappa.$$

% Now by the MLE loss, there exists a constant $C$ such that 
% \begin{align*}
%     \sum_{t=1}^T \sum_{i=1}^m &\eta (L_i^t(\theta_i^*) - L_i^t(\theta_i^t))
%     \\&\le 2T\sum_{i=1}^m \eta \log(|\cR|/\delta) - C\sum_{t=1}^T \sum_{i=1}^m\eta\sum_{j \in \cD_i^{t-1}}\EE_{y\sim \pi^j} \left[\Delta_i^t(x,y)^2\right],
% \end{align*}
% where $\kappa = \sup_{x,y} \frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}(y\mid x)}}$ \citep{cen2024value}.
% Choose $\mu_i = \frac{C}{\alpha_i^* \exp(4/\beta)\kappa\sqrt{T}}$, then we can get 
% \begin{align*}
%     \textrm{Reg}(T) &\le 2T\sum_{i=1}^m \eta \log(|\cR|/\delta) - \eta\cdot C \underbrace{\sum_{t=1}^T \sum_{i=1}^m  \sum_{j \in \cD_i^{t-1}}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\mathrm{base}}}[\Delta_i^t(x,y)^2]}_{\text{(A)}} \\&\qquad + \frac{C}{\sqrt{T}}\underbrace{\cdot \sum_{t=1}^T \sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)^2]}_{\text{(B)}}+Bd_{\mathrm{cover}}(1/T) + \sqrt{d_{\mathrm{cover}}(1/T)}\\
%     &\qquad + \frac{d_{\mathrm{cover}}(1/T)\exp(4/\beta) \kappa}{4}\sqrt{T} + \exp(4/\beta)\cdot \mathrm{poly}(m)\cdot \underbrace{\widetilde{\cO}\left(\sqrt{T}\log(T/\delta) + \sum_{t=1}^T \sum_{j=1}^{t-1}\sum_{i=1}^m \frac{1}{t}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}\Delta_i^t(x,y)\hat{\alpha}_i^t\right)}_{\textrm{(C)}}.
% \end{align*}

% \paragraph{(A) and (B):} \noindent First, we need to derive the relationship between (A) and (B).  We rewrite the key term as 
% \begin{align}
%     \textrm{(A)}&\ge\sum_{t=1}^T \sum_{i=1}^m \sum_{j \in \cD_i^{t-1}}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)^2] \\ &= \sum_{t=1}^T \sum_{i=1}^m \sum_{j=1}^{t-1}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}, I \sim M(x,y_1,y_2, r^*)}[\Delta_i^t (x,y)^2 \mathbb{I}\{I^j = i\}]\\
%     &\ge \kappa_1\sum_{i=1}^m \sum_{j=1}^T \sum_{t=j}^T \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}, I= i}[\Delta_i^t(x^j,y^j)^2 \mathbb{I}\{I^j = i\}]\\
%     &= \kappa_1\sum_{i=1}^m \sum_{j=1}^T \sum_{t=j}^T \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}} [\Delta_i^t(x,y)^2]\\
%     &= \kappa_1\cdot \textrm{(B)}
% \end{align}
% where the inequality uses the fact that 
% $\sup_{y,x,j,t, I}\frac{1}{\cdot M(I\mid x,y_1,y_2, r^*)} = \kappa_1$ for some constant $\kappa_1.$ Since the distribution of index is a bounded softmax distribution, we can derive that 
% The constant $\kappa_1\ge \frac{e^0}{e^0+(m-1)e^B}\ge \frac{1}{me^B}$. Thus we can get 
% \begin{align*}
%     \mathrm{(A)}\ge \kappa_1 \cdot \mathrm{(B)} \ge \frac{1}{me^B}\mathrm{(B)}.
% \end{align*}
% \paragraph{(C):} Now we want to bound the term $\textrm{(C)}.$
% It can be achieved by estimating $\hat{\alpha}$ with $\tilde{\theta}_1^t, \tilde{\theta}_2^t, \cdots, \tilde{\theta}_m^t$, where $\tilde{\theta}_i^t = \argmin_\theta L_i^t(\theta)$ only minimizes the log-likelihood loss without optimistic exploration. The corresponding $\Delta_i^t(x^j,y^j)$ will also change. Denote it as $\Delta_i^{t,+}(x^j,y^j)=\left|\hat{r}_i^{\tilde{\theta}_i^t}(x^j,y_1^j) - \hat{r}_i^{\tilde{\theta}_i^t}(x^j,y_2^j) - (r_i^*(x^j,y_1^j) - r_i^*(x^j,y_2^j))\right|$.

% Then we can get 
% \begin{align*}\Delta_i^{t,+}(x^j,y^j) 
% & = \left| \langle \tilde{\theta}_i^t-\theta_i^*, \phi_i(x^j,y_1^j) - \phi_i(x^j, y_2^j) \rangle\right|\\
% & \le \|\tilde{\theta}_i^t - \theta_i^* \|_{\Sigma_{\cD_i}^{t-1}}\cdot \|\phi_i(x^j,y_1^j)- \phi_i(x^j, y_2^j)\|_{(\Sigma_{\cD_i}^{t-1})^{-1}}
% \end{align*}
% Then by Lemma 3.1 in \citep{zhu2023principled}, we can get $\|\tilde{\theta}_i^t - \theta_i^*\|_{\Sigma_{\cD_i}^{t-1}} \le C(d, B, \delta) = \textrm{poly}(d,B, \log(1/\delta))$ for some constant $C(d,B,\delta)$, and then we can get 
% \begin{align*}
%     \Delta_i^{t,+}(x^j,y^j) \le C(d,B,\delta)\cdot \|\phi_i(x^j,y_1^j)- \phi_i(x^j, y_2^j)\|_{(\Sigma_{\cD_i}^{t-1})^{-1}}.
% \end{align*}
% Now apply the same technique in Problem 1, we can get 
% \begin{align*}
%     \sum_{i=1}^m \sum_{t=1}^T \sum_{j=1}^{t-1} \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}} \frac{1}{t}[\Delta_i^{t,+}(x,y)\hat{\alpha}_i^t] &\le \frac{1}{\kappa_1}\sum_{t=1}^T \sum_{i=1}^m \sum_{j \in \cD_i^{t-1}}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}} \frac{1}{t}[\Delta_i^{t,+}(x,y)\hat{\alpha}_i^t]\\
%     & =\frac{1}{\kappa_1}\sum_{j=1}^T  \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}} \sum_{t\ge j} \left[\frac{1}{t}\Delta_{I^j}^{t,+}(x,y)\hat{\alpha}_{I^j}^t\right]\\
%     & \le\frac{1}{\kappa_1}\sum_{j=1}^T  \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}} \sum_{t\ge j} \left[\frac{1}{t}\Delta_{I^j}^{t,+}(x,y)\right]
% \end{align*}
% The second line is because that, the summation is over
% \begin{align*}\{(t,i,j)\mid t \in [T],i \in [m] ,j \in \cD_i^{t-1}\}&=\{(t,i,j)\mid t \in [T], i \in [m], j \le t-1, I^j = i\}\\
% &=\{(t,i,j)\mid j \in [T], t > j, i = I^j\}.\end{align*}
% The last inequality uses the fact that $\hat{\alpha}_{I^j}^t\le 1$.
% Then we can use the Azuma-Hoeffding's inequality, to further get 
% \begin{align*}
%     \sum_{i=1}^m \sum_{t=1}^T \sum_{j=1}^{t-1} \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}} \frac{1}{t}[\Delta_i^{t,+}(x,y)\hat{\alpha}_i^t] \le \frac{1}{\kappa_1}\sum_{j=1}^T  \sum_{t> j} \left[\frac{1}{t}\Delta_{I^j}^{t,+}(x^j,y^j)\right] + \cO(\sqrt{T}\log (T/\delta)).
% \end{align*}
% Now to present the proof in a simple way, we simplify $\Sigma_{\cD_{I^j}}^{t-1} $ as $\Sigma^{t-1,(j)}$.
% The second term $\textrm{(C)}$ will becomes 
% \begin{align*}
%     & \frac{1}{\kappa_1}\sum_{j=1}^T \sum_{t>j} \frac{1}{t}\hat{\alpha}_i^t\cdot C(d,B,\delta) \cdot \|\phi_{I^j}(x^j,y_1^j)- \phi_{I^j}(x^j, y_2^j)\|_{(\Sigma^{t-1,(j)})^{-1}}\\
%     &\le  \frac{1}{\kappa_1}\sum_{j=1}^T \sum_{t>j}\frac{1}{t}\hat{\alpha}_i^t\cdot C(d,B,\delta) \cdot \|\phi_{I^j}(x^j,y_1^j)- \phi_{I^j}(x^j, y_2^j)\|_{(\Sigma^{j,(j)})^{-1}}\\
%     &\le  \frac{1}{\kappa_1}\sum_{j=1}^T C(d,B,\delta)\|\phi_{I^j}(x^j,y_1^j)- \phi_{I^j}(x^j, y_2^j)\|_{(\Sigma^{j,(j)})^{-1}}\sum_{t=j}^{T} \frac{1}{t}\\
%     &\le \frac{\log T}{\kappa_1} \cdot \sum_{j=1}^T C(d,B,\delta)\|\phi_{I^j}(x^j,y_1^j)- \phi_{I^j}(x^j, y_2^j)\|_{(\Sigma^{j,(j)})^{-1}}
%     \end{align*}
% Now, we can decompose $\{1,2,\cdots, T\}$ into $m$ different set $\cD_i = \{j \in [T]: I^j = i\}$. Then, we fixed $i$ and denote $\|\phi_i(x^j,y_1^j)-\phi_i(x^j,y_2^j)\|^2_{(\Sigma_{\cD_i}^{j})^{-1}}=S_j$ with $\|S_j\| \le B^2$, by Cauchy's inequality,
% \begin{align*}
%     &\sum_{j \in \cD_i}\|\phi_{I^j}(x^j,y_1^j)- \phi_{I^j}(x^j, y_2^j)\|_{(\Sigma^{j,(j)})^{-1}}\\
%     &\le \sqrt{T}\sqrt{\sum_{j \in \cD_i}S_j}\\
%     &\le \sqrt{T}\sqrt{\sum_{j \in \cD_i}S_j\II\{S_j\le 1\}} + \sqrt{T}\sqrt{\sum_{j \in \cD_i}S_j \II\{S_j > 1\}}\\
%     &\le \sqrt{T}\cdot \left(\sqrt{\sum_{j \in \cD_i}\min\{1,S_j\}} + \sqrt{B^2\sum_{j \in \cD_i}\II\{S_j>1\}}\right)\\
%     &\le \widetilde{\cO}(Bd\sqrt{T})
% \end{align*}
% Then summing over $i \in [m]$, we can get 
%     \begin{align}
%     &\frac{\log T}{\kappa_1} \cdot \sum_{j=1}^T C(d,B,\delta)\|\phi_{I^j}(x^j,y_1^j)- \phi_{I^j}(x^j, y_2^j)\|_{(\Sigma_{\cD_i}^{j})^{-1}}\nonumber\\
%     &\le \frac{\log T}{\kappa_1}\cdot m \cdot C(d,B,\delta) \cdot \widetilde{\cO}(Bd\sqrt{T})\nonumber\\
%     & = \widetilde{\cO}(m^2 e^B\cdot Bd\cdot C(d,B,\delta)\sqrt{T}). \label{ineq: bound(C)}
% \end{align}
% Hence we try to provide the final proof. Then if we choose $\eta = \frac{me}{ \sqrt{T}}$,
% \begin{align*}
%     \mathrm{Reg}(T) &\le 2T m\eta  \log(|\cR|/\delta) - \eta C  \mathrm{(A)} + \frac{C}{\sqrt{T}}\cdot \mathrm{(B)} + \frac{d_{\mathrm{cover}}(1/T)\exp(4/\beta) \kappa}{4}\sqrt{T} +Bd_{\mathrm{cover}}(1/T) + \sqrt{d_{\mathrm{cover}}(1/T)} +\mathrm{(C)}\\
%     &\le 2Tm\eta \log(|\cR|/\delta) -  \frac{\eta C }{me}\mathrm{(B)} + \frac{C}{\sqrt{T}} \cdot \mathrm{(B)} + \frac{d_{\mathrm{cover}}(1/T)\exp(4/\beta) \kappa}{4}\sqrt{T} +Bd_{\mathrm{cover}}(1/T) + \sqrt{d_{\mathrm{cover}}(1/T)}+ \mathrm{(C)}\\
%     & \le 2m^2e \sqrt{T} \log(|\cR|/\delta) + \frac{d_{\mathrm{cover}}(1/T)\exp(4/\beta) \kappa}{4}\sqrt{T} + Bd_{\mathrm{cover}}(1/T) + \sqrt{d_{\mathrm{cover}}(1/T)}
%     \\
%     &\qquad +\widetilde{\cO}(\mathrm{poly}(\exp(1/\beta), e^B, m,d, \kappa)\cdot C(d,B,\delta)\sqrt{T})\\
%     &\le \widetilde{\cO}(\mathrm{poly}(\exp(1/\beta), e^B, m,d, \kappa,\log (|\cR|T/\delta))\cdot \sqrt{T}).
% \end{align*}
% Hence we complete the proof.

% \end{proof}



% % \subsection{Proof of Theorem \ref{thm:alphabound_way1}}
% % \begin{proof}
% % First, suppose we have $\left||\hat{r}_i^t(x,y_1) -\hat{r}_i^{t}(x,y_2)|-|r_i^*(x,y_1) - r_i^*(x,y_2)|\right| = \Delta_i^t(x,y)$ , then by Theorem \ref{thm:mle}, we have 
% % \begin{align*}
% %     \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}&\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}^2\\
% %     &\le 2 \log (|\cF|/\delta),
% % \end{align*}
% %     where $\cF = \{\mathrm{Softmax}(x_i)\mid 1\le i\le m, x_i \le 1\}$, and the log of $\varepsilon-$covering number is linear in $m\log(1/\varepsilon)$.

% %     Thus by the Cauchy's inequality, we can get 
% %     \begin{align*}
% %         &\sqrt{2t\log(|\cF|/\delta)}\\&\ge \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}\\
% %         &\ge \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}\\
% %         &\qquad - \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\hat{\alpha}_i^t\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}
% %     \end{align*}

% %     Now we bound the difference of $\alpha$ based on the difference of the softmax distribution.

% %     Fixed $t-1$, since the upper bound of $0\le \hat{r}(x,y) \le B$ and $0\le r^*(x,y) \le B$, consider $X_i = |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|\le B$, $X_i^* = |r^*_i(x,y_1) - r^*_i(x,y_2)|\le B$, then 
% %     \begin{align*}
% %         &\left\|\textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\hat{\alpha}_i^{t}\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\|_{\mathrm{TV}}\\
% %         &= \sum_i \left|\frac{e^{X_i\cdot \hat{\alpha}_i^t}}{\sum_j e^{X_i\cdot \hat{\alpha}_j^t}}-\frac{e^{X_j^*\cdot \hat{\alpha}_i^t}}{\sum_j e^{X_j^*\cdot \hat{\alpha}_j^t}}\right|\\
% %         &=\sum_i \left|\frac{\sum_{j\neq i}e^{X_j^*\cdot \hat{\alpha}_j^t+X_i\hat{\alpha}_i^t }-e^{X_j\cdot \hat{\alpha}_j^t+X_i^*\hat{\alpha}_i^t }}{(\sum_j e^{X_j\cdot \hat{\alpha}_j^t})(\sum_j e^{X_j^*\cdot \hat{\alpha}_j^t})}\right|\\
% %         &\le \sum_i \left|\frac{\sum_{j\neq i}e^{X_j^* \hat{\alpha}_j^t+X_i\hat{\alpha}_i^t}(e^{\Delta_j^t \hat{\alpha}_j^t + \Delta_i^t \hat{\alpha}_i^t}-1)}{m^2}\right|,\end{align*}
% % where the last inequality uses the fact that $\sum_{j}e^{X_j\cdot \hat{\alpha}_i^t} \ge m$ and $\sum_j e^{X_j^* \hat{\alpha}_j^t} \ge m$.
% % Now since $e^{X_j^* \hat{\alpha}_j^t + X_i \hat{\alpha}_i^t}\le e^{B(\hat{\alpha}_i^t + \hat{\alpha}_j^t)}\le e^B$, and $e^a-1\le e^B\cdot a$ for every $0\le a \le B$, we can have 
% %         \begin{align*}
% %         &\le \sum_i \left|\frac{\sum_{j\neq i}e^{2B}(\Delta_j^t \hat{\alpha}_j^t + \Delta_i^t \hat{\alpha}_i^t)}{m^2}\right|\\
% %         &\le \frac{e^{2B}}{m^2}\sum_i \sum_{j\neq i}(\Delta_j^t \hat{\alpha}_j^t + \Delta_i^t \hat{\alpha}_i^t)\\
% %         &\le \frac{e^{2B}}{m}\sum_i \Delta_i^t \hat{\alpha}_i^t.
% %     \end{align*}

% %     Now 
% %     choose the index $i = \argmax_i X_i^* \circ |\alpha_i^* - \hat{\alpha}_i^t|$, and WLOG, assume $\hat{\alpha}_i^t = \alpha_i^* + \varepsilon$
    
% %     we can also bound 
% %     \begin{align*}
% %         &\left\|\textrm{Softmax}(\hat{\alpha}_i^t \cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\|_{\mathrm{TV}}\\
% %         &\ge \left|\frac{e^{X_i^*\hat{\alpha}_i^t}}{\sum_j e^{X_j^*\hat{\alpha}_j^t}} - \frac{e^{X_i^*\cdot \alpha_i^*}}{\sum_j e^{X_j^*\alpha_j^*}}\right|\\
% %         &= \left|\frac{e^{X_i^*(\alpha_i^* + \varepsilon)}}{e^{X_i^*(\alpha_i^* + \varepsilon)}+\sum_{j\neq i} e^{X_j^*\hat{\alpha}_j^t}} - \frac{e^{X_i^*\cdot \alpha_i^*}}{e^{X_i^* \alpha_i^*}+\sum_j e^{X_j^*\alpha_j^*}}\right|\\
% %         &=\left|\frac{\sum_{j\neq i}e^{X_j^* \alpha_j^* + X_i^*(\alpha_i^* + \varepsilon)}-e^{X_j^* \hat{\alpha}_j^t + X_i^* \alpha_i^*}}{(\sum_j e^{X_j^*\hat{\alpha}_j^t})(\sum_j e^{X_j^*\alpha_j^*})}.\right|
% %     \end{align*}
% %     Now by the selection of the $i$, we can have 
% %     $$X_j^* \alpha_j^* + X_i^*(\alpha_i^* + \varepsilon) \ge X_j^* \hat{\alpha}_j^t + X_i^* \alpha_i^*,$$
% %     hence 
% %     $$e^{X_j^* \alpha_j^* + X_i^*(\alpha_i^* + \varepsilon)}\ge e^{X_j^* \hat{\alpha}_j^t + X_i^* \alpha_i^*}.$$
% %     Also, since $\sum_i\alpha_i^* = \sum_i \hat{\alpha}_i^t = 1,$ and the fact that $\hat{\alpha}_i^t = \alpha_i^* + \varepsilon,$ we can further derive 
% %     $$\sum_{j\neq i} \alpha_j^* = \sum_{j\neq i} \hat{\alpha}_j^t + \varepsilon.$$

% %     Then at least one $j'$ such that $\alpha_{j'}^* \ge \hat{\alpha}_{j'}^t + \varepsilon/m$.
% %     Then 
% %     \begin{align*}
% %         e^{X_{j'}^* \alpha_{j'}^*+ X_i^*(\alpha_i^*+\varepsilon)}-e^{\hat{\alpha}_{j'}^t X_{j'}^* + X_i^*\alpha_i^*}&\ge ^{X_{j'}^* \hat{\alpha}_{j'}^t+ X_i^*(\alpha_i^*+\varepsilon)}-e^{\hat{\alpha}_{j'}^t X_{j'}^* + X_i^*\alpha_i^*}
% %         \\&\ge e^{X_i^*(\alpha_i^* + \varepsilon)} - e^{X_i^* \alpha_i^*}\\
% %         &\ge e^{\alpha_i^* X_i^*}(e^{\varepsilon X_i^*}-1)\\
% %         &\ge e^{\alpha_i^* X_i^*}\cdot \varepsilon X_i^*.
% %     \end{align*}
% %     Thus,
% %     \begin{align*}
% %         &\left\|\textrm{Softmax}(\hat{\alpha}_i^t \cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\|_{\mathrm{TV}}\\
% %         &\ge \frac{e^{\alpha_i^* X_i^*}}{(\sum_j e^{X_j^*\hat{\alpha}_j^t})(\sum_j e^{X_j^*\alpha_j^*})} \cdot \varepsilon X_i^*\\
% %         &\ge \frac{1}{(me^B)^2}  \cdot \varepsilon X_i^*.
% %     \end{align*}
% %     Now we can get 
% %     $$\|X^*\circ |\alpha_i^* - \hat{\alpha}_i^t|\|_\infty\le m^2e^{2B} \left\|\textrm{Softmax}(\hat{\alpha}_i^t \cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\|_{\mathrm{TV}},$$
% %     then take the expectation we can get 
% %     \begin{align*}
% %         &\EE_{x,y\sim \cD_j}\|X^*\circ |\alpha_i^* - \hat{\alpha}_i^t|\|_\infty\\&\qquad \le m^2e^{2B} \EE_{x,y\sim \cD_j}\left\|\textrm{Softmax}(\hat{\alpha}_i^t \cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\|_{\mathrm{TV}}.
% %     \end{align*}


    
% % %     Now suppose $i' = \argmax_i |\alpha_i^*- \hat{\alpha}_i^t|$, then by the definition of $i$ and $i'$, we have 
% % %     \begin{align*}
% % %         \|\alpha^* - \hat{\alpha}^t\|_\infty &= |\alpha_{i'}^* - \hat{\alpha}_{i'}^t|\\
% % %         & \le \frac{X_{i'}|\alpha_{i'}^* - \hat{\alpha}_{i'}^t|}{X_i} \cdot \gamma\\
% % %         &\le \gamma |\alpha_i^* - \hat{\alpha}_i^t| = \varepsilon\gamma.    \end{align*}
% % % Hence 
% % % \begin{align*}
% % %     &\left\|\textrm{Softmax}(\hat{\alpha}_i^t \cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\|_{\mathrm{TV}}\\
% % %     &\ge \frac{1}{(me^2)^2} \gamma \cdot \varepsilon\\
% % %     &\ge \frac{1}{(me^2)^2} \gamma^2\|\alpha^* - \hat{\alpha}^t\|_\infty
% % % \end{align*}

% % % Now we have 
% % % \begin{align*}
% % %     &2\log(|\cF|/\delta) \\
% % %     &\ge \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}^2\\
% % %     &\ge \frac{1}{t}\left(\sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}\right)^2.
% % % \end{align*}
% %     % Hence 
% %     % \begin{align*}
% %     %     &\sqrt{2t\log(|\cF|/\delta)}\\&\ge \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}\\
% %     %     &\ge \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}\\
% %     %     &\qquad - \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\hat{\alpha}_i^t\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}\\
% %     %     &\ge \frac{(t-1)\gamma^2}{m^2e^{2B}}\|\alpha^* - \hat{\alpha}^t\|_\infty \\
% %     %     &\qquad - \sum_{j=1}^{t-1}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x^j,y_1^j)-\hat{r}_i^{t-1}(x^j,y_2^j)|)-\textrm{Softmax}(\hat{\alpha}_i^t\cdot |r_i^*(x^j,y_1^j)-r_i^*(x^j,y_2^j)|)\right\| _{\mathrm{TV}} - O(\sqrt{t})\\
% %     %     &\ge \frac{(t-1)\gamma^2}{m^2e^{2B}}\|\alpha^* - \hat{\alpha}^t\|_\infty - \frac{e^{2B}}{m}\sum_{j=1}^{t-1}\sum_i \Delta_i^t(x^j,y^j)\hat{\alpha}_i^t
% %     % \end{align*}

% %      Hence 
% %     \begin{align*}
% %         &\sqrt{2t\log(|\cF|/\delta)}\\&\ge \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}\\
% %         &\ge \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)-\textrm{Softmax}(\alpha_i^*\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}\\
% %         &\qquad - \sum_{j=1}^{t-1} \EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\hat{\alpha}_i^t\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}}\\
% %         &\ge \sum_{j=1}^{t-1}\EE_{x, y\sim \cD_j}\frac{1}{m^2e^{2B}}\|X^*(x,y)|\alpha^* - \hat{\alpha}^t|\|_\infty \\
% %         &\qquad - \sum_{j=1}^{t-1}\EE_{x, y\sim \cD_j}\left\| \textrm{Softmax}(\hat{\alpha}_i^t \cdot |\hat{r}_i^{t-1}(x,y_1)-\hat{r}_i^{t-1}(x,y_2)|)-\textrm{Softmax}(\hat{\alpha}_i^t\cdot |r_i^*(x,y_1)-r_i^*(x,y_2)|)\right\| _{\mathrm{TV}} \\
% %         &\ge \sum_{j=1}^{t-1}\EE_{x, y\sim \cD_j}\frac{1}{m^2e^{2B}}\|X^*(x,y)\cdot |\alpha^* - \hat{\alpha}^t|\|_\infty - \frac{e^{2B}}{m}\sum_{j=1}^{t-1}\sum_i \EE_{x,y\sim \cD_j}[\Delta_i^t(x,y)\hat{\alpha}_i^t]
% %     \end{align*}
% %     Hence we finally get 
% %     \begin{align*}
% %         \sum_{j=1}^{t-1}\EE_{x, y\sim \cD_j}\|X^*(x,y)\circ|\alpha^* - \hat{\alpha}^t|\|_\infty &\le m^2e^{2B}\left(\sqrt{2t\log (|\cF|/\delta)} + \frac{e^{2B}}{m} \sum_{j=1}^{t-1}\sum_{i=1}^m \Delta_i^t(x^j,y^j) \hat{\alpha}_i^t\right)\\
% %         & = \mathrm{poly}(m, e^B)\cdot \widetilde{\cO}\left(\sqrt{t\log (T/\delta))} + \sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1,y_2\sim \pi^j}[\Delta_i^t(x,y)]\hat{\alpha}_i^t\right).
% %     \end{align*}
% %     % Then because $\|X^*(x,y)\cdot |\alpha^* - \hat{\alpha}^t|\|_\infty \le B$ is bounded ,  by the Azuma-Hoeffding's inequality we can derive 
% %     % \begin{align*}
% %     %     &\sum_{j=1}^{t-1}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\text{base}}}\|X^*(x,y)\circ|\alpha^* - \hat{\alpha}^t|\|_\infty \\&\qquad = \mathrm{poly}(m, e^B)\cdot \widetilde{\cO}\left(\sqrt{t\log (|\cF|/\delta))} + \sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)]\hat{\alpha}_i^t\right) + \cO(B\sqrt{t}\log(1/\delta))\\
% %     %     &\qquad = \mathrm{poly}(m, e^B)\cdot \widetilde{\cO}\left(\sqrt{t}\log (|\cF|/\delta)) + \sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)]\hat{\alpha}_i^t\right).
% %     % \end{align*}
% %     Now by Lemma \ref{lemma:policy diff}, we can get  $\sup_{j,y,x}\frac{\pi^*(y\mid x)}{\pi^j(y\mid x)} \le  \exp(4/\beta)$, we can get 
% %     \begin{align}
% %     &\gamma(t-1) \|\alpha^* - \hat{\alpha}^{t}\|_\infty
% %         \\&\le (t-1) \EE_{y_1,y_2\sim \pi^*}\|X^*(x,y)\circ|\alpha^* - \hat{\alpha}^t|\|_\infty\nonumber\\
% %         &\le  \exp(4/\beta)
% %         \sum_{j=1}^{t-1}\EE_{y_1,y_2\sim \pi^j}\|X^*(x,y)\circ|\alpha^* - \hat{\alpha}^j|\|_\infty \nonumber\\&= \exp(4/\beta)\cdot \mathrm{poly}(m, e^B)\cdot \widetilde{\cO}\left(\sqrt{t}\log (|\cF|/\delta)) +  \sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1,y_2\sim \pi^j}[\Delta_i^t(x,y)]\hat{\alpha}_i^t\right)\nonumber\\
% %         &\le \exp(4/\beta)\cdot \mathrm{poly}(m, e^B)\cdot \widetilde{\cO}\left(\sqrt{t}\log (|\cF|/\delta)) +  \sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1,y_2\sim \pi^j}[\Delta_i^t(x,y)]\right).\label{eq:estimate alpha step 1}
% %     \end{align}
% %     Now we further derive the  final result.
% %     Frist, by $\alpha^t = \frac{1}{t}\sum_{i=1}^t \hat{\alpha}^t$, we can get 
% %     \begin{align*}
% %         \|\alpha^* - \alpha^t\|_\infty &\le \frac{1}{t}\sum_{j=1}^{t}\|\alpha^* - \hat{\alpha}^j\|_\infty\\
% %         &\le \gamma^{-1}\exp(4/\beta)\cdot \mathrm{poly}(m,e^B)\sum_{j=1}^t\frac{1}{j}\widetilde{\cO}\left(\sqrt{j}\log (|\cF|/\delta) + \sum_{s=1}^{j-1}\sum_{i=1}^m \EE_{y_1,y_2\sim \pi^s}[\Delta_i^j(x,y)]\right)
% %     \end{align*}
% %  \end{proof}
   
% %     % Hence we finally get 
% %     % \begin{align*}
% %     %     \|\alpha^* - \hat{\alpha}^t\|_\infty &\le \frac{m^2e^{2B}}{\gamma^2(t-1)}\left(\sqrt{2t\log (|\cF|/\delta)} + \frac{e^{2B}}{m} \sum_{j=1}^{t-1}\sum_{i=1}^m \Delta_i^t(x^j,y^j) \hat{\alpha}_i^t\right)\\
% %     %     & = \cO\left(\frac{1}{\sqrt{t}} + \frac{1}{t}\sum_{j=1}^{t-1}\sum_{i=1}^m \Delta_i^t(x^j,y^j)\hat{\alpha}_i^t\right).
% %     % \end{align*}
% %     % Now since $\Delta_i^t(x^j,y^j) = \langle \theta_i^t - \theta_i^*, \phi_i(x^j,y_1^j), \phi_i(x^j,y_2^j)\rangle$, then by the MLE result, we can further get 
% %     % $$\|\theta_i^t - \theta_i^*\|_{\Sigma_{\cD_i}^{t}}\le C\sqrt{d+\log(1/\delta)}:=C(d,B,\delta).$$

% %     % Now by Cauchy's inequality, 
% %     % \begin{align*}
% %     %     \Delta_i^t(x^j,y^j) \le C(d,B,\delta) \|\phi_i(x^j,y_1^j)-\phi_i(x^j,y_2^j)\|_{(\Sigma_{\cD_i}^t)^{-1}}.
% %     % \end{align*}
% %     % Then (This should be careful by $I(\|\phi_i(\cdot,\cdot)\|_\Sigma \ge 1)$
% %     % \begin{align*}
% %     %     \|\alpha^* - \hat{\alpha}^t\|_\infty &=\cO\left(\frac{1}{\sqrt{t}} + \frac{1}{t}\sum_{j=1}^{t-1}\sum_{i=1}^m \Delta_i^t(x^j,y^j)\hat{\alpha}_i^t\right)\\
% %     %     &\le \cO\left(\frac{1}{\sqrt{t}} + \frac{1}{t}C(d,B,\delta)\sum_{j=1}^{t-1}\sum_{i=1}^m \hat{\alpha}_i^t \|\phi_i(x^j,y_1^j)-\phi_i(x^j,y_2^j)\|_{(\Sigma_{\cD_i}^t)^{-1}}\right)\\
% %     %     &\le \cO\left(\frac{1}{\sqrt{t}} + \frac{1}{t}C(d,B,\delta)\sum_{j=1}^{t-1}\sum_{i=1}^m \hat{\alpha}_i^t \|\phi_i(x^j,y_1^j)-\phi_i(x^j,y_2^j)\|_{(\Sigma_{\cD_i}^j)^{-1}}\right)\\
% %     %     & = \cO\left(\frac{1}{\sqrt{t}} + \frac{1}{\sqrt{t}}C(d,B,\delta) \sum_{j=1}^{t-1}\sum_{i=1}^m \hat{\alpha}_i^t \|\phi_i(x^j,y_1^j)-\phi_i(x^j,y_2^j)\|_{(\Sigma_{\cD_i}^j)^{-1}}^2\right)\\
% %     %     &= \cO\left(\frac{C(d,B,\delta)}{\sqrt{t}}\right).
% %     % \end{align*}

% %     % Then final    assumption is that $\sum_{i=1}^m \hat{\alpha}_i^t \Sigma_{\cD_i}^j\succeq C_j\cdot \Sigma_\cD^j$
% %     % \textcolor{red}{This assumption is dependent on the online process! Not make sense}

    
% %     % Now we only prove when the absolute value disappear. The other side holds by symmetry.
    
% %     % Suppose $\sum_j e^{X_j^*\hat{\alpha}_j^t} \le \sum_j e^{X_j^*\alpha_j^*}$, thus the RHS is not less than 
% %     % $$\frac{e^{X_i^* \hat{\alpha}_i^t}-e^{X_i^* \alpha_i^*}}{\sum_j e^{X_j^* \hat{\alpha}_j^t}}$$
    
% %     % then 
% %     % define $S_1 = \{j \in [m]: \hat{\alpha}_j^t \ge \alpha_j^*\}$, $S_2 = [m]\setminus S_1$, 
% %     % we have 
% %     % $$\sum_{j \in S_1} e^{X_j^*\hat{\alpha}_j^t}-e^{X_j^*\alpha_j^*} \le \sum_{j \in S_2}e^{X_j^*\alpha_j^*}-e^{X_j^*\hat{\alpha}_j^t}.$$

% %     % Now 




% \section{Help}
\input{temp}
% \section{Experiment Details}\label{app:experiment}
% \subsection{Practical Algorithms and Details}
% \begin{algorithm}[H] 
%      \begin{algorithmic}[1] 
%          \caption{MOP(Practical Version)-Offline} 
%          \label{alg: vpo-fl-prac} 
%          \STATE \textbf{Initial}: $\overline{d}^0 = (\frac{1}{m},\cdots, \frac{1}{m})^\top $, dataset $\cD_{\mathrm{offline}}$, W.
%          \STATE Calculate the optimal policy $\pi_i$ for each objective $i \in [m]$ using offline dataset $\cD_{\mathrm{offline}}$.
%          \FOR{$t=1,2,\cdots,T$} 
%          %\STATE Calculate $\pi_i^t = \text{PPO}(r_i)$ for all $i \in [m]$. 
%             \STATE Execute $\pi^t=\mathrm{MOD}(\{\pi_i\}_{i \le m}, \overline{d^{t-1}})$. 
%             \STATE Calculate the point $V^t \in \RR^m$. %Calculate $\overline{V}^t = \frac{t-1}{t}\overline{V}^{t-1} + \frac{1}{t}V^t$.
%             \STATE Calculate the direction $d^{t} = \mathrm{Proj}(W, V^t),$ and get the average direction $\overline{d^{t}} = \frac{1}{t}\sum_{j=1}^t \frac{d^j}{\|d^j\|_1}.$
%          \ENDFOR 
%      \end{algorithmic} 
% \end{algorithm} 
% Note that the algorithm average the direction instead of averaging the estimated reward vector function, which can lead to a more stable result. 

% To execute the Line 2, following the previous paper \citep{shi2024decoding}, we first fine-tune the model LLAMA2-7B on the Anthropic-HH dataset \citep{ouyang2022training} to get the reference policy $\pi_{\mathrm{ref}}$. We then get the optimal policy $\pi_i$ for each objective $i \in \{1,2,3\}$ using PPO approach trained on three off-sheld reward model:
% \begin{itemize}
%     \item Harmlessness: \url{https://huggingface.co/Ray2333/gpt2-large-harmless-reward_model}
% \item Helpfulness: \url{https:
% //huggingface.co/Ray2333/gpt2-large-helpful-reward_model}
% \item Humor: \url{https://huggingface.co/mohameddhiab/humor-no-humor}
% \end{itemize}
% To compute the expected reward vector $V^t$, we calculate the expectation by taking the expectation over 100 training samples, and we believe the performance of MOPO can be improved by using more training samples. 

% \subsection{Harmless and Humor}
% The following table presents the results for MORLHF with the objectives Harmless and Humor, where we use $W_{0.5,1.3}^\alpha$ as the final target set. Our algorithm generally outperforms the previous one. Due to time constraints, the evaluation results are based on a down-sampled dataset of size 500. Additionally, since the aggregation only works for non-negative rewards, when using AR to aggregate the reward, we take 
% $\max\{r_i,0\}$ instead of 
% $r_i$
%   for each objective. Although this is the only reasonable approach, we observe that it performs poorly. This may be due to the vanishing gradient problem, as the gradient of 
% $\max\{r_i,0\}$ becomes zero when the reward is negative.
% \begin{table}[H]\footnotesize \centering
%  \caption{Comparison of previous representative work for MORLHF with $p=0.5$, $c = 1.3$ and the objective Harmless and Humor. The score is the distance between the evaluated reward vector and the target set. The smaller one is better.}
% \begin{tabular}{ccccc}

% \midrule[1.5pt]
% $\alpha$  &  \makecell{Ours} &  \makecell{RS}& \makecell{MOD}  &  \makecell{AR}\\ \hline
% \makecell{(0.1,0.9)}    &\textbf{0.335}&   0.362 & 0.337 & 1.767\\ 
% \makecell{(0.3,0.7)}      & 0.578 & 0.678  & \textbf{0.572}  & 2.011 \\ 
% \makecell{(0.5,0.5)} &   \textbf{0.720}   &   0.882   & 0.723   & 1.970  \\
% %\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
% \makecell{(0.7,0.3)} & \textbf{0.630} &   0.860   &  0.722 &2.411\\ 
% \makecell{(0.9,0.1)} & \textbf{0.217}  & 0.391  & 0.396 & 2.068\\ 
% \bottomrule[1.5pt]
% \end{tabular}
% \label{table:0.5}
% \end{table}


% \subsection{Multi-Group Experiments}
% We also perform the experiments on Harmless and Humor dataset when we have $N=2$ groups. One group has the target set $W_{0.5,1.3}^\alpha$ and the other has the target set $W_{-\infty,1}^\alpha$. We compare our consensus algorithm with Eq.~\eqref{eq:dir consensus} and a variant of max-min RLHF. In this variant of max-min RLHF, we use $\min\{r_1,r_2, \alpha_1\cdot (\max\{r_1,0\})^{0.5} + \alpha_2\cdot (\max\{r_2,0\})^{0.5} \}$ as the reward. The following table shows the experiment results. The results show that our algorithms perform relatively stable, while this variant of max-min RLHF performs unstable.  However, note that this variant of max-min RLHF also needs retraining whenever one group changes the aggregation approach, which is very time-consuming for the real-world application.

% \begin{table}[H]\footnotesize \centering
%  \caption{Comparison of previous representative work for MORLHF with $p=0.5$, $c = 1.3$ and the objective Harmless and Humor. The score is the distance between the evaluated reward vector and the target set. The smaller one is better.}
% \begin{tabular}{ccc}

% \midrule[1.5pt]
% $\alpha$  &  \makecell{Ours} &   \makecell{Max-Min RLHF}\\ \hline
% \makecell{(0.1,0.9)}    &\textbf{0.408}& 0.992\\ 
% \makecell{(0.3,0.7)}   &0.577 & \textbf{0.377}? \\ 
% \makecell{(0.5,0.5)} &  0.708 & \textbf{0.429}  \\
% %\makecell{Generalized Linear MDP\vspace{-0.3em}\\\tiny\citepp{wang2019optimism}} & \tiny\XSolidBrush &   \tiny\XSolidBrush   &  \tiny\Checkmark    &   \tiny\Checkmark   \\ \hline
% \makecell{(0.7,0.3)} & \textbf{0.619}&1.342\\ 
% \makecell{(0.9,0.1)} & 0.406& \textbf{0.208}\\ 
% \bottomrule[1.5pt]
% \end{tabular}
% \label{table:0.5}
% \end{table}

% % \section{Analysis of Algorithm 2}\label{sec:analysis of algorithm2}




% % Since we do not assume the target set $W^*$ is approachable, we have the following property for the approachability:

% % \begin{lemma}\label{lemma:approach}
% % For each $\theta \in \RR^m$ with $\|\theta\| = 1$, we have 
% % $$\min_{x \in W^*}\langle \theta, x\rangle\le  \EE_{\pi^*}[\langle \theta, r_i^*(x,y)\rangle-\sum_{i=1}^m \theta_i\beta \DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})] + D(\pi^*) = J(r_1^*, \cdots, r_m^*, \theta, W^*, \pi^*) +  D(\pi^*).$$    
% % \end{lemma}
% % \begin{proof}
% %     By the definition of $D(\pi^*) = d(S(\pi^*), W^*)$, we know that there exists a vector $p$ with $S(\pi^*) + p \in W^*$ and $\|p\|_2 = D(\pi^*).$
% % Then we can have $$\min_{x \in W^*}\langle \theta, x\rangle \le \langle \theta, S(\pi^*) + p\rangle\le \EE_{\pi^*}[\langle \theta, r_i^*(x,y)\rangle-\sum_{i=1}^m \theta_i\beta \DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})] + D(\pi^*)$$
% % \end{proof}

% % Denote $$D(\pi) = d(W^*, \EE_{\pi^t}[r(x,y)]-\beta \DD_{\mathrm{KL}}(\pi\|\pi_{\mathrm{ref}})).$$ Thus $\pi^* = \min_\pi D(\pi^*)$. Now by the Lemma \ref{lemma:approach}, for each $\theta \in \RR^m$ with $\|\theta\| \le 1$, we have 
% % $$\min_{x \in W^*}\langle \theta, x\rangle\le  \EE_{\pi^*}[\langle \theta, r_i^*(x,y)\rangle-\sum_{i=1}^m \theta_i\beta \DD_{\mathrm{KL}}(\pi^*\|\pi_{\mathrm{ref}})] + D(\pi^*) = J(r_1^*, \cdots, r_m^*, \theta, W^*, \pi^*) +  D(\pi^*).$$

% % Denote $V^t \in \RR^m$ with $(V^t)_i = \EE_{\pi^t}[\hat{r}_i^t(x,y) - \beta\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})]$, and $\frac{1}{t}\overline{V}^t = \sum_{i=1}^t V^i$. We have 
% % \begin{align*}
% %     d(\overline{V}^T, W^*)^2 &= \|\overline{V}^T - \Pi_{W^*} (\overline{V}^T)\|^2 \\
% %     &\le \|\overline{V}^T - \Pi_{W^*} (\overline{V}^{T-1})\|^2\\
% %     & = \left(\frac{T-1}{T}\right)^2 d(\overline{V}^{T-1}, W^*)^2 + \frac{1}{T^2} \|V^T - \Pi_{W^*}(\overline{V}^{T-1})\|^2 \\
% %     &\qquad + \frac{2(T-1)}{T^2}(\overline{V}^{T-1} - \Pi_{W^*} (\overline{V}^{T-1}))\cdot (V^T - \Pi_{W^*}(\overline{V}^{T-1}))\end{align*}
% % First, based on the definition of $W^*$, it is easy to show that $d^t \succeq 0.$
% % $\pi^t$ is the optimal policy such that $$\EE_{\pi^t}[\langle d^t, \hat{r}(x,y) \rangle - \sum_{i=1}^m d^t_i\beta \DD_{\mathrm{KL}}(\pi^t \|\pi_{\mathrm{ref}})] \ge  \EE_{\pi_{\mathrm{ref}}}[\langle d^t , \hat{r}(x,y) \rangle]\ge 0,$$
% % thus $(\sum_{i=1}^m d_i^t)\cdot \beta \DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}}) \le \EE_{\pi^t}[d^t\cdot  \hat{r}(x,y)] \le B$. Hence, given $d^t \succeq 0$ and $\|d^t\|_2 =1,$
% % \begin{align*}
% %     \beta \DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}}) \le \frac{B}{\sum_{i=1}^m d_i^t}\le B.
% % \end{align*}
% % we have $|(V^t)_i| \le B$ and 
% % \begin{align*}
% %     \|V^T - \Pi_{W^*}(\overline{V}^{T-1})\|^2 \le B^2m
% % \end{align*}
% % Thus by iteration we can have 
% % \begin{align*}
% %     T^2 d(\overline{V}^T, W^*)^2 \le T\cdot B^2m + \sum_{t=1}^T 2(t-1) (\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*}(\overline{V}^{t-1}))
% % \end{align*}
% % Now, by the definition of $d^t$, we have
% % \begin{align*}
% %     (\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*}(\overline{V}^{t-1})=
% %     d(\overline{V}^{t-1}, W^*)\cdot d^t\cdot (\Pi_{W^*}(\overline{V}^{t-1}) - V^t).
% %     \end{align*}
% %     Then, we prove the following lemma.
% %     \begin{lemma}\label{lemma:projection}
% %         $\min_{x\in W^*} \langle d^t, x\rangle = d^t \cdot \Pi_{W^*}(\overline{V}^{t-1}).$
% %     \end{lemma}
% %     \begin{proof}
% %      In fact, we only need to prove that for any $x \in W^*$, $\langle \overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}), x-\Pi_{W^*}(\overline{V}^{t-1})\rangle \le 0$. Suppose there exists $x \in W^*$ such that $\langle \overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}), x-\Pi_{W^*}(\overline{V}^{t-1})\rangle >0$, then since $W^*$ is a convex set, for any $\lambda \in (0,1)$, we have $x_\lambda = \lambda x + (1-\lambda) \Pi_{W^*}(\overline{V}^{t-1}) \in W^*$. Consider the line $$\Pi_{W^*}(\overline{V}^{t-1}) + t \frac{\Pi_{W^*}(\overline{V}^{t-1})-x}{\|\Pi_{W^*}(\overline{V}^{t-1})-x\|},\  \  t \in \RR.$$ Also, we consider the  projection of $\overline{V}^{t-1}$ on this line, and denote it as $p$. Then we can get 
% %     $$0<\langle\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1})- p + p, x -\Pi_{W^*}(\overline{V}^{t-1})\rangle  = \langle p-\Pi_{W^*}(\overline{V}^{t-1}),  x -\Pi_{W^*}(\overline{V}^{t-1})\rangle $$
% %     Hence when $\lambda \to 0$, $x_\lambda$ is between $p$  and $\Pi_{W^*}(\overline{V}^{t-1})$. Also, $$\|\overline{V}^{t-1}-x_\lambda \|^2 = \|\overline{V}-p\|^2 + \|p-x_\lambda\|^2 \le \|\overline{V}-p\|^2 + \|p-\Pi_{W^*}(\overline{V}^{t-1})\|^2 \le \|\Pi_{W^*}(\overline{V}^{t-1})-d^t \|^2,$$ which contradicts the selection of $\Pi_{W^*}(\overline{V}^{t-1}).$
% %  \end{proof}
% % Now, by Lemma \ref{lemma:approach} and Lemma \ref{lemma:projection}, we can get 
% % \begin{align*}
% %     d^t \cdot \Pi_{W^*}(\overline{V}^{t-1}) \le J(r_1^*, \cdots, r_m^*, d^t, \pi^*) + D(\pi^*).
% % \end{align*}
% % Then, we can continue the analysis by
% %     \begin{align*}
% %     (\overline{V}^{t-1}-\Pi_{W^*}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*}(\overline{V}^{t-1}))&=d(\overline{V}^{t-1}, W^*)\cdot \left( J(r_1^*, r_2^*, \cdots, r_m^*, d^t, \pi^*) + D(\pi^*) - d^t\cdot  V^t\right)\\
% %     & = d(\overline{V}^{t-1}, W^*)\cdot (J(\hat{r}_1^*, \cdots, \hat{r}_m^*, d^t, \pi^*) - J(\hat{r}_1, \cdots, \hat{r}_m, d^t, \pi^t) + D(\pi^*))\\
% %     & = d(\overline{V}^{t-1}, W^*) \cdot (\eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t) + D(\pi^*)).
% % \end{align*}
% % Thus we can get 
% % \begin{align*}
% %     T d(\overline{V}^T, W^*)^2 &\le   B^2m +
% %     \sum_{t=1}^T \frac{2(t-1)}{T}d(\overline{V}^{t-1}, W^*) \cdot (\eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t) + D(\pi^*)).
% % \end{align*}

% % Now we use induction method to show that $$d(\overline{V}^t, W^*) \le D(\pi^*) + \frac{\eta }{t}\sum_{j=1}^{t}\sum_{i=1}^m (L_i^j(\theta^*) - L_i^j(\theta^j)) + Cm/\sqrt{t},$$ where $C$ is a fixed constant which is to be determined. When $t = 1$, choose $C = 2B$ and the inequality holds by 
% % \begin{align*}
% %     \|d(\overline{V}^1, W^*)-D(\pi^*)\|\le d(\overline{V}^1, S(\pi^*)) \le 2B.
% % \end{align*}
% % Denote $A_j=\eta\cdot (\sum_{i=1}^m (L_i^t(\theta^*) - L_i^t(\theta^j)))$ and $S_t = \sum_{j=1}^{t}A_j$, then for all $t \in [T-1],$  
% % suppose we have 
% % \begin{align*}
% %     d(\overline{V}^{t-1}, W^*) \le D(\pi^*) + \frac{1}{t-1} S_{t-1} + C\left(\frac{\sqrt{m}}{\sqrt{t-1}}\right).
% % \end{align*}
% % Then we substitute these induction hypothesis into the recursion inequality and get 
% % \begin{align*}
% %     &T d(\overline{V}^T, W^*)^2\\&\le m + \sum_{t=1}^T \frac{2(t-1)}{T} \left(D(\pi^*) + \frac{1}{t-1}S_{t-1}+ C\left(\frac{\sqrt{m}}{\sqrt{t-1}}\right)\right)\left(A_t + D(\pi^*)\right)\\
% %     &\le B^2m + \sum_{t=1}^T \left( \frac{2(t-1)}{T}D(\pi^*) + \frac{1}{T}S_{t-1} + C\left(\frac{2m\sqrt{t-1}}{T}\right)\right)\left(A_t + D(\pi^*)\right)\\
% %     &= B^2m + (T-1)D(\pi^*)^2 + \sum_{t=1}^T \frac{1}{T}S_{t-1}A_t + \sum_{t=1}^T \left(\frac{1}{T} S_{t-1} + \frac{2(t-1)}{T}A_t\right) D(\pi^*)\\
% %     &\qquad + \sum_{t=1}^T C\left(\frac{2m\sqrt{t-1}}{T}\right)(A_t + D(\pi^*))\\
% %     & \le  B^2m + (T-1)D(\pi^*)^2 + \frac{1}{T} S_T^2 + \sum_{t=1}^T D(\pi^*) \cdot \left(\frac{T+t-1}{T}A_t\right)  + 2\sqrt{m}C\sqrt{T} D(\pi^*) + (2\sqrt{m}C/\sqrt{T}) S_T\\
% %     &\le B^2m + (T-1)D(\pi^*)^2 + \frac{1}{T} S_T^2 +  D(\pi^*) \cdot \left(2S_T\right) + 2\sqrt{m}C\sqrt{T} D(\pi^*) + 2\sqrt{m}C/\sqrt{T} S_T\\
% %     &\le T \cdot (Cm/\sqrt{T} + D(\pi^*) + \frac{1}{T}S_T)^2.
% % \end{align*}
% % The final inequality uses the fact that $C > B$. 

% % Thus we have $$d(\overline{V}^T, W^*) \le D(\pi^*) + \frac{\eta}{T}\sum_{j=1}^T \sum_{i=1}^m (L_i^j(\theta^*) - L_i^j(\theta^j)) + \frac{m}{\sqrt{T}}.$$

% % Now we derive the final regret.
% % \begin{align*}
% %     &D(\tilde{\pi}^T) - D(\pi^*) \\ &= d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\beta \DD_{\mathrm{KL}}(\tilde{\pi}^T\| \pi_{\mathrm{ref}})) - D(\pi^*)\\
% %     &\le d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - D(\pi^*)\\
% %     &= \underbrace{d(W^*, \EE_{\tilde{\pi}^T}[r^*(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - d(W^*, \frac{1}{T}\sum_{t=1}^T\EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}}))}_{\textrm{(A)}} \\
% %     &\qquad + d(W^*, \frac{1}{T}\sum_{t=1}^T \EE_{\pi^t}[\hat{r}^t(x,y)]-\frac{\beta}{T} \sum_{t=1}^T \DD_{\mathrm{KL}}(\pi^t\|\pi_{\mathrm{ref}})) - D(\pi^*)\\
% %     &=\mathrm{(A)}
% %     + d(W^*, \EE_{\tilde{\pi}^T}[\hat{r}^T(x,y)]-\frac{\beta}{T} \sum_{t=1}^T\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})) - D(\pi^*).
% % \end{align*}
% % The finally inequality uses the fact that 
% % $$\DD_{\mathrm{KL}}(\tilde{\pi}\| \pi_{\mathrm{ref}}) \le \frac{1}{T}\left(\sum_{t=1}^T\DD_{\mathrm{KL}}(\pi^t \| \pi_{\mathrm{ref}})\right).$$
% % Then we can get 
% % \begin{align*}D(\tilde{\pi}^T) - D(\pi^*)&\le \mathrm{(A)} + d(W^*, \overline{V}^T) - D(\pi^*)\\
% % &\le \mathrm{(A)} +  \frac{\eta}{T}\sum_{j=1}^T \sum_{i=1}^m (L_i^j(\theta^*) - L_i^j(\theta^j)) + \frac{m}{\sqrt{T}}.\end{align*}
% % Now we consider the error term $(A)$, which represents the approximation error of the reward function. Now by Eq. \ref{ineq:apr error of reward}, we have 
% % \begin{align*}
% %     (A)&\le \EE_{\tilde{\pi}^T} [\hat{r}^t(x,y) - r^*(x,y)]\\
% %     &\le \frac{1}{T}\sum_{i=1}^m \alpha_i^* \cdot \left(\mu_i \exp(4/\beta)\kappa\cdot \sum_{t=1}^T   \sum_{j=1}^{t-1}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\mathrm{base}}}[\left(r_i^t(x,y_1)-r_i^t(x,y_2) - (r_i^*(x,y_1)-r_i^*(x,y_2))\right)^2] + \frac{d_{\mathrm{cover}}(1/T)}{4\mu_i}\right),
% % \end{align*}
% % where $\kappa = \sup_{x,y} \frac{\pi_{\mathrm{base}}(y\mid x)}{\pi_{\mathrm{ref}(y\mid x)}}$ \citep{cen2024value}.

% % Now by the MLE loss, there exists a constant $C$ such that 
% % \begin{align*}
% %     \sum_{t=1}^T \sum_{i=1}^m &\frac{\eta}{T} (L_i^t(\theta_i^*) - L_i^t(\theta_i^t))
% %     \\&\le 2\sum_{i=1}^m \eta \log(|\cR|/\delta) - \frac{C}{T}\sum_{t=1}^T \sum_{i=1}^m\eta\sum_{j \in \cD_i^{t-1}}\EE_{y\sim \pi^j} \left[\Delta_i^t(x,y)^2\right].
% % \end{align*}

% % Choose $\mu_i = \frac{C}{\alpha_i^* \exp(4/\beta)\kappa\sqrt{T}}$, then we can get 

% % \begin{align*}
% %     D(\tilde{\pi}^T) - D(\pi^*) &\le 2 \sum_{i=1}^m \eta\log(|\cR|/\delta) - \eta\cdot \frac{C}{T} \underbrace{\sum_{t=1}^T \sum_{i=1}^m  \sum_{j \in \cD_i^{t-1}}\EE_{y_1\sim \pi^j, y_2\sim \pi_{\mathrm{base}}}[\Delta_i^t(x,y)^2]}_{\text{(A)}} \\
% %     &\qquad + \frac{d\exp(4/\beta) \kappa}{4\sqrt{T}} + \frac{C}{T^{3/2}}\underbrace{\cdot \sum_{t=1}^T \sum_{j=1}^{t-1}\sum_{i=1}^m \EE_{y_1\sim \pi^j, y_2\sim \pi_{\textrm{base}}}[\Delta_i^t(x,y)^2]}_{\text{(B)}}.
% % \end{align*}
% % Note that we have $$\mathrm{(A)} \ge \frac{1}{me^B}\mathrm{(B)},$$
% % then we choose $\eta = \frac{me}{\sqrt{T}}$, 
% % \begin{align*}
% %     D(\tilde{\pi}^T) - D(\pi^*) &\le 2 \sum_{i=1}^m \log(|\cR|/\delta) - \eta\cdot  \frac{C}{T} \mathrm{(A)} + \frac{d\exp(4/\beta) \kappa}{4\sqrt{T}} + \frac{C}{T^{3/2}} \mathrm{(B)}\\
% %     &\le 2m\eta  \log(|\cR|/\delta) + \frac{d\exp(4/\beta) \kappa}{4\sqrt{T}}\\
% %     &\le \frac{2m^2e\log (|\cR|/\delta)}{\sqrt{T}}+ \frac{d\exp(4/\beta) \kappa}{4\sqrt{T}}.
% % \end{align*}



% %\section{Proof of Theorem \ref{thm:malfareoffline}}
% %TODO. 


% % \subsection{Proof of Theorem \ref{thm:welfare}}
% % \begin{proof}
% %     The change mainly focus on the calculation of $\sum_{n=1}^N d^2(\overline{V}^t, W_n^*).$
% %     We have 
% %     \begin{align*}
% %         \sum_{n=1}^N d^2(\overline{V}^t, W_n^*)& = \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^T)\|^2\\
% %         & \le \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^{T-1})\|^2\\
% %         &=\sum_{n=1}^N\left(\frac{T-1}{T}\right)^2 d(\overline{V}^{T-1}, W^*_n)^2 + \frac{1}{T^2} \|V^T - \Pi_{W^*_n}(\overline{V}^{T-1})\|^2 \\
% %     &\qquad + \sum_{n=1}^N \frac{2(T-1)}{T^2}(\overline{V}^{T-1} - \Pi_{W^*_n} (\overline{V}^{T-1}))\cdot (V^T - \Pi_{W^*_n}(\overline{V}^{T-1}))\\
% %     &\le \sum_{n=1}^N\left(\frac{T-1}{T}\right)^2 d(\overline{V}^{T-1}, W^*_n)^2 + \frac{NB^2m}{T^2}\\
% %     &\qquad + \sum_{n=1}^N \frac{2(T-1)}{T^2}(\overline{V}^{T-1} - \Pi_{W^*_n} (\overline{V}^{T-1}))\cdot (V^T - \Pi_{W^*_n}(\overline{V}^{T-1}))
% %     \end{align*}


% % Thus by iteration we can have 
% % \begin{align*}
% %      T^2 \sum_{n=1}^N d(\overline{V}^T, W^*_n)^2 \le T\cdot B^2mN + \sum_{n=1}^N\sum_{t=1}^T 2(t-1) (\overline{V}^{t-1}-\Pi_{W^*_n}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*_n}(\overline{V}^{t-1}))
% % \end{align*}

% % The term \begin{align*}&\sum_{n=1}^N(\overline{V}^{t-1}-\Pi_{W^*_n}(\overline{V}^{t-1}))\cdot (V^t - \Pi_{W^*_n}(\overline{V}^{t-1}))\\
% % &=\sum_{n=1}^N d(\overline{V}^{t-1}, W_n^*)\cdot d_n^t \cdot ( \Pi_{W^*_n}(\overline{V}^{t-1})-V^t)\\
% % &= \sum_{n=1}^N d(\overline{V}^{t-1}, W_n^*)\cdot \left(J(r_1^*, \cdots, r_m^*, d^t_n, \pi^*) + d(S(\pi^*), W_n^*) - J(\hat{r}_1, \cdots, \hat{r}_m, d_n^t ,\pi^t)\right)\\
% % & \le  \sqrt{\sum_{n=1}^N d^2(\overline{V}^{t-1}, W_n^*)}\cdot\left(J(r_1^*, \cdots, r_m^*, d^t, \pi^*) - J(\hat{r}_1, \cdots, \hat{r}_m, d^t, \pi^t)+\sqrt{\sum_{n=1}^N d^2(S(\pi), W_n^*)}\right).\end{align*}
% % The last inequality uses the Cauchy's inequality and the definition of $d^t$.
% % \end{proof}

% % % \subsection{Proof of Theorem \ref{thm:generalp}}

% % % \begin{proof}

% % % \begin{align}
% % %     \sum_{n=1}^N d^p(\overline{V}^t, W_n^*) & = \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^T)\|^p\\
% % %     & \le \sum_{n=1}^N \|\overline{V}^T - \Pi_{W_n^*}(\overline{V}^{T-1})\|^p\\
% % %     & = \sum_{n=1}^N \left\|\frac{T-1}{T}(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1})) + \frac{1}{T}(V^T - \Pi_{W_n^*}(\overline{V}^{T-1}))\right\|^p. \label{ineq:pnorm}
% % % \end{align}
% % % By the basic inequality, for $q>1$, we have 
% % % $$(a+b)^q - a^q - b^q $$ is non-decreasing for both $a,b>0$. 
% % % Then for the vector $x_n,y_n \in \RR^m$ with $x_n=(T-1)(\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1})), y_n= (V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))$ and $p>2,$ we know $q>1$ and $\|x_n\| \le 2TB, \|y_n\| \le 2B$. Hence, 
% % % \begin{align*}\|x_n+y_n\|^p &\le (\|x_n\|^2 + \|y_n\|^2 + 2\langle x_n,y_n\rangle )^{q}\le (\|x_n\|^2 + \|y_n\|^2)^{q} + 2^{q}\langle x_n,y_n\rangle (\|x_n\|^2+\|y_n\|^2)^{q-1}\\&\le \|x_n\|^p  + \|y_n\|^p + (2B)^p \cdot \left((T^2+1)^{q}-T^p - 1\right)+ 2^{q}\langle x_n,y_n\rangle (\|x_n\|^2+\|y_n\|^2)^{q-1}\\
% % % &\le \|x_n\|^p  + \|y_n\|^p + (4B)^{2q} \cdot \left(T^{p-2}\right)+ 2^{q}\langle x_n,y_n\rangle (\|x_n\|^2+\|y_n\|^2)^{q-1}\end{align*}
% % %  we can further bound the inequality \eqref{ineq:pnorm} as 
% % % \begin{align}
% % %     T^p \sum_{n=1}^N d^p(\overline{V}^{T}, W_n^*) &\le  \sum_{n=1}^N \|x_n+y_n\|^p \\&\le (T-1)^p\sum_{n=1}^N d^p(\overline{V}^{T-1}, W_n^*) + n(2B)^p  + n(4B)^{2q} \cdot T^{p-2} \\
% % %     &\qquad  + 2^{q} (T-1)\sum_{n=1}^N (\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))(\|x\|^2 + \|y\|^2)^{q-1}.
% % % \end{align}
% % % Note that $\|y\| \le 2B,$ then $\langle x_n, y_n\rangle (\|x\|^2 + \|y\|^2)^{q-1} \le \max\{2^{q-1}\cdot \|x\|^{p-2}\cdot \langle x_n,y_n\rangle , (4B)^{p}\}\le 2^{q-1}\cdot \|x\|^{p-2}\cdot \langle x_n,y_n\rangle + (4B)^{p},$ then we can finally get 
% % % \begin{align}
% % % T^p \sum_{n=1}^N d^p(\overline{V}^{T}, W_n^*) &\le (T-1)^p \sum_{n=1}^N d^p(\overline{V}^{T-1}, W_n^*) + \\
% % % &\qquad + 2^{p}(T-1)\sum_{n=1}^N (\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))d^{p-2}(\overline{V}^{T-1}, W_n^*)\\
% % % &\qquad \qquad + n(4B)^{2q}+ n(2B)^p + n(4B)^{2q} T^{p-2}
% % % \end{align}
% % % Now for $p>2$, we have 
% % % \begin{align}
% % %     T^p \sum_{n=1}^N d^p(\overline{V}^{T}, W_n^*) &\le(T-1)^p \sum_{n=1}^N d^p(\overline{V}^{T-1}, W_n^*) + \cO(n(4B)^{2q}T^{p-2}) \\&\qquad + 2^{p}(T-1)\sum_{n=1}^N (\overline{V}^{T-1}-\Pi_{W_n^*}(\overline{V}^{T-1}))(V^T-\Pi_{W_n^*}(\overline{V}^{T-1}))d^{p-2}(\overline{V}^{T-1}, W_n^*).
% % % \end{align}
% % % Hence by the recursion, we can get 
% % % \begin{align}
% % %     &\sum_{t=1}^T \sum_{n=1}^N (t-1)(\overline{V}^{t-1}-\Pi_{W_n^*}(\overline{V}^{t-1}))(V^t-\Pi_{W_n^*}(\overline{V}^{t-1}))d^{p-2}(\overline{V}^{t-1}, W_n^*)\\
% % %     &\le \sum_{t=1}^T \sum_{n=1}^N (t-1)d^{p-1}(\overline{V}^{t-1}, W_n^*)d_n^t\cdot (\Pi_{W_n^*}(\overline{V}^{t-1}) - V^t)\\
% % %     &\le  \sum_{t=1}^T\sum_{n=1}^N (t-1)d^{p-1}(\overline{V}^{t-1}, W_n^*)\left(J(r_1^*, \cdots, r_m^*, d_n^t, \pi^*)+d(S(\pi^*), W_n^*) - J(\hat{r}_1, \cdots, \hat
% % %     {r}_m), d_n^t, \pi^t)\right)\\
% % %     &\le \sum_{t=1}^T(t-1)\left(\sum_{n=1}^N d^p(\overline{V}^{t-1}, W_n^*)\right)^{\frac{p-1}{p}} \cdot \left(J(r_1^*, \cdots, r_m^*, d^t, \pi^*)+ \sqrt[p]{\sum_{n=1}^N d^p(S(\pi^*), W_n^*)}- J(\hat{r}_1, \cdots, \hat
% % %     {r}_m, d^t, \pi^t)\right)\\
% % %     &\le \sum_{t=1}^T(t-1)\left(\sum_{n=1}^N d^p(\overline{V}^{t-1}, W_n^*)\right)^{\frac{p-1}{p}} \cdot \left(D_p(\pi^*) + \eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t)\right)
% % % \end{align}

% Let $S_T = \sqrt[p]{\sum_{n=1}^N d^p(\overline{V}^T, W_n^*)}$, then we can get 
% \begin{align*}
%     T S_T^p\le \cO(n(4B)^{2q}) + 2^{2q} \sum_{t=1}^T \frac{(t-1)^{2q-1}}{T^{2q-1}}S_{t-1}^{2q-1}\cdot \left(D_p(\pi^*) + \eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t)\right).
% \end{align*}
% Define $A_t = D_p(\pi^*)+ \eta \sum_{i=1}^m L_i^t(\theta^*) - \eta \sum_{i=1}^m L_i^t(\theta^t),$ then we use the induction to show that there exists a constant $C_q$ such that 
% \begin{align}
%     S_{t} \le C_q\left(\frac{1}{t}\sum_{s=1}^t A_s + T^{-1/2q}\right). 
% \end{align}
% In fact, it holds when $t = 1$. Now suppose it holds for $t=1,2,\cdots, T-1$, we have 
% \begin{align*}
%     T S_T^p&\le \cO(n(4B)^{2q}) + 2^{2q} \sum_{t=1}^T \frac{(t-1)^{2q-1}}{T^{2q-1}}S_{t-1}^{2q-1}\cdot A_t\\
%     &\le \cO(n(4B)^{2q}) + 2^{2q} \sum_{t=1}^T \frac{(t-1)^{2q-1}}{T^{2q-1}}\cdot \left(\frac{C_q}{t-1}\sum_{s=1}^{t-1}A_s + C_qT^{-1/2q}\right)^{p-1} \cdot A_t\\
%     &\le \cO(n(4B)^{2q}) + 2^{4q} (C_q)^{2q-1}\sum_{t=1}^T \frac{(t-1)^{2q-1}}{T^{2q-1}}\cdot \left(\left(\frac{1}{t-1}\sum_{s=1}^{t-1} A_s\right)^{2q-1} A_t+  T^{-\frac{2q-1}{2q}}A_t\right)\\
%     &\le \cO(n(4B)^{2q}) + 2^{4q} (C_q)^{2q-1}T\left(\sum_{t=1}^T A_t/T\right)^{2q} + 2^{4q}(C_q)^{2q-1} \frac{1}{T^{2q-2+\frac{2q-1}{2q}}}\left(\sum_{t=1}^TA_t/T\right).
% \end{align*}
% Hence, by choosing $C_q \ge \max\{n^{1/p}\cdot 4B, 2^{4q}\}$, we have $2^{4q}(C_q)^{2q-1} \le (C_q)^p$. Note that $p\ge 2$ implies that $(x+y)^p\ge x^p + y^p + xy^{p-1}$ 
% \begin{align*}
%     S_T^p &\le \cO((C_q)^p/T) + (C_q)^{p}\left(\sum_{t=1}^TA_t/T\right)^p + (C_q)^p\frac{1}{T^{\frac{p-1}{p}}}\left(\sum_{t=1}^TA_t/T\right)\\
%     &\le (C_q)^p\cdot\left(\frac{1}{T}\sum_{s=1}^T A_s + T^{-1/2q}\right)^p,
% \end{align*}
% which implies that 
% \begin{align}
%     S_T \le C_q\cdot \left(\frac{1}{T}\sum_{s=1}^T A_s + T^{-1/2q}\right).
% \end{align}
% \end{proof}
\section{Error of Estimating the Target Set} 
First we provide a lemma to show that the projection on $W^*$ is also bounded. 
\begin{lemma}\label{lemma: bounded proj}
    Fixed the requirement $p^{(n)}, c^{(n)}$ for all $k \in [K]$. For  any importance weight $\{\alpha^{(n)}\}_{k \in [K]}$ such that $\alpha^{(n)}\succeq 0$ and $\|\alpha^{(n)}\|_1=1$ for all $k \in [K]$, for $B_1 = 2\sqrt{m}(B+\max_n c^{(n)})$, we have 
    $$\|\Pi_{W^*}(x)\|_\infty\le B_1, \ \ \ W^* = \bigcap_{i=1}^K W^{\alpha^{(n)}}_{p^{(n)},c^{(n)}}$$
    holds for all $\|x\|_\infty\le B.$
\end{lemma}

\begin{proof}
    Suppose we choose any $y \in W^*$, then by the definition of projection, we can get $$\|\Pi_{W^*}(x)\|_\infty-\sqrt{m}B\le \|x - \Pi_{W^*}(x)\|_\infty \le \|x - \Pi_{W^*}(x)\|_2 \le \|x - y\| \le \sqrt{m}B + \|y\|,$$
    which induces $$\|\Pi_{W^*}(x)\| \le 2\sqrt{m}B + \|y\|.$$
    Now consider $y = (z, \cdots, z)^\top \in \RR^m$, when $z= \max_n c^{(n)}$, for any $\alpha^{(n)}$
    \begin{align*}
        \left(\sum_{i=1}^m \alpha_i^{(n)} y_i^{|p^{(n)}|}\right)^{1/p^{(n)}} = z\cdot \left(\sum_{i=1}^m \alpha_i^{(n)}\right)^{1/p^{(n)}}= z \ge c^{(n)}.
    \end{align*}
    That means $y \in W^{\alpha^{(n)}}_{p^{(n)}, c^{(n)}}$ and then $y \in W^*$ for any $k \in [K].$ Hence we have 
    \begin{align*}
        \|\Pi_{W^*}(x)\| \le 2B + \|y\| \le 2\sqrt{m}(B+\max_n c^{(n)}).
    \end{align*}
    We complete the proof of lemma.
\end{proof}
Now we consider the estimation of the $W^*$. First, we consider the estimation error of $W^\alpha$ when we have an estimation error of $\alpha.$ The following lemma tells us the estimation error of parameterized target set.

 \begin{lemma}[Estimation error of parameterized target set]\label{lemma:estimation error of parameterized target set}
     Suppose we have two different $\alpha, \alpha'$, the distance between $W_{p,c}^{\alpha}$ and $W_{p,c}^{\alpha'}$ can be bounded by 
     \begin{equation*}
         d_B(W_{p,c}^{\alpha}, W_{p,c}^{\alpha'}) \le \frac{m^{3/2}B\|\alpha-\alpha'\|_\infty}{|p|},
     \end{equation*}
     where $$d_B(S,S') = \max\left\{\max_{x \in S, \|x\|_\infty \le B}d(x,\Pi_{S'}(x)), \max_{x \in S', \|x\|_\infty \le B}d(x,\Pi_{S}(x))\right\}$$ represents the distance of two sets $S$ and $S'$ restricted to some bounded set.
 \end{lemma}

 \begin{proof}
     Suppose $p \in [0,1]$ and $x \in W_{p,c}^{\alpha}$ with $\|x\|_\infty \le B$, then we have 
     $$\sum_{i=1}^m \alpha_i x_i^p \ge c^p.$$
First, if $\sum_{i=1}^m \alpha_i' x_i^p \ge c^p,$ then $x \in W_{p,c}^{\alpha'}$ and the distance $d(x, \Pi_{W_{p,c}^{\alpha'}}(x))  = 0 .$
Now we consider the auxillary vector $y \in \RR^m$ where $y_i = x_i^p$ for $i \in [m].$ Then $\sum_{i=1}^m \alpha_i y_i \ge c^p.$ By the formula of the distance between one point to a line, the distance between $y$ and $W_{p,c}^{\alpha'} = \{y: \sum_{i=1}^m \alpha_i y_i \ge c^p, y_i \succeq 0\}$ can have the following upper bound:
\begin{equation*}
    d(y, \Pi_{W_{p,c}^{\alpha'}}(y)) = \frac{\max\{c^p - \sum_{i=1}^m\alpha_i'y_i, 0\}}{\sqrt{\sum_{i=1}^m (\alpha_i')^2}} \le \frac{\max\{\sum_{i=1}^m (\alpha_i-\alpha_i')y_i,0\}}{\sqrt{\sum_{i=1}^m (\alpha_i')^2}}\le \frac{\|\alpha-\alpha'\|_\infty mB^p}{\sqrt{\sum_{i=1}^m (\alpha_i')^2}}.
\end{equation*}
Now consider $p<0$ we have $\sum_{i=1}^m \alpha_i x_i^p \le c^p.$ If $\sum_{i=1}^m\alpha_i' y_i \le c^p,$ then $x \in W_{p,c}^{\alpha'}$ and the distance $d(x, \Pi_{W_{p,c}^{\alpha'}}(x)) = 0.$ Otherwise, note that we can rewrite $W_{p,c}^{\alpha} = \{y:\sum_{i=1}^m \alpha_i y_i \le c^p, y\succeq 0\}.$ We have 
\begin{equation*}
    d(y, \Pi_{W_{p,c}^{\alpha'}}(y)) = \frac{\sum_{i=1}^m \alpha_i'y_i -c^p}{\sqrt{\sum_{i=1}^m (\alpha_i')^2}}\le \frac{\|\alpha-\alpha'\|_\infty \sum_{i=1}^m y_i}{\sqrt{\sum_{i=1}^m (\alpha_i')^2}}\le \frac{\|\alpha-\alpha'\|_\infty \cdot mB^p}{\sqrt{\sum_{i=1}^m (\alpha_i')^2}}.
\end{equation*}
So in both cases, we can find 
$$d(y, \Pi_{W_{p,c}^{\alpha'}}(y)) \le \frac{\|\alpha- \alpha'\|_\infty \cdot mB^p}{\sqrt{\sum_{i=1}^m (\alpha_i')^2}} \le \frac{\|\alpha- \alpha'\|_\infty \cdot mB^p}{1/\sqrt{m}} = m^{3/2}B^p \cdot \|\alpha-\alpha'\|_\infty.$$
Now since by Langarian mean value theorem we have $|x^p-y^p| \ge |pB^{p-1}||x-y|$, the distance between $x$ can be bounded by 
\begin{align*}
    d(x, \Pi_{W_{p,c}^{\alpha'}}(x)) \le \frac{1}{|pB^{p-1}|}d(y, \Pi_{W_{p,c}^{\alpha'}}(y)) \le \frac{m^{3/2}B^p \cdot \|\alpha-\alpha'\|_\infty}{|p|B^{p-1}}= \frac{m^{3/2}B\|\alpha-\alpha'\|_\infty}{|p|}.
\end{align*}
\end{proof}
The second lemma shows that the distance between the projection of one point on different convex set.
\begin{lemma}[Distance of Projections]\label{lemma:dis of proj}
    Fixed a point $x$ with $\|x\|_\infty \le B$. Suppose we have two convex sets $A_1,A_2$, then the distance of two projections can be bounded by 
    \begin{equation*}
        \|\Pi_{A_1}(x)-\Pi_{A_2}(x)\|_2^2 \le 4d(x, A_1)d_{B_1}(A_1,A_2)+2d_{B_1}(A_1,A_2)^2.
    \end{equation*}
\end{lemma}
 \begin{proof}
 WLOG, we can assume $d(x, A_1)\le d(x, A_2).$
     First, we consider $\Pi_{A_2}(\Pi_{A_1}(x)) \in A_2$ and $d(\Pi_{A_2}(\Pi_{A_1}(x)), \Pi_{A_1}(x)) \le d_{B_1}(A_1,A_2)$, where $B_1$ is from the bounded assumption of the target set.
     Now we only need to consider $d(\Pi_{A_2}(\Pi_{A_1}(x)), \Pi_{A_2}(x))$.
     Since $A_2$ is a convex set and $\Pi_{A_2}(\Pi_{A_1}(x)) \in A_2$, we can have 
     $$\langle x - \Pi_{A_2}(x), \Pi_{A_2}(x) - \Pi_{A_2}(\Pi_{A_1}(x))\ge 0,$$ then it is easy to get
     \begin{equation*}
         d(\Pi_{A_2}(\Pi_{A_1}(x)),x)^2 \ge d(x, A_2)^2 + d(\Pi_{A_2}(\Pi_{A_1}(x)), \Pi_{A_2}(x))^2.
     \end{equation*}
     Also, by the triangle inequality, we can derive 
     \begin{equation*}
         d(\Pi_{A_2}(\Pi_{A_1}(x)),x) \le d(x, A_1) + d(\Pi_{A_1}(x), \Pi_{A_2}(\Pi_{A_1}(x)))\le d(x, A_1) + d_{B_1}(A_1,A_2).
    \end{equation*}
    By combining these two inequality we can get 
    \begin{align*}
        d(\Pi_{A_2}(\Pi_{A_1}(x)), \Pi_{A_2}(x))^2 \le 2d(x, A_1)d_{B_1}(A_1,A_2) + d_{B_1}(A_1,A_2)^2.
    \end{align*}
     Hence we can finally get 
     \begin{align*}
         \|\Pi_{A_1}(x)- \Pi_{A_2}(x)\|_2^2 &\le 2d(\Pi_{A_2}(\Pi_{A_1}(x)), \Pi_{A_2}(x))^2 + 2d(\Pi_{A_2}(\Pi_{A_1}(x)), \Pi_{A_1}(x))^2 \\
         &\le 4d(x, A_1)d_{B_1}(A_1,A_2)+2d_{B_1}(A_1,A_2)^2.
     \end{align*}
 \end{proof}
Now we derive the difference between the direction. 
\begin{lemma}\label{lemma:direc}
    If the angle between the direction $ \frac{\Pi_{A_1}(x)-x}{d(x, A_1)}$ and $\frac{\Pi_{A_2}(x)-x}{d(x, A_2)}$ is less than $\pi/2$, then the  difference between them can be bounded by 
    $$\frac{\Pi_{A_1}(x)-x}{d(x, A_1)} - \frac{\Pi_{A_2}(x)-x}{d(x, A_2)}\le\frac{4\sqrt{d(x, A_1)d_{B_1}(A_1,A_2)}+2d_{B_1}(A_1,A_2)}{\max\{d(x, A_1), d(x, A_2)\}}. $$
\end{lemma}
\begin{proof}
    Denote the angle as $\Delta$ Consider the triangle $(x, \Pi_{A_1}(x), \Pi_{A_2}(x))$. By the law of sines, we can get 
    \begin{align*}
        \sin \Delta \le \frac{d(\Pi_{A_1}(x), d(\Pi_{A_2}(x)))}{\max\{d(x, A_1), d(x, A_2)\}}.
    \end{align*}
    By Lemma \ref{lemma:dis of proj}, we can get 
    \begin{align*}
         \sin \Delta \le \frac{2\sqrt{d(x, A_1)d_{B_1}(A_1,A_2)}+\sqrt{2}d_{B_1}(A_1,A_2)}{\max\{d(x, A_1), d(x, A_2)\}}. 
    \end{align*}
    Now since $\Delta \le \pi/2$ and the direction can be bounded by 
    \begin{align*}
        \frac{\Pi_{A_1}(x)-x}{d(x, A_1)} - \frac{\Pi_{A_2}(x)-x}{d(x, A_2)}&\le \frac{\sin \Delta}{\sin(\frac{\pi-\Delta}{2})}\le \sqrt{2}\sin \Delta\le \frac{4\sqrt{d(x, A_1)d_{B_1}(A_1,A_2)}+2d_{B_1}(A_1,A_2)}{\max\{d(x, A_1), d(x, A_2)\}}.
    \end{align*}
\end{proof}

\section{Auxiliary Lemmas}

\begin{lemma}[MLE Lemma]\label{thm:mle}
    We are given a dataset $D:=\{(x_i,y_i)\},$ where $x_i \sim \cD_i = \cD_i(x_{1:i-1}, y_{1:i-1})$ and $y_i \sim p(\cdot \mid x_i) = f^*(x_i,\cdot)$. Now if we calculate the MLE by 
    \begin{align*}
        \hat{f} = \argmax_{f \in \cF} \sum_{i=1}^n \log f(x_i,y_i),
    \end{align*}
    then fixed $\delta \in (0,1)$, assume $|\cF|<\infty$ and $f^* \in \cF$, then with probability at least $1-\delta$, we have 
    \begin{align*}
        \sum_{i=1}^n \EE_{x \in \cD_i}\left\|\hat{f}(x,\cdot)  - f^*(x,\cdot)\right\|_{\mathrm{TV}}^2 \le 2\log (|\cF|/\delta).
    \end{align*}
\end{lemma}
\begin{lemma}\label{lemma:policy diff}
    For any $\pi, \pi' \in \{\pi^1, \cdots, \pi^t, \pi^*, \pi_{\mathrm{ref}}\}$, we can have $$\sup_{x,y}\frac{\pi(y\mid x)}{\pi'(y\mid x)} \le \exp(4/\beta).$$
\end{lemma}
\begin{proof}
    First, note that $\pi$ and $\pi'$ are both optimal policy with respect to some reward $\hat{r}$, then $\pi$  can be rewritten as 
    \begin{gather*}
        \pi(y\mid x) \propto \pi_{\mathrm{ref}}(y\mid x) \exp(\langle \hat{\alpha}, \hat{r}\rangle/\beta).
    \end{gather*}
    Thus by the Appendix A.2 in \citep{cen2022fast}, then for any $y$ and $x$, we have 
    \begin{align*}
        |\log \pi(y\mid x) - \log \pi_{\mathrm{ref}}(y\mid x)|\le 2B/\beta.
    \end{align*}
    Then $$\sup_{x,y} \frac{\pi(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)} \le \exp(2B/\beta),\ \ \sup_{x,y}\frac{\pi_{\mathrm{ref}}(y\mid x)}{\pi(y\mid x)} \le \exp(2B/\beta).$$
    Now from the two inequalities following
    \begin{gather*}
              \sup_{x,y} \frac{\pi(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)} \le \exp(2B/\beta),\\
              \sup_{x,y} \frac{\pi_{\mathrm{ref}}(y\mid x)}{\pi'(y\mid x)} \le \exp(2B/\beta).
    \end{gather*}
    we can multiply them and get 
    \begin{align*}
        \sup_{x,y}\frac{\pi(y\mid x)}{\pi'(y\mid x)}\le \exp(4B/\beta).
    \end{align*}
\end{proof}

\begin{lemma}[Linear Structure]\label{lemma:linearstructure}
    Suppose that we have reward sequence $\{r^t(x)\}_{t \in [T]}$ with $r^t(x) = \langle \theta^t, \phi(x)\rangle$ with $\|\theta\| \le 1, \|\phi(x) \|\le B$, then for any policy $\{\pi^t\}_{t \in [T]}$ for any $\mu>0$, we can have 
    \begin{align*}
        \sum_{t=1}^T \EE_{x \sim \pi^t}[r^t(x)] \le \mu \cdot \sum_{t=1}^T\sum_{j=1}^{t-1} \EE_{x\sim \pi^j}[(r^t(x)]^2 + \widetilde{\cO}(Bd) +  \frac{d_{\mathrm{cover}}(1/T)}{4\mu}.
    \end{align*}
\end{lemma}
\begin{proof}
First, denote $X^t = \EE_{x\sim pi^t}[\phi(x)]$, then
    \begin{align*}
        \sum_{t=1}^T \EE_{x\sim \pi^t}[r^t(x)]&= \sum_{t=1}^T \EE_{x\sim \pi^t}[\langle \theta^t, \phi(x) \rangle]\\
        &=\sum_{t=1}^T \langle \theta^t, X^t\rangle.
    \end{align*}
   Now define $\Sigma_t = \varepsilon I + \sum_{i=1}^{t-1}X^i (X^i)^\top$, then we can decompose the term above as 
    \begin{align*}
        \sum_{t=1}^T \langle \theta^t, X^t\rangle &= \underbrace{\sum_{t=1}^T \langle \theta^t, X^t\rangle \II\{\|X^t\|_{\Sigma_t^{-1}}\le 1\}}_{\textrm{(A)}} + \underbrace{\sum_{t=1}^T \langle \theta^t, X^t\rangle \II\{\|X^t\|_{\Sigma_t^{-1}}> 1\}}_{\textrm{(B)}}.
    \end{align*}
The term (A) can be bounded as 
\begin{align*}
    \textrm{(A)}&= \sum_{t=1}^T \|\theta^t\|_{\Sigma_t} \|X^t\|_{\Sigma_{t}^{-1}} \II\{\|X^t\|_{\Sigma_t^{-1}}\le  1\}\\
    &\le\sum_{t=1}^T\|\theta^t\|_{\Sigma_t} \min\{1, \|X^t\|_{\Sigma_t^{-1}}^2\}^{1/2}\\
    &\le \sum_{t=1}^T \left[\varepsilon\|\theta^t\|^2 + \sum_{i=1}^{t-1}\langle \theta^t, X^i\rangle ^2\right]^{1/2}\min\{1, \|X^t\|_{\Sigma_t^{-1}}^2\}^{1/2}\\
    &\le \sqrt{\left[\sum_{t=1}^T\left(\varepsilon\|\theta^t\|^2 + \sum_{i=1}^{t-1}\langle \theta^t, X^i\rangle ^2\right)\right]\cdot \left[\sum_{t=1}^T  \min\{1,\|X^t\|_{\Sigma_t^{-1}}^2\}\right]},
\end{align*}
where the last inequality uses the Cauchy's inequality.

 
    Now we recall the elliptical potential lemma in \citep{abbasi2011improved}, we can get 
\begin{align}
    \sum_{t=1}^T \min\{1,\|X^t\|^2_{\Sigma_t^{-1}}\}\le d(\varepsilon) = \widetilde{\cO}(d\log(1/\varepsilon)).\label{ineq:eplemma}
\end{align}
Thus substitute it into the the inequality for (A), we can get 
\begin{align*}
    \textrm{(A)} \le \sqrt{d(\varepsilon)\cdot \left[\sum_{t=1}^T\left(\varepsilon\|\theta^t\|^2 + \sum_{i=1}^{t-1}\langle \theta^t, X^i\rangle ^2\right)\right]}.
\end{align*}
Now by the inequality that $\sqrt{a+b} \le \sqrt{a} + \sqrt{b}$,  we can get 
\begin{align*}
    \textrm{(A)} &\le \sqrt{d(\varepsilon)\cdot \left[\sum_{t=1}^T\left(\varepsilon\|\theta^t\|^2 + \sum_{i=1}^{t-1}\langle \theta^t, X^i\rangle ^2\right)\right]}\\
    &\le \sqrt{d(\varepsilon)\varepsilon T} + \sqrt{d(\varepsilon)\cdot \sum_{t=1}^T \sum_{t=1}^{t-1}\langle \theta^t, X^i\rangle^2}\\
    & \le \sqrt{d(\varepsilon)\varepsilon T} + \frac{d(\varepsilon)}{4\mu} + \mu \cdot \sum_{t=1}^T \sum_{i=1}^{t-1}\langle \theta^t, X^i\rangle^2\\
    & = \sqrt{d(\varepsilon)\varepsilon T} + \frac{d(\varepsilon)}{4\mu} + \mu \cdot \sum_{t=1}^T \sum_{j=1}^{t-1} (\EE_{\pi^i}[r^t(x)])^2.
    \end{align*}

    Now if we choose $\varepsilon=1/T$, then $d(\varepsilon) = \widetilde{\cO}(d)$, and the upper bound of $\textrm{(A)}$ becomes
    \begin{align*}
        \textrm{(A)} \le \sqrt{d_{\mathrm{cover}}(1/T)}+\frac{d_{\mathrm{cover}}(1/T)}{4\mu} + \mu \cdot \sum_{t=1}^T \sum_{j=1}^{t-1} (\EE_{\pi^i}[r^t(x)])^2.
    \end{align*}
    Now we derive the upper bound of (B).
    \begin{align*}
        \textrm{(B)}& = \sum_{t=1}^T\langle \theta^t, X^t\rangle \II\{\|X^t\|_{\Sigma_t^{-1}}>1\}\\
        &\le B\cdot \sum_{t=1}^T\II\{\|X^t\|_{\Sigma_t^{-1}}>1\}\\
        &\le B \sum_{t=1}^T \min\{1,\|X^t\|_{\Sigma_t^{-1}}^2\}\\
        &\le B d_{\mathrm{cover}}(1/T) = \widetilde{\cO}(Bd).
    \end{align*}
    So by adding (A) and (B), we can finally get 
    \begin{align*}
        \sum_{t=1}^T \langle \theta^t, X^t \rangle &\le Bd_{\mathrm{cover}}(1/T) + \sqrt{d_{\mathrm{cover}}(1/T)} +  \frac{d_{\mathrm{cover}}(1/T)}{4\mu} + \mu \cdot \sum_{t=1}^T \sum_{j=1}^{t-1} (\EE_{\pi^i}[r^t(x)])^2\\
        &\le \widetilde{\cO}(Bd) +  \frac{d_{\mathrm{cover}}(1/T)}{4\mu} + \mu \cdot \sum_{t=1}^T \sum_{j=1}^{t-1} (\EE_{\pi^i}[r^t(x)])^2.
    \end{align*}



\end{proof}

\section{Some Derivations in Section \ref{sec:moalg} and Section \ref{sec:pref aggregation}}\label{app:derivation}
\subsection{Derivation of Reward-free Modification}
Now we derive the equation 
\begin{align*}
    J(r_1^{\theta_1}, r_2^{\theta_2}, \cdots, r_m^{\theta_m}, \alpha, \pi^\theta)-\sum_{i=1}^m \eta L_i(\theta_i)=C-\beta \EE_{x\sim \rho, y\sim\pi_{\mathrm{base}}}\left[\log \frac{\pi^\theta(y\mid x)} { \pi_{\mathrm{ref}}(y\mid x)}\right]-\eta \sum_{i=1}^m L_i(\theta_i).
\end{align*}
In fact, since 
\begin{align*}J(r_1^{\theta_1}, r_2^{\theta_2},\cdots, r_m^{\theta_m},\alpha,\pi) &= \EE_{y\sim\pi^\theta(\cdot \mid x)}\left[\sum_{i=1}^m \alpha_i r_i^{\theta_i}(x,y) - \beta\cdot \sum_{i=1}^m \alpha_i\cdot (\log \pi^\theta(y\mid x)-\log \pi_{\mathrm{ref}}(y\mid x))\right]\\
&=\EE_{y\sim \pi^\theta(\cdot \mid x)}\left[\sum_{i=1}^m \alpha_ir(x,y) - \beta\cdot \sum_{i=1}^m \alpha_i\cdot (\log \pi^\theta(y\mid x)-\log \pi_{\mathrm{ref}}(y\mid x))\right]\\
&=\EE_{y\sim \pi^\theta(\cdot \mid x)}\left[\log Z(r,x)\right],\end{align*}
where $Z(r,x) = \sum_{y \in \cY}\pi_{\mathrm{ref}}(y\mid x)\exp(r(x,y)/\beta)$ is a normalization factor independent with $y$ \citep{rafailov2024direct}. Now, since $Z(r,x)$ is independent with $y$, we can get 
\begin{align*}
    J(r_1^{\theta_1}, r_2^{\theta_2},\cdots, r_m^{\theta_m},\alpha,\pi) &=\EE_{y\sim \pi^\theta(\cdot \mid x)}\left[\log Z(r,x)\right]\\
    &=\EE_{y\sim \pi_{\mathrm{base}}(\cdot \mid x)}\left[\log Z(r,x)\right]\\
    &=\EE_{y\sim \pi_{\mathrm{base}}(\cdot \mid x)}\left[r(x,y) - \beta (\log \pi^\theta(y\mid x)-\log \pi_{\mathrm{ref}}(y\mid x))\right]\\
    &=C-\beta \EE_{y\sim \pi_{\mathrm{base}}(\cdot \mid x)}\left[\log \frac{\pi^\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right].
\end{align*}
We complete the derivation. 
\subsection{Update Rule of Gradient Descent}
In this section, we show that the computational cost of Eq.~\eqref{eq:rfupdate} can be easily computed once the expectation of the score function can be derived. 

In fact, 
\begin{align*}
    &\nabla_{\theta_1}\left(-\beta \EE_{x\sim \rho, y\sim\pi_{\mathrm{base}}}[\log \pi^\theta(y\mid x)]\right)-\eta \nabla_{\theta_1}\sum_{i=1}^m \ell(\cD_i, \theta_i)\\
    &=-\beta \underbrace{\EE_{x\sim \rho, y\sim\pi_{\mathrm{base}}}[\nabla_{\theta_1}\log \pi^\theta(y\mid x)]}_{\text{(a)}}-\underbrace{\eta \nabla_{\theta_1}\ell(\cD_1, \theta_1)}_{\text{(b)}}.
\end{align*}
Term (b) in the last line is the gradient of log-likelihood loss that appears in classical reward-free algorithm like DPO. For term (a), note that if $\|d\|_1 = 1$, we have \begin{align*}\pi^\theta  \propto \pi_{\mathrm{ref}}(y \mid x) \cdot \exp\left(\sum_{i=1}^m \beta d_i r_i^{\theta_i}(x,\cdot)\right) =  \prod_{i=1}^m (\pi^{\theta_i}(y\mid x))^{d_i}.\end{align*} Hence, denote $s(\theta,\pi^*) = \EE_{\pi^*}[\nabla_\theta \log\pi^\theta(y\mid x)]$ is the expectation of the score function, we can then derive that 
\begin{align*}
    \text{(a)}= \beta d_1 \left(s(\theta_1, \pi_{\mathrm{base}}) - s(\theta_1, \pi^\theta)\right).
\end{align*}
Hence, the update rule can be efficiently computed as long as the score function is available, which commonly appears in previous RL algorithms such as REINFORCE.

Thus, if the learning rate is $\xi>0$, the gradient descent update rule of $\theta_1$ is 
\begin{align*}\theta_1^t &= \theta_1^{t-1}-\xi\left(\beta d_1(s(\theta_1,\pi_{\mathrm{base}}) - s(\theta_1, \pi^\theta))  - \eta \nabla_{\theta_1^{t-1}}L_1^t(\theta_1^{t-1})\right).\end{align*}
Also, for the reward-free version, we can change the term $L_1^t(\theta_1^{t-1})$ to $$\sum_{(x,y_w,y_l) \in \cD_1} \log \sigma \left(\beta\cdot \left(\log\frac{\pi^{\theta_1}(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}-\log\frac{\pi^{\theta_1}(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}\right)\right).$$

\subsection{Derivation of the reward-free equation of expected reward vector}\label{app:expected reward vector derivation}
We now prove that 
\begin{align*}
    (V^t_i) = \EE_{\pi^t}[r_i^{\theta_i^t}(x,y) - \beta\DD_{\mathrm{KL}}(\pi^t\| \pi_{\mathrm{ref}})] = C-\beta \EE_{y\sim \pi_{\mathrm{base}}}\left[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right] - \beta \EE_{y\sim \pi^t}\left[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi^t(y\mid x)}\right].
\end{align*}
\begin{proof}
We note that 
\begin{align*}
    \EE_{\pi^t}[r_i^{\theta_i^t}(x,y) - \beta\DD_{\mathrm{KL}}(\pi^t\| \pi_{\mathrm{ref}})] &= \EE_{\pi^t}\left[r_i^{\theta_i^t}(x,y) - \beta \left(\log\frac{\pi^t(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right)\right]\\
    &=\EE_{\pi^t}\left[Z(r_i^{\theta_i^t},x)  + \beta\left(\log\frac{\pi^{\theta_i^t}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right) - \beta\left(\log\frac{\pi^t(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right)\right]\\
    &=\EE_{\pi^t}[Z(r_i^{\theta_i^t},x)] +\beta \EE_{\pi^t}\left[\left(\log\frac{\pi^{\theta_i^t}(y\mid x)}{\pi^t(y\mid x)}\right)\right].
\end{align*}
Now note that $Z(r_i^{\theta_i^t},x)$ is independent on $y$, hence 
\begin{align*}
    \EE_{\pi^t}[Z(r_i^{\theta_i^t},x)] &=\EE_{\pi_{\mathrm{base}}}[Z(r_i,x)]\\
    &=\EE_{\pi_{\mathrm{base}}}\left[r_i^{\theta_i^t}(x,y) - \beta (\log \pi^{\theta_i^t}(y\mid x) - \log \pi_{\mathrm{ref}}(y\mid x))\right]\\
    &=C - \beta \EE_{y\sim \pi_{\mathrm{base}}}\left[\log \frac{\pi^{\theta_i^t}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right]. 
\end{align*}
\end{proof}
