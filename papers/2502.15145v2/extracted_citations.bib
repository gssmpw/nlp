@inproceedings{azar2024general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4447--4455},
  year={2024},
  organization={PMLR}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{chakrabortymaxmin,
  title={MaxMin-RLHF: Alignment with Diverse Human Preferences},
  author={Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Manocha, Dinesh and Huang, Furong and Bedi, Amrit and Wang, Mengdi},
  booktitle={Forty-first International Conference on Machine Learning},
    year={2024}
}

@article{chen2024pal,
  title={PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences},
  author={Chen, Daiwei and Chen, Yi and Rege, Aniket and Vinayak, Ramya Korlakai},
  journal={arXiv preprint arXiv:2406.08469},
  year={2024}
}

@article{conitzer2024social,
  title={Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback},
  author={Conitzer, Vincent and Freedman, Rachel and Heitzig, Jobst and Holliday, Wesley H and Jacobs, Bob M and Lambert, Nathan and Moss{\'e}, Milan and Pacuit, Eric and Russell, Stuart and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2404.10271},
  year={2024}
}

@article{ge2024axioms,
  title={Axioms for AI Alignment from Human Feedback},
  author={Ge, Luise and Halpern, Daniel and Micha, Evi and Procaccia, Ariel D and Shapira, Itai and Vorobeychik, Yevgeniy and Wu, Junlin},
  journal={arXiv preprint arXiv:2405.14758},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{park2024rlhf,
  title={RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation},
  author={Park, Chanwoo and Liu, Mingyang and Kong, Dingwen and Zhang, Kaiqing and Ozdaglar, Asuman E},
  booktitle={ICML 2024 Workshop: Aligning Reinforcement Learning Experimentalists and Theorists},
  year={2024}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{rame2024rewarded,
  title={Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards},
  author={Rame, Alexandre and Couairon, Guillaume and Dancette, Corentin and Gaya, Jean-Baptiste and Shukor, Mustafa and Soulier, Laure and Cord, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ramesh2024group,
  title={Group Robust Preference Optimization in Reward-free RLHF},
  author={Ramesh, Shyam Sundhar and Hu, Yifan and Chaimalas, Iason and Mehta, Viraj and Sessa, Pier Giuseppe and Ammar, Haitham Bou and Bogunovic, Ilija},
  journal={arXiv preprint arXiv:2405.20304},
  year={2024}
}

@article{shi2024decoding,
  title={Decoding-time language model alignment with multiple objectives},
  author={Shi, Ruizhe and Chen, Yifang and Hu, Yushi and Liu, ALisa and Smith, Noah and Hajishirzi, Hannaneh and Du, Simon},
  journal={arXiv preprint arXiv:2406.18853},
  year={2024}
}

@inproceedings{sorensenposition,
  title={Position: A Roadmap to Pluralistic Alignment},
  author={Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell L and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{wang2023beyond,
  title={Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints},
  author={Wang, Chaoqi and Jiang, Yibo and Yang, Chenghao and Liu, Han and Chen, Yuxin},
  journal={arXiv preprint arXiv:2309.16240},
  year={2023}
}

@article{wu2023fine,
  title={Fine-grained human feedback gives better rewards for language model training},
  author={Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={59008--59033},
  year={2023}
}

@article{zhong2024provable,
  title={Provable multi-party reinforcement learning with diverse human feedback},
  author={Zhong, Huiying and Deng, Zhun and Su, Weijie J and Wu, Zhiwei Steven and Zhang, Linjun},
  journal={arXiv preprint arXiv:2403.05006},
  year={2024}
}

@article{zhou2023beyond,
  title={Beyond one-preference-for-all: Multi-objective direct preference optimization},
  author={Zhou, Zhanhui and Liu, Jie and Yang, Chao and Shao, Jing and Liu, Yu and Yue, Xiangyu and Ouyang, Wanli and Qiao, Yu},
  journal={arXiv preprint arXiv:2310.03708},
  year={2023}
}

