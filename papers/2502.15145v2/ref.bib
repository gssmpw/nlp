@article{chen2023actions,
  title={Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks},
  author={Chen, Siyu and Wang, Mengdi and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2307.14085},
  year={2023}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@article{cen2024value,
  title={Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF},
  author={Cen, Shicong and Mei, Jincheng and Goshvadi, Katayoon and Dai, Hanjun and Yang, Tong and Yang, Sherry and Schuurmans, Dale and Chi, Yuejie and Dai, Bo},
  journal={arXiv preprint arXiv:2405.19320},
  year={2024}
}

@article{cen2022fast,
  title={Fast global convergence of natural policy gradient methods with entropy regularization},
  author={Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
  journal={Operations Research},
  volume={70},
  number={4},
  pages={2563--2578},
  year={2022},
  publisher={INFORMS}
}


@article{liu2024provably,
  title={Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer},
  author={Liu, Zhihan and Lu, Miao and Zhang, Shenao and Liu, Boyi and Guo, Hongyi and Yang, Yingxiang and Blanchet, Jose and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2405.16436},
  year={2024}
}

@article{rosset2024direct,
  title={Direct nash optimization: Teaching language models to self-improve with general preferences},
  author={Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang},
  journal={arXiv preprint arXiv:2404.03715},
  year={2024}
}



@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}



@inproceedings{xiong2024iterative,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}


@article{jin2021bellman,
  title={Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13406--13418},
  year={2021}
}
@book{zhang2023mathematical,
  title={Mathematical analysis of machine learning algorithms},
  author={Zhang, Tong},
  year={2023},
  publisher={Cambridge University Press}
}
@article{freedman1975tail,
  title={On tail probabilities for martingales},
  author={Freedman, David A},
  journal={the Annals of Probability},
  pages={100--118},
  year={1975},
  publisher={JSTOR}
}

@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{liu2024maximize,
  title={Maximize to explore: One objective function fusing estimation, planning, and exploration},
  author={Liu, Zhihan and Lu, Miao and Xiong, Wei and Zhong, Han and Hu, Hao and Zhang, Shenao and Zheng, Sirui and Yang, Zhuoran and Wang, Zhaoran},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{he2007survey,
  title={A survey of Stackelberg differential game models in supply and marketing channels},
  author={He, Xiuli and Prasad, Ashutosh and Sethi, Suresh P and Gutierrez, Genaro J},
  journal={Journal of Systems Science and Systems Engineering},
  volume={16},
  pages={385--413},
  year={2007},
  publisher={Springer}
}

@article{ghosh20212,
  title={E 2 M 3: energy-efficient massive MIMO--MISO 5G HetNet using Stackelberg game},
  author={Ghosh, Subha and De, Debashis},
  journal={The Journal of Supercomputing},
  volume={77},
  number={11},
  pages={13549--13583},
  year={2021},
  publisher={Springer}
}

@INPROCEEDINGS{Koh2020,
  author={Koh, Joewie J. and Ding, Guohui and Heckman, Christoffer and Chen, Lijun and Roncone, Alessandro},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Cooperative Control of Mobile Robots with Stackelberg Learning}, 
  year={2020},
  volume={},
  number={},
  pages={7985-7992},
  keywords={Transportation;Stochastic processes;Games;Reinforcement learning;Mobile robots;Intelligent robots},
  doi={10.1109/IROS45743.2020.9341376}}

 @book{von2010market,
  title={Market structure and equilibrium},
  author={Von Stackelberg, Heinrich},
  year={2010},
  publisher={Springer Science \& Business Media}
}
@article{keyhani2003leader,
  title={Leader-follower framework for control of energy services},
  author={Keyhani, Ali},
  journal={IEEE Transactions on Power Systems},
  volume={18},
  number={2},
  pages={837--841},
  year={2003},
  publisher={IEEE}
}
@article{9165150,
  author={Qiu, Haifeng and Gu, Wei and Wang, Lu and Pan, Guangsheng and Xu, Yinliang and Wu, Zhi},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Trilayer Stackelberg Game Approach for Robustly Power Management in Community Grids}, 
  year={2021},
  volume={17},
  number={6},
  pages={4073-4083},
  keywords={Robustness;Job shop scheduling;Optimal scheduling;Games;Uncertainty;Iterative methods;Distributed algorithm;optimal power scheduling;robust optimization;stackelberg game (SG);transactive pricing},
  doi={10.1109/TII.2020.3015733}}
@INPROCEEDINGS{6557607,
  author={Sinha, Ankur and Malo, Pekka and Frantsev, Anton and Deb, Kalyanmoy},
  booktitle={2013 IEEE Congress on Evolutionary Computation}, 
  title={Multi-objective Stackelberg game between a regulating authority and a mining company: A case study in environmental economics}, 
  year={2013},
  volume={},
  number={},
  pages={478-485},
  keywords={Government;Companies;Gold;Pollution;Games;Cost function;Stackelberg games;multi-criteria decision making;genetic algorithm;bilevel programming;environmental economics},
  doi={10.1109/CEC.2013.6557607}}
@article{busoniu2008comprehensive,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={38},
  number={2},
  pages={156--172},
  year={2008},
  publisher={IEEE}
}

@InProceedings{pmlr-v54-perolat17a,
  title = 	 {{Learning Nash Equilibrium for General-Sum Markov Games from Batch Data}},
  author = 	 {Perolat, Julien and Strub, Florian and Piot, Bilal and Pietquin, Olivier},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {232--241},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/perolat17a/perolat17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/perolat17a.html},
  abstract = 	 {This paper addresses the problem of learning a Nash equilibrium in $γ$-discounted multiplayer general-sum Markov Games (MGs) in a batch setting. As the number of players increases in MG, the agents may either collaborate or team apart to increase their final rewards. One solution to address this problem is to look for a Nash equilibrium. Although, several techniques were found for the subcase of two-player zero-sum MGs, those techniques fail to find a Nash equilibrium in general-sum Markov Games.  In this paper, we introduce a new definition of $ε$-Nash equilibrium in MGs which grasps the strategy’s quality for multiplayer games. We prove that minimizing the norm of two Bellman-like residuals implies to learn such an $ε$-Nash equilibrium. Then, we show that minimizing an empirical estimate of the $L_p$ norm of these Bellman-like residuals allows learning for general-sum games within the batch setting. Finally, we introduce a neural network architecture that successfully learns a Nash equilibrium in generic multiplayer general-sum turn-based MGs.}
}

@InProceedings{pmlr-v162-sessa22a,
  title = 	 {Efficient Model-based Multi-agent Reinforcement Learning via Optimistic Equilibrium Computation},
  author =       {Sessa, Pier Giuseppe and Kamgarpour, Maryam and Krause, Andreas},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {19580--19597},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/sessa22a/sessa22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/sessa22a.html},
  abstract = 	 {We consider model-based multi-agent reinforcement learning, where the environment transition model is unknown and can only be learned via expensive interactions with the environment. We propose H-MARL (Hallucinated Multi-Agent Reinforcement Learning), a novel sample-efficient algorithm that can efficiently balance exploration, i.e., learning about the environment, and exploitation, i.e., achieve good equilibrium performance in the underlying general-sum Markov game. H-MARL builds high-probability confidence intervals around the unknown transition model and sequentially updates them based on newly observed data. Using these, it constructs an optimistic hallucinated game for the agents for which equilibrium policies are computed at each round. We consider general statistical models (e.g., Gaussian processes, deep ensembles, etc.) and policy classes (e.g., deep neural networks), and theoretically analyze our approach by bounding the agents’ dynamic regret. Moreover, we provide a convergence rate to the equilibria of the underlying Markov game. We demonstrate our approach experimentally on an autonomous driving simulation benchmark. H-MARL learns successful equilibrium policies after a few interactions with the environment and can significantly improve the performance compared to non-optimistic exploration methods.}
}
@inproceedings{cigler2011reaching,
  title={Reaching correlated equilibria through multi-agent learning},
  author={Cigler, Ludek and Faltings, Boi},
  booktitle={10th Conference on Autonomous Agents and Multiagent Systems AAMAS},
  year={2011}
}
@article{zhong2023can,
  title={Can reinforcement learning find Stackelberg-Nash equilibria in general-sum Markov games with myopically rational followers?},
  author={Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={35},
  pages={1--52},
  year={2023}
}
@inproceedings{chen2022adaptive,
  title={Adaptive model design for Markov decision process},
  author={Chen, Siyu and Yang, Donglin and Li, Jiayang and Wang, Senmiao and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={3679--3700},
  year={2022},
  organization={PMLR}
}

@article{bai2021sample,
  title={Sample-efficient learning of stackelberg equilibria in general-sum games},
  author={Bai, Yu and Jin, Chi and Wang, Huan and Xiong, Caiming},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25799--25811},
  year={2021}
}

@inproceedings{kao2022decentralized,
  title={Decentralized cooperative reinforcement learning with hierarchical information structure},
  author={Kao, Hsu and Wei, Chen-Yu and Subramanian, Vijay},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={573--605},
  year={2022},
  organization={PMLR}
}

@inproceedings{zhao2023online,
  title={Online learning in stackelberg games with an omniscient follower},
  author={Zhao, Geng and Zhu, Banghua and Jiao, Jiantao and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={42304--42316},
  year={2023},
  organization={PMLR}
}
@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low bellman rank are pac-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@inproceedings{sun2019model,
  title={Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches},
  author={Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  booktitle={Conference on learning theory},
  pages={2898--2933},
  year={2019},
  organization={PMLR}
}

@inproceedings{xiong2022self,
  title={A self-play posterior sampling algorithm for zero-sum markov games},
  author={Xiong, Wei and Zhong, Han and Shi, Chengshuai and Shen, Cong and Zhang, Tong},
  booktitle={International Conference on Machine Learning},
  pages={24496--24523},
  year={2022},
  organization={PMLR}
}

@article{dann2021provably,
  title={A provably efficient model-free posterior sampling method for episodic reinforcement learning},
  author={Dann, Christoph and Mohri, Mehryar and Zhang, Tong and Zimmert, Julian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12040--12051},
  year={2021}
}

@article{krishnamurthy2016pac,
  title={Pac reinforcement learning with rich observations},
  author={Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{weisz2021exponential,
  title={Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions},
  author={Weisz, Gell{\'e}rt and Amortila, Philip and Szepesv{\'a}ri, Csaba},
  booktitle={Algorithmic Learning Theory},
  pages={1237--1264},
  year={2021},
  organization={PMLR}
}

@article{wang2020reinforcement,
  title={Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension},
  author={Wang, Ruosong and Salakhutdinov, Russ R and Yang, Lin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6123--6135},
  year={2020}
}

@inproceedings{perolat2015approximate,
  title={Approximate dynamic programming for two-player zero-sum Markov games},
  author={Perolat, Julien and Scherrer, Bruno and Piot, Bilal and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={1321--1329},
  year={2015},
  organization={PMLR}
}

@inproceedings{jin2022power,
  title={The power of exploiter: Provable multi-agent rl in large state spaces},
  author={Jin, Chi and Liu, Qinghua and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={10251--10279},
  year={2022},
  organization={PMLR}
}

@article{liu2020provably,
  title={Provably good batch off-policy reinforcement learning without great exploration},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1264--1274},
  year={2020}
}


@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline rl?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}

@article{cousins2021axiomatic,
  title={An axiomatic theory of provably-fair welfare-centric machine learning},
  author={Cousins, Cyrus},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16610--16621},
  year={2021}
}

@article{chen2024pal,
  title={PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences},
  author={Chen, Daiwei and Chen, Yi and Rege, Aniket and Vinayak, Ramya Korlakai},
  journal={arXiv preprint arXiv:2406.08469},
  year={2024}
}

@article{ge2024axioms,
  title={Axioms for AI Alignment from Human Feedback},
  author={Ge, Luise and Halpern, Daniel and Micha, Evi and Procaccia, Ariel D and Shapira, Itai and Vorobeychik, Yevgeniy and Wu, Junlin},
  journal={arXiv preprint arXiv:2405.14758},
  year={2024}
}

@inproceedings{park2024rlhf,
  title={RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation},
  author={Park, Chanwoo and Liu, Mingyang and Kong, Dingwen and Zhang, Kaiqing and Ozdaglar, Asuman E},
  booktitle={ICML 2024 Workshop: Aligning Reinforcement Learning Experimentalists and Theorists},
  year={2024}
}

@article{zhong2024provable,
  title={Provable multi-party reinforcement learning with diverse human feedback},
  author={Zhong, Huiying and Deng, Zhun and Su, Weijie J and Wu, Zhiwei Steven and Zhang, Linjun},
  journal={arXiv preprint arXiv:2403.05006},
  year={2024}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{wang2023beyond,
  title={Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints},
  author={Wang, Chaoqi and Jiang, Yibo and Yang, Chenghao and Liu, Han and Chen, Yuxin},
  journal={arXiv preprint arXiv:2309.16240},
  year={2023}
}
@inproceedings{azar2024general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4447--4455},
  year={2024},
  organization={PMLR}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{guo2024direct,
  title={Direct language model alignment from online ai feedback},
  author={Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and others},
  journal={arXiv preprint arXiv:2402.04792},
  year={2024}
}
@article{zeng2024token,
  title={Token-level Direct Preference Optimization},
  author={Zeng, Yongcheng and Liu, Guoqing and Ma, Weiyu and Yang, Ning and Zhang, Haifeng and Wang, Jun},
  journal={arXiv preprint arXiv:2404.11999},
  year={2024}
}
@article{pardeshi2024learning,
  title={Learning Social Welfare Functions},
  author={Pardeshi, Kanad Shrikar and Shapira, Itai and Procaccia, Ariel D and Singh, Aarti},
  journal={arXiv preprint arXiv:2405.17700},
  year={2024}
}
@article{anshelevich2021distortion,
  title={Distortion in social choice problems: The first 15 years and beyond},
  author={Anshelevich, Elliot and Filos-Ratsikas, Aris and Shah, Nisarg and Voudouris, Alexandros A},
  journal={arXiv preprint arXiv:2103.00911},
  year={2021}
}
@inproceedings{sorensenposition,
  title={Position: A Roadmap to Pluralistic Alignment},
  author={Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell L and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@article{conitzer2024social,
  title={Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback},
  author={Conitzer, Vincent and Freedman, Rachel and Heitzig, Jobst and Holliday, Wesley H and Jacobs, Bob M and Lambert, Nathan and Moss{\'e}, Milan and Pacuit, Eric and Russell, Stuart and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2404.10271},
  year={2024}
}
@article{ramesh2024group,
  title={Group Robust Preference Optimization in Reward-free RLHF},
  author={Ramesh, Shyam Sundhar and Hu, Yifan and Chaimalas, Iason and Mehta, Viraj and Sessa, Pier Giuseppe and Ammar, Haitham Bou and Bogunovic, Ilija},
  journal={arXiv preprint arXiv:2405.20304},
  year={2024}
}
@article{fishburn1973binary,
  title={Binary choice probabilities: on the varieties of stochastic transitivity},
  author={Fishburn, Peter C},
  journal={Journal of Mathematical psychology},
  volume={10},
  number={4},
  pages={327--352},
  year={1973},
  publisher={Elsevier}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{chakrabortymaxmin,
  title={MaxMin-RLHF: Alignment with Diverse Human Preferences},
  author={Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Manocha, Dinesh and Huang, Furong and Bedi, Amrit and Wang, Mengdi},
  booktitle={Forty-first International Conference on Machine Learning},
    year={2024}
}

@article{yang2024rewards,
  title={Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment},
  author={Yang, Rui and Pan, Xiaoman and Luo, Feng and Qiu, Shuang and Zhong, Han and Yu, Dong and Chen, Jianshu},
  journal={arXiv preprint arXiv:2402.10207},
  year={2024}
}
@article{rame2024rewarded,
  title={Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards},
  author={Rame, Alexandre and Couairon, Guillaume and Dancette, Corentin and Gaya, Jean-Baptiste and Shukor, Mustafa and Soulier, Laure and Cord, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{zhou2023beyond,
  title={Beyond one-preference-for-all: Multi-objective direct preference optimization},
  author={Zhou, Zhanhui and Liu, Jie and Yang, Chao and Shao, Jing and Liu, Yu and Yue, Xiangyu and Ouyang, Wanli and Qiao, Yu},
  journal={arXiv preprint arXiv:2310.03708},
  year={2023}
}
@article{wu2023fine,
  title={Fine-grained human feedback gives better rewards for language model training},
  author={Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={59008--59033},
  year={2023}
}

@article{shi2024decoding,
  title={Decoding-time language model alignment with multiple objectives},
  author={Shi, Ruizhe and Chen, Yifang and Hu, Yushi and Liu, ALisa and Smith, Noah and Hajishirzi, Hannaneh and Du, Simon},
  journal={arXiv preprint arXiv:2406.18853},
  year={2024}
}

@inproceedings{zhu2023principled,
  title={Principled reinforcement learning with human feedback from pairwise or k-wise comparisons},
  author={Zhu, Banghua and Jordan, Michael and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={43037--43067},
  year={2023},
  organization={PMLR}
}

@inproceedings{yu2021provably,
  title={Provably efficient algorithms for multi-objective competitive rl},
  author={Yu, Tiancheng and Tian, Yi and Zhang, Jingzhao and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={12167--12176},
  year={2021},
  organization={PMLR}
}

@inproceedings{xie2020learning,
  title={Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium},
  author={Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={Conference on learning theory},
  pages={3674--3682},
  year={2020},
  organization={PMLR}
}

@article{huang2021towards,
  title={Towards general function approximation in zero-sum markov games},
  author={Huang, Baihe and Lee, Jason D and Wang, Zhaoran and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2107.14702},
  year={2021}
}