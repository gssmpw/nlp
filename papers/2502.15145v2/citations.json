[
  {
    "index": 0,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "azar2024general",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wang2023beyond",
        "author": "Wang, Chaoqi and Jiang, Yibo and Yang, Chenghao and Liu, Han and Chen, Yuxin",
        "title": "Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints"
      },
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wu2023fine",
        "author": "Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh",
        "title": "Fine-grained human feedback gives better rewards for language model training"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhou2023beyond",
        "author": "Zhou, Zhanhui and Liu, Jie and Yang, Chao and Shao, Jing and Liu, Yu and Yue, Xiangyu and Ouyang, Wanli and Qiao, Yu",
        "title": "Beyond one-preference-for-all: Multi-objective direct preference optimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "rame2024rewarded",
        "author": "Rame, Alexandre and Couairon, Guillaume and Dancette, Corentin and Gaya, Jean-Baptiste and Shukor, Mustafa and Soulier, Laure and Cord, Matthieu",
        "title": "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "shi2024decoding",
        "author": "Shi, Ruizhe and Chen, Yifang and Hu, Yushi and Liu, ALisa and Smith, Noah and Hajishirzi, Hannaneh and Du, Simon",
        "title": "Decoding-time language model alignment with multiple objectives"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhong2024provable",
        "author": "Zhong, Huiying and Deng, Zhun and Su, Weijie J and Wu, Zhiwei Steven and Zhang, Linjun",
        "title": "Provable multi-party reinforcement learning with diverse human feedback"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chakrabortymaxmin",
        "author": "Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Manocha, Dinesh and Huang, Furong and Bedi, Amrit and Wang, Mengdi",
        "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences"
      },
      {
        "key": "ramesh2024group",
        "author": "Ramesh, Shyam Sundhar and Hu, Yifan and Chaimalas, Iason and Mehta, Viraj and Sessa, Pier Giuseppe and Ammar, Haitham Bou and Bogunovic, Ilija",
        "title": "Group Robust Preference Optimization in Reward-free RLHF"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "park2024rlhf",
        "author": "Park, Chanwoo and Liu, Mingyang and Kong, Dingwen and Zhang, Kaiqing and Ozdaglar, Asuman E",
        "title": "RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation"
      },
      {
        "key": "sorensenposition",
        "author": "Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell L and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others",
        "title": "Position: A Roadmap to Pluralistic Alignment"
      },
      {
        "key": "conitzer2024social",
        "author": "Conitzer, Vincent and Freedman, Rachel and Heitzig, Jobst and Holliday, Wesley H and Jacobs, Bob M and Lambert, Nathan and Moss{\\'e}, Milan and Pacuit, Eric and Russell, Stuart and Schoelkopf, Hailey and others",
        "title": "Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ge2024axioms",
        "author": "Ge, Luise and Halpern, Daniel and Micha, Evi and Procaccia, Ariel D and Shapira, Itai and Vorobeychik, Yevgeniy and Wu, Junlin",
        "title": "Axioms for AI Alignment from Human Feedback"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chen2024pal",
        "author": "Chen, Daiwei and Chen, Yi and Rege, Aniket and Vinayak, Ramya Korlakai",
        "title": "PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wu2023fine",
        "author": "Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh",
        "title": "Fine-grained human feedback gives better rewards for language model training"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "rame2024rewarded",
        "author": "Rame, Alexandre and Couairon, Guillaume and Dancette, Corentin and Gaya, Jean-Baptiste and Shukor, Mustafa and Soulier, Laure and Cord, Matthieu",
        "title": "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "shi2024decoding",
        "author": "Shi, Ruizhe and Chen, Yifang and Hu, Yushi and Liu, ALisa and Smith, Noah and Hajishirzi, Hannaneh and Du, Simon",
        "title": "Decoding-time language model alignment with multiple objectives"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wang2019optimism",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhong2024provable",
        "author": "Zhong, Huiying and Deng, Zhun and Su, Weijie J and Wu, Zhiwei Steven and Zhang, Linjun",
        "title": "Provable multi-party reinforcement learning with diverse human feedback"
      }
    ]
  }
]