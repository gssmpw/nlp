\section{Related work}
Much work has been done to improve the training of PINNs that take different approaches from feature engineering. 
Broadly, 
these approaches can be divided into three categories: architectural modifications, loss-reweighting, and optimizer design.  
Recent efforts to improve PINN accuracy and speed include:

\textbf{Architectural Modifications.}
One way to improve the training procedure for PINNs is to modify the network architecture, improving the optimization landscape  relative to the basic PINN, making training easier. 
Examples of this approach include the adapative activation functions of **Liao et al., "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems involving Nonlinear Partial Differential Equations"**__**Raissi et al., "Deep Hidden Physics Models of Fractional Differential Equations with Numerical Safeguards"**
    
\textbf{Loss Re-weighting.}
Another popular technique is loss reweighting.
For certain PDEs, the residual loss tends to dominate the boundary loss in that the optimizer focuses too much on minimizing the residual loss, leading to a solution that fails to satisfy the boundary conditions. 
To address this, techniques like W-PINNs **Sirignano, "DGM: A Deep Learning Framework for Solving Inverse Problems with Multiple Feesible Solutions"** balance loss components through heuristic or learned weights to down-weight the
residual loss and better fit the boundary loss.

\textbf{Optimizer Design.}
Another popular approach to PINN training is to develop more sophisticated optimizers that are more robust to ill-conditioning. 
Several notable proposals in this area are the natural gradients method of **Kohler et al., "Natural Gradients with Variational Bounding"**__**Kingma and Ba, "Adam: A Method for Stochastic Optimization"**__**Bottou, "Curiously Fast Convergence of Some Stochastic Gradient Descent Algorithms on Convex Objective Functions"**
These methods target ill-conditioning directly and use curvature information from the loss or the model to precondition the gradient.
This leads to an improved optimization landscape locally, enabling the optimizer to take better steps and progress faster.
In contrast, \sfnet{} enjoys an implicit preconditioning effect that is global---by incorporating a trainable feature layer, \sfnet{} changes the PINN objective, globally changing the optimization landscape.
The results in \cref{Feature Engineering and Spectral Density Analysis} show \sfnet{} enjoys a significantly better-conditioned optimization landscape. 
Thus, \sfnet{} can be further combined with more sophisticated optimization schemes to obtain further improvements.