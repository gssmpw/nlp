@article{berglund2023reversal,
  title={The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A"},
  author={Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  journal={arXiv preprint arXiv:2309.12288},
  year={2023},
  url={https://arxiv.org/pdf/2309.12288.pdf}
}

@article{biderman2023emergent,
  title={Emergent and predictable memorization in large language models},
  author={Biderman, Stella and Prashanth, USVSN Sai and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raf, Edward},
  journal={arXiv preprint arXiv:2304.11158},
  year={2023}
}

@inproceedings{chen2021evaluating,
	title        = {Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models},
	author       = {Priyan Vaithilingam and Tianyi Zhang and Elena L. Glassman},
	year         = 2022,
	booktitle    = {{CHI} '22: {CHI} Conference on Human Factors in Computing Systems - 5 May 2022, Extended Abstracts},
	publisher    = {{ACM}},
	pages        = {332:1--332:7},
	doi          = {10.1145/3491101.3519665},
	url          = {https://doi.org/10.1145/3491101.3519665},
}

@misc{chen2024evaluating,
      title={Evaluating Large Language Models with Runtime Behavior of Program Execution}, 
      author={Junkai Chen and Zhiyuan Pan and Xing Hu and Zhenhao Li and Ge Li and Xin Xia},
      year={2024},
      eprint={2403.16437},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{chen2024language,
      title={Can Language Models Pretend Solvers? Logic Code Simulation with LLMs}, 
      author={Minyu Chen and Guoqiang Li and Ling-I Wu and Ruibang Liu and Yuxin Su and Xi Chang and Jianxin Xue},
      year={2024},
      eprint={2403.16097},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{eldan2023s,
  title={Who’s Harry Potter? Approximate Unlearning in LLMs},
  author={Eldan, Ronen and Russinovich, Mark},
  journal={arXiv preprint arXiv:2310.02238},
  year={2023}, 
  url={https://www.thetalkingmachines.com/sites/default/files/2023-10/2310.02238.pdf}
}

@article{frieder2023mathematical,
	title        = {Mathematical Capabilities of {ChatGPT}},
	author       = {Frieder, Simon and Pinchetti, Luca and Chevalier, Alexis and Griffiths, Ryan-Rhys and Salvatori, Tommaso and Lukasiewicz, Thomas and Petersen, Philipp Christian and Berner, Julius},
	year         = 2023,
	journal      = {ArXiv preprint},
	volume       = {abs/2301.13867},
	url          = {https://arxiv.org/abs/2301.13867}
}

@article{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2301.13196},
  year={2023}
}

@misc{gu2024cruxeval,
      title={CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution}, 
      author={Alex Gu and Baptiste Rozière and Hugh Leather and Armando Solar-Lezama and Gabriel Synnaeve and Sida I. Wang},
      year={2024},
      eprint={2401.03065},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{hou2023large,
  title={Large language models for software engineering: A systematic literature review},
  author={Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  journal={arXiv preprint arXiv:2308.10620},
  year={2023}
}

@article{la2023arrt,
  title={The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges},
  author={La Malfa, Emanuele and Petrov, Aleksandar and Frieder, Simon and Weinhuber, Christoph and Burnell, Ryan and Cohn, Anthony G and Shadbolt, Nigel and Wooldridge, Michael},
  journal={arXiv preprint arXiv:2309.16573},
  year={2023},
  url={https://arxiv.org/pdf/2309.16573.pdf}
}

@article{la2024code,
  title={Code simulation challenges for large language models},
  author={La Malfa, Emanuele and Weinhuber, Christoph and Torre, Orazio and Lin, Fangru and Cohn, Anthony and Shadbolt, Nigel and Wooldridge, Michael},
  journal={arXiv preprint arXiv:2401.09074},
  year={2024}
}

@inproceedings{liang2023code,
  title={Code as policies: Language model programs for embodied control},
  author={Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={9493--9500},
  year={2023},
  organization={IEEE}
}

@article{liu2023code,
  title={Code Execution with Pre-trained Language Models},
  author={Liu, Chenxiao and Lu, Shuai and Chen, Weizhu and Jiang, Daxin and Svyatkovskiy, Alexey and Fu, Shengyu and Sundaresan, Neel and Duan, Nan},
  journal={arXiv preprint arXiv:2305.05383},
  year={2023}
}

@misc{liu2024codemind,
      title={CodeMind: A Framework to Challenge Large Language Models for Code Reasoning}, 
      author={Changshu Liu and Shizhuo Dylan Zhang and Ali Reza Ibrahimzada and Reyhaneh Jabbarvand},
      year={2024},
      eprint={2402.09664},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{lyu2024largelanguagemodelscode,
      title={Large Language Models as Code Executors: An Exploratory Study}, 
      author={Chenyang Lyu and Lecheng Yan and Rui Xing and Wenxi Li and Younes Samih and Tianbo Ji and Longyue Wang},
      year={2024},
      eprint={2410.06667},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.06667}, 
}

@article{mccoy2023embers,
  title={Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve},
  author={McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2309.13638},
  year={2023},
  url={https://arxiv.org/pdf/2309.13638.pdf}
}

@article{mccoy2023much,
	title        = {How much do language models copy from their training data? {E}valuating linguistic novelty in text generation using {RAVEN}},
	author       = {McCoy, R Thomas and Smolensky, Paul and Linzen, Tal and Gao, Jianfeng and Celikyilmaz, Asli},
	year         = 2023,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	volume       = 11,
	pages        = {652--670},
        url = {https://aclanthology.org/2023.tacl-1.38}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{perez2021attention,
  title={Attention is turing complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={3463--3497},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{santos2023always,
  title={Always Provide Context: The Effects of Code Context on Programming Error Message Enhancement},
  author={Santos, Eddie Antonio and Prasad, Prajish and Becker, Brett A},
  booktitle={Proceedings of the ACM Conference on Global Computing Education Vol 1},
  pages={147--153},
  year={2023}
}

@article{schuurmans2023memory,
  title={Memory augmented large language models are computationally universal},
  author={Schuurmans, Dale},
  journal={arXiv preprint arXiv:2301.04589},
  year={2023}
}

@article{tufano2023predicting,
  title={Predicting Code Coverage without Execution},
  author={Tufano, Michele and Chandel, Shubham and Agarwal, Anisha and Sundaresan, Neel and Clement, Colin},
  journal={arXiv preprint arXiv:2307.13383},
  year={2023}
}

@article{wei2022statistically,
  title={Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12071--12083},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/file/4ebf1d74f53ece08512a23309d58df89-Paper-Conference.pdf}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}, 
  url={https://proceedings.mlr.press/v139/weiss21a/weiss21a.pdf}
}

@article{west2023generative,
  title={The Generative AI Paradox:" What It Can Create, It May Not Understand"},
  author={West, Peter and Lu, Ximing and Dziri, Nouha and Brahman, Faeze and Li, Linjie and Hwang, Jena D and Jiang, Liwei and Fisher, Jillian and Ravichander, Abhilasha and Chandu, Khyathi and others},
  journal={arXiv preprint arXiv:2311.00059},
  year={2023}
}

@article{widjojo2023addressing,
  title={Addressing Compiler Errors: Stack Overflow or Large Language Models?},
  author={Widjojo, Patricia and Treude, Christoph},
  journal={arXiv preprint arXiv:2307.10793},
  year={2023}
}

@article{yang2023code,
  title={What do code models memorize? an empirical study on large language models of code},
  author={Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, DongGyun and Lo, David},
  journal={arXiv preprint arXiv:2308.09932},
  year={2023}
}

@article{yuan2023well,
  title={How well do Large Language Models perform in Arithmetic tasks?},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang},
  journal={arXiv preprint arXiv:2304.02015},
  year={2023},
  url={https://arxiv.org/pdf/2304.02015.pdf}
}

@inproceedings{zan2023large,
  title={Large language models meet NL2Code: A survey},
  author={Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Yongji, Wang and Lou, Jian-Guang},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7443--7464},
  year={2023}
}

@article{zhang2023can,
  title={Can Transformers Learn to Solve Problems Recursively?},
  author={Zhang, Shizhuo Dylan and Tigges, Curt and Biderman, Stella and Raginsky, Maxim and Ringer, Talia},
  journal={arXiv preprint arXiv:2305.14699},
  year={2023}, 
  url={https://arxiv.org/pdf/2305.14699.pdf}
}

@misc{zhang2024transformerbased,
      title={Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion}, 
      author={Dylan Zhang and Curt Tigges and Zory Zhang and Stella Biderman and Maxim Raginsky and Talia Ringer},
      year={2024},
      eprint={2401.12947},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

