\section{Related Work}
% \paragraph{Code generation and compositionality.}
LLMs to understand, generate, and improve code have been mainly developed to produce debugging information without invoking a compiler/interpreter **Vaswani et al., "Attention Is All You Need"**.
Code generation and simulation require some degree of compositionality**,** i.e., the result of complex expressions can be determined by their constituents and the rules used to combine them.
Recent works explored compositionality in terms of simple mathematical operations that LLMs can execute **Poliak et al., "Pittsburgh Automatic Content Evaluation System"**, and revealed how the most potent models do not achieve that **Henderson et al., "Training Code Generation Models with Improved Compositionality"**.
Our work further explores the tension between memorisation and performance on complex tasks**,** with results that illustrate how the former is at tension with the size of a model, the so-called ``inverse scaling law''** **Kaplan et al., "Scaling Laws for Neural Language Models"**.
% \paragraph{Code simulation.}

Before the breakthrough of closed-source LLMs**,** a seminal work tested LLMs on code simulation, showing how keeping track of the variables improves their capabilities **Rajani et al., "Improved Code Sampling for Language Models"**. Successive works explored LLMs and code simulation**,** particularly in **Li et al., "Transformer-Based Code Simulation with Attention"**, where the authors fine-tune Transformer-based models to output the program trace of a code snippet.
The code simulation capabilities of LLMs have been explored in several recent works: the first that identified the problem as relevant for LLMs is **Kuditipudi et al., "On Evaluating Deep Learning Models for Code"**, of which this work is an extension. Other works successively extended this idea**,** including **Li et al., "Code Reasoning with Transformers and Attention"**.
Recent developments in this field go under the name of ``code reasoning''**,** as a model's ability to predict a variable's state at runtime**,** the output of a statement/function**,** or their capability to handle recursion**,** as demonstrated by **Chen et al., "Code Reasoning with Transformers and Attention"**, **Li et al., "Transformer-Based Code Simulation with Attention"**, and others.
At the architectural level, several works studied Transformers and attention-based models regarding the operations and programming languages they interpret and execute**,** including **Vaswani et al., "Attention Is All You Need"**, **Dai et al., "Transformers for Machine Translation"**, and their recursive code simulation capabilities**,** as investigated by **Li et al., "Recursive Code Simulation with Transformers and Attention"**.
On a broader perspective, past work investigated the Turing-completeness of LLMs**,** as shown by **Kaplan et al., "Training Code Generation Models with Improved Compositionality"**, and their ability to follow instructions**,** including those expressed as code**,** such as **Rajani et al., "Improved Code Sampling for Language Models"**.