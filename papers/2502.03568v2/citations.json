[
  {
    "index": 0,
    "papers": [
      {
        "key": "hou2023large",
        "author": "Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu",
        "title": "Large language models for software engineering: A systematic literature review"
      },
      {
        "key": "santos2023always",
        "author": "Santos, Eddie Antonio and Prasad, Prajish and Becker, Brett A",
        "title": "Always Provide Context: The Effects of Code Context on Programming Error Message Enhancement"
      },
      {
        "key": "chen2021evaluating",
        "author": "Priyan Vaithilingam and Tianyi Zhang and Elena L. Glassman",
        "title": "Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"
      },
      {
        "key": "widjojo2023addressing",
        "author": "Widjojo, Patricia and Treude, Christoph",
        "title": "Addressing Compiler Errors: Stack Overflow or Large Language Models?"
      },
      {
        "key": "zan2023large",
        "author": "Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Yongji, Wang and Lou, Jian-Guang",
        "title": "Large language models meet NL2Code: A survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "mccoy2023much",
        "author": "McCoy, R Thomas and Smolensky, Paul and Linzen, Tal and Gao, Jianfeng and Celikyilmaz, Asli",
        "title": "How much do language models copy from their training data? {E}valuating linguistic novelty in text generation using {RAVEN}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "frieder2023mathematical",
        "author": "Frieder, Simon and Pinchetti, Luca and Chevalier, Alexis and Griffiths, Ryan-Rhys and Salvatori, Tommaso and Lukasiewicz, Thomas and Petersen, Philipp Christian and Berner, Julius",
        "title": "Mathematical Capabilities of {ChatGPT}"
      },
      {
        "key": "yang2023code",
        "author": "Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, DongGyun and Lo, David",
        "title": "What do code models memorize? an empirical study on large language models of code"
      },
      {
        "key": "yuan2023well",
        "author": "Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang",
        "title": "How well do Large Language Models perform in Arithmetic tasks?"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "mccoy2023embers",
        "author": "McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L",
        "title": "Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve"
      },
      {
        "key": "west2023generative",
        "author": "West, Peter and Lu, Ximing and Dziri, Nouha and Brahman, Faeze and Li, Linjie and Hwang, Jena D and Jiang, Liwei and Fisher, Jillian and Ravichander, Abhilasha and Chandu, Khyathi and others",
        "title": "The Generative AI Paradox:\" What It Can Create, It May Not Understand\""
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "berglund2023reversal",
        "author": "Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain",
        "title": "The Reversal Curse: LLMs trained on\" A is B\" fail to learn\" B is A\""
      },
      {
        "key": "eldan2023s",
        "author": "Eldan, Ronen and Russinovich, Mark",
        "title": "Who\u2019s Harry Potter? Approximate Unlearning in LLMs"
      },
      {
        "key": "yang2023code",
        "author": "Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, DongGyun and Lo, David",
        "title": "What do code models memorize? an empirical study on large language models of code"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "biderman2023emergent",
        "author": "Biderman, Stella and Prashanth, USVSN Sai and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raf, Edward",
        "title": "Emergent and predictable memorization in large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "la2023arrt",
        "author": "La Malfa, Emanuele and Petrov, Aleksandar and Frieder, Simon and Weinhuber, Christoph and Burnell, Ryan and Cohn, Anthony G and Shadbolt, Nigel and Wooldridge, Michael",
        "title": "The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "nye2021show",
        "author": "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others",
        "title": "Show your work: Scratchpads for intermediate computation with language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2024language",
        "author": "Minyu Chen and Guoqiang Li and Ling-I Wu and Ruibang Liu and Yuxin Su and Xi Chang and Jianxin Xue",
        "title": "Can Language Models Pretend Solvers? Logic Code Simulation with LLMs"
      },
      {
        "key": "tufano2023predicting",
        "author": "Tufano, Michele and Chandel, Shubham and Agarwal, Anisha and Sundaresan, Neel and Clement, Colin",
        "title": "Predicting Code Coverage without Execution"
      },
      {
        "key": "zhou2023algorithms",
        "author": "Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum",
        "title": "What algorithms can transformers learn? a study in length generalization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2023code",
        "author": "Liu, Chenxiao and Lu, Shuai and Chen, Weizhu and Jiang, Daxin and Svyatkovskiy, Alexey and Fu, Shengyu and Sundaresan, Neel and Duan, Nan",
        "title": "Code Execution with Pre-trained Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "la2024code",
        "author": "La Malfa, Emanuele and Weinhuber, Christoph and Torre, Orazio and Lin, Fangru and Cohn, Anthony and Shadbolt, Nigel and Wooldridge, Michael",
        "title": "Code simulation challenges for large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lyu2024largelanguagemodelscode",
        "author": "Chenyang Lyu and Lecheng Yan and Rui Xing and Wenxi Li and Younes Samih and Tianbo Ji and Longyue Wang",
        "title": "Large Language Models as Code Executors: An Exploratory Study"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chen2024evaluating",
        "author": "Junkai Chen and Zhiyuan Pan and Xing Hu and Zhenhao Li and Ge Li and Xin Xia",
        "title": "Evaluating Large Language Models with Runtime Behavior of Program Execution"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gu2024cruxeval",
        "author": "Alex Gu and Baptiste Rozi\u00e8re and Hugh Leather and Armando Solar-Lezama and Gabriel Synnaeve and Sida I. Wang",
        "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution"
      },
      {
        "key": "liu2024codemind",
        "author": "Changshu Liu and Shizhuo Dylan Zhang and Ali Reza Ibrahimzada and Reyhaneh Jabbarvand",
        "title": "CodeMind: A Framework to Challenge Large Language Models for Code Reasoning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhang2024transformerbased",
        "author": "Dylan Zhang and Curt Tigges and Zory Zhang and Stella Biderman and Maxim Raginsky and Talia Ringer",
        "title": "Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "weiss2021thinking",
        "author": "Weiss, Gail and Goldberg, Yoav and Yahav, Eran",
        "title": "Thinking like transformers"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zhang2023can",
        "author": "Zhang, Shizhuo Dylan and Tigges, Curt and Biderman, Stella and Raginsky, Maxim and Ringer, Talia",
        "title": "Can Transformers Learn to Solve Problems Recursively?"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "giannou2023looped",
        "author": "Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris",
        "title": "Looped transformers as programmable computers"
      },
      {
        "key": "perez2021attention",
        "author": "P{\\'e}rez, Jorge and Barcel{\\'o}, Pablo and Marinkovic, Javier",
        "title": "Attention is turing complete"
      },
      {
        "key": "schuurmans2023memory",
        "author": "Schuurmans, Dale",
        "title": "Memory augmented large language models are computationally universal"
      },
      {
        "key": "wei2022statistically",
        "author": "Wei, Colin and Chen, Yining and Ma, Tengyu",
        "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "liang2023code",
        "author": "Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy",
        "title": "Code as policies: Language model programs for embodied control"
      }
    ]
  }
]