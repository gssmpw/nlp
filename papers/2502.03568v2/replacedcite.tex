\section{Related Work}
% \paragraph{Code generation and compositionality.}
LLMs to understand, generate, and improve code have been mainly developed to produce debugging information without invoking a compiler/interpreter____.
Code generation and simulation require some degree of compositionality____, i.e., the result of complex expressions can be determined by their constituents and the rules used to combine them.
Recent works explored compositionality in terms of simple mathematical operations that LLMs can execute____, and revealed how the most potent models do not achieve that____.
Our work further explores the tension between memorisation and performance on complex tasks____, with results that illustrate how the former is at tension with the size of a model, the so-called ``inverse scaling law''____.
% \paragraph{Code simulation.}

Before the breakthrough of closed-source LLMs____, a seminal work tested LLMs on code simulation, showing how keeping track of the variables improves their capabilities____. Successive works explored LLMs and code simulation____, particularly in____, where the authors fine-tune Transformer-based models to output the program trace of a code snippet. 
The code simulation capabilities of LLMs have been explored in several recent works: the first that identified the problem as relevant for LLMs is____, of which this work is an extension. Other works successively extended this idea____.
Recent developments in this field go under the name of ``code reasoning'', as a model's ability to predict a variable's state at runtime____, the output of a statement/function____, or their capability to handle recursion____.
At the architectural level, several works studied Transformers and attention-based models regarding the operations and programming languages they interpret and execute____ and their recursive code simulation capabilities____.
On a broader perspective, past work investigated the Turing-completeness of LLMs____, and their ability to follow instructions____ and policies expressed as code____.