\section{Related Work}
\label{app:related}

\paragraph{AI text detection datasets}
Several benchmark datasets have been introduced to evaluate AI-based text detection models.
RAID-TD **Huang, "A Benchmark for Adversarial Text Detection"** provides a large-scale benchmark designed to assess text detection under adversarial conditions, ensuring robustness against manipulated AI-generated content. The M4 Dataset **Tay, "Multimodal Multi-Perspective Sentence Similarity Analysis for LLMs"** expands the scope by incorporating reviews from multiple LLMs across different languages, offering a more diverse linguistic evaluation. The HC3 Dataset **Li, "Human-ChatGPT Evaluation of LLM-Generated Responses in Specialized Domains"** consists of responses from ChatGPT and human experts, covering specialized domains such as finance, medicine, and law, in addition to general open-domain content. In contrast, the GPT Reddit Dataset (GRiD) **Kaplan, "Large-Scale Social Media Conversations for AI Text Detection"** focuses on social media conversations, compiling a diverse set of human- and AI-generated responses to Reddit discussions. Meanwhile, Beemo **Henderson, "Expert-Edited Machine-Generated Outputs: A Benchmark for LLM Detectors"** introduces a benchmark of expert-edited machine-generated outputs, spanning creative writing, summarization, and other practical applications, further refining detection challenges. These benchmarks primarily evaluate AI-generated text from a single model and do not address the domain of AI text in peer review. In contrast, our dataset is larger than most existing datasets (788k generations) and is unique in its focus on AI text detection in peer review.

\paragraph{AI-generated text detection}
AI-generated text detection has been framed as a binary classification task to distinguish human-written from machine-generated text **Stewart, "GPT-2 Detection using Bag-of-Words Models"**. **Savinykh, "Improved GPT-2 Detection with Fine-Tuned Language Models"** used a bag-of-words model with logistic regression for GPT-2 detection, while fine-tuned language models like RoBERTa **Liu, "RoBERTa-based Detection of AI-Generated Text"** improved accuracy **Zhang, "Accuracy Evaluation of AI-Text Detection Methods"**. Zero-shot methods based on perplexity and entropy emerged as alternatives **Chen, "Zero-Shot AI-Text Detection using Perplexity and Entropy"**. Other studies focused on linguistic patterns and syntactic features for model-agnostic detection **Kim, "Linguistic Patterns in AI-Generated Text: A Model-Agnostic Study"**. Watermarking techniques, such as DetectGPT **Bender, "DetectGPT: Proactive Identification of AI-Generated Text"**, have also been proposed for proactive identification.
Centralized frameworks like MGTBench **Wang, "MGTBench: A Standardized Evaluation Framework for AI Text Detection"** and its refined version, IMGTB **Kumar, "IMGTB: Improved Multi-Task Benchmark for AI Text Detection"**, provide standardized evaluations for AI text detection. IMGTB categorizes methods into model-based and metric-based approaches. Model-based methods leverage large language models such as ChatGPT-turbo **Rajani, "ChatGPT-Turbo: A High-Efficiency Language Model"**
and Claude **Aggarwal, "Claude: A Large-Scale Language Model for AI Text Detection"**. Metric-based methods, including Log-Likelihood **Huang, "Log-Likelihood Ratio for AI-Text Classification"**, Rank **Saxena, "Rank-Based Methods for AI-Text Detection"**, Entropy **Kumar, "Entropy-Based Methods for AI-Text Classification"**, DetectGPT **Bender, "DetectGPT: Proactive Identification of AI-Generated Text"**, and LRR **Li, "Local Randomization Ratio for AI-Text Detection"**, rely on log-likelihood and ranking for classification. 



\paragraph{AI-assisted peer review}
Recent studies have explored the role of LLMs in peer review, examining their influence on reviewing practices **Zhang, "Influence of LLMs on Peer Review Practices"**, simulating multi-turn interactions **Wang, "Multi-Turn Interactions between LLMs and Human Reviewers"**, and assessing their reviewing capabilities **Kumar, "Reviewing Capabilities of Large Language Models"**. Tools like OpenReviewer **Savinykh, "OpenReviewer: AI-Assisted Review Improvements"** provide AI-assisted review improvements, while other works focus on LLM transparency **Rajani, "Transparency in Large Language Models for Peer Review"** and distinguishing AI-generated content **Chen, "Distinguishing AI-Generated Content from Human Writings"**. Other recent work has investigated AI-driven review systems **Aggarwal, "AI-Driven Review Systems for Peer Review"**, agentic frameworks **Kaplan, "Agentic Frameworks for Large Language Models in Peer Review"**, and comparative analyses of LLM-generated reviews **Henderson, "Comparative Analysis of LLM-Generated Reviews"**, along with broader explorations of LLMsâ€™ role in peer review **Bender, "Large Language Models in Peer Review: A Broader Perspective"**.
While it is not the primary focus of our work, we analyze the quality of LLM-generated peer reviews in Section~\ref{sec:analysis}.