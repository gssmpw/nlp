\appendix
\newpage



\section{Baseline methods.}

We compare our approach to 18 baseline methods from IGMBT\footnote{https://github.com/kinit-sk/IMGTB} with its default setting \citep{Spiegel.2023}, which are categorized into metric-based and pretrained model-based methods. The metric-based methods include Binoculars ~\cite{hans2401spotting}, DetectLLM-LLR ~\cite{su2023detectllm}, DNAGPT ~\cite{yang2023dna}, Entropy ~\cite{gehrmann2019gltr}, FastDetectGPT ~\cite{bao2023fast}, GLTR ~\cite{gehrmann2019gltr}, LLMDeviation ~\cite{wu2023mfd}, Loglikelihood ~\cite{solaiman2019release}, LogRank ~\cite{Mitchell.2023}, MFD ~\cite{wu2023mfd}, Rank ~\cite{gehrmann2019gltr}, and S5 ~\cite{Spiegel.2023}. The model-based methods include NTNU-D ~\cite{sivesind2023turning}, ChatGPT-D ~\cite{guo2023close}, OpenAI-D ~\cite{solaiman2019release}, OpenAI-D-lrg ~\cite{solaiman2019release}, RADAR-D ~\cite{solaiman2019release}, and MAGE-D ~\cite{li2024mage}. 

\label{app:baselines}
\subsection{Metric based methods}
\subsubsection{Binoculars} 
Binoculars ~\cite{hans2401spotting}, analyzes text through two perspectives. First, it calculates the log perplexity of the text using an observer LLM. Then, a performer LLM generates next-token predictions, whose perplexity is evaluated by the observer—this metric is termed cross-perplexity. The ratio of perplexity to cross-perplexity serves as a strong indicator for detecting LLM-generated text.

\subsubsection{DNAGPT}
DNAGPT ~\cite{yang2023dna} is a training-free detection method designed to identify machine-generated text. Unlike conventional approaches that rely on training models, DNAGPT uses Divergent N-Gram Analysis (DNA) to detect discrepancies in text origin. The method works by truncating a given text at the midpoint and using the preceding portion as input to an LLM to regenerate the missing section. By comparing the regenerated text with the original through N-gram analysis (black-box) or probability divergence (white-box), DNAGPT reveals distributional differences between human and machine-written text, offering a flexible and explainable detection strategy.

\subsubsection{Entropy} Similar to the Rank score, the Entropy score for a text is determined by averaging the entropy values of each word, conditioned on its preceding context ~\cite{gehrmann2019gltr}.

\subsubsection{GLTR} The Entropy score, like the Rank score, is computed by averaging the entropy values of each word within a text, considering the preceding context ~\cite{gehrmann2019gltr}.

\subsubsection{MFD}
The Multi-level Fine-grained Detection (MFD) ~\cite{wu2023mfd} framework enhances text detection by combining statistical, semantic, and linguistic features at the sentence level. It first extracts low-level statistical features like readability and author style to quantify sentence structure. Simultaneously, high-level semantic differences are captured using an encoder with contrastive learning to distinguish LLM-generated text from human-written content. Additionally, advanced LLMs analyze the full text, extracting deep linguistic features related to lexicon, grammar, and syntax for more precise detection.

\subsubsection{Loglikelihood} This method utilizes a language model to compute the token-wise log probability. Specifically, given a text, the log probability of each token is averaged to produce a final score. A higher score indicates a greater likelihood that the text is machine-generated ~\cite{solaiman2019release}.

\subsubsection{LogRank} Unlike the Rank metric, which relies on absolute rank values, the Log-Rank score is derived by applying a logarithmic function to the rank value of each word ~\cite{Mitchell.2023}.

\subsubsection{Rank} The Rank score is calculated by determining the absolute rank of each word in a text based on its preceding context. The final score is obtained by averaging the rank values across the text. A lower score suggests a higher probability that the text was machine-generated ~\cite{gehrmann2019gltr}.

\subsubsection{DetectLLM-LLR} This approach integrates Log-Likelihood and Log-Rank scores, leveraging their complementary properties to analyze a given text ~\cite{su2023detectllm}.

\subsubsection{FastDetectGPT} This method assesses changes in a model’s log probability function when small perturbations are introduced to a text. The underlying idea is that LLM-generated text often resides in a local optimum of the model’s probability function. Consequently, minor perturbations to machine-generated text typically result in lower log probabilities, whereas perturbations to human-written text may lead to either an increase or decrease in log probability ~\cite{Mitchell.2023}.

\subsection{Model-based methods}
\subsubsection{NTNU-D}
It is a fine-tuned classification model based on the RoBERTa-base model, and three sizes of the bloomz-models ~\cite{sivesind2023turning}
\subsubsection{ChatGPT-D}
The ChatGPT Detector ~\cite{guo2023close} is designed to differentiate between human-written text and content generated by ChatGPT. It is based on a RoBERTa model that has been fine-tuned for this specific task. The authors propose two training approaches: one that trains the model solely on generated responses and another that incorporates both question-answer pairs for joint training. In our evaluation, we adopt the first approach to maintain consistency with other detection methods.

\subsubsection{OpenAI-D and RADAR-D}
The OpenAI Detector ~\cite{solaiman2019release} are models fine-tuned on RoBERTa to identify outputs generated by GPT-2. Specifically, it was trained using text generated by the largest GPT-2 model (1.5B parameters) and is capable of determining whether a given text is machine-generated.


\subsubsection{MAGE-D}
MAGE (MAchine-GEnerated text detection) ~\cite{li2024mage} is a large-scale benchmark designed for detecting AI-generated text. It compiles human-written content from seven diverse writing tasks, including story generation, news writing, and scientific writing. Corresponding machine-generated texts are produced using 27 different LLMs, such as ChatGPT, LLaMA, and Bloom, across three representative prompt types.

\section{Dataset Details}
\subsection{Dataset File Structure}
The calibration, test, and extended sets are in separate directories. Each directory contains subdirectories for different models that were used to generate AI peer review samples. In each model's subdirectory, you will find multiple CSV files, with each file representing peer review samples of a specific conference. The directory and file structure are outlined below.
\\

{\fontsize{8.2pt}{10pt}
\begin{verbatim}
|-- calibration
    |-- gpt4o
        |-- (format: <conference>.<subset>.<LLM>.csv)
        |-- ICLR2017.calibration.gpt-4o.csv
        |-- ...
        |-- ICLR2024.calibration.gpt-4o.csv
        |-- NeurIPS2016.calibration.gpt-4o.csv
        |-- ...
        |-- NeurIPS2024.calibration.gpt-4o.csv
    |-- claude
        |-- ...
    |-- gemini
        |-- ...
    |-- llama
        |-- ...
    |-- qwen
        |-- ...
|-- extended
    |-- gpt4o
        |-- ICLR2018.extended.gpt-4o.csv
        |-- ...
        |-- ICLR2024.extended.gpt-4o.csv
        |-- NeurIPS2016.extended.gpt-4o.csv
        |-- ...
        |-- NeurIPS2024.extended.gpt-4o.csv
    |-- llama
        |-- ...
|-- test
    |-- gpt4o
        |-- ICLR2017.test.gpt-4o.csv
        |-- ...
        |-- ICLR2024.test.gpt-4o.csv
        |-- NeurIPS2016.test.gpt-4o.csv
        |-- ...
        |-- NeurIPS2024.test.gpt-4o.csv
    |-- claude
        |-- ...
    |-- gemini
        |-- ...
    |-- llama
        |-- ...
    |-- qwen
        |-- ...
\end{verbatim}
}

\subsection{CSV File Content}
CSV files may differ in their column structures across conferences and years. These differences are due to updates in the required review fields over time as well as variations between conferences. See Table \ref{tab:review_template_fields} for review fields of individual conferences.


\begin{table}%
\centering
\resizebox{1\columnwidth}{!}{
\begin{tabularx}{.51\textwidth}{l X}
\toprule
Conference & Required Fields \\
\midrule
ICLR2017 & review, rating, confidence \\
ICLR2018 & review, rating, confidence \\
ICLR2019 & review, rating, confidence \\
ICLR2020 & review, rating, confidence, experience assessment, checking correctness of derivations and theory, checking correctness of experiments, thoroughness in paper reading \\
ICLR2021 & review, rating, confidence \\
ICLR2022 & summary of the paper, main review, summary of the review, correctness, technical novelty and significance, empirical novelty and significance, flag for ethics review, recommendation, confidence \\
ICLR2023 & summary of the paper, strength and weaknesses, clarity quality novelty and reproducibility, summary of the review, rating, confidence \\
ICLR2024 & summary, strengths, weaknesses, questions, soundness, presentation, contribution, flag for ethics review, rating, confidence \\
NeurIPS2016 & review, rating, confidence \\
NeurIPS2017 & review, rating, confidence \\
NeurIPS2018 & review, overall score, confidence score \\
NeurIPS2019 & review, overall score, confidence score, contribution \\
NeurIPS2021 & summary, main review, limitations and societal impact, rating, confidence, needs ethics review, ethics review area \\
NeurIPS2022 & summary, strengths and weaknesses, questions, limitations, ethics flag, ethics review area, rating, confidence, soundness, presentation, contribution \\
NeurIPS2023 & summary, strengths, weaknesses, questions, limitations, ethics flag, ethics review area, rating, confidence, soundness, presentation, contribution \\
NeurIPS2024 & summary, strengths, weaknesses, questions, limitations, ethics flag, ethics review area, rating, confidence, soundness, presentation, contribution \\
\bottomrule
\end{tabularx}
}
\caption{Required fields in the review templates for each conference.}
\label{tab:review_template_fields}
\end{table}

\FloatBarrier
\subsection{Dataset Sample Numbers per Conference Year}
\label{sec:sample_breakdown}
In this section, we present further breakdowns of sample numbers by conference, year, and LLM, as shown in Table~\ref{tab:dataset-statistics}.
 
\begin{table}[h!]
\centering
\resizebox{.59\columnwidth}{!}{
\input{samples_numbers/N_entire-set}
}
\caption{Entire set sample size, including both human and AI reviews. They are exactly balanced.}
\label{tab:sample_numbers_entire}
\end{table}

\begin{table}[h!]
\centering
\resizebox{1\columnwidth}{!}
{
\input{samples_numbers/N_test-set}
}
\caption{Test set sample size, including both human and AI reviews. They are exactly balanced.}
\label{tab:sample_numbers_test}
\end{table}

\begin{table}[h!]
\centering
\resizebox{1\columnwidth}{!}{
\input{samples_numbers/N_calibration-set}
}
\caption{Calibration set sample size, including both human and AI reviews. They are exactly balanced.}
\label{tab:sample_numbers_calibration}
\end{table}

\FloatBarrier

\section{Additional results}

\subsection{Calibration using ICLR + NeurIPS reviews}
\label{app:main-result-iclr-plus-neurips-calibration}

Our main results in Table~\ref{tab:main-results} of Section~\ref{sec:experiments-main-result} utilized ICLR review from our calibration set to calibrate each detection method. This simulates the scenario in which some of the reviews in the test set are "out-of-domain" in the sense that they belong to a different conference than the reviews used for calibration. In Table~\ref{tab:main-results-iclr-plus-neurips-calibration}, we provide additional results for the same evaluation setting as before, but using both ICLR and NeurIPS reviews for calibration (i.e., fully ``in domain''). We generally see similar trends regarding relative performance between methods as before, with the exception that the Binoculars method achieves slightly higher detection rates than our Anchor method for Gemini reviews. This suggests that existing methods such as Binoculars may be more sensitive to the use of in-domain data during calibration. 

\begin{table}[h!]
\begin{center}
\resizebox{1\columnwidth}{!}
{
\begin{tabular}{p{0.1cm}lcccccc}
\toprule
& Target FPR: & \multicolumn{2}{c}{0.1\%} & \multicolumn{2}{c}{0.5\%} & \multicolumn{2}{c}{1\%}\\
\cmidrule(lr){3-4}
\cmidrule(lr){5-6}
\cmidrule(lr){7-8}
& & FPR & TPR & FPR & TPR & FPR & TPR \\
\midrule
\multirow{8}{*}{\rotatebox[origin=c]{90}{GPT-4o Reviews}} 
& Anchor & 0.1 & \textbf{61.4} & 0.3 & \textbf{80.1} & 0.8 & \textbf{87.4} \\
& Binoculars & 0.3 & 18.8 & 0.7 & 37.5 & 1.2 & 49.3 \\
& MAGE-D & 0.1 & 2.3 & 0.7 & 9.6 & 1.3 & 14.5  \\
& s5 & 0.3 & 0.7 & 1.0 & 8.0 & 1.6 & 16.3 \\
& MFD & 0.3 & 0.9 & 0.9 & 7.8 & 1.6 & 14.9 \\
& GLTR & 0.1 & 0.1 & 0.5 & 2.4 & 1.2 & 5.9 \\
& DetectGPT & 0.1 & 0.2 & 0.6 & 1.1 & 1.0 & 2.1 \\
& Loglikelihood & 0.1 & 0.0 & 0.3 & 0.2 & 0.6 & 1.0 \\
\midrule
\multirow{10}{*}{\rotatebox[origin=c]{90}{Gemini Reviews}} 
& Binoculars & 0.3 & \textbf{63.8} & 0.7 & \textbf{80.9} & 1.1 & \textbf{87.6} \\
& Anchor & 0.2 & 57.2 & 0.5 & 75.5 & 1.1 & 84.2 \\
& MFD & 0.1 & 1.9 & 0.6 & 11.0 & 1.0 & 18.1 \\
& s5 & 0.1 & 1.4 & 0.5 & 10.5 & 1.0 & 18.3 \\
& GLTR & 0.2 & 0.6 & 1.0 & 6.3 & 1.8 & 12.9 \\
& FastDetectGPT & 0.1 & 1.1 & 0.4 & 4.9 & 0.9 & 8.9 \\
& MAGE-D & 0.1 & 0.4 & 0.7 & 3.8 & 1.3 & 6.9 \\
& DetectGPT & 0.1 & 0.5 & 0.5 & 3.2 & 1.1 & 6.3  \\
& Loglikelihood & 0.1 & 0.0 & 0.3 & 0.2 & 0.6 & 1.6 \\
& NTNU-D & 11.5 & 0.0 & 21.8 & 0.0 & 26.3 & 0.1 \\
\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{Claude Reviews}}
& Anchor & 0.1 & \textbf{53.8} & 0.3 & \textbf{72.6} & 0.8 & \textbf{80.0} \\
& Binoculars & 0.3 & 46.4 & 0.7 & 70.2 & 1.1 & \textbf{80.0} \\
& MFD & 0.0 & 1.0 & 0.2 & 8.7 & 0.4 & 15.9  \\
& s5 & 0.0 & 0.7 & 0.2 & 8.4 & 0.4 & 16.4 \\
& DetectGPT & 0.1 & 0.6 & 0.5 & 4.9 & 1.0 & 10.1 \\
& GLTR & 0.0 & 0.0 & 0.3 & 0.7 & 0.6 & 1.9 \\
\bottomrule
\end{tabular}
}
\caption{Actual FPR and TPR calculated from the withheld test dataset at varying detection thresholds, which are calibrated using ICLR and NeurIPS reviews from our calibration set at different target FPRs. Best TPRs are in \textbf{bold}.}
\label{tab:main-results-iclr-plus-neurips-calibration}
\end{center}
\end{table}










\subsection{Additional results for test set: Llama and Qwen Detection Results}
\label{app:llama-qwen-results}
Table~10 is organized similarly to Table \ref{tab:main-results}, but it presents results for Llama and Qwen reviews. Both tables use the same set of thresholds for each method, which were calibrated using ICLR reviews generated by GPT-4o, Gemini, and Claude along with their matching human-written reviews.

\begin{table}[h]
\centering
\resizebox{1\columnwidth}{!}
{
\begin{tabular}{p{0.1cm}lcccccc}
\toprule
& Target FPR: & \multicolumn{2}{c}{0.1\%} & \multicolumn{2}{c}{0.5\%} & \multicolumn{2}{c}{1\%}\\
\cmidrule(lr){3-4}
\cmidrule(lr){5-6}
\cmidrule(lr){7-8}
& & FPR & TPR & FPR & TPR & FPR & TPR \\
\midrule
\multirow{15}{*}{\rotatebox[origin=c]{90}{Llama Reviews}} &Binoculars & 0.2 & \textbf{98.4} & 0.6 & \textbf{99.0} & 1.0 & \textbf{99.2} \\
&MAGE-D & 0.1 & 63.8 & 0.7 & 91.8 & 1.3 & 95.7 \\
&MFD & 0.0 & 57.6 & 0.1 & 81.9 & 0.1 & 87.4 \\
&GLTR & 0.1 & 55.4 & 0.2 & 75.2 & 0.3 & 81.7 \\
&FastDetectGPT & 0.1 & 54.7 & 0.5 & 73.8 & 1.2 & 80.6 \\
&s5 & 0.0 & 53.5 & 0.1 & 83.2 & 0.1 & 88.0 \\
&OpenAI-D & 0.2 & 38.8 & 0.6 & 48.3 & 1.6 & 57.1 \\
&LLMDeviation & 0.0 & 37.0 & 0.0 & 37.0 & 0.0 & 37.0 \\
&ChatGPT-D & 0.0 & 26.4 & 0.0 & 26.4 & 0.0 & 26.4 \\
&DetectLLM-{LLR} & 0.0 & 19.3 & 0.0 & 19.3 & 0.0 & 19.3 \\
&LogRank & 0.0 & 18.2 & 0.0 & 18.2 & 0.0 & 18.2 \\
&Loglikelihood & 0.0 & 13.4 & 0.3 & 76.0 & 0.5 & 85.5 \\
&Anchor & 0.1 & 12.2 & 0.5 & 28.5 & 1.0 & 36.8 \\
&DetectGPT & 0.1 & 1.7 & 0.7 & 10.8 & 1.3 & 19.7 \\
&RADAR-D & 0.9 & 0.0 & 2.6 & 1.6 & 4.2 & 12.5 \\
\midrule
\multirow{8}{*}{\rotatebox[origin=c]{90}{Qwen Reviews}} & Binoculars & 0.2 & \textbf{99.4} & 0.6 & \textbf{99.8} & 1.0 & \textbf{99.9} \\
& Anchor & 0.1 & 67.1 & 0.5 & 83.8 & 1.0 & 88.1 \\
& FastDetectGPT & 0.1 & 54.3 & 0.5 & 77.6 & 1.1 & 85.4  \\
& MFD & 0.1 & 37.6 & 0.4 & 73.0 & 0.6 & 82.3 \\
& s5 & 0.1 & 34.7 & 0.4 & 76.0 & 0.6 & 83.9 \\
& MAGE-D  & 0.1 & 33.2 & 0.7 & 73.6 & 1.3 & 86.2  \\
& GLTR & 0.1 & 31.1 & 0.3 & 64.7 & 0.6 & 77.4 \\
& Loglikelihood  & 0.0 & 0.5 & 0.3 & 30.8 & 0.5 & 56.6 \\
\bottomrule
\end{tabular}
}
\caption{Actual FPR and TPR calculated from the withheld test dataset at varying detection thresholds, which are calibrated using ICLR reviews from our calibration set at different target FPRs. Best TPRs are in \textbf{bold}.}

\label{tab:main-results_open-source-llm}

\end{table}

\FloatBarrier
\subsection{Experimental Results on Full Dataset}
\label{app:extended-set}

We test existing AI text generation text detection models on our entire dataset (i.e., the test set + the extended set). The results are shown in Tables \ref{tab:iclr_entire_dataset} for ICLR reviews and \ref{tab:neurips_entire_set} for NeurIPS reviews.


\input{tables/iclr_gpt4o.tex}
\input{tables/neurips_gpt4o.tex}


\FloatBarrier
\begin{table*}[]
\centering
\resizebox{1\textwidth}{!}
{
\begin{tabular}{p{3cm} p{10cm} p{10cm}}
    \toprule
    Category & Human review example & GPT-4o review examples \\
    \midrule
    References to specific details in the paper & ``Table 2 confirms that MDR outperforms Graph Rec Retriever (Asai et al.). This result shows the feasibility of a more accurate multi-hop QA model without external knowledge such as Wikipedia hyperlinks.'' & ``The paper extensively evaluates on multiple datasets and situates the contributions clearly within existing literature, substantiating claims with thorough quantitative analysis.'' \\
    \midrule
    Specific references to prior work & ``My only serious concern is the degree of novelty with respect to (Yuan et al., 2020), which was published at ECCV 2020. The main difference seems to be that in the proposed method the graph is dynamic (i.e., it depends on the input sentences), instead in (Yuan et al., 2018) the graph is learned but fixed for all the input samples.'' & ``The novelty of the TDM is not strong enough relative to prior work.'' \\
    \midrule
    Generic criticisms & N/A & ``Lack of clarity'' (without pointing to specific statements in the paper which need clarification); ``lack of discussion of limitations or computational considerations''; ``need more discussion of hyperparameter sensitivity''; ``need comparisons to more datasets'' (without suggesting any in particular); ``technical language used in the paper may be difficult to follow for unfamiliar readers''\\
    \bottomrule
\end{tabular}
}
\caption{Examples of differences identified in human analysis of human and AI-written peer reviews}
\label{tab:human_analysis_examples}
\end{table*}

\section{Additional analyses}

\subsection{Examples from human analysis of differences between human and AI-written peer reviews}
\label{app:human_analysis}

Table~\ref{tab:human_analysis_examples} provides examples of the issues identified in our qualitative analysis of human and AI-written peer reviews. In general, we observe that GPT-4o reviews lack references to specific details in the paper, lack references to specific prior work, and contain overly generic criticisms. See Section~\ref{sec:human-analysis} for additional discussion.



\subsection{Comparison of numeric scores assigned by human and AI reviewers}
\label{app:numeric_scores}
While Section~\ref{sec:analysis_numeric_score} focused on the misalignment between human and AI peer reviews from three commercial LLMs (GPT-4o, Gemini, and Claude) from the NeurIPS2022 review samples, this section presents the corresponding results for two open-source LLMs (Llama and Qwen), as shown in Figure~\ref{fig:numeric_scores}. The main findings from GPT-4o, Gemini, and Claude also hold for these two open-source models, with one notable difference: Llama and Qwen exhibit an even larger divergence in Presentation scores than Claude, the most overly-positive one amongst the commercial LLMs across all categories. In terms of Contribution scores, the evaluations from Llama and Qwen were similar to those of Claude.

In addition, we examine data from three other conferences (NeurIPS2023, NeurIPS2024, and ICLR2024). Although the results from these conferences are slightly less reliable---given that human reviews may have been influenced by AI use following the release of ChatGPT---the overall trend persists: LLMs tend to inflate the quality of papers compared to human reviewers.


\begin{figure*}
    \centering
    \underline{NeurIPS2022}
    \includegraphics[width=1\linewidth]{figures/figs_nuemric_scores/NeurIPS2022.ai_vs_human.png}
    \vspace{.01em}\\
    \underline{NeurIPS2023}
    \includegraphics[width=1\linewidth]{figures/figs_nuemric_scores/NeurIPS2023.ai_vs_human.png}
    \vspace{.01em}\\
    \underline{NeurIPS2024}
    \includegraphics[width=1\linewidth]{figures/figs_nuemric_scores/NeurIPS2024.ai_vs_human.png}
    \vspace{.01em}\\
    \underline{ICLR2024}
    \includegraphics[width=1\linewidth]{figures/figs_nuemric_scores/ICLR2024.ai_vs_human.png}
    \caption{Difference between AI and human scores. For each matched review (aligned by paper ID and recommendation), score differences were computed and displayed as histograms. Scores range from 1 to 4 for all metrics except Confidence, which ranges from 1 to 5. Statistical significance was assessed using a two-sided Wilcoxon signed‐rank test, with p-values shown in the legend. This figure includes only NeurIPS2022--2024 and ICLR2024, because they are the onyl conferences that required reviewers to submit these scores in their review templates.}
    \label{fig:numeric_scores}
\end{figure*}


\section{Prompts}
\label{sec:prompts}
This section includes the prompts we used to generate AI peer review texts. Due to space limitations, we provide only the ICLR2022 review guideline and review template here. Those for other years and other conferences (e.g., NeurIPS) are available on the respective conference official websites\footnote{https://icml.cc/Conferences/\{2016..2024\} \\ and https://neurips.cc/Conferences/\{2016..2024\} }.
\subsection{Prompts for Generating Reviews}
\noindent\underline{System prompt}:

\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
You are an AI researcher reviewing a paper submitted to a prestigious AI research conference. 
You will be provided with the manuscript text, the conference's reviewer guidelines, and the decision for the paper.
Your objective is to thoroughly evaluate the paper, adhering to the provided guidelines, and return a detailed assessment that supports the given decision using the specified response template.
Ensure your evaluation is objective, comprehensive, and aligned with the conference standards.

{reviewer_guideline}

{review_template}
\end{Verbatim}

\noindent\underline{User prompt}:
\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
Here is the paper you are asked to review. Write a well-justified review of this paper that aligns with a '{human_reviewer_decision}' decision.

```
{text}
```
\end{Verbatim}

\noindent\underline{ICLR2022 Reviewer Guideline}\\
\underline{(\texttt{\{reviewer\_guideline\}} in the system prompt)}:
\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
## Reviewer Guidelines

1. Read the paper: It’s important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it. Be sure to give yourself sufficient time for this step.

2. While reading, consider the following:
    - Objective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.
    - Strong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?
    - Weak points: is it weak in any of the aspects listed in b.?
    - Be mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even if it may not be very interesting for you.

3. Answer three key questions for yourself, to make a recommendation to Accept or Reject:
    - What is the specific question and/or problem tackled by the paper?
    - Is the approach well motivated, including being well-placed in the literature?
    - Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.

4. Write your initial review, organizing it as follows: 
    - Summarize what the paper claims to contribute. Be positive and generous.
    - List strong and weak points of the paper. Be as comprehensive as possible.
    - Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.
    - Provide supporting arguments for your recommendation.
    - Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. 
    - Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.

5. General points to consider:
    - Be polite in your review. Ask yourself whether you’d be happy to receive a review like the one you wrote.
    - Be precise and concrete. For example, include references to back up any claims, especially claims about novelty and prior work
    - Provide constructive feedback.
    - It’s also fine to explicitly state where you are uncertain and what you don’t quite understand. The authors may be able to resolve this in their response.
    - Don’t reject a paper just because you don’t find it “interesting”. This should not be a criterion at all for accepting/rejecting a paper. The research community is so big that somebody will find some value in the paper (maybe even a few years down the road), even if you don’t see it right now.
\end{Verbatim}

\noindent\underline{ICLR2022 Review Template}\\ 
\underline{(\texttt{\{reviewer\_template\}} in the system prompt)}:
\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
## Response template (JSON format)

Provide the review in valid JSON format with the following fields. Ensure all fields are completed as described below. The response must be a valid JSON object.

- "summary_of_the_paper": Briefly summarize the paper and its contributions. This is not the place to critique the paper; the authors should generally agree with a well-written summary. You may use paragraphs and bulleted lists for formatting, but ensure that the content remains a single, continuous text block. Do not use nested JSON or include additional fields.

- "main_review": "Provide review comments as a single text field (a string). Consider including assessment on the following dimensions: a comprehensive list of strong and weak points of the paper, your recommendation, supporting arguments for your recommendation, questions to clarify your understanding of the paper or request additional evidence, and additional feedback with the aim to improve the paper. You may use paragraphs and bulleted lists for formatting, but ensure that the content remains a single, continuous text block. Do not use nested JSON or include additional fields."

- "summary_of_the_review": Concise summary of 'main_review'. You may use paragraphs and bulleted lists for formatting, but ensure that the content remains a single, continuous text block. Do not use nested JSON or include additional fields.

- "correctness": A numerical rating on the following scale to indicate that the claims and methods are correct. The value should be between 1 and 4, where:
    - 1 = The main claims of the paper are incorrect or not at all supported by theory or empirical results.
    - 2 = Several of the paper’s claims are incorrect or not well-supported.
    - 3 = Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.
    - 4 = All of the claims and statements are well-supported and correct.

- "technical_novelty_and_significance": A numerical rating on the following scale to indicate technical novelty and significance. The value should be between 1 and 4, where:
    - 1 = The contributions are neither significant nor novel.
    - 2 = The contributions are only marginally significant or novel.
    - 3 = The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.
    - 4 = The contributions are significant and do not exist in prior works.
       
- "empirical_novelty_and_significance": A numerical rating on the following scale to indicate empirical novelty and significance. The value should be between 1 and 4, or -999 if not applicable, where:
    - 1 = The contributions are neither significant nor novel.
    - 2 = The contributions are only marginally significant or novel.
    - 3 = The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.
    - 4 = The contributions are significant and do not exist in prior works.
    - -999 = Not applicable.

- "flag_for_ethics_review": A boolean value (`true` or `false`) indicating whether there are ethical concerns in the work.

- "recommendation": A string indicating the final decision, which must strictly be one of the following options: 'strong reject', 'reject, not good enough', 'marginally below the acceptance threshold', 'marginally above the acceptance threshold', 'accept, good paper', or 'strong accept, should be highlighted at the conference'.

- "confidence": A nuemrical values to indicate how confident you are in your evaluation. The value should be between 1 and 5, where:
    - 1 = You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.
    - 2 = You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
    - 3 = You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
    - 4 = You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
    - 5 = You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
\end{Verbatim}


\subsection{Anchor Review Generation Prompt}
\label{sec:prompt_anchor}
\noindent\underline{System prompt}:
\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
You are an AI research scientist tasked with reviewing paper submissions for a top AI research conference. Carefully read the provided paper, then write a detailed review following a common AI conference review format (e.g., including summary, strengths and weakness, limitations, questions, suggestions for improvement). Make sure to include recommendation for the paper, either 'Accept' or 'Reject'. Your review should be fair and objective.
\end{Verbatim}

\noindent\underline{User prompt}:
\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
Here is the paper you are asked to review:
```
{text}
```
\end{Verbatim}

\subsection{Editing Prompts}
\label{sec:prompt_editing}
\label{sec:editing_prompt}
\noindent\underline{Minimal Editing}:
\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
Please proofread my review for typos and grammatical errors without altering the content. Keep the original wordings as much as you can, except for typo or grammatical  error.
\end{Verbatim}
\noindent\underline{Moderate Editing}:
\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
Please polish my review to improve sentence structure and readability while keeping the original intent clear.
\end{Verbatim}
\noindent\underline{Extensive Editing}:
\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
Please rewrite my review into a polished, professional piece that effectively communicates its main points.
\end{Verbatim}
\noindent\underline{Maximum Editing}:
\begin{Verbatim}[breaklines, breaksymbolleft={}, fontsize=\small]
Please transform my review into a high quality piece, using professional language and a polished tone. Please also extend my review with additional details from the oringial paper.
\end{Verbatim}

\section{Artifact Use Consistent With Intended Use}
In our work, we ensured that the external resources we utilized were applied in a manner that aligns with their intended purposes. We used several LLMs (including GPT-4o, Gemini, Claude, Qwen, and Llama) as well as an open-source package, IMGTB, with a focus on advancing research in a non-commercial, open-source context. The artifacts from our work will be non-commercial, for-research, and open-sourced.

\section{Use of AI Tool}
The authors of this paper used Github Co-pilot for coding assistance for analysis. 
