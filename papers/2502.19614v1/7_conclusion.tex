\section{Conclusion}

In this work, we introduced a new large-scale dataset of parallel human and AI-written peer reviews for identical papers submitted to leading AI research conferences.
Our evaluations showed that 18 existing open-source approaches for detecting AI text are poorly suited to the challenging problem of identifying AI-generated peer reviews. While high detection rates are possible with existing methods, this comes at the cost of relatively high rates of falsely identifying human-written reviews as containing AI text, which must be minimized in practice. We proposed a new approach which intentionally generates AI-written reviews for a given paper to serve as a basis for comparing semantic similarity to other evaluated reviews, achieving much higher accuracy in identifying AI-written peer reviews while maintaining a low level of false positives. Our results demonstrate the promise of this approach while also motivating the need for further research on methods for detecting unethical applications of LLMs in the peer review process.

\vspace{1em}
\section*{Limitations}

Our dataset primarily focuses on two conferences, both within the computer science domain. To broaden its applicability and relevance, incorporating additional conferences from diverse research domains would be beneficial.
Additionally, our proposed method requires generating reviews for manuscripts, making its use cases more constrained compared to previous approaches. However, in real-world scenarios, when reviewers submit their reviews, they typically accompany the corresponding manuscript. 
Therefore, our method remains applicable in this setting. 
Furthermore, our main results are based on evaluations of three commercial LLMs. Given the rapid emergence of new models, conducting comprehensive experiments across all available LLMs is infeasible. In addition, we leverage an open-source platform to conduct the baseline experiments, where baseline performance may vary depending on the choice of surrogate models. However, given the large number of baselines we evaluate, performing an exhaustive search for the optimal surrogate model for each method would be prohibitively expensive. Therefore, we use the default settings.
Lastly, we used a fixed set of prompts for different LLMs to generate reviews. In reality, a potential AI-tool reviewer might leverage different prompt and therefore could affect the detection performance. 


\section*{Ethics Statement}

Our work adheres to ethical AI principles. Peer review plays a critical role in advancing scientific discovery; however, the misuse of AI tools by reviewers to generate reviews without proper diligence can compromise the integrity of the review process.
Furthermore, consistent with previous studies, we have observed that AI-generated reviews tend to be overly generic, often failing to provide actionable feedback for authors. Additionally, AI reviewers generally assign higher scores compared to human reviewers, raising concerns that AI-assisted reviews could contribute to the acceptance of work that may not meet established human evaluation standards.
By developing methods to detect AI-generated reviews, our work seeks to mitigate the misuse of AI tools in peer review and promote a more rigorous and fair scientific review process.
