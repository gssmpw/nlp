\section{Analysis}
\label{sec:analysis}

\subsection{Human Analysis of Differences Between Human and AI-Written Peer Reviews}
\label{sec:human-analysis}

To better understand the characteristics which differentiate peer reviews written by humans and LLMs, we conducted a quantitative analysis of 32 reviews authored by humans and GPT-4o for 5 papers submitted to ICLR 2021. Specifically, we read an equal number of human and GPT-4 written reviews for each paper and noted differences in the content between them. A distinguishing characteristic of the analyzed human reviews was that they usually contained details or references to specific sections, tables, figures, or results in the paper. In contrast, peer reviews authored by GPT-4o lacked such specific details, instead focusing on higher-level comments. 
Another key difference identified in our qualitative analysis was the lack of any specific references to prior or related work in peer reviews generated by GPT-4o. Human authored peer reviews often point out missing references, challenge the novelty of the paper by referencing related work, or suggest specific baselines with references that should be included in the study. In contrast, none of the analyzed GPT-4o reviews contained such specific references to related work.
Finally, we found that the vast majority of GPT-4o reviews mentioned highly similar generic criticisms which were not found in human-authored reviews for the same paper. 
Examples of these issues are provided in Table~\ref{tab:human_analysis_examples} of Appendix~\ref{app:human_analysis}.

Prior work has shown that peer reviews written by GPT-4 and humans have a level of semantic similarity which is comparable to that between different human-authored peer reviews, which has been used to advocate for the usefulness of feedback from GPT-4 in the paper writing process \citep{liang2024can}. In our qualitative analysis, we found that GPT-4 does indeed generate similar higher-level comments as human reviewers, which could account for this semantic similarity. Despite being generic in nature, we would agree that such feedback could be useful to authors seeking to improve their manuscripts. Nevertheless, we believe that the lack of specificity, detail, and consideration of related work in peer reviews authored by GPT-4 demonstrates that it is not suitable for replacing human domain experts in the peer review process.


\subsection{Misalignment Between Human and AI Reviews}
\label{sec:analysis_numeric_score}

In addition to qualitative differences in the content of human and AI-written reviews, we also observe a divergence in numeric scores assigned as part of the review. Figure~\ref{fig:numeric_scores} (Appendix~\ref{app:numeric_scores}) provides histograms depicting the distribution of score differences for soundness, presentation, contribution, and confidence, which are computed by subtracting scores assigned for each category by human reviewers from those assigned by AI reviewers. AI-written peer reviews were matched with their corresponding human review (aligned by paper ID and overall recommendation) to compute the score differences. Confidence scores range from 1 to 5, while all other categories of scores range from 1 to 4. We focus on reviews from NeurIPS 2022, which were produced prior to the release of ChatGPT. This provides greater confidence that the human-labeled reviews were indeed written by humans, with little to no potential AI influence.

All LLMs produce higher scores than human reviews with a high degree of statistical significance, assessed using a two-sided Wilcoxon signed‚Äêrank test (see legend for $p$-values). While the difference between human and AI confidence scores are relatively consistent across all three  LLMs, Claude exceeds human scores by the greatest magnitude for soundness, presentation, and contribution. GPT-4o and and Gemini exceed human scores by a similar magnitude for presentation and contribution, while GPT-4o exhibits a greater divergence for soundness scores. Overall these results indicate that AI-written peer reviews are more favorable w.r.t. assigned scores than human-written peer reviews, which raises fairness concerns as scores are highly correlated with acceptance decisions. Our findings are consistent with prior work which has shown that papers reviewed by LLMs have a higher chance of acceptance \citep{drori2024human,latona2024ai,ye2024we}. 












%
