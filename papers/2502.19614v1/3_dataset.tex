\begin{table*}[h]
\centering
\small
\begin{tabular}{rrrrrrr}
\toprule
      & \multicolumn{2}{c}{Calibration set} & \multicolumn{2}{c}{Test set} & \multicolumn{2}{c}{Extended set} \\
            & ICLR  & NeurIPS & ICLR   & NeurIPS & ICLR    & NeurIPS \\
\midrule
GPT-4o       & 7,710 & 7,484   & 27,342 & 30,212  & 121,278 & 91,994  \\
Gemini 1.5 Pro      & 7,704 & 7,476   & 27,278 & 30,146  & -    & -     \\
Claude Sonnet 3.5 v2      & 7,672 & 7,484   & 27,340 & 30,208  & -    & -     \\
Llama 3.1 70b       & 7,708 & 7,472   & 27,284 & 30,086  & 121,130 & 91,706  \\
Qwen 2.5 72b        & 7,662 & 7,452   & 27,190 & 29,966  & -     & -     \\
Total & \multicolumn{2}{r}{\underline{75,824}}          & \multicolumn{2}{r}{\underline{287,052}}  & \multicolumn{2}{r}{\underline{426,108}}      \\
Grand total & \multicolumn{6}{r}{\textbf{788,984}}                          \\
\bottomrule
\end{tabular}
\caption{Dataset statistics. Each subset in the dataset is balanced, containing an equal number of human-written and AI-generated peer reviews. The calibration and test sets include reviews generated by five different large language models (LLMs), whereas the extended set includes reviews generated by GPT-4o and Llama 3.1.}
\label{tab:dataset-statistics}
\end{table*}

\section{Methods and Dataset Construction}
\subsection{Human reviews} 
We used the OpenReview API \citep{url_openreview_api} to collect submitted manuscripts and their reviews for the ICLR conferences from 2019 to 2024, as well as for NeurIPS conferences from 2021 to 2024. Additionally, we used the ASAP dataset \cite{yuan2022can} to collect manuscripts and reviews for ICLR 2017 to 2018 and NeurIPS 2016 to 2019\footnote{NeurIPS 2020 reviews are not publicly available}.

\subsection{AI reviews}
We generated 788,984 AI-generated reviews using five widely-used LLMs: GPT-4o, Claude Sonnet 3.5, Gemini 1.5 pro, Qwen 2.5 72b, and Llama 3.1 70b.

\textbf{Prompts.} To control the content and structure of these AI-generated reviews, we included conference-specific reviewer guidelines and review templates in the prompts. Note that this is nontrivial since the review templates have evolved significantly over the years (Table \ref{tab:review_template_fields}). In addition, we aligned the paper decisions by prompting the LLMs with specific decisions derived from the corresponding human reviews (see Appendix~\ref{sec:prompts} for complete prompt details). This step is important, as we found that AI review content and recommendations vary substantially depending on how input prompts are constructed. Thus, these measures represent our efforts to control the influence of text prompts.

\textbf{Computation.} We used Azure OpenAI Service, Amazon Web Services, and Google Cloud Platform to generate GPT-4o, Claude, and Gemini reviews, respectively. For Qwen reviews, we used NVIDIA RTX 6000 GPUs, and for Llama reviews, we used Intel Gaudi-2 accelerators.

\subsection{Dataset Statistics}
Table~\ref{tab:dataset-statistics} provides complete statistics for our generated dataset (see Appendix \ref{sec:sample_breakdown} for a breakdown by conference year and review-generating LLM). We withheld a randomly sampled subset of reviews to serve as a calibration set, which is used in our experiments to determine classification thresholds for each evaluated method. This calibration set contains 75,824 AI-generated and human-generated peer reviews, divided approximately evenly across all five LLMs. To construct the calibration set, we randomly selected 500 papers from ICLR (2021, 2022) and NeurIPS (2021, 2022) and generated AI reviews corresponding to the human reviews for each paper. Because our sampling was done at the paper level rather than the review level, the number of reviews per paper—and consequently per conference—varies slightly. To facilitate the evaluation of detection methods which are more computationally expensive (e.g., methods which requires using LLMs as surrogates), we also withheld a separate test set consisting of human reviews and those generated by all five LLMs for 500 randomly sampled papers from each conference \& year. This test split contains a total of 287,052 reviews and is used throughout our main experimental results. A further 426,108 reviews were generated from GPT-4o and Llama 3.1 70b, which we designate as an extended set of reviews (additional experimental results for the extended set are provided in Appendix~\ref{app:extended-set}).

We note slight variations in sample sizes among the five LLMs. These differences stem from LLM-specific factors such as context window limits, generation errors (e.g., malformed JSON or gibberish outputs), and input prompt safety filters.





\subsection{Review Editing}

LLMs have been widely used for editing human writing~\cite{laban2024beyond,raheja2024medit}, including tasks such as grammar checking and enhancing language fluency, which are particularly beneficial for non-native speakers in conveying their reviews more clearly. However, some users may overly rely on LLMs, leading to significant alterations of the original review. To simulate such scenarios in our dataset, we generate edited reviews with four varying levels of modification using LLMs (minimum, moderate, extensive, and maximum). This approach enables us to assess how robust detection methods are for text with different degrees of editing. Appendix \ref{sec:editing_prompt} illustrates the types of prompts used to generate these edits.







To validate whether these prompts produce reviews with varying levels of modification, we conducted a similarity checking experiment. Our hypothesis is that the similarity between the edited review and the original review decreases as the level of modification increases, ranging from minimal edits to extensive revisions. To test this, we used a sentence embedding model to compute embeddings for both the original and edited reviews and calculated their average cosine similarity scores. The results, summarized in Table~\ref{tab:edit_similarity}, confirm that the editing levels are indeed distinct.

\begin{table}[h]
\small
\centering
\begin{tabular}{cccc}
\toprule
Minimum & Moderate & Extensive & Maximum \\
\midrule
0.9841 & 0.9261 & 0.8616 & 0.6799\\
\bottomrule
\end{tabular}
\caption{Similarity score of edited reviews and original human-written reviews using different prompts. 
}
\label{tab:edit_similarity}
\end{table}
