\section{Experimental Results}






\subsection{Fully AI-Written Review Detectability} 
\label{sec:experiments-main-result}

\begin{figure*}[]
    \centering
    \includegraphics[trim={2mm 2mm 2mm 
    2mm},clip,width=.475\textwidth]{figures/fig2_calibration_roc/main_cal500_iclr_review-3llms.anchor-any.png}
    \includegraphics[trim={2mm 2mm 2mm 
    2mm},clip,width=.475\textwidth]{figures/fig2_calibration_roc/main_cal500_neurips_review-3llms.anchor-any.png}
    \caption{
    ROC plots computed from the combined GPT-4o, Gemini, and Claude review calibration dataset, showing results for ICLR (left) and NeurIPS (right); AUC values are shown in parentheses.
    }
    \label{fig:auc}
\end{figure*}
We compare our approach with 18 baseline methods from IGMBT (see Appendix~\ref{app:baselines} for details) and utilize the calibration set to determine appropriate thresholds for the test data. The threshold is then determined by setting an target False Positive Rate (FPR), which is achieved by adjusting the threshold until the FPR equals the target value. 
In our setup, we have studied varying thresholds (e.g., 0.1\%, 0.5\%, and 1\%). We focus on low FPR targets because false positive classifications—where human-written reviews are mistakenly identified as AI-generated—carry high stakes, potentially damaging an individual's reputation. Besides, we focus on AI review text samples generated by three commercial LLMs (GPT-4o, Gemini, and Claude) because these models are more advanced, making the text detection task harder. Also, general users are more likely to choose them over open-source LLMs due to their convenient user interfaces and limited access to the compute resources required for running open-source models.

AI text detection models can be calibrated for varying levels of sensitivity in order to balance the trade-off between true positive and false positive detections. Receiver operating characteristic (ROC) curves are therefore commonly used to compare different methods, as they provide a visualization of the true-positive rate (TPR) which can be achieved by a model when its decision boundary is calibrated for a range of different false positive rates (FPR). Figure~\ref{fig:auc} provides the ROC curves for our anchor embedding approach and baseline methods, calculated using our GPT-4o, Gemini, and Claude review calibration subset separately for reviews submitted to ICLR2021-2022 (left) and NeurIPS2021-2022 (right). The area under the curve (AUC) is provided for each method in the legend; higher values indicate better detection accuracy across the entire range of FPR values. These plots show that our anchor embedding approach consistently achieves the highest AUC for AI peer review detection among evaluated methods, followed by the baseline method Binoculars. 


While ROC curves are useful for comparing the overall performance of different classifiers, only the far left portion of these plots are typically relevant for practical applications of AI text detection models. This is particularly true in the domain of peer review, where the cost of a false positive is high. Reviewers volunteer their time and expertise to sustain this important process; false accusations have the potential to further reduce the availability of reviewers due to disengagement and can also lead to significant reputational harm. Therefore, it is vital that AI text detection systems for peer review be calibrated for a low FPR in order to avoid such negative outcomes.

Prior work has shown that AUC is not necessarily reflective of how models perform at very low FPR values \citep{yang2023dna,krishna2024paraphrasing,tufts2024examination}. Therefore, we also report the \textit{actual} TPR and FPR achieved by different detection methods at discrete low values of target FPR (0.1\%, 0.5\%, and 1\%), which we believe to be of greatest interest for practical applications. The target FPR is used to calculate each method's classification threshold using our calibration dataset, with the actual TPR and FPR computed over the withheld test dataset. To simulate a more challenging evaluation setting where some of the test reviews are ``out-of-domain'' in the sense that they come from a different conference than the calibration dataset, we use only the ICLR reviews to calibrate each method (see Section~\ref{app:main-result-iclr-plus-neurips-calibration} for in-domain evaluations). Table~\ref{tab:main-results} provides these results separately for the detection of GPT-4o, Gemini, and Claude reviews (Llama and Qwen results are provided in Appendix~\ref{app:llama-qwen-results}). Other baseline methods that failed achieve a TPR of at least 1\% at a target FPR of 1\% (i.e., TPR@1\%FPR < 1\%) are omitted.

The results in Table~\ref{tab:main-results} show that our anchor embedding approach achieves the highest TPR at low FPR values among evaluated methods for GPT-4o and Claude-generated reviews. This is notable, as GPT-4o and Claude are among the most advanced and widely-used LLMs currently available. The performance difference between our approach and the next best-performing method (Binoculars) is particularly large for GPT-4o reviews and low target FPR settings (e.g., 0.1\%), with our anchor embedding approach achieving absolute improvements of up to 50.1\% in TPR.  Furthermore, our anchor embedding approach exactly achieves the target FPR on the test set for these reviews, whereas other methods exceed the target FPR by as much as 0.7\%. Gemini-generated reviews appear to be easier for baseline methods to detect than GPT-4o and Claude reviews, and we observe that our anchor embedding approach performs similarly on these reviews as Binoculars. This suggests that the anchor embedding method provides the greatest benefits over existing methods for reviews which are the most challenging to detect; in fact, our approach performs best for detecting GPT-4o reviews, whereas other strong baselines such as Binoculars perform the worst on these reviews.
\begin{table}[h!]
\begin{center}
\resizebox{1\columnwidth}{!}
{
\begin{tabular}{p{0.1cm}lcccccc}
\toprule
& Target FPR: & \multicolumn{2}{c}{0.1\%} & \multicolumn{2}{c}{0.5\%} & \multicolumn{2}{c}{1\%}\\
\cmidrule(lr){3-4}
\cmidrule(lr){5-6}
\cmidrule(lr){7-8}
& & FPR & TPR & FPR & TPR & FPR & TPR \\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{GPT-4o Reviews}} 
& Anchor & 0.1 & \textbf{63.5} & 0.5 & \textbf{83.7} & 1.0 & \textbf{88.8} \\
& Binoculars & 0.2 & 17.1 & 0.6 & 33.6 & 1.0 & 45.2 \\
& MAGE-D & 0.1 & 2.3 & 0.6 & 8.8 & 1.3 & 14.7 \\
& s5 & 0.1 & 0.1 & 0.9 & 7.2 & 1.7 & 17.5 \\
& MFD & 0.2 & 0.1 & 0.8 & 6.0 & 1.6 & 15.6 \\
& GLTR & 0.1 & 0.1 & 0.4 & 1.9 & 1.1 & 5.7 \\
& DetectGPT & 0.1 & 0.1 & 0.6 & 1.1 & 1.2 & 2.3 \\
\midrule
\multirow{9}{*}{\rotatebox[origin=c]{90}{Gemini Reviews}} 
& Anchor & 0.2 & 59.7 & 0.8 & \textbf{80.3} & 1.3 & \textbf{86.5} \\
& Binoculars & 0.2 & \textbf{61.5} & 0.6 & 78.0 & 1.0 & 85.5 \\
& s5 & 0.0 & 0.2 & 0.5 & 9.6 & 1.1 & 19.4 \\
& MFD & 0.1 & 0.5 & 0.4 & 8.9 & 1.1 & 18.8  \\
& FastDetectGPT & 0.1 & 1.1 & 0.5 & 5.8 & 1.1 & 10.3  \\
& GLTR & 0.2 & 0.5 & 0.8 & 5.2 & 1.8 & 12.4 \\
& DetectGPT & 0.1 & 0.4 & 0.5 & 3.5 & 1.2 & 7.0 \\
& MAGE-D & 0.1 & 0.4 & 0.6 & 3.3 & 1.3 & 7.0 \\
& Loglikelihood & 0.0 & 0.0 & 0.3 & 0.1 & 0.5 & 1.0 \\
\midrule
\multirow{5}{*}{\rotatebox[origin=c]{90}{Claude Reviews}}
& Anchor & 0.1 & \textbf{59.6} & 0.5 & \textbf{75.8} & 1.0 & \textbf{81.8} \\
& Binoculars & 0.2 & 43.5 & 0.6 & 65.8 & 1.0 & 77.0 \\
& s5 & 0.0 & 0.1 & 0.2 & 7.6 & 0.5 & 17.5 \\
& MFD & 0.0 & 0.2 & 0.1 & 6.8 & 0.4 & 16.5 \\
& DetectGPT & 0.1 & 0.5 & 0.6 & 5.3 & 1.2 & 11.1 \\
& GLTR & 0.0 & 0.0 & 0.2 & 0.5 & 0.6 & 1.8  \\
\bottomrule
\end{tabular}
}
\caption{Actual FPR and TPR calculated from the withheld test dataset at varying detection thresholds, which are calibrated using ICLR reviews from our calibration set at different target FPRs. Highest TPRs are in \textbf{bold}.}
\label{tab:main-results}
\end{center}
\end{table}

Reviews generated by open-source LLMs (see Appendix~\ref{app:llama-qwen-results}) show somewhat different trends. For both Llama and Qwen reviews, the Binoculars method achieves near-perfect detection performance. While the Anchor method ranks second for Qwen reviews with slightly lower FPRs, it is not as performant on Llama reviews. This is surprising, as Llama reviews appear easier to detect---e.g., nearly twice as many methods achieve TPR@1\%FPR > 10\% for Llama reviews compared to Qwen, GPT-4o, and Claude reviews. One possibility is that the semantic similarity between the three anchor reviews generated by higher-quality LLMs (GPT-4o, Gemini, and Claude) and Llama reviews is low enough to overlap with that between the anchor reviews and human reviews, potentially blurring the decision boundary. Although the underlying causes of these differences need further investigation, these findings are less central to our study since most LLM users are likely to rely on commercial models due to their ease of use and superior quality.

\begin{table}
\centering
\resizebox{1\columnwidth}{!}
{
\begin{tabular}{ccccccc}
\toprule
Method & NDCG & Minimum & Moderate & Extensive & Maximum & Full\\
\midrule
Binoculars & 0.94 & 0.14 & 0.25 & 0.34 & 0.56 & 0.79\\
Anchor & 0.98 & 0.05 & 0.09 & 0.16 & 0.87 & 0.94\\
\bottomrule
\end{tabular}
}
\caption{NDCG and the proportion of reviews flagged as AI-written for different levels of editing. 
}
\label{tab:edit_results}
\end{table}

\subsection{Detecting AI-Edited Peer Reviews}
We also evaluated the ability of our method to correctly rank reviews in ascending order of their likelihood of being AI-generated. The ground truth order is: human-written, minimally-edited, moderately-edited, extensively-edited, maximally-edited, and fully AI-generated reviews.
To measure this, we compute the probability that each review type is AI-generated and rank them accordingly. The ranking is compared with the ground truth using the NDCG metric~\cite{jarvelin2002cumulated}, with higher values indicating that the predicted ranking is closer to the ground truth ranking.
In addition, we analyze the detection rates for each type of review. For each category, we calculate the percentage of reviews identified as AI-generated. Ideally, the detection rate should be lowest for minimally edited reviews and should progressively increase with the extent of AI involvement. 

Table~\ref{tab:edit_results} compares our method with the best baseline method from our prior experiments (Binoculars). Our anchor embedding method achieves a higher NDCG score compared to the Binoculars approach, indicating that our method ranks each type of review more accurately and closer to the ground truth.
Both methods detect increasing percentages of AI-generated text as the extent of editing increases. This suggests that reviews of more extensive editing are more likely to be detected as AI-generated.
For Minimum, Moderate, and Extensive types of edited reviews, our method shows a lower AI-generated detection percentage compared to Binoculars. On the other hand, for Maximum and fully AI-generated reviews, our method shows a higher detection rate than Binoculars.
This suggests that our approach is better calibrated to detect reviews where AI involvement is predominant.


