\section{Related Work}
\label{app:related}

\paragraph{AI text detection datasets}
Several benchmark datasets have been introduced to evaluate AI-based text detection models.
RAID-TD ~\cite{dugan2024raid} provides a large-scale benchmark designed to assess text detection under adversarial conditions, ensuring robustness against manipulated AI-generated content. The M4 Dataset ~\cite{Wang.2024} expands the scope by incorporating reviews from multiple LLMs across different languages, offering a more diverse linguistic evaluation. The HC3 Dataset ~\cite{guo2023close} consists of responses from ChatGPT and human experts, covering specialized domains such as finance, medicine, and law, in addition to general open-domain content. In contrast, the GPT Reddit Dataset (GRiD) ~\cite{qazi2024gpt} focuses on social media conversations, compiling a diverse set of human- and AI-generated responses to Reddit discussions. Meanwhile, Beemo \cite{artemova2024beemo} introduces a benchmark of expert-edited machine-generated outputs, spanning creative writing, summarization, and other practical applications, further refining detection challenges. These benchmarks primarily evaluate AI-generated text from a single model and do not address the domain of AI text in peer review. In contrast, our dataset is larger than most existing datasets (788k generations) and is unique in its focus on AI text detection in peer review.

\paragraph{AI-generated text detection}
AI-generated text detection has been framed as a binary classification task to distinguish human-written from machine-generated text ~\cite{bakhtin2019real, jawahar2020automatic, fagni2021tweepfake, mitchell2023detectgpt}. \citet{solaiman2019release} used a bag-of-words model with logistic regression for GPT-2 detection, while fine-tuned language models like RoBERTa \cite{liu1907roberta} improved accuracy ~\cite{zellers2019defending, uchendu2020authorship, gehrmann2019gltr}. Zero-shot methods based on perplexity and entropy emerged as alternatives ~\cite{ippolito2020automatic, gehrmann2019gltr}. Other studies focused on linguistic patterns and syntactic features for model-agnostic detection ~\cite{uchendu2020authorship, gehrmann2019gltr}. Watermarking techniques, such as DetectGPT ~\cite{mitchell2023detectgpt}, have also been proposed for proactive identification.
Centralized frameworks like MGTBench ~\cite{He.2023} and its refined version, IMGTB ~\cite{Spiegel.2023}, provide standardized evaluations for AI text detection. IMGTB categorizes methods into model-based and metric-based approaches. Model-based methods leverage large language models such as ChatGPT-turbo ~\cite{ChatGPT}
and Claude ~\cite{Claude}. Metric-based methods, including Log-Likelihood ~\cite{solaiman2019release}, Rank ~\cite{gehrmann2019gltr}, Entropy ~\cite{gehrmann2019gltr}, DetectGPT ~\cite{mitchell2023detectgpt}, and LRR ~\cite{su2023detectllm}, rely on log-likelihood and ranking for classification. 



\paragraph{AI-assisted peer review}
Recent studies have explored the role of LLMs in peer review, examining their influence on reviewing practices ~\cite{liang2024monitoring, liang2024can}, simulating multi-turn interactions ~\cite{Tan.2024}, and assessing their reviewing capabilities ~\cite{zhou-etal-2024-llm}. Tools like OpenReviewer ~\cite{tyseropenreviewer} provide AI-assisted review improvements, while other works focus on LLM transparency ~\cite{kuznetsov2024can} and distinguishing AI-generated content ~\cite{mosca2023distinguishing}. Other recent work has investigated AI-driven review systems ~\cite{tyser2024ai}, agentic frameworks ~\cite{liang2024can}, and comparative analyses of LLM-generated reviews ~\cite{liu2023reviewergpt}, along with broader explorations of LLMsâ€™ role in peer review ~\cite{jin2024agentreview, santu2024prompting, sukpanichnant2024peerarg}.
While it is not the primary focus of our work, we analyze the quality of LLM-generated peer reviews in Section~\ref{sec:analysis}.