\section{Introduction}


\begin{figure*}
    \centering
\includegraphics[width=0.95\textwidth, trim=0.1cm 4cm 0cm 0.3cm, clip]{figures/figure_resized.pdf}
    \caption{Left panel shows the data construction pipeline and the right panel shows the proposed method Anchor Embedding Approach for detecting AI generated review.}
    \label{fig:framework}
\end{figure*}
Recent advancements in large language models (LLMs) have enabled their application to a broad range of domains, where LLMs have demonstrated the ability to produce plausible and authoritative responses to queries even in highly technical subject areas. These advancements have coincided with a surge in interest in AI research, resulting in increased paper submissions to leading AI conferences \citep{audibert2022evolution}. Consequently, workloads for peer reviewers have also increased significantly, which could make LLMs an appealing tool for lessening the burden of fulfilling their peer review obligations \citep{kuznetsov2024can,kousha2024artificial,zhuang2025large}. 

Despite their impressive capabilities, the use of LLMs in the peer review process raises several ethical and methodological concerns which could compromise the integrity of the publication process \citep{hosseini2023fighting,latona2024ai,seghier2024ai,zhou-etal-2024-llm}. Reviewers are selected based on their expertise in a technical domain related to a submitted manuscript, which is necessary to critically evaluate the proposed research. Offloading this responsibility to an LLM circumvents the role that reviewer selection plays in ensuring proper vetting of a manuscript. Furthermore, LLMs are prone to hallucination and may not possess the ability to rigorously evaluate research publications. Therefore, the use of LLMs in an undisclosed manner in peer review poses a significant ethical concern that could undermine confidence in this important process. 

Motivating the need for evaluation resources and detection tools to address this problem is the apparent increase in AI-generated text among peer reviews submitted to recent AI research conferences. 
Recent studies revealed an upward trend in AI-generated texts among peer reviews at the corpus level \citep{liang2024monitoring,latona2024ai}. This trend is particularly concerning given that evaluations from human and AI reviewers are not aligned \citep{drori2024human,latona2024ai,ye2024we}, suggesting that the unregulated and undisclosed use of LLMs in peer review could undermine the integrity of the current system.


Despite the growing recognition of this problem, there is a lack of existing dataset resources for comprehensively evaluating the performance of AI text detection methods in the domain of peer review.
To address this deficiency, we introduce the largest dataset to-date of parallel human-written and LLM-written peer reviews for identical papers submitted to 8 years of papers for two leading AI research conferences, NeurIPS and ICLR (Figure~\ref{fig:framework}). Our dataset consolidates human-written peer reviews from existing sources with AI-written peer reviews that we generated for the same paper using five state-of-the-art LLMs: GPT-4o \cite{achiam2023gpt}, Claude Sonnet 3.5 \cite{Claude}, Gemini 1.5 pro \cite{team2023gemini}, Qwen 2.5 72b \cite{bai2023qwen}, and Llama 3.1 70b \cite{dubey2024llama}. 
In total, our dataset contains 788,984 peer reviews, evenly balanced between human-written reviews and AI-generated peer reviews created by these five LLMs.

We use our dataset to investigate the suitability of various AI text detection methods for identifying LLM generations in the peer review process. While limited prior work has analyzed the presence of AI-generated text in peer reviews at the corpus level \citep{liang2024monitoring} or used conference-submitted reviews only \citep{latona2024ai}, our study is the first to investigate the detectability LLM generations at the individual review level using systematically generated AI samples along with human samples, which is necessary to address this problem in practice. Specifically, we evaluate 18 open-source methods.

Our results show that most existing AI-text detection methods are limited in their ability to robustly detect AI-generated reviews while maintaining a low number of false positives. Motivated by this finding, we propose an alternative approach to detecting AI-generated peer reviews by comparing the semantic similarity of a given review to a set of reference AI-generated reviews for the same paper, which surpasses the performance of all existing approaches in detecting GPT-4o and Claude written peer reviews. We conduct analyses to understand how different levels of AI use for editing reviews impacts detectability and false positives, as well as the characteristics which distinguish LLM-written peer reviews from those written by humans. Our work demonstrate the challenge of detecting AI-written text in peer reviews and motivates the need for further research on methods to address this unethical use of LLMs in the peer review process.



To summarize, our contributions are as follows: (1) We introduce a new dataset containing 788,984 AI-written peer reviews generated by five widely-used LLMs along with corresponding human-written peer reviews for the same papers, which is the largest resource of its kind for evaluating the detectability of AI text in peer review. (2) Using our dataset, we evaluate 18 existing open source AI text detection algorithms as well as a commercial API, finding that most methods struggle to reliably detect fully AI-written peer reviews at low levels of false positives. (3) We propose a new detection method which compares the semantic similarity of a given peer review to a reference LLM-generated peer review for the same paper, which outperforms all existing detection approaches in our evaluations. (4) We conduct analyses to highlight the characteristics which distinguish human-written peer reviews from those generated by LLMs, show LLM reviews are generally more favorable and more confident it their assessment, and evaluate how the use of LLMs for editing peer reviews impact false positive rates. (5) We make our dataset publicly available under an open source license to facilitate future work on detecting AI generated text in peer review.
