\section{Anchor Embeddings Detection Methodology}

We propose a novel method for detecting AI-generated reviews by comparing their semantic similarity to a reference AI-generated review for the same article, referred to as the ``Anchor Review" (AR).
The AR can be generated by any LLM. We use a simple, generic prompt (Appendix \ref{sec:prompt_anchor}) to generate the AR without prior knowledge of the user prompts (i.e., the AR prompt differs from those used to create reviews in the testing dataset). Once an AR is generated for a given paper (Eq.\ref{eq_ar}), and a testing review (TR) is provided, we obtain their embeddings using a text embedding model (EM, Eqs.\ref{eq_em_ar} and \ref{eq_em_tr}). The semantic similarity between the embeddings of the AR and TR is then computed using the cosine similarity function (Eq.\ref{eq_score}). Finally, this similarity score is compared against a learned threshold ($\theta$): if the score exceeds the threshold, the review is classified as AI-generated; otherwise, it is not (Eq.~\ref{eq_label_condition}).
Putting everything together, the method is formalized as:

\vspace{-1em}
\begingroup
\small
\begin{flalign}
&\text{AR} = \text{LLM}(\text{paper}, \text{Prompt}_{\text{AR}}) \label{eq_ar}\\
&\text{Emb}_{\text{AR}} = \text{EM(AR)} \label{eq_em_ar}\\
&\text{Emb}_{\text{TR}} = \text{EM(TR)} \label{eq_em_tr}\\
&\text{Score} = \text{Cosine\_similarity}(\text{Emb}_{\text{AR}}, \text{Emb}_{\text{TR}}) \label{eq_score} \\
&\text{Label} =
\begin{cases} 
1 & \text{if } \text{Score} > \theta, \\
0 & \text{otherwise}. \label{eq_label_condition}
\end{cases}
\end{flalign}
\endgroup

In our study, we use OpenAI's embedding model (text-embedding-003-small). 
The threshold $\theta$ is learned from the calibration set. Specifically, for each review in the calibration data, we apply the steps outlined in Eqs.~\ref{eq_ar} to \ref{eq_score}. 

\paragraph{Voting of Multiple Anchors}
Intuitively, our approach performs best when the anchor embeddings are generated using the same model that produced the test review (source LLM). %
However, in real-world scenarios, the source LLM is typically unknown (a situation commonly referred to as ``black-box'' detection scenario). To address this challenge, we propose a voting-based technique. Specifically, we generate multiple anchor embeddings using different types of LLMs (anchor LLMs). For each anchor embedding, we compute the Score (Eq.\ref{eq_score}) and derive the corresponding label assignment (Eq.\ref{eq_label_condition}).  If at least one anchor embedding assigns a positive label, the final label is positive. Otherwise, the final label is negative. 

In our experiments in the following sections, we use three anchor reviews for voting---each generated by GPT-4o, Gemini, and Claude, respectively. %





