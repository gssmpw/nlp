@inproceedings{kapoor2024omniact,
	title        = {Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web},
	author       = {Kapoor, Raghav and Butala, Yash Parag and Russak, Melisa and Koh, Jing Yu and Kamble, Kiran and AlShikh, Waseem and Salakhutdinov, Ruslan},
	year         = 2024,
	booktitle    = {European Conference on Computer Vision},
	pages        = {161--178},
	organization = {Springer}
}
@inproceedings{naparstek2024kvp10k,
	title        = {KVP10k: A Comprehensive Dataset for Key-Value Pair Extraction in Business Documents},
	author       = {Naparstek, Oshri and Azulai, Ophir and Shapira, Inbar and Amrani, Elad and Yaroker, Yevgeny and Burshtein, Yevgeny and Pony, Roi and Rubinstein, Nadav and Dahood, Foad Abo and Prince, Orit and others},
	year         = 2024,
	booktitle    = {International Conference on Document Analysis and Recognition},
	pages        = {97--116},
	organization = {Springer}
}
@inproceedings{smock2022pubtables,
	title        = {PubTables-1M: Towards comprehensive table extraction from unstructured documents},
	author       = {Smock, Brandon and Pesala, Rohith and Abraham, Robin},
	year         = 2022,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {4634--4642}
}
@article{zheng2020global,
	title        = {Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context},
	author       = {Zheng, Xinyi and Burdick, Doug and Popa, Lucian and Zhong, Peter and Wang, Nancy Xin Ru},
	year         = 2021,
	journal      = {Winter Conference for Applications in Computer Vision (WACV)}
}
@misc{liu2023improvedllava,
	title        = {Improved Baselines with Visual Instruction Tuning},
	author       = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
	year         = 2023,
	publisher    = {arXiv:2310.03744}
}
@inproceedings{liu2023llava,
	title        = {Visual Instruction Tuning},
	author       = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	year         = 2023,
	booktitle    = {NeurIPS}
}
@misc{laurencon2024matters,
	title        = {What matters when building vision-language models?},
	author       = {Hugo Laurençon and Léo Tronchon and Matthieu Cord and Victor Sanh},
	year         = 2024,
	eprint       = {2405.02246},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{wang2024qwen2,
	title        = {Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
	author       = {Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2409.12191}
}
@misc{Li2024LLaVAOneVisionEV,
	title        = {LLaVA-OneVision: Easy Visual Task Transfer},
	author       = {Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
	year         = 2024,
	journal      = {ArXiv},
	volume       = {abs/2408.03326}
}
@misc{Li2024RedTV,
	title        = {Red Teaming Visual Language Models},
	author       = {Mukai Li and Lei Li and Yuwei Yin and Masood Ahmed and Zhenguang Liu and Qi Liu},
	year         = 2024,
	journal      = {ArXiv},
	volume       = {abs/2401.12915},
	url          = {https://api.semanticscholar.org/CorpusID:267094801}
}
@misc{zauner2010implementation,
	title        = {Implementation and benchmarking of perceptual image hash functions},
	author       = {Zauner, Christoph},
	year         = 2010,
	publisher    = {na}
}
@misc{marino2019ok,
	title        = {Ok-vqa: A visual question answering benchmark requiring external knowledge},
	author       = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},
	pages        = {3195--3204}
}
@misc{lu2022dynamic,
	title        = {Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning},
	author       = {Lu, Pan and Qiu, Liang and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Rajpurohit, Tanmay and Clark, Peter and Kalyan, Ashwin},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2209.14610}
}
@misc{zhao2023robut,
	title        = {Robut: A systematic study of table qa robustness against human-annotated adversarial perturbations},
	author       = {Zhao, Yilun and Zhao, Chen and Nan, Linyong and Qi, Zhenting and Zhang, Wenlin and Tang, Xiangru and Mi, Boyu and Radev, Dragomir},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2306.14321}
}
@misc{zhao2022multihiertt,
	title        = {MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data},
	author       = {Zhao, Yilun and Li, Yunxiang and Li, Chenying and Zhang, Rui},
	year         = 2022,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages        = {6\input{main}588--6600}
}
@misc{zhu2021tat,
	title        = {TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance},
	author       = {Zhu, Fengbin and Lei, Wenqiang and Huang, Youcheng and Wang, Chao and Zhang, Shuo and Lv, Jiancheng and Feng, Fuli and Chua, Tat Seng},
	year         = 2021,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	pages        = {3277--3287}
}
@misc{cheng2022hitab,
	title        = {HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation},
	author       = {Cheng, Zhoujun and Dong, Haoyu and Wang, Zhiruo and Jia, Ran and Guo, Jiaqi and Gao, Yan and Han, Shi and Lou, Jian-Guang and Zhang, Dongmei},
	year         = 2022,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages        = {1094--1110}
}
@misc{chen2021finqa,
	title        = {Finqa: A dataset of numerical reasoning over financial data},
	author       = {Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and others},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2109.00122}
}
@misc{chen2024unified,
	title        = {Unified hallucination detection for multimodal large language models},
	author       = {Chen, Xiang and Wang, Chenxi and Xue, Yida and Zhang, Ningyu and Yang, Xiaoyan and Li, Qiang and Shen, Yue and Gu, Jinjie and Chen, Huajun},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2402.03190}
}
@misc{zong2024safety,
	title        = {Safety fine-tuning at (almost) no cost: A baseline for vision large language models},
	author       = {Zong, Yongshuo and Bohdal, Ondrej and Yu, Tingyang and Yang, Yongxin and Hospedales, Timothy},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2402.02207}
}
@misc{rope-paper,
	title        = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
	author       = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2104.09864}
}
@misc{kafle2018dvqa,
	title        = {Dvqa: Understanding data visualizations via question answering},
	author       = {Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {5648--5656}
}
@misc{methani2020plotqa,
	title        = {Plotqa: Reasoning over scientific plots},
	author       = {Methani, Nitesh and Ganguly, Pritha and Khapra, Mitesh M and Kumar, Pratyush},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
	pages        = {1527--1536}
}
@misc{kahou2017figureqa,
	title        = {Figureqa: An annotated figure dataset for visual reasoning},
	author       = {Kahou, Samira Ebrahimi and Michalski, Vincent and Atkinson, Adam and K{\'a}d{\'a}r, {\'A}kos and Trischler, Adam and Bengio, Yoshua},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1710.07300}
}
@misc{obeid2020chart,
	title        = {Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model},
	author       = {Obeid, Jason and Hoque, Enamul},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.09142}
}
@misc{tang2023vistext,
	title        = {Vistext: A benchmark for semantically rich chart captioning},
	author       = {Tang, Benny J and Boggust, Angie and Satyanarayan, Arvind},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2307.05356}
}
@misc{liu2023aligning,
	title        = {Aligning large multi-modal model with robust instruction tuning},
	author       = {Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2306.14565}
}
@misc{masry2023unichart,
	title        = {Unichart: A universal vision-language pretrained model for chart comprehension and reasoning},
	author       = {Masry, Ahmed and Kavehzadeh, Parsa and Do, Xuan Long and Hoque, Enamul and Joty, Shafiq},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2305.14761}
}
@misc{zhang2024tinychart,
	title        = {Tinychart: Efficient chart understanding with visual token merging and program-of-thoughts learning},
	author       = {Zhang, Liang and Hu, Anwen and Xu, Haiyang and Yan, Ming and Xu, Yichen and Jin, Qin and Zhang, Ji and Huang, Fei},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2404.16635}
}
@misc{laurençon2024building,
	title        = {Building and better understanding vision-language models: insights and future directions.},
	author       = {Hugo Laurençon and Andrés Marafioti and Victor Sanh and Léo Tronchon},
	year         = 2024,
	eprint       = {2408.12637},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{kim2022ocr,
	title        = {Ocr-free document understanding transformer},
	author       = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
	year         = 2022,
	booktitle    = {European Conference on Computer Vision},
	pages        = {498--517},
	organization = {Springer}
}
@misc{deitke2024molmo,
	title        = {Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models},
	author       = {Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2409.17146}
}
@misc{li2024multimodal,
	title        = {Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models},
	author       = {Li, Lei and Wang, Yuqi and Xu, Runxin and Wang, Peiyi and Feng, Xiachong and Kong, Lingpeng and Liu, Qi},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2403.00231}
}
@misc{yuan2022syntax,
	title        = {Syntax-aware network for handwritten mathematical expression recognition},
	author       = {Yuan, Ye and Liu, Xiao and Dikubab, Wondimu and Liu, Hui and Ji, Zhilong and Wu, Zhongqin and Bai, Xiang},
	year         = 2022,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {4553--4562}
}
@misc{marti2002iam,
	title        = {The IAM-database: an English sentence database for offline handwriting recognition},
	author       = {Marti, U-V and Bunke, Horst},
	year         = 2002,
	journal      = {International journal on document analysis and recognition},
	publisher    = {Springer},
	volume       = 5,
	pages        = {39--46}
}
@misc{tanaka2021visualmrc,
	title        = {Visualmrc: Machine reading comprehension on document images},
	author       = {Tanaka, Ryota and Nishida, Kyosuke and Yoshida, Sen},
	year         = 2021,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 35,
	number       = 15,
	pages        = {13878--13888}
}
@misc{mishra2019ocr,
	title        = {Ocr-vqa: Visual question answering by reading text in images},
	author       = {Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
	year         = 2019,
	booktitle    = {2019 international conference on document analysis and recognition (ICDAR)},
	pages        = {947--952},
	organization = {IEEE}
}
@misc{singh2021textocr,
	title        = {Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text},
	author       = {Singh, Amanpreet and Pang, Guan and Toh, Mandy and Huang, Jing and Galuba, Wojciech and Hassner, Tal},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {8802--8812}
}
@misc{singh2019towards,
	title        = {Towards vqa models that can read},
	author       = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {8317--8326}
}
@misc{sidorov2020textcaps,
	title        = {Textcaps: a dataset for image captioning with reading comprehension},
	author       = {Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
	year         = 2020,
	booktitle    = {Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
	pages        = {742--758},
	organization = {Springer}
}
@misc{zhang2023llavar,
	title        = {Llavar: Enhanced visual instruction tuning for text-rich image understanding},
	author       = {Zhang, Yanzhe and Zhang, Ruiyi and Gu, Jiuxiang and Zhou, Yufan and Lipka, Nedim and Yang, Diyi and Sun, Tong},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2306.17107}
}
@misc{biten2019scene,
	title        = {Scene text visual question answering},
	author       = {Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Valveny, Ernest and Jawahar, CV and Karatzas, Dimosthenis},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF international conference on computer vision},
	pages        = {4291--4301}
}
@misc{li2024llava,
	title        = {Llava-onevision: Easy visual task transfer},
	author       = {Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2408.03326}
}
@misc{mishra2012scene,
	title        = {Scene text recognition using higher order language priors},
	author       = {Mishra, Anand and Alahari, Karteek and Jawahar, CV},
	year         = 2012,
	booktitle    = {BMVC-British machine vision conference},
	organization = {BMVA}
}
@misc{kembhavi2017you,
	title        = {Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension},
	author       = {Kembhavi, Aniruddha and Seo, Minjoon and Schwenk, Dustin and Choi, Jonghyun and Farhadi, Ali and Hajishirzi, Hannaneh},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern recognition},
	pages        = {4999--5007}
}
@misc{cheng2024seeclick,
	title        = {Seeclick: Harnessing gui grounding for advanced visual gui agents},
	author       = {Cheng, Kanzhi and Sun, Qiushi and Chu, Yougang and Xu, Fangzhi and Li, Yantao and Zhang, Jianbing and Wu, Zhiyong},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2401.10935}
}
@misc{wang2021screen2words,
	title        = {Screen2words: Automatic mobile UI summarization with multimodal learning},
	author       = {Wang, Bryan and Li, Gang and Zhou, Xin and Chen, Zhourong and Grossman, Tovi and Li, Yang},
	year         = 2021,
	booktitle    = {The 34th Annual ACM Symposium on User Interface Software and Technology},
	pages        = {498--510}
}
@misc{hsiao2022screenqa,
	title        = {Screenqa: Large-scale question-answer pairs over mobile app screenshots},
	author       = {Hsiao, Yu-Chung and Zubach, Fedir and Baechler, Gilles and Carbune, Victor and Lin, Jason and Wang, Maria and Sunkara, Srinivas and Zhu, Yun and Chen, Jindong},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2209.08199}
}
@misc{belouadi2023automatikz,
	title        = {Automatikz: Text-guided synthesis of scientific vector graphics with tikz},
	author       = {Belouadi, Jonas and Lauscher, Anne and Eger, Steffen},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2310.00367}
}
@misc{laurenccon2024unlocking,
	title        = {Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset},
	author       = {Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Sanh, Victor},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2403.09029}
}
@misc{turski2023ccpdf,
	title        = {Ccpdf: Building a high quality corpus for visually rich documents from web crawl data},
	author       = {Turski, Micha{\l} and Stanis{\l}awek, Tomasz and Kaczmarek, Karol and Dyda, Pawe{\l} and Grali{\'n}ski, Filip},
	year         = 2023,
	booktitle    = {International Conference on Document Analysis and Recognition},
	pages        = {348--365},
	organization = {Springer}
}
@misc{harley2015evaluation,
	title        = {Evaluation of deep convolutional nets for document image classification and retrieval},
	author       = {Harley, Adam W and Ufkes, Alex and Derpanis, Konstantinos G},
	year         = 2015,
	booktitle    = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
	pages        = {991--995},
	organization = {IEEE}
}
@misc{borchmann2021due,
	title        = {Due: End-to-end document understanding benchmark},
	author       = {Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Stanislawek, Tomasz and Jurkiewicz, Dawid and Turski, Micha{\l} and Szyndler, Karolina and Grali{\'n}ski, Filip},
	year         = 2021,
	booktitle    = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}
}
@misc{deng2022turl,
	title        = {Turl: Table understanding through representation learning},
	author       = {Deng, Xiang and Sun, Huan and Lees, Alyssa and Wu, You and Yu, Cong},
	year         = 2022,
	journal      = {ACM SIGMOD Record},
	publisher    = {ACM New York, NY, USA},
	volume       = 51,
	number       = 1,
	pages        = {33--40}
}
@misc{zhong2020image,
	title        = {Image-based table recognition: data, model, and evaluation},
	author       = {Zhong, Xu and ShafieiBavani, Elaheh and Jimeno Yepes, Antonio},
	year         = 2020,
	booktitle    = {European conference on computer vision},
	pages        = {564--580},
	organization = {Springer}
}
@misc{hu2024mplug,
	title        = {mplug-docowl 1.5: Unified structure learning for ocr-free document understanding},
	author       = {Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2403.12895}
}
@misc{gurari2018vizwiz,
	title        = {Vizwiz grand challenge: Answering visual questions from blind people},
	author       = {Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {3608--3617}
}
@misc{goyal2017making,
	title        = {Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
	author       = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {6904--6913}
}
@misc{yue2024mmmu,
	title        = {Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
	author       = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
	year         = 2024,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {9556--9567}
}
@misc{shabtay2024livexiv,
	title        = {LiveXiv--A Multi-Modal Live Benchmark Based on Arxiv Papers Content},
	author       = {Shabtay, Nimrod and Polo, Felipe Maia and Doveh, Sivan and Lin, Wei and Mirza, M Jehanzeb and Chosen, Leshem and Yurochkin, Mikhail and Sun, Yuekai and Arbelle, Assaf and Karlinsky, Leonid and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2410.10783}
}
@misc{liu2024ocrbench,
	title        = {OCRBench: on the hidden mystery of OCR in large multimodal models},
	author       = {Liu, Yuliang and Li, Zhang and Huang, Mingxin and Yang, Biao and Yu, Wenwen and Li, Chunyuan and Yin, Xu-Cheng and Liu, Cheng-Lin and Jin, Lianwen and Bai, Xiang},
	year         = 2024,
	journal      = {Science China Information Sciences},
	publisher    = {Springer},
	volume       = 67,
	number       = 12,
	pages        = 220102
}
@misc{mathew2022infographicvqa,
	title        = {Infographicvqa},
	author       = {Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
	year         = 2022,
	booktitle    = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
	pages        = {1697--1706}
}
@inproceedings{dai2023instructblip,
	title        = {Instruct{BLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},
	author       = {Wenliang Dai and Junnan Li and Dongxu Li and Anthony Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=vvoWPYqZJA}
}
@article{OpenAI2023GPT4TR,
	title        = {GPT-4 Technical Report},
	author       = {OpenAI},
	year         = 2023,
	journal      = {ArXiv},
	volume       = {abs/2303.08774}
}
@inproceedings{chen2024internvl,
	title        = {Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
	author       = {Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
	year         = 2024,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {24185--24198}
}
@misc{wu2024deepseekvl2,
	title        = {DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding},
	author       = {Zhiyu Wu and Xiaokang Chen and Zizheng Pan and Xingchao Liu and Wen Liu and Damai Dai and Huazuo Gao and Yiyang Ma and Chengyue Wu and Bingxuan Wang and Zhenda Xie and Yu Wu and Kai Hu and Jiawei Wang and Yaofeng Sun and Yukun Li and Yishi Piao and Kang Guan and Aixin Liu and Xin Xie and Yuxiang You and Kai Dong and Xingkai Yu and Haowei Zhang and Liang Zhao and Yisong Wang and Chong Ruan},
	year         = 2024,
	url          = {https://arxiv.org/abs/2412.10302},
	eprint       = {2412.10302},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@article{molmo,
	title        = {Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models},
	author       = {Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and Yen-Sung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Jen Dumas and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross B. Girshick and Ali Farhadi and Aniruddha Kembhavi},
	year         = 2024,
	journal      = {CoRR},
	volume       = {abs/2409.17146},
	url          = {https://doi.org/10.48550/arXiv.2409.17146},
	publtype     = {informal},
	cdate        = 1704067200000
}
@misc{abdin2024phi3technicalreporthighly,
	title        = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
	author       = {Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio César Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
	year         = 2024,
	url          = {https://arxiv.org/abs/2404.14219},
	eprint       = {2404.14219},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{kembhavi2016diagram,
	title        = {A diagram is worth a dozen images},
	author       = {Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
	year         = 2016,
	booktitle    = {Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
	pages        = {235--251},
	organization = {Springer}
}
@misc{yang2021tap,
	title        = {Tap: Text-aware pre-training for text-vqa and text-caption},
	author       = {Yang, Zhengyuan and Lu, Yijuan and Wang, Jianfeng and Yin, Xi and Florencio, Dinei and Wang, Lijuan and Zhang, Cha and Zhang, Lei and Luo, Jiebo},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {8751--8761}
}
@misc{masry2022chartqa,
	title        = {Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
	author       = {Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2203.10244}
}
@misc{zhang2024lmmsevalrealitycheckevaluation,
	title        = {LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models},
	author       = {Kaichen Zhang and Bo Li and Peiyuan Zhang and Fanyi Pu and Joshua Adrian Cahyono and Kairui Hu and Shuai Liu and Yuanhan Zhang and Jingkang Yang and Chunyuan Li and Ziwei Liu},
	year         = 2024,
	url          = {https://arxiv.org/abs/2407.12772},
	eprint       = {2407.12772},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{lmms_eval2024,
	title        = {LMMs-Eval: Accelerating the Development of Large Multimodal Models},
	author       = {Bo Li and Peiyuan Zhang and Kaichen Zhang and Fanyi Pu and Xinrun Du and Yuhao Dong and Haotian Liu and Yuanhan Zhang and Ge Zhang and Chunyuan Li and Ziwei Liu},
	year         = 2024,
	month        = {March},
	publisher    = {Zenodo},
	url          = {https://github.com/EvolvingLMMs-Lab/lmms-eval},
	version      = {v0.1.0}
}
@misc{granite2024granite,
	title        = {Granite 3.0 Language Models},
	author       = {Granite Team, IBM},
	year         = 2024
}
@misc{granite31github,
	title        = {{Granite 3.1 Language Models}},
	author       = {Granite Team, IBM},
	year         = 2024,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/ibm-granite/granite-3.1-language-models}}
}
@misc{ainslie2023gqa,
	title        = {Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
	author       = {Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2305.13245}
}
@misc{su2024roformer,
	title        = {Roformer: Enhanced transformer with rotary position embedding},
	author       = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
	year         = 2024,
	journal      = {Neurocomputing},
	publisher    = {Elsevier},
	volume       = 568,
	pages        = 127063
}
@misc{liu-2022-deplot,
	title        = {DePlot: One-shot visual language reasoning by plot-to-table translation},
	author       = {Fangyu Liu and Julian Martin Eisenschlos and Francesco Piccinno and Syrine Krichene and Chenxi Pang and Kenton Lee and Mandar Joshi and Wenhu Chen and Nigel Collier and Yasemin Altun},
	year         = 2023,
	booktitle    = {Findings of the 61st Annual Meeting of the Association for Computational Linguistics},
	url          = {https://arxiv.org/abs/2212.10505}
}
@misc{li2023blip2,
	title        = {{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
	author       = {Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
	year         = 2023,
	booktitle    = {ICML}
}
@misc{Huang2024MultimodalTV,
	title        = {Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning},
	author       = {Brandon Huang and Chancharik Mitra and Assaf Arbelle and Leonid Karlinsky and Trevor Darrell and Roei Herzig},
	year         = 2024,
	booktitle    = {NeurIPS}
}
@misc{mitra2024sparse,
	title        = {Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers},
	author       = {Mitra, Chancharik and Huang, Brandon and Chai, Tianning and Lin, Zhiqiu and Arbelle, Assaf and Feris, Rogerio and Karlinsky, Leonid and Darrell, Trevor and Ramanan, Deva and Herzig, Roei},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2412.00142}
}
@misc{zhai2023siglip,
	title        = {Sigmoid Loss for Language Image Pre-Training},
	author       = {Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},
	year         = 2023,
	eprint       = {2303.15343},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{huggingface_docmatix,
	title        = {Docmatix: A New Approach to Document Understanding},
	author       = {HuggingFace},
	year         = 2025,
	url          = {https://huggingface.co/blog/docmatix},
	note         = {Accessed: 2025-01-14}
}
@article{abdin2024phi,
	title        = {Phi-3 technical report: A highly capable language model locally on your phone},
	author       = {Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2404.14219}
}
@inproceedings{mathew2021docvqa,
	title        = {Docvqa: A dataset for vqa on document images},
	author       = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
	pages        = {2200--2209}
}
@misc{Docling,
	title        = {Docling Technical Report},
	author       = {Deep Search Team},
	year         = 2024,
	month        = 8,
	doi          = {10.48550/arXiv.2408.09869},
	url          = {https://arxiv.org/abs/2408.09869},
	eprint       = {2408.09869},
	version      = {1.0.0}
}
@misc{koster2022rfc,
	title        = {Rfc 9309 robots exclusion protocol},
	author       = {Koster, M and Illyes, G and Zeller, H and Sassman, L},
	year         = 2022,
	journal      = {Internet Engineering Task Force}
}
@misc{CCpdf,
	title        = {CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data},
	author       = {Turski, Micha{\l} and Stanis{\l}awek, Tomasz and Kaczmarek, Karol and Dyda, Pawe{\l} and Grali{\'{n}}ski, Filip},
	year         = 2023,
	booktitle    = {Document Analysis and Recognition - ICDAR 2023},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {348--365},
	isbn         = {978-3-031-41682-8},
	editor       = {Fink, Gernot A. and Jain, Rajiv and Kise, Koichi and Zanibbi, Richard},
	abstract     = {In recent years, the field of document understanding has progressed a lot. A significant part of this progress has been possible thanks to the use of language models pretrained on large amounts of documents. However, pretraining corpora used in the domain of document understanding are single domain, monolingual, or nonpublic. Our goal in this paper is to propose an efficient pipeline for creating a big-scale, diverse, multilingual corpus of PDF files from all over the Internet using Common Crawl, as PDF files are the most canonical types of documents as considered in document understanding. We analyzed extensively all of the steps of the pipeline and proposed a solution which is a trade-off between data quality and processing time. We also share a CCpdf corpus in a form or an index of PDF files along with a script for downloading them, which produces a collection useful for language model pretraining. The dataset and tools published with this paper offer researchers the opportunity to develop even better multilingual language models.}
}
@misc{li2024llavaonevision,
	title        = {LLaVA-OneVision: Easy Visual Task Transfer},
	author       = {Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Peiyuan Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
	year         = 2024,
	url          = {https://arxiv.org/abs/2408.03326},
	eprint       = {2408.03326},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{zhang2024mm15,
	title        = {MM1.5: Methods, Analysis \& Insights from Multimodal LLM Fine-tuning},
	author       = {Haotian Zhang and Mingfei Gao and Zhe Gan and Philipp Dufter and Nina Wenzel and Forrest Huang and Dhruti Shah and Xianzhi Du and Bowen Zhang and Yanghao Li and Sam Dodge and Keen You and Zhen Yang and Aleksei Timofeev and Mingze Xu and Hong-You Chen and Jean-Philippe Fauconnier and Zhengfeng Lai and Haoxuan You and Zirui Wang and Afshin Dehghan and Peter Grasch and Yinfei Yang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2409.20566},
	eprint       = {2409.20566},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{dai2024nvlm,
	title        = {NVLM: Open Frontier-Class Multimodal LLMs},
	author       = {Wenliang Dai and Nayeon Lee and Boxin Wang and Zhuolin Yang and Zihan Liu and Jon Barker and Tuomas Rintamaki and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},
	year         = 2024,
	url          = {https://arxiv.org/abs/2409.11402},
	eprint       = {2409.11402},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{geminiteam2024gemini15,
	title        = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
	author       = {Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
	year         = 2024,
	url          = {https://arxiv.org/abs/2403.05530},
	eprint       = {2403.05530},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{openai_gpt4v,
	title        = {GPT-4V(ision) technical work and authors},
	author       = {OpenAI},
	year         = 2023,
	url          = {https://openai.com/contributions/gpt-4v/},
	note         = {Accessed: 2025-01-15}
}
@misc{openai_gpt4o,
	title        = {Hello GPT-4o},
	author       = {OpenAI},
	year         = 2024,
	url          = {https://openai.com/index/hello-gpt-4o/},
	note         = {Accessed: 2025-01-15}
}
@misc{anthropic_claude3_5,
	title        = {Claude 3.5 Sonnet},
	author       = {Anthropic},
	year         = 2024,
	url          = {https://www.anthropic.com/news/claude-3-5-sonnet},
	note         = {Accessed: 2025-01-15}
}
@misc{grattafiori2024llama3,
	title        = {The Llama 3 Herd of Models},
	author       = {Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and ...},
	year         = 2024,
	url          = {https://arxiv.org/abs/2407.21783},
	eprint       = {2407.21783},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@misc{alayrac2022flamingo,
	title        = {Flamingo: a Visual Language Model for Few-Shot Learning},
	author       = {Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
	year         = 2022,
	url          = {https://arxiv.org/abs/2204.14198},
	eprint       = {2204.14198},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{yao2024minicpm,
	title        = {MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
	author       = {Yuan Yao and Tianyu Yu and Ao Zhang and Chongyi Wang and Junbo Cui and Hongji Zhu and Tianchi Cai and Haoyu Li and Weilin Zhao and Zhihui He and Qianyu Chen and Huarong Zhou and Zhensheng Zou and Haoye Zhang and Shengding Hu and Zhi Zheng and Jie Zhou and Jie Cai and Xu Han and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
	year         = 2024,
	url          = {https://arxiv.org/abs/2408.01800},
	eprint       = {2408.01800},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{deitke2024molmopixmo,
	title        = {Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models},
	author       = {Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi},
	year         = 2024,
	url          = {https://arxiv.org/abs/2409.17146},
	eprint       = {2409.17146},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{agrawal2024pixtral,
	title        = {Pixtral 12B},
	author       = {Pravesh Agrawal and Szymon Antoniak and Emma Bou Hanna and Baptiste Bout and Devendra Chaplot and Jessica Chudnovsky and Diogo Costa and Baudouin De Monicault and Saurabh Garg and Theophile Gervet and Soham Ghosh and Amélie Héliou and Paul Jacob and Albert Q. Jiang and Kartik Khandelwal and Timothée Lacroix and Guillaume Lample and Diego Las Casas and Thibaut Lavril and Teven Le Scao and Andy Lo and William Marshall and Louis Martin and Arthur Mensch and Pavankumar Muddireddy and Valera Nemychnikova and Marie Pellat and Patrick Von Platen and Nikhil Raghuraman and Baptiste Rozière and Alexandre Sablayrolles and Lucile Saulnier and Romain Sauvestre and Wendy Shang and Roman Soletskyi and Lawrence Stewart and Pierre Stock and Joachim Studnia and Sandeep Subramanian and Sagar Vaze and Thomas Wang and Sophia Yang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2410.07073},
	eprint       = {2410.07073},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{bai2023qwenvl,
	title        = {Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
	author       = {Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
	year         = 2023,
	url          = {https://arxiv.org/abs/2308.12966},
	eprint       = {2308.12966},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{tong2024cambrian1,
	title        = {Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs},
	author       = {Shengbang Tong and Ellis Brown and Penghao Wu and Sanghyun Woo and Manoj Middepogu and Sai Charitha Akula and Jihan Yang and Shusheng Yang and Adithya Iyer and Xichen Pan and Ziteng Wang and Rob Fergus and Yann LeCun and Saining Xie},
	year         = 2024,
	url          = {https://arxiv.org/abs/2406.16860},
	eprint       = {2406.16860},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{wadekar2024evolutionmultimodalmodelarchitectures,
	title        = {The Evolution of Multimodal Model Architectures},
	author       = {Shakti N. Wadekar and Abhishek Chaurasia and Aman Chadha and Eugenio Culurciello},
	year         = 2024,
	url          = {https://arxiv.org/abs/2405.17927},
	eprint       = {2405.17927},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@article{Yin_2024_survey,
	title        = {A survey on multimodal large language models},
	author       = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
	year         = 2024,
	month        = nov,
	journal      = {National Science Review},
	publisher    = {Oxford University Press (OUP)},
	volume       = 11,
	number       = 12,
	doi          = {10.1093/nsr/nwae403},
	issn         = {2053-714X},
	url          = {http://dx.doi.org/10.1093/nsr/nwae403}
}
}
@misc{hu2024mplugdocowl15,
	title        = {mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding},
	author       = {Anwen Hu and Haiyang Xu and Jiabo Ye and Ming Yan and Liang Zhang and Bo Zhang and Chen Li and Ji Zhang and Qin Jin and Fei Huang and Jingren Zhou},
	year         = 2024,
	url          = {https://arxiv.org/abs/2403.12895},
	eprint       = {2403.12895},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{hu2024mplugpaperowl,
	title        = {mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model},
	author       = {Anwen Hu and Yaya Shi and Haiyang Xu and Jiabo Ye and Qinghao Ye and Ming Yan and Chenliang Li and Qi Qian and Ji Zhang and Fei Huang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2311.18248},
	eprint       = {2311.18248},
	archiveprefix = {arXiv},
	primaryclass = {cs.MM}
}
@misc{wang2023docllm,
	title        = {DocLLM: A layout-aware generative language model for multimodal document understanding},
	author       = {Dongsheng Wang and Natraj Raman and Mathieu Sibue and Zhiqiang Ma and Petr Babkin and Simerjot Kaur and Yulong Pei and Armineh Nourbakhsh and Xiaomo Liu},
	year         = 2023,
	url          = {https://arxiv.org/abs/2401.00908},
	eprint       = {2401.00908},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{smolVLM,
	title        = {SmolVLM},
	author       = {Hugging-Face},
	year         = 2024,
	url          = {https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct}
}
@misc{liao2024doclayllm,
	title        = {DocLayLLM: An Efficient and Effective Multi-modal Extension of Large Language Models for Text-rich Document Understanding},
	author       = {Wenhui Liao and Jiapeng Wang and Hongliang Li and Chengyu Wang and Jun Huang and Lianwen Jin},
	year         = 2024,
	url          = {https://arxiv.org/abs/2408.15045},
	eprint       = {2408.15045},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{huang2022layoutlmv3,
	title        = {LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
	author       = {Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},
	year         = 2022,
	url          = {https://arxiv.org/abs/2204.08387},
	eprint       = {2204.08387},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{kim2022ocrfreedocumentunderstandingtransformer,
	title        = {OCR-free Document Understanding Transformer},
	author       = {Geewook Kim and Teakgyu Hong and Moonbin Yim and Jeongyeon Nam and Jinyoung Park and Jinyeong Yim and Wonseok Hwang and Sangdoo Yun and Dongyoon Han and Seunghyun Park},
	year         = 2022,
	url          = {https://arxiv.org/abs/2111.15664},
	eprint       = {2111.15664},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{liu2024hrvda,
	title        = {HRVDA: High-Resolution Visual Document Assistant},
	author       = {Chaohu Liu and Kun Yin and Haoyu Cao and Xinghua Jiang and Xin Li and Yinsong Liu and Deqiang Jiang and Xing Sun and Linli Xu},
	year         = 2024,
	url          = {https://arxiv.org/abs/2404.06918},
	eprint       = {2404.06918},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{feng2024docpedia,
	title        = {DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding},
	author       = {Hao Feng and Qi Liu and Hao Liu and Jingqun Tang and Wengang Zhou and Houqiang Li and Can Huang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2311.11810},
	eprint       = {2311.11810},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{blecher2023nougat,
	title        = {Nougat: Neural Optical Understanding for Academic Documents},
	author       = {Lukas Blecher and Guillem Cucurull and Thomas Scialom and Robert Stojnic},
	year         = 2023,
	url          = {https://arxiv.org/abs/2308.13418},
	eprint       = {2308.13418},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{liu2024textmonkey,
	title        = {TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document},
	author       = {Yuliang Liu and Biao Yang and Qiang Liu and Zhang Li and Zhiyin Ma and Shuo Zhang and Xiang Bai},
	year         = 2024,
	url          = {https://arxiv.org/abs/2403.04473},
	eprint       = {2403.04473},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{ye2023ureader,
	title        = {UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model},
	author       = {Jiabo Ye and Anwen Hu and Haiyang Xu and Qinghao Ye and Ming Yan and Guohai Xu and Chenliang Li and Junfeng Tian and Qi Qian and Ji Zhang and Qin Jin and Liang He and Xin Alex Lin and Fei Huang},
	year         = 2023,
	url          = {https://arxiv.org/abs/2310.05126},
	eprint       = {2310.05126},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{rodriguez2024bigdocs,
	title        = {BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks},
	author       = {Juan Rodriguez and Xiangru Jian and Siba Smarak Panigrahi and Tianyu Zhang and Aarash Feizi and Abhay Puri and Akshay Kalkunte and François Savard and Ahmed Masry and Shravan Nayak and Rabiul Awal and Mahsa Massoud and Amirhossein Abaskohi and Zichao Li and Suyuchen Wang and Pierre-André Noël and Mats Leon Richter and Saverio Vadacchino and Shubbam Agarwal and Sanket Biswas and Sara Shanian and Ying Zhang and Noah Bolger and Kurt MacDonald and Simon Fauvel and Sathwik Tejaswi and Srinivas Sunkara and Joao Monteiro and Krishnamurthy DJ Dvijotham and Torsten Scholak and Nicolas Chapados and Sepideh Kharagani and Sean Hughes and M. Özsu and Siva Reddy and Marco Pedersoli and Yoshua Bengio and Christopher Pal and Issam Laradji and Spandanna Gella and Perouz Taslakian and David Vazquez and Sai Rajeswar},
	year         = 2024,
	url          = {https://arxiv.org/abs/2412.04626},
	eprint       = {2412.04626},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{appalaraju2021docformer,
	title        = {DocFormer: End-to-End Transformer for Document Understanding},
	author       = {Srikar Appalaraju and Bhavan Jasani and Bhargava Urala Kota and Yusheng Xie and R. Manmatha},
	year         = 2021,
	url          = {https://arxiv.org/abs/2106.11539},
	eprint       = {2106.11539},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@online{wikidump,
	title        = {Wikimedia Downloads},
	author       = {Wikimedia Foundation},
	url          = {https://dumps.wikimedia.org}
}
@inproceedings{schwartz-etal-2024-numerologic,
	title        = {{N}umero{L}ogic: Number Encoding for Enhanced {LLM}s' Numerical Reasoning},
	author       = {Schwartz, Eli  and Choshen, Leshem  and Shtok, Joseph  and Doveh, Sivan  and Karlinsky, Leonid  and Arbelle, Assaf},
	year         = 2024,
	month        = nov,
	booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Miami, Florida, USA},
	pages        = {206--212},
	doi          = {10.18653/v1/2024.emnlp-main.12},
	url          = {https://aclanthology.org/2024.emnlp-main.12/},
	editor       = {Al-Onaizan, Yaser  and Bansal, Mohit  and Chen, Yun-Nung},
	abstract     = {Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of {\textquotedblleft}42{\textquotedblright}, we suggest using {\textquotedblleft}2:42{\textquotedblright} as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.}
}
@misc{DoclingTechReport,
	title        = {Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion},
	author       = {Nikolaos Livathinos and Christoph Auer and Maksym Lysak and Ahmed Nassar and Michele Dolfi and Panos Vagenas and Cesar Berrospi Ramis and Matteo Omenetti and Kasper Dinkla and Yusik Kim and Shubham Gupta and Rafael Teixeira de Lima and Valery Weber and Lucas Morin and Ingmar Meijer and Viktor Kuropiatnyk and Peter W. J. Staar},
	year         = 2025,
	url          = {https://arxiv.org/abs/2501.17887},
	eprint       = {2501.17887},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{doclaynet,
	title        = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Segmentation},
	author       = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S. and Staar, Peter},
	year         = 2022,
	booktitle    = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	location     = {Washington DC, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {KDD '22},
	pages        = {3743–3751},
	doi          = {10.1145/3534678.3539043},
	isbn         = 9781450393850,
	url          = {https://doi.org/10.1145/3534678.3539043},
	abstract     = {Accurate document layout analysis is a key requirement for high-quality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we presentDocLayNet, a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10\% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNet-trained models are more robust and thus the preferred choice for general-purpose document-layout analysis.},
	numpages     = 9,
	keywords     = {computer vision, document management and text processing, machine learning, neural networks, object detection}
}
@inproceedings{tableformer,
	title        = {TableFormer: Table Structure Understanding With Transformers},
	author       = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
	year         = 2022,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {4614--4623}
}
@inproceedings{otsl,
	title        = {Optimized Table Tokenization for Table Structure Recognition},
	author       = {Lysak, Maksym and Nassar, Ahmed and Livathinos, Nikolaos and Auer, Christoph and Staar, Peter},
	year         = 2023,
	booktitle    = {Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos\'{e}, CA, USA, August 21–26, 2023, Proceedings, Part II},
	location     = {San Jos\'{e}, CA, USA},
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	pages        = {37–50},
	doi          = {10.1007/978-3-031-41679-8_3},
	isbn         = {978-3-031-41678-1},
	url          = {https://doi.org/10.1007/978-3-031-41679-8_3},
	abstract     = {Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a significant impact on the accuracy and run-time performance of any Im2Seq model, we investigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and specific rules. The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves significantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syntactically correct. This in turn eliminates most post-processing needs. Popular table structure data-sets will be published in OTSL format to the community.},
	numpages     = 14,
	keywords     = {Optimization, Transformers, Data Representation, Table Structure Recognition}
}
