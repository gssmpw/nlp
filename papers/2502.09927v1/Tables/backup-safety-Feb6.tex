
\subsection{Safety Benchmarks}


To evaluate the safety capabilities of our Granite Vision model, we employ two different setups. First, we utilize the standard VLM-as-a-Judge setup as described in \citep{Li2024RedTV}. Second, we also introduce a new safety classification setup, where we formalize a range of safety tasks as classification problems. This new setup is aimed at building safer and more reliable AI models by leveraging strong classification capabilities. While existing generative MLLMs typically do not excel at classification tasks, we believe that enhancing their discriminative capabilities is essential for enabling diverse applications and more sophisticated reasoning.

% Although existing generative MLLMs do not usually excel at classification tasks, we believe that strong MLLM classification abilities are essential for building safer, more reliable AI models, which enable diverse applications and more sophisticated reasoning capabilities.   

For the new safety classification setup, we also apply our Safety Vectors (SVs) approach (See Section~\ref{sec:model:sav}) to our Granite Vision model. We made comparison of our method to other strong baselines on the VLGuard~\citep{zong2024safety} public benchmark.

% a variety of public safety benchmarks, such as VLGuard~\citep{zong2024safety}, RTVLM~\citep{Li2024RedTV}, and LMM-Halucination~\citep{chen2024unified}. 

% Next, we provide more details for the evaluation process.

\paragraph{Datasets.} (i) \textbf{VLGuard}~\citep{zong2024safety} focuses on vision-language safety and identifies four main categories of harmful content: Privacy, Risky Behavior, Deception and Hateful Speech. The dataset is constructed of images from diverse sources and the instructions are generated by GPT-4V~\citep{OpenAI2023GPT4TR} with each safe image having both safe and unsafe instructions, and each unsafe image having a single instruction. (ii) \textbf{RTVLM}~\citep{Li2024RedTV} is the first red teaming dataset to benchmark current MLLMs in terms of several different aspects: faithfulness, privacy, safety, and fairness. In the RTVLM dataset, 5,200 samples are annotated by humans, or generated by GPT-4 accompanied by examples provided by humans. To ensure that the test data are unique and have not been seen by the evaluated VLMs, the authors used new question-image pairs from publicly available images or generated from diffusion. 
% (iii) \textbf{MHalu}~\citep{chen2024unified} is a dataset that evaluates the hallucinations of the models when answering multi-modal tasks. We use the default evaluation method provided in the dataset to identify whether this scenario is ``hallucinating'' or ``not hallucinating'', and compute the accuracy rate on correctly identified scenarios. 


% \paragraph{Baselines.} To evaluate the safety of Granite Vision, we use two different setups. In the VLM-as-a-Judge setup, we compare Granite Vision against the following MLLMs: (i) Phi-3.5-vision~\citep{abdin2024phi} is part of a family of small language models, which are designed for high capability and cost-effectiveness, performing well on tasks like language understanding, reasoning, coding, and math. Their smaller size and greater efficiency compared to larger language models make them suitable for deployment on devices with limited computing power, such as mobile phones; (ii) InternVL2~\citep{chen2024internvl} is a new family of open-source MLLM, from a 1B model for edge devices to a powerful 108B model. It uses a progressive alignment training strategy, efficiently scaling model size and refining data for strong performance with limited resources. Supporting diverse input modalities and output formats, InternVL2 generalizes across hundreds of vision-language tasks, matching specialized models' performance; (iii) DeepSeek-VL2~\citep{wu2024deepseekvl2} is a new series of large Mixture-of-Experts (MoE) Vision-Language Models. This model excels in various tasks, including visual question answering, OCR, document understanding, and visual grounding.



% For the safety classification setup, we compared the performance of Granite Vision against the following MLLMs: (i) Qwen2-VL~\citep{wang2024qwen2} achieves state-of-the-art performance on a variety of image-text and video-text tasks by dynamically processing varying-resolution images into visual tokens and employing novel positional embeddings that effectively fuse positional information across modalities. In our implementation we extract the SVs from the train set and evaluate on the test set. We note that the image-instruction pairs can be categorized into three categories: safe image and safe instruction, unsafe image and unsafe instruction, unsafe image and safe instructions. We merged the last two categories into one, as they are harmful in general; (ii) LLaVA-OneVision~\citep{Li2024LLaVAOneVisionEV} is an open source MLLM that performs well in single-image, multi-image and video tasks;  (iii) MOLMO~\citep{molmo} is a recent MLLM based on PixMo, a collection of new datasets for MLLM training, including highly detailed image captions, free-form image QA, and an innovative 2D pointing dataset, all collected without external VLMs; (4) InstructBLIP~\citep{dai2023instructblip} is based on the pretrained BLIP-2 models, which are large-scale vision-language models trained on 400M image-text pairs.

\paragraph{Baselines.} To evaluate the safety of Granite Vision, we use two different setups. In the VLM-as-a-Judge setup, we compare Granite Vision against the following MLLMs: (i) Phi-3.5-vision~\citep{abdin2024phi} is part of a family of small language models, which are designed for high capability and cost-effectiveness, performing well on tasks like language understanding, reasoning, coding, and math; (ii) InternVL2~\citep{chen2024internvl} is a new family of open-source MLLM, from a 1B model for edge devices to a powerful 108B model. It uses a progressive alignment training strategy, efficiently scaling model size and refining data for strong performance with limited resources; (iii) DeepSeek-VL2~\citep{wu2024deepseekvl2} is a new series of large Mixture-of-Experts (MoE) Vision-Language Models. This model excels in various tasks, including visual question answering, OCR, document understanding, and visual grounding.



For the safety classification setup, we compared the performance of Granite Vision against the following MLLMs: (i) Qwen2-VL~\citep{wang2024qwen2} achieves state-of-the-art performance on a variety of image-text and video-text tasks by dynamically processing varying-resolution images into visual tokens and employing novel positional embeddings that effectively fuse positional information across modalities. (ii) LLaVA-OneVision~\citep{Li2024LLaVAOneVisionEV} is an open source MLLM that performs well in single-image, multi-image and video tasks;  (iii) MOLMO~\citep{molmo} is a recent MLLM based on PixMo, a collection of new datasets for MLLM training, including highly detailed image captions, free-form image QA, and an innovative 2D pointing dataset, all collected without external VLMs; (iv) InstructBLIP~\citep{dai2023instructblip} is based on the pretrained BLIP-2 models, which are large-scale vision-language models trained on 400M image-text pairs.


    % Phi3.5-vision-ins & 24.0 & 26.9 & - & - & - & -  \\
    % InstructBLIP & 24.0 & 26.9 & - & - & - & -  \\
    % MOLMO & 24.0 & 26.9 & - & - & - & -  \\




% \begin{wraptable}{r}{0\textwidth}
%     % \vspace{-0.9cm}
%     \begin{tabular}{lccc}
%     \toprule
%     & Unsafe Img. & Safe Img.-Unsafe Inst. \\
%     \midrule
%     Phi-3.5-vision   & 91.2 & 90.7  \\
%     InternVL2-4B & 92.3 & 91.0  \\
%     DeepSeek-VL2  & 92.7 & 91.3 \\
%     Granite Vision  & \textbf{93.5} & \textbf{91.4} \\
%     \bottomrule
%     \end{tabular}
%     \caption{\textbf{Results} for VLM-as-a-Judge}
%     \label{tab:vlm-judge}
% \end{wraptable}


In what follows, we discuss the two distinct safety evaluation setups.

\input{Tables/safety-table2}

\paragraph{VLM-as-a-Judge Setup.} Here, we perform a standard evaluation using the approach of VLM-as-a-Judge in the same manner as described in the RTVLM evaluation procedure~\citep{Li2024RedTV} (See Table 8). Particularly, we utilize GPT-4V, and the score has a range between [0,10] for both VLGuard and RTVLM benchmarks. In the evaluation of VLGuard, we distinguish between two categories of image-instruction pairs as provided by the original categories: ``Unsafe'', where both the images and instructions are unsafe, and ``Safe-Unsafe'', where the images are unsafe but the instructions are safe (see Table~\ref{tab:vlm-judge}). For RTVLM, we evaluate using the same setup as in~\citep{Li2024RedTV}, focusing on four safety categories: `Mislead', `Politic', `Racial', and `Jailbreak', with images from existing datasets or generated by diffusion. Last, we compared to other baselines, like Phi-3.5-vision~\citep{abdin2024phi}, InternVL2~\citep{chen2024internvl}, and DeepSeek-VL2~\citep{wu2024deepseekvl2}.

% \begin{wraptable}{r}{0\textwidth}
%     % \vspace{-10pt}
%     \begin{tabular}{lcccc}
%     \toprule
%     & \multicolumn{2}{c}{VLGuard} & RTVLM & \\
%     \cmidrule(lr){2-3}
%     & Unsafe & Safe-Unsafe \\
%     \midrule
%     Phi-3.5-vision   & \textbf{8.7} & \textbf{9.3} & 8.4 \\
%     InternVL2-4B & 7.7 & 8.9  & - \\
%     DeepSeek-VL2  & 6.4 & 8.7 & - \\
%     Granite Vision  & 6.5 & 8.6 & 7.2 \\
%     \bottomrule
%     \end{tabular}
%     % \vspace{-10pt}
%     \caption{\textbf{Results} for \textit{VLM-as-a-Judge} setup.}
%     \label{tab:vlm-judge}
%     \vspace{-15pt}
% \end{wraptable}

%Results are shown in Table~\ref{tab:vlm-judge}. As can be seen, Granite Vision performs comparably to other baselines on the standard safety evaluation setup. While this is encouraging, we believe there is much more to be done to improve the model's safety, and we leave this for future work. In the next section, we describe our new efforts to formalize a wide range of safety tasks as classification problems.
Results are shown in Table~\ref{tab:vlm-judge}. As can be seen, Granite Vision performs comparably to other baselines on the standard safety evaluation setup. Readers must note that Granite Vision is the most light-weight model compared to baselines. We believe that there's definitely scope for further improvement and we discuss this in our future work. 
%there is much more to be done to improve the model's safety, and we leave this for future work. 
In the next section, we describe our new efforts to formalize a wide range of safety tasks as classification problems.
%Dhiraj Joshi made some changes above - please review Roi, Shafiq, Rogerio



\paragraph{Safety Classification Setup.} In this setup, we reformat the tasks in VLGuard to be a classification task and we report the raw classification accuracy of our method. Specifically, we prompt the model with labels, and ask him to select the right answer, comparing it to the ground truth. The motivation behind reformulating safety tasks as classification problems is that it encourages the development of MLLMs with robust discriminative abilities, crucial for safety-critical applications, and provides a clear, quantifiable metric for assessing safety performance. Last, we also apply our Safety Vectors (SVs) approach (See Section~\ref{sec:model:sav}) to our Granite Vision model.



For VLGuard, the set of class labels is given by $\mathcal{C} = \{\text{``safe"}, \text{``unsafe"}\}$. We use the metric VLGuard proposes Attack Success Rate (ASR) for evaluating unsafe inputs and Helpfulness for evaluating safe inputs. Note that the ASR = 1 - unsafe subset accuracy. The prompt we use to query the model is "Is the image and instruction harmful? Answer the question with Yes or No." In our implementation we extract the SVs from the train set and evaluate on the test set. We note that the image-instruction pairs can be categorized into three categories: safe image and safe instruction, unsafe image and unsafe instruction, unsafe image and safe instructions. We merged the last two categories into one, as they are harmful in general.



% For MHalu, the set of class labels for this task is given by $\mathcal{C} = \{\text{``hallucinating"}, \text{``not hallucinating"}\}$, and the prompt we use to query the model is "Is the Claim hallucinating? Answer the question with Yes or No." 


% For RTVLM in the classification setup, we focused specifically on four categories: `Mislead', `Politic', `Racial', and `Jailbreak', using images from existing datasets or generated by diffusion. The set of class labels is given by $\mathcal{C} = \{\text{``proceed with caution"}, \text{``safe to answer"}, \text{``refuse to answer"}\}$, and the prompt we use to query the model is "How would you classify this query? A. Proceed with Caution B. Safe to Answer C. Refuse to Answer?". 

% \vspace{-15pt}
\begin{wraptable}{r}{0.4\textwidth}
    \vspace{-10pt}
    \begin{tabular}{lcc}
    \toprule
    & \multicolumn{1}{c}{VLGuard} & \\
    % \cmidrule(lr){2-3}
    % & Unsafe \\
    \midrule
    Phi-3.5-vision   & 25.4  \\
    InstructBLIP & 18.5   \\
    MOLMO  & 35.8  \\
    Qwen2-VL-7B-ZS  & 26.9  \\
    LLaVA-OV-7B-ZS  & 31.4  \\
    \hline
    Granite Vision (Ours)  & 40.3  \\
    \textbf{+Safety Vectors (Ours)}  & \textbf{96.7}  \\
    \bottomrule
    \end{tabular}
    \vspace{-3pt}
    \caption{\textit{Safety classification} {Results}.}
    \label{tab:vlm-judge}
    % \vspace{-15pt}
\end{wraptable}


Results are shown in Table~\ref{tbl:safety}. It can be seen from the table that most MLLMs do not perform well for safety when the task is reformulated as a classification task, potentially because they are designed primarily for generative tasks (e.g., image captioning or open-ended question answering). We note that these models often struggle with general discriminative vision-and-language tasks, and thus reformulating safety tasks as classification tasks poses a major challenge for these models, which may result in a low performance. On the other hand, our approach for finding SVs seems to be more effective, probably due to the fact that this approach can utilize multimodal features for downstream discriminative tasks. Overall, we believe that framing safety evaluation as a classification task may offer a valuable framework for improving the safety of AI models, and we anticipate that future research on this aspect will provide further advancements in safety research.



% We note that while both Granite Vision and its combination with SVs perform well for several safety classification benchmarks, VLGuard contains a set of words and phrases that do not always have malicious intent. As a result, to further test our model, we have provided an additional experiment on VLGuard that uses the approach of VLM-as-a-Judge. In particular, we utilize GPT-4V as a judge in the same manner as described in the RTVLM evaluation procedure~\citep{Li2024RedTV}. We note that we evaluate the image-instruction pairs with their original categories: unsafe image and unsafe instruction, unsafe image and safe instructions. 


% We highlight that evaluation can  



%\paragraph{Future Directions.} In this section, we review the safety of our Granite Vision model, as ensuring the safety of generative MLLMs is crucial in order to prevent harm, build trust, address ethical concerns, and enable their responsible deployment in real-world applications. Despite its generally strong performance, Granite Vision has limitations in high-level reasoning and can sometimes produce ungrounded outputs, making it potentially unreliable in sensitive tasks, which requires nuanced classification. To address these issues, we will incorporate more reasoning-focused and structured-related data into the training process in the future.

%In addition, we believe that finding SVs in Granite Vision's attention heads could lead to significant improvements when reformulated safety tasks as classification problems. However, the current reliance on few-shot samples presents a limitation as these demonstrations, while informative, are inherently limited in their scope and may not cover the full range of potential safety issues. This could lead to gaps in the model's ability to identify and address safety concerns. To address this limitation, we plan to investigate scaling up SVs using more training data in future research.

%Dhiraj Joshi rewrote the above 2 paragraphs below - original version above - please review Roi, Shafiq, and Rogerio
%\paragraph{Future Directions.} 
\paragraph{Summary and Future Directions.} Ensuring safety of generative MLLMs is absolutely crucial in order to prevent harm, build trust, address ethical concerns, and enable their responsible deployment in real-world applications. Our results demonstrate that Granite Vision performs almost at par with baselines (despite being the lightest MLLM in the comparison pool) for {\it VLM-as-a-Judge} task. At the same time, Granite Vision + {\bf Safety Vectors} outperforms all other baselines for {\it safety classification} task. We do acknowledge that further work needs to be done to improve high-level reasoning and correct occasional incorrect outputs to improve reliability in sensitive tasks, which require nuanced classification. To address these, we will incorporate more reasoning-focused and structure-related data into the training process in the future.

In addition, we showed in this paper that finding safety vectors (SVs) in Granite Vision's attention heads led to significant improvements when safety tasks were reformulated as classification problems. Current reliance for SVs is on few-shot samples which are informative but may have limited scope in terms of capturing the range of possible safety issues that can be encountered. To further improve the model's ability to identify and address all safety concerns, we plan to investigate scaling up SVs using more training data in future research.
