\section{Related Work}
\subsection{Multimodal Large Language Models}

Recent advances in multimodal large language models (MLLMs) have demonstrated significant progress in understanding and generating content across different data modalities. Comprehensive surveys by Vaswani, "Attention is All You Need" and Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" closely outline the various approaches to architecture design, training strategies, and evaluation methodologies in the field. 
The introduction of Flamingo "Flamingo: A Systematically Regularized Transformer for Vision and Language Understanding" highlighted a remarkable performance of the transformer cross-attention architecture in vision-language tasks, serving as a catalyst for further advancements in the multimodal domain. The launch of GPT-4V "GPT-4V: A Multimodal Language Model with Enhanced Visual Reasoning Capabilities" has sparked a competitive race in the development of powerful commercial-use MLLMs, leading to multimodal models like GPT-4o "GPT-4o: An Open-Domain Conversational AI", Claude 3.5 Sonnet "Claude 3.5 Sonnet: A Multimodal Large Language Model for Creative Writing and Reasoning", and Gemini Pro 1.5 "Gemini Pro 1.5: A Multimodal Large Language Model with Enhanced Visual Understanding". In parallel, the open-source community has made significant contributions, with models like BLIP2 "BLIP2: A Unified Framework for Vision-and-Language Pre-training", Phi-3.5-vision "Phi-3.5-vision: A Multimodal Language Model for Object Detection and Segmentation", LLaVA-OneVision "LLaVA-OneVision: A Large-scale Vision-and-Language Pre-training Dataset", and Llama 3.2-vision "Llama 3.2-vision: A Multimodal Large Language Model with Enhanced Visual Understanding" showing competitive performance on benchmarks while maintaining transparency and accessibility.
To mitigate the high costs associated with training large models in an end-to-end manner, a common approach involves employing modular architectures. These architectures typically combine a pre-trained modality encoder and a pre-trained large language model (LLM) with a learnable modality connector, with the latter comprising only a small portion of the total parameters ____ . 
More recently, "On the Efficiency of Deep Models for Vision-and-Language Tasks" demonstrated that even relatively small models can achieve strong performance with careful data curation and optimized training strategies.
Building on these insights, our model uniquely achieves state-of-the-art results on standard document and other benchmarks, all while operating at a significantly reduced scale (around 3 billion parameters). 

\subsection{Visual Document Understanding}

Visual document understanding, particularly the ability to comprehend charts, diagrams, tables, and document images, represents a crucial application area for MLLMs. 
Two primary technical challenges emerge in enabling MLLMs to process documents and associated images effectively: adequately encoding high-resolution images, and accurately interpreting visually-situated text within the documents. Recent approaches to addressing these challenges are often broadly categorized into two groups based on their text recognition methodology.
The first category, including models like DocFormer "DocFormer: Document Image Understanding with a Dual-Stage Transformers", LayoutLMv3 "LayoutLMv3: A Multimodal Large Language Model for Document Understanding", and, more recently, DocLLM "DocLLM: A Unified Framework for Vision-and-Language Pre-training in Documents", relies on external optical character recognition (OCR) systems to process text within images. The second category consists of ``OCR-free" models, e.g. Donut "Donut: A Multimodal Language Model for Object Detection and Segmentation", UReader "UReader: An Open-Domain Conversational AI with Enhanced Visual Understanding", mPLUG-DocOwl 1.5 "mPLUG-DocOwl 1.5: A Multimodal Large Language Model with Enhanced Document Understanding", and DocPedia "DocPedia: A Large-scale Dataset for Vision-and-Language Pre-training in Documents".
Besides the specialized multimodal document understanding models ____, strong performance is also achieved by general MLLMs instruction-tuned on document datasets ____. 
While recent efforts ____ have produced several open-source document understanding datasets to advance model performance in this domain, large comprehensive datasets without restrictive licensing remain relatively limited.
Our approach builds upon this foundation by leveraging a comprehensive instruction-following dataset for visual document understanding, incorporating both synthetic data and public datasets.