@article{Yin_2024_survey,
	title        = {A survey on multimodal large language models},
	author       = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
	year         = 2024,
	month        = nov,
	journal      = {National Science Review},
	publisher    = {Oxford University Press (OUP)},
	volume       = 11,
	number       = 12,
	doi          = {10.1093/nsr/nwae403},
	issn         = {2053-714X},
	url          = {http://dx.doi.org/10.1093/nsr/nwae403}
}
}

@article{abdin2024phi,
	title        = {Phi-3 technical report: A highly capable language model locally on your phone},
	author       = {Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2404.14219}
}

@misc{alayrac2022flamingo,
	title        = {Flamingo: a Visual Language Model for Few-Shot Learning},
	author       = {Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
	year         = 2022,
	url          = {https://arxiv.org/abs/2204.14198},
	eprint       = {2204.14198},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@misc{anthropic_claude3_5,
	title        = {Claude 3.5 Sonnet},
	author       = {Anthropic},
	year         = 2024,
	url          = {https://www.anthropic.com/news/claude-3-5-sonnet},
	note         = {Accessed: 2025-01-15}
}

@misc{appalaraju2021docformer,
	title        = {DocFormer: End-to-End Transformer for Document Understanding},
	author       = {Srikar Appalaraju and Bhavan Jasani and Bhargava Urala Kota and Yusheng Xie and R. Manmatha},
	year         = 2021,
	url          = {https://arxiv.org/abs/2106.11539},
	eprint       = {2106.11539},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@misc{bai2023qwenvl,
	title        = {Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
	author       = {Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
	year         = 2023,
	url          = {https://arxiv.org/abs/2308.12966},
	eprint       = {2308.12966},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@inproceedings{chen2024internvl,
	title        = {Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
	author       = {Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
	year         = 2024,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {24185--24198}
}

@misc{dai2024nvlm,
	title        = {NVLM: Open Frontier-Class Multimodal LLMs},
	author       = {Wenliang Dai and Nayeon Lee and Boxin Wang and Zhuolin Yang and Zihan Liu and Jon Barker and Tuomas Rintamaki and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},
	year         = 2024,
	url          = {https://arxiv.org/abs/2409.11402},
	eprint       = {2409.11402},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@misc{feng2024docpedia,
	title        = {DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding},
	author       = {Hao Feng and Qi Liu and Hao Liu and Jingqun Tang and Wengang Zhou and Houqiang Li and Can Huang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2311.11810},
	eprint       = {2311.11810},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@misc{geminiteam2024gemini15,
	title        = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
	author       = {Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
	year         = 2024,
	url          = {https://arxiv.org/abs/2403.05530},
	eprint       = {2403.05530},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@misc{grattafiori2024llama3,
	title        = {The Llama 3 Herd of Models},
	author       = {Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and ...},
	year         = 2024,
	url          = {https://arxiv.org/abs/2407.21783},
	eprint       = {2407.21783},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}

@misc{hu2024mplugdocowl15,
	title        = {mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding},
	author       = {Anwen Hu and Haiyang Xu and Jiabo Ye and Ming Yan and Liang Zhang and Bo Zhang and Chen Li and Ji Zhang and Qin Jin and Fei Huang and Jingren Zhou},
	year         = 2024,
	url          = {https://arxiv.org/abs/2403.12895},
	eprint       = {2403.12895},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@misc{huang2022layoutlmv3,
	title        = {LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
	author       = {Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},
	year         = 2022,
	url          = {https://arxiv.org/abs/2204.08387},
	eprint       = {2204.08387},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@misc{huggingface_docmatix,
	title        = {Docmatix: A New Approach to Document Understanding},
	author       = {HuggingFace},
	year         = 2025,
	url          = {https://huggingface.co/blog/docmatix},
	note         = {Accessed: 2025-01-14}
}

@misc{kim2022ocrfreedocumentunderstandingtransformer,
	title        = {OCR-free Document Understanding Transformer},
	author       = {Geewook Kim and Teakgyu Hong and Moonbin Yim and Jeongyeon Nam and Jinyoung Park and Jinyeong Yim and Wonseok Hwang and Sangdoo Yun and Dongyoon Han and Seunghyun Park},
	year         = 2022,
	url          = {https://arxiv.org/abs/2111.15664},
	eprint       = {2111.15664},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@misc{li2023blip2,
	title        = {{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
	author       = {Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
	year         = 2023,
	booktitle    = {ICML}
}

@misc{li2024llavaonevision,
	title        = {LLaVA-OneVision: Easy Visual Task Transfer},
	author       = {Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Peiyuan Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
	year         = 2024,
	url          = {https://arxiv.org/abs/2408.03326},
	eprint       = {2408.03326},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@misc{liao2024doclayllm,
	title        = {DocLayLLM: An Efficient and Effective Multi-modal Extension of Large Language Models for Text-rich Document Understanding},
	author       = {Wenhui Liao and Jiapeng Wang and Hongliang Li and Chengyu Wang and Jun Huang and Lianwen Jin},
	year         = 2024,
	url          = {https://arxiv.org/abs/2408.15045},
	eprint       = {2408.15045},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@misc{liu2024hrvda,
	title        = {HRVDA: High-Resolution Visual Document Assistant},
	author       = {Chaohu Liu and Kun Yin and Haoyu Cao and Xinghua Jiang and Xin Li and Yinsong Liu and Deqiang Jiang and Xing Sun and Linli Xu},
	year         = 2024,
	url          = {https://arxiv.org/abs/2404.06918},
	eprint       = {2404.06918},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@inproceedings{mathew2021docvqa,
	title        = {Docvqa: A dataset for vqa on document images},
	author       = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
	pages        = {2200--2209}
}

@misc{openai_gpt4o,
	title        = {Hello GPT-4o},
	author       = {OpenAI},
	year         = 2024,
	url          = {https://openai.com/index/hello-gpt-4o/},
	note         = {Accessed: 2025-01-15}
}

@misc{openai_gpt4v,
	title        = {GPT-4V(ision) technical work and authors},
	author       = {OpenAI},
	year         = 2023,
	url          = {https://openai.com/contributions/gpt-4v/},
	note         = {Accessed: 2025-01-15}
}

@misc{rodriguez2024bigdocs,
	title        = {BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks},
	author       = {Juan Rodriguez and Xiangru Jian and Siba Smarak Panigrahi and Tianyu Zhang and Aarash Feizi and Abhay Puri and Akshay Kalkunte and François Savard and Ahmed Masry and Shravan Nayak and Rabiul Awal and Mahsa Massoud and Amirhossein Abaskohi and Zichao Li and Suyuchen Wang and Pierre-André Noël and Mats Leon Richter and Saverio Vadacchino and Shubbam Agarwal and Sanket Biswas and Sara Shanian and Ying Zhang and Noah Bolger and Kurt MacDonald and Simon Fauvel and Sathwik Tejaswi and Srinivas Sunkara and Joao Monteiro and Krishnamurthy DJ Dvijotham and Torsten Scholak and Nicolas Chapados and Sepideh Kharagani and Sean Hughes and M. Özsu and Siva Reddy and Marco Pedersoli and Yoshua Bengio and Christopher Pal and Issam Laradji and Spandanna Gella and Perouz Taslakian and David Vazquez and Sai Rajeswar},
	year         = 2024,
	url          = {https://arxiv.org/abs/2412.04626},
	eprint       = {2412.04626},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@misc{tong2024cambrian1,
	title        = {Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs},
	author       = {Shengbang Tong and Ellis Brown and Penghao Wu and Sanghyun Woo and Manoj Middepogu and Sai Charitha Akula and Jihan Yang and Shusheng Yang and Adithya Iyer and Xichen Pan and Ziteng Wang and Rob Fergus and Yann LeCun and Saining Xie},
	year         = 2024,
	url          = {https://arxiv.org/abs/2406.16860},
	eprint       = {2406.16860},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@misc{wadekar2024evolutionmultimodalmodelarchitectures,
	title        = {The Evolution of Multimodal Model Architectures},
	author       = {Shakti N. Wadekar and Abhishek Chaurasia and Aman Chadha and Eugenio Culurciello},
	year         = 2024,
	url          = {https://arxiv.org/abs/2405.17927},
	eprint       = {2405.17927},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}

@misc{wang2023docllm,
	title        = {DocLLM: A layout-aware generative language model for multimodal document understanding},
	author       = {Dongsheng Wang and Natraj Raman and Mathieu Sibue and Zhiqiang Ma and Petr Babkin and Simerjot Kaur and Yulong Pei and Armineh Nourbakhsh and Xiaomo Liu},
	year         = 2023,
	url          = {https://arxiv.org/abs/2401.00908},
	eprint       = {2401.00908},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@misc{ye2023ureader,
	title        = {UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model},
	author       = {Jiabo Ye and Anwen Hu and Haiyang Xu and Qinghao Ye and Ming Yan and Guohai Xu and Chenliang Li and Junfeng Tian and Qi Qian and Ji Zhang and Qin Jin and Liang He and Xin Alex Lin and Fei Huang},
	year         = 2023,
	url          = {https://arxiv.org/abs/2310.05126},
	eprint       = {2310.05126},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@misc{zhang2024mm15,
	title        = {MM1.5: Methods, Analysis \& Insights from Multimodal LLM Fine-tuning},
	author       = {Haotian Zhang and Mingfei Gao and Zhe Gan and Philipp Dufter and Nina Wenzel and Forrest Huang and Dhruti Shah and Xianzhi Du and Bowen Zhang and Yanghao Li and Sam Dodge and Keen You and Zhen Yang and Aleksei Timofeev and Mingze Xu and Hong-You Chen and Jean-Philippe Fauconnier and Zhengfeng Lai and Haoxuan You and Zirui Wang and Afshin Dehghan and Peter Grasch and Yinfei Yang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2409.20566},
	eprint       = {2409.20566},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

