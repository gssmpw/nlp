[
  {
    "index": 0,
    "papers": [
      {
        "key": "Yin_2024_survey",
        "author": "Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong",
        "title": "A survey on multimodal large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wadekar2024evolutionmultimodalmodelarchitectures",
        "author": "Shakti N. Wadekar and Abhishek Chaurasia and Aman Chadha and Eugenio Culurciello",
        "title": "The Evolution of Multimodal Model Architectures"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan",
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "openai_gpt4v",
        "author": "OpenAI",
        "title": "GPT-4V(ision) technical work and authors"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "openai_gpt4o",
        "author": "OpenAI",
        "title": "Hello GPT-4o"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "anthropic_claude3_5",
        "author": "Anthropic",
        "title": "Claude 3.5 Sonnet"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "geminiteam2024gemini15",
        "author": "Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "li2023blip2",
        "author": "Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi",
        "title": "{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "abdin2024phi",
        "author": "Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others",
        "title": "Phi-3 technical report: A highly capable language model locally on your phone"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "li2024llavaonevision",
        "author": "Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Peiyuan Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li",
        "title": "LLaVA-OneVision: Easy Visual Task Transfer"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "grattafiori2024llama3",
        "author": "Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and ...",
        "title": "The Llama 3 Herd of Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "bai2023qwenvl",
        "author": "Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou",
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
      },
      {
        "key": "tong2024cambrian1",
        "author": "Shengbang Tong and Ellis Brown and Penghao Wu and Sanghyun Woo and Manoj Middepogu and Sai Charitha Akula and Jihan Yang and Shusheng Yang and Adithya Iyer and Xichen Pan and Ziteng Wang and Rob Fergus and Yann LeCun and Saining Xie",
        "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs"
      },
      {
        "key": "chen2024internvl",
        "author": "Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",
        "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2024mm15",
        "author": "Haotian Zhang and Mingfei Gao and Zhe Gan and Philipp Dufter and Nina Wenzel and Forrest Huang and Dhruti Shah and Xianzhi Du and Bowen Zhang and Yanghao Li and Sam Dodge and Keen You and Zhen Yang and Aleksei Timofeev and Mingze Xu and Hong-You Chen and Jean-Philippe Fauconnier and Zhengfeng Lai and Haoxuan You and Zirui Wang and Afshin Dehghan and Peter Grasch and Yinfei Yang",
        "title": "MM1.5: Methods, Analysis \\& Insights from Multimodal LLM Fine-tuning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "appalaraju2021docformer",
        "author": "Srikar Appalaraju and Bhavan Jasani and Bhargava Urala Kota and Yusheng Xie and R. Manmatha",
        "title": "DocFormer: End-to-End Transformer for Document Understanding"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "huang2022layoutlmv3",
        "author": "Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei",
        "title": "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "wang2023docllm",
        "author": "Dongsheng Wang and Natraj Raman and Mathieu Sibue and Zhiqiang Ma and Petr Babkin and Simerjot Kaur and Yulong Pei and Armineh Nourbakhsh and Xiaomo Liu",
        "title": "DocLLM: A layout-aware generative language model for multimodal document understanding"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "kim2022ocrfreedocumentunderstandingtransformer",
        "author": "Geewook Kim and Teakgyu Hong and Moonbin Yim and Jeongyeon Nam and Jinyoung Park and Jinyeong Yim and Wonseok Hwang and Sangdoo Yun and Dongyoon Han and Seunghyun Park",
        "title": "OCR-free Document Understanding Transformer"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "ye2023ureader",
        "author": "Jiabo Ye and Anwen Hu and Haiyang Xu and Qinghao Ye and Ming Yan and Guohai Xu and Chenliang Li and Junfeng Tian and Qi Qian and Ji Zhang and Qin Jin and Liang He and Xin Alex Lin and Fei Huang",
        "title": "UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "hu2024mplugdocowl15",
        "author": "Anwen Hu and Haiyang Xu and Jiabo Ye and Ming Yan and Liang Zhang and Bo Zhang and Chen Li and Ji Zhang and Qin Jin and Fei Huang and Jingren Zhou",
        "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "feng2024docpedia",
        "author": "Hao Feng and Qi Liu and Hao Liu and Jingqun Tang and Wengang Zhou and Houqiang Li and Can Huang",
        "title": "DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "liao2024doclayllm",
        "author": "Wenhui Liao and Jiapeng Wang and Hongliang Li and Chengyu Wang and Jun Huang and Lianwen Jin",
        "title": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large Language Models for Text-rich Document Understanding"
      },
      {
        "key": "liu2024hrvda",
        "author": "Chaohu Liu and Kun Yin and Haoyu Cao and Xinghua Jiang and Xin Li and Yinsong Liu and Deqiang Jiang and Xing Sun and Linli Xu",
        "title": "HRVDA: High-Resolution Visual Document Assistant"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "li2024llavaonevision",
        "author": "Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Peiyuan Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li",
        "title": "LLaVA-OneVision: Easy Visual Task Transfer"
      },
      {
        "key": "dai2024nvlm",
        "author": "Wenliang Dai and Nayeon Lee and Boxin Wang and Zhuolin Yang and Zihan Liu and Jon Barker and Tuomas Rintamaki and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping",
        "title": "NVLM: Open Frontier-Class Multimodal LLMs"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "mathew2021docvqa",
        "author": "Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV",
        "title": "Docvqa: A dataset for vqa on document images"
      },
      {
        "key": "huggingface_docmatix",
        "author": "HuggingFace",
        "title": "Docmatix: A New Approach to Document Understanding"
      },
      {
        "key": "rodriguez2024bigdocs",
        "author": "Juan Rodriguez and Xiangru Jian and Siba Smarak Panigrahi and Tianyu Zhang and Aarash Feizi and Abhay Puri and Akshay Kalkunte and Fran\u00e7ois Savard and Ahmed Masry and Shravan Nayak and Rabiul Awal and Mahsa Massoud and Amirhossein Abaskohi and Zichao Li and Suyuchen Wang and Pierre-Andr\u00e9 No\u00ebl and Mats Leon Richter and Saverio Vadacchino and Shubbam Agarwal and Sanket Biswas and Sara Shanian and Ying Zhang and Noah Bolger and Kurt MacDonald and Simon Fauvel and Sathwik Tejaswi and Srinivas Sunkara and Joao Monteiro and Krishnamurthy DJ Dvijotham and Torsten Scholak and Nicolas Chapados and Sepideh Kharagani and Sean Hughes and M. \u00d6zsu and Siva Reddy and Marco Pedersoli and Yoshua Bengio and Christopher Pal and Issam Laradji and Spandanna Gella and Perouz Taslakian and David Vazquez and Sai Rajeswar",
        "title": "BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks"
      }
    ]
  }
]