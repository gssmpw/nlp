\section{Related Work}
\subsection{Multimodal Large Language Models}

Recent advances in multimodal large language models (MLLMs) have demonstrated significant progress in understanding and generating content across different data modalities. Comprehensive surveys by \cite{Yin_2024_survey} and \cite{wadekar2024evolutionmultimodalmodelarchitectures} closely outline the various approaches to architecture design, training strategies, and evaluation methodologies in the field. 
The introduction of Flamingo \citep{alayrac2022flamingo} highlighted a remarkable performance of the transformer cross-attention architecture in vision-language tasks, serving as a catalyst for further advancements in the multimodal domain. The launch of GPT-4V \citep{openai_gpt4v} has sparked a competitive race in the development of powerful commercial-use MLLMs, leading to multimodal models like GPT-4o \citep{openai_gpt4o}, Claude 3.5 Sonnet \citep{anthropic_claude3_5}, and Gemini Pro 1.5 \citep{geminiteam2024gemini15}. In parallel, the open-source community has made significant contributions, with models like BLIP2 \citep{li2023blip2}, Phi-3.5-vision \citep{abdin2024phi}, LLaVA-OneVision \citep{li2024llavaonevision}, and Llama 3.2-vision \citep{grattafiori2024llama3} showing competitive performance on benchmarks while maintaining transparency and accessibility.
To mitigate the high costs associated with training large models in an end-to-end manner, a common approach involves employing modular architectures. These architectures typically combine a pre-trained modality encoder and a pre-trained large language model (LLM) with a learnable modality connector, with the latter comprising only a small portion of the total parameters \citep{bai2023qwenvl, tong2024cambrian1, chen2024internvl}. 
More recently, \cite{zhang2024mm15} demonstrated that even relatively small models can achieve strong performance with careful data curation and optimized training strategies.
Building on these insights, our model uniquely achieves state-of-the-art results on standard document and other benchmarks, all while operating at a significantly reduced scale (around 3 billion parameters). 

\subsection{Visual Document Understanding}

Visual document understanding, particularly the ability to comprehend charts, diagrams, tables, and document images, represents a crucial application area for MLLMs. 
Two primary technical challenges emerge in enabling MLLMs to process documents and associated images effectively: adequately encoding high-resolution images, and accurately interpreting visually-situated text within the documents. Recent approaches to addressing these challenges are often broadly categorized into two groups based on their text recognition methodology.
The first category, including models like DocFormer \citep{appalaraju2021docformer}, LayoutLMv3 \citep{huang2022layoutlmv3}, and, more recently, DocLLM \citep{wang2023docllm}, relies on external optical character recognition (OCR) systems to process text within images. The second category consists of ``OCR-free" models, e.g. Donut \citep{kim2022ocrfreedocumentunderstandingtransformer}, UReader \citep{ye2023ureader}, mPLUG-DocOwl 1.5 \citep{hu2024mplugdocowl15}, and DocPedia \citep{feng2024docpedia}.
Besides the specialized multimodal document understanding models \citep{liao2024doclayllm, liu2024hrvda}, strong performance is also achieved by general MLLMs instruction-tuned on document datasets \citep{li2024llavaonevision, dai2024nvlm}. 
While recent efforts \citep{mathew2021docvqa, huggingface_docmatix, rodriguez2024bigdocs} have produced several open-source document understanding datasets to advance model performance in this domain, large comprehensive datasets without restrictive licensing remain relatively limited.
Our approach builds upon this foundation by leveraging a comprehensive instruction-following dataset for visual document understanding, incorporating both synthetic data and public datasets.



%