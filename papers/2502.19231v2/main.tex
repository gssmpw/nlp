\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
%\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL
\usepackage{float}
\usepackage[table,xcdraw,dvipsnames]{xcolor}

\usepackage{cite}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

\usepackage{listings}
\lstset{
    breaklines=true,
    basicstyle=\footnotesize,     % or \footnotesize or \tiny
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookrightarrow}},
    breakatwhitespace=true
}
% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1in}%
\addtolength{\topmargin}{-0.5in}%

% package for algorithm box
%\usepackage{algorithm,algorithmic}
%\usepackage[algo2e]{algorithm2e}
%\usepackage[]{algorithm2e}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{amsfonts}
% some very useful LaTeX packages include:
\usepackage{cite}      % Written by Donald Arseneau

\usepackage{graphicx}   % Written by David Carlisle and Sebastian Rahtz

\usepackage{amsthm}
%\RequirePackage{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{amssymb,calc}
\usepackage{thmtools}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{bbm}
%\usepackage[round]{natbib}

\usepackage{enumerate}
\usepackage{rotating}
\RequirePackage{cleveref}
\usepackage{hyphenat}
 \usepackage[numbers]{natbib}
%\usepackage{graphicx,epstopdf,epsfig}
\usepackage{caption,subcaption}
\usepackage{tikz}
\usepackage{pgfplots}

% https://mjtsai.com/blog/2005/07/29/roman-straight-quotes-in-pdflatex/
\usepackage[T1]{fontenc}
\usepackage{textcomp} % for \textquotesingle
\usepackage{lmodern}


\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\makeatletter
\renewcommand{\ALG@name}{Algorithm}
\makeatother



\usepackage[framemethod=TikZ]{mdframed}

\newcommand{\prompt}[1]{
\begin{mdframed}[topline=false, bottomline=false, leftline=true, rightline=false, innertopmargin=0pt, innerbottommargin=0pt, innerrightmargin=0pt, innerleftmargin=10pt, leftmargin=3em, rightmargin=3em, linewidth=2pt,linecolor=black, skipabove=12pt, skipbelow=7pt]
\begin{flushleft}
#1
\end{flushleft}
\end{mdframed}
}



% ------------------------------------------------------------------------------------------------------------------------ %

\pgfplotsset{width=10cm}

% https://tex.stackexchange.com/a/520121
\tikzset{declare function={gamma(\x)=sqrt(2*pi)*\x^(\x-0.5)*exp(-\x)*exp(1/(12*\x));}}
\tikzset{declare function={tpdf(\x,\nu)=gamma(0.5*(\nu+1))/(sqrt(pi*\nu)*gamma(\nu/2))*(1+\x^2/\nu)^(-(\nu+1)/2);}}
\tikzset{declare function={invgampdf(\x,\a,\b)=(\b/\x)^\a/\x/gamma(\a)*exp(-\b/\x);}}


% ------------------------------------------------------------------------------------------------------------------------ %

%\definecolor{lightgray}{gray}{0.8}
%\pagecolor{lightgray}

% ------------------------------------------------------------------------------------------------------------------------ %

% https://tex.stackexchange.com/questions/161338/can-cleveref-be-made-to-use-the-oxford-comma-for-multiple-citations
\newcommand{\creflastconjunction}{, and\nobreakspace}

% https://tex.stackexchange.com/questions/316426/negative-phantom-inside-equations
\newcommand{\nhphantom}[1]{\ifmmode\settowidth{\dimen0}{$#1$}\else\settowidth{\dimen0}{#1}\fi\hspace*{-\dimen0}}

% https://tex.stackexchange.com/questions/29359/pgfplots-how-to-fill-the-area-under-a-curve-with-oblique-lines-hatching-as-a
\usetikzlibrary{patterns}
\tikzset{
	hatch distance/.store in=\hatchdistance,
	hatch distance=5pt,
	hatch thickness/.store in=\hatchthickness,
	hatch thickness=0.5pt,
}

\newcommand{\wh}[1]{\widehat{#1}}

\definecolor{pink}{rgb}{0.9, 0.17, 0.31}


\newcommand\mC{\mathcal C}
\newcommand\Y{\bm Y}
\def\C {\,|\:}
\newcommand\mM{\mathcal M}
\newcommand\mN{\mathcal N}
\newcommand\mT{\mathcal T}
\newcommand\mG{\mathcal G}
\newcommand\mP{\mathcal P}
\newcommand\mX{\mathcal X}
\newcommand\bw{\bm w}
\newcommand\mE{\mathcal E}
\newcommand\E{\mathbb E}
\newcommand\Xm{\wt X^{(m)}}
\renewcommand\d{\mathrm d}
\newcommand\bG{\Gamma}
\renewcommand\P{\mathbb P}
\newcommand\mS{\mathcal S}
\newcommand\mL{\mathcal L}
\newcommand\mA{\mathcal A}
\newcommand\Xn{X^{(n)}}
\newcommand\R{\mathbb R}
\renewcommand\b{\bm{\beta}}
\newcommand\e{\mathrm e}
\newcommand\N{\mathbb N}
\renewcommand\bG{\mathbb G}
\newcommand\Yn{\Y^{(n)}}
\newcommand\bT{\mathbb T}
\newcommand\g{\gamma}
\newcommand{\Lmax}{L_{max}}


%\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\wt}[1]{\widetilde{#1}}

\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}

% ------------------------------------------------------------------------------------------------------------------------ %

%\definecolor{lightgray}{gray}{0.8}
%\pagecolor{lightgray}

% ------------------------------------------------------------------------------------------------------------------------ %

% https://tex.stackexchange.com/questions/161338/can-cleveref-be-made-to-use-the-oxford-comma-for-multiple-citations
\renewcommand{\creflastconjunction}{, and\nobreakspace}

% https://tex.stackexchange.com/questions/316426/negative-phantom-inside-equations
\renewcommand{\nhphantom}[1]{\ifmmode\settowidth{\dimen0}{$#1$}\else\settowidth{\dimen0}{#1}\fi\hspace*{-\dimen0}}

% ------------------------------------------------------------------------------------------------------------------------ %



\numberwithin{equation}{section}

%\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{coro}[thm]{Corollary}

%\theoremstyle{definition}
%\newtheorem*{defn}{Definition}
\newtheorem{exa}{Example}
%\newtheorem*{exa*}{Example}
\newtheorem{alg}{Algorithm}
\newtheorem{asm}{Assumption}

%\theoremstyle{remark}
\newtheorem{rem}{Remark}

\crefname{thm}{Theorem}{Theorems}
\crefname{prop}{Proposition}{Propositions}
\crefname{lem}{Lemma}{Lemmas}
\crefname{coro}{Corollary}{Corollaries}
\crefname{add}{Addendum}{Addendums}
\crefname{asm}{Assumption}{Assumptions}
\crefname{alg}{Algorithm}{Algorithms}
\crefname{proc}{Procedure}{Procedures}
\crefname{exe}{Exercise}{Exercises}
\crefname{exa}{Example}{Examples}
\crefname{prob}{Problem}{Problems}
\crefname{section}{Section}{Sections}
\crefname{subsection}{Section}{Sections}
\crefname{appendix}{Appendix}{Appendices}

\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\as}{as}
\DeclareMathOperator{\oas}{as\ast}
\DeclareMathOperator*{\conv}{\mathchoice{%
	\,\longrightarrow\,}{%		% displaystyle
	\rightarrow}{%				% textstyle
	\rightarrow}{%				% scriptstyle
	\rightarrow}%				% scriptscriptstyle
}

\def\argmax{\mathop{\arg\max}}
\def\argmin{\mathop{\arg\min}}
\def\argsup{\mathop{\arg\sup}}
\def\arginf{\mathop{\arg\inf}}

%\endlocaldefs

% ------------------------------------------------------------------------------------------------------------------------ %

\allowdisplaybreaks[3]

% ------------------------------------------------------------------------------------------------------------------------ %


\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}




\pdfminorversion=4
\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\if1\blind
%{
	\title{\sf  AI-Powered Bayesian Inference}
	\author{ Sean O'Hagan\footnote{Sean  O'Hagan is a 4th year PhD student at the Department of Statistics of the University of Chicago.} \,\, and \,\,  Veronika Ro\v{c}kov\'{a}\footnote{Veronika Ro\v{c}kov\'{a} is the Bruce Lindsay Professor of Econometrics and Statistics in the Wallman Society of Fellows at the Booth School of Business at the University of Chicago. 
	 The author would like to thank Tijana Zrnic for pointing out a reference to catalytic priors which was a catalyst for this research. 
	This research was supported by the NSF (DMS: 1944740).}
			}
	\maketitle
%} \fi


\bigskip
\begin{abstract}
The advent of Generative Artificial Intelligence (GAI) has heralded an inflection point that has changed how society thinks about knowledge acquisition.
While GAI cannot be fully trusted for decision-making, it may still provide valuable information that can be integrated into a decision pipeline.
Rather than seeing the lack of certitude and  inherent randomness of GAI as a problem, we view it as an opportunity. Indeed, variable answers to given prompts can be leveraged to  construct a prior distribution which reflects assuredness of AI predictions. This prior distribution may be combined with tailored datasets for a fully Bayesian analysis with an AI-driven prior.
In this paper, we explore such a possibility within a non-parametric Bayesian framework. The basic idea  consists of assigning a Dirichlet process prior distribution on the data-generating distribution with AI generative model as its baseline.  Hyper-parameters of the prior can be tuned out-of-sample to assess the informativeness of the AI prior.
Posterior simulation is achieved by computing a suitably randomized functional on an augmented data that consists of observed (labeled) data as well as fake data whose labels have been imputed using AI. This strategy can be parallelized and rapidly produces iid samples from the posterior by optimization as opposed to sampling from conditionals. Our method enables (predictive) inference and uncertainty quantification leveraging AI predictions in a coherent probabilistic manner.
\end{abstract}

\noindent%
{\bf Keywords:} {\em Dirichlet Process Prior, Imaginary Data, Non-parametric Bayes }
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!


\section{Introduction}
Due to its ability to synthesize information from various sources,  Generative Artificial Intelligence (GAI) is quickly becoming the   source of  knowledge for many of its users.
However,  the practical utility of GAI models largely depends on the user's understanding of their mechanistic (probabilistic) properties.  Generative models simply
produce stochastic responses to prompts, with the level of randomness influenced by both the prompt's specificity and the AI's ability and confidence in providing an accurate answer. 
 This inherent randomness raises questions about the extent to which important
decisions can be based  on a single AI output answer. We argue that  the variability in the answers themselves  offers an opportunity to generate (a) a random prior guess at the correct answer, and (b) probabilistic predictions via AI-induced distributions obtained by repeated prompting.
While our society has  resisted surrendering important decision-making to artificial intelligence,  AI predictive systems may serve as a useful primer for further analysis. 
This work explores the possibility of  
 %AI predictions   may result in misleading recommendations not only due to the  inherent randomness but also possible 
%While delegating decisions entirely to  AI intelligence has been approached with reluctance,
  articulating prior information through  AI data augmentation  for a fully Bayesian analysis.

Our setup consists of independent labeled data $\mathcal D_n= \{(Y_i,\bm X_i)\}_{i=1}^n$  which are tailored to a specific question regarding a parameter of interest $\theta_0$. For example, we will later analyze a dermatology dataset where the label is one of six distinct but closely overlapping skin conditions within the group of Erythemato-Squamous Diseases (ESDs) \citep{wang2025identifying}. While the parameter of interest $\theta_0$  may index a statistical model (including deep learning models involving high-dimensional $\theta_0$), we regard it more generally as a minimizer of a certain loss function \citep{bissiri2016general}.  Our goal is to understand  how generative AI can be used for prior elicitation to conduct  fully Bayesian inference on  $\theta_0$ as well as predictive inference on   $Y_i^*$ given $\bm X^*_i\notin\mathcal D_n$.

This work is based on the premise that generative models produce synthetic  data which can be converted into (informative) priors.  The idea of using imaginary training data for prior construction is nearly as old as Bayesian statistics itself, dating to at least Laplace in the 18th century \citep{good}.
In the context of   Bayes factor model comparisons, intrinsic priors \citep{berger1996intrinsic} result from converting an improper uninformative prior into a proper posterior on a sample of  a ``minimal" training size.
Arithmetic and geometric mean aggregates of Bayes factors under all plausible training data subsets approximately correspond to a Bayes factor under the so-called ``intrinsic prior".
The expected-posterior prior \citep{perez2002expected} is a special case  which results from averaging  posterior distributions, given imaginary data, over all imaginary data arising from a suitable predictive measure (for example predictive distribution under a simpler model or empirical distribution of the observed data).
Besides intrinsic and expected-posterior priors, data augmentation ideas for prior constructions for model determination have been explored by many others, including \cite{spiegelhalter1982bayes,gelfand1992model,laud1996predictive} or \citep{ohagan1995fractional}.
We are not necessarily concerned with objective prior  elicitation for  model selection but rather with subjective prior articulation for actual inference about $\theta_0$ under one posited model.


Only very few priors have had as much practical and theoretical impact as the Zellner's $g$-prior \citep{zellner1986assessing}.
Motivated by imaginary data, this prior adopts the covariance structure from  observed data to facilitate conjugate analysis in normal regression.  Inspired by \citep{diaconis1979conjugate},  \citep{ibrahim2000power,chen2003conjugate} propose  predictive elicitation of a proper conjugate prior for generalized linear models based on observable quantities (historical data) which serve as a prior guess at  observable outcomes. Similar data augmentation (DA) priors, that have  the same form as the likelihood, have also been studied by \citep{bedrick1996new} who considered a broader class of conditional mean priors  arguing that it is easier to elicit prior information on means of observables rather than on regression coefficients. Related ideas occurred in specialized contexts including proportional hazards regression \citep{greenland2001data}.  A more recent  incarnation of DA priors  is the catalytic prior  \citep{huang2020catalytic} that involves simulated data from a posterior predictive distribution under a simpler (donor) model trained on $\mathcal D_n$. The catalytic prior is then constructed from the, suitably down-weighted,  fake training samples so that it is conjugate with the posited model. While the catalytic prior appears conceptually related to the expected-posterior prior \citep{perez2002expected,fouskakis2018power,fouskakis2016power,neal2001transferring} as well as power-priors \citep{chen2003conjugate}, but it originates from a different set of desiderata.
With complex models and small datasets,    likelihood-based analysis can be unstable or infeasible and may  be enriched by
augmenting the observed data with imaginary data generated from a posterior predictive under a simpler model \citep{huang2020catalytic,neal2001transferring}. This results in a posterior distribution that is pulled  towards the posterior  under the simpler model, resulting in estimates and predictions with potentially better properties.  Our work has been motivated by  catalytic priors\footnote{This reference was pointed out to one of the authors by Tijana Zrnic during her visit at Booth.} in the sense that we also view  generative AI   as a posterior predictive simulator  but we approach AI prior articulation differently.
First,  we use simulations from  generative AI models trained on massive dataset that are different from the proprietary observed data $\mathcal D_n$. Catalytic priors use  simulations from posterior predictive distributions under models trained on  $\mathcal D_n$, inherently using $\mathcal D_n$ twice. Second, we consider a non-parametric Bayesian inference framework by constructing a prior  for $F_0$, the distribution function underlying independent realizations inside $\mathcal D_n$, as opposed to  a prior on $\theta_0$. Shifting focus from inference on an unknown parameter  $\theta_0$ to inference on an unknown data-generating distribution  $F_0$ allows harnessing generative AI output in a more transparent way.  Indeed,  generative AI  can be leveraged to produces imaginary data   $\mathcal D^*_m=\{(Y_i^*,\bm X_i^*)\}_{i=1}^m$ which could be regarded as samples from the prior on $F_0$. We align with  \citep{chen2003conjugate} who state that ``{\em it is easier to think of observable quantities when eliciting priors, rather than
specifying priors for regression parameters directly, since parameters are always unobserved}."  However, unlike \citep{chen2003conjugate}, we focus directly on non-parametric inference on $F_0$  which  obviates the need to commit to a particular model and  breaks the precarious codependence between the (conjugate) prior and the model. 


%Our construction is predictive in nature and focuses on observable quantities, it is based on specifying a prior prediction y0 for the response
%vector, and a scalar precision parameter a0 which quantifies one?s prior belief in y0.
Our idea is extremely simple. We view generative AI as a base distribution for a Dirichlet process prior on $F_0$ \citep{ferguson1973bayesian}.  
Inference on $\theta_0$ can be accomplished through the loss-based posterior framework of \citep{bissiri2016general,lyddon2019general, chamberlain1996nonparametric}
  where the parameter of interest $\theta_0$ is a functional of $F_0$. The Dirichlet process prior admits an embarrassingly parallel posterior bootstrap algorithm \citep{lyddon2019general,fong2019scalable} that generates independent and exact samples from the nonparametric posterior distribution over the data-generating process.
Since the AI prior on $\theta_0$ is  defined only implicitly through an AI prior on $F_0$, Markov chain Monte Carlo (MCMC) analysis that relies on sampling from conditionals is not immediately unavailable. Instead of obtaining dependent samples using MCMC, we obtain independent posterior samples by optimization of  suitably randomized objective functions along the lines of \cite{fong2019scalable}. This strategy allows us to obtain posterior distributions of functionals as well as posterior predictive distributions. These distributions can be used to report (predictive) uncertainty  in optimization-based machine learning techniques (such as random forests or deep learning) for which uncertainty quantification has been challenging or unavailable. Our work has been inspired by the prediction-powered inference framework \citep{angelopoulos2023prediction,ji2025predictions,zrnic2024cross} for constructing valid confidence intervals and  $p$-values that allows for AI systems to be used for data augmentation. {The prediction-powered inference (PPI) framework posits estimators for minimizers of convex objectives that leverage predictions from a black-box AI model on additional unlabeled data in order to aid estimation. PPI does this by making use of a rectifier which relates AI prediction error to bias and employing a strategy reminiscent of de-biasing.} Our work can be viewed as a Bayesian counterpart to this framework.

The paper is structured as follows. In Section \ref{sec:Bayesian_AI} we review  various   prior elicitation methods using AI data augmentation in likelihood-driven Bayesian inference.  Section \ref{sec:likelihood_free} presents AI prior elicitation in loss-function driven Bayesian inference. Section \ref{sec:theory} presents theory and hyper-parameter calibration strategies. Section \ref{sec:Applications} shows two real data applications including  AI-assisted disease diagnosis and astrophysics. Section \ref{sec:Discussion} wraps up with a discussion.

\section{AI-Powered Bayesian Inference}\label{sec:Bayesian_AI}
Suppose that we observe labeled data $\mathcal D_n =\{(Y_i,\bm X_i)\}_{i=1}^n$ and want to perform a supervised analysis involving a parameter of interest $\theta_0\in \Theta\subseteq\R^d$ as well as predictive inference about $Y_{new}$ given $\bm X_{new}=\bm x_{new}$ where $(Y_{new},\bm X_{new})\notin \mathcal D_n$. In addition to observations $\mathcal D_{n}$, we have access to a (stochastic) black-box predictive model $\hat{\mu}_{AI}(\bm x)$  which  generates random labels $\wt Y_i=\hat{\mu}_{AI}(\bm x)$ when prompted by $\bm x$.
One can regard $\hat{\mu}_{AI}(\bm x)$ as an implicit simulator from a predictive distribution of a complex model (e.g. a large language model underlying generative AI) trained on massive data $\mathcal{D}_{AI}$ that is unavailable to the user and different from $\mathcal D_n$.

To conduct  predictive inference, a Bayesian forecaster would typically issue a posterior predictive distribution
\begin{equation}
\pi(Y_{new}\C \bm x_{new}, \mathcal D_n)=\int \pi(Y_{new}\C \bm x_{new},\theta)\pi(\theta\C \mathcal D_n)\d\theta\label{eq:predictive_inference}
\end{equation}
 based on the posterior distribution of $\theta$ given $\mathcal D_n$
$$
\pi(\theta\C \mathcal D_n)\propto \pi(\theta)\pi(\mathcal D_n\C \theta)
$$
under a postulated model $\pi(\mathcal D_n\C \theta)$ and a chosen prior $\pi(\theta)$. 
 There are two conundrums that have given a pause to some practitioners before fully embracing Bayesian inference: (1) the specification of the prior $\pi(\theta)$, and (2) the specification of the model, e.g. $\pi(\mathcal D_{n}\C \theta)=\prod_{i=1}^n\pi(Y_i\C \bm X_i,\theta)$ for independent realizations. While Bayesians have historically faced taunts about (subjective) priors, model choice is  far more consequential in a likelihood-based analysis and should be defended  at least  as fiercely as the prior choice.
 This work  deals with (subjective) prior elicitation based on observable predictions from an AI predictive model.   The issue of model specification will be tackled using a non-parametric framework Section \ref{sec:NP_AI_priors} which allow Statisticians ``{\em to remain honest about their  ability to perfectly model the data}" \citep{lyddon2019general}.
 
In the following three subsections, we present several approaches for AI prior elicitation that could be used in the context of likelihood-driven Bayesian inference. Our main discussion will center around  nonparametric Bayesian  inference driven by loss function in Section \ref{sec:likelihood_free}.

 \subsection{Likelihood-Driven AI Priors}\label{sec:P}
 While the AI model  is a black box  predictive machine, it implicitly defines a model and a prior if one were willing to assume that $\hat{\mu}_{AI}(\cdot)$ generates  samples from a posterior predictive distribution \eqref{eq:predictive_inference} under  some label distribution $\pi_{AI}(Y\C \bm x,\theta)$, prior $\pi_{AI}(\theta)$  and a training model $\pi(\mathcal{D}_{AI}\C\theta)$.
This argument might be justifiable from the ``prequential" point of view \citep{dawid1992prequential} that focuses solely on predictive distributions (as opposed to models and priors) and argues that the quality of an inference method can truly be gauged  by the quality of its forecasts. We regard AI forecasts as a potentially useful proxy for the true unobserved outcomes.


One hypothetical (but impossible)  strategy of turning  AI knowledge into priors would be to utilize $\pi_{AI}(\theta\C \mathcal D_{AI})\propto \pi_{AI}(\theta)\pi(\mathcal{D}_{AI}\C\theta)$ as a prior $\pi(\theta)$ for   the predictive distribution $\pi(Y_{new}\C \bm x_{new}, \mathcal D_n)$ based on labeled data $\mathcal D_n$. However, we cannot directly access the parameter posterior simulator $\pi_{AI}(\theta\C \mathcal D_{AI})$ from $\hat{\mu}_{AI}(\cdot)$. What we can access, however, is imaginary data $\mathcal D^*_m=\{(Y_i^*,\bm X_i^*)\}_{i=1}^m$ consisting of  predictive imputations from $\hat{\mu}_{AI}(\cdot)$. The idea of turning such  imaginary data $\mathcal D^*_m$  into a prior  has bountiful rewards and can be approached in various different ways. We explore  data augmentation strategies for parametric priors in Section \ref{sec:Par} and for non-parametric priors in Section \ref{sec:NP_AI_priors}. These approaches
should be distinguished from martingale posteriors \citep{fong2021martingale} which also leverage predictive imputation for posterior computation but in a very different way.
Martingale posteriors are based on large sequences of missing data generated from one-step-ahead  predictive distributions that are continuously updated with newly generated data.
A posterior distribution over a parameter of interest for exchangeable observations is obtained using the Doob's theorem  by computing a functional (such as the mean) of observed data augmented with the  imputed sequence. In contrast, we do not consider predictive imputation  of an (infinite) sequence  of observations  from continuously updated posterior predictive. Rather, we  perform  imputation of a finite number of  fake observations from a  given ``predictive" distribution which does not update with newly generated imaginary data points.
 





\subsubsection{Power AI Priors}\label{sec:Par}
We align with the insight by  \cite{chen2003conjugate} that ``{\em it is much easier to elicit information about the typical outcome than to attempt the extremely difficult task of eliciting prior knowledge about $\theta$}". Transmitting prior information through data augmentation has a long history in Bayesian statistics \citep{good},  Zellner's $g$-prior being  perhaps the most prominent example.  
If we were to generate fake training data $\mathcal D^*_m=\{(Y_i^*,\bm X_i^*)\}_{i=1}^m$ (using either ${\bm X}_i^*=\bm X_i$ or by sampling with replacement from  $\bm X_i$'s), we can augment  $\mathcal D_n$ with $\mathcal D_m^*$ and apply any (Bayesian) procedure on this joint sample under some posited model $\pi(Y\C\bm X,\theta)$. The contribution of imaginary data could be possibly down-played by raising their contribution to the joint likelihood to a small power $1/\delta>0$. This is the basic premise of data-augmentation priors. The power-prior  \cite{ibrahim2000power, chen2003conjugate} is one of the early examples within the context of generalized linear models, where  $\bm X_j^*=\bm X_j$ for $1\leq j\leq m=n$  and where auxiliary labels $Y_j^*$ serve as a fixed prior guess at $Y_j$ given $\bm X_j$. Following \citep{diaconis1979conjugate},    \cite{chen2003conjugate} construct a conjugate prior  by plugging $\mathcal D_m^*$ into the likelihood of an exponential family model assuming that $Y_j^*$'s are conditionally independent, given a model parameter $\theta$. In addition, each imaginary data point $Y_j^*$ (given $\bm X_j^*=\bm X_j)$ is down-weighted by some small parameter $1/ \delta>0$ and could be interpreted as a prior prediction (or guess) for $E[Y_j\C \bm X_j]$.  Zellner's $g$-prior is a special case in normal regression  where $g=\delta$. The construction in \citep{chen2003conjugate} is related but different from data-augmentation priors of  \citep{bedrick1996new} who allow for $m$ to be different from $n$ and where $\bm X_i^*$ is not necessarily one of the $\bm X_i$'s.  Moreover,  while \cite{chen2003conjugate} assign the same weight parameter
$\delta$ to $Y_j^*$ that acts as an effective prior sample size for the prior, the framework of 
\citep{bedrick1996new} assigns a weight $w_j^*$ to each new observation $(Y_j^*,\bm X_j^*)$ where $w_j^*$ can be viewed as a possibly fractional number of observations associated with a particular $(Y_j^*,\bm X_j^*)$. We will see later in  Section \ref{sec:NP_AI_priors}  that thinking of $w_j^*$'s as random rather than fixed will correspond to a  Bayesian bootstrap style strategy.   

Power AI priors could be constructed by regarding   AI predictions $Y_j^*=\hat \mu_{AI}(\bm X_j^*)$ as arising from the same model $\pi(Y\C\bm X,\theta)$ as the data $\mathcal D_n$ using either \cite{chen2003conjugate} or  \citep{bedrick1996new} as follows:
$$
\pi_{Power}(\theta)\propto \prod_{i=1}^m \pi(Y_i^*\C\bm X_i^*,\theta)^{1/\delta}\pi_W(\theta)
$$
where $\pi_W(\theta)$ is some baseline working prior. Power priors could be   enhanced  by incorporating the randomness in $\mathcal D_m^*$. Instead of building a prior from one particular realization of $\mathcal D_m^*$, expected-posterior priors \citep{perez2002expected,fouskakis2018power,fouskakis2016power,neal2001transferring}  and catalytic priors \cite{huang2020catalytic} incorporate randomness of $\mathcal D_m^*$ but do so in different ways. We explain the differences below.


\subsubsection{Expected-Posterior AI Priors}\label{sec:EP}


The expected-posterior  AI prior along the lines of Definition 1 in \citep{perez2002expected} could be constructed as a typical power prior after margining out the imaginary data
\begin{equation}
\pi_{EP}(\theta)\propto \int \prod_{i=1}^m \pi(Y_i^*\C\bm X_i^*,\theta)\pi_W(\theta) \pi_{AI}(\mathcal D_m^*)\d \mathcal D_m^*,\label{eq:EP_AI}
\end{equation}
where $\pi_{AI}(\cdot)$ consists of first generating prompts $\bm X^*$ (possibly using observed $\bm X_j$'s)  and labels $Y^*$ from the posterior predictive distribution underlying the simulator $\hat\mu(\bm X^*)$ (as discussed at the beginning of Section \ref{sec:P}). The posterior distribution $\pi(\theta\C \mathcal D_n)$ under the prior \eqref{eq:EP_AI} corresponds to a typical joint posterior under the prior $\pi_{W}(\theta)$ after averaging out   $ \mathcal D_m^*$. Indeed, under the prior \eqref{eq:EP_AI} we have
\begin{equation}
\pi(\theta\C  \mathcal D_n)= \int \pi(\theta\C  \mathcal D_n,\mathcal D_{m}^*)\pi_{AI}(\mathcal D_{m}^*)\d \mathcal D_m^*.\label{eq:post_EP_AI}
\end{equation}
This characterization has a  practical benefit  for posterior simulation from  \eqref{eq:post_EP_AI}.
A Markov chain  $\{ \theta^{(t)}\}_{t=1}$ with a stationary distribution \eqref{eq:post_EP_AI} can be obtained  by generating a joint chain $\{(\theta^{(t)},\mathcal D_{m}^{*(t)}\}_{t=1}^T$ by first refreshing $\mathcal D_m^*$ from $\pi_{AI}(\mathcal D_{m}^*)$ at every MCMC iteration and then, given $\mathcal D_m^*$, generate $\theta^{(t)}$ from the joint posterior. 
Marginally, $\theta^{(t)}$'s would be distributed according to \eqref{eq:post_EP_AI}.
Unlike with power priors, this strategy refreshes the fake data during simulation as opposed to conditioning on them a-priori. A related approach was considered by \cite{neal2001transferring} in the context of exchangeable observations where predictions from a simple donor model, for which prior elicitation was feasible, were transferred to a more complex recipient model through imaginary data. The implications of treating the imaginary data as random as opposed to fixed were explored in \cite{kaji2023metropolis} in the context of contrastive learning for Bayesian computation using the Metropolis-Hastings algorithm.


\subsubsection{Catalytic AI Priors}\label{sec:catalytic}
In catalytic priors \citep{huang2020catalytic},  imaginary data $\mathcal D_m^*$ are generated from a Bayesian predictive distribution under a simple donor model trained on $\mathcal D_n$ for which prior elicitation was easier.  The data $\mathcal D_m^*$ are then plugged into a likelihood representing  a more complex recipient model whose parameters would be difficult to estimate using only $\mathcal D_n$.
 Formally, the catalytic version of an AI prior could be written as
\begin{equation}
\pi_{CAT, m}(\theta)\propto \left(\prod_{i=1}^m \pi(Y_i^*\C\bm X_i^*,\theta)\right)^{\alpha/m}=\exp\left\{\frac{\alpha}{m}\sum_{i=1}^m\log \pi(Y_i^*\C\bm X_i^*,\theta)\right\}\label{eq:prior_CAT}
\end{equation}
for some $\alpha>0$ which regulates the influence of the prior and where $1/m$ performs averaging over the contributions of single imaginary data points $Y_i^*$. A similar idea could be implemented using AI predictions. Unlike catalytic priors, however, generating fresh data $\mathcal D^*_m$ from an AI model precludes from the double use of data $\mathcal D_n$.  The practical implementation of Bayesian analysis with catalytic priors \eqref{eq:prior_CAT} would entail choosing $\alpha$ using some criterion and then simulating very many fake observations $m$ so that the  averaging in \eqref{eq:prior_CAT} performs satisfactory approximation to Monte Carlo integration. Indeed, as $m\rightarrow\infty$ the prior approaches
\begin{equation}
\pi_{CAT, \infty}(\theta)\propto \exp\left\{\alpha\int  \log \pi(Y^*\C\bm X ^*,\theta) \pi_{AI}(Y^*, \bm X^*)\d (Y^*,\bm X^*) \right\}.\label{eq:CAT_inf}
\end{equation}
We highlight an important difference  between \eqref{eq:CAT_inf} and the expected-posterior prior  \eqref{eq:EP_AI}. If we denote the unnormalized expressions in \eqref{eq:CAT_inf} and \eqref{eq:EP_AI} as $f_{CAT,\infty}$ and $f_{EP}$ respectively, then from the Jensen's inequality $E \log X\leq \log E X$, the
population catalytic prior satisfies $f_{CAT,\infty}(\theta)\leq f_{EP}(\theta)$ for all $\theta\in\Theta$, $\alpha\in \mathbb N$ with  $m=\alpha$, $\pi_{W}(\theta)\propto 1$, and $\pi_{AI}(\mathcal D_{m}^*)=\prod_{i=1}^m \pi_{AI}(Y^*_i, \bm X^*_i)$. The expected-posterior prior is more general and allows for information blending from a larger set of imaginary data that are not necessarily iid. In summary,  $m$ in the prior  \eqref{eq:EP_AI} actually corresponds to  the imaginary data sample size represented by $\alpha$ in \eqref{eq:prior_CAT} with $m=\infty$.  %For more general $\alpha>0$, it is a lower bound to a power expected-posterior prior by Fouskakis et al. (2015).

While the expected-posterior prior  \eqref{eq:EP_AI} allows for exact posterior simulation by updating the fake data at each MCMC simulation step by averaging out uncertainty in $\mathcal D_m^*$, catalytic priors \eqref{eq:prior_CAT} compute a different object due to the Jensen's gap. The population version actually corresponds to posteriors  obtained by augmenting a single typical imaginary distribution using estimated moments of $(Y^*,\bm X^*)$. This can be seen, for example, in Gaussian linear regression with unit variance, where  the catalytic prior based on simulations $\mathcal D_{m}^*$ would become
$$
\pi_{CAT,m}=N\left(\hat \theta,\frac{m}{\alpha}(\bm X^{*'}\bm X^*)^{-1}\right)
$$
where $\hat\theta=(\bm X^{*'}\bm X^*)^{-1}\bm X^{*'}\bm Y^* $. If we choose $\bm X^*=\bm X$ where only the labels $Y_i^*$ are subject to predictive imputation, this corresponds to the $g$-prior with  $g=m/\alpha$.
The population catalytic prior would then become $N(\theta^*,\frac{1}{\alpha}\Sigma_{X}^{-1})$, where $\Sigma_X=\lim_{m\rightarrow\infty}\frac{1}{m}\bm X^{*'}\bm X^*$ and $\theta^*=\Sigma_X^{-1}c$, where $c=\lim_{m\rightarrow\infty}\frac{1}{m}\bm X^{*'}\bm Y^*$. While the population catalytic prior plugs moments into the Gaussian prior, the expected-posterior prior is a Gaussian mixture
$$
 \int N\left(\hat \theta,\frac{m}{\tau}(\bm X^{*'}\bm X^*)^{-1}\right)\pi_{AI}(\mathcal D_m^*)\d\mathcal D_{m}^*.
$$

 All of the prior constructions in Section \ref{sec:Par}, \ref{sec:EP} and \ref{sec:catalytic}  force the observed data $\mathcal D_n$ and AI-generated data $\mathcal D_{m}^*$ into a conjugate relationship. This prescription is much stronger than just assuming that the model is well-specified  because it demands that the prior has arrived from the very same model.   We prefer  avoiding the double mis-specification (model and the prior) and will therefore focus on a non-parametric Bayesian analysis based on  AI-informed priors on $F_0$ as opposed to   $\theta_0$.

 \section{ Bayes without  the Likelihood} \label{sec:likelihood_free}

Instead of assuming that there exists $\theta_0$ such that the observed data $\mathcal D_n$ has been independently realized from $ \pi(Y \C \bm X,\theta_0)$, we adopt a non-parametric viewpoint, where the   $\mathcal D_n$  arrives from an iid experiment involving an unknown distribution function $F_0$ for $(Y,\bm X)$. Similarly as in \citep{bissiri2016general, lyddon2019general}, we shift focus from $\theta_0$ to $F_0$. 
The question of prior elicitation will  be tackled by converting observable predictions from an AI model into non-parametric priors on $F_0$.
Suppose that the unknown parameter $\theta_0$ is a solution to the optimization problem
\begin{equation}
\theta_0(F_0)=\argmin_{\theta}\int \ell(\theta,Y,\bm X)\d F_0[(Y,\bm X)],\label{parameter_loss}
\end{equation}
where $\ell(\theta, Y,\bm X)$ is a  loss function  and $F_0$ is the unknown distribution for $(Y,\bm X)$. The parameter of interest is not necessarily tied to a statistical model and is defined more generally  as a minimizer of a population loss under an unknown sampling distribution $F_0$. This parameter may correspond to an actual parameter of a statistical model if one takes $\ell(\theta, Y,\bm X)=-\log\pi(Y\C\bm X, \theta)$.  



\subsection{Gibbs AI Priors}
Bissiri et al. \citep{bissiri2016general} formalized a framework for coherent probabilistic updating  of prior beliefs $\pi(\theta)$ about $\theta_0$ from observations $\mathcal D_n$ through a functional 
$
\pi_{GP}(\theta\C \mathcal D_n)\propto \pi(\theta)\exp\left[- w \ell_n(\theta,\mathcal D_n)\right],
$
where $\ell_n(\theta,\mathcal D_n)\equiv\frac{1}{n}\sum_{i=1}^n \ell(\theta, Y_i,\bm X_i)$ and where $w>0$ is referred to as a learning rate. This so-called ``Gibbs posterior"  
corresponds to the actual posterior when $\ell(\theta, Y,\bm X)=-\log\pi(Y\C\bm X, \theta)$. Similarly as in the likelihood-driven power AI priors from Section \eqref{sec:Par}, we can incorporate $\mathcal D_m^*$ through  
\begin{equation}
\pi_{GP}(\theta)\propto \pi_W(\theta)\exp\left[- \alpha\, \ell_m(\theta,\mathcal D_m^*)\right]\label{eq:Gibbs_prior}
\end{equation}
for some working prior $\pi_W(\cdot)$ and a learning rate $\alpha>0$.  Similarly as in Section \ref{sec:EP}, one could consider an expected Gibbs posterior prior version where 
the data $\mathcal D_m^*$ is marginalized out.
Due to the coherency, the Gibbs posterior under the ``Gibbs prior" \eqref{eq:Gibbs_prior} writes as
\begin{equation}
\pi_{GP}(\theta\C \mathcal D_n,\mathcal D_m^*)\propto \pi_W(\theta)\exp\left[- w\, \ell_n(\theta,\mathcal D_n)-\alpha\, \ell_m(\theta,\mathcal D_m^*)\right]. \label{eq:Gibbs_posterior}
\end{equation}
When $r=n/m$ for some $r>0$, i.e. $m\rightarrow\infty$ as $n\rightarrow\infty$,  
 and under suitable regularity conditions on $\ell(\cdot)$ (see  e.g. \cite{Chernozhukov2003} or supplemental material of \cite{lyddon2019general}) the Gibbs posterior has the following asymptotic normal distribution  as $n\rightarrow\infty$
$$
\sqrt{n(1+1/r)}\left(\theta-\wh{\theta}_{n,m}^\alpha\right)\rightarrow z\sim \mathcal N(0,[w J_1(\theta_0)+\alpha J_2(\theta_0)]^{-1}), 
$$ 
where 
\begin{equation}
J_1(\theta )=\int\nabla^2\ell(\theta, Y ,\bm X )\d F_0[(Y,\bm X)]\quad\text{and}\quad J_2(\theta)=\int\nabla^2\ell(\theta, Y ,\bm X )\d F_{AI}[(Y,\bm X)]\label{eq:Js}
\end{equation}
where 
\begin{equation}
\wh\theta_{n,m}^\alpha=\arg\min\{w\, \ell_n(\theta,\mathcal D_n)+\alpha\, \ell_m(\theta,\mathcal D_m^*)\}\label{eq:biased_estimate}
\end{equation} 
is a potentially biased estimator of $\theta_0$. This result can be shown by simple adaptation of general Gibbs posterior theory developed earlier in \cite{Chernozhukov2003}.
While the Gibbs posterior \eqref{eq:Gibbs_posterior}  can be a useful inferential object, simulating from it using MCMC can be at least as challenging as simulating from regular posteriors (see \cite{Nie2022} and references therein). We consider a related, but computationally far more feasible, strategy
 that performs simulation through optimization of randomized objectives. Such strategies have proven useful in various contexts including high-dimensional variable selection \citep{Nie2023}.

 \subsection{Non-parametric AI Priors}\label{sec:NP_AI_priors}
 Rather than expressing prior beliefs about $\theta_0$ defined in \eqref{parameter_loss} through  $\pi(\cdot)$, we can express them through a prior $\pi(F)$ on $F_0$. This will lead to a procedure that is related conceptually but computationally  quite different compared to MCMC sampling from \eqref{eq:Gibbs_posterior}. 
Since the sampling distribution $F_0$ is unknown, we can place a Dirichlet process  (DP) prior with an AI base prior as follows
\begin{equation}
F\sim DP(\alpha, F_{AI}),\label{eq:DP_prior}
\end{equation}
where $\alpha>0$ is the usual concentration parameter and $F_{AI}$ is the base measure which can only be accessed though its  simulations $(Y_i^*,\bm X_i^*)$. 
\subsubsection{AI Base Measure}\label{subsec:AI_base}
Denote the density of this base distribution
as $f_{AI}(Y^*, \bm X^*)$  and factorize it into 
$$
f_{AI}(Y^*, \bm X^*)= f_{AI}^X(\bm X^*)\times f_{AI}^{Y}(Y^*\C \bm X^*).
$$ 
The density $f_{AI}^X(\bm X^*)$ can be viewed as a  distribution over prompts. 
For our practical illustrations, we will assume that it is based on the observed covariates, i.e. $f_{AI}^X(\bm X^*)=\sum_{i=1}^ng_i\delta_{\bm X_i}$ for some (fixed or random) weights $g_i>0$ such that $\sum_{i=1}^ng_i=1$.  Given the prompt $\bm X^*$, the density $f_{AI}^{Y}(Y^*\C \bm X^*)$ is defined implicitly by the AI generator $\hat\mu(\bm X^*)$, be it ChatGPT or any other black-box predictive model.  Perhaps the simplest way to construct $f_{AI}^{Y}(Y^*\C \bm X^*)$ would be an empirical distribution of this historical data, i.e.  $f_{AI}^{Y}(Y^*\C \bm X^*)=\frac{1}{m}\sum_{j=1}^m\delta_{(Y_j^*,\bm X_j^*)}$, where  $\mathcal D_m^*=\{(Y_j^*,\bm X_j^*)\}_{j=1}^m$ have been generated hierarchically from $\bm X_j^*\sim f_{AI}^X$ and then $Y_j^*= \hat\mu(\bm X_j^*)$. Using the log-likelihood loss function, this strategy is closely related to  the power priors discussed in Section \ref{sec:Par} that treat the historical observations as fixed. Just like with posterior-expected priors  from Section \ref{sec:EP}, however,  it might be desirable to incorporate randomness in $\mathcal D_m^*$ and treat $f_{AI}$ (or at least $f_{AI}^{Y}$) as a continuous density.   From the properties of the DP prior \citep{van1998asymptotic},  as $n\rightarrow\infty$ asymptotic consistency of the posterior \eqref{eq:DP_posterior} is achieved under certain regularity conditions regardless of the choice of $F_{AI}$ \citep{lyddon2019general}. 

   
    
\subsubsection{Posterior Bootstrap}
 The prior distribution on $\theta$ is implied by a prior distribution on $F$  in \eqref{eq:DP_prior} using the mapping \eqref{parameter_loss} where
 $$
 \theta\sim \argmin_{\theta'}\int \ell(\theta',Y,\bm X)\d F [(Y,\bm X)]\quad\text{for}\quad F\sim DP(\alpha, F_{AI}).
 $$ 
 From the conjugacy of the DP process, we see that having observed $\mathcal D_n$, the posterior on $F$ satisfies
$F\C \mathcal D_n\sim DP(\alpha+n, G_n)$ where $G_n=\frac{\alpha}{\alpha+n}F_{AI}+\frac{1}{\alpha+n}\sum_{i=1}^n\delta_{Y_i,\bm X_i}.$
The non-parametric posterior on $\theta$ can be then computed \cite{fong2019scalable}  simply by taking a functional of samples $F$ from its posterior using
\begin{equation}
 \theta\sim \argmin_{\theta'}\int \ell(\theta',Y,\bm X)\d F [(Y,\bm X)]\quad\text{for}\quad F\sim DP(\alpha+n, G_n).\label{eq:DP_posterior}
 \end{equation}
With an empirical AI base measure based on historical data $\mathcal D_m^*$,  the $t^{th}$ posterior sample $\theta^{(t)}$ can be simply computed as
\begin{equation}
\theta^{(t)}=\arg\min_{\theta'}\left[\sum_{i=1}^{n} w_j^{(t)} \ell(\theta,Y_i,\bm X_i)+\sum_{j=1}^{m} w_j^{*(t)} \ell(\theta,Y^*_j,\bm X^*_j) \right]\label{eq:DP_posterior_finite}
\end{equation}
 where $w_j^{(t)}$ and $w_j^{*(t)}$  are DP-posterior implied weights whose refreshment  induces a posterior for $\theta$. With a continuous base prior $f_{AI}^{Y}$  for the labels $Y^*$, the second sum in \eqref{eq:DP_posterior_finite} is infinite and approximate computation is required. One possibility is to perform approximate sampling from the DP posterior using the Posterior Bootstrap Algorithm  (Algorithm 2 in \cite{fong2019scalable} outlined in Algorithm~\ref{alg:PB}). The idea is to assign a random weight to each observation in $\mathcal D_n$ and $\mathcal D_m^*$  perform repeated optimization of the randomized objective through \eqref{eq:DP_posterior_finite}.  The posterior distribution is induced by uncertainty in $F$  and, since $\theta$  is a functional of $F$, we can obtain posterior distribution for a wider class of parameters $\theta$ than possible within a classical likelihood-based Bayesian analysis \cite{chamberlain1996nonparametric}. This  could be viewed as one possible  Bayesian approach to M-estimation.
 
 
\begin{algorithm}[!t]
\caption{Posterior Bootstrap}\label{alg:PB}
\begin{algorithmic}[1]
\Require Input observed data $\mathcal D_n$, concentration $\alpha>0$ and approximation truncation  $m$.
\For{$t \gets 1$ to $B$}
    \State Draw imaginary data $\mathcal D_m^*=\{(Y_i,\bm X_i^*)\}_{i=1}^m$ from $f_{AI}$ defined in Section \ref{subsec:AI_base}.
    \State Draw weights $(w_{1:n}^{(t)},w_{1:m}^{*(t)} )$ from $\mathrm{Dir}(1,\dots, 1,\alpha/m,\dots, \alpha/m)$.
    \State Compute $\theta^{(t)}$ from \eqref{eq:DP_posterior_finite}.
\EndFor
\State \Return Posterior Bootstrap sample $\{\theta^{(t)}\}_{t=1}^B$.
\end{algorithmic}
\end{algorithm}

Having obtained samples  $\{\theta^{(t)}\}_{t=1}^B$ through optimization over a dataset consisting of observed and fake labeled data, we can 
proceed with inference (uncertainty quantification) on $\theta_0$ defined in \eqref{parameter_loss} or  posterior predictive inference as follows.
For a likelihood-based loss function $\ell(\theta, Y,\bm X)=-\log\pi(Y\C\bm X, \theta)$, the predictive distribution for $Y_{new}$ given $X_{new}$ could be computed
 though \eqref{eq:predictive_inference} as
 $$
 \pi(Y_{new}\C \bm X_{new})=\frac{1}{B}\sum_{t=1}^B \pi(Y\C\bm X, \theta^{(t)}).
 $$
For example, predicting $Y_{new}\in\{1,\dots C\}$ from $\bm X_{new}$ using a deep learning (DL) classification model with class probabilities $f_\theta(\cdot) \equiv [f_\theta^1(\cdot),\dots, f_\theta^C(\cdot)]$ parametrized by DL weights  $\theta$, we could obtain the non-parametric posterior for $f_\theta$ under the AI prior  using Posterior Bootstrap. The predictive distribution of 
$P[Y_{new}=c\C \bm X_{new},\mathcal D_n]$ for the new label would then be the posterior-averaged class probability $\frac{1}{B}\sum_{t=1}^B f_{\theta^{(t)}}^c(\bm X_{new}).$
  
  
The practical utility of the bootstrap posterior for inference on $\theta_0$ can be gauged from its asymptotic distribution.


\section{Theory}\label{sec:theory}

Consider data $Y_{1},\ldots,Y_{n}\sim F_{0}$ and a base measure $F_{AI}$. If $F_{AI}$ is a mixture of $m$ point masses, we denote these by $Y^{*}_{1},\ldots,Y^{*}_{m}$. Otherwise, we let $Y^{*}_{1},\ldots,Y^{*}_{m}\sim F_{AI}$ for some suitably large $m$ as in the posterior bootstrap algorithm. Since we are considering asymptotic behavior, we let $\alpha$ depend on $n$ and fix $\alpha=\gamma n$ for some constant $\gamma>0$. We define the oracle risk minimizer
\begin{equation}
  \theta_{0}^{\gamma} = \argmin_{\theta}\int \ell(\theta,Y)\,\d F_{0}(Y) + \gamma \int \ell(\theta, Y)\,\d F_{AI}(Y)\,.
  \label{eq:oracle-riskmin}
\end{equation}
noting its dependence on $\gamma$. Similarly, we define the empirical risk minimizer
\begin{equation}
  \wh{\theta}^{\alpha}_{n} = \argmin_{\theta} \sum_{i=1}^{n} \ell(\theta,Y_{i}) +\frac{\alpha}{m}\sum_{j=1}^{m} \ell(\theta,Y^{*}_{j})
  \label{eq:empirical-riskmin}
\end{equation}

Define the generalized information matrix $I:\Theta \to \R^{p\times p}$ by
\[
I(\theta) = I_{1}(\theta) + \gamma I_{2}(\theta)
\]
where
\[
I_{1}(\theta) = \int \nabla \ell(\theta,Y) \nabla \ell(\theta,Y)^{\top} \,\d F_{0}(Y)\,,\qquad I_{2}(\theta) = \int \nabla \ell(\theta,Y) \nabla \ell(\theta,Y)^{\top} \,\d F_{AI}(Y).
\]


\begin{theorem}\label{thm:PB-appendix}
\sloppy Let $\theta^{*}$ be the posterior bootstrap sample  obtained from Algorithm~\ref{alg:PB} and denote with $\Pi_{PB}$ its probability measure. For simplicity, consider the base measure $F_{AI}$ to be atomic with $m$ atoms\footnote{When $F_{AI}$ is absolutely continuous with respect to Lebesgue measure and we approximate it with a finite truncation of $m$ samples, as in the posterior bootstrap algorithm, the same results hold but with $F_{AI}$ replaced by the empirical distribution of the $m$ drawn samples.}. Under regularity conditions, for any Borel set $A\subset\Theta\subseteq \R^d$ with $\alpha=\gamma n$ as $n\rightarrow\infty$ we have
$$
\Pi_{PB}\left[\sqrt{n(1+\gamma)} \left(\theta^*-\wh\theta_{n}^\alpha\right)\in A\right]\rightarrow P(z\in A)
$$
$\mathcal D_n$-almost surely   where $\wh\theta_{n}^\alpha$ is the empirical risk minimizer \eqref{eq:empirical-riskmin} and where $z\sim N\left(0, J(\theta_0)^{-1}I(\theta_0)J(\theta_0)^{-1}\right)$ with $J(\theta)=J_1(\theta)+\gamma J_2(\theta)$ and $I(\theta)=I_1(\theta)+\gamma I_2(\theta)$ where  (denoting $\nabla$ the gradient operator with respect to $\theta$)
\begin{equation}
J_1(\theta )=\int\nabla^{2} \ell(\theta, Y )\d F_0(Y) \,,\qquad J_2(\theta)=\int\nabla^{2}\ell(\theta, Y )\d F_{AI}(Y).\label{eq:Js}
\end{equation}
and
\begin{equation}
I_1(\theta )=\int\nabla \ell(\theta, Y )\nabla \ell(\theta, Y )^T\d F_0(Y) \,,\qquad I_2(\theta)=\int\nabla\ell(\theta, Y )\nabla \ell(\theta, Y )^T\d F_{AI}(Y).\label{eq:Is}
\end{equation}
\end{theorem}

 

% prev - before modification
% \begin{theorem}\label{thm:PB}
% Let $\theta^{*}$ be the posterior bootstrap sample  obtained from Table \ref{alg:PB} and denote with $\Pi_{PB}$ its probability measure. Under regularity conditions, for any Borel set $A\subset\Theta\subseteq \R^d$ with $n/m=r$ as $n\rightarrow\infty$ we have
% $$
% \Pi_{PB}\left[\sqrt{n(1+1/r)} \left(\theta^*-\wh\theta_{n,m}^\alpha\right)\in A\right]\rightarrow P(z\in A)
% $$
% $\{\mathcal D_n,\mathcal D_m^*\}$-almost surely   where $\wh\theta_{n,m}^\alpha$ is the empirical risk minimizer~\eqref{eq:biased_estimate} with $\omega=1$ and where $z\sim N\left(0, J(\theta_0)^{-1}I(\theta_0)J(\theta_0)^{-1}\right)$ with $J(\theta)=J_1(\theta)+\alpha J_2(\theta)$ as defined in \eqref{eq:Js} and $I(\theta)=I_1(\theta)+\alpha I_2(\theta)$ where  (denoting $\nabla$ the gradient operator with with respect to $\theta$)
% \begin{equation}
% I_1(\theta )=\int\nabla \ell(\theta, \cdot )\nabla \ell(\theta, \cdot )^T\d F_0 \quad\text{and}\quad J_2(\theta)=\int\nabla\ell(\theta, \cdot )\nabla \ell(\theta, \cdot )^T\d F_{AI}.\label{eq:Js}
% \end{equation}
% \end{theorem}

\proof{Appendix~(See Section \ref{sec:proof}). The result follow the same process as Theorem 1 in \cite{lyddon2019general} whose proof hinges on Theorem 7 and Chapter 3 in \cite{Newton1991} where all the regularity conditions are stated.}
% {\color{red} Sean, can you please verify that this theorem is correct? Also the Gibbs posterior limiting distribution I derived earlier. It might be nice to have a formal proof if you could write it down. It might not be necessary though if it is somewhat obvious.} {\color{purple} In appendix}

It it curious to compare the asymptotic distribution of posterior bootstrap and the Gibbs posterior \eqref{eq:Gibbs_posterior}. As noted earlier by \cite{bissiri2016general} in the context of loss-likelihood bootstrap, the centering is the same but the covariance matrices are different when the loss function is not the usual log-likelihood in which case the bootstrap supplies the usual sandwich covariance matrix.  
The centering $\wh\theta_{n}^\alpha$ of the asymptotic distribution may be a biased estimator of $\theta_0$ when the prior influence does not vanish as $n\rightarrow \infty $ (i.e. when $\alpha=\gamma n$ and $\gamma\geq 0$) and when the AI algorithm provides predictions that are systematically biased.  Indeed, while in the parametric Bayesian framework the prior influence  vanishes as $n\rightarrow\infty$, in Theorem \ref{thm:PB-appendix} we allow for $\alpha\rightarrow\infty$ yielding a possibly biased centering with the amount of bias determined by $\gamma>0$. The prediction-powered inference framework \citep{angelopoulos2023prediction,Angelopoulos2023} estimates the severity of the  bias on the labeled data $\mathcal D_n$ by comparing observed labels $Y_i$ to the AI-predicted ones $\wh\mu(\bm X_i)$. We could apply a de-biasing strategy similar to theirs in order to obtain a centering that is unbiased for $\theta_0$.

\subsection{The Concentration Parameter $\alpha$}\label{sec:alpha}
The concentration parameter $\alpha>0$ measures the assuredness of the prior about $F_{AI}$ which can be interpreted as the effective sample size of the imaginary data $\mathcal D_m^*$.
This can be seen from the characterization of the posterior in \eqref{eq:DP_posterior}. While $m$ is the actual sample size for $\mathcal D_m^*$, we treat it more as a truncation parameter in an approximation to the DP posterior where (similarly as for the catalytic priors in Section \ref{sec:catalytic}) the larger $m$ is, the better. We can choose $\alpha$ adaptively from out-of-sample experiments to determine the relevance of the AI non-parametric prior for prediction and to find the most suitable degree of AI prior subjectivity.

\subsubsection{Calibration via Coverage}
{Another option is to choose $\alpha$ in order to calibrate the coverage of posterior credible intervals in the frequentist sense. One way to do this would be via an adaptation of the general posterior calibration algorithm of \citep{syring2018calibration}. Given access to mechanism to repeatedly accrue samples of size $n$ from the data-generating process, as well as knowledge of a true parameter of interest $\theta^{*}$, one would be able to choose $\alpha$ such that a $1-\delta$ posterior credible interval arising from the $\alpha$-AI prior has frequentist coverage at level $1-\delta$. This would be approximated by repeatedly sampling datasets $\mathcal{D}_{n}$, computing the $1-\delta$ credible region, and determining the proportion of times that the true parameter lies within. The practitioner would then want to choose the largest value of $\alpha$ such that the $1-\delta$ credible interval has frequentist coverage at level $1-\delta$, maximizing the informativeness of the prior under the constraint of well-calibrated posterior credible regions. Of course, practitioners do not have access to $\theta^{*}$ or the ability to generate new data samples. Adapting the general posterior calibration algorithm~\citep{syring2018calibration}, we can replace sampling independent datasets with bootstrapping datasets of size $n$ from the empirical distribution of the actual sample $\mathcal{D}_{n}$. Similarly, knowledge of $\theta^{*}$ is replaced with the empirical risk minimizer on the bootstrapped dataset. One can then solve for $\alpha$ using the same criterion-- the largest value of $\alpha$ such that the estimate of the coverage arising from the bootstrapped samples is atleast $1-\delta$.
}
  


\subsubsection{Asymptotic Calibration}

Adaptive tuning has been also considered in the context of prediction-powered inference by \cite{Angelopoulos2023} who consider a weighted average of loss functions for estimating $\theta_0$ with the weight chosen adaptively from data to minimize the Fisher information number, i.e. trace of the inverse  asymptotic covariance matrix. This weight calibration is related to the one considered in \cite{bissiri2016general} who calibrate a weight  of the Gibbs posterior by matching the asymptotic covariance matrices of the Gibbs posterior and the loss-likelihood bootstrap. 
While these calibrations are ultimately asymptotic as $n\rightarrow\infty$, we could consider a similar strategy to find $\alpha$ that minimizes the trace of an estimate of $J(\theta_0)^{-1}$ in Theorem~\ref{thm:PB-appendix}. This estimate could be obtained by replacing $J_0(\cdot)$ and $I_0(\cdot)$ with their finite-sample counterparts and replacing $\theta_0$ with $\wh \theta_n$. Defining $\hat{\Sigma}(\alpha)$ to be the estimate of the asymptotic covariance, this implicitly defines a function $\alpha\mapsto \hat{\Sigma}(\alpha)$ which can be solved for $\alpha$ when equated to a reasonable target.
In many cases, such a target can be identified from the asymptotic marginal variances used to construct confidence intervals for the PPI estimator \citep{angelopoulos2023prediction}. Indeed, defining $\hat{\sigma}^{2}_{n,N,j}$ to be the $j$th component of the $p$-dimensional parameter of interest, we can solve the equation $\mathrm{tr}\left(\wh{\Sigma}(\alpha)\right)^{-1}=\sum_{j=1}^{p}\hat{\sigma}^{2}_{n,N,j}$ to find a value of $\alpha$ such that the size of the credible intervals are calibrated relative to PPI. In our experiments, we use both of the aforementioned strategies for eliciting values of $\alpha$ for the DP prior, and generally find relatively compatible results (refer to Section~\ref{sec:additional-experiments}).


We can calibrate the DP prior parameter $\alpha$ via the asymptotic covariance in Theorem~\ref{thm:PB-appendix}. Define $\Sigma(\gamma)=J(\theta_{0}^{\gamma})^{-1}I(\theta_{0}^{\gamma})J(\theta_{0}^{\gamma})^{-1}$. For any estimand $\theta$ that is a convex risk minimizer, we let $\Sigma^{PPI}_{n}$ be the asymptotic covariance for a sample of size $n$ used to construct PPI confidence intervals~\citep{angelopoulos2023prediction}. In practice, the true risk minimizer $\theta_{0}^{\gamma}$ as well as population quantities $J(\theta_{0}^{\gamma})$,$I(\theta_{0}^{\gamma})$, are not available. We can estimate the asymptotic covariance using the empirical versions of the information matrices and the empirical risk minimizer as
\[
\wh{\Sigma}(\alpha)=J_{n}^{\alpha}(\wh{\theta}_{n}^{\alpha})^{-1}I_{n}^{\alpha}(\wh{\theta}_{n}^{\alpha})J_{n}^{\alpha}(\wh{\theta}_{n}^{\alpha})^{-1}\,.
\]
Theorem C.1 of~\citet{angelopoulos2023prediction} provides an asymptotically valid confidence interval for the PPI estimator for estimands arising from nondegenerate convex optimization problems. For a $p$-dimensional estimated $\theta^{*}$, the theorem shows marginally that for each component $j$, we have, in their notation,
\[
\sqrt{N} \left(\wh{\Delta}_{\theta^{*}, j} + \wh{g}^{f}_{\theta^{*},j} - \E\left[\wh{\Delta}_{\theta^{*},j}-\E[\wh{\Delta}_{\theta^{*},j}]\right]\right) \overset{d}{\to} N\left(0, \frac{1}{p}\sigma^{2}_{\Delta,j}(\theta^{*})+\sigma^{2}_{g,j}(\theta^{*})\right)\,.
\]
Indeed, this asymptotic marginal variance can also be approximated by plugging in an estimate for $\theta^{*}$, and replace population quantities with their empirical versions.

We can proceed to calibrate our DP prior parameter $\alpha$ by equating the trace of the estimated asymptotic covariance of our posterior bootstrap samples with the estimated asymptotic covariance that implicitly is used to construct the asymptotic PPI confidence interval. This yields the equation
\[
\frac{\mathrm{tr}(\wh{\Sigma}(\alpha))}{n+\alpha} = \sum_{j=1}^{p} \frac{\wh{\sigma}^{2}_{\Delta,j}(\hat{\theta})}{n} + \frac{\wh{\sigma}^{2}_{g,j}(\hat{\theta})}{m}
\]
which can be solved in terms of $\alpha$ via an iterative root-finding algorithm.







% {\color{red} Sean, I wonder if you could derive this estimator by minimizing the trace of the inverse in Theorem 1. I also think that $\theta_0$ should be replaced by the unbiased estimator $\wh\theta_n$ as opposed to $\wh\theta_{n,m}^\alpha$. Let me know what you think.

% It seems that $\alpha$ should solve something like
% $$
% (1+\alpha g)^2(2 A_{21}+2\alpha A_{22})=A_{11}+A_{21}^C+A_{22}^C
% $$
% where $g=tr(I_2 I_1^{-1})$ and $A_{ij}=tr(J_i I_1^{-1}J_j)$ and $A_{ij}^C=tr(J_iCJ_j)$ with $C=I_1^{-1}I_2 I_1^{-1}$.
% I might be wrong.
% }

\section{Generative AI Illustrations}\label{sec:Applications}
We demonstrate our approach on two classification datasets, where generative AI input could be incorporated in predictive inference for medical diagnosis or parameter inference in labeling massive galaxy images. 
\subsection{Skin Disease Prediction}
%We use multi-category random forests for predictive inference. This means that we will integrate predictions out-of-sample over the bootstrap posterior obtained on combined observed and fake data.

We apply our methodology towards the classification of Erythemato-Squamous diseases from descriptions of clinical symptoms. Erythemato-Squamous diseases (ESDs) comprise a group of six distinct but closely overlapping skin conditions that pose significant diagnostic challenges due to their similar clinical and histopathological features. Machine learning approaches have been applied to predict the disease subtype from these clinical and histopathological features with high accuracy, additionally providing interpretable patterns \citep{wang2025identifying}. This dataset has also been employed for exploring uncertainty quantification in large language model-based medical diagnosis. \citet{kim2024adaptive} used ChatGPT to diagnose ESDs from descriptions of clinical features only, applying conformal prediction techniques to aid in uncertainty quantification. Notably, ChatGPT's diagnoses from clinical symptoms only were less accurate than that of bespoke machine learning algorithms (i.e. a simple random forest model, for example), but still substantially better than random guessing.

ESDs are divided into the following six subtypes, which are the labels in this classification problem: {\em psoriasis, seborrheic dermatitis, lichen planus, pityriasis rosea, chronic dermatitis and pityriasis rubra pilaris}. There are twelve clinical features, ten of which are ordinal variables that take values in the set $\{0,1,2,3\}$, describing levels of prevalence. These features include things like redness, itchiness, or incidence in certain regions of the body. The other variables are family history (binary) and age (integer valued).  {Clinical features are those based on observable signs and symptoms that can be identified through physical examination and patient history, in contrast to histopathological features which are typically examined under a microscope after a biopsy. %We denote the feature space by $\mathcal{X}$.
}

Through our framework of generative AI priors, we leverage pre-trained large language models (ChatGPT) as complementary diagnostic tools to enhance the predictive capabilities of traditional machine learning systems at classifying skin disease diagnoses correctly.

\subsubsection{Data}

We analyze the dermatology data\footnote{Available at \url{https://archive.ics.uci.edu/dataset/33/dermatology}} available from the UCI machine learning repository \citep{dermatology_33}, removing histopathological features so as only to diagnose disease from clinical features. For this experiment, we split the total number of observations in the dataset (366) as follows: 20\% is used as training data, treated as correctly labeled pairs. 20\% is held-out to assess test accuracy. The remaining 60\% is considered to be extra unlabeled data, for which the clinical symptoms are known to the practitioner but the labels are not. We let $\mathcal D_n$ denote the labeled training data, $  {\mathcal D}^{Test}_{T}$ the labeled testing data and denote the extra unlabeled data as $\mathcal D_m^*$.
%For each unlabeled observation $\bm X^*_{i}$ for $i=1,\ldots,m$, we impute $R$ labels $\tilde{Y}_{i}^{(1)},\ldots,\tilde{Y}_{i}^{(R)}$ by sampling from the distribution on labels induced by querying ChatGPT 4o to assign a diagnosis to this set of features. {\color{red} this is different than using $m$ random samples from empirical X and then querying. Can we try this approach instead?} Yes -- i'm updating accordingly

\subsubsection{Prompting ChatGPT to Impute Diagnoses}\label{subsec:prompting}
As discussed in Section \ref{subsec:AI_base}, the base measure $F_{AI}$ for our AI prior is characterized by a probability distribution on both clinical features $\bm X$ and labels $Y$. In this case, we define such a base measure as follows: the marginal distribution of clinical features is from the empirical distribution of extra unlabeled data, that is, $f_{AI}^X(\bm X^*)=\frac{1}{n}\sum_{i=1}^n \delta_{\bm X_i}(\bm X^*)$. {Then, the conditional AI prior on the labels is given by the GPT-imputed conditional distribution on the feature $\bm{X}$ using the strategy  described below.


 

In order to convert use ChatGPT to predict labels (diagnoses) from clinical features, we first convert the set of features to a prompt. We elicit these responses from the GPT-4o language model, using a temperature setting of 0.7. For this experiment, we used prompts with the following general format\footnote{The prompt shown here is slightly simplified, excluding instructions on how to format the output. We give the exact verbatim prompt in Section~\ref{sec:prompt}. }

\prompt{
\textbf{Prompt:} Predict the diagnosis of Erythemato-Squamous disease from the following clinical features. The age feature simply represents the age of
    the patient. Family history is a binary variable. Every other feature was given a degree in the range of 0 to 3. Here, 0 indicates that the feature was not present, 3 indicates the largest amount possible, and 1, 2 indicate the relative intermediate values.

    erythema: 2, scaling: 2, itching: 3, [...], age: 55

	[...] Estimate the probability of each possible diagnosis for this case [...]

\textbf{GPT-4o:} psoriasis: 0.45, lichen planus: 0.20,  [...]
}


We parse the textual response into a probability distribution on classes using regular expressions. We employ an additional normalizing step to ensure the given probabilities sum to one, though it is almost never necessary. We take the class with largest probability as the AI-predicted label, breaking ties uniformly at random if necessary.

% the empirical distribution of the $R$ GPT-imputed labels {\color{red} I think you should do m random samples not R samples for each i} that correspond to the given clinical feature value.

\begin{figure}
    \centering
	\resizebox{0.75\textwidth}{!}{\input{fig/plot.pgf}}
    \caption{Classification accuracy of ESD on held-out test data, using a neural network trained on $n=58$ observations. The line indicates  mean performance after 10 repetitions. Horizontal lines indicate the test performance of a fitted neural network fit on training data only, and ChatGPT imputations. }
    \label{fig:skin-disease-1}
\end{figure}



\subsubsection{Non-parametric AI Bayesian Inference}\label{subsubsec:np-ai-bayes}
For this data, we posit a parametric model for the conditional probabilities of each label via a three-layer neural-network parameterized vector function $f_\theta:\mathcal{X}\to \mathcal{S}^{6}$, where $\mathcal{S}^{6}:=\{v\in \mathbb{R}^{6}:\sum_{i=1}^{6}v_{i}=1,\,v_{i}\geq 0 \,\forall i=1,\ldots,6\}$ denotes the simplex on $6$-elements. {The architecture of the neural network is rather uncomplicated and is described below.

The neural network parameterizes a class of functions $f_{\theta}:\mathcal{X}\to \mathcal{S}^{K}$ through the following function composition
\[
f_{\theta} = \mathrm{softmax}\circ f_{\bm{W}_{3},\bm{b}_{3}} \circ \sigma \circ f_{\bm{W}_{2},\bm{b}_{2}}\circ \sigma \circ f_{\bm{W}_{1},\bm{b}_{1}}
\]
where $\theta=(\bm{W}_{1},\bm{b}_{1},\bm{W}_{2},\bm{b}_{2},\bm{W}_{3},\bm{b}_{3})'$, and $\mathbf{W}_1 \in \mathbb{R}^{64 \times 12}, \mathbf{b}_1 \in \mathbb{R}^{64}$, $\mathbf{W}_2 \in \mathbb{R}^{32 \times 64}, \mathbf{b}_2 \in \mathbb{R}^{32}$, $\mathbf{W}_3 \in \mathbb{R}^{6 \times 32}, \mathbf{b}_3 \in \mathbb{R}^{6}$. In addition, $\sigma$ denotes the ReLU activation function $\sigma(x)=\max\{0,x\}$, $f_{W,b}$ denotes the affine transformation $f_{\bm{W},\bm{b}}(x)=\bm{W}x+\bm{b}$, and $\mathrm{softmax}$ is defined via
\[
\text{softmax}(\mathbf{z})_i = \frac{\exp(z_i)}{\sum_{j=1}^6 \exp(z_j)}\,.
\]
The neural network parameters are fit using the Adam optimizer to minimize the weighted cross-entropy loss, with a learning rate of 0.001. We also employ dropout with $p=0.2$ during training.
The clinical symptom covariates are first preprocessed by standardizing the age feature (by subtracting its mean and dividing by the unbiased estimate of its standard deviation).

  Our inferential target is the minimizer of the induced empirical classification loss on the neural network weights $\theta$. The Posterior Bootstrap  distribution  $\{\theta^{(t)}\}_{t=1}^B$ obtained from Algorithm~\ref{alg:PB} induces a posterior distribution on $f_\theta(\cdot)$ and thereby also posterior predictive distribution on the label $Y_{j}$ corresponding to test data $\bm X_{j}\in \mathcal D_{T}^{Test}$ for $1\leq j\leq T$. The final prediction of the label will be the majority vote
\begin{equation}
\widehat Y_{j}=\arg\max\limits_{c\in \{1,\dots, 6\}} \frac{1}{B}\sum_{t=1}^Bf_{\theta^{(t)}}^c(\bm X_{j})\label{eq:class}
\end{equation}
where $f_{\theta^{(t)}}^c(\cdot)=P[Y=1\C \theta^{(t)},\cdot]$.  We evaluate the performance of this classification rule through the estimated misclassification rate $P[Y_{j}\neq \widehat Y_{j}]$ from $T$ out-of-sample observations.  

We approximately sample $B=100$ samples from the posterior predictive distribution on the label of each held-out test observation using the posterior bootstrap algorithm, using a truncation size of $m=100$. This essentially materializes as the following: for each posterior sample $t=1,\ldots,B$, we fit the neural network using a weighted loss induced by the AI prior, and classify each test point using \eqref{eq:class}.  We fit the neural network using the Adam optimizer with a finite maximum number of epochs, which we note adds an additional degree of approximation to the posterior sampling procedure.  

We employ this procedure for a range of $\alpha$ values (the concentration parameter in the Dirichlet Process AI prior discussed in Section~\ref{sec:alpha}), and repeat it for ten repetitions. Figure~\ref{fig:skin-disease-1} displays the average out-of-sample classification accuracy as a function of the concentration parameter $\alpha$. The blue dashed horizontal line indicates the  classification accuracy of a classification rule $\widehat Y_{j, DL}=\arg\max\limits_{c\in \{1,\dots, 6\}} f_{\hat \theta}^c(\bm X_{j})$, where $\hat\theta$ has been estimated purely on the training data. {Due to stochasticity in predictions from the optimization procedure (see Section~\ref{subsubsec:np-ai-bayes}), the line indicates the average accuracy over ten such estimations of $\hat\theta$.}   
% The classification accuracy has been averaged over   ten repetitions of estimating the parameters { {\color{red} why did you resample the training data? It should be a deterministic prediction} to incorporate stochasticity in fitting the model).
Employing our specified ChatGPT-powered AI prior, we see that a range of relatively small $\alpha$ values leads to posterior predictive distributions that are more accurate than predictions that arise when simply excluding the additional unlabeled data.

Interestingly, we note in this case that for $\alpha>10$, the classification accuracy is diminished by the AI prior. Since $\alpha$ can be interpreted as an effective sample size, this means that when we have more than $\approx 18\%$ fake observations, the performance worsens.  As shown in Figure~\ref{fig:skin-disease-1}, the baseline performance of ChatGPT imputations, {computed as the average accuracy taken over ten repetitions using GPT-4o to classify each test point. That is, we repeat the following procedure ten times: impute $\hat{Y}_{j}=\hat{\mu}(\bm{X}_{j})$ for each test point indexed by $j$ and compute the test accuracy.}   hovers at around 60\% classification accuracy, and the majority vote accuracy of the AI posterior shrinks to this level as $\alpha$ grows towards the value of $n=58$. As $\alpha$ becomes much larger than $n$, the performance further degrades. This indicates that predictive Bayesian analysis based on  augmented  data consisting of more imaginary than observed values does worse than baseline ChatGPT point predictions. {The gray dashed line in Figure~\ref{fig:skin-disease-1} indicates the performance for $\alpha=0$, where the procedure boils down to the Bayesian bootstrap (Algorithm 2 in \citep{lyddon2019general}) where we obtain uncertainty quantification based on only $\mathcal D_n$. The best out-of-sample prediction error was achieved for $\alpha=1.0$ which yielded a $5\%$ increase in prediction accuracy over a procedure that does not use the fake data (with $\alpha=0$). This increase is notable as the AI predictions themselves are not of significantly high quality.}


% {\color{red} Can we try $\alpha=0?$} For $\alpha=0$, this procedure boils down to the Bayesian bootstrap (Algorithm 2 in \citep{lyddon2019general}) where we obtain uncertainty quantification based on only $\mathcal D_n$. {\color{red} Maybe the horizontal line on your plot should be the average misclassification with $\alpha=0$ instead.} Done!

% {\color{red} We should conclude with a statement like....
% The best out-of-sample prediction error was achieved for $\alpha=?$ which yielded $??\%$ increase in prediction accuracy over a procedure that does not use the fake data (with $\alpha$).
% Can you fill in the exact numbers?} Done


\subsection{Proportion of Spiral Galaxies}\label{subsec:proportion-of-spiral}

Prior work has collected human annotations of galaxy morphologies through the Galaxy Zoo 2 citizen science initiative \citep{willett2013galaxy}, which contains over 1.3 million labeled images from the Sloan Digital Sky Survey. \citet{angelopoulos2023prediction} estimate the proportion of galaxies exhibiting spiral arm features, which is useful for understanding stellar evolution and star formation.
The setting is that the practitioner has access to a small number $n\leq 1000$ of human-labeled data (galaxy images pair with human annotations), and a large quantity $N \approx 1.5\times 10^{4}$ of unlabeled galaxy images. A computer vision model is leveraged to impute the labels of these data points. {For the sake of our experiment, we have access to the true labels of the $N$ data points as well, which we additionally use to estimate $\theta^{*}\approx 0.26$ as the ``true mean proportion of spiral galaxies. However, we use knowledge of $\theta^{*}$ purely for validation, and do not use it nor the true labels of the $N$ computer-vision imputed data points for our analysis.} 


We adapt this setting to our AI prior framework, seeking to estimate the proportion of spiral galaxies in the universe. However, rather than using the AI-generated labels on additional data to debias an estimator \citep{angelopoulos2023prediction}, we perform a Bayesian inference on the unknown proportion of spiral galaxies leveraging the AI-predictions to elicit our prior knowledge.

\begin{figure}
    \centering
	\resizebox{0.9\textwidth}{!}{\input{fig/galaxies-alpha.pgf}}
    \caption{90\% Credible intervals for estimating the proportion of spiral galaxies. The left plot visualizes a credible interval from our method, compared with 90\% confidence intervals around the classical and PPI estimator. The orange bar displays a confidence interval around the classical estimator when all imputed data is treated as real data. The right plot displays the width of credible/confidence intervals as a function of the labeled training data size $n$. }
    \label{fig:galaxy}
\end{figure}


\subsubsection{AI Priors on the Proportion of Spiral Galaxies}\label{subsubsec:aipriorspiral}

Suppose we have galaxy images $\bm X_{1},\ldots,\bm X_{n}$ with human annotations of their spirality $Y_{1},\ldots,Y_{n}$. Additionally, we have unlabeled galaxy images $\bm{X}^*_{1},\ldots,\bm{X}^*_{N}$ with imputed labels ${Y}^*_{1},\ldots,{Y}^*_{N}$ produced via a large computer vision model. We take the nonparametric approach in Section~\ref{sec:NP_AI_priors} and define an AI base measure $F_{AI}$ by the empirical distribution on the labels ${Y}^*_{1},\ldots,{Y}^*_{N}$ imputed by the computer vision model. This means that we do not have an underlying continuous base measure and can do an exact algorithm without refreshing the labels for a fixed sub-sample of size $m$.  We elicit our AI prior in turn as previously, via $F\sim\mathrm{DP}(\alpha,F_{AI})$ where $F_{AI}$ is now finitely supported on ${Y}^*_{j}$'s.  We still use the approximate algorithm in Algorithm~\ref{alg:PB} using truncation $m=10^{5}$, resampling from the empirical distribution of the atoms that make up the AI base measure here (the set of computer vision-imputed labels).}

 
\subsubsection{Nonparametric AI Inference on the Mean}

Our inferential conclusions on the proportion of spiral galaxies stem from the posterior on the risk minimizer $\theta(F)$, defined via $\theta(F)=\argmin_{\theta}\int (y-\theta)\,\mathrm{d}F(y)$. We obtain $B=1000$ samples from the approximate posterior distribution of $\theta (F)$ using the exact variant of the posterior bootstrap algorithm in Algorithm~\ref{alg:PB}. We repeat this procedure 10 times each for various values of the DP concentration parameter $\alpha$ in the AI prior. Figure~\ref{fig:galaxy} displays the sizes of a single 90\% credible interval for varying $\alpha$ values, as well as the size of the 90\% confidence interval for the classical sample mean (using only the human labeled data), the PPI estimator, and the sample mean when treating the imputed labels as real. The right hand side of the plot also visualizes the size of these confidence/credible intervals as the sample size $n$ of human labeled data increases.

The posterior distribution on the proportion of spiral galaxies which arises from our AI prior obtains tight 90\% credible intervals that concentrate around the mean. As $\alpha$ grows larger, the predictions from the computer vision model become more heavily incorporated.

\subsubsection{Calibrating the Concentration Parameter}


\begin{figure}[!t]
    \centering
	\resizebox{0.7\textwidth}{!}{\input{fig/covplot.pgf}}
    \caption{Frequentist coverage of AI posterior 90\% credible intervals for the proportion of spiral galaxies. Intervals are from the 0.05 to 0.95 posterior quantile, using $n=1000$ samples in the analysis, and the AI prior described in Section~\ref{subsubsec:aipriorspiral}. We display the actual coverage computed using oracle knowledge, and a bootstrapped estimated of the coverage computable via sample information only. Vertical lines denote possible choices of $\alpha$.}
    \label{fig:galaxy-coverage}
\end{figure}

The practitioner may also consider wish to consider choosing $\alpha$ such that posterior credible regions are well-calibrated in a frequentist sense. Figure~\ref{fig:galaxy-coverage} displays the frequentist coverage of 90\% posterior credible intervals around the mean (0.05 to 0.95 posterior quantiles). In order to gain the most from the AI prior while maintaining calibration, we can consider choosing the largest $\alpha$ such that the credible interval stays well-calibrated. Credible intervals are constructed using the AI priors in Section~\ref{subsubsec:aipriorspiral}, conditioning on $n=1000$ data points. {Actual coverage is calculated from the proportion of intervals containing the true proportion of spiral galaxies $\theta^{*}\approx 0.26$, which in this case is taken to be the mean from the entire set of 16,743 labels available (see Section~\ref{subsec:proportion-of-spiral})}. The actual coverage (black line) in Figure~\ref{fig:galaxy-coverage} requires the ability to sample new datasets as well as oracle knowledge of the true parameter $\theta^{*}$. The bootstrap estimate of the coverage (orange line) is computed by bootstrapping samples from the empirical distribution of the 1000 labeled data points, and computing the proportion of times that the mean of this bootstrapped sample lies inside the interval. This procedure is similar to that in \citep{syring2018calibration} and may be used to approximately calibrate posterior credible regions in absence of the knowledge of the true parameter. In this experiment, the largest $\alpha$ value at which the posterior 90\% credible interval is well-calibrated in the frequentist sense occurs approximately at $\alpha=200$. Asymptotic covariance matching to PPI yields the value $\alpha=753$, while the bootstrapping calibration algorithm yields the value $\alpha=783$.   The average width of the credible interval when the AI prior has this concentration parameter value of $\alpha=500$ is about 0.030, which is very similar to the width of the 90\% confidence interval around the PPI estimator.





\subsection{Additional Experiments}\label{sec:additional-experiments}

\begin{figure}
    \centering
	\resizebox{0.9\textwidth}{!}{\input{fig/output.pgf}}
    \caption{Interval width and coverage of AI posterior credible intervals on experiments from~\citet{angelopoulos2023prediction}. Left: size of the AI posterior credible interval for specific $\alpha$ choices as a function of $n$. Middle: interval width as a function of $\alpha$. Right: empirical coverage of the intervals as a function of $\alpha$.}
    \label{fig:appendix-results}
\end{figure}

{We repeat our experimental procedure on six experiments studied in~\citet{angelopoulos2023prediction}. The data was obtained via the \texttt{ppi-python} package. Please refer to this work or the references therein for further information regarding any of these datasets. We provide any relevant details and experimental hyperparameters for each dataset below.


Each experiment follows the setup in prediction-powered inference, in which we have $n$ labeled datapoints, and $N$ unlabeled AI-imputed datapoints. In our case, we refine these $N$ unlabeled datapoints into an AI prior by setting the base measure $F_{AI}$ of our DP prior to be given by their empirical distribution.

For the optimization in each posterior bootstrap iteration, we optimize numerically using the L-BGFS solver. For each method, we obtain $B=1000$ samples using the posterior bootstrap algorithm. We construct a 90\% credible interval from these set of samples by taking the 0.05 and 0.95 quantiles respectively as endpoints. We repeat each experiment varying over a range of $n$, and $\alpha$ values (where $\alpha$ is the DP prior parameter). We also repeat the experiment for a varying number of repetitions for each combination, so that we can assess variation and compute empirical coverage of the credible intervals. In each repetition, the $n$ labeled datapoints are resampled from a larger body of available data.


Figure~\ref{fig:appendix-results} displays the results regarding 90\% posterior credible intervals for the parameter of interest using AI priors. In the leftmost column, we show the resulting interval widths for two choices of the DP prior parameter $\alpha$. In purple, $\alpha$ was obtained by matching the asymptotic covariance to match that of PPI. In orange, we choose $\alpha$ via the empirical calibration strategy derived from that used for Gibbs posteriors proposed by~\citet{syring2018calibration}. We conclude that across our experiments, leveraging machine learning predictions via AI priors allows us to earn a concentration in posterior mass around the true parameter value. In the second column, we fix the value of $n$ to be the largest that was analyzed for each dataset. We show the size of the 90\% credible interval as a function of $\alpha$. Similarly, in the last column, we visualize the empirical coverage computed as the proportion of repetitions in which the true parameter falls into the interval. We note that this estimation may not be accurate for some of our experiments where only ten repetitions were performed.


\paragraph{AlphaFold.} We construct a credible interval for the odds ratio based on credible intervals for the mean in each group, using the transformation used in~\citep{angelopoulos2023prediction}. We use values $n\in\{200, 400, 800, 1500, 3000\}$ and perform $100$ repetitions.

\paragraph{Census Healthcare.} The parameter of interest is the logistic regression coefficient. We sample from the AI posterior using the posterior bootstrap algorithm minimizing the binary cross-entropy loss. We use values $n\in\{500,1000,2000\}$ and perform $10$ repetitions.

\paragraph{Census Income (covariate shift).} The parameter of interest is the ordinary least squares regression coefficient in the covariate shifted population. We sample from the AI posterior using the posterior bootstrap algorithm minimizing the weighted least-squares loss. We use $n\in\{10,100,2000\}$ and perform 100 repetitions.

\paragraph{Spiral Galaxies.} The parameter of interest is the mean of binary data. This example is described in detail in Section~\ref{sec:Applications}. We use $n\in\{10,50,1000\}$ and perform 10 repetitions.

\paragraph{Forest Cover.} This experiment is carried out exactly as in the spiral galaxy data, as we are interested in the mean of binary data. We use $n\in\{10,50,500\}$ and perform 100 repetitions.

\paragraph{Gene Expression.} The parameter of interest is the 0.5 quantile (also known as the median). We use the posterior bootstrap minimizing the absolute error. We use $n\in\{5,100,2000\}$ and perform 10 repetitions.

}




 \section{Discussion}\label{sec:Discussion}
 This research note proposes a Bayesian alternative to prediction-powered inference  framework  introduced by \cite{angelopoulos2023prediction} for performing valid statistical inference when an experimental dataset is augmented with predictions from an AI system. Our approach is based on prior construction based on simulations from an auxiliary black-box model. Our framework enables uncertainty quantification through non-parametric posteriors by viewing the machine learning system as a simulator from a prior on the unknown distribution function. Treating the generative black-box model as a base measure in the Dirichlet process prior $DP(\alpha, F_{AI})$, we achieve fully Bayesian inference about various quantities of interest (parameters associated with statistical models, parameters defined as minimizers of loss functions) using non-parametric posteriors. These posteriors give rise to posterior predictive distributions in parametric models which can be leveraged for decision making based on both AI input as well as observed data. We estimate the concentration parameter $\alpha\geq 0$ from out-of-sample experiments to determine the inferential usefulness of AI predictions. The estimated value at $\alpha=0$ would signify that AI predictions do not add value and one is better off proceeding without them.
 We find that Bayesian analysis can be meaningfully enhanced with generative AI predictions on two real examples. We found that while AI predictions should not be taken literally for decision making, they can serve as a useful proxy (prior) for the correct answer which could enhance Bayesian analysis of observed data.

\bibliographystyle{chicago}
\bibliography{sources}


\appendix

\section{Appendix}

\subsection{Prompting}\label{sec:prompt}

The exact prompt used for the AI base measure in the skin disease experiment was as follows:

\begin{lstlisting}
Predict the diagnosis of Eryhemato-Squamous disease in the following case, using the following clinical features. The age feature simply represents the age of
    the patient. Family history is a binary variable. Every other feature was given a degree in the range of 0 to 3. Here, 0 indicates that the feature was not present, 3 indicates the largest amount possible, and 1, 2 indicate the relative intermediate values.

erythema: 2.0, scaling: 2.0, definite borders: 0.0, itching: 3.0, koebner phenomenon: 0.0, polygonal papules: 0.0, follicular papules: 0.0, oral mucosal involvement: 0.0, knee and elbow involvement: 1.0, scalp involvement: 0.0, family history: 0.0, age: 55.0

    The possible classes are: psoriasis, seboreic dermatitis, lichen planus, pityriasis rosea, cronic dermatitis, pityriasis rubra pilaris.

    Please estimate the probability of each possible diagnosis for this case. The following is for research purposes only. I understand that a real patient must see a qualified doctor with such a concern.

    Format your answer as:
    psoriasis: (prob),
    seboreic dermatitis: (prob),
    ...

    Do your best to provide an accurate answer strictly in this format, and do not include anything else in your response.
\end{lstlisting}



% \begin{figure}
%     \centering
% \includegraphics[width=1.0\textwidth]{fig/output.png}
%     \caption{Interval width and coverage of AI posterior credible intervals. On the left, we choose the best $\alpha$, and visualize the size of the AI posterior credible interval as a function of $n$. On the right, we fix $n$ to be 1000 and plot the interval width and coverage of the AI posterior credible interval as a function of $\alpha$.}
%     \label{fig:appendix}
% \end{figure}


\subsection{Proof of Theorem \ref{thm:PB-appendix}}\label{sec:proof}


\begin{proof}
  Define the weighted generalized score function by
  \[
\wt{S}_{n}^{\alpha}=\sum_{i=1}^{n}w_{i}\nabla\ell(\theta,Y_{i})+\sum_{j=1}^{m} \wt{w}_{j}\nabla\ell(\theta, Y^{*}_{j})\,,
  \]
  and similarly the weighted sample generalized information matrix by
  \[
    \wt{J}_{n}^{\alpha}(\theta) = \sum_{i=1}^{n} w_{i} \nabla^{2} \ell(\theta,Y_{i}) + \sum_{j=1}^{m} \wt{w}_{j} \nabla^{2} \ell(\theta, Y^{*}_{j})\,.
  \]
  where $(w_{1},\ldots,w_{n},\wt{w}_{1},\ldots,\wt{w}_{m})'\sim\mathrm{Dirichlet}(1,\ldots,1,\alpha/m,\ldots,\alpha/m)$. We first argue that $(n+\alpha)^{1/2}\wt{S}_{n}^{\alpha}(\wh{\theta}_{n}^{\alpha})\overset{d}{\to} N(0, I(\theta_{0}^{\gamma}))$ using the Cram\'{e}r-Wold device as follows. Let $z\in \mathbb{R}^{p}$ with $\|z\|_{1}=1$, and define
  \begin{align*}
    t_{n,\alpha}(z) &\equiv \sqrt{n+\alpha}\, z^{\top} \wt{S}_{n}^{\alpha}(\wh{\theta}_{n}^{\alpha}) \\
    &= \sqrt{n+\alpha}\,\sum_{k=1}^{p} z_{k} \left(\frac{\sum_{i=1}^{n} V_{i} g_{k}(\wh{\theta}_{n}^{\alpha},Y_{i})+\sum_{j=1}^{m} \wt{V}_{j}g_{k}(\wh{\theta}_{n}^{\alpha},Y^{*}_{j}) }{\sum_{i=1}^{n}V_{i}+\sum_{j=1}^{m} \wt{V}_{j}}\right)
  \end{align*}
  where $V_{1},\ldots,V_{n}\overset{iid}{\sim}\mathrm{Exp}(1)$, $\wt{V}_{1},\ldots,\wt{V}_{m}\overset{iid}{\sim}\mathrm{Gam}(\alpha/m,1)$, and $g_{k}(\theta',Y)\equiv \frac{\partial \ell(\theta,Y)}{\partial \theta_{k}}\rvert_{\theta=\theta'}$. This can be rewritten as
  \[
t_{n,\alpha}(z)=\frac{1}{(n+\alpha)^{-1}\sum_{i}V_{i}+\sum_{j}\wt{V}_{j}}\cdot \frac{\sum_{i=1}^{n} a_{i}V_{i} + \sum_{j=1}^{m} \wt{a}_{j}\wt{V}_{j}}{\sqrt{n+m}}
  \]
  The first factor in the product converges almost surely to $1$. Thus, it is sufficient to show that
  \[
    t_{n,\alpha}'(z)=\frac{\sum_{i=1}^{n} a_{i}V_{i} + \sum_{j=1}^{m} \wt{a}_{j}\wt{V}_{j}}{\sqrt{n+m}}
  \]
  converges in distribution to $N(0,z^{\top} I(\theta_{0}^{\gamma})z)$. Indeed, we appeal to the Lindeberg-Feller-L\'{e}vy CLT. The average variance of the random variables summated in the numerator is given by
  \[
\bar{\sigma}^{2}_{n,\alpha} = (n+\alpha)^{-1}\left(\sum_{i=1}^{n} a_{i}^{2} + \frac{\alpha}{m}\sum_{j=1}^{m} \wt{a}_{j}^{2}\right)
  \]
  and the asymptotic variance is thus
  \[
    z^{\top}\left(I_{1}(\theta_{0}^{\gamma})+\gamma I_{2}(\theta_{0}^{\gamma})\right)z\,.
  \]
  Therefore, we have $\sqrt{n+\alpha}\, \wt{S}_{n}^{\alpha}(\wh{\theta}_{n}^{\alpha})\overset{d}{\to} N(0,I(\theta_{0}^{\gamma})$.

  Assuming smoothness conditions \citep{lyddon2019general, Newton1991}  hold, we can perform a Taylor expansion of the weighted score function around the empirical risk minimizer $\wh{\theta}_{n}^{\alpha}$ as
  \[
    \wt{S}_{n}^{\alpha}(\wh{\theta}_{n}^{\alpha}) = (\wt{J}_{n}(\wh{\theta}_{n}) - R_{n})(\theta^{*}-\wh{\theta}_{n}^{\alpha})
  \]
  for a remainder term $R_{n}$. Following~\citet{lyddon2019general}, $\wt{J}_{n}^{\alpha}(\wh{\theta}_{n}^{\alpha})-R_{n}$ converges to $J(\theta_{0}^{\gamma})$ and is invertible with high probability. Slutsky's theorem then yields the desired result, with asymptotic covariance given by $J(\theta_{0}^{\gamma})^{-1}I(\theta_{0}^{\gamma})J(\theta_{0}^{\gamma})^{-1}$.
\end{proof}


\end{document}
