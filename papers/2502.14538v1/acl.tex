% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{booktabs} 
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{array}


\usepackage{booktabs} % 负责处理表格线条
\usepackage{multirow} % 负责表格的多行合并

\usepackage{subcaption}
% Define colors
\usepackage[ruled,vlined]{algorithm2e}


\usepackage{amsmath}
\usepackage{array}
\usepackage{booktabs}
\definecolor{lightgray}{gray}{0.9}
\definecolor{darkgray}{gray}{0.7}
\usepackage{xcolor}
\definecolor{burntorange}{RGB}{204, 85, 0} % Example RGB values

% Standard package includes
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
% \usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algpseudocode}
\usepackage{adjustbox}

\usepackage{times}
\usepackage{bm}

\usepackage{amsmath} 
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{xcolor}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{tabularx}

\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{float}  % Add this line to use the H float option
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{amsmath} % for math commands like \sum
 \usepackage{cleveref}
% \usepackage{hyperref}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{multirow} 
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{tikz}
\usepackage[utf8]{inputenc}  % 添加对 UTF-8 编码的支持
\usepackage{subcaption} 
\usepackage{breqn}

\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{etoc}
\usepackage{tcolorbox}

\usepackage{dblfloatfix}

\usepackage{booktabs} 
\usepackage{amsmath} % 用于数学符号
% \usepackage{algorithm}


\newcommand{\llms}{LLMs\xspace}
\usepackage{booktabs}

\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{todonotes}

\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}



\title{LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization}


% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}
\author{
        Yupeng Chang$^{1}$ \quad Chenlu Guo$^{1}$ \quad  Yi Chang$^{1,2,3}$ \quad Yuan Wu$^{1}$\footnotemark[1] \\
        $^{1}$School of Artificial Intelligence, Jilin University \\
        $^{2}$Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China \\
        $^{3}$International Center of Future Science, Jilin University\\
        changyp23@mails.jlu.edu.cn, guocl23@mails.jlu.edu.cn, yichang@jlu.edu.cn, yuanwu@jlu.edu.cn \\   
}


\begin{document}
\etocdepthtag.toc{chapter}
\etocsettagdepth{chapter}{none}
\etocsettagdepth{appendix}{none}

\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding authors}


\begin{abstract}
Large Language Models (LLMs) have achieved remarkable success in natural language processing, but their full fine-tuning remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), have emerged as a practical solution by approximating parameter updates with low-rank matrices. However, LoRA often exhibits a “double descent” phenomenon during fine-tuning, where model performance degrades due to overfitting and limited expressiveness caused by low-rank constraints. To address this issue, we propose \textbf{LoRA-GGPO (Gradient-Guided Perturbation Optimization)}, a novel method that leverages gradient and weight norms to generate targeted perturbations. By optimizing the sharpness of the loss landscape, LoRA-GGPO guides the model toward flatter minima, mitigating the double descent problem and improving generalization. Extensive experiments on natural language understanding (NLU) and generation (NLG) tasks demonstrate that LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore, extended experiments specifically designed to analyze the double descent phenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing more robust and generalizable models. Our work provides a robust and efficient solution for fine-tuning LLMs, with broad applicability in real-world scenarios.
The code is available at \url{https://github.com/llm172/LoRA-GGPO}.

\end{abstract}


% \section{Introduction}
% \label{sec:intro}


% Large Language Models (LLMs) are powerful deep learning models capable of handling a wide range of language tasks such as text generation, translation, and question-answering \citep{wei2022emergent, bommasani2021opportunities, chang2024survey}. While LLMs exhibit broad applicability, they may not perform optimally for specific tasks or domains. This is where fine-tuning becomes crucial. By fine-tuning LLMs on task-specific data, the model can be better adapted to meet particular requirements, significantly improving accuracy and efficiency in specialized applications \citep{ lester2021power, fu2023effectiveness, ding2023parameter}. 

% Due to the limitation of computing resources and time, the method of low-rank fine-tuning LLMs has become mainstream at present. It mainly includes two types of methods, one is LoRA Variants with Original Structure, mainly including rsLoRA \citep{kalajdzievski2023rankstabilizationscalingfactor}, LoRA+ \citep{hayou2024loraefficientlowrank} and LoRA-GA \citep{wang2024loragalowrankadaptationgradient}, etc.; the other is LoRA Variants with Modified Structure, mainly including DoRA \citep{liu2024doraweightdecomposedlowrankadaptation}, AdaLoRA \citep{zhang2023adaloraadaptivebudgetallocation}, etc. Although these methods have shown excellent performance in various tasks, we now observe the phenomenon of double descent in the loss during LoRA fine-tuning. Double descent refers to the nonlinear relationship between model complexity and generalization performance, where increasing complexity initially improves performance, then reduces performance, and then improves performance again, forming a "U"-shaped curve \citep{belkin2019reconciling, nakkiran2021deep, luo2024investigatingimpactmodelcomplexity}. Since the occurrence of this phenomenon in machine learning usually has a negative impact on model performance, this motivates us to explore whether this phenomenon will have a negative impact on the performance of the fine-tuned model, what is the specific reason for this problem in the low-rank fine-tuning process, and how to design methods to alleviate this problem.


% To overcome these challenges, this study introduces LoRA-Opt, an advanced optimization framework designed to enhance the scalability and efficiency of LoRA-based fine-tuning for LLMs. LoRA-Opt integrates two key innovations: Gradient Alignment (GA) and Sharpness-Aware Minimization (SAM). GA optimizes the training process by aligning the gradients of low-rank adaptations with those derived from full parameter fine-tuning, thereby stabilizing performance and mitigating the Double Descent phenomenon. Building on GA, SAM is incorporated to identify flatter minima in the loss landscape, reducing the model's susceptibility to overfitting and enhancing robustness against training noise and perturbations.

% The theoretical foundation of LoRA-Opt lies in its ability to harmonize gradient directions and smooth the loss surface, complementing the inherent strengths of LoRA. Comprehensive experimental evaluations across multiple benchmarks demonstrate that LoRA-Opt effectively mitigates the Double Descent issue, consistently outperforming various state-of-the-art PEFT techniques. These results highlight the superior performance and practical applicability of LoRA-Opt, underscoring its significant contribution to advancing parameter-efficient fine-tuning methods. Consequently, LoRA-Opt facilitates the deployment of large-scale language models in resource-constrained environments without sacrificing their exceptional performance, broadening the accessibility and utility of LLMs in diverse real-world applications.

% Our main contributions are:
% \begin{itemize}
%     \item This is the first paper to investigate in detail the impact of the Double descent problem in the LoRA fine-tuning process on the performance of the fine-tuned model.
%     \item We innovatively proposed our LoRA-Opt method, which first aligns the fine-tuning initial matrix and the full parameter matrix at the beginning of fine-tuning, and then uses the SAM method during fine-tuning training to alleviate the drastic fluctuations that may occur during the fine-tuning process.
%     \item Extensive experiments and case studies strongly demonstrate the effectiveness of our proposed method, and our research also points out a new research direction for the problem of capability emergence in the LoRA fine-tuning process.
% \end{itemize}

\section{Introduction}
% Large Language Models (LLMs) have become integral tools for a wide range of natural language processing (NLP) tasks, including text generation, translation, and question-answering \citep{wei2022emergent, bommasani2021opportunities, chang2024survey}. While these models excel in many domains, their performance can often be suboptimal in specialized tasks, necessitating fine-tuning. By fine-tuning LLMs on domain-specific datasets, their performance can be significantly improved, making them better suited for particular applications \citep{lester2021power, fu2023effectiveness, ding2023parameter}.

% Given the computational challenges associated with training LLMs, low-rank adaptation (LoRA) methods have gained popularity. LoRA reduces the computational burden of fine-tuning by decomposing model weight updates into low-rank matrices. LoRA variants can be classified into two types: LoRA variants with original structure, such as rsLoRA \citep{kalajdzievski2023rankstabilizationscalingfactor}, LoRA+ \citep{hayou2024loraefficientlowrank}, and LoRA-GA \citep{wang2024loragalowrankadaptationgradient}; and LoRA variants with modified structure, such as DoRA \citep{liu2024doraweightdecomposedlowrankadaptation} and AdaLoRA \citep{zhang2023adaloraadaptivebudgetallocation}. While these approaches offer improved efficiency, they are prone to the double descent phenomenon, where performance initially improves as model complexity increases, then worsens, and eventually improves again, forming a "U"-shaped curve \citep{belkin2019reconciling, nakkiran2021deep, luo2024investigatingimpactmodelcomplexity}. This instability hampers the effectiveness of LoRA fine-tuning, resulting in poor generalization and reduced model performance.

% Large Language Models (LLMs) are powerful deep learning models capable of handling a wide range of language tasks such as text generation, translation, and question-answering \citep{wei2022emergent, bommasani2021opportunities, chang2024survey}. While LLMs exhibit broad applicability, they may not perform optimally for specific tasks or domains. This is where fine-tuning becomes crucial. By fine-tuning LLMs on task-specific data, the model can be better adapted to meet particular requirements, significantly improving accuracy and efficiency in specialized applications \citep{ lester2021power, fu2023effectiveness, ding2023parameter}. 

% Due to the limitation of computing resources and time, the method of low-rank fine-tuning LLMs has become mainstream at present. It mainly includes two types of methods, one is LoRA Variants with Original Structure, mainly including rsLoRA \citep{kalajdzievski2023rankstabilizationscalingfactor}, LoRA+ \citep{hayou2024loraefficientlowrank} and LoRA-GA \citep{wang2024loragalowrankadaptationgradient}, etc.; the other is LoRA Variants with Modified Structure, mainly including DoRA \citep{liu2024doraweightdecomposedlowrankadaptation}, AdaLoRA \citep{zhang2023adaloraadaptivebudgetallocation}, etc. Although these methods have shown excellent performance in various tasks, we now observe the phenomenon of double descent in the loss during LoRA fine-tuning. Double descent refers to the nonlinear relationship between model complexity and generalization performance, where increasing complexity initially improves performance, then reduces performance, and then improves performance again, forming a "U"-shaped curve \citep{belkin2019reconciling, nakkiran2021deep, luo2024investigatingimpactmodelcomplexity}. Since the occurrence of this phenomenon in machine learning usually has a negative impact on model performance, this motivates us to explore whether this phenomenon will have a negative impact on the performance of the fine-tuned model, what is the specific reason for this problem in the low-rank fine-tuning process, and how to design methods to alleviate this problem.

% Double descent is a well-known issue in machine learning, which occurs when a model’s performance first improves with increasing model complexity, then declines as overfitting occurs, and later improves again as the model finds better solutions. This phenomenon is particularly problematic for LoRA fine-tuning, where the instability can hinder the model’s ability to generalize well.

% In this paper, we introduce LoRA-Opt, an optimization framework designed to address the double descent issue in LoRA fine-tuning. The core of LoRA-Opt lies in two key innovations:
% 1. Sharpness-Aware Minimization (SAM): SAM optimizes the loss surface to avoid sharp minima, ensuring better generalization by seeking flatter minima. This helps mitigate the double descent phenomenon and stabilizes the fine-tuning process.
% 2. Expectation Perturbation Optimization (EPO): EPO improves SAM’s efficiency by reducing memory consumption and computational overhead. Rather than storing perturbation copies, EPO samples perturbations and computes their expected values, maintaining SAM's effectiveness while enhancing scalability.

% Through extensive experiments, we demonstrate that LoRA-Opt consistently outperforms existing state-of-the-art parameter-efficient fine-tuning (PEFT) methods, offering scalable and efficient solutions for fine-tuning large models in resource-constrained environments without compromising performance.

% Our contributions are as follows:
% \begin{itemize}
%     \item We provide a detailed analysis of the double descent phenomenon in LoRA fine-tuning and its negative impact on model performance.
%     \item We propose LoRA-Opt, which integrates Sharpness-Aware Minimization (SAM) to stabilize the fine-tuning process and improve generalization, and Expectation Perturbation Optimization (EPO) to reduce computational costs and memory usage.
%     \item We conduct extensive experiments that demonstrate the superior performance of LoRA-Opt over existing state-of-the-art PEFT methods.
%     \item Our work introduces new directions for addressing instability and improving generalization in LoRA fine-tuning.
% \end{itemize}

Large Language Models (LLMs) have revolutionized natural language processing (NLP) with their exceptional performance across diverse tasks \cite{wei2022emergent, bommasani2021opportunities, chang2024survey}. However, fine-tuning these models requires updating billions of parameters, which is computationally expensive and memory-intensive. This limitation has spurred the development of Parameter-Efficient Fine-Tuning (PEFT) techniques \citep{han2024parameter, xu2023parameter, zhang2025parameter}, which aim to achieve high performance with minimal resource consumption \citep{lester2021power, fu2023effectiveness, ding2023parameter}.

Among PEFT methods, Low-Rank Adaptation (LoRA) \citep{hu2021lora} and its variants \citep{kalajdzievski2023rankstabilizationscalingfactor, hayou2024loraefficientlowrank, meng2024pissaprincipalsingularvalues, wang2024loragalowrankadaptationgradient, wang2024loraprolowrankadaptersproperly} have gained prominence by approximating parameter updates using low-rank matrices. Specifically, LoRA modifies the weight matrix \( W \in \mathbb{R}^{d \times d'} \) as:
\[
W' = W + \Delta W = W + A B,
\]
where \( A \in \mathbb{R}^{d \times r} \) and \( B \in \mathbb{R}^{r \times d'} \) are low-rank matrices, and \( r \ll \min(d, d') \). During fine-tuning, only \( A \) and \( B \) are updated, while the original pre-trained weights \( W \) remain frozen. This design significantly reduces the number of trainable parameters. However, the low-rank constraint can limit the model’s expressiveness, hindering its ability to fit complex feature spaces. This limitation often leads to overfitting in the middle stages of training, resulting in the “double descent” phenomenon, where test loss initially increases, decreases due to overfitting, and then rises again \citep{wei2023inversescalingushaped, luo2024investigating, huang2024unified}.

The “double descent” phenomenon refers to a non-monotonic behavior in model performance, where test loss initially increases, decreases due to overfitting, and then rises again \citep{belkin2019reconciling, nakkiran2021deep, luo2024investigatingimpactmodelcomplexity}. This behavior is particularly problematic in LoRA fine-tuning, as it hinders the model’s ability to explore complex feature spaces and negatively impacts generalization. To address this issue, we pose the following research questions:  
1. How does the double descent phenomenon affect the performance of LoRA-fine-tuned models?  
2. What are the underlying causes of this phenomenon in low-rank fine-tuning?  
3. Can we design an effective method to mitigate this issue?

To tackle these challenges, we propose \textbf{LoRA-GGPO (Gradient-Guided Perturbation Optimization)}, a novel approach that integrates gradient and weight norms to generate targeted perturbations. Specifically, LoRA-GGPO optimizes the sharpness of the loss landscape by introducing random perturbations guided by the weighted combination of gradient norms and weight norms. This ensures that perturbations are concentrated in regions most sensitive to changes in the loss function, enabling the model to find flatter minima and improve generalization. Compared to traditional methods like Sharpness-Aware Minimization (SAM), LoRA-GGPO reduces computational and memory overhead while achieving superior performance.

Our extensive experiments validate the effectiveness of LoRA-GGPO. In natural language understanding (NLU) and generation (NLG) tasks, LoRA-GGPO consistently outperforms LoRA and its state-of-the-art variants. Furthermore, through a series of extended experiments specifically targeting the double descent phenomenon, we empirically demonstrate that LoRA-GGPO successfully mitigates this issue. These findings highlight the potential of LoRA-GGPO as a practical solution for fine-tuning LLMs, producing models that are both robust and highly generalizable.

The main contributions of this paper are as follows:  
\begin{itemize}
    \item We systematically analyze the causes and implications of the double descent phenomenon in LoRA fine-tuning.
    \item We propose LoRA-GGPO, a novel method that leverages gradient-guided perturbations to mitigate double descent and enhance model generalization.
    \item Through extensive experiments, we demonstrate the superior performance of LoRA-GGPO across multiple tasks, showcasing its practical applicability in real-world scenarios.
\end{itemize}

% \section{Related Work}
% \label{sec:related}

% \textbf{Parameter-Efficient Fine-Tuning}
% The widespread adoption of LLMs in natural language processing (NLP) has highlighted the need for efficient fine-tuning methods. Traditional fine-tuning approaches, which update all model parameters, are computationally expensive and require significant storage, especially as model sizes grow \citep{howard2018universal, devlin2018bert}. To address these challenges, Parameter-Efficient Fine-Tuning (PEFT) techniques have been developed, aiming to adapt pre-trained models to downstream tasks by modifying only a subset of parameters \citep{houlsby2019parameter, lester2021power, fu2023effectiveness, ding2023parameter, han2024parameter}.

% Among PEFT methods, Low-Rank Adaptation (LoRA) has gained prominence due to its effectiveness in reducing the number of trainable parameters without compromising performance \citep{hu2021lora}. LoRA achieves this by introducing low-rank matrix decompositions into the model architecture, allowing fine-tuning to focus on smaller, more manageable parameter subsets. This approach not only enhances computational efficiency but also maintains performance comparable to full fine-tuning. Other notable PEFT methods include Adapter Layers \citep{houlsby2019parameter}, which insert small bottleneck layers within the network; BitFit \citep{zaken2021bitfit}, which fine-tunes only the bias terms; and Prefix-Tuning \citep{li2021prefix}, which optimizes continuous prompts while keeping the rest of the model fixed. These techniques collectively demonstrate that efficient adaptation of LLMs is achievable without extensive parameter modifications, facilitating the deployment of large-scale models in resource-constrained environments.

% Despite significant advancements, PEFT methods face challenges in balancing efficiency and performance as model complexity increases. LoRA, with its low-rank decomposition strategy, offers a scalable solution by effectively managing the trade-off between parameter reduction and performance retention \citep{han2024parameter}.
% Rank-Stabilized LoRA (rsLoRA) \citep{kalajdzievski2023rankstabilizationscalingfactor} enhances learning efficiency and performance in high-rank scenarios by adjusting the adapter scaling factor to the square root of the rank. 
% LoRA+ \citep{hayou2024loraefficientlowrank} enhances training efficiency by assigning higher learning rates to matrix B compared to matrix A, leading to faster convergence and improved performance. 
% LoRA-GA \citep{wang2024loragalowrankadaptationgradient} enhances the initialization process by aligning the gradients of low-rank matrices with those of full fine-tuning at the outset, thereby accelerating convergence and improving performance. 
% LoRA-Pro \citep{wang2024loraprolowrankadaptersproperly} dynamically adjusts the gradient in each optimization step to ensure that the parameter update trajectory of the entire training process is aligned with the full fine-tuning, rather than just the initialization phase.
% BA-LoRA \citep{chang2024balorabiasalleviatinglowrankadaptation} introduces three regularization terms—consistency, diversity, and singular value decomposition—to the LoRA fine-tuning process, enhancing model performance and mitigating issues like "catastrophic inheritance". 
% PiSSA \citep{meng2024pissaprincipalsingularvalues} enhances parameter-efficient fine-tuning by initializing low-rank adaptation matrices with the principal singular values and vectors of the original model's weight matrix, leading to faster convergence and improved performance compared to traditional methods like LoRA.
% DoRA \citep{liu2024doraweightdecomposedlowrankadaptation} enhances parameter-efficient fine-tuning by decomposing pre-trained weights into magnitude and direction components, applying LoRA specifically to the directional updates, thereby improving learning capacity and stability without adding inference overhead.
% AdaLoRA \citep{zhang2023adaloraadaptivebudgetallocation} enhances the LoRA fine-tuning process by enabling the model to learn the importance of parameters and dynamically adjust the rank of adaptation matrices for each module, thereby improving efficiency and performance. 


% \textbf{Double Descent Phenomenon and Optimization Strategies}
% The Double Descent phenomenon has emerged as a critical concept in deep learning, revealing a non-monotonic relationship between model complexity and generalization performance \citep{belkin2019reconciling, nakkiran2021deep}. Initially, increasing model complexity leads to improved performance, followed by a regime where further complexity results in decreased performance, and ultimately, additional complexity again enhances performance. This challenges the classical bias-variance trade-off and necessitates new perspectives on model selection and training dynamics.

% In the context of fine-tuning LLMs using PEFT methods like LoRA, Double Descent manifests as fluctuations in model performance relative to the rank of the low-rank matrices \citep{luo2024investigatingimpactmodelcomplexity}. Specifically, increasing the rank initially enhances performance by providing greater adaptation capacity. However, beyond a critical threshold, additional complexity induces overfitting, degrading performance before potentially improving again with further rank increases. This instability poses significant challenges for the reliable deployment of LoRA in large-scale settings.

% To mitigate the adverse effects of Double Descent, advanced optimization strategies have been proposed. Gradient Approximation (GA) techniques aim to align the gradients of low-rank matrices with those derived from full-parameter fine-tuning, thereby optimizing the direction of parameter updates and reducing the impact of Double Descent \citep{wang2024lora}. Additionally, Sharpness-Aware Minimization (SAM) enhances model generalization by modifying the loss landscape to favor flatter minima, reducing sensitivity to parameter variations \citep{foret2020sharpness}. Integrating GA and SAM with LoRA has shown promise in stabilizing performance and improving the reliability of fine-tuned models \citep{mao2025survey}.

% Recent studies have explored the synergistic effects of combining multiple optimization techniques to counteract Double Descent. For instance, integrating GA with SAM provides a dual approach to both align gradient directions and smooth the loss landscape, offering a robust framework for enhancing the generalization capabilities of PEFT methods \citep{dong2024fine}. These combined strategies not only offer new theoretical insights into the Double Descent phenomenon but also demonstrate practical improvements in model performance, paving the way for more stable and efficient fine-tuning of LLMs \citep{han2024parameter}.

\section{Related Work}

The widespread adoption of LLMs in NLP has driven the development of efficient fine-tuning techniques. Traditional fine-tuning methods, which update all model parameters, are computationally expensive and require significant storage resources \citep{howard2018universal, devlin2018bert}. To address these challenges, PEFT methods have emerged, enabling adaptation to downstream tasks by modifying only a subset of parameters \citep{houlsby2019parameter, lester2021power, fu2023effectiveness, ding2023parameter, han2024parameter}. Low-Rank Adaptation (LoRA) and its variants are one of the most prominent PEFT approaches, reducing the number of trainable parameters through low-rank matrix decompositions while maintaining high performance \citep{hu2021lora, chang2024balorabiasalleviatinglowrankadaptation}. Other notable PEFT methods include Adapter Layers \citep{houlsby2019parameter}, BitFit \citep{zaken2021bitfit}, and Prefix-Tuning \citep{li2021prefix}, all of which demonstrate the feasibility of efficient adaptation without extensive parameter updates. To further enhance performance, researchers have proposed several improvements: Rank-Stabilized LoRA (rsLoRA) adjusts the scaling factor to improve learning efficiency \citep{kalajdzievski2023rankstabilizationscalingfactor}; LoRA+ assigns different learning rates to matrices for faster convergence and better performance \citep{hayou2024loraefficientlowrank}; PiSSA initializes low-rank matrices using the principal singular values and vectors of the original weight matrix, accelerating convergence and improving performance \citep{meng2024pissaprincipalsingularvalues}; and DoRA decomposes weights into magnitude and direction components to optimize learning capacity \citep{liu2024doraweightdecomposedlowrankadaptation}. Additionally, AdaLoRA dynamically adjusts the rank of adaptation matrices to balance efficiency and performance \citep{zhang2023adaloraadaptivebudgetallocation}. 
Furthermore, \textbf{LoRA-GA} \citep{wang2024loragalowrankadaptationgradient} accelerates convergence by aligning low-rank matrix gradients with those of full fine-tuning, ensuring better gradient flow throughout training. 
Similarly, \textbf{LoRA-Pro} \citep{wang2024loraprolowrankadaptersproperly} dynamically adjusts the gradient in each optimization step to ensure that the parameter update trajectory of the entire training process is aligned with full fine-tuning, rather than just the initialization phase. 

The Double Descent phenomenon highlights a non-monotonic relationship between model complexity and generalization performance, challenging the classical bias-variance trade-off theory~\citep{belkin2019reconciling, nakkiran2021deep}. In PEFT methods like LoRA, Double Descent manifests as performance fluctuations with respect to the rank of low-rank matrices: increasing the rank initially improves performance, but beyond a critical threshold, overfitting occurs, leading to degradation~\citep{luo2024investigatingimpactmodelcomplexity}. This poses significant challenges to the reliability and stability of PEFT methods, particularly when fine-tuning LLMs. To address this issue, techniques such as Entropy-SGD~\citep{chaudhari2019entropy} and Sharpness-Aware Minimization (SAM)~\citep{foret2020sharpness} have been proposed to promote flatter minima and improve generalization. However, their computational overhead limits scalability in PEFT scenarios. More recently, random weight perturbation (RWP) approaches~\citep{bisla2022low} have shown promise in balancing efficiency and generalization, though they may still introduce additional computational demands. These insights underscore the need for scalable solutions tailored to mitigating Double Descent in LoRA-based fine-tuning.

% The Double Descent phenomenon highlights a non-monotonic relationship between model complexity and generalization performance, challenging the classical bias-variance trade-off theory \citep{belkin2019reconciling, nakkiran2021deep}. When applying PEFT methods like LoRA to fine-tune LLMs, Double Descent manifests as fluctuations in performance with respect to the rank of low-rank matrices: increasing the rank initially improves performance, but beyond a critical threshold, overfitting occurs, leading to performance degradation \citep{luo2024investigatingimpactmodelcomplexity}. This phenomenon poses new challenges to the reliability and stability of PEFT methods, offering important insights for optimizing model design and fine-tuning strategies.

% \section{Method}

% This section introduces the proposed LoRA-Opt framework, designed to overcome the double descent problem during LoRA fine-tuning for large language models (LLMs). The core idea of LoRA-Opt is to integrate Sharpness-Aware Minimization (SAM) with a novel Expectation Perturbation Optimization (EPO) method to improve generalization while reducing computational costs.

% \subsection{Review of LoRA Framework}

% \textbf{LoRA} (Low-Rank Adaptation) is a method for fine-tuning pre-trained models by adding low-rank matrices to the weight matrices. Suppose the weight matrix of a given layer in the original model is \( W \), with dimensions \( d \times d' \). LoRA adjusts it by inserting two low-rank matrices, \( A \) and \( B \), such that the new weight matrix becomes:

% \[
% W' = W + \Delta W = W + A B
% \]

% Here, the matrix \( A \) has dimensions \( d \times r \), and matrix \( B \) has dimensions \( r \times d' \), where \( r \) is typically much smaller than \( d \) or \( d' \), implementing the low-rank property.

% The key idea behind LoRA is to fine-tune only the low-rank matrices \( A \) and \( B \), while keeping the original weight matrix \( W \) fixed. Since the low-rank matrices involve fewer parameters, the number of parameters updated during fine-tuning is significantly reduced, which lowers the computational and storage costs, making it particularly suitable for resource-constrained environments.


% However, LoRA is susceptible to the double descent phenomenon, where performance initially improves as the rank increases, then degrades, and later improves again, leading to instability and poor generalization.
% To address these issues, we propose LoRA-Opt, an optimization framework that integrates Sharpness-Aware Minimization (SAM) to stabilize training and improve generalization, while using Expectation Perturbation Optimization (EPO) to reduce memory and computational overhead.






% \subsection{LoRA-Opt Framework}

% The LoRA-Opt framework integrates Sharpness-Aware Minimization (SAM) into the LoRA fine-tuning process. SAM helps mitigate double descent by finding flatter minima in the loss surface, which reduces the model’s sensitivity to noise and small perturbations in the training data.

% SAM works by optimizing the following objective:

% \[
% \mathcal{L}_{\mathrm{SAM}} = \max_{\epsilon \in \mathcal{B}(\rho)} \mathcal{L}(\theta + \epsilon)
% \]

% where \( \mathcal{L} \) is the loss function, \( \theta \) represents the model parameters, and \( \mathcal{B}(\rho) \) defines a small neighborhood around \( \theta \) with radius \( \rho \). This encourages the model to find minima that are flatter and less sensitive to small perturbations, improving generalization and reducing overfitting.

% \subsection{Innovative Improvements to SAM: Expectation Perturbation Optimization}

% While SAM effectively reduces the sharpness of minima, it has two main limitations: increased training cost and high memory consumption. SAM requires storing perturbation copies and calculating additional gradients, which can be computationally expensive, especially for large models.

% To address these challenges, we propose Expectation Perturbation Optimization (EPO), which improves the efficiency of SAM. Rather than storing perturbations, EPO samples perturbations and computes their expected values over multiple samples, significantly reducing memory usage and computational overhead. The optimization formula for EPO is as follows:

% \[
% \min_{\mathbf{A}, \mathbf{B}} \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^2 \mathbf{I})} \left[ L\left( \mathbf{W} + s \cdot \mathbf{B} \mathbf{A} + \boldsymbol{\epsilon} \right) \right]
% \]

% In addition to reducing memory usage, we also propose a perturbation seed storage strategy, where only the seed of the perturbation and its norms are stored, rather than the entire perturbation matrix \( \boldsymbol{\epsilon} \). This reduces memory usage while maintaining perturbation consistency. Furthermore, we optimize SAM by applying perturbations only to local regions of the low-rank matrices \( A \) and \( B \), thereby reducing computational complexity and enhancing training efficiency.

\section{Method}

This section first reviews the basic framework of LoRA and analyzes the double descent problem caused by low-rank constraints limiting model expressiveness during fine-tuning. Subsequently, we explore the idea of perturbation-based optimization and propose a new Gradient-Guided Perturbation Optimization (GGPO) method to alleviate these issues effectively.

\subsection{LoRA Framework Review}

\textbf{LoRA} (Low-Rank Adaptation) \citep{hu2021lora} is a parameter-efficient fine-tuning method that approximates changes in the full parameter space by introducing a low-rank matrix \( AB \), thereby adjusting the pre-trained model. For a given layer’s weight matrix \( W \in \mathbb{R}^{d \times d’} \), LoRA modifies it as:
\begin{equation}
W' = W + \Delta W = W + A B,
\end{equation}
where \( A \in \mathbb{R}^{d \times r} \) and \( B \in \mathbb{R}^{r \times d'} \) are low-rank matrices, and \( r \ll \min(d, d') \). During fine-tuning, only \( A \) and \( B \) are updated, while the original weight matrix \( W \) remains frozen. This design significantly reduces the parameter scale, making it suitable for resource-constrained environments. However, this low-rank constraint may also limit the model’s expressiveness, leading to subsequent issues.

\subsection{Double Descent Problem and Its Causes}

% \subsubsection{Overview of Double Descent Phenomenon}

The double descent phenomenon is a common non-monotonic behavior in machine learning, where model performance exhibits an “increase-decrease-increase” trend with complexity or training time. This reflects the dynamic process of transitioning from underfitting to overfitting and then generalization. From the perspective of loss landscapes, the model may initially fall into sharp minimum regions during training, leading to sensitivity to noise and poor generalization performance. In contrast, in later stages of training, the model may find flatter minimum regions, improving generalization \citep{nakkiran2021deep, bisla2022low, liu2023over}.

To further quantify the double descent phenomenon, we define the model’s generalization error \( \mathcal{E}_{\text{gen}} \) as:
\begin{equation}
\mathcal{E}_{\text{gen}} = \mathbb{E}_{\mathcal{D}} \left[ L_{\text{test}}(\theta) - L_{\text{train}}(\theta) \right],
\end{equation}
where \( L_{\text{train}}(\theta) \) and \( L_{\text{test}}(\theta) \) are the losses on the training and test sets, respectively, and \( \mathcal{D} \) represents the data distribution. When the model is in a sharp minima region, \( \mathcal{E}_{\text{gen}} \) is large, indicating sensitivity to noise and poor generalization. Conversely, when the model finds flatter minima regions, \( \mathcal{E}_{\text{gen}} \) decreases, indicating improved generalization.

\begin{figure}[!h]
\centering
\includegraphics[width=.9\columnwidth]{figures/1.pdf}

\caption{An example of double descent during the LoRA fine-tuning process, with the fine-tuned model being LLaMA-3.2-3B \citep{grattafiori2024llama3herdmodels}, the dataset being WikiQA \citep{wikiqa}, and the rank set to 8.}
\label{fig: 1}
\end{figure}

\subsubsection{Double Descent in LoRA Fine-Tuning}

When using LoRA for fine-tuning, we observed a significant double descent phenomenon as shown in Figure \ref{fig: 1}: test loss initially increases, then decreases due to overfitting, and finally rises again. This indicates that although the low-rank constraint effectively reduces the number of parameters, it may also hinder the model’s ability to fit high-dimensional data by restricting its exploration of complex feature spaces. Exacerbating overfitting in the middle of training ultimately affects the model’s final generalization performance.

To address this issue, we consider introducing perturbations to optimize the sharpness of the loss function, guiding the model to find flatter minimum regions. Specifically, we aim to introduce random perturbations \( \epsilon \), enabling the model to explore broader regions of the loss landscape and avoid getting trapped in sharp minima. Next, we will delve into the motivation behind perturbation-based optimization.

\subsection{Motivation Based on Perturbation Optimization}

Implicit regularization mechanisms encourage optimization algorithms to select specific solutions (e.g., flat minima), even if they are not explicitly included in the loss function. For example, SAM adds perturbations \( \mathcal{L}(\theta + \epsilon) \) to the loss function, where \( \epsilon \) is a random perturbation vector, encouraging the model to find solutions insensitive to perturbations. The core mechanism of implicit regularization lies in the fact that the optimization process naturally favors regions where the loss function changes less (i.e., flat minima, corresponding to solutions insensitive to noise), thereby reducing the model’s sensitivity to noise \citep{li2025implicit}.

\subsection{LoRA-GGPO Framework}

The core innovation of GGPO lies in generating targeted perturbations using gradient information, reducing the computational and memory overhead of traditional SAM methods while retaining their ability to find flat minima. By incorporating a weighted combination of gradient norms and weight norms, GGPO ensures that perturbations are concentrated in directions where the loss function changes most significantly (corresponding to the most sensitive parameter regions of the model), enhancing exploration efficiency.

Traditional SAM \citep{foret2020sharpness} aims to simultaneously minimize the loss value and smooth the loss landscape by solving the following min-max problem:
\begin{equation}
\min_{\mathbf{W}'} \max_{\|\boldsymbol{\epsilon}\|_2 \leq \rho} L(\mathbf{W}' + \boldsymbol{\epsilon}),
\end{equation}

where \( L(\mathbf{W}' + \boldsymbol{\epsilon}) \) represents the loss value after adding perturbation \( \boldsymbol{\epsilon} \) to the parameters \( \mathbf{W}' \), and \( \rho \) is the perturbation radius. By explicitly searching for the maximum loss in the perturbation space, SAM encourages the model to find solutions insensitive to perturbations (flat minima). However, this explicit min-max optimization approach has two main limitations: (1) the computational cost of maximizing perturbation loss; and (2) the high memory overhead of storing perturbations.

To address these challenges, LoRA-GGPO introduces Gradient-Guided Perturbation Optimization (GGPO), which generates targeted perturbations using gradient information, reducing computational and memory costs.

\subsection{Simplified Optimization Objective}

To efficiently mitigate the double descent problem in LoRA fine-tuning, we propose a simplified optimization objective based on random perturbations, formulated as:
\begin{equation}
\min_{A, B} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2 I)} \left[ L(W + AB + \epsilon) \right],
\end{equation}
where \( W \) is the frozen pre-trained weight matrix, \( A \in \mathbb{R}^{d \times r} \) and \( B \in \mathbb{R}^{r \times d’} \) are low-rank matrices used to approximate parameter updates; \( \epsilon \sim \mathcal{N}(0, \sigma^2 I) \) is Gaussian random noise used to smooth the loss function; and \( L(W + AB + \epsilon) \) represents the loss value calculated under the perturbed weights. By introducing random perturbations \( \epsilon \), this objective helps the model find solutions insensitive to noise (i.e., flat minima), thereby improving generalization performance. Compared to traditional SAM methods that explicitly search for maximum perturbation loss, our approach simplifies the optimization steps through an expectation form, avoiding additional gradient computation overhead.

\subsection{Perturbation Generation}

To efficiently generate targeted random weight perturbations, GGPO combines weight norms and gradient information, guiding the model to find flat minima to optimize sharpness and improve generalization. Specifically, the perturbation generation takes the following form:
\begin{equation}
\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{\Sigma}),
\end{equation}
where the covariance matrix \( \mathbf{\Sigma} \) is defined as:
\begin{multline}
\mathbf{\Sigma} = \frac{\sigma^2}{n} \cdot \mathrm{diag}(\|\mathbf{W}'_{1,:}\|_2^2 + \beta \cdot \|\nabla_{\mathbf{W}'_{1,:}} L\|_2^2, \\
\cdots, \|\mathbf{W}'_{m,:}\|_2^2 + \beta \cdot \|\nabla_{\mathbf{W}'_{m,:}} L\|_2^2) \cdot \mathbf{J}_{m \times n}.
\end{multline}
Here, \( \mathbf{W}’ = W + AB \) is the combined weight matrix; \( \|\mathbf{W}’_{i,:}\|_2^2 \) represents the squared norm of the \( i \)-th row of weights, quantifying its contribution to the model output; \( \|\nabla_{\mathbf{W}’_{i,:}} L\|_2^2 \) represents the sensitivity of the loss function to changes in the \( i \)-th row of weights, indicating the most sensitive regions of the model to perturbations; \( \sigma > 0 \) controls the overall strength of the perturbation, with larger \( \sigma \) increasing the perturbation range; \( \beta > 0 \) balances the contributions of weight norms and gradient norms, with larger \( \beta \) focusing more on gradient sensitivity and smaller \( \beta \) emphasizing weight importance; \( \mathbf{J}_{m \times n} \) is a matrix of size \( m \times n \) with all elements equal to 1, used to extend scalars to matrix form.

\subsection{Gradient Norms and Weight Norms}

To compute the weight norm \( \|\mathbf{W}’_{i,:}\|_2^2 \) and gradient norm \( \|\nabla_{\mathbf{W}’_{i,:}} L\|_2^2 \), we utilize PyTorch’s automatic differentiation mechanism and simple mathematical operations. These two norms reflect the contribution of weights to the model output and the sensitivity of model parameters to changes in the loss function, respectively, and are core components of the perturbation generation formula.

The computation of the weight norm \( \|\mathbf{W}'_{i,:}\|_2^2 \) is based on the concept of filter structure \citep{bisla2022low}. Specifically, the combined weight matrix \( \mathbf{W}’ = W + AB \) contains \( m \) filters \( \mathbf{W}’_{i,:} \), and the norm of each filter is defined as:
\begin{equation}
\|\mathbf{W}'_{i,:}\|_2^2 = \sum_{j=1}^{d'} (\mathbf{W}'_{i,j})^2,
\end{equation}
where \( \mathbf{W}'_{i,j} \) is the weight element at the \( i \)-th row and \( j \)-th column. Larger weight norms indicate stronger processing capabilities of the filter for input data, thus having a more significant impact on model performance.

The gradient norm \( \|\nabla_{\mathbf{W}’_{i,:}} L\|_2^2 \) reflects the sensitivity of model parameters to changes in the loss function. This concept has been widely applied in multi-task learning, such as GradNorm dynamically adjusting gradient magnitudes to achieve adaptive loss balancing, thereby improving training efficiency and performance \citep{chen2018gradnormgradientnormalizationadaptive}. Specifically, during each training iteration, the model computes gradients \( \nabla L \) of the loss function \( L \) through backpropagation. For LoRA parameters \( A \) and \( B \), we directly extract the corresponding gradient tensors from the optimizer and compute the gradient norm for each row:
\begin{equation}
\|\nabla_{\mathbf{W}'_{i,:}} L\|_2^2 = \sum_{j=1}^{d'} (\nabla_{\mathbf{W}'_{i,j}} L)^2.
\end{equation}
where \( \nabla_{\mathbf{W}’_{i,j}} L \) is the partial derivative of the loss function with respect to \( \mathbf{W}’_{i,j} \). Larger gradient norms indicate a greater impact of the row weights on the model output.

\subsection{Computation and Memory Efficiency}
The design of GGPO takes both computation and memory efficiency into full consideration. In terms of computation, the additional costs in GGPO mainly come from calculating the weight norm, gradient norm, and sampling perturbations. The complexity of these operations is approximately \( O(m \cdot n) \), which accounts for about 5\% of the total training cost. In terms of memory, we only store the random seed and the norms of the weights and gradients (\( \|\mathbf{W}’_{i, :}\|_2^2 \) and \( \|\nabla_{\mathbf{W}’_{i, :}} L\|_2^2 \)), instead of the full perturbation matrix \( \boldsymbol{\epsilon} \), thus significantly reducing memory overhead. By fixing the random seed, we ensure the reproducibility of the perturbations, making GGPO an efficient and practical optimization method.

% , and therefore can be considered negligible

\section{Experiments}
\label{sec:experiments}

% \section{Performance Evaluation of LoRA-Opt}

% In this section, we examine LoRA-Opt's performance on various benchmark datasets. First, we evaluate its Natural Language Understanding (NLU) capabilities using a subset of the GLUE dataset with the T5-Base model. Next, we assess its performance in dialogue generation, mathematical reasoning, and code generation tasks using the Llama 2-7B model. Finally, we conduct ablation studies to confirm the method’s efficacy.

\input{tables/nlu}

% \textbf{Baselines}
% To evaluate the effectiveness of LoRA-GGPO, we compare it with several baseline approaches. 
% First, \textbf{Full Fine-Tuning} updates all model parameters but demands substantial computational resources, making it impractical for LLMs. 
% In contrast, \textbf{Vanilla LoRA} \citep{hu2021lora} incorporates a low-rank matrix product $BA$ into linear layers, where $A$ is initialized using Kaiming initialization and $B$ is set to zero, providing a more efficient alternative in terms of parameter usage.
% Next, we examine \textbf{LoRA variants with structural modifications}, which introduce architectural changes to enhance performance. 
% For instance, \textbf{DoRA} \citep{liu2024doraweightdecomposedlowrankadaptation} improves model expressiveness by incorporating learnable magnitudes, offering greater flexibility in representation. 
% Meanwhile, \textbf{AdaLoRA} \citep{zhang2023adaloraadaptivebudgetallocation} dynamically prunes less significant weights during fine-tuning using singular value decomposition (SVD), reallocating rank to critical areas within a fixed parameter budget to optimize resource utilization.
% Finally, we explore \textbf{LoRA variants that retain the original structure}, which refine the implementation while preserving the fundamental framework. 
% For example, \textbf{rsLoRA} \citep{kalajdzievski2023rankstabilizationscalingfactor} introduces a scaling factor to stabilize the scale of LoRA during training. 
% \textbf{LoRA+} \citep{hayou2024loraefficientlowrank} applies separate learning rates for matrices $A$ and $B$, enhancing adaptability and training efficiency. 
% Additionally, \textbf{PiSSA} \citep{meng2024pissaprincipalsingularvalues} leverages singular value decomposition (SVD) on the weight matrix $W$ at the start of training, initializing $A$ and $B$ based on components with larger singular values to improve initialization quality. 
% Furthermore, \textbf{LoRA-GA} \citep{wang2024loragalowrankadaptationgradient} accelerates convergence by aligning low-rank matrix gradients with those of full fine-tuning, ensuring better gradient flow throughout training. 
% Similarly, \textbf{LoRA-Pro} \citep{wang2024loraprolowrankadaptersproperly} dynamically adjusts the gradient in each optimization step to ensure that the parameter update trajectory of the entire training process is aligned with full fine-tuning, rather than just the initialization phase. 
\textbf{Baselines}  
\textbf{Baselines}  
To evaluate LoRA-GGPO, we compare it with several baselines. \textbf{Full Fine-Tuning} updates all model parameters but is computationally expensive for LLMs. \textbf{Vanilla LoRA} \citep{hu2021lora} introduces a low-rank matrix product $BA$ into linear layers, initializing $A$ with Kaiming initialization and $B$ as zero, offering parameter efficiency.  
We also examine \textbf{LoRA variants with structural modifications}. For instance, \textbf{DoRA} \citep{liu2024doraweightdecomposedlowrankadaptation} enhances expressiveness by incorporating learnable magnitudes. \textbf{AdaLoRA} \citep{zhang2023adaloraadaptivebudgetallocation} dynamically prunes less significant weights via singular value decomposition (SVD), reallocating rank to optimize resource use.  
Additionally, we explore \textbf{LoRA variants preserving the original structure}. \textbf{rsLoRA} \citep{kalajdzievski2023rankstabilizationscalingfactor} stabilizes training with a scaling factor. \textbf{LoRA+} \citep{hayou2024loraefficientlowrank} applies separate learning rates for $A$ and $B$, improving adaptability. \textbf{PiSSA} \citep{meng2024pissaprincipalsingularvalues} leverages SVD on the weight matrix $W$ at initialization, using larger singular values for better initialization. \textbf{LoRA-GA} \citep{wang2024loragalowrankadaptationgradient} aligns low-rank gradients with full fine-tuning for faster convergence. Similarly, \textbf{LoRA-Pro} \citep{wang2024loraprolowrankadaptersproperly} dynamically adjusts gradients to align the entire parameter update trajectory with full fine-tuning.
% These baselines collectively provide a comprehensive framework for assessing LoRA-GGPO.

\textbf{Implementation Details}
To ensure a fair comparison, our experimental setup aligns with that of LoRA-GA~\citep{wang2024loragalowrankadaptationgradient}. 
By default, we fine-tune the models using the AdamW optimizer~\citep{loshchilov2019decoupled}, with hyperparameters set to $\sigma=0.05$, $\beta=0.01$, $\beta_1=0.9$, $\beta_2=0.999$, and weight decay fixed at 0. A cosine learning rate schedule is applied, with a warmup ratio of 0.03. 
LoRA is utilized across all linear modules, excluding the embedding, normalization, and classification layers. Unless otherwise specified, the rank $r$ is set to 8, and the scaling factor $\alpha$ is set to 16. 
For natural language understanding tasks, we fine-tune the T5-base~\citep{raffel2020exploring} model with a learning rate of 1e-4. The sequence length is capped at 128, and the training batch size is set to 32. The random perturbation strength $\sigma$ is set to 0.03. $\beta$ is set to 0.01.
In tasks involving dialogue generation, mathematical reasoning, and code generation, we fine-tune the Llama-2-7B~\citep{touvron2023llama} model with a learning rate of 2e-5. The sequence length is extended to 1024, and the macro batch size is configured to 32. The random perturbation strength $\sigma$ is set to 0.03. $\beta$ is set to 0.01.
All experiments were conducted on NVIDIA A40 (48G) GPUs. To ensure reliable and robust results, each experiment was repeated three times with different random seeds. The final results are reported as the average of these runs, along with the standard deviation to account for variability.
\input{tables/nlg}

\subsection{Results on Natural Language Understanding Tasks}
We evaluate the performance of our proposed method, LoRA-GGPO, on a series of NLU tasks from the GLUE benchmark \citep{wang2018glue}: MNLI, SST2, CoLA, QNLI, and MRPC. The experiments are conducted using the T5-base model~\citep{raffel2020exploring}, with the LoRA rank fixed at 8. To provide a comprehensive comparison, we include several baseline methods: full fine-tuning, standard LoRA~\citep{hu2021lora}, and its variants, including DoRA~\citep{liu2024doraweightdecomposedlowrankadaptation}, AdaLoRA~\citep{zhang2023adaloraadaptivebudgetallocation}, rsLoRA~\citep{kalajdzievski2023rankstabilizationscalingfactor}, LoRA+~\citep{hayou2024loraefficientlowrank}, PiSSA~\citep{meng2024pissaprincipalsingularvalues}, LoRA-GA~\citep{wang2024loragalowrankadaptationgradient}, and LoRA-Pro \citep{wang2024loraprolowrankadaptersproperly}. The results are summarized in Table~\ref{tab:addlabel}.

Our experimental results demonstrate that LoRA-GGPO achieves the highest average performance across all NLU tasks, outperforming both full fine-tuning and other LoRA variants, as shown in Table~\ref{tab:addlabel}. Notably, LoRA-GGPO surpasses LoRA-Pro, which is the second-best performing method, by an aLoRA-GGPO’sin of \textbf{1.26} points. Specifically, on the MRPC dataset, LoRA-GGPO achieves an accuracy of 86.78, compared to LoRA-Pro’s 86.60. Similarly, on the CoLA dataset, LoRA-GGPO achieves a score of 82.13, surpassing LoRA-Pro’s 81.94. This consistent improvement can be attributed to LoRA-GGPO's ability to mitigate the double descent phenomenon by optimizing the sharpness of the loss landscape through gradient-guided perturbations. Compared to other methods, LoRA-GGPO exhibits stronger generalization, particularly on small datasets with limited training data. These findings validate the effectiveness of LoRA-GGPO as a robust and efficient solution for parameter-efficient fine-tuning in NLU tasks.

\label{subsec:nlu}



\subsection{Results on Natural Language Generation Tasks}

We evaluate the performance of our proposed method, LoRA-GGPO, on NLG tasks, focusing on three key capabilities: dialogue generation, mathematical reasoning, and code generation. Our experimental setup follows the configuration used in LoRA-GA~\citep{wang2024loragalowrankadaptationgradient}. Specifically, the experiments are conducted using the Llama-2-7B model~\citep{touvron2023llama}, with a default rank of 8 and $\alpha=16$. The scaling factor is initialized as $s = \frac{\alpha}{\sqrt{r}}$, consistent with rsLoRA~\citep{kalajdzievski2023rankstabilizationscalingfactor}. For dialogue generation, we fine-tune the model on a 52k subset of the WizardLM dataset~\citep{xu2024wizardlm} and evaluate it using the MT-Bench dataset~\citep{zheng2024judging}, reporting the first-turn score assessed by GPT-4. For mathematical reasoning, we fine-tune the model on a 100k sample from the MetaMathQA dataset~\citep{yu2024metamath} and evaluate it on the GSM8K test set~\citep{cobbe2021training}, reporting accuracy. For code generation, we fine-tune the model on a 100k subset of the CodeFeedback dataset~\citep{zheng2024opencodeinterpreter} and test it on the HumanEval dataset~\citep{chen2021evaluating}, reporting the PASS@1 metric. We compare LoRA-GGPO with several baselines, including full fine-tuning, standard LoRA~\citep{hu2021lora}, and its variants, including DoRA~\citep{liu2024doraweightdecomposedlowrankadaptation}, AdaLoRA~\citep{zhang2023adaloraadaptivebudgetallocation}, rsLoRA~\citep{kalajdzievski2023rankstabilizationscalingfactor}, LoRA+~\citep{hayou2024loraefficientlowrank}, PiSSA~\citep{meng2024pissaprincipalsingularvalues}, LoRA-GA~\citep{wang2024loragalowrankadaptationgradient}, and LoRA-Pro~\citep{wang2024loraprolowrankadaptersproperly}. The results are summarized in Table~\ref{tab:addlabel2}.


% \subsection{Performance Analysis and Conclusion}
Our experimental results demonstrate that LoRA-GGPO consistently outperforms other methods across all NLG tasks, as shown in Table~\ref{tab:addlabel2}. Specifically, LoRA-GGPO achieves the highest scores on two out of three tasks: MT-Bench (6.23) and HumanEval (22.90), while achieving competitive performance on GSM8K (54.52). Compared to LoRA-Pro, the second-best performing method, LoRA-GGPO shows clear improvements: 0.37 points higher on MT-Bench, 0.29 points higher on GSM8K, and 0.14 points higher on HumanEval. When increasing the rank to 32 and 128, LoRA-GGPO further narrows the gap with full fine-tuning while maintaining superior scalability. For example, at rank 128, LoRA-GGPO achieves a PASS@1 score of 30.84 on HumanEval, surpassing LoRA-Pro (34.55) and approaching full fine-tuning (35.31). These results validate the effectiveness of LoRA-GGPO in mitigating the double descent phenomenon through gradient-guided perturbations, enabling stronger generalization and robust performance across various NLG tasks and rank settings.

\begin{figure}[!h]
\centering
\includegraphics[width=.98\columnwidth]{figures/double.pdf}

\caption{Analysis of double descent: Comparing LoRA-GGPO with Vanilla LoRA on T5-base model fine-tuned for SST-2 from GLUE benchmark.}
\label{fig: double}
\end{figure}

\subsection{Analysis of the effectiveness of mitigating double descent}

In this experiment, we assess the effectiveness of the proposed LoRA-GGPO method in mitigating the double descent phenomenon during the fine-tuning process. The experiment utilizes the T5-base model, fine-tuned on the GLUE benchmark’s SST-2 task. The primary aim is to evaluate how LoRA-GGPO improves upon traditional LoRA fine-tuning, specifically in terms of enhancing generalization and reducing performance degradation during the fine-tuning procedure. 
% LoRA-GGPO introduces gradient-guided perturbation optimization to direct the model towards flatter minima, alleviating the overfitting issues commonly induced by the low-rank constraints in standard LoRA.

The results, as illustrated in Figure~\ref{fig: double}, clearly demonstrate the advantages of LoRA-GGPO over the conventional LoRA method. Both the training and evaluation curves reveal that LoRA-GGPO (shown in the right plot) maintains more stable performance throughout the training steps compared to the standard LoRA (left plot). The insets focusing on the first 300 steps further highlight that LoRA-GGPO exhibits less fluctuation and improved convergence, effectively avoiding the sharp performance drops typically associated with the double descent phenomenon. This observation supports the hypothesis that LoRA-GGPO, by optimizing the sharpness of the loss landscape, guides the model towards a flatter minimum, thus mitigating the double descent issue and enhancing the overall robustness of the fine-tuned model.

% In contrast, the LoRA fine-tuning curve shows a rapid initial decline followed by a plateau, which suggests the occurrence of overfitting as training progresses.

% Our results, as summarized in Table \ref{tab:Llama-result2x}, indicate that \ours outperforms or is comparable to other methods, including full-finetuning. Specifically, \ours achieves superior performance on both the GSM8K and Human-eval datasets, underscoring its effectiveness in handling tasks with higher complexity and diversity. On MT-Bench, \ours also demonstrates competitive performance, although it slightly trails behind DoRA. Nevertheless, \ours achieves this with fewer parameters and approximately 70\% of the training time required by DoRA. Additionally, as illustrated in Figure \ref{fig:fig_ablation} (Left), our method exhibits a significantly faster convergence rate compared to Vanilla LoRA, with convergence rates comparable to those of full-finetuning.
% \paragraph{Effect of Rank}
% We attribute the performance discrepancies on the GSM8K and Human-eval datasets, when compared to full-finetuning, primarily to the representational limitations imposed by the low-rank approximation. To address this, we experimented with higher ranks, specifically rank=32 and rank=128. Our findings reveal that \ours maintains stability across different rank settings and, in some cases, even surpasses full-finetuning performance. As shown in Figure \ref{fig:fig_ablation} (Left), higher ranks with our initialization also result in loss curves that closely resemble those of full-finetuning.
% \input{tables/ablation}

\begin{figure}[!h]
\centering
\includegraphics[width=.98\columnwidth]{figures/ablation.pdf}

\caption{Ablation Study on GLUE Benchmark with T5-Base: Performance Comparison of Different Normalization Methods. The LoRA Rank is set to 8.}
\label{fig: ablation}
\end{figure}

\subsection{Ablation Study}

We conducted ablation studies to evaluate the contributions of weight normalization and gradient normalization in LoRA-GGPO. For NLG tasks, we used the LLaMA-2-7B model and evaluated its performance on three benchmark datasets: MT-Bench for dialogue generation, GSM8K for mathematical reasoning, and HumanEval for code generation. For NLU tasks, we employed the T5-base model and tested it on five subtasks from the GLUE benchmark: MNLI, SST2, CoLA, QNLI, and MRPC. The results were averaged across these tasks to provide a comprehensive evaluation. In all experiments, the rank of LoRA was fixed at 8 to ensure consistency.

The ablation results, summarized in Figure~\ref{fig: ablation}, demonstrate the effectiveness of LoRA-GGPO. On NLU tasks, LoRA-GGPO achieves the highest average score of 88.70, surpassing full fine-tuning by 0.79 points. Notably, combining weight and gradient normalization yields superior results compared to using either technique alone, with the + Weight Norm achieving 86.85 and the + Gradient Norm reaching 87.66. For NLG tasks, LoRA-GGPO similarly excels, achieving state-of-the-art performance on MT-Bench and HumanEval while maintaining competitive results on GSM8K. These findings confirm that both normalization techniques contribute significantly to mitigating the Double Descent phenomenon and enhancing generalization.

 % and outperforming other configurations on four out of five subtasks

% \subsection{Memory Costs and Running Time}

% We benchmark ours on a single RTX 3090 24GB GPU, a 128-core CPU, and 256GB of RAM. As shown in Table \ref{tab:tab_cost}, the memory consumption of our new method does not exceed that used for training with LoRA, indicating no extra memory is needed. Additionally, the time cost of this operation is relatively negligible compared to the subsequent fine-tuning process. For instance, in the Code-Feedback task, the training process took approximately 10 hours, while the initialization required only about 1 minute, which is insignificant.

% \begin{table}[H]
%     \centering
%     \caption{Memory and Time Costs for Initialization and Fine-Tuning. "Parameters" indicates the number of parameters in the model, "Time(\oursns)" represents the time required for initialization, "Memory(\oursns)" shows the memory usage during initialization, "LoRA" and "Full-FT" display the memory usage during LoRA and full fine-tuning, respectively.}
%     \label{tab:tab_cost}
%     \begin{tabular}{lccccc}
%     \toprule
%          & Parameters & Time(\oursns) & Memory(\oursns) & LoRA & Full-FT \\
%          \midrule
%         T5-Base & 220M & 2.8s & 1.69G & 2.71G & 3.87G \\
%         Llama 2-7B & 6738M & 74.7s & 18.77G & 23.18G & 63.92G \\ 
%         \bottomrule
%     \end{tabular}
% \end{table}


\section{Conclusion}

The main objective of this work was to address the “double descent” phenomenon in Low-Rank Adaptation (LoRA) fine-tuning for LLMs, which often results in overfitting and limited generalization due to low-rank constraints. To achieve this, we proposed LoRA-GGPO (Gradient-Guided Perturbation Optimization), a novel method that leverages gradient and weight norms to generate targeted perturbations, thereby guiding the model toward flatter minima in the loss landscape. Extensive experiments on natural language understanding and generation tasks demonstrated that LoRA-GGPO consistently outperforms LoRA and its state-of-the-art variants while effectively alleviating the double descent problem. Our work provides a robust and efficient solution for parameter-efficient fine-tuning, paving the way for more reliable and scalable applications of LLMs in real-world scenarios.

\subsection{Limitations}

While our proposed method, LoRA-GGPO, demonstrates significant improvements in mitigating the Double Descent phenomenon and enhancing generalization across various NLG tasks, it is not without limitations. First, the gradient-guided perturbation mechanism may exhibit sensitivity to noise or variations in data distribution, potentially affecting its robustness in scenarios with noisy or imbalanced datasets. Second, while LoRA-GGPO outperforms other LoRA variants in scalability, the performance of models fine-tuned by our method falls slightly below that of full fine-tuning on certain tasks. Due to computational resource constraints, we have not yet explored the fine-tuning of extremely large models with significantly higher parameter counts, leaving room for further investigation in scenarios demanding ultra-high model capacity. Lastly, the current implementation is primarily designed for LLMs, and its applicability to other model families or modalities (e.g., vision models) has yet to be explored. These limitations offer valuable avenues for future research, such as improving robustness to noise and optimizing performance for larger-scale models.

\bibliography{refs}

\newpage
\appendix

\vspace{2em}
\begin{center}
    \Large{\textbf{appendix}}
\end{center}
\vspace{2em}

\etocdepthtag.toc{appendix}
\etocsettagdepth{chapter}{none}
\etocsettagdepth{appendix}{subsection}
\tableofcontents


\section{Models of Datasets}

\subsection{Details of Models}

In this work, we primarily utilize two pre-trained language models: LLaMA-2-7B and T5-base. Below, we provide a brief overview of these models along with their respective configurations.

\begin{itemize}
    \item \textbf{LLaMA-2-7B}: This is a large language model developed by Meta, featuring 7 billion parameters. It is part of the LLaMA-2 series, which is known for its strong performance across various natural language understanding and generation tasks. The model architecture is based on the transformer decoder, making it particularly effective for autoregressive tasks such as text generation. More details about the model can be found on its Hugging Face page\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b}}.
    
    \item \textbf{T5-base}: The Text-to-Text Transfer Transformer (T5) is a versatile encoder-decoder model introduced by Google. The base version of T5 consists of approximately 220 million parameters and is widely used for a variety of tasks, including translation, summarization, and question-answering. Its unified text-to-text framework allows for seamless adaptation to different downstream applications. Additional information about T5-base is available on its Hugging Face repository\footnote{\url{https://huggingface.co/t5-base}}.
\end{itemize}

Both models were fine-tuned on our specific datasets to align with this study’s objectives experiments were conducted using the implementations provided by Hugging Face’s Transformers library.

\input{appendix/glue}

\subsection{Details of Datasets}


Table \ref{tab: glue_datasets} summarizes the GLUE benchmark datasets and their respective evaluation metrics \citep{wang2018glue}. The GLUE benchmark encompasses a variety of natural language understanding tasks, such as grammatical correctness (CoLA \citep{cola}), sentiment classification (SST-2 \citep{sst2}), paraphrase identification (MRPC \citep{mrpc}), natural language inference (MNLI \citep{mnli}, QNLI \citep{qnli}). The datasets differ significantly in size, with training examples ranging from as many as 393,000 in MNLI. These tasks involve either binary or multi-class classification. Each task uses specific evaluation metrics, including accuracy, F1 score, Matthews correlation coefficient, and Pearson/Spearman correlation coefficients, depending on the nature of the task. This extensive collection serves as a standardized benchmark for evaluating and comparing model performance across a broad spectrum of linguistic challenges.

Specific metrics are applied to natural language generation (NLG) tasks. For instance, Accuracy is used for GSM8K; Pass@1 is employed for HumanEval, representing the proportion of initially generated code snippets that successfully pass all unit tests; and GPT-4 Evaluation is utilized for MT-Bench, where GPT-4 evaluates the quality of the model’s outputs.

% \section{Baselines}
% To evaluate the effectiveness of LoRA-GGPO, we compare it with several baseline approaches. 
% First, \textbf{Full Fine-Tuning} updates all model parameters but demands substantial computational resources, making it impractical for LLMs. 
% In contrast, \textbf{Vanilla LoRA} \citep{hu2021lora} incorporates a low-rank matrix product $BA$ into linear layers, where $A$ is initialized using Kaiming initialization and $B$ is set to zero, providing a more efficient alternative in terms of parameter usage.

% Next, we examine \textbf{LoRA variants with structural modifications}, which introduce architectural changes to enhance performance. 
% For instance, \textbf{DoRA} \citep{liu2024doraweightdecomposedlowrankadaptation} improves model expressiveness by incorporating learnable magnitudes, offering greater flexibility in representation. 
% Meanwhile, \textbf{AdaLoRA} \citep{zhang2023adaloraadaptivebudgetallocation} dynamically prunes less significant weights during fine-tuning using singular value decomposition (SVD), reallocating rank to critical areas within a fixed parameter budget to optimize resource utilization.

% Finally, we explore \textbf{LoRA variants that retain the original structure}, which refine the implementation while preserving the fundamental framework. 
% For example, \textbf{rsLoRA} \citep{kalajdzievski2023rankstabilizationscalingfactor} introduces a scaling factor to stabilize the scale of LoRA during training. 
% \textbf{LoRA+} \citep{hayou2024loraefficientlowrank} applies separate learning rates for matrices $A$ and $B$, enhancing adaptability and training efficiency. 
% Additionally, \textbf{PiSSA} \citep{meng2024pissaprincipalsingularvalues} leverages singular value decomposition (SVD) on the weight matrix $W$ at the start of training, initializing $A$ and $B$ based on components with larger singular values to improve initialization quality. 
% Furthermore, \textbf{LoRA-GA} \citep{wang2024loragalowrankadaptationgradient} accelerates convergence by aligning low-rank matrix gradients with those of full fine-tuning, ensuring better gradient flow throughout training. 
% Similarly, \textbf{LoRA-Pro} \citep{wang2024loraprolowrankadaptersproperly} dynamically adjusts the gradient in each optimization step to ensure that the parameter update trajectory of the entire training process is aligned with full fine-tuning, rather than just the initialization phase. 
% \section{Appendix: Algorithm Details}

% \subsection{Explanation of GGPO}

% The proposed **Gradient-Guided Perturbation Optimization (GGPO)** method addresses the double descent problem in LoRA fine-tuning by introducing targeted perturbations guided by gradient information. GGPO combines weight norms ($ \|W'_{i,:}\|_2^2 $) and gradient norms ($ \|\nabla_{W'_{i,:}} L\|_2^2 $) to construct a covariance matrix $ \Sigma $, ensuring that perturbations are concentrated in regions where the loss function is most sensitive. This approach reduces computational and memory overhead compared to traditional methods like SAM while improving the model's ability to find flat minima, thereby enhancing generalization performance.

% By sampling random perturbations $ \epsilon \sim \mathcal{N}(0, \Sigma) $, GGPO encourages the model to explore broader regions of the loss landscape and avoid sharp minima. Additionally, the method ensures efficiency by storing only random seeds and norms instead of the full perturbation matrix, with additional computational costs accounting for approximately 5\% of the total training cost. The pseudo-code below outlines the implementation details of GGPO.

% \subsection{Pseudo-code for Gradient-Guided Perturbation Optimization (GGPO)}

% To provide a clear understanding of the GGPO algorithm, we present its pseudo-code in a two-column format below:

% \begin{multicols}{2}
% \begin{algorithm}[H]
% \caption{Gradient-Guided Perturbation Optimization (GGPO)}
% \label{alg:ggpo}
% \textbf{Input:} 
% \begin{itemize}
%     \item Pre-trained weights $ W \in \mathbb{R}^{d \times d'} $
%     \item Low-rank matrices $ A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{r \times d'} $ (randomly initialized)
%     \item Learning rate $ \eta $
%     \item Perturbation strength $ \sigma > 0 $
%     \item Balance factor $ \beta > 0 $
% \end{itemize}
% \textbf{Output:} Fine-tuned low-rank matrices $ A, B $

% \begin{algorithmic}[1]
% \State Freeze pre-trained weights $ W $
% \State Initialize optimizer (e.g., Adam) for $ A $ and $ B $
% \While{not converged}
%     \State Compute combined weight matrix: $ W' = W + AB $
%     \State Compute loss: $ L(W') $
%     \State Compute gradients of $ L(W') $ w.r.t. $ A $ and $ B $ using backpropagation
%     \State Compute weight norms for each row of $ W' $:
%     \begin{equation}
%     \|W'_{i,:}\|_2^2 = \sum_{j=1}^{d'} (W'_{i,j})^2, \quad \forall i \in \{1, \dots, m\}
%     \end{equation}
%     \State Compute gradient norms for each row of $ W' $:
%     \begin{equation}
%     \|\nabla_{W'_{i,:}} L\|_2^2 = \sum_{j=1}^{d'} (\nabla_{W'_{i,j}} L)^2, \quad \forall i \in \{1, \dots, m\}
%     \end{equation}
%     \State Construct covariance matrix $ \Sigma $:
%     \begin{multline}
%     \Sigma = \frac{\sigma^2}{n} \cdot \mathrm{diag}(\|W'_{1,:}\|_2^2 + \beta \cdot \|\nabla_{W'_{1,:}} L\|_2^2, \\
%     \dots, \|W'_{m,:}\|_2^2 + \beta \cdot \|\nabla_{W'_{m,:}} L\|_2^2) \cdot J_{m \times n}
%     \end{multline}
%     \State Sample perturbation $ \epsilon \sim \mathcal{N}(0, \Sigma) $
%     \State Compute perturbed loss: $ L_{\text{perturbed}} = L(W' + \epsilon) $
%     \State Compute gradients of $ L_{\text{perturbed}} $ w.r.t. $ A $ and $ B $
%     \State Update $ A $ and $ B $ using the optimizer:
%     \begin{align}
%     A &\gets A - \eta \cdot \nabla_A L_{\text{perturbed}} \\
%     B &\gets B - \eta \cdot \nabla_B L_{\text{perturbed}}
%     \end{align}
% \EndWhile
% \State \textbf{Return} fine-tuned $ A, B $
% \end{algorithmic}
% \end{algorithm}
% \end{multicols}

\end{document}
