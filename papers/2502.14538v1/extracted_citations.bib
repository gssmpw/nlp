@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@misc{chang2024balorabiasalleviatinglowrankadaptation,
      title={BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models}, 
      author={Yupeng Chang and Yi Chang and Yuan Wu},
      year={2024},
      eprint={2408.04556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.04556}, 
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{ding2023parameter,
  title={Parameter-efficient fine-tuning of large-scale pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={Nature Machine Intelligence},
  volume={5},
  number={3},
  pages={220--235},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{dong2024fine,
  title={Fine-tuning and deploying large language models over edges: Issues and approaches},
  author={Dong, Yanjie and Zhang, Haijun and Li, Chengming and Guo, Song and Leung, Victor and Hu, Xiping},
  journal={arXiv preprint arXiv:2408.10691},
  year={2024}
}

@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}

@inproceedings{fu2023effectiveness,
  title={On the effectiveness of parameter-efficient fine-tuning},
  author={Fu, Zihao and Yang, Haoran and So, Anthony Man-Cho and Lam, Wai and Bing, Lidong and Collier, Nigel},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={11},
  pages={12799--12807},
  year={2023}
}

@article{han2024parameter,
  title={Parameter-efficient fine-tuning for large models: A comprehensive survey},
  author={Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
  journal={arXiv preprint arXiv:2403.14608},
  year={2024}
}

@misc{hayou2024loraefficientlowrank,
      title={LoRA+: Efficient Low Rank Adaptation of Large Models}, 
      author={Soufiane Hayou and Nikhil Ghosh and Bin Yu},
      year={2024},
      eprint={2402.12354},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12354}, 
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@misc{kalajdzievski2023rankstabilizationscalingfactor,
      title={A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA}, 
      author={Damjan Kalajdzievski},
      year={2023},
      eprint={2312.03732},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.03732}, 
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@misc{liu2024doraweightdecomposedlowrankadaptation,
      title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
      author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
      year={2024},
      eprint={2402.09353},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09353}, 
}

@misc{luo2024investigatingimpactmodelcomplexity,
      title={Investigating the Impact of Model Complexity in Large Language Models}, 
      author={Jing Luo and Huiyuan Wang and Weiran Huang},
      year={2024},
      eprint={2410.00699},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.00699}, 
}

@article{mao2025survey,
  title={A survey on lora of large language models},
  author={Mao, Yuren and Ge, Yuhang and Fan, Yijiang and Xu, Wenyi and Mi, Yu and Hu, Zhonghao and Gao, Yunjun},
  journal={Frontiers of Computer Science},
  volume={19},
  number={7},
  pages={197605},
  year={2025},
  publisher={Springer}
}

@misc{meng2024pissaprincipalsingularvalues,
      title={PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models}, 
      author={Fanxu Meng and Zhaohui Wang and Muhan Zhang},
      year={2024},
      eprint={2404.02948},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.02948}, 
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}

@article{wang2024lora,
  title={Lora-ga: Low-rank adaptation with gradient approximation},
  author={Wang, Shaowen and Yu, Linxi and Li, Jian},
  journal={arXiv preprint arXiv:2407.05000},
  year={2024}
}

@misc{wang2024loragalowrankadaptationgradient,
      title={LoRA-GA: Low-Rank Adaptation with Gradient Approximation}, 
      author={Shaowen Wang and Linxi Yu and Jian Li},
      year={2024},
      eprint={2407.05000},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.05000}, 
}

@misc{wang2024loraprolowrankadaptersproperly,
      title={LoRA-Pro: Are Low-Rank Adapters Properly Optimized?}, 
      author={Zhengbo Wang and Jian Liang and Ran He and Zilei Wang and Tieniu Tan},
      year={2024},
      eprint={2407.18242},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.18242}, 
}

@article{zaken2021bitfit,
  title={Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2106.10199},
  year={2021}
}

@misc{zhang2023adaloraadaptivebudgetallocation,
      title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, 
      author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2303.10512},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10512}, 
}

