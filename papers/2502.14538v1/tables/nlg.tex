\begin{table*}[htbp]
  \centering
  \caption{Fine-tuning results for Llama-2-7B model on NLG-related tasks. The LoRA Rank is set to 8. The best and second-best results are highlighted in \textbf{bold} and {\underline{underline}}.}
    \begin{tabular}{l|ccc|c}
    \toprule
    \textbf{Method} & \textbf{MT-Bench} & \textbf{GSM8K} & \textbf{HumanEval} & \textbf{Avg} \\
    \midrule
    Full FT & 5.30$_{\pm0.11}$ & \textbf{59.36$_{\pm0.85}$} & \textbf{35.31$_{\pm2.13}$} & \textbf{33.32} \\
    LoRA  & 5.61$_{\pm0.10}$ & 42.08$_{\pm0.04}$ & 14.76$_{\pm0.17}$ & 20.82  \\
    \midrule
    DoRA  & \underline{5.97$_{\pm0.02}$} & 53.07$_{\pm0.75}$ & 19.75$_{\pm0.41}$ & 26.26  \\
    AdaLoRA & 5.57$_{\pm0.05}$ & 50.72$_{\pm1.39}$ & 17.80$_{\pm0.44}$ & 24.70  \\
    \midrule
    PiSSA & 5.30$_{\pm0.02}$ & 44.54$_{\pm0.27}$ & 16.02$_{\pm0.78}$ & 21.95  \\
    rsLoRA & 5.25$_{\pm0.03}$ & 45.62$_{\pm0.10}$ & 16.01$_{\pm0.79}$ & 22.29  \\
    LoRA+ & 5.71$_{\pm0.08}$ & 52.11$_{\pm0.62}$ & 18.17$_{\pm0.52}$ & 25.33  \\
    \midrule
    LoRA-GA & 5.95$_{\pm0.16}$ & 53.60$_{\pm0.30}$ & 19.81$_{\pm1.46}$ & 26.45  \\
    LoRA-GA (rank=32) & 5.79$_{\pm0.09}$ & 55.12$_{\pm0.30}$ & 20.18$_{\pm0.19}$ & 27.03  \\
    LoRA-GA (rank=128) & 6.13$_{\pm0.07}$ & 55.07$_{\pm0.18}$ & 23.05$_{\pm0.37}$ & 28.08  \\
    \midrule
    LoRA-Pro & 5.86$_{\pm0.06}$ & 54.23$_{\pm0.79}$ & 22.76$_{\pm0.35}$ & 27.62  \\
    LoRA-Pro (rank=32) & 6.01$_{\pm0.05}$ & 55.14$_{\pm1.73}$ & 28.05$_{\pm0.00}$ & 29.73  \\
    LoRA-Pro (rank=128) & 5.68$_{\pm0.14}$ & 56.48$_{\pm0.23}$ & \underline{34.55$_{\pm2.46}$} & 32.24  \\
    \midrule
    LoRA-GGPO & \textbf{6.23$_{\pm0.15}$} & \underline{54.52$_{\pm0.41}$} & \underline{22.90$_{\pm0.28}$} & \underline{27.88}  \\
    LoRA-GGPO (rank=32) & 6.09$_{\pm0.12}$ & 55.29$_{\pm0.27}$ & 28.31$_{\pm0.55}$ & 29.90  \\
    LoRA-GGPO (rank=128) & 6.37$_{\pm0.17}$ & 56.33$_{\pm0.71}$ & 30.84$_{\pm0.71}$ & 31.18  \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel2}%
\end{table*}%

