\section{Related Work}
% \label{sec:related}

% \textbf{Parameter-Efficient Fine-Tuning}
% The widespread adoption of LLMs in natural language processing (NLP) has highlighted the need for efficient fine-tuning methods. Traditional fine-tuning approaches, which update all model parameters, are computationally expensive and require significant storage, especially as model sizes grow \citep{howard2018universal, devlin2018bert}. To address these challenges, Parameter-Efficient Fine-Tuning (PEFT) techniques have been developed, aiming to adapt pre-trained models to downstream tasks by modifying only a subset of parameters \citep{houlsby2019parameter, lester2021power, fu2023effectiveness, ding2023parameter, han2024parameter}.

% Among PEFT methods, Low-Rank Adaptation (LoRA) has gained prominence due to its effectiveness in reducing the number of trainable parameters without compromising performance \citep{hu2021lora}. LoRA achieves this by introducing low-rank matrix decompositions into the model architecture, allowing fine-tuning to focus on smaller, more manageable parameter subsets. This approach not only enhances computational efficiency but also maintains performance comparable to full fine-tuning. Other notable PEFT methods include Adapter Layers \citep{houlsby2019parameter}, which insert small bottleneck layers within the network; BitFit \citep{zaken2021bitfit}, which fine-tunes only the bias terms; and Prefix-Tuning \citep{li2021prefix}, which optimizes continuous prompts while keeping the rest of the model fixed. These techniques collectively demonstrate that efficient adaptation of LLMs is achievable without extensive parameter modifications, facilitating the deployment of large-scale models in resource-constrained environments.

% Despite significant advancements, PEFT methods face challenges in balancing efficiency and performance as model complexity increases. LoRA, with its low-rank decomposition strategy, offers a scalable solution by effectively managing the trade-off between parameter reduction and performance retention \citep{han2024parameter}.
% Rank-Stabilized LoRA (rsLoRA) \citep{kalajdzievski2023rankstabilizationscalingfactor} enhances learning efficiency and performance in high-rank scenarios by adjusting the adapter scaling factor to the square root of the rank. 
% LoRA+ \citep{hayou2024loraefficientlowrank} enhances training efficiency by assigning higher learning rates to matrix B compared to matrix A, leading to faster convergence and improved performance. 
% LoRA-GA \citep{wang2024loragalowrankadaptationgradient} enhances the initialization process by aligning the gradients of low-rank matrices with those of full fine-tuning at the outset, thereby accelerating convergence and improving performance. 
% LoRA-Pro \citep{wang2024loraprolowrankadaptersproperly} dynamically adjusts the gradient in each optimization step to ensure that the parameter update trajectory of the entire training process is aligned with the full fine-tuning, rather than just the initialization phase.
% BA-LoRA \citep{chang2024balorabiasalleviatinglowrankadaptation} introduces three regularization terms—consistency, diversity, and singular value decomposition—to the LoRA fine-tuning process, enhancing model performance and mitigating issues like "catastrophic inheritance". 
% PiSSA \citep{meng2024pissaprincipalsingularvalues} enhances parameter-efficient fine-tuning by initializing low-rank adaptation matrices with the principal singular values and vectors of the original model's weight matrix, leading to faster convergence and improved performance compared to traditional methods like LoRA.
% DoRA \citep{liu2024doraweightdecomposedlowrankadaptation} enhances parameter-efficient fine-tuning by decomposing pre-trained weights into magnitude and direction components, applying LoRA specifically to the directional updates, thereby improving learning capacity and stability without adding inference overhead.
% AdaLoRA \citep{zhang2023adaloraadaptivebudgetallocation} enhances the LoRA fine-tuning process by enabling the model to learn the importance of parameters and dynamically adjust the rank of adaptation matrices for each module, thereby improving efficiency and performance. 


% \textbf{Double Descent Phenomenon and Optimization Strategies}
% The Double Descent phenomenon has emerged as a critical concept in deep learning, revealing a non-monotonic relationship between model complexity and generalization performance \citep{belkin2019reconciling, nakkiran2021deep}. Initially, increasing model complexity leads to improved performance, followed by a regime where further complexity results in decreased performance, and ultimately, additional complexity again enhances performance. This challenges the classical bias-variance trade-off and necessitates new perspectives on model selection and training dynamics.

% In the context of fine-tuning LLMs using PEFT methods like LoRA, Double Descent manifests as fluctuations in model performance relative to the rank of the low-rank matrices \citep{luo2024investigatingimpactmodelcomplexity}. Specifically, increasing the rank initially enhances performance by providing greater adaptation capacity. However, beyond a critical threshold, additional complexity induces overfitting, degrading performance before potentially improving again with further rank increases. This instability poses significant challenges for the reliable deployment of LoRA in large-scale settings.

% To mitigate the adverse effects of Double Descent, advanced optimization strategies have been proposed. Gradient Approximation (GA) techniques aim to align the gradients of low-rank matrices with those derived from full-parameter fine-tuning, thereby optimizing the direction of parameter updates and reducing the impact of Double Descent \citep{wang2024lora}. Additionally, Sharpness-Aware Minimization (SAM) enhances model generalization by modifying the loss landscape to favor flatter minima, reducing sensitivity to parameter variations \citep{foret2020sharpness}. Integrating GA and SAM with LoRA has shown promise in stabilizing performance and improving the reliability of fine-tuned models \citep{mao2025survey}.

% Recent studies have explored the synergistic effects of combining multiple optimization techniques to counteract Double Descent. For instance, integrating GA with SAM provides a dual approach to both align gradient directions and smooth the loss landscape, offering a robust framework for enhancing the generalization capabilities of PEFT methods \citep{dong2024fine}. These combined strategies not only offer new theoretical insights into the Double Descent phenomenon but also demonstrate practical improvements in model performance, paving the way for more stable and efficient fine-tuning of LLMs \citep{han2024parameter}.