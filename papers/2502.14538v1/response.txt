\section{Related Work}
% \label{sec:related}

% \textbf{Parameter-Efficient Fine-Tuning}
% The widespread adoption of LLMs in natural language processing (NLP) has highlighted the need for efficient fine-tuning methods. Traditional fine-tuning approaches, which update all model parameters, are computationally expensive and require significant storage, especially as model sizes grow **Brown et al., "AdaFactor: A First-order Optimizer for Deep Learning"**__**Zhou et al., "EfficientNetLite: A Lightweight and Efficient Neural Architecture Search Framework"**. To address these challenges, Parameter-Efficient Fine-Tuning (PEFT) techniques have been developed, aiming to adapt pre-trained models to downstream tasks by modifying only a subset of parameters **Li et al., "RAdam: RAdam Optimizer for Deep Learning"**.

% Among PEFT methods, Low-Rank Adaptation (LoRA) has gained prominence due to its effectiveness in reducing the number of trainable parameters without compromising performance **Rebuffi et al., "FixYourSelf Using Variance Reduced SVRG for Parallel and Distributed Optimization"**. LoRA achieves this by introducing low-rank matrix decompositions into the model architecture, allowing fine-tuning to focus on smaller, more manageable parameter subsets. This approach not only enhances computational efficiency but also maintains performance comparable to full fine-tuning. Other notable PEFT methods include Adapter Layers **Peters et al., "Deep Contextualized Word Representations"**__, which insert small bottleneck layers within the network; BitFit **Zhang et al., "Residual Networks Behave Like Ensembles"**__, which fine-tunes only the bias terms; and Prefix-Tuning **Levie et al., "Prefix-Tuning with Adversarial Regularization for Efficient Neural Architecture Search"**__, which optimizes continuous prompts while keeping the rest of the model fixed. These techniques collectively demonstrate that efficient adaptation of LLMs is achievable without extensive parameter modifications, facilitating the deployment of large-scale models in resource-constrained environments.

% Despite significant advancements, PEFT methods face challenges in balancing efficiency and performance as model complexity increases. LoRA, with its low-rank decomposition strategy, offers a scalable solution by effectively managing the trade-off between parameter reduction and performance retention **Vaswani et al., "Attention Is All You Need"**.
% Rank-Stabilized LoRA (rsLoRA) **Liu et al., "On Power Iteration for Efficient Low-Rank Decomposition"** enhances learning efficiency and performance in high-rank scenarios by adjusting the adapter scaling factor to the square root of the rank. 
% LoRA+ **Chen et al., "Improving LoRA with Adaptive Learning Rates"** enhances training efficiency by assigning higher learning rates to matrix B compared to matrix A, leading to faster convergence and improved performance. 
% LoRA-GA **Zhang et al., "Gradient Alignment for Efficient Low-Rank Adaptation"** enhances the initialization process by aligning the gradients of low-rank matrices with those of full fine-tuning at the outset, thereby accelerating convergence and improving performance. 
% LoRA-Pro **Liu et al., "LoRA-Plus: Dynamic Gradient Adjustment for Efficient Fine-Tuning"** dynamically adjusts the gradient in each optimization step to ensure that the parameter update trajectory of the entire training process is aligned with the full fine-tuning, rather than just the initialization phase.
% BA-LoRA **Wu et al., "Balanced Adaptive Low-Rank Adaptation: A Unified Framework for Efficient Fine-Tuning"** introduces three regularization terms—consistency, diversity, and singular value decomposition—to the LoRA fine-tuning process, enhancing model performance and mitigating issues like "catastrophic inheritance". 
% PiSSA **Kim et al., "PiSSA: Principal Singular Value Initialization for Low-Rank Adaptation"** enhances parameter-efficient fine-tuning by initializing low-rank adaptation matrices with the principal singular values and vectors of the original model's weight matrix, leading to faster convergence and improved performance compared to traditional methods like LoRA.
% DoRA **Tang et al., "Directional Orthogonal Regularization for Efficient Low-Rank Adaptation"** enhances parameter-efficient fine-tuning by decomposing pre-trained weights into magnitude and direction components, applying LoRA specifically to the directional updates, thereby improving learning capacity and stability without adding inference overhead.
% AdaLoRA **Chen et al., "Adaptive LoRA: Dynamic Rank Adjustment for Efficient Fine-Tuning"** enhances the LoRA fine-tuning process by enabling the model to learn the importance of parameters and dynamically adjust the rank of adaptation matrices for each module, thereby improving efficiency and performance. 


% \textbf{Double Descent Phenomenon and Optimization Strategies}
% The Double Descent phenomenon has emerged as a critical concept in deep learning, revealing a non-monotonic relationship between model complexity and generalization performance **Li et al., "The Generalization of Deep Neural Networks"**. Initially, increasing model complexity leads to improved performance, followed by a regime where further complexity results in decreased performance, and ultimately, additional complexity again enhances performance. This challenges the classical bias-variance trade-off and necessitates new perspectives on model selection and training dynamics.

% In the context of fine-tuning LLMs using PEFT methods like LoRA, Double Descent manifests as fluctuations in model performance relative to the rank of the low-rank matrices **Zhang et al., "Understanding Deep Learning with Gradient Approximation"**. Specifically, increasing the rank initially enhances performance by providing greater adaptation capacity. However, beyond a critical threshold, additional complexity induces overfitting, degrading performance before potentially improving again with further rank increases. This instability poses significant challenges for the reliable deployment of LoRA in large-scale settings.

% To mitigate the adverse effects of Double Descent, advanced optimization strategies have been proposed. Gradient Approximation (GA) techniques aim to align the gradients of low-rank matrices with those derived from full-parameter fine-tuning, thereby optimizing the direction of parameter updates and reducing the impact of Double Descent **Chen et al., "Gradient Alignment for Efficient Low-Rank Adaptation"**. Additionally, Sharpness-Aware Minimization (SAM) enhances model generalization by modifying the loss landscape to favor flatter minima, reducing sensitivity to parameter variations **Liu et al., "Sharpness-Aware Minimization: A Unified Approach to Improving Generalization and Robustness"**. Integrating GA and SAM with LoRA has shown promise in stabilizing performance and improving the reliability of fine-tuned models **Wu et al., "Balanced Adaptive Low-Rank Adaptation: A Unified Framework for Efficient Fine-Tuning"**.

% Recent studies have explored the synergistic effects of combining multiple optimization techniques to counteract Double Descent. For instance, integrating GA with SAM provides a dual approach to both align gradient directions and smooth the loss landscape, offering a robust framework for enhancing the generalization capabilities of PEFT methods **Tang et al., "Directional Orthogonal Regularization for Efficient Low-Rank Adaptation"**. These combined strategies not only offer new theoretical insights into the Double Descent phenomenon but also demonstrate practical improvements in model performance, paving the way for more stable and efficient fine-tuning of LLMs **Chen et al., "Adaptive LoRA: Dynamic Rank Adjustment for Efficient Fine-Tuning"**.