\section{Related Work}
\subsection{Mixture of Experts}
The Mixture of Experts (MoE) architecture was first introduced by ____ as a method to increase model capacity and performance without a proportional increase in computation by stacking sparsely gated LSTM blocks ____.
____ utilized MoE layers in Transformers for machine translation and showed improvements in multilingual translation as the model size increased, while only incurring a sublinear increase in training time.
____ simplified the routing, reduced training instability, and reduced communication costs to achieve a 7x improvement in pre-training speed.
____ found that MoEs frequently experienced training instabilities, and introduced an auxiliary loss to stabilize the model training without harming its quality. 


Recent advances in MoE training, such as upcycling from pretrained transformers ____ and efficient block-sparse implementations ____, have made MoE training even more efficient. However, these advances have primarily focused on language modeling tasks. While ____ explored domain-specific MoE embeddings and ____ investigated using MoE language model states as embeddings, our work is the first to develop a general-purpose MoE architecture specifically for text embeddings. Concurrent work GRITLM ____ demonstrates that MoE models like Mixtral 8x7B can effectively handle both embedding and generation tasks through instruction tuning. In contrast, our work focuses on optimizing MoE architectures for embedding efficiency through large-scale contrastive pretraining and finetuning.


\subsection{Monolingual Text Embeddings}
Modern monolingual text embedders typically follow a two-stage approach: contrastive pretraining on large weakly-supervised datasets, followed by contrastive finetuning on human-labeled data ____.
Recent work has focused on scaling and data curation ____ or adapting decoder-only LLMs for embedding tasks ____.



\subsection{Multilingual Text Embeddings}
While multilingual encoders like mBert ____ and XLM-Roberta ____ provide a foundation for cross-lingual representation, they require additional training for high-quality sentence embeddings. Current approaches either rely on translation data ____ or scale up model size ____, typically requiring 3-5x more parameters than monolingual models to achieve comparable English performance - a phenomenon known as the ``curse of multilinguality."

Recent work like Arctic Embed 2.0 ____ demonstrates that multilingual models can achieve strong English performance without compromising multilingual capability. However, existing approaches still face fundamental challenges with efficiency: state-of-the-art models require large parameter counts and generate large embedding vectors, increasing both computational and economic costs of dense retrieval.

Our MoE-based approach directly addresses this efficiency challenge, maintaining strong performance across both English and multilingual tasks while significantly reducing the active parameter count during inference. This represents a fundamental shift from previous scaling approaches that relied solely on increasing dense model capacity.


% \subsection{Knowledge Distillation}
% Knowledge distillation has been successfully applied to language models ____ and text embeddings ____. While MoE models have been distilled for speech recognition ____ and language modeling ____, our work is the first to explore distillation of MoE text embedding models into efficient dense models. Unlike previous approaches that focus on maintaining performance at moderate size reductions, our method achieves significant compression while preserving competitive performance across both monolingual and multilingual tasks.


\begin{table}[h]
\centering
\caption{MLM Hyperparameters}
\vspace{1.5pt}
\begin{tabular}{ll}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\
\hline
Batch Size & 4,096 \\
Peak Learning Rate & 4e-4 \\
Warmup Steps & 500 \\
Total Steps & 10,000 \\
Grad. Accumulation Steps & 8 \\
Learning Rate Schedule & Linear \\
Sequence Length & 2,048 \\
Rotary Base & 10,000 \\
MLM Probability & 0.3 \\
Language Sampling $\alpha$ & 0.3 \\ 
Max Grad Norm & 1.0 \\
\hline
\end{tabular}
\label{tab:mlm_hyperparameters}
\end{table}

\begin{table*}
\caption{Hyperparameters used for finetuning all models on GLUE benchmark tasks. For mGTE, warmup percentage is set to 6\% and max gradient norm to 1.}
\vskip 0.15in
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc|cccccccc}
\toprule
&&&&& \multicolumn{2}{c}{\bf Single Sentence} & \multicolumn{3}{c}{\bf Paraphrase and Similarity} & \multicolumn{3}{c}{\bf Natural Language Inference} \\
\textbf{Model} & \textbf{Params} & \bf Pos. & \bf Seq. & \bf Avg. & CoLA & SST-2 &  MRPC & STS-B & QQP & MNLI & QNLI & RTE \\
\midrule
\midrule
XLM-R-Base & 279M & Abs.  & 512  & 82.35 & 46.95 & 92.54 & 87.37 & 89.32 & 90.69 & 84.34 & 90.35 & 77.26 \\
mNomic-BERT  & 279M & RoPE  & 2048 & 81.63 & 44.69 & 91.97 & 87.50 & 88.48 & 90.93 & 83.59 & 89.38 & 76.54   \\
mGTE-Base & 306M & RoPE & 8192 & 80.77 & 27.22 & 91.97 & 89.71 & 89.55 & 91.20 & 85.16 & 90.91 & 80.41  \\
\bottomrule
\end{tabular}
}
\label{tab:glue-compare}
\end{table*}