%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

\usepackage{listings}
% \usepackage{minted}

\lstset{
    language=Python,
    basicstyle=\scriptsize\ttfamily,
    breaklines=true,
    breakatwhitespace=false,
    numbers=none,
    numberstyle=\tiny\color{gray},
    showspaces=false,
    showstringspaces=false,
    frame=none,
    rulecolor=\color{black},
    backgroundcolor=\color{white},
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red}
}

\usepackage{multirow} % for borders and merged ranges
\usepackage{soul}% for underlines
\PassOptionsToPackage{prologue,dvipsnames}{xcolor}
\usepackage{colortbl} % for cell colors
\usepackage{changepage,threeparttable} % for wide tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%%%%%%%% From Dissection

\usepackage{inconsolata}

\usepackage{colortbl} % 引入colortbl包
\usepackage{xcolor}   % 引入xcolor包，以便定义和使用颜色
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{fancyhdr}
\usepackage{tabularx}
\usepackage{diagbox}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{newtxmath}
\usepackage{enumitem}
\usepackage{graphicx} % 用于插入图片
\usepackage{subcaption} % 允许带子标题并排放置图片

\usepackage[normalem]{ulem}
\usepackage{float} % 在导言区加入
\usepackage{lipsum} % 用于生成示例文本


\usepackage{svg}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



% Table formatting from Model-Written Evals
\usepackage{xcolor, colortbl}
\definecolor{msgrgray}{HTML}{FAF9F7}
\definecolor{msgrdarkgray}{HTML}{EAE9E7}
\definecolor{msgrpalepurple}{HTML}{e6d6dd}
\definecolor{paleorange}{HTML}{F2E0BD}
\definecolor{paleblue}{HTML}{77C7F2}
\definecolor{palegreen}{HTML}{62BDA3}
\definecolor{softgreen}{HTML}{98FB98}
\definecolor{vibrantgreen}{HTML}{32CD32}

\definecolor{bettergreen}{HTML}{74A77F}
\definecolor{betterred}{HTML}{E27A74}

\definecolor{goodgreen}{RGB}{87,156,55}

\definecolor{tab1color1}{HTML}{f9d8bf}
\definecolor{tab1color2}{HTML}{fff0c2}
\definecolor{tab1color3}{HTML}{c2def6}

\definecolor{fig1blue}{HTML}{1F77B4}
\definecolor{fig1orange}{HTML}{FF7F0E}
\definecolor{fig1green}{HTML}{2CA02C}

\newcommand*{\myalign}[2]{\multicolumn{1}{#1}{#2}}
\newcommand{\contextb}[1]{{\colorbox{msgrgray}{\parbox{19em}{#1}}}}
\newcommand{\botc}[1]{{\colorbox{paleorange}{\parbox{19em}{#1}}}}
\newcommand{\adjcontextb}[2]{{\colorbox{msgrgray}{\parbox{#1}{#2}}}}
\newcommand{\adjcontextdark}[2]{{\colorbox{msgrdarkgray}{\parbox{#1}{#2}}}}
\newcommand{\adjbotc}[2]{{\colorbox{paleorange}{\parbox{#1}{#2}}}}
\newcommand{\adjbotbaseline}[2]{{\colorbox{paleblue}{\parbox{#1}{#2}}}}
\newcommand{\adjbotsycophancy}[2]{{\colorbox{paleorange}{\parbox{#1}{#2}}}}
\newcommand{\adjbotaligned}[2]{{\colorbox{palegreen}{\parbox{#1}{#2}}}}

\newcommand{\widecontextb}[1]{{\colorbox{msgrgray}{\parbox{48em}{#1}}}}
\newcommand{\widebotc}[1]{{\colorbox{paleorange}{\parbox{48em}{#1}}}}
\newcommand{\widecontextbnew}[1]{{\colorbox{msgrgray}{\parbox{62em}{#1}}}}
\newcommand{\widebotcnew}[1]{{\colorbox{paleorange}{\parbox{62em}{#1}}}}
\newcommand{\narrowbotc}[1]{{}}

\newcommand{\tab}[1]{{\colorbox{paleorange}{\parbox[t][][t]{12em}{#1}}}}

\newcommand{\mytab}{\phantom{xx}}



\definecolor{likegreen}{HTML}{006600}
\definecolor{dislikered}{HTML}{990000}
\definecolor{tablegray}{gray}{0.955}

\usepackage{tikz}
\usetikzlibrary{shapes,snakes}
\definecolor{myred}{rgb}{0.8352941176470589, 0.3686274509803922, 0.0}
\definecolor{mypurple}{rgb}{0.8, 0.47058823529411764, 0.7372549019607844}
\definecolor{refpurple}{rgb}{0.501, 0.0, 0.501}
\definecolor{myorange}{rgb}{0.87, 0.56, 0.02}
\definecolor{myblue}{rgb}{0.00392156862745098, 0.45098039215686275, 0.980392156862745}
\definecolor{mygreen}{rgb}{0.00784313725490196, 0.6196078431372549, 0.45098039215686275}
\definecolor{mybrown}{rgb}{0.792156862745098, 0.5686274509803921, 0.3803921568627451}
\definecolor{myskyblue}{rgb}{0.33725490196078434, 0.7058823529411765, 0.9137254901960784}
\newcommand{\responseA}{\tikz[baseline=(myanchor.base)] \node[circle,fill=myred,inner sep=0.15pt] (myanchor) {\color{-.}\bfseries\footnotesize A};}
\newcommand{\responseB}{\tikz[baseline=(myanchor.base)] \node[circle,fill=myred,inner sep=0.15pt] (myanchor) {\color{-.}\bfseries\footnotesize B};}

% 定义2种颜色
\definecolor{Blueback}{RGB}{218, 227, 243} 
\definecolor{Greenback}{RGB}{226, 240, 217}
\definecolor{Redback}{RGB}{251, 229, 214} 
\definecolor{asparagus}{rgb}{0.53, 0.66, 0.42}
\definecolor{brightmaroon}{rgb}{0.77, 0.12, 0.23}

% 自定义指令
\newcommand{\blueback}[1]{
  \begingroup
  \sethlcolor{tab1color3}%背景色
  \textcolor{black}{\hl{#1}}%textcolor里面对应文字颜色
  \endgroup
}

\newcommand{\redback}[1]{
  \begingroup
  \sethlcolor{tab1color1}%背景色
  \textcolor{black}{\hl{#1}}%textcolor里面对应文字颜色
  \endgroup
}

\newcommand{\yellowback}[1]{
  \begingroup
  \sethlcolor{tab1color2}%背景色
  \textcolor{black}{\hl{#1}}%textcolor里面对应文字颜色
  \endgroup
}

\newcommand{\cyanback}[1]{
  \begingroup
  \sethlcolor{cyan!15}%背景色
  \textcolor{black}{\hl{#1}}%textcolor里面对应文字颜色
  \endgroup
}

\newcommand{\greenback}[1]{
  \begingroup
  \sethlcolor{Greenback}%背景色
  \textcolor{black}{\hl{#1}}%textcolor里面对应文字颜色
  \endgroup
}


%%%%%%%% From Dissection

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\codeio}{\textsc{CodeI/O}}
\newcommand{\codeiopp}{\textsc{CodeI/O++}}

\newcommand{\jl}[1]{{\small \color{blue}[jl: #1]}}
\newcommand{\jh}[1]{\textcolor{magenta}{\bf\small [#1 --JH]}}
\newcommand{\jhc}[2]{\textcolor{magenta}{\sout{#1} #2}}
\newcommand{\jlc}[2]{\textcolor{blue}{\sout{#1} #2}}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\codeio: Condensing Reasoning Patterns via Code Input-Output Prediction}

\begin{document}

\twocolumn[
\icmltitle{\codeio: Condensing Reasoning Patterns via Code Input-Output Prediction} % this is a one line title
% \icmltitle{\codeio: Condensing Diverse Reasoning Patterns \\via Predicting Code Inputs and Outputs} % this is a two line title

% \icmltitle{\codeio: Mid-Training for Generalizable Reasoning \\via Predicting Code Inputs and Outputs}
% \icmltitle{Mid-Training for Generalizable Reasoning on Code Inputs and Outputs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Junlong Li}{comp,yyy,sch,equal}
\icmlauthor{Daya Guo}{comp}
\icmlauthor{Dejian Yang}{comp}
\icmlauthor{Runxin Xu}{comp}
% \icmlauthor{Fan Zhou}{yyy}
\icmlauthor{Yu Wu}{comp}
\icmlauthor{Junxian He}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}


\icmlaffiliation{yyy}{Shanghai Jiao Tong University}
\icmlaffiliation{comp}{DeepSeek-AI}
\icmlaffiliation{sch}{HKUST}

\icmlcorrespondingauthor{Junlong Li}{lockonlvange@gmail.com}
\icmlcorrespondingauthor{Junxian He}{junxianh@cse.ust.hk}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\textsuperscript{*}Work done during internship at DeepSeek-AI.} % otherwise use the standard text.

\begin{abstract}
% \jh{we need to note the project website under the author names (or note the github repo at abstract footnote in a more traditional way}
Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation,
% \jhc{many other reasoning tasks 
% like symbolic, scientific, and logic reasoning 
% suffer from}{
improving performance on many other reasoning tasks remains challenging due to
% }
sparse and fragmented training data. 
% \jhc{We}{
To address this issue, we
% }
propose \codeio, a novel 
approach
% \jl{or dataset}
that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes,
% \jhc{}{
through transforming the original code into a code input-output prediction format.
% } 
By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives—like logic flow planning, state-space searching, decision tree traversal, and modular decomposition—while decoupling structured reasoning from code-specific syntax and preserving procedural rigor.
% By training models to predict code inputs and outputs expressed entirely in natural language, we expose them to universal reasoning primitives -- such as logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- that are intrinsically encoded in diverse contextually-grounded code semantics.
% Specifically, the model need to reason what the output or a feasible input is given code, textual query and a test case, and the correctness can be verified by re-executing the code.
% but rarely observable in standard instruction-tuning data. 
% Crucially, we reimagines code input/output predictions as natural language Chain-of-Thought (CoT) rationales, decoupling structured reasoning patterns from code-specific syntax while preserving their procedural rigor.
% \jhc{We implement this through a two-stage framework: first training on code-derived reasoning patterns via \codeio, then doing general instruction-tuning.}{} 
Experimental results demonstrate
% \jhc{}{
\codeio{} leads to
% } 
consistent improvements across symbolic, scientific, logic, math \& numerical, and commonsense reasoning tasks. 
By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in \codeiopp{} and achieving higher performance.
% To further enhance data quality, we introduce \codeiopp, which leverages code’s verifiable nature for iterative multi-turn revisions of reasoning chains. 
Our data and models are available at \url{https://github.com/hkust-nlp/CodeIO}.


% Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on improving specific types of reasoning, such as mathematical reasoning or coding, our aim is to develop a more \emph{generalizable} approach — one that enables gains to transfer to unseen tasks rather than merely overfitting to the training tasks. To this end, we propose \codeio{}, a mid-training strategy that situates between the standard pre-training and post-training phases. In this intermediate stage, we design a task that involves predicting code inputs and outputs, expressed entirely in natural language. This mid-training objective encourages the model to engage in diverse structured, task-agnostic reasoning activities — such as planning, calculating, searching, backtracking, and traversing — that are essential for multifaceted downstream needs. Through comprehensive experiments, we demonstrate that \codeio{} facilitates far more reasoning tasks beyond the specified code-related ones, including symbolic, scientific, logic, math \& numerical, and commonsense reasoning tasks. By leveraging the verifiable features of code, we can further enhance data quality through multi-turn revision, resulting in \codeiopp{} and achieving higher performance. Our mid-training data and models will be publicly available.

% , making it a persistent goal to enhance model's reasoning abilities. 
% Reasoning is the foundational ability for Large Language Models (LLMs), yet no practical methods have been revealed to improve it in a generalizable way. In this paper, we introduce \codeio{}, a mid-training stage situated between the standard pre- and post-training phases. During this stage, we design a task to predict code inputs and outputs in pure natural language.
% without writing or executing any code. 
% Additionally, \codeio{} is easy to scale up by sampling diverse input arguments and their corresponding outputs through code execution to generate new training instances. 
% at \url{https://github.com/deepseek-ai/codeio}.


% Our experimental evaluation demonstrates that incorporating the \codeio{} mid-training stage consistently enhances performance across a variety of reasoning benchmarks compared to standard fine-tuning pipelines. These results highlight the value of task-specific intermediate training for building more generalizable and robust reasoning models.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/overview_2.pdf}
    \caption{Overview of our training data construction: Raw code files are gathered from various sources and converted into a unified format. Input-output pairs are then generated by executing the code, while natural language CoTs for predictions are collected from DeepSeek-V2.5. The verified CoTs can undergo optional revisions to further enhance reasoning chains.
    % \jh{better to have a brief description in the caption for easier read of the figure}
    }
    \label{fig:pipeline_overview}
    \vspace{-15pt}
\end{figure*}



Reasoning is a fundamental aspect of human cognition and problem-solving, forming the basis for quickly transferring and adapting to new tasks \cite{dehaene2004arithmetic, knauff2010complex, wang2010cognitive}. It is also recognized as a cornerstone of advanced Large Language Models (LLMs) and a critical step toward achieving Artificial General Intelligence (AGI) \cite{huang2022towards, qiao2022reasoning, jaech2024openaio1, xiang2025towards}. 
Current approaches, however, face a fundamental paradox: while tasks like math problem solving \cite{shao2024deepseekmath, yang2024qwen25math, zeng2024skyworkmath, ying2024internlmmath, toshniwal2024openmathinstruct} and code generation \cite{roziere2023code, codestral, zhu2024deepseekcoder,hui2024qwen25coder} benefit from abundant structured training data, most other reasoning domains—including logical deduction, scientific inference, and symbolic reasoning—suffer from sparse and fragmented supervision signals. As a result, it becomes crucial to identify training data that is rich in diverse reasoning patterns while remaining scalable to obtain.
% Despite its recognized importance, there is still no definitive solution for improving reasoning abilities in a generalizable manner
% \jhc{}{ 
% -- previous works primarily target math \cite{shao2024deepseekmath, yang2024qwen25math, zeng2024skyworkmath, ying2024internlmmath, toshniwal2024openmathinstruct} and code 
 % \cite{roziere2023code, zhu2024deepseekcoder,hui2024qwen25coder} domains, failing to achieve satisfactory generalization across other reasoning tasks such as logical reasoning and planning.
% }
% \jhc{Learning from chain-of-thought (CoT) samples or data that exhibits CoT patterns is considered a promising approach.}{} 
% \jhc{However,}{
% This limitation arises from the scarcity of
% } the scarcity of 
% intricate and detailed
% } \jhc{CoT}{
% chain-of-thought (CoT)
% }
% samples, especially verifiable ones.
% \jhc{ — presents a significant challenge}{ 
% — are scarce in most domains.
% }
% \jhc{}{
% As a result, it becomes crucial to identify training data are rich in diverse reasoning patterns while remaining scalable to obtain.
% that can generalize effectively across diverse reasoning domains while remaining scalable to obtain.
% } 

% Current efforts to enhance reasoning abilities largely focus on single domains, with mathematics being a prominent example \cite{shao2024deepseekmath, yang2024qwen25math, zeng2024skyworkmath, ying2024internlmmath, toshniwal2024openmathinstruct}. This is because large volumes of intricate and detailed CoT data can be generated or synthesized within mathematical tasks. However, models trained in such single domain often face limited generalizability beyond that domain \cite{zhou2024math4ai}. 
% In some cases, this narrow specialization may even lead to performance declines in untrained scenarios, thereby undermining their practical value. Conversely, for other domains, additional challenges arise, such as the difficulty of scaling up high-quality training data to a sufficient size.

% Another one is the most recent trend of test-time-scaling, such as OpenAI-o1 \cite{jaech2024openaio1}, DeepSeek-R1 \cite{deepseekr1}, QwQ \cite{qwq}, to boost model reasoning via super long Chain-of-Thought in inference time.

% \jl{I do not know whether we need to mention any test-time-scaling things in introduction, they are orthogonal I think.}

% \jl{One thing I feel a bit awkward is whether codeio a training phase or data collection? Although I know they are actually the same thing for this era.}

% Cognitive neuroscience ...
% \jl{Add some big concept here! to make a high position}



% \jhc{To address these limitations, we propose \codeio{}, an additional mid-training stage positioned between the standard pre- and post-training phases.}{} 
% \jh{this paragraph should have the phrase ``reasoning patterns'' if our title has it}
We believe that real-world code programs reflect the integration of a wide range of reasoning patterns across diverse contexts, 
% , making them an ideal source for training without the risk of domain over-fitting.
making them an ideal source for training while minimizing the risk of overfitting. 
However, conventional continual pre-training on raw code 
% \cite{roziere2023code, zhu2024deepseekcoder, hui2024qwen25coder, huang2024opencoder} 
is suboptimal because the relevant reasoning signals are often implicit and intertwined with noisy information. 
% Even converting them into more informative code generation instruction tuning data
Even the cleaner objective of directly training on text-to-code generation also faces challenges, as it is constrained by the requirement to generate code-specific syntax, making it difficult to generalize to tasks beyond code-specific ones.
To address such limitations, 
we propose transforming raw code files into executable functions and designing a more straightforward task: given a function along with its corresponding textual query, the model needs to predict either the execution outputs given inputs or feasible inputs given outputs entirely in natural language as CoT rationales. This approach aims to disentangle core reasoning flow from code-specific syntax while preserving logical rigor.
% \jh{maybe we can emphasize natural language a bit as we did in the abstract}.
By gathering and transforming functions from diverse sources, the resulting data incorporates a variety of foundational reasoning skills, such as logic flow orchestration, state-space exploration, recursive decomposition, and decision-making. Learning from these samples across the diverse contexts provided by the raw code files enables models to gain repeated exposure to these reasoning processes, allowing them to better internalize these skills.

% \jh{should cite figure 1 in this paragraph}
% Similar to continual pre-training on raw code, our code input/output prediction learning is introduced as a distinct training stage positioned before general instruction tuning in a two-stage manner, serving as an intermediate step to enhance the reasoning abilities of the base model. We refer to our approach as \codeio{}. 
% Notably, the training data of \codeio{} is highly scalable to collect, as we can easily sample hundreds of inputs from a separate python input generator for each function and execute the code to obtain ground-truth outputs. 
% To maintain simplicity, we structure training on \codeio{} as a supervised fine-tuning process. The prompt includes the function, the textual query, and the given input or output, while the response is directly collected by prompting a strong open-source model, DeepSeek-V2.5 \cite{liu2024deepseek}.
% Specifically, we collect over 450K functions from multiple sources, and for each function, several unique input-output pairs are generated by executing the corresponding code, resulting in a total of 3.5M instances. yielding the \codeio{} data.
Similar to continual pre-training on raw code, our code input/output prediction learning is introduced as a distinct training stage positioned before general instruction tuning in a two-stage manner, serving as an intermediate step to enhance the reasoning abilities of the base model.
% We refer to our approach as \codeio{}.
% \jhc{To maintain simplicity, we structure training as a supervised fine-tuning process.}{} 
The prompt includes the function, the textual query, and the given input or output, while the response is directly collected by prompting a strong open-source model, DeepSeek-V2.5 \cite{liu2024deepseek}.
Notably, the instances for input-output prediction are highly scalable to collect, as we can sample hundreds of inputs from a separate Python input generator for each function and execute the code to obtain ground-truth outputs.
Finally, we collect over 450K functions from multiple sources, and for each function, several input-output pairs are generated by executing the corresponding code. Synthesizing CoTs for them results in a total of 3.5M training samples, yielding the \codeio{} data.
To further leverage the verifiable characteristics of code, we verify all predictions based on code execution and prompt DeepSeek-V2.5 for a second turn of revisions on the responses it initially got wrong. These multi-turn revisions are concatenated into longer responses. The resulting \codeiopp{} dataset further enhances performance, demonstrating the effectiveness of this refinement process.
% We believe that real-world code programs reflect the integration of a wide range of reasoning activities across diverse contexts.
% % , making them an ideal source for training without the risk of domain over-fitting. 
% However, conventional continual pre-training on raw code \cite{roziere2023code, zhu2024deepseekcoder, hui2024qwen25coder, huang2024opencoder} is suboptimal because the relevant reasoning signals are often implicit and entangled with noisy information. 
% % Even converting them into more informative code generation instruction tuning data 
% % \jhc{Therefore}{
% Even the cleaner objective of directly training on pure code generation also face certain challenges, as it is bounded by the requirement to generate code-specific syntax, thus making it hard to generlize to tasks beyond code-specific ones.
% To address such limitation,
% % }
% we 
% % \jhc{}{
% propose
% % }
% to transform raw files into executable functions and design a more straightforward task: given a function along with its corresponding textual query, the model needs to predict either the execution outputs given inputs or feasible inputs given outputs in natural language entirely.
% By gathering and transforming functions from diverse sources, the resulting data incorporates a variety of foundational reasoning skills, such as logic flow orchestration, state-space exploration, recursive decomposition, and decision-making. Learning from these samples across the diverse contexts provided by the raw code files enables the models to gain repeated exposure to these reasoning processes, allowing them to better internalize and generalize these skills.
% % \jhc{}{
% % } 
% % outputs of the function. 
% % \jhc{For output prediction, the function's input arguments are provided, while for input prediction, the function's output values are given.}{} 
% % \jhc{}{
% Similar to continual pre-training on raw code, our code input/output prediction learning is introduced as a distinct training stage positioned before general instruction-tuning as a two-stage framework, serving as a intermidate step to enhance reasoning abilities of the base model. We refer to our approach as \codeio{}. 
% Notably, the training data of \codeio{} is highly scalable to collect, as we can easily sample hundreds of inputs from a separate input generator for the function and execute the code to obtain ground-truth outputs. We collect over 450K functions from multiple sources, and for each function, several unique input-output pairs are generated by executing the corresponding code, resulting in a total of 3.5M instances.

% \jl{avoid to first mention explicitly gt is not useful at least in intro} 
% \jh{Until here I feel the part before in the paragraph should go to the last paragraph before conlcuding the stats of the data.}
% Unlike previous methods that filtering out every incorrect response in training, we retain all of them, even though approximately 50\% are incorrect.
% \jh{are the ground-truth output even useful in our approach? It sounds like a particular merit of codeio is we can get ground-truth output in a scalable way from the writing above, but here we are saying it is not necessary to have ground-truth outputs?} 
% We find in many cases that, even when the final prediction is incorrect, the CoT remains meaningful and reflects the reasoning logic. 
% As a result, fine-tuning on \codeio{} as a mid-training stage still provides significant advantages after post-training, compared to models without mid-training or those trained on other strong mid-training data baselines. 


We validate the effectiveness of \codeio{} and \codeiopp{} across four base models with parameter sizes ranging from 7B to 30B
% \jh{clarify the sizes, like }
.  Assessments across 14 different benchmarks show training on them enhances performance on a diverse range of reasoning tasks, not only limited to code-related tasks but also more generalized tasks such as logic, symbolic, mathematical \& numerical, scientific, commonsense, etc. 
% \jhc{Notably, these improvements are observed even when the data used for instruction tuning is already diverse and strong.}{} 
Compared to several strong data baselines, such as OpenMathInstruct2 \cite{toshniwal2024openmathinstruct}, OpenCoder-SFT-Stage1 \cite{huang2024opencoder}, WebInstruct \cite{yue2024mammoth2}, and high-quality raw code \cite{benallal2024smollmcorpus}, \codeio{} achieves not only higher average scores across all four tested base models but also more balanced performance -- Instead of boosting scores on only a small subset of evaluation benchmarks while causing declines on others, \codeio{} delivers consistent improvements across nearly all benchmarks, demonstrating balanced and generalizable reasoning abilities.
% \jl{optionally add some analysis results}

% \jl{overview figure: pipeline and examples}

% The concepts involved in \codeio, planning, execuation, 

% We propose/hypothesize \codeio, as we think code programs naturally contain diverse reasoning structure ...

% Through massive analysis and ablations, we identify the optimal use method in our setting, and validate its effectiveness on models with different sizes, architectures, and pre-training focus. We also beat a lot of very strong baselines.

% The key contribution - balance and harmonization, no severe regression on benchmarks, and generalization.

% Resources released!

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figs/ioexample_v2.pdf}
    \caption{Two examples for the collected responses for input and output prediction respectively.}
    \label{fig:ioexample}
    \vspace{-15pt}
\end{figure*}

\section{\codeio}
\label{sec:codeio}

Our data construction pipeline is presented in this section. We begin with collecting raw code files from various sources (\S \ref{subsec:source_collection}). They are then transformed into a unified format (\S \ref{subsec:unified_format}). Next, I/O pairs are sampled from the transformed functions (\S \ref{subsec:collecing_io}). Finally, the complete training dataset is assembled (\S \ref{subsec:building_training_samples}). An overview is depicted in Figure \ref{fig:pipeline_overview}.

% This section introduces the \codeio{} data collection.
% \jh{we need a paragraph to motivate this section, it is kinda sudden to start with ``source collection''. For example, describe the pipeline (figure 1) we first do source collection, then we xxxx}
\subsection{Collecting Raw Code Files}
\label{subsec:source_collection}

The effectiveness of \codeio{} lies in selecting diverse raw code sources that encompass a wide range of reasoning patterns. To achieve this, we select sources with different emphases: \textbf{CodeMix}, a large collection of raw Python code files retrieved from an in-house code pre-training corpus, where we filter out files that are either overly simplistic or excessively complex; and \textbf{PyEdu-R} (reasoning), a subset of Python-Edu \cite{benallal2024smollmcorpus} that focuses on complex reasoning tasks such as STEM, system modeling or logic puzzles. To avoid overlap with CodeMix, we deliberately exclude files centered on pure algorithms. Beyond these two sources, we also incorporate high-quality code files from a variety of smaller, reputable sources, including comprehensive algorithm repositories, challenging math problems, and well-known online coding platforms. In total, merging these sources yields approximately 810.5K code files. Further details on the data sources can be found in Appendix \ref{app:source_details}.

% \jh{need to briefly describe the considerations on selecting sources}
% such as: The Algorithms, Project Euler, LeetCode, CodeWars\footnote{}, Edabit\footnote{}, Codeforces\footnote{}, AtCoder\footnote{}, and CodeChef\footnote{}. 

% \subsection{The I/O Collection}
% \label{subsec:io_collection}

% \jl{Instead of providing the prompt here, we just provide an example instead.}
% \jh{I suggest trying to avoid subsubsection, rarely used in papers I think}
\subsection{Transforming to a Unified Format}
\label{subsec:unified_format}



% \jh{need more reasoning parts to write here, like to give readers a general impression that the raw code data needs several kinds of preprocessing and why}Once we have collected all the code files, we use DeepSeek-V2.5 to convert them into a unified format containing the following components:

The collected raw code files often lack structure, contain irrelevant elements, and are hard to execute in a self-contained way.
% \jh{logic is kinda broken, why do we need the code to execute? Either in the end of 2.1 or beginning of 2.2, we need to re-mention that raw code data is not ideal and what we plan to do, just as we did in the intro. Motivation generally needs to be written again after the intro}
Therefore, we preprocess them using DeepSeek-V2.5 \cite{liu2024deepseek}, which refines them into a unified format that emphasizes main logical functionality and makes it executable for us to collect input-output pairs for later prediction tasks.
This transformation organizes the data into the following components, and we provide a complete example in Table \ref{tab:unified_format_example} in Appendix \ref{app:examples}:\ \ \textbf{1) Cleaned Reference Code:}  
% The raw code files are often disorganized and difficult to execute. 
We preprocess the raw code files by cleaning and refactoring the code to extract core logical functionalities into functions. Non-essential elements like visualization (e.g., print, plot) and file processing (e.g., read, write) are excluded.
\textbf{2) Main Entrypoint Function:}  
A main entrypoint function is added to summarize the overall logic of the code. It can call other functions or import external libraries and must have non-empty arguments (inputs) as well as return meaningful outputs. All inputs and outputs are required to be JSON-serializable to facilitate further processing.
\textbf{3) Input/Output Description:}  
The inputs and outputs of the main entrypoint function are clearly defined, including information on data types, constraints (e.g., output ranges), or more complex requirements (e.g., keys in a dictionary).
\textbf{4) Input Generator:}  
Rather than generating test cases directly, a standalone rule-based python input generator function is created. This generator returns non-trivial inputs that follow the requirements of the main entrypoint function. Randomness is applied subject to constraints, enabling scalable data generation.
% \jh{is this just a simple rule-based generator or an LLM? If it is rule-based, better to clarify, and also clarify in the intro. Because when you mention an input generator, it sounds like another LLM for some people.}
\textbf{5) Query:} 
A concise problem statement is generated based on the main entrypoint function, serving as a query to describe its intended functionality of the code.

% [START]

% \textbf{Reference Code} The original code files can be messy and difficult to execute. Therefore, we clean and rewrite the files so that important functionalities are abstracted into functions. We focus solely on the logical aspects by removing code related to visualization (e.g., print, plot) or file processing (e.g., read, write).

% \textbf{Main Entrypoint Function} At the end of the reference code, we add a main entrypoint function to summarize the high-level idea of the entire code file. The main entrypoint function is able to import external libraries or call any other functions defined in the reference code. It is also required to have non-empty arguments (inputs) and return values (outputs). To facilitate collection and processing, the inputs and outputs should be JSON-serializable.

% \textbf{Input/Output Description} We describe the format and requirements for the inputs and outputs of the entrypoint function. For instance, we specify the data structure type of these objects, any constraints on the output range, or more complex details, such as mandatory keys in a dictionary, etc.

% \textbf{Input Generator} Instead of asking DeepSeek-V2.5 to directly generate test cases with inputs and outputs, we instead require it to create an input generator function with some degree of randomness. The input generator is designed to return non-trivial input arguments (e.g., by applying constraints to the random selection range) that align with the requirements of the entrypoint function. The separate input generator is particularly beneficial, as it allows us to easily scale up the data size by executing the generator multiple times.

% \textbf{Query} Finally, we ask DeepSeek-V2.5 to write a query as the problem statement based on the main entrypoint function.

% [END]

% We provide a full example in Table/Figure xx in Appendix xx.

\subsection{Collecting Input and Output Pairs}
\label{subsec:collecing_io}
% After converting the collected raw code files into the unified format, we sample multiple inputs via sampling from the input generator for each sample and obtain the corresponding outputs by executing the code. To ensure the outputs are deterministic, we skip all functions that include randomness, such as those using \texttt{import random}. During the execution of these codes, we also set a series of limits on the runtime and complexity of the input/output objects. Details can be found in Appendix \ref{app:size_check_func}. For each transformed function, we sample multiple input-output pairs, with the exact number depending on the source it originates from (details in Appendix \ref{app:io_pairs_source}). After filtering out non-executable code, samples that exceed the runtime limit, and input-output pairs surpassing the desired complexity, we obtain approximately 3.5M instances derived from 454.9K raw code files. The distribution of input and output prediction instances is roughly balanced at 50\%/50\%.

After converting the collected raw code files into a unified format, we sample multiple inputs using the input generator for each function and obtain the corresponding outputs by executing the code. To ensure the outputs are deterministic, we skip all functions that include randomness, such as those using \texttt{import random}. During the execution of these codes, we also impose a series of limits on the runtime and the complexity of the input/output objects (details in Appendix \ref{app:size_check_func}). For each transformed function, we sample multiple input-output pairs, with the exact number depending on the source from which it originates (details in Appendix \ref{app:io_pairs_source}). After filtering out non-executable code, samples that exceed the runtime limit, and input-output pairs surpassing the desired complexity, we obtain 3.5M instances derived from 454.9K raw code files. The distribution of input and output prediction instances is roughly balanced at 50\%/50\%.





\subsection{Building Samples for Input-Output Prediction}
\label{subsec:building_training_samples}
% \jh{need more to write here to give readers a general notion that we are gonna do in this subsection, and why do we need to do this? Generally we need more reasonining to write to convey our thinking, currently the writing of section 2 is like a tech report.}
% \jh{either in the end of 2.3 or in the beginning of 2.4, better to remind people that job is not complete after we have input and output pairs, because we still need the natural language cot in the middle that is missing. And then you can emphasize again that we think the natural language cot is good to cover diverse reasoning patterns, better than raw code training and code generation (this is like our core novelty and intuition that is worth mentioning multiple times throughout the paper). These stuff is like a transition from 2.3 to 2.4 making the logic smoother.}
After collecting the input-output pairs as well as the transformed functions, we need to assemble them into a trainable format. For the supervised fine-tuning process we adopt, a prompt and a response are needed for each training sample. Since we aim for the input-output prediction tasks, we construct the prompt using a designed template to combine the function, the query, the reference code, and either a specific input or output. We provide an example prompt in Figure \ref{fig:prompt_example} in Appendix \ref{app:examples}. The response should ideally be a natural language CoT to reason about how to derive the correct output or a feasible input. In general, we choose the following two ways to construct the desired CoT responses:
% \jh{In the following paragraphs, it seems the writing assumes ``responses'' are the thoughts process (cot). This is a bit confusing to me, because any outputs predicted by the model can be called ``response''. For example, even if we train the model to directly predict the output without cot, the execuation output can be ``response'' as well. Thus I don't feel response is equivalent to CoT, some places it needs clarification}

% Finally, to build samples that models can be directly trained on, we design prompts to present the input and output prediction tasks. Based on these prompts, we synthesize corresponding responses to build the final training dataset.

% \textbf{Prompts}
% For each instance described in \S\ref{subsec:collecing_io}, we construct the prompt using a designed template. We provide an example in Figure \ref{fig:prompt_example} in Appendix \ref{app:examples}.

\textbf{Direct Prompting -- \codeio}
% For each prompt, either input or output prediction, we collect a corresponding response. 
While having full executable code theoretically allows us to generate reliable execution trajectories as responses, two challenges arise: 1) Obtaining a deterministic reverse function for input prediction is impractical; 2) Automatically constructed trajectories are constrained by pre-designed templates and lack the expressiveness and generalizability of free-form natural language reasoning. Thus, we adopt a fully LLM-based approach for synthesizing all the desired responses using DeepSeek-V2.5, as it has top-tier performance but extremely low cost.
% \jh{maybe can use one sentence to justify that why we choose this particular model} 
The dataset generated here is referred to as \codeio. We provide two examples of collected responses in Figure \ref{fig:ioexample}. 

\textbf{Making Full Use of Code -- \codeiopp} A common approach to enhance data quality is reject sampling \cite{yuan2023rft}, where incorrect predictions are discarded. Though this approach suits \codeio{} well as we can verify all responses by re-executing the codes,  
% by (a) matching predictions with the stored outputs for output prediction or (b) re-executing the reference code for input prediction. However, 
we find it leads to suboptimal performance (\S \ref{subsec:ablation}). Therefore, we take an alternative approach to fully utilize the execution feedback from our reference code. For responses with incorrect predictions, we append the feedback as the second turn of input messages and ask DeepSeek-V2.5 to regenerate another response. 
% \jl{should we mention the details of the feedback in appendix? I was thinking demonstrate a full example but that may be too long for a single page.} <<<>>>
In practice, we capture multiple types of feedback:
For output prediction, we simply inform the model that it generated an incorrect answer.
For input prediction, we additionally provide the executed output based on the incorrect input.
For instances where the code fails to execute (e.g., due to a format error, argument mismatch error, or other runtime error), we also include these feedback explicitly.

After the second turn, we re-check the correctness of the newly generated responses. We then construct the final response by concatenating all of the four components: Turn 1 response + Turn 1 feedback + Turn 2 response + Turn 2 feedback. 
% \jh{so it is enforced two turns, and the second turn may not be correct right?}. 
For correct responses in the first turn, the Turn 1 feedback is simply "Success" with no Turn 2 contents. In general, in first turn, 50\% of the responses are correct and 10\% of the incorrect ones can be successfully revised in the second turn. Similar to \codeio{}, we keep all responses, either correct or incorrect, after the revision. The dataset we collect through this way is referred to as \codeiopp, and we provide a complete example in Table \ref{tab:codeiopp_example} in Appendix \ref{app:examples}.

% We provide an example in Figure xx in Appendix xx.




\section{Experiments}
\label{sec:experiments}

% Describe experiments in this section.

\subsection{Settings}
\label{subsec:settings}

\textbf{Models} We select the following base models as the backbones: Qwen 2.5 7B Coder \cite{hui2024qwen25coder}, Deepseek v2 Lite Coder (MoE) \cite{zhu2024deepseekcoder}, LLaMA 3.1 8B \cite{dubey2024llama3}, and Gemma 2 27B \cite{team2024gemma}. These models were chosen for being the most advanced base models currently, differing in architecture, size, and pre-training focus.
% , and they originate from a diverse set of research groups. 
Notably, we include two coder models, as previous studies have shown that coder models exhibit stronger reasoning capabilities compared to general-purpose models \cite{suzgun2023bbh,shao2024deepseekmath}. 
% The details of these models are provided in Table \ref{tab:model_details}.

% % Table generated by Excel2LaTeX from sheet 'models'
% \begin{table}[]
%   \centering
%     \setlength{\tabcolsep}{4pt}
%   \caption{The details of models we selected.}
%   % \vspace{3mm}
%     \begin{tabular}{llll}
%     \toprule
%     Model & \# Params & Arch  & Domain \\
%     \midrule
%     Qwen 2.5 Coder & 7B    & Dense & Code \\
%     LLaMA 3.1 & 8B    & Dense & General \\
%     DeepSeek Coder v2 Lite & 16B   & MoE   & Code \\
%     Gemma 2 & 27B   & Dense & General \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab:model_details}%
% \end{table}%

\textbf{Instruction Tuning Data}
We utilize an in-house instruction-tuning dataset containing approximately 1.18M samples from different languages, encompassing a wide range of domains such as math, coding, writing, and more. Tuning the model on this dataset enables it to effectively follow diverse instructions, making it applicable to and testable on a broad spectrum of downstream tasks.




\textbf{Training Setups}
Similar to continual pre-training, we employ a two-stage training strategy in most of our experiments. The first stage involves training on the \codeio{} or \codeiopp{} dataset, followed by a second stage of general instruction-tuning. 

The reason for adopting this two-stage training approach is rooted in the characteristics of our datasets. The \codeio{}(++) dataset contains a significantly larger number of samples compared to the instruction-tuning data. Simply mixing the two datasets would result in a biased distribution, which could lead to insufficient learning on the instruction-tuning data. This might prevent the model from fully demonstrating its capacity to follow diverse instructions in downstream tasks. To address this, the two-stage training first strengthens the model as a more robust base model for general reasoning, and then adapts it into a versatile instruction-following model through instruction tuning. Detailed training hyper-parameters are in Appendix \ref{app:hyper_para}.

\begin{table*}[!t]
  \centering
  \setlength{\tabcolsep}{2pt}
  \small
  \caption{Main evaluation results on all benchmarks. WI = WebInstruct, OMI2 = OpenMathInstruct2, OC-SFT-1 = OpenCoder-SFT-Stage-1, PyEdu = PythonEdu. We also report the number of training samples for each dataset. Color-coded cells (\textcolor{mygreen}{green}/\textcolor{myred}{red}) are employed to denote improvements or declines relative to the single-stage baseline, with deeper shades indicating larger score shifts.}

    \begin{tabular}{lc|cccccccccccccc|c}
    \toprule
    \multicolumn{2}{c|}{1st Stage} & Wino  & \multirow{2}[2]{*}{DROP} & GSM   & \multirow{2}[2]{*}{MATH} & \multirow{2}[2]{*}{GPQA} & MMLU  & LC    & \multicolumn{2}{c}{CRUX} & \multicolumn{2}{c}{BBH} & Zebra & Kor   & Live  & \multirow{2}[2]{*}{AVG} \\
    Dataset  & \# (M) & Grande &       & 8K    &       &       & -STEM & -O    & -I    & -O    & -EN   & -ZH   & Logic & Bench & Bench &  \\
    \midrule
    \multicolumn{17}{c}{\textit{Qwen 2.5 Coder 7B}} \\
    \midrule
    \multicolumn{2}{c|}{2nd Stage Only} & 66.9  & 70.7  & 83.4  & 71.6  & 41.5  & 77.2  & 20.7  & 61.3  & 60.0  & 68.3  & 70.6  & 10.9  & 38.7  & 26.0  & \cellcolor[rgb]{ .992,  .996,  .996}54.8 \\
    \midrule
    WI    & 3.5   & \cellcolor[rgb]{ .902,  .486,  .451}66.3 & \cellcolor[rgb]{ .784,  .914,  .851}73.5 & \cellcolor[rgb]{ .541,  .816,  .682}87.0 & \cellcolor[rgb]{ .969,  .851,  .843}71.4 & \cellcolor[rgb]{ .953,  .765,  .749}39.1 & \cellcolor[rgb]{ .863,  .945,  .902}77.5 & \cellcolor[rgb]{ .902,  .486,  .451}18.3 & \cellcolor[rgb]{ .91,  .533,  .502}59.1 & \cellcolor[rgb]{ .867,  .949,  .91}61.6 & \cellcolor[rgb]{ .929,  .973,  .953}68.6 & \cellcolor[rgb]{ .961,  .8,  .784}68.7 & \cellcolor[rgb]{ .914,  .549,  .518}10.2 & \cellcolor[rgb]{ .643,  .859,  .753}42.5 & 26.0  & \cellcolor[rgb]{ .961,  .984,  .973}55.0 \\
    % WI+   & 3.5   & \cellcolor[rgb]{ .949,  .741,  .722}66.6 & \cellcolor[rgb]{ .525,  .808,  .671}76.8 & \cellcolor[rgb]{ .38,  .753,  .569}88.2 & \cellcolor[rgb]{ .592,  .835,  .714}72.6 & \cellcolor[rgb]{ .49,  .796,  .643}42.9 & \cellcolor[rgb]{ .392,  .753,  .576}78.5 & \cellcolor[rgb]{ .925,  .973,  .949}21.1 & \cellcolor[rgb]{ .906,  .506,  .471}59.0 & \cellcolor[rgb]{ .878,  .953,  .914}61.5 & \cellcolor[rgb]{ .855,  .941,  .898}68.9 & \cellcolor[rgb]{ .6,  .839,  .722}72.8 & 10.9  & \cellcolor[rgb]{ .902,  .486,  .451}38.3 & \cellcolor[rgb]{ .902,  .486,  .451}25.6 & \cellcolor[rgb]{ .733,  .894,  .816}56.0 \\
    WI (Full) & 11.6  & \cellcolor[rgb]{ .937,  .976,  .957}67.0 & \cellcolor[rgb]{ .667,  .867,  .769}75.0 & \cellcolor[rgb]{ .541,  .816,  .682}87.0 & \cellcolor[rgb]{ .929,  .631,  .604}71.1 & \cellcolor[rgb]{ .49,  .796,  .643}42.9 & \cellcolor[rgb]{ .341,  .733,  .541}78.6 & \cellcolor[rgb]{ .933,  .659,  .635}19.1 & \cellcolor[rgb]{ .914,  .561,  .529}59.3 & \cellcolor[rgb]{ .902,  .486,  .451}59.8 & \cellcolor[rgb]{ .976,  .992,  .984}68.4 & \cellcolor[rgb]{ .992,  .976,  .976}70.4 & 10.9  & \cellcolor[rgb]{ .702,  .878,  .792}41.9 & \cellcolor[rgb]{ .663,  .863,  .765}27.6 & \cellcolor[rgb]{ .812,  .925,  .871}55.6 \\
    OMI2  & 3.5   & \cellcolor[rgb]{ .541,  .816,  .682}67.6 & \cellcolor[rgb]{ .722,  .886,  .804}74.3 & \cellcolor[rgb]{ .91,  .965,  .937}84.1 & \cellcolor[rgb]{ .714,  .886,  .8}72.3 & \cellcolor[rgb]{ .902,  .486,  .451}36.2 & \cellcolor[rgb]{ .91,  .965,  .937}77.4 & \cellcolor[rgb]{ .965,  .988,  .976}20.9 & \cellcolor[rgb]{ .961,  .804,  .792}60.4 & \cellcolor[rgb]{ .878,  .953,  .914}61.5 & \cellcolor[rgb]{ .878,  .953,  .918}68.8 & \cellcolor[rgb]{ .973,  .863,  .851}69.3 & \cellcolor[rgb]{ .902,  .486,  .451}10.1 & \cellcolor[rgb]{ .624,  .851,  .741}42.7 & \cellcolor[rgb]{ .745,  .898,  .824}27.2 & \cellcolor[rgb]{ .91,  .965,  .937}55.2 \\
    OMI2 (Full) & 14.0  & 66.9  & \cellcolor[rgb]{ .745,  .898,  .824}74.0 & \cellcolor[rgb]{ .341,  .733,  .541}88.5 & \cellcolor[rgb]{ .341,  .733,  .541}73.2 & \cellcolor[rgb]{ .988,  .941,  .937}40.9 & \cellcolor[rgb]{ .722,  .886,  .804}77.8 & \cellcolor[rgb]{ .965,  .827,  .816}19.9 & \cellcolor[rgb]{ .925,  .612,  .588}59.5 & \cellcolor[rgb]{ .8,  .922,  .863}62.4 & 68.3  & \cellcolor[rgb]{ .875,  .949,  .914}71.3 & \cellcolor[rgb]{ .341,  .733,  .541}11.2 & \cellcolor[rgb]{ .765,  .906,  .839}41.2 & \cellcolor[rgb]{ .49,  .796,  .647}28.4 & \cellcolor[rgb]{ .737,  .894,  .816}56.0 \\
    OC-SFT-1 & 4.2   & \cellcolor[rgb]{ .949,  .741,  .722}66.6 & \cellcolor[rgb]{ .643,  .855,  .749}75.3 & \cellcolor[rgb]{ .576,  .831,  .706}86.7 & \cellcolor[rgb]{ .902,  .486,  .451}70.9 & \cellcolor[rgb]{ .929,  .631,  .604}37.7 & \cellcolor[rgb]{ .627,  .851,  .741}78.0 & \cellcolor[rgb]{ .98,  .914,  .906}20.3 & \cellcolor[rgb]{ .98,  .914,  .906}60.9 & \cellcolor[rgb]{ .992,  1,  .996}60.1 & \cellcolor[rgb]{ .902,  .486,  .451}67.5 & \cellcolor[rgb]{ .937,  .682,  .663}67.6 & \cellcolor[rgb]{ .984,  .933,  .929}10.8 & \cellcolor[rgb]{ .871,  .949,  .91}40.1 & \cellcolor[rgb]{ .682,  .875,  .78}27.5 & \cellcolor[rgb]{ .957,  .984,  .969}55.0 \\
    PyEdu & 7.7   & \cellcolor[rgb]{ .965,  .827,  .816}66.7 & \cellcolor[rgb]{ .682,  .871,  .776}74.8 & \cellcolor[rgb]{ .69,  .878,  .784}85.8 & \cellcolor[rgb]{ .969,  .851,  .843}71.4 & \cellcolor[rgb]{ .988,  .941,  .937}40.9 & \cellcolor[rgb]{ .91,  .965,  .937}77.4 & \cellcolor[rgb]{ .933,  .655,  .631}19.1 & \cellcolor[rgb]{ .902,  .486,  .451}58.9 & \cellcolor[rgb]{ .8,  .922,  .863}62.4 & \cellcolor[rgb]{ .937,  .678,  .655}67.8 & \cellcolor[rgb]{ .902,  .486,  .451}65.7 & \cellcolor[rgb]{ .961,  .804,  .792}10.6 & \cellcolor[rgb]{ .945,  .98,  .961}39.3 & \cellcolor[rgb]{ .949,  .741,  .725}25.8 & 54.8 \\
    \midrule
    \codeio & 3.5   & \cellcolor[rgb]{ .341,  .733,  .541}67.9 & \cellcolor[rgb]{ .557,  .82,  .69}76.4 & \cellcolor[rgb]{ .616,  .847,  .733}86.4 & \cellcolor[rgb]{ .878,  .953,  .918}71.9 & \cellcolor[rgb]{ .341,  .733,  .541}43.3 & \cellcolor[rgb]{ .957,  .984,  .969}77.3 & \cellcolor[rgb]{ .435,  .773,  .608}23.7 & \cellcolor[rgb]{ .341,  .733,  .541}63.6 & \cellcolor[rgb]{ .592,  .835,  .718}64.9 & \cellcolor[rgb]{ .757,  .902,  .831}69.3 & \cellcolor[rgb]{ .6,  .839,  .722}72.8 & \cellcolor[rgb]{ .973,  .871,  .859}10.7 & \cellcolor[rgb]{ .475,  .788,  .635}44.3 & \cellcolor[rgb]{ .471,  .788,  .631}28.5 & \cellcolor[rgb]{ .451,  .78,  .62}57.2 \\
    \codeiopp & 3.5   & 66.9  & \cellcolor[rgb]{ .341,  .733,  .541}79.1 & \cellcolor[rgb]{ .706,  .882,  .796}85.7 & \cellcolor[rgb]{ .796,  .918,  .859}72.1 & \cellcolor[rgb]{ .98,  .91,  .906}40.6 & \cellcolor[rgb]{ .675,  .871,  .773}77.9 & \cellcolor[rgb]{ .341,  .733,  .541}24.2 & \cellcolor[rgb]{ .659,  .863,  .761}62.5 & \cellcolor[rgb]{ .341,  .733,  .541}67.9 & \cellcolor[rgb]{ .341,  .733,  .541}71.0 & \cellcolor[rgb]{ .341,  .733,  .541}74.2 & \cellcolor[rgb]{ .973,  .871,  .859}10.7 & \cellcolor[rgb]{ .341,  .733,  .541}45.7 & \cellcolor[rgb]{ .341,  .733,  .541}29.1 & \cellcolor[rgb]{ .341,  .733,  .541}57.7 \\
    \midrule
    \multicolumn{17}{c}{\textit{LLaMA 3.1 8B}} \\
    \midrule
    \multicolumn{2}{c|}{2nd Stage Only} & 71.3  & 73.1  & 83.2  & 49.9  & 40.6  & 70.0  & 4.1   & 44.5  & 46.9  & 65.8  & 65.6  & 9.8   & 39.8  & 25.7 & 49.3 \\
    \midrule
    WI    & 3.5   & \cellcolor[rgb]{ .416,  .765,  .596}72.1 & \cellcolor[rgb]{ .341,  .733,  .541}76.3 & \cellcolor[rgb]{ .969,  .851,  .839}82.8 & \cellcolor[rgb]{ .788,  .918,  .855}52.8 & \cellcolor[rgb]{ .478,  .792,  .639}42.9 & \cellcolor[rgb]{ .973,  .871,  .859}69.6 & 4.1   & \cellcolor[rgb]{ .965,  .816,  .8}44.0 & \cellcolor[rgb]{ .914,  .549,  .518}44.8 & \cellcolor[rgb]{ .925,  .627,  .6}64.5 & \cellcolor[rgb]{ .714,  .886,  .8}67.8 & \cellcolor[rgb]{ .816,  .925,  .871}10.0 & \cellcolor[rgb]{ .439,  .776,  .612}42.7 & \cellcolor[rgb]{ .902,  .486,  .451}23.1 & \cellcolor[rgb]{ .953,  .769,  .753}49.8 \\
    OMI2  & 3.5   & \cellcolor[rgb]{ .341,  .733,  .541}72.2 & \cellcolor[rgb]{ .651,  .859,  .757}74.8 & \cellcolor[rgb]{ .341,  .733,  .541}86.2 & \cellcolor[rgb]{ .341,  .733,  .541}58.9 & \cellcolor[rgb]{ .902,  .486,  .451}38.2 & \cellcolor[rgb]{ .341,  .733,  .541}70.1 & \cellcolor[rgb]{ .812,  .925,  .871}5.8 & \cellcolor[rgb]{ .824,  .929,  .878}46.1 & \cellcolor[rgb]{ .976,  .89,  .882}46.4 & \cellcolor[rgb]{ .749,  .902,  .827}67.4 & \cellcolor[rgb]{ .608,  .843,  .725}68.6 & \cellcolor[rgb]{ .902,  .486,  .451}9.5 & \cellcolor[rgb]{ .906,  .965,  .933}40.3 & \cellcolor[rgb]{ .953,  .761,  .745}24.5 & \cellcolor[rgb]{ .945,  .98,  .965}50.6 \\
    OC-SFT-1 & 4.2   & \cellcolor[rgb]{ .957,  .776,  .765}71.0 & \cellcolor[rgb]{ .965,  .824,  .812}71.9 & \cellcolor[rgb]{ .902,  .486,  .451}81.8 & \cellcolor[rgb]{ .914,  .965,  .941}51.1 & \cellcolor[rgb]{ .902,  .486,  .451}38.2 & \cellcolor[rgb]{ .902,  .486,  .451}68.4 & \cellcolor[rgb]{ .824,  .929,  .878}5.7 & \cellcolor[rgb]{ .929,  .631,  .604}43.5 & \cellcolor[rgb]{ .918,  .569,  .541}44.9 & \cellcolor[rgb]{ .988,  .941,  .937}65.6 & \cellcolor[rgb]{ .737,  .894,  .82}67.6 & \cellcolor[rgb]{ .341,  .733,  .541}10.5 & \cellcolor[rgb]{ .576,  .831,  .706}42.0 & \cellcolor[rgb]{ .961,  .8,  .788}24.7 & \cellcolor[rgb]{ .906,  .514,  .482}49.1 \\
    PyEdu & 7.7   & \cellcolor[rgb]{ .902,  .486,  .451}70.6 & \cellcolor[rgb]{ .902,  .486,  .451}69.6 & 83.2  & \cellcolor[rgb]{ .902,  .486,  .451}49.8 & \cellcolor[rgb]{ .592,  .835,  .718}42.4 & \cellcolor[rgb]{ .941,  .71,  .69}69.1 & \cellcolor[rgb]{ .878,  .953,  .918}5.2 & \cellcolor[rgb]{ .902,  .486,  .451}43.1 & \cellcolor[rgb]{ .902,  .486,  .451}44.5 & \cellcolor[rgb]{ .902,  .486,  .451}64.0 & 65.6  & \cellcolor[rgb]{ .627,  .851,  .741}10.2 & \cellcolor[rgb]{ .459,  .784,  .624}42.6 & 25.7  & \cellcolor[rgb]{ .902,  .486,  .451}49.0 \\
    \midrule
    \codeio & 3.5   & \cellcolor[rgb]{ .71,  .882,  .796}71.7 & \cellcolor[rgb]{ .835,  .933,  .886}73.9 & \cellcolor[rgb]{ .914,  .965,  .941}83.6 & \cellcolor[rgb]{ .718,  .886,  .804}53.8 & \cellcolor[rgb]{ .341,  .733,  .541}43.5 & \cellcolor[rgb]{ .937,  .678,  .655}69.0 & \cellcolor[rgb]{ .42,  .769,  .596}9.3 & \cellcolor[rgb]{ .376,  .749,  .565}50.1 & \cellcolor[rgb]{ .341,  .733,  .541}53.3 & \cellcolor[rgb]{ .733,  .894,  .816}67.5 & \cellcolor[rgb]{ .902,  .486,  .451}65.3 & \cellcolor[rgb]{ .435,  .773,  .608}10.4 & \cellcolor[rgb]{ .788,  .918,  .855}40.9 & \cellcolor[rgb]{ .961,  .8,  .788}24.7 & \cellcolor[rgb]{ .71,  .882,  .796}51.2 \\
    \codeiopp & 3.5   & \cellcolor[rgb]{ .635,  .855,  .749}71.8 & \cellcolor[rgb]{ .592,  .835,  .714}75.1 & \cellcolor[rgb]{ .827,  .929,  .878}84.0 & \cellcolor[rgb]{ .761,  .906,  .835}53.2 & \cellcolor[rgb]{ .933,  .973,  .953}40.9 & \cellcolor[rgb]{ .902,  .486,  .451}68.4 & \cellcolor[rgb]{ .341,  .733,  .541}10.0 & \cellcolor[rgb]{ .341,  .733,  .541}50.4 & \cellcolor[rgb]{ .365,  .745,  .557}53.1 & \cellcolor[rgb]{ .341,  .733,  .541}70.0 & \cellcolor[rgb]{ .341,  .733,  .541}70.6 & \cellcolor[rgb]{ .341,  .733,  .541}10.5 & \cellcolor[rgb]{ .341,  .733,  .541}43.2 & \cellcolor[rgb]{ .341,  .733,  .541}28.1 & \cellcolor[rgb]{ .341,  .733,  .541}52.1 \\
    \midrule
    \multicolumn{17}{c}{\textit{DeepSeek Coder v2 Lite 16B}} \\
    \midrule
    \multicolumn{2}{c|}{2nd Stage Only} & 68.4  & 73.4 & 82.5 & 60.0 & 38.6  & 68.5 & 14.8  & 53.0  & 54.9 & 61.1  & 69.2  & 6.7   & 44.7  & 26.6 & 51.6 \\
    \midrule
    WI    & 3.5   & \cellcolor[rgb]{ .894,  .957,  .925}68.5 & \cellcolor[rgb]{ .784,  .914,  .851}73.8 & \cellcolor[rgb]{ .643,  .855,  .753}83.7 & \cellcolor[rgb]{ .933,  .973,  .953}60.5 & \cellcolor[rgb]{ .341,  .733,  .541}39.5 & \cellcolor[rgb]{ .929,  .973,  .949}68.7 & \cellcolor[rgb]{ .902,  .486,  .451}14.3 & \cellcolor[rgb]{ .953,  .98,  .969}53.5 & \cellcolor[rgb]{ .82,  .929,  .875}57.1 & \cellcolor[rgb]{ .894,  .961,  .929}61.6 & \cellcolor[rgb]{ .902,  .486,  .451}65.7 & \cellcolor[rgb]{ .882,  .953,  .918}6.9 & \cellcolor[rgb]{ .941,  .694,  .675}43.1 & \cellcolor[rgb]{ .937,  .69,  .667}25.4 & \cellcolor[rgb]{ .996,  .984,  .984}51.6 \\
    OMI2  & 3.5   & \cellcolor[rgb]{ .902,  .486,  .451}67.6 & \cellcolor[rgb]{ .62,  .847,  .733}74.1 & \cellcolor[rgb]{ .341,  .733,  .541}84.7 & \cellcolor[rgb]{ .341,  .733,  .541}64.7 & \cellcolor[rgb]{ .984,  .918,  .914}38.4 & \cellcolor[rgb]{ .416,  .765,  .596}70.1 & \cellcolor[rgb]{ .918,  .588,  .557}14.4 & \cellcolor[rgb]{ .922,  .969,  .945}53.8 & \cellcolor[rgb]{ .925,  .973,  .949}55.8 & \cellcolor[rgb]{ .471,  .788,  .631}63.6 & \cellcolor[rgb]{ .922,  .588,  .561}66.4 & \cellcolor[rgb]{ .949,  .741,  .725}6.4 & \cellcolor[rgb]{ .902,  .486,  .451}42.0 & \cellcolor[rgb]{ .902,  .498,  .463}24.7 & \cellcolor[rgb]{ .902,  .961,  .933}51.9 \\
    OC-SFT-1 & 4.2   & \cellcolor[rgb]{ .973,  .871,  .863}68.2 & \cellcolor[rgb]{ .894,  .957,  .925}73.6 & \cellcolor[rgb]{ .761,  .906,  .835}83.3 & \cellcolor[rgb]{ .875,  .949,  .914}60.9 & \cellcolor[rgb]{ .902,  .486,  .451}37.3 & \cellcolor[rgb]{ .784,  .914,  .851}69.1 & \cellcolor[rgb]{ .976,  .894,  .886}14.7 & \cellcolor[rgb]{ .902,  .486,  .451}52.8 & \cellcolor[rgb]{ .902,  .961,  .933}56.1 & \cellcolor[rgb]{ .976,  .89,  .882}60.9 & \cellcolor[rgb]{ .961,  .808,  .796}67.9 & \cellcolor[rgb]{ .902,  .486,  .451}6.1 & \cellcolor[rgb]{ .925,  .616,  .592}42.7 & \cellcolor[rgb]{ .929,  .639,  .612}25.2 & \cellcolor[rgb]{ .902,  .486,  .451}51.3 \\
    PyEdu & 7.7   & \cellcolor[rgb]{ .984,  .933,  .929}68.3 & \cellcolor[rgb]{ .341,  .733,  .541}74.6 & \cellcolor[rgb]{ .851,  .941,  .898}83.0 & \cellcolor[rgb]{ .918,  .969,  .945}60.6 & \cellcolor[rgb]{ .969,  .839,  .827}38.2 & \cellcolor[rgb]{ .561,  .824,  .694}69.7 & \cellcolor[rgb]{ .906,  .965,  .933}15.6 & \cellcolor[rgb]{ .808,  .925,  .867}54.9 & \cellcolor[rgb]{ .827,  .929,  .878}57.0 & \cellcolor[rgb]{ .341,  .733,  .541}61.9 & \cellcolor[rgb]{ .98,  .91,  .902}68.6 & \cellcolor[rgb]{ .824,  .929,  .878}7.0 & 44.7  & \cellcolor[rgb]{ .902,  .486,  .451}24.6 & \cellcolor[rgb]{ .851,  .941,  .898}52.1 \\
    \midrule
    \codeio & 3.5   & 68.4  & \cellcolor[rgb]{ .341,  .733,  .541}74.6 & \cellcolor[rgb]{ .675,  .871,  .773}83.6 & \cellcolor[rgb]{ .875,  .949,  .914}60.9 & 38.6  & \cellcolor[rgb]{ .341,  .733,  .541}70.3 & \cellcolor[rgb]{ .533,  .812,  .678}18.7 & \cellcolor[rgb]{ .455,  .78,  .62}58.4 & \cellcolor[rgb]{ .341,  .733,  .541}62.8 & \cellcolor[rgb]{ .576,  .831,  .706}63.1 & \cellcolor[rgb]{ .341,  .733,  .541}70.8 & \cellcolor[rgb]{ .341,  .733,  .541}7.8 & \cellcolor[rgb]{ .467,  .784,  .627}46.0 & \cellcolor[rgb]{ .973,  .855,  .847}26.1 & \cellcolor[rgb]{ .341,  .733,  .541}53.6 \\
    \codeiopp & 3.5   & \cellcolor[rgb]{ .341,  .733,  .541}69.0 & \cellcolor[rgb]{ .949,  .98,  .965}73.5 & \cellcolor[rgb]{ .914,  .965,  .941}82.8 & \cellcolor[rgb]{ .875,  .949,  .914}60.9 & \cellcolor[rgb]{ .855,  .941,  .902}38.8 & \cellcolor[rgb]{ .451,  .78,  .62}70.0 & \cellcolor[rgb]{ .341,  .733,  .541}20.3 & \cellcolor[rgb]{ .341,  .733,  .541}59.5 & \cellcolor[rgb]{ .494,  .796,  .647}61.0 & \cellcolor[rgb]{ .341,  .733,  .541}64.2 & \cellcolor[rgb]{ .918,  .969,  .945}69.4 & 6.7   & \cellcolor[rgb]{ .341,  .733,  .541}46.3 & \cellcolor[rgb]{ .341,  .733,  .541}26.9 & \cellcolor[rgb]{ .361,  .741,  .557}53.5 \\
    \midrule
    \multicolumn{17}{c}{\textit{Gemma 2 27B}} \\
    \midrule
    \multicolumn{2}{c|}{2nd Stage Only} & 72.4 & 80.1  & 90.1 & 66.3  & 44.4  & 82.8  & 19.1  & 62.5  & 66.9  & 77.1 & 80.4  & 13.5 & 47.8  & 30.0 & 59.5 \\
    \midrule
    WI    & 3.5   & \cellcolor[rgb]{ .851,  .941,  .898}73.2 & \cellcolor[rgb]{ .902,  .486,  .451}79.0 & \cellcolor[rgb]{ .341,  .733,  .541}91.5 & \cellcolor[rgb]{ .341,  .733,  .541}70.6 & \cellcolor[rgb]{ .871,  .949,  .91}44.9 & \cellcolor[rgb]{ .988,  .945,  .945}82.7 & \cellcolor[rgb]{ .863,  .945,  .906}20.7 & \cellcolor[rgb]{ .737,  .894,  .82}63.5 & \cellcolor[rgb]{ .969,  .843,  .831}66.3 & \cellcolor[rgb]{ .749,  .898,  .827}77.6 & \cellcolor[rgb]{ .902,  .486,  .451}77.2 & \cellcolor[rgb]{ .341,  .733,  .541}17.1 & \cellcolor[rgb]{ .945,  .714,  .694}47.3 & \cellcolor[rgb]{ .8,  .922,  .859}33.3 & \cellcolor[rgb]{ .729,  .89,  .812}60.4 \\
    OMI2  & 3.5   & \cellcolor[rgb]{ .871,  .949,  .91}73.1 & \cellcolor[rgb]{ .925,  .624,  .6}79.3 & \cellcolor[rgb]{ .671,  .867,  .773}90.8 & \cellcolor[rgb]{ .878,  .953,  .918}67.1 & \cellcolor[rgb]{ .902,  .486,  .451}44.0 & \cellcolor[rgb]{ .341,  .733,  .541}83.4 & \cellcolor[rgb]{ .992,  1,  .996}19.2 & \cellcolor[rgb]{ .91,  .525,  .494}61.4 & \cellcolor[rgb]{ .953,  .769,  .749}66.0 & 77.1  & \cellcolor[rgb]{ .341,  .733,  .541}80.5 & \cellcolor[rgb]{ .929,  .973,  .949}13.9 & \cellcolor[rgb]{ .341,  .733,  .541}49.7 & \cellcolor[rgb]{ .341,  .733,  .541}40.7 & \cellcolor[rgb]{ .698,  .878,  .792}60.4 \\
    OC-SFT-1 & 4.2   & \cellcolor[rgb]{ .796,  .918,  .859}73.5 & \cellcolor[rgb]{ .98,  .906,  .898}79.9 & \cellcolor[rgb]{ .533,  .812,  .675}91.1 & \cellcolor[rgb]{ .933,  .655,  .631}66.1 & \cellcolor[rgb]{ .341,  .733,  .541}46.9 & \cellcolor[rgb]{ .902,  .486,  .451}81.8 & \cellcolor[rgb]{ .906,  .965,  .933}20.2 & \cellcolor[rgb]{ .922,  .969,  .945}62.8 & \cellcolor[rgb]{ .933,  .663,  .639}65.6 & \cellcolor[rgb]{ .902,  .961,  .929}77.3 & \cellcolor[rgb]{ .953,  .757,  .737}78.9 & \cellcolor[rgb]{ .91,  .965,  .937}14.0 & \cellcolor[rgb]{ .902,  .486,  .451}46.9 & \cellcolor[rgb]{ .675,  .871,  .776}35.3 & \cellcolor[rgb]{ .839,  .937,  .89}60.0 \\
    PyEdu & 7.7   & \cellcolor[rgb]{ .757,  .902,  .831}73.7 & \cellcolor[rgb]{ .945,  .718,  .698}79.5 & \cellcolor[rgb]{ .906,  .965,  .937}90.3 & \cellcolor[rgb]{ .902,  .486,  .451}66.0 & \cellcolor[rgb]{ .765,  .906,  .835}45.3 & 82.8  & \cellcolor[rgb]{ .902,  .486,  .451}18.7 & \cellcolor[rgb]{ .902,  .486,  .451}61.3 & \cellcolor[rgb]{ .902,  .486,  .451}64.9 & \cellcolor[rgb]{ .851,  .941,  .894}77.4 & \cellcolor[rgb]{ .957,  .773,  .757}79.0 & \cellcolor[rgb]{ .875,  .949,  .914}14.2 & \cellcolor[rgb]{ .62,  .847,  .737}48.9 & \cellcolor[rgb]{ .757,  .902,  .831}34.0 & \cellcolor[rgb]{ .941,  .976,  .961}59.7 \\
    \midrule
    \codeio & 3.5   & \cellcolor[rgb]{ .341,  .733,  .541}75.9 & \cellcolor[rgb]{ .792,  .918,  .859}80.7 & \cellcolor[rgb]{ .482,  .792,  .643}91.2 & \cellcolor[rgb]{ .835,  .933,  .886}67.4 & \cellcolor[rgb]{ .871,  .949,  .91}44.9 & \cellcolor[rgb]{ .455,  .78,  .62}83.3 & \cellcolor[rgb]{ .714,  .886,  .8}22.4 & \cellcolor[rgb]{ .341,  .733,  .541}65.0 & \cellcolor[rgb]{ .396,  .757,  .58}70.3 & \cellcolor[rgb]{ .596,  .839,  .718}77.9 & \cellcolor[rgb]{ .945,  .725,  .706}78.7 & \cellcolor[rgb]{ .8,  .922,  .863}14.6 & \cellcolor[rgb]{ .553,  .82,  .686}49.1 & \cellcolor[rgb]{ .922,  .969,  .945}31.3 & \cellcolor[rgb]{ .545,  .816,  .686}60.9 \\
    \codeiopp & 3.5   & \cellcolor[rgb]{ .871,  .949,  .91}73.1 & \cellcolor[rgb]{ .341,  .733,  .541}82.0 & \cellcolor[rgb]{ .388,  .753,  .576}91.4 & \cellcolor[rgb]{ .91,  .965,  .937}66.9 & \cellcolor[rgb]{ .58,  .831,  .71}46.0 & \cellcolor[rgb]{ .784,  .914,  .851}83.0 & \cellcolor[rgb]{ .341,  .733,  .541}26.6 & \cellcolor[rgb]{ .502,  .8,  .655}64.4 & \cellcolor[rgb]{ .341,  .733,  .541}70.6 & \cellcolor[rgb]{ .341,  .733,  .541}78.4 & \cellcolor[rgb]{ .918,  .58,  .553}77.8 & \cellcolor[rgb]{ .471,  .788,  .631}16.4 & \cellcolor[rgb]{ .447,  .776,  .616}49.4 & \cellcolor[rgb]{ .675,  .871,  .776}35.3 & \cellcolor[rgb]{ .341,  .733,  .541}61.5 \\
    \bottomrule
    \end{tabular}%

  \label{tab:main_results}%
      \vspace{-10pt}
\end{table*}%


% Table generated by Excel2LaTeX from sheet 'Ablation'
\begin{table*}
  \centering
  \setlength{\tabcolsep}{2pt}
  \small
  \caption{Key ablations we tested and the number of training samples under each condition. For a fairer comparison, we also provide results on a $\sim$ 50\% subset of \codeio{} to ensure the number of training samples remains comparable.}
    \begin{tabular}{lc|cccccccccccccc|c}
    \toprule
          & & Wino  & \multirow{2}[2]{*}{DROP} & GSM & \multirow{2}[2]{*}{MATH} & \multirow{2}[2]{*}{GPQA} & MMLU  & LC    & \multicolumn{2}{c}{CRUX} & \multicolumn{2}{c}{BBH} & Zebra & Kor   & Live  & \multirow{2}[2]{*}{AVG} \\
          &\# (M) & Grande &       &  8K     &       &       & -STEM & -O    & -I    & -O    & -EN   & -ZH   & Logic & Bench & Bench &  \\
    \midrule
    \codeio &3.52 & 67.9  & 76.4  & 86.4  & 71.9  & 43.3  & 77.3  & 23.7  & 63.6  & 64.9  & 69.3  & 72.8  & 10.7  & 44.3  & 28.5  & 57.2 \\
    \ \ \ \ $\sim$ 50\% subset &1.59 & 67.5  & 74.7  & 86.7  & 71.6  & 42.9  & 77.3  & 23.0  & 62.8  & 65.9  & 69.1  & 70.8  & 10.5  & 42.1  & 28.9  & 56.7 \\
    \midrule
    \multicolumn{16}{l}{\textit{Effect of prediction inputs or outputs only.}} \\
    \midrule
    \ \ \ \  I. Pred. only &1.75 & 66.3  & 75.9  & 85.8  & 71.6  & 38.8  & 77.7  & 22.9  & 62.8  & 64.5  & 68.3  & 69.4  & 11.4  & 44.4  & 26.2  & 56.1 \\
    \ \ \ \  O. Pred. only &1.76 & 66.9  & 75.2  & 84.6  & 71.5  & 42.4  & 76.5  & 23.3  & 61.1  & 65.6  & 70.1  & 72.1  & 11.4  & 42.2  & 26.9  & 56.4 \\
    \midrule
    \multicolumn{16}{l}{\textit{Effect of rejection sampling.}} \\
    \midrule
    \ \ \ \  w/o wrong &1.79 & 66.8  & 74.9  & 87.4  & 71.5  & 39.1  & 76.7  & 22.6  & 62.6  & 66.6  & 68.3  & 71.9  & 11.5  & 42.6  & 27.8  & 56.5 \\
    \ \ \ \  wrong$\to$gt &3.52 & 66.4  & 76.8  & 86.0  & 70.6  & 42.4  & 76.5  & 24.3  & 62.1  & 67.6  & 68.0  & 71.1  & 11.5  & 43.1  & 26.6  & 56.6 \\
    % \midrule
    % \multicolumn{16}{l}{\textit{Effect of different data sources.}} \\
    % \midrule
    % \ \ \ \  w/o CodeMix &1.84 & 65.8  & 76.6  & 87.3  & 70.9  & 42.6  & 77.0  & 21.8  & 62.0  & 65.0  & 68.5  & 69.5  & 10.7  & 43.8  & 26.8  & 56.3 \\
    % \ \ \ \  w/o PyEdu-R &1.89 & 66.8  & 75.4  & 86.0  & 71.4  & 40.6  & 77.0  & 24.1  & 61.8  & 64.8  & 69.8  & 72.3  & 11.0  & 46.3  & 30.1  & 57.0 \\
    \bottomrule
    \end{tabular}%
    % \vspace{-20pt}
  \label{tab:ablation}%
      \vspace{-10pt}
\end{table*}%

% \jl{should the hyper-parameters be moved to appendix?} <<<>>>
% \jh{in case space is tight, this paragraph can go to appendix}
% Both training and evaluation are performed using the HAI-LLM \cite{haillm}.

\textbf{Evaluation Benchmarks} We evaluate all models on these benchmarks: DROP \cite{dua2019drop}, WinoGrande \cite{sakaguchi2020winogrande}, GSM8K \cite{cobbe2021gsm8k}, MATH \cite{hendrycks2021math}, MMLU-STEM \cite{hendrycks2021mmlu}, BBH \cite{suzgun2023bbh}, GPQA \cite{rein2024gpqa}, CruxEval \cite{gu2024cruxeval}, ZebraGrid \cite{zebralogic2024}. These benchmarks span multiple key reasoning domains, including science, math \& numerical, symbolic, commonsense, logic, and code understanding. We also include two comprehensive benchmarks as well: LiveBench \cite{white2024livebench}\footnote{We adopt the 2406-2407 split, excluding the code generation and instruction-following subtasks as they are not our focus.}, and KorBench \cite{ma2024korbench}. Besides these established benchmarks, we test on two extra ones: BBH-ZH, a Chinese version of 9 BBH subtasks\footnote{For clarity, BBH is referred to as BBH-EN in later sections.} as our instruction tuning data contains both English and Chinese examples,
% \jh{better to mention the instruction tuning data is bilingual when introducing it, otherwise it seems weird why we are testing Chinese if both the base model and data are in English}
and LeetCode-O (LC-O), designed for bilingual output prediction for LeetCode questions with test cases. All evaluations are done with greedy decoding in a zero-shot setting, except for BBH-EN/-ZH where we use a 3-shot setup. Details of all benchmarks are in Appendix \ref{app:benchmark_details}.

\textbf{Baselines}  The primary baseline is to directly fine-tune the base model on the instruction-tuning dataset in a single stage (2nd Stage only). This serves to evaluate whether the additional training stage provides any tangible benefits.
We also select several strong datasets as baselines in the first Stage: \textit{WebInstruct} \cite{yue2024mammoth2}: A large instruction-tuning dataset with 11.6M samples mined from the Internet and refined by LLMs. \textit{OpenMathInstruct-2} \cite{toshniwal2024openmathinstruct}: A 14M-sample dataset focused on math problem solving, augmented from GSM8K and MATH using LLaMA 3.1 405B-Inst \cite{dubey2024llama3}.  \textit{OpenCoder-SFT-Stage-1} \cite{huang2024opencoder}: A 4.2M QA-pair dataset synthesized from general code data, covering diverse computer science domains.
% such as algorithms, data structures, and networking
\textit{Python-Edu} \cite{benallal2024smollmcorpus}: Following findings that continued pre-training on code tends to enhance reasoning, we adopt its full 7.7M code corpus and train on it using a standard language modeling loss.  For \textit{WebInstruct} and \textit{OpenMathInstruct-2}, we use 3.5M subsets for most experiments to align with the size of our \codeio{} dataset, but also report the scores when training on the complete datasets for Qwen 2.5 7B Coder.  



\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figs/distill_is_suboptimal.pdf}
    \caption{Average scores of Stage 1 training on \codeio{}, a 3.5M WebInstruct subset (WI) and an enhanced version distilled from DeepSeek-V2.5 Directly (WI-DS25).}
    \label{fig:distill_is_suboptimal}
        \vspace{-15pt}
\end{figure}

\subsection{Main Results}
\label{subsec:main_results}
% \jh{the following description for the table visuals better go to the caption of that table, rather than here}

% As shown, \codeio{} provides notably stable and universal gains across multiple benchmarks—outperforming not only the single-stage training baseline but also other mid-training data, even if some of them have larger sizes.
% % (PyEdu, WebInstruct-Full, OpenMathInstruct2-Full). 
% While these competing datasets may excel on some tasks (e.g., OpenMathInstruct2 on math-related ones) but regress on others (mixed \textcolor{mygreen}{green} and \textcolor{myred}{red} cells), \codeio{} displays consistent performance jumps (predominantly \textcolor{mygreen}{green} pattern). Although we only exploit code-centric training data, it enhance all the other non-code tasks beyond the expected code reasoning tasks, suggesting that the learned reasoning capability generalizes effectively beyond coding scenarios. This finding emphasizes that performance improvements do not merely stem from data volume; rather, carefully designed mid-training objectives can yield disproportionately larger benefits compared to sheer dataset size.
We demostrate the main evaluation results in Table \ref{tab:main_results}.
As shown, \codeio{} provides universal gains across benchmarks, outperforming both the single-stage baseline and other datasets, even larger ones.
% (e.g., PyEdu, WebInstruct-Full, OpenMathInstruct2-Full).
While competing datasets may excel in specific tasks (e.g., OpenMathInstruct2 on math) but regress in others (mixed \textcolor{mygreen}{green} and \textcolor{myred}{red} cells), \codeio{} shows consistent improvements (mainly \textcolor{mygreen}{green} patterns). Despite using only code-centric data, it enhances all other tasks beyond code reasoning as well, suggesting its generalizable capabilities. 
We also observe that training on raw code files (PythonEdu) results in only minor, and occasionally even negative, improvements compared to the single-stage baseline, significantly underperforming when compared to \codeio{}, suggesting that learning from such less-structured data is suboptimal.
This further highlights that performance gains are driven not merely by data size but by thoughtfully designed training tasks that encompass diverse, structured reasoning patterns in generalized CoTs.

\begin{figure*}[!t]
\centering
% 左图
\begin{subfigure}[b]{0.47\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{figs/scaling_instances_cropped.pdf} % 替换为你的图片路径
\caption{Size of randomly sampled subset.}
\label{fig:scaling_instances}
\end{subfigure}\begin{subfigure}[b]{0.47\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{figs/scaling_testcases_cropped.pdf} % 替换为你的图片路径
\caption{Ratio of testcases per sample compared to the full set.}
\label{fig:scaling_testcases}
\end{subfigure}

\caption{The scaling effect of \codeio{} in the first stage training.}
\label{fig:scaling}
    \vspace{-15pt}
\end{figure*}

% We also note that training on raw code files (PythonEdu) leads to minor and sometimes even negative improvement compared with the single-stage baseline, lagging far behind \codeio{}.
% Indictating learning these less structed data are suboptimal.
% This further highlights that performance gains stem not just from data size but from well-designed training tasks covering diverse structred reasoning patterns in a general language CoT style.
% \jh{I think it would be nice to have a dedicated description in this paragraph to compare with code continual pretraining, given that we are talking it is suboptimal in the introduction. You may say xx results verify that raw code is suboptimal xxxxx}

% Additionally, \codeiopp{} demonstrates systematic advantages over \codeio{}, consistently pushing average scores higher while avoiding trade-offs on individual tasks. This improvement illustrates that our execution-feedback-based multi-turn revision can further improve data quality and amplify gains in reasoning across diverse domains. Crucially, both \codeio{} and \codeiopp{} show universal effectiveness across different model sizes and architectures, further underscoring their versatility. Overall, the green cells that dominate the \codeio{} and \codeiopp{} rows validate that our proposed mid-training stage to predict code inputs and outputs empowers models to master a broad spectrum of reasoning tasks without compromising performance on specialized benchmarks.

Additionally, \codeiopp{} systematically outperforms \codeio{}, boosting average scores without trade-offs on individual tasks. This highlights how execution-feedback-based multi-turn revision improves data quality and enhances reasoning across domains. Most importantly, both \codeio{} and \codeiopp{} exhibit universal effectiveness across model sizes and architectures. The further validates that our training approach, predicting code inputs and outputs, enables models to excel in diverse reasoning tasks without sacrificing specialized benchmark performance.








\section{Analysis}
% \jh{I prefer a motivation start, like the analysis is to investigate the effect of the key components of our approach. Currently without motivation it sounds like analysis is part of a writing template for no reasons}
% To investigate the effects of different key conponents of our approach, we conduct sevreal anaysis experiments here. If not explicitly mentioned, all experiments are done on Qwen 2.5 Coder 7B for simplicity and the results after the second-stage general instruction-tuning are reported.
To examine the influence of different critical aspects of our approach, we carry out multiple analysis experiments. Unless explicitly stated otherwise, all experiments are performed using Qwen 2.5 Coder 7B for simplicity, and the results reported are those obtained after the second-stage general instruction tuning.

\label{sec:analysis}

% If not mention, we do all ablation on Qwen-2.5-7B-Coder only.





\subsection{Ablation Studies}
\label{subsec:ablation}
% \jl{add a \~50\% codeio here.}

% To understand how each component contributes to the final performance, we conduct several key ablation studies. The results are presented in Table \ref{tab:ablation}.

We first perform two key ablation studies on our data construction process, with results presented in Table \ref{tab:ablation}:
% \jh{regarding layout, I think you can try to move table 2 to this page which may be better}
% \jh{I think it needs to be mentioned in these ablations whether the training data amount is still the same as the original version, or the training data becomes less}

% \textbf{Input/Output Prediction}
% We first investigate the differences between input prediction and output prediction by training on each of them separately. On most benchmarks, the scores are similar. As expected, predicting input or output individually shows specific advantages on CRUXEval-I and -O, respectively. Interestingly, input prediction seems to hurt performance on GPQA but results in higher scores on KorBench. Conversely, output prediction provides greater benefits for symbolic reasoning tasks, such as BBH.

\textbf{Input/Output Prediction} We examine input and output prediction by training on each separately. The scores are generally similar, but input prediction excels on KorBench while slightly hurting GPQA, and output prediction shows greater benefits on symbolic reasoning tasks like BBH. CRUXEval-I and -O also favor input and output prediction, respectively. 

% \begin{table*}[]
%   \centering
%   \setlength{\tabcolsep}{2pt}
%   \small
%   \caption{Main results - Part 2. WI = WebInstruct, OMI2 = OpenMathInstruct2, OC-SFT-1 = OpenCoder-SFT-Stage-1.}

%     \begin{tabular}{lc|cccccccccccccc|c}
%     \toprule
%     \multicolumn{2}{c|}{Mid-Training} & Wino  & \multirow{2}[2]{*}{DROP} & GSM   & \multirow{2}[2]{*}{MATH} & \multirow{2}[2]{*}{GPQA} & MMLU  & LC    & \multicolumn{2}{c}{CRUX} & \multicolumn{2}{c}{BBH} & Zebra & Kor   & Live  & \multirow{2}[2]{*}{AVG} \\
%     Data  & \# (M) & Grande &       & 8K    &       &       & -STEM & -O    & -I    & -O    & -EN   & -ZH   & Logic & Bench & Bench &  \\
%     % \midrule
%     % \multicolumn{17}{c}{\textit{Qwen 2.5 Coder 7B}} \\
%     % \midrule
%     % \multicolumn{2}{c|}{w/o Mid-Training} & 66.9  & 70.7  & 83.4  & 71.6  & 41.5  & 77.2  & 20.7  & 61.3  & 60.0  & 68.3  & 70.6  & 10.9  & 38.7  & 26.0  & \cellcolor[rgb]{ .992,  .996,  .996}54.8 \\
%     % \midrule
%     % WI    & 3.5   & \cellcolor[rgb]{ .902,  .486,  .451}66.3 & \cellcolor[rgb]{ .784,  .914,  .851}73.5 & \cellcolor[rgb]{ .541,  .816,  .682}87.0 & \cellcolor[rgb]{ .969,  .851,  .843}71.4 & \cellcolor[rgb]{ .953,  .765,  .749}39.1 & \cellcolor[rgb]{ .863,  .945,  .902}77.5 & \cellcolor[rgb]{ .902,  .486,  .451}18.3 & \cellcolor[rgb]{ .91,  .533,  .502}59.1 & \cellcolor[rgb]{ .867,  .949,  .91}61.6 & \cellcolor[rgb]{ .929,  .973,  .953}68.6 & \cellcolor[rgb]{ .961,  .8,  .784}68.7 & \cellcolor[rgb]{ .914,  .549,  .518}10.2 & \cellcolor[rgb]{ .643,  .859,  .753}42.5 & 26.0  & \cellcolor[rgb]{ .961,  .984,  .973}55.0 \\
%     % % WI+   & 3.5   & \cellcolor[rgb]{ .949,  .741,  .722}66.6 & \cellcolor[rgb]{ .525,  .808,  .671}76.8 & \cellcolor[rgb]{ .38,  .753,  .569}88.2 & \cellcolor[rgb]{ .592,  .835,  .714}72.6 & \cellcolor[rgb]{ .49,  .796,  .643}42.9 & \cellcolor[rgb]{ .392,  .753,  .576}78.5 & \cellcolor[rgb]{ .925,  .973,  .949}21.1 & \cellcolor[rgb]{ .906,  .506,  .471}59.0 & \cellcolor[rgb]{ .878,  .953,  .914}61.5 & \cellcolor[rgb]{ .855,  .941,  .898}68.9 & \cellcolor[rgb]{ .6,  .839,  .722}72.8 & 10.9  & \cellcolor[rgb]{ .902,  .486,  .451}38.3 & \cellcolor[rgb]{ .902,  .486,  .451}25.6 & \cellcolor[rgb]{ .733,  .894,  .816}56.0 \\
%     % WI (Full) & 11.6  & \cellcolor[rgb]{ .937,  .976,  .957}67.0 & \cellcolor[rgb]{ .667,  .867,  .769}75.0 & \cellcolor[rgb]{ .541,  .816,  .682}87.0 & \cellcolor[rgb]{ .929,  .631,  .604}71.1 & \cellcolor[rgb]{ .49,  .796,  .643}42.9 & \cellcolor[rgb]{ .341,  .733,  .541}78.6 & \cellcolor[rgb]{ .933,  .659,  .635}19.1 & \cellcolor[rgb]{ .914,  .561,  .529}59.3 & \cellcolor[rgb]{ .902,  .486,  .451}59.8 & \cellcolor[rgb]{ .976,  .992,  .984}68.4 & \cellcolor[rgb]{ .992,  .976,  .976}70.4 & 10.9  & \cellcolor[rgb]{ .702,  .878,  .792}41.9 & \cellcolor[rgb]{ .663,  .863,  .765}27.6 & \cellcolor[rgb]{ .812,  .925,  .871}55.6 \\
%     % OMI2  & 3.5   & \cellcolor[rgb]{ .541,  .816,  .682}67.6 & \cellcolor[rgb]{ .722,  .886,  .804}74.3 & \cellcolor[rgb]{ .91,  .965,  .937}84.1 & \cellcolor[rgb]{ .714,  .886,  .8}72.3 & \cellcolor[rgb]{ .902,  .486,  .451}36.2 & \cellcolor[rgb]{ .91,  .965,  .937}77.4 & \cellcolor[rgb]{ .965,  .988,  .976}20.9 & \cellcolor[rgb]{ .961,  .804,  .792}60.4 & \cellcolor[rgb]{ .878,  .953,  .914}61.5 & \cellcolor[rgb]{ .878,  .953,  .918}68.8 & \cellcolor[rgb]{ .973,  .863,  .851}69.3 & \cellcolor[rgb]{ .902,  .486,  .451}10.1 & \cellcolor[rgb]{ .624,  .851,  .741}42.7 & \cellcolor[rgb]{ .745,  .898,  .824}27.2 & \cellcolor[rgb]{ .91,  .965,  .937}55.2 \\
%     % OMI2 (Full) & 14.0  & 66.9  & \cellcolor[rgb]{ .745,  .898,  .824}74.0 & \cellcolor[rgb]{ .341,  .733,  .541}88.5 & \cellcolor[rgb]{ .341,  .733,  .541}73.2 & \cellcolor[rgb]{ .988,  .941,  .937}40.9 & \cellcolor[rgb]{ .722,  .886,  .804}77.8 & \cellcolor[rgb]{ .965,  .827,  .816}19.9 & \cellcolor[rgb]{ .925,  .612,  .588}59.5 & \cellcolor[rgb]{ .8,  .922,  .863}62.4 & 68.3  & \cellcolor[rgb]{ .875,  .949,  .914}71.3 & \cellcolor[rgb]{ .341,  .733,  .541}11.2 & \cellcolor[rgb]{ .765,  .906,  .839}41.2 & \cellcolor[rgb]{ .49,  .796,  .647}28.4 & \cellcolor[rgb]{ .737,  .894,  .816}56.0 \\
%     % OC-SFT-1 & 4.2   & \cellcolor[rgb]{ .949,  .741,  .722}66.6 & \cellcolor[rgb]{ .643,  .855,  .749}75.3 & \cellcolor[rgb]{ .576,  .831,  .706}86.7 & \cellcolor[rgb]{ .902,  .486,  .451}70.9 & \cellcolor[rgb]{ .929,  .631,  .604}37.7 & \cellcolor[rgb]{ .627,  .851,  .741}78.0 & \cellcolor[rgb]{ .98,  .914,  .906}20.3 & \cellcolor[rgb]{ .98,  .914,  .906}60.9 & \cellcolor[rgb]{ .992,  1,  .996}60.1 & \cellcolor[rgb]{ .902,  .486,  .451}67.5 & \cellcolor[rgb]{ .937,  .682,  .663}67.6 & \cellcolor[rgb]{ .984,  .933,  .929}10.8 & \cellcolor[rgb]{ .871,  .949,  .91}40.1 & \cellcolor[rgb]{ .682,  .875,  .78}27.5 & \cellcolor[rgb]{ .957,  .984,  .969}55.0 \\
%     % PyEdu & 7.7   & \cellcolor[rgb]{ .965,  .827,  .816}66.7 & \cellcolor[rgb]{ .682,  .871,  .776}74.8 & \cellcolor[rgb]{ .69,  .878,  .784}85.8 & \cellcolor[rgb]{ .969,  .851,  .843}71.4 & \cellcolor[rgb]{ .988,  .941,  .937}40.9 & \cellcolor[rgb]{ .91,  .965,  .937}77.4 & \cellcolor[rgb]{ .933,  .655,  .631}19.1 & \cellcolor[rgb]{ .902,  .486,  .451}58.9 & \cellcolor[rgb]{ .8,  .922,  .863}62.4 & \cellcolor[rgb]{ .937,  .678,  .655}67.8 & \cellcolor[rgb]{ .902,  .486,  .451}65.7 & \cellcolor[rgb]{ .961,  .804,  .792}10.6 & \cellcolor[rgb]{ .945,  .98,  .961}39.3 & \cellcolor[rgb]{ .949,  .741,  .725}25.8 & \cellcolor[rgb]{ .902,  .486,  .451}54.8 \\
%     % \midrule
%     % \codeio & 3.5   & \cellcolor[rgb]{ .341,  .733,  .541}67.9 & \cellcolor[rgb]{ .557,  .82,  .69}76.4 & \cellcolor[rgb]{ .616,  .847,  .733}86.4 & \cellcolor[rgb]{ .878,  .953,  .918}71.9 & \cellcolor[rgb]{ .341,  .733,  .541}43.3 & \cellcolor[rgb]{ .957,  .984,  .969}77.3 & \cellcolor[rgb]{ .435,  .773,  .608}23.7 & \cellcolor[rgb]{ .341,  .733,  .541}63.6 & \cellcolor[rgb]{ .592,  .835,  .718}64.9 & \cellcolor[rgb]{ .757,  .902,  .831}69.3 & \cellcolor[rgb]{ .6,  .839,  .722}72.8 & \cellcolor[rgb]{ .973,  .871,  .859}10.7 & \cellcolor[rgb]{ .475,  .788,  .635}44.3 & \cellcolor[rgb]{ .471,  .788,  .631}28.5 & \cellcolor[rgb]{ .451,  .78,  .62}57.2 \\
%     % \codeiopp & 3.5   & 66.9  & \cellcolor[rgb]{ .341,  .733,  .541}79.1 & \cellcolor[rgb]{ .706,  .882,  .796}85.7 & \cellcolor[rgb]{ .796,  .918,  .859}72.1 & \cellcolor[rgb]{ .98,  .91,  .906}40.6 & \cellcolor[rgb]{ .675,  .871,  .773}77.9 & \cellcolor[rgb]{ .341,  .733,  .541}24.2 & \cellcolor[rgb]{ .659,  .863,  .761}62.5 & \cellcolor[rgb]{ .341,  .733,  .541}67.9 & \cellcolor[rgb]{ .341,  .733,  .541}71.0 & \cellcolor[rgb]{ .341,  .733,  .541}74.2 & \cellcolor[rgb]{ .973,  .871,  .859}10.7 & \cellcolor[rgb]{ .341,  .733,  .541}45.7 & \cellcolor[rgb]{ .341,  .733,  .541}29.1 & \cellcolor[rgb]{ .341,  .733,  .541}57.7 \\
%     % \midrule
%     % \multicolumn{17}{c}{\textit{LLaMA 3.1 8B}} \\
%     % \midrule
%     % \multicolumn{2}{c|}{w/o Mid-Training} & 71.3  & 73.1  & 83.2  & 49.9  & 40.6  & 70.0  & 4.1   & 44.5  & 46.9  & 65.8  & 65.6  & 9.8   & 39.8  & 25.7 & 49.3 \\
%     % \midrule
%     % WI    & 3.5   & \cellcolor[rgb]{ .416,  .765,  .596}72.1 & \cellcolor[rgb]{ .341,  .733,  .541}76.3 & \cellcolor[rgb]{ .969,  .851,  .839}82.8 & \cellcolor[rgb]{ .788,  .918,  .855}52.8 & \cellcolor[rgb]{ .478,  .792,  .639}42.9 & \cellcolor[rgb]{ .973,  .871,  .859}69.6 & 4.1   & \cellcolor[rgb]{ .965,  .816,  .8}44.0 & \cellcolor[rgb]{ .914,  .549,  .518}44.8 & \cellcolor[rgb]{ .925,  .627,  .6}64.5 & \cellcolor[rgb]{ .714,  .886,  .8}67.8 & \cellcolor[rgb]{ .816,  .925,  .871}10.0 & \cellcolor[rgb]{ .439,  .776,  .612}42.7 & \cellcolor[rgb]{ .902,  .486,  .451}23.1 & \cellcolor[rgb]{ .953,  .769,  .753}49.8 \\
%     % OMI2  & 3.5   & \cellcolor[rgb]{ .341,  .733,  .541}72.2 & \cellcolor[rgb]{ .651,  .859,  .757}74.8 & \cellcolor[rgb]{ .341,  .733,  .541}86.2 & \cellcolor[rgb]{ .341,  .733,  .541}58.9 & \cellcolor[rgb]{ .902,  .486,  .451}38.2 & \cellcolor[rgb]{ .341,  .733,  .541}70.1 & \cellcolor[rgb]{ .812,  .925,  .871}5.8 & \cellcolor[rgb]{ .824,  .929,  .878}46.1 & \cellcolor[rgb]{ .976,  .89,  .882}46.4 & \cellcolor[rgb]{ .749,  .902,  .827}67.4 & \cellcolor[rgb]{ .608,  .843,  .725}68.6 & \cellcolor[rgb]{ .902,  .486,  .451}9.5 & \cellcolor[rgb]{ .906,  .965,  .933}40.3 & \cellcolor[rgb]{ .953,  .761,  .745}24.5 & \cellcolor[rgb]{ .945,  .98,  .965}50.6 \\
%     % OC-SFT-1 & 4.2   & \cellcolor[rgb]{ .957,  .776,  .765}71.0 & \cellcolor[rgb]{ .965,  .824,  .812}71.9 & \cellcolor[rgb]{ .902,  .486,  .451}81.8 & \cellcolor[rgb]{ .914,  .965,  .941}51.1 & \cellcolor[rgb]{ .902,  .486,  .451}38.2 & \cellcolor[rgb]{ .902,  .486,  .451}68.4 & \cellcolor[rgb]{ .824,  .929,  .878}5.7 & \cellcolor[rgb]{ .929,  .631,  .604}43.5 & \cellcolor[rgb]{ .918,  .569,  .541}44.9 & \cellcolor[rgb]{ .988,  .941,  .937}65.6 & \cellcolor[rgb]{ .737,  .894,  .82}67.6 & \cellcolor[rgb]{ .341,  .733,  .541}10.5 & \cellcolor[rgb]{ .576,  .831,  .706}42.0 & \cellcolor[rgb]{ .961,  .8,  .788}24.7 & \cellcolor[rgb]{ .906,  .514,  .482}49.1 \\
%     % PyEdu & 7.7   & \cellcolor[rgb]{ .902,  .486,  .451}70.6 & \cellcolor[rgb]{ .902,  .486,  .451}69.6 & 83.2  & \cellcolor[rgb]{ .902,  .486,  .451}49.8 & \cellcolor[rgb]{ .592,  .835,  .718}42.4 & \cellcolor[rgb]{ .941,  .71,  .69}69.1 & \cellcolor[rgb]{ .878,  .953,  .918}5.2 & \cellcolor[rgb]{ .902,  .486,  .451}43.1 & \cellcolor[rgb]{ .902,  .486,  .451}44.5 & \cellcolor[rgb]{ .902,  .486,  .451}64.0 & 65.6  & \cellcolor[rgb]{ .627,  .851,  .741}10.2 & \cellcolor[rgb]{ .459,  .784,  .624}42.6 & 25.7  & \cellcolor[rgb]{ .902,  .486,  .451}49.0 \\
%     % \midrule
%     % \codeio & 3.5   & \cellcolor[rgb]{ .71,  .882,  .796}71.7 & \cellcolor[rgb]{ .835,  .933,  .886}73.9 & \cellcolor[rgb]{ .914,  .965,  .941}83.6 & \cellcolor[rgb]{ .718,  .886,  .804}53.8 & \cellcolor[rgb]{ .341,  .733,  .541}43.5 & \cellcolor[rgb]{ .937,  .678,  .655}69.0 & \cellcolor[rgb]{ .42,  .769,  .596}9.3 & \cellcolor[rgb]{ .376,  .749,  .565}50.1 & \cellcolor[rgb]{ .341,  .733,  .541}53.3 & \cellcolor[rgb]{ .733,  .894,  .816}67.5 & \cellcolor[rgb]{ .902,  .486,  .451}65.3 & \cellcolor[rgb]{ .435,  .773,  .608}10.4 & \cellcolor[rgb]{ .788,  .918,  .855}40.9 & \cellcolor[rgb]{ .961,  .8,  .788}24.7 & \cellcolor[rgb]{ .71,  .882,  .796}51.2 \\
%     % \codeiopp & 3.5   & \cellcolor[rgb]{ .635,  .855,  .749}71.8 & \cellcolor[rgb]{ .592,  .835,  .714}75.1 & \cellcolor[rgb]{ .827,  .929,  .878}84.0 & \cellcolor[rgb]{ .761,  .906,  .835}53.2 & \cellcolor[rgb]{ .933,  .973,  .953}40.9 & \cellcolor[rgb]{ .902,  .486,  .451}68.4 & \cellcolor[rgb]{ .341,  .733,  .541}10.0 & \cellcolor[rgb]{ .341,  .733,  .541}50.4 & \cellcolor[rgb]{ .365,  .745,  .557}53.1 & \cellcolor[rgb]{ .341,  .733,  .541}70.0 & \cellcolor[rgb]{ .341,  .733,  .541}70.6 & \cellcolor[rgb]{ .341,  .733,  .541}10.5 & \cellcolor[rgb]{ .341,  .733,  .541}43.2 & \cellcolor[rgb]{ .341,  .733,  .541}28.1 & \cellcolor[rgb]{ .341,  .733,  .541}52.1 \\
%     \midrule
%     \multicolumn{17}{c}{\textit{DeepSeek Coder v2 Lite 16B}} \\
%     \midrule
%     \multicolumn{2}{c|}{w/o Mid-Training} & 68.4  & 73.4 & 82.5 & 60.0 & 38.6  & 68.5 & 14.8  & 53.0  & 54.9 & 61.1  & 69.2  & 6.7   & 44.7  & 26.6 & 51.6 \\
%     \midrule
%     WI    & 3.5   & \cellcolor[rgb]{ .894,  .957,  .925}68.5 & \cellcolor[rgb]{ .784,  .914,  .851}73.8 & \cellcolor[rgb]{ .643,  .855,  .753}83.7 & \cellcolor[rgb]{ .933,  .973,  .953}60.5 & \cellcolor[rgb]{ .341,  .733,  .541}39.5 & \cellcolor[rgb]{ .929,  .973,  .949}68.7 & \cellcolor[rgb]{ .902,  .486,  .451}14.3 & \cellcolor[rgb]{ .953,  .98,  .969}53.5 & \cellcolor[rgb]{ .82,  .929,  .875}57.1 & \cellcolor[rgb]{ .894,  .961,  .929}61.6 & \cellcolor[rgb]{ .902,  .486,  .451}65.7 & \cellcolor[rgb]{ .882,  .953,  .918}6.9 & \cellcolor[rgb]{ .941,  .694,  .675}43.1 & \cellcolor[rgb]{ .937,  .69,  .667}25.4 & \cellcolor[rgb]{ .996,  .984,  .984}51.6 \\
%     OMI2  & 3.5   & \cellcolor[rgb]{ .902,  .486,  .451}67.6 & \cellcolor[rgb]{ .62,  .847,  .733}74.1 & \cellcolor[rgb]{ .341,  .733,  .541}84.7 & \cellcolor[rgb]{ .341,  .733,  .541}64.7 & \cellcolor[rgb]{ .984,  .918,  .914}38.4 & \cellcolor[rgb]{ .416,  .765,  .596}70.1 & \cellcolor[rgb]{ .918,  .588,  .557}14.4 & \cellcolor[rgb]{ .922,  .969,  .945}53.8 & \cellcolor[rgb]{ .925,  .973,  .949}55.8 & \cellcolor[rgb]{ .471,  .788,  .631}63.6 & \cellcolor[rgb]{ .922,  .588,  .561}66.4 & \cellcolor[rgb]{ .949,  .741,  .725}6.4 & \cellcolor[rgb]{ .902,  .486,  .451}42.0 & \cellcolor[rgb]{ .902,  .498,  .463}24.7 & \cellcolor[rgb]{ .902,  .961,  .933}51.9 \\
%     OC-SFT-1 & 4.2   & \cellcolor[rgb]{ .973,  .871,  .863}68.2 & \cellcolor[rgb]{ .894,  .957,  .925}73.6 & \cellcolor[rgb]{ .761,  .906,  .835}83.3 & \cellcolor[rgb]{ .875,  .949,  .914}60.9 & \cellcolor[rgb]{ .902,  .486,  .451}37.3 & \cellcolor[rgb]{ .784,  .914,  .851}69.1 & \cellcolor[rgb]{ .976,  .894,  .886}14.7 & \cellcolor[rgb]{ .902,  .486,  .451}52.8 & \cellcolor[rgb]{ .902,  .961,  .933}56.1 & \cellcolor[rgb]{ .976,  .89,  .882}60.9 & \cellcolor[rgb]{ .961,  .808,  .796}67.9 & \cellcolor[rgb]{ .902,  .486,  .451}6.1 & \cellcolor[rgb]{ .925,  .616,  .592}42.7 & \cellcolor[rgb]{ .929,  .639,  .612}25.2 & \cellcolor[rgb]{ .902,  .486,  .451}51.3 \\
%     PyEdu & 7.7   & \cellcolor[rgb]{ .984,  .933,  .929}68.3 & \cellcolor[rgb]{ .341,  .733,  .541}74.6 & \cellcolor[rgb]{ .851,  .941,  .898}83.0 & \cellcolor[rgb]{ .918,  .969,  .945}60.6 & \cellcolor[rgb]{ .969,  .839,  .827}38.2 & \cellcolor[rgb]{ .561,  .824,  .694}69.7 & \cellcolor[rgb]{ .906,  .965,  .933}15.6 & \cellcolor[rgb]{ .808,  .925,  .867}54.9 & \cellcolor[rgb]{ .827,  .929,  .878}57.0 & \cellcolor[rgb]{ .341,  .733,  .541}61.9 & \cellcolor[rgb]{ .98,  .91,  .902}68.6 & \cellcolor[rgb]{ .824,  .929,  .878}7.0 & 44.7  & \cellcolor[rgb]{ .902,  .486,  .451}24.6 & \cellcolor[rgb]{ .851,  .941,  .898}52.1 \\
%     \midrule
%     \codeio & 3.5   & 68.4  & \cellcolor[rgb]{ .341,  .733,  .541}74.6 & \cellcolor[rgb]{ .675,  .871,  .773}83.6 & \cellcolor[rgb]{ .875,  .949,  .914}60.9 & 38.6  & \cellcolor[rgb]{ .341,  .733,  .541}70.3 & \cellcolor[rgb]{ .533,  .812,  .678}18.7 & \cellcolor[rgb]{ .455,  .78,  .62}58.4 & \cellcolor[rgb]{ .341,  .733,  .541}62.8 & \cellcolor[rgb]{ .576,  .831,  .706}63.1 & \cellcolor[rgb]{ .341,  .733,  .541}70.8 & \cellcolor[rgb]{ .341,  .733,  .541}7.8 & \cellcolor[rgb]{ .467,  .784,  .627}46.0 & \cellcolor[rgb]{ .973,  .855,  .847}26.1 & \cellcolor[rgb]{ .341,  .733,  .541}53.6 \\
%     \codeiopp & 3.5   & \cellcolor[rgb]{ .341,  .733,  .541}69.0 & \cellcolor[rgb]{ .949,  .98,  .965}73.5 & \cellcolor[rgb]{ .914,  .965,  .941}82.8 & \cellcolor[rgb]{ .875,  .949,  .914}60.9 & \cellcolor[rgb]{ .855,  .941,  .902}38.8 & \cellcolor[rgb]{ .451,  .78,  .62}70.0 & \cellcolor[rgb]{ .341,  .733,  .541}20.3 & \cellcolor[rgb]{ .341,  .733,  .541}59.5 & \cellcolor[rgb]{ .494,  .796,  .647}61.0 & \cellcolor[rgb]{ .341,  .733,  .541}64.2 & \cellcolor[rgb]{ .918,  .969,  .945}69.4 & 6.7   & \cellcolor[rgb]{ .341,  .733,  .541}46.3 & \cellcolor[rgb]{ .341,  .733,  .541}26.9 & \cellcolor[rgb]{ .361,  .741,  .557}53.5 \\
%     \midrule
%     \multicolumn{17}{c}{\textit{Gemma 2 27B}} \\
%     \midrule
%     \multicolumn{2}{c|}{w/o Mid-Training} & 72.4 & 80.1  & 90.1 & 66.3  & 44.4  & 82.8  & 19.1  & 62.5  & 66.9  & 77.1 & 80.4  & 13.5 & 47.8  & 30.0 & 59.5 \\
%     \midrule
%     WI    & 3.5   & \cellcolor[rgb]{ .851,  .941,  .898}73.2 & \cellcolor[rgb]{ .902,  .486,  .451}79.0 & \cellcolor[rgb]{ .341,  .733,  .541}91.5 & \cellcolor[rgb]{ .341,  .733,  .541}70.6 & \cellcolor[rgb]{ .871,  .949,  .91}44.9 & \cellcolor[rgb]{ .988,  .945,  .945}82.7 & \cellcolor[rgb]{ .863,  .945,  .906}20.7 & \cellcolor[rgb]{ .737,  .894,  .82}63.5 & \cellcolor[rgb]{ .969,  .843,  .831}66.3 & \cellcolor[rgb]{ .749,  .898,  .827}77.6 & \cellcolor[rgb]{ .902,  .486,  .451}77.2 & \cellcolor[rgb]{ .341,  .733,  .541}17.1 & \cellcolor[rgb]{ .945,  .714,  .694}47.3 & \cellcolor[rgb]{ .8,  .922,  .859}33.3 & \cellcolor[rgb]{ .729,  .89,  .812}60.4 \\
%     OMI2  & 3.5   & \cellcolor[rgb]{ .871,  .949,  .91}73.1 & \cellcolor[rgb]{ .925,  .624,  .6}79.3 & \cellcolor[rgb]{ .671,  .867,  .773}90.8 & \cellcolor[rgb]{ .878,  .953,  .918}67.1 & \cellcolor[rgb]{ .902,  .486,  .451}44.0 & \cellcolor[rgb]{ .341,  .733,  .541}83.4 & \cellcolor[rgb]{ .992,  1,  .996}19.2 & \cellcolor[rgb]{ .91,  .525,  .494}61.4 & \cellcolor[rgb]{ .953,  .769,  .749}66.0 & 77.1  & \cellcolor[rgb]{ .341,  .733,  .541}80.5 & \cellcolor[rgb]{ .929,  .973,  .949}13.9 & \cellcolor[rgb]{ .341,  .733,  .541}49.7 & \cellcolor[rgb]{ .341,  .733,  .541}40.7 & \cellcolor[rgb]{ .698,  .878,  .792}60.4 \\
%     OC-SFT-1 & 4.2   & \cellcolor[rgb]{ .796,  .918,  .859}73.5 & \cellcolor[rgb]{ .98,  .906,  .898}79.9 & \cellcolor[rgb]{ .533,  .812,  .675}91.1 & \cellcolor[rgb]{ .933,  .655,  .631}66.1 & \cellcolor[rgb]{ .341,  .733,  .541}46.9 & \cellcolor[rgb]{ .902,  .486,  .451}81.8 & \cellcolor[rgb]{ .906,  .965,  .933}20.2 & \cellcolor[rgb]{ .922,  .969,  .945}62.8 & \cellcolor[rgb]{ .933,  .663,  .639}65.6 & \cellcolor[rgb]{ .902,  .961,  .929}77.3 & \cellcolor[rgb]{ .953,  .757,  .737}78.9 & \cellcolor[rgb]{ .91,  .965,  .937}14.0 & \cellcolor[rgb]{ .902,  .486,  .451}46.9 & \cellcolor[rgb]{ .675,  .871,  .776}35.3 & \cellcolor[rgb]{ .839,  .937,  .89}60.0 \\
%     PyEdu & 7.7   & \cellcolor[rgb]{ .757,  .902,  .831}73.7 & \cellcolor[rgb]{ .945,  .718,  .698}79.5 & \cellcolor[rgb]{ .906,  .965,  .937}90.3 & \cellcolor[rgb]{ .902,  .486,  .451}66.0 & \cellcolor[rgb]{ .765,  .906,  .835}45.3 & 82.8  & \cellcolor[rgb]{ .902,  .486,  .451}18.7 & \cellcolor[rgb]{ .902,  .486,  .451}61.3 & \cellcolor[rgb]{ .902,  .486,  .451}64.9 & \cellcolor[rgb]{ .851,  .941,  .894}77.4 & \cellcolor[rgb]{ .957,  .773,  .757}79.0 & \cellcolor[rgb]{ .875,  .949,  .914}14.2 & \cellcolor[rgb]{ .62,  .847,  .737}48.9 & \cellcolor[rgb]{ .757,  .902,  .831}34.0 & \cellcolor[rgb]{ .941,  .976,  .961}59.7 \\
%     \midrule
%     \codeio & 3.5   & \cellcolor[rgb]{ .341,  .733,  .541}75.9 & \cellcolor[rgb]{ .792,  .918,  .859}80.7 & \cellcolor[rgb]{ .482,  .792,  .643}91.2 & \cellcolor[rgb]{ .835,  .933,  .886}67.4 & \cellcolor[rgb]{ .871,  .949,  .91}44.9 & \cellcolor[rgb]{ .455,  .78,  .62}83.3 & \cellcolor[rgb]{ .714,  .886,  .8}22.4 & \cellcolor[rgb]{ .341,  .733,  .541}65.0 & \cellcolor[rgb]{ .396,  .757,  .58}70.3 & \cellcolor[rgb]{ .596,  .839,  .718}77.9 & \cellcolor[rgb]{ .945,  .725,  .706}78.7 & \cellcolor[rgb]{ .8,  .922,  .863}14.6 & \cellcolor[rgb]{ .553,  .82,  .686}49.1 & \cellcolor[rgb]{ .922,  .969,  .945}31.3 & \cellcolor[rgb]{ .545,  .816,  .686}60.9 \\
%     \codeiopp & 3.5   & \cellcolor[rgb]{ .871,  .949,  .91}73.1 & \cellcolor[rgb]{ .341,  .733,  .541}82.0 & \cellcolor[rgb]{ .388,  .753,  .576}91.4 & \cellcolor[rgb]{ .91,  .965,  .937}66.9 & \cellcolor[rgb]{ .58,  .831,  .71}46.0 & \cellcolor[rgb]{ .784,  .914,  .851}83.0 & \cellcolor[rgb]{ .341,  .733,  .541}26.6 & \cellcolor[rgb]{ .502,  .8,  .655}64.4 & \cellcolor[rgb]{ .341,  .733,  .541}70.6 & \cellcolor[rgb]{ .341,  .733,  .541}78.4 & \cellcolor[rgb]{ .918,  .58,  .553}77.8 & \cellcolor[rgb]{ .471,  .788,  .631}16.4 & \cellcolor[rgb]{ .447,  .776,  .616}49.4 & \cellcolor[rgb]{ .675,  .871,  .776}35.3 & \cellcolor[rgb]{ .341,  .733,  .541}61.5 \\
%     \bottomrule
%     \end{tabular}%

%   \label{tab:main_results_extra}%
% \end{table*}%

% \textbf{Rejection Sampling}
% It is intuitive to consider filtering out incorrect responses when synthesizing the data. To explore this, we perform rejection sampling by removing all incorrect responses from \codeio{}, which eliminates approximately 50\% of the training instances. However, we observe a general performance drop across most benchmarks, suggesting that removing incorrect responses does not bring additional benefits and may reduce the diversity of the data.

% We also experiment with replacing all incorrect responses with corresponding correct answers obtained through code execution (without CoT). While this approach improves scores on some benchmarks like LeetCode-O and CRUXEval-O—benchmarks designed to measure prediction accuracy—it lowers scores on other benchmarks, resulting in a reduced average score. Therefore, to maintain a balanced performance, we retain all incorrect responses in the main experiments to preserve data diversity.

\textbf{Rejection Sampling} We explore filtering incorrect responses using rejection sampling, which removes ~50\% of the training data. However, this results in a general performance drop, suggesting a loss of data diversity. We also experiment with replacing all incorrect responses with ground-truth answers through code execution (without CoT). We see improvements on benchmarks like LeetCode-O and CRUXEval-O designed to measure output prediction accuracy, but it lowers scores elsewhere, reducing the average performance. When comparing these two with training on a $\sim$ 50\% subset of \codeio{} where the number of samples are comparable, they still have no advantages. Therefore, to maintain performance balance, we retain all incorrect responses in the main experiments without any modification.
% We explore filtering incorrect responses with rejection sampling, which removes ~50\% of the training data. However, this leads to a general performance drop, suggesting loss in data diversity. We also try replacing all incorrect responses with correct answers through code execution (without CoT). We see improvements on benchmarks like LeetCode-O and CRUXEval-O designed to measure prediction accuracy but lowers scores elsewhere, reducing the average performance. To maintain balance, we retain all incorrect responses in the main experiments.



\subsection{Effect of Different Synthesis Model}

% An essential step of \codeio{} is to collect the responses from DeepSeek-V2.5. Consequently, it becomes unavoidable to compare \codeio{} with directly distilling responses using other prompts from DeepSeek-V2.5 as well. This comparison is necessary to demonstrate that predicting code inputs and outputs indeed contributes to improving reasoning, rather than simply relying on knowledge distillation from advanced models. 

% We utilize the aforementioned 3.5M subset of the WebInstruct dataset and regenerate responses for each prompt using DeepSeek-V2.5 to construct a new dataset, which we refer to as WebInstruct-DS25. The results are illustrated in Figure \ref{fig:distill_is_suboptimal}, where we plot the average scores across all benchmarks for \codeio{}, the vanilla WebInstruct, and WebInstruct-DS25.

% The evaluation shows that on both Qwen 2.5 Coder 7B and LLaMA 3.1 8B, the distilled version of WebInstruct demonstrates better performance compared to the vanilla one. However, it still falls short of \codeio{}, highlighting the importance of carefully selecting appropriate tasks when constructing large-scale mid-training data. These findings further underline the value of diverse reasoning activities contained in codes and the benefits of training models on them.

% \jhc{An essential step of \codeio{} is collecting responses from DeepSeek-V2.5, making it necessary to compare \codeio{} with directly distilling responses using other prompts from DeepSeek-V2.5.}{
Some of our baselines such as WebInstruct synthesize responses with Qwen-72B \cite{bai2023qwen} and Mixtral 22Bx8 \cite{jiang2024mixtral}, while \codeio{} uses DeepSeek-V2.5. To ablate the effect of different synthesis models,
% } \jhc{To this end,}{} 
we regenerate responses for the 3.5M WebInstruct (as it covers massive domains) subset using DeepSeek-V2.5, creating an updated dataset called WebInstruct-DS25. As shown in Figure \ref{fig:distill_is_suboptimal}, while WebInstruct-DS25 outperforms the vanilla dataset on Qwen 2.5 Coder 7B and LLaMA 3.1 8B, it still falls short of \codeio{}. This highlights the value of diverse reasoning patterns in code and the importance of task selection in training. Overall, this comparison shows that predicting code inputs and outputs improves reasoning beyond mere knowledge distillation from an advanced model.





\subsection{Scaling Effect of \codeio}
% We test how \codeio{} scales with different numbers of training instances. We first random sample various numbers of training instances and re-run the mid-training and post-training processes. The results in Figure \ref{fig:scaling_instances} display a clear trend: with more training instances, the model generally achieves better performance across all benchmarks. This indicates that \codeio{} data is scalable and effective in improving the generalizable reasoning ability of models.
% We test how \codeio{} scales with different amounts of training data. By randomly sampling training samples, Figure \ref{fig:scaling_instances} shows a clear trend: more instances in general lead to better performance across benchmarks,
% highlighting \codeio{}'s scalability and effectiveness in enhancing reasoning.
We evaluate how \codeio{} scales with varying amounts of training data. By randomly sampling training instances, Figure \ref{fig:scaling_instances} reveals a clear trend: increasing the number of training samples generally leads to improved performance across benchmarks. Specifically, using the smallest amount of data exhibits relatively weak performance on most benchmarks, as the model lacks sufficient training to generalize effectively. In contrast, when trained on the full dataset, \codeio{} achieves the most comprehensive and robust performance. Intermediate amounts of data yield results that fall between these two extremes, demonstrating a gradual improvement in performance as more training samples are introduced. This highlights \codeio{}'s scalability and effectiveness in enhancing reasoning capabilities.
% \jh{it does not seem the trend is clear, from the figure only the full one is obviously better than smaller ones, but the other three sizes are that not clear. In this description, better to be more precise} 

We also scale the data on the dimension of input-output pairs by fixing and using all unique raw code samples but changing the number of input-output prediction instances for each sample. Figure \ref{fig:scaling_testcases} shows the ratio of used I/O pairs compared to the full set. While the scaling effect is less pronounced than with training samples, we still observe clear benefits, particularly when increasing from 1/6 to 6/6. This suggests some reasoning patterns require multiple test cases to fully capture and learn their complex logic flow.

% We also conduct a stricter setting, in which we retain all samples corresponding to unique raw code files but scale up the number of input-output prediction instances. This setting is more appealing, as we can scale the size indefinitely by simply sampling more inputs and get outputs through execution. The results are shown in Figure \ref{fig:scaling_testcases}. Here, we denote the ratio of used test cases compared to the full set employed in the main experiments. Although the scaling effect is less pronounced compared to scaling training instances, we still observe benefits from using more test cases, especially when comparing 1/6 to 6/6. This suggests that, for certain samples, the model may require multiple test cases to fully learn the internal reasoning flow.

% See results in Figure \ref{fig:scaling}.
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[!t]
  \centering
  \setlength{\tabcolsep}{2pt}
  \small
  \caption{The effect of different data formats. We make \textbf{bold} the highest and \underline{underline} the lowest scores in each column. }
    \begin{tabular}{lc|cccccccccccccc|c}
    \toprule
    \multicolumn{2}{c|}{Data Format} & Wino  & \multirow{2}[2]{*}{DROP} & GSM   & \multirow{2}[2]{*}{MATH} & \multirow{2}[2]{*}{GPQA} & MMLU  & LC    & \multicolumn{2}{c}{CRUX} & \multicolumn{2}{c}{BBH} & Zebra & Kor   & Live  & \multirow{2}[2]{*}{AVG} \\
    Prompt  & Response & Grande &       & 8K    &       &       & -STEM & -O    & -I    & -O    & -EN   & -ZH   & Logic & Bench & Bench &  \\
    \midrule
    Q+Code & CoT   & \textbf{67.9}  & 76.4  & 86.4  & \textbf{71.9}  & \textbf{43.3}  & 77.3  & 23.7  & 63.6  & 64.9  & 69.3  & 72.8  & 10.7  & 44.3  & \textbf{28.5}  & \textbf{57.2} \\
    Q & CoT   & 67.2  & \textbf{76.8}  & 87.2  & \uline{70.4}  & \uline{37.5}  & 77.3  & \textbf{25.2}  & 62.6  & 65.3  & 69.2  & 71.1  & \textbf{11.5}  & 44.9  & \textbf{28.5}  & 56.8 \\
    Code  & CoT   &  \textbf{67.9}      &  76.4      & 87.0       &  70.8     & 39.5      &  \uline{76.5}    & 25.0     &  \textbf{64.1}    & 65.8      & 68.8      & 71.3      & 10.6      &  \textbf{45.2}     &  \textbf{28.5}     & 57.0 \\
    Q & Code+CoT & \uline{65.9}  & 76.1  & \textbf{87.5}  & 71.7  & 42.2  & 76.9  & 22.9  & 63.9  & \textbf{66.1}  & \textbf{69.6}  & \textbf{72.9}  & 10.9  & 41.4  & \textbf{28.5}  & 56.9 \\
    Q & Code  & 66.9  & \uline{73.1}  & \uline{84.8}  & 71.6  & 40.0    & \textbf{77.4}  & \uline{20.8}  & \uline{59.5}  & \uline{62.4}  & \uline{67.2}  & \uline{68.3}  & \uline{10.1}  & \uline{40.3}  & \uline{26.3}  & \uline{54.9} \\
    \bottomrule
    \end{tabular}%
  \label{tab:data_format}%
      \vspace{-10pt}
\end{table*}%

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/multi_turn_rev_line.pdf}
    \caption{Average benchmark scores from training on data from different turns of revision.}
    \label{fig:multi-turn-rev}
        \vspace{-15pt}
\end{figure}


\subsection{Different Data Format}

We investigate how to best arrange the query, reference code, and CoT in training samples. As shown in Table \ref{tab:data_format}, placing the query and reference code in the prompt and the CoT in the response achieves the highest average score and most balanced performance across benchmarks. Other formats show slightly lower but comparable performance, with the worst results occurring when the query is in the prompt and the reference code in the response, resembling a standard code generation task but with much fewer training samples. This highlights the importance of CoT and the scaling of test cases for learning transferable reasoning ability.

% Since there are multiple components, i.e., the query, reference code, and CoT in each training instance, we investigate how to arrange them in the prompt and response ends to achieve optimal performance. The results are shown in Table \ref{tab:data_format}, where we highlight the highest and lowest scores in each column. We observe that placing the query and reference code in the prompt and the CoT in the response is the optimal format, as it achieves the best average score and is also the most balanced across all benchmarks. For other formats, the scores are slightly lower, but, in general, the performance is similar. The worst performance occurs when the query is placed in the prompt and the reference code in the response, resembling a standard code generation task. Without the scaling of test cases, the number of training instances is much smaller, and without the CoT, it is also difficult to learn transferable reasoning ability.

% See Table \ref{tab:data_format}. We want to show which prompt-response format is the best.




\subsection{Multi-turn Revision}
Based on \codeio{} (no revision) and \codeiopp{} (single-turn revision), we extended revisions to a second turn to evaluate further improvements by regenerating predictions for instances still incorrect after the first revision. 
We visualize the distribution of response types in each turn in Figure \ref{fig:multi-turn-rev-stat} in Appendix \ref{app:multi_turn_rev_app}. It shows that most correct responses are predicted in the initial turn, with about 10\% of incorrect responses corrected in the first-turn revision. However, the second turn yields significantly fewer corrections, we find by checking the cases that the model often repeats the same incorrect CoT without adding new useful information. After incorporating multi-turn revisions, we observe consistent improvement from turn 0 to turn 1 but minimal gains from turn 1 to turn 2 in Figure \ref{fig:multi-turn-rev} -- showing slight improvement for LLaMA 3.1 8B but regression for Qwen 2.5 Coder 7B. Hence, we stop at single-turn revision, i.e., \codeiopp{}, in our main experiments.
% Based on \codeio{} (no revision) and \codeiopp{} (1 turn revision), we want to extend the revision to more turns to see if get further improvement.
% % We have tested \codeio{} and \codeiopp{} in the main experiments, corresponding to no revision and a single-turn revision, respectively. Naturally, we want to investigate whether revising for additional turns can further improve performance. 
% To this end, we conducted a second-turn revision by continuing to regenerate predictions for instances that remained incorrect after the first-turn revision. We have visualized the proportion of correct and incorrect predictions (including other error types) across each turn. 

% As shown in Figure \ref{fig:multi-turn-rev-stat}, most of the correct responses are already predicted in the initial turn. During the first-turn revision, approximately 10\% of the incorrect responses are successfully revised. However, in the second turn, the proportion of corrected responses decreases significantly. Upon examining specific cases, we often observe that the model merely repeats the same incorrect CoT, failing to introduce any new useful information.

% We also incorporate the results of multi-turn revisions during mid-training, as shown in Figure \ref{fig:multi-turn-rev}. From turn 0 to turn 1, we observe a consistent improvement. However, from turn 1 to turn 2, while the score slightly increases for LLaMA 3.1 8B, there is no additional improvement for Qwen 2.5 Coder 7B. Consequently, we decided to stop at the first-turn revision, i.e., \codeiopp{}, which is used in our main experiments.

% Stat in Figure \ref{fig:multi-turn-rev-stat}

% See results in Figure \ref{fig:multi-turn-rev}. We want to show how multi-turn revision works and the statistics of each turn of revision.





\subsection{The Necessity of Two Stage Training}

Lastly, we highlight the necessity of a separate training stage with \codeio{} data by testing both single-stage mixed training and two-stage training with different data mixtures. As shown in Table \ref{tab:why_mid_training}, all two-stage variants outperform single-stage training. Meanwhile, the effect of mixing data during two-stage training varies across models. For Qwen 2.5 Coder 7B, the best result is keeping \codeio{} and instruction-tuning data fully separate, while LLaMA 3.1 8B performs better with mixed data, either in the first stage or in the second stage. To simplify our methodology, we use fully separated data in our main experiments, leaving optimal data-mixing strategies for future work.

% Lastly, we demonstrate the necessity of incorporating the \codeio{} data in a separate mid-training stage. We verify this by testing several training settings, including single-stage mixed training and two-stage training with different data mixtures. The results in Table \ref{tab:why_mid_training} underscore the importance of the separate mid-training stage: all variants of two-stage training consistently outperform single-stage training.

% When analyzing the effect of mixing data during the two-stage training, the conclusions vary across models. For Qwen 2.5 Coder 7B, the best results are achieved when keeping the \codeio{} and instruction-tuning data completely separate. On the other hand, for LLaMA 3.1 8B, mixing \codeio{} with instruction-tuning data—whether during mid-training or in the instruction-tuning stage—achieves better performance.

% To simplify the methodology, we adopt the approach of keeping the data fully separated in our main experiments. Further exploration of optimal data-mixing strategies is left as future work.



% See results in Table \ref{tab:why_mid_training}. We compare several training recipe, single stage and double stage, to show why we need mid-training, and why we only use single source data in each stage.



% Table generated by Excel2LaTeX from sheet 'MidTrain'
\begin{table}[]
  \centering
  \setlength{\tabcolsep}{4pt}
  \small
  \caption{Average benchmark score under different training strategy. IT stands for our instruction-tuning data.}
    \begin{tabular}{cc|cc}
    \toprule
    First   & Second & \multicolumn{2}{c}{Model}\\
    Stage & Stage & Qwen & LLaMA \\
    \midrule
    -     & IT   & 54.8  & 49.3 \\
    -     & \codeio (10\%)+IT & 56.6     & 50.5 \\
    \codeio+IT & -     & 55.9     & 49.7 \\
    \codeio & IT   & 57.2  & 51.2 \\
    \codeio+IT & IT   & 56.8  & 51.5 \\
    \codeio & \codeio (10\%)+IT & 57.0     & 52.7 \\
    \bottomrule
    \end{tabular}%
  \label{tab:why_mid_training}%
      \vspace{-10pt}
\end{table}%



\section{Related Work}
\label{sec:related_work}

\paragraph{Learning about Code Execution}
The topic of learning code execution has existed long before the era of LLMs \cite{zaremba2014learning, graves2014neural}. However, most related works focus solely on the output prediction task itself when learning from code execution \cite{nye2021show, liu2023code, ding2024traced}. Other works seek to utilize code execution, either through the final feedback \cite{ding2024cycle, wang2024leti} or the intermediate trace \cite{ding2024semcoder, ni2024next}, to improve code generation abilities. There are also specific benchmarks designed to evaluate a model's ability to predict execution results, such as CRUXEval \cite{gu2024cruxeval} and LiveCodeBench-Exec \cite{jain2024livecodebench}. Unlike the above works, which set a narrow scope within code-related tasks, we are the first to train LLMs on large-scale, diverse code input-output predictions and demonstrate its efficacy in improving general reasoning ability beyond code.

\paragraph{Inference Time Scaling}
% Though of great importance, there is still no ultimate solution to enhance the reasoning capabilities of language models (LMs). Some previous researchers have pointed out that continual pre-training on code can improve reasoning abilities to some extent \cite{suzgun2023bbh}. However, its effectiveness is questioned when the base model is already strong or when it exhibits significant diminishing returns (i.e., reaching a point where further improvement requires an enormous amount of additional data). 
A very recent approach to enhance reasoning is inference-time scaling, such as OpenAI's o1 \cite{jaech2024openaio1} or DeepSeek's R1 \cite{deepseekai2025r1}, which typically encourages models to generate ultra-long reasoning process to solve problems through large-scale reinforcement learning. Such methods are pushing models to new limits on massive challenge tasks, while also significantly altering the output patterns of models. We believe that \codeio{} is orthogonal to these methods, and we hope it can provide a better basis to further incentivize the reasoning abilities of LLMs.
% Though of great importance, there is still not ultimate solution to enhance LM reasoning. Some previous researchers point out that continual pre-training on code can improve reasoning capabilities to some extent \cite{suzgun2023bbh}, but its effectiveness is questioned when the base model is already strong or with large edge effort. Another recent effective path is inference-time scaling, such as OpenAI's o1 \cite{jaech2024openaio1} or DeepSeek's R1 \cite{deepseekai2025r1}, which usually encourage the models to generate ultra long CoT to solve problems by large scale reinforcement learning. Methods like are making models pushing new limits on massive challenge benchmarks like AIME or codeforces, and also greatly change the output pattern of models. We believe \ccodeio{} is onthogonal to this type of methods, and we hope it can provide better model to further incentize the intrinsic ability.
% \paragraph{Mid-Training: Bridging Pre- and Post-Training}
% The most classical example of mid-training is continual pre-training \cite{gururangan2020don}, which typically involves enhancing the model's ability by training on raw documents of a specific targeted domain, such as math \cite{yang2024qwen25math,shao2024deepseekmath,zeng2024skyworkmath,ying2024internlmmath}, code \cite{zhu2024deepseekcoder,hui2024qwen25coder,roziere2023code}, or agents \cite{xu2024lemur}. Among these, code has been shown to improve reasoning capabilities to some extent \cite{suzgun2023bbh}; however, its effectiveness is questioned when the base model is already strong. More recently, large-scale supervised fine-tuning \cite{yue2024mammoth2,huang2024opencoder} has also emerged as an additional stage before general instruction tuning. Our work can be regarded as an extension of these efforts. However, instead of pursuing performance in a single domain, we aim to enhance generalizable reasoning ability across multiple scenarios by predicting code inputs and outputs as mid-training.

% \paragraph{Towards Generlizable Reasoning}
% \cite{ding2024semcoder}
% \cite{gu2024cruxeval}




\section{Conclusion}
\label{sec:conclusion}
% In this work, we introduced \codeio, a mid-training framework that improves reasoning by predicting code inputs and outputs between pre-training and fine-tuning. Experiments show that this approach significantly enhances performance across various reasoning tasks, demonstrating the potential of task-specific mid-training objectives. Future work can explore extending this method to other domains and refining its integration with fine-tuning strategies.
In conclusion, we introduced \codeio{}, an approach to improve the reasoning abilities of LLMs by training them to predict code inputs and outputs in pure natural language CoTs. This approach leverages the structured and scalable nature of code to learn diverse reasoning patterns, including symbolic, logical, mathematical, and commonsense reasoning. Extensive experiments show that \codeio{} as well as the enhanced \codeiopp{} consistently outperforms existing baselines, delivering balanced improvements across benchmarks without sacrificing performance in any domain, underscoring its robustness and versatility.



% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.

\nocite{langley00}

\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Details of Checking Execution Complexity}
\label{app:size_check_func}

% \jl{should this be put in appendix? though I think putting them here makes the paper look professional} <<<>>>>
During the execution of these codes, we set a runtime limit of 5 seconds for each sample. Additionally, we impose constraints on the complexity of the input and output objects to ensure they remain predictable and within the generation capability of general LLMs: total size of objects must be less than 1024 bytes, length of lists and dictionaries should be less than 20, and strings should be no longer than 100 characters. For objects other than these simple types, we enforce a size limit of 128 bytes. These checks are conducted recursively to ensure that all sub-objects within the full input/output object comply with these constraints. The exact code for these checks is shown below:

% \begin{minted}[fontsize=\small, breaklines, breakanywhere]{python}
% \begin{verbatim}
% ```python
% \end{verbatim}
\begin{lstlisting}[basicstyle=\small\ttfamily]
from pympler import asizeof

def strict_check_size(obj):
    if asizeof.asizeof(obj) >= 1024: 
        return False
    if isinstance(obj, dict):
        if len(obj) >= 20:  
            return False
        for k, v in obj.items():
            if not strict_check_size(k) or not strict_check_size(v):
                return False
    elif isinstance(obj, (list, tuple, set)):
        if len(obj) >= 20:  
            return False
        for item in obj:
            if not strict_check_size(item):
                return False
    elif isinstance(obj, str):
        if len(obj) >= 100: 
            return False
    else:
        if asizeof.asizeof(obj) >= 128:  
            return False
    return True
\end{lstlisting}

\section{Details of Selected Benchmarks}
\label{app:benchmark_details}
We introduce the details of all the benchmarks we use in this work. The sizes of their test sets are shown in Table \ref{tab:benchmark_stat}. Some parts of these descriptions largely refer to \citet{yue2024mammoth2}. The following are the established ones:

% \begin{enumerate}
\ \  \textbf{WinoGrande} \cite{sakaguchi2020winogrande}: WinoGrande is a benchmark for commonsense reasoning with expert-crafted pronoun resolution problems.

\ \  \textbf{DROP} \cite{dua2019drop}: DROP is a benchmark for numerical reasoning in reading comprehension. It demands resolving references and performing operations like addition, counting, or sorting. We report the F1 score as the metric.

\ \  \textbf{GSM8K} \cite{cobbe2021gsm8k}: GSM8K contains diverse grade-school math problems, intended to test basic arithmetic and reasoning abilities in an educational context.

\ \  \textbf{MATH} \cite{hendrycks2021math}: MATH comprises intricate competition-level problems across 5 levels to evaluate the models’ ability to perform complex mathematical reasoning.

\ \  \textbf{GPQA} \cite{rein2024gpqa}: GPQA provides ``Google-proof" questions in biology, physics, and chemistry, designed to test deep domain expertise and reasoning under challenging conditions. We use its complete set.

\ \  \textbf{MMLU-STEM} \cite{hendrycks2021mmlu}: MMLU spans 57 subjects across multiple disciplines. MMLU evaluates the breadth and depth of a model’s knowledge in a manner akin to academic and professional testing environments. We select the STEM subset of MMLU.
    % \item LeetCode-O:

\ \  \textbf{CRUXEval} \cite{gu2024cruxeval}: CRUXEval is designed to test a model's ability to predict the inputs or outputs given an anonymized Python function.

\ \  \textbf{BBH} \cite{suzgun2023bbh}: BBH consists of 23 tasks previously found challenging for language models from BIG-Bench \cite{srivastava2022beyond}.

\ \  \textbf{ZebraLogic} \cite{zebralogic2024}: ZebraLogic is a benchmark using Logic Grid Puzzles (Zebra Puzzles) to test LLMs' logical reasoning. It involves deducing unique feature assignments for N houses based on clues, evaluating Constraint Satisfaction Problem (CSP) solving skills, similar to human reasoning tests like the LSAT.

\ \  \textbf{KorBench} \cite{ma2024korbench}: KorBench is designed to evaluate models' intrinsic reasoning and planning abilities by minimizing interference from pretrained knowledge. It introduces new rules that are independent of prior knowledge, allowing for a more accurate assessment of how models adapt to novel rule-driven tasks. KorBench consists of five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual, each containing 25 manually defined rules.

\ \  \textbf{LiveBench} \cite{white2024livebench}: LiveBench is a benchmark for LLMs designed to prevent test set contamination and ensure objective evaluation. It releases new questions monthly, sourced from recent datasets, arXiv papers, news, and IMDb synopses. Each question has verifiable, objective answers, enabling accurate, automatic scoring without LLM judges. It includes 18 diverse tasks across 6 categories, including reasoning, coding, mathematics, data analysis, language, and instruction following.
% \end{enumerate}

We also have two extra ones created by ourselves:

% \begin{enumerate}
\ \  \textbf{BBH-ZH}: We translate 9 tasks from BBH, i.e., \textit{boolean expressions, date understanding, logical deduction five objects, navigate, object counting, reasoning about colored objects, temporal sequences, tracking shuffled objects five objects, web of lies}, to build a Chinese version.

\ \  \textbf{LeetCode-O}: We build a benchmark for testing code output prediction based on LeetCode problems. Different from CRUXEval, which only provides the Python function but not the textual query, LeetCode-O instead only provides the textual query but not the function, making it more challenging as the model needs to come up with the implicit solution by itself. In constructing this benchmark, we deliberately skip all LeetCode problems that have been covered by our training dataset and keep a balanced distribution of 300/300/300 across easy/medium/hard problems. For each problem, we also have both the original English version and a Chinese-translated version. We report the problem-level accuracy, which gets 1 point if and only if the model correctly predicts the outputs for all inputs under both languages; otherwise, it gets 0 points. The input-output pairs are collected solely from the original problem descriptions; thus, each problem usually has 2 or 3 test cases. We provide an example for this benchmark in Figure \ref{fig:leetcode_o_example}.
% \end{enumerate}

% Numbers of test samples of each evaluation benchmark are in Table \ref{tab:benchmark_stat}.

\begin{table*}[]
  \centering
  \setlength{\tabcolsep}{3pt}
  \small
  \caption{The number of test samples in the test set of each benchmark.}
    \begin{tabular}{cccccccccccccc}
    \toprule
     Wino  & \multirow{2}[2]{*}{DROP} & GSM   & \multirow{2}[2]{*}{MATH} & \multirow{2}[2]{*}{GPQA} & MMLU  & LC    & \multicolumn{2}{c}{CRUX} & \multicolumn{2}{c}{BBH} & Zebra & Kor   & Live \\

    Grande &       & 8K    &       &       & -STEM & -O    & -I    & -O    & -EN   & -ZH   & Logic & Bench & Bench\\
         \midrule
    1,267 & 9,536 & 1,319 & 5,000 & 448 & 3,153 & 900 & 800 & 800 & 6,511 & 2,250 & 1,000 &1,250 & 672\\
    
    \bottomrule
    \end{tabular}%
  \label{tab:benchmark_stat}%
  % \vspace{-10pt}
\end{table*}%

\begin{figure*}
    \centering
    \small
    \adjcontextb{48em}{\texttt{You have n coins and you want to build a staircase with these coins. The staircase consists of k rows where the ith row has exactly i coins. The last row of the staircase may be incomplete.\newline{}\newline{}Given the integer n, return the number of complete rows of the staircase you will build.\newline{}\newline{}n = 5\newline{}\newline{}What is the Output? Solve the problem without writing any code, and present your final answer at the end in the following json format: \{"output": $<$my output$>$\}.}}
    \caption{An example in the constructed Leetcode-O benchmark.}
    \label{fig:leetcode_o_example}
    \vspace{-15pt}
\end{figure*}





\section{Details of Processing Different Data Sources}
\subsection{Source Distribution}
\label{app:source_details}

\ \  \textbf{CodeMix} CodeMix is a large collection of raw Python code files retrieved and curated from an in-house code pre-training corpus. To ensure the quality and relevance of the dataset, we filter out files that are either overly simplistic or excessively complex. This filtering process is based on the success rate of the DeepSeek-Coder-V2-Lite-Inst model in doing a function completion task derived from each file. Files with a success rate between 10\% and 90\% are retained, resulting in a collection of approximately 427K code files.

\ \  \textbf{PyEdu-R} Python-Edu \cite{benallal2024smollmcorpus} is a large dataset containing about 7.7M high-quality Python code files sourced from the Stack-V2 dataset \cite{lozhkov2024starcoder2_stackv2}. These files are annotated with an additional scoring model to evaluate their educational quality. Since our analysis indicates that a significant portion of the CodeMix dataset focuses on algorithms, we intentionally exclude similar content from Python-Edu. To achieve this, we classify all code files into several categories, such as algorithms, logic puzzles, math-related tasks, scientific computation, system modeling, other complex reasoning, and non-reasoning codes, using both DeepSeek-Coder-V2-Lite-Inst and DeepSeek-V2.5. We remove files classified as algorithms and non-reasoning. The resulting subset is referred to as PyEdu-R(easoning) and contains approximately 369K code files.

\ \  \textbf{Other Sources} In addition to the two major datasets described above, we also collect high-quality code files from a variety of other reputable sources. These include comprehensive algorithm repositories\footnote{ \url{https://github.com/TheAlgorithms/Python}}, challenging mathematical problem collections\footnote{ \url{https://projecteuler.net}}, and well-known online coding platforms\footnote{ \url{https://github.com/doocs/leetcode}, \url{https://www.codewars.com}, \url{https://edabit.com}, \url{https://codeforces.com}, \url{https://atcoder.jp},  \url{https://www.codechef.com}}. After consolidating all of these sources, we obtain a total of approximately 14.5K code files.


\subsection{Input-Output Pairs for Each Source}
\label{app:io_pairs_source}
\ \  \textbf{CodeMix} For each sample in this subset, we select at most 3 pairs of input-output examples, resulting in 3 input prediction instances and 3 output prediction instances per sample. After filtering, we obtain 300K samples, with an average of 2.78 input and 2.80 output prediction instances per sample, totaling 1,674,345 instances.

\ \  \textbf{Pyedu-R} For each sample in this subset, we select at most 6 pairs of input-output examples, resulting in 6 input prediction instances and 6 output prediction instances per sample. After filtering, we obtain 141K samples, with an average of 5.77 input and 5.79 output prediction instances per sample, totaling 1,630,716 instances.

\ \  \textbf{Other Sources} For each sample in this subset, we select at most 10 pairs of input-output examples, resulting in 10 input prediction instances and 10 output prediction instances per sample. After filtering, we obtain 13.9K samples, with an average of 7.70 input and 7.87 output prediction instances per sample, totaling 216,159 instances.


\subsection{The Effect of Using Different Sources}
We analyze the contributions of our two main data sources, CodeMix and PyEdu-R, by excluding the training samples from each. The results, presented in Table \ref{tab:ablation_source}, indicate that removing PyEdu-R reduces performance on mathematical and scientific benchmarks (e.g., DROP, GSM8K, GPQA), consistent with its construction process. In contrast, removing CodeMix has a greater negative impact on symbolic or logical tasks, reflecting its focus on algorithmic content.

Nevertheless, combining both data sources yields the best overall performance. When comparing them to a similarly sized subset of \codeio{}, we observe that removing CodeMix results in a performance decline, while removing PyEdu-R has a smaller effect. Upon further inspection of the samples from PyEdu-R, we find that many focus on complex calculations involving nontrivial floating-point numbers, but place less emphasis on high-level reasoning or problem-solving flows. This characteristic makes PyEdu-R challenging for models to learn from effectively. Future work could explore refining or cleaning PyEdu-R to enhance its learnability and utility.
% We analyze contributions from our two main data sources, CodeMix and PyEdu-R, by removing the training samples of each. The results are in Table \ref{tab:ablation_source}. We find that removing PyEdu-R lowers scores on math/scientific benchmarks (e.g., DROP, GSM8K, GPQA), which aligns with its construction process, while CodeMix has a greater impact on symbolic or logical tasks due to its focus on algorithms. 
% Nevertheless, combining both yields the best overall performance. When comparing them with a subset of \codeio{} with similar size, we can see removing CodeMix leads to a performance drop, while removing PythonEdu is better. After checking the samples from PythonEdu, we identify that a large number of its samples are for complex calculation with non-trival floating numbers, but with less emphasis on high-level reasoning flow, which makes this data source hard to learn. Future works may include further cleaning PythonEdu to make it easy to learn.  
% \jh{again, we need to convince our comparison is fair. If people think that combining both just gives us more data and training steps then of course more data will be better}
% We also analyze the contributions of our two main data sources, CodeMix and PyEdu-R. Specifically, we examine the impact of separately removing training instances from each dataset. Our findings indicate that removing PyEdu-R generally leads to lower scores on math and scientific reasoning benchmarks (e.g., DROP, GSM8K, GPQA), aligning with the dataset's construction. On the other hand, CodeMix plays a more significant role in symbolic and logical reasoning tasks since its primary focus is on algorithms. Nevertheless, combining both datasets produces the best overall performance.

\begin{table*}
  \centering
  \setlength{\tabcolsep}{2pt}
  \small
  \caption{Ablation study on data sources, we remove all training samples originate from one major data source.}
    \begin{tabular}{lc|cccccccccccccc|c}
    \toprule
          & & Wino  & \multirow{2}[2]{*}{DROP} & GSM & \multirow{2}[2]{*}{MATH} & \multirow{2}[2]{*}{GPQA} & MMLU  & LC    & \multicolumn{2}{c}{CRUX} & \multicolumn{2}{c}{BBH} & Zebra & Kor   & Live  & \multirow{2}[2]{*}{AVG} \\
          &\# (M) & Grande &       &  8K     &       &       & -STEM & -O    & -I    & -O    & -EN   & -ZH   & Logic & Bench & Bench &  \\
    \midrule
    \codeio &3.52 & 67.9  & 76.4  & 86.4  & 71.9  & 43.3  & 77.3  & 23.7  & 63.6  & 64.9  & 69.3  & 72.8  & 10.7  & 44.3  & 28.5  & 57.2 \\
    \ \ \ \ $\sim$ 50\% subset &1.59 & 67.5  & 74.7  & 86.7  & 71.6  & 42.9  & 77.3  & 23.0  & 62.8  & 65.9  & 69.1  & 70.8  & 10.5  & 42.1  & 28.9  & 56.7 \\
    \midrule
    \ \ \ \  w/o CodeMix &1.84 & 65.8  & 76.6  & 87.3  & 70.9  & 42.6  & 77.0  & 21.8  & 62.0  & 65.0  & 68.5  & 69.5  & 10.7  & 43.8  & 26.8  & 56.3 \\
    \ \ \ \  w/o PyEdu-R &1.89 & 66.8  & 75.4  & 86.0  & 71.4  & 40.6  & 77.0  & 24.1  & 61.8  & 64.8  & 69.8  & 72.3  & 11.0  & 46.3  & 30.1  & 57.0 \\
    \bottomrule
    \end{tabular}%
    % \vspace{-20pt}
  \label{tab:ablation_source}%
      % \vspace{-30pt}
\end{table*}%


\section{Detailed Statistics in Multi-turn Revision}
\label{app:multi_turn_rev_app}
We report the detailed distribution of response types after each revision turn in Figure \ref{fig:multi-turn-rev-stat}, including both input and output predictions. In general, we observe that most correct predictions are already made in the initial turn. During the first revision turn, approximately 16\% and 10\% of incorrect predictions are revised for input and output predictions, respectively. However, in the second revision turn, significantly fewer predictions are revised, indicating a rapidly diminishing benefit from further revisions. Therefore, to save computing resources—both in prompting DeepSeek-V2.5 and executing the code for verification—we limit the revision process to a single turn in our main experiments.
% We report the detailed distribution of response type after each turn of revision in Figure \ref{fig:multi-turn-rev-stat}, including the input prediction and the output prediction. In general we see that most of the correct ones have already been predicted in the initial turn. In the first turn of revision, about 16\% and 10\% of the incorrect ones are revised for input and output prediction respectively. While in the second turn of revision, much fewer ones are revised, indicating a quick decreasing benefit to do further revision.



\begin{figure}
\centering
% 左图
\begin{subfigure}[b]{0.47\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{figs/input_pred_sankey_cropped.pdf} % 替换为你的图片路径
\caption{Input prediction revision.}
\label{fig:multi-turn-rev-stat_input}
\end{subfigure}\begin{subfigure}[b]{0.47\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{figs/output_pred_sankey_cropped.pdf} % 替换为你的图片路径
\caption{Output prediction revision.}
\label{fig:multi-turn-rev-stat_output}

\end{subfigure}

\caption{In multi-turn revision, we track the percentage (\%) of each response type across the entire dataset after each revision turn.}
\label{fig:multi-turn-rev-stat}
\end{figure}

\begin{table*}[!t]
  \centering
  \setlength{\tabcolsep}{2pt}
  \small
  \caption{Performance when switching the second stage instruction-tuning data to Tulu-3 \cite{lambert2024tulu3}.}
    \begin{tabular}{lc|cccccccccccccc|c}
    \toprule
    \multicolumn{2}{c|}{} & Wino  & \multirow{2}[2]{*}{DROP} & GSM   & \multirow{2}[2]{*}{MATH} & \multirow{2}[2]{*}{GPQA} & MMLU  & LC    & \multicolumn{2}{c}{CRUX} & \multicolumn{2}{c}{BBH} & Zebra & Kor   & Live  & \multirow{2}[2]{*}{AVG} \\
    Stage 1  & Stage 2 & Grande &       & 8K    &       &       & -STEM & -O    & -I    & -O    & -EN   & -ZH   & Logic & Bench & Bench &  \\
    \midrule
    - & Tulu3   & 66.9  & 59.9  & 81.0  & 38.3  & 27.2  & 69.3  & 16.6  & 53.9  & 59.9  & 63.9  & 69.8  & 8.9  & 35.2  & 22.5  & 48.1 \\
    \codeio{} & Tulu3     & 68.1 &59.4 &79.3 &39.8 &30.4 &71.3 &17.6 &57.1 &64.3&65.8&72.5&8.6&43.4&22.4 &50.0\\
    \codeiopp{}  & Tulu3  &67.6 &54.9 &80.5 &39.9 &27.5 &71.9 &20.6 &57.3 &63.3 &66.6&70.0&9.3&41.8&24.3 &49.7 \\
    \bottomrule
    \end{tabular}%
  \label{tab:tulu3}%
      % \vspace{-10pt}
\end{table*}%

\section{Training Hyper-parameters}
\label{app:hyper_para}
During the first stage, we train for 1 epoch using a constant learning rate, which is set to 1e-5 for the three smaller models and 4e-6 for Gemma 2 27B. The batch size is 1024. In the second stage, we train for 700 steps with the batch size of 1024 as well, corresponding to about 3 epochs of the entire instruction-tuning dataset. The learning rate is set to 3e-5 for the three smaller models and 1e-5 for Gemma 2 27B, using a cosine scheduler decaying to 1e-6 and 3e-7, respectively. In both training stages, no warmup period is applied and the maximum sequence length is set to 4096.

\section{The Effect of Using Other Instruction-Tuning Data}
\label{tulu3}
Besides our in-house instruction-tuning data, we also conduct experiments using another strong public instruction-tuning dataset, Tulu-3 \cite{lambert2024tulu3}. We present the benchmark performance when training on Qwen 2.5 Coder 7B in Table \ref{tab:tulu3}. After switching to this new dataset for the second stage of training, we still observe a significant improvement over the single-stage baseline, indicating the robustness of \codeio{}. However, contrary to our main experiments, \codeiopp{} performs slightly worse than \codeio{}. A potential reason for this could be that Tulu-3 does not cover as many instruction types as our in-house dataset, which may limit the model's ability to fully leverage its reasoning capabilities after training.




\section{Examples Mentioned in the Main Text}
\label{app:examples}
In this section, we present the examples mentioned in the main text. Table \ref{tab:unified_format_example} illustrates an example of how we transform a raw code file into our desired unified format.
% This includes the reference code (with the main entrypoint function), the textual query, the input/output description, and the Python input generator function. 
Figure \ref{fig:prompt_example} provides an example of the exact prompt used in both response collection and training. Additionally, Table \ref{tab:codeiopp_example} demonstrates a complete training sample in \codeiopp{}. In this sample, the initial response is incorrect, but after incorporating feedback and regenerating the response, DeepSeek-V2.5 successfully revise it to a correct prediction at the second turn.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}
  \small
  \centering
  \caption{A complete example showing how we transform a raw code file into our designed unified format.}
    \begin{tabular}{p{24em}|p{24em}}
    \toprule
    \multicolumn{1}{c|}{\textbf{Raw Code File}} & \multicolumn{1}{c}{\textbf{Cleaned Reference Code (with Main Entrypoint Function)}} \\
    \midrule
    \begin{lstlisting}
#get the vertical acceleration data 
acceleration [. . . . . . . . ]
# pass acceleration data to low pass filter
time[. . . . . . . . . . . . ]

#code to find speed at each point
initial_speed = current_speed
delta_t = t_current  - t_prev
curr_acc = acc[i]
current_speed = initial_speed + (curr_acc) * delta_t 

#code to find dispacement 

initial_dis = current_disp 
delta_t = t_current  - t_prev
curr_speed = ("call above algorithm")
current_disp = initial_dis + (initial_speed + current_speed)/2 * delta_t ; 

#code to find horizontal displacement

#use the above same code and find horizontal displacement 
#pass both the displacement data through low pass filter 
#map horizontal and vertical displacement to give road profile
    \end{lstlisting}
    % \#get the vertical acceleration data \newline{}acceleration [. . . . . . . . ]\newline{}\# pass acceleration data to low pass filter\newline{}time[. . . . . . . . . . . . ]\newline{}\newline{}\#code to find speed at each point\newline{}initial\_speed = current\_speed\newline{}delta\_t = t\_current  - t\_prev\newline{}curr\_acc = acc[i]\newline{}current\_speed = initial\_speed + (curr\_acc) * delta\_t \newline{}\newline{}\#code to find dispacement \newline{}\newline{}initial\_dis = current\_disp \newline{}delta\_t = t\_current  - t\_prev\newline{}curr\_speed = (``call above algorithm")\newline{}current\_disp = initial\_dis + (initial\_speed + current\_speed)/2 * delta\_t ; \newline{}\newline{}\#code to find horizontal displacement\newline{}\newline{}\#use the above same code and find horizontal displacement \newline{}\#pass both the displacement data through low pass filter \newline{}\#map horizontal and vertical displacement to give road profile
    &
    \begin{lstlisting}
# import necessary packages
import numpy as np

# main function
def main_solution(acceleration, time, initial_speed, initial_displacement):
    # Convert inputs to numpy arrays if they are not already
    acceleration = np.array(acceleration)
    time = np.array(time)
    
    # Initialize variables
    current_speed = initial_speed
    current_disp = initial_displacement
    
    # Calculate speed and displacement
    speeds = []
    displacements = []
    
    for i in range(1, len(time)):
        delta_t = time[i] - time[i-1]
        curr_acc = acceleration[i]
        current_speed = current_speed + curr_acc * delta_t
        speeds.append(current_speed)
        
        current_disp = current_disp + (initial_speed + current_speed) / 2 * delta_t
        displacements.append(current_disp)
        
        initial_speed = current_speed
    
    # Convert outputs to JSON serializable format
    speeds = [float(speed) for speed in speeds]
    displacements = [float(disp) for disp in displacements]
    
    return {"speeds": speeds, "displacements": displacements}
    \end{lstlisting}
    % \# import necessary packages\newline{}import numpy as np\newline{}\newline{}\# main function\newline{}def main\_solution(acceleration, time, initial\_speed, initial\_displacement):\newline{}   \mytab  \# Convert inputs to numpy arrays if they are not already\newline{}  \mytab   acceleration = np.array(acceleration)\newline{}  \mytab   time = np.array(time)\newline{}    \newline{}  \mytab   \# Initialize variables\newline{}   \mytab  current\_speed = initial\_speed\newline{}  \mytab   current\_disp = initial\_displacement\newline{}    \newline{}  \mytab   \# Calculate speed and displacement\newline{}  \mytab   speeds = []\newline{} \mytab    displacements = []\newline{}    \newline{}  \mytab   for i in range(1, len(time)):\newline{}     \mytab \mytab    delta\_t = time[i] - time[i-1]\newline{}  \mytab \mytab       curr\_acc = acceleration[i]\newline{}        \mytab \mytab current\_speed = current\_speed + curr\_acc * delta\_t\newline{}        \mytab \mytab speeds.append(current\_speed)\newline{}        \newline{}     \mytab \mytab    current\_disp = current\_disp + (initial\_speed + current\_speed) / 2 * delta\_t\newline{}        \mytab \mytab displacements.append(current\_disp)\newline{}        \newline{}    \mytab \mytab     initial\_speed = current\_speed\newline{}    \newline{}  \mytab   \# Convert outputs to JSON serializable format\newline{}    \mytab speeds = [float(speed) for speed in speeds]\newline{}  \mytab   displacements = [float(disp) for disp in displacements]\newline{}    \newline{}    \mytab return \{``speeds": speeds, ``displacements": displacements\} 
    \\
    \midrule
    \multicolumn{2}{c}{\textbf{Query}} \\
    \midrule
    \multicolumn{2}{p{48em}}{
    \texttt{Given a set of vertical acceleration data and corresponding time points, how can we determine the speed and displacement of a vehicle at each time point, starting from an initial speed and displacement?}} \\
    \midrule
    \multicolumn{1}{c|}{\textbf{Input/Output Description}} & \multicolumn{1}{c}{\textbf{Input Generator}} \\
    \midrule
    \begin{lstlisting}
Input:
    acceleration (list of float): List of vertical acceleration values at each time point.
    time (list of float): List of time points corresponding to the acceleration values.
    initial_speed (float): Initial speed at the first time point.
    initial_displacement (float): Initial displacement at the first time point.

Output:
    return (dict): A dictionary with two keys:
    - speeds (list of float): List of calculated speeds at each time point.
    - displacements (list of float): List of calculated displacements at each time point.
    \end{lstlisting}

    % Input:\newline{}    \mytab `acceleration` (list of float): List of vertical acceleration values at each time point.\newline{}    \mytab `time` (list of float): List of time points corresponding to the acceleration values.\newline{}    \mytab `initial\_speed` (float): Initial speed at the first time point.\newline{}    \mytab `initial\_displacement` (float): Initial displacement at the first time point.\newline{}\newline{}Output:\newline{}    \mytab  `return` (dict): A dictionary containing two keys:\newline{}    \mytab - `speeds` (list of float): List of calculated speeds at each time point.\newline{}    \mytab - `displacements` (list of float): List of calculated displacements at each time point. 
    &
    \begin{lstlisting}
def input_generator():
    # Generate random acceleration data
    acceleration = [np.random.uniform(-10, 10) for _ in range(10)]
    
    # Generate corresponding time data
    time = [0.1 * i for i in range(10)]
    
    # Generate initial speed and displacement
    initial_speed = np.random.uniform(0, 10)
    initial_displacement = np.random.uniform(0, 10)
    
    return {
        "acceleration": acceleration,
        "time": time,
        "initial_speed": initial_speed,
        "initial_displacement": initial_displacement
    }
    \end{lstlisting}
    % def input\_generator():\newline{} \mytab    \# Generate random acceleration data\newline{}  \mytab   acceleration = [np.random.uniform(-10, 10) for \_ in range(10)]\newline{}    \newline{}  \mytab   \# Generate corresponding time data\newline{}  \mytab   time = [0.1 * i for i in range(10)]\newline{}    \newline{}  \mytab   \# Generate initial speed and displacement\newline{}    \mytab initial\_speed = np.random.uniform(0, 10)\newline{}  \mytab   initial\_displacement = np.random.uniform(0, 10)\newline{}    \newline{}    \mytab return \{\newline{}    \mytab \mytab     ``acceleration": acceleration,\newline{}     \mytab \mytab    ``time": time,\newline{}    \mytab \mytab     ``initial\_speed": initial\_speed,\newline{}   \mytab \mytab      ``initial\_displacement": initial\_displacement\newline{}  \mytab   \} 
    \\
    \bottomrule
    \end{tabular}%
  \label{tab:unified_format_example}%
\end{table*}%

\begin{figure*}
    \centering
    \small
    \adjcontextb{48em}{\texttt{You are given a question that requires some input and output variables as follows:\newline{}\newline{}Given two jugs with capacities of `x` and `y` liters, is it possible to measure exactly `z` liters of water using these two jugs? What is the result of this measurement attempt?\newline{}\newline{}The input and output requirements are as follows:\newline{}\newline{}Input:\newline{}    \mytab `x` (int): The capacity of the first jug in liters.\newline{}   \mytab  `y` (int): The capacity of the second jug in liters.\newline{}  \mytab   `z` (int): The desired amount of water to measure in liters.\newline{}\newline{}Output:\newline{}    `return` (bool): True if it is possible to measure exactly z liters using the two jugs, False otherwise.\newline{}\newline{}
  \adjcontextdark{47.5em}{Given the following output:\mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \textcolor{myred}{Input Prediction}\newline{}\newline{}True\newline{}\newline{}Can you predict a feasible input without writing any code? Please reason and put your final answer in the following json format: {"input": <your input>}, where <your input> should be a dictionary, even if the there is only one input variable, with keys strictly match the input variables' names as specified.}
  \newline{}\mytab\newline{}
  \adjcontextdark{47.5em}{Given the following input:\mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \mytab \textcolor{myred}{Output Prediction}\newline{}\newline{}\{"x":5, ”y": 6, "z": 7\}\newline{}\newline{}Can you predict the output without writing any code? Please reason and put your final answer in the following json format: {"output": <your output>}, where <your output> should strictly match the the output requirement as specified.}
  \newline{}\newline{}Tip: Here is a reference code snippet for this question. You can refer to this code to guide your reasoning but not copy spans of code directly.\newline{}\newline{}\# import necessary packages\newline{}import random\newline{}\newline{}\# main function\newline{}def main\_solution(x, y, z):\newline{}    \mytab """\newline{}   \mytab  Determines if it is possible to measure exactly z liters using two jugs with capacities x and y liters.\newline{}   \mytab  \newline{}  \mytab   :param x: int, capacity of the first jug in liters\newline{}  \mytab   :param y: int, capacity of the second jug in liters\newline{}   \mytab  :param z: int, the desired amount of water to measure in liters\newline{}  \mytab   :return: bool, True if it is possible to measure exactly z liters, False otherwise\newline{} \mytab    """\newline{}   \mytab  if z == 0:\newline{}     \mytab \mytab    return True\newline{}  \mytab   if x + y < z:\newline{}   \mytab \mytab      return False\newline{}   \mytab  if x > y:\newline{}    \mytab \mytab     x, y = y, x\newline{}   \mytab  if x == 0:\newline{}    \mytab \mytab     return y == z\newline{}  \mytab   while y \% x != 0:\newline{}    \mytab \mytab     y, x = x, y \% x\newline{}  \mytab   return z \% x == 0
  }}
    \caption{An example prompt we used for collecting responses and training. Most parts of the prompt for input and output prediction remain the same, except for the two gray background blocks, which is used as needed.}
    \label{fig:prompt_example}
\end{figure*}

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \scriptsize
  \caption{A complete example for input prediction in \codeiopp{} training data. The reference code is omitted in the \textbf{prompt}, and the\blueback{given input} is highlighted. For the \textbf{concatenated response}, we highlight the\yellowback{template}, the\redback{Turn 1 feedback}, and the\greenback{Turn 2 feedback}, and all the other contents are generated by DeepSeek-V2.5 in two turns.}
    \begin{tabular}{p{60em}}
    \toprule
    \multicolumn{1}{c}{\textbf{Prompt}} \\
    \midrule
    You are given a question that requires some input and output variables as follows:\newline{}\newline{}Given a list of integers \texttt{numbers} and an integer \texttt{target}, determine the length of the shortest contiguous subarray whose sum is at least \texttt{target}. If no such subarray exists, return 0. How can you find this shortest subarray length efficiently?\newline{}\newline{}The input and output requirements are as follows:\newline{}\newline{}
    Input:\newline{}    \mytab- \texttt{target} (int): The target sum that the subarray should at least reach.\newline{}    \mytab- \texttt{numbers} (list of int): A list of integers representing the array from which the subarray is to be found.\newline{}\newline{}Output:\newline{}    \mytab- \texttt{return} (int): The length of the shortest contiguous subarray whose sum is at least the target. Returns 0 if no such subarray exists.\newline{}\newline{}\blueback{Given the following output: \texttt{4}}\newline{}\newline{}Can you predict a feasible input without writing any code? Please reason and put your final answer in the following json format: \texttt{\{"input": $<$your input$>$\}}, where \texttt{$<$your input$>$} should be a dictionary with keys strictly match the input variables' names as specified.
    % \newline{}\newline{}
    % Tip: Here is a reference code snippet for this question. You can refer to this code to guide your reasoning but not copy spans of code directly.\newline{}\newline{}import sys\newline{}\newline{}def minimum\_subarray\_sum(target: int, numbers: list[int]) -> int:\newline{}    if not numbers:\newline{}        return 0\newline{}    if target == 0 and target in numbers:\newline{}        return 0\newline{}    if not isinstance(numbers, (list, tuple)) or not all(\newline{}        isinstance(number, int) for number in numbers\newline{}    ):\newline{}        raise ValueError("numbers must be an iterable of integers")\newline{}\newline{}    left = right = curr\_sum = 0\newline{}    min\_len = sys.maxsize\newline{}\newline{}    while right < len(numbers):\newline{}        curr\_sum += numbers[right]\newline{}        while curr\_sum >= target and left <= right:\newline{}            min\_len = min(min\_len, right - left + 1)\newline{}            curr\_sum -= numbers[left]\newline{}            left += 1\newline{}        right += 1\newline{}\newline{}    return 0 if min\_len == sys.maxsize else min\_len\newline{}\newline{}def main\_solution(target: int, numbers: list[int]) -> int:\newline{}    return minimum\_subarray\_sum(target, numbers) 
    \\
    \midrule
    \multicolumn{1}{c}{\textbf{Concatenated Response (Turn 1 generation + Turn 1 feedback + Turn 2 generation + Turn 2 feedback)}} \\
    \midrule
    To predict a feasible input that results in the output \texttt{4}, we need to consider the following:\newline{}\newline{}1. \textbf{Target Sum}: The target sum is the minimum sum that a subarray should achieve.\newline{}2. \textbf{Numbers List}: The list of integers from which we need to find the shortest subarray.\newline{}3. \textbf{Output}: The length of the shortest subarray whose sum is at least the target.\newline{}\newline{}Given the output is \texttt{4}, this means the shortest subarray that meets or exceeds the target sum has a length of 4.\newline{}\newline{}\#\#\# Reasoning:\newline{}- \textbf{Target Sum}: Let's assume the target sum is \texttt{10}.\newline{}- \textbf{Numbers List}: We need to construct a list where the shortest subarray that sums to at least \texttt{10} has a length of \texttt{4}.\newline{}\newline{}\#\#\# Example Construction:\newline{}- If we have a list like \texttt{[1, 2, 3, 4, 5]}, the subarray \texttt{[2, 3, 4, 5]} sums to \texttt{14}, which is greater than \texttt{10}, and its length is \texttt{4}.\newline{}\newline{}Thus, a feasible input that results in the output \texttt{4} could be:
    \begin{verbatim}```json
{"input": {"target": 10, "numbers": [1, 2, 3, 4, 5]}}
```\end{verbatim}This input satisfies the condition that the shortest subarray with a sum of at least \texttt{10} has a length of \texttt{4}.\newline{}\newline{}\yellowback{Let me check if I did it correctly ...... Oops! Something went wrong and I find this}\newline{}\newline{}\redback{[Mismatch] Your input is not feasible! Given the output \texttt{4}, your predicted input is \texttt{\{"target": 10, "numbers": [1, 2, 3, 4, 5]\}}, which actually gets a wrong output as 3}\newline{}\newline{}\yellowback{Well ......} I apologize for the oversight. Let's re-evaluate the input to ensure it matches the output \texttt{4}.\newline{}\newline{}We need to find a list of integers where the shortest contiguous subarray whose sum is at least \texttt{target} has a length of exactly \texttt{4}.\newline{}\newline{}\#\#\# Revised Reasoning:\newline{}- \textbf{Target Sum}: Let's keep the target sum as \texttt{10}.\newline{}- \textbf{Numbers List}: We need a list where a subarray of length \texttt{4} sums to \texttt{10} or more.\newline{}\newline{}\#\#\# Example Construction:\newline{}- Consider the list \texttt{[1, 2, 2, 2, 2, 2]}:\newline{}    \mytab- The subarray \texttt{[2, 2, 2, 2]} sums to \texttt{8}, which is less than \texttt{10}.\newline{}    \mytab- However, if we add another \texttt{2}, making it \texttt{[1, 2, 2, 2, 2, 2, 2]}, the subarray \texttt{[2, 2, 2, 2]} still sums to \texttt{8}.\newline{}\newline{}- Let's try a different approach with larger numbers to ensure the subarray of length \texttt{4} sums to at least \texttt{10}:\newline{}    \mytab- Consider the list \texttt{[1, 3, 2, 2, 5, 1]}:\newline{}    \mytab\mytab- The subarray \texttt{[1, 3, 2, 2]} sums to \texttt{8}, which is less than \texttt{10}.\newline{}    \mytab\mytab- The subarray \texttt{[3, 2, 2, 5]} sums to \texttt{12}, which is more than \texttt{10}, and its length is \texttt{4}.\newline{}\newline{}Thus, a feasible input that results in the output \texttt{4} could be:\begin{verbatim}```json
{"input": {"target": 10, "numbers": [1, 3, 2, 2, 5, 1]}}
```
\end{verbatim}This input satisfies the condition that the shortest subarray with a sum of at least \texttt{10} has a length of \texttt{4}.\newline{}\newline{}\yellowback{Let me check if I did it correctly ......} \greenback{Yes, that's correct! I made it!} \\
    \bottomrule
    \end{tabular}%
  \label{tab:codeiopp_example}%
\end{table}%


\end{document}



% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
