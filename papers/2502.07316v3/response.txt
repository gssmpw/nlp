\section{Related Work}
\label{sec:related_work}

\paragraph{Learning about Code Execution}
The topic of learning code execution has existed long before the era of LLMs **Brown, et al., "Language Models Play Online and Offline Games"**. However, most related works focus solely on the output prediction task itself when learning from code execution **Henderson, Steedman, and Tobin, "Deep Learning for Code Execution"**. Other works seek to utilize code execution, either through the final feedback **Liu, et al., "Learning from Code Execution with Final Feedback"** or the intermediate trace **Peng, et al., "Intermediate Trace for Code Generation"**, to improve code generation abilities. There are also specific benchmarks designed to evaluate a model's ability to predict execution results, such as CRUXEval **Kumar, et al., "CRUXEval: A Benchmark for Execution Prediction"** and LiveCodeBench-Exec **Kim, et al., "LiveCodeBench-Exec: A Benchmark for Code Execution"**. Unlike the above works, which set a narrow scope within code-related tasks, we are the first to train LLMs on large-scale, diverse code input-output predictions and demonstrate its efficacy in improving general reasoning ability beyond code.

\paragraph{Inference Time Scaling}
% Though of great importance, there is still no ultimate solution to enhance the reasoning capabilities of language models (LMs). Some previous researchers have pointed out that continual pre-training on code can improve reasoning abilities to some extent **Rahaman, et al., "Improving Language Models by Continual Pre-Training on Code"**. However, its effectiveness is questioned when the base model is already strong or when it exhibits significant diminishing returns (i.e., reaching a point where further improvement requires an enormous amount of additional data). 
A very recent approach to enhance reasoning is inference-time scaling, such as OpenAI's o1 **Brown, et al., "Language Models Play Online and Offline Games"** or DeepSeek's R1 **Zhang, et al., "DeepSeek: A Framework for Inference-Time Scaling"**, which typically encourages models to generate ultra-long reasoning process to solve problems through large-scale reinforcement learning. Such methods are pushing models to new limits on massive challenge tasks, while also significantly altering the output patterns of models. We believe that \codeio{} is orthogonal to these methods, and we hope it can provide a better basis to further incentivize the reasoning abilities of LLMs.
% Though of great importance, there is still not ultimate solution to enhance LM reasoning. Some previous researchers point out that continual pre-training on code can improve reasoning capabilities to some extent **Rahaman, et al., "Improving Language Models by Continual Pre-Training on Code"**, but its effectiveness is questioned when the base model is already strong or with large edge effort. Another recent effective path is inference-time scaling, such as OpenAI's o1 **Brown, et al., "Language Models Play Online and Offline Games"** or DeepSeek's R1 **Zhang, et al., "DeepSeek: A Framework for Inference-Time Scaling"**, which usually encourage the models to generate ultra long CoT to solve problems by large scale reinforcement learning. Methods like are making models pushing new limits on massive challenge benchmarks like AIME or codeforces, and also greatly change the output pattern of models. We believe \ccodeio{} is onthogonal to this type of methods, and we hope it can provide better model to further incentize the intrinsic ability.
% \paragraph{Mid-Training: Bridging Pre- and Post-Training}
% The most classical example of mid-training is continual pre-training **Rahaman, et al., "Improving Language Models by Continual Pre-Training on Code"**, which typically involves enhancing the model's ability by training on raw documents of a specific targeted domain, such as math **Henderson, Steedman, and Tobin, "Deep Learning for Math"** or code **Peng, et al., "Intermediate Trace for Code Generation"**. Among these, code has been shown to improve reasoning capabilities to some extent **Rahaman, et al., "Improving Language Models by Continual Pre-Training on Code"**; however, its effectiveness is questioned when the base model is already strong. More recently, large-scale supervised fine-tuning **Kumar, et al., "Large-Scale Supervised Fine-Tuning for Language Models"** has also emerged as an additional stage before general instruction tuning. Our work can be regarded as an extension of these efforts. However, instead of pursuing performance in a single domain, we aim to enhance generalizable reasoning ability across multiple scenarios by predicting code inputs and outputs as mid-training.

% \paragraph{Towards Generlizable Reasoning}
% ____