\section{Related Work}
\label{sec:relatedwork}
\textbf{Diffusion Models:} Denoising Diffusion Probabilistic Models (DDPMs) \cite{ddpm_ho_neurips2020} have demonstrated remarkable image synthesis quality, outperforming previous generative models by a substantial margin \cite{diffusion_beats_gans_neurips2021}. Consequently, various text-to-image foundation models, including DALL$\cdot$E-2 \cite{dalle2_arxiv2022} and Imagen \cite{imagen_neurips2022}, were developed. However, to address the significant training and inference complexities, the LDM approach was introduced in \cite{stablediffusion_cvpr2022}, where the diffusion model is trained within the compressed latent space of a VAE. The SD family of models \cite{stablediffusion_cvpr2022, sdxl_arxiv2023} is based on LDM.

\textbf{IBE:} In terms of text-prompt guided image editing, InstructPix2Pix (IP2P) \cite{ip2p_cvpr2023} was a pioneering work. The authors utilized Prompt-to-Prompt \cite{p2p_iclr2023} and DDIM inversion \cite{ddim_inversion_iclr2021} to generate source and edited image pairs by replacing the cross-attention maps of common tokens in the edited image reverse process. IP2P \cite{ip2p_cvpr2023} generated approximately $450,000$ image and caption pairs, capable of performing a wide range of simple and complex edits. MagicBrush \cite{magicbrush_neurips2023} further improved IP2P's performance by manually curating $10,000$ image caption pairs for fine-tuning. Other approaches such as GLIDE \cite{glide_icml2022} and SmartBrush \cite{smartbrush_cvpr2023} trained Stable Diffusion \cite{stablediffusion_cvpr2022} for mask and text-guided image editing. Blended Latent Diffusion \cite{blended_latent_diffusion_siggraph2023} attempted zero-shot image editing using a mask and an associated text instruction, but it is unreliable and requires test-time optimization similar to \cite{blended_diffusion_cvpr2022} for more reliable edits.