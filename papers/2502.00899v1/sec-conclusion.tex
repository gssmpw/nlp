% !TEX root = ../main.tex

We present \ourframework, a unified framework for one-shot sparse plus low-rank matrix decomposition for foundation models. \ourframework employs an Alternating Minimization approach to minimize the local layer-wise reconstruction objective without any approximations at the objective level. It scales to models with billions of parameters and it is made efficient by exploiting the problem structure (e.g. Hessian-invariance throughout iterations and diagonal rescaling of a minimization approach). Our experiments show that \ourframework outperforms existing methods for sparse plus low-rank decomposition of LLMs on a wide-range of LLM evaluation benchmarks, especially perplexity. 
Future work can extend \Pone (subproblem pertaining to sparsity) to include quantization and quantized-sparse compression. This would give a better understanding of optimization-based approaches in decomposing dense pre-trained weights into a compressed version (e.g. quantized) plus a low-rank component.