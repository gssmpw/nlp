% !TEX root = ../main.tex

Before discussing our method, let us briefly introduce some important notations that we will use throughout the paper.

\textbf{Notation.\,\,\,\,} For a general matrix $\bfZ \in \R^{m \times n}$, 
$\rk{\bfZ}$ denotes the rank of $\bfZ$.
For a given rank $r \in \N$, $C_r(\mathbf{Z}) = \bfU_r \mathbf{\Sigma}_r \bfV_r^T$, corresponding to the matrices formed by retaining only the top-$r$ singular vectors and singular values from the full SVD of $\mathbf{Z}$. The \citet{eckart1936approximation} theorem shows that $C_r(\bfZ) = \argmin_\bfM \norm{\bfZ - \bfM},\,\, \rk{\bfM} \leq r.$

For a square matrix $\bfZ \in \R^{n \times n}$,
$\Tr{\bfZ} = \sum_{i \in [n]}\bfZ_{ii}$ denotes the trace of $\bfZ$,
$\diag{\bfZ}$ denotes the diagonal matrix $\bfD \in \R^{n \times n}$ such that $\bfD_{ii} = \bfZ_{ii}$ for any $i \in [n]$, and $\bfD_{ij} = 0$ for any $i \neq j, \, i,j \in [n]$.
We also note $\mathbf{1}_n$ and $\mathbf{0}_n$ the vector of entries all ones and all zeros respectively of size $n \in \N$.

\textbf{Layer-wise Reconstruction Error.\,\,\,\,} 
A common approach in post-training LLM compression is to decompose the full-model compression problem into layer-wise subproblems. The quality of the solution for each subproblem is assessed by measuring the $\ell_2$ error between the output of the dense layer and that of the compressed one, given a set of input activations.\\
More formally, let $\bfWold \in \R^{\Nin \times \Nout}$ denote the (dense) weight matrix of layer $\ell$, where $\Nin$ and $\Nout$ denote the input and output dimension of the layer, respectively. Given a set of $N$ calibration samples, the input activation matrix can be represented as $\bfX \in \R^{NL \times \Nin}$, where $L$ is the sequence length of an LLM. It corresponds to the output of the previous layer $\ell - 1$.  The goal of the matrix decomposition algorithm is to find a sum of a sparse weight matrix $\bfWS$ and a low-rank weight matrix $\bfM$ that minimizes the reconstruction error between the original and new layer outputs, while satisfying a target sparsity constraint and a low-rank constraint. The optimization problem is given by
\begin{equation}\label{eq:matrix-decomposition}
       \min\nolimits_{\bfWS, \bfM} \,\, \left\|\bfX \bfWold-\bfX \pr{\bfWS + \bfM}\right\|_F^2 ~~~~\text{s.t. } ~~~ \bfWS\in \calCS, ~~~\rk{\bfM} \leq r.
\end{equation}
where $\bfWS, \bfM \in \R^{\Nin \times \Nout}$, $\calCS$ denotes the the sparsity-pattern constraint set.