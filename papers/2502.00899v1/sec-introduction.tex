% !TEX root = ../main.tex

Large Language Models (LLMs) have shown remarkable capabilities on numerous tasks in Natural Language Processing (NLP), 
ranging from language understanding to generation \cite{bubeck2023sparks, achiam2023gpt,team2023gemini, dubey2024llama}. The huge success of LLMs comes with important challenges to deploy them due to their massive size and computational costs. For instance,  Llama-3-405B \cite{dubey2024llama} requires 780GB of storage in half precision (FP16) and hence multiple high-end GPUs are needed just for inference. \textit{Model compression} has emerged as an important line of research to reduce the costs associated with deploying these foundation models. In particular, neural network pruning \cite{obd, hassibi1992second, benbaki2023fast}, where model weights are made to be sparse after training, has garnered significant attention. Different sparsity structures (Structured, Semi-Structured and Unstructured) obtained after neural network pruning result in different acceleration schemes. \textit{Structured pruning} removes entire structures such as channels, filters, or attention heads \cite{lebedev2016fast,wen2016learning,voita2019analyzing,el2022data} and readily results in acceleration as model weights dimensions are reduced. \textit{Semi-Structured pruning}, also known as, N:M sparsity \cite{zhou2021learning} requires that at most $N$ out of $M$ consecutive elements are non-zero elements. Modern NVIDIA GPUs provide support for 2:4 sparsity acceleration. \textit{Unstructured pruning} removes individual weights \cite{han2015learning, guo2016dynamic} from the model's weights and requires specialized hardware for acceleration. For instance, DeepSparse \cite{kurtic2022optimal, pmlr-v119-kurtz20a, DBLP:journals/corr/abs-2111-13445} provide CPU inference acceleration for unstructured sparsity.\\
Specializing to LLMs, one-shot pruning~\cite{meng2024alps, frantar2023sparsegpt, sun2023simple, zhang2023dynamic}, where one does a single forward pass on a small amount of calibration data, and prunes the model without expensive fine-tuning/retraining, is of particular interest. This setup requires less hardware requirements. For instance, \citet{meng2024alps} show how to prune an OPT-30B \cite{opt} using a single consumer-level V100 GPU with 32GB of CUDA memory, whereas full fine-tuning of such model using Adam \cite{kingma2014adam} at half-precision requires more than 220GB of CUDA memory.

Although one-shot pruning has desirable computational properties, it can degrade models' predictive and generative performance. To this end, recent work has studied extensions of model pruning to achieve smaller utility drop of model performance from compression. 
% Multiple one-shot methods have been developed in quantization \cite{frantar2022gptq, frantar2023sparsegpt, lin2024awq, behdin2023quantease, dettmers2023spqr} and neural network pruning \cite{frantar2023sparsegpt, meng2024alps, zhang2024oats}, which is closer to this paper's line of research. These one-shot methods do not require retraining--which is extremely expensive for models of the size of Llama-3-405B-- and work as resource-saving techniques that retain the model's performance. 

An interesting compression mechanism in the field of \textit{model compression} is the Sparse plus Low-Rank Matrix-Decomposition problem which aims to approximate model's weights by a sparse component plus a low-rank component~\cite{hintermuller2015robust, candes2011robust, lin2011linearized, 5394889, zhou2011godec, JMLR:v24:21-1130, NIPS2014_443cb001, yu2017compressing, li2023losparse}. Specializing to LLMs,~\citet{zhang2024oats} propose OATS 
%that addresses this type of %compression and 
that outperforms pruning methods for the same compression ratio (number of non-zero elements) on a wide range of LLM evaluation benchmarks (e.g. perplexity in Language generation). 

OATS \cite{zhang2024oats} is however a matrix decomposition algorithm inspired from a pruning algorithm Wanda \cite{sun2023simple}. Wanda has been designed as a relaxation/approximation of another state-of-the-art pruning algorithm SparseGPT \cite{frantar2023sparsegpt}. While Wanda has been found to be extremely useful and efficient in practice, recent work \cite{meng2024alps} show results where Wanda fails for high-sparsity regimes. In this paper, we provide a unified optimization framework to decompose pre-trained model weights into sparse plus low-rank components based on a layer-wise loss function. Our framework is modular and can incorporate different pruning and matrix-decomposition algorithms (developed independently in different contexts).
%under the umbrella of the local %layer-wise reconstruction error; 
Similar to~\cite{meng2024alps} we observe that our optimization-based framework results in models with better model utility-compression tradeoffs. The difference is particularly pronounced for higher compression regimes. 
%especially for higher compression %budgets, where SOTA methods 
% Our numerical results also show similar findings to \citet{meng2024alps} where high-sparsity significantly degrades the performance of approximation-based optimization methods like OATS.

Concurrently, in a different and complementary line of work,~\citet{mozaffari2024slope} have open-sourced highly-specialized CUDA kernels designed for N:M sparse \cite{zhou2021learning} plus low-rank matrix decompositions that result in significant acceleration and memory reduction for the pre-training of LLMs.
We note that our focus here is on improved algorithms for one-shot sparse plus low-rank matrix decompositions for foundation models with billions of parameters which is different from the work of \citet{mozaffari2024slope} that focuses on accelerating the pre-training of LLMs. The designed CUDA kernels \cite{mozaffari2024slope} can be exploited in our setting for faster acceleration and reduced memory footprint during inference.





% \textbf{Summary of approach and contributions:} We propose \ourmethod: an accurate and scalable framework for Sparse plus Low-Rank Matrix Decomposition for LLMs. Following the previous work on one-shot pruning and model compression, we pursue a layerwise approach. In particular, the reconstruction error resulting from compression in the output of each layer is minimized, under the compression constraints (i.e., sparsity and low-rank constraints).

\textbf{Summary of approach.\,\,\,\,} Our framework is coined \ourframework: \underline{H}ardware-\underline{A}ware (Semi-\underline{S}tructured) \underline{S}parse plus \underline{L}ow-rank \underline{E}fficient \& approximation-\underline{free} matrix decomposition for foundation models.

Hardware-aware refers to the fact that we mostly focus on a N:M sparse \cite{zhou2021learning} plus low-rank decomposition, for which acceleration on GPUs is possible, although \ourframework supports any type of sparsity pattern (unstructured, semi-structured, structured) in the sparsity constraint. Approximation-free refers to the fact that we directly minimize the local layer-wise reconstruction error introduced in \cref{eq:matrix-decomposition}, whereas we show prior work minimizes an approximation of this objective.

%Our unified framework introduces a well-posed 
%%As a part of our proposed framework, we consider an 
%%optimization form
We formulate the compression/decomposition task as a clear optimization problem; we minimize a local layer-wise reconstruction objective where the weights are given by the sum of a sparse and low-rank component. 
%%%of dense model weights under the  
%This optimization problem is decoupled into a sparse minimization subproblem and a low-rank minimization subproblem. 
We propose an efficient Alternating-Minimization approach that scales to models with billions of parameters relying on 
two key components: one involving sparse minimization (weight sparsity) and the other involving a low-rank optimization. 
For each of these subproblems 
we discuss how approximations to the optimization task can retrieve prior algorithms.
%the introduced subproblems, 
%we consider approximations to the minimization objective and retrieve different algorithms from related works given different %approximations.

% We provide an efficient and scalable algorithm based on Alternating-Minimization that does not rely on any approximation at the objective minimization level. 
% While \ourframework supports any sparsity pattern (unstructured, semi-structured, structured) in the sparsity constraint, we mostly focus on N:M sparsity \cite{zhou2021learning}, to make the decomposition Hardware-aware, as \citet{mozaffari2024slope} show how to get acceleration on modern GPUs for N:M sparse plus low-rank decomposition.

We note that \ourframework~differs from prior one-shot (sparse) pruning methods~\cite{frantar2023sparsegpt, meng2024alps, benbaki2023fast} as we seek a sparse plus low-rank decompositon of weights.
%%%%%introducing the low-rank component. 
Additionally, it differs from prior one-shot sparse plus low-rank matrix decomposition methods~\cite{zhang2024oats}
%by considering an approximation-free minimization approach of the 
as we directly minimize the local layer-wise reconstruction objective introduced in \cref{eq:matrix-decomposition}.

Our main \textbf{contributions} can be summarized as follows.
\begin{compactitem}
    \item We introduce \ourframework a unified one-shot LLM compression framework that scales to models with billions of parameters where we directly minimize the local layer-wise reconstruction error subject to  a sparse plus low-rank matrix decomposition of the pre-trained dense weights. 
    %    formulates a sparse plus low-rank matrix decomposition as an optimization problem with a local layer-wise reconstruction objective. We discuss approximations of this objective and show that OATS a popular method is recovered in a particular approximation.

    
    \item \ourframework uses an Alternating-Minimization approach that iteratively minimizes a Sparse and a Low-Rank component. \ourframework uses a given pruning method as a plug-in for the subproblem pertaining to the sparse component. Additionally, it uses Gradient-Descent type methods for the subproblem pertaining to the Low-Rank component.
    
    % \item In the subproblem pertaining to the sparse component, a rewrite of the optimization formulation shows that one can use any pruning algorithm, that minimizes the layer-wise reconstruction error, as a plug-in to sparsify the weights. We choose to show results for the algorithm SparseGPT.
    
    % In this pruning subproblem, we also enhance the performance of \ourmethod by exploiting the invariance of the Hessian--of the layer-wise reconstruction error--in each subproblem of the Alternating Minimization procedure, for a given layer. In particular, we use a pre-processing step that computes and stores the Hessian inverse--of the objective--, which is then passed to the deployed pruning algorithm (e.g. SparseGPT). 
    % \item In the subproblem  pertaining to the Low-Rank component, we give a theoretical closed form solution to the subproblem.
    % which does not scale to problems with billions of parameters. 
    % We also present a more tractable first-order optimization method for a reparametrization of the the low-rank problem, which is scalable to models with billions of parameters.
    
    % as $\bfUVt$ and use first-order optimization methods to minimize the layer-wise reconstruction objective.

    \item We discuss how special cases of our framework relying on specific approximations of the objective retrieve popular methods such as OATS, Wanda and MP --- \cite{zhang2024oats, sun2023simple,han2015learning, sze2020efficient}. This provides valuable insights into the underlying connections across different methods. 

    \item \ourframework improves upon state-of-the-art methods for one-shot sparse plus low-rank matrix decomposition. 
    For the Llama3-8B model with a 2:4 sparsity component plus a 64-rank component decomposition, \ourframework reduces the test perplexity by $12\%$ for the WikiText-2 dataset and reduces the gap (compared to the dense model) of the average of eight popular zero-shot tasks by $15\%$ compared to existing methods.
\end{compactitem}



