% !TEX root = ../main.tex

\textbf{Network pruning.\,\,\,\,} Network pruning is a well-established technique for reducing the complexity of deep neural networks by removing redundant weights \cite{obd,han2015learning}. Pruning methods can be classified based on the structure of the resulting sparse network. In terms of structure, pruning can be categorized into unstructured pruning, semi-structured pruning, and structured pruning. Unstructured pruning offers better flexibility and higher sparsity levels but requires specialized hardware for acceleration, while structured pruning is more hardware-friendly but may suffer from larger performance degradation. 
Semi-structured sparsity combines benefits of unstructured sparsity in terms of retaining the model's performance thanks to its flexibility and the benefits of structured sparsity in terms of efficiency. For instance, NVIDIA has recently introduced sparse tensor cores \cite{mozaffari2024slope} to their hardware that accelerate Gemm with N:M sparsity on modern NVIDIA GPUs. In this paper, we mostly consider N:M sparsity \cite{zhou2021learning} for the sparsity constraint, although \ourframework supports other sparsity structures.

\textbf{Sparse plus Low-Rank Matrix Decomposition.\,\,\,\,} Decomposing a weight matrix into a low-rank matrix plus a sparse matrix (also known as robust PCA) is a well-studied problem from both theoretical and algorithmic perspectives \cite{hintermuller2015robust, candes2011robust, lin2011linearized, 5394889, zhou2011godec}. These approaches have been explored by \citet{JMLR:v24:21-1130, NIPS2014_443cb001, yu2017compressing} in the context of deep learning. They have been extended to LLMs by \citet{li2023losparse} in the context of improving the utility of fine-tuning LLMs and by \citet{zhang2024oats} for sparse plus low-rank model compression.

\textbf{One-shot matrix decompositions in LLMs.\,\,\,\,} Matrix decomposition in the context of LLMs has gathered a lot of attention recently. \citet{li2023loftq, guo2023lq} decompose models' weights into a quantized weight plus a low-rank component. \citet{li2023loftq} solve this problem using an alternating-minimization approach in a data-free fashion (without using a calibration dataset). \citet{guo2023lq} consider both a data-free and a data-aware decomposition for the quantized plus low-rank decomposition problem. Their data-aware decomposition relies on an approximation of the Fisher importance matrix. \citet{zhang2024oats} develop OATS and consider a sparse plus low-rank decomposition of model's weights, and they take inspiration from the pruning algorithm Wanda \cite{sun2023simple} to incorporate outlier information (from a calibration dataset) in their decomposition. 
This paper builds on top of OATS \cite{zhang2024oats} to design an algorithm that incorporates more information from the calibration dataset in the decomposition.