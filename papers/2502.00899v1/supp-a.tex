% !TEX root = ../main.tex

% \begin{proof}
%   Given $\bfX \in \R^{m \times p}, \bfW \in \R^{p \times n}$, and the compact singular value decomposition of $\bfX$ as $\bfX = \bfU_{m \times r} \bfSigma_{r \times r} \bfV_{p \times r}^\top$. If $r = p < m$, then
%   \begin{equation*}
%     \pr{\bfXTX}^{-1}\bfX^\top C_r\pr{\bfX \bfW} = \argmin\nolimits_{\bfM \in \R^{p \times n}} \,\, \left\|\bfX \bfW -\bfX \bfM\right\|_F^2 ~~~~\text{s.t. } ~~~\rk{\bfM} \leq r.
%   \end{equation*}
%   If we introduce $\bfY = \bfX \bfW$,  \citet{mazumder2020computing} have shown that under the assumption that $r = p < m$, the solution to the above problem is \textbf{unique} and given by
%   \begin{align*}
%     \bfM_r(\bfY) 
%     &= \pr{\bfXTX}^{-1}\bfX^\top C_r\pr{\bfU\bfU^\top\bfY}\\
%     &= \pr{\bfXTX}^{-1}\bfX^\top C_r\pr{\bfU\bfU^\top\bfX \bfW}\\
%     &= \pr{\bfXTX}^{-1}\bfX^\top C_r\pr{\bfU\bfU^\top\bfU \bfSigma \bfV^\top \bfW}\\
%     &= \pr{\bfXTX}^{-1}\bfX^\top C_r\pr{\bfU \bfSigma \bfV^\top \bfW} \tag{$\bfU$ is orthogonal}\\
%     &= \pr{\bfXTX}^{-1}\bfX^\top C_r\pr{\bfX \bfW}.\\
%   \end{align*}
% \end{proof}


\subsection{Experimental Setup}
Following the framework proposed by \citet{frantar2023sparsegpt} for one-shot pruning, we minimize \cref{eq:matrix-decomposition} sequentially, layer by layer. For a given layer $\ell$, the input activation matrix $\bfX$ introduced in \cref{sec:optimization-formulation} is the output of the previous $\ell - 1$ compressed layers (sparse plus low-rank) using $N$ calibration samples. 

\textbf{Implementation details.}
\begin{itemize}
    \item For the construction of the Hessian matrix $\bfH = \bfXTX$ introduced in \cref{sec:algorithm-design}, we use the same setup of SparseGPT \cite{frantar2023sparsegpt} and we use the author's implementation of SparseGPT---as a pruning plug-in method to minimize \Pone (codes available on GitHub).
    \item We utilize the author's implementation of OATS \cite{zhang2024oats} with the default hyperparameter settings to show LLM evaluation benchmarks and layer-wise reconstruction error in \cref{fig:reconstruction-error}.
    \item The LLM evaluation benchmarks reported in \cref{tab:slr-fixed-compression} are retrieved from the paper ALPS by \citet{meng2024alps} which uses the same evaluation strategy (and code) we do for the reported tasks [other zero-shot tasks are not reported in ALPS]. We report all zero-shot tasks results for OATS and \ourframework in \cref{tab:supp-slr-fixed-compression}.
\end{itemize}



\subsection{Hyperparameter Choice}
The hyperparameters used in \ourframework are the following: $\lambda = 0.01 \Tr{\bfH}$; default value in SparseGPT. $T_\text{AM}$ is set to be 80; default value in OATS. $T_\text{LR} = 50$; we propose this default value for all experiments. $\eta = 1e^{-2}$; we propose this default value for all experiments (only works well with the scaling introduced in \cref{scaling-low-rank}). $r$ is either set to $64$ and fixed for all layers, or is flexible and given by the formula $r = \left\lfloor {(1 - \rho - \frac{N}{M}) \cdot(\Nout \cdot \Nin)} / \pr{\Nout + \Nin} \right\rfloor$ introduced in \cref{sec:experimental-results}. \texttt{Prune}; we propose by default to use SparseGPT. \texttt{Optimizer}; we propose the Adam optimizer. \textbf{is\_scaled}; we propose to set this to True by default. It converges faster in practice and allows to skip the tuning of the learning rate $\eta$.

\subsection{Additional Experimental Resuls}
\textbf{N:M Sparsity + Fixed Compression Ratio:}
This is the same setting described in \cref{sec:experimental-results}. We extend the results reported in \cref{tab:slr-fixed-compression} to include the 8 zero-shot tasks and the Llama3.2 model. Results are reported in \cref{tab:supp-slr-fixed-compression}.
\begin{table}[h!]
\centering
\resizebox{1.0\textwidth}{!}{%
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cccccccccccccc}
\toprule
\multirow{2.25}{*}{\textbf{Model}} & \multirow{2.25}{*}{\textbf{Algorithm}} & \multicolumn{3}{c}{\textbf{Perplexity ($\downarrow$)}} & \multicolumn{9}{c}{\textbf{Zero-shot ($\uparrow$)}} \\ 
\cmidrule(rl){3-5} \cmidrule(rl){6-14}
& & \textbf{C4} & \textbf{WT2} & \textbf{PTB} & \textbf{PIQA} & \textbf{HS} & \textbf{ARC-E} & \textbf{ARC-C} & \textbf{WG} & \textbf{RTE} & \textbf{OQA} & \textbf{BoolQ} & \textbf{Avg}\\ 
\midrule
\multirow{5.5}{*}{Llama3-8B} 
&\texttt{OATS-2:8+LR}       & 21.03 & \textbf{14.54} & 24.15 & 73.67 & \textbf{62.42} & 59.68 & \textbf{37.12} & 65.43 & 55.23 & \textbf{36.40} & 73.98 & 57.99 \\
&\texttt{Ours-2:8+LR}       & \textbf{20.05} & 15.03 & \textbf{22.01} & \textbf{74.05} & 60.69 & \textbf{60.52} & 36.18 & \textbf{66.77} & \textbf{57.04} & 35.00 & \textbf{76.02} & \textbf{58.28} \\
\cmidrule(rl){2-2}
&\texttt{OATS-3:8+LR}       & 16.87 & 11.43 & 18.53 & 75.24 & 66.90 & 65.91 & 39.85 & 68.90 & 61.37 & 39.00 & 76.61 & 61.72 \\
&\texttt{Ours-3:8+LR}       & \textbf{16.16} & \textbf{11.36} & \textbf{16.71} & \textbf{75.79} & \textbf{67.33} & \textbf{67.55} & \textbf{41.04} & \textbf{69.53} & \textbf{58.48} & \textbf{39.20} & \textbf{79.91} & \textbf{62.35} \\
\cmidrule(rl){2-2}
&\texttt{dense}               & 9.44 & 6.14 & 11.18 & 80.79 & 79.17 & 77.69 & 53.33 & 72.85 & 69.68 & 45.00 & 81.44 & 69.99 \\

\midrule
\multirow{5.5}{*}{Llama3.2-1B} 
&\texttt{OATS-2:8+LR}       & 78.18 & 53.05 & 80.17 & 59.03 & 36.42 & 37.08 & 22.87 & 52.80 & 52.71 & 27.40 & 61.77 & 43.76 \\
&\texttt{Ours-2:8+LR}       & \textbf{41.08} & \textbf{30.92} & \textbf{48.85} & \textbf{63.22} & \textbf{39.07} & \textbf{42.55} & \textbf{25.77} & \textbf{55.17} & \textbf{53.07} & \textbf{28.00} & \textbf{62.11} & \textbf{46.12} \\
\cmidrule(rl){2-2}
&\texttt{OATS-3:8+LR}       & 42.81 & 29.35 & 47.58 & 63.49 & 42.25 & 43.43 & 25.09 & 54.85 & 52.35 & \textbf{29.60} & 62.05 & 46.64 \\
&\texttt{Ours-3:8+LR}       & \textbf{31.35} & \textbf{22.89} & \textbf{34.99} & \textbf{66.43} & \textbf{45.00} & \textbf{46.42} & \textbf{25.85} & \textbf{56.43} & \textbf{52.71} & 28.80 & \textbf{62.26} & \textbf{47.99} \\
\cmidrule(rl){2-2}
&\texttt{dense}               & 14.01 & 9.75 & 17.59 & 74.59 & 63.66 & 60.48 & 36.26 & 60.69 & 56.68 & 37.20 & 63.98 & 56.69 \\

\midrule
\multirow{5.5}{*}{Llama3.2-3B} 
&\texttt{OATS-2:8+LR}       & 30.73 & 22.65 & 36.31 & 68.55 & 51.76 & 54.46 & \textbf{31.14} & 61.17 & \textbf{58.48} & \textbf{30.80} & \textbf{70.43} & \textbf{53.35} \\
&\texttt{Ours-2:8+LR}       & \textbf{25.22} & \textbf{19.61} & \textbf{29.54} & \textbf{69.59} & \textbf{52.94} & \textbf{55.30} & 29.69 & \textbf{62.67} & 55.23 & 30.60 & 69.24 & 53.16\\
\cmidrule(rl){2-2}
&\texttt{OATS-3:8+LR}       & 21.96 & 15.84 & 26.22 & \textbf{72.69} & 58.61 & \textbf{58.92} & \textbf{34.13} & 63.14 & \textbf{58.12} & 33.60 & 67.22 & 55.80 \\
&\texttt{Ours-3:8+LR}       & \textbf{20.03} & \textbf{14.85} & \textbf{22.92} & 72.42 & \textbf{58.92} & 56.69 & 33.53 & \textbf{64.01} & 56.32 & \textbf{37.00} & \textbf{70.31} & \textbf{56.15} \\
\cmidrule(rl){2-2}
&\texttt{dense}               & 11.33 & 7.81 & 13.53 & 77.48 & 73.61 & 71.63 & 45.99 & 69.85 & 54.51 & 43.00 & 73.39 & 63.68 \\


\bottomrule
\end{tabular}
}
\vspace{3pt}
\caption{Performance analysis for one-shot N:M sparse plus a low-rank matrix decomposition of Llama3 and Llama3.2 models. The compression ratio is fixed to be $\rho=0.5$. For Perplexity, $(\downarrow)$ lower values are preferred. For zero-shot tasks, $(\uparrow)$ higher values are preferred.}
\label{tab:supp-slr-fixed-compression}
\end{table}

\textbf{Unstructured Sparsity + Fixed Rank Ratio:} This is the setting introduced in OATS \cite{zhang2024oats}. This scheme takes as inputs a compression ratio $\rho$ (e.g. $50\%$) and rank ratio $\kappa$ (e.g. $0.3$; default value in OATS for the Llama3-8B model). The rank of the low-rank component $r$ and the number of non-zeros $k$ in the unstructured sparsity are given by.
\begin{equation*}
    r = \left\lfloor \kappa \cdot (1 - \rho) \cdot \frac{\Nout \cdot \Nin}{\Nout + \Nin} \right\rfloor, \quad \quad k = \left\lfloor (1 - \kappa) \cdot (1 - \rho) \cdot \Nout \cdot \Nin \right\rfloor.
\end{equation*}
See OATS for a discussion on how to choose the rank ratio $\kappa$ for a given model. Note that OATS introduces OWL ratios--different sparsity budgets for different layers to reduce the utility drop. The results for this setting do not apply OWL and consider uniform unstructured sparsity throughout layers. Results for OATS and \ourframework are reported in \cref{tab:supp-slr-unstructured}.




\begin{table}[h!]
\centering
\resizebox{1.0\textwidth}{!}{%
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{ccc@{\hskip 8pt}cccc@{\hskip 8pt}ccccccccc}
\toprule
\multirow{2.25}{*}{\textbf{Model}} & \multirow{2.25}{*}{\textbf{Algorithm}} && \multicolumn{3}{c}{\textbf{Perplexity ($\downarrow$)}} && \multicolumn{9}{c}{\textbf{Zero-shot ($\uparrow$)}} \\ 
\cmidrule(rl){4-6} \cmidrule(r){8-16}
&&& \textbf{C4} & \textbf{WT2} & \textbf{PTB} && \textbf{PIQA} & \textbf{HS} & \textbf{ARC-E} & \textbf{ARC-C} & \textbf{WG} & \textbf{RTE} & \textbf{OQA} & \textbf{BoolQ} & \textbf{Avg}\\ 

\midrule
\multirow{7.5}{*}{Llama3-8B}
&\texttt{OATS-60\%+LR}       && 23.61 & 16.52 & 25.85 && 72.91 & 59.65 & \textbf{60.10} & 33.36 & 65.35 & 53.07 & 31.60 & \textbf{75.96} & 56.50 \\
&\texttt{Ours-60\%+LR}       &&  \textbf{20.70} & \textbf{15.66} & \textbf{23.31} && \textbf{73.29} & \textbf{60.58} & 59.26 & \textbf{34.64} & \textbf{67.88} & \textbf{53.43} & \textbf{35.40} & 75.08 & \textbf{57.44} \\
\cmidrule(rl){2-2}
&\texttt{OATS-70\%+LR}       &&  106.98 & 81.77 & 110.44 && 55.60 & 30.30 & 32.45 & 20.05 & 49.96 & \textbf{52.71} & 27.00 & 62.35 & 41.30 \\
&\texttt{Ours-70\%+LR}       && \textbf{50.07} & \textbf{49.13} & \textbf{60.89} && \textbf{60.50} & \textbf{39.67} & \textbf{37.21} & \textbf{23.38} & \textbf{55.25} & \textbf{52.71} & \textbf{27.40} & \textbf{66.09} & \textbf{45.27} \\

\cmidrule(rl){2-2}
&\texttt{OATS-80\%+LR}       && 748.40 & 909.75 & 1601.02 && 52.29 & 27.25 & 26.81 & \textbf{24.40} & 47.59 & \textbf{52.71} & \textbf{26.60} & 37.83 & 36.93 \\
&\texttt{Ours-80\%+LR}       && \textbf{164.27} & \textbf{265.28} & \textbf{235.38} && \textbf{53.32} & \textbf{28.53} & \textbf{29.38} & 20.22 & \textbf{49.49} & \textbf{52.71} & \textbf{26.60} & \textbf{38.84} & \textbf{37.39} \\
\cmidrule(rl){2-2}
&\texttt{dense}               && 11.33 & 7.81 & 13.53 && 77.48 & 73.61 & 71.63 & 45.99 & 69.85 & 54.51 & 43.00 & 73.39 & 63.68 \\


\midrule
\multirow{7.5}{*}{Llama3.2-3B} 
&\texttt{OATS-60\%+LR}       && 34.57 & 24.94 & 41.51 && 67.79 & 48.40 & \textbf{52.57} & \textbf{30.38} & 57.70 & 54.15 & \textbf{30.80} & 65.66 & 50.93 \\
&\texttt{Ours-60\%+LR}       && \textbf{27.67} & \textbf{21.90} & \textbf{33.40} && \textbf{69.15} & \textbf{52.04} & 51.26 & 29.52 & \textbf{61.96} & \textbf{58.12} & 29.80 & \textbf{69.72} & \textbf{52.70} \\
\cmidrule(rl){2-2}
&\texttt{OATS-70\%+LR}       &&  155.48 & 121.76 & 167.60 && 54.57 & 29.83 & 30.43 & 21.42 & \textbf{49.64} & \textbf{52.71} & \textbf{28.20} & 60.43 & 40.90 \\
&\texttt{Ours-70\%+LR}       && \textbf{78.65} & \textbf{75.23} & \textbf{103.10} && \textbf{58.43} & \textbf{32.44} & \textbf{35.27} & \textbf{21.67} & 49.41 & \textbf{52.71} & 27.00 & \textbf{62.29} & \textbf{42.40} \\
\cmidrule(rl){2-2}
&\texttt{OATS-80\%+LR}       && 1085.27 & 1610.87 & 2546.29 && 50.60 & 26.60 & 26.68 & \textbf{24.40} & 47.67 & \textbf{52.71} & \textbf{26.60} & 37.83 & 36.64 \\
&\texttt{Ours-80\%+LR}       && \textbf{217.62} & \textbf{320.98} & \textbf{320.02} && \textbf{53.10} & \textbf{27.86} & \textbf{29.12} & 22.01 & \textbf{47.75} & 50.54 & \textbf{26.60} & \textbf{46.61} & \textbf{37.95} \\
\cmidrule(rl){2-2}
&\texttt{dense}               && 11.33 & 7.81 & 13.53 && 77.48 & 73.61 & 71.63 & 45.99 & 69.85 & 54.51 & 43.00 & 73.39 & 63.68 \\

\bottomrule
\end{tabular}
}
\vspace{2pt}
\caption{Performance analysis for one-shot unstructured sparsity plus a low-rank matrix decomposition of Llama3 and Llama3.2-3B model. The rank ratio of the low-rank component is fixed to be $\kappa=0.3$. For Perplexity, $(\downarrow)$ lower values are preferred. For zero-shot tasks, $(\uparrow)$ higher values are preferred.}
\label{tab:supp-slr-unstructured}
\end{table}
