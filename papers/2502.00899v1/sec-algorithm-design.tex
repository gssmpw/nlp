% !TEX root = ../main.tex

The Optimization of Problem \eqref{eq:matrix-decomposition} is challenging: we need to jointly find a support for $\bfWS$ so that it is feasible for the set $\calCS$, a subspace of dimension $r$ where the low-rank matrix $\mathbf{M}$ lies, and optimal weights, within these constrained sets, for both matrices to minimize the layerwise-reconstruction error. While such formulation relates to the Robust-PCA literature \cite{chandrasekaran2011rank, candes2011robust, hintermuller2015robust}, the size of parameters in $\bfWS$ and $\bfM$ can reach over 100 million in the LLM setting. For instance, the size of a down projection in a FFN of a Llama3-405b \cite{dubey2024llama} has more than 800 million parameters. 
Classical methods fail to be deployed at this scale, which makes designing novel algorithms that are more computationally efficient a necessity to solve the layerwise-reconstruction matrix decomposition problem \eqref{eq:matrix-decomposition}.

In this paper, we propose to optimize problem \eqref{eq:matrix-decomposition} using an Alternating-Minimization approach \cite{hintermuller2015robust, zhou2011godec}. We aim to decompose the problem into two 'friendlier' subproblems and iteratively minimize each one of them. In particular, we would like to iteratively solve, at iteration $t$, the subproblem \Pone, which pertains to the sparse component of the matrix decomposition.
\begin{align}\label{eq:pone-pruning}
       \bfWS^{(t+1)} &\in \argmin\nolimits_{\bfWS} \,\, \left\|\bfX \bfWold-\bfX \pr{\bfWS + \bfM^{(t)}}\right\|_F^2 ~~~~\text{s.t. } ~~~ \bfWS\in \calCS\\
       &= \argmin\nolimits_{\bfWS} \,\, \left\|\bfX \pruningW^{(t)} - \bfX \bfWS\right\|_F^2 ~~~~\text{s.t. } ~~~ \bfWS\in \calCS \tag{$\pruningW^{(t)} := \bfWold - \bfM^{(t)}$}.
\end{align}
The second subproblem to be solved, at iteration $t$, which pertains to the low-rank component of the matrix decomposition problem, is \Ptwo.
\begin{align}\label{eq:ptwo-low-rank}
       \bfM^{(t+1)} &\in \argmin\nolimits_{\bfM} \,\, \left\|\bfX \bfWold-\bfX \pr{\bfWS^{(t+1)} + \bfM}\right\|_F^2 ~~~~\text{s.t. } ~~~\rk{\bfM} \leq r\\
        &= \argmin\nolimits_{\bfM} \,\, \left\|\bfX \lowrankW^{(t+1)} -\bfX \bfM\right\|_F^2 ~~~~\text{s.t. } ~~~\rk{\bfM} \leq r \tag{$\lowrankW^{(t+1)} := \bfWold - \bfWS^{(t+1)}$}.
\end{align}
Before proceeding to discuss algorithms that solve different variations of \Pone and \Ptwo, and draw connections between existing methods in the literature of \textit{model compression}, we remove the dependence on the iteration $t$ and study \eqref{eq:pone-pruning} rewritten as follows.
\begin{align}\label{eq:general-pruning}
       \bfWS^\star 
       &\in \argmin\nolimits_{\bfWS} \,\, \left\|\bfX \pruningW - \bfX \bfWS\right\|_F^2 ~~~~\text{s.t. } ~~~ \bfWS\in \calCS\\
       &= \argmin\nolimits_{\bfWS} \,\, \Tr{(\pruningW - \bfWS)^\top \bfH (\pruningW - \bfWS)}~~~~\text{s.t. } ~~~ \bfWS\in \calCS \tag{$\bfH = \bfXTX$}. 
\end{align}

Similarly, we study \eqref{eq:ptwo-low-rank} rewritten as follows.
\begin{align}\label{eq:general-low-rank}
   \bfM^\star &\in \argmin\nolimits_{\bfM} \,\, \left\|\bfX \lowrankW -\bfX \bfM\right\|_F^2 ~~~~\text{s.t. } ~~~\rk{\bfM} \leq r\\
   &= \argmin\nolimits_{\bfM} \,\, \Tr{(\lowrankW - \bfM)^\top \bfH (\lowrankW - \bfM)} ~~~~\text{s.t. } ~~~\rk{\bfM} \leq r \notag{}.
\end{align}




\subsection{Minimizing Subproblem \Pone}\label{section-pone}
\vspace{-2pt}
To solve \eqref{eq:general-pruning}, one can consider multiple variations for $\bfH$, the Hessian of the local layer-wise reconstruction error.
\vspace{-3pt}
\subsubsection{Data-Free version: $\bfX = \bfI_{\Nin \times \Nin} \implies \bfH = \bfI_{\Nin \times \Nin}$} 
\vspace{-3pt}
A data-free pruning method (without a calibration dataset) considers $\bfX$ to be an identity matrix in \eqref{eq:general-pruning}. When $\bfH$ is an identity matrix, equation \eqref{eq:general-pruning} can be solved to optimality and an optimal solution is obtained with Magnitude Pruning (MP, \cite{han2015learning, sze2020efficient}) using a simple Hard-Thresholding operator on the dense weight $\pruningW$ -- keeping the largest values and setting the remaining values to zero. Note that MP can be applied to unstructured \cite{han2015learning}, semi-structured N:M sparsity \cite{zhou2021learning}, and structured pruning \cite{meng2024alps}. This accommodates most sparsity sets $\calCS$ in the pruning literature.

\subsubsection{Diagonal-approximation: $\bfH = \diagn{\bfXTX}$}\label{subsection-pruning-diagonal-approximation}
\vspace{-3pt}
An efficient way to approach problem \eqref{eq:general-pruning} is to approximate the Hessian of the local layer-wise reconstruction error by its diagonal. An optimal solution in this case, can be obtained by Hard-Thresholding $\bfD \pruningW$, where $\bfD = \sqrt{\diagn{\bfXTX}}$. Note that this approximation results in the state-of-the-art pruning algorithm Wanda \cite{sun2023simple}. In fact, the importance metric, $S_{ij}$ introduced in Wanda for each entry $\pruningW_{ij}$ reads as follows. Here $\bfX_j$ denotes the $j^{th}$ column of the input activation matrix $\bfX$.
\begin{equation}\label{eq:wanda-metric}
    S_{ij} = \abs{\pruningW_{ij}} \cdot \norm{\bfX_j}_2 = \abs{\bfD \pruningW}_{ij}. \tag{$\bfD = \sqrt{\diagn{\bfXTX}}$}
\end{equation}
\citet{sun2023simple} show impressive results with this approximation for unstructured and semi-structured sparsity. OATS \cite{zhang2024oats} is inspired by Wanda and decomposes model weights into sparse plus low-rank using Alternating-Minimization; their sparse update reduces to this approximation (diagonal of the local layer-wise objective's Hessian).
\vspace{-3pt}
\subsubsection{Full Hessian: $\bfH = \bfXTX + \lambda \bfI$}\label{full-hessian-pruning}
\vspace{-3pt}
This approach aims to directly minimize \eqref{eq:general-pruning}. \citet{frantar2023sparsegpt} are the first to use the full Hessian of the local layer-wise reconstruction objective \Pone at the scale of LLMs for pruning using approximations at the algorithmic level (as opposed to an approximation at the optimization formulation level). \citet{meng2024alps} extend this formulation using the operator splitting technique ADMM \cite{boyd2011distributed} and show impressive results for unstructured sparsity and N:M sparsity. \citet{meng2024osscar} extend the formulation for structured sparsity by leveraging combinatorial optimization techniques.

Our framework works as a plug-in method for any pruning algorithm to minimize \Pone at iteration $t$. Since we aim to minimize \cref{eq:matrix-decomposition} in an approximation-free manner, we select methods that use the entire Hessian as they tend to give better performance for high compression ratios. SparseGPT \cite{frantar2023sparsegpt} is a popular pruning method that considers the entire Hessian. In particular, for our numerical results, we use SparseGPT to minimize \Pone.
\vspace{-3pt}
\subsection{Minimizing Subproblem \Ptwo}
\vspace{-3pt}
As in the previous section \ref{section-pone}, we discuss algorithms and related work for different variations of $\bfH$.
\vspace{-3pt}
\subsubsection{Data-Free version: $\bfX = \bfI_{\Nin \times \Nin}$} 
\vspace{-3pt}
Drawing a line from the pruning literature, a data-free version is introduced that does not require a calibration dataset. In this case, a closed-form solution of the minimizer is given by the Truncated-SVD $C_r(\lowrankW)$. This corresponds to the best rank-$r$ approximation of $\lowrankW$. \citet{li2023loftq} use SVD on the full matrix during their low-rank minimization step for quantization plus low-rank matrix decomposition. \citet{guo2023lq} use a Randomized-SVD \cite{halko2011finding} approach for the same problem (quantization plus low-rank decomposition) instead of the full SVD since it is reduces runtime significantly while maintaining the minimization performance.
\vspace{-3pt}
\subsubsection{Diagonal-approximation: $\bfH = \diagn{\bfXTX}$}\label{subsection-diagonal-approximation}
\vspace{-3pt}
The diagonal approximation of $\bfH$ has been made popular in the pruning literature thanks to Wanda \cite{sun2023simple}. We analyze the minimizing \Ptwo with this approximation. Similar to \ref{subsection-pruning-diagonal-approximation}, we introduce $\bfD = \sqrt{\diagn{\bfXTX}}$ in equation \eqref{eq:general-low-rank}. Here we use the fact that $\bfD$ is symmetric.
\begin{align*}
   \bfM^\star 
   &\in \argmin\nolimits_{\bfM} \,\, \Tr{(\lowrankW - \bfM)^\top \bfD^2 (\lowrankW - \bfM)} ~~~~\text{s.t. } ~~~\rk{\bfM} \leq r\\
   &= \argmin\nolimits_{\bfM} \,\, \left\|\bfD \lowrankW -\bfD \bfM\right\|_F^2 ~~~~\text{s.t. } ~~~\rk{\bfM} \leq r.
\end{align*}

\begin{assumption}\label{ass:full-rank-diagonal}
The input activations matrix $\bfX$ satisfies $\diag{\bfXTX}$ is full-rank. Equivalently, no column of $\bfX$ is identically $\mathbf{0}_{N \cdot L}$.
\end{assumption}
\begin{theorem}\label{theorem:low-rank-closed-form-diag-approx}
    If \cref{ass:full-rank-diagonal} holds, then the closed-form minimizer of \eqref{eq:general-low-rank} is given by
    \begin{equation*}
        \bfM^\star = \bfD^{-1} C_r(\bfD \lowrankW).
    \end{equation*}
\end{theorem}
The proof of theorem \ref{theorem:low-rank-closed-form-diag-approx} is obtained by introducing the auxialiary variable $\tilde{\bfM} = \bfD \bfM$ and noting that $\rkn{\tilde{\bfM}} = \rkn{\bfM}$, when \cref{ass:full-rank-diagonal} holds.

Interestingly, OATS \cite{zhang2024oats} uses the same operation in the Low-Rank update of the Alternating-Minimization approach (for sparse plus low rank matrix decomposition). 
%OATS uses a full SVD on the matrix $\bfD \lowrankW$. 
% \zx{Do we know what exactly OATS did? It looks to me we can do $\bfM^\star = C_r(\lowrankW)$ as $\bfD$ is diagonal? Computationally, it probably does not matter much as $\bfD$ is diagonal.}
% \mm{Actually it happens that $\bfD^{-1} C_r(\bfD \lowrankW) \neq C_r(\lowrankW)$ in the general case, that's why OATS is an interesting approach.}
\begin{corollary}
OATS \cite{zhang2024oats} exactly minimizes \eqref{eq:matrix-decomposition} with a diagonal approximation of the Hessian of the local layer-wise reconstruction error, since they minimize \Pone and \Ptwo with the same diagonal approximation $\bfH = \diagn{\bfXTX}$.
\end{corollary}
\vspace{-2pt}
\subsubsection{Full Hessian: $\bfH = \bfXTX + \lambda \bfI$}
\vspace{-3pt}
The state-of-the-art pruning algorithms in terms of retaining compressed LLMs performance on multiple benchmarks are the ones that use the full Hessian \ref{full-hessian-pruning} \cite{meng2024alps,frantar2023sparsegpt}. This motivates minimizing equation \eqref{eq:general-low-rank} using the full Hessian as well.
Dealing with low-rank constraints can be challenging, we therefore propose to reparametrize the low-rank matrix $\bfM \in \R^{\Nin \times \Nout}$ by  $\bfUVt$, with $\bfU \in \R^{\Nin \times r}, \bfV \in \R^{\Nout \times r}$. We can therefore use more computationally efficient first-order optimization methods to minimize the layer-wise reconstruction objective, which can be rewritten as follows.
\begin{equation}\label{eq:uvt-general-low-rank}
    \bfM^\star = \bfU^\star \bfV^{\star^\top}, \quad \bfU^\star, \bfV^\star \in \argmin\nolimits_{\bfU, \bfV} \,\, \Tr{\pr{\lowrankW - \bfUVt}^\top \bfH \pr{\lowrankW - \bfUVt}}.
\end{equation}
\textbf{Diagonal Scaling for \Ptwo Minimization Stability.\,\,\,\,}\label{scaling-low-rank}
Our initial experiments to minimize equation \eqref{eq:uvt-general-low-rank}, using Gradient-Descent type methods on $\bfU$ and $\bfV$, have shown that the optimization problem can be ill-conditioned in some tranformer layers. This can lead to numerical instability in the optimization procedure. To address this, we follow a similar rescaling approach proposed by \citet{meng2024alps}. Define (similar to \ref{subsection-diagonal-approximation}) the matrix $\bfD = \sqrt{\diagn{\bfXTX}}$ and reformulate the optimization equation \eqref{eq:uvt-general-low-rank} as follows (when \cref{ass:full-rank-diagonal} holds).
\begin{equation}\label{eq:scaled-uvt-general-low-rank}
    \bfM^\star = \bfD\bfU^\star \bfV^{\star^\top}, \quad \bfU^\star, \bfV^\star \in \argmin\nolimits_{\bfU, \bfV} \,\, \Tr{\pr{\bfD\lowrankW - \bfUVt}^\top \bfD^{-1}\bfH\bfD^{-1} \pr{\bfD\lowrankW - \bfUVt}}.
\end{equation}
It is important to note that the minimization problems in equations \eqref{eq:scaled-uvt-general-low-rank} and \eqref{eq:uvt-general-low-rank} are equivalent, in terms of objective minimization and feasibility of $\bfM^\star$, which has rank at most $r$. This scaling, which sets the diagonal of the new Hessian to $\mathbf{1}_{\Nin}$, only modifies the steps of Gradient Descent and leads to faster convergence in practice. See the \Cref{fig:reconstruction-error} for an objective minimization comparison of the effect of this Diagonal scaling.
It is also worth noting that this scaling allows to use the same learning rate $\eta$ for Gradient-Descent type methods for all layers and all models.

\subsection{Our Proposed Approach}
\vspace{-5pt}
Our goal is to minimize \eqref{eq:matrix-decomposition} using Alternating-Minimization with the full Hessian approach $\bfH = \bfXTX$. Our numerical results show that leveraging the entire Hessian outperforms OATS \cite{zhang2024oats}, which minimizes \eqref{eq:matrix-decomposition} with the diagonal approximation of the Hessian approach $\bfH = \diagn{\bfXTX}$, on a wide-range of LLM benchmarks and compression ratios. 

In our numerical experiments, we show results with the SparseGPT \cite{frantar2023sparsegpt} algorithm to minimize \Pone and the Adam algorithm \cite{kingma2014adam} to minimize \Ptwo reparametrized and rescaled as in \cref{eq:scaled-uvt-general-low-rank}. 

\textbf{Computational Efficiency.\,\,\,\,} Note that for a given layer $\ell$, the Hessian of the local layer-wise reconstruction problem $\bfXTX$ in \eqref{eq:general-pruning} as well as the rescaled version $\bfD^{-1}\bfXTX\bfD^{-1}$ in \eqref{eq:general-low-rank} are invariant throughout iterations. 
This is very important as pruning algorithms that use the entire Hessian information \cite{frantar2023sparsegpt, meng2024alps} need the Hessian inverse in their algorithm update. This inversion and associated costs of Hessian construction are done only once and then \textit{amortized} throughout iterations. In the \cref{algo:low-rank-gd}, we use $\bfU^{(t-1)}$ and $\bfV^{(t-1)}$ as initializations for the optimizer, as they are close to the minimizers of \Ptwo at iteration $t$. This accelerates the convergence in practice.

% We are now ready to present our proposed algorithm \ourframework in Algorithm \ref{algo:gdprune}.

\begin{algorithm}[h]
    \caption{\texttt{Low-Rank-GD}
    % \zx{Maybe inline this two line algorithm?}\mm{I like to keep it this way, to show (i) warm-up variables Uinit which makes optimization much faster and (ii) that scaling and no scaling can be solved using 'same' way.}
    }
    \label{algo:low-rank-gd}
    \begin{algorithmic}[]
        \State \textbf{Input} \texttt{Optimizer} (optimization algorithm, e.g. Adam), $\bfH$ (Hessian), $\bfW$ (Weights), $\bfU_\text{init}$, $\bfV_\text{init}$ (warm-up initialization for the joint minimization of $\bfU, \bfV$), $T_{\text{LR}}$ (\# iterations), $\eta$ (learning rate).
        \State $\text{Obj}(\bfU, \bfV) \gets \Tr{\prn{\bfW - \bfUVt}^\top \bfH \prn{\bfW - \bfUVt}}$        \vspace*{0.4em}
        \State $\bfU^\star, \bfV^\star \gets \texttt{Optimizer}_{\bfU, \bfV}\pr{\text{Obj}, \bfU_\text{init}, \bfV_\text{init}, N, \eta}$
        \vspace*{0.4em}
        \State \textbf{Output} $\bfU^\star, \bfV^\star$.
    \end{algorithmic}
\end{algorithm}
\vspace{-\baselineskip}
\begin{algorithm}[h]
    \caption{\ourframework}
    \label{algo:gdprune}
    \begin{algorithmic}[]
        \State \textbf{Input} for a given layer $\ell$: $\mathbf{H} = (\bfXTX + \lambda \mathbf{I})$ (Hessian of \eqref{eq:matrix-decomposition}, plus a regularization term for numerical stability), $\widehat{\bfW}$ (dense pre-trained weights), $T_{\text{AM}}$ (\# iterations of Alternating-Minimization), $T_{\text{LR}}$ (\# iterations of \texttt{Low-Rank-GD}), $\eta$ (learning rate for $\bfU, \bfV$), $\calCS$ (sparsity pattern), $r$ (rank of low-rank components), \texttt{Prune} (any pruning algorithm, e.g. SparseGPT), \texttt{Optimizer} (any first-order algorithm, e.g. Adam), \textbf{is\_scaled} (bool to apply scaling \ref{scaling-low-rank}).
        \vspace*{0.1em}
        \State $\bfD \gets \sqrt{\diag{\bfH}}$ \quad \Comment{Diagonal of the Hessian.}
        % \vspace*{0.1em}
        \State $\mathbf{H}^{-1} \gets \texttt{inv}\pr{\mathbf{H}}$ \quad \Comment{Inverse the Hessian.}
        % \vspace*{0.1em}
        \State $\bfWS \gets \mathbf{0}_{\Nin \times \Nout}$
        % \vspace*{0.1em}
        \State $\bfU \gets \mathbf{0}_{\Nin \times r}$
        % \vspace*{0.1em}
        \State $\bfV \gets \mathcal{N}_{\Nout \times r}$ \quad  \Comment{element-wise independent gaussian initialization.}
        % \vspace*{0.1em}
        \For{$t = 1 \dots T$}
            % \vspace*{0.1em}
            \State $\bfWS \gets \texttt{Prune}\pr{\mathbf{H}^{-1}, \widehat{\bfW} - \bfUVt, \calCS}$
            \State \Comment{$\bfWS \approx \widehat{\bfW} - \bfUVt$, satisfies $\calCS$ sparsity pattern \& minimizes \Pone.}
            \vspace*{0.2em}
            \State $\eta_t \gets \text{get\_lr}(t, \eta)$ \quad \Comment{In practice, $\eta_t = \eta / (t + 10)$.}
            \vspace*{0.2em}
            \If{\textbf{is\_scaled}}
                % \vspace*{0.1em}
                \State $\bfU, \bfV \gets \texttt{Low-Rank-GD}\pr{\texttt{Optimizer}, \bfD^{-1}\mathbf{H}\bfD^{-1}, \bfD\pr{\widehat{\bfW} - \bfWS}, \bfD\bfU, \bfV, T_{\text{LR}}, \eta_t}$
                \State $\bfU \gets \bfD \bfU$ \quad \Comment{Rescale $\bfU$ back.}
            \Else
                \State $\bfU, \bfV \gets \texttt{Low-Rank-GD}\pr{\texttt{Optimizer}, \mathbf{H}, \widehat{\bfW} - \bfWS, \bfU, \bfV, T_{\text{LR}}, \eta_t}$ 
                % \zx{Do we still need to keep this unscaled version?} \mm{I'm thinking of keeping it and include a very brief ablation study, I think it's nice to show that both work but scaled requires much less iterations in practice}
            \EndIf
            % \State $\bfU, \bfV \gets \texttt{Low-Rank-GD}\pr{\texttt{Optimizer}, \bfD^{-1}\mathbf{H}\bfD^{-1}, \bfD\pr{\widehat{\bfW} - \bfWS}, \eta_t}$
            % \vspace*{0.3em}
            % \State $\bfU \gets \bfD \bfU$ \quad \Comment{Rescale $\bfU$ back.}
            \State \Comment{$\bfUVt \approx \widehat{\bfW} - \bfWS$, has rank at most $r$ \& minimizes \Ptwo.}
        \EndFor
        % \vspace*{0.2em}
        \State $\bfM \gets \bfUVt$
        % \vspace*{0.1em}
        \State \textbf{Output} for a given layer $\ell$: $\bfWS, \bfM$.
    \end{algorithmic}
\end{algorithm}