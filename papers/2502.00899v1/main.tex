\documentclass{article}

\input{preamble}
\usepackage[preprint]{cpal_2025}
\input{macros}

\begin{document}
\input{title}
\maketitle
% \mm{Thanks everyone for the comments. I am going to rewrite most of the paper on Sunday with some new results and integrating the feedback. If you could have a look at the paper again sometime on Monday that would be great! New deadline is Monday midnight.}
% \zx{We probably want to work on a better title -- revisit after reading the whole paper.}

% \begin{abstract}
% The impressive capabilities of Large Language Models (LLMs) in Natural Language Processing (NLP) comes at a cost of substantial computing resources. One line of work to address this issue is model compression that effectively reduces the model's size, while maintaining its performance. In particular, a novel compression scheme is to decompose the model's dense weights into a sum of sparse plus low-rank matrices. Despite recent works on such matrix decompositions in LLMs, designing principled one-shot methods for this type of compression remains partially addressed. We propose a framework coined \ourmethod for (semi-structured) sparse plus low-rank matrix decomposition of LLMs, that minimizes a well-posed optimization problem, solves it using principled algorithms, and achieves state-of-the-art performance on a wide-range of LLMs evaluation benchmarks, for different compression regimes: e.g. N:M sparse + Low-Rank, a regime for which highly-specialized CUDA kernels have recently been developed for runtime speedups and memory efficiency.
% \end{abstract}

\begin{abstract}
The impressive capabilities of large foundation models come at a cost of substantial computing resources to serve them. Compressing these pre-trained models is of practical interest as it can democratize deploying them to the machine learning community at large by lowering the costs associated with inference.
A promising compression scheme is to decompose foundation models' dense weights into a sum of sparse plus low-rank matrices.
In this paper, we design a unified framework coined \ourframework for (semi-structured) sparse plus low-rank matrix decomposition of foundation models.
Our framework introduces the local layer-wise reconstruction error objective for this decomposition, we demonstrate that prior work solves a relaxation of this optimization problem; and we provide efficient and scalable methods to minimize the \textit{exact} introduced optimization problem. 
\ourframework substantially outperforms state-of-the-art methods in terms of the introduced objective and a wide range of LLM evaluation benchmarks. For the Llama3-8B model with a 2:4 sparsity component plus a 64-rank component decomposition, a compression scheme for which recent work shows important inference acceleration on GPUs, \ourframework reduces the test perplexity by $12\%$ for the WikiText-2 dataset and reduces the gap (compared to the dense model) of the average of eight popular zero-shot tasks by $15\%$ compared to existing methods.
\end{abstract}


\vspace{-5pt}
\section{Introduction}
\label{sec:introduction}
\vspace{-5pt}
\input{sec-introduction}

\vspace{-5pt}
\section{Related Work}
\label{sec:related-work}
\vspace{-5pt}
\input{sec-related-work}

\vspace{-5pt}
\section{Problem Formulation}
\label{sec:optimization-formulation}
\vspace{-5pt}
\input{sec-optimization-formulation}

\vspace{-5pt}
\section{Algorithm Design}
\label{sec:algorithm-design}
\vspace{-5pt}
\input{sec-algorithm-design}


\vspace{-5pt}
\section{Experimental Results}
\label{sec:experimental-results}
\vspace{-5pt}
\input{sec-experimental-results}

\vspace{-15pt}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-5pt}
\input{sec-conclusion}

\section*{Acknowledgements}
This research is supported in part by grants from Google and the Office of Naval Research. We acknowledge the MIT SuperCloud~\cite{reuther2018interactive} for providing HPC resources that have contributed to the research results reported within this paper. We also acknowledge Google for providing us with Google Cloud Credits for computing. 

\clearpage


% Reference
% For natbib users:
\bibliography{reference}
\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Experimental Details}
\label{supp:experiments-supp}
\input{supp-a}

\end{document}
