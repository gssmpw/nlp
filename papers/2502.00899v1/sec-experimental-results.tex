% !TEX root = ../main.tex

\subsection{Experiment Setup}
\vspace{-3pt}
\textbf{Models and datasets} We evaluate our proposed method \ourframework on two families of large language models: Llama-3 and Llama-3.2 \cite{dubey2024llama} with sizes ranging from 1 to 8 billion parameters. 
To construct the Hessian $\bfXTX$, we follow the approch of \citet{frantar2023sparsegpt}: we use 128 segments of 2048 each, randomly sampled from the first shard of the C4 training dataset \cite{JMLR:v21:20-074}. To ensure consistency, we utilize the same calibration data for all pruning algorithms we benchmark. We also consider one-shot compression results, without retraining. 
We assess the performance using perplexity and zero-shot evaluation benchmarks, with perplexity calculated according to the procedure described by HuggingFace \cite{Perplexity}, using full stride. For perplexity evaluations, we use the test sets of raw-WikiText2 \cite{merity2017pointer}, PTB \cite{Marcus1994}, and a subset of the C4 validation data, which are popular benchmarks in LLM pruning literature \cite{frantar2023sparsegpt,meng2024alps,meng2024osscar}. Additionally, we evaluate the following zero-shot tasks using LM Harness by \citet{gao10256836framework}: PIQA \cite{bisk2020piqa}, ARC-Easy (ARC-E) \& ARC-Challenge (ARC-C) \cite{clark2018think}, Hellaswag (HS) \cite{zellers2019hellaswag}, Winogrande (WG) \cite{sakaguchi2021winogrande}, RTE \cite{poliak2020survey}, OpenbookQA (OQA) \cite{banerjee2019careful} and BoolQ \cite{clark2019boolq}. The average of the eight zero-shot tasks is also reported.

\vspace{-5pt}
\subsection{Results}
\vspace{-10pt}
In order to benchmark the performance of our matrix decomposition algorithm, \ourframework uses the same number of Alternating-Minimization steps as OATS \cite{zhang2024oats} which is $80$. We report results for the scaled version of \ourframework, which uses the same learning rate $\eta = 1e^{-2}$ for all layers and considered models. We consider the following two settings.

\textbf{N:M Sparsity + Fixed Rank:}
We impose the sparsity pattern $\calCS$ to be $N:M$ sparsity and we fix the target rank $r = 64$ of the low-rank component for all layers. We benchmark our method with OATS \cite{zhang2024oats}. The results are reported in \cref{tab:slr-fixed-rank}.

\textbf{N:M Sparsity + Fixed Compression Ratio:}
This is similar to the setting described by \citet{zhang2024oats} for N:M sparsity evaluations. Each layer, with dense weight matrix $\widehat{\bfW}$, is compressed to a prefixed compression ratio $\rho$ (e.g. $50\%$) so that $\widehat{\bfW} \approx \bfW_{N:M} + \bfM$, and the target rank is given by\\[0.2em] 
$r = \left\lfloor {(1 - \rho - \frac{N}{M}) \cdot(\Nout \cdot \Nin)} / \pr{\Nout + \Nin} \right\rfloor$.\\[0.2em]
Note that the effective number of parameters stored is therefore\\[0.1em]
$\#\text{params } \bfW_{N:M} + \#\text{params } \bfU + \#\text{params } \bfV = \frac{N}{M} \cdot (\Nout \cdot \Nin) + r \Nin + r \Nout \leq (1 - \rho) \cdot \#\text{params } \widehat{\bfW}$,\\[0.5em]
hence the comparison to other pruning methods matched at the same compression ratio $\rho$. The results are reported for the Llama3-8B model in \cref{tab:slr-fixed-compression} for \ourframework, \texttt{OATS}, and different N:M pruning algorithms (SparseGPT \cite{frantar2023sparsegpt}, Wanda \cite{sun2023simple}, DSNoT \cite{zhang2023dynamic}) compressed at $\rho = 50\%$. The results are expanded for \ourframework and OATS in \cref{supp:experiments-supp}. 
% We vary the compression ratio $\rho$, we then select some N:M sparsity values that result in a more aggressive compression, we then match this compression ratio thanks to the low-rank component.

% \begin{table}[h!]
% \centering
% \resizebox{1.0\textwidth}{!}{%
% \renewcommand{\arraystretch}{1.3}
% \begin{tabular}{cccccccccccccc}
% \toprule
% \multirow{2.25}{*}{\textbf{Model}} & \multirow{2.25}{*}{\textbf{Algorithm}} & \multicolumn{3}{c}{\textbf{Perplexity ($\downarrow$)}} & \multicolumn{9}{c}{\textbf{Zero-shot ($\uparrow$)}} \\ 
% \cmidrule(rl){3-5} \cmidrule(rl){6-14}
% & & \textbf{C4} & \textbf{WT2} & \textbf{PTB} & \textbf{PIQA} & \textbf{HS} & \textbf{ARC-E} & \textbf{ARC-C} & \textbf{WG} & \textbf{RTE} & \textbf{OQA} & \textbf{BoolQ} & \textbf{Avg}\\ 
% \midrule
% \multirow{2}{*}{Llama3-8B} & \texttt{OATS-2:8+64LR}       & 368.24 & 858.90 & --.-- & 52.29 & 27.32 & \textbf{22.7} & 37.61 & -- & -- & -- & -- \\ 
%                           & \texttt{Ours-2:8+64LR}       & \textbf{90.46}  & \textbf{88.58} & --.-- & \textbf{54.52} & \textbf{31.44} & 20.73 & \textbf{40.93} & -- & -- & -- & -- \\ 
% \bottomrule
% \end{tabular}%
% }
% \caption{Evaluation results for Llama3-8B. PPL columns minimize ($\downarrow$), and accuracy columns maximize ($\uparrow$).}
% \label{tab:results}
% \end{table}

\vspace{-8pt}
\noindent
\begin{minipage}[t]{0.5\textwidth}
    \raggedleft
    \vspace{25pt}
    \captionof{table}{Performance analysis for one-shot N:M sparse plus a low-rank matrix decomposition of the Llama3-8b model. The compression ratio is fixed to be $\rho=0.5$. For Perplexity, $(\downarrow)$ lower values are preferred. For zero-shot tasks, $(\uparrow)$ higher values are preferred.\\
    Bolded values correspond to a comparison between sparse plus low-rank decomposition algorithms. Underlined values correspond to the overall best comopression scheme given a compression ratio $\rho = 50\%$.}
    \label{tab:slr-fixed-compression}
\end{minipage}%
\hspace{5pt}
\begin{minipage}[t]{0.5\textwidth}
\centering
\begin{table}[H]
\centering
\resizebox{1.0\textwidth}{!}{%
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{ccccccccc}
\toprule
\multirow{2.25}{*}{\textbf{Algorithm}} && \multicolumn{3}{c}{\textbf{Perplexity ($\downarrow$)}} && \multicolumn{3}{c}{\textbf{Zero-shot ($\uparrow$)}} \\ 
\cmidrule(rl){3-5} \cmidrule(rl){7-9}
&&\textbf{C4} & \textbf{WT2} & \textbf{PTB} && \textbf{PIQA} & \textbf{ARC-E} & \textbf{ARC-C}\\ 
\midrule
\texttt{SparseGPT-4:8}     && \underline{14.94} & 12.40 & 17.90 && 73.20 & \underline{68.54} & 34.86 \\ 
\texttt{Wanda-4:8}         && 18.88 & 14.52 & 24.26 && 71.52 & 64.91 & 34.03 \\ 
\texttt{DSNoT-4:8}         && 18.89 & 14.76 & 23.90 && 71.49 & 65.65 & 33.57 \\ 
\cmidrule(rl){1-1}
\texttt{SparseGPT-2:4}     && 18.89 & 16.35 & 25.08 && 70.54 & 63.09 & 31.84 \\ 
\texttt{Wanda-2:4}         && 30.81 & 24.36 & 44.89 && 67.56 & 56.20 & 26.11 \\ 
\texttt{DSNoT-2:4}         && 28.78 & 23.09 & 40.95 && 67.70 & 56.46 & 25.68 \\
\cmidrule(rl){1-1}
\texttt{OATS-2:8+LR}       && 21.03 & \textbf{14.54} & 24.15 && 73.67  & 59.68 & \textbf{37.12}\\
\texttt{Ours-2:8+LR}       && \textbf{20.05} & 15.03 & \textbf{22.01} && \textbf{74.05} & \textbf{60.52} & 36.18\\
\cmidrule(rl){1-1}
\texttt{OATS-3:8+LR}       && 16.87 & 11.43 & 18.53 && 75.24 & 65.91 & 39.85 \\
\texttt{Ours-3:8+LR}       && \textbf{16.16} & \underline{\textbf{11.36}} & \underline{\textbf{16.71}} && \underline{\textbf{75.79}} & \textbf{67.55} & \underline{\textbf{41.04}} \\
\cmidrule(rl){1-1}
\texttt{dense}               && 9.44 & 6.14 & 11.18 && 80.79 & 77.69 & 53.33  \\
\bottomrule
\end{tabular}
}
\end{table}
\end{minipage}



\begin{table}[h!]
\centering
\resizebox{1.0\textwidth}{!}{%
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{ccc@{\hskip 8pt}cccc@{\hskip 8pt}ccccccccc}
\toprule
\multirow{2.25}{*}{\textbf{Model}} & \multirow{2.25}{*}{\textbf{Algorithm}} && \multicolumn{3}{c}{\textbf{Perplexity ($\downarrow$)}} && \multicolumn{9}{c}{\textbf{Zero-shot ($\uparrow$)}} \\ 
\cmidrule(rl){4-6} \cmidrule(r){8-16}
&&& \textbf{C4} & \textbf{WT2} & \textbf{PTB} && \textbf{PIQA} & \textbf{HS} & \textbf{ARC-E} & \textbf{ARC-C} & \textbf{WG} & \textbf{RTE} & \textbf{OQA} & \textbf{BoolQ} & \textbf{Avg}\\ 
\midrule
\multirow{9.5}{*}{Llama3-8B} 
&\texttt{OATS-2:8+64LR}       && 368.24 & 416.14 & 565.46 && 52.29 & 28.03 & 27.53 & \textbf{22.70} & 49.17 & \textbf{52.71} & 26.40 & 42.08 & 37.61 \\
&\texttt{Ours-2:8+64LR}       && \textbf{90.46} & \textbf{92.59} & \textbf{108.80} && \textbf{54.52} & \textbf{30.85} & \textbf{31.44} & 20.73 & \textbf{50.20} & \textbf{52.71} & \textbf{26.60} & \textbf{60.37} & \textbf{40.93} \\
\cmidrule(rl){2-2}
&\texttt{OATS-3:8+64LR}       && 48.21 & 35.65 & 56.52 && 65.23 & 42.05 & 47.01 & 25.94 & 58.01 & 52.71 & 27.40 & 67.89 & 48.28 \\
&\texttt{Ours-3:8+64LR}       && \textbf{28.88} & \textbf{21.48} & \textbf{32.54} && \textbf{68.99} & \textbf{52.19} & \textbf{50.55} & \textbf{29.86} & \textbf{62.90} & \textbf{53.07} & \textbf{29.80} & \textbf{72.84} & \textbf{52.53} \\
\cmidrule(rl){2-2}
&\texttt{OATS-4:8+64LR}       && 15.97 & 10.52 & 16.71 && 75.14 & 68.69 & 66.67 & 40.87 & 69.69 & \textbf{54.87} & 39.40 & \textbf{79.76} & 61.89 \\
&\texttt{Ours-4:8+64LR}       && \textbf{14.67} & \textbf{9.93} & \textbf{15.28} && \textbf{76.39} & \textbf{70.48} & \textbf{68.48} & \textbf{42.58} & \textbf{70.32} & 54.15 & \textbf{39.80} & 79.48 & \textbf{62.71} \\
\cmidrule(rl){2-2}
&\texttt{OATS-2:4+64LR}       && 21.05 & 14.42 & 22.62 && 72.85 & 62.47 & 60.69 & 36.35 & 67.09 & 54.87 & 35.00 & 75.11 & 58.05 \\
&\texttt{Ours-2:4+64LR}       && \textbf{18.06} & \textbf{12.66} & \textbf{18.66} && \textbf{74.86} & \textbf{64.77} & \textbf{63.85} & \textbf{37.37} & \textbf{69.22} & \textbf{56.68} & \textbf{36.40} & \textbf{76.12} & \textbf{59.91} \\
\cmidrule(rl){2-2}
&\texttt{dense}               && 9.44 & 6.14 & 11.18 && 80.79 & 79.17 & 77.69 & 53.33 & 72.85 & 69.68 & 45.00 & 81.44 & 69.99 \\

\midrule
\multirow{9.5}{*}{Llama3.2-1B} 
&\texttt{OATS-2:8+64LR}       && 740.37 & 825.40 & 754.22 && 52.12 & 27.46 & 28.37 & \textbf{23.72} & 48.86 & 52.71 & 24.60 & 37.77 & 36.95 \\
&\texttt{Ours-2:8+64LR}       && \textbf{167.87} & \textbf{133.01} & \textbf{162.73} && \textbf{54.30} & \textbf{28.73} & \textbf{30.35} & 21.93 & \textbf{50.51} & \textbf{53.43} & \textbf{25.20} & \textbf{51.68} & \textbf{39.52} \\
\cmidrule(rl){2-2}
&\texttt{OATS-3:8+64LR}       && 96.32 & 74.10 & 93.70 && 59.52 & 33.51 & 36.41 & 22.70 & 50.99 & \textbf{52.71} & 25.80 & 62.14 & 42.97 \\
&\texttt{Ours-3:8+64LR}       && \textbf{45.79} & \textbf{34.15} & \textbf{52.20} && \textbf{62.08} & \textbf{38.24} & \textbf{41.04} & \textbf{23.63} & \textbf{54.54} & \textbf{52.71} & \textbf{30.40} & \textbf{62.20} & \textbf{45.60} \\
\cmidrule(rl){2-2}
&\texttt{OATS-4:8+64LR}       && 26.75 & 18.49 & 31.94 && 67.30 & 49.52 & 50.51 & 28.41 & 56.67 & \textbf{55.96} & \textbf{32.40} & \textbf{62.87} & 50.46 \\
&\texttt{Ours-4:8+64LR}       && \textbf{22.71} & \textbf{16.05} & \textbf{26.80} && \textbf{68.28} & \textbf{51.42} & \textbf{51.22} & \textbf{29.18} & \textbf{58.64} & 53.07 & 30.00 & 62.51 & \textbf{50.54} \\
\cmidrule(rl){2-2}
&\texttt{OATS-2:4+64LR}       && 36.89 & 26.26 & 42.35 && 64.36 & 43.35 & \textbf{47.77} & 26.45 & 55.80 & \textbf{52.71} & 30.40 & \textbf{62.66} & 47.94 \\
&\texttt{Ours-2:4+64LR}       && \textbf{27.09} & \textbf{19.57} & \textbf{31.73} && \textbf{67.03} & \textbf{47.53} & 47.43 & \textbf{28.16} & \textbf{58.64} & \textbf{52.71} & \textbf{30.60} & 62.60 & \textbf{49.34} \\
\cmidrule(rl){2-2}
&\texttt{dense}               && 14.01 & 9.75 & 17.59 && 74.59 & 63.66 & 60.48 & 36.26 & 60.69 & 56.68 & 37.20 & 63.98 & 56.69 \\

\midrule
\multirow{9.5}{*}{Llama3.2-3B} 
&\texttt{OATS-2:8+64LR}       && 444.37 & 543.53 & 851.16 && 52.56 & 27.54 & 27.99 & \textbf{23.46} & \textbf{50.43} & 51.99 & \textbf{26.60} & 37.86 & 37.30 \\
&\texttt{Ours-2:8+64LR}       && \textbf{122.14} & \textbf{114.74} & \textbf{165.78} && \textbf{54.57} & \textbf{28.93} & \textbf{30.09} & 21.08 & 49.49 & \textbf{52.71} & 26.20 & \textbf{62.14} & \textbf{40.65} \\
\cmidrule(rl){2-2}
&\texttt{OATS-3:8+64LR}       && 56.80 & 41.62 & 72.75 && 62.68 & 40.49 & 41.84 & 24.06 & 53.91 & 52.35 & 26.60 & 64.10 & 45.75 \\
&\texttt{Ours-3:8+64LR}       && \textbf{35.07} & \textbf{27.12} & \textbf{39.63} && \textbf{66.43} & \textbf{46.08} & \textbf{46.42} & \textbf{26.62} & \textbf{58.17} & \textbf{55.96} & \textbf{29.00} & \textbf{65.47} & \textbf{49.27} \\
\cmidrule(rl){2-2}
&\texttt{OATS-4:8+64LR}       && 18.52 & 12.85 & 20.69 && 72.85 & 61.68 & 62.42 & 36.01 & 64.17 & \textbf{60.29} & 36.40 & \textbf{72.75} & 58.32 \\
&\texttt{Ours-4:8+64LR}       && \textbf{17.19} & \textbf{12.15} & \textbf{19.24} && \textbf{73.99} & \textbf{63.59} & \textbf{62.92} & \textbf{36.26} & \textbf{67.48} & 57.76 & \textbf{39.20} & 71.90 & \textbf{59.14} \\
\cmidrule(rl){2-2}
&\texttt{OATS-2:4+64LR}       && 24.32 & 17.06 & 28.54 && \textbf{71.98} & 55.87 & 58.80 & 33.36 & 59.91 & 53.07 & \textbf{33.80} & \textbf{70.18} & 54.62 \\
&\texttt{Ours-2:4+64LR}       && \textbf{20.82} & \textbf{15.65} & \textbf{23.77} && 71.71 & \textbf{57.88} & \textbf{58.84} & \textbf{34.39} & \textbf{62.12} & \textbf{58.12} & 33.60 & 67.92 & \textbf{55.57} \\
\cmidrule(rl){2-2}
&\texttt{dense}               && 11.33 & 7.81 & 13.53 && 77.48 & 73.61 & 71.63 & 45.99 & 69.85 & 54.51 & 43.00 & 73.39 & 63.68 \\

\bottomrule
\end{tabular}
}
\vspace{3pt}
\caption{Performance analysis for one-shot N:M sparse plus a 64-rank low-rank matrix decomposition of Llama3 and Llama3.2 models. The rank of the low-rank component is fixed to be $r=64$. For Perplexity, $(\downarrow)$ lower values are preferred. For zero-shot tasks, $(\uparrow)$ higher values are preferred.}
\label{tab:slr-fixed-rank}
\end{table}
\subsection{Reconstruction error on a single Transformer block}
In order to show the performance of OATS and \ourframework on the layer-wise reconstruction objective \eqref{eq:matrix-decomposition}, we compute the error produced with the two algorithms (both after $80$ iterations--default value used in OATS \cite{zhang2024oats}), given by $\|\bfX \bfWold-\bfX \pr{\bfWS + \bfM}\|_F^2$, when applied to the model Llama-3-8B \cite{dubey2024llama}, and using the decomposition $\calCS$ corresponding to $2:4$ sparsity and a fixed rank $r = 64$. Results of the local layer-wisre error are reported in \cref{fig:reconstruction-error} for OATS, \ourframework scaled and \ourframework unscaled.

\vspace{-20pt}
\begin{minipage}[t]{0.49\textwidth}
    \raggedleft
    \vspace{30pt}
    \captionof{figure}{Local layer-wise reconstruction error $\downarrow$ (lower values are preferred) analysis of the decomposition of the layers of the \textbf{first} transformer block in Llama-3-8B into a 2:4 sparse component plus a 64-rank low-rank component. All methods use the same number of Alternating-Minimization steps $80$.}
    \label{fig:reconstruction-error}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
    \vspace{0pt}
    \centering
    \includegraphics[width=\textwidth]{layerwise_error.pdf}
\end{minipage}
\vspace{-\baselineskip}