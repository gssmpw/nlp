@inproceedings{charakorn2024instant,
  title={Instant Transformer Adaption via HyperLoRA},
  author={Charakorn, Rujikorn and Cetin, Edoardo and Tang, Yujin and Lange, Robert Tjarko},
  booktitle={Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning},
  year={2024}
}

@article{chen2024multiplicative,
  title={One is Not Enough: Parameter-Efficient Fine-Tuning With Multiplicative Sparse Factorization},
  author={Chen, Xuxi and Chen, Tianlong and Cheng, Yu and Chen, Weizhu},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  year={2024},
  month={Sep},
  note={To appear}
}

@inproceedings{he2022hyperprompt,
  title     = {Hyperprompt: Prompt-based Task-Conditioning of Transformers},
  author    = {He, Yun and Zheng, Steven and Tay, Yi and Gupta, Jai and Du, Yu and Aribandi, Vamsi and Zhao, Zhe and Li, YaGuang and Chen, Zhao and Metzler, Donald and others},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  pages     = {8678--8690},
  year      = {2022},
  organization = {PMLR}
}

@article{ivison2022hyperdecoders,
  title  = {Hyperdecoders: Instance-specific Decoders for Multi-task NLP},
  author = {Ivison, Hamish and Peters, Matthew E},
  journal = {arXiv preprint arXiv:2203.08304},
  year   = {2022}
}

@article{karimi2021parameter,
  title={Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks},
  author={Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James},
  journal={arXiv preprint arXiv:2106.04489},
  year={2021}
}

@article{kim2024single,
  title  = {A Single Linear Layer Yields Task-Adapted Low-Rank Matrices},
  author = {Kim, Hwichan and Sasaki, Shota and Hoshino, Sho and Honda, Ukyo},
  journal = {arXiv preprint arXiv:2403.14946},
  year   = {2024}
}

@inproceedings{lv2024hyperlora,
  title     = {HyperloRA: Efficient Cross-Task Generalization via Constrained Low-Rank Adapters Generation},
  author    = {Lv, Chuancheng and Li, Lei and Zhang, Shitou and Chen, Gang and Qi, Fanchao and Zhang, Ningyu and Zheng, Hai-Tao},
  booktitle = {ACL Findings},
  year      = {2024},
  url       = {https://openreview.net/forum?id=xa4GYUSvhW}
}

@article{ortiz2024hyperloader,
  title  = {Hyperloader: Integrating Hypernetwork-Based LoRA and Adapter Layers into Multi-Task Transformers for Sequence Labelling},
  author = {Ortiz-Barajas, Jesus-German and Gomez-Adorno, Helena and Solorio, Thamar},
  journal = {arXiv preprint arXiv:2407.01411},
  year   = {2024}
}

@article{ostapenko2024modular,
  title   = {Towards Modular LLMs by Building and Reusing a Library of LoRAs},
  author  = {Ostapenko, Oleksiy and Su, Zhan and Ponti, Edoardo Maria and Charlin, Laurent and Le Roux, Nicolas and Pereira, Matheus and Caccia, Lucas and Sordoni, Alessandro},
  journal = {arXiv preprint arXiv:2405.11157},
  year    = {2024}
}

@article{schug2024attention,
  title  = {Attention as a Hypernetwork},
  author = {Schug, Simon and Kobayashi, Seijin and Akram, Yassir and Sacramento, Jo{\~a}o and Pascanu, Razvan},
  journal = {arXiv preprint arXiv:2406.05816},
  year   = {2024}
}

@article{sun2025text,
  title={$\text {Transformer}^ 2$: Self-adaptive LLMs},
  author={Sun, Qi and Cetin, Edoardo and Tang, Yujin},
  journal={arXiv preprint arXiv:2501.06252},
  year={2025}
}

@inproceedings{xiao2023task,
  title     = {Task-Agnostic Low-Rank Adapters for Unseen English Dialects},
  author    = {Xiao, Zedian and Held, William and Liu, Yanchen and Yang, Diyi},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages     = {7857--7870},
  year      = {2023},
  organization = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2023.emnlp-main.487},
  url       = {https://aclanthology.org/2023.emnlp-main.487}
}

@misc{zheng2024dlo,
  title={DLO: Dynamic Layer Operation for Efficient Vertical Scaling of LLMs},
  author={Tan, Zhen and Dong, Daize and Zhao, Xinyu and Peng, Jie and Cheng, Yu and Chen, Tianlong},
  year={2024},
  note={ArXiv preprint arXiv:2407.11030}
}

