[
  {
    "index": 0,
    "papers": [
      {
        "key": "chen2024multiplicative",
        "author": "Chen, Xuxi and Chen, Tianlong and Cheng, Yu and Chen, Weizhu",
        "title": "One is Not Enough: Parameter-Efficient Fine-Tuning With Multiplicative Sparse Factorization"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kim2024single",
        "author": "Kim, Hwichan and Sasaki, Shota and Hoshino, Sho and Honda, Ukyo",
        "title": "A Single Linear Layer Yields Task-Adapted Low-Rank Matrices"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "xiao2023task",
        "author": "Xiao, Zedian and Held, William and Liu, Yanchen and Yang, Diyi",
        "title": "Task-Agnostic Low-Rank Adapters for Unseen English Dialects"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ostapenko2024modular",
        "author": "Ostapenko, Oleksiy and Su, Zhan and Ponti, Edoardo Maria and Charlin, Laurent and Le Roux, Nicolas and Pereira, Matheus and Caccia, Lucas and Sordoni, Alessandro",
        "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "sun2025text",
        "author": "Sun, Qi and Cetin, Edoardo and Tang, Yujin",
        "title": "$\\text {Transformer}^ 2$: Self-adaptive LLMs"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "karimi2021parameter",
        "author": "Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James",
        "title": "Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "he2022hyperprompt",
        "author": "He, Yun and Zheng, Steven and Tay, Yi and Gupta, Jai and Du, Yu and Aribandi, Vamsi and Zhao, Zhe and Li, YaGuang and Chen, Zhao and Metzler, Donald and others",
        "title": "Hyperprompt: Prompt-based Task-Conditioning of Transformers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ivison2022hyperdecoders",
        "author": "Ivison, Hamish and Peters, Matthew E",
        "title": "Hyperdecoders: Instance-specific Decoders for Multi-task NLP"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zheng2024dlo",
        "author": "Tan, Zhen and Dong, Daize and Zhao, Xinyu and Peng, Jie and Cheng, Yu and Chen, Tianlong",
        "title": "DLO: Dynamic Layer Operation for Efficient Vertical Scaling of LLMs"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "schug2024attention",
        "author": "Schug, Simon and Kobayashi, Seijin and Akram, Yassir and Sacramento, Jo{\\~a}o and Pascanu, Razvan",
        "title": "Attention as a Hypernetwork"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ortiz2024hyperloader",
        "author": "Ortiz-Barajas, Jesus-German and Gomez-Adorno, Helena and Solorio, Thamar",
        "title": "Hyperloader: Integrating Hypernetwork-Based LoRA and Adapter Layers into Multi-Task Transformers for Sequence Labelling"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lv2024hyperlora",
        "author": "Lv, Chuancheng and Li, Lei and Zhang, Shitou and Chen, Gang and Qi, Fanchao and Zhang, Ningyu and Zheng, Hai-Tao",
        "title": "HyperloRA: Efficient Cross-Task Generalization via Constrained Low-Rank Adapters Generation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "charakorn2024instant",
        "author": "Charakorn, Rujikorn and Cetin, Edoardo and Tang, Yujin and Lange, Robert Tjarko",
        "title": "Instant Transformer Adaption via HyperLoRA"
      }
    ]
  }
]