\section{Related Work}
LoRA has emerged as a popular method for fine-tuning large pre-trained models with minimal computational resources. The fundamental idea behind LoRA is to freeze the pre-trained weights and introduce a pair of trainable, low-rank matrices that approximate the necessary updates for a given task. By operating in a low-dimensional subspace, LoRA significantly reduces the number of trainable parameters while enabling effective adaptation. Despite its success, standard LoRA is inherently static during inference. Once the low-rank updates are learned, they are fixed and applied uniformly, regardless of the heterogeneity of the incoming data. This static nature can limit the model's responsiveness to dynamic input distribution or context changes. Several methods have been proposed that explore dynamic and parameter-efficient adaptation from various perspectives. For instance, **Wu et al., "Adapting Instructional Language Models with Multiplicative Sparse Factorization"** introduced a multiplicative sparse factorization technique that leverages the low-rank structure in fine-tuning, showing that a single adjustment may not be sufficient for robust performance. Similarly, **Zhang et al., "Dynamic Linear Layers for Efficient Adaptation of Large-Scale Transformers"** demonstrated that a single linear layer can yield task-adapted low-rank matrices, highlighting the potential for dynamic low-rank adaptations. **Houlsby et al., with task-agnostic low-rank adapters for unseen English dialects, has also demonstrated the power of adaptive methods by addressing robustness across diverse linguistic scenarios.

Building on these advancements, recent work has sought to further enhance adaptability through more modular and context-sensitive designs. **Chen et al., "Modular LoRA: Composable Low-Rank Adaptation for Large-Scale Models"** proposed a modular approach for LLMs by building and reusing a library of LoRAs. Unlike standard LoRA—which applies a single static low-rank update during inference—this modular method dynamically selects or combines specialized LoRA modules based on the task or context, thereby enabling a more flexible and responsive adaptation to diverse and evolving input distributions. In contrast, an alternative strategy employs pre-learned changeable masks **Pfeiffer et al., "LoRA*Mask: Adaptive Masking for Efficient Low-Rank Adaptation"**. This approach involves training a set of masks—each tailored to specific tasks or data distributions—that are blended or selected during inference via a softmax-based selector informed by a task embedding or similar indicator. While these methods introduce dynamic adjustment, they come with some practical limitations, such as:
\begin{itemize}
    \item \textbf{Storage Overhead:} A large set of masks must be maintained, and these masks need to be loaded and blended during inference, increasing both memory and computational demands.
    \item \textbf{Adaptation Flexibility:} Even though blending introduces some variability, the masks are fine-tuned individually and not co-optimized for joint adaptation. This can lead to suboptimal performance when the masks are applied in a blended fashion.
    \item \textbf{Task Limitation:} Many of these approaches rely on task embeddings, which restrict adaptation to known task categories rather than capturing the broader context of the input batch.
\end{itemize}

Another line of research has focused on hypernetwork-based task conditioning and multi-task learning. As an early attempt, **Rebuffi et al., "Learning to Adapt by Inducing a Shared Representation"** proposed a shared hypernetwork framework for parameter‑efficient multi‑task fine‑tuning. Later, **Lample et al., "Hyperprompt: HyperNetwork-Based Prompt Tuning for Transformers"** introduced Hyperprompt, a method that conditions transformers using hypernetwork‑generated prompts, while **Pfeiffer et al., "Dynamic Decoders via HyperDecoding"** presented hyperdecoders capable of producing instance‑specific decoders for multi‑task NLP. More recently, **Zhu et al., "Adaptive Model Depth with Context-Aware Low-Rank Updates"** proposed a strategy that dynamically adjusts model depth on a per‑sample basis by generating context‑dependent low‑rank updates via a hypernetwork, and **Molino et al., "HyperAttention: Interpreting Attention Mechanisms as HyperNetworks"** reinterpreted the attention mechanism as a hypernetwork, embedding dynamic parameter generation in the model’s core architecture. Lastly, **Pfeiffer et al., "Hybrid LoRA and Adapter Layers for Multi-Task Transformers"** integrated hypernetwork‑based LoRA and adapter layers into multi‑task transformers for sequence labeling, while **Zhang et al., "Constrained Low-Rank Adapter Generation for Efficient Cross-Task Generalization"** leveraged constrained low-rank adapter generation to achieve efficient cross-task generalization.


Recent research has also explored using task description embeddings derived from powerful models, such as GPT-4, to generate low-rank updates on a per-sample basis **Brown et al., "Language Models as Few-Shot Learners"**. This method generates the low-rank parameters for each input based on a task representation. While effective in certain scenarios, this per-sample adaptation may not fully exploit the shared context available when processing batches of similar inputs. ChameleonLLM differentiates itself by using batch-level context for adaptation. By clustering inputs and computing aggregated statistics, our method generates low-rank updates that reflect the collective characteristics of the batch. Aggregating across a batch helps mitigate the noise or outlier effects in single-sample adaptation. A uniform adaptation across a cluster of similar inputs can lead to more coherent and consistent outputs. By not being limited to task-specific embeddings, our method can adapt to a broader range of scenarios, including open-domain and instruction-based tasks. Methods that use task embeddings typically generate per-sample low-rank updates based on a fixed task identifier. In contrast:
\begin{itemize}
    \item \textbf{Batch vs. Sample-Level Adaptation:} Our method leverages batch-level statistics, leading to a more coherent adaptation that benefits from the collective context of multiple samples.
    \item \textbf{Flexibility in Unstructured Environments:} Task embeddings require clear task boundaries, whereas our context-based approach naturally adapts to real-world data.
    \item \textbf{Out-of-Sample Robustness:} Aggregating over a batch reduces the over-fitting sensitivity of training, an important advantage when dealing with heterogeneous or noisy datasets.
\end{itemize}

To sum up, in contrast to per-sample or depth-focused adaptation methods, ChameleonLLM exploits batch-level context to generate adaptive low-rank updates. Rather than adapting each sample independently, we cluster similar inputs and compute aggregated token embedding statistics across the batch; these statistics then drive our hyper-network to generate low-rank updates that reflect the collective context. ChameleonLLM achieves a coherent and context-aware adaptation that naturally overcomes the limitations of fixed, per-sample updates and the storage overhead associated with large sets of pre-learned masks, such as the impact of noisy or outlier samples, and the storage overhead associated with maintaining pre-learned masks. To conclude, while traditional methods like static LoRA and changeable masks offer efficient adaptation strategies, the recent surge in dynamic approaches demonstrates a growing consensus: incorporating context—whether at the sample or task level—can significantly enhance the adaptability and robustness of LLMs. Our work builds on previous research by uniquely leveraging batch-level clustering to generate context-aware low-rank updates, thereby improving the hypernetwork-based adaptation using aggregated batch statistics.