\section{Related Work}
\subsection{System 2 Methods in LLMs}
Recent research on large language models for System 2 tasks focus on inference-time computation optimization to stimulate the inherent reasoning ability of LLMs. Few-shot learning methods ____ utilize the in-context-learning ability of LLMs for enhanced generation. Retrieval-augmented generation (RAG) approaches ____ further introduce domain knowledge into LLMs. 
Techniques such as Chain-of-Thought (CoT) ____, Tree-of-Thought (ToT) ____, and Monte Carlo Tree Search (MCTS) ____ are used to explore the inherent reasoning process, often based on the self-play mechanism to reflect on previously generated contents to learn from itself ____.
During inference, error position can be beneficial in improving the reliability and performance of the model. With identification and analysis of where and why errors occur, recent research ____ has made significant strides in quantifying and mitigating errors during model inference. Refinement ____ and reflexion ____ are also powerful techniques for enhancing the inference capabilities of LLMs, usually by enabling iterative improvement and self-correction.

\subsection{Model Composition}
Model composition technique gains notable attention in cross-tasks generalization. 
Traditional methods for multiple tasks are to train models on a mixture of datasets of different skills ____, with the high cost of data mixing and lack of scalability of the model though. Model merging is a possible solution to this. Linear merging is a classic merging method that consists of simply averaging the model weights ____. Furthermore, Task Arithmetic ____ computes task vectors for each model, merges them linearly, and then adds back to the base, and SLERP ____ spherically interpolates the parameters of two models. Based on Task Arithmetic framework, TIES ____ specifies the task vectors and applies a sign consensus algorithm to resolve interference between models, and DARE ____ matches the performance of original models by random pruning.

Recently, LoRA merging methods are also widely applied to cross-task generalization. CAT ____ introduces learnable linear concatenation of the LoRA layers, and Mixture of Experts(MoE) ____ method has input-dependent merging coefficients. Other linear merging methods of LoRAs, such as LoRA Hub ____, involve additional cross-terms compared to simple concatenation. %\whj{这句没看懂}