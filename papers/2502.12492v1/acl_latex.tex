% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,enumitem}
\usepackage{booktabs}

%\usepackage[table]{xcolor}
%\usepackage{xcolor}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{inconsolata}
\usepackage{mdframed}
\usepackage{makecell}
\usepackage{amssymb}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{xspace}
\newcommand{\modelname}{BDC\xspace}

\newcommand{\weinan}[1]{{\color{blue}[\textbf{weinan: #1}]}}
\newcommand{\minisection}[1]{\vspace{0pt}\noindent\textbf{#1.}}
\newcommand{\dk}[1]{{\color{orange}[\textbf{dk: #1}]}}
\newcommand{\ljx}[1]{{\color{cyan}[\textbf{ljx: #1}]}}
\newcommand{\whj}[1]{{\color{brown}[\textbf{whj: #1}]}}

%\title{\modelname: Learning to Weight Over Disentangled Experts \\ for Robust Code Generation}

\title{Boost, Disentangle, and Customize: \\
A Robust System2-to-System1 Pipeline for Code Generation}


\author{Kounianhua Du$^1$, Hanjing Wang$^1$, Jianxing Liu$^1$, Jizheng Chen$^1$, \\
{\bf Xinyi Dai$^2$, Yasheng Wang$^2$, Ruiming Tang$^2$, Yong Yu$^1$, Jun Wang$^3$, Weinan Zhang$^1$}\\
  $^1$Shanghai Jiao Tong University, $^2$ Huawei Noah’s Ark Lab, $^3$ University College London \\
  Shanghai, China\\
  \texttt{\{kounianhuadu, wnzhang\}@sjtu.edu.cn}
  %\texttt{\{774581965,ruirenting,fatcat,fulingyue,yyu,wnzhang\}@sjtu.edu.cn}, \\ 
  %\texttt{\{xiawei24,wangyasheng,tangruiming\}@huawei.com}
  }

\begin{document}
\maketitle

% \begin{abstract}
% Large language models (LLMs) have demonstrated remarkable capabilities in many domains, yet their ability in System 2 tasks remain opaque. The inherent high-thinking demand and complex latent patterns of data make it hard for llms to accurately infer solutions for these tasks. And the simple train-once-for-all tuning mechanism may then fail under this situation. In this paper, we propose \textbf{\modelname} to \textbf{Disen}tangle the complex problem solving procedure and heterogeneous data distribution into meta-units and adaptively aggregate the resulting \textbf{Lora}-experts to generate problem solver for each problem instance. Concretely, 1) we disentangle the problem-solving stages into problem2thought and thought2solution processes and integrate the search process using a novel multi-expert MCTS algorithm, armed with reflextion-based pruning and refinement. 2) The resulting training data is thereafter disentangled into different meta clusters based on semantic distance, on which we finetune different lora experts capable of different levels of tasks.  3) Then, we train an input-aware hyper-network to adaptively aggregate the lora experts rank-wise for contextualized problem solver. Experiment results and various ablation studies validate the superiority of the data collection algorithm, the effectiveness of the data disentanglement process, and the performance gain brought by the input-aware hyper-network.
% \end{abstract}

%\input{latex/sections/abstract}
%\input{latex/sections/intro}


%\input{latex/sections/related}
%\input{latex/sections/prelim}
%\input{latex/sections/method}
%\input{latex/sections/expr}
%\input{latex/sections/conclusion}
%\input{latex/sections/limit}
%\input{latex/sections/ack}


\begin{abstract}
%Large language models (LLMs) have demonstrated remarkable capabilities in many domains, yet their ability in System 2 tasks remain opaque. The inherent high-thinking demand and complex latent patterns of data, make it hard for llms to accurately infer solutions. Under this situation, the simple train-once-for-all post-training mechanism may then fail. In this paper, we propose \textbf{\modelname} to \textbf{Disen}tangle the complex problem solving procedure and heterogeneous data distribution into meta-units and adaptively aggregate the resulting \textbf{Lora}-experts to generate problem solver for each problem instance. 
%Concretely, 1) we disentangle the problem-solving stages into problem2thought and thought2solution processes, integrating the search process with a novel multi-expert MCTS algorithm, where reflexion-based pruning and refinement help to boost performance. 2) The resulting training data is thereafter disentangled into different meta clusters based on semantic distance, on which we finetune lora experts capable of different aspects of tasks.  3) Then, we train an input-aware hyper-network to adaptively aggregate the lora experts rank-wise for contextualized problem solver. Experiment results and various ablation studies validate the superiority of the data collection algorithm, the effectiveness of the data disentanglement process, and the performance gain brought by the input-aware hyper-network.
%\whj{第一句要改，如果重点是2->1的话}
Large language models (LLMs) have demonstrated remarkable capabilities in various domains, particularly in system 1 tasks, yet the intricacies of their problem-solving mechanisms in system 2 tasks are not sufficiently explored. 
Recent research on System2-to-System1 methods surge, exploring the System 2 reasoning knowledge via inference-time computation and compressing the explored knowledge into System 1 process. In this paper, we focus on code generation, which is a representative System 2 task, and identify two primary challenges: (1) the complex hidden reasoning processes and (2) the heterogeneous data distributions that complicate the exploration and training of robust LLM solvers. To tackle these issues, we propose a novel BDC framework that explores insightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with mutual \textbf{B}oosting, \textbf{D}isentangles the heterogeneous training data for composable LoRA-experts, and obtain \textbf{C}ustomized problem solver for each data instance with an input-aware hypernetwork to weight over the LoRA-experts, offering effectiveness, flexibility, and robustness. This framework leverages multiple LLMs through mutual verification and boosting, integrated into a Monte-Carlo Tree Search process enhanced by reflection-based pruning and refinement. Additionally, we introduce the DisenLora algorithm, which clusters heterogeneous data to fine-tune LLMs into composable Lora experts, enabling the adaptive generation of customized problem solvers through an input-aware hypernetwork. Our contributions include the identification of critical challenges in existing methodologies, the development of the MC-Tree-of-Agents algorithm for insightful data collection, and the creation of a robust and flexible solution for code generation. This work lays the groundwork for advancing LLM capabilities in complex reasoning tasks,  offering a novel System2-to-System1 solution.

%\whj{
%State-of-the-art (SOTA) Large Language Models (LLMs) demonstrate System-2-like intelligence through multi-step logical reasoning, yet remain limited in achieving human-level Artificial General Intelligence (AGI) across diverse tasks. Focusing on code generation, which are proven as an effective proxy for complex problem-solving, we identify two critical challenges: (1) xxxx, and (2) heterogeneous task distributions that resist unified modeling.

%We propose our Boost-Disentangle-Customize (BDC) framework, bridging System-2 collective reasoning with System-1 skill specialization. Our approach features:
%(1) MC-Tree-of-Agent: Collective Monte-Carlo searching and mutual verification boosted reasoning. (2) DisenLoRA Adaption: Parameter-efficient specialization via semantic clustering of reasoning trajectories and dynamic experts composition with input-awareness. Empirical results have shown up to $13.8\%$ improvement over GPT4o-mini on CodeContest-Hard and superior robustness against existing adapter merging algorithms.

%}
\end{abstract} 

\section{Introduction}

%\whj{Some questions here: (1) How should we build the corresponding inter-connections between system-1/2 and our two-stages problem-solving framework? system-1 -> P2T with system-2 -> T2S or system-2 -> P2T + T2S or backbone LLM as system-2 with lora adapters as system-1 }

%\begin{itemize}
%    \item LLM achievement
%    \item System 1-2
%    \item 
%    \item moe, sparsity
%\end{itemize}

%The core contributions of our work are two-fold: (1) We propose a two-stage problem-solving framework, namely Problem2Thought and Thought2Problem to enhance LLMs in complex reasoning tasks. And the performance of our framework, as a strong system-2 solver, is evaluated on enhanced trajectories from SOTA commercial LLMs. (2) Based on the collected trajectory data, we streamline a pipeline to strengthen the reasoning abilities of locally deployed LMs by training specialized LoRA experts and ensembling them in the inference-time with input-awareness, aiming to provide capable and specilized system-1 solvers.

%We like to organize the intro section in several paragraphs:

%1: Basic intro of recent progress of LLMs: The emerging and improving abilities of Large Language Models(LLM) as they scaling have attracted extensive attention from the community. They have exhibited comparable even human-surparsing capabilities in many tasks, for example autodriving, robotics, ... While unlike traditional deep learning models that excel in a certain narrow and highly specialized category of tasks, failing to generalize to a broader range of problems, LLMs produce general answer sequences for tasks grounded in text format, failing in given meaningful solution for specialized tasks. 

%2. If we consider human intelligence, SOTA commercial LLMs(e.g. GPT 4o, Claude-3.5) are close to the idea of system-2, featuring slow, rational and general reasoning capabilities. As contrast, system-1, identified by psycologist, represents quick, instinctful and specialized intelligence.

%3. Previous literature argues that the fusion paradigm of system-1 and system-2, for example materialized by a hierachicial structure contains dual reasoning and controling loop of both large and small models, can be essential for AGI. And LoRA adapters can adapt to various narrow downstream tasks without harming the general world knowledge held in the backbone LMs, which inspires us to adopt peft methods to instantiate system-1 solvers.

%4. Given the success of MoE structure and DeepSeek, the sparsely task-aware activativation of model parameters can be an important road balancing between  scaling for better reason abilities and optimizing inference cost for specific tasks. Thus we propose our HyperNet model for inference-time adatpers ensemble, while also leaving space for future optimization opportunity of sparsity.

%5. Introduce our core contributions.



%--------- \whj{Draft Writing Here} ---------
% Para 1: Importance of llms and the obstacles of system 2 tasks. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/intro.pdf}
    \caption{Illustration of the motivation.}
    \label{fig:intro}
\end{figure}

Large language models show significant intelligence in various domains, striking both the academic and industrial institutions. Despite their prominent problem-solving abilities in system 1 tasks, the mechanism behind the system 2 task solving procedure remain opaque. In this paper, we focus on the code generation task, which emerges as a captivating frontier \citep{zheng2023codegeex, roziere2023code, shen2023pangu}, promising to revolutionize software development by enabling machines to write and optimize code with minimal human intervention. Recent research of llms for code focus on inference-time computation (System 2 methods) \citep{yang2024chain, yao2024tree, zhang2023planning} and post-training. While during post-training, distilling system 2 knowledge into system 1 backbones is important and widely-used \citep{yu2024distilling21}. 

However, the complex hidden reasoning process and the heterogeneous data distribution pose challenges to the existing System2-to-System1 pipeline. On one hand, the hidden reasoning process for code generation is complex and hard to explore \textbf{(C1)}. On the other hand, the heterogeneous data distribution, e.g., jumping structure like branching, recursion, etc., makes the existing train-once-for-all strategy hard to fit the complex latent patterns for robust and generalizable llm solvers \textbf{(C2)}. 

For \textbf{(C1)}, we propose to disentangle the problem solving process into problem2thought and thought2solution stages, exploring the inherent reasoning clues via combining the strengths of multiple llms by mutually-verification and boosting. The exploration is integrated into a Monte-Carlo Tree Search process, where reflexion-based pruning and refinement are designed for more efficient and effective reasoning clues search.

For \textbf{(C2)}, we propose to disentangle the heterogeneous data into clusters, finetuning llms capable of different aspects of tasks to obtain the meta LoRA experts hub, and then adaptively generate customized problem solver for each code problem.  Concretely, we design an input-aware hypernetwork to generate rank-wise weights over meta LoRA experts for customized problem solver, offering robustness and flexibility.

The main contributions of our work can be summarized below.
\begin{itemize}
    \item \textbf{Identification of problems and novel BDC framework.} We identify the high-reasoning demand and heterogeneous latent patterns problems that hinders the performance of existing methods and propose a BDC framework that explores insightful inherent reasoning clues via multi-llms boosting, generates meta-LoRA experts via finetuning on disentangled data, and offer customized problem solver with an input-aware hypernet for rank-wise LoRA merging.
    \item \textbf{Novel MC-Tree-of-Agents algorithm for insightful data collection.} We disentangle the System 2 solving process into problem2thought and thought2solution stages, integrating the exploration process into a reflexion-based monte carlo tree search armed with pruning and refinement, enabling mutually verification and boosting of different agents for insightful data collection. %\whj{Should we reorganize it as a multi-agent problem?}
    \item \textbf{Novel DisenLoRA algorithm that offers customized problem solver for robust code generation.} We disentangle the heterogeneous data distribution into clusters on which meta-LoRA experts are trained, and design an input-aware hypernetwork to weight over the LoRA-experts for customized problem solver, offering robustness and flexibility.
\end{itemize}

%\whj{
%The evolution of large language models (LLMs) has revealed distinct cognitive paradigms mirroring human dual-process theory [1]. Commercial gigantic LMs like GPT-4o and Claude-3.5 exhibit System-2-like characteristics - deliberate, multi-step reasoning through chain-of-thought (CoT) paradigms. This contrasts with earlier deep-learning models' System-1-like narrow intelligence in pattern recognition. Yet as (https://openreview.net/pdf?id=0ofzEysK2D) demonstrate, even state-of-the-art LLMs only reach "Level-2 AGI" - equivalent to unskilled human reasoning in a wide range of non-physical tasks - highlighting the need for frameworks that bridge systematic reasoning with specialized skill acquisition.

%We present a efficient pipeline inspired by human cognitive development: System-2-powered exploration followed by System-1 specialization. Our Boost-Disentangle-Compose (BDC) pipeline addresses two fundamental challenges in code generation: 1) The need for high-quality reasoning traces to bootstrap System-1 skill acquisition, and 2) The heterogeneous latent patterns in real-world coding tasks that resist monolithic adaptation approaches. In this paper, we focus on the code generation task, which is shown to be an effective proxy for various purposed tasks, including robotic control and math solving.

%By decomposing the code generation process into problem-to-thought(refining task specification) and thought-to-solution(synthesizing implementations) phases, we further strengthen the collective searching paradigm of the existing reason-and-reflect frameworks by:
%\begin{itemize}
%    \item Fine-grained mutual verification and pruning.
%    \item Complementary refinement boosting generation process across models.
%    \item Dynamic reward shaping via compilation feedback.
%\end{itemize}
%This collaborative reasoning process boosts System-2-like characteristics, yielding up to $13.8\%$ improvement on complex benchmarks.

%Additionally, the high-quallity samples collected from System-2 exploration unfold a resultant solution space featuring high intrinistic heterogeneity. Our analysis shows coding samples naturally clustered into several semantic groups, as visualized in 
%Fig.\ref{fig:mc}. This pattern underlines the importance of specialized skill acquisition via System-1-like frameworks.  We therefore propose DisenLoRA, which adapts parameter-efficient fine-tuning through:
%\begin{itemize}
%    \item Semantic disentanglement of solution clusters.
%    \item LoRA experts as per cluster.
%    \item A input-aware HyperNet for dynamic expert composition.
%\end{itemize}
%The proposed System-1-like framework achieves robust performance on both IID and OOD benchmarks. The dynamic composition with input-awareness also outperform existing adapter ensembling framework by $26\%$.

%The main contributions of our work can be summarized below.
%\begin{itemize}
%    \item \textbf{Identification of problems and novel BDC framework.} We identify the high-reasoning demand and heterogeneous latent patterns problems that hinders the performance of existing methods and propose a BDC framework that explores insightful inherent reasoning clues via multi-llms boosting, generates meta-lora experts via finetuning on disentangled data, and offer customized problem solver with an input-aware hypernet for rank-wise lora merging.
%    \item \textbf{Novel MC-Tree-of-Agents algorithm for insightful data collection.} We disentangle the System 2 solving process into problem2thought and thought2solution stages, integrating the exploration process into a reflexion-based monte carlo tree search armed with pruning and refinement, enabling mutually verification and boosting of different agents for insightful data collection. 
%    \item \textbf{Novel DisenLora algorithm that offers customized problem solver for robust code generation.} We disentangle the heterogeneous data distribution into clusters on which meta-lora experts are trained, and design an input-aware hypernetwork to weight over the lora-experts for customized problem solver, offering robustness and flexibility.
%\end{itemize}

%}


\section{Related Work}
\subsection{System 2 Methods in LLMs}
Recent research on large language models for System 2 tasks focus on inference-time computation optimization to stimulate the inherent reasoning ability of LLMs. Few-shot learning methods \cite{wang2022code4struct,madaan2022language} utilize the in-context-learning ability of LLMs for enhanced generation. Retrieval-augmented generation (RAG) approaches \cite{nashid2023retrieval,du2024codegragbridginggapnatural} further introduce domain knowledge into LLMs. 
Techniques such as Chain-of-Thought (CoT) \cite{yang2024chain,jiang2024self,li2023structured}, Tree-of-Thought (ToT) \cite{yao2024tree,la2024can}, and Monte Carlo Tree Search (MCTS) \cite{li2024rethinkmcts,zhang2023planning,hu2024uncertainty,hao2023reasoning,feng2024alphazeroliketreesearchguidelarge} are used to explore the inherent reasoning process, often based on the self-play mechanism to reflect on previously generated contents to learn from itself \cite{haluptzok2022language,chen2023gaining,lu2023self,chen2023teaching,madaan2024self,shinn2024reflexion}.
During inference, error position can be beneficial in improving the reliability and performance of the model. With identification and analysis of where and why errors occur, recent research \cite{yao2024mulberry, luo2024improve, wu2025error} has made significant strides in quantifying and mitigating errors during model inference. Refinement \cite{madaan2024self, gou2023critic} and reflexion \cite{shinn2024reflexion, lee2025evolving} are also powerful techniques for enhancing the inference capabilities of LLMs, usually by enabling iterative improvement and self-correction.

\subsection{Model Composition}
Model composition technique gains notable attention in cross-tasks generalization. 
Traditional methods for multiple tasks are to train models on a mixture of datasets of different skills \cite{caruana1997multitask, chen2018gradnorm}, with the high cost of data mixing and lack of scalability of the model though. Model merging is a possible solution to this. Linear merging is a classic merging method that consists of simply averaging the model weights \cite{izmailov2018averaging, smith2017investigation}. Furthermore, Task Arithmetic \cite{ilharco2022editing} computes task vectors for each model, merges them linearly, and then adds back to the base, and SLERP \cite{white2016sampling} spherically interpolates the parameters of two models. Based on Task Arithmetic framework, TIES \cite{yadav2024ties} specifies the task vectors and applies a sign consensus algorithm to resolve interference between models, and DARE \cite{yu2024language} matches the performance of original models by random pruning.

Recently, LoRA merging methods are also widely applied to cross-task generalization. CAT \cite{prabhakar2024lora} introduces learnable linear concatenation of the LoRA layers, and Mixture of Experts(MoE) \cite{buehler2024x, feng2024mixture} method has input-dependent merging coefficients. Other linear merging methods of LoRAs, such as LoRA Hub \cite{huang2023lorahub}, involve additional cross-terms compared to simple concatenation. %\whj{这句没看懂}

\section{Preliminaries}
\subsection{Monte-Carlo Tree Search}
Monte Carlo Tree Search (MCTS) is a decision-making algorithm widely used in environments with large state and action spaces, particularly in game AI and planning.  It incrementally builds search trees to estimate optimal actions by simulating random plays from various nodes and gradually improving action-value estimates based on simulation outcomes. Over iterations, this approach gradually converges to near-optimal decision-making policies. Notably, its integration with reinforcement learning has driven breakthroughs in systems like AlphaGo and AlphaZero \cite{silver2017mastering}, achieving superhuman performance in games.
%The combination of MCTS with reinforcement learning has achieved notable successes, such as in AlphaGo and AlphaZero \cite{silver2017mastering}.

Classical MCTS consists of four stages: selection, expansion, simulation, and backpropagation. It typically employs Upper Confidence Bounds for Trees (UCT) \cite{kocsis2006bandit}, which balances exploration and exploitation by guiding the search to promising nodes. After simulation, results propagate back through the tree, updating node values. However, MCTS struggles in domains with large action spaces, where excessive branching can degrade performance. Progressive Widening and Double Progressive Widening techniques have been proposed to mitigate this by dynamically limiting the number of actions considered at each decision node \cite{coulom2006efficient}.

\subsection{LoRA Finetuning}
%\dk{simple introduction and equations}
LoRA (Low-Rank Adaptation) \cite{hu2021lora} fine-tuning is a technique used to adapt large pre-trained models, such as transformers, to specific tasks with minimal computational overhead. The key idea behind LoRA is to introduce low-rank matrices into the model's weight updates, which reduces the number of trainable parameters and makes fine-tuning more efficient. 

LoRA starts with a model that has been trained on a large dataset. During finetuning, instead of updating the full weight matrix $W \in \mathbb{R}^{m \times n}$, LoRA introduces two low-rank matrices $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r \times n}$, where $r \ll \min(m, n)$. The updated weight matrix $W'$ is then given by:
\begin{equation}
    W' = W + \Delta W = W + A \cdot B.
\end{equation}

During fine-tuning, only the matrices $A$ and $B$ are updated, while the original weight matrix $W$ remains frozen. This reduces the number of trainable parameters from $m \times n$ to $m \times r + r \times n$, which is much smaller when $r$ is small. For a given task with loss function $\mathcal{L}$, the objective is to minimize:
\begin{equation}
\mathcal{L}(y, f(x; W + A \cdot B)),
\end{equation}
where $y$ is the target output, $x$ is the input, and $f$ is the model's forward function.

By introducing low-rank matrices, both the number of trainable parameters and memory footprint are reduced. This approach is particularly useful in scenarios where computational resources are limited or when fine-tuning needs to be done quickly.


\section{Methodology}
In this section, we introduce the overall methodology of BDC, addressing challenges in the System2-to-System1 pipeline for code generation, specifically the complexity of hidden reasoning processes and heterogeneous data distributions. 
The proposed BDC pipeline consists of three main stages: 1) explore the System 2 knowledge via mutual verification and boosting between LLMs; 2) disentangle the obtained data into clusters over which composible LoRA experts are tuned; 3) customize problem solver by weighting over the composable LoRA experts using an input-aware hypernetwork.

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/framework.pdf}
   \caption{Illustration of the overall framework of \modelname.}
    \label{fig:overview}
\end{figure*}

\subsection{System 2 Knowledge Exploration}

%\textbf{Select.} The select phase aims to traverse the decision tree by selecting nodes that balance exploration of unvisited nodes with exploitation of high-value nodes. Starting from the root node, the process iterates through the tree by applying a selection policy. In our implementation, the selection process is governed by the Upper Confidence Bound (UCB) strategy, with a variant of probability-weighted UCB (P-UCB) for better exploration.
%Each node is either a DecisionNode (representing agent decisions) or a ChanceNode (representing state-action transitions). When at a DecisionNode, the node selection is made based on the node's estimated value and visitation frequency, guided by the UCB formula:
%$$\mathrm{PUCB}(node)=Q(s,a)+c\cdot P(a|s)\cdot\frac{\sqrt{\log N(s)}}{1+N(s,a)}$$ 

%where $Q(s, a)$ represents the action value, $P(a|s)$ is the policy’s probability of selecting action $a$ in state $s$, and $N(s)$ is the number of visits to state $s$. The selection proceeds until a terminal node or an unvisited node is encountered.

%\textbf{Expand.} Expansion occurs when a previously unvisited state or a ChanceNode is reached during the selection phase. This step involves adding a new DecisionNode to the tree. In our implementation, when a ChanceNode transitions to a new state $s_t$ after an action $a$, the reward $r(s_t ,a)$ is computed. The reward evaluates the correctness of the state $s_t$ based on the output's pass rate, which is quantified by the evaluation function $R(s_t)$, as seen in the equation:
%$$R(s_t)=\mathrm{PassRate}(s_t)$$
%In our setup, the PassRate is determined by the proportion of correct outputs as evaluated by a code correctness checker. Additionally, future rewards are calculated if the node reaches a terminal state, incorporating the step rewards from the current node to future nodes through rollouts. This is expressed as:
%$$r(s_t,a)=R(s_t)+\gamma\cdot\sum_{i=1}^Tr(s_{t+i},a)$$
%Here, $\gamma$ represents the discount factor applied to future rewards during backpropagation, while the total reward is a combination of the current reward and discounted future rewards. The expansion step also adds new DecisionNodes to the tree, allowing subsequent actions to be explored.

%\textbf{Simulate (Rollout).} The Simulate phase, also known as Rollout, begins after a ChanceNode has expanded. In this phase, a sequence of actions is sampled from the current policy, and the rewards are accumulated until a terminal state is reached or the maximum rollout depth is achieved. Each simulation (rollout) evaluates a potential trajectory from the current state, allowing the model to estimate future rewards. The rollout process can be expressed as: 
%\begin{enumerate}
%  \item For each state $s_t$  and action $a$, the state transitions to $s_{t+1}$ according to the transition function $f(s_t, a)$, which is defined as: $$s_{t+1}=f(s_t,a)=s_t+a$$ where $s_t$ is the prompt already generated before the action and $a$ is the continuing prompt (thought).
%  \item If a terminal condition is met, i.e., if the state reaches the terminal token or exceeds the maximum length, a terminal reward $R(s_t)$ is calculated based on the correctness of the output. Otherwise, an intermediate reward of 0 is assigned: $$r(s_t,a)=\begin{cases}R(s_t)&\text{if terminal,}\\0&\text{otherwise.}\end{cases}$$
%  \item The overall reward for the rollout is the sum of rewards across all steps, discounted by $\gamma$ (the discount factor for future rewards): $$\mathrm{RolloutReward}(s_t)=\sum_{i=0}^T\gamma^i\cdot r(s_{t+i},a)$$ Here, $r(s_{t+i} ,a)$ refers to the reward at each step during the rollout, and $T$ is the depth of the simulation, i.e., the number of steps taken until the terminal state or maximum depth is reached.
%\end{enumerate}

%\textbf{Back Up.} Once a terminal state is reached during the simulation, the back up phase updates the values of nodes from the terminal node back to the root. This step ensures that the accumulated rewards and visit counts are propagated upwards through the tree, refining the decision-making process over time: $$Q(s_t,a)\leftarrow r(s_t,a)+\gamma V(s_{t+1})$$ $$V(s_t)\leftarrow\sum_aN(s_{t+1})Q(s_t,a)/\sum_aN(s_{t+1})$$ $$N(s_t)\leftarrow N(s_t)+1$$ where $\gamma$ is the discount factor.

%\ljx{
%In this paper, we describe the multi-llms MCTS procedure employed in our system, adapted to solve sequential decision-making problems under uncertainty, including 2 parts of problem2thought and thought2solution, leveraging probabilistic state transitions and reward signals. Pruning and refinement mechanism are also applied to minimize the influence of wrong thoughts or make some use of them. The overview of our strategies is shown in \ref{fig:copilot}.

%\textbf{Select.} The select phase aims to traverse the decision tree by selecting nodes that balance exploration of unvisited nodes with exploitation of high-value nodes. Starting from the root node, the process iterates through the tree by applying a selection policy. In our implementation, the selection process is governed by the Upper Confidence Bound (UCB) strategy, with a variant of probability-weighted UCB (P-UCB) for better exploration.
%Node selection begins at the root node. It is made based on the node's estimated value and visitation frequency, guided by the UCB formula:
%$$\mathrm{PUCB}(S^T_d)=Q(s)+c\cdot P(a|s_p)\cdot\frac{\sqrt{\log N(s_p)}}{1+N(s)}$$

%where $s$ is the state of the node $S^T_d$, $s_p$ is its parent's state $Q(s_p, a)$ represents the node value, $P(a|s_p)$ is the policy’s probability of selecting action $a$ in state $s_p$, and $N(s)$ is the number of visits to state $s$. The child of the current node with the highest P-UCB value is chosen for the next selection. The selection proceeds until a terminal node or an unvisited node is encountered.

%\textbf{Expand.} Expansion occurs when a previously unvisited state during the selection phase. This step involves adding candidate reasoning nodes as the children of the current selected node. LLMs($\pi_1, \pi_2$) are given the current node $S^T_d$ with state $s$ to generate next thoughts for solving the problem along with the probability$P(a|s)$ of each thought:
%$$a_i, P(a_i|s) \sim \pi_i(·|Q,s, \text{prompt}_{thought})$$

%This is actually the problem2thought process. New generated nodes can be represented by $S^{T_i}_{d+1}$ with state $s_i=s+a_i$, where $T_i = join(T,i)$ and $i$ means that this node is generated by $\pi_i$.

%\textbf{Simulate.}In this operation, LLMs utilizes the selected thoughts to generate solution, i.e. the thought2solution process. Given the selected state, each LLM generates its own version of solution:
%$$So_i\sim \pi_i(·|Q, s)$$
%And we estimate the node reward through the average passrate of the solutions:
%$$R(s)=\frac{1}{n}\sum_{i=1}^n\text{PassRate}(So_i)$$.
%If a terminal condition is met, i.e., if the state reaches the terminal token or exceeds the maximum length, a terminal reward $r(s_t)$ is calculated and $R(s_t)$ is then set to $0$. Otherwise, an intermediate reward of 0 is assigned: $$r(s_t)=\begin{cases}R(s_t)&\text{if terminal,}\\0&\text{otherwise.}\end{cases}$$

%\textbf{Back Up.}After the simulation operation in which we get the node reward, the back up phase updates the values of nodes from bottom to the root. This step ensures that the accumulated rewards and visit counts are propagated upwards through the tree, refining the decision-making process over time:
%$$Q(s_t) \leftarrow f(Q(s_t),r(s_t)+ \gamma Q(s_{t+1}))$$
%$$N(s_t)\leftarrow N(s_t)+1$$
%where states from root to the bottom is represented as $s_0,s_1,...,s_d$, $Q(s_d)=r(s_d)+\gamma R(s_d)$, $f$ is a value calculation function, normally $max$ or $averaging$ all the values that backpropagate, and $\gamma$ is the discount factor.

%\textbf{Prune.}If the pruning mechanism is applied, the new selected node will be examined to show if it is a wrong node. A node is thought to be a wrong node if its reward is less than its parent's, i.e. $R(s_t) < R(s_{t-1})$, which means the new thought brings no useful information. The wrong node will be pruned, so no expansion will be conducted. But \textbf{Back Up} operation is still carried on to reduce the likelihood of selecting this trajectory.

%\textbf{Refine.}After a wrong node is found, we can get its error information and analysis. Then the information can be summarized in natural language so that LLMs can understand it more easily. With the summary, LLMs generate a new thought to replace the past one, which contains information about avoiding current errors. For the sake of debate, wrong thoughts generated by one LLM is analyzed by another LLM. If the new thought is still recognized as a wrong thought, \textbf{Refine} operation is conducted again. But all rethinking times for one problem can't exceed the max thinking times, and each wrong node is regenerated at least once.
%For coding, we utilizes failed test cases and block analysis to generate error summary:
%$$Su(S^{T_i}_d)\sim\pi_j(·|Q,s_d,\text{BlockAnalysis}(s_d))$$
%And the summary and failed test cases are used for refined thought generation:
%$$a_i' \sim \pi_i(·|Q,s_p,Su(S^{T_i}_d),\text{FailedTestCases}(s_d)$$
%Here, only a part of failed test cases are given to the LLM if there are too many. Then, both the rewards of the first wrong thought and the final thought are backpropagated for fair information, and the node left will be expanded.

%\whj{ -------- my draft of Sec4.1 starts from here -------}
In this subsection, we introduce the mechanism design for the data collection process. Due to the complex reasoning nature embodied, code blocks are hard to evaluate and estimate before mature. Reliable reward signals of a reasoning path therefore mainly depend on the dynamic compilation and execution feedbacks, which are extremely sparse and require extensive simulations. To simplify the generation paradigm and exploit the mutual verification capabilities of the collective searching, we decompose the generation process into two distinct stages: problem-to-thought and thought-to-solution.

\subsubsection{Problem-to-thought}

Traditional Monto-Carlo Tree Searching comprises three key operations in each iteration: (a) Select, (b) Expand, (c) Backup. In the problem-to-thought stage, we further extend MCTS by two distinct operations (d) Prune, and (e) Refine to reduce the searching space. We elaborate on these operations as follows.

\minisection{Select} 
Starting from the root, the reasoning path is prolonged by iteratively adding a specific child of the latest node. The operation is usually governed by certain policies, among which we adopt Probability-weighted Upper Confidence Bound(P-UCB) to balance the exploration and exploitation:
\begin{equation}
\mathrm{PUCB}(S_c)=Q(S)+c\cdot P(a|S_p)\cdot\frac{\sqrt{\log N(S)}}{1+N(S_c)},
\end{equation}
where $S_c$ is the state of the child node. $S$ and $Q(S)$ denote the parent node's state and value. $P(a|S)$ is the conditional probability of sampling the sequence $a$. $N(S)$ is the total number of times the parent node $S$ has been visited during simulations, while $N(S_c)$ tracks visits to the child node $S_c$. The selection process will stop if either a semantic or rule-based(e.g. length limits) terminal state encounters.

\minisection{Expand}
The Expand operation is triggered if a non-terminal leaf node of the tree is selected. A set of predefined LLM polices $\pi_0, \cdots, \pi_n$ generate subsequent thought sequences ${a_i}_n$ given the state $S$ of the current node, forming new leaf nodes:
\begin{equation}
    \forall i\in[n], P(a_i|S) \sim \pi_i(·|S).
\end{equation}

%following sequence concatenation as a deterministic transition function $S^i_c = S + a_i$ and question prompt as the initial state, the newly included leaf nodes are set with states ${S_i_c}$.


\minisection{Backup}
For well-defined problems, a reasoning path ${S_t}$ will eventually end at a terminal leaf node $S_T$ by iterating the Select and Expand operations. The reward $r_T$ is set according to the evaluation. We will skip the definition of reward $r_t$ and passrate $PR(S_t)$, which will be detailed in the explanation of the Simulate operation. The reward value is back-propagated along the reasoning path to update the state values of corresponding ancestor nodes:
\begin{equation}
    Q(S_{t-1}) = f(Q(S_t), r_t + \gamma PR(S_t)),
\end{equation}
where $f$ is the value function.

Additionally, the visit counts of ancestors are updated alongside the reasoning path:
\begin{equation}
    N(S_t) = N(S_t) + 1.
\end{equation}



We further extend and formalize reflective reason settings proposed in CoMCTS into Prune and Refine operations as shown in Figure~\ref{fig:mc}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/datacollection.pdf}
    \caption{Pruning and refinement operations.}
    \label{fig:mc}
\end{figure}

\minisection{Pruning}
The pruning operation on a selected node will examine and compare its passrate with that of its parent. With powerful LLMs, we consider valid and reasonable thoughts to bring non-negative influence solution seeking, thus featuring monotonically non-decreasing values in the passrate $PR(S_t) <= PR(S_{t+1})$.

A child node alleviating this principle will be considered as an ill node that introduces wrong thoughts. The ill node will be removed and trigger an instant Backup operation with zero reward to downweight its ancestors.


\minisection{Refine}
The truncated error and state information left by ill nodes will be analyzed in the Refine operations. To mitigate the bootstrapping bias of LLMs, a distinct policy LLM will be adopted to infer and summarize the information in natural language, which will be later utilized to refine and replace the ill nodes:
\begin{align}
    isIll(S^{\pi_i})&== 1,\nonumber\\
    Summary(S^{\pi_i}) &\sim \pi_{j}(Q(S^{\pi_i}),\\
    S^{\pi_i}, & BlockAnalysis(S^{\pi_i})),\nonumber
\end{align}
where $S^{\pi_i}$ denotes a ill node generated by $\pi_i$. A refined node is generated to replace the ill one:
\begin{equation}
    a' \sim \pi_{i}(Q(S^{\pi_i}), Summary).
\end{equation}

We enforce global and local constraints on possible times of calling Refine operation to avoid infinite loops and balance performance with compute budgets. A successful Refine operation will cause an in-place replacement of the ill-node, triggering another Backup operation to re-weight its ancestors.


\subsubsection{Thought-to-solution}

\minisection{Simulate}
For the thought-to-solution, we repurpose the Simulate operation for the collective solution generation process from the given state $S$. The operation will produce a set of possible solutions, each from a policy LLM:
\begin{equation}
    Solut.(S)_i \sim \pi_i(S).
\end{equation}

We define the passrate of a state as the average passrate of its corresponding solutions:
\begin{equation}
PR(S) = \frac{1}{n}\sum_{i}^{n} Passed(Solut.(S)_i),
\end{equation}
where $Passed(\cdot)$ represents the supervising signal from dynamic compilation and execution feedback.

The node's value $Q(S_t)$ is determined by its $PR(S_t)$ and reward $r_t$. Sincere additional solution string will be appended to a non-terminal state $S_t$ before evaluation, $PR(S_t)$ is an indirect supervising signal for the $S_t$, and the direct signal $r_t$ is set to zero.

The terminal state $S_T$ is treated as the unique solution itself since no string concatenation applies, therefore featuring a non-trivial reward $r_T$. Putting everything together, we have:
\begin{equation}
    Q(S) =\begin{cases} r_T &\text{if terminal,}\\\gamma PR(S_t) &\text{otherwise.}\end{cases}
\end{equation}

%\whj{ -------- my draft of Sec4.1 ends from here -------}
%}

\subsection{System2-to-System1 Training}
\subsubsection{Heterogeneous Distribution Disentanglement}
\label{sec:dis}
After the data collection, the resulting training data obtained from the  MC-Tree-Of-Agents process consists of problem2thought data $D^{p2t}=\{\langle X_i^{p2t},y_i^{p2t}\rangle| i\in\mathbf{P}\}$ and thought2solution data $D^{t2s}=\{\langle X_i^{t2s},y_i^{t2s}\rangle| i\in\mathbf{P}\}$: $D_{train} = \{D^{p2t}, D^{t2s}\}$. As discussed in the introduction section, the latent patterns of coding problems are complex and tend to be heterogeneously distributed, e.g., the branching and recursion flow existing in the code blocks, different strategies of algorithm solutions, etc. Therefore, we disentangle the training data based on the latent semantics of the data into different clusters for fine-grained modeling.

The clustering objective can be summarized as below:
\begin{align}
    minimize_{\mathcal{C}} &\sum_k\sum_{i\in C_k} cosine(e_i, \mu_k), \\
    e_i = &\Phi_\theta (\langle X_i, y_i \rangle),\nonumber \\ 
    \mu_k = &mean\{e_i | i\in C_k\},\nonumber
\end{align}
where $\Phi_{theta}$ is the encoder of a code llm and $\mu_k$ denotes the centroid of cluster $C_k$.

\subsubsection{Composable LoRA Experts Preparation}
Having obtained the disentangled data clusters, we then finetune on them to obtain the meta LoRA experts for specialized experts of different aspects.
\begin{align}
    \forall C_k &\in \mathcal{C}, \nonumber\\
    \pi_{\theta_k} & \leftarrow SFT(\pi_{\theta}, \{\langle X_i, y_i\rangle | i \in C_k\}),
\end{align}
where $\pi_\theta$ denotes the base LLM and $\pi_{\theta_k}$ denotes the parameters of the LoRA adapter obtained by finetuning $\pi_\theta$ on $C_k$.

\subsubsection{Input-Aware Hypernetwork for Customized Solver}

Given specialized LoRA experts ${\pi_{\theta_1},\cdots,\pi_{\theta_K}}$ trained on distinct data clusters, we design an input-aware Hypernetwork $f(\cdot)$ to dynamically compose these experts through rank-wise adaption for customized problem solver.

For each input instance, the hypernetwork generates customized expert weights digesting its encoding and semantic distances to the cluster centroids. we identify "rank" as the minimal unit for aggregation and generate rank-wise weights for different experts at each decoding layer:
\begin{equation}
    G_i \leftarrow f(e_i, \langle cosine(e_i, \mu_1), \dots, cosine(e_i, \mu_K)\rangle),
\end{equation}
where $e_i$ is the encoding of input $X_i$, $G_i\in R^{K\times r\times 1}$ is the output weight matrix, $r$ is the rank of the LoRA matrices, and $K$ is the number of LoRA experts.

The aggregated $\Delta W$ of the linear projection layer is then obtained by
\begin{align}
    \mathbf{A}^* = [A_1, \dots, A_K] \odot G_i, \\
    \mathbf{\Delta W}^* = [B_1A_1^*, \dots, B_KA_K^*],\\
    \Delta W = ReduceSum(\mathbf{\Delta W}^*).
\end{align}

%To keep the magnitude of the customized $\Delta W$ from collision, we regularize the obtained $\Delta W$ using an anchored matrix aggregated using distances from each centroid.
%\begin{equation}
%    \Delta W = \Delta W \frac{||\Delta W_{anchor}||}{||\Delta W||},
%\end{equation}
%where $\Delta W_{anchor}$ is the distance-weighted matrix over the LoRA-experts using the distance of input to each centroid and $||\cdot||$ denotes the infinity norm. 

% The resulting $\Delta W$ is then merged into the original weight matrix for forwarding:
The projection output of $\Delta W$ is then merged during forwarding via:
\begin{equation}
    y= W_0x+\Delta Wx.
\end{equation}

%During the process, only $f(\cdot)$ is trainable while other parameters are frozen. 
We adopt a dedicated training phase for the Hypernetwork where all parameters are frozen except for the $f(\cdot)$. The training is supervised by the cross-entropy loss, with the randomly permuted input-output pairs from $D_{train}$.



\section{Experiments}
We conduct empirical studies starting from the following research questions.
\begin{itemize}[leftmargin=27pt]
    \item [\textbf{RQ1}] Does the proposed data collection algorithm explore insightful reasoning knowledge?
    \item [\textbf{RQ2}] Do the complex latent patterns of reasoning data impact the training performance?
    \item [\textbf{RQ3}] Can the disentangle-and-compose mechanism help to promote performance?
    \item [\textbf{RQ4}] Do the proposed input-aware hypernet work outperform other model composition techniques?
    %Does the proposed \modelname model the high-level thought-of-codes? Can \modelname offer cross-lingual augmentation?
    \item [\textbf{RQ5}] How does DisenLoRA perform on untrained datasets?
\end{itemize}

%\whj{Possible experiment data that can directly show the importance of disentanglement: (1) Different models show diverse performance in different stages(p2t, t2s). (2) Models perform better in verifying decomposed thoughts and solutions.}

\subsection{Setup}
In this section, we provide detailed setup information for the evaluation of the proposed \modelname, including datasets, trajectory data collection, and competing methods. 

The overall evaluation is conducted on two benchmark datasets: the competition-style APPS dataset and the CodeContest dataset. Both datasets categorize problems from easy to hard. We randomly sample problem subsets from each category of these two datasets. Each subset contains approximately 100 problems, except for the CodeContest-Hard category, which consists of around 50 problems due to inherent limitation in size. 

We conduct isolated assessments of both stages of \modelname to ensure a comprehensive comparison. 

\minisection{ Data collection} For Python code generation, we compare the performance of MCTS over different methods: zeroshot, LDB \cite{zhong2024ldb}, RAP \cite{hao2023reasoning}, Reflexion, LATS \cite{zhou2023language}, ToT and RethinkMCTS \cite{li2024rethinkmcts}. To mitigate the influence of factors such as context window limitations and instruction-following capabilities, we employ two advanced base models: GPT-4o-mini and Claude-3.5-Sonnet. Aligned with the purpose, we adopt a greedy decoding strategy for both models. Additionally, we provided peer comparisons between these two base models when driven by the MC-Tree-Of-Agents method in terms of their error position and refinement capability.

\minisection{ Fine-tuning} For fine-tuning, \modelname is compared against several alternative methods, including SFT on clustered subsets, TIES, DARES, and TWINS \cite{liu2023twins}.

%\whj{In this section, we provide detailed setup information for evaluating of the proposed \modelname(ours), including datasets, trajectory data collection and competeting methods. 

%The overall evaluation is conducted on two benchmark datasets: the competition-style APPS dataset and the CodeContest dataset. Both datasets categorize problems from easy to hard. We randomly sample problem subsets from each category of these two datasets. Each subset contains approximately 100 problems, except for the CodeContest-Hard category, which consists of around 50 problems due to inherent limitation in size. 

%We conduct isolated assessments of both stages of \modelname to ensure a comphensive comparsion. 

%\minisection{ Data collection} For Python code generation, we compare the perfomance of MCTS over different methods: zeroshot, LDB\cite{zhong2024ldb}, RAP\cite{hao2023reasoning}, Reflexion, LATS\cite{zhou2023language}, ToT and RethinkMCTS\cite{li2024rethinkmcts}. To mitigate the influence of factors such as context window limitations and instruction-following capabilities, we employ two advanced base models: GPT-4o-mini and Claude-3.5-Sonnet. Aligned with the purpose, we adopt greedy decoding strategy(temperature and other args?) for both models. Additionally, we provided peer comparsions between these two base models when driven by the MC-Tree-Of-Agents method in terms of their error position and refinement capability.

%\minisection{ Fine-tuning} For fine-tuning, \modelname is compared against several alternative methods, including SFT on clustered subsets, TIES, DARES and TWINS\cite{liu2023twins}.}

%In this paper, we evaluate \modelname with the competition-style APPS dataset and the CodeContest dataset. Both datasets are categorized by different difficulty levels. For each level of every dataset, we select\whj{random sample?} about 100 problems for testing, except for CodeContest Hard category, which contains about 50 problems. Greedy decoding strategy is applied to the generation. The evaluation metric is PassRate(PR) and Accuracy(AC).

%For data collection, we evaluate the python code generation abilities of GPT4omini and Claude-3.5-sonnet with different methods: zeroshot, LDB\cite{zhong2024ldb}, RAP\cite{hao2023reasoning}, Reflexion, LATS\cite{zhou2023language}, ToT, MCTS and RethinkMCTS\cite{li2024rethinkmcts}. Based on MC-Tree-Of-Agents, we also evaluate both models with error position and refinement. For tuning, we evaluate different finetuning methods including SFT on different clusters, TIES, DARES, TWINS\cite{liu2023twins} and \modelname(ours).

\begin{table*}[ht]
\centering
\caption{Main results on System 2 knowledge exploration.}
\label{tab:datacollection}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{c|cccccc|cccc} 
\hline
& \multicolumn{6}{c|}{APPS} & \multicolumn{4}{c}{CodeContest}\\
       Models                 & \multicolumn{2}{c}{Intro.}                            & \multicolumn{2}{c}{Inter.} & \multicolumn{2}{c|}{Comp.} & \multicolumn{2}{c}{Easy} & \multicolumn{2}{c}{Hard} \\ \cline{2-11} 
                        & PR                     & AC                    & PR       & AC       & PR       & AC       & PR          & AC         & PR          & AC         \\ \hline
ZeroShot                & 56.56 & 35.00 &      40.57           &     19.00          &   23.67              &     9.00          &29.03& 19.61 &   28.24                 &19.23              \\
LDB                     & 60.64 & 40.00 &     46.78            &     22.00          &      21.00           &    8.00           & 34.76              & 25.58           & 33.52                   & 16.28                \\
RAP                     & 64.24                         & 39.00                         &   43.32              &     14.00          &         22.83        &    8.00           & 43.08              & 33.33            &39.99  &26.92                 \\
Reflexion               & 60.65                         & 40.00                         &  45.58               &     21.00          &      17.50           &     7.00          & 56.16              & 47.83           &34.09 &21.15                 \\
LATS                    & 69.46                         & 50.00                         &  45.86               &     20.00          &       21.83          &        7.00       &57.70              & 47.83           &39.10                    &30.77                 \\
ToT                     & 74.34                         & 55.00                         &  63.49               &     33.00          &         26.30        &       11.00        & 51.89              & 41.18           &49.07                    & 32.69                \\ 
RethinkMCTS                     & 76.60                         & 59.00                        &  74.35               &     49.00         &         42.50        &       28.00        & 60.84             & 51.53          & 55.79                & 48.04               \\ \hline
%MCTS-Line               &                               &                            &                 &               &                 &               &                    &                 &                    &                 \\
%MCTS-Thought            &                               &                            &                 &               &                 &               &                    &                 &                    &                 \\ \hline
Single (GPT4omini) & 77.99                         & 60.00                         &     72.89            &      50.00            &      44.17         &      25.00     & 55.79     &48.04                &      45.72              & 26.92                \\
%Single-MCTS (Yi)        & 70.92                         & 53.75                      &                 &               &                 &               &                    &                 &                    &                 \\
Single (Claude)    &  73.80  &  61.00   &  73.60   & 57.00 &      54.67           &      42.00         &                  58.75& 53.92                &68.41          &55.76             \\ \hline
MC-Tree-Of-Agents              & 79.72                         & 64.00                      &       79.42          &    63.00           &       59.17          &    45.00           & 62.49               &54.64             & 70.49                   &56.41                 \\ 
%+ Error Position (v1)           & 83.07                         & 69.00                      &       78.65          &    64.00           &       57.67          &    41.00           & 65.92               &58.82             & 70.14                   &51.92                 \\
%+ Refine   (v1)        & 83.24                         & 72.00                      &       79.65          &    63.00           &       55.83         &    38.00           & 62.73               &52.94             & 66.95                   &50.00                 \\
+ Pruning           & 85.18                         & 76.00                     & 81.95                & 67.00             &54.00              & 38.00            & 64.62            &  59.80         &73.12             &59.62                 \\
+ Refine      & 81.29                         & 68.00                    & 78.85              & 62.00            & 60.33             & 44.00            & 63.23             &  56.86          & 73.80                   &63.46\\                 \hline
\end{tabular}
}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[ht]
\centering
\caption{Main results on System2-to-System1 tuning.}
\label{tab:tuning}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccccccc}
\hline
\multicolumn{15}{c}{ Meta-llama-3.1-instruct-8b}                                                                                                                                                                                                                  \\ \hline
\multicolumn{1}{c|}{}                                         & \multicolumn{2}{c}{Intro. (100)} & \multicolumn{2}{c}{Inter. (100)} & \multicolumn{2}{c|}{Comp. (100)}  & \multicolumn{2}{c|}{Overall}      & \multicolumn{2}{c}{Easy (102)} & \multicolumn{2}{c|}{Hard (51)}     & \multicolumn{2}{c}{Overall} \\ \cline{2-15} 
\multicolumn{1}{c|}{\multirow{-2}{*}{Finetune Method}}        & PR              & AC             & PR              & AC             & PR    & \multicolumn{1}{c|}{AC}   & PR    & \multicolumn{1}{c|}{AC}   & PR             & AC            & PR    & \multicolumn{1}{c|}{AC}    & PR           & AC           \\ \hline
\multicolumn{1}{c|}{w/o tuning}                               & 21.14           & 4.00           & 20.72           & 4.00           & 12.83 & \multicolumn{1}{c|}{1.00} & 18.23 & \multicolumn{1}{c|}{3.00} & 25.54          & 17.65         & 15.46 & \multicolumn{1}{c|}{5.77}  & 22.18        & 13.69        \\ \hline
\multicolumn{1}{c|}{SFT on all}                               & 22.55           & 7.00           & 26.40           & 3.00           & 10.67 & \multicolumn{1}{c|}{1.00} & 19.87 & \multicolumn{1}{c|}{3.67} & 25.33          & 17.65         & 16.73 & \multicolumn{1}{c|}{7.69}  & 22.46        & 14.33        \\ \hline
\multicolumn{1}{c|}{ SFT on cluster 0} & 20.67           & 6.00           & 24.23           & 3.00           & 11.50 & \multicolumn{1}{c|}{1.00} & 18.80 & \multicolumn{1}{c|}{3.33} & 27.31          & 17.65         & 11.69 & \multicolumn{1}{c|}{1.92}  & 22.10        & 12.41        \\
\multicolumn{1}{c|}{ SFT on cluster 1} & 21.22           & 4.00           & 20.69           & 4.00           & 12.00 & \multicolumn{1}{c|}{2.00} & 17.97 & \multicolumn{1}{c|}{3.33} & 27.78          & 20.59         & 18.12 & \multicolumn{1}{c|}{9.62}  & 24.56        & 16.93        \\
\multicolumn{1}{c|}{SFT on cluster 2}                         & 16.65           & 7.00           & 23.97           & 3.00           & 17.33 & \multicolumn{1}{c|}{4.00} & 19.32 & \multicolumn{1}{c|}{4.67} & 26.82          & 20.59         & 19.50 & \multicolumn{1}{c|}{9.62}  & 24.38        & 16.93        \\ \hline
\multicolumn{1}{c|}{Ties}                                     & 22.75           & 4.00           & 23.06           & 4.00           & 12.67 & \multicolumn{1}{c|}{4.00} & 19.49 & \multicolumn{1}{c|}{4.00} & 26.64          & 21.57         & 18.71 & \multicolumn{1}{c|}{9.62}  & 24.00        & 17.59        \\
\multicolumn{1}{c|}{Dare}                                     & 24.97           & 7.00           & 26.66           & 5.00           & 12.50 & \multicolumn{1}{c|}{3.00} & 21.38 & \multicolumn{1}{c|}{5.00} & 23.05          & 13.73         & 19.65 & \multicolumn{1}{c|}{15.38} & 21.92        & 14.28        \\
\multicolumn{1}{c|}{Twin}                                     & 19.10           & 5.00           & 23.85           & 5.00           & 8.50  & \multicolumn{1}{c|}{1.00} & 17.15 & \multicolumn{1}{c|}{3.67} & 26.87          & 17.64         & 12.92 & \multicolumn{1}{c|}{9.62}  & 22.22        & 14.97        \\ \hline
\multicolumn{1}{c|}{DisenLoRA}                & \textbf{27.11}           & \textbf{9.00}           & 23.11           & 3.00           & 11.50 & \multicolumn{1}{c|}{\textbf{4.00}} & \textbf{20.57} & \multicolumn{1}{c|}{\textbf{5.33}} & \textbf{32.24}          & \textbf{22.55}         & 19.43 & \multicolumn{1}{c|}{9.62}  & \textbf{27.97}        & \textbf{18.24}        \\ \hline
\end{tabular}
}
\end{table*}

\subsection{Empirical Analysis and Discussion}
\subsubsection{\textbf{RQ1}. MC-Tree-Of-Agents}

We evaluate MC-Tree-Of-Agents against widely-used baseline methods, the results are summarized in Table~\ref{tab:datacollection}.
From the results, we can draw the following conclusions. 
\begin{itemize}[leftmargin=10pt]
    \item The proposed MC-Tree-Of-Agents outperforms all the baseline methods, which effectively explores the insightful  System 2 knowledge. 
    \item Comparing with the single LLM as agents version, MC-Tree-Of-Agents allows for mutual verification and boosting between different LLMs, offering a superior performance over each distinct-LLM-as-agent method. This showcases the effectiveness of the interaction between LLMs of different wisdom.
    \item The pruning and refinement operations both contribute to the final performance, offering a notable accuracy gain. This validates that the designed pruning and refinement mechanism, based on the difference between rewards of parent-child nodes, can save the algorithm from erroneous exploration and lead to beneficial directions in limited rollouts.
\end{itemize}


\subsubsection{\textbf{RQ2}. Impact of latent patterns}

To study the distribution of the latent patterns of coding problems, we first conduct the T-SNE visualization on the encodings of reasoning data collected by MC-Tree-Of-Agents on APPS dataset. The visualization is displayed in Figure~\ref{fig:t-sne}.
%\vspace{-10pt}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/apps_tsne.png}
    \caption{T-sne visualization of the APPS data encoding.}
    \vspace{-10pt}
    \label{fig:t-sne}
\end{figure}

From the visualization, we can see that there different clusters of data distributions existing in the latent reasoning semantic space, which poses a potential challenge to robust and generalizable LLMs on code.

Furthermore, we perform finetuning on different clusters of data obtained in Section~\ref{sec:dis} and evaluate the corresponding models on the test data. From the results in Table~\ref{tab:tuning}, we can see the following conclusions. 1) LLMs finetuning on all the clusters can offer better performance than that of the non-tuning version, validating the quality of the collected System2 knowledge data. 2) Llm experts obtained from different clusters show different performances on different levels of tasks. One expert can demonstrate outstanding capability on one level of tasks, even outperforming the LLM finetuning on all the data, while performing weakly on a different level of task. This phenomenon further justifies the heterogeneous latent patterns of data distribution and serves as supportive evidence for disentangling LLM experts.

\subsubsection{\textbf{RQ3}. Effectiveness of the Experts Composition}

During the empirical study, we test different model merging methods that combine wisdom from different LoRA experts. We evaluate the well-known Ties, Dare, and the recently proposed TWIN merging methods. All of them yield a static composed model that takes in the strength of the candidate experts to be merged via solving parameter interference. From the results, we can see that merging over decomposed LoRA-experts can offer more robust problem solvers, outperforming the simple train-once-for-all mechanism. The experiments justify our major rationale that disentanglement-and-compose pipeline can offer more robust System2-to-System1 performance.

\subsubsection{\textbf{RQ4}. Superiority of DisenLoRA over other composition methods}

Although the static-composed expert model can promote robustness to some extent, its static nature lacks flexibility to different styles of inputs. As discussed in the previous contents, the data distribution of coding problems is complex, making the one-fits-all mechanism easy to fail. Therefore, we design DisenLoRA algorithm to yield a customized problem solver with input-awareness. From the results, we can see that  DisenLoRA outperforms the competing merging methods, validating the effectiveness of the proposed input-aware hypernetwork that dynamically aggregates the candidate composable LoRA experts at a rank-wise level.

\subsubsection{\textbf{RQ5}. Discussion of the Cross-Dataset Generalization of DisenLoRA}

Despite the flexibility offered by the input-aware hypernetwork, its performance may degrade on new datasets where the hypernetwork is not trained. To study this scenario, we use the model trained on APPS to generate solutions for CodeContest and use the model trained on CodeContest to generate solutions for APPS. The results are displayed in Table~\ref{tab:ood4code}.

\begin{table}[h]
    \centering
    \caption{Cross-dataset generalization test.}
    \label{tab:ood4code}
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{c|cc|cc}
    \hline
        OOD Dataset & \multicolumn{2}{c|}{APPS}&\multicolumn{2}{c}{CodeContest}\\\hline
      Method   & PR & AC &PR & AC\\
      \hline
      w/o tuning   & 18.23 &	3.00 & 22.18 &	13.69 \\
      w/ SFT & 17.44 &	4.33 & 20.99	&14.29 \\
      \hline
      DisenLoRA & 18.25 &	4.33& 25.09&14.34\\ \hline
    \end{tabular}
    }
\end{table}

From the results, we can see that the proposed DisenLoRA has the generalization ability to the untrained dataset, outperforming the train-once-for-all mechanism still. This demonstrates that the parameters of the trained hypernetwork have the awareness of semantic similarities across datasets.


\section{Conclusion}
We identify the complexity of inherent reasoning exploration and the heterogeneous data distribution problems that hinder the performance of System2-to-System1 methods. Correspondingly, we propose the BDC pipeline that explores insightful System2 knowledge via mutually \textbf{B}oosting between llm agents, \textbf{D}isentangle heterogeneous data distribution for composable LoRA experts, and \textbf{C}ustomize problem solver for each instance, offering flexibility and robustness.
Correspondingly, we propose the MC-Tree-Of-Agents algorithm to efficiently and effectively explore the insightful System2 knowledge via mutual verification and boosting of different LLM agents, armed with reward-guided pruning and refinement to explore more beneficial states in limited rollouts for better performance. 
Additionally, we design an input-aware hypernetwork to aggregate over the disentangled composable LoRA experts trained on different clusters of data collected from MC-Tree-Of-Agents. This mechanism offers a customized problem solver for each data instance.
Various experiments and discussions validate the effectiveness of different model components.

\section*{Limitations}
%\whj{
While our work presents an efficient pipeline for transferring specialized knowledge from collective system-2-like LLMs to locally deployed language models through multiple LoRA adapters—enabling rapid, precise, system-1-like reasoning—three limitations merit discussion. First, despite code generation serving as an effective proxy for complex reasoning, our evaluation is restricted to this domain, leaving open questions about generalizability to broader textual reasoning tasks (e.g., commonsense reasoning and semantic parsing). Second, while we focus on their performance on the specific benchmarks, the safety alignment of derived models remains unaddressed. Systematic evaluation is required to assess whether our distilled experts preserve human values and mitigate harmful outputs. Finally, our ensemble methodology for LoRA experts, while input-aware, does not fully exploit potential sparsity optimizations in parameter activation, leaving room for computational efficiency improvements through advanced routing mechanisms.
%}

\section*{Acknowledgments}

This document has been adapted by Emily Allaway from the instructions for earlier ACL and NAACL proceedings, including those for NAACL 2024 by Steven Bethard, Ryan Cotterell and Rui Yan,
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan,
NAACL 2017 by Margaret Mitchell,
ACL 2012 by Maggie Li and Michael White,
ACL 2010 by Jing-Shin Chang and Philipp Koehn,
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
ACL 2002 by Eugene Charniak and Dekang Lin,
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.


\bibliography{custom}

\appendix
%\input{latex/sections/impl}

% \section{Methodology}

% \begin{figure*}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/DebateThinker.pdf}
%     \caption{Illustration of the overall framework of \modelname.\dk{Todo: replace it.}}
%     \label{fig:copilot}
% \end{figure*}

% \subsection{Data collection}
% In this paper, we describe the MCTS procedure employed in our system, adapted to solve sequential decision-making problems under uncertainty, leveraging probabilistic state transitions and reward signals.

% \textbf{Select.} The select phase aims to traverse the decision tree by selecting nodes that balance exploration of unvisited nodes with exploitation of high-value nodes. Starting from the root node, the process iterates through the tree by applying a selection policy. In our implementation, the selection process is governed by the Upper Confidence Bound (UCB) strategy, with a variant of probability-weighted UCB (P-UCB) for better exploration.
% Each node is either a DecisionNode (representing agent decisions) or a ChanceNode (representing state-action transitions). When at a DecisionNode, the node selection is made based on the node's estimated value and visitation frequency, guided by the UCB formula:
% $$\mathrm{PUCB}(node)=Q(s,a)+c\cdot P(a|s)\cdot\frac{\sqrt{\log N(s)}}{1+N(s,a)}$$

% where $Q(s, a)$ represents the action value, $P(a|s)$ is the policy’s probability of selecting action $a$ in state $s$, and $N(s)$ is the number of visits to state $s$. The selection proceeds until a terminal node or an unvisited node is encountered.

% \textbf{Expand.} Expansion occurs when a previously unvisited state or a ChanceNode is reached during the selection phase. This step involves adding a new DecisionNode to the tree. In our implementation, when a ChanceNode transitions to a new state $s_t$ after an action $a$, the reward $r(s_t ,a)$ is computed. The reward evaluates the correctness of the state $s_t$ based on the output's pass rate, which is quantified by the evaluation function $R(s_t)$, as seen in the equation:
% $$R(s_t)=\mathrm{PassRate}(s_t)$$
% In our setup, the PassRate is determined by the proportion of correct outputs as evaluated by a code correctness checker. Additionally, future rewards are calculated if the node reaches a terminal state, incorporating the step rewards from the current node to future nodes through rollouts. This is expressed as:
% $$r(s_t,a)=R(s_t)+\gamma\cdot\sum_{i=1}^Tr(s_{t+i},a)$$
% Here, $\gamma$ represents the discount factor applied to future rewards during backpropagation, while the total reward is a combination of the current reward and discounted future rewards. The expansion step also adds new DecisionNodes to the tree, allowing subsequent actions to be explored.

% \textbf{Simulate (Rollout).} The Simulate phase, also known as Rollout, begins after a ChanceNode has expanded. In this phase, a sequence of actions is sampled from the current policy, and the rewards are accumulated until a terminal state is reached or the maximum rollout depth is achieved. Each simulation (rollout) evaluates a potential trajectory from the current state, allowing the model to estimate future rewards. The rollout process can be expressed as: 
% \begin{enumerate}
%   \item For each state $s_t$  and action $a$, the state transitions to $s_{t+1}$ according to the transition function $f(s_t, a)$, which is defined as: $$s_{t+1}=f(s_t,a)=s_t+a$$ where $s_t$ is the prompt already generated before the action and $a$ is the continuing prompt (thought).
%   \item If a terminal condition is met, i.e., if the state reaches the terminal token or exceeds the maximum length, a terminal reward $R(s_t)$ is calculated based on the correctness of the output. Otherwise, an intermediate reward of 0 is assigned: $$r(s_t,a)=\begin{cases}R(s_t)&\text{if terminal,}\\0&\text{otherwise.}\end{cases}$$
%   \item The overall reward for the rollout is the sum of rewards across all steps, discounted by $\gamma$ (the discount factor for future rewards): $$\mathrm{RolloutReward}(s_t)=\sum_{i=0}^T\gamma^i\cdot r(s_{t+i},a)$$ Here, $r(s_{t+i} ,a)$ refers to the reward at each step during the rollout, and $T$ is the depth of the simulation, i.e., the number of steps taken until the terminal state or maximum depth is reached.
% \end{enumerate}

% \textbf{Back Up.} Once a terminal state is reached during the simulation, the back up phase updates the values of nodes from the terminal node back to the root. This step ensures that the accumulated rewards and visit counts are propagated upwards through the tree, refining the decision-making process over time: $$Q(s_t,a)\leftarrow r(s_t,a)+\gamma V(s_{t+1})$$ $$V(s_t)\leftarrow\sum_aN(s_{t+1})Q(s_t,a)/\sum_aN(s_{t+1})$$ $$N(s_t)\leftarrow N(s_t)+1$$ where $\gamma$ is the discount factor.

% \subsection{Disentangled Lora Training}

% \subsection{Input-Aware Hypernetwork}

% \section{Experiment}
% \begin{itemize}[leftmargin=27pt]
%     \item [\textbf{RQ1}] Does the proposed \modelname offer performance gain against the base model?
%     \item [\textbf{RQ2}] Does the disentanglement help in training?
%     \item [\textbf{RQ3}] Can soft prompting enhance the capability of the backbone LLMs? Does finetuning with the soft prompting outperforms the simple supervised finetuning?
%     \item [\textbf{RQ4}] Are the proposed pretraining objectives for the GNN expert effective?
%     %Does the proposed \modelname model the high-level thought-of-codes? Can \modelname offer cross-lingual augmentation?
%     \item [\textbf{RQ5}] What is the impact of each of the components of the graphical view?
%     \item [\textbf{RQ6}] How is the compatibility of the graphical view? 
% \end{itemize}

% \subsection{Setup}
% In this paper, we evaluate \modelname with the competition-style APPS dataset and the CodeContest dataset. Both datasets are categorized by different difficulty levels. For each level of every dataset, we select about 100 problems for testing, except for CodeContest Hard category, which contains about 50 problems. Greedy decoding strategy is applied to the generation. The evaluation metric is PassRate(PR) and Accuracy(AC).

% For data collection, we evaluate the python code generation abilities of GPT4omini and Claude-3.5-sonnet with different methods: zeroshot, LDB\cite{zhong2024ldb}, RAP\cite{hao2023reasoning}, Reflexion, LATS\cite{zhou2023language}, ToT, MCTS and RethinkMCTS\cite{li2024rethinkmcts}. Based on multi-MCTS, we also evaluate both models with error position and refinement. For tuning, we evaluate different finetuning methods including SFT on different clusters, TIES, DARES, TWINS\cite{liu2023twins} and \modelname(ours).
% \begin{table*}[ht]
% \centering
% \caption{Main results on data collection.}
% \label{tab:code}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{c|cccccc|cccc} 
% \hline
% & \multicolumn{6}{c|}{APPS} & \multicolumn{4}{c}{CodeContest}\\
%        Models                 & \multicolumn{2}{c}{Intro.}                            & \multicolumn{2}{c}{Inter.} & \multicolumn{2}{c|}{Comp.} & \multicolumn{2}{c}{Easy} & \multicolumn{2}{c}{Hard} \\ \cline{2-11} 
%                         & PR                     & AC                    & PR       & AC       & PR       & AC       & PR          & AC         & PR          & AC         \\ \hline
% ZeroShot                & 56.56 & 35.00 &      40.57           &     19.00          &   23.67              &     9.00          &29.03& 19.61 &   28.24                 &19.23              \\
% LDB                     & 60.64 & 40.00 &     46.78            &     22.00          &      21.00           &    8.00           & 34.76              & 25.58           & 33.52                   & 16.28                \\
% RAP                     & 64.24                         & 39.00                         &   43.32              &     14.00          &         22.83        &    8.00           & 43.08              & 33.33            &39.99  &26.92                 \\
% Reflexion               & 60.65                         & 40.00                         &  45.58               &     21.00          &      17.50           &     7.00          & 56.16              & 47.83           &34.09 &21.15                 \\
% LATS                    & 69.46                         & 50.00                         &  45.86               &     20.00          &       21.83          &        7.00       &57.70              & 47.83           &39.10                    &30.77                 \\
% ToT                     & 74.34                         & 55.00                         &  63.49               &     33.00          &         26.30        &       11.00        & 51.89              & 41.18           &49.07                    & 32.69                \\ 
% RethinkMCTS                     & 76.60                         & 59.00                        &  74.35               &     49.00         &         42.50        &       28.00        & 60.84             & 51.53          & 55.79                & 48.04               \\ \hline
% %MCTS-Line               &                               &                            &                 &               &                 &               &                    &                 &                    &                 \\
% %MCTS-Thought            &                               &                            &                 &               &                 &               &                    &                 &                    &                 \\ \hline
% Single (GPT4omini) & 77.99                         & 60.00                         &     72.89            &      50.00            &      44.17         &      25.00     & 55.79     &48.04                &      45.72              & 26.92                \\
% %Single-MCTS (Yi)        & 70.92                         & 53.75                      &                 &               &                 &               &                    &                 &                    &                 \\
% Single (Claude)    &  73.80  &  61.00   &  73.60   & 57.00 &      54.67           &      42.00         &                  58.75& 53.92                &68.41          &55.76             \\ \hline
% Multi-MCTS              & 79.72                         & 64.00                      &       79.42          &    63.00           &       59.17          &    45.00           & 62.49               &54.64             & 70.49                   &56.41                 \\ 
% + Error Position (v1)           & 83.07                         & 69.00                      &       78.65          &    64.00           &       57.67          &    41.00           & 65.92               &58.82             & 70.14                   &51.92                 \\
% + Refine   (v1)        & 83.24                         & 72.00                      &       79.65          &    63.00           &       55.83         &    38.00           & 62.73               &52.94             & 66.95                   &50.00                 \\
% + Error Position (v2)           & 85.18                         & 76.00                     & 81.95                & 67.00             &54.00              & 38.00            & 64.62            &  59.80         &73.12             &59.62                 \\
% + Refine   (v2)        &                          &                     &               &             & 60.33             & 44.00            &              &            & 73.80                   &63.46\\                 \hline
% \end{tabular}
% }
% \end{table*}

% % Please add the following required packages to your document preamble:
% % \usepackage[table,xcdraw]{xcolor}
% % Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% % Please add the following required packages to your document preamble:
% % \usepackage[table,xcdraw]{xcolor}
% % Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% \begin{table*}[ht]
% \centering
% \caption{Main results on \modelname tuning.}
% \label{tab:tuning}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{ccccccccccccccc}
% \hline
% \multicolumn{15}{c}{\cellcolor[HTML]{EFEFEF}{Meta-llama-3.1-instruct-8b}}\\ \hline
% \multicolumn{1}{c|}{}                                         & \multicolumn{8}{c|}{IID Dataset (APPS Test)}                                                                                               & \multicolumn{6}{c}{OOD Dataset (CodeContest)}                                                     \\ \cline{2-15} 
% \multicolumn{1}{c|}{}                                         & \multicolumn{2}{c}{Intro.   } & \multicolumn{2}{c}{Inter.   } & \multicolumn{2}{c|}{Comp.   }  & \multicolumn{2}{c|}{Overall}      & \multicolumn{2}{c}{Easy   } & \multicolumn{2}{c|}{Hard   }     & \multicolumn{2}{c}{Overall} \\ \cline{2-15} 
% \multicolumn{1}{c|}{\multirow{-3}{*}{Finetune method}}        & PR              & AC             & PR              & AC             & PR    & \multicolumn{1}{c|}{AC}   & PR    & \multicolumn{1}{c|}{AC}   & PR             & AC            & PR    & \multicolumn{1}{c|}{AC}    & PR           & AC           \\ \hline
% \multicolumn{1}{c|}{w/o tuning}                               & 21.14           & 4.00           & 20.72           & 4.00           & 12.83 & \multicolumn{1}{c|}{1.00} & 18.23 & \multicolumn{1}{c|}{3.00} & 25.54          & 17.65         & 15.46 & \multicolumn{1}{c|}{5.77}  & 22.18        & 13.69        \\ \hline
% \multicolumn{1}{c|}{SFT on all}                               & 22.55           & 7.00              & 26.4            & 3.00              & 10.67 & \multicolumn{1}{c|}{1.00}    & 19.87 & \multicolumn{1}{c|}{3.67} & 22.42          & 14.71         & 18.12 & \multicolumn{1}{c|}{13.46} & 20.99        & 14.29        \\ \hline
% \multicolumn{1}{c|}{ SFT on cluster 0} & 20.67           & 6.00              & 24.23           & 3.00              & 11.5  & \multicolumn{1}{c|}{1.00}    & 18.80 & \multicolumn{1}{c|}{3.33} & 31.14          & 20.59         & 19.89 & \multicolumn{1}{c|}{11.54} & 27.39        & 17.57        \\
% \multicolumn{1}{c|}{ SFT on cluster 1} & 21.22           & 4.00              & 20.69           & 4.00              & 12.00    & \multicolumn{1}{c|}{2.00}    & 17.97 & \multicolumn{1}{c|}{3.33} & 29.4           & 21.57         & 17.39 & \multicolumn{1}{c|}{9.62}  & 25.40        & 17.59        \\
% \multicolumn{1}{c|}{SFT on cluster 2}                         & 16.65           & 7.00              & 23.97           & 3.00              & 17.33 & \multicolumn{1}{c|}{4.00}    & 19.32 & \multicolumn{1}{c|}{4.67} & 23.94          & 16.67         & 18.32 & \multicolumn{1}{c|}{7.69}  & 22.07        & 13.68        \\ \hline
% \multicolumn{1}{c|}{Ties}                                     & 22.75           & 4.00              & 23.06           & 4.00              & 12.67 & \multicolumn{1}{c|}{4.00}    & 19.49 & \multicolumn{1}{c|}{4.00} & 24.14          & 13.73         & 11.58 & \multicolumn{1}{c|}{5.77}  & 19.95        & 11.08        \\
% \multicolumn{1}{c|}{Dare}                                     & 24.97           & 7.00              & 26.66           & 5.00              & 12.5  & \multicolumn{1}{c|}{3.00}    & 21.38 & \multicolumn{1}{c|}{5.00} & 30.59          & 24.51         & 12.6  & \multicolumn{1}{c|}{7.69}  & 24.59        & 18.90        \\
% \multicolumn{1}{c|}{Twin}                                     & 19.1            & 5.00              & 23.85           & 5.00              & 8.5   & \multicolumn{1}{c|}{1.00}    & 17.15 & \multicolumn{1}{c|}{3.67} & 26.11          & 16.67         & 15.43 & \multicolumn{1}{c|}{3.85}  & 22.55        & 12.40        \\ \hline
% \multicolumn{1}{c|}{\modelname}                 & \textbf{27.11}           & \textbf{9.00}              & 23.11           & 3.00              & 11.5  & \multicolumn{1}{c|}{\textbf{4.00}}    & \textbf{20.57} & \multicolumn{1}{c|}{\textbf{5.33}} & 27.91          & 18.63         & 19.45 & \multicolumn{1}{c|}{5.77}  & 25.09        & 14.34        \\ \hline
% \end{tabular}
% }
% \end{table*}


% \section{Conclusion}


% \section*{Limitations}
% In this paper, we propose a graphical retrieval augmented generation method that can offer enhanced code generation. Despite the efficiency and effectiveness, there are also limitations within this work.  For example, dependency on the quality of the external knowledge base could be a potential concern. The quality of the external knowledge base could be improved with regular expression extraction on the noisy texts and codes. 


%\section*{Ethics Statement}

%\section*{Acknowledgements}




% \section*{Acknowledgments}

% This document has been adapted by Emily Allaway from the instructions for earlier ACL and NAACL proceedings, including those for NAACL 2024 by Steven Bethard, Ryan Cotterell and Rui Yan,
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
% \bibliography{custom}

% \appendix
% \section{Implementation Details}
% \label{app:setup}
% For the size of retrieval pool, we use 11,913 C++ code snippets and 2,359 python code snippets. Due to the limited access, we do not use a large retrieval corpus for our experiment, which can be enlarged by other people for better performance. We also attach the graph extraction codes for both languages and all other expeirment codes here: https://anonymous.4open.science/r/Code-5970/

% For the fintuning details, the learning rate and weight decay for the expert GNN training is 0.001 and 1e-5, repectively. We apply 8-bit quantization and use LoRA for parameter-efficient fine-tuning. The rank of the low-rank matrices in LoRA is uniformly set to 8, alpha set to 16, and dropout is set to 0.05. The LoRA modules are uniformly applied to the Q and V parameter matrices of the attention modules in each layer of the LLM. All the three models are optimized using the AdamW optimizer. For the CodeContest dataset, totally 10609 datapoints are used, and for APPS dataset, 8691 data samples are used to train the model.


\end{document}
