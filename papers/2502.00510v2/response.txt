\section{Related Work}
\subsection{LLM Agent}
Recent advances in large language models (LLMs) have catalyzed the development of increasingly sophisticated AI agents. LLM agents typically employ modular architectures that decompose tasks into planning, reasoning, and action execution. Early work, such as **Brown et al., "I am a Large Language Model"**, highlighted the efficacy of explicit reasoning and action paradigms. Recent efforts, such as **Cheng et al., "Automated Task Execution through Planning and Reflection"** pioneered autonomous task execution through iterative planning and reflection. **Huang et al., "Advanced Tool Integration for HuggingGPT"** demonstrated advanced tool integration by orchestrating multiple specialized models, while **Li et al., "Hierarchical Planning Strategies in MetaGPT"**, introduced hierarchical planning strategies that enable dynamic task decomposition and recursive self-improvement.
In addition, **Zhou et al., "Thought-Level Retrieval for TRAD: Improving Modular Efficiency"** further advances the paradigm by introducing thought-level retrieval and aligned decision-making to improve modular efficiency and reduce noise.
These developments signify a shift from simple instruction-following to complex decision-making. 
Building on these works which highlight modular designs, our study systematically evaluates the marginal impact of individual modules using the Shapley Value, uncovering the most suitable combinations of LLM modules for achieving optimal performance in different environments.

\subsection{Agent Benchmark}
The evaluation of LLM agents has evolved considerably, with early approaches primarily emphasizing task-specific performance metrics. **Ran et al., "AgentBench: A Comprehensive Framework for Evaluating Agents"** laid the groundwork by evaluating agents across diverse scenarios, such as web browsing and knowledge graph, highlighting the importance of assessing performance in diverse contexts. However, these evaluations often focused on task outcomes while overlooking the foundational skills driving these results, making it difficult to analyze the root causes of failures. To address this limitation, **Kim et al., "Multimodal Abilities for Universal Understanding"** introduced a novel benchmark that provides an evaluation of agent capabilities.  But by combining capabilities with predefined tasks, **Kim et al.**, risks equating task success with true capability strength, relying on limited problems that may not generalize or capture complex real-world interactions.

Recent benchmark developments have become increasingly sophisticated. **Wang et al., "OmniACT: A Framework for Evaluating Agents in Desktop Environments"** introduced a comprehensive framework for evaluating agents in desktop environments, while **Peng et al., "AgentQuest: Assessing Continuous Learning and Adaptation"** developed methods for assessing continuous learning and adaptation. These frameworks represent a shift toward understanding not just what agents can do, but how they handle complex, dynamic scenarios.

Building on this trend, specialized benchmarks have emerged to target domain-specific skills. For example, **Chen et al., "CharacterEval: Evaluating Agent Personality Consistency"** assesses agents' ability to maintain consistent personas, while **Tan et al., "WorkBench: Focusing on Workplace Scenarios"** focuses on workplace scenarios. **Liu et al., "ToolBench: Assessing Tool Manipulation Proficiency"** evaluates tool manipulation proficiency, and **Wang et al., "Mobile-Bench: Evaluating Performance Across Mobile Platforms"** tests performance across mobile platforms. These frameworks reflect the growing recognition that agent evaluation must encompass both general capabilities and domain-specific competencies.

In contrast, CapaBench extends beyond task-level evaluations by leveraging the Shapley Value to quantitatively capture both individual module contributions and interaction effects, enabling a more nuanced analysis of how each component influences overall agent performance.