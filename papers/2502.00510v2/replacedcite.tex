\section{Related Work}
\subsection{LLM Agent}
Recent advances in large language models (LLMs) have catalyzed the development of increasingly sophisticated AI agents. LLM agents typically employ modular architectures that decompose tasks into planning, reasoning, and action execution. Early work, such as ReAct____, highlighted the efficacy of explicit reasoning and action paradigms. Recent efforts, such as AutoGPT____ pioneered autonomous task execution through iterative planning and reflection. HuggingGPT____ demonstrated advanced tool integration by orchestrating multiple specialized models, while MetaGPT____, introduced hierarchical planning strategies that enable dynamic task decomposition and recursive self-improvement.
In addition, TRAD____ further advances the paradigm by introducing thought-level retrieval and aligned decision-making to improve modular efficiency and reduce noise.
These developments signify a shift from simple instruction-following to complex decision-making. 
Building on these works which highlight modular designs, our study systematically evaluates the marginal impact of individual modules using the Shapley Value, uncovering the most suitable combinations of LLM modules for achieving optimal performance in different environments.

\subsection{Agent Benchmark}
The evaluation of LLM agents has evolved considerably, with early approaches primarily emphasizing task-specific performance metrics. AgentBench____ laid the groundwork by evaluating agents across diverse scenarios, such as web browsing and knowledge graph, highlighting the importance of assessing performance in diverse contexts. However, these evaluations often focused on task outcomes while overlooking the foundational skills driving these results, making it difficult to analyze the root causes of failures. To address this limitation, MMAU____ introduced a novel benchmark that provides an evaluation of agent capabilities.  But by combining capabilities with predefined tasks, MMAU risks equating task success with true capability strength, relying on limited problems that may not generalize or capture complex real-world interactions.

Recent benchmark developments have become increasingly sophisticated. OmniACT ____ introduced a comprehensive framework for evaluating agents in desktop environments, while AgentQuest ____ developed methods for assessing continuous learning and adaptation. These frameworks represent a shift toward understanding not just what agents can do, but how they handle complex, dynamic scenarios.

Building on this trend, specialized benchmarks have emerged to target domain-specific skills. For example, CharacterEval ____ assesses agents' ability to maintain consistent personas, while WorkBench ____ focuses on workplace scenarios. ToolBench ____ evaluates tool manipulation proficiency, and Mobile-Bench ____ tests performance across mobile platforms. These frameworks reflect the growing recognition that agent evaluation must encompass both general capabilities and domain-specific competencies.

In contrast, CapaBench extends beyond task-level evaluations by leveraging the Shapley Value to quantitatively capture both individual module contributions and interaction effects, enabling a more nuanced analysis of how each component influences overall agent performance.