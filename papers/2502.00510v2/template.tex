\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{multirow, colortbl, xcolor}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{array} % 用于定义新列类型
\usepackage{subcaption} % 引入子图宏包
\usepackage{enumitem} % 用于自定义列表格式
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\usepackage{colortbl} % 确保加载colortbl包
\usepackage{xcolor}
\usepackage{siunitx}
\sisetup{table-format=3.3} % 控制数字格式
\definecolor{headergray}{gray}{0.9} % 表头灰色
\definecolor{groupgray}{gray}{0.95} % 分组浅灰色
\definecolor{rowaltgray}{gray}{0.98} % 条纹背景（行）


\newcommand{\customsize}[1]{{\fontsize{10}{12}\selectfont #1}}
% Define colors for listings
\definecolor{lightgray}{gray}{0.95}
\definecolor{darkgray}{gray}{0.4}
\definecolor{purple}{rgb}{0.58,0,0.82}
\definecolor{blue}{rgb}{0.13,0.13,1}



% Set up the listings environment
\lstset{
    backgroundcolor=\color{lightgray}, % Background color
    basicstyle=\ttfamily\small, % Font style and size
    breaklines=true, % Automatic line breaking
    frame=single, % Frame around the code
    rulecolor=\color{darkgray}, % Frame color
    keywordstyle=\color{purple}, % Keywords color
    stringstyle=\color{blue}, % Strings color
    commentstyle=\color{darkgray}, % Comments color
    showstringspaces=false, % Do not show spaces in strings
    captionpos=b % Caption position
}


\usepackage{titlesec} % 如果需要自定义标题格式可以加载titlesec包
\usepackage{tocloft} % 用于更好地控制目录显示

% 设置大写APPENDIX样式
\titleformat{\section}[block]{\Large\bfseries}{}{0pt}{}
% 自定义 APPENDIX 的大标题样式
% \newcommand{\bigappendix}{%
%     \newpage
%     \vspace*{0cm} % 控制距离顶部的空白，改小
%     \begin{center}
%         {\Huge\bfseries APPENDIX} % 大字体 APPENDIX 标题
%     \end{center}
%     \vspace{0.5cm} % 控制标题下的空白
% }
\newcommand{\bigappendix}{%
    \newpage
    \vspace*{0cm} % 控制距离顶部的空白，可以根据需要调整
    {\Huge\bfseries APPENDIX} % 大字体 APPENDIX 标题，默认左对齐
    \vspace{0.5cm} % 控制标题下的空白
}


\title{Who's the MVP? \\A Game-Theoretic Evaluation Benchmark for\\ Modular Attribution in LLM Agents}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
% \date{} 					% Or removing it
\author{
\textbf{Yingxuan Yang}\textsuperscript{1}, \textbf{Bo Huang}\textsuperscript{1}, \textbf{Siyuan Qi}\textsuperscript{1}, \textbf{Chao Feng}\textsuperscript{1}, \textbf{Haoyi Hu}\textsuperscript{1}, \textbf{Yuxuan Zhu}\textsuperscript{2},  \textbf{Jinbo Hu}\textsuperscript{1}, \textbf{Haoran Zhao}\textsuperscript{1},\\ \textbf{Ziyi He}\textsuperscript{3},  \textbf{Xiao Liu}\textsuperscript{4}, \textbf{Zongyu Wang}\textsuperscript{4}, \textbf{Lin Qiu}\textsuperscript{4},  \textbf{Xuezhi Cao}\textsuperscript{4},  \textbf{Xunliang Cai}\textsuperscript{4}, \textbf{Yong Yu}\textsuperscript{1}, \textbf{Weinan Zhang}\textsuperscript{1}\\
\textsuperscript{1}\customsize {Shanghai Jiao Tong University}\
\textsuperscript{2}\customsize {University of Chicago}\
\textsuperscript{3}\customsize {University of Toronto}\
\textsuperscript{4}\customsize {Meituan}\\
\customsize {\texttt{\{zoeyyx, wnzhang\}@sjtu.edu.cn}}
}


% \author{ \href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}David S.~Hippocampus}\thanks{Use footnote for providing further
% 		information about author (webpage, alternative
% 		address)---\emph{not} for acknowledging funding agencies.} \\
% 	Department of Computer Science\\
% 	Cranberry-Lemon University\\
% 	Pittsburgh, PA 15213 \\
% 	\texttt{hippo@cs.cranberry-lemon.edu} \\
% 	%% examples of more authors
% 	\And
% 	\href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Elias D.~Striatum} \\
% 	Department of Electrical Engineering\\
% 	Mount-Sheikh University\\
% 	Santa Narimana, Levand \\
% 	\texttt{stariate@ee.mount-sheikh.edu} \\
% 	%% \AND
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% 	%% \And
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% 	%% \And
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% }

% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{Technical Report}
\renewcommand{\undertitle}{}
\renewcommand{\shorttitle}{CapaBench}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
% \hypersetup{
% pdftitle={A template for the arxiv style},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
% }

\begin{document}
\maketitle

\begin{abstract}  
Large Language Model (LLM) agents frameworks often employ modular architectures, incorporating components such as planning, reasoning, action execution, and reflection to tackle complex tasks. However, quantifying the contribution of each module to overall system performance remains a significant challenge, impeding optimization and interpretability. To address this, we introduce \textbf{CapaBench} (\textbf{Capa}bility-level Assessment \textbf{Bench}mark), an evaluation framework grounded in cooperative game theory's Shapley Value, which systematically measures the marginal impact of individual modules and their interactions within an agent's architecture. By replacing default modules with test variants across all possible combinations, CapaBench provides a principle method for attributing performance contributions. \textbf{Key contributions} include: (1) We are the first to propose a Shapley Value-based methodology for quantifying the contributions of capabilities in LLM agents; (2) Modules with high Shapley Values consistently lead to predictable performance gains when combined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,500 entries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic system assessment, providing actionable insights for optimizing modular LLM agents and advancing their deployment in complex, real-world scenarios. 
\end{abstract}  
% keywords can be removed
\keywords{LLM Agent \and Evaluation Benchmark \and Shapley Value \and Capability}


\section{Introduction}
The rapid advancements in Large Language Models (LLMs) have ushered in a transformative era for artificial intelligence agents. These models demonstrate unprecedented capabilities in understanding, generating, and integrating natural language across diverse domains \citep{brown2020language, openai2024gpt4technicalreport}. 
However, LLMs still face notable challenges as foundational models for supporting AI agents in real-world applications. These include accurately interpreting subtle contextual shifts, effectively integrating with external tools, and ensuring both the accuracy and reliability of outputs.
To overcome these challenges, researchers have increasingly adopted modular architectures, decomposing agents into distinct components responsible for planning, reasoning, and action execution. Such modular frameworks not only enhance the overall performance but also improve the interpretability and maintainability of the systems. Frameworks such as ReAct \citep{yao2022react} and AutoGPT \citep{autogpt} exemplify how structured workflows, achieved by breaking down tasks into manageable modules, can lead to more efficient task processing. These modular architectures lay the groundwork for systematic evaluations of LLM agents' internal designs and effectiveness in various applications.


% \vspace{-0.1cm}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Figure/shapley-value-illustration.png}
    \vspace{-0.1cm}
    \caption{Conceptual Mapping between Coalition Game Theory and LLM Agent Evaluation. The left panel illustrates the mapping from coalition game theory to LLM agents, the right lists all possible module combinations ($2^4=16$) with their performance values.}
    \label{fig:SV illustration}
    \vspace{-0.5cm}
\end{figure}



Despite the impressive capabilities of LLM agents, accurately evaluating their performance remains an open challenge. Traditional evaluation methods have predominantly focused on task-specific benchmarks and domain-specific datasets. For instance, AgentBench \citep{liu2023agentbench} assesses agents' abilities through specialized tasks, while ToolBench \citep{guo2024stabletoolbench} evaluates the effectiveness of LLM agents in leveraging external tools across diverse application scenarios. Additionally, MMAU \citep{yin2024mmauholisticbenchmarkagent} investigates the capabilities of LLM Agents across a wide range of tasks. However, these benchmarks often rely on reductive assumptions, equating task success (e.g., solving a math problem) with broader cognitive abilities (e.g., reasoning). This simplification neglects the complex interactions between an agent's internal components, leading to an incomplete understanding of their true potential.
The current task-oriented evaluation framework faces several key challenges. First, LLM agents simultaneously require the integration of multiple capabilities to solve complex tasks. For example, solving a mathematical problem may necessitate reading comprehension, tool usage, and structured output generation. Second, existing methods fail to account for the interactions between architectural components and their collective contributions to overall system behavior. Additionally, task-specific success rates provide limited insight into the relative contributions of individual modules, making it difficult to identify key areas for optimization. Consequently, there is a pressing need for evaluation frameworks that can dissect and quantify the contributions of each module within modular LLM agents.

To address these challenges, we propose a novel evaluation framework, \textbf{CapaBench}, which integrates the assessment of modular architectures with the evaluation of agent capabilities. CapaBench systematically quantifies the contributions of individual modules (e.g., planning, reasoning, action execution, reflection) within LLM architectures using the Shapley Value \citep{Hart1989}, a cooperative game theory metric that fairly attributes performance based on all possible permutations of module contributions. This approach captures direct contributions and interaction effects at the same time, offering a rigorous and interpretable evaluation of system dynamics.
Our method provides several key advantages: (1) evaluating the contributions of each module by capturing nuanced dynamics; (2) using a mathematically sound attribution method to enhance interpretability of agent performance; and (3) enabling predictions about system performance based on specific module combinations, supporting targeted optimizations. To the best of our knowledge, CapaBench is \textbf{the first framework} to systematically quantify and attribute module contributions in LLM-based agents using the Shapley Value approach.

Furthermore, to ensure that our evaluation reflects realistic, multi-faceted application scenarios, we build a \textbf{large-scale} dataset of over \textbf{1,500 multi-round tasks} spanning a diverse range of categories (e.g., shopping, navigation planning, ticket ordering, operation system, robot control, math, and theorem proving). These tasks integrate various capabilities such as planning, tool usage, and reflection, thereby requiring holistic agent performance rather than isolated skill assessments. Our dataset will be open-sourced in the future to support further research and development, and we are actively adding more scenarios to broaden its coverage and applicability.

Overall, \textbf{CapaBench} makes the following contributions:
\vspace{-0.1cm}
\begin{itemize}[itemsep=0.1em] 
\item \textbf{Novel Evaluation Framework: }We propose a rigorous methodology based on the Shapley Value to systematically quantify the contributions of capabilities within LLM agents—which is the first work to adopt such an approach for evaluating LLM agents. 
\item \textbf{Predictive Module Combinations: }Through comprehensive experiments, we show that modules attaining higher Shapley Values consistently enhance task success when combined. These findings guide developers in pinpointing and integrating high-value modules for performance gains.
\item \textbf{Large-Scale Dataset: }We build a multi-round dataset with over 1,500 entries spanning diverse domains such as daily activities, computation, and role control. The dataset is designed to challenge multiple agent capabilities simultaneously, serving as a robust testbed for evaluating LLM agents. Our dataset will be released in the future to facilitate further research and development.
\end{itemize}


\section{Related Work}
\subsection{LLM Agent}
Recent advances in large language models (LLMs) have catalyzed the development of increasingly sophisticated AI agents. LLM agents typically employ modular architectures that decompose tasks into planning, reasoning, and action execution. Early work, such as ReAct~\citep{yao2022react}, highlighted the efficacy of explicit reasoning and action paradigms. Recent efforts, such as AutoGPT~\citep{autogpt} pioneered autonomous task execution through iterative planning and reflection. HuggingGPT~\citep{shen2023hugginggpt} demonstrated advanced tool integration by orchestrating multiple specialized models, while MetaGPT~\citep{hong2024metagpt}, introduced hierarchical planning strategies that enable dynamic task decomposition and recursive self-improvement.
In addition, TRAD~\citep{zhou2024tradenhancingllmagents} further advances the paradigm by introducing thought-level retrieval and aligned decision-making to improve modular efficiency and reduce noise.
These developments signify a shift from simple instruction-following to complex decision-making. 
Building on these works which highlight modular designs, our study systematically evaluates the marginal impact of individual modules using the Shapley Value, uncovering the most suitable combinations of LLM modules for achieving optimal performance in different environments.

\subsection{Agent Benchmark}
The evaluation of LLM agents has evolved considerably, with early approaches primarily emphasizing task-specific performance metrics. AgentBench~\citep{liu2023agentbench} laid the groundwork by evaluating agents across diverse scenarios, such as web browsing and knowledge graph, highlighting the importance of assessing performance in diverse contexts. However, these evaluations often focused on task outcomes while overlooking the foundational skills driving these results, making it difficult to analyze the root causes of failures. To address this limitation, MMAU~\citep{yin2024mmauholisticbenchmarkagent} introduced a novel benchmark that provides an evaluation of agent capabilities.  But by combining capabilities with predefined tasks, MMAU risks equating task success with true capability strength, relying on limited problems that may not generalize or capture complex real-world interactions.

Recent benchmark developments have become increasingly sophisticated. OmniACT \citep{zhang2024omniact} introduced a comprehensive framework for evaluating agents in desktop environments, while AgentQuest \citep{yang2024agentquest} developed methods for assessing continuous learning and adaptation. These frameworks represent a shift toward understanding not just what agents can do, but how they handle complex, dynamic scenarios.

Building on this trend, specialized benchmarks have emerged to target domain-specific skills. For example, CharacterEval \citep{chen2024charactereval} assesses agents' ability to maintain consistent personas, while WorkBench \citep{liu2024workbench} focuses on workplace scenarios. ToolBench \citep{guo2024stabletoolbench} evaluates tool manipulation proficiency, and Mobile-Bench \citep{wang2024mobilebench} tests performance across mobile platforms. These frameworks reflect the growing recognition that agent evaluation must encompass both general capabilities and domain-specific competencies.

In contrast, CapaBench extends beyond task-level evaluations by leveraging the Shapley Value to quantitatively capture both individual module contributions and interaction effects, enabling a more nuanced analysis of how each component influences overall agent performance.


\section{Benchmark Design}

We introduce the agent framework shown in Figure~\ref{fig:Agent Workflow} as the foundation of our benchmark. This framework is specifically designed to assess LLM agents' abilities in various environments and task scenarios. It follows established agent processes and features a modular design, which supports both single-turn and multi-turn interactions. This ensures that our evaluations are comprehensive and adaptable.

\vspace{-0.2cm}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figure/Agent_workfolw.png}
    % \vspace{-0.2cm}
    \caption{Agent Workflow in CapaBench.}
    \label{fig:Agent Workflow}
    % \vspace{-0.5cm}
\end{figure*}
 
\subsection{Agent Capability} Building upon established agent architectures \citep{yao2022react,autogpt,hong2024metagpt}, our framework integrates four fundamental capabilities essential for LLM agents: Planning, Reasoning, Action, and Reflection, as illustrated in Figure~\ref{fig:Agent Workflow}. These capabilities represent the core functionalities widely recognized in current agent systems, enabling agents to handle immediate completions and perform complex tasks.

\textbf{Planning module} initiates the agent workflow by decomposing complex instructions into structured subtasks, following principles established in hierarchical planning systems \citep{brown2020language}. This decomposition enables effective task prioritization and resource allocation, particularly crucial for multi-step operations requiring strategic foresight.

\textbf{Reasoning module} extends the ReAct framework \citep{yao2022react} by incorporating both instruction context and environmental observations. Through chain-of-thought mechanisms \citep{wei2022chain}, this module performs logical inference and causal analysis to determine appropriate action sequences. Integration with the planning module enables dynamic adjustment of reasoning strategies based on evolving task requirements.

\textbf{Action module} implements the execution interface, translating cognitive processes into concrete operations. This approach builds on established action space formalization \citep{guo2024stabletoolbench}, ensuring consistent mapping between internal state representations and external behaviors. The module maintains state awareness through continuous environment monitoring, enabling responsive behavior adaptation.

\textbf{Reflection module} completes the architecture by implementing systematic performance analysis, drawing from recent advances in self-improving systems \citep{yin2024mmauholisticbenchmarkagent}. Operating primarily in multi-turn scenarios, this module enables iterative refinement of agent behavior through structured outcome analysis and strategy adjustment.

\subsection{Evaluation Methodology}
To evaluate the contribution of individual capability modules within LLM agent architectures, we leverage Shapley Value \citep{Hart1989} analysis, a principled framework grounded in cooperative game theory. This methodology quantifies the marginal impact of each module on system performance by systematically evaluating all possible module configurations. By capturing both independent contributions and interaction effects among modules, this approach provides a robust mechanism for evaluating the modular design of LLM systems, while naturally handling the nonlinear dynamics inherent in such architectures.


\paragraph{Shapley Value Framework}
Shapley Value provides a theoretical foundation for fairly allocating the overall performance of a system to its individual components. For a set of $N$ modules, Shapley Value $\phi_i(v)$ for module $i$ is defined as:
\begin{equation}
\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} [v(S \cup \{i\}) - v(S)],
\end{equation}
where $S$ denotes any subset of $N$ that excludes module $i$, and $v(S)$ represents the performance(task success rate) of the agent when only the modules in $S$ are active. 
The term $v(S \cup \{i\}) - v(S)$ quantifies the marginal impact of adding module $i$ to the subset $S$, while the weight $\frac{|S|!(|N|-|S|-1)!}{|N|!}$ ensures fair averaging across all possible subsets.



\paragraph{Evaluation Flow}
CapaBench systematically evaluates the contributions of four key modules in the agent architecture: Planning (\(P\)), Reasoning (\(R\)), Action (\(A\)), and Reflection (\(F\)). As shown in Figure~\ref{fig:SV illustration}, the evaluation involves testing all possible combinations of these modules ($2^4=16$ combinations) by replacing default implementations with test variants provided by the target LLM model. The default "whiteboard" modules, implemented using Llama3-8b-instruct, serve as a fixed baseline to isolate the performance impact of each test module. Llama3-8b-instruct was chosen as the default model implementation because it is open-source, lightweight, and easy to deploy, making it practical for extensive testing. While it possesses basic task completion capabilities, its moderate success rates provide an ideal baseline to observe and quantify the impact of replacing modules with more advanced test models. 

For each combination, CapaBench computes performance values to quantify the contribution of individual modules and their interactions. Diverse task benchmarks (\(B\)), including multi-step scenarios designed to simulate practical agent applications, are used to evaluate the system, providing insights into the optimal module configurations for various environments.

% \vspace{-0.2cm}
\begin{algorithm}[t]
\caption{CapaBench Evaluation Framework}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Default model, Test model, Benchmarks \( B \)
\STATE \textbf{Output:} Shapley Value \( \phi_i(v) \) for each test module \( i \)
\STATE Fix all modules to their default implementations: \( \{Pd, Rd, Ad, Fd\} \)
\FORALL{subset \( S \subseteq \{Pt, Rt, At, Ft\} \)}
    \STATE Replace default modules in \( S \) with test modules
    \STATE Evaluate task success rate \( v(S) \) using benchmarks \( B \)
\ENDFOR
\FORALL{test module \( i \in \{Pt, Rt, At, Ft\} \)}
    \STATE Compute Shapley Value \( \phi_i(v) \)
\ENDFOR
\RETURN \( \phi_i(v) \) for all test modules \( i \)
\end{algorithmic}
\end{algorithm}


\paragraph{Capturing Synergistic Effects and Nonlinear Dynamics}
Shapley Value provides a robust framework to quantify both the independent contributions and synergistic interactions among modules in a modular architecture. By systematically evaluating all possible subsets $S \subseteq N$, it inherently captures the nonlinear dynamics and interdependencies between modules. For instance, Planning provides structured outputs for Reasoning, while Reasoning refines these outputs to guide Action execution. Tasks often require at least two modules to collaborate, such as Reasoning  and Action working together to decompose and solve complex tasks. These collaborative effects are reflected in the marginal contributions $v(S \cup \{i\}) - v(S)$, where \(v(S)\) represents the system's performance (e.g., task success rate) with subset \(S\). 
Shapley Value is particularly well-suited for nonlinear dynamics, as it fairly distributes contributions even when module interactions exhibit synergy or competition. Unlike linear or additive methods, it ensures unbiased attribution of both individual and collaborative contributions, making it ideal for evaluating modular LLM agents with complex interdependencies.


\subsection{Dataset Construction}
\paragraph{Online Shopping}
Online Shopping tasks are based on the simulated online shopping platform WebShop \citep{yao2023webshopscalablerealworldweb}. The dataset includes 110 tasks, of which we modified 48 tasks to enhance the diversity and complexity of the instructions. For example, the original instruction “find me scrubs \& body treatments made with tea tree and other natural ingredients” is rewritten as “Given my upcoming spa weekend, I'm on the lookout for scrubs \& body treatments. Can you recommend ones specifically made with tea tree and other natural ingredients as I have sensitive skin?” These modified prompts reflect more natural and contextually rich user queries, challenging the agent to demonstrate reasoning, personalization, and relevance in its recommendations. The reward model and product definitions align with WebShop, providing a consistent evaluation framework for agents' performance in online shopping scenarios.



 \paragraph{Navigation Planning}
The Navigation Planning task evaluates agents' ability to collaboratively generate travel itineraries with a user while adapting to evolving constraints and preferences, inspired by \citep{lin2023decision}.  This dataset's 250 tasks are designed to reflect a wide range of planning challenges. In our setup, the user provides an initial set of three travel requirements sampled from a pool of potential preferences, such as budget limits, preferred activities, or group constraints. 

To simulate real-world planning scenarios where user preferences may evolve, the evaluation process introduces dynamic updates.  In each iteration, there is a 50\% chance that a new preference is sampled from the predefined pool.  This new preference will be added to the current instruction set, leading to updated instructions.  If no new preference is introduced (also with 50\% probability), the agent's current proposal is evaluated directly.  

The evaluation consists of two components: the first part is based on the precision derived from the experimental results, and the second part evaluates the rationality of the planned route, based on how well the proposal aligns with user preferences, considering factors such as budget adherence, inclusion of specified activities, and efficient travel distances.  This feedback measures the agent's ability to prioritize user needs and adaptively produce actionable travel plans.

\paragraph{Ticket Ordering}
The Ticket Ordering task, inspired by \citep{lin2023decision}, evaluates an agent's ability to determine the optimal flight combination based on user-specified constraints. This dataset comprises 150 tasks designed to simulate everyday ticket ordering scenarios. In our setup, two users provide their daily calendars along with requirements such as the flight price.

To mirror real-world ticket ordering, users can choose from a wide array of flights—each differing in price, duration, arrival time, and more—which makes it challenging for agents to offer sound advice.

The evaluation involves three components: the flight price, the significance of calendar conflicts with flight schedules, and the difference in arrival times between the two users. Lower prices, fewer calendar conflicts, and smaller differences in arrival times indicate a more favorable flight combination as determined by the agents.


 
\paragraph{Math Solver}
The Math Solver task evaluates the ability of agents to solve diverse mathematical problems by integrating \textbf{ the usage of tools} into the problem solving process. This task spans two categories: Algebra and Geometry. The problems in these categories were generated based on Math \citep{hendrycksmath2021} with the assistance of GPT-4, resulting in a newly created set of problems.

To address the challenges posed by Math's lack of detailed classification of points of knowledge and difficulty, we organized tasks into five distinct points of knowledge and 10 levels of difficulty. Each combination of knowledge point and difficulty level contains 5 unique problems, resulting in a total of 250 problems. These were systematically distributed to ensure balanced coverage across all knowledge points and difficulty levels.

To support agents in solving these problems, we introduced two tools:
\vspace{-0.2cm}
\begin{itemize}
    \item A \textbf{pseudo `search engine'} containing 200 curated knowledge points for Algebra and Geometry. This search engine allows agents to retrieve the top three most relevant knowledge points by using a BERT model based on query similarity.\vspace{-0.1cm}
    \item A \textbf{calculator} provided to LLM agents for performing numerical computations.
\end{itemize}

These tools enable agents to simulate human-like problem solving by integrating both retrieval-based and computational capabilities.

\paragraph{Automatic Theorem Proving}
The automatic theorem-proving(ATP) aspect of the task evaluates the ability of agents to construct formal proofs for logical problems. The MINIF2f \citep{zheng2021minif2f} dataset stands out in ATP, featuring a series of complex Olympiad-level mathematical problems. However, a subset of this data set is oriented to Lean 3, and currently Lean 3 has been upgraded to Lean 4 and is no longer in use. In addition, Coq is also a popular formal proof language, but MINIF2F is not involved. More importantly, an important feature of formal proof is that humans can interact with the compiler's information to complete the proof. However, the previous benchmark only tested whether the response given by LLM could complete the proof in one step.

To address theorem-proving challenges, agents use \textbf{ formal verification tools} specific to Coq, Lean4 and Isabelle3 \citep{coq,lean4,isabelle3}. These tools require agents to work within formal syntax constraints, iteratively constructing proofs step by step. The problem solving process involves dynamically adjusting their strategies based on the current proof state, simulating human-like reasoning in formal logic. By engaging with these tools and frameworks, agents are required to navigate the complexities of theorem proving, demonstrating the ability to reason rigorously and adaptively in formal systems.

% 来源环境/改动/题目数量/任务流程/评价方式/图示
\begin{table*}[t]
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\caption{\textbf{Capability Coverage Across Dataset Categories.} Each row corresponds to a core capability in our modular framework (planning, reasoning, action, reflection), and each column represents a task in our dataset.}
\label{tab:dataset_capability}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l p{3.5cm} c c c c c c c c c}
\specialrule{1pt}{0pt}{0pt} % 第一行加粗
\multicolumn{2}{l}{\multirow{2}{*}{\textbf{}}} & \multicolumn{3}{c}{\textbf{Daily Activities}} & \multicolumn{3}{c}{\textbf{Computation}} & \multicolumn{1}{c}{\textbf{Role Control}} \\
\cline{3-5}
\cline{6-8}
\cline{9-9}
\multicolumn{2}{c}{} & \textbf{Shopping} & \textbf{Navigation} & \textbf{Ticket} & \textbf{Math} & \textbf{ATP} & \textbf{OS} & \textbf{Robot} \\
\hline
\multirow{2}{*}{\textbf{Planning}} & Task Steps & {\color{red}$\checkmark$} & & & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & &  \\
& Resource Constraints & & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & & & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} \\
\hline
\multirow{3}{*}{\textbf{Reasoning}} & Logical Validation & & & & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & \\
& Knowledge Inference & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & & & & {\color{red}$\checkmark$} \\

\hline
    \textbf{Action} & Environmental Actions  & & & & \color{red}$\checkmark$ & \color{red}$\checkmark$ & \color{red}$\checkmark$ &  \\
& Interactive Actions & \textbf{\color{red}$\checkmark$} & \textbf{\color{red}$\checkmark$} & \textbf{\color{red}$\checkmark$} & & &
&\color{red}$\checkmark$ \\

\hline
\textbf{Reflection} & Failure Analysis & {\color{red}$\checkmark$}& {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} & {\color{red}$\checkmark$} \\

\specialrule{1pt}{0pt}{0pt} % 最后一行加粗
\end{tabular}}
\end{table*}


\paragraph{Operation System}

The Operation System dataset evaluates an agent's ability to interact with a simulated OS terminal by executing commands for both Ubuntu and git tasks. For Ubuntu tasks, we utilized the AgentBench-OS framework \citep{liu2023agentbench} and expanded the dataset with GPT-4, covering key areas such as file system manipulation, system setting and process running. During evaluation, agents propose bash commands to be executed in Ubuntu terminal and get the feedback from the terminal to complete the given task. The reflection module is designed as when last command failed (use $(echo\; \$?)$ to get the execute success result of command), prompting agents to reflect on the error to improve future interactions.

For git tasks, we adopted data from Learn Git Branching \citep{learnGitBranching}, which provides a sandbox environment that dynamically updates the git tree based on input terminal commands. The task form is given target git tree information and init git tree information, agents are required to propose git command to transform init git tree into target git tree state. The reflection module is designed as if no changes occur in the git tree after two interaction steps, agents are prompted to reflect on their previous commands to enhance their reasoning processes.



\paragraph{Robot Cooperation}
The Robot Cooperation task is based on scenarios from RoCo \citep{mandi2023rocodialecticmultirobotcollaboration}, designed to evaluate LLM agents in diverse real-world-inspired robotic environments. We adopted and reformed five core tasks from the original benchmark: Sweep Floor, Move Rope, Arrange Cabinet, Make Sandwich, and Sort Cubes. Each task was expanded with specific instances to ensure diversity and precision in evaluation.

To further challenge and assess the agents' capabilities, we enhanced these tasks by incorporating additional constraints. For instance, the Sweep Floor task was refined by requiring the agent to sweep cubes in a specific sequence (e.g., first red, then blue, and finally green), thereby assessing the agent's ability to plan with order sensitivity. Similarly, the Arrange Cabinet task now requires the agent to first remove a cup or mug and place it on a designated coaster before handling other items, emphasizing the importance of sequential logic.

Building on these enhanced tasks, we adopted the Central Plan mode from RoCo, wherein an oracle LLM-planner is provided with complete environmental observations, comprehensive information on all robots' capabilities, and uniform plan feedback. This setup prompts the LLM to devise actions for all robots simultaneously. To further enhance this approach, we modified it to allow the agent to plan multiple action steps within a single interaction. Unlike the original single-step-single-action approach, this modification reduces the number of required interactions, enabling the agent to create more comprehensive and integrated action plans. 

\begin{table}[t]
\centering
\caption{Number of Data Entries per Dataset}
\label{tab:dataset_counts_transposed}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l 
                >{\centering\arraybackslash}p{1.5cm} 
                >{\centering\arraybackslash}p{1.5cm} 
                >{\centering\arraybackslash}p{1.5cm} 
                >{\centering\arraybackslash}p{1.5cm} 
                >{\centering\arraybackslash}p{1.5cm}
                >{\centering\arraybackslash}p{1.5cm} 
                >{\centering\arraybackslash}p{1.5cm}
                >{\centering\arraybackslash}p{1.5cm} 
                >{\centering\arraybackslash}p{1.5cm} 
                >{\centering\arraybackslash}p{1.5cm} 
                >{\centering\arraybackslash}p{1.5cm} }
\toprule
\textbf{Category} & \multicolumn{2}{c}{\textbf{Shopping}}  & \textbf{Navigation} & \textbf{Ticket} & \multicolumn{2}{c}
{\textbf{Math Solver}}& \multicolumn{3}{c}{\textbf{Automatic Theorem Proving}} & \textbf{Robot} & \textbf{OS} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7} \cmidrule(lr){8-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12}
\textbf{Subcategory} & Black & White & None & None & Algebra & Geometry & Coq & Lean4 & Isabelle & None & None \\
\midrule
\textbf{Count} & 48 & 62 & 250 & 150 & 250 & 250 & 111 & 111 & 111 & 100 & 102 \\
\bottomrule
\end{tabular}
}
\end{table}

% \paragraph{Role Playing.}
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figure/sherrif.png}
%     \vspace{-0.2cm}
%     \caption{Sheriff of Nottingham.}
%     \label{fig:Sheriff of Nottingham}
% \end{figure}

% The Role Playing task is based on the party card game \textbf{"Sheriff of Nottingham"}, which centers on honesty and deception. Players take on the roles of medieval merchants attempting to bring goods into the city while negotiating with the sheriff. Key interactions include negotiation, bribery, and deception, making it an ideal environment to evaluate an agent's social manipulation skills, strategic planning, and ability to handle complex scenarios.
              
% \begin{figure}[h]
%     % \vspace{-0.2cm} 
%     \begin{minipage}{0.45\textwidth}
%         \paragraph{Role Playing.} We introduce a novel role-playing task based on the party card game \textbf{"Sheriff of Nottingham"} in LLM Agent Enviroment, which centers on honesty and deception. Players take on the roles of medieval merchants attempting to bring goods into the city while negotiating with the sheriff. 
        
%         Key interactions include negotiation, bribery, and deception, making it an ideal environment to evaluate an agent's social manipulation skills, strategic planning, and ability to handle complex scenarios. The roles and game process are illustrated as shown in Figure \ref{fig:Sheriff of Nottingham}.
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \includegraphics[width=0.95\linewidth]{Figure/sherrif.png}
%         \vspace{-0.1cm} 
%         \caption{Sheriff of Nottingham.}
%         \label{fig:Sheriff of Nottingham}
%     \end{minipage}
%     \vspace{-0.3cm} 
% \end{figure}

% \begin{figure}[h]
%     % \vspace{-0.2cm} 
%     \begin{minipage}{0.45\textwidth}
%         \paragraph{Role Playing} We introduce a novel role-playing task based on the party card game \textbf{"Sheriff of Nottingham"} in LLM Agent Enviroment, which centers on honesty and deception. Players take on the roles of medieval merchants attempting to bring goods into the city while negotiating with the sheriff. 
        
%         Key interactions include negotiation, bribery, and deception, making it an ideal environment to evaluate an agent's social manipulation skills, strategic planning, and ability to handle complex scenarios. The roles and game process are illustrated as shown in Figure \ref{fig:Sheriff of Nottingham}.
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \includegraphics[width=0.95\linewidth]{Figure/sherrif.png}
%         \vspace{-0.1cm} 
%         \caption{Sheriff of Nottingham.}
%         \label{fig:Sheriff of Nottingham}
%     \end{minipage}
% \end{figure}
% \vspace{-0.1cm}

% This game provides clear intermediate metrics (e.g., round-based profits) and final objectives (total gold count), enabling systematic analysis of agent performance. Compared to similar communication-based games like Avalon and Werewolf, "Sheriff of Nottingham" incorporates mathematical elements and diverse scenario factors, offering richer evaluation dimensions. Unlike turn-based games, this task emphasizes dynamic multi-round communication, where merchants and the sheriff negotiate agreements to maximize individual gains.

% Agents are evaluated using the \textbf{TrueSkill rating system}, ensuring consistent and fair assessment across varying roles and game states. This task provides a unique benchmark to assess agents' decision-making, social interaction, and adaptability in a negotiation-driven environment.

% \begin{table*}[t]
%   \caption{Experimental Results Across Datasets. Metrics for baseline models are highlighted in blue. The evaluation covers nine models across five primary tasks, showcasing notable performance variations and unique module contributions. Results marked with `*` below each dataset indicate the best-performing model combinations computed based on Shapley Value.}
%   \label{tab:consolidated_results_vertical}
%   \vspace{-0.2cm}
%   \centering
%   \begin{flushleft}
%   \end{flushleft}
%   \definecolor{headergray}{RGB}{240,240,240}
%   \definecolor{highlightblue}{RGB}{230,240,255}
%   \definecolor{highlightgray}{RGB}{248,248,248}
%   \resizebox{\textwidth}{!}{%
%   \begin{tabular}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X ccccccccccc}
%    % \begin{tabular}{llccccccccccc}
%     \toprule
%     \rowcolor{headergray}
%     \textbf{Dataset} & \textbf{Metric} & \textbf{\texttt{\shortstack{Llama3\\8B}}} & \textbf{\texttt{\shortstack{Claude\\3.5}}} & \textbf{\texttt{\shortstack{gpt-4o\\mini}}} & \textbf{\texttt{\shortstack{glm-4\\air}}} & \textbf{\texttt{\shortstack{qwen2.5\\32B}}} & \textbf{\texttt{\shortstack{Mistral\\8X7B}}} & \textbf{\texttt{\shortstack{Mistral\\7B}}} & \textbf{\texttt{\shortstack{gpt-4\\turbo}}} & \textbf{\texttt{\shortstack{doubao\\pro-4k}}} & \textbf{\texttt{\shortstack{Llama3\\70B}}} \\
%     \midrule

%     \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Online\\Shopping}}} 
%     & Pt            & \cellcolor{highlightblue}--      & -0.004   & 0.071 & \textbf{0.106} & -0.030  & -0.048  & 0.024  & 0.026   & \underline{0.071}   & -0.028  \\
%     & Rt            & \cellcolor{highlightblue}--      & 0.019   & -0.025 & \textbf{0.077} & 0.004  & \underline{0.036} & 0.016  & -0.074  & 0.011   & 0.005  \\
%     & At            & \cellcolor{highlightblue}--      & 0.056   & 0.068  & -0.059 & \textbf{0.156} & 0.080  & 0.004  & 0.014   & -0.045  & \underline{0.117}  \\
%     & Ft            & \cellcolor{highlightblue}--      & -0.009   & \underline{-0.003} & -0.011 & -0.021  & -0.015 & -0.022  & \textbf{0.024}  & -0.040  & -0.030  \\
%     \rowcolor{highlightgray}
%     & Acc (\%)      & \cellcolor{highlightblue}26.3   & 32.4   & \underline{37.4}   & \textbf{37.5}  & 37.2   & 31.7   & 28.5    & 25.3   & 26.0   & 32.6  \\
%     \rowcolor{highlightgray}
%     & $\Delta$ Acc (\%) & \cellcolor{highlightblue}--  & +6.16    & \underline{+11.2}  & \textbf{+11.2}  & +10.9  & +5.40   & +2.21   & -0.96   & -0.32   & +6.34   \\
%     \midrule

%     \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Math\\Algebra}}} 
%     & Pt            & \cellcolor{highlightblue}/       & 0.021    & \underline{0.070}  & 0.053  & 0.059  & 0.006   & -0.065   & 0.058    & \textbf{0.124}   & 0.040   \\
%     & Rt            & \cellcolor{highlightblue}/       & \textbf{0.177}    & 0.020  & 0.069  & \underline{0.146} & -0.010  & -0.015   & 0.082    & 0.086    & 0.051   \\
%     & At            & \cellcolor{highlightblue}/       & 0.398    & 0.313  & 0.346  & \underline{0.436} & 0.190   & -0.053   & \textbf{0.456}  & 0.178    & 0.321   \\
%     & Ft            & \cellcolor{highlightblue}/       & \underline{0.031} & \textbf{0.053}  & 0.004  & 0.011 & -0.010  & -0.003   & 0.020    & 0.004    & 0.007   \\
%     \rowcolor{highlightgray}
%     & Acc (\%)      & \cellcolor{highlightblue}21.6    & \underline{84.4}  & 67.2   & 68.8   & \textbf{86.8}   & 39.2    & 8.0      & 83.2    & 60.8    & 63.6    \\
%     \rowcolor{highlightgray}
%     & $\Delta$ Acc (\%) & \cellcolor{highlightblue}/    & \underline{+62.8} & +45.6  & +47.2  & \textbf{+65.2}   & +17.6   & -13.6    & +61.6   & +39.2   & +42.0   \\
%     \midrule

%     \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Math\\Geometry}}} 
%     & Pt            & \cellcolor{highlightblue}/       & \underline{0.055} & 0.065  & 0.059  & \textbf{0.071}  & 0.004   & -0.055   & 0.038    & 0.105   & 0.015   \\
%     & Rt            & \cellcolor{highlightblue}/       & \textbf{0.085}    & 0.024  & 0.019  & \underline{0.067}  & 0.016   & 0.014    & 0.047    & 0.032    & 0.011   \\
%     & At            & \cellcolor{highlightblue}/       & 0.486    & 0.368  & 0.349  & \textbf{0.530}  & 0.138   & -0.035   & \underline{0.527}  & 0.186    & 0.333   \\
%     & Ft            & \cellcolor{highlightblue}/       & \textbf{0.054}    & 0.035  & 0.006  & \underline{0.051} & -0.018  & -0.004   & 0.025    & -0.007   & 0.005   \\
%     \rowcolor{highlightgray}
%     & Acc (\%)      & \cellcolor{highlightblue}14.4    & \underline{82.4}  & 63.6   & 57.6   & \textbf{86.4}   & 28.4    & 6.4      & 78.0    & 46.0    & 50.8    \\
%     \rowcolor{highlightgray}
%     & $\Delta$ Acc (\%) & \cellcolor{highlightblue}/    & \underline{+68.0} & +49.2  & +43.2  & \textbf{+72.0}   & +14.0   & -8.0     & +63.6   & +31.6   & +36.4   \\
%     \midrule
    

%     \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{ATP\\Coq}}} 
%     & Pt            & \cellcolor{highlightblue}/      & 0.010   & \textbf{0.038} & 0.015  & 0.014  & 0.014   & 0.018   & \underline{0.032} & 0.007    & 0.018   \\
%     & Rt            & \cellcolor{highlightblue}/      & \textbf{0.067}    & -0.016 & 0.016  & 0.029  & \underline{0.056} & 0.013   & 0.038   & 0.039    & -0.137   \\
%     & At            & \cellcolor{highlightblue}/      & \textbf{0.795}    & 0.391  & 0.115  & 0.615  & 0.122   & 0.028   & \underline{0.706}  & 0.204    & 0.190   \\
%     & Ft            & \cellcolor{highlightblue}/      & \underline{0.027}    & 0.018  & \textbf{0.033}  & 0.026 & 0.014   & -0.015  & 0.024  & 0.001    & 0.009   \\
%     \rowcolor{highlightgray}
%     & Acc (\%)      & \cellcolor{highlightblue}6.4    & \textbf{96.4}     & 49.5   & 24.3   & 74.8   & 27.0    & 10.8    & \underline{86.5}   & 31.5    & 14.4    \\
%     \rowcolor{highlightgray}
%     & $\Delta$ Acc (\%) & \cellcolor{highlightblue}/   & \textbf{+90.0}    & +43.1  & +17.9  & +68.4   & +20.6   & +4.4    & \underline{+80.1}   & +25.1   & +8.0    \\
%     \midrule

%     \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{ATP\\Lean 4}}} 
%     & Pt            & \cellcolor{highlightblue}/      & 0.002    & -0.013  & -0.004 & -0.007  & \underline{0.003}  & \textbf{0.020}   & -0.015  & -0.017   & -0.005   \\
%     & Rt            & \cellcolor{highlightblue}/      & \textbf{0.059}    & -0.020  & 0.005  & 0.020  & -0.017  & 0.011   & -0.006  & \underline{0.029}   & -0.000   \\
%     & At            & \cellcolor{highlightblue}/      & \textbf{0.662}    & 0.396  & 0.193  & \underline{0.486} & 0.068   & 0.012   & 0.375  & 0.095    & 0.030   \\
%     & Ft            & \cellcolor{highlightblue}/      & \textbf{0.098}    & 0.007  & 0.013  & \underline{0.050}  & -0.018  & 0.012   & 0.033  & 0.028    & 0.020   \\
%     \rowcolor{highlightgray}
%     & Acc (\%)      & \cellcolor{highlightblue}2.7    & \textbf{84.7}     & 39.6   & 23.4   & \underline{57.7}   & 6.3     & 8.1     & 41.4    & 16.2    & 7.2     \\
%     \rowcolor{highlightgray}
%     & $\Delta$ Acc (\%) & \cellcolor{highlightblue}/   & \textbf{+82.0}    & +36.9  & +20.7  & \underline{+55.0}   & +3.6    & +5.4    & +38.7   & +13.5   & +4.5    \\
%     \midrule

%     \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{ATP\\Isabelle}}} 
%     & Pt            & \cellcolor{highlightblue}/      & 0.025    & 0.030  & -0.006 & \underline{0.048}  & \textbf{0.058}   & -0.014  & 0.020  & 0.035    & 0.043   \\
%     & Rt            & \cellcolor{highlightblue}/      & \underline{0.046}    & -0.012 & -0.006 & 0.041  & 0.014   & 0.006   & \textbf{0.048}  & 0.007    & -0.032   \\
%     & At            & \cellcolor{highlightblue}/      & \underline{0.523}    & 0.249  & 0.176  & 0.434  & -0.071  & -0.068  & \textbf{0.542}  & -0.064   & 0.155   \\
%     & Ft            & \cellcolor{highlightblue}/      & \textbf{0.082}    & 0.021  & 0.017  & \underline{0.036}  & -0.028  & 0.003   & 0.012  & 0.004    & 0.005   \\
%     \rowcolor{highlightgray}
%     & Acc (\%)      & \cellcolor{highlightblue}7.2    & \textbf{74.8}     & 36.0   & 25.2   & 63.1   & 4.5     & 0.0     & \underline{69.4}   & 5.4     & 24.3    \\
%     \rowcolor{highlightgray}
%     & $\Delta$ Acc (\%) & \cellcolor{highlightblue}/   & \textbf{+67.6}    & +28.8  & +18.0  & +55.9  & -2.7    & -7.2    & \underline{+62.2}   & -1.8    & +17.1   \\
%     \midrule

%     \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Robot\\Cooperation}}} 
%     & Pt            & \cellcolor{highlightblue}--      & \textbf{0.1140}   & 0.0748  & -0.0235  & 0.0895  & -0.0049  & -0.0142  & \underline{0.1069}   & 0.0208   & 0.0426   \\
%     & Rt            & \cellcolor{highlightblue}--      & \textbf{0.3879}   & 0.1888  & 0.1157   & 0.2683  & 0.0329   & -0.0001  & \underline{0.3292}   & -0.004   & 0.1524   \\
%     & At            & \cellcolor{highlightblue}--      & \textbf{0.3186}   & 0.1957  & 0.0078   & 0.2768  & 0.0521   & -0.0211  & \underline{0.3162}   & 0.2043   & 0.1750   \\
%     & Ft            & \cellcolor{highlightblue}--      & \textbf{0.0172}   & -0.0034 & -0.0124  & 0.0029  & \underline{0.0040}  & -0.0014  & 0.0011   & -0.0122  & -0.0078  \\
%     \rowcolor{highlightgray}
%     & Reward (\%)   & \cellcolor{highlightblue}8.85   & \textbf{92.63}    & 54.43   & 17.60   & 72.59   & 17.27    & 5.17    & \underline{84.18}   & 29.75    & 45.06    \\
%     \rowcolor{highlightgray}
%     & $\Delta$ Reward (\%) & \cellcolor{highlightblue}-- & \textbf{+83.78}   & +45.58  & +8.75   & +63.74   & +8.42    & -3.68   & \underline{+75.33}   & +20.90   & +36.21   \\
%     \midrule

%     \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Operating\\System}}} 
%     & Pt            & \cellcolor{highlightblue}--      & \textbf{0.0777}   & 0.0420  & 0.0465  & 0.0596  & 0.0318   & 0.0042  & 0.0501   & 0.0645   & \underline{0.0769}   \\
%     & Rt            & \cellcolor{highlightblue}--      & \textbf{0.4578}   & 0.3050  & 0.3051  & 0.3113  & 0.1938   & 0.0465  & \underline{0.3949}   & 0.2149   & 0.3126   \\
%     & At            & \cellcolor{highlightblue}--      & \textbf{0.0705}   & 0.0645  & 0.0414  & 0.0531  & 0.0089   & 0.0188  & \underline{0.0700}   & 0.0597   & 0.0397   \\
%     & Ft            & \cellcolor{highlightblue}--      & -0.0079   & \underline{0.0199}  & 0.0044  & \textbf{0.0368}  & 0.0008   & 0.0188  & 0.0045   & -0.0057  & 0.0119   \\
%     \rowcolor{highlightgray}
%     & Reward (\%)   & \cellcolor{highlightblue}0.98   & \textbf{60.78}    & 44.12   & 40.71   & 47.06   & 24.51    & 9.80    & \underline{52.94}   & 34.31    & 45.1    \\
%     \rowcolor{highlightgray}
%     & $\Delta$ Reward (\%) & \cellcolor{highlightblue}--   & \textbf{+59.80}   & +43.14  & +39.73  & +46.08  & +23.53   & +8.82   & \underline{+51.96}   & +33.33   & +44.12   \\
%     \midrule
    
%   \end{tabular}%
%   }
%   \vspace{-0.5cm}
% \end{table*}


\section{Evaluation}
\subsection{Experimental Implementation}
In our experiments, we establish \textbf{Llama3-8B-Instruct} as the default implementation for all four core modules: planning, reasoning, action, and reflection. For each evaluation, we systematically replace the default implementation of one module with its test variant(driven by the test model), while keeping other modules in their default state. This systematic replacement generates $2^4 = 16$ distinct configurations for the four-module architecture. For each configuration $S$, we measure the task success rate $v(S)$ across a range of benchmark scenarios to ensure robust and representative performance data.

We evaluate nine large language models, which are categorized into three groups:
\begin{itemize}
 \item \textbf{Closed API Models}: This includes four widely used commercial API-based models: Anthropic/Claude-3.5-Sonnet, OpenAI/GPT-4-turbo-0409, OpenAI/GPT-4o-mini, GLM-4-air, and Doubao-pro-4k.
 \item \textbf{Mid-parameter Open-Source Models (32B-100B)}: To assess mid-scale architectures, we evaluate three models: Llama3.1-70B-Instruct and Mixtral-8x7B-Instruct-v0.1 (46.7B).
 \item \textbf{Low-parameter Open-Source Models ($\leq$32B)}: For lightweight implementations, we include Qwen2.5-32B-Instruct and Mistral-8B-Instruct-v0.2.
\end{itemize}

The selected models span a broad parameter range, including both open-source and closed-source architectures, enabling a comprehensive comparison of their performance and adaptability within our benchmark framework. All experiments are conducted on NVIDIA A100-80GB GPUs, with vLLM employed for efficient inference of open-source models.



\begin{table*}[t]
    \caption{Experimental Results Across Datasets. Metrics for baseline models are highlighted in blue. The evaluation covers nine models across five primary tasks, showcasing notable performance variations and unique module contributions. Results marked with `*` below each dataset indicate the best-performing model combinations computed based on Shapley Value.}

  \label{tab:consolidated_results_vertical}
  \vspace{-0.5cm}
  \centering
  \begin{flushleft}
  \end{flushleft}
  \definecolor{headergray}{RGB}{240,240,240}
  \definecolor{highlightblue}{RGB}{230,240,255}
  \definecolor{highlightgray}{RGB}{248,248,248}
  \resizebox{\textwidth}{!}{%
  % \begin{tabular}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X ccccccccccc}
   \begin{tabular}{llccccccccccc}
    \hline    
    \toprule
    \rowcolor{headergray}
    \textbf{Dataset} & \textbf{Metric} & \textbf{\texttt{\shortstack{Llama3\\8B}}} & \textbf{\texttt{\shortstack{Claude\\3.5}}} & \textbf{\texttt{\shortstack{gpt-4o\\mini}}} & \textbf{\texttt{\shortstack{glm-4\\air}}} & \textbf{\texttt{\shortstack{qwen2.5\\32B}}} & \textbf{\texttt{\shortstack{Mistral\\8X7B}}} & \textbf{\texttt{\shortstack{Mistral\\7B}}} & \textbf{\texttt{\shortstack{gpt-4\\turbo}}} & \textbf{\texttt{\shortstack{doubao\\pro-4k}}} & \textbf{\texttt{\shortstack{Llama3\\70B}}} \\
    \midrule

    \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Online\\Shopping\\ \textit{Acc: 43.31*}}}} 
    & Pt            & \cellcolor{highlightblue}--      & -0.004   & 0.071 & \textbf{0.106} & -0.030  & -0.048  & 0.024  & 0.026   & \underline{0.071}   & -0.028  \\
    & Rt            & \cellcolor{highlightblue}--      & 0.019   & -0.025 & \textbf{0.077} & 0.004  & \underline{0.036} & 0.016  & -0.074  & 0.011   & 0.005  \\
    & At            & \cellcolor{highlightblue}--      & 0.056   & 0.068  & -0.059 & \textbf{0.156} & 0.080  & 0.004  & 0.014   & -0.045  & \underline{0.117}  \\
    & Ft            & \cellcolor{highlightblue}--      & -0.009   & \underline{-0.003} & -0.011 & -0.021  & -0.015 & -0.022  & \textbf{0.024}  & -0.040  & -0.030  \\
    % \rowcolor{highlightgray}
    & \cellcolor{highlightgray} Acc (\%)      & \cellcolor{highlightblue}26.27   & \cellcolor{highlightgray} 32.43   & \cellcolor{highlightgray} \underline{37.43}   & \cellcolor{highlightgray} \textbf{37.50}  & \cellcolor{highlightgray} 37.18   & \cellcolor{highlightgray}31.67   & \cellcolor{highlightgray}28.48    & \cellcolor{highlightgray}25.31  & \cellcolor{highlightgray}25.95   & \cellcolor{highlightgray}32.61  \\
    \rowcolor{highlightgray}
    & $\Delta$ Acc (\%) & \cellcolor{highlightblue}--  & +6.16    & \underline{+11.16}  & \textbf{+11.23}  & +10.91  & +5.40   & +2.21   & -0.96   & -0.32   & +6.34   \\
    \midrule

    \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Navigation\\Planning\\ \textit{Acc: 74.42*}}}} 
    & Pt            & \cellcolor{highlightblue}--      &  0.000   & 0.006 & 0.001 & -0.002  & \underline{0.021} & \textbf{0.023}  & 0.008  & 0.001   & -0.009  \\
    & Rt            & \cellcolor{highlightblue}--      & \underline{0.030}   & 0.027 & -0.008 & 0.012  & -0.035  & \textbf{0.055}  & 0.014  & -0.003  & -0.019  \\
    & At            & \cellcolor{highlightblue}--      & \textbf{0.106}   & 0.081 & 0.005 & \underline{0.099}  & 0.048 & 0.042  & \underline{0.099}  & -0.051  & 0.046  \\
    & Ft            & \cellcolor{highlightblue}--      &  -0.006  & 0.002  & -0.021 & \underline{0.018}  & -0.029 & \textbf{0.007}  & 0.004 & -0.033  & -0.011  \\    
    % \rowcolor{highlightgray}
    & \cellcolor{highlightgray} Acc (\%)      & \cellcolor{highlightblue}58.70   & \cellcolor{highlightgray} \textbf{71.90}   & \cellcolor{highlightgray} 70.29   & \cellcolor{highlightgray} 61.91  & \cellcolor{highlightgray} 68.26   & \cellcolor{highlightgray} 64.45   & \cellcolor{highlightgray} \underline{71.48}    & \cellcolor{highlightgray}71.23  & \cellcolor{highlightgray}50.90  & \cellcolor{highlightgray}59.32  \\
    \rowcolor{highlightgray}
    & $\Delta$ Acc (\%) & \cellcolor{highlightblue}--  & \textbf{+13.20}   & +11.59  & +3.21   & +9.56   & +5.75   & +12.78   & \underline{+12.53}   & -7.8    & +0.62   \\
    \midrule

    \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Ticket\\Ordering\\ \textit{Acc: 67.18*}}}} 
    & Pt            & \cellcolor{highlightblue}--      & 0.003   & 0.032 & -0.195 & 0.119  & \textbf{0.183}  & -0.111  & -0.043  & \underline{0.151}  & 0.004  \\
    & Rt            & \cellcolor{highlightblue}--      & 0.186   & \underline{0.243} & 0.172  & 0.181  & 0.054  & -0.070  & \textbf{0.301}  & -0.001  & 0.089  \\
    & At            & \cellcolor{highlightblue}--      & \textbf{0.217}   & 0.049 & -0.020  & -0.000 & -0.083 & -0.020  & \underline{0.028}  & 0.006  & -0.275  \\
    & Ft            & \cellcolor{highlightblue}--      & 0.024   & 0.005  & -0.006 & \textbf{0.043}  & -0.011 & 0.002   & \underline{0.058}  & -0.027  & -0.001  \\
    % \rowcolor{highlightgray}
    & \cellcolor{highlightgray} Acc (\%)      & \cellcolor{highlightblue}19.94   & \cellcolor{highlightgray} \textbf{62.85}   & \cellcolor{highlightgray} 51.82   & \cellcolor{highlightgray} 15.01   & \cellcolor{highlightgray} 54.25   & \cellcolor{highlightgray} 34.24   & \cellcolor{highlightgray} 0.00   & \cellcolor{highlightgray} \underline{54.37}   & \cellcolor{highlightgray} 32.88  & \cellcolor{highlightgray} 1.59  \\
    \rowcolor{highlightgray}
    & $\Delta$ Acc (\%) & \cellcolor{highlightblue}--  & \textbf{+42.91}   & +31.88  & -4.93  & +34.31  & +14.30   & -19.94   & \underline{+34.43}   & +12.94   & -18.35   \\

    \midrule
    
    \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Math \\ \textit{Acc:83.80*}}}} 
    & Pt            & \cellcolor{highlightblue}--      & 0.038& \underline{0.067}& 0.056& 0.065& 0.005& -0.060& 0.048& \textbf{0.115}& 0.028 \\
    & Rt            & \cellcolor{highlightblue}--     & \textbf{0.131}& 0.021& 0.044& \underline{0.107}& 0.003& -0.000& 0.065& 0.059& 0.031\\
    & At            & \cellcolor{highlightblue}--     & 0.442& 0.343& 0.348& \underline{0.483}& 0.164& -0.044& \textbf{0.492}& 0.182& 0.327 \\
    & Ft            & \cellcolor{highlightblue}--     & \underline{0.042}& \textbf{0.043}& 0.005& 0.031& -0.014& -0.003& 0.022& -0.002& 0.006\\
    \rowcolor{highlightgray}
    & Acc (\%)      & \cellcolor{highlightblue}18.00    & \underline{83.40}& 65.40& 63.20& \textbf{86.60}& 33.80& 7.20& 80.60& 53.40& 57.20\\
    \rowcolor{highlightgray}
    & $\Delta$ Acc (\%) & \cellcolor{highlightblue}--   & \underline{65.40}& 47.40& 45.20& \textbf{68.60}& 15.80& -10.80& 62.60& 35.40& 39.20 \\
    \midrule
    
    \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{ATP\\ \textit{Acc: 86.79*}}}} 
    & Pt            & \cellcolor{highlightblue}-- & 0.012& 0.018& 0.002& 0.018& \textbf{0.025}& 0.008& 0.012& 0.016& \underline{0.019}        \\
    & Rt            & \cellcolor{highlightblue}--  & \textbf{0.057}& -0.016& 0.005& \underline{0.030}& 0.018& 0.010& 0.027& 0.019& -0.056       \\
    & At            & \cellcolor{highlightblue}-- & \textbf{0.660}& 0.345& 0.161& 0.511& 0.039& -0.009& \underline{0.541}& 0.084& 0.125       \\
    & Ft            & \cellcolor{highlightblue}--  & \textbf{0.069}& 0.015& 0.021& \underline{0.037}& -0.011& -0.000& 0.023& 0.004& 0.011      \\
    \rowcolor{highlightgray}
    & Acc (\%)      & \cellcolor{highlightblue}5.45  & \textbf{85.29}& 41.74& 24.32& 65.17& 12.61& 6.31& \underline{65.77}& 17.72& 15.32     \\
    \rowcolor{highlightgray}
    & $\Delta$ Acc (\%) & \cellcolor{highlightblue}--  & \textbf{79.84}& 36.29& 18.87& 59.72& 7.16& 0.86& \underline{60.32}& 12.27& 9.874  \\
    \midrule

    \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Robot\\Cooperation\\ \textit{Rwd: 92.63*}}}} 
    & Pt            & \cellcolor{highlightblue}--      & \textbf{0.114}   & 0.075  & -0.024  & 0.090  & -0.005  & -0.014  & \underline{0.107}   & 0.021   & 0.043   \\
    & Rt            & \cellcolor{highlightblue}--      & \textbf{0.388}   & 0.189  & 0.116   & 0.268  & 0.033   & -0.000  & \underline{0.329}   & -0.004   & 0.152   \\
    & At            & \cellcolor{highlightblue}--      & \textbf{0.319}   & 0.196  & 0.008   & 0.277  & 0.052   & -0.021  & \underline{0.316}   & 0.204   & 0.175   \\
    & Ft            & \cellcolor{highlightblue}--      & \textbf{0.017}   & -0.003 & -0.012  & 0.003  & \underline{0.004}  & -0.001  & 0.001   & -0.012  & -0.008  \\
    % \rowcolor{highlightgray}
    & \cellcolor{highlightgray}Reward (\%)   & \cellcolor{highlightblue}8.85   & \cellcolor{highlightgray}\textbf{92.63}    & \cellcolor{highlightgray}54.43   & \cellcolor{highlightgray}17.60   & \cellcolor{highlightgray}72.59   & \cellcolor{highlightgray}17.27    & \cellcolor{highlightgray}5.17    & \cellcolor{highlightgray}\underline{84.18}   & \cellcolor{highlightgray}29.75    & \cellcolor{highlightgray}45.06    \\
    \rowcolor{highlightgray}
    & $\Delta$ Reward (\%) & \cellcolor{highlightblue}-- & \textbf{+83.78}   & +45.58  & +8.75   & +63.74   & +8.42    & -3.68   & \underline{+75.33}   & +20.90   & +36.21   \\
    \midrule

    \multirow{6}{*}{\cellcolor{highlightgray}\textbf{\shortstack{Operating\\System\\ \textit{Acc: 60.78*}}}} 
    & Pt            & \cellcolor{highlightblue}--      & \textbf{0.078}   & 0.042  & 0.047  & 0.060  & 0.032   & 0.004  & 0.050   & 0.065   & \underline{0.077}   \\
    & Rt            & \cellcolor{highlightblue}--      & \textbf{0.458}   & 0.305  & 0.305  & 0.311  & 0.194   & 0.047  & \underline{0.395}   & 0.215   & 0.313   \\
    & At            & \cellcolor{highlightblue}--      & \textbf{0.071}   & 0.065  & 0.041  & 0.053  & 0.009   & 0.019  & \underline{0.070}   & 0.060   & 0.040   \\
    & Ft            & \cellcolor{highlightblue}--      & -0.008   & \underline{0.020}  & 0.004  & \textbf{0.037}  & 0.001   & 0.019  & 0.005   & -0.006  & 0.012   \\
    % \rowcolor{highlightgray}
    & \cellcolor{highlightgray}Acc (\%)   & \cellcolor{highlightblue}0.98   & \cellcolor{highlightgray}\textbf{60.78}    & \cellcolor{highlightgray}44.12   & \cellcolor{highlightgray}40.71   & \cellcolor{highlightgray}47.06   & \cellcolor{highlightgray}24.51    & \cellcolor{highlightgray}9.80    & \cellcolor{highlightgray}\underline{52.94}   & \cellcolor{highlightgray}34.31    & \cellcolor{highlightgray}45.10    \\
    \rowcolor{highlightgray}
    & $\Delta$ Acc (\%) & \cellcolor{highlightblue}--   & \textbf{+59.80}   & +43.14  & +39.73  & +46.08  & +23.53   & +8.82   & \underline{+51.96}   & +33.33   & +44.12   \\
    \midrule
    
  \end{tabular}
  }
  \vspace{-0.3cm}
\end{table*}



\subsection{Main Results}
\input{Figure/radarChart/radar}

We conducted a systematic evaluation of nine different models across five primary tasks, revealing significant performance disparities and distinct module contribution patterns. The following sections provide a detailed analysis of key findings in each task domain, supplemented by comprehensive insights derived from the experimental results presented in Table~\ref{tab:consolidated_results_vertical}. Results for the sub-datasets under the MATH solver and ATP can be found in the appendix.

\paragraph{Online Shopping Performance}
In the e-commerce evaluation, model performance exhibited clear hierarchical differentiation. High-performance models, specifically \texttt{GLM-4-air} (37.50\%) and \texttt{GPT-4o-mini} (37.43\%), significantly outperformed the baseline model (\texttt{Llama3-8B}: 26.27\%). This improvement is primarily attributed to effective module synergy and optimized action execution. \texttt{GLM-4-air} demonstrated superior performance in the Planning ($P$: 0.1058) and Reasoning ($R$: 0.0770) modules, underscoring the importance of advanced cognitive abilities in managing complex shopping tasks. Additionally, \texttt{Qwen2.5}'s notable performance in the Action module ($A$: 0.1557) highlights the critical role of precise action selection in enhancing task success rates. The reflection capabilities of \texttt{GPT-4o-turbo} ($F$: 0.0244) further emphasize the significance of dynamic strategy adjustments in interactive scenarios.


\paragraph{Math Solver Performance}
The mathematical problem-solving evaluation encompassed both algebra and geometry sub-tasks, revealing distinct performance characteristics. In algebra, \texttt{Qwen2.5} achieved an impressive accuracy of 86.8\%, marking a 65.2 percentage point improvement over the baseline. This performance is largely due to its robust Planning ($P$: 0.059) and Action ($A$: 0.436) modules, which facilitate effective strategy formulation and execution. Similarly, \texttt{Claude-3.5} excelled in the Reasoning module ($R$: 0.177), highlighting its capacity for complex mathematical derivations.
In geometry, \texttt{Qwen2.5} maintained a leading accuracy of 86.4\%, supported by balanced contributions across Planning ($P$: 0.071), Reasoning ($R$: 0.067), and Action ($A$: 0.530) modules. This balance indicates the necessity of multi-dimensional capabilities in solving geometric problems. The consistently high Shapley values for the Action module across models further emphasize the importance of precise step execution in this domain.

\paragraph{Automatic Theorem Proving Performance}
The Automatic Theorem Proving task evaluates models' abilities to reason and execute formal proofs in Coq, Lean4, and Isabelle. Table \ref{tab:consolidated_results_vertical} highlights that \texttt{Claude-3.5} achieves the best performance across all three systems, with significant $\Delta$ Accuracy improvements (+90.0\%, +82.0\%, and +67.6\%), driven by its strong Action (\texttt{At}) contributions. \texttt{qwen2.5} also performs well, particularly in Isabelle, with competitive Reasoning (\texttt{Rt}) and Action (\texttt{At}) scores.
The results emphasize the importance of precise execution (Action) and logical inference (Reasoning) for success in theorem proving, while Reflection (\texttt{Ft}) plays a limited role. This outcome may stem from the highly structured nature of theorem proving, which rewards models capable of following strict formal rules and applying precise, sequential reasoning without extensive trial-and-error.



\paragraph{Operation System Performance}  
The Operation System task highlights the critical role of reasoning ability, as much of the necessary task information is acquired through ongoing interactions rather than being fully available initially. This explains the higher Shapley Values for Reasoning ($R$: up to 0.4578) compared to Planning ($P$: up to 0.0777). Additionally, the benchmark places relatively low demands on action ability due to the close alignment of benchmark commands with real-world formats, reducing the complexity of action execution. Reflection ($F$) contributes minimally, as the task lacks strong feedback signals for iterative improvement. \texttt{Claude-3.5} achieved the best performance (60.78\% accuracy), emphasizing the importance of reasoning in dynamic OS environments.

\paragraph{Robot Cooperation Performance}
Robot cooperation tasks best demonstrated the models' comprehensive capabilities. \texttt{Claude-3.5} led with a reward score of 92.63\% and achieved the highest Reasoning module Shapley value ($R$: 0.3879) across all tasks. This result highlights the central role of reasoning abilities in multi-agent collaboration. Additionally, all modules exhibited relatively high contributions ($P$: 0.1140, $A$: 0.3186, $F$: 0.0172), confirming that complex cooperative scenarios necessitate balanced development across all functional areas.
The Shapley Values, based on marginal contribution averages, consistently align with model performance across tasks, demonstrating their stability and reliability. High-performing models, such as \texttt{Claude-3.5} and \texttt{Qwen2.5}, exhibit strong Shapley Values in key modules (e.g., Action for theorem proving, Reasoning for math solving), which correspond to their high task success rates. Conversely, weaker models like \texttt{Mistral-7B} show uniformly low or negative Shapley Values, reflecting their poor performance. Moreover, the Shapley Values adapt to task-specific demands, emphasizing Planning and Reasoning in Shopping and Math tasks, while prioritizing Action in theorem proving. This consistency validates Shapley Value as a robust framework for assessing modular contributions in diverse tasks.


% \vspace{-0.1cm}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/Model_Configuration3.pdf}
    \vspace{-0.2cm}
    % \caption{Model Configuration Accuracy Comparison}
    \caption{Shapley value results of all combinations in Math (Algebra) for Claude-3.5-Sonnet under different model configurations. The pattern of the bars indicates the number of modules (ranging from 0 to 4) that Claude is involved in.}
    \label{fig:Model Configuration Accuracy Comparison}
    \vspace{-0.4cm}
\end{figure}
% \vspace{-0.5cm}


\paragraph{Module Impact via Replacement}
The experimental results in Figure~\ref{fig:Model Configuration Accuracy Comparison} confirm that module replacement accurately reflects its impact on system performance, as demonstrated by \texttt{Claude-3.5-Sonnet} on Algebra. 
High-contribution module configurations, identified through Shapley Value calculations, achieve significantly better performance. For instance, the configuration \texttt{(P,R,A)} achieves a success rate of 78.0\%, far surpassing the baseline configuration with Llama3-8b-Instruct at 21.6\%. Incremental module replacements align with theoretical predictions: only substituting the default Planning module with the test Planning module improves performance to 18.4\% \texttt{(P)}, while further integrating a strong Action module (\texttt{A}) boosts it to 63.2\% \texttt{(P,A)}.
Synergistic effects are particularly evident in configurations like \texttt{(P,R,A)}, which leverage robust Planning and Action capabilities to achieve peak success rates. In contrast, configurations with low-contribution modules result in diminished performance, as seen in \texttt{(P,F)}, which achieves only 0.212. These results highlight the predictive power of Shapley Values in quantifying module contributions and confirm the alignment of task outcomes with theoretical expectations, reinforcing the validity of the framework.

\paragraph{Predictive Module Combinations}  
The experimental results in Table 2 demonstrate that modules with higher Shapley Values consistently lead to improved task performance when combined. For instance, in the "Online Shopping" dataset, the optimal combination achieves an accuracy of 43.31\%, which is significantly higher compared to the other models, indicating the advantage of leveraging high-contribution modules. Similarly, in ATP, the best combination computed based on Shapley Values results in an 86.79\% accuracy, showcasing a marked improvement over alternatives. These results demonstrate that identifying and integrating key modules with high Shapley Values enables CapaBench to systematically maximize performance across tasks, validating Shapley Values as a reliable guide for module selection and optimization.





\subsection{Ablation Study}
In this section, we examine how changing the default model in our evaluation framework affects the Shapley Value results and the relative ranking of various LLMs. Specifically, we replace our original default model (\texttt{Llama3-8B-instruct}) with the model (\texttt{gpt-3.5-turbo-0613}) and re-run the evaluation on the same set of seven test LLMs over the Robot Cooperation Task.  Our aim is to examine (i) whether our evaluation framework is robust against different baseline capabilities, and (ii) to what extent the relative ranking of the test models is affected by this change.

\vspace{-0.2cm}
\begin{figure}[htbp]
    \centering
    % 第一行 1x4 图片
    \begin{minipage}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figure/ablation/P.png} 
        \subcaption{Planning}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figure/ablation/R.png}
        \subcaption{Reasoning}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figure/ablation/A.png} 
        \subcaption{Action}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figure/ablation/F.png} 
        \subcaption{Reflection}
    \end{minipage}

    % 第二行图例
    \vspace{0.1cm} 
    \includegraphics[width=0.3\textwidth]{Figure/ablation/legend.png} % 图例宽度设置
    \vspace{-0.1cm}     
    \caption{Comparation of Shapley Value under different default models.}
    \label{fig:ablation study}
\vspace{-0.3cm}   
\end{figure}
% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[t]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/ablation/P.png}
%         \caption{Planning}
%     \end{subfigure}
%     \hspace{0.02\textwidth} % 调整子图之间的间距
%     \begin{subfigure}[t]{0.43\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/ablation/R.png}
%         \caption{Reasoning}
%     \end{subfigure}
%     \hspace{0.02\textwidth} % 调整子图之间的间距
%     \begin{subfigure}[t]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/ablation/A.png}
%         \caption{Action}
%     \end{subfigure}
%     \hspace{0.02\textwidth} % 调整子图之间的间距
%     \begin{subfigure}[t]{0.43\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figure/ablation/F.png}
%         \caption{Reflection}
%     \end{subfigure}
%     \vspace{0.1cm} % 控制图片和图例的间距
%     \includegraphics[width=0.25\textwidth]{Figure/ablation/legend.png} 
%     \vspace{-0.3cm}
%     \caption{Comparison of default models on Robot Cooperation.}
%     \label{fig:ablation study}
%     \vspace{-0.4cm} % 控制整个图和正文的间距
% \end{figure}


Figure~\ref{fig:ablation study} illustrates the Shapley Value results for the four modules under 2 default models. Although the absolute Shapley Values vary due to the differences in baseline model capabilities, our primary focus is on the consistency of test model rankings. 

To quantify this consistency, we define the \emph{preference pair consistency rate} as \[
\textnormal{Pairwise Consistency Rate} = \frac{\{\textnormal{Consistent Preference Pairs}\}}{\{\textnormal{All Model Pairs}\}}
\],
which measures the proportion of test model pairs that maintain the same relative ranking across both experiments. A higher rate indicates that changes to the default model have minimal impact on the relative ranking of test models.

The Results show that Reasoning achieves the highest consistency rate (91.67\%), followed by Action (86.11\%), Planning (72.22\%), and Reflection (58.33\%). 
The high overall consistency (85.18\%) confirms that our evaluation framework is robust against changing the default model for most modules. Notably, \emph{Reasoning} and \emph{Action}, which contribute most to task success according to Shapley Values, also exhibit the highest ranking consistency. By contrast, \emph{Reflection} shows the lowest consistency (58.33\%), suggesting that its assessment may be more sensitive to the default model choice or that the reflection module requires further refinement. Overall, while absolute Shapley Values naturally shift under a stronger or weaker default model, the \emph{relative} ordering of test models—and thus the key insights into each model's strengths and weaknesses—remains largely stable. 



% \textbf{Experiment Results.} 
% Figure.\ref{fig:four_images} demonstrates a comparison of Shapley values under two experimental setups: one using \texttt{Llama3-8B-instruct} as the default model and the other using \texttt{GPT-3.5-turbo-0613} as the default model.

% Shapley Value itself reflects the relative improvement of the test model's capability in a specific module compared to the default model's capability in that same module. Thus the primary focus should be on the ordering of seven test models rather than the absolute Shapley Value when replacing the default model. We introduced an innovative approach by using preference pair consistency rate to demonstrate the relative ranking of module capabilities remained consistent across two different default model settings.

% Preference pair consistency rate means the number of the same model preference pair judged by two default model experiment over all different model pairs, AKA, randomly given a test model pair (test model A, test model B), the probability of the two sets of shapley values give the same partial order on this test model pair, that is, both sets of experiments simultaneously show that test model A is better than test model B or test model A is worse than test model B.

% The calculation equation is
% $$
% \text{preference pair consistency rate} = \frac{\text{\# of different model pairs that two experiments give the same partial order}}{\text{\# of different model pairs}}
% $$

% Table.\ref{tab:preference_consistence} demonstrates the preference consistence rate results

% \begin{table*}[h]
%   \caption{preference pair consistence rate of Llama-3-8B-instruct as default model and GPT-3.5-turbo-0613 as default model on Robot Cooperation}
%   \label{tab:preference_consistence}
%   \centering
%   \resizebox{\textwidth}{!}{%
%   \begin{tabular}{cccccc}
%     \toprule
%      & Planning & Reasoning & Action & Reflection & Overall  \\
%     \midrule
%     \texttt{Preference Pair Consistence Rate} & 72.22\% & 91.67\% & 86.11\% & 58.33\% & 85.18\%\\
%     \bottomrule
%   \end{tabular}}
% \end{table*}

% From our experimental results, we observe an overall preference consistency rate of 85.18\% across all four modules. This indicates that our experiments are relatively robust, showing that replacing the default model does not introduce significant perturbations on evaluation.

% Combining our observations with the Shapley Value analysis, it comes that module with higher Shapley Value shows more robustness over changing default model. The shapley value results demonstrate that generally reasoning contributes most to the success of task completion, followed by action, planning and reflection. And reasoning also has the highest preference pair consistence rate of 91.67\% followed by action, planning and reflection. This implies that Robot Cooperation primarily evaluates the reasoning and action capabilities of LLMs. Consequently, the perturbations caused by replacing the default model are minimal in these two areas, reflecting their stability and importance within the task framework.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figure/radarChart/radar_chart2.png}
    % \vspace{-0.2cm}
    % \caption{Model Configuration Accuracy Comparison}
    \caption{Radar plot comparing model performance across tasks with key contributions.}
    \label{fig:radar chart}
    \vspace{-0.5cm}
\end{figure}

\vspace{-0.2cm}
\subsection{Analysis}
Based on Table~\ref{tab:consolidated_results_vertical}, we further enrich our analysis with the following insights:
\vspace{-0.2cm}

\paragraph{Cross-Task Model Performance Comparison}
A high-level comparison of model performance across diverse tasks reveals distinct strengths and weaknesses. Notably, \texttt{Claude-3.5} outperforms other models in most categories, showing particular prowess in formal verification (e.g., Coq, Lean 4, Isabelle) and robot cooperation tasks. This advantage suggests that \texttt{Claude-3.5} has a robust underlying chain-of-thought reasoning mechanism and effective multi-agent collaboration strategies—capabilities essential for tasks that demand precise logical proof structures and synchronized actions.
On the other hand, open-source models like \texttt{Qwen-2.5} and \texttt{Mistral-8X7B} exhibit moderate gains in more straightforward domains, such as shopping or basic Algebra, but underperform in cognitive-heavy tasks. Their lag in automatic theorem proving and robot cooperation implies that while these models may be adept at handling routine queries and procedural problem-solving, they lack the deeper reasoning, advanced planning, or specialized modules needed for high-stakes coordination and rigorous proof validation. Strengthening these areas—possibly through fine-tuning on specialized corpora or integrating more advanced tool usage—could help bridge the gap between open-source and proprietary models in complex, multi-stage tasks.



\paragraph{Module Contribution Patterns}  
Our findings highlight that module contributions vary according to task demands, reflecting the distinct cognitive processes involved. Specifically:  
% \vspace{-0.1cm}  
\begin{itemize}   
    \item \textbf{Tasks with High Cognitive Complexity (e.g., Online Shopping, Robot Cooperation, and OS):} \emph{Reasoning} and \emph{Planning} play pivotal roles. Online shopping requires balancing constraints (e.g., budget and preferences) and sequencing decisions effectively. In robot cooperation, \emph{Reasoning} enables dynamic information updates and efficient task distribution among agents. Operation system tasks, involving troubleshooting and resource management, rely heavily on real-time problem-solving and feedback interpretation. Across these tasks, robust \emph{Reasoning} ensures logical inference and decision-making under uncertainty.  
    \item \textbf{Tasks Requiring Precision (e.g., Math Solvers and ATP):} \emph{Action} is the dominant module. In math solvers, particularly geometry, precise procedural execution, such as applying theorems or constructing diagrams, outweighs strategic planning. Similarly, in formal verification tasks (e.g., Coq or Lean), strict adherence to syntactic and semantic correctness is critical. Both scenarios demand meticulous step-by-step actions to ensure reliability and prevent errors.  
\end{itemize}  
\vspace{-0.1cm}  

\input{Figure/radarChart/radar}

By identifying module-specific dependencies, developers can target optimizations, such as enhancing \emph{Reasoning} for dynamic decision-making or refining \emph{Action} for procedural accuracy, to maximize performance across diverse domains.

\paragraph{Low Reflection Contribution}
We conclude the seemingly low contribution of the Reflection module to overall task performance through two main considerations. First, whether or not the reflection directly translates into a higher success rate does not necessarily reflect the true quality or efficacy of the reflection itself. In other words, task success alone may not be the best measure of how well the model is “thinking about” its own mistakes. Second, when the model reflects on its own errors without extra information or guidance from a more capable model, it may fail to pinpoint the actual causes behind its mistakes. As a result, the lack of deeper insights into error sources means reflection often does not generate meaningful improvements in task outcomes. Consequently, while the Reflection module is present, its practical impact on success rates remains limited.




\paragraph{Comparative Study}
This experiment investigates whether Shapley Values can accurately capture model-specific abilities in core competencies, including planning, reasoning, and action. To this end, we conducted a capability evaluation experiment on a subset of 238 questions from successful trajectories in the Algebra dataset, focusing on correctly completed tasks. Using successful trajectories ensures reliable annotations for Planning, Reasoning, and Action modules by providing clear labels. From these trajectories, we extracted full interaction data and split it into \emph{single-step QA samples} based on the three core modules. This process generated 2180 single-step samples. The reflection module was excluded due to its minimal impact on overall success rates and the insufficient number of successful trajectories required to build a reliable dataset for this dimension.
For each single-step sample, we asked the tested models to provide responses, which were then evaluated by GPT-o1-mini as an independent evaluator. The evaluation focused on two aspects for the Planning and Reasoning modules: \textbf{semantic rationality}, assessing whether the response is clear and comprehensible, and \textbf{task completion degree}, measuring whether the agent effectively completed the task. For the Action module, the evaluation centered on \textbf{logical comprehension ability}, which reflects the model's understanding of task logic and its ability to execute correct actions based on Planning and Reasoning.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/Math/At_Pt_Rt_line_chart.png}
    \vspace{-0.3cm}
    \caption{Planning, Reasoning, and Action Evaluation on Algebra. Each color represents an ability.  The left Y-axis shows the Shapley value with solid lines and the right Y-axis shows the GPT scores with dashed lines.  }
    \label{fig:comp_line_chart}
    \vspace{-0.4cm}
\end{figure}

Figure~\ref{fig:comp_line_chart} shows the Shapley Values and the scores given by GPT-o1-mini for each model, with Pearson correlation coefficients of \textbf{0.81, 0.77, 0.67} for the Planning, Reasoning, and Action modules, respectively. These high correlations validate the effectiveness of Shapley Values in quantifying each module's specific contribution to task success.

Furthermore, our method addresses critical limitations of ground truth-dependent evaluation approaches. Traditional methods rely on predefined ground truth, which is vulnerable to changes in task prompts or adjustments to the ground truth itself, leading to potential penalization of reasonable outputs due to reduced similarity. Additionally, traditional evaluations often ignore the diversity of valid responses and fail to capture interactions between modules, such as the interplay between planning and reasoning in guiding actions. In contrast, the Shapley-based framework holistically evaluates each module's marginal contributions and their interactions, offering a robust and flexible approach for modular analysis.

\vspace{-0.2cm}

\section{Conclusion and Future Works}
This paper introduced \textbf{CapaBench}, a game-theoretic framework that employs the Shapley Value to rigorously evaluate the contributions of individual modules in LLM agents. By calculating effects among planning, reasoning, action, and reflection components, CapaBench enables more precise attribution, guiding targeted optimization and offering predictive insights into performance across diverse tasks. Moreover, our approach can potentially extend to LLM-based Multi-Agent Systems \citep{Guo2024LargeLM,Yang2024LLMbasedMS,Sun2024LLMbasedMR}, where each module operates as a specialized sub-agent, paving the way for future explorations in agent coordination, communication, and emergent behaviors.
Moving forward, we aim to expand the variety of tasks in CapaBench to improve the robustness and transferability of our evaluation. Additionally, we plan to explore refined, domain-specific evaluation protocols that reduce computational overhead without compromising module-level insights. Ultimately, by incorporating these enhancements and investigating \emph{multi-agent} paradigms, we hope to advance both the theoretical underpinnings and practical applications of modular LLM-based AI systems.


\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}

\newpage
% 添加大标题 APPENDIX
\bigappendix
\appendix
% \section{Framework}
% \subsection{Previous Work}
% \subsection{Our Framework}

\section{Online Shopping.}
\subsection{Dataset Deatils}
The Online Shopping dataset is designed to evaluate agents' planning, reasoning, and action capabilities in completing e-commerce tasks. The dataset consists of \textbf{110 tasks}, divided into two parts: \textbf{white-box tasks (62)}, which are from the Webshop dataset, and \textbf{black-box tasks (48)}, which are expanded using GPT-4 to enhance instruction diversity and complexity.

Dataset expansion was constructed by modifying instructions from the original dataset. GPT-4 was used to rephrase instructions for greater linguistic diversity, adding context or background such as \textit{“Next week is Halloween, and I need themed decorations.”} Additionally, parameters were enriched with attributes like size, color, or material to increase task complexity. For challenging cases, explicit prompts were created to guide planning, for example, \textit{“First search for desks with wood finishes, then filter by size and price.”}

A typical instruction in Online Shopping might be:
\textit{“I'm looking for a small portable folding desk that is already fully assembled; it should have a khaki wood finish, and price lower than 140 dollars, and length bigger than 40 inches.”}

Agents are evaluated based on their ability to follow optimal trajectories, such as:
\begin{itemize}
    \item \textbf{Ideal Trajectory 1:} Search for all attributes directly \textit{("desk, wood, folding, khaki, 40 inches, \$140")} and proceed to the target item.
    \item \textbf{Ideal Trajectory 2:} Broad search \textit{("desk, wood, folding")}, filter by price, and then refine attributes (color, size).
\end{itemize}

\subsection{Experiment Deatils}
Table \ref{tab:praf_webshop} summarizes the experimental results for the Online Shopping task, including Shapley values for the four modules (Planning (\texttt{Pt}), Reasoning (\texttt{Rt}), Action (\texttt{At}), and Reflection (\texttt{Ft})), as well as task success rates (Accuracy (\%)) and their improvement ($\Delta$ Accuracy (\%)) relative to the baseline (\texttt{Llama3-8B-instruct}).

\begin{table*}[h]
  \caption{Experimental Results on Online Shopping(110 pieces)}
  \label{tab:praf_webshop}
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{ccccccc}
    \toprule
    LLM & Pt & Rt & At & Ft & Acc (\%) & $\Delta$ Acc (\%) \\
    \midrule
    \texttt{Llama3-8B-instruct (Default)} & - & - & - & - & 26.27 & - \\
    \texttt{claude\_3.5\_sonnet} &-0.0038 & 0.0187 & 0.0555 & -0.0088 & 32.43\% & +6.16 \\
    \texttt{gpt-4o-mini} & 0.0711 & -0.0251 & 0.0684 & \underline{-0.0028} & \underline{37.43} & \underline{+11.16} \\
    \texttt{glm-4-air} & \textbf{0.1058} & \textbf{0.077} & -0.0591 & -0.0114 & \textbf{37.50} & \textbf{+11.23} \\
    \texttt{gpt-4-turbo-0409} &  0.0255 & -0.0737 & 0.0142 & \textbf{0.0244} & 25.31 & -0.96 \\
    \texttt{qwen2.5-32b-ins} & -0.0299 & 0.0041 & \textbf{0.1557} & -0.0209 & 37.18 & +10.91 \\
    \texttt{Mistral-7B-Instruct} &  0.0243 & 0.0155 & 0.0043 & -0.0221 & 28.48 & +2.21 \\
    \texttt{Llama-3-70B-Instruct} &  -0.0279 & 0.0045 & \underline{0.1167} & -0.03 & 32.61 & +6.34 \\
    \texttt{doubao-pro-4k} & \underline{0.0712} & 0.0107 & -0.045 & -0.0402 & 25.95 & -0.32 \\
    \texttt{Mistral-8X7B-instruct} & -0.0476 & \underline{0.0364} & 0.0797 & -0.0147 & 31.67 & +5.40 \\
    \midrule
    \texttt{best} & / & / & / & / & \textit{43.31} & \textit{+17.04}\\
    \bottomrule
  \end{tabular}}
\end{table*}

The baseline model (\texttt{Llama3-8B-instruct}) achieves a task success rate of 26.27\%. The best-performing models, \texttt{glm-4-air} and \texttt{gpt-4o-mini}, achieve accuracies of 37.50\% and 37.43\%, corresponding to improvements of +11.23\% and +11.16\%, respectively. These results highlight their strong overall performance relative to the baseline.

% \vspace{-0.2cm}
% \begin{itemize}
%     \item \textbf{Planning (\texttt{Pt}) and Reasoning (\texttt{Rt}):} \texttt{glm-4-air} demonstrates the strongest contributions in these modules, with Shapley values of 0.105 and 0.0702, respectively, showcasing its superior ability in high-level decision-making and logical reasoning.\vspace{-0.2cm}
%     \item \textbf{Action (\texttt{At}):} \texttt{qwen2.5-32b-ins} and \texttt{Llama-3-70B-Instruct} stand out in action execution, achieving the highest Shapley values of 0.1686 and 0.1506, indicating their effectiveness in task completion.
%     \item \textbf{Reflection (\texttt{Ft}):} Contributions from this module are generally smaller, with \texttt{gpt-4-turbo-0409} showing the best performance (\textbf{0.0383}). This suggests that Reflection has a less pronounced impact on task success for this benchmark.
% \end{itemize}

The experimental results on the Online Shopping dataset reveal several notable characteristics of the evaluated models and their performance on this task. Notably, the dataset places a strong emphasis on Planning and Action capabilities, as evidenced by the high Shapley values for these modules among the top-performing models (\texttt{glm-4-air}, \texttt{qwen2.5-32b-ins}, and \texttt{Llama-3-70B-Instruct}). The task's structured nature, requiring precise attribute filtering and logical decision-making, heavily rewards models with strong planning abilities (e.g., high \texttt{Pt} values) and effective action execution (\texttt{At}). 

Additionally, the relatively low contributions from the Reflection (\texttt{Ft}) module suggest that this task does not involve significant trial-and-error or iterative refinement, which limits the importance of reflective reasoning. The dataset therefore primarily evaluates an agent's ability to efficiently process structured instructions, identify relevant attributes, and execute a coherent sequence of actions to achieve success. These findings highlight the suitability of this dataset for benchmarking models' structured decision-making and planning abilities in e-commerce-like environments, while pointing to areas where iterative reasoning may play a lesser role.

\subsection{Prompt Example}

\subsubsection{Planning Module}
\begin{lstlisting}[language=Python]
prompt_system_planning = """ 
Welcome to the Online Shopping Challenge! Four LLM agents are working together to do web-shopping tasks step by step (planning -> reasoning -> acting -> reflecting). They are responsible for planning, reasoning, acting, and reflecting respectively. 
You are the first llm agent, who is a helpful web-shopping guidance assistant in charge of planning. 
Your role is to assist players by generating strategic plans based on the game's instructions.

Here is how the game is structured:
- Each round, you will be given an instruction that describes the objective need to achieve.
- Based on the instruction, you are to generate a clear and brief strategic plan.
- Your plan will be used to guide other agents through the shopping site efficiently.
- If there is no response click[Buy Now] within 15 actions, the game fails.

Your Responsibilities:
- Analyze the original problem and break it into clear, actionable steps.
- Ensure the steps are logically ordered and comprehensive for achieving the goal.
- Use concise language, focusing only on the key actions needed to complete the task successfully.

OUTPUT FORMAT:
Keep your response concise and structure:
  Strategic Plan: (A list of sequential steps to achieve the objective)
	Step 1: ...
	Step 2: ...
	Step 3: ...
(Add more steps as necessary, but keep it streamlined and goal-oriented)

Enclose the plan with three backticks ```.

For example:
"""
\end{lstlisting}

\subsubsection{Reasoning Module Prompt}
\begin{lstlisting}[language=Python]
prompt_system_reasoning = """
Welcome to the Online Shopping Challenge!
Four llm agents are working together to do web-shopping tasks step by step(planning -> reasoning -> acting -> reflecting). They are responsible for planning, reasoning, acting and reflecting respectively.
You are the second LLM agent, who is a helpful web-shopping guidance assistant in charge of reasoning.
Your reasoning thought will guide the acting agent in making informed decisions. You should generate a thought that will be used as part of the PROMPT for acting agents.

In each round, following information will be given to you:
1. CURRENT OBSERVATION AND AVAILABLE ACTIONS
2. PLANNING STRATEGY
3. HISTORICAL ACTIONS
4. REFLECTION INFORMATION(if any)

Here is what you need to focus on:
- Every round, you will receive updated information about the shopping scenario, including the current observation, available actions, planning strategy, and past actions.
- Based on the current state, develop a clear thought process to guide the acting agent's next move.
- Ensure your response is directly actionable and aligns with the goal of achieving success in the game within 15 actions.
- If the game is nearing the interaction limit, prioritize quick decisions over perfect matches to ensure a [Buy Now] action happens promptly.
- When you determine that a sufficient match is found (even if not perfect), guide the acting agent to click [Buy Now] immediately.

OUTPUT FORMAT:
Based on the provided observation and available actions, generate a clear and brief thought in one sentence that outlines your analysis and considerations for the next move.
Note: Please surround the reasoning content you generated with three backticks. That is:
"""
\end{lstlisting}

\subsubsection{Action Module Prompt}
\begin{lstlisting}[language=Python]
prompt_system_action = """
Welcome to the Online Shopping Challenge!
Four llm agents are working together to do web-shopping tasks step by step(planning -> reasoning -> acting -> reflecting). They are responsible for planning, reasoning, acting and reflecting respectively. 
You are the third LLM agent, who is a helpful web-shopping guidance assistant in charge of acting.
As an acting agent, your role is to integrate various elements such as the instruction, the current state, historical actions, strategic planning, and current reasoning to recommend the best possible action for the next step.

In each round, the following information will be given to you:
1. ORIGINAL PROBLEM
2. PLANNING STRATEGY
3. HISTORICAL ACTIONS
4. CURRENT REASONING

Your Role:
- Each round, you will receive updated information, including the current observation, available actions, strategic plan, reasoning, and past actions.
- Based on this information, decide and respond with the best possible action to move closer to completing the objective.
- Actions you can perform:
	Search if a search bar is available.
	Click one of the provided clickable buttons.
- Follow the reasoning closely, but only deviate if you are confident that your choice is better.

Important Rules:
- You must click [Buy Now] as soon as you are confident that a suitable match has been found to avoid exceeding the 15-round limit.
- If no valid action is available, perform no action and wait for the next round.
- Ensure the clicked value exactly matches the available options, including case sensitivity and punctuation.
- Attention: Although you need to click to buy as early as possible to get rewards, remember that you must click on a product before clicking to buy; 
			 if you click to buy without clicking on the product, you will receive 0 rewards.

OUTPUT FORMAT:
Use the following formats for your action:
	- searching: search [keywords]
	- clicking: click [value]
- For example: click [b06xdg8xfx]
- Keywords in search is up to you, but value in click must be a value in the list of available actions.
- The value must exactly match the original text, including case sensitivity (uppercase/lowercase) and all symbols/punctuation.

Note: Please surround the action content you generated with three backticks. That is:
"""
\end{lstlisting}

\subsubsection{Reflection Module Prompt}
\begin{lstlisting}[language=Python]
prompt_system_reflection = """
Welcome to the Online Shopping Challenge!
Four llm agents are working together to do web-shopping tasks step by step(planning -> reasoning -> acting -> reflecting). They are responsible for planning, reasoning, acting and reflecting respectively. 
You are the fourth llm agent in charge of reflecting. Your role is to reflect on whether there was an error in the previous reasoning and action sequence.
Remember, your clear and brief reflection will be used as part of the PROMPT for the later agents to guide them to make wise decisions and succeed in the game.

In each round, the following information will be given to you:
1. ORIGINAL PROBLEM
2. HISTORICAL REASONINGS
3. HISTORICAL ACTIONS

Here is your role:
As an LLM Agent, your role is to reflect on the recent outcomes and consider the following points:
1. Identify why the current result is unsatisfactory. Explore factors such as inadequate search queries, irrelevant clicks, or repeated useless actions.
2. Evaluate the effectiveness of past actions and thoughts. Were there missed signals or incorrect assumptions?
3. Propose improvements for the next steps. Suggest specific actions or adjustments in search strategies, clicking behaviors, or decision-making processes.
4. Consider the overall goal of achieving successful purchases within the game's constraints. How can future actions better align with this objective?
Use these as a guide, and generate a plan for the next reasoning and action steps. Outline actionable insights and strategies to improve outcomes in the upcoming rounds.

OUTPUT FORMAT:
- You should carefully examine reasoning history and action history to find out where things may have gone wrong, summarize where they went wrong.
- Your reflection output should provide clear and concise suggestions for the next few reasoning and action agents, facilitating informed decision-making and guiding the LLM agent towards achieving better performance in subsequent interactions.
- Ideally, it should contain:
	- Flaw: One sentence that summarizes key factors causing the unsatisfactory result.
	- Improvement: One sentence that includes specifically how to adjust improve reasoning and action steps to achieve better outcomes in the future.

Note: Please enclose the flaw and improvement with three backticks:
"""
\end{lstlisting}






\section{Navigation Planning.}
\subsection{Dataset Details}
% \vspace{-0.1cm}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Figure/planning.png}
    \vspace{-0.2cm}
    \caption{Dynamic Navigation Planning Task Framework. The task evaluates the agent's ability to collaboratively generate and adapt travel itineraries based on evolving user constraints and preferences.}
    \label{fig:Navigation Planning}
    \vspace{-0.3cm}
\end{figure}

The Navigation Planning task evaluates agents' ability to collaboratively generate travel itineraries with a user while adapting to evolving constraints and preferences, The dataset includes 250 tasks,designed to benchmark performance in navigation planning/

In navigation tasks, agents are required to collaboratively generate and adapt travel itineraries based on evolving user constraints and preferences.Inspired by\citep{lin2023decision}, we utilized the  framework to employ the evaluation.It evaluates the rationality of the planned route, based on how well the proposal aligns with user
preferences.

We enhanced the automated data generation method from \citep{lin2023decision} to construct our new dataset.The dataset provides a list of locations and situations, and by randomly generating the conditions of tourist destinations for each instance, it facilitates the next step of decision-making, thereby enabling significant scalability.

\subsection{Experiment Details}

\begin{table*}[h]
  \caption{Experimental Results on Navigation Planning(250 tasks)}
  \label{tab:praf_planning}
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{ccccccccc}
    \toprule
    LLM & Pt & Rt & At & Ft & Acc (\%) & $\Delta$ Acc (\%)\\
    \midrule
    \texttt{Llama3-8B-instruct(Default)} 
    & - & - & - & - & 58.70 & - \\
    \texttt{claude-3.5-sonnet} 
    & 0.0002 & \underline{0.0297} & \textbf{0.1058} & -0.0056 & \textbf{71.90} & \textbf{+13.20} \\
    \texttt{gpt-4-turbo-0409} 
    & 0.0083 & 0.0136 & \underline{0.0994} & 0.004 & 71.23 & +12.53 \\
    \texttt{qwen2.5-32b-Instruct} 
    & -0.0022 & 0.0124 & 0.0985 & \textbf{0.0182} & 68.26 & +9.56 \\
    \texttt{gpt-4o-mini} 
    & 0.0056 & 0.0273 & 0.0810 & 0.0017 & 70.29 & +11.59 \\
    \texttt{Llama-3.1-70B-Instruct} 
    & -0.009 & -0.019 & 0.0455 & -0.0108 & 59.32 & +11.79 \\
    \texttt{doubao-pro-4k} 
    & 0.0008 & -0.0029 & -0.0508 & -0.0328 & 50.90 & -7.8 \\
    \texttt{glm-4-air} 
    &0.0011 & -0.0080 & 0.0050 & -0.0214 & 61.91 & +3.21 \\
    \texttt{Mistral-8X7B-instruct} 
    & \underline{0.021} & -0.035 & 0.048 & -0.029 & 64.45 & +5.75 \\
    \texttt{Mistral-7B-Instruct} 
    & \textbf{0.0230} & \textbf{0.0552} & 0.0423 & \underline{0.0065} & \underline{71.48} & \underline{+12.78}  \\    
    \midrule
    \texttt{best} & / & / & / & / & \textit{74.42} & \textit{+15.72}\\
    \bottomrule
  \end{tabular}}
\end{table*}



\subsection{Prompt Example}

\subsubsection{Planning Module}
\input{Prompt/navigation-planning/planning}


\subsubsection{Reasoning Module Prompt}
\input{Prompt/navigation-planning/reasoning}


\subsubsection{Action Module Prompt}
\input{Prompt/navigation-planning/action}


\subsubsection{Reflection Module Prompt}
\input{Prompt/navigation-planning/reflection}









\section{Ticket Ordering.}
\subsection{Dataset Details}
The Ticket Ordering task evaluates the ability of agents to collaboratively provide the best flight combinations for two users. The dataset consists of 150 tasks, which are designed to benchmark the performance of different agents in ticket ordering.

Inspired by the framework presented by \citep{lin2023decision}, we build our evaluation framework based on their structure. Specifically, we use the provided code to generate the dataset, which includes two users' calendars. The tasks are created by combining the users' calendar data, and agents are then asked to provide flight recommendations based on this information.



\subsection{Experiment Details}

\begin{table*}[h]
  \caption{Experimental Results on Ticket Ordering(150 tasks)}
  \label{tab:praf_meditation}
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{ccccccccc}
    \toprule
    LLM & Pt & Rt & At & Ft & Acc(\%) & $\Delta$ Acc(\%)\\
    \midrule
    \texttt{Llama3-8B-instruct (Default)} & - & - & - & - & 19.94 & - \\
    \texttt{Claude-3.5-Sonnet} & 0.0026 & 0.1855 & \textbf{0.2165} & 0.0244 & \textbf{62.85} & \textbf{+42.91} \\
    \texttt{gpt-4-turbo-0409} & -0.0426 & \textbf{0.3011} & \underline{0.0275} & \textbf{0.0583} & \underline{54.37} & \underline{+34.43} \\
    \texttt{qwen2.5-32b-Instruct} & 0.1190 & 0.1812 & -0.0002 & \underline{0.0431} & 54.25 & +34.31 \\
    \texttt{gpt-4o-mini} & 0.0315 & \underline{0.2434} & 0.0491 & 0.0047 & 51.82 & +31.88 \\
    \texttt{Llama-3.1-70B-Instruct} & 0.0035 & 0.0891 & -0.2751 & -0.0010 & 1.59 & -18.35 \\
    \texttt{doubao-pro-4k} & \underline{0.1512} & -0.0008 & 0.0058 & -0.0268 & 32.88 & +12.94 \\
    \texttt{glm-4-air} & -0.1951 & 0.1718 & -0.0199 & -0.0061 & 15.01 & -4.93 \\
    \texttt{Mistral-8X7B-instruct} & \textbf{0.1830} & 0.0535 & -0.0825 & -0.0111 & 34.24 & +14.30 \\
    \texttt{Mistral-7B-Instruct} & -0.1113 & -0.0702 & -0.0197 & 0.0018 & 0.0 & -19.94
    \\
    \midrule
    \texttt{best} & / & / & / & / & \textit{67.18} & \textit{47.24}\\
    \bottomrule
  \end{tabular}}
\end{table*}




Table \ref{tab:praf_meditation} summarizes the experimental results for the Ticket Ordering task.
The baseline model achieves an accuracy of 19.94\%. \texttt{Claude-3.5-Sonnet} achieves the highest accuracy of 62.85\%, improving by +42.91\%. \texttt{gpt-4-turbo-0409} follows with an accuracy of 54.37\%, improving by +34.43\%. The accuracy range, from 0.0\% (\texttt{Mistral-7B-Instruct}) to 62.85\%, highlights the dataset's ability to differentiate models based on their performance.

The dataset emphasizes Reasoning and Action capabilities, as seen in the high \texttt{Rt} and \texttt{At} Shapley values for top models like \texttt{Claude-3.5-Sonnet}, \texttt{gpt-4-turbo-0409}, and \texttt{qwen2.5-32b-Instruct}. Models with stronger Reasoning and Action abilities show significant accuracy improvements, whereas those with lower values for these modules, such as \texttt{Mistral-7B-Instruct}, experience considerable performance deficits.


\subsection{Prompt Example}

\subsubsection{Planning Module}
\input{Prompt/TicketOrdering/planning}


\subsubsection{Reasoning Module Prompt}
\input{Prompt/TicketOrdering/reasoning}


\subsubsection{Action Module Prompt}
\input{Prompt/TicketOrdering/action}


\subsubsection{Reflection Module Prompt}
\input{Prompt/TicketOrdering/reflection}


\section{Math Solver.}
\input{Table/MathSolver/praf_math}


\subsection{Dataset Deatils}
The Math Solver dataset evaluates agents' planning, reasoning, and action capabilities in solving diverse mathematical problems, with a particular focus on tool usage during the problem-solving process. This dataset is divided into two categories: \textbf{Algebra} and \textbf{Geometry}, comprising a total of \textbf{500 tasks} (\textbf{250 Algebra tasks} and \textbf{250 Geometry tasks}).

\textbf{Dataset Construction.} The dataset is derived from the MATH dataset \citep{hendrycksmath2021} and enhanced with GPT-4 to improve diversity and relevance. The MATH dataset's original structure includes a large number of highly similar questions without detailed knowledge point categorization, making evaluation costly and inefficient. To address this, we synthesized new data by:
\begin{itemize}
    \item [(1)]Summarizing Knowledge Points: All problems in the MATH dataset were analyzed using GPT-4 to extract a comprehensive list of key concepts.
    \item [(2)]Condensing Categories: GPT-4 distilled the extracted concepts into \textbf{10 key knowledge points} for Algebra and Geometry, respectively.
    \item [(3)]Mapping Labels: Each problem in the original dataset was mapped to one of the 10 knowledge points and assigned a difficulty level (1–5).
    \item [(4)]Synthesizing New Problems: For each unique combination of knowledge point and difficulty level, GPT-4 generated five new problems, ensuring coverage across all categories.
\end{itemize}
Overall, both algebra and geometry each include ten knowledge points. Each knowledge point is divided into five levels, and for each combination, there are five problems. Therefore, the total amount of data is $2 \times 10 \times 5 \times 5 = 500$.
Knowledge points and corresponding examples can be seen in Table.\ref{tab:konwledge_point_math}.

\input{Table/MathSolver/knowledge_point_math}




\subsection{Experiment Details}
Table \ref{tab:praf_math} summarizes the experimental results for the Math task, including Shapley values for the four modules (Planning (\texttt{Pt}), Reasoning (\texttt{Rt}), Action (\texttt{At}), and Reflection (\texttt{Ft})), as well as task success rates (Accuracy (\%)) and their improvement ($\Delta$ Accuracy (\%)) relative to the baseline (\texttt{Llama3-8B-instruct}).

The baseline model (\texttt{Llama3-8B-instruct}) achieves task success rates of 21.6\% (algebra) and 14.4\% (geometry). The best-performing model, \texttt{qwen2.5-32B}, achieves accuracies of 86.8\% and 86.4\%, with significant improvements of +65.2\% and +72.0\%, respectively. This highlights its strong overall performance, driven by its balanced capabilities in reasoning, acting, and reflection.

\texttt{Claude-3.5-Sonnet} demonstrates excellent reasoning but falls short in acting, leading to slightly lower success rates compared to \texttt{qwen2.5-32B}. Notably, \texttt{doubao-pro-4k} excels in planning but lacks strength in other components, limiting its overall accuracy. Open-source models lag significantly behind closed-source models, underscoring the current gap in performance.

The evaluation also reveals the importance of tool usage during acting phases, where agents successfully leverage calculators and search engines to solve complex tasks. Reflection phases are crucial for iterative problem-solving, enabling corrections and better outcomes in challenging mathematical scenarios.

Note that in the last line, \textit{best} refers to combining the optimal models of the four modules to conduct the p-r-a-f experiment again. On the Algebra dataset, this approach increases the task success rate from the optimal model \texttt{qwen2.5-32B}'s 86.8\% to \textbf{\textit{88.4\%}}. This indicates that our evaluation method is meaningful, as combining the best models in each capability can outperform using a single best model. Unfortunately, there is no performance improvement on the Geometry dataset, which may be related to poor collaboration between the models.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Figure/Math/math_ag.png}
    % \vspace{-0.2cm}
    \caption{Example in Geometry.}
    \label{fig:math1}
\end{figure*}

\subsection{Prompt Example}

\subsubsection{Planning Module Prompt}
\input{Prompt/MathSolver/planning}

\subsubsection{Reasoning Module Prompt}
\input{Prompt/MathSolver/reasoning}

\subsubsection{Acting Module Prompt}
\input{Prompt/MathSolver/acting}

\subsubsection{Reflection Module Prompt}
\input{Prompt/MathSolver/reflection}

\section{Automatic Theorem Proving.}
% \subsection{Dataset Deatils}
% The Automatic Theorem Proving dataset evaluates agents' planning, reasoning, and action capabilities in solving formal proof problems, with a particular focus on writing code for logical proofs. This dataset is divided into three categories: \textbf{Coq}, \textbf{Lean 4} and \textbf{Isabelle}, comprising a total of \textbf{333 tasks} (with each 111 tasks).

% \textbf{Dataset Construction.} This task includes 111 original Coq problems curated from the course material. These proof problems involve the following aspects:
% \begin{itemize}
%     \item [(1)]Algebraic Calculations, such as derivation of linear systems;
%     \item [(2)]Properties of Functions, such as the translation and monotonicity of functions;
%     \item [(3)]Properties of Recursive Structures, such as operations on tree structures;
%     \item [(4)]Logical Problems, such as the relationships between AND, OR, and NOT;
%     \item [(5)]Properties of Natural Numbers, such as proving that 6 is not a prime number.
% \end{itemize}
% These proof problems are introductory exercises for learning formal proofs in college courses, examining basic syntax and some simple logical relationships. They are somewhat challenging for college students, making their difficulty level suitable for evaluating the capabilities of LLM agents.

% To more comprehensively and integratively assess the capability of large models in formal proof, we have further translated these problems into Lean 4 and Isabelle versions. These three languages are currently popular and widely used formal proof languages. Assessing from multiple aspects can more rigorously reveal the differences in capabilities among various large models.

\subsection{Dataset Details}
The Automatic Theorem Proving dataset evaluates agents' capabilities in solving formal proof problems, focusing on generating code for logical proofs. The dataset includes three categories: \textbf{Coq}, \textbf{Lean 4}, and \textbf{Isabelle}, with a total of \textbf{333 tasks} (111 tasks per category).

\textbf{Dataset Construction.}  The dataset originates from 111 Coq problems curated from course material, covering the following topics:

\begin{itemize}
    \item [(1)] Algebraic Calculations, e.g., derivation of linear systems.
    \item [(2)] Properties of Functions, e.g., translation and monotonicity of functions.
    \item [(3)] Properties of Recursive Structures, e.g., operations on tree structures.
    \item [(4)] Logical Problems, e.g., relationships between AND, OR, and NOT.
    \item [(5)] Properties of Natural Numbers, e.g., proving 6 is not a prime number.
\end{itemize}

These proof problems serve as introductory exercises in college formal proof courses, focusing on basic syntax and simple logical relationships. They are challenging for students, making them a suitable benchmark for evaluating the performance of large language models (LLMs).

To comprehensively assess LLMs' formal proof capabilities, these problems were further translated into Lean 4 and Isabelle versions. Coq, Lean 4, and Isabelle are widely used formal proof languages, and using multiple languages allows for a more rigorous comparison of model capabilities. And Figure.\ref{fig:coq_lean_isabelle} shows different language versions of the same question.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/Math/coq_lean_isabelle.png}
    % \vspace{-0.2cm}
    \caption{An Example Problem in Three Languages.}
    \label{fig:coq_lean_isabelle}
\end{figure*}

% \subsection{Experiment Details}
% Table \ref{tab:praf_formal} summarizes the experimental results for the Automatic Theorem Proving task, including Shapley values for the four modules (Planning (\texttt{Pt}), Reasoning (\texttt{Rt}), Action (\texttt{At}), and Reflection (\texttt{Ft})), as well as task success rates (Accuracy (\%)) and their improvement ($\Delta$ Accuracy (\%)) relative to the baseline (\texttt{Llama3-8B-instruct}).

% \input{Table/MathSolver/praf_formal}

% The baseline model (\texttt{Llama3-8B-instruct}) achieves task success rates of 6.4\% (Coq), 2.7\% (Lean 4) and 7.2\%(Isabelle). The best-performing model, \texttt{Claude-3.5}, achieves accuracies of 96.4\%, 84.7\% and 67.6\%, with significant improvements of +90.0\%, +82.0\% and +67.6\%, respectively. This highlights its strong overall performance, driven by its balanced capabilities in reasoning, acting, and reflection.

% \texttt{gpt-4-turbo}'s performance on the Coq and Isabelle datasets is second only to \texttt{Claude-3.5}. On Coq, the main reason is that its core reasoning and acting capabilities are slightly weaker. On Isabelle, although its reasoning and acting are the highest, they are only slightly better than \texttt{Claude-3.5}'s, while its reflection ability is significantly inferior to \texttt{Claude-3.5}'s. In the Isabelle scenario, error messages are returned whenever a proof is incomplete, triggering reflection, hence reflection occupies a higher proportion. Consequently, \texttt{Claude-3.5}'s stronger reflection ability leads to the best success rate.

% It is worth noting that \texttt{gpt-4-turbo} performed poorly on the Lean 4 dataset. \citep{tsoukalas2024putnambench}'previous work has suggested that \texttt{gpt-4-turbo} tends to provide syntax for Lean 3, leading to failures. Since the other two datasets were translated from the Coq dataset, the content of the problems is the same, hence the evaluation results are largely similar and reasonable.

% Figure.\ref{fig:math1} is a good example. In the first round of interaction, the acting agent provided code that failed to compile. Then, the reflection agent identified the shortcomings and proposed improvements. In the next round of interaction, both the reasoning agent and the acting agent took advantage of the reflections, corrected the issues in the code, and ultimately successfully completed the proof of the problem.

\subsection{Experiment Details}
Table \ref{tab:praf_formal} summarizes the experimental results for the Automatic Theorem Proving task, presenting Shapley values for the four modules (Planning (\texttt{Pt}), Reasoning (\texttt{Rt}), Action (\texttt{At}), and Reflection (\texttt{Ft})), task success rates (Accuracy (\%)), and improvement ($\Delta$ Accuracy (\%)) over the baseline model (\texttt{Llama3-8B-instruct}).

\input{Table/MathSolver/praf_formal}

The baseline model achieves task success rates of 6.4\% (Coq), 2.7\% (Lean 4), and 7.2\% (Isabelle). The best-performing model, \texttt{Claude-3.5}, achieves 96.4\%, 84.7\%, and 67.6\% on these datasets, with significant improvements of +90.0\%, +82.0\%, and +67.6\%, respectively. This demonstrates \texttt{Claude-3.5}'s strong overall performance, driven by balanced reasoning, acting, and reflection abilities.

\texttt{gpt-4-turbo} ranks second on Coq and Isabelle, mainly due to slightly weaker reasoning and acting capabilities on Coq and inferior reflection ability on Isabelle. For Isabelle, error messages trigger reflection frequently, making strong reflection critical. \texttt{Claude-3.5}'s superior reflection ability ensures the highest success rate in this scenario.

On Lean 4, \texttt{gpt-4-turbo} underperforms significantly. Prior research \citep{tsoukalas2024putnambench} suggests this is due to its tendency to generate Lean 3 syntax, leading to failures. For Coq and Isabelle, the datasets share content since they were translated from the Coq dataset, resulting in comparable evaluation outcomes.

Note that in the last line, \textit{best} refers to combining the optimal models of the four modules to conduct the p-r-a-f experiment again. On the Lean 4 and Isabelle dataset, this approach increases the task success rate from the optimal model \texttt{Claude-3.5}'s 84.7\% and 74.8\% to \textbf{\textit{87.4\%}} and \textbf{\textit{78.4\%}}. This indicates that our evaluation method is meaningful, as combining the best models in each capability can outperform using a single best model. Unfortunately, there is no performance improvement on the Coq dataset, which may be related to poor collaboration between the models.
On the other hand, \texttt{Claude-3.5} already has a high accuracy on this dataset, making it difficult to achieve further improvements.

Figure \ref{fig:math1} illustrates this process. Initially, the acting agent provided code that failed to compile. The reflection agent identified the issue and proposed improvements. In the next interaction, the reasoning and acting agents used these reflections to correct the code, ultimately completing the proof successfully.


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/Math/math_p.png}
    % \vspace{-0.2cm}
    \caption{Example in Logical Proofs.}
    \label{fig:math1}
\end{figure*}

\subsection{Prompt Example(Coq)}
\subsubsection{Planning Module Prompt}
\input{Prompt/ATP/planning}

\subsubsection{Reasoning Module Prompt}
\input{Prompt/ATP/reasoning}

\subsubsection{Acting Module Prompt}
\input{Prompt/ATP/acting}

\subsubsection{Reflection Module Prompt}
\input{Prompt/ATP/reflection}

\section{Operation System.}
\subsection{Dataset Deatils}
The Operation System dataset evaluates an agent's ability to interact with a simulated OS terminal by executing commands to address OS-related tasks, comprising 71 Ubuntu terminal tasks and 31 Git tasks.

In Ubuntu tasks, agents are required to propose bash commands to execute in Ubuntu Terminal and get feedback from the terminal to complete the task. We utilized the AgentBench-OS framework \cite{liu2023agentbench} to employ the evaluation.

We enhanced the automated data generation method from AgentBench-OS to construct our new dataset, primarily generating operation-type data. The original method leverages LLMs to generate tasks and employs unit tests to ensure their accuracy. While creating the dataset, we used specific prompts to guide the generation of desired data types. The dataset comprises 71 AgentBench-OS tasks, categorized into 40 file system manipulation, 20 system setting, and 11 process running tasks.

\input{Table/OS/OS_classification}

For the git tasks, we selected data from learngitbranching\cite{learnGitBranching}. The learngitbranching website itself is a tutorial git beginner. It provides terminal and sandbox environment that simulates git using a tree structure. Git tree dynamically updates along with each git command from the terminal. Given initial and target states for both local and remote git trees, agents must interact with the git tree via the terminal to transform it from its initial state to the target state. The dataset assesses proficiency in fundamental git commands and their combination to execute advanced git functionalities.


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{Figure/OperatingSystem/os.png}
    \caption{Illustration of OS-git task}
    \label{fig:coq_lean_isabelle}
    \vspace{-0.2cm}
\end{figure*}


\subsection{Experiment Deatils}
Table \ref{tab:praf_OS} summarizes the experimental results for the Operation System task, including Shapley values for the four modules (Planning (\texttt{Pt}), Reasoning (\texttt{Rt}), Action (\texttt{At}), and Reflection (\texttt{Ft})), as well as task success rates (Accuracy (\%)) and their improvement ($\Delta$ Accuracy (\%)) relative to the baseline (\texttt{Llama3-8B-instruct}).

\input{Table/OS/praf_OS_102}


\textbf{Reasoning is more crucial than planning in terminal-based OS scenarios.} In these environments, essential information is obtained through ongoing interactions rather than being available upfront. For Ubuntu terminal tasks, details about the system, such as file system layout and settings, are mostly acquired interactively. Similarly, in git tasks, while the git tree state is visible, the sandbox setting requires further interaction to clarify the exact command forms supported. Our prompting method starts with planning based on limited initial information, which reduces its impact due to insufficient data for comprehensive task execution. Thus, reasoning becomes vital, enabling models to adapt to new information and make informed decisions. This is evident in performance metrics, where reasoning scores surpass planning scores, highlighting the importance of effective reasoning for success in these tasks.

\textbf{OS demands less in action compared to other senarios, due to its lower sim2real gap.} Action module's main function is to translate reasoning outputs into actions that fit the environment's input specifications. Most real terminal commands are also available for our benchmark. This alignment means the action formats are not unique to the evaluation but are prevalent in existing data. As a result, models require less adaptation or transformation to meet the benchmark's requirements, reducing the complexity of action processing and the demand on action ability compared to benchmarks needing adaptation to novel task formats.

The minimal contributions from the Reflection (Ft) module suggest that this task lacks strong feedback signals through reflection. Thus proving  reasoning is the primary focus evaluation module in Operation System Tasks.

\subsection{Prompt Example}
\subsubsection{Planning Module}
\input{Prompt/OS/planning}


\subsubsection{Reasoning Module Prompt}
\input{Prompt/OS/reasoning}


\subsubsection{Action Module Prompt}
\input{Prompt/OS/action}


\subsubsection{Reflection Module Prompt}
\input{Prompt/OS/reflection}



% \section{Role Playing.}
% \subsection{Dataset Deatils}
% \subsection{Prompt Example}




\section{Robot Cooperation.}

\subsection{Dataset Details}

The Robot Cooperation dataset evaluates agents' planning, reasoning, action, and reflection capabilities in multi-robot collaboration tasks. The dataset includes \textbf{100 tasks}, designed to benchmark performance in robot planning scenarios.

\textbf{Framework and Dataset Construction.}  
The dataset is built upon the RoCoBench environment framework \citep{mandi2023rocodialecticmultirobotcollaboration}, which provides an environment simulator and reward mechanisms for multi-robot collaboration tasks. We extended the original task set by introducing sequential constraints and leveraging random seed variations to generate diverse task instances.
\vspace{-0.2cm}
\begin{itemize}
    \item \textbf{Task Extension:} Sequential constraints were added to existing tasks, making them more complex. Examples include:
    \begin{itemize}
        \item \textit{Sweep Floor Task:} Added order constraints. In the \textit{Sweep RGB} task, robots must first sweep the Red Cube into the dustpan and dump it into the bin, followed by the Green Cube, and finally the Blue Cube.
        \item \textit{Arrange Cabinet Task:} Introduced sequential object retrieval. In the \textit{CabinetCup} task, robots must first place the Cup on the Cup Coaster, followed by placing the Mug on the Mug Coaster.
        \item \textit{Sandwich Task:} Expanded with additional recipes requiring more planning steps.
    \end{itemize}
    \item \textbf{Task Instances:} Random seed variations in the RoCoBench environment were used to create different initial states, generating 100 unique task instances. Each instance was manually verified to ensure it has a correct solution, ensuring robustness and reliability for model evaluation.
\end{itemize}

\textbf{Reward Mechanism Improvements.}  
To better evaluate model capabilities, we proposed new reward methods tailored to the characteristics of the extended tasks:
\vspace{-0.2cm}
\begin{itemize}
    \item Tasks were divided into smaller sub-tasks with rewards granted for completing each sub-task in sequence.
    \item For example, in the \textit{Sweep RGB} task, rewards are distributed as $\frac{1}{3}$ for successfully completing each step (e.g., sweeping the Red Cube, Green Cube, and Blue Cube in order). This approach incentivizes correct sequencing and provides granular feedback on agent performance.
    \item These new reward methods ensure even smaller models can effectively receive feedback, improving evaluation sensitivity.
\end{itemize}

\textbf{Model Differentiation Enhancements.}  
To further enhance the differentiation capability of the models, we adopt a method where multiple actions are proposed within a single interaction. This approach, combined with a constraint on the number of timesteps, improves the differentiation among models. By allowing the agent to plan and propose multiple actions at once, we can better assess the agent's planning and reasoning abilities. The constraint on timesteps ensures that the agent must efficiently utilize its planning capabilities within a restricted timeframe, thereby providing a clearer distinction between the performance of different models.





\subsection{Experiment Details}

\input{Table/RoCo/praf_Llama_3_8b}


Table \ref{tab:praf_RoCo} summarizes the experimental results for the Robot Cooperation task, including Shapley values for the four modules (Planning (\texttt{Pt}), Reasoning (\texttt{Rt}), Action (\texttt{At}), and Reflection (\texttt{Ft})), as well as Rewards (Reward (\%)) and their improvement ($\Delta$ Reward (\%)) relative to the baseline (\texttt{Llama3-8B-instruct}).


% The baseline model (\texttt{Llama3-8B-instruct}) achieves rewards as 8.85\%. The 2 best-performing models, \texttt{claude-3.5-sonnet} and \texttt{gpt-4-turbo-0409}, achieve rewards as 92.63\% and 84.18\%, corresponding to improvements of +83.68\% and +75.33\%, respectively. These results highlight their strong overall performance relative to the baseline.

% The experimental results on the Robot Cooperation dataset reveal several notable characteristics of the evaluated models and their performance on this task. Notably, the dataset places a strong emphasis on Planning and Action capabilities, as evidenced by the high Shapley values for these modules among the top-performing models (\texttt{glm-4-air}, \texttt{qwen2.5-32b-ins}, and \texttt{Llama-3-70B-Instruct}). The task's structured nature, requiring precise attribute filtering and logical decision-making, heavily rewards models with strong planning abilities (e.g., high \texttt{Pt} values) and effective action execution (\texttt{At}). 

% 基于数据集构造的相应技巧, 当前数据集对于不同能力模型之间有较强的区分度, 从mistral-7B-instruct 的 5.17 的 reward 到 claude-3.5-sonnet 的 92.63 的 reward, 而且reward 之间分布比较均匀, 有较好的层次性, 充分体现了不同模型的差异.
% The experimental results indicate that the current evaluation scenario provides substantial differentiation among the evaluated models. Due to the specific techniques employed in constructing the dataset, the current dataset exhibits a strong differentiation capability among models with varying abilities. This is evident from the reward range, spanning from 5.17\% for Mistral-7B-Instruct to 92.63\% for Claude-3.5-sonnet. Moreover, the distribution of rewards is relatively uniform, displaying a well-defined hierarchy that effectively highlights the differences between models.


The baseline model achieves a reward of 8.85\%. \texttt{Claude-3.5-sonnet} and \texttt{gpt-4-turbo-0409} achieve the highest rewards, 92.63\% and 84.18\%, improving by +83.78\% and +75.33\%, respectively. The reward range, from 5.17\% (\texttt{Mistral-7B-Instruct}) to 92.63\%, highlights the dataset's strong ability to differentiate models.

The dataset emphasizes Reasoning and Action capabilities, as reflected by high \texttt{Rt} and \texttt{At} Shapley values for top-performing models (\texttt{claude-3.5-sonnet}, \texttt{gpt-4-turbo-0409}, and \texttt{qwen2.5-32b-instruct}). Its structured design rewards precise reasoning and efficient execution. 
% The Robot Cooperation tasks thoroughly evaluate the models' Planning, Reasoning, and Action capabilities, as described below:
% \begin{itemize}
%     \item \textbf{Planning Ability:} The tasks demand advanced planning due to the need for collaboration among multiple robots. Effective coordination is crucial, and each robot must plan its actions carefully to ensure task success.
    
%     \item \textbf{Reasoning Ability:} Limited timesteps impose stringent requirements on reasoning ability. Models relying on straightforward, single-step reasoning approaches are unlikely to succeed. Instead, the scenario encourages models to generate multiple actions within a single reasoning step, enabling more comprehensive evaluation of reasoning capabilities.
    
%     \item \textbf{Action Ability:} The action phase converts abstract reasoning steps into executable actions while adhering to real-world constraints, such as avoiding object collisions. This phase tests the model's understanding of physical constraints and its ability to produce precise, executable instructions based on action output.
% \end{itemize}



\subsection{Prompt Example}


\subsubsection{Planning Module}
\input{Prompt/RoCo/planning}


\subsubsection{Reasoning Module Prompt}
\input{Prompt/RoCo/reasoning}


\subsubsection{Action Module Prompt}
\input{Prompt/RoCo/action}


\subsubsection{Reflection Module Prompt}
\input{Prompt/RoCo/reflection}



\end{document}
