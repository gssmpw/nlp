% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").



% Introduction

@article{saharia2022photorealistic,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={36479--36494},
  year={2022}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@article{dalle-3,
  title={Improving image generation with better captions},
  author={Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and others},
  journal={Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf},
  volume={2},
  number={3},
  pages={8},
  year={2023}
}

@article{podell2023sdxl,
  title={Sdxl: Improving latent diffusion models for high-resolution image synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  journal={arXiv preprint arXiv:2307.01952},
  year={2023}
}

@inproceedings{blattmann2023align,
  title={Align your latents: High-resolution video synthesis with latent diffusion models},
  author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22563--22575},
  year={2023}
}


@inproceedings{esser2024scaling,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{liu2024draw,
  title={Draw like an artist: Complex scene generation with diffusion model via composition, painting, and retouching},
  author={Liu, Minghao and Zhang, Le and Tian, Yingjie and Qu, Xiaochao and Liu, Luoqi and Liu, Ting},
  journal={arXiv preprint arXiv:2408.13858},
  year={2024}
}

@inproceedings{wang2023stylediffusion,
  title={Stylediffusion: Controllable disentangled style transfer via diffusion models},
  author={Wang, Zhizhong and Zhao, Lei and Xing, Wei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7677--7689},
  year={2023}
}

@inproceedings{zhang2023inversion,
  title={Inversion-based style transfer with diffusion models},
  author={Zhang, Yuxin and Huang, Nisha and Tang, Fan and Huang, Haibin and Ma, Chongyang and Dong, Weiming and Xu, Changsheng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10146--10156},
  year={2023}
}

@article{cao2024exploring,
  title={Exploring visual culture awareness in gpt-4v: A comprehensive probing},
  author={Cao, Yong and Li, Wenyan and Li, Jiaang and Yuan, Yifei and Karamolegkou, Antonia and Hershcovich, Daniel},
  journal={arXiv preprint arXiv:2402.06015},
  year={2024}
}

@inproceedings{basu2023inspecting,
  title={Inspecting the geographical representativeness of images from text-to-image models},
  author={Basu, Abhipsa and Babu, R Venkatesh and Pruthi, Danish},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5136--5147},
  year={2023}
}

@inproceedings{kannenbeyond,
  title={Beyond Aesthetics: Cultural Competence in Text-to-Image Models},
  author={Kannen, Nithish and Ahmad, Arif and Prabhakaran, Vinodkumar and Prabhu, Utsav and Dieng, Adji Bousso and Bhattacharyya, Pushpak and Dave, Shachi and others},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year      = {2024},
}

@inproceedings{nayak2024benchmarking,
  title={Benchmarking Vision Language Models for Cultural Understanding},
  author={Nayak, Shravan and Jain, Kanishk and Awal, Rabiul and Reddy, Siva and Steenkiste, Sjoerd and Hendricks, Lisa and Stanczak, Karolina and Agrawal, Aishwarya},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={5769--5790},
  year={2024}
}

@article{li2024foodieqa,
  title={FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture},
  author={Li, Wenyan and Zhang, Xinyu and Li, Jiaang and Peng, Qiwei and Tang, Raphael and Zhou, Li and Zhang, Weijia and Hu, Guimin and Yuan, Yifei and S{\o}gaard, Anders and others},
  journal={arXiv preprint arXiv:2406.11030},
  year={2024}
}


@article{li2024culture,
  title={Culture-gen: Revealing global cultural perception in language models through natural language prompting},
  author={Li, Huihan and Jiang, Liwei and Hwang, Jena D and Kim, Hyunwoo and Santy, Sebastin and Sorensen, Taylor and Lin, Bill Yuchen and Dziri, Nouha and Ren, Xiang and Choi, Yejin},
  journal={arXiv preprint arXiv:2404.10199},
  year={2024}
}


@article{henrich2010weirdest,
  title={The weirdest people in the world?},
  author={Henrich, Joseph and Heine, Steven J and Norenzayan, Ara},
  journal={Behavioral and brain sciences},
  volume={33},
  number={2-3},
  pages={61--83},
  year={2010},
  publisher={Cambridge University Press}
}


@article{joshi2020state,
  title={The state and fate of linguistic diversity and inclusion in the NLP world},
  author={Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  journal={arXiv preprint arXiv:2004.09095},
  year={2020}
}

@inproceedings{jha-etal-2023-seegull,
    title = "{S}ee{GULL}: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models",
    author = "Jha, Akshita  and
      Mostafazadeh Davani, Aida  and
      Reddy, Chandan K  and
      Dave, Shachi  and
      Prabhakaran, Vinodkumar  and
      Dev, Sunipa",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.548/",
    doi = "10.18653/v1/2023.acl-long.548",
    pages = "9851--9870",
    abstract = "Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America."
}

@article{myung2024blend,
  title={BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages},
  author={Myung, Junho and Lee, Nayeon and Zhou, Yi and Jin, Jiho and Putri, Rifki Afina and Antypas, Dimosthenis and Borkakoty, Hsuvas and Kim, Eunsu and Perez-Almendros, Carla and Ayele, Abinew Ali and others},
  journal={arXiv preprint arXiv:2406.09948},
  year={2024}
}




% Related Work

@article{wu2023human,
  title={Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis},
  author={Wu, Xiaoshi and Hao, Yiming and Sun, Keqiang and Chen, Yixiong and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
  journal={arXiv preprint arXiv:2306.09341},
  year={2023}
}


@article{huang2023t2i,
  title={T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation},
  author={Huang, Kaiyi and Sun, Kaiyue and Xie, Enze and Li, Zhenguo and Liu, Xihui},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={78723--78747},
  year={2023}
}

@article{li2024genai,
  title={GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual Generation},
  author={Li, Baiqi and Lin, Zhiqiu and Pathak, Deepak and Li, Jiayao and Fei, Yixin and Wu, Kewen and Ling, Tiffany and Xia, Xide and Zhang, Pengchuan and Neubig, Graham and others},
  journal={arXiv preprint arXiv:2406.13743},
  year={2024}
}


@article{schuhmann2021laion,
  title={Laion-400m: Open dataset of clip-filtered 400 million image-text pairs},
  author={Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  journal={arXiv preprint arXiv:2111.02114},
  year={2021}
}



@article{birhane2021multimodal,
  title={Multimodal datasets: misogyny, pornography, and malignant stereotypes},
  author={Birhane, Abeba and Prabhu, Vinay Uday and Kahembwe, Emmanuel},
  journal={arXiv preprint arXiv:2110.01963},
  year={2021}
}


@inproceedings{liu2021visually,
  title={Visually Grounded Reasoning across Languages and Cultures},
  author={Liu, Fangyu and Bugliarello, Emanuele and Ponti, Edoardo Maria and Reddy, Siva and Collier, Nigel and Elliott, Desmond},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={10467--10485},
  year={2021}
}

@article{gaviria2022dollar,
  title={The Dollar Street dataset: Images representing the geographic and socioeconomic diversity of the world},
  author={Gaviria Rojas, William and Diamos, Sudnya and Kini, Keertan and Kanter, David and Janapa Reddi, Vijay and Coleman, Cody},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12979--12990},
  year={2022}
}

@article{liu2023towards,
  title={Towards equitable representation in text-to-image synthesis models with the cross-cultural understanding benchmark (ccub) dataset},
  author={Liu, Zhixuan and Shin, Youeun and Okogwu, Beverley-Claire and Yun, Youngsik and Coleman, Lia and Schaldenbrand, Peter and Kim, Jihie and Oh, Jean},
  journal={arXiv preprint arXiv:2301.12073},
  year={2023}
}


@inproceedings{jha2024visage,
  title={ViSAGe: A global-scale analysis of visual stereotypes in text-to-image generation},
  author={Jha, Akshita and Prabhakaran, Vinodkumar and Denton, Remi and Laszlo, Sarah and Dave, Shachi and Qadri, Rida and Reddy, Chandan and Dev, Sunipa},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={12333--12347},
  year={2024}
}

@article{salimans2016improved,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chen2023quantifying,
  title={On quantifying and improving realism of images generated with diffusion},
  author={Chen, Yunzhuo and Akhtar, Naveed and Haldar, Nur Al Hasan and Mian, Ajmal},
  journal={arXiv preprint arXiv:2309.14756},
  year={2023}
}



@inproceedings{hessel2021clipscore,
  title={CLIPScore: A Reference-free Evaluation Metric for Image Captioning},
  author={Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7514--7528},
  year={2021}
}

@inproceedings{lin2025evaluating,
  title={Evaluating text-to-visual generation with image-to-text generation},
  author={Lin, Zhiqiu and Pathak, Deepak and Li, Baiqi and Li, Jiayao and Xia, Xide and Neubig, Graham and Zhang, Pengchuan and Ramanan, Deva},
  booktitle={European Conference on Computer Vision},
  pages={366--384},
  year={2025},
  organization={Springer}
}


@article{xu2024imagereward,
  title={Imagereward: Learning and evaluating human preferences for text-to-image generation},
  author={Xu, Jiazheng and Liu, Xiao and Wu, Yuchen and Tong, Yuxuan and Li, Qinkai and Ding, Ming and Tang, Jie and Dong, Yuxiao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{nguyen2024quality,
  title={Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design},
  author={Nguyen, Quan and Dieng, Adji Bousso},
  journal={arXiv preprint arXiv:2405.02449},
  year={2024}
}



@article{alexey2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Alexey, Dosovitskiy},
  journal={arXiv preprint arXiv: 2010.11929},
  year={2020}
}


@misc{flux2023,
    author={Black Forest Labs},
    title={FLUX},
    year={2023},
    howpublished={\url{https://github.com/black-forest-labs/flux}},
}

@inproceedings{cheng-etal-2023-improving,
    title = "Improving Contrastive Learning of Sentence Embeddings from {AI} Feedback",
    author = "Cheng, Qinyuan  and
      Yang, Xiaogui  and
      Sun, Tianxiang  and
      Li, Linyang  and
      Qiu, Xipeng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.707/",
    doi = "10.18653/v1/2023.findings-acl.707",
    pages = "11122--11138",
    abstract = "Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings.However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve Contrastive Learning of sentence embeddings from AI Feedback (CLAIF).Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings.Experimental results show that our method achieves state-of-the-art performance on several semantic textual similarity (STS) and transfer learning tasks compared to other unsupervised and supervised contrastive learning methods."
}

@inproceedings{an2024capturing,
  title={Capturing the Relationship Between Sentence Triplets for LLM and Human-Generated Texts to Enhance Sentence Embeddings},
  author={An, Na Min and Waheed, Sania and Thorne, James},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  pages={624--638},
  year={2024}
}




@inproceedings{mo2023multi,
  title={Multi-level contrastive learning for self-supervised vision transformers},
  author={Mo, Shentong and Sun, Zhun and Li, Chao},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2778--2787},
  year={2023}
}


@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9650--9660},
  year={2021}
}



@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}

@article{wan2023personalized,
  title={Are personalized stochastic parrots more dangerous? evaluating persona biases in dialogue systems},
  author={Wan, Yixin and Zhao, Jieyu and Chadha, Aman and Peng, Nanyun and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2310.05280},
  year={2023}
}

@article{cao2024cultural,
  title={Cultural adaptation of recipes},
  author={Cao, Yong and Kementchedjhieva, Yova and Cui, Ruixiang and Karamolegkou, Antonia and Zhou, Li and Dare, Megan and Donatelli, Lucia and Hershcovich, Daniel},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={80--99},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{zhou-etal-2023-cultural,
    title = "Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features",
    author = "Zhou, Li  and
      Karamolegkou, Antonia  and
      Chen, Wenyu  and
      Hershcovich, Daniel",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.845/",
    doi = "10.18653/v1/2023.findings-emnlp.845",
    pages = "12684--12702",
    abstract = "The increasing ubiquity of language technology necessitates a shift towards considering cultural diversity in the machine learning realm, particularly for subjective tasks that rely heavily on cultural nuances, such as Offensive Language Detection (OLD). Current understanding underscores that these tasks are substantially influenced by cultural values, however, a notable gap exists in determining if cultural features can accurately predict the success of cross-cultural transfer learning for such subjective tasks. Addressing this, our study delves into the intersection of cultural features and transfer learning effectiveness. The findings reveal that cultural value surveys indeed possess a predictive power for cross-cultural transfer learning success in OLD tasks, and that it can be further improved using offensive word distance. Based on these results, we advocate for the integration of cultural information into datasets. Additionally, we recommend leveraging data sources rich in cultural information, such as surveys, to enhance cultural adaptability. Our research signifies a step forward in the quest for more inclusive, culturally sensitive language technologies."
}

@inproceedings{huang2023culturally,
  title={Culturally aware natural language inference},
  author={Huang, Jing and Yang, Diyi},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={7591--7609},
  year={2023}
}

@article{wang2004image,
  title={Image quality assessment: from error visibility to structural similarity},
  author={Wang, Zhou and Bovik, Alan C and Sheikh, Hamid R and Simoncelli, Eero P},
  journal={IEEE transactions on image processing},
  volume={13},
  number={4},
  pages={600--612},
  year={2004},
  publisher={IEEE}
}




@article{liu2025culturevlm,
  title={CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries},
  author={Liu, Shudong and Jin, Yiqiao and Li, Cheng and Wong, Derek F and Wen, Qingsong and Sun, Lichao and Chen, Haipeng and Xie, Xing and Wang, Jindong},
  journal={arXiv preprint arXiv:2501.01282},
  year={2025}
}


@article{ananthram2024see,
  title={See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding},
  author={Ananthram, Amith and Stengel-Eskin, Elias and Vondrick, Carl and Bansal, Mohit and McKeown, Kathleen},
  journal={arXiv preprint arXiv:2406.11665},
  year={2024}
}




@article{liu2024culturally,
  title={Culturally Aware and Adapted NLP: A Taxonomy and a Survey of the State of the Art},
  author={Liu, Chen Cecilia and Gurevych, Iryna and Korhonen, Anna},
  journal={arXiv preprint arXiv:2406.03930},
  year={2024}
}


@article{adilazuarda2024towards,
  title={Towards measuring and modeling" culture" in llms: A survey},
  author={Adilazuarda, Muhammad Farid and Mukherjee, Sagnik and Lavania, Pradhyumna and Singh, Siddhant and Aji, Alham Fikri and O'Neill, Jacki and Modi, Ashutosh and Choudhury, Monojit},
  journal={arXiv preprint arXiv:2403.15412},
  year={2024}
}



@inproceedings{ruiz2023dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={22500--22510},
  year={2023}
}


@misc{japanese_stable_diffusion,
    author    = {Shing, Makoto and Sawada, Kei},
    title     = {Japanese Stable Diffusion},
    howpublished = {\url{https://github.com/rinnakk/japanese-stable-diffusion}},
    month     = {September},
    year      = {2022},
}




@inproceedings{xu2023imagereward,
  title={ImageReward: learning and evaluating human preferences for text-to-image generation},
  author={Xu, Jiazheng and Liu, Xiao and Wu, Yuchen and Tong, Yuxuan and Li, Qinkai and Ding, Ming and Tang, Jie and Dong, Yuxiao},
  booktitle={Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages={15903--15935},
  year={2023}
}



@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}