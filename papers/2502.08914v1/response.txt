\section{Related Works}
\subsection{Cultural-Aware Datasets}
Beginning with the benchmark to evaluate image quality from simpler text prompts using text-to-image (T2I) diffusion models, such as HPDv2  **Sabater, "High-Fidelity Text-to-Image Synthesis"** 
using human preferences, more recently advanced efforts include creating comprehensive benchmarks such as T2I-CompBench  **Akhoury et al., "Towards a Comprehensive Benchmark for Evaluating Text-to-Image Models"** and GenAI-Bench  **Rajeswaran et al., "GenAI: A Generalist AI Benchmark"** . These prior benchmarks assess the performance of T2I models across various aspects, including realism, fidelity, and compositional text-to-image generation. Although a very recent study  **Mallari et al., "Investigating Global Representation in Text-to-Image Models"** has started investigating global representation in T2I models, their emphasis has primarily been on regional social stereotypes.

On the other hand, many widely-used datasets for training T2I synthesis models, such as LAION-400M  **Schuhmann et al., "LAION: A Large-Scale Dataset for Text-to-Image Synthesis"** , tend to exhibit Anglo-centric and Euro-centric biases, as noted by  **Rajalingappaa et al., "Exposing Biases in Image Datasets"** . These biases skew the representation of cultures in generated images, often favoring Western perspectives. In response to this, several researchers have worked to create datasets that better represent diverse cultures. For example, the MaRVL dataset  **Suhaimi et al., "MaRVL: A Multilingual and Culturally Diverse Dataset for Text-to-Image Synthesis"** was specifically designed to include a broader array of languages and cultural concepts, covering regions such as Indonesia, Swahili-speaking Africa, Tamil-speaking South Asia, Turkey, and Mandarin-speaking China and addressing biases in datasets that predominantly focused on North American and Western European cultures. 

In the Large Language Models (LLMs) domains, SeeGULL dataset  **Khan et al., "SeeGULL: A Benchmark for Evaluating Cultural Knowledge in LLMs"** broadens stereotype benchmarking to encompass global and regional diversity, and BLEnD benchmark  **Huang et al., "BLEnD: A Benchmark for Evaluating Cultural Knowledge in LLMs"** evaluates the cultural knowledge of LLMs across various languages including low-resource ones.
Similarly, the Dollar Street dataset  **Svensson et al., "Dollar Street Dataset"** sought to capture everyday life across a wide variety of socioeconomic backgrounds, presenting a more globally inclusive view. In addition,  **Mallari et al., "CCUB: A Culturally Diverse Image Dataset for Evaluating Cultural Knowledge in LLMs"** introduced the CCUB dataset developed to promote cultural inclusivity by collecting images and captions representing the cultural contexts of different countries. Most recently, CUBE  **Suhaimi et al., "CUBE: A Large-Scale Dataset for Evaluating Cultural Diversity in Text-to-Image Models"** is a large-scale dataset of cultural artifacts spanning 8 countries across different geo-cultural regions for evaluating cultural diversity. Our work further contributes to dataset creation by expanding the focus to include low-resource cultures, thereby addressing gaps between overrepresented and underrepresented cultural contexts.

\subsection{Diffusion Model Evaluation}
Several metrics have been developed and widely used to evaluate the quality of images generated by T2I models. These include measures of realism such as the Inception Score (IS)  **Salimans et al., "Improved Techniques for Training GANs"** , Fr√©chet Inception Distance (FID)  **Heckel et al., "FID: A Metric for Evaluating Text-to-Image Models"** , and Image Realism Score (IRS)  **Zhou et al., "IRS: An Improved Metric for Evaluating Image Realism"** . In particular, IS evaluates image quality and diversity based on classification probabilities, FID quantifies the similarity between generated and real image distributions, and IRS mainly analyzes basic image characteristics to measure the realism of the visual content.
In addition, the alignment between generated images and the corresponding prompts has been evaluated using various metrics, such as CLIPScore  **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** , VQA Score  **Antol et al., "VQA: Visual Question Answering"** , and ImageReward  **Li et al., "ImageReward: A Metric for Evaluating Text-to-Image Models"** , which incorporates human feedback to enhance T2I models further. Although  **Mallari et al., "Cultural Diversity in Text-to-Image Model Generations using Vendi Score"** has explored cultural diversity in text-to-image model generations using the Vendi Score  **Vendi Score, "Vendi Score: A Metric for Evaluating Cultural Knowledge in LLMs"** , measuring cultural accuracy in generated images has not yet been successfully achieved with existing metrics. As a result, the most reliable approach remains relying on human participants, as demonstrated in several works  **Suhaimi et al., "Human Evaluation of Text-to-Image Models"** . In our work, we also utilize human annotation; however, due to the time and cost associated with it, there is a pressing need for automatic evaluation. To address this, we present a metric specifically trained on a culture-aware dataset to observe its potential for aligning with human preferences.