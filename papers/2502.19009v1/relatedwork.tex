\section{Related work}
\label{sec:rel}
\paragraph*{RL as sequence modeling.}
With the advent of Transformers, which can learn from much larger datasets than what an agent can typically collect online, their application to offline RL \citep{offlineRL} has gained prominence \citep{DT, TTO, MGDT, GATO}.
Despite these advances, \citet{AD} point out a key limitation: these approaches struggle to improve their policy in-context through trial and error.
The primary reason is that they are designed to imitate the dataset policy, which makes them unsuitable for performing in-context RL on novel tasks.
To address this, \citet{AD} propose Algorithm Distillation (AD), an in-context RL approach where a Transformer is trained to distill learning histories from a source RL algorithm across diverse tasks.
Notably, AD becomes effective when the context length of Transformers exceeds the episode horizon.
Instead of imitating the source algorithm's actions, Decision-Pretrained Transformer (DPT; \citet{DPT}) is designed to predict \emph{optimal} actions.
In-context Decision Transformer (IDT; \citet{IDT}) implements a hierarchical approach to in-context RL, where the Transformer predicts high-level decisions that guide a sequence of actions, rather than predicting each action individually.
In addition, \citet{Headless-AD} propose an in-context RL approach for variable action spaces, while \citet{AD-eps} synthesize learning histories by gradually denoising policies rather than directly executing a source algorithm.

\paragraph*{Meta-RL.}
Deep meta-RL began with online approaches, where RNNs are employed to learn RL algorithms that generalize across environments \citep{RL2, LtoRL}.
In parallel, gradient-based approaches \citep{MAML, Reptile} aim to discover parameter initializations that rapidly adapt to new tasks.
Recently, \emph{offline} meta-RL has gained attention, leveraging pre-collected meta-training datasets to address the meta-RL problem \citep{MACAW,BOReL,CORRO,IDAQ,MoSS}.
Various methods have been explored for this problem, including gradient-based \citep{MACAW}, Bayesian \citep{BOReL}, and contrastive learning approaches \citep{CORRO}.
Furthermore, recent in-context RL approaches \citep{AD, DPT, IDT, Headless-AD}, including our own, fall within the category of offline meta-RL.

\paragraph*{Model-based meta-RL.}
In the prior research on model-based meta-RL, 
\cite{VariBAD,HyperX,BOReL} focus on learning to infer belief states about environment dynamics, while \citet{MoSS} aim to infer task representations.
\citet{IDAQ} quantify uncertainty using an ensemble of meta-training dynamics when facing novel meta-test tasks.
\citet{ReBAL,TFsearch,MAMBA} learn to construct dynamics models for planning.
Our approach aligns with the latter category but stands out by integrating both the policy and the dynamics model within the same sequence model.
This contrasts with prior work that either relies on separate modules for modeling dynamics \citep{ReBAL,MAMBA}, or omits a policy network while depending solely on the dynamics model \citep{TFsearch}.
We provide a more comprehensive overview of related works in App.~\ref{sec:rel_ext}.