\section{Related work}
\label{sec:rel}
\paragraph*{RL as sequence modeling.}
With the advent of Transformers, which can learn from much larger datasets than what an agent can typically collect online, their application to offline RL ____ has gained prominence ____.
Despite these advances, ____ point out a key limitation: these approaches struggle to improve their policy in-context through trial and error.
The primary reason is that they are designed to imitate the dataset policy, which makes them unsuitable for performing in-context RL on novel tasks.
To address this, ____ propose Algorithm Distillation (AD), an in-context RL approach where a Transformer is trained to distill learning histories from a source RL algorithm across diverse tasks.
Notably, AD becomes effective when the context length of Transformers exceeds the episode horizon.
Instead of imitating the source algorithm's actions, Decision-Pretrained Transformer (DPT; ____) is designed to predict \emph{optimal} actions.
In-context Decision Transformer (IDT; ____) implements a hierarchical approach to in-context RL, where the Transformer predicts high-level decisions that guide a sequence of actions, rather than predicting each action individually.
In addition, ____ propose an in-context RL approach for variable action spaces, while ____ synthesize learning histories by gradually denoising policies rather than directly executing a source algorithm.

\paragraph*{Meta-RL.}
Deep meta-RL began with online approaches, where RNNs are employed to learn RL algorithms that generalize across environments ____.
In parallel, gradient-based approaches ____ aim to discover parameter initializations that rapidly adapt to new tasks.
Recently, \emph{offline} meta-RL has gained attention, leveraging pre-collected meta-training datasets to address the meta-RL problem ____.
Various methods have been explored for this problem, including gradient-based ____, Bayesian ____, and contrastive learning approaches ____.
Furthermore, recent in-context RL approaches ____, including our own, fall within the category of offline meta-RL.

\paragraph*{Model-based meta-RL.}
In the prior research on model-based meta-RL, 
____ focus on learning to infer belief states about environment dynamics, while ____ aim to infer task representations.
____ quantify uncertainty using an ensemble of meta-training dynamics when facing novel meta-test tasks.
____ learn to construct dynamics models for planning.
Our approach aligns with the latter category but stands out by integrating both the policy and the dynamics model within the same sequence model.
This contrasts with prior work that either relies on separate modules for modeling dynamics ____, or omits a policy network while depending solely on the dynamics model ____.
We provide a more comprehensive overview of related works in App.~\ref{sec:rel_ext}.