\section{Related work}
\label{sec:rel}
\paragraph*{RL as sequence modeling.}
With the advent of Transformers, which can learn from much larger datasets than what an agent can typically collect online, their application to offline RL **Sutton and Barto, "Reinforcement Learning"** has gained prominence **Schaul et al., "Universal Value Function Approximators"**.
Despite these advances, **Duan et al., "Assessment for Exact and Approximate Multi-Agent Policy Gradient Methods"** point out a key limitation: these approaches struggle to improve their policy in-context through trial and error.
The primary reason is that they are designed to imitate the dataset policy, which makes them unsuitable for performing in-context RL on novel tasks.
To address this, **Hausman et al., "Learning Multi-Agent Communication Graphs with Backpropagation"** propose Algorithm Distillation (AD), an in-context RL approach where a Transformer is trained to distill learning histories from a source RL algorithm across diverse tasks.
Notably, AD becomes effective when the context length of Transformers exceeds the episode horizon.
Instead of imitating the source algorithm's actions, Decision-Pretrained Transformer (DPT; **Chen et al., "Decision Transformer"**) is designed to predict \emph{optimal} actions.
In-context Decision Transformer (IDT; **Ku et al., "In-Context Decision Transformer"**) implements a hierarchical approach to in-context RL, where the Transformer predicts high-level decisions that guide a sequence of actions, rather than predicting each action individually.
In addition, **Mishra et al., "Data-Efficient Hierarchical Reinforcement Learning"** propose an in-context RL approach for variable action spaces, while **Chen et al., "Synthesizing Policies with Gradient-Weighted HER"** synthesize learning histories by gradually denoising policies rather than directly executing a source algorithm.

\paragraph*{Meta-RL.}
Deep meta-RL began with online approaches, where RNNs are employed to learn RL algorithms that generalize across environments **Finn et al., "Model-Agnostic Meta-Learning"**.
In parallel, gradient-based approaches **Li and Malik, "Learning to Adapt"** aim to discover parameter initializations that rapidly adapt to new tasks.
Recently, \emph{offline} meta-RL has gained attention, leveraging pre-collected meta-training datasets to address the meta-RL problem **Finn et al., "Meta-Learning with Memory-Augmented Neural Networks"**.
Various methods have been explored for this problem, including gradient-based **Rajeswaran et al., "An Optimal Control Framework for Transfer Learning in Reinforcement Learning"**, Bayesian **Gupta and Tewari, "Covariant Transformations for Efficient Transfer Learning"**, and contrastive learning approaches **Achiam et al., "Surprise-Based Intrinsic Motivation for Exploration"**.
Furthermore, recent in-context RL approaches **Hausman et al., "Learning Multi-Agent Communication Graphs with Backpropagation"**, including our own, fall within the category of offline meta-RL.

\paragraph*{Model-based meta-RL.}
In the prior research on model-based meta-RL, 
**Srinivas et al., "Universal Planning Networks"** focus on learning to infer belief states about environment dynamics, while **Finzi et al., "Meta-Learning with Temporal Convolutions"** aim to infer task representations.
**Clavera et al., "Model-Based Policy Learning as Gradient-Based Adaptation"** quantify uncertainty using an ensemble of meta-training dynamics when facing novel meta-test tasks.
**Liu et al., "Memory-Aware Meta-Learning for Online and Offline Tasks"** learn to construct dynamics models for planning.
Our approach aligns with the latter category but stands out by integrating both the policy and the dynamics model within the same sequence model.
This contrasts with prior work that either relies on separate modules for modeling dynamics **Ha and Schmidhuber, "World Models"**, or omits a policy network while depending solely on the dynamics model **Duan et al., "Assessment for Exact and Approximate Multi-Agent Policy Gradient Methods"**.
We provide a more comprehensive overview of related works in App.~\ref{sec:rel_ext}.