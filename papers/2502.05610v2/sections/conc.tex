
\section{Concluding discussions}
\label{sec:conc}

In this study, we benchmarked the power consumption of various large language models (LLMs) during inference across diverse NLP tasks. 


\vspace{2mm}
\noindent \textbf{General implications:} 
Our primary high-level takeaways can be summarized as follows: 
%
\textbf{(1)}~%
\newadd{
Inference energy displays a strong correlation with response time for locally run open-source LLMs, making it a reliable proxy for estimating energy consumption, with significantly less overhead compared to using specialized energy measurement tools.
In the case of models accessed through online APIs (such as closed-source models), 
% it is difficult to estimate the energy from just the response time due to a variety of factors, including network latency, number of concurrent user requests, type of batch scheduling/processing, etc. 
% However, 
in the absence of energy-related metrics from the API providers, response time can play as a proxy for energy estimation (see Appendix~\ref{app:blackbox}). 
}
%
\textbf{(2)}~While input size shows a linear relationship with energy use, output length has a stronger influence on inference energy.
%
\textbf{(3)}~Task complexity has little impact on inference time independent of input and output lengths.
%
\textbf{(4)}~ %Encoder-decoder models require less energy compared to similar-sized decoder-only models while having more accurate performance on NLP tasks.
Selecting models from the encoder-decoder family for NLP tasks is recommended from an energy-efficiency perspective, as well as their performance.
%
\textbf{(5)}~Increasing batch size reduces inference energy. However, it is constrained by the GPU memory availability, recommending an optimal batch size for a particular model, task pair. 
%
\textbf{(6)}~Quantization allows larger batches, resulting in lower energy use without degrading the inference accuracy much.
%, indicating a black-box solution for reducing the inference energy. % much and should be used to speed up inference time by increasing batch size.
%
\textbf{(7)}~Introducing targeted phrases achieves energy reduction for older decoder-only models by restricting their output for discriminative tasks. 


\vspace{2mm}
\noindent \textbf{Implications in energy-constrained environments:} In situations where LLMs are to be deployed in resource/energy constrained settings, our experiments lead to the following insights: 
\textbf{(i)}~The response time can be used as a proxy for energy consumed. This is useful not only since measuring power/energy may be difficult on most hardware but also because measuring response time requires much less overhead (in terms of energy/latency) than actually measuring the energy consumption. In low resource settings, this overhead can be large in terms of percentage. For example, we found that removing the energy-tracking libraries and just recording the system time can reduce inference time between 15\%--50\% under different scenarios.
\textbf{(ii)}~Where possible, input compression and output optimization should be employed. At least, some targeted phrases should be incorporated so that the model generates only what is needed.
\textbf{(iii)}~Fine-tuned encoder-decoder models (especially Flan-T5 models), are better suited for low-resource settings, particularly for NLP tasks such as classification, summarization, etc. 
\textbf{(iv)}~Quantized models should be employed, allowing larger batches and larger models on such settings, from the point of memory constraint.  
%(Check Appendix~\ref{sec:implications}.)

\vspace{2mm}


%(Check Appendix~\ref{sec:energy-proxy}.)


% We select LLMs from two families: decoder-only models and encoder-decoder models. Our chosen tasks range from short answer-based tasks like text classification and text entailment to complex generative tasks like text summarization and question-answering. To measure energy consumption, we use two widely employed packages. Our primary focus is to identify the factors affecting inference energy. We measure the correlation of energy with various factors, including input/output token length, response time, model size.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% outside the normal 8 page limit
% \cleardoublepage
\section*{Limitations}
Despite the comprehensive analysis and valuable insights provided by this study, the following limitations should be considered. First, the benchmarking experiments were conducted under controlled conditions, which may not fully capture the variability and complexity of real-world deployment environments. The results might differ when models are deployed on different hardware, infrastructure or in varying operational contexts.
Also, the study focuses primarily on specific NLP tasks and may not generalize fully to other domains like vision or time series analysis. Additionally, while the study explores a range of system-related factors, it does not account for all possible variables that could influence inference energy, such as network latency or hardware-specific optimizations.

% Lastly, the recommendations provided for improving energy efficiency, such as optimal batch sizes or targeted prompt phrases, may require additional experimentation and fine-tuning in practical applications to achieve the desired outcomes. These limitations highlight the need for ongoing research to refine further and expand the findings presented here.

\section*{Ethical Considerations}
One of the main ethical issues in our experimentation was the substantial energy consumption and carbon emissions it produced. We perform 1024 inferences for 11 datasets over 10 models in several configurations, necessitating multiple repetitions of the inferences, along with several pilot experiments to finalize the experimental setup. This led to an approx total energy use of $3000$ kWh. To reduce our environmental impact, we limited our experiments to only 1024 test examples sampled from the datasets. 
We hope that the insights from this study will lead the community to a much larger reduction of the energy consuption of LLMs.

% \newpage