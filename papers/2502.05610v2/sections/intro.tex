\section{Introduction}

\input{tables/comparison}


%Understanding the environmental impact of AI algorithms, especially large language models (LLMs) used in natural language processing (NLP), is crucial due to their escalating energy demands~\cite{hintemann2022cloud,kamiya2022data}.

%Over the past five years, the energy consumption of commercially available cloud providers has undergone exponential growth. The energy usage of global data centers has increased by 20-40\% annually, significantly contributing to overall greenhouse gas emissions. Therefore, it is vital to conduct comprehensive studies to accurately measure the energy impact of recent generative AI models, particularly LLMs.

Recent discussions on the energy and carbon impact of machine learning (ML) algorithms have mainly concentrated on quantifying energy usage during the training phase of these models~\cite{dodge2022measuring, luccioni2023counting, patterson2021carbon, raffel2020exploring}. Studies on inference energy are much rarer because a single inference operation consumes considerably less energy and resources. However, under deployment, inference is performed many more times, making its energy impact significant and warranting separate investigation~\cite{wu2022sustainable,patterson2022carbon}. For example, 90\% of total cloud computing demand for AWS, the largest global cloud provider, were for model inference purpose~\cite{barr2019amazon}. 
% \citet{wu2022sustainable} claim that one-third of their internal end-to-end carbon footprint is contributed by model inference. Similarly, \citet{patterson2022carbon} show that 60\% of their ML energy usage is attributed by model inference. 
%
Moreover, a key motivation for training large language models is that a single model can achieve state-of-the-art performance across diverse NLP tasks due to its impressive zero-shot and few-shot capabilities, making it energy-efficient from a training perspective. However, when we consider the total carbon footprint of the entire lifetime of the model, the energy requirement for model inference plays a significant role, considering the number of inferences that are carried out during the model's lifetime. 
%
Thus it is crucial to conduct a systematic study to quantify the energy requirements and carbon emissions for model inference across various models and tasks.


\iffalse 
% \vspace{1mm}
% \noindent 
\textbf{Limitations of existing approaches:} Some existing works attempt to address the gap, where inference energy of LLMs are studied from varied perspectives. 
%
\citet{everman2023evaluating} evaluates a bunch of GPT-based models on a number of hand-crafted NLP prompts (open-ended question answering).%, reporting the carbon impact. 
%utilizing Software Carbon Intensity released by green software. 
\citet{samsi2023words} study the inference energy of LLMs 
%per second, energy per prompt and energy per output token 
for selected question-answering datasets for Llama family models on different GPU architecture.
%from the Llama family for A100 (80 GB) and V100 (32 GB) system. %Most of the results are presented for Llama 65 B. 
%\citet{liu2022few} compare fine-tuning with few-shot learning, where they consider variants from GPT family with variants from sequence-to-sequence family while selecting tasks from SuperGLUE and RAFT, and report the inference cost by FLOPs per example. 
%We offer a more comprehensive discussion on the topic by showing fine-tuning vs few-shot learning on a more diverse range of datasets for more recent LLMs from both family, where we directly report inference energy consumed.  
%
%However, our work differs from this work by including a number of additional setups, namely, variation of energy with factors like batch size, model complexity, input length, quantization, presence of energy directives in prompt, fine-tuning vs few-shot setup, etc. Further, it analyses variation of energy with response time, input token length and output token length at a higher granular level by reporting per-batch inference energy. Moreover, we vary the input and output token length at a finer granular level, keeping other factors fixed and report the variation in energy to have a better understanding of the correlation between energy and these factors. To capture the trade-off between inference time and performance, we provide inference performance wherever appropriate. Finally, we include more recent LLMs from both architecture families.  
%
\citet{li2024toward} compare the effect of prompt directives on inference energy and performance on 3 applications for Llama-2 (7B \& 13B).
% of different sizes.
%, where we offer a wider range of prompt directives on a wider set of tasks for more diverse and comprehensive range of LLMs to provide a more comprehensive benchmark. 
%

However, most of these works focus on analyzing the inference energy of LLMs from some particular aspect while limiting themselves to a limited set of LLMs and tasks, mostly ignoring the performance-energy tradeoff. 
%\citet{luccioni2024power} offers the most comprehensive view of the inference cost of LLMs, by selecting LLMs from both prominent families, selecting tasks from a wide range, and presenting inference energy, along with showing its variance with model size and architecture, task type and output token length. 
%
\citet{luccioni2024power} offers benchmarking inference of LLMs for a diverse range of LLMs and tasks, while primarily focusing on the influence of model complexity and task type on inference energy. %, and output length only. 
\fi

% \vspace{1mm}
% \noindent 
\subsection{Literature survey}  Existing works have attempted to study the inference energy of LLMs from various perspectives. \citet{everman2023evaluating} evaluated energy usage of GPT models on question-answering tasks, employing Software Carbon Intensity (SCI) released by green software. \citet{samsi2023words} provides a detailed account of energy usage during inference for question-answering tasks for Llama models on various GPU architectures. \citet{liu2022few} study the energy consumption trade-off for fine-tuning vs few-shot learning for both encoder-decoder and decoder-only models on SuperGLUE and RAFT tasks, measuring energy in FLOPS. These preliminary works mostly ignore the performance-energy tradeoff. 

In recent times, \citet{li2024toward} compare the effect of prompt directives on inference energy and performance for Llama-2 models for 3 question-answering tasks. \citet{luccioni2024power} offers a comparatively detailed account of inference energy usage while choosing LLMs and tasks from a diverse range, showing the dependency of energy with model size and architecture, task type, and output token length. While these works provide an initial account of inference energy usage of LLMs in different configurations, they are often limited by the number and diversity of the models and tasks (Check Appendix~\ref{sec:appen-literature} for details).

% \vspace{1mm}
% \noindent 
\subsection{Our contributions \& differences with prior works}
% On the contrary, 
In this work, we present a comprehensive study of LLMs, running models from both encoder-decoder and decoder-only models on both discriminative and generative NLP tasks, while analyzing the impact of different models, tasks, prompts, and system-related factors on inference energy.
% \vspace{2mm}
% \noindent \textbf{Contributions:}
%
Specifically, 
%we present a comprehensive study on the energy requirements of large language models on a diverse range of models and tasks, which we start with 
(i)~We start with a detailed analysis of various model-related factors that affect the inference energy of LLMs, where we systematically study the correlation between inference energy and influencing factors like input and output token length, response time, model size and complexity. 
% Next, we also conduct experimental settings where these factors are varied and corresponding change in inference energy is documented, allowing us to understand the influence of each factor distinctly. 
(ii)~We conduct various experiments to study the connection between inference energy and batch size, level of quantization, and prompt editing. 
(iii)~We then complement our analysis by introducing the Normalized Accuracy metric, providing an accuracy-energy usage tradeoff analysis across tasks and models. 
(iv)~Finally, we present a list of efficiency guidelines in Section~\ref{sec:conc}.
%

%Various factors vs energy
% 1. Input token length *
% 2. Output token length *
% 3. Response time *
% 3. Task complexity
% 4. Model size *
% 5. Model architecture *
% 6. Batch size *
% 7. Quantization
% Fine-tuning vs in-context
% Prompt directives

%Our work is the first to provide a comprehensive benchmarking of energy consumption of LLMs during inference for a diverse range of configurations, i.e., variation of input and output token length (for both coarse and granular level), correlation with response time, model size and family, task complexity, batch size, quantization level, presence of targeted phrases, and energy directives in prompt for a more complete set of LLM and tasks.

%Table~\ref{tab:comparison} compares our work with existing literature in eight different aspects. 
%different experimental settings. 
In particular, our contributions include an energy analysis from eight aspects, as given in Table~\ref{tab:comparison}. Out of these aspects, several were primarily unexplored in the literature, as per our knowledge - these are indicated in the table. %Table~\ref{tab:comparison}. 
We vary the input and output token length at a finer granular level while reporting the variation in energy to have a better understanding of the correlation between energy and these factors, revealing several valuable insights (Section~\ref{sec:input-output}). A more thorough analysis of energy with variation of input or output while keeping the other fixed provides a more accurate insight into their comparative influence in inference energy, implying immediate solutions for energy-efficient inference approaches. The correlation between energy and response time validates response time as a good proxy of inference energy in black-box models (Section~\ref{sec:responsetime}). Our work first explores task complexity's effect with inference energy (Section~\ref{sec:complexity}). We also provide the performance change for quantization (Section~\ref{sec:batch}) and use of targeted phrases (Section~\ref{sec:phrases}) along with improvement in energy. While the effects of quantization and increasing batch size have been studied individually, we show that using them together under fixed memory constraints can help speed up the computations. Finally, we explore each of the above aspects for a diverse range of LLMs from both architecture families along with a diverse range of tasks, which was mostly lacking in previous attempts. 