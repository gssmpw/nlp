
\iffalse 
\section{Literature Survey}


\textbf{Sustainable Large Language Models:}
% Green AI
\cite{schwartz2020green} discuss the growing compute cost of deep learning research and advocate for making AI both greener and more inclusive by making efficiency an evaluation criteria.
% 
%
%\textbf{Energy-efficient LLM inference:}
Following that trend, 
black-box approaches for reducing energy consumption of LLMs include usage of generation directives~\cite{li2024toward},  hardware and datacenter-oriented settings~\cite{mcdonald2022great}, LLM cascading, prompt adaptation ~\cite{chen2023frugalgpt}, etc, while 
%
white-box approaches include 
%early exits
speculative decoding~\cite{leviathan2023fast}, prunning~\cite{kurtic2024ziplm}, embedding recycling~\cite{saad2022embedding}, quantization~\cite{bai2022towards,frantar2022gptq,xiao2023smoothquant}, few-shot PEFT~\cite{liu2022few}, etc.


\noindent \textbf{Tools for measuring energy impact:}
%
Toward systematic tracking of the energy consumption and carbon
emissions in these models, researchers propose various tools, namely
%, namely 
CodeCarbon~\cite{codecarbon}, CarbonTracker~\cite{anthony2020carbontracker}, Experiment impact tracker~\cite{henderson2020towards}, EnergyScope~\cite{limpens2019energyscope}, %etc. 
%
% Green Algorithms~\cite{lannelongue2021green},
%is another online tool, enabling a user to estimate and report the carbon footprint of their computation. 
Eco2AI~\cite{budennyy2022eco2ai},
%is another open-source package to help data scientists and researchers to track energy consumption and equivalent CO$_2$ emissions of their models in a straightforward way. %
Carburacy~\cite{moro2023carburacy}, etc.
% proposes the first carbon-aware accuracy measure that captures both model effectiveness and eco-sustainability for generative transformer-based models~\cite{moro2023carburacy}. 
%
%Researchers explore energy impact analysis in terms of carbon footprints of ML algorithms in various domains, namely differential privacy~\cite{naidu2021towards}, medical image analysis~\cite{selvan2022carbon}, etc. 
% In this work, we focus on the carbon footprint of developing DL models for medical image analysis\cite{selvan2022carbon}
% This paper investigates the impact of differential privacy on learning algorithms in terms of their carbon footprint due to either longer run-times or failed experiments.
%%
Recent literature benchmarks these tools 
in various configurations~\cite{cao2020towards,jay2023experimental,bouza2023estimate}, unanimously recommending CodeCarbon,
followed by CarbonTracker.%, with more variability between infrastructures. 
\fi


\iffalse
\noindent \textbf{Benchmarking inference energy of LLMs:} 
%
%for various deep learning based ML models. 
%\cite{cao2020towards} compare energy returned by software-based energy measurements with hardware power meter (WhattsUPMeter) on various NLP models.% and report experiment impact tracker as not so accurate. 
%
%Jay et al~
%\cite{jay2023experimental} qualitatively and experimentally compare several software-based power meters
%against high-precision physical power meters
%while executing various intensive workloads, where they 
%concluding that for measuring energy, Carbon Tracker, Code Carbon, Energy Scope, and Experiment Impact Tracker are suitable fits. However, \cite{bouza2023estimate} recommend 
%establish that the energy value reported by 
%CodeCarbon,
%is closest to Wattmeter, 
%followed by CarbonTracker.%, with more variability between infrastructures. 
%
%\textbf{Benchmarking LLMs:}
Using the abovementioned tools, researchers attempt to benchmark the inference energy of LLMs in a diverse range of tasks and configurations.   
% “Evaluating the Carbon Impact of Large Language Models at the Inference Stage (IPCCC 2023)”  
\cite{everman2023evaluating} conducts a thorough study on the carbon impact of GPT-variants,
%various open-source LLMs, including GPT-J 6B, GPT Neo 2.7B, GPT-NEO 1.3B, and GPT-2 at the inference stage, utilizing the Software Carbon Intensity (SCI) specification released by the Green Software Foundation, 
concluding high-carbon LLMs do not necessarily provide
superior model quality than their low-carbon counterparts.
%
\cite{samsi2023words} benchmark the inference energy of Llama models on diverse GPUs (NVIDIA V100 and A100).% in QA datasets.%and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. 
%
%\cite{liu2022few} recommends few-shot PEFT over in-context learning from the energy-efficiency perspective. %parameter-efficient fine-tuning is less energy-intensive without affecting the inference performance. 
% "Power Hungry Processing: Watts Driving the Cost of AI Deployment?“ (arXiv 2023) 
%\cite{desislavov2021compute} study the correlation between model complexity and inference energy 
%(measured using GFLOPs) 
%for DNN-based NLP models.%and Computer Vision models.  
~\cite{luccioni2024power} propose the first systematic comparison of the inference energy of flan-t5 and Bloom models from a diverse range of task and model-related aspects. %, namely 
%cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ‘general-purpose’ models. 
Table~\ref{tab:comparison} compares existing approaches with our work in various experimental settings.
%
(Check Table~\ref{tab:existing-literature} in Appendix for an overview of existing approaches and their limitations.) 
\fi