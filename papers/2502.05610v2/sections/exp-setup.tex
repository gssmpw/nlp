

\section{Experimental setup}

In this section, we describe the models, datasets and various settings we use for our experiments.
% , We describe the tools we use to measure the energy consumed during inference stage, followed by the models, tasks and datasets studied in this work. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Models}
\label{sub:models}

% Based on architecture, LLM models can be broadly categorized into two categories, encoder-decoder models, and decoder-only models. 
We select $6$ popular and recent GPT-style models from the decoder-only family and $4$ Flan-T5 models from the encoder-decoder family, adding to $10$ models in total 
(details in Appendix~\ref{app:models}).


\noindent \textbf{Decoder-only Models} generate output in an autoregressive manner by predicting the next token in the sequence based on the context (key-value-query) vectors corresponding to the input and previously generated tokens. 
We consider the following models from decoder family in our study. 
(D1)~\textbf{Tiny-LLama}~(1.1B params);
(D2)~\textbf{Phi-3-mini}~(3.8B params);
(D3)~\textbf{Mistral-7B}~(7.2B params);
(D4)~\textbf{Llama-2-7B}~(6.7B params);
(D5)~\textbf{Llama-3-8B}~(8.0B params);
(D6)~\textbf{Llama-2-13B}~(13B params); 


\noindent \textbf{Encoder-Decoder models} process the input data and convert it into context (key-value) vectors. Then the decoder takes these vectors and generates output autoregressively. Models from this family, considered in our study, include: 
%
(ED1)~\textbf{Flan-T5-base}~(248M params); 
(ED2)~\textbf{Flan-T5-large}~(783M params);
(ED3)~\textbf{Flan-T5-xl}~(2.8B params);
(ED4)~\textbf{Flan-T5-xxl}~(11B params);




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Tasks and Datasets} 
In this work, we select a diverse range of NLP tasks, from generative to question-answering, classification, and single-sentence tasks. This includes both general GLUE / SuperGLUE benchmarks, as well as domain specific \vax and \caves (for studying effect of task complexity).
% We detail the tasks in App.~\ref{app:tasks}. 
We describe the tasks and their corresponding datasets in Table~\ref{tab:tasks}. 
%
% $\bullet$ \textbf{Single-sentence NLP tasks (GLUE)}: From GLUE benchmark~\cite{wangglue}, we select three single-sentence tasks, namely sentence entailment (\mnli), sentiment classification (\sst), and linguistic acceptability check (\cola).
%
% $\bullet$ \textbf{Complex logical NLP tasks (SUPER-GLUE)}: From Super-GLUE benchmark~\cite{wang2019superglue}, we select comparatively complex reading tasks, including contextual question answering (\boolq), causal reasoning (\copa), and entity question answering (\record). 
%
% $\bullet$ \textbf{Reading comprehension and question answering}: We select SQUAD~v2~\cite{rajpurkar-etal-2016-squad} for complex question answering task. 
%
% $\bullet$ \textbf{Generation}: The generation tasks include news article summarization (\cnndm{}~\cite{nallapati2016abstractive}) and dialogue summarization (\samsum{}~\cite{gliwa2019samsum}).
%
% $\bullet$ \textbf{Domain-specific Classification}: Finally, we include two tweet classification datasets from social domain, namely \vax{}~\cite{poddar2022winds} (3-class stance classification) and \caves{}~\cite{poddar2022caves} (12 class multi-label classification).
%
For each dataset, we selected $1024$ data samples randomly and performed all experiments on the same set for comparable results.
% \subsection{Performance Metrics}
Performance metrics are chosen depending on the tasks. For summarization tasks, average of ROUGE1, ROUGE2, and ROUGE-L are reported, whereas some form of F1 score are reported for the other tasks. Description/prompts of the datasets and the individual metrics have been given in Appendix~\ref{app:tasks}.



\vspace{1mm}
\noindent \textbf{Normalized Accuracy (NA) Metric:}
Since different tasks use different metrics on different scales, it is difficult to compare the accuracy performance of models across the tasks. To gauge the overall performance of the models across multiple tasks, we introduce the \textit{NA} metric that is obtained as follows. For each dataset, we first perform Z-score normalization across all the models, followed by a sigmoid operation to scale models between $0$ and $1$. We then average the scores for each model across all datasets and multiply by $100$.
Note that this metric depends on the set of models used and will vary if models are added/removed. However, it allows us to quantify how well a model performs compared to others in the set.




\input{tables/tasks}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Hardware and Energy metrics}
We perform our experiments on a single NVIDIA A6000 GPU with 48GB VRAM hosted in a local server with Intel Xeon Silver 4210R processor and 128GB RAM, running Ubuntu 20.04-LTS. 
The server also hosted an NVIDIA A5000 GPU (24 GB), which was used for only one experiment, but otherwise was unused.
We also performed some experiments on two other systems to verify the generalizability of our findings, as detailed in Appendix~\ref{app:mgpu}.
We use Pytorch version 2.3 (with CUDA 12.1) and huggingface transformers version 4.41.
% \todo{Mention packages. Specify chosen packages. Explain the reason. }

We use the popular Code Carbon~\cite{schmidt2021codecarbon} and Carbon Tracker~\cite{anthony2020carbontracker} packages to measure the energy consumed in different experiments. 
% We select CodeCarbon and CarbonTracker among many existing energy tracker because 
\citet{jay2023experimental} demonstrated the suitability and accuracy of CarbonTracker, CodeCarbon, Energy Scope, and Experiment Impact Tracker across various software-based power meter setups, while \citet{bouza2023estimate} further established the superiority of CodeCarbon and CarbonTracker among these tools. CodeCarbon is especially the most user-friendly and works out of the box, provided appropriate NVIDIA libraries and permissions to Intel RAPL files.


% Both of these energy tracking libraries provide various important information including duration, CO2 emission rate, total power/energy and that consumed by GPU, CPU, and RAM; etc.  
%
These two packages measure the GPU-power usage using pynvml and CPU-power using Intel RAPL files every $\mathcal{X}$ seconds, and integrates it over time. Carbon-tracker reports sum of these as the total energy.
Code-carbon also adds an estimate of the RAM-power being used depending on the RAM size. 
We use $\mathcal{X} = 10secs$ for a balance between overhead costs and tracking accuracy.
We keep the Power Usage Effectiveness~(PUE) to the default $1.0$ since we run all experiments on the same server, but this implies that the actual energy usage is higher than reported.



During inference, we provide test samples in batches to the LLM, 
and measure the total energy required for 1024 samples per dataset using these tools. This includes both the input tokenization process by each model's tokenizer and the output generation from the model.
We keep the batch size to $8$ for most experiments, except on the \cnndm and \samsum dataset for which we use a batch size of $4$. 
While reporting results, we average the energy usage and report the \textbf{energy per sample} in mWh (milli-Watt-hour). Unless otherwise stated, these are the default settings used for experiments. 



% In our study, we omit the amount of carbon emission because we perform all the experiments in a single region where the carbon intensity is fixed and therefore, energy consumed is closely related with the amount of $CO_2$ emission. Furthermore, the CO$_2$ emission strongly varies depending on the region and the type of electricity source. Thus, we prefer to report the total energy consumed instead of the amount of CO$_2$ emission. 

