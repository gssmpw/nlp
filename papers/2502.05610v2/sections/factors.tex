

\section{Factors Affecting Energy / Accuracy}
In this section, we discuss how different task, model, and setup-related factors contribute to the inference energy and accuracy metrics.



\begin{figure*}[!t]
\centering

\includegraphics[height=4.25mm,trim={85mm 141mm 70mm 2mm},clip]{Plots_main/scatter_model/flan-t5-base-energy_cc_vs_energy_ct.pdf}
\vspace*{-6mm}
%\subfloat{\includegraphics[height=3.45cm,trim={3.5mm 11mm 3.5mm 4mm},clip]{Plots_main/scatter_model/flan-t5-xl-energy_vs_response_time.pdf}}
%\hfill
%\subfloat{\includegraphics[height=3.45cm,trim={10.5mm 11mm 3.5mm 4mm},clip]{Plots_main/scatter_model/flan-t5-xl-energy_vs_input_len.pdf}}
%\hfill
%\subfloat{\includegraphics[height=3.45cm,trim={10.5mm 11mm 3.5mm 3.5mm},clip]{Plots_main/scatter_model/flan-t5-xl-energy_vs_output_len.pdf}}

%\vspace*{-2mm}
\subfloat{\includegraphics[height=4cm,trim={3.5mm 4mm 3.5mm 3.5mm},clip]{Plots_main/scatter_model/Mistral-7B-Instruct-v0.2-energy_vs_response_time.pdf}}
\hfill
\subfloat{\includegraphics[height=4cm,trim={10.5mm 4mm 3.5mm 3.5mm},clip]{Plots_main/scatter_model/Mistral-7B-Instruct-v0.2-energy_vs_input_len.pdf}}
\hfill
\subfloat{\includegraphics[height=4cm,trim={10.5mm 4mm 3.5mm 3.5mm},clip]{Plots_main/scatter_model/Mistral-7B-Instruct-v0.2-energy_vs_output_len.pdf}}

% \vspace*{-2mm}
\caption{Inference energy vs response time, input and output-token length averaged across samples in a batch plotted across all datasets for  \textbf{Mistral-7B}. Dots correspond to distinct batches of different datasets.}% Linear patterns can be seen in all cases. Response time is maximally correlated with energy, the output token length affect inference energy significantly more than input token length. }
\label{fig:energy-scatter}
% \vspace*{-3mm}
\end{figure*}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Response time}
\label{sec:responsetime}
Response time is an important indicator for actual inference energy, as given a (somewhat) fixed amount of power draw, the energy consumed is proportional to inference response time. 
To this end, we track the energy for each batch where the tracking interval is set to $1 sec$ for the energy-measuring libraries. The batches were formed after sorting the inputs (prompt + query) by length (so that similar-length queries end up together, allowing optimal padding and energy usage). 

Figure~\ref{fig:energy-scatter} (left column)
%and ~\ref{fig:energy-scatter-mistral}~(left) 
compares per-sample average response time and inference energy.
They report the comparison for Mistral (rest of the models in Appendix~\ref{app:scatter}). 
Points in the plot correspond to the average scores per query for the individual batches, with distinct color for each dataset. %s in the plot indicate results for different datasets. 


We find a strong correlation between response time and the inference energy (pearson $r = 0.996$, spearman $rs = 0.968$), indicating a strong possibility of using the response time as a reliable proxy for the energy consumed if demographic factors like location, energy grid, model, etc, are fixed.
However, for different datasets, the slope of the dependency is different, which may be because of slightly different power draws due to datasets having different-sized inputs.
We also compare the energy measures returned by CarbonTracker and CodeCarbon package and find a good correlation (pearson $r = 0.610$, spearman $rs = 0.912$), indicating reliable tracking.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \begin{figure*}[!t]
% \centering
% \subfloat[Flan-T5-xl]{\includegraphics[width=0.3\textwidth]{Plots_main/scatter_model/flan-t5-xl-energy_vs_response_time.pdf}}
% \subfloat[Mistral-7b]{\includegraphics[width=0.3\textwidth]{Plots_main/scatter_model/Mistral-7B-Instruct-v0.2-energy_vs_response_time.pdf}}
% \subfloat[Llama-2-13b]{\includegraphics[width=0.3\textwidth]{Plots_main/scatter_model/Llama-2-13b-chat-hf-energy_vs_response_time.pdf}}
% \caption{Inference energy vs Response Time across all tasks, where we observe response time closely correlates with inference energy.}
% \label{fig:energy-vs-response}
% \end{figure*}


% \begin{figure*}[!t]
% \centering
% \subfloat[Flan-T5-xl]{\includegraphics[width=0.3\textwidth]{Plots_main/scatter_model/flan-t5-xl-energy_vs_input_len.pdf}}
% \subfloat[Mistral-7b]{\includegraphics[width=0.3\textwidth]{Plots_main/scatter_model/Mistral-7B-Instruct-v0.2-energy_vs_input_len.pdf}}
% \subfloat[Llama-2-13b]{\includegraphics[width=0.3\textwidth]{Plots_main/scatter_model/Llama-2-13b-chat-hf-energy_vs_input_len.pdf}}
% \caption{Input token length vs inference energy across all tasks. }%\todo{remove some datasets so that the plot seems linear}}
% \label{fig:energy-vs-input}
% \end{figure*}


% \begin{figure*}[!t]
% \centering
% \subfloat[Flan-T5-xl]{\includegraphics[width=0.3\textwidth]{Plots_main/scatter_model/flan-t5-xl-energy_vs_output_len.pdf}}
% \subfloat[Mistral-7b]{\includegraphics[width=0.3\textwidth]{Plots_main/scatter_model/Mistral-7B-Instruct-v0.2-energy_vs_output_len.pdf}}
% \subfloat[Llama-2-13b]{\includegraphics[width=0.3\textwidth]{Plots_main/scatter_model/Llama-2-13b-chat-hf-energy_vs_output_len.pdf}}
% \caption{Output tokens length vs Inference energy across all datasets. }
% \label{fig:energy-vs-output}
% \end{figure*}




\subsection{Input and Output token length}
\label{sec:input-output}

The complexity of each attention block in a transformer decoder model is given by the following equation ~\cite{vaswani2017attention}, where $n$ is the \#input~tokens, $d$ is the hidden dimension, and $t$ is the \#output tokens.
%
% \vspace*{-3mm}
\begin{equation}
\label{eq:llm-complexity}
O(n,d,t) = (n.d^2+n^2.d).t
\end{equation}
% \vspace*{-6mm}


This equation suggests input and output length play a major role in deciding the computational complexity of large language models, which consist of several consecutive layers of multiple such attention blocks, and thereby, the required inference energy. 
In this section, we attempt to explore the influence of the aforementioned factors in a more systematic manner. 
%
Toward that, 
%\noindent \textbf{At the batch level:}
we first explore a similar setup explained in Section~\ref{sec:responsetime} to plot the batch-wise energy. Sorting the inputs by their length before batching is especially important because the batches with random input lengths can all get averaged out to have similar values otherwise, making it difficult to visualize the effect of energy with input/output sizes.
%
% For all datasets, we sorted all the input texts (prompt + query) by their length, in order to group together similar-length inputs into the same batch.



% \st{While FLOPs are theoretically a quadratic function of input token length (eq ~\ref{eq:llm-complexity}), our empirical results indicate a linear trend. This apparent discrepancy can be explained by considering the practical factors influencing energy consumption. As discussed in Section 3.2, modern hardware systems integrate various optimizations – such as parallel processing, caching mechanisms, memory hierarchies – that significantly alter the relationship between computational complexity and energy usage.
% For instance, Transformer models (including LLMs) leverage GPU parallelism to process the input, which effectively mitigates the quadratic scaling predicted by theoretical calculations~\cite{vaswani2017attention}. Additionally, mechanisms like key-value (KV) caching ensure that input tokens are processed only once, further reducing energy demands compared to the theoretical worst-case scenario (as we briefly mentioned in lines 304–308) \cite{pope2022efficiently}. These optimizations are likely the primary contributors to the observed linear correlation between input token length and energy consumption. We will add this discussion and the relevant references in the final version of the paper.
% }

\vspace{2mm}
\noindent \textbf{Input Length:}
Figure~\ref{fig:energy-scatter} (middle column)
%and \ref{fig:energy-scatter-mistral}~(center) 
compares per-sample average inference energy with per-sample average input token length.  
% The figure has been truncated to remove the top 10\% plot points (which mostly belong to the \cnndm{} dataset), to improve visibility of smaller clusters
The bigger, spread out clusters primarily belong to the generative tasks, namely, \cnndm{} and \samsum{} datasets, due to their larger outputs than the other discriminative tasks, which lay in the bottom clusters. 
Even though the input size appears as a quadratic term in Eq.~\ref{eq:llm-complexity}, we see a linear variation of energy usage with input size.
\newadd{This discrepancy can be attributed to various factors -- hardware-related optimizations such as parallel processing, caching mechanisms, and memory hierarchies – that significantly influence the relationship between computational complexity and energy usage.
%
For instance, Transformer models (including LLMs) leverage GPU parallelism to process the input, which effectively mitigates the quadratic scaling predicted by theoretical calculations~\cite{vaswani2017attention}. Additionally, mechanisms like key-value (KV) caching ensure that input tokens are processed only once, further reducing energy demands compared to the theoretical worst-case scenario~\cite{pope2022efficiently}. These optimizations are likely the primary contributors to the observed linear correlation between input token length and energy consumption. }
%\note{Another observation is that the slope is steeper for decoder-only models compared to encoder-decoder models.}
%



\vspace{2mm}
\noindent \textbf{Output Length:}
Figure~\ref{fig:energy-scatter} (right column)
%-flan-t5},\ref{fig:energy-scatter-mistral}~(right) 
compares per-sample average output token length with per-sample average inference energy for individual batches. Here we observe a similar trend indicating linear increment of energy with output size, in accordance with Eq.~\ref{eq:llm-complexity}. 
Here most tasks except \cnndm{} and \samsum{} cluster around the bottom left because of their short outputs, whereas the widespread clusters of \cnndm{} and \samsum{} towards the top provide a better visualization of the linear dependency.
\newadd{Note that, the slope of variation of energy consumption is steeper for the output length, in comparison to the slope for the input length despite the input being larger than the output (input Pearson $r = 0.697$, output Pearson $r = 0.952$). These observations indicate \textit{a stronger role played by output length (than the input length) in deciding the inference energy}, which can be explained as follows. 
During inference, LLMs leverage key-value (KV) caching, which allows tokens to be processed only once,
% when generating the first output token. 
% For generating subsequent tokens, the model uses cached representations, 
avoiding repeated prior token processing. Furthermore, input processing can be parallelized on GPUs, leading to significant speedups and lower energy consumption~\cite{vaswani2017attention}. 
In contrast, generating output tokens is inherently a sequential process, as each token depends on the previous one. This lack of parallelism for output token generation leads to the steeper slope of output token length. 
}


\begin{figure}[!t]
\centering
{\includegraphics[height=50mm,trim={4mm 4mm 3mm 4mm},clip]{Plots_main/vary_in/cnndm-energy-avg.pdf}}
% \vspace*{-2mm}
\caption{Inference energy on \cnndm where we vary input token lengths fixing \#output tokens to 1.}
\label{fig:vary-input}
% \vspace*{-2mm}
\end{figure}

\vspace{2mm}
\noindent \textbf{Controlled setup:}
For better and more exclusive insights into the relation between inference time and input and output length, we perform the following controlled experiment where we fix either input or output length and vary the other. 




\noindent \textbf{\textit{Effect of varying input length}:}
For \cnndm dataset, we truncated each input text into $N$ tokens and ask the model to summarize the input, where we vary $N$ from $100$ to $400$ at fixed intervals of $50$ tokens, by means of truncation/padding. If input has more than N token,  
the query text is truncated, and if the input contains less than N tokens, then padding tokens are added to the left of the input.
%
To eliminate the influence of generated output on inference energy, we stop generation after the first token, which allows us to monitor the influence of input length on model inference energy in an exclusive manner.

Figure~\ref{fig:vary-input} plots the inference energy (in terms of \%) relative to the energy required for the input with $100$ tokens. The results indicate a linear increase in energy with longer input lengths, with a steeper slope observed for decoder-only models. This is likely due to the longer decoder modules present in these models.




\begin{figure}[!t]
\centering
% \subfloat{\includegraphics[width=0.3\textwidth]{Plots_main/cnndm-input-avg.pdf}}
% \subfloat{\includegraphics[width=0.3\textwidth]{Plots_main/cnndm-output-avg.pdf}}
% % \subfloat{\includegraphics[width=0.3\textwidth]{Plots_main/cnndm-response-avg.pdf}}
\includegraphics[height=50mm,trim={4mm 4mm 3mm 4mm},clip]{Plots_main/vary_out/cnndm-energy-avg.pdf}
% \vspace*{-2mm}
\caption{Inference energy on \cnndm{} dataset when the output length is varied, keeping input length fixed. }
%We observe that the relative increment in inference energy slowly increases as the number of output token increases. 
%We observe the energy required for generating the first token is significantly higher than the energy required for generating each additional token. }
\label{fig:vary-output}
% \vspace*{-2mm}
\end{figure}






\noindent \textbf{\textit{Effect of varying output length}:}
Similarly, to study the effect of output length on inference energy exclusively, we take the \cnndm dataset, fix the input text length and allow the generation length to vary from $1$ to $25$ tokens. Specifically, we instruct the model to summarize the input text 
% into $250$ tokens \pk{Soham: check} and 
but force the model to stop after it generates the required number of tokens. 

Figure~\ref{fig:vary-output} plots the energy (in \%) relative to the energy required for generating a single token, confirming linear energy increment with generation length.
However, the energy of generating the $1^{st}$ token is much more than additional tokens, e.g. generating $2$ tokens takes only about 12\% more energy than generating $1$ token. This is because the model processes the entire input in the first time step, but only 1 token for subsequent steps (by means of caching the K-V computations for prior tokens).
Also, note that, the increment of energy is larger with increasing output length, compared to input length.  
%
Contrary to Figure~\ref{fig:vary-input}, the relative increase is higher for the encoder-decoder family here, attributed to the fact that initial energy requirement is smaller for these families, along with higher jumps in energy with output length. 
%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \iffalse
\subsection{Task complexity}
\label{sec:complexity}

Next, we explore whether task `complexity' has a significant impact on inference energy. Toward that, we conduct a series of controlled experiments %using three datasets: \vax{}, \caves{}, and \cnndm{}.
%In these experiments, 
where inference energy of two tasks with identical input and output length and distinctly different levels of complexity are compared. %hile keeping the input text identical and limiting the length of the generated output. 
%Although there is no precise way to quantify task complexity, 
Here, we interpret "complexity" based on human cognition.


We compare the inference energy between the \vax and \caves datasets, where the input texts are similar—tweets related to vaccines—but the tasks differ: a 3-class single-label classification for \vax{} versus a 12-class multi-label classification for \caves{}.
\newadd{
We ensure consistent input length via padding and fix the output to a single token.
We find 
the difference in energy consumption between the two tasks to be very small ($<1\%$), as shown in Table~\ref{tab:task_complexity}.
}


Similarly, we compare the average inference energy for the summarization (hard) vs returning the first three sentences of the input (trivial) over the \cnndm dataset. 
We find the energy difference between the two tasks as less than 1.3\% (Table~\ref{tab:task_complexity}), again indicating that task complexity has hardly any impact on inference energy if input and output lengths are kept fixed.  
This observation follows from the fact that the computational steps per token are fixed by the model's architecture, with  
%. If input and output lengths are constant, the steps remain the same, regardless of task complexity. 
LLMs processing the inputs uniformly, without additional branches or conditional logic that would increase the load for more complex tasks.

\newadd{
Although we explore task complexity as perceived by humans, results could be different in more complex Chain of Thought~\cite{wei2022chain,wang2022towards}, Tree of Thought~\cite{yao2023tree} and other reasoning frameworks. 
These frameworks have different ways of processing a task, which could impact the energy consumption based on the settings used. 
Comparing energy consumption for these different settings would need a study of its own, and is left as future work.
}


\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{|l|c|c|}
    \hline
               & \textbf{CAVES vs} & \textbf{Summarization vs} \\
               &              \textbf{VAX-Stance} & \textbf{first 3 sent extraction} \\
    \hline
    fT5-large  & $-0.2$\% &  1.3\% \\
    fT5-xl     &  0.5\% & $-0.6$\% \\
    Phi3-mini  &  0.4\% & $-0.1$\% \\
    Mistral-7B &  0.4\% & $-0.3$\% \\
    Llama3-8B  &  0.5\% &  0.8\% \\
    \hline
    \end{tabular}
    \caption{Percentage difference in energy consumption for two tasks with very different complexities, keeping input and output lengths same. The differences are very small.}
    \label{tab:task_complexity}
\end{table}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model family and size}
\label{sec:model-family}

We now compare the energy usage and normalized accuracy of different models with respect to their size (number of parameters). The model sizes and family have been listed in Section~\ref{sub:models}. 
%
Figure~\ref{fig:model_size} 
compares the size of models with per-sample inference energy, averaged across all samples, showing 
%
a linear increase with the size of the model 
(note that only Y-axis is in log scale in Fig~\ref{fig:model_size}), that are
individually visible for both the encoder-decoder and the decoder-only models.

%The complexity equation of LLMs suggest that a linear increase with the size of the model, and we see such trends individually for the encoder-decoder and the decoder-only models, as shown in Figure~\ref{fig:energy_vs_model_size}.

%Figure~\ref{fig:energy_vs_model_size} reports average inference energy per sample for different models where x-axis indicates corresponding model size. 

Encoder-decoder models typically consume less energy than decoder-only models with a comparable number of parameters. 
%
For instance, Flan-T5-xl and Phi-3-mini have a similar parameter count but use significantly less energy. 
%
The same pattern holds true for Flan-T5-large versus tinyLlama, and Flan-T5-xxl versus Llama-2-13B. 
%
This is because the decoder part in the encoder-decoder models is half the size, which reduces computational demands during the autoregressive decoding phase.

%
% requiring lesser computation during the autoregressive decoding phase. 
%
%Encoder-decoder and Decoder-only models will be compared in detail later in Section~\ref{sec:ed-vs-d}.
%


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth,trim={4mm 9mm 3mm 3mm},clip]{Plots_main/vary_model_size/average-energy-avg.pdf}
    % \vspace*{-5mm}
    \caption{Average per-prompt inference energy vs model size for all models and datasets. The black lines join the median energy for each model family.}% Inference time increases with model size. The encoder-decoder models require an order less energy even with similar model sizes. Among decoder models, Llama3 consumes slightly less energy as it's output is more optimized.}
    \label{fig:model_size}
    % \vspace*{-2mm}
\end{figure}

\input{tables/metrics}


\vspace{1mm}
\noindent \textbf{Accuracy metrics:} The top row of Table~\ref{tab:metrics} lists the Normalized accuracy (NA) scores for each model across all datasets (performance on each dataset given in Appendix~\ref{app:scores}). 
Here, we observe that performance depends on both model size and family. Smaller models perform poorly, with TinyLlama giving the worst performance, followed by Phi-3-mini and flan-t5-base. Llama models tend to perform inferior to flan-t5 models of similar size (flan-t5-large, -xl, and -XXL). Mistral-7B is the only exception among the decoder-only models that performs comparably with the flan-t5 family. 

As a general statement, it can be commented that selecting models from the encoder-decoder family for NLP tasks is recommended from an energy-efficiency perspective, as well as their performance which is improved by sequence to sequence instruction tuning. 
In contrast, decoder-only models trained on vast amounts of general data is more suited as an informational chatbot (though instruction tuned versions of Llama3, Mistral and Phi-3 try to bridge the gap).






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Batch size and Quantization}
\label{sec:batch}
We try to understand the effect of batch size on the energy usage during inference. 
%
Intuitively, increasing the batch size should require more energy per batch but we show that it
% lead to lower runtimes, requiring 
requires less energy per individual sample. 
 
%

We have used the \textit{bitsandbytes} package to load the transformers model weights in 8-bit and 4-bit quantized format. These quantized versions take up much less GPU memory to load (and thus can be run with larger batch sizes), though the computations still get executed in 16-bit single precision format. 
%
We run 4-bit quantized models on all the tasks under different batch sizes and plotted the average energy consumption per sample across all tasks in Figure~\ref{fig:vary-batch-quant4}. 
%
%We have plotted 
%Figure~\ref{fig:vary-batch} plots the average per-sample energy usage across all the tasks, showing that increasing batch size reduces per-sample inference energy.
We observe that increasing the batch size leads to a decrease in per-sample inference energy.
%
%as the samples per batch are increased, inference energy per sample decreases.
%

However, the maximum batch size possible is limited by size of the GPU VRAM (48GB for A6000).
For certain datasets and larger model combinations, higher batch sizes can result in out-of-memory errors, suggesting that there is an optimal batch size for each dataset and model size combination. To achieve energy-efficient inferences, it is advisable to perform inferences close to this optimal batch size.%

%
% Note that for the cases where larger batch sizes lead to out-of-memory errors for certain batches with larger texts, the energy comparisons may not be completely fair. However, we observe in such cases, less than 10\% batches are usually skipped. We have marked such cases with an asterisk.


\vspace{2mm}
\noindent \textbf{A5000 GPU:}
We repeated this experiment on an NVIDIA A5000 GPU instead of the A6000, reported in Appendix~\ref{app:bs_quant}; however, we did not find a significant difference in the inference time. 
% reports corresponding energy plots. 
This also signifies that GPUs with similar usable power require similar energy. Instead, the GPU VRAM size plays a more important role, allowing larger batches.

% \begin{figure}[!t]
% \centering
% {\includegraphics[height=50mm,trim={4mm 5mm 3mm 3mm},clip]{Plots_main/vary_batch/original.pdf}\label{fig:vary-batch-original}}
% \vspace*{-2mm}
% \caption{Per-sample inference energy averaged across all datasets when the batch size is varied. }%We see a linear decrease with batch size (both axes in log scale).}
% \label{fig:vary-batch}
% \vspace*{-2mm}
% \end{figure}





\begin{figure}[!t]
\centering
% \subfloat[4-bit Quantized (A6000 GPU)]
{\includegraphics[height=50mm,trim={4mm 5mm 3mm 3mm},clip]{Plots_main/vary_batch/quant4.pdf}\label{fig:vary-batch-quantized}}
% \vspace*{-2mm}
\caption{Per-sample inference energy with 4-bit quantized models when the batch size is varied, averaged across all datasets. 
}
\label{fig:vary-batch-quant4}
% \vspace*{-5mm}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Quantization}
% \label{sec:quant}

\vspace{2mm}
\noindent \textbf{Quantization Energy:}
% Finally, we verify the effect of quantization on energy usage while comparing the loss in performance metrics. 
%
% We have used the \textit{bitsandbyes} package to load the transformers model weights in 8-bit and 4-bit quantized format. These quantized versions take up much less GPU memory to load (and thus can be run with larger batch sizes), though the computations still get executed in 16-bit single precision format. 
%
% We also loaded the Flan-T5 models with their original 32-bit precision weights to understand if they improve performance during inference. However, the models seem to produce exactly same outputs, yet requiring almost twice the energy, and thus 32-bit precision should not be used in production during inference.
%
Figure~\ref{fig:vary-batch-quant4} shows the average change in energy for the 4-bit quantized model, while the energy required by original model is given in the Appendix~\ref{app:bs_quant}. Interestingly, keeping all factors same, quantization increases the energy used to almost $2\times$, because of the overhead of additional data format conversions to 16-bit. However, using the 4-bit quantized model with larger batch size of 256 reduces the energy to about $0.33\times$ of the original 16-bit model with batch size of 64.
%
We noticed very similar results with 8-bit quantization and thus, its energy plot is given in Appendix~\ref{app:bs_quant}.



\vspace{2mm}
\noindent \textbf{Quantization Accuracy metrics:} the change in performance of the quantized models compared to the original is given in the middle set of rows of Table~\ref{tab:metrics}. Quantization seems to reduce the performance by less than 5\% for some models (mostly decoder-only models in 8-bit quantization and most of the models for 4-bit quantization) and even increases performance slightly for some smaller models, which may have been overfitting earlier.
%
Thus, quantization does not seem to degrade performance too much and should be used to speed up inference time by increasing batch size.



