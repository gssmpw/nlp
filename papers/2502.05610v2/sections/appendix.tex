
\section{Additional Literature Survey}
\label{sec:appen-literature}

\textbf{Sustainable Large Language Models:}
% Green AI
Schwartz et al. ~\cite{schwartz2020green} discuss the growing compute cost of deep learning research and advocate for making efficiency an evaluation criterion alongside accuracy and related measures with a focus on making AI both greener and more inclusive. 
% 
Lacoste et al.~\cite{lacoste2019quantifying} consider various factors like energy grid, energy draw of server, make and model of training hardware to assess the environmental impact of machine learning algorithms. 
%
%\textbf{Energy-efficient LLM inference:}
Following that trend, recent literature focuses on various alternatives to reduce the inference energy of large language models. Among the black-box approaches, Li et al~\cite{li2024toward} append generation directives to user prompts for carbon-friendly LLM inferences. \cite{mcdonald2022great} focus on techniques to measure energy usage and propose various hardware and datacenter-oriented settings that can be tuned to reduce energy consumption for training and inference for language models. Frugal GPT~\cite{chen2023frugalgpt} explores strategies like prompt adaptation, LLM cascade, and LLM approximation for reducing the inference cost for a large set of queries. 
%
On the contrary, white-box approaches include 
%early exits
speculative decoding~\cite{leviathan2023fast}, speculative sampling~\cite{chen2023accelerating}, prunning~\cite{kurtic2024ziplm}, embedding recycling~\cite{saad2022embedding}, quantization~\cite{bai2022towards,frantar2022gptq,xiao2023smoothquant}, and many more. 


\textbf{Tools for measuring energy impact:}
%
Researchers propose various tools for tracking the realtime energy consumption and carbon
emissions during model training and inferences. These tools include CodeCarbon~\cite{codecarbon}, CarbonTracker~\cite{anthony2020carbontracker}, Experiment impact tracker~\cite{henderson2020towards}, EnergyScope~\cite{limpens2019energyscope}, etc. 
%
Green Algorithms~\cite{lannelongue2021green} is another online tool, enabling a user to estimate and report the carbon footprint of their computation. Eco2AI is another open-source package to help data scientists and researchers to track energy consumption and equivalent CO$_2$ emissions of their models in a straightforward way~\cite{budennyy2022eco2ai}. %
Carburacy~\cite{moro2023carburacy} proposes the first carbon-aware accuracy measure that captures both model effectiveness and eco-sustainability for generative transformer-based models~\cite{moro2023carburacy}. 
%
Researchers explore energy impact analysis in terms of carbon footprints of ML algorithms in various domains, namely differential privacy~\cite{naidu2021towards}, medical image analysis~\cite{selvan2022carbon}, etc. 
% In this work, we focus on the carbon footprint of developing DL models for medical image analysis\cite{selvan2022carbon}
% This paper investigates the impact of differential privacy on learning algorithms in terms of their carbon footprint due to either longer run-times or failed experiments.
%%

\textbf{Benchmarking energy tools:} Researchers benchmark the tools for measuring carbon footprints in various configurations for various deep learning based ML models. 
Cao et al~\cite{cao2020towards} compare energy returned by software-based energy measurements with hardware power meter (WhattsUPMeter) on various NLP models and report experiment impact tracker as not so accurate. 
%
Jay et al~\cite{jay2023experimental} qualitatively and experimentally compare several software-based power meters
against high-precision physical power meters while executing various intensive workloads, where they conclude that for measuring energy, Carbon Tracker, Code Carbon, Energy Scope, and Experiment Impact Tracker are suitable fits. However, Bouza et al~\cite{bouza2023estimate} establish that the energy value reported by CodeCarbon is closest to Wattmeter, followed by CarbonTracker, with more variability between infrastructures. 

\iffalse 
\textbf{Benchmarking LLMs:}
Recently, researchers attempt to benchmark the inference energy of a broad range of LLMs in a diverse range of tasks and configurations.   
% “Evaluating the Carbon Impact of Large Language Models at the Inference Stage (IPCCC 2023)”  
\cite{everman2023evaluating} conducts a thorough study on the carbon impact of various open-source LLMs, including GPT-J 6B, GPT Neo 2.7B, GPT-NEO 1.3B, and GPT-2 at the inference stage, utilizing the Software Carbon Intensity (SCI) specification released by the Green Software Foundation, concluding high-carbon LLMs do not necessarily provide
superior model quality than their low-carbon counterparts.
%
Samsi et al~\cite{samsi2023words} benchmark the inference performance and inference energy costs of different sizes of LLaMA on two generations of popular GPUs (NVIDIA V100 and A100) and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. 
%
Liu et al.~\cite{liu2022few} propose that few-shot parameter-efficient fine-tuning is less energy-intensive without affecting the inference performance. 
% "Power Hungry Processing: Watts Driving the Cost of AI Deployment?“ (arXiv 2023) 
Desislavov et al~\cite{desislavov2021compute} study the correlation between model complexity and inference energy (measured using GFLOPs) for various NLP and Computer Vision models.  
Luccioni et al.~\cite{luccioni2024power} propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ‘general-purpose’ models. Table~\ref{tab:existing-literature} presents a comprehensive overview of existing approaches and their limitations. 
\fi

\textbf{Benchmarking LLMs:}  Existing works have attempted to study the inference energy of LLMs from various perspectives. \citet{everman2023evaluating} evaluated energy usage of GPT models on question-answering tasks, employing Software Carbon Intensity (SCI) released by green software. \citet{samsi2023words} provides a detailed account of energy usage during inference for question-answering tasks for Llama models on various GPU architectures. \citet{liu2022few} study the energy consumption trade-off for fine-tuning vs few-shot learning for both encoder-decoder and decoder-only models on SuperGLUE and RAFT tasks, measuring energy in FLOPS. These preliminary works mostly ignore the performance-energy tradeoff. 

In recent times, \citet{li2024toward} compare the effect of prompt directives on inference energy and performance for Llama-2 models for 3 question-answering tasks. \citet{luccioni2024power} offers a comparatively detailed account of inference energy usage while choosing LLMs and tasks from a diverse range, showing the dependency of energy with model size and architecture, task type, and output token length. While these works provide an initial account of inference energy usage of LLMs in different configurations, they are often limited by the number and diversity of the models and tasks. Our work is the first to provide a comprehensive benchmarking of energy consumption of LLMs during inference for a diverse range of configurations, i.e., variation of input and output token length (for both coarse and granular level), correlation with response time, model size and family, task complexity, batch size, quantization level, presence of targeted phrases, and energy directives in prompt for a more complete set of LLM and tasks.
%
%In particular, our contributions include an energy analysis of eight aspects, as shown in the table. Out of these aspects, several were primarily unexplored in the literature, as per our knowledge - these are indicated in the table. We vary the input and output token length at a finer granular level while reporting the variation in energy to have a better understanding of the correlation between energy and these factors, revealing a number of valuable insights (Section 4.2). A more thorough analysis of energy with variation of input or output while keeping the other fixed provides a more accurate insight into their comparative influence in inference energy, implying immediate solutions for energy-efficient inference approaches. The correlation between energy and response time validates response time as a good proxy of inference energy in black-box models (Sec 4.1). Our work is first to explore the effect of task complexity with inference energy (Sec 4.3). We also provide the performance change for quantization (Sec 4.6) and use of targeted phrases (Sec 4.7) along with improvement in energy. While the effects of quantization and increasing batch size have been studied individually, we show that using them together under fixed memory constraints can help speed up the computations. Finally, we explore each of the above aspects for a diverse range of LLMs from both architecture families along with a diverse range of tasks, which was mostly lacking in previous attempts. 
Table~\ref{tab:existing-literature} presents an overview of existing approaches and their limitations.
\input{tables/existing_lit}


\section{Model Descriptions}
\label{app:models}
Check Table~\ref{tab:models-app}
\input{tables/models_app}



\section{Dataset Examples and Metrics}
\label{app:tasks}
Check Table~\ref{tab:tasks-app}

\input{tables/tasks_app}





\section{Scatter Plots of batch-wise energy tracking}
\label{app:scatter}

Check Figure~\ref{fig:scatter-app}

\begin{figure*}[!ht]
    \centering
    
    \includegraphics[height=4.2mm,trim={86mm 141mm 40mm 2mm},clip]{Plots_main/scatter_model/flan-t5-base-energy_cc_vs_energy_ct.pdf}
    \vspace*{-4mm}
    
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/flan-t5-base-energy_vs_response_time.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/flan-t5-base-energy_vs_input_len.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/flan-t5-base-energy_vs_output_len.pdf}}

    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/flan-t5-large-energy_vs_response_time.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/flan-t5-large-energy_vs_input_len.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/flan-t5-large-energy_vs_output_len.pdf}}

    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/flan-t5-xl-energy_vs_response_time.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/flan-t5-xl-energy_vs_input_len.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/flan-t5-xl-energy_vs_output_len.pdf}}


    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/TinyLlama-1.1B-Chat-v1.0-energy_vs_response_time.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/TinyLlama-1.1B-Chat-v1.0-energy_vs_input_len.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/TinyLlama-1.1B-Chat-v1.0-energy_vs_output_len.pdf}}


    % {\includegraphics[width=0.32\linewidth]{Plots_main/scatter_model/Mistral-7B-Instruct-v0.2-energy_vs_response_time.pdf}}
    % {\includegraphics[width=0.32\linewidth]{Plots_main/scatter_model/Mistral-7B-Instruct-v0.2-energy_vs_input_len.pdf}}
    % {\includegraphics[width=0.32\linewidth]{Plots_main/scatter_model/Mistral-7B-Instruct-v0.2-energy_vs_output_len.pdf}


    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/Llama-2-7b-chat-hf-energy_vs_response_time.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/Llama-2-7b-chat-hf-energy_vs_input_len.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/Llama-2-7b-chat-hf-energy_vs_output_len.pdf}}


    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/Llama-2-13b-chat-hf-energy_vs_response_time.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/Llama-2-13b-chat-hf-energy_vs_input_len.pdf}}
    {\includegraphics[width=0.3\linewidth]{Plots_main/scatter_model/Llama-2-13b-chat-hf-energy_vs_output_len.pdf}}

    
    
    \caption{Average per-sample inference energy vs average per-sample response time, input and output-token length across all datasets for different models where points in the image correspond to individual batches of different datasets.}
    \label{fig:scatter-app}
\end{figure*}


\section{Original Accuracy Metrics of individual datasets}
\label{app:scores}
Check Table~\ref{tab:metrics-app}

\input{tables/metrics_app}


\section{Batch Size \& Quantization experiments}
\label{app:bs_quant}
Check Figure~\ref{fig:bs-quant-app}



\section{Testing on other systems}
\label{app:mgpu}

To verify the generalizability of our findings, we additionally ran limited experiments on two different systems. 

\noindent \textbf{System Descriptions:}
The first system contains an NVIDIA Tesla P100 GPU with 16GB VRAM paired with Intel Xeon Gold 6126 CPU and 128GB RAM. For the second system we used Kaggle~\footnote{www.kaggle.com/code} which contains an NVIDIA Tesla T4 GPU with 16GB VRAM, Intel Xeon CPU and 30GB RAM.


\begin{figure*}
    \centering
    \includegraphics[width=0.45\linewidth]{Plots_main/multi-gpu.pdf}
    \caption{Energy consumption of running Mistral-7B and flan-T5-large on different systems (identified by their GPUs), averaged across 4 datasets. A6000 refers to the original setup.}
    \label{fig:mgpu}
\end{figure*}


\noindent \textbf{Experiments:}
Figure~\ref{fig:mgpu} shows the energy consumptions of 2 representative models from each family -- Mistral-7B and flan-t5-large on the different systems. The results are averaged over 4 representative datasets -- \boolq , \mnli, \cnndm and \squad.
We observe very similar trends for each model across different systems, even though the magnitude is different. We found that the system with P100 required less energy than our original setup, though with a limited GPU VRAM, which does not allow very high batch sizes. Interestingly, the system with T4 GPU consumes a lot more power even though it is a weaker system.  

We further hypothesize that modern improvements like flash-attention-2 will benefit newer GPU architectures and newer models like Phi-3 and Llama-3 more. However, the overall trends should still be similar.


% \noindent \textbf{Input and Output size experiments:}




% \subsection{Implications in energy-constrained environments}
% \label{sec:implications}
% In situations where LLMs are to be deployed in resource/energy constrained settings, our experiments lead to the following insights: 
% (i) For tracking energy consumption during inference using different models on the same hardware, we demonstrated that the response time can be used as a proxy for energy consumed. This is useful not only since measuring power/energy may be difficult on most hardware, but also measuring response time requires much less overhead (in terms of energy/latency) than to actually measure the energy consumption. In low resource settings, this overhead can be large in terms of percentage. For example, in our experiments we found that removing the energy-tracking libraries and just recording the system time can reduce inference time between 15\%--50\% under different scenarios.
% (ii) Where possible, input compression and output optimization should be employed. We observed that the output length is more important in reducing energy, hence at least, some targeted phrases should be incorporated so that the model generates only what is needed. In particular, we demonstrated that targeted phrases like “Do not output anything else” helps in reducing energy consumption of models, especially the Llama family of models that are prone to generating sizable amounts of extra information.
% (iii) Fine-tuned encoder-decoder models, especially models from the Flan-T5 family, are better suited for low-resource settings, compared to larger decoder-only models, particularly for NLP tasks such as classification, summarization, etc. These models perform close to decoder-only models with much less energy consumption, helping in low resource settings.
% (iv) Quantized models should be employed in resource-constrained settings, allowing larger batch size to speed up computations, resulting in lower energy consumption. Furthermore, quantization is necessary to run larger models on such settings, from the point of memory constraint.  


% \subsection{Response time as an energy proxy for black-box models}
% \label{sec:energy-proxy}
% In this work, though not generalizable to every scenario, we demonstrated that the inference time can be used as a proxy for energy consumption while comparing different input/output strategies for the same model or comparing different models on the same hardware (check Figure 1). However, in the case of models accessed through online APIs (such as closed-source models), it is difficult to estimate the energy from just the response time due to a variety of factors, including network latency, number of concurrent user requests, type of batch scheduling/processing, etc. It can be noted that, to date, the API providers do not provide any energy-related metrics; hence, there exists no better way yet to estimate the energy for such black-box models than using the response time. A workaround for this could be to run experiments multiple times at different times of the day or through multiple days and then to consider the average time taken across all experiments, which will hopefully even out the effects of the varying factors such as network latency.


\begin{figure*}[!ht]
    \centering
    
\subfloat[Per-sample inference energy averaged across all datasets when the batch size is varied. 
Overhead of using 4-bit precision can increase energy to almost $2\times$ for the same batch size. 
However, a 4-bit model in BS=256 takes only about $0.33\times$ the energy as default model in BS=64]{\includegraphics[height=50mm,trim={4mm 5mm 3mm 3mm},clip]{Plots_main/vary_batch/original.pdf}\label{fig:vary-batch-original}}
    \hfill

    
    \subfloat[Per-sample inference energy averaged across all datasets when the batch size is varied, on the A5000 GPU instead of A6000.]{\includegraphics[width=0.45\linewidth]{Plots_main/vary_batch/a5000.pdf}}
    \hfill
    \subfloat[Per-sample inference energy averaged across all datasets with 8-bit quantized models.]{\includegraphics[width=0.45\linewidth]{Plots_main/vary_batch/quant8.pdf}}    
    \caption{Additional Batch size experiments on the A5000 GPU, and using 8-bit quantization.}
    \label{fig:bs-quant-app}
\end{figure*}

\section{Energy proxy for black-box models}
\label{app:blackbox}
{
Our work demonstrates that for locally run open-source LLMs, inference time is a reliable proxy for estimating energy consumption, with significantly less overhead compared to using specialized energy measurement tools.
However, in the case of models accessed through online APIs (such as closed-source models), it is difficult to estimate the energy from just the response time due to a variety of factors, including network latency, number of concurrent user requests, type of batch scheduling/processing, etc. 
However, to date, the API providers do not provide any energy-related metrics; hence, there exists no way yet to estimate the energy for such black-box models. For the purpose of improving sustainability research, API providers should provide some energy metrics with the outputs.
In the absence of such estimates, inference time can be used to compare energy use of different black-box strategies. A workaround for variations in network latency could be to run experiments multiple times and averaging the time taken across all experiments. This will hopefully even out the effects of the varying factors such as network latency, when comparing different settings under the same hardware.
Though this would not completely eliminate the effect of network latency, as it is not a mean-zero random variable, and is influenced by a variety of complex, hardware-related factors, this is the most straightforward workaround for now.
In future work, it would be beneficial to develop a model that accounts for network latency variations to improve energy consumption estimation.
}
