
\subsection{Effect of targeted phrases in prompts on inference energy}
\label{sec:phrases}






Finally, we attempt to find whether addition of phrases targeted towards energy optimization can affect the inference energy. Specifically, we wanted to see if adding a few more input tokens can lead to a larger decrease in energy by reducing the output token length.
%
In this experiment, for each dataset, we append certain targeted phrases after the default prompt, as shown in Table~\ref{tab:prompts}.
%
% Examples of such phrases include "Answer quickly", "Answer in an energy-efficient way", "Do not output anything else", etc. We list the complete list of phrases added in Table~\ref{tab:prompts}.  
%

Figure~\ref{fig:vary-prompt-averages} reports the \% inference energy usage using modified prompts compared to default prompts, averaged across the two generative and rest discriminative tasks. %on datasets and others. 
%
Significant energy reduction is observed for Mistral and Llama-2 models. %and Mistral models . 
The reduction is less pronounced in generative tasks, where it mainly results from slightly shorter outputs. However, for discriminative tasks, the reductions are much more significant. This difference arises because these models typically include explanations with their answers, leading to longer outputs by default. By instructing the model to be concise %and avoid generating additional content, 
we can limit the output length and, thus, reduce inference energy.
However, the change in energy is negligible for the encoder-decoder models, Llama-3 and Phi-3-mini, as they typically generate short, brief answers, leaving little scope for reducing the output. Thereby, additional phrases in the prompt increase the input without reducing the generation, resulting in higher inference energy. 
% Figure~\ref{fig:vary-prompt-output} supports the argument by reporting their respective output length. 
TinyLlama always generates long outputs, often stopping only at the generation limit, rendering the targeted phrases useless.

%
% We also notice that implicit prompts like "Answer in energy-efficient manner" are not very effective for any model, while rest prompts are shown to be effective in reducing inference energy for Llama-family. Therefore selecting appropriate phrases is important. 



\input{tables/prompts}



\begin{figure}[!t]
\centering
% \subfloat[Relative increment in the inference energy]
{\includegraphics[width=\linewidth,trim={4mm 5mm 3mm 3mm},clip]{Plots_main/vary_prompt/average-energy-avg.pdf}\label{fig:vary-prompt-energy}}
% \hfill
% \subfloat[Relative increment in the generation length]
% {\includegraphics[width=0.45\textwidth]{Plots_main/vary_prompt/average-output-avg.pdf}\label{fig:vary-prompt-output}}
% \vspace*{-5mm}
\caption{Effect of inserting targeted phrases in prompt on inference energy, as a percentage of default prompt. 'ee': energy-efficient, 'q': quick (see Table~\ref{tab:prompts}). }%Respective prompts are given in Table~\ref{tab:prompts}. 
% Average results across the two generation datasets are given at the bottom, and across other datasets are on the top. 
%Energy reduction is significantly higher for Llama models in comparison to flan-t5 models. Further, such targeted phrases work bets with discriminative tasks.}
% \caption{Effect of inserting specific energy-efficiency targeted phrases in the prompt on both inference energy (left) and generation length (right). Corresponding prompts are given in Table~\ref{tab:prompts}. Average results across all datasets are reported. Both results present the change of respective metrics with respect to the default prompt.  
% \todo{add result}
% }
\label{fig:vary-prompt-averages}
% \vspace*{-2mm}
\end{figure}




\vspace{1mm}
\noindent \textbf{Accuracy metrics:} 
The performance metrics for modified prompts are given in bottom rows of Table~\ref{tab:metrics}, we observe that the introduction of such phrases in prompts results in diverse behavior depending on model size and architecture. 
% Occurrences of such phrases improve performance for larger models in both families (up to ~2\% in flan-t5-xl/xxl, ~15\% in Mistral 7B, ~10\% in Llama-13B). 
% For the rest of the models, performance degrades less than 7\% for the flan-t5 family. However, the Llama models of moderate size  (Llama-2-7B and Llama-3-8B) seem to perform poorly in adding such phrases. 
Performance degrades in most of the cases, especially where output token length had also reduced, taking lesser energy.
In summary, we can see that the introduction of such phrases turns out to be useful only for Mistral-7B and Llama-2-13B, considering energy efficiency without affecting performance.   








% % \vspace{2mm}



% \begin{figure*}[!t]
% \centering
% \subfloat[boolq]{\includegraphics[width=0.3\textwidth]{Plots_main/vary_prompt/boolq-bs-8-avg-output-len-avg.pdf}}
% \subfloat[record]{\includegraphics[width=0.3\textwidth]{Plots_main/vary_prompt/record-bs-8-avg-output-len-avg.pdf}}
% \subfloat[squad]{\includegraphics[width=0.3\textwidth]{Plots_main/vary_prompt/boolq-bs-8-avg-output-len-avg.pdf}}
% \caption{Output length for 3 datasets across different LLMs we compare the effect of different generation directives augmented in the prompt for energy-efficient inference. }
% \label{fig:vary-prompt-output}
% \end{figure*}



% \begin{figure*}[!t]
% \centering
% \subfloat[boolq]{\includegraphics[width=0.3\textwidth]{Plots_main/vary_prompt/boolq-bs-8-energy-CC(Wh)-avg.pdf}}
% \subfloat[record]{\includegraphics[width=0.3\textwidth]{Plots_main/vary_prompt/record-bs-8-energy-CC(Wh)-avg.pdf}}
% \subfloat[squad]{\includegraphics[width=0.3\textwidth]{Plots_main/vary_prompt/squad-bs-8-energy-CC(Wh)-avg.pdf}}

% \caption{Energy for 3 datasets where the four-generation directive are augmented in the prompt \todo{describe}}
% \label{fig:vary-prompt-energy}
% \end{figure*}
