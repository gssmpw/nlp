
\begin{table*}[!t]
    \begin{center}
    \footnotesize
    
    % \begin{adjustbox}{max width=.95\textwidth}
        \begin{tabular}{|c||c|c|c|c||c|c|c|c|c|c|}
        % \toprule 
        \hline
        \ & flan-t5 & flan-t5 & flan-t5 & flan-t5 & TinyLlama & Phi-3 & Mistral & Llama-2 &  Llama-3 & Llama-2 \\
                                & base    & large    & xl      & xxl    & 1.1B      & mini  & 7B      & 7B      & 8B & 13B\\
        \hline \hline
        \multicolumn{11}{|c|}{\it \textbf{I.} Average \textbf{Normalized Accuracy} across all datasets with original settings} \\
        \hline
        % default & 40.4 & 86.4 & 63.2 & 68.0 & 8.8 & 38.0 & 67.5 & 42.2 & 59.8 & 56.4 \\
        default & 42.85 & 69.77 & 55.45 & 58.12 & 23.5 & 41.82 & 59.91 & 42.83 & 55.09 & 52.31\\ 
        \hline \hline

        \multicolumn{11}{|c|}{\it \textbf{II.} Average \textbf{change in performance (\%)} on Quantization} \\
        \hline
        8-bit  & 0.47 & 0.03 & -1.16 & 1.43 & 0.9 & -2.92 & -0.55 & -0.46 & -2.43 & -4.66 \\  
        % \hline \hline
        % \multicolumn{11}{|c|}{\it 4-bit Quantized} \\
        % \hline
        4-bit  & 1.78 & -1.83 & -0.63 & -0.61 & 9.66 & -4.19 & 4.27 & -1.57 & -1.95 & -2.76\\ 
        \hline \hline
        
        \multicolumn{11}{|c|}{\it \textbf{III.} Average \textbf{change in performance (\%)} on introducing targeted phrases in prompts} \\
        \hline
        fix-output & -1.7 & -2.42 & -1.22 & 0.39 & 4.71 & -8.21 & 11.64 & -12.54 & -8.53 & 3.24\\
        \hline
        
        energy-eff & -1.28 & -4.2 & -0.89 & 0.84 & 1.44 & 9.55 & 2.52 & 0.33 & -5.19 & 5.2\\
        
         + fix-output & -1.33 & -2.68 & -1.78 & 0.74 & 4.07 & 4.11 & 12.4 & -15.73 & -10.07 & 5.38\\
        \hline
        quick & 1.29 & -3.88 & -0.41 & -1.71 & 2.63 & 8.14 & 5.82 & -4.24 & -14.88 & 3.69\\
        
        + fix-output & -0.72 & -5.48 & -0.27 & 0.2 & 4.64 & -2.55 & 12.45 & -13.23 & -18.88 & 2.64\\
        \hline
        \end{tabular}
        % \end{adjustbox}
        % \vspace*{-2mm}
        \caption{Accuracy metrics for LLM inferences averaged across all datasets. 
        \textbf{I.}~Encoder-Decoder models perform better or close to Decoder only models.
        \textbf{II.}~Quantization does not decrease performance by much ($<5\%$)
        \textbf{III.}~Performance degrades with most phrases, more so where energy / output token length had also reduced. 
        % \textbf{I.}~we report the normalized accuracy averaged across all datasets for all models. \textbf{II.}~we report the average change in performance for 4-bit and 8-bit quantization with comparison to 16-bit precision models. \textbf{III.}~we report the average change in performance for introducing targeted phrases in prompts. 
        % \todo{update}
        }% and the average accuracy of the type predictions.}
        \label{tab:metrics}
    \end{center}
    % \vspace*{-5mm}
\end{table*}


% \begin{table*}
%     \begin{center}
%     % \begin{adjustbox}{max width=.95\textwidth}
%         \begin{tabular}{||c||c|c|c|c|c||}
%         \toprule 
%         name&dataset&response time(s)&avg input len&avg output len&energy CT(Wh)\\
%         Mistral-7B-Instruct-v0.2&boolq&7.1405&179.8621&214.7693&0.9468\\
%         flan-t5-large&boolq&0.4313&172.5852&2.6953&0.0159\\
%         flan-t5-xl&boolq&0.7439&172.5852&2.73&0.0336\\
%         Llama-2-13b-chat-hf&cola&1.0182&37.4893&42.9402&0.0536\\
%         Llama-2-7b-chat-hf&cola&0.7287&37.4893&45.3865&0.0269\\
%         Mistral-7B-Instruct-v0.2&copa&21.1872&74.4303&162.1923&3.1651\\
%         TinyLlama-1.1B-Chat-v1.0&copa&36.8905&74.9375&702.5312&4.6258\\
        % flan-t5-large&copa&0.2995384615384615&67.4375&2.3028846153846154&0.009076923076923
% flan-t5-xl&copa&0.4377692307692308&67.4375&2.84375&0.0178461538461538
% flan-t5-xxl&copa&0.6970769230769229&67.4375&2.0288461538461537&0.0281538461538461
% Llama-2-13b-chat-hf&mnli&1.664171875&87.441162109375&97.37939453125&0.1190078125
% Llama-2-7b-chat-hf&mnli&1.0678828124999995&87.441162109375&97.30615234375&0.0611875
% Mistral-7B-Instruct-v0.2&mnli&1.0458906249999995&85.934326171875&95.929931640625&0.0613125
% TinyLlama-1.1B-Chat-v1.0&mnli&0.4965390625000001&87.441162109375&97.441162109375&0.0186406249999999
% Llama-2-13b-chat-hf&mrpc&1.890322916666667&98.09407552083331&103.7109375&0.1441145833333333
% Llama-2-7b-chat-hf&mrpc&1.2426875000000008&98.09407552083331&108.09244791666669&0.0878749999999999
% Mistral-7B-Instruct-v0.2&mrpc&1.1365104166666666&94.41243489583331&104.41178385416669&0.0759270833333333
% TinyLlama-1.1B-Chat-v1.0&mrpc&0.5491666666666671&98.09407552083331&108.091796875&0.0194479166666666
% Llama-2-13b-chat-hf&qnli&1.7742343749999991&92.155517578125&96.6103515625&0.140515625
% Llama-2-7b-chat-hf&qnli&1.1589062500000005&92.155517578125&101.92578125&0.0748046875
% Llama-2-7b-chat-hf&sst2&5.297507812500003&42.649169921875&73.74267578125&0.663828125
% Mistral-7B-Instruct-v0.2&sst2&31.043265625&43.012939453125&114.20703125&4.729046875000001
% TinyLlama-1.1B-Chat-v1.0&sst2&36.799234375000005&42.649169921875&880.2041015625&4.783656249999999
% flan-t5-large&sst2&0.219015625&36.505859375&2.264404296875&0.0072421875
% flan-t5-xl&sst2&0.260953125&36.505859375&2.342041015625&0.0089453125
% flan-t5-xxl&sst2&0.5177734375000002&36.505859375&2.2958984375&0.0199999999999999
% Llama-2-13b-chat-hf&stsb&1.4819609374999998&70.14501953125&80.061767578125&0.1049999999999999
% Llama-2-7b-chat-hf&stsb&0.959015625&70.14501953125&80.14501953125&0.0451640624999999
% Llama-2-13b-chat-hf&wnli&1.52875&81.66030092592592&91.38686342592592&0.10915
% Llama-2-7b-chat-hf&wnli&0.9489&81.66030092592592&91.66030092592592&0.04635
%         \bottomrule
%         \end{tabular}
%         % \end{adjustbox}
%         \vspace{1mm}
%         \caption{Ablation study. }% and average accuracy of the type predictions.}
%         \label{tab:ablation}
%     \end{center}
%     \vspace{7mm}
% \end{table*}
