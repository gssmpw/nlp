\begin{table*}[ht]
    \centering
    \footnotesize
    \begin{tabular}{p{27mm}|p{28mm}|p{24mm}|p{64mm}}
    \toprule
    \textbf{Reference } & \textbf{LLM} & \textbf{Tasks/Datasets} & \textbf{Observations}\\
    \midrule
    \cite{everman2023evaluating}  & GPT-styled models (4) & 10 manual prompts &  Study on the energy-performance tradeoff of LLMs.\\
    \hline
    %
    \cite{samsi2023words} &  LlaMA models (3) & QA tasks (2) & Study inference cost on diverse GPUs.\\% (NVIDIA V100 and A100) \\
    \hline
    \cite{desislavov2021compute} & DNN-based NLP models (7) & GLUE (9) & Study model complexity vs inference cost.\\%and inference energy (measured using GFLOPs)\\  
    \hline
    \cite{liu2022few} & T5 models (3), GPT-styled models (3) & NLP tasks (9), RAFT & Study energy consumption of few-shot PEFT vs in-context learning.\\% parameter-efficient fine-tuning is less energy-intensive without affecting the inference performance. \\
    %sentence completion (3COPA, H-SWAG, and Story Cloze), natural language inference (ANLI, CB, and RTE), coreference resolution (WSC and Winogrande), and word sense disambiguation, RAFT 
    \hline
    \cite{li2024toward} & Llama models (2) & QA tasks (3) & Study effect of prompt directives on inference cost.\\%energy and performance.\\
    \hline 
    \cite{luccioni2024power} & Flan-T5 models (4), BLOOMz models (4) & NLP + vision tasks (10) & Study inference energy vs model complexity, task type, output, etc.\\%Propose the first systematic comparison of the inference cost with model size/complexity, output, task \\
    \hline 
    Our work & GPT styled models (6), Flan-T5 models (4) & NLP tasks (11) & Study inference cost vs input, output, response time, model size \& family, task complexity, quantization, batch size, targeted phrases\\
    \bottomrule
    \end{tabular}
    \vspace*{-2mm}
    \caption{Comparison of our approach with existing literature on benchmarking inference cost of LLMs}
    \label{tab:existing-literature}
    \vspace*{-2mm}
\end{table*}

%

