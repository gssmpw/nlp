\section{Related Work}
\label{sec:related work}


\subsection{Visual Grounding}
Visual grounding (VG)~\cite{yu2018mattnet,yang2019fast,deng2021transvg,xiao2023clip,liu2024dara,xiao2024hivg,lu2024lgr} aims to identify and localize regions within images that correspond to given text descriptions. 
There are many extensions of VG in other fields, such as Remote Sensing VG \cite{li2024language,li2024show,ding2024visual}.
Early visual grounding methods, given their resemblance to detection tasks, initially aligned with the prevailing object detection architectures. These architectures evolve from the initial two-stage designs to the recently one-stage methods. Two-stage designs methods~\cite{liu2019learning,hong2019learning,chen2020referring} follow a two-stage pipeline that first utilizes pre-trained object detectors to obtain a set of region proposals, which are then ranked based on their similarity scores with the given textual description. However, these two-stage methods face challenges in terms of the performance of the proposal generators and the additional ranking mechanisms. With the introduction of ViT, the Transformer-based methods~\cite{deng2021transvg,du2022vgtr,yang2022vltvg,zhu2022seqtr,su2023referring,vg-law,liu2024dara,zhu2023jmri} further propose an end-to-end framework which reformulate the prediction process as a regression problem. Most recently, grounding multimodal large language models~\cite{bai2023qwen,wang2024visionllm,wang2023cogvlm} have propelled the state-of-the-art (SOTA) performance, these works require a large amounts of in-domain and other domain datasets. Despite the transformer-based models exhibiting ideal performance in VG, most methods involve fully fine-tuning the text and visual branches separately, followed by a heavyweight vision-language encoder for simple multimodal fusion. This not only makes it difficult to focus on the areas most relevant to the text description but is also inefficient.



\subsection{Parameter-Efficient Transfer Learning}

Transfer learning aims to adapt pre-trained models to specific tasks or datasets. With the growth of model sizes and the complexity of the specific tasks, fully fine-tuning paradigm demands significant computational resources. To address these challenges, researchers in the NLP and CV domains have explored PETL methods ~\cite{lester2021power,hu2021lora,chen2022adaptformer,yuan2023mrsadapter,zhou2024dynamic}. One method, known as Prompt Tuning \cite{wang2023aprompt,khattak2023maple,liu2024dap}, involves the introduction of trainable tokens at the input space, thereby learning task-specific representations. Adapter-like methods \cite{yuan2023mrsadapter,liu2024sparse} involve inserting additional trainable weights, such as Multi-Layer Perceptrons (MLPs) equipped with activation functions and residual connections, within the network architecture to enhance transfer learning capabilities. Meanwhile, LoRA-like methods \cite{hu2021lora} adjust pre-trained models by using the idea of low-rank matrix decomposition, and only trains the parameters of the low-rank matrix. LoRA-like methods have been proposed in the field of natural language processing for Large Language Models (LLM) such as GPT-4 \cite{achiam2023gpt}, LLaMA2 \cite{touvron2023llama}, and GLM-4 \cite{glm2024chatglm}. By focusing on updating only a small subset of parameters, PETL methods effectively simulate the fine-tuning of the entire modelâ€™s parameters without directly modifying them. Recently, some pioneering works like MaPPER~\cite{liu2024mapper}, HiVG\cite{xiao2024hivg}, DARA~\cite{liu2024dara} and M$^2$IST \cite{liu2024m} sought to utilize adapters to adapt pre-trained models to visual grounding. However, they all use a burdensome vision-language module for multimodal fusion, which is not an efficient enough method. 




\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\textwidth]{figures/main.pdf}
% \vspace{-6mm}
\caption{Overall architecture of the proposed SwinVG, which freezes the pre-trained vision encoder and language encoder. SwimVG integrates step-wise multimodal prompts (Swip) and cross-modal interactive adapters, which bridges the visual and language encoders, ensuring the visual encoder concentrates on the text-relevant areas.}
% \vspace{-4mm}
\label{fig:overview}
\end{figure*}





