\section{Conclusion and Future Work}
\subsection{Conclusion}
In this paper, we aims at improving both the effectiveness and efficiency of visual-text alignment. We propose SwimVG by the foundational design of step-wise multimodal prompts (Swip) and cross-modal interactive adapters (CIA). SwimVG integrates a novel multimodal fusion strategy of token-level Swip and weight-level CIA to enable the visual encoder can concentrate on the text-relevant regions. Extensive experiments and ablation studies have validated the high effectiveness of our method. Our proposed framework significantly outperforms the baseline and achieves comparable results with the state-of-the-art methods while tiny parameter budget.

\subsection{Future Work}



In the future, implementing our SwimVG in real-world applications is a challenging and meaningful direction. Currently, our SwimVG has only been evaluated on benchmark datasets. However, its performance against datasets from different domains remains unknown.
In addition, the efficient multi-modal fusion strategies of SwimVG can be verified on other multimodal tasks, such as visual question answering and video caption. Motivated by efficient Multimodal Large Language Model \cite{liu2024multi}, we will explore efficient training and inference model for visual grounding.
