\section{Introduction}

Visual grounding (VG) \cite{kamath2021mdetr,vg-law,qiao2020referring,xiao2024towards} refers to locating the bounding box region described by a textual expression in a specific image, which is one of the most challenging tasks in multimodal fields. In contrast to vanilla detection tasks, VG requires fine-grained vision-language alignment so as to precisely locate an object described through a language expression. The evolution of VG has considerable potential to promote vision-language understanding, and enjoys broad applications in fields such as robot navigation\cite{das2018embodied}, visual Q\&A \cite{antol2015vqa} and automatic driving\cite{zhang2022ri,motroni2020sensor}.


\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/intro.pdf}
% \vspace{-6mm}
\caption{Comparison of multimodal fusion strategy between (a) mainstream {framework} and (b) SwimVG (ours) for visual grounding. Freezing the pre-trained models (\protect\includegraphics[height=0.5cm]{figures/frozen.png}) and only updating (\protect\includegraphics[height=0.5cm]{figures/tunable.png}) the tiny modules in SwimVG reduces 97.96\% updated parameters while achieving even stronger performance.}
% \vspace{-4mm}
\label{fig:intro}
\end{figure}


Early visual grounding methods followed target detection frameworks, evolving from the initial two-stage approaches to the recent one-stage methods. Benefiting from the open source of transformer-based pre-trained models,  
a growing number of approaches \cite{deng2021transvg,kamath2021mdetr,transvg++,shi2023dynamic} transfer the language and vision knowledge from pre-trained models by fully fine-tuning, such as TransVG \cite{deng2021transvg}, TransVG++ \cite{transvg++}, and VG-LAW \cite{vg-law}. These methods commonly adopt visual and textual encoders to extract {features}, respectively, which are subsequently input into a vision-language (VL) transformer for cross-modal interaction. As shown in Fig.\ref{fig:intro}(a), we visualize the last layer of vision-language transformer in mainstream method, it indicates that the visual attentions focus on the foreground area of the image, rather than the text-relevant region (``$standing$''). We have summarized two reasons for this phenomenon:

\begin{itemize}
  \item The vision-language encoder for multimodal fusion is a coarse stack of transformers, the mechanism not only limits the sufficient interaction between vision and language contexts, but also exacerbates the computational cost due to the deep transformer-based structure.
  \item 
  Fine-tuning the entire backbone might suffer catastrophic forgetting and undermine the extensive prior knowledge learned from pre-training. In addition, fully training large pre-trained models and the VL transformer can be computationally expensive and time-consuming in practice.

\end{itemize}



Several previous works have noticed the insufficient
interaction problem, QRNet \cite{ye2022shifting} achieves the expression-aware visual feature extraction by inserting carefully designed interaction modules into the visual backbone. VG-LAW \cite{vg-law} proposes a language adaptive weight generator, generating weights for the fusion of visual and text features. However, they still require fully fine-tuning backbone and the sophisticated designs of interactive modules. More recently, Parameter-Efficient Transfer Learning (PETL) methods have also been introduced into visual grouding \cite{xiao2024hivg,liu2024dara}, HiVG \cite{xiao2024hivg} adopts LoRA to fine-tune the frozen CLIP model, and DARA\cite{liu2024dara} designs DA adapter and RA adapter to transfer intra- and inter-modality representations for the VG domain. However, due to the simple fusion strategy of vision-language transformer, they are not sufficient for multimodal alignment,  which could potentially compromise the modelâ€™s ability to capture text-relevant visual details.


In this paper, we aim to explore an efficient tuning and lightweight cross-modal interaction strategy. Inspired by the efficiency of Prompt Tuning \cite{khattak2023maple,liu2024dap,shi2024explicit} and Adapter \cite{chen2022adaptformer}, which only require fine-tuning a tiny number of parameters to adapt pre-trained models to various downstream tasks. We propose a step-wise multimodal fusion and adaption framework (SwimVG). {As depicted in Fig. \ref{fig:intro}(b), we design step-wise multimodal prompts (Swip) for multimodal fusion step by step, and explore a cross-modal interactive adapter (CIA) for further vision-text alignment. The visualizations of Swip (Fig. \ref{fig:more visualizations}(b)) and CIA (Fig. \ref{fig:more visualizations}(c)) demonstrate that both of them can independently facilitate multimodal interaction. Their integration, namely SwimVG, as visualized in Fig. \ref{fig:more visualizations}(c), leads to enhanced multimodal fusion.}
Through these elaborate designs, we implement an efficient and effective multimodal fusion strategy, abandoning the additional vision-language transformers used in previous methods \cite{deng2021transvg,transvg++,xiao2023clip,liu2024dara,xiao2024hivg}. As shown in Fig. \ref{fig:overview} (b), the vision attentions of the last layer in vision encoder indicate that SwimVG focuses exactly the text-relevant region.
 
Specifically, to efficient tuning the whole network, we frozen the vision and text backbone, and adopt domain-specific adapters (DoSA) for transferring pre-trained language knowledge to the specific task. To achieve adequate multimodal alignment, we investigate two strategies, namely token-level and weight-level. {For token-level multimodal fusion, we design step-wise multimodal prompts, which is formed by gradually integrating a learnable token that can represent the global text semantics into the visual backbone layer by layer.} These tokens are initially placed on the language encoder layers, and then mapped to the visual encoder from shallow to deep layers. To further enhance the multimodal fusion in a weight-level manner, we propose a novel cross-modal interactive adapter, which integrate visual and textual features by multi-head cross-attention mechanism. The multimodal adaptation process involves a set of low-rank weight matrices reorganized, producing the crucial alignment capabilities for visual grounding. By the multi-level design of token- and weight-level, for a given image input, the visual encoder can focus more on the text-relevant area, without fully fine-tuning the pre-trained models. 

We conduct extensive experiments on RefCOCO \cite{yu2016refcoco}, RefCOCO+ \cite{yu2016refcoco}, RefCOCOg \cite{mao2016refcocogg,nagaraja2016refcocogu} and Flickr30K Entities \cite{plummer2015flickr30k}, and our method achieves state-of-the-art (SOTA) performance on the four widely used datasets. In addition, we demonstrate the efficiency of our framework in Table \ref{tab:cost}, it can be seen that the inference time of SwimVG is about 40\% faster than these mainstream methods using the vision-language transformer. The main contributions can be summarized as three-fold:
\begin{itemize}
  \item 
  We proposed a concise and efficient framework of multimodal fusion and adaption, which adapt pre-trained models to visual grounding step by step. SwimVG achieves token-level and weight-level interaction between visual and language representations, and significantly alleviates the task gap between pre-trained models and grounding.
  
  \item We replace the heavyweight vision-language transformer with cross-modal interactive adapters and step-wise multimodal prompts, which allow for fine-tuning the entire model in a lightweight manner.

  \item
  Extensive experiments demonstrate that our method outperforms the SOTA methods in VG tasks, with only \textbf{2.04\%} tunable parameters. Moreover, SwimVG offers significant computing efficiency advantages.

\end{itemize}

