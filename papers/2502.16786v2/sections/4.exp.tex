

\section{Experiments}
In this section, we will give a detailed experimental analysis of the whole framework, including the datasets, evaluation protocol, implementation details, comparisons with the state-of-the-art methods, and ablation analysis.

\subsection{Experimental Setup}
\noindent \textbf{Datasets.}
To verify the effectiveness and efficiency of our method, we have conducted comprehensive experiments on the RefCOCO \cite{yu2016modeling}, RefCOCO+ \cite{yu2016modeling}, RefCOCOg \cite{mao2016refcocogg,nagaraja2016refcocogu} and Flickr30K Entities \cite{plummer2015flickr30k} datasets, all of which are widely used as benchmarks for visual grounding.

\begin{itemize}
  \item \textbf{RefCOCO} features 19,994 images with 50,000 referred objects and 142,210 expressions. The dataset is divided into four subsets, consisting of 120,624 train, 10,834 validation, 5,657 test A, and 5,095 test B samples, respectively. The average length of the expressions is 3.6 words, and each image contains a minimum of two objects.

 \item
\textbf{RefCOCO+} with similar content but richer expressions, includes 19,992 images with 49,856 referred objects and 141,564 referring expressions. The dataset is divided into four subsets: 120,624 train, 10,758 validation, 5,726 test A, and 4,889 test B samples. Notably, the RefCOCO+ dataset has been constructed to be more challenging than the RefCOCO dataset by excluding certain types of absolute-location words. The average length of the expressions is 3.5 words, including the attribute and location of referents.

 \item
\textbf{RefCOCOg} , unique for its detailed annotations and longer referential expressions, contains 25,799 images with 49,856 objects. There are two commonly used split protocols for this dataset. One is RefCOCOg-google \cite{mao2016refcocogg}, and the other is RefCOCOg-umd \cite{nagaraja2016refcocogu}. We report our performance on both RefCOCOg-google (val-g) and RefCOCOg-umd (val-u and test-u) to make comprehensive comparisons. The average length of expressions within the dataset is 8.4 words, including both the attributes and the locations of the referents. This rich detail description facilitates a more nuanced understanding of the visual grounding tasks, as it captures the intricacies of how objects are referenced in various contexts.

 \item
\textbf{Flickr30K Entities} \cite{plummer2015flickr30k}, is an enhanced version of the original Flickr30K \cite{young2014image}, fortified with the addition of short region phrase correspondence annotations. This expansion yields a collection of 31,783 images, encompassing 427,000 referred entities. Following the previous studies \cite{xiao2024hivg,wang2023cogvlm}, we have divided the dataset into 29,783 images for training, 1,000 for validation, and another 1,000 for testing purposes.
\end{itemize}



\noindent
\textbf{Evaluation Metrics.} We follow the previous research that employs top-1 accuracy (\%) as the evaluation metric for visual grounding. Specifically, a prediction is deemed accurate only when its Intersection-over-Union (IoU) exceeds or equals 0.5. In addition to Precision@0.5, we also report the number of tunable parameters in the pre-trained encoders to compare the fine-tuning efficiency with traditional full fine-tuning and other PETL methods.

\input{./table/petl}

\noindent \textbf{Implementation Details.} The vision encoder is initialized with DINOv2-L/14~\cite{oquab2023dinov2}, while the language encoder uses CLIP-B~\cite{radford2021learning}. The resolution of the input image is 224Ã—224. The DINOv2-L/14 model processes tokens with a feature dimension of 768, while 
and the CLIP-B model handles tokens with a feature dimension of 512. All prompts use Xavier initialization, and all adapters are initialized with Kaiming normal initialization. The bottleneck dimension $C_d$ for both CIA and domain-specific adapters is 56, and more dimension comparisons can be seen in Table \ref{Table:neck}. The batchsize for training is 32. For fair comparisons, other PETL methods in Tab. \ref{Table:comparisons with PETL} use the same base architecture and original hyperparameters, and keeping the vision and language encoder frozen. For RefCOCO \cite{yu2016refcoco}, RefCOCOg \cite{mao2016refcocogg,nagaraja2016refcocogu}, and Flickr30K Entities \cite{plummer2015flickr30k} datasets, the entire network is trained for 65 epochs using the AdamW optimizer. While for RefCOCO+ \cite{yu2016refcoco} dataset, the network is trained for 90 epochs. Note that most mainstream methods train RefCOCO/RefCOCOg/Flickr30K Entities for 90 epochs and RefCOCO+ for 180 epochs, which demonstrates the higher efficiency of our SwimVG. We conduct all experiments on one A800 GPU.




\subsection{Main Results}
We compare our SwimVG comprehensively with a series of previous visual grounding (VG) methods. The main experimental results are displayed in Tab. \ref{Table:comparisons with SOTA}. We can notice from these results that SwimVG reaches the best accuracy and also ensures parameter efficiency compared with all other methods, which validates its effectiveness and efficiency.

\noindent
\textbf{Effectiveness.} As Tab. \ref{Table:comparisons with SOTA} shown, on the three commonly challenging benchmarks, SwimVG outperforms all traditional full fine-tuning methods. Compared to DARA~\cite{liu2024dara}, a parameter-efficient transfer learning method, we achieves an average accuracy improvement of 10.85\% on the three benchmarks. Notably, even compared to some methods that are pre-trained on the the RefCOCO/+/g and Flickr30K Entities (indicated by $\dagger$ in Tab. \ref{Table:comparisons with SOTA}), our SwimVG model achieves the highest scores across all evaluation tasks, with particularly strong performance on the RefCOCO+, which present greater challenges compared to RefCOCO.




\noindent
\textbf{Efficiency.} Tab. \ref{Table:comparisons with SOTA} clearly illustrates that SwimVG not only achieves the best performance, but also highlights its huge advantages in parameter efficiency. SwimVG reduced the tunable backbone parameters by 97.96\% compared to the traditional full fine tuning method. In order to verify more efficient aspects such as training and inference time, experimental results on the mainstream methods using the conventional VL transformer, and the other PETL methods are shown in Tab. \ref{tab:cost}. It can be seen that SwimVG achieves significant energy efficiency advantages.

\input{./table/efficiency}

\subsection{Comparison with Other PETL Methods}

\textbf{Details of Baseline PETL Methods.}

This section furnishes additional details of the PETL baselines employed in our primary manuscript. Notably, all these baselines follow the same base architecture.
\begin{itemize}

    \item \textbf{AdaptFormer \cite{chen2022adaptformer}:} We add adapters in parallel to MHA and FFN in both Vision Encoder and Language Encoder. Following the original work, we set the same bottleneck dimensions of AdaptFormer for both vision and language branch.
    
    \item \textbf{LoRA \cite{hu2021lora}:} We incorporate trainable matrices in parallel to the weight matrices in MHA and FFN in both Vision Encoder and Language Encoder. 
    We have set the same bottleneck dimensions for both the vision and language branches of LoRA, following the original setup.

    \item \textbf{UniAdapter \cite{lu2024uniadapter}:} We add UniAdapter  in both Vision Encoder and Language Encoder, according to their basic designs. 

    \item \textbf{DAPT \cite{zhou2024dynamic}:} We insert Dynamic Adapters in paralle to the weight matrices in MHA and FFN in both Vision Encoder and Language Encoder, and use their task-agnostic feature transform strategy. Other sets such as bottleneck dimensions are same as the DAPT.

   

    %\item \textbf{DARA \cite{liu2024dara}:} We add DA Adapters after the MHA, and RA Adapters in parallel to FFN in the Vision Encoder and Language Encoder, according to their basic designs. We set the bottleneck dimensions of DA and RA Adapters to 128, and the weight-sharing dimensions of RA Adapters to 256.
    
\end{itemize}

We conduct experiments comparing our SwimVG with other parameter-efficient transfer learning (PETL) methods. To ensure fairness, we retain the original parameter settings from previous methods. As these PETL methods lack the capability of multimodal fusion, we complement them with the traditional VL transformer for cross-modal understanding, thereby enabling a direct comparison with our SwimVG. Tab. \ref{Table:comparisons with PETL} illustrates that SwimVG outperforms other PETL methods on all three benchmarks. Through introducing step-wise multimodal prompts and cross-modal interactive adapters, SwimVG enhances the modeling of the vision-text alignment capability. Previous PETL methods lack this ability, rendering them less effective for VG tasks. This also proves that the multimodal fusion mechanism in SwimVG is more efficient than the VL transformer. To summarize, by the specific design for the VG domain, SwimVG achieves superior performance with only \textbf{2.04} \% tunable parameters.

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\textwidth]{./figures/visual.pdf}
% \vspace{-1mm}
\caption{Visualizations of attention maps, prediction results (yellow bounding boxes) and ground truth (red bounding boxes).}
% \vspace{-4mm}
\label{fig:visualizations}
\end{figure*}


\subsection{Convergence Analysis}
Figure \ref{other-convergence} shows a comparison of the convergence epoch between SwimVG and other models. It is observed that DARA and TransVG converge around epoch 85, while CLIP-VG converges at approximately epoch 105. In contrast, SwimVG achieves convergence at around epoch 65. This demonstrates the efficiency of our method, as fewer training epochs are required, thereby reducing training costs. In addtion, we have also visualized the convergence comparison of SwimVG across the RefCOCO, RefCOCOg-u, RefCOCOg-g, and Flicker 30K datasets. Figure \ref{self-conver} indicates that convergence is achieved around epoch 65 for all these datasets.




\begin{figure}
\centering
\includegraphics[width=1\linewidth]{./figures/other-comp.png}
\vspace{-4mm}
\caption{The convergence comparison between SwimVG and other SOTA models on RefCOCO.}
\vspace{-3mm}
\label{other-convergence}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=1\linewidth]{./figures/self-comp.png}
\vspace{-4mm}
\caption{The convergence comparison of SwimVG on RefCOCO, RefCOCOg and Flicker 30K datasets.}
\vspace{-3mm}
\label{self-conver}
\end{figure}

\subsection{Ablation Study}




\noindent
\textbf{Effectiveness of Multimodal Interaction in SwimVG.} We assess the impact of step-wise multimodal prompts (Swip) and cross-modal interactive adapters (CIA) by performing an ablation study, and report the results on RefCOCOg-u validation and test datasets. Considering the substantial number of parameters occupied by the encoder, we freeze all the encoder parameters during fine-tuning for efficiency. From Tab. \ref{Table:swip-cia}, it is evident that only introducing the Swip yields a ideal results (Tab. \ref{Table:swip-cia} (a)). Only by using the CIA for cross-modal fusion can achieve better results (Tab. \ref{Table:swip-cia} (b)). Compared with the previous methods using the traditional vision-language encoder, such as TransVG \cite{deng2021transvg}, DARA \cite{liu2024dara} in Tab. \ref{Table:comparisons with SOTA}, it shows that we can achieve the better results using only Swip or CIA. Tab. \ref{Table:swip-cia} (c) indicates that incorporating Swip and CIA for multimodal fusion results in an average improvement of 3.49\% across the RefCOCOg-u, achieving the best performance among these ablation variants. Swip achieves progressive multimodal fusion by gradually introducing linguistic information, while CIA explores deeper correlations by enhancing cross-modal interaction. Combining the two can simultaneously promote multimodal fusion in terms of breadth and depth.

\input{./table/swip-cia}



\noindent
\textbf{Effectiveness of Domain-specific Adapters.} Because the text encoder is pre-trained on a general domain, freezing the entire text backbone restricts the specific language understanding in visual grounding domain, thereby weakening the proper interaction between text and vision semantics. To enable the domain text semantics to interact with the visual encoder efficiently, we adopt domain-specific adapters to learn the domain knowledge, thus making the text encoder match with visual grounding. Tab. \ref{Table:ablation on text adapter} shows that domain-specific adapters efficiently transfer the language knowledge of the pre-trained model to VG domain, further improving an average improvement of 4.39\% across the RefCOCOg-u.



\input{./table/text_adapter}


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{./figures/visual-appendix.pdf}
\caption{The visualizations of attention maps from vision encoder with different strategies of SwimVG. Red bounding boxes represent ground truth, and yellow bounding boxes are prediction results.}
\vspace{-4mm}
\label{fig:more visualizations}
\end{figure*}


\noindent
\textbf{Effects of Different Insertion Positions of SwimVG.} To determine the optimal configuration of the Cross-modal Interactive Adapter (CIA) and Text Adapter, we conducted an ablation study varying both different layers and the dimensions of the adapters. Firstly, we evaluated the impact of different adapter layers. In this experiment, the visual CIA and the Text Adapter were inserted at the same layers. From Table \ref{tab:layer}, we can see that: \textbf{(1)} Only inserting three layers for vision and text encoder can brings great performance (Table \ref{tab:layer} (a)); \textbf{(2)} observing Table \ref{tab:layer} (b), (c), and (d), it can be seen that inserting CIA later in the vision encoder can exhibit better performance; \textbf{(3)} 
from the observation of Table \ref{tab:layer} (e) and (f), it is evident that inserting text adapter later in the text encoder results in a minor performance decline; \textbf{(4)} 
adding adapters from 13 layers to 24 layers not only reduces performance but also increases the tunable parameters. This might be because the visual backbone is more likely to adapt to the VG domain at deeper layers, while the text needs to adapt from the shallow layers to the deep layers. It should be noted that the text encoder is composed of 12 layers, while the vision encoder comprises 24 layers.


\input{./table/layers}

\input{./table/neck}

\noindent
\textbf{Effects of Different Hyper-parameter Settings of SwimVG.} We first ablate the bottleneck dimensions $C_d$ of all adapters (see Table \ref{Table:neck} (a,b,c)), and follow the design shown in Table \ref{Table:neck} (a). $C_d$ determines the number of tunable parameters introduced by SwimVG. As shown in Table \ref{Table:neck}, higher $C_d$ introduces more parameters, and the performance consistently increases when $C_d$ increases up to 56. $C_d$ 128 exhibits considerable performance, but its tunable parameter count is about twice that of $C_d$ 56. Thus, we select the $C_d$ as 56. This indicates that a small bottleneck may not provide sufficient adaptation capabilities, while a large dimension may lead to over-adaptation. An intermediate dimension can achieve a better adaptation to the VG domain.


\input{./table/backbone}
\noindent
\textbf{The contribution degree of different pre-trained models.} To facilitate the analysis of the contribution of different backbones to performance, we excluded the SwimVG method and compared different backbones based on TransVG\cite{deng2021transvg}. We selected ResNet101+DETR and DINOv2-L as the vision backbone and chose the mainstream BERT-Base and the text encoder in CLIP-Base as the text backbone. As see in Table \ref{Table:backbone}, the vision backbone has a relatively large impact on visual grounding, whereas the text backbones have a relatively small impact. Under the same backbone, our method outperforms TransVG, which indicates that our multimodal fusion strategy is highly effective.

\vspace{-4mm}
\subsection{More Evaluation Metrics}
We compared more challenging evaluation metrics, such as the prediction accuracy when IoU $>$ 0.6 (Pr@0.6) and Pr@0.8. Under the same metrics, we compared the latest MaPPER \cite{liu2024mapper}. As seen in Table \ref{Table:iou}, SwimVG outperforms the latest MaPPER under both the settings of Pr@0.6 and Pr@0.8.

\input{./table/iou}


\subsection{Qualitative Results}
\textbf{The comparison of multimodal fusion strategy.}
To verify that the multimodal fusion strategy of SwimVG is superior to the traditional vision-language transformer (VL encoder), we visualize the attention maps from the last layer of vision encoder in SwimVG. Due to the suboptimal multimodal fusion methods employed by other mainstream approaches, namely the visual language transformer (VL encoder), which lack open-source code or checkpoints, we opt to visualize the last layer of the VL encoder from TransVG. As shown in Fig.\ref{fig:visualizations}, TransVG fails to pay sufficient attention to text-relevant regions in a images. For example, TransVG lacks the alignment ability of ``$different$'', ``$black$'', and ``$standing$'' with images. The comparison with TransVG demonstrates the ability of our proposed SwimVG to focus more on the text-relevant regions, and our multimodal fusion strategy is superior to the traditional VL encoder.



\noindent
\textbf{The effectivess of CIA and Swip.}
In this section, we present more visualization of the attention maps from the vision encoder under different mixing strategies. As depicted in Figure \ref{fig:more visualizations}, we can see that: \textbf{(1)} introducing either cross-modal interactive adapters (CIA) or step-wise multimodal prompts (Swip) facilitates the interaction between the vision and language encoders. (Figure \ref{fig:more visualizations} (b,c)); \textbf{(2)} compared to CIA, the attention map of only introducing is slightly scattered (Figure \ref{fig:more visualizations} (b,c)); 
integrating CIA and Swip can further enhances the facilitation of cross-modal interaction (Figure \ref{fig:more visualizations} (d)). The interaction between the vision and language encoder, facilitated by CIA and Swip, allows the model to focus more effectively on the referred objects in diverse expression cases. 







