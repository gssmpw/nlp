\section{Method}

Our method is designed to enhance the generalization capabilities of pre-trained models in the realm of visual grounding efficiently. This is achieved through step-wise multimodal prompts, light domain-specific adapters, and cross-modal interactive adapters. Fig. \ref{fig:overview} shows the overall architecture of our proposed SwimVG framework. 



\subsection{Text \& Image Backbone}
Given an image and a text, we extract their features through the image encoder and text encoder, respectively.

\noindent
\textbf{Text Encoder.} Given the input text expression $T$ with a length of $L$, we utilize the pre-trained text branch of CLIP \cite{radford2021learning} for extracting text features. The text expression is firstly converted into a one-hot vector. Subsequently, each one-hot vector is tokenized into a series of linguistic tokens, and the sequence of tokens is then fed into a stack of 12 transformer encoder layers to progressively capture and model the intricate language tokens.
The input embeddings $\bm{\hat T} \in \mathbb{R}^{L\times C_t}$, where $\bm{\hat T} = [t^1, t^2, \cdots, t^L]$, and $C_t$ is the dimension of text embeddings. 


\label{text encoder:clip}


% 

\noindent
\textbf{Visual Encoder.} 
We use DINOv2 \cite{oquab2023dinov2} as the visual backbone. The model involves training the Vision Transformer (ViT) model \cite{dosovitskiy2020vit} on the extensive LVD-142M dataset, and employs a self-supervised learning strategy. This method allows the model to extract powerful visual features, thereby offering remarkable performance in various downstream tasks. Given an input image
${I}_0 \in \mathbb{R}^{H_0\times W_0\times3}$, the image is initially divided into $N$ non-overlapping patches, which are then linearly projected into $D$-dim patch embeddings $\bm{I'}_0 \in \mathbb{R}^{D\times C_v}$. Motivated by TransVG \cite{deng2021transvg} and HiVG \cite{xiao2024hivg} appending a learnable [REG] in vision-language transformer, we also adopt a learnable [REG] token to directly predict the 4-dim coordinates of a referred object. Unlike the previous method, we omit the complex vision-language fusion structure and directly pre-append the [REG] token to $\bm{I'}_0$, and the token is processed by the visual encoder layer gradually.


As the vision and language backbones contain most model parameters and have acquired rich knowledge during pre-training, we attempt to freeze them during fine-tuning. This strategy allows for a more efficient allocation of computational resources and focuses the learning on the adjustments of other modules.



\subsection{Step-wise Multimodal Prompting} 
The intuitive idea of achieving token-level multimodal alignment is to directly concatenate text tokens and vision tokens together for learning. However, an increase in the input length will bring about a computational burden. To efficiently establish token-level multimodal alignment, we design step-wise multimodal prompts, and introduce these learnable tokens in the layers of both vision and language branches from shallow to deep layer. This means that these tokens are added to transformer layers in a hierarchical way. The hierarchical multimodal prompts utilize the knowledge embedded in pre-trained models to effectively learn task-relevant cross-modal representations. 

% By adding the learnable tokens hierarchically, we aim to better capture the complex relationships between the vision and language semantics at different levels of the model.

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{figures/adapter.pdf}
% \vspace{-6mm}
\caption{The Domian-specific adapter and cross-modal interactivate adapter.}
% \vspace{-4mm}
\label{fig:adapter}
\end{figure}

\noindent
\textbf{Text Prompting.}
To learn the represent the global textual representation, a learnable token $p \in \mathbb{R}^{H}$ is introduced in the text encoder. The input embeddings $\bm {\hat T}$ is converted to $ \bm{\hat T'}$, follow the form $[p,t^1, t^2, \cdots, t^L]$. The new token is further processed by each transformer block of the language encoder $\mathcal{L}_{i}$. This process can be formulated as below:

\begin{equation}
\begin{aligned}
[{p_i}, \bm {T}_{i}] = \mathcal{L}_{i}([p_{i-1}, \bm {T}_{i-1}]) ~~~~ i=1, 2, \cdots, l.
\end{aligned}
\end{equation}


\noindent
where $p_i$ and \bm ${T}_{i}$ represent the prompt and text embeddings processed by the $i$-th language encoder layer, respectively. The $l$ refers to the depth of the language encoder. The prompt is initialized by Xavier initialization.

\noindent
\textbf{Step-wise Multimodal Fusion.}
To efficiently fuse textual and visual semantics step-by-step, we gradually convey the prompt processed by the text encoder to vision encoder.
Due to the different feature dimensions of the text and vision encoder, we need to adjust the features to the same dimension. Therefore, we design a bridge layer to transport text features, making them adaptable for visual branch. For the visual encoder layer $\mathcal{V}_{i-1}$, we introduce the token from the $\mathcal{L}_{i}$ language encoder layer to the layer $\mathcal{V}_{i-1}$ of vision encoder. Since the prompt added to the visual token set is initialized by global textual semantics, when the prompt is introduced into the corresponding visual layer, it can facilitate multimodal fusion step by step, namely step-wise multimodal prompt (Swip). Each Swip is further processed by the deeper visual layer. The process can be formalized as:


\begin{equation}
m_i = p_i\mathbf{W}_{bridge}
\end{equation}

\noindent



\begin{equation}
\begin{aligned}
[m_{i}^0,\cdots,m_{i}^{i-1}, \bm V_{i}] = \mathcal{V}_{i}([m_{i-1}^0,\cdots,m_{i-1}^{i-1},\bm V_{i-1}]) \\
\end{aligned}
\end{equation}


\noindent
where $\mathbf{W}_{\text{bridge}} \in \mathbb{R}^{C_t \times C_v}$ is the weights of bridge layer. The $m_i$ are multimodal prompts transformed from text prompts. The $n$ refers to the depth of vision encoder, and $m_i^{0}$ represents the $0$-th swip token processed by the $i$-th vision encoder layer. The $\bm V_{i}$ is vision emdeddings processed by the $i$-th vision encoder layer.


\subsection{Cross-modal Interactive \& Domain Adaption}

To efficiently transfer pre-trained text semantics knowledge to visual grounding, and further facilitate multimodal interaction, we introduce domain-specific adapters in the text encoder and cross-modal interactive adapters in vision encoder, respectively.

\noindent
\textbf{Cross-modal Interactive Adapter.}
As shown in Fig. \ref{fig:adapter}(b), we design a Cross-modal Interactive Adapter (CIA) to make the interaction of modal information between the visual encoder and text encoder, which enhances the capability of multimodal fusion while fixing the backbone parameters. The main difference between the design of CIA and previous adapters lies in the integration of a cross-modal attention module. To ensure the lightweight and efficiency of whole structure, we firstly adopt a down-projection to transform the visual features to low-rank features. CIA module is inserted between the activation and up-projection layers. Similar to step-wise multimodal fusion, text features should be converted by bridge layer to dimensions that match the visual branches. Given vision features $f_i^v$ process by the Multi-Head Attention (MHA) of the layer $\mathcal{V}_{i}$, and text features $f_i^t$ process by the MHA of the layer $\mathcal{L}_{i}$, this process can be formulated as below:

\begin{equation}
c_i^t = f_t\mathbf{W}_{bridge}
\end{equation}



\begin{equation}
\begin{aligned}
\begin{split}
    f_{down}^v = f_i^v\mathbf{W}_{down}, \\
    f_{act}^v=\operatorname{ReLU}\boldsymbol(f_{down}), \\
    f_{l}^v = f_{act}\mathbf{W}_{linear}.
\end{split}
\end{aligned}
\end{equation}

\begin{equation}
\small
\begin{aligned}
\mathbf{MHCA}(f_{l}^v,c_i^t)=\operatorname{Softmax}\left(\frac{f_i^vW_qc_i^tW_k}{\sqrt{C}}\right)(c_i^tW_v)
\end{aligned}
\end{equation}


\begin{equation}
f_{up} = (f_{l}^v+\mathbf{MHCA}(f_{l}^v,c_i^t))\mathbf{W}_{up}
\end{equation}

\begin{equation}
 \mathbf{CIA}(f_i^v,c_i^t)= f_i^v + s_{vt} \cdot f_{up}.
\end{equation}

\noindent
Here, $\mathbf{W}_{\text{down}} \in \mathbb{R}^{C_v \times C_d}$ and $\mathbf{W}_{\text{up}} \in \mathbb{R}^{C_d \times C_v}$ are the weights of down- and up-projection layers, and $s_{vt}$ is the scaling factor for multimodal fusion. The $\mathbf{MHCA}$ is Multi-Head Cross-Attention module in CIA.


\noindent
\textbf{Domain-specific Adapter.}
Due to fully freezing of the text backbone, there exists a gap between pre-trained model and visual grounding. To address the issue, we incorporate domain-specific adapters (DoSA) to improve the text encoder for domain understanding, As shown in Fig. \ref{fig:adapter}(a). Compared to the CIA adapter, the domain-specific adapter adopts a more straightforward design, focusing on learning text representation efficiently without complex structure. This neat yet effective approach ensures efficient processing of text semantics while maintaining compatibility with the overall model architecture. By taking advantage of these enhanced features, the model facilitates aligning visual and linguistic features. Specifically, the domain-specific adapter follows a standard ``Down-ReLU-Up" structure to bridge the gap between pre-trained knowledge and visual grounding. Given the text features $f_i^t$ processed by the Multi-Head Attention (MHA) of the layer $\mathcal{L}_{i}$, the learning process can be formalized as:

\input{./table/main}


\begin{equation}
\begin{aligned}
\begin{split}
    t_{down} = f_i^t\mathbf{W}_{down}, \\
    t_{act}=\operatorname{ReLU}\boldsymbol(t_{down}), \\
    t_{up} = t_{act}\mathbf{W}_{up},    
\end{split}
\end{aligned}
\end{equation}

\begin{equation}
\text{DoSA}(f_i^t)=f_i^t + s_t \cdot t_{up},
\end{equation}


\noindent
where $\mathbf{W}_{\text{down}} \in \mathbb{R}^{C_t \times C_d}$ and $\mathbf{W}_{\text{up}} \in \mathbb{R}^{C_d \times C_t}$ are the weights of down- and up-projection layers, and $s_t$ is the scaling factor of domain-specific adapters.
In this way, DoSA can refine the rich pre-trained language representations into more fine-grained representations for the VG domain during fine-tuning.

\subsection{Prediction Head}
Followed by HiVG \cite{xiao2024hivg} and TransVG++ \cite{transvg++}, a regression block with a MLP and a linear layer are adopted to perform box coordinates prediction. Given the [REG] token from the last layer of vision encoder, the regression block generates the 4-dim bounding box coordinates.

\subsection{Training Objectives}
Following the previous work \cite{deng2021transvg,liu2024dara}, the L1 loss and Generalized IoU (GIoU) loss are used between the the predicted bounding box coordinates $\tilde{b} = (\tilde{x}, \tilde{y}, \tilde{w}, \tilde{h})$ and the the ground truth ${b} = ({x}, {y}, {w}, {h})$, the training objective for VG is defined as follows:
\begin{equation}
\mathcal{L}_{\text{rec}} = \lambda_1 \mathcal{L}_{L1}(b, \tilde{b}) + \lambda_{\text{giou}} \mathcal{L}_{\text{giou}}(b, \tilde{b}),
\end{equation}



\noindent
where $\mathcal{L}_{L1}(\cdot)$ and $\mathcal{L}_{\text{giou}}(\cdot)$ represent L1 loss and GIoU loss \cite{rezatofighi2019generalized}], respectively. The $\lambda_1$ and $\lambda_{\text{giou}}$ are the weight coefficient to balance the two detection loss functions.





