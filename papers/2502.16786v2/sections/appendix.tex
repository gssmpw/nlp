\newpage

\section*{Appendix}

In the appendix, we provide a detailed introduction of the used datasets, more implementation details, details of other PETL methods, additional ablation study, and more visualization results. 

\section{Details of Datasets}

To verify the effectiveness and efficiency of our method, we have conducted comprehensive experiments on the RefCOCO \cite{yu2016modeling}, RefCOCO+ \cite{yu2016modeling}, and RefCOCOg \cite{mao2016refcocogg} datasets, all of which are widely used as benchmarks for visual grounding.
\\
\textbf{RefCOCO} features 19,994 images with 50,000 referred objects and 142,210 expressions. The dataset is divided into four subsets, consisting of 120,624 train, 10,834 validation, 5,657 test A, and 5,095 test B samples, respectively. The average length of the expressions is 3.6 words, and each image contains a minimum of two objects.
\\
\textbf{RefCOCO+} with similar content but richer expressions, includes 19,992 images with 49,856 referred objects and 141,564 referring expressions. The dataset is divided into four subsets: 120,624 train, 10,758 validation, 5,726 test A, and 4,889 test B samples. Notably, the RefCOCO+ dataset has been constructed to be more challenging than the RefCOCO dataset by excluding certain types of absolute-location words. The average length of the expressions is 3.5 words, including the attribute and location of referents.
\\
\textbf{RefCOCOg} , unique for its detailed annotations and longer referential expressions, contains 25,799 images with 49,856 objects. There are two commonly used split protocols for this dataset. One is RefCOCOg-google \cite{mao2016refcocogg}, and the other is RefCOCOg-umd \cite{nagaraja2016refcocogu}. We report our performance on both RefCOCOg-google (val-g) and RefCOCOg-umd (val-u and test-u) to make comprehensive comparisons. The average length of expressions within the dataset is 8.4 words, including both the attributes and the locations of the referents. This rich detail description facilitates a more nuanced understanding of the visual grounding tasks, as it captures the intricacies of how objects are referenced in various contexts.

\section{More Implementation Details}
\label{sec:more imple}

\paragraph{Hyper-parameters Settings.} SwimVG are inserted into different transformer encoder layers at the same indices as those in the Vision Encoder and Language Encoder. The scaling factor $s$ for all adapters is set to 0.2. Relevant ablation studies on these hyperparameters are presented in Table \ref{tab:layer} and \ref{Table:neck}.  


\paragraph{Training Details.} For RefCOCO \cite{yu2016refcoco} and RefCOCOg \cite{mao2016refcocogg,nagaraja2016refcocogu} datasets, the entire network is trained for 65 epochs using the AdamW optimizer. While for RefCOCO+ \cite{yu2016refcoco} dataset, the network is trained for 90 epochs. Note that most mainstream methods train RefCOCO for 90 epochs and RefCOCO+ for 180 epochs, which demonstrates the higher efficiency of our SwimVG. We conduct all experiments on one A800 GPU.


\section{Details of Baseline PETL Methods}
\label{sec:details of baseline petl}


This section furnishes additional details of the PETL baselines employed in our primary manuscript. Notably, all these baselines follow the same base architecture.
\begin{itemize}

    \item \textbf{AdaptFormer \cite{chen2022adaptformer}:} We add adapters in parallel to MHA and FFN in both Vision Encoder and Language Encoder. Following the original work, we set the same bottleneck dimensions of AdaptFormer for both vision and language branch.
    
    \item \textbf{LoRA \cite{hu2021lora}:} We incorporate trainable matrices in parallel to the weight matrices in MHA and FFN in both Vision Encoder and Language Encoder. 
    We have set the same bottleneck dimensions for both the vision and language branches of LoRA, following the original setup.

    \item \textbf{UniAdapter \cite{lu2024uniadapter}:} We add UniAdapter  in both Vision Encoder and Language Encoder, according to their basic designs. 

    \item \textbf{DAPT \cite{zhou2024dynamic}:} We insert Dynamic Adapters in paralle to the weight matrices in MHA and FFN in both Vision Encoder and Language Encoder, and use their task-agnostic feature transform strategy. Other sets such as bottleneck dimensions are same as the DAPT.

   

    %\item \textbf{DARA \cite{liu2024dara}:} We add DA Adapters after the MHA, and RA Adapters in parallel to FFN in the Vision Encoder and Language Encoder, according to their basic designs. We set the bottleneck dimensions of DA and RA Adapters to 128, and the weight-sharing dimensions of RA Adapters to 256.
    
\end{itemize}


\section{Additional Ablation Study}
\label{sec:addional exp}

In this section, we conduct more ablative experiments to further explore the impact of various factors in SwimVG. All experiments are performed on RefCOCOg-u. 


\paragraph{Effects of Different Insertion Positions of SwimVG.} To determine the optimal configuration of the Cross-modal Interactive Adapter (CIA) and Text Adapter, we conducted an ablation study varying both different layers and the dimensions of the adapters. Firstly, we evaluated the impact of different adapter layers. In this experiment, the visual CIA and the Text Adapter were inserted at the same layers. From Table \ref{tab:layer}, we can see that: \textbf{(1)} Only inserting three layers for vision and text encoder can brings great performance (Table \ref{tab:layer} (a)); \textbf{(2)} observing Table \ref{tab:layer} (b), (c), and (d), it can be seen that inserting CIA later in the vision encoder can exhibit better performance; \textbf{(3)} 
from the observation of Table \ref{tab:layer} (e) and (f), it is evident that inserting text adapter later in the text encoder results in a minor performance decline; \textbf{(4)} 
adding adapters from 13 layers to 24 layers not only reduces performance but also increases the tunable parameters. It should be noted that the text encoder is composed of 12 layers, while the vision encoder comprises 24 layers.




\input{LaTeX/tables/layers}

\input{LaTeX/tables/neck}

\paragraph{Effects of Different Hyper-parameter Settings of SwimVG.} We first ablate the bottleneck dimensions $C_d$ of all adapters (see Table \ref{Table:neck} (a,b,c)), and follow the design shown in Table \ref{Table:neck} (a). $C_d$ determine the number of tunable parameters introduced by SwimVG. As shown in Table \ref{Table:neck}, higher $C_d$ introduces more parameters, and the performance consistently increases when $C_d$ increases up to 56. $C_d$ 128 exhibits considerable performance, but its tunable parameter count is about twice that of $C_d$ 56. Thus, we select the $C_d$ as 56. 


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{LaTeX/images/visual-appendix.pdf}
\caption{The visualizations of attention maps from vision encoder with different strategies of SwimVG. Red bounding boxes represent ground truth, and yellow bounding boxes are prediction results.}
\vspace{-4mm}
\label{fig:more visualizations}
\end{figure*}






\section{More Visualization Results}
\label{sec:more vis}

In this section, we present more visualization of the attention maps from the vision encoder under different mixing strategies. As depicted in Figure \ref{fig:more visualizations}, we can see that: \textbf{(1)} introducing either cross-modal interactive adapters (CIA) or step-wise multimodal prompts (Swip) facilitates the interaction between the vision and language encoders. (Figure \ref{fig:more visualizations} (b,c)); \textbf{(2)} compared to CIA, the attention map of only introducing is slightly scattered (Figure \ref{fig:more visualizations} (b,c)); 
integrating CIA and Swip can further enhances the facilitation of cross-modal interaction (Figure \ref{fig:more visualizations} (d)). The interaction between the vision and language encoder, facilitated by CIA and Swip, allows the model to focus more effectively on the referred objects in diverse expression cases. 







