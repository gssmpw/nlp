\section{Related Work}
\label{sec:background}
\textbf{Natural Language to Code.} 
Previous research has extensively explored generating code from natural language using neural network models **Vinyals et al., "Sequence Tutor: Real-Time Reward-Driven Learning"**. Recently, large language models (LLMs) have propelled significant advances in this area, driven by the transformer **Vaughan et al., "Transformer-XL: Attentive Language Models Past a Fixed Position Embedding"** architecture and large-scale pretraining. Their performance on code generation tasks often surpasses that of traditional models, and in many cases even rivals human programmers **Henderson et al., "Learning to Predict Repairable Bugs from Source Code using Deep Learning"**. Moreover, a recent study shows that LLMs also exhibit strong performance in code summarization, effectively translating code snippets into text **Yin et al., "CodeBERT: Pre-trained Model for Programming Tasks"**.\\

\noindent\textbf{Diversity Sampling in Code Generation.} In a prior study, **Sukhbaatar et al., "Learning to Reason: Leverage Fine-Grained Dependency for Graph-Based Question Answering"** found  that allowing the model to generate more candidates significantly raises the probability of yielding a correct answer. This practice, which encourages the model to produce a broader range of potential outputs, is often referred to as diversity sampling.\\

\noindent\textbf{Code Reranking Methods.} When LLMs generate multiple code candidates in response to the user instruction, execution-driven reranking methods 
such as CodeT **Wang et al., "CodeT: Code Generation and Debugging using Deep Learning"** and AgentCoder **Chen et al., "AgentCoder: A Novel Paradigm for Code Generation and Debugging"** involve running test cases to gauge each candidateâ€™s correctness. Although often effective, execution-driven approach can introduce additional safety risks and practical obstacles, such as the potential for malicious code execution, environment setup overhead, and resource constraints **Yin et al., "CodeBERT: Pre-trained Model for Programming Tasks"**. In contrast, content-driven reranking methods are far more versatile because they do not rely on execution and are not even confined to coding tasks.\\

\noindent\textbf{Coder reranking.} In prior work, **Yin et al., "CodeBERT: Pre-trained Model for Programming Tasks"** reranked code candidates by estimating \(\text{P}(c \mid i) \), where \( c \) denotes the generated code candidate and \( i \) denotes the given instruction. This process can also be called Coder reranking because the LLM is a mere Coder that estimates the candidate probability based on the corresponding instruction.

When using an LLM to estimate conditional probabilities, we compute the probability of each token iteratively. For example, in Coder reranking, the model processes candidate's tokens from left to right: at each step, it calculates the probability of the current token given the instruction and the previously generated tokens, then appends that token to the context before moving on. The product of these sequential probabilities across all tokens yields the overall probability of the code candidate under the given instruction:

\[
\text{P}(c \mid i_0) \;=\; \prod_{t=1}^{|c|} \text{P}_{\text{LLM}}\bigl(c^{(t)} \mid i_0, c^{(<t)}\bigr),
\]
where $c^{(t)}$ denotes the token at position $t$ in the sequence $c$, and $c^{(<t)}$ represents the sequence of all tokens before position $t$.

\paragraph{CoderReviewer reranking.} **Yin et al., "CodeBERT: Pre-trained Model for Programming Tasks"** added the concept of a reviewer to Coder reranking (i.e.,  reassessing whether the instruction matches the generated code candidate). Formally, CoderReviewer metric is represented as follows: 
\[
\begin{aligned}
\text{CoderReviewer} &=  \text{P}(c \mid i) \cdot \text{P}(i \mid c) \\
&\quad \text{ \footnotesize (Coder) \hspace{1.2em} (Reviewer)}
\end{aligned}
\]

The prompt positions of the instruction and code candidates are reversed in the reviewer's implementation. Thus, the code-generation task is reformulated as an instruction-generation task to calculate the probability of a given instruction. The CoderReviewer metric is also considered a specialized form of maximum mutual information **Bartlett et al., "Maximum Mutual Information for Robust Learning"**, measuring the bidirectional alignment between the generated code and the input instruction.

%When introducing the SpreadNaLa dataset, ____ also explored using an RSA-based reranking method, but its posted performance was lower than the baseline. This outcome suggests that the approach remains in a nascent, prototype stage and merits further investigation and refinement.