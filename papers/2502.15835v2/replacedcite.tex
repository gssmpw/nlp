\section{Related Work}
\label{sec:background}
\textbf{Natural Language to Code.} 
Previous research has extensively explored generating code from natural language using neural network models ____. Recently, large language models (LLMs) have propelled significant advances in this area, driven by the transformer ____ architecture and large-scale pretraining. Their performance on code generation tasks often surpasses that of traditional models, and in many cases even rivals human programmers ____. Moreover, a recent study shows that LLMs also exhibit strong performance in code summarization, effectively translating code snippets into text ____.\\

\noindent\textbf{Diversity Sampling in Code Generation.} In a prior study, ____ found  that allowing the model to generate more candidates significantly raises the probability of yielding a correct answer. This practice, which encourages the model to produce a broader range of potential outputs, is often referred to as diversity sampling.\\

\noindent\textbf{Code Reranking Methods.} When LLMs generate multiple code candidates in response to the user instruction, execution-driven reranking methods 
such as CodeT ____ and AgentCoder ____ involve running test cases to gauge each candidateâ€™s correctness. Although often effective, execution-driven approach can introduce additional safety risks and practical obstacles, such as the potential for malicious code execution, environment setup overhead, and resource constraints ____. In contrast, content-driven reranking methods are far more versatile because they do not rely on execution and are not even confined to coding tasks.\\

\noindent\textbf{Coder reranking.} In prior work, ____ reranked code candidates by estimating \(\text{P}(c \mid i) \), where \( c \) denotes the generated code candidate and \( i \) denotes the given instruction. This process can also be called Coder reranking because the LLM is a mere Coder that estimates the candidate probability based on the corresponding instruction.

When using an LLM to estimate conditional probabilities, we compute the probability of each token iteratively. For example, in Coder reranking, the model processes candidate's tokens from left to right: at each step, it calculates the probability of the current token given the instruction and the previously generated tokens, then appends that token to the context before moving on. The product of these sequential probabilities across all tokens yields the overall probability of the code candidate under the given instruction:

\[
\text{P}(c \mid i_0) \;=\; \prod_{t=1}^{|c|} \text{P}_{\text{LLM}}\bigl(c^{(t)} \mid i_0, c^{(<t)}\bigr),
\]
where $c^{(t)}$ denotes the token at position $t$ in the sequence $c$, and $c^{(<t)}$ represents the sequence of all tokens before position $t$.

\paragraph{CoderReviewer reranking.} ____ added the concept of a reviewer to Coder reranking (i.e.,  reassessing whether the instruction matches the generated code candidate). Formally, CoderReviewer metric is represented as follows: 
\[
\begin{aligned}
\text{CoderReviewer} &=  \text{P}(c \mid i) \cdot \text{P}(i \mid c) \\
&\quad \text{ \footnotesize (Coder) \hspace{1.2em} (Reviewer)}
\end{aligned}
\]

The prompt positions of the instruction and code candidates are reversed in the reviewer's implementation. Thus, the code-generation task is reformulated as an instruction-generation task to calculate the probability of a given instruction. The CoderReviewer metric is also considered a specialized form of maximum mutual information ____, measuring the bidirectional alignment between the generated code and the input instruction.

%When introducing the SpreadNaLa dataset, ____ also explored using an RSA-based reranking method, but its posted performance was lower than the baseline. This outcome suggests that the approach remains in a nascent, prototype stage and merits further investigation and refinement.