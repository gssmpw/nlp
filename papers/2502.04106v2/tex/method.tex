% \begin{figure}[t]
%     \centering
%     \vspace{0.0cm}  %调整图片与上文的垂直距离
%     \setlength{\abovecaptionskip}{0.0cm}   %调整图片标题与图距离
%     \setlength{\belowcaptionskip}{-0.6cm}   %调整图片标题与下文距离
%     \includegraphics[scale=0.12]{figs/overview2.pdf}
%     \caption{General overview of the proposed method FHT.}
%     \label{overview2}
% \end{figure}
% In this section, we introduce a robust detection metric for existing MGLAs and the proposed model poisoning methods for improving attack stealthiness and coverage. We present iD-SNR, a more sensitive detection metric for poisoned parameters. Next, we introduce EGGV, which increases the vulnerability of the gradient space across an entire data batch, thus enhancing attack stealthiness and coverage.
The above-mentioned challenges suggest that instead of controlling $\lambda$ to achieve reconstruction, we should focus on increasing the concentration of input features and enhancing feature representation at the source rather than compressing the features of some samples to amplify others. Following this inspiration, we introduce \name, a new attack that poisons the model parameters $\theta$ to equally enhance the leakage potential of all samples in a batch, thus ensuring a comprehensive attack and evading detection.
% , as existing detection metrics rely on gradient bias
Figure \ref{showchallenges} provides an intuitive comparison between \name and existing Gradient-biased AGLAs. While existing attacks achieve reconstruction by suppressing the features of non-target samples to amplify those of specific ones in the gradient, \name instead uniformly enhances the encoded features of each sample in the gradient, following an entirely different principle.
\begin{figure}[t]
    \centering
    \vspace{0.0cm}  %调整图片与上文的垂直距离
    \includegraphics[scale=0.1]{figs/optimize.pdf}
    % \vspace{-0.1cm}  %调整图片与上文的垂直距离
    \caption{Fundamental principle comparison between \name and gradient-biased AGLAs.}
    % \vspace{-0.2cm}  %调整图片与上文的垂直距离
    \label{showchallenges}
\end{figure}
\begin{figure*}[t]
    \centering
    \vspace{0.0cm}  %调整图片与上文的垂直距离
    \includegraphics[scale=0.23]{figs/overview1.pdf}
    % \vspace{-0.1cm}  %调整图片与上文的垂直距离
    \caption{Overview of the proposed \name, consisting of three steps: \ding{172} poison the model parameters to make its gradient space vulnerable; \ding{173} distribute poisoned model and gather vulnerable gradients; \ding{174} implement existing GLAs on these gradients.}
    % \vspace{-0.1cm}  %调整图片与上文的垂直距离
    \label{EGGVoverview}
\end{figure*}
\begin{algorithm}
\caption{Poisoning and Reconstruction of \name}
\label{algorithm1}
\begin{algorithmic}[1]
\State \textbf{Input:} Auxiliary dataset $D_a$, global model $F(\theta)$, acceptable error $\epsilon$, number of iterations $N$
\State \textbf{Output:} Reconstructed data $x^{\prime}$

\State \textbf{Main Process:}
\State $F(\theta^*) \gets$ \texttt{PoisonModel}($D_a, F(\theta), N$)
\State $\nabla \theta^* \gets$ \texttt{CollectClientGradients}($F(\theta^*)$)
\State $x^{\prime} \gets$ \texttt{ReconstructData}($\nabla \theta^*$)
\State \Return $x^{\prime}$

\Function{PoisonModel}{$D_a, F(\theta), N$}
\State Initialize $\theta_{0}$ randomly
\State $t \gets 0$
\While{$L(\theta_{t}, \phi_{t}) < \epsilon$}
    \For{each $(x_j, y_j) \in D_a$}
        \State $L(\theta_{t}, \phi_{t}) \gets \left\|x_j - \mathcal{D}\left(\Pi\left(\frac{\partial \ell(F(x_j, \theta_{t}), y_j)}{\partial \theta_{t}}\right), \phi_{t}\right) \right\|_2^2$
        % \State \resizebox{.95\linewidth}{!}{$
        % L(\theta_t, \phi_t) \gets \left\|x_j - \mathcal{D}\left(\Pi\left(\nabla_{\theta_t} \ell(F(x_j, \theta_t), y_j)\right), \phi_t\right)\right\|_2^2
        % $}
        \State Update $\theta_{t}, \phi_{t}$ using gradient descent:
        \State \quad $\theta_{t+1} \gets \theta_{t} - \alpha_1 \nabla_{\theta_{t}} L(\theta_{t}, \phi_{t})$
        \State \quad $\phi_{t+1} \gets \phi_{t} - \alpha_2 \nabla_{\phi_{t}} L(\theta_{t}, \phi_{t})$
        \State $t \gets t + 1$
    \EndFor
\EndWhile
\State \Return $F(\theta^*)$ with updated $\theta$
\EndFunction

\Function{CollectClientGradients}{$F(\theta^*)$}
    \State Client $i$ receives the global model $F(\theta^*)$
    \State Client $i$ calculates the gradient $\nabla_{\theta^*} \ell(F(x, \theta^*), y)$ using its data $(x, y)$
    \State \Return $\nabla_{\theta^*} \ell(F(x,\theta^*), y)$
\EndFunction

\Function{ReconstructData}{$\nabla \theta^*$}
    \State Select any prior PGLA methods $R(\cdot)$
    \State Reconstruct client data $x^{\prime}$ through $R(\nabla \theta^*)$
    \State \Return reconstructed data $x^{\prime}$
\EndFunction

\end{algorithmic}
\end{algorithm}
% Unlike biased AGLAs that amplify gradients of specific samples, \name disperses information uniformly, mimicking natural learning gradients and thus reducing detectability.

% \subsection{iD-SNR for Enhanced Detectability of Poisoned Model Parameters}
% Previous works \cite{Garov2024Hiding} show that existing MGLAs are often easily detected by clients through principled checks. D-SNR \cite{Garov2024Hiding} is currently the only detection metric for such malicious model modifications. Its definition is provided in Equation (\ref{dsnr}). Its effectiveness stems from the fact that if model parameters are poisoned to favor certain specific samples, then the gradients corresponding to these samples will be significantly larger than others. However, as the batch size increases, the effectiveness of D-SNR may diminish. This is because the gradients of more samples are included in the calculation, leading to an increase in the overall gradient sum. As a result, the prominence of the gradients of malicious samples is diluted, thereby masking the malicious modification of model parameters.

% To address this limitation, we propose a novel detection metric:
% \begin{equation}
% \small
%     \mathrm{iD-SNR}(\theta)=\left\|1-\max_{w\in\theta_{lw}}\frac{max_{i\in\{1,...,B\}}\left\|\frac{\partial \ell(x_i,y_i)}{\partial W}\right\|}{\frac1B\sum_i^B\left\|\frac{\partial \ell(x_i,y_i)}{\partial W}\right\|}\right\|^2,
% \end{equation}
% Unlike D-SNR, iD-SNR is designed to be independent of batch size and focuses solely on the model parameters. Specifically, iD-SNR evaluates the degree of abnormality by comparing the ratio of the maximum gradient and the average gradient within a batch with 1. A ratio close to 1 suggests a small difference between the maximum gradient and the average gradient, indicating a uniform gradient distribution of each sample of the data batch with no clear evidence of malicious modifications. In contrast, a ratio significantly greater than 1 indicates a significant deviation in the gradients of the model, pointing to potential manipulations in the model.

% 先用一个小节讨论隐蔽性，先前的攻击为什么不隐蔽，先前提出的指标为什么不好，为什么就没效果，然后我提出了我的方法和我的指标

% \subsection{Enhanced Global Gradient Vulnerability (EGGV) through Model Poisoning}
\subsection{Attack Overview}
The proposed \name is comprised of three main steps. Algorithm \ref{algorithm1} shows the detailed process. Figure \ref{EGGVoverview}  provides a visual overview of \name.
% \ding{172} model parameter poisoning, \ding{173} global model distribution, and \ding{174} data reconstruction. 

\textbf{Step I:} The malicious server poisons the global model before its distribution (Step \ding{172} in Figure \ref{EGGVoverview}, \texttt{PoisonModel} in Algorithm \ref{algorithm1}). In this stage, the server iteratively optimizes the global model and discriminator locally with the objective function $L(\theta, \phi)$ using an auxiliary dataset, enriching the gradients with encoded features.

\textbf{Step II:} The server distributes the poisoned model to clients (Step \ding{173} in Figure \ref{EGGVoverview}, \texttt{CollectClientGradients} in Algorithm \ref{algorithm1}). Each client, following the FL protocol, feeds its own training data into the poisoned model to generate gradients that are then uploaded back to the server.

\textbf{Step III:} The server uses these uploaded gradients to perform data reconstruction using any existing PGLAs (Step \ding{174} in Figure \ref{EGGVoverview}, \texttt{ReconstructData} in Algorithm \ref{algorithm1}).
% As shown in Figure \ref{EGGVoverview}, the process begins with server poisoning the global model (Step \ding{173} in Figure \ref{EGGVoverview}, \texttt{CollectClientGradients} function in Algorithm \ref{algorithm1}).

% In the model poisoning stage, the attacker iteratively optimizes the global model and discriminator locally with the objective function $L(\theta, \phi)$ using an auxiliary dataset. This enables the model to encode richer features into its gradients. In the data reconstruction stage, the server distributes the poisoned global model to the victim client. The victim client, following the federated learning protocol, feeds its own training data into the global model to generate gradients that are then uploaded back to the server. The server then uses these gradients to perform data reconstruction through any existing HGLAs. Algorithm \ref{algorithm1} presents the detailed steps of the EGGV. Figure \ref{EGGVoverview}  provides a visual overview of EGGV.
\subsection{Problem Formulation}
%观察到D-SNR依赖于梯度偏向进行检测后，这激发了我们
% Recent work \cite{du2024sok} shows that model parameters play a crucial role in the success of GLAs. In particular, the training stage of the model (e.g., trained or untrained) is commonly identified as the most critical factor for the success of GLAs. Inspired by this, we propose that an adversary can pre-poison the model parameters to increase data vulnerability in the gradient space, thereby gaining control over the gradient leakage potential. 
The objective of the malicious server is to minimize the difference between the reconstructed data and the original data. Formally, we have the following objective:
\begin{equation}
    \theta^*=\underset{\theta}{\operatorname*{argmin}}\|x-x^{\prime}\|_p,
\end{equation}
where $x$ represents one data batch from the auxiliary dataset $D$, and $x^{\prime}$ denotes the reconstructed data obtained by the attacker using the gradient leakage method $R(\cdot)$. These gradients are computed on the client side during local training. 
Specifically, the client inputs local training data $x$ into the model, yielding the output $\hat{y}=F(x,\theta)$, 
% . The client then computes the loss $\ell(F(x,\theta),y)$ between the model outputs and the ground-truth labels $y$, and subsequently 
and then calculates the gradient $\frac{\partial \ell(F(x,\theta),y)}{\partial\theta}$. The optimization objective can therefore be expanded as follows:
\begin{equation}
\label{poisonform}
    \theta^*=\arg\min_\theta\|x-R\left(\frac{\partial \ell(F(x,\theta),y)}{\partial\theta}\right)\|_p.
\end{equation}
The server aims to optimize the model parameters $\theta$ such that the reconstructed data obtained via the gradient leakage method $R(\cdot)$, closely approaches the original input data $x$.

\subsection{Detailed Solution}
As shown in Equation~\eqref{poisonform}, the performance of the gradient leakage attack depends on the model parameters, meaning there exists an optimal set of model parameters, denoted as $\theta^\ast$, that minimizes the reconstruction loss between the original data $x$ and the recovered data $x'$ given a specific reconstruction function $R(\cdot)$. 

To search in the continuous parameter space for the optimal model parameters, 
% that maximize the vulnerability of gradient space to data leakage. Given the high dimensions of gradients and the varying dimensions across model layers, 
we introduce a dimension reduction projector $\prod(\cdot)$. 
% This projector performs sparse sampling at fixed positions for the gradients of each layer, ensuring that the gradients are selected from consistent positions during the iterations. 
% The sampled gradients are then concatenated into a lower-dimensional vector. 
Specifically, for a data batch $x$, the gradients $g=\nabla_\theta\ell(F(x,\theta),y)$ on the global model are sampled by the projector $\prod(\cdot)$ at fixed positions, ensuring that the gradients have consistent positions during the iterations:
\begin{equation}
    \Pi(g)=(g_{1}[p_1],...,g_{L}[p_L], \rho)^T,
\end{equation}
where $p_1$, $\ldots$, $p_L$ represent pre-specified sets of gradient positions from the $1^{st}$ to the $L^{th}$ layer, indicating the fixed positions of the gradients sampled during each iteration. $\rho$ represents the ratio of the number of parameters of the projected gradient to that of the original gradient, and we hereinafter refer to it as the projection ratio. By applying the projector $\Pi(\cdot)$, we map the high-dimensional gradient space into a lower-dimensional vector space as follows:
\begin{equation}
    \tilde{g}=\Pi\left(\frac{\partial \ell(F(x,\theta),y)}{\partial\theta}\right).
\end{equation}
With the projected gradient, we introduce a discriminator $\mathcal{D}(\cdot)$ to evaluate the potential for gradient leakage by the projected gradient. 
Equation~\eqref{poisonform} measures the vulnerability of the corresponding gradient space by performing an end-to-end reconstruction attack to compute the similarity between the reconstructed and original data. 
Traditional end-to-end reconstruction attacks require performing gradient matching in Equation (\ref{dlg}) and then optimizing on dummy data. This method has a high computational cost, as it requires iterative operations in the continuous gradient space for each update of $\theta$. Additionally, iterative reconstruction introduces a nested optimization structure, making it challenging to compute the second-order derivative of the loss.

% To overcome this, we propose an approximate method that evaluates the potential for gradient leakage by decoding the projected gradient. 
Prior GLAs suggest that the gradient vulnerability stems from the data features they encode. The more data features a gradient contains, the more vulnerable that gradient becomes. To this end, \textit{the gradients can be encoded representations of input data}, while the forward and backward propagation within the model is the encoding process.

The goal of the proposed \name is to refine this encoding process to maximize the retention of input data features within the gradients. 
To achieve this, we design a decoder that decodes the projected gradients to the original input. 
The attacker jointly optimizes the model and the decoder by the following loss function:
\begin{equation} \label{eq:decoder}
    L(\theta, \phi) = \left\|x - \mathcal{D}\left(\Pi\left(\frac{\partial \ell(F(x, \theta), y)}{\partial \theta}\right), \phi\right) \right\|_2^2,
\end{equation}
where $\phi$ represents the parameters of the decoder $\mathcal{D}$.
% We observe an interesting phenomenon: the simpler the structure of the discriminator, the more readily the gradients of the global model can be poisoned to its vulnerable points. A plausible explanation for this is that the discriminator $\mathcal{D}$ evaluates gradient vulnerability by decoding the projected gradients and comparing the reconstructed data with the original input. This similarity is a joint contribution of the model parameters and the discriminator's capacity. When the decoder is overly powerful, the global model acting as an encoder may become lazy and not strive to learn to produce gradients with rich data features. Therefore, when the decoder is simple enough, the global model will work harder to adjust its parameters to learn to produce gradients with rich data features, thus driving the model toward its most vulnerable gradient point. Hence, we used a simple fully connected layer as $\mathcal{D}$ in our experiments, and the results demonstrating this phenomenon are presented in the experimental evaluation section.

% We observe an interesting phenomenon that the discriminator can be employed to evaluate the gradient space vulnerability of model parameters that have not been seen before. In previous research, evaluating the gradient vulnerability often depends on end-to-end gradient reconstruction, a process requiring complex and time-consuming optimization steps. In contrast, our method projects the model's gradients into a lower-dimensional space, where the discriminator evaluates and scores the projected gradients to predict the likelihood of a successful attack. This approach offers an efficient means of assessing attack viability without the computational expense of end-to-end gradient reconstruction.
% Table generated by Excel2LaTeX from sheet 'Sheet3'

% \begin{table*}[t]
%   \centering
%   \vspace{0.0cm}
%   \caption{Performance comparison of the proposed EGGV against baseline model initializations Random, Xavier, and He on CIFAR10, CIFAR100, and TinyImageNet datasets.}
%     \begin{tabularx}{\linewidth}{ccccccccccccc}
%     \toprule
%     \multirow{2}[4]{*}{Method} & \multicolumn{3}{c}{CIFAR10} &       & \multicolumn{3}{c}{CIFAR100} &       & \multicolumn{3}{c}{TinyImageNet} \\
% \cmidrule{2-4}\cmidrule{6-8}\cmidrule{10-12}          & PSNR $\uparrow$  & SSIM $\uparrow$ & LPIPS $\downarrow$ &       & PSNR $\uparrow$  & SSIM $\uparrow$  & LPIPS $\downarrow$ &       & PSNR $\uparrow$  & SSIM $\uparrow$  & LPIPS $\downarrow$ \\
%     \midrule
%     Random+iDLG & 15.8685  & 0.5955  & 0.2896  &       & 16.9960  & 0.5372  & 0.3441  &       & 14.0472  & 0.2229  & 0.5755  \\
%     Xavier+iDLG & 20.7717  & 0.7864  & 0.2469  &       & 19.8529  & 0.7310  & 0.2683  &       & 12.1854  & 0.2305  & 0.5859  \\
%     He+iDLG & -1.1551  & -0.0005  & 0.7238  &       & -1.9460  & -0.0017  & 0.7527  &       & -1.0535  & -0.0004  & 0.8001  \\
%     EGGV(Ours)+iDLG & \textbf{29.7010}  & \textbf{0.8649}  & \textbf{0.1081}  &       & \textbf{28.4425}  & \textbf{0.8970}  & \textbf{0.0906}  &       & \textbf{19.9437}  & \textbf{0.6267}  & \textbf{0.2210}  \\
%     \midrule
%     Random+IG & 19.1621  & 0.6219  & 0.3076  &       & 19.3464  & 0.6383  & 0.3140  &       & 15.4700  & 0.2563  & 0.5208  \\
%     Xavier+IG & 24.4702  & 0.8633  & 0.1444  &       & 23.1615  & 0.8044  & 0.1723  &       & 13.0624  & 0.2185  & 0.5737  \\
%     He+IG & 13.3073  & 0.1019  & 0.6242  &       & 10.9789  & 0.0907  & 0.6628  &       & 12.7034  & 0.2088  & 0.7256  \\
%     EGGV(Ours)+IG & \cellcolor{cyan!25}\textbf{31.9651}  & \textbf{0.9166}  & \textbf{0.0735}  &       & \textbf{31.5515}  & \textbf{0.9267}  & \textbf{0.0617}  &       & \textbf{28.6232}  & \textbf{0.9140}  & \textbf{0.0757}  \\
%     \bottomrule
%     \end{tabularx}%
%   \label{compare2}%
%   \vspace{-0.3cm}
% \end{table*}%

\subsection{Optimal Model for the Gradient Leakage}
We prove a global minimum exists for the proposed loss function, where the corresponding model parameters maximize the vulnerability of gradient space to data leakage.
\begin{assumption}
\label{assumption1}
The global model $F(\theta)$ is continuous, and the parameter space $\Theta$ of $\theta$ is a non-empty compact set.
\end{assumption}
\begin{assumption}
\label{assumption2}
The loss function $\ell(\cdot, \cdot)$ for the client training is continuously differentiable with respect to the model parameters $\theta$, allowing for gradient computation with respect to $\theta$.
\end{assumption}
\begin{assumption}
\label{assumption3}
The decoder $\mathcal{D}$ is a linear function of the form $\mathcal{D}(\tilde{g})=W\cdot\tilde{g}+b$ and is continuous. The parameter space $\Phi$ of $\phi$ is a non-empty compact set.
\end{assumption}
\begin{theorem}
\label{theorem1}
Under the above assumptions, there exists parameters $\theta^*\in\Theta$, $\phi^*\in\Phi$ such that the loss function $L(\theta, \phi)$ defined in Equation~\eqref{eq:decoder} attains its global minimum:
\begin{equation}
    \theta^*, \phi^*=\arg\min_{\theta\in\Theta, \phi\in\Phi}L(\theta, \phi).
\end{equation}
\end{theorem}
% \begin{proof}
At $\theta=\theta^{*}, \phi=\phi^{*}$, the gradient $\nabla_\theta \ell(F(x,\theta),y)$ encodes the maximum amount of feature from the input data $x$, making the gradient space most susceptible to leakage. 
\begin{proof}
\textbf{Continuity of $L(\theta, \phi)$.}
From Assumption (\ref{assumption2}), since $\ell(\cdot,\cdot)$ is continuously differentiable with respect to $\theta$, the gradient $\nabla_\theta \ell(F(x,\theta),y)$ is continuous with respect to $\theta$. The projector $\Pi$ is a fixed-position sparse sampling linear operator, so the composite function $\Pi(\nabla_\theta l(F(x,\theta),y))$ is also continuous with respect to $\theta$. By Assumption \ref{assumption3}, the decoder $\mathcal{D}$ is continuous. Therefore, the composite function $\mathcal{D}(\Pi(\nabla_\theta l(F(x,\theta),y)))$ is continuous with respect to $\theta$ and $\phi$. The squared Euclidean norm $\|\cdot\|_2^2$ is a continuous. Hence, the loss function $L(\theta, \phi)$ is continuous with respect to $\theta$ and $\phi$.

\textbf{Existence of Global Minimum.}
From Assumption (\ref{assumption1}) and Assumption (\ref{assumption3}), the parameter space $\Theta$ and $\Phi$ is a non-empty compact set. By the Weierstrass Extreme Value Theorem \cite{rudin1976principles, bartle2011introduction}, any continuous function on a compact set attains its maximum and minimum values. Therefore, there exists ${\theta}^* \in \Theta$ and ${\phi}^* \in \Phi$ such that $\theta^*, {\phi}^*=\arg\min_{\theta\in\Theta, \phi\in\Phi}L(\theta, \phi)$.

\textbf{Maximum Vulnerability of the Gradient Space.}
At $\theta = \theta^{*}$ and $\phi = \phi^{*}$, the loss function $L(\theta, \phi)$ attains its global minimum, which indicates that the reconstruction error is minimized. This indicates that the encoding and decoding processes of the gradient have reached an optimal state. If a better decoder or gradient construction existed, it would further reduce $L(\theta, \phi)$, contradicting the minimality of $L(\theta^{*}, \phi^{*})$. Therefore, at $\theta = \theta^{*}$ and $\phi = \phi^{*}$, the risk of data leakage from the gradient space to the input data $x$ is maximized. This means the gradient $\nabla_\theta \ell(F(x,\theta^*),y)$ contains the most features of $x$, rendering the gradient space most vulnerable.
\end{proof}

\subsection{Theoretical Guarantee of Stealthiness}
\label{sec:stealthy-proof}

In the previous subsection, we established the existence of optimal poisoned parameters $(\theta^*, \phi^*)$ minimizing the poisoning loss in Equation~\eqref{eq:decoder}. Here we theoretically demonstrate that gradients derived from $\theta^*$ exhibit strong stealthiness, undermining detection methods such as D-SNR~\cite{Garov2024Hiding}.

Stealthiness can be quantified by the variance of gradient norms within a batch. Let $g_i$ be the gradient norm of the $i$-th sample using optimal parameters $\theta^*$:
\begin{equation}
g_i = \left\|\frac{\partial \ell(F(x_i; \theta^*), y_i)}{\partial \theta^*}\right\|_2.
\end{equation}

We define stealthiness as minimizing the variance of $g_i$:
\begin{equation}
\mathrm{Var}(g_i) = \frac{1}{B}\sum_{i=1}^{B}(g_i - \mu_g)^2, \quad \text{with} \quad \mu_g = \frac{1}{B}\sum_{i=1}^{B} g_i.
\end{equation}

\begin{theorem}[Gradient Uniformity and Stealthiness]
Under Assumptions~\ref{assumption1}--\ref{assumption3}, the optimal poisoned parameters $(\theta^*, \phi^*)$ ensure that the gradient variance is bounded by a small constant $\epsilon$, leading to bounded D-SNR indistinguishable from naturally trained gradients:
\begin{equation}
\mathrm{Var}(g_i) \leq \epsilon, \quad D-SNR(\theta^*) \leq \gamma\cdot\epsilon,
\end{equation}
where $\gamma>0$ is a constant dependent on the batch size and model structure.
\end{theorem}

\begin{proof}
At the optimal $(\theta^*,\phi^*)$, the reconstruction loss (Equation~\eqref{eq:decoder}) attains its minimum, ensuring uniform reconstruction errors across the batch:
\begin{equation}
\|x_i - x'_i\|_2 \leq \delta, \quad \forall i,
\end{equation}
for a small constant $\delta$. Given the Lipschitz continuity \cite{rudin1976principles} of decoder $\mathcal{D}$ with constant $L_D$, we have:
\begin{equation}
\resizebox{\linewidth}{!}{$
\|\Pi(\nabla_{\theta^*}\ell(F(x_i,\theta^*),y_i))-\Pi(\nabla_{\theta^*}\ell(F(x_j,\theta^*),y_j))\|_2 \leq 2L_D^{-1}\delta.
$}
\end{equation}

Since the projector $\Pi$ preserves norm differences up to a constant factor $M_\Pi$, we derive:
\begin{equation}
|g_i - g_j| \leq 2M_\Pi L_D^{-1}\delta, \quad \forall i,j.
\end{equation}

Thus, gradient variance satisfies:
\begin{equation}
\mathrm{Var}(g_i) \leq 4(M_\Pi L_D^{-1})^2\delta^2 := \epsilon.
\end{equation}

As D-SNR monotonically decreases with gradient variance~\cite{Garov2024Hiding}, we conclude:
\begin{equation}
D-SNR(\theta^*)\leq \gamma\cdot\epsilon,
\end{equation}
establishing that gradients from $\theta^*$ remain indistinguishable from naturally trained models, ensuring stealthiness.
\end{proof}


% \begin{proof}
% The proof is in Appendix \ref{proof2}.
% \end{proof}
% \end{proof}

