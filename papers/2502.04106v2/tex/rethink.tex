We introduce a new approach for AGLA analysis. It
% deepens the understanding of MGLAs and 
offers a deep insight into the relationship between model parameters and gradient bias, and explains why existing AGLAs are detectable and cannot recover all samples in a batch. 
% The adversary may also leverage publicly accessible model architectures, such as GANs, to further enhance the effectiveness of data reconstruction.

% \subsection{Formulation of Gradient Leakage in FL}
% We begin with the simplest classification model consisting of FC layers. 
For a simple neural network that is only comprised of fully connected layers $F(x)=xW+b$, where $x\in\mathbb{R}^{B\times m}$ is a batch of data, $W\in\mathbb{R}^{m \times n}$ is the weight parameters. $b\in\mathbb{R}^{1 \times n}$ is the bias, with $B$ being the batch size and $n$ being the number of classification categories. When data $x$ is fed into the model, the output is represented by $\hat{y}=xW+b$. 
As seen in prior work \cite{fowl2021robbing}, the gradients of the weights and biases of the FC layer can be directly used to reconstruct a weighted average of the input data:
\begin{equation}
\label{avgx}
    \bar{x}^{(k)}=\frac{\nabla_{W^k}\ell(F(x,\theta),y)}{\nabla_{b^k}\ell(F(x,\theta),y)}=\sum_{i=1}^B\lambda_i^k\cdot x_i,
\end{equation}
where $k\in[1,n]$ is the class index, $\nabla_{W^k}\ell(F(x,\theta),y)$ (abbreviated as $\nabla W^{k}$) denotes the gradient of the $k^{th}$ column of the weight matrix $W$, and $\nabla_{b^k}\ell(F(x,\theta),y)$ (abbreviated as $\nabla b^{k}$) represents the gradient of the $k^{th}$ element of the bias $b$. 
% The following theorem defines 
$\lambda$ is defined as below.
% and outlines the constraints on the relationships within $\lambda$.

\begin{theorem}
\label{lambda}
    Let $F(x)=xW+b$ be the classification model with one FC layer, where $x$ is the input data, $W$ is the weight matrix, and $b$ is the bias vector, and the corresponding model output is $\hat{y}=F(x)$. Suppose $\ell(\hat{y}, y)$ is the loss function between the model output $\hat{y}$ and the ground-truth labels $y$. For any class index $k\in\{1, 2, ..., C\}$ and sample index $i\in\{1, 2, ..., B\}$, the coefficient $\lambda$ holds that:
    \begin{equation}
    \lambda_i^k=\frac{\frac{\partial \ell(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}}{\sum_{j=1}^B\frac{\partial \ell(\hat{y}_j^k,y_j)}{\partial\hat{y}_j^k}}, and \sum_{i=1}^B\lambda_i^k=1,
    \end{equation}
    where ${\partial \ell(\hat{y}_i^k,y_i)}/{\partial\hat{y}_i^k}$ denotes the partial derivative of the loss function with respect to the output $\hat{y}_i^k$.
\end{theorem}
\begin{proof}
    Consider a batch of $B$ samples $\{(x_i,y_i)\}_{i=1}^B$, where $x\in\mathbb{R}^{B\times\text{Channel}\times\text{Height}\times\text{Width}}$ represents the input data and $y_i$ are the corresponding ground-truth labels. The model outputs for each sample within a batch are given by: $\hat{y}_i=x_iW+b\in\mathbb{R}^C$, where $C$ denotes the number of classification classes. The total loss over the batch is $\frac1B\sum_{i=1}^B\ell(\hat{y}_i,y_i)$. Next, we derive the gradients of weights and biases for the $k^{th}$ class and express the ratio $\frac{\nabla W^k}{\nabla b^k}$ in terms of $\lambda_{i}^{k}$ and $x_i$. The gradient of the weight matrix $W$ with respect to the loss for class index $k$ is given by the average of the gradients over all samples:
    \begin{equation}
    \nabla W^k=\frac1B\sum_{i=1}^B\nabla W_i^k.
    \end{equation}
    According to the chain rule, we can further obtain:
    \begin{equation}
    \begin{aligned}
    \nabla W^{k}& =\frac1B\sum_{i=1}^B\nabla W_i^k=\frac1B\sum_{i=1}^B\frac{\partial l(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}\cdot\frac{\partial\hat{y}_i^k}{\partial W_i^k} \\
    &=\frac1B\sum_{i=1}^B\frac{\partial l(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}\cdot x_i.
    \end{aligned}
    \end{equation}
    Similarly, the gradient of the bias corresponding to the $k^{th}$ category index can be derived as:
    \begin{equation}
    \begin{gathered}
    \nabla b^{k}=\frac1B\sum_{i=1}^B\nabla b_i^k=\frac1B\sum_{i=1}^B\frac{\partial l(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}\cdot\frac{\partial\hat{y}_i^k}{\partial b^k} \\
    =\frac1B\sum_{i=1}^B\frac{\partial l(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}\cdot1
    =\frac1B\sum_{i=1}^B\frac{\partial l(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}.
    \end{gathered}
    \end{equation}
    Therefore, $\nabla W^k/\nabla b^k$ can be derived as:
    \begin{equation}
    \resizebox{\linewidth}{!}{$
    \begin{aligned}
    \frac{\nabla W^k}{\nabla b^k}& =\frac{\frac1B\sum_{i=1}^B\frac{\partial l(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}\cdot x_i}{\frac1B\sum_{i=1}^B\frac{\partial l(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}}=\frac{\sum_{i=1}^B\frac{\partial l(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}\cdot x_i}{\sum_{i=1}^B\frac{\partial l(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}} \\
    &=\frac{\frac{\partial l(\hat{y}_{1}^{k},y_{1})}{\partial\hat{y}_{1}^{k}}\cdot x_{1}}{\sum_{i=1}^{B}\frac{\partial l(\hat{y}_{i}^{k},y_{i})}{\partial\hat{y}_{i}^{k}}}+\frac{\frac{\partial l(\hat{y}_{2}^{k},y_{2})}{\partial\hat{y}_{2}^{k}}\cdot x_{2}}{\sum_{i=1}^{B}\frac{\partial l(\hat{y}_{i}^{k},y_{i})}{\partial\hat{y}_{i}^{k}}}+\cdots+\frac{\frac{\partial l(\hat{y}_{B}^{k},y_{B})}{\partial\hat{y}_{B}^{k}}\cdot x_{B}}{\sum_{i=1}^{B}\frac{\partial l(\hat{y}_{i}^{k},y_{i})}{\partial\hat{y}_{i}^{k}}} \\
    &=\sum_{i=1}^{B}\frac{\frac{\partial l(\hat{y}_{i}^{k},y_{i})}{\partial\hat{y}_{i}^{k}}}{\sum_{j=1}^{B}\frac{\partial l(\hat{y}_{j}^{k},y_{j})}{\partial\hat{y}_{j}^{k}}}\cdot x_{i}.
    \end{aligned}
    $}
    \end{equation}
    This expression can be rewritten as:
    \begin{equation}
    \frac{\nabla W^k}{\nabla b^k}=\sum_{i=1}^{B}\frac{\frac{\partial l(\hat{y}_{i}^{k},y_{i})}{\partial\hat{y}_{i}^{k}}}{\sum_{j=1}^{B}\frac{\partial l(\hat{y}_{j}^{k},y_{j})}{\partial\hat{y}_{j}^{k}}}\cdot x_{i}=\sum_{i=1}^B\lambda_i^k\cdot x_i.
    \end{equation}
    Therefore, $\lambda_i^k={\frac{\partial \ell(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}}/{\sum_{j=1}^B\frac{\partial \ell(\hat{y}_j^k,y_j)}{\partial\hat{y}_j^k}}, and \sum_{i=1}^B\lambda_i^k=1.$
\end{proof}
% \begin{proof}
% See our proof in Appendix \ref{proof1}.
% \end{proof}

Taking a binary classification network with an input of 4 samples as an example, the weighted average sample obtained by the gradient of the weights and biases of the two categories can be expressed as:
\begin{equation}
\label{lambdaexample}
    \begin{bmatrix}\frac{\nabla W^1}{\nabla b^1}\\\frac{\nabla W^2}{\nabla b^2}\end{bmatrix}=\begin{bmatrix}\bar{x}^{(1)}\\\bar{x}^{(2)}\end{bmatrix}=\begin{bmatrix}\lambda_1^1&\lambda_2^1&\lambda_3^1&\lambda_4^1\\\lambda_1^2&\lambda_2^2&\lambda_3^2&\lambda_4^2\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\\x_4\end{bmatrix}=\mathbf{\Lambda} \mathbf{X}.
\end{equation}
Equation (\ref{lambdaexample}) shows the weighted average data resolved by the gradient of the weights and biases $w.r.t$ a given class, which is actually a weighted summation of the features of the input layer. $\lambda$ is exactly the weight factor to quantify the bias in the neurons' gradient space toward specific samples within a batch.

$\lambda$ is crucial for the gradient-biased AGLAs, as it controls the weighted feature proportions computed from gradients. It is the first technical measure to quantify the relative contribution of each sample in the batch to the activation of each class of neurons in the model. Interestingly, we find the AGLAs are very analogous to poisoning-based backdoor attacks. In the later, the attacker poisons the model training process to an expected state to manipulate the model output and eventually control the distribution of $\lambda$. Therefore, we call this approach the backdoor-theoretical perspective.
% for manipulating gradient space bias: control model parameters → control model outputs → control $\lambda$ → control the weighted feature proportions computed from gradients. 

% model output is also called a backdoor attack, so we call this perspective the backdoor-theoretical perspective. 
% This framework suggests that many backdoor attacks could potentially be adapted for gradient leakage attacks. 
% All existing gradient-biased AGLAs operate through this pipeline: while they do not directly modify the model's output, they indirectly influence the output to control $\lambda$, thereby biasing the gradient space. However, prior work failed to formalize this bias quantitatively.
The core mechanism of manipulating $\lambda$ inherently results in several challenges: (1) samples without gradient space bias cannot be reconstructed; (2) reconstruction becomes impossible when two samples with the required properties coexist; and (3) the presence of anomalous gradients in the gradient space makes the attack detectable. This explains the fundamental limitations of existing AGLAs. 

This is the first theoretical analysis for AGLAs, which systematically reveals the underlying mechanism behind gradient bias via the backdoor-theoretical lens. It not only bridges a critical gap in the current literature but also provides principled insights into why existing AGLAs are inherently incomplete in reconstruction and detectable due to gradient anomalies.
% This framework lays a solid theoretical foundation for the future design of both more effective attacks and robust defenses.

% The weights 

% As shown in Definition~\ref{lambda}, $\lambda$ solely depends on the model outputs and the ground-truth labels.
% Given fixed labels, an adversary can manipulate the model parameters to control its output, thereby re-distributing $\lambda$ to a specific proportion. 
% This enables the targeted leakage of samples with specific properties where these properties function as triggers in backdoor attacks.\todo{It's hard to understand, could you re-write the above two sentences and make them more clear.} 
% In addition, Definition~\ref{lambda} sheds light on why MGLAs can control model parameters to leak samples with specific properties and why MGLAs is inherently detectable.

% \subsection{Fast Hidden Targeted Gradient Leakage Attack via Neuron-Level Poisoning}
% To track the challenge of attack stealthiness, we propose FHT, an innovative framework for MGLAs that selectively exposes the most sensitive samples in a batch while maintaining stealthy. To make our method easier to understand, we will first introduce our attack technique on fully connected (FC) layers and then extend it to convolutional neural networks (CNNs).

% \subsubsection{FHT for FC Model}
% Based on Equation (\ref{avgx}),  we conclude that for any category index $k$, the gradients of weights and biases can be used to calculate the weighted average of the input data. The weights $\lambda$ can be calculated as $\lambda_i^k={\frac{\partial \ell(\hat{y}_i^k,y_i)}{\partial\hat{y}_i^k}}/{\sum_{j=1}^B\frac{\partial \ell(\hat{y}_j^k,y_j)}{\partial\hat{y}_j^k}}$ according to Theorem \ref{lambda} and they depend solely on the model outputs and the ground-truth labels. 


% Based on this observation, we propose a method where the server can poison the model to inject a backdoor, making the model outputs match the target values the attackers expected. Consequently, the weighted average data obtained through the gradients of weights and biases closely approximates the target sample. The optimization objective for model poisoning is given as follows:
% \begin{equation}
% \label{poisonobject}
%     \mathcal{L}oss=\left\|\bar{x}^{(k)}-x_p\right\|_2^2=\left\|\frac{\nabla W^k}{\nabla b^k}-x_p\right\|_2^2,
% \end{equation}
% In an increasing number of FL scenarios, clients tend to combine private data with publicly accessible data on the internet, making attackers more interested in the client's private data. Thus, $p \in [1, B]$ denotes the index of the most sensitive data within the batch. This optimization goal aims to activate the $k^{th}$ neuron only for the most sensitive data. In essence, we propose poisoning a specific class neuron to insert a backdoor in the model, enabling control over the outputs for that class and selectively leaking the most sensitive data within the batch. Compared to existing active attacks, our method is more stealthy as it only requires poisoning a single class neuron instead of the entire model.

% \subsubsection{Extension of FHT for CNNs}
% Since FHT is designed for poisoning neurons in FC layers, it cannot be directly applied to CNNs. In a CNN, the inputs first pass through the convolutional layer to be transformed into feature $f$ before being input into the FC layer. To adapt FHT for CNNs, we propose integrating it with Feature Inversion Attacks (FIA), which enables us to reverse the feature maps back into the input data. Figure \ref{overview2} illustrates this workflow. FHT attacks CNNs through the following two steps:

% \textbf{1) Poisoning FC neurons and inverting feature maps.} Poison the neurons of the $k^{th}$ category in the FC layer using FHT and train a generator GAN to invert the input feature $f$ back to the model inputs. Building on the poisoning objective in Equation (\ref{poisonobject}), we define the following optimization objective:
% \begin{equation}
%     \mathcal{L}oss(\theta_{conv},W,b)=\left\|\frac{\nabla W^k}{\nabla b^k}-F_{conv}(x_p)\right\|_2^2+\left\|G(F_{conv}(x))-x\right\|_2^2,
% \end{equation}
% where $p \in [1, B]$ represents the index of the most private data in a batch, $\theta_{conv}$ denotes the parameters of the convolutional part of the global model $\theta$, and $G(\cdot)$ is the generator network. We take the BEGAN \cite{Berthelot2017BEGANBE} as our generator in our experiment. In this setup, the first term poisons the neurons in the last FC layer to be activated by the most sensitive data in the batch, while the second term encourages the CNN layers to encode feature maps rich in features about the original input, facilitating feature inversion. The optimization objective for the training generator is:
% \begin{equation}
%     \mathcal{L}oss(G)=\|G(F_{conv}(x))-x\|_{2}^{2}.
% \end{equation}

% \textbf{2) Attack Execution.}
% \begin{itemize}
% \item The server distributes the poisoned model to the clients.

% \item The client uploads the gradients calculated on its local data.

% \item The server receives the gradients returned by the client and approximately calculates the features of the most private data in a batch using $\frac{\nabla W^k}{\nabla b^k}$.

% \item The trained generator then inverts these feature maps to recover the original sensitive input data  $G(\frac{\nabla W^k}{\nabla b^k})$.

% \end{itemize}

% \begin{remark}
% FHT is stealthy. Although it modifies the entire model parameters, only the gradients of the FC layer are biased as the convolutional layers are poisoned to enhance the features of all samples within the batch. Since D-SNR relies on gradient bias, it can only check out bias in the final layer, making FHT more stealthy than previous attacks, which require all layers biased. Additionally, FHT is fast, as it only needs to parse features and generate data once the gradients are captured, allowing for rapid reconstruction.
% \end{remark}







