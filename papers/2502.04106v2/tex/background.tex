This section provides the necessary background for understanding our work. We first present an overview of the FL paradigm. We then describe the adversarial assumptions and capabilities underlying our threat model. We also discuss the motivation behind our proposed attack. For clarity, the main notations used in this paper are summarized in Table~\ref{tab:notation}.
\begin{table}[t]
\centering
% \vspace{0.0cm}
\caption{Major notations used in this paper.}
% \vspace{-0.2cm}
\label{tab:notation}
\begin{tabular}{ll}
\toprule
Notation & Description \\
\midrule
$x$ & Data batch input into the model \\
$x_i$ & $i$-th sample in a batch $x$ \\
$y$, $y_i$ & Ground-truth label for $x$ or $x_i$ \\
$\hat{y}$ & Model prediction, i.e., $\hat{y}=F(x,\theta)$ \\
$F(\cdot)$ & Neural network model \\
$\theta$ & Model parameters \\
$W$, $W^k$ & Weight matrix and its $k$-th column in an FC layer \\
$\eta$ & Learning rate \\
$\mathcal{D}(\cdot)$ & Gradient-to-input decoder network \\
$\ell(\cdot, \cdot)$ & Loss function (e.g., cross-entropy) \\
$\nabla_\theta \ell$ & Gradient of the loss with respect to $\theta$ \\
$R(\cdot)$ & Gradient leakage reconstruction function \\
$x'$ & Reconstructed input from gradients \\
$B$ & Batch size \\
$C$ & The number of classification classes \\
$k$ & Class index \\
$\theta^*$ & Optimized model parameters maximizing gradient leakage \\
$\phi$ & Parameters of the decoder $\mathcal{D}$ \\
$\lambda_i^k$ & Contribution weight of $x_i$ to class-$k$ neuron gradient \\
$\mathbf{\Lambda}$ & Weight matrix formed by $\lambda_i^k$ over $i$ and $k$ \\
$\bar{x}^{(k)}$ & Weighted average input reconstructed via class-$k$ gradient \\
$b$, $b^k$ & Bias vector and its $k$-th element \\
$\Pi(\cdot)$ & Gradient projection operator \\
$\tilde{g}$ & Projected gradient vector \\
$\rho$ & Projection ratio of gradient dimensionality \\
$L(\theta, \phi)$ & \name training loss for model and decoder \\
$D$ / $D_a$ & Client dataset / Auxiliary dataset available to attacker \\
\bottomrule
\end{tabular}
% \vspace{-0.3cm}
\end{table}

\subsection{Federated Learning}
Federated Learning (FL) is a decentralized learning framework that enables multiple clients to collaboratively train a global model without exchanging their raw data, thereby preserving data privacy~\cite{mcmahan2017communication}. In each communication round $t$, a central server selects a subset of clients $\mathcal{C} = \{c_1, c_2, \dots, c_n\}$ and distributes the current global model $F(\theta_t)$ to them. Each client $c_i$ performs local training on its private dataset $D_i$ by minimizing the empirical loss $\ell(F(\theta_t), D_i)$ and computes the corresponding gradient:
\begin{equation}
    g_i^t = \nabla_{\theta_t} \ell(F(\theta_t), D_i).
\end{equation}
Clients then upload their gradients $\{g_i^t\}_{i=1}^n$ to the server. The server aggregates the gradients (e.g., via weighted averaging) and updates the global model as follows:
\begin{equation}
    \theta_{t+1} = \theta_t - \eta \sum_{i=1}^n w_i \cdot g_i^t, \quad \text{where } w_i = \frac{|D_i|}{\sum_j |D_j|},
\end{equation}
where $\eta$ denotes the learning rate. This process is repeated iteratively until model convergence. Although FL avoids direct access to raw data, recent studies have shown that the shared gradients can still reveal client data. This vulnerability has led to a line of research on GLAs, which aim to reconstruct private data from gradients. Our work builds upon this threat by exploring more general and stealthy attack mechanisms.

\subsection{Threat Model}
Our threat model operates within an FL framework where the server is dishonest and curious. It attempts to infer private client data by poisoning the global model parameters before distribution. However, the server is constrained from modifying the model architecture, as structural changes (e.g., inserting layers) are easily detected by clients via architecture verification, integrity checks, or test queries. Following the setting in many prior works \cite{Garov2024Hiding, yang2022using, 285479}, we assume the malicious client can take some publicly available datasets as the auxiliary dataset, which can be easily obtained from open repositories such as Hugging Face\cite{huggingface}, Kaggle\cite{kaggle}, or OpenML\cite{OpenML}. This is a realistic assumption, as adversaries can readily download such datasets in practical scenarios. Moreover, we experimentally demonstrate that the effectiveness of our attack is not sensitive to the distribution gap between the auxiliary and target datasets, further supporting the rationality of this setting.

\subsection{Motivation}
While PGLAs have been extensively studied and shown to be effective in controlled experimental settings, their success is highly sensitive to the model's parameter initialization. Our empirical observations reveal that models initialized using commonly adopted schemes such as Random, Xavier \cite{glorot2010understanding}, and He \cite{he2015delving} may fail to embed enough data features into the gradients, rendering even SOTA PGLAs incapable of reconstructing the original samples. This strong dependence on the model parameters fundamentally limits PGLAs to a passive roleâ€”attackers lack control over the feature distribution in the gradient space, leading to unpredictable and unreliable performance in real-world deployments.

AGLAs have attempted to overcome these limitations by modifying the model architecture or injecting targeted biases into the parameter space. However, such approaches often amplify the gradient of only a subset of samples while suppressing others, resulting in low attack coverage. Moreover, these manipulations typically introduce detectable anomalies in the gradients or model behavior, thereby increasing the risk of being discovered by defensive mechanisms on the client side. This indicates that existing AGLAs are inherently incapable of simultaneously ensuring attack stealth and integrity, due to their fundamental principles.

Inspired by these observations, we propose a novel attack: enhancing the overall information capacity of the gradient space through balanced strengthening. We note that if we can actively guide the model to learn a ``balanced encoding" gradient space during training, ensuring that all samples have similar information expression strengths in the gradient (i.e., enhancing the leakage potential of each sample), this would not only guarantee the reconstruction ability of all samples but also maintain the naturalness of the gradient distribution, reducing the risk of detection. Thus, our method not only breaks free from the traditional passive reliance of PGLAs on waiting for data characteristics to leak but also avoids the incomplete reconstruction coverage and detectability issues caused by the ``intentional bias" in existing AGLAs. This provides a more stable, comprehensive, and stealthy GLA.

% It manipulates the model parameters before distributing them to clients but is restricted from altering the model structure. By exploiting this capability, the server poisons the model parameters to amplify the gradient vulnerabilities, thereby enabling the reconstruction of clients' data from the uploaded gradients. Furthermore, we assume the malicious client has access to publicly available datasets, which can be used as auxiliary data to facilitate the attack.

