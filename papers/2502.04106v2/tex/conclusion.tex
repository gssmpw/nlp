% In this work, we identify that all prior AGLAs suffer from detectability issues and incomplete attack coverage, and we develop stealthy and comprehensive MGLAs to address these challenges. Firstly, we propose FHT, a poisoning method for targeted leakage of sensitive data within a batch. FHT enables fast and targeted data leakage with high stealthiness. Additionally, we propose EGGV, a model poisoning method that enhances the performance of MGLAs across an entire batch. This is the first MGLA to inverse entire data samples and extensive experiments also confirm that our method significantly enhances the steathiness and coverage of the SOTA. Finally, we introduce iD-SNR, a new detection metric to raise the sensitivity of the SOTA metric to biased gradients. We encourage the federated learning community to explore further privacy protection mechanisms to counter these emerging security challenges.

In this work, we introduce a new backdoor-theoretic perspective to rethink and frame existing AGLAs. Through this lens, we theoretically identify that all prior AGLAs suffer from incomplete attack coverage and detectability issues. We further propose \name, a new solution that extends existing AGLAs to be more comprehensive and stealthier to address the above challenges. \name is the first AGLA capable of fully inverting all samples within a target batch while evading existing detection metrics. Extensive experiments demonstrate that EGGV significantly outperforms SOTA AGLAs in both stealthiness and attack coverage. These results encourage the FL community to explore further privacy protection mechanisms to counter these emerging security risks.