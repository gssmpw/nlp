% Federated Learning (FL) \cite{mcmahan2017communication, Towardsk, chilimbi2014project} enables multiple clients to collaboratively train a model without sharing raw data. In the standard FL protocol, the server sends the global model to clients, who then use their own data to compute the gradient and send it back to the server. While FL avoids direct transmission of client data, shared gradients are demonstrated to leak client data, an attack known as Gradient Leakage Attacks (GLAs) \cite{zhu2019deep}. GLAs can be broadly classified into two major categories \cite{nowak2024qbi}: Honest Gradient Leakage Attacks (HGLAs) \cite{RGAP, yang2022using, 285479} and Malicious Gradient Leakage Attacks (MGLAs) \cite{boenisch2023curious, zhao2023loki, fowl2021robbing, nowak2024qbi}.

Federated Learning (FL) \cite{mcmahan2017communication, Towardsk, chilimbi2014project} has emerged as a promising framework for privacy-preserving distributed learning, allowing multiple clients to jointly train a global model without sharing raw data. In each communication round, the server distributes the model to clients, who compute gradients on their private data and return them to the server for aggregation. However, a growing body of research has revealed that these shared gradients can be exploited by adversaries to reconstruct private clients' data, an attack known as Gradient Leakage Attacks (GLAs) \cite{zhu2019deep}. GLAs severely compromise the core privacy promise of FL and are broadly categorized into two types \cite{nowak2024qbi}: Passive Gradient Leakage Attacks (PGLAs) \cite{RGAP, yang2022using, 285479}, and Active Gradient Leakage Attacks (AGLAs) \cite{boenisch2023curious, zhao2023loki, fowl2021robbing, nowak2024qbi}.

% While FL avoids direct transmission of client data, shared gradients are demonstrated to leak client data, an attack known as Gradient Leakage Attacks (GLAs) \cite{zhu2019deep}.

% In HGLAs, attackers reconstruct client data from the gradients shared in the FL system, without manipulating the model architecture, model parameters, and the FL protocol. However, the effectiveness of these attacks heavily relies on the initialization of model parameters. Our experimental results in Table \ref{compare2} and Figure \ref{visualresults} first reveal that even HGLAs previously considered effective fail to attack the model with improper initialization methods. This limitation led to the development of MGLAs, which implement reconstruction by modifying the model structure and model parameters. 

In PGLAs, attackers reconstruct client data from the gradients shared in the FL system, without manipulating the model architecture, model parameters, and the FL protocol. Zhu et al. \cite{zhu2019deep} first demonstrate the possibility of reconstructing data by optimizing randomly initialized pixel data to produce gradients that match the observed ones. Subsequent methods such as iDLG \cite{zhao2020idlg}, IG \cite{geiping2020inverting}, and STG \cite{yin2021see} improve the reconstruction results by adding image priors as an optimization objective.
% Meanwhile, methods such as GI \cite{jeon2021gradient}, GGL \cite{li2022auditing}, and GGDM \cite{gu2024federated} further enhance the capabilities of these attacks with generative images priors.
However, the effectiveness of these attacks heavily depends on the initialization of model parameters. The model parameters in an unfavorable position produce the gradients lacking data features, rendering these attacks ineffective \cite{du2024sok}. For instance, when model parameters are initialized to zero, the gradients will also be zero and thus contain no data feature, thereby preventing any successful reconstruction by existing GLAs. Our experimental results in Table \ref{compare2} and Figure \ref{visualresults} demonstrate that, for the first time, even previously considered effective PGLAs fail to reconstruct the data when improper initialization methods are used by the server. 
\begin{figure}[t]
    \centering
    \vspace{0.1cm}  %调整图片与上文的垂直距离
    % \setlength{\abovecaptionskip}{-0.5cm}   %调整图片标题与图距离
    % \setlength{\belowcaptionskip}{-1.8cm}   %调整图片标题与下文距离
    \includegraphics[scale=0.07]{figs/show.pdf}
    % \vspace{-0.3cm}
    \caption{Illustration of the core principles and reconstruction outcomes of Fishing \cite{wen2022fishing}, SEER \cite{Garov2024Hiding}, and \name (Ours). Each cylinder denotes the gradient space of a batch, with inner circle sizes indicating per-sample information retention. Existing AGLAs amplify a few samples while suppressing others, leading to partial recovery. In contrast, \name (Ours) uniformly enhances the feature of all samples in the gradient, enabling full-batch reconstruction.
    }
    % \vspace{-0.3cm}
    \label{showdiff}
\end{figure}

In contrast, in AGLAs, attackers achieve data reconstruction by modifying the model structure and parameters. Some AGLAs \cite{boenisch2023curious, zhao2023loki, fowl2021robbing, nowak2024qbi} achieve direct and accurate reconstruction by inserting a fully connected layer at the beginning of the model and modifying the parameters of this layer. However, such structural modifications are easily detected. Other AGLAs \cite{wen2022fishing, Garov2024Hiding} improve PGLAs by reducing the effective samples used for gradient computation. However, this reduction also limits the improved reconstruction of PGLAs to only a few samples or even just a single sample, as shown in Figure \ref{showdiff}. Moreover, recent work \cite{Garov2024Hiding} reveals that all prior AGLAs are detectable by detection metric. In summary, existing AGLAs exhibit several limitations in attack coverage and stealthiness.

%We refer to these attacks as gradient-biased AGLAs and their gradients as biased gradients. Despite the empirical success in existing AGLAs, they lack a unified theoretical foundation to explain their underlying mechanisms and limitations.

% Moreover, recent work \cite{Garov2024Hiding} reveals that all prior AGLAs are detectable by the proposed detection metric named Disaggregation Signal-to-Noise Ratio (D-SNR), exploiting a bias they introduce in the gradient space.

% \textbf{Weak client-side detectability.} D-SNR is the only existing detection metric for MGLAs. However, our experimental results show that it fails to distinguish between poisoned and clean gradients on large batches. \textbf{This work: effective detection.} We propose a robust detection metric, dubbed, the \underline{i}mproved \underline{D}isaggregation \underline{S}ignal-to-\underline{N}oise \underline{R}atio (iD-SNR). iD-SNR is a groundbreaking detection metric that overcomes the batch size dependency of D-SNR through a novel normalization mechanism, effectively amplifying biased gradients to reveal malicious modifications. ID-SNR motivates us to fundamentally rethink the stealthiness of MGLAs.

This paper addresses the above critical challenges with two major contributions. First, we propose a new theoretical approach to rethinking and analyzing AGLAs. The core of our approach is the introduction of a parameter $\lambda$, which can quantify the relative contribution of each sample within a batch to the activation of neurons in each class. It discloses that the fundamental principle of existing AGLAs is to prioritize the reconstruction of a small subset of samples with specific properties, while sacrificing the majority of other samples. Such properties are analogous to triggers in backdoor attacks. This principle explains the critical limitations of existing AGLAs, underscoring the pressing need for an advanced attack with a fundamentally different principle. 

%\textbf{This principle inherently leads to two critical limitations: (i) poor stealthiness: all existing AGLAs are detectable by principled client-side detection; and (ii) incomplete attack coverage: all AGLAs only reconstruct part samples in the attacked batch.} These limitations underscore the pressing need for an attack method with a fundamentally different principle.

Second, based on the above theoretical analysis, we propose \underline{E}nhanced \underline{G}radient \underline{G}lobal \underline{V}ulnerability (\name), a novel AGLA that ensures complete attack coverage and evades client-side detection. 
%parameter poisoning method that ensures complete attack coverage and evades client-side detection.
Different from existing attacks, \name equally enhances the gradient vulnerability of all samples in a batch, as illustrated in Figure \ref{showdiff}. Its key insights include: (i) treating gradients as a latent space of data, with the forward and backward propagation of the model as an encoding process; (ii) introducing a discriminator jointly trained with the model to access the gradient vulnerability of the model. Importantly, \name opens up a new research path thoroughly different from gradient-biased AGLAs.

\textbf{Our contributions are summarized as:} 
\begin{itemize}

\item We introduce a backdoor-theoretic perspective to frame the fundamental principles of AGLAs and identify two critical limitations in their principles: incomplete attack coverage and poor stealthiness.

\item We propose a novel GLA \name that extends AGLAs to achieve both complete coverage and enhanced stealthiness. We further provide theoretical guarantees for the existence of optimal poisoned parameters for gradient leakage and the stealthiness of the proposed attack.

\item Extensive experiments show that the proposed \name significantly surpasses SOTA methods, achieving at least a 43\% improvement in reconstruction quality (measured by PSNR) and a 45\% enhancement in stealthiness (measured by D-SNR) with a complete attack coverage.

\end{itemize}

\textbf{Paper Organization.} The remainder of this paper is organized as follows. Section~\ref{sec:related-work} reviews related work on GLAs. Section~\ref{sec:background} provides the necessary background, including the FL framework, the threat model, and motivation for this work. In Section~\ref{sec:backdoor-analysis}, we present a backdoor-theoretic analysis of AGLAs to reveal the fundamental limitations in their principles. Section~\ref{sec:EGGV} introduces the proposed attack, detailing its formulation, optimization approach, and theoretical guarantees. Experimental results are presented in Section~\ref{sec:experiments}. Finally, Section~\ref{sec:conclusion} draw a conclusion.













% \textbf{This work: developing stealthy and comprehensive MGLAs.} 

% The existing detection metric \cite{Garov2024Hiding} is only effective for detecting gradients computed on the data of small batch sizes and is insensitive to poisoned model parameters.

% MGLAs~\cite{nowak2024qbi, fowl2021robbing} achieve direct and accurate reconstruction by inserting a fully connected (FC) layer at the beginning of the model.
% and modifying the parameters of this layer. 
% Other MGLAs \cite{wen2022fishing, Garov2024Hiding} improve HGLAs by reducing effective samples used for gradient computation. 
% Despite these advancements, we clearly identify that 
% However, existing MGLAs still face the following critical unresolved challenges.

% \textbf{Key Challenges.} A satisfied MGLA must address two critical challenges: (i) \textbf{stealthiness}: existing MGLAs are easily perceived by the principled detection~\cite{Garov2024Hiding}. Figure~\ref{showchallenges}(a) illustrates that SOTA attacks Fishing \cite{wen2022fishing} and SEER \cite{Garov2024Hiding} are easily detected by D-SNR and iD-SNR metrics. (ii) \textbf{coverage}: no existing MGLAs are capable of recovering all samples within a data batch. As shown in Figure~\ref{showchallenges}(b), current SOTA attacks Fishing and SEER only manage to reconstruct a single sample from a gradient computed on a data batch. 
%, as they exhibit significantly high values in both.

% \textbf{Active Gradient Leakage Attacks (AGLAs).} AGLAs \cite{boenisch2023curious, zhao2023loki, fowl2021robbing, nowak2024qbi} enhance the effectiveness of GLAs by allowing attackers to manipulate the model structure and model parameters. Some AGLAs \cite{boenisch2023curious, zhao2023loki, fowl2021robbing, nowak2024qbi} achieve direct and accurate reconstruction by inserting a fully connected layer at the beginning of the model and modifying the parameters of this layer. However, such structural modifications are easily detected. Other AGLAs \cite{wen2022fishing, Garov2024Hiding} improve PGLAs by reducing effective samples used for gradient computation. However, this reduction also limits the improved reconstruction of PGLAs to only a few samples or even just a single sample. Moreover, recent work \cite{Garov2024Hiding} reveals that all prior AGLAs are detectable by detection metric, as they bias the entire gradient space towards a small subset of data. We refer to the gradients generated from such parameter modifications as biased gradients. Despite this, the existing detection metric \cite{Garov2024Hiding} is only effective for detecting gradients computed on the data of small batch sizes.

% Naturally, three challenges emerge. \textit{(1) First, can an active attacker enhance the reconstruction performance of all PGLAs on the \textbf{entire data batch}?  (2) Second, is it possible to develop a detection metric that remains effective in \textbf{detecting biased gradients as batch size increases?} (3) Third, even if an attacker only aims to reconstruct a single sample from a batch, can the \textbf{stealthiness} of biased gradient space be ensured?} \textbf{This work:} we propose two methods to poison model parameters to address challenges (1) and (3), and a detection metric for the biased gradients to tackle challenge (2).

% \textbf{In the work: developing stealthy and comprehensive MGLAs.} Firstly, to minimize the modification of the model parameters, we propose a \underline{F}ast \underline{H}idden \underline{T}argeted (FHT) algorithm to selectively modify the neurons in the final classification layer. Therefore, FHT can guide the gradient flow to amplify the leakage of sensitive data points while keeping other parts of the gradient space inconspicuous. 
% Experimental results in Figure \ref{showchallenges} show that FHT requires less than $1/69$ of the time required by SOTA methods and achieves at least 4 times their stealthiness on iD-SNR metric.

% Secondly, to further enhance attack stealthiness and coverage, we propose \underline{E}nhanced \underline{G}radient \underline{G}lobal \underline{V}ulnerability (EGGV), a novel method that poisons model parameters to equally increase the gradient leakage of all samples in a batch. Unlike previous MGLAs, they can only reconstruct the specific sample within a batch. 
% enhance the leakage potential of specific samples by sacrificing the reconstruction of most others, EGGV equally enhances the leakage potential of all samples in a batch without sacrificing any. 
% Our key insights include: (i) treating gradients as a latent space of data, with the forward and backward propagation of the model as an encoding process; (ii) introducing a discriminator jointly trained with the model to access the gradient vulnerability of the model. 
% The experimental results in Figure \ref{showchallenges} demonstrate that EGGV significantly outperforms SOTA methods, achieving 100\% reconstruction coverage and at least 20 times the stealthiness compared to SOTA methods.



% We introduce a novel perspective on the gradient generation process, viewing the client's forward and backward propagation as an encoding process of the data. The gradients produced in this process are treated as a latent space representation, $g$, of the data features. By poisoning the model parameters with a single fully connected layer, we enhance the encoding process such that the gradient latent space captures more data features. This allows PGLAs to be more effective when applied to our poisoned model, as its gradient contains more data features than benign ones.

% Finally, we propose a robust detection metric, dubbed, the \underline{i}mproved \underline{D}isaggregation \underline{S}ignal-to-\underline{N}oise \underline{R}atio (iD-SNR). The existing detection metric D-SNR \cite{Garov2024Hiding} is insensitive to poisoned model parameters. Consequently, we propose iD-SNR to amplify the poisoned model parameters through normalization. The experimental results in Figure \ref{showchallenges}(a) show that iD-SNR amplifies the malicious parameters by up to 9 times compared to the SOTA detection metric.

% For challenge (3), we observe that an increasing number of clients tend to combine publicly available data and privately sensitive data as their training data, and adversaries may only be interested in the private sensitive data among them. Therefore, we propose a model parameters poisoning method called HTGLA, which can reconstruct the most sensitive data from the gradients of a data batch. HTGLA works by selectively poisoning the neurons of one category in the final classification layer of the model. This focused poisoning helps evade detectability metrics, as it affects far fewer gradients compared to methods that poison the entire parameters space.


% Our contributions can be summarized as follows:
% \begin{itemize}
% \item We propose FHT, a fast and targeted algorithm that poisons a single layer to minimize the modification of the model. Therefore, FHT is more difficult to detect because it does not rely on biased poisoning of the entire model.
% guarantees for the existence of an optimal point in the continuous model parameters space.

% \item We introduce EGGV, the first framework that poisons model parameters to increase the gradient leakage potential of all samples within a batch. We also shed light on the relationship between the optimized parameters and its underlying leakage risk theoretically.

% \item We propose iD-SNR, a novel detection metric that amplifies poisoned model parameters to better verify poisoned parameters. ID-NSR improves the predictability of data leakage and increases the detectability of existing MGLAs.

% \item Extensive experiments show that both FHT and EGGV overperform SOTA methods in reconstruction quality. FHT is 69 times faster while maintaining 4 times more stealthy and EGGV is at least 20 times more stealthy than SOTA methods. Moreover, iD-SNR improves the detection of malicious model poisoning by 9 times compared to D-SNR.

% \item We present extensive experimental evaluations of EGGV, iD-SNR, and HTGLA on practical network architectures and multiple datasets. The results show that EGGV significantly enhances the speed and accuracy of existing PGLAs compared to standard model initialization methods. Additionally, iD-SNR further amplifies the biased gradients. HTGLA excels in targeted data leakage, outperforming SOTA methods in both effectiveness and stealth.

% \end{itemize}




