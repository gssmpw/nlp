In this section, we present a series of experiments to evaluate the effectiveness of the proposed method. The experiment results show that \name significantly outperforms the SOTA AGLAs in reconstruction quality and stealthiness.

\subsection{Setup}
We use the ResNet18 \cite{he2016deep} as the default global model for FL. The CIFAR10, CIFAR100 \cite{krizhevsky2009learning}, and TinyImageNet \cite{le2015tiny} datasets are employed as the training data for clients. 
% Following \cite{Garov2024Hiding}, we generally use the training set as auxiliary data and implement an attack on randomly sampled batches of size $B$ from the test set. 
The three classic evaluation metrics for reconstruction quality, namely PSNR \cite{hore2010image}, SSIM \cite{zhang2018unreasonable}, and LPIPS \cite{wang2004image}, are employed to assess the attack quality. We use the detection metric D-SNR \cite{Garov2024Hiding} to evaluate the stealthiness of model modifications. We set the default projection ratio to 0.4\% and employ a linear layer as the default structure for the discriminator. We compare our method with the closely related SOTA methods, Fishing \cite{wen2022fishing} and SEER \cite{Garov2024Hiding} with the maximal brightness as the selected property. As our method is the first to poison model parameters for enhanced data leakage across entire batches, we also evaluate its performance against popular naive model initialization methods, including Random, Xavier \cite{glorot2010understanding}, and He \cite{he2015delving}. In our implementation, Xavier initialization uses a uniform distribution to balance variance across layers, while He initialization adapts weights for leaky ReLU activations to ensure smoother gradient flow during training.
\begin{table}[h]
  \centering
  % \vspace{-0.3cm}
  \caption{Comparison of reconstruction performance among Fishing \cite{wen2022fishing}, SEER \cite{Garov2024Hiding}, and \name (Ours) on CIFAR100 with a batch size of 8.}
  % \vspace{-0.2cm}
  \begin{tabularx}{\linewidth}{c|XXX}
    \toprule
          & Min PSNR $\uparrow$ & Pruned Average PSNR $\uparrow$ & Max PSNR $\uparrow$ \\
    \midrule
    Fishing \cite{wen2022fishing}  & 0.00000     & 0.00000     & 12.92526 \\
    SEER \cite{Garov2024Hiding}  & 0.00000     & 0.00000     & 15.97548 \\
    \name (Ours)  & \cellcolor{customblue}\textbf{20.37788} & \cellcolor{customblue}\textbf{21.59001} & \cellcolor{customblue}\textbf{22.86605} \\
    \bottomrule
  \end{tabularx}%
  \label{compareFishSeer}%
  % \vspace{-0.3cm}
\end{table}%
\begin{table*}[b]
  \centering
  % \vspace{-0.3cm}
  \caption{Performance comparison of the proposed \name against baseline model initializations Random, Xavier, and He on CIFAR10, CIFAR100, and TinyImageNet datasets.}
  % \vspace{-0.3cm}
    \begin{tabular}{ccccccccccccc}
    \toprule
    \multirow{2}[4]{*}{Method} & \multirow{2}[4]{*}{Interation} & \multicolumn{3}{c}{CIFAR10} &       & \multicolumn{3}{c}{CIFAR100} &       & \multicolumn{3}{c}{TinyImageNet} \\
\cmidrule{3-5}\cmidrule{7-9}\cmidrule{11-13}          &       & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ &       & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ &       & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
    \midrule
    Random+iDLG & \multirow{4}[2]{*}{200} & 15.86852 & 0.595493 & 0.289589 &       & 16.996 & 0.537246 & 0.344143 &       & 14.04722 & 0.222929 & 0.575494 \\
    Xavier \cite{glorot2010understanding}+iDLG &       & 20.77170  & 0.78643  & 0.24686  &       & 19.85292  & 0.73102  & 0.26831  &       & 12.18539  & 0.23047  & 0.58592  \\
    He \cite{he2015delving}+iDLG &       & -1.15507  & -0.00052  & 0.72385  &       & -1.94603  & -0.00166  & 0.75271  &       & -1.05347  & -0.00043  & 0.80013  \\
    \name(Ours)+iDLG &       & \cellcolor{customblue}\textbf{29.70104}  & \cellcolor{customblue}\textbf{0.86494}  & \cellcolor{customblue}\textbf{0.10815}  &       & \cellcolor{customblue}\textbf{28.44256}  & \cellcolor{customblue}\textbf{0.89706}  & \cellcolor{customblue}\textbf{0.09068}  &       & \cellcolor{customblue}\textbf{19.94374}  & \cellcolor{customblue}\textbf{0.62674}  & \cellcolor{customblue}\textbf{0.22103}  \\
    \midrule
    Random+IG & \multirow{4}[2]{*}{1000} & 19.16213  & 0.62193  & 0.30759  &       & 19.34636  & 0.63830  & 0.31395  &       & 15.47002  & 0.25633  & 0.52080  \\
    Xavier \cite{glorot2010understanding}+IG &       & 24.47016  & 0.86330  & 0.14445  &       & 23.16149  & 0.80442  & 0.17232  &       & 13.06239  & 0.21848  & 0.57367  \\
    He\cite{he2015delving}+IG &       & 13.30730  & 0.10187  & 0.62424  &       & 10.97889  & 0.09065  & 0.66285  &       & 12.70339  & 0.20885  & 0.72560  \\
    \name(Ours)+IG &       & \cellcolor{customblue}\textbf{31.96512}  & \cellcolor{customblue}\textbf{0.91660}  & \cellcolor{customblue}\textbf{0.07358}  &       & \cellcolor{customblue}\textbf{31.55152}  & \cellcolor{customblue}\textbf{0.92673}  & \cellcolor{customblue}\textbf{0.06176}  &       & \cellcolor{customblue}\textbf{28.62325}  & \cellcolor{customblue}\textbf{0.91401}  & \cellcolor{customblue}\textbf{0.07571}  \\
    \bottomrule
    \end{tabular}%
  \label{compare2}%
  % \vspace{0.0cm}
\end{table*}%
% Table generated by Excel2LaTeX from sheet 'Sheet4'
% \begin{table}[t]
%   \centering
%   \small
%   \vspace{0.1cm}
%   \caption{Reconstruction results of FHT on FC and Resnet18 models using CIFAR100 + PicAlert and TinyImageNet + PicAlert datasets.}
%   \vspace{-0.4cm}
%     \begin{tabularx}{\linewidth}{cc|ccc}
%     \toprule
%     Model & Dataset & PSNR $\uparrow$  & SSIM $\uparrow$  & LPIPS $\downarrow$ \\
%     \midrule
%     \multirow{2}[2]{*}{FC} & CIFAR100 + PicAlert & 33.92949  & 0.93997  & 0.00637  \\
%           & TinyImageNet + PicAlert & 39.73195  & 0.97800  & 0.00682  \\
%     \midrule
%     \multirow{2}[2]{*}{Resnet18} & CIFAR100 + PicAlert & 19.05834 & 0.58137 & 0.44857 \\
%           & TinyImageNet + PicAlert & 18.93190  & 0.59969  & 0.42994  \\
%     \bottomrule
%     \end{tabularx}%
%   \label{FHT1}%
%   \vspace{0.0cm}
% \end{table}%
% Table generated by Excel2LaTeX from sheet 'Sheet5'
% \begin{table}[t]
%   \centering
%   \small
%   \vspace{0.0cm}
%   \caption{Reconstructions of FHT, Fishing, and SEER. FHT outperforms the SOTA methods across both datasets.}
%   \vspace{-0.4cm}
%     \begin{tabularx}{\linewidth}{cccccc}
%     \toprule
%     \multirow{2}[4]{*}{Method} & \multirow{2}[4]{*}{Time $\downarrow$} & \multicolumn{2}{c}{CIFAR100+PicAlert} & \multicolumn{2}{c}{TinyImageNet+PicAlert} \\
%     \cmidrule{3-6}          &       & PSNR $\uparrow$  & SSIM $\uparrow$  & PSNR $\uparrow$  & SSIM $\uparrow$ \\
%     \midrule
%     Fishing & 1640.96s & 11.8690  & 0.1827   & 16.5589  & 0.5670  \\
%     SEER  & 20.79s & 14.4852  & 0.3164   & 13.8496  & 0.2161  \\
%     FHT \textbf{(Ours)}   & \textbf{0.30s} & \textbf{19.0583}  & \textbf{0.5813}   & \textbf{18.9319}  & \textbf{0.5996}  \\
%     \bottomrule
%     \end{tabularx}%
%   \label{compareFishSeer}%
%   \vspace{0.0cm}
% \end{table}%


% \subsection{Experimental Evidence for iD-SNR Metric}
% 对比D-SNR和iD-SNR在batch size增大的情况下二者的表现
% \subsection{Validating the Effectiveness of the iD-SNR Metric and Comparing the Stealthiness of FHT and EGGV with SOTA Methods}
% To assess the effectiveness of the iD-SNR metric and compare the stealthiness of FHT and EGGV with the SOTA, we calculate 100 gradients for each ResNet18 poisoned by FHT, EGGV, Fishing, SEER and initialized by Xavier and He initialization, using a combined CIFAR100 and PicAlert dataset. We then report the D-SNR and iD-SNR values for these gradients. As illustrated in Figure \ref{dsnridsnr}, FHT and EGGV demonstrate performance comparable to the widely used Xavier and He initialization methods across both detection metrics, indicating that our attack remains highly stealthy and challenging to detect. In contrast, Fishing and SEER show significantly higher iD-SNR values, suggesting that clients can detect these methods more easily. This is because FHT biases only the gradients of the final FC layer, while EGGV evenly enhances the leakage potential of all samples without introducing any gradient bias. In contrast, SOTA methods exhibit biased gradients across all layers, making them more prone to detection. We also find that D-SNR lacks sensitivity to biased gradients and can hardly distinguish between poisoned gradients and those from standard initialization. In contrast, the proposed iD-SNR metric effectively identifies biased gradients.

\subsection{Main Results}
\textbf{Comparison between \name and SOTA AGLAs}.
% 这里写EGGV与SOTA（SEER, Fish）的对比
Firstly, we compare the performance of the proposed \name with two SOTA AGLAs, Fishing \cite{wen2022fishing}, and SEER \cite{Garov2024Hiding}, on CIFAR100 with a batch size of 8. Table \ref{compareFishSeer} reports the minimum PSNR, pruned average PSNR, and maximum PSNR over 100 batches, where a PSNR of 0 indicates no reconstruction. \name achieves significantly higher PSNR, consistently reconstructing all samples per batch with minimal variation, while SOTA methods reconstruct only one sample per batch. 
% We also provide a visual comparison of reconstruction results in Figure \ref{visualresults}.
The visual comparison in Figure~\ref{visualresults} demonstrates the superiority of the proposed EGGV method. Specifically, EGGV successfully reconstructs every sample in the batch with a high similarity to the originals. In contrast, SOTA methods (Fishing and SEER) reconstruct only a single image, and the similarity between these reconstructions and the originals is significantly lower than that of EGGV.
\begin{remark} 
Our method applies not only to model initialization but to any round in FL. Unlike prior work~\cite{wen2022fishing, zhang2022compromise} producing suspicious parameters (e.g., zeros or ones), our poisoned parameters exhibit natural distributions. Moreover, since each training round aggregates updates from multiple clients, individual clients remain unaware of the aggregated parameter state, enabling our attack to stealthily poison model parameters at any training round.
\end{remark}
\begin{figure*}[t]
    \centering
    % \vspace{0.0cm}  % 调整图片与上文的垂直距离
    \includegraphics[scale=0.2]{figs/show2.pdf}
    % \vspace{-0.3cm}
    \caption{Visual reconstruction of IG on the model with \name poisoning, Xavier initialization, and He initialization.}
    % \vspace{-0.2cm}
    \label{visualresults}
\end{figure*}
% \begin{table}[htbp]
%   \centering
%   \footnotesize
%   \caption{Add caption}
%     \begin{tabularx}{\linewidth}{c|ccc}
%     \toprule
%           & Min PSNR & Pruned Average PSNR & Max PSNR \\
%     \midrule
%     Fishing \cite{wen2022fishing}  & 0.00000     & 0.00000     & 12.92526 \\
%     SEER \cite{Garov2024Hiding}  & 0.00000     & 0.00000     & 15.97548 \\
%     EGGV (Ours)  & 20.37788 & 21.59001 & 22.86605 \\
%     \bottomrule
%     \end{tabularx}%
%   \label{tab:addlabel}%
% \end{table}%

\textbf{Comparison between \name and Popular Model Initialization Methods in Enhancing PGLAs}. Considering that \name is the first AGLA to reconstruct all samples in the batch, we compare its performance with three naive model initialization methods. We implement the iDLG \cite{zhao2020idlg} and IG \cite{geiping2020inverting} on models that proposed \name poisons, naively initialized by Random, Xavier, and He. As shown in Table \ref{compare2}, the \name significantly outperforms Random, Xavier, and He initialization methods across all three datasets, regardless of whether iDLG or IG is used. Figure \ref{visualresults} provides a visual comparison of reconstruction results, clearly illustrating the superiority of \name.

He initialization is widely recognized for its advantages in global model training when used by honest servers, but it often results in attack failures for adversaries, including the server itself. This indicates that relying solely on original model parameters results in poor attack performance. Our research further shows that to improve the effectiveness of attacks, adversaries cannot rely only on standard model parameters. Instead, they should adopt poisoning techniques like \name to actively manipulate the gradient space.

The experimental results also highlight the critical importance of the gradient position within the gradient space for the success of GLAs. Unfortunately, previous PGLAs have overlooked this factor. Traditional PGLAs are typically limited by the current state of the model parameters, thus making it difficult to achieve optimal results. Although AGLAs attempt to address this by poisoning model parameters, their effectiveness is limited to a small subset of samples within the batch, as illustrated in Figure \ref{visualresults}. Notably, \name is the first method to tackle this key challenge for both PGLAs and AGLAs by poisoning model parameters to enhance the gradient vulnerability across the entire batch.
% Our findings suggest that attackers should actively poison the model parameters to control the vulnerability of the gradient space. 
% Notably, the EGGV is the first to address this key challenge by providing a technique for poisoning model parameters and, consequently, the global vulnerability of the gradient space.

\textbf{Stealthiness Comparisons of \name with SOTA AGLAs}.
We calculate 100 gradients on CIFAR100 with each ResNet18 poisoned by Fishing, SEER, and \name and initialized by naive initialization methods Random, Xavier, and He. We then report the D-SNR values for these gradients. As illustrated in Figure \ref{dsnridsnr}, \name demonstrates high stealthiness, achieving D-SNR values similar to those of the naive initialization methods Random, Xavier, and He. In contrast, Fishing and SEER show significantly higher D-SNR values, suggesting that clients can detect these methods more easily. This is because \name evenly enhances the leakage potential of all samples without introducing any gradient bias. In contrast, SOTA methods exhibit biased gradients across all layers, making them more prone to detection.
\begin{figure}[t]
    \centering
    \vspace{0.0cm}  %调整图片与上文的垂直距离
    \includegraphics[scale=0.14]{figs/dsnr.png}
    % \vspace{-0.3cm}
    \caption{Bar chart of D-SNR value of gradients generated by models with three naive initialization methods (Random, Xavier, He) and three poisoning methods (Fishing, SEER, \name) on 100 same batches. Lower values indicate greater stealthiness. \name achieves a highly stealthy, closely approaching D-SNR of standard initialization methods.}
    \label{dsnridsnr}
    \vspace{-0.3cm}
\end{figure}

\subsection{Ablation Study}
\textbf{Evaluating Gradient Space Vulnerability Using the Discriminator Instead of End-to-End Iterative Attacks}. We now turn to explore the effectiveness of using a discriminator to assess gradient space vulnerability, as opposed to traditional end-to-end iterative attacks. We randomly select two model directions, $x$ and $y$, in the model parameters space and systematically shift the poisoned model parameter $\theta^{*}$ along these axes, generating 441 model parameters. The discriminator evaluates the gradient vulnerability for each of these parameters, and the resulting contour map of gradient vulnerability is shown in Figure \ref{results2}(a). In this map, we select four points: $\theta_{1}$, $\theta_{2}$, $\theta_{3}$, $\theta^{*}$, with corresponding vulnerability scores of 14.99276, 5.42985, 1.28999, and 0.00655 assigned by the discriminator. Subsequently, we perform the IG attack on these four models with the CIFAR10 dataset. Figure \ref{results2}(b) depicts the PSNR convergence during the attacks on these four models. As expected, $\theta^{*}$ yields the best reconstruction. The reconstructed images are highly similar to the original input, and the PSNR value remains the highest throughout the convergence process, significantly outperforming the other three parameters. These findings demonstrate the discriminator’s effectiveness in evaluating gradient space vulnerability and predicting the likelihood of a successful reconstruction attack. In contrast to traditional end-to-end reconstruction methods, this method enables attackers to quickly identify and poison model parameters that could lead to attack failure, thereby improving the overall success rate for attacks.
\begin{figure*}[b]
    \centering
    % \vspace{-0.3cm}  %调整图片与上文的垂直距离
    \includegraphics[scale=0.155]{figs/results2.pdf}
    % \vspace{-0.3cm}
    \caption{Figure (a): Contour map of the discriminator loss landscape across 441 model parameters generated by shifting $\theta^{*}$ along two random directions. The map marks the selected points $\theta_{1}$, $\theta_{2}$, $\theta_{3}$, and $\theta^{*}$. Figure (b): PSNR curves from IG attacks on a ResNet18 model at $\theta_{1}$, $\theta_{2}$, $\theta_{3}$, and $\theta^{*}$, demonstrating that $\theta^{*}$ achieves the best reconstruction quality.}
    % \vspace{0.0cm}
    \label{results2}
\end{figure*}

\textbf{Exploring the Relationship Between Gradient Space Vulnerability and Model Accuracy}.
To explore the relationship between gradient vulnerability and model accuracy, we conduct experiments by shifting the poisoned model parameter $\theta^{*}$ evenly 21 times along two randomly selected directions, $x$ and $y$, generating 441 model parameters. Each parameter receives a gradient vulnerability score from the discriminator, visualized in the 3D surface plot at the top of Figure \ref{dsnridsnr}, representing the gradient vulnerability landscape across the parameters space. We then evaluate the classification accuracy of these same 441 model parameters using the CIFAR10 dataset, producing the lower plot of Figure \ref{dsnridsnr}. This plot shows the model accuracy at the same parameter positions as in the gradient vulnerability plot. A comparison between the two plots reveals that the model parameters with the highest gradient vulnerability do not coincide with those that yield the highest accuracy. In fact, model parameters with the greatest gradient vulnerability often show low accuracy, indicating no direct correlation between gradient vulnerability and model accuracy.
\begin{figure*}[t]
    \centering
    % \vspace{0.0cm}  % 调整图片与上文的垂直距离
    \includegraphics[scale=0.135]{figs/diffaux.pdf}
    % \vspace{-0.3cm}
    \caption{Visualization of distributional differences between auxiliary and target datasets and IG reconstruction results on \name-poisoned models using each auxiliary dataset.}
    % \vspace{-0.4cm}
    \label{diffaux}
\end{figure*}
\begin{figure}[h]
    \centering
    % \vspace{0.0cm}  %调整图片与上文的垂直距离
    \includegraphics[scale=0.14]{figs/model_gradient_space.pdf}
    % \vspace{-0.2cm}
    \caption{3D surface plots of the discriminator loss (top) and model accuracy (bottom) for the same 441 model parameters shifted $\theta^{*}$ along two random directions, illustrating the relationship between gradient vulnerability and model accuracy across the parameters space.}
    \label{model_gradient_space}
    % \vspace{-0.3cm}
\end{figure}
\begin{table}[t]
  \centering
  % \footnotesize
  % \vspace{0.0cm}
  \caption{Reconstruction results of IG attack ResNet18 poisoned by \name with projection ratios of 1.6\%, 0.8\%, and 0.4\%. \name at 0.4\% projection ratio achieves the best overall performance.}
  % \vspace{-0.2cm}
  \resizebox{\linewidth}{!}{ % 这里开始缩放
    \begin{tabularx}{\linewidth}{c|XXX}
    \toprule
    Initiation Method & \multicolumn{1}{c}{PSNR $\uparrow$} & \multicolumn{1}{c}{SSIM $\uparrow$} & \multicolumn{1}{c}{LPIPS $\downarrow$} \\
    \midrule
    Random + IG & 15.47002  & 0.25633  & 0.52080 \\
    Xavier \cite{glorot2010understanding} + IG & 13.06239  & 0.21848  & 0.57367 \\
    He \cite{he2015delving} + IG & 12.70339  & 0.20885  & 0.72560 \\
    \name (Ours) ($\rho: 1.60\%$) + IG & 25.24615 & 0.80658 & 0.12803 \\
    \name (Ours) ($\rho: 0.80\%$) + IG & 27.63074 & 0.85759 & 0.09526 \\
    \name (Ours) ($\rho: 0.40\%$) + IG & \cellcolor{customblue}\textbf{28.62325} & \cellcolor{customblue}\textbf{0.91402} & \cellcolor{customblue}\textbf{0.07575} \\
    \bottomrule
    \end{tabularx}
  } % 这里结束缩放
  \label{difprojectratio}%
  % \vspace{-0.4cm}
\end{table}

\textbf{The Effect of Different Gradient Projection Ratios}.
% on Guiding the Gradient Space Vulnerability}
A crucial component of the \name is the projector, which compresses high-dimensional gradients into a one-dimensional vector. Next, we examine how different projection ratios influence \name to enhance the vulnerability of the gradient space. We select commonly used Random, Xavier, and He initialization methods as comparison benchmarks, and set three different gradient projection ratios of 1.60\%, 0.80\%, and 0.40\%. IG is used to conduct gradient leakage on the TinyImageNet dataset with the models initialized by the Random and \name. Comparison experimental results in Table \ref{difprojectratio} show that \name consistently enhances the gradient vulnerability across all three projection ratios, outperforming Random, Xavier, and He initialization methods. We observe an interesting phenomenon: the smallest projection ratio of 0.4\% achieves the best effect. This phenomenon can be attributed to the fact that smaller projection ratios force the model to embed more data features into the entire gradient, ensuring that the projected gradients retain enough data features to be inverted by the discriminator back to the original input. Under a smaller projection ratio, the model will more actively adjust its own parameters, thus containing more data features in the entire gradient, which is more conducive to the attack of subsequent attack methods.

% \subsection{Experimental Evidence for FHT}
% \subsubsection{Evaluating the Effectiveness of the FHT Across Different Models and Datasets} We assess the effectiveness of the FHT on two types of neural networks: a fully connected network (FC) and a convolutional neural network (ResNet18). The evaluation uses two datasets: CIFAR100 + PicAlert and TinyImageNet + PicAlert. The adversary's goal is to leak privacy-sensitive data from PicAlert by exploiting the gradients generated from the combined dataset. The experimental results in Table \ref{FHT1} show that the FHT achieves significantly better reconstruction on the FC model than ResNet18. For the FC model, attackers can directly extract the input of the FC layer from the poisoned neurons, which is the original input. In the ResNet18 model, attackers need to invert these features to the input space of the model. Despite this complexity, FHT successfully carries out effective attacks on both architectures, demonstrating its robustness across different models and datasets.

% \subsubsection{Comparison of FHT with SOTA Methods} We compare the performance of the FHT against two SOTA baselines, Fishing, and SEER, by assessing their reconstruction quality on the CIFAR100 and TinyImageNet datasets. As shown in Table \ref{compareFishSeer}, FHT significantly outperforms both Fishing and SEER across both datasets in reconstruction quality and speed.

% Table generated by Excel2LaTeX from sheet 'Sheet5'
% \begin{table}[t]
%   \centering
%   \small
%   \vspace{0.0cm}
%   \caption{Reconstructions of FHT, Fishing, and SEER. FHT outperforms the SOTA methods across both datasets.}
%   \vspace{-0.4cm}
%     \begin{tabularx}{\linewidth}{c|ccccc}
%     \toprule
%     \multirow{2}[4]{*}{Method} & \multicolumn{2}{c}{CIFAR100 + PicAlert} &       & \multicolumn{2}{c}{TinyImageNet + PicAlert} \\
% \cmidrule{2-3}\cmidrule{5-6}          & PSNR $\uparrow$  & SSIM $\uparrow$  &       & PSNR $\uparrow$  & SSIM $\uparrow$ \\
%     \midrule
%     Fishing  & 11.86905  & 0.18272  &       & 16.55892 & 0.567006 \\
%     SEER  & 14.48522 & 0.31641 &       & 13.84963 & 0.21619 \\
%     FHT \textbf{(Ours)} & \textbf{19.05834} & \textbf{0.58137} &       & \textbf{18.93190}  & \textbf{0.59969}  \\
%     \bottomrule
%     \end{tabularx}%
%   \label{compareFishSeer}%
%   \vspace{-0.2cm}
% \end{table}%

% \begin{table*}[t]
%   \centering
%   \small
%   \vspace{0.0cm}
%   \caption{Performance comparison of the proposed EGGV against baseline model initializations Xavier and He on CIFAR10, CIFAR100, and TinyImageNet datasets.}
%   \vspace{-0.4cm}
%     \begin{tabularx}{\linewidth}{ccccccccccccc}
%     \toprule
%     \multirow{2}[4]{*}{Method} & \multirow{2}[4]{*}{Interations} & \multicolumn{3}{c}{CIFAR10} &       & \multicolumn{3}{c}{CIFAR100} &       & \multicolumn{3}{c}{TinyImageNet} \\
% \cmidrule{3-5}\cmidrule{7-9}\cmidrule{11-13}          &       & PSNR $\uparrow$  & SSIM $\uparrow$  & LPIPS $\downarrow$ &       & PSNR $\uparrow$  & SSIM $\uparrow$  & LPIPS $\downarrow$ &       & PSNR $\uparrow$  & SSIM $\uparrow$  & LPIPS $\downarrow$ \\
%     \midrule
%     Xavier+iDLG & \multirow{3}[2]{*}{200} & 20.77170  & 0.78643  & 0.24686  &       & 19.85292  & 0.73102  & 0.26831  &       & 12.18539  & 0.23047  & 0.58592  \\
%     He+iDLG &       & -1.15507  & -0.00052  & 0.72385  &       & -1.94603  & -0.00166  & 0.75271  &       & -1.05347  & -0.00043  & 0.80013  \\
%     EGGV+iDLG \textbf{(Ours)} &       & \textbf{29.70104}  & \textbf{0.86494}  & \textbf{0.10805}  &       & \textbf{28.44246}  & \textbf{0.89696}  & \textbf{0.09058}  &       & \textbf{19.94374}  & \textbf{0.62674}  & \textbf{0.22103}  \\
%     \midrule
%     Xavier+IG & \multirow{3}[2]{*}{1000} & 24.47016  & 0.86330  & 0.14445  &       & 23.16149  & 0.80442  & 0.17232  &       & 13.06239  & 0.21848  & 0.57367  \\
%     He+IG &       & 13.30730  & 0.10187  & 0.62424  &       & 10.97889  & 0.09065  & 0.66285  &       & 12.70339  & 0.20885  & 0.72560  \\
%     EGGV+IG \textbf{(Ours)} &       & \textbf{31.96512}  & \textbf{0.91660}  & \textbf{0.07348}  &       & \textbf{31.55152}  & \textbf{0.92673}  & \textbf{0.06166}  &       & \textbf{28.62325}  & \textbf{0.91401}  & \textbf{0.07571}  \\
%     \bottomrule
%     \end{tabularx}%
%   \label{compare1}%
%   \vspace{-0.15cm}
% \end{table*}

\textbf{Exploring the Effect of Distribution Differences Between Auxiliary and Target Datasets on \name}.
Next, we explore the effect of distributional differences between auxiliary and target datasets on the performance of the \name. We select CIFAR10 as the target dataset and CIFAR10, CIFAR100, and TinyImageNet as the auxiliary datasets, respectively. To align the class counts with CIFAR10 to ensure compatibility for model poisoning, 10 classes are randomly sampled from CIFAR100 and TinyImageNet to serve as auxiliary datasets. Table \ref{diffauxtable} reports the performance of iDLG and IG attacks on \name-poisoned models under the above setting. The results show that \name achieves comparable PSNR values across CIFAR10, CIFAR100, and TinyImageNet auxiliary datasets, highlighting its robustness to distributional differences between auxiliary and target datasets. In addition, we use the t-SNE algorithm \cite{van2008visualizing} to reduce the dimensionality of these datasets to 2D and visualize the data distribution of the auxiliary and target datasets, along with the corresponding reconstruction results shown in Figure \ref{diffaux}. The visualizations confirm that the effectiveness of the \name attack is independent of distributional differences between the auxiliary and target datasets. In contrast, some SOTA methods, such as SEER, require the auxiliary dataset to be the training dataset of the target dataset, which limits their application to practical FL systems.
% Table generated by Excel2LaTeX from sheet 'Sheet4'
\begin{table}[t]
  \centering
  \footnotesize % 更小的字体
  \vspace{0.0cm}
  \caption{Performance of iDLG and IG attacks on \name-poisoned models with CIFAR10 as target and different auxiliary datasets, showing \name's robustness to distributional shifts.}
  \vspace{-0.2cm}
    \begin{tabularx}{\linewidth}{cXXX}
    \toprule
    \multirow{2}[3]{*}{\parbox[t]{2.3cm}{\centering Attacked Dataset: CIFAR10}} & \multicolumn{3}{c}{Auxiliary Datasets (PSNR $\uparrow$)} \\
    \cmidrule{2-4}          & \centering CIFAR10 & CIFAR100 & TinyImageNet \\
    \midrule
    \name + iDLG & 29.70104  & 29.93789  & 31.83867  \\
    \name + IG & 31.96511  & 31.28306  & 31.64095  \\
    \bottomrule
    \end{tabularx}%
    % \vspace{-0.4cm}
  \label{diffauxtable}%
\end{table}%



