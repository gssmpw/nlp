In this section, we provide an overview of prior GLAs.

% \subsection{Related Work}
\subsection{Passive Gradient Leakage Attack (PGLA)}
Most PGLAs are carried out through gradient matching. The DLG attack \cite{zhu2019deep} is the first to optimize dummy inputs and corresponding dummy labels by matching their gradients to the observed gradients. Its optimization objective is: % formulated as follows:
\begin{equation}
\label{dlg}
    x'^*,y'^*=\underset{x',y'}{\operatorname*{argmin}}\left\|\frac{\partial\ell(F(x',\theta),y')}{\partial\theta}-\nabla\theta\right\|^2,
\end{equation}
DLG works well on small models, such as LeNet-5 \cite{lecun1998gradient}. Later, iDLG \cite{zhao2020idlg} further improves DLG by inferring data labels from the gradients, but this method is limited to batches where each sample has a unique label. Subsequent works, such as LLG \cite{wainakh2022user}, extend iDLG to handle larger batch sizes, while other methods \cite{ma2023instance, wangtowards}, such as instance-wise reconstruction \cite{ma2023instance}, successfully recover ground-truth labels even in large batches with duplicate labels. IG \cite{geiping2020inverting} introduces Total Variation and a Regularization term to improve the optimization objective further. The study \cite{yin2021see} leverages the mean and variance from batch normalization layers as priors to enhance GLAs. Instead of optimizing dummy inputs directly, GI \cite{jeon2021gradient} proposes optimizing the generator and its input latent to generate dummy images whose gradients match the observed gradients. GGL \cite{li2022auditing} simplifies this by focusing solely on the latent space of pre-trained BigGAN for gradient matching. More recently, GGDM \cite{gu2024federated} uses captured gradients to guide a diffusion model for reconstruction. However, despite these advancements, PGLAs fail under improper model parameter initializations that yield gradients containing minimal data features. Our experiments in Table \ref{compare2} and Figure \ref{visualresults}, for the first time, question the practical effectiveness of these attacks under popular model initialization methods. This exposes a critical limitation in real-world scenarios.

% 主动
\subsection{Active Gradient Leakage Attack (AGLA)}
Based on the manipulation strategies, most AGLAs can be classified into two categories: structure-modified AGLAs and gradient-biased AGLAs.

\textbf{Structure-Modified AGLAs}.
This type of attack mainly enhances PGLAs by assuming a dishonest server that manipulates the model structure \cite{boenisch2023curious, fowl2021robbing, nowak2024qbi, zhao2023loki}. Some studies \cite{fowl2021robbing, nowak2024qbi} insert an FC layer at the beginning of the model, while another work \cite{zhao2023loki} inserts a convolutional layer and two FC layers. These inserted layers are referred to as ``trap weights", which are maliciously modified so that the neurons inside are activated only by samples with specific properties, enabling the reconstruction of the sample with the strongest property. However, modifications to the model structure are inherently detectable due to their explicit changes to the model architecture, rendering them impractical in real-world scenarios. Therefore, this work focuses on another type of AGLAs \cite{zhang2022compromise, wen2022fishing, Garov2024Hiding} which poisons model parameters instead of modifying the model structure. 

\textbf{Gradient-Biased AGLAs}.
Gradient-biased AGLAs \cite{zhang2022compromise, pasquini2022eluding, wen2022fishing, Garov2024Hiding} poison model parameters to skew the gradient space, ensuring that selected samples dominate the batch-averaged gradients while suppressing others. For instance, the attack \cite{zhang2022compromise} zeros out most of the convolutional layers, ensuring that only one sample's features reach the classification layer, activating the relevant neurons.  The method \cite{wen2022fishing} assigns many 0s and 1s to the last FC layer to make the averaged gradient close to the gradient of a single sample, thereby enhancing the reconstruction of PGLAs on a single sample. The method \cite{pasquini2022eluding} distributes inconsistent models to clients, forcing non-target users’ ReLU layers \cite{nair2010rectified} to output zero gradients, thereby retaining only the target user’s gradients, which can then be exploited to leak the targeted private data. Recent work \cite{Garov2024Hiding} observes that all these AGLAs bias the averaged gradient toward the gradients of a small subset of data within a batch while suppressing the gradients of other samples. Exploiting this bias in the biased gradients, research \cite{Garov2024Hiding} introduces a D-SNR detection metric to check poisoned model parameters, which is calculated as below:
\begin{equation}
\label{dsnr}
\resizebox{\linewidth}{!}{$
\begin{aligned}
    D-SNR(\boldsymbol{\theta})= \max_{W\in\boldsymbol{\theta}_{lw}}\frac{\max_{i\in\{1,...,B\}}\left\|\frac{\partial\ell(F(x_i),y_i)}{\partial W}\right\|}{\sum_{i=1}^B\left\|\frac{\partial\ell(F(x_i),y_i)}{\partial W}\right\|-\max_{i\in\{1,...,B\}}\left\|\frac{\partial\ell(F(x_i),y_i)}{\partial W}\right\|},
\end{aligned}
$}
\end{equation}
where ${\theta}_{lw}$ denotes the set of weights of all dense and convolutional layers. D-SNR claims that all prior AGLAs are detectable by principled checks.


% As the largest gradient in biased gradients often stands out significantly compared to the gradients of other samples, we refer to it as the outlier gradient. Here, it is important to clarify that biased gradients refer to an averaged gradient dominated by a single sample's gradient, making the batch gradient appear "biased." The outlier gradient denotes an abnormally large gradient within a batch. D-SNR \cite{Garov2024Hiding} is currently the only detection metric for such malicious model modifications and argues that these biased AGLAs are detected by principled client-side checks. Notably, the D-SNR metric is not only related to the outlier gradients within the batch but also negatively correlated with batch size. To ensure the metric focuses solely on outlier gradients, we introduce iD-SNR, a more sensitive measure detailed in the Method section. In summary, the discussed model parameter poisoning AGLAs can be referred to as biased AGLAs, detectable through outlier gradients. We then propose a backdoor-theoretic perspective to reconsider these biased AGLAs, providing theoretical insights into the model parameters and potential for sample leakage.

% Although SEER \cite{Garov2024Hiding} introduces a "Disaggregator" to disaggregate the averaged gradient into the gradients for individual samples to evade client detection, in their experiments, this “Disaggregator” acts as an identity function. The averaged gradient in this work also aligns with the gradient of the selected sample, while the gradients of other samples approach zero. This severe gradient bias violates a series of requirements for realistic MGLAs proposed by this paper.


% However, SEER \cite{Garov2024Hiding} argues that these attacks are not practically feasible as they can be detected by clients through principled checks. Therefore, SEER proposes the D-SNR metric to detect malicious parameter modifications in the model:
% \begin{equation}
% \label{dsnr}
% \small
%     \begin{aligned}&D-SNR(\boldsymbol{\theta})=\max_{W\in\boldsymbol{\theta}_{lw}}\frac{\max_{i\in\{1,...,B\}}\left\|\frac{\partial\ell(F(x_i),y_i)}{\partial W}\right\|}{\sum_{i=1}^B\left\|\frac{\partial\ell(F(x_i),y_i)}{\partial W}\right\|-\max_{i\in\{1,...,B\}}\left\|\frac{\partial\ell(F(x_i),y_i)}{\partial W}\right\|},\end{aligned}
% \end{equation}
% where ${\theta}_{lw}$ denotes the set of weights of all dense and convolutional. Although SEER \cite{Garov2024Hiding} introduces a "Disaggregator" to disaggregate the averaged gradient into the gradients for individual samples to evade client detection, in their experiments, this “Disaggregator” acts as an identity function. The averaged gradient in this work also aligns with the gradient of the selected sample, while the gradients of other samples approach zero. This severe gradient bias violates a series of requirements for realistic MGLAs proposed by this paper.


% \subsection{Existing Challenges}
% Despite the above progress made in GLAs, several significant challenges continue to hinder their effectiveness and applicability. Below, we discuss these challenges in detail.

% \subsubsection{Lack of effective method to enhance gradient leakage potential for all samples in a batch.} The effectiveness of HGLAs heavily relies on the model parameters. AGLAs attempt to improve this by modifying the model structure or model parameters to enlarge the gradient of the target sample within a batch while suppressing others. This allows HGLAs to reconstruct this target sample from the averaged gradient. However, these attacks only improve the leakage of a limited subset of the batch data. The real challenge lies in developing a method for the averaged gradients to capture more features of every sample in a batch, thereby increasing the vulnerability of the global gradient space.

% \subsubsection{Lack of effective detection mechanisms for malicious model parameter modifications.} The effectiveness of the existing detection method \cite{Garov2024Hiding} relies on the characteristics of AGLAs in which the gradient of the target sample is amplified relative to others. They use the ratio of the largest gradient to the sum of the remaining gradients in a batch to spot anomalies. However, as batch size increases, the sum of gradients also increases, making individual anomalies less distinguishable and reducing detection accuracy. Therefore, there is a critical need to design a detection mechanism that remains effective regardless of batch size and can effectively identify malicious modifications in model parameters.

% \subsubsection{Exposure of malicious intent in the biased gradient space.} Existing AGLAs manipulate model parameters to amplify specific sample gradients while suppressing others. However, such manipulations often expose a significant bias in the gradient space toward certain samples, rendering the malicious modifications easily detectable. Once clients recognize the poisoned parameters from the malicious server, the carefully prepared attack fails. Thus, the challenge lies in designing a method to effectively leak a specific sample while concealing maliciously poisoned model parameters.

