In this paper, we propose a provable, training-free model steering technique, coined Curvature Tuning (CT), which enables adjustment of a model's decision boundary curvature through a single parameter. We empirically demonstrate that CT enhances both the generalization and robustness of models across various scenarios.

CT offers an off-the-shelf solution that is agnostic to input modality, task type, or loss function. However, it does have a structural limitation: its applicability requires the use of specific activation functions such as ReLU, Swish, or SoftPlus due to its underlying formulation. While some current large-scale models, including DINOv2 and Llama3, do not adhere to this restriction, we have demonstrated that CT is effective on ReLU-based transformers. Moreover, a resurgence in the use of ReLU-based architectures \cite{mirzadeh2023relullm} suggests that CT may soon become relevant to an even broader range of state-of-the-art models with minimal effort.

Although CT and other Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA cater to distinct fine-tuning needs, the theoretical foundations of CT have the potential to inspire further advancements in state-of-the-art techniques such as LoRA. We hope this work serves as a stepping stone for future research into efficient, principled approaches to post-training model steering.