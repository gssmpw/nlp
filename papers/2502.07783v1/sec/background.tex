This section presents a concise review of current fine-tuning techniques and their limitations in \cref{sec:finetune}, followed by an introduction to relevant concepts in splines and their connections to Deep Networks (DNs), which are foundational for understanding CT.

\subsection{The Fine-tuning Menagery}\label{sec:finetune}
Fine-tuning, in the context of this paper, refers to adapting a pretrained model to improve its ability to solve a particular downstream task of interest. Initially, the common practice was to take the downstream task and continue training all of the model parameters, a process commonly referred to as \textit{full fine-tuning}. Notable examples include GPT \cite{radford2018gpt} and DINO \cite{caron2021dino}. However, as model sizes continue to grow, performing full fine-tuning on the latest models would require immense infrastructure and often result in poor performance due to the small size of many downstream task datasets. Given these challenges, \textit{parameter-efficient fine-tuning (PEFT)} methods were developed to mitigate the cost while maintaining effectiveness.

To better understand the landscape of PEFT approaches, we adopt the categorization proposed by Han et al. \yrcite{han2024peft}, which organizes these methods into four primary categories. \textbf{Additive PEFT} introduces additional trainable parameters to the pretrained model, training only these new parameters during fine-tuning. Examples include Serial Adapter \cite{houlsby2019serialadapter}, Prefix-tuning \cite{li2021prefixtuning}, and (IA)$^3$ \cite{liu2022ia3}. \textbf{Selective PEFT} identifies a subset of existing parameters for fine-tuning, with examples such as U-Diff pruning and S-Diff pruning \cite{guo2020diffpruning}. \textbf{Reparameterized PEFT:} decomposes pretrained weights into low-rank matrices, fine-tuning only the low-rank components, which are converted back during inference; examples include LoRA \cite{hu2021lora} and DyLoRA \cite{valipour2022dylora}. \textbf{Hybrid PEFT} combines multiple PEFT approaches, such as UniPELT \cite{mao2021unipelt} and S4 \cite{chen2023s4}.

While these techniques vary in the parameters they modify, they all require further training, which remains computationally expensive. In particular, backpropagation presents significant challenges for larger models. Additionally, their application often involves tuning numerous hyperparameters, typically guided by heuristics with limited theoretical justification, making it difficult to determine optimal values. Moreover, deep learning training remains largely opaque, complicating the understanding of how pretrained knowledge is preserved and limiting interpretability. For instance, deploying LoRA involves multiple design choices, including selecting the layers where it should be applied \cite{gao2024lorawhere}, determining its rank \cite{valipour2022dylora, chen2024autorank}, choosing the scaling factor during inference \cite{kalajdzievski2023lorascaling}, and initializing its parameters \cite{hayou2024lorainit}, all of which rely primarily on heuristics. Furthermore, even with a low-rank configuration, fine-tuning LoRA variants of ResNets—relatively small models compared to contemporary large models—still requires tens of thousands to over a million parameters, as shown in \cref{tab:lora}.

In contrast, our proposed method, CT, bypasses training entirely, eliminating the need for backpropagation, significantly improving efficiency. Moreover, CT offers greater interpretability, as it directly and provably adjusts the model's decision boundary, as demonstrated in later sections.

% It is worth noting, however, that we compare our method with fine-tuning approaches like LoRA under scenarios with extremely limited computational resources. While these methods address fundamentally different fine-tuning needs and are not directly comparable in a general sense, we hope the theoretical grounding of CT will inspire new directions and advancements in state-of-the-art solutions such as LoRA.

% Next, we introduce relevant concepts in splines and its connection to DNs, which are essential for understanding the working mechanism of CT.

\subsection{The Spline formulation of Deep Networks}\label{sec:spline}
In this subsection, we review relevant concepts in splines, which provide a mathematical framework for understanding the relationship between piecewise-affine functions and DNs.

A {\em spline function} is a function $s: \mathbb{R}^D \rightarrow \mathbb{R}$ defined piecewise by polynomials. An {\em affine spline function} is a special case where each piece is defined by an affine function. Such a function can be parameterized by three components: 
\begin{itemize}
    \item $\mathbf{A} \in \mathbb{R}^{R \times D}$: A matrix representing the slopes of the affine functions.
    \item $\mathbf{b} \in \mathbb{R}^R$: A vector representing the offsets of the affine functions.
    \item $\Omega \triangleq \{\omega_1, \dots, \omega_R\}$: A partition of the input space $\mathbb{R}^D$ into $R$ regions.
\end{itemize}
For an input $\mathbf{x} \in \mathbb{R}^D$, the affine spline function is defined as:
\begin{equation}
s[\mathbf{A}, \mathbf{b}, \omega](x) = \sum_{r=1}^R \big( \langle \mathbf{A}_{r,\cdot}, \mathbf{x} \rangle + \mathbf{b}_r \big) \mathbf{1}_{\{\mathbf{x}\in\omega_r\}},
\end{equation}
where $\mathbf{1}_{\{\mathbf{x}\in\omega_r\}}$ is an indicator function that equals 1 if $\mathbf{x}$ belongs to region $\omega_r$ and 0 otherwise.

A {\em max-affine spline function} is a special case of an affine spline function that does not explicit knowledge of $\Omega$. Instead, its output is computed as the maximum value over the affine functions:

\begin{equation}
s[\mathbf{A},\mathbf{b}](\mathbf{x}) = \max_{r=1 \dots R} \big( \langle \mathbf{A}_{r,\cdot}, \mathbf{x} \rangle + \mathbf{b}_r \big)\label{eq:maso}
\end{equation}

The key result underpinning our study is that many deep network layers—such as fully connected layers, convolutional layers, and convex piecewise-affine activations (e.g., ReLU, max pooling, or maxout)—can be exactly represented as max-affine spline functions \cite{balestriero2018spline}. Further details on this connection are provided in \cref{app:spline-theory}. While we primarily leverage this result from the spline formulation of deep networks, interested readers can refer to \cite{balestriero2019splinegeometry} for a deeper exploration of how the spline of each layer composes to make the entire input-output mapping of a DN an affine spline.

Now that we have reviewed the current fine-tuning methods and their limitations, and introduced relevant concepts in splines along with their connection to deep networks, which provide the theoretical foundation for our method, we proceed to present our proposed method in \cref{sec:method}.
