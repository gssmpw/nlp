In this section, we empirically validate the effectiveness of CT by demonstrating its ability to improve model generalization on natural image classification (\cref{sec:exp-natural-img}), as well as on medical image classification and more fine-grained tasks (\cref{sec:exp-med-task}). Additionally, we show that CT enhances robustness against adversarial and corrupted data (\cref{sec:exp-rob}). We then demonstrate that CT is effective for transformers, even with partial theoretical guarantees (\cref{sec:exp-transformer}). Finally, we conduct ablation studies on its implementation in \cref{sec:exp-abl}.

For all experiments, the parameter $\beta$ is searched within the range $[0.5, 1)$ with a step size of 0.01. Each result is reported as the mean across three independent runs with seeds 42, 43, and 44. Additional experimental details and full results with standard deviations are provided in \cref{app:exp}.

\begin{table*}[t]
  \centering
  \caption{\small Accuracy of ResNet-18 trained and tested across MNIST, CIFAR-10, CIFAR-100, and ImageNet (\textbf{bold} entries indicate improvement with CT). \textbf{CT consistently enhances generalization across models and datasets, with $\beta$ values close to 1.} Reported values are means over three runs; the complete results for ResNet-18, ResNet-50, and ResNet-152, including standard deviations, are provided in \cref{tab:gen-full}.}
  \label{tab:gen}
  \resizebox{0.8\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
      \hline
      \multirow{2}{*}{\diagbox[]{Test}{Train}} & \multicolumn{3}{c|}{MNIST} 
        & \multicolumn{3}{c|}{CIFAR-10} & \multicolumn{3}{c|}{CIFAR-100} 
        & \multicolumn{3}{c|}{ImageNet} \\
      \cline{2-13}
      & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} 
      & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} \\
      \hline
          MNIST     & 99.59 & 99.59 & \textcolor{gray}{1.00} 
                    & 86.08 & \textbf{87.30} & \textcolor{gray}{0.92} 
                    & 89.56 & \textbf{92.85} & \textcolor{gray}{0.88} 
                    & 98.10 & \textbf{98.95} & \textcolor{gray}{0.68} \\
          \hline
          CIFAR-10  & 45.02 & \textbf{47.68} & \textcolor{gray}{0.51}
                    & 94.87 & 94.87 & \textcolor{gray}{1.00} 
                    & 76.03 & \textbf{76.90} & \textcolor{gray}{0.92} 
                    & 85.68 & \textbf{85.83} & \textcolor{gray}{0.93} \\
          \hline
          CIFAR-100 & 20.30 & \textbf{21.80} & \textcolor{gray}{0.51} 
                    & 35.21 & \textbf{35.61} & \textcolor{gray}{0.97} 
                    & 76.19 & \textbf{76.21} & \textcolor{gray}{0.97} 
                    & 63.15 & 63.15 & \textcolor{gray}{1.00} \\
          \hline
          ImageNet  & - & - & \textcolor{gray}{-} 
                    & - & - & \textcolor{gray}{-} 
                    & - & - & \textcolor{gray}{-} 
                    & 69.76 & \textbf{69.84} & \textcolor{gray}{0.94} \\
          \hline
    \end{tabular}
  }
\end{table*}


\begin{table*}[t]
\caption{\small Accuracy of ImageNet-pretrained ResNet-18, ResNet-50, and ResNet-152 when transferred to 9 downstream datasets (\textbf{bold} entries indicate improvement with CT). \textbf{CT consistently enhances generalization across models and datasets, with $\beta$ values close to 1.} Reported values are means over three runs; the complete results, including standard deviations, are provided in \cref{tab:natural-img-full}.}
\label{tab:natural-img}
\begin{center}
\begin{small}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\diagbox[]{Dataset}{Model}} & \multicolumn{3}{c|}{\textbf{ResNet-18}} & \multicolumn{3}{c|}{\textbf{ResNet-50}} & \multicolumn{3}{c|}{\textbf{ResNet-152}} \\ \cline{2-10} 
& ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} \\ \hline
Arabic Characters & 86.46 & \textbf{92.11} & \textcolor{gray}{0.70} 
                  & 88.02 & \textbf{89.87} & \textcolor{gray}{0.91} 
                  & 87.86 & \textbf{88.70} & \textcolor{gray}{0.95} \\ \hline
Arabic Digits     & 97.92 & \textbf{98.92} & \textcolor{gray}{0.72} 
                  & 98.70 & \textbf{98.79} & \textcolor{gray}{0.87} 
                  & 98.23 & \textbf{98.55} & \textcolor{gray}{0.95} \\ \hline
Beans             & 85.94 & \textbf{94.53} & \textcolor{gray}{0.60} 
                  & 93.75 & \textbf{94.79} & \textcolor{gray}{0.94} 
                  & 91.41 & \textbf{93.75} & \textcolor{gray}{0.91} \\ \hline
CUB-200-2011      & 62.93 & \textbf{63.60} & \textcolor{gray}{0.90} 
                  & 66.09 & \textbf{66.57} & \textcolor{gray}{0.93} 
                  & 68.76 & \textbf{69.74} & \textcolor{gray}{0.94} \\ \hline
DTD               & 64.38 & \textbf{64.50} & \textcolor{gray}{0.92} 
                  & 70.46 & \textbf{70.82} & \textcolor{gray}{0.95} 
                  & 70.48 & \textbf{70.57} & \textcolor{gray}{0.98} \\ \hline
Fashion MNIST     & 88.54 & \textbf{89.52} & \textcolor{gray}{0.87} 
                  & 90.99 & \textbf{91.30} & \textcolor{gray}{0.94} 
                  & 90.48 & \textbf{90.84} & \textcolor{gray}{0.93} \\ \hline
FGVC-Aircraft     & 43.75 & \textbf{48.30} & \textcolor{gray}{0.77} 
                  & 47.62 & \textbf{51.09} & \textcolor{gray}{0.89} 
                  & 49.93 & \textbf{50.35} & \textcolor{gray}{0.94} \\ \hline
Flowers102        & 87.80 & \textbf{87.96} & \textcolor{gray}{0.86} 
                  & 89.56 & 89.56 & \textcolor{gray}{1.00} 
                  & 88.97 & \textbf{89.15} & \textcolor{gray}{0.96} \\ \hline
Food101           & 59.70 & \textbf{60.48} & \textcolor{gray}{0.89} 
                  & 68.07 & \textbf{68.13} & \textcolor{gray}{0.97} 
                  & 70.95 & \textbf{71.02} & \textcolor{gray}{0.99} \\ \hline
\multirow{2}{*}{Avg Rel Improve \& $\beta$} & \multicolumn{2}{c|}{Rel Improve (\%)} & \textcolor{gray}{($\beta$)} & \multicolumn{2}{c|}{Rel Improve (\%)} & \textcolor{gray}{($\beta$)} & \multicolumn{2}{c|}{Rel Improve (\%)} & \textcolor{gray}{($\beta$)} \\ \cline{2-10} 
                  & \multicolumn{2}{c|}{3.53} 
                  & \textcolor{gray}{0.80}
                  & \multicolumn{2}{c|}{1.36} 
                  & \textcolor{gray}{0.93}
                  & \multicolumn{2}{c|}{0.77} 
                  & \textcolor{gray}{0.95} \\ \hline
\end{tabular}
}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Improving Generalization on Natural Image Datasets}\label{sec:exp-natural-img}
In this subsection, we evaluate the effectiveness of CT in improving model generalization across natural image classification datasets through two experiments:

\begin{enumerate}
    \item \textbf{Cross-Dataset Transfer:} We pretrain ResNet-18, ResNet-50, and ResNet-152 on MNIST, CIFAR-10, CIFAR-100, or ImageNet, apply CT to the pretrained model, and evaluate its performance when transferred to the remaining datasets\footnote{We exclude transfers to ImageNet due to computational costs.}. This experiment assesses CT’s impact when models are pretrained and transferred across datasets of varying sizes.
    \item \textbf{ImageNet-to-Multiple-Datasets Transfer:} We apply CT to ImageNet-pretrained ResNet-18, ResNet-50, and ResNet-152 before transferring them to 9 additional datasets: Arabic Characters \cite{el2017arabchar}, Arabic Digits \cite{el2016arabdigit}, Beans \cite{makerere2020beans}, CUB-200-2011 \cite{wah2011CUB}, DTD \cite{cimpoi14dtd}, Fashion MNIST \cite{xiao2017fashionmnist}, FGVC-Aircraft \cite{maji2013fgvc}, Flowers102 \cite{nilsback2008flowers102}, Food101 \cite{bossard14food101}. This experiment evaluates CT in a more realistic setting, where ImageNet-pretrained models are commonly used for downstream tasks.
\end{enumerate}

While CT is applied consistently, the transfer learning approach differs depending on the dataset. For the same dataset (test set evaluation), the model is directly tested after applying CT, whereas for a new dataset transfer, the classification layer is removed, and linear probing (via logistic regression) is used for evaluation.

Table~\ref{tab:gen} presents the results of the first experiment for ResNet-18. The complete results, including ResNet-50 and ResNet-152, are provided in \cref{tab:gen-full}. CT consistently improves generalization across models and datasets, yielding average relative accuracy gains of 1.68\%, 1.96\%, and 0.40\% for ResNet-18, ResNet-50, and ResNet-152, respectively. Notably, in 25 out of 27 transfer cases, CT enhances accuracy. Even when evaluated on the test set of the pretraining dataset—where distribution shift is minimal—CT still provides improvements in 50\% of cases, albeit with reduced effectiveness. Additionally, the average $\beta$ values\footnote{Computed only for cases where improvements are observed.} for the three models are 0.82, 0.89, and 0.95—values close to 1. This suggests that CT efficiently identifies an appropriate $\beta$.

To validate CT’s robustness\footnote{Referring to the robustness of CT itself, not the models.} under different linear probing configurations, we provide additional experiments in \cref{app:exp-natural-img}.

The results for the second experiment is shown in \cref{tab:natural-img}. CT improves performance in 26 out of 27 cases, achieving average relative improvements of 3.53\%, 1.36\%, and 0.77\% for ResNet-18, ResNet-50, and ResNet-152, respectively. These improvements even exceed those in the first part of the experiment, further highlighting CT’s effectiveness in real-world generalization scenarios. Moreover, the average $\beta$ values are 0.80, 0.93, and 0.95, respectively, once again demonstrating CT’s efficiency. Additionally, we provide visualizations of accuracy trends during the $\beta$ search process in \cref{fig:acc-trend}, showing a sharp increase leading to a distinct peak, followed by a gradual decline as $\beta$ increases.

In summary, our results demonstrate that CT effectively enhances model generalization across natural image classification datasets. In the following section, we extend this analysis to medical image datasets and more fine-grained tasks to further assess CT’s impact on generalization.

\subsection{Improving Generalization on Medical Image Datasets and Fine-grained Tasks}\label{sec:exp-med-task}
In this subsection, we further evaluate CT’s impact on model generalization in more complex scenarios, specifically on medical image datasets and more fine-grained tasks than single-label classification:

\begin{enumerate}
    \item \textbf{Medical Image Datasets:} We apply CT to ImageNet-pretrained ResNet-18, ResNet-50, and ResNet-152 before transferring them to three medical image datasets: PathMNIST, OCTMNIST, and DermaMNIST from MedMNIST \cite{yang2023medmnistv2}.\item \textbf{Fine-grained Tasks: } We apply CT to ImageNet-pretrained ResNet-18, ResNet-50, and ResNet-152 for more fine-grained downstream tasks beyond single-label classification, including multi-label prediction on CelebA \cite{liu2015celeba}, regression on dSprites \cite{Matthey2017dsprites}\footnote{We regress the orientation of the shapes in dSprites.}, and semantic segmentation on VOC2012 \cite{everingham2012voc}\footnote{Only tested on ResNet-50 due to the architecture of the PSPNet used.}. Detailed settings are provided in \cref{app:exp-med-task}.
\end{enumerate}

The results, summarized in \cref{tab:gen-med-task}, show that CT consistently enhances generalization across these more challenging datasets and tasks. CT achieves average relative improvements of 2.69\%, 1.74\%, and 4.25\% for ResNet-18, ResNet-50, and ResNet-152, respectively. Additionally, the average $\beta$ values—0.83, 0.86, and 0.85—further underscore CT’s effectiveness and efficiency in complex scenarios.

Furthermore, we compare per-attribute accuracy, balanced accuracy, and F1-score of an ImageNet-pretrained ResNet-18/50/152 when transferred to multi-label prediction on CelebA, evaluating two selection methods for $\beta$: one optimized per attribute based on the best metric value and one using a globally optimal $\beta$ selected for the highest mean accuracy across attributes. \cref{tab:per-attr-celeba} presents the partial results for ResNet-18, while complete results for all models are provided in \cref{tab:per-attr-celeba-micro-full-resnet18}, \cref{tab:per-attr-celeba-macro-full-resnet18}, \cref{tab:per-attr-celeba-micro-full-resnet50}, \cref{tab:per-attr-celeba-macro-full-resnet50}, \cref{tab:per-attr-celeba-micro-full-resnet152} and \cref{tab:per-attr-celeba-macro-full-resnet152}. The performance gap between the two methods is minimal: using a globally optimal $\beta$ results in an average relative reduction of 0.21\%/0.18\%/0.07\% in accuracy, 0.96\%/0.65\%/0.30\% in balanced accuracy, and 2.81\%/1.87\%/1.00\% in F1-score for ResNet-18/50/152 compared to per-attribute optimization. These results highlight the stability of CT in fine-grained downstream tasks like multi-label prediction.

In conclusion, we demonstrate that CT significantly improves model generalization even in more challenging settings, including medical imaging and fine-grained tasks. Next, we investigate its role in enhancing model robustness.

\begin{table}[h]
  \centering
  \caption{\small Performance of ImageNet-pretrained ResNet-18, ResNet-50, and ResNet-152 when transferred to challenging medical image datasets and fine-grained tasks (\textbf{bold} entries indicate improvement with CT). \textbf{CT consistently improves generalization across diverse datasets and tasks.} Reported values are means over three runs; the complete results, including standard deviations, are provided in \cref{tab:gen-med-task-full}.}
  \label{tab:gen-med-task}
  \begin{subtable}[]{\linewidth}
      \centering
      \caption{\small \textbf{ResNet-18.} Avg rel improve: 2.69\%. Avg $\beta$: 0.83.}
      \label{tab:gen-med-task-ResNet-18}
      \resizebox{\linewidth}{!}{%
      \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Dataset & Metric & ReLU & + CT & Rel Improve (\%) & \textcolor{gray}{($\beta$)}\\
        \hline
        PathMNIST    & Acc (\%) $\uparrow$ & 86.21 & \textbf{87.27} 
                     & 1.23 & \textcolor{gray}{0.81} \\
        \hline
        OCTMNIST     & Acc (\%) $\uparrow$ & 65.47 
                     & \textbf{69.00} & 5.40 & \textcolor{gray}{0.80} \\
        \hline
        DermaMNIST   & Acc (\%) $\uparrow$ & 73.44
                     & \textbf{77.74} & 5.86 & \textcolor{gray}{0.80} \\
        \hline
        CelebA       & Mean Acc (\%) $\uparrow$ & 87.88  & \textbf{88.44} 
                     & 0.64 & \textcolor{gray}{0.75} \\
        \hline
        dSprites     & MSE $\downarrow$ & 4.09 & \textbf{4.08} 
                     & 0.33 & \textcolor{gray}{0.98} \\
        \hline
        VOC2012      & mIoU $\uparrow$ & - & - 
                     & - & - \\
        \hline
      \end{tabular}
      }
  \end{subtable}

  \vspace{0.5em}

  \begin{subtable}[]{\linewidth}
      \centering
      \caption{\small \textbf{ResNet-50.} Avg rel improve: 1.74\%. Avg $\beta$: 0.86.}
      \resizebox{\linewidth}{!}{%
      \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Dataset & Metric & ReLU & + CT & Rel Improve (\%) & \textcolor{gray}{($\beta$)}\\
        \hline
        PathMNIST    & Acc (\%) $\uparrow$ & 89.83 & \textbf{89.88} 
                     & 0.06 & \textcolor{gray}{0.98} \\
        \hline
        OCTMNIST     & Acc (\%) $\uparrow$ & 68.60 
                     & \textbf{69.93} & 1.94 & \textcolor{gray}{0.90} \\
        \hline
        DermaMNIST   & Acc (\%) $\uparrow$ & 73.67
                     & \textbf{77.44} & 5.12 & \textcolor{gray}{0.89} \\
        \hline
        CelebA       & Mean Acc (\%) $\uparrow$ & 89.18 & \textbf{89.42} 
                     & 0.27 & \textcolor{gray}{0.91} \\
        \hline
        dSprites     & MSE $\downarrow$ & 4.40 & \textbf{4.28} 
                     & 2.62 & \textcolor{gray}{0.53} \\
        \hline
        VOC2012      & mIoU $\uparrow$ & 0.68 & \textbf{0.69} 
                     & 0.41 & \textcolor{gray}{0.96} \\
        \hline
      \end{tabular}
      }
  \end{subtable}

  \vspace{0.5em}

  \begin{subtable}[]{\linewidth}
      \centering
      \caption{\small \textbf{ResNet-152.}  Avg rel improve: 4.25\%. Avg $\beta$: 0.85.}
      \resizebox{\linewidth}{!}{%
      \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Dataset & Metric & ReLU & + CT & Rel Improve (\%) & \textcolor{gray}{($\beta$)}\\
        \hline
        PathMNIST    & Acc (\%) $\uparrow$ & 90.15 & \textbf{90.75} 
                     & 0.66 & \textcolor{gray}{0.92} \\
        \hline
        OCTMNIST     & Acc (\%) $\uparrow$ & 70.23 & \textbf{71.03} & 1.14 & \textcolor{gray}{0.98} \\
        \hline
        DermaMNIST   & Acc (\%) $\uparrow$ & 74.39
                     & \textbf{77.92} & 4.75 & \textcolor{gray}{0.93} \\
        \hline
        CelebA       & Mean Acc (\%) $\uparrow$ & 89.16 & \textbf{89.40} 
                     & 0.27 & \textcolor{gray}{0.92} \\
        \hline
        dSprites     & MSE $\downarrow$ & 4.46 & \textbf{3.82} 
                     & 14.27 & \textcolor{gray}{0.52} \\
        \hline
        VOC2012      & mIoU $\uparrow$ & - & - 
                     & - & - \\
        \hline
      \end{tabular}
      }
  \end{subtable}
\vskip -0.1in
\end{table}

\begin{table}[h]
  \centering
  \caption{\small Comparison of per-attribute accuracy, balanced accuracy, and F1-score of ImageNet-pretrained ResNet-18 when transferred to multi-label prediction on CelebA, evaluating two selection methods for $\beta$: one optimized per attribute (\textbf{Micro Best}) and one using a globally optimal $\beta$ (\textbf{Macro Best}) (\textbf{bold} values indicate cases where the metric remains the same under both selection methods). \textbf{The small performance gap between Micro Best and Macro Best demonstrates the stability of CT on more fine-grained downstream tasks.} Reported values are means over three runs for the first four attributes; the complete results for ResNet-18, ResNet-50, and ResNet-152, including standard deviations, are provided in \cref{tab:per-attr-celeba-micro-full-resnet18}, \cref{tab:per-attr-celeba-macro-full-resnet18}, \cref{tab:per-attr-celeba-micro-full-resnet50}, \cref{tab:per-attr-celeba-macro-full-resnet50}, \cref{tab:per-attr-celeba-micro-full-resnet152} and \cref{tab:per-attr-celeba-macro-full-resnet152}.}
  \label{tab:per-attr-celeba}

    \begin{subtable}[]{\linewidth}
        \centering
        \caption{\small \textbf{Micro Best.}}
        \label{tab:per-attr-celeba-micro}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{|c|c|c|c|}
        \hline
        \diagbox[]{Attribute}{Metric} & Accuracy (\%) & Balanced Accuracy (\%) & F1 (\%)\\ \hline
        5 Shadow           & 91.54 & 65.91 & 44.16 \\ \hline
        Arch. Eyebrows     & 79.63 & 72.46 & 60.91 \\ \hline
        Attractive         & 79.44 & 79.47 & 80.15 \\ \hline
        Bags Un. Eyes      & 82.80 & 65.29 & 45.80 \\ \hline
        \end{tabular}
        }
    \end{subtable}

  \vspace{0.5em}

    \begin{subtable}[]{\linewidth}
        \centering
        \caption{\small \textbf{Macro Best.}}
        \label{tab:per-attr-celeba-macro}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{|c|c|c|c|}
        \hline
        \diagbox[]{Attribute}{Metric} & Accuracy (\%) & Balanced Accuracy (\%) & F1 (\%)\\ \hline
        5 Shadow           & 91.34 & 65.57 & 43.56 \\ \hline
        Arch. Eyebrows     & 79.34 & 71.97 & 60.14 \\ \hline
        Attractive         & 79.41 & 79.46 & 80.14 \\ \hline
        Bags Un. Eyes      & \textbf{82.80} & 65.12 & 45.48 \\ \hline
        Avg Rel Reduct (\%) & 0.16 & 0.37 & 0.83 \\ \hline
        \end{tabular}
        }
    \end{subtable}
    \vskip -0.1in
\end{table}

\subsection{Improving Robustness on Adversarial and Corrupted Data}\label{sec:exp-rob}
\begin{table*}[t]
  \centering
  \caption{\small Robust accuracy of ImageNet-pretrained ResNet-18, ResNet-50, and ResNet-152 under $\ell_2$/$\ell_\infty$ adversarial attacks and common corruptions on CIFAR-10, CIFAR-100, and ImageNet (\textbf{bold} entries indicate improvement with CT). \textbf{CT consistently enhances robustness across models, datasets, and robustness settings, with $\beta$ values close to 1.} Reported values are means over three runs; the complete results, including standard deviations, are provided in \cref{tab:rob-full}.}
  \label{tab:rob}

    \begin{subtable}[]{0.65\linewidth}
        \centering
        \caption{\small \textbf{ResNet-18.} Avg rel improve: 11.76\%. Avg $\beta$: 0.92.}
        \label{tab:rob-ResNet-18}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
          \hline
          \multirow{2}{*}{\diagbox[]{Dataset}{Attack}} & \multicolumn{3}{c|}{$\ell_2$} 
            & \multicolumn{3}{c|}{$\ell_\infty$} & \multicolumn{3}{c|}{Corruption} \\
          \cline{2-10}
          & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} 
          & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} \\
          \hline
          CIFAR-10     & 53.67 & 53.67 & \textcolor{gray}{1.00} 
                       & 11.17 & \textbf{14.93} & \textcolor{gray}{0.90} 
                       & 77.73 & 77.73 & \textcolor{gray}{1.00} \\
          \hline
          CIFAR-100    & 24.30 & \textbf{25.50} & \textcolor{gray}{0.92}
                       & 4.47 & \textbf{6.90} & \textcolor{gray}{0.92} 
                       & 51.81 & \textbf{51.95} & \textcolor{gray}{0.94} \\
          \hline
          ImageNet     & 23.37 & 23.37 & \textcolor{gray}{1.00} 
                       & 0.00 & \textbf{7.00} & \textcolor{gray}{0.89} 
                       & 33.11 & \textbf{33.32} & \textcolor{gray}{0.92} \\
          \hline
        \end{tabular}
        }
    \end{subtable}

  \vspace{0.5em}

    \begin{subtable}[]{0.65\linewidth}
        \centering
        \caption{\small \textbf{ResNet-50.} Avg rel improve: 348.44\%. Avg $\beta$: 0.95.}
        \label{tab:rob-ResNet-50}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
          \hline
          \multirow{2}{*}{\diagbox[]{Dataset}{Attack}} & \multicolumn{3}{c|}{$\ell_2$} 
            & \multicolumn{3}{c|}{$\ell_\infty$} & \multicolumn{3}{c|}{Corruption} \\
          \cline{2-10}
          & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} 
          & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} \\
          \hline
          CIFAR-10     & 55.10 & \textbf{56.53} & \textcolor{gray}{0.97} 
                       & 10.10 & \textbf{14.83} & \textcolor{gray}{0.95} 
                       & 77.26 & 77.26 & \textcolor{gray}{1.00} \\
          \hline
          CIFAR-100    & 23.83 & \textbf{25.80} & \textcolor{gray}{0.96} 
                       & 4.43 & \textbf{7.90} & \textcolor{gray}{0.93} 
                       & 53.91 & \textbf{53.93} & \textcolor{gray}{0.98} \\
          \hline
          ImageNet     & 31.90 & 31.90 & \textcolor{gray}{1.00} 
                       & 0.30 & \textbf{9.30} & \textcolor{gray}{0.93} 
                       & 39.64 & 39.64 & \textcolor{gray}{1.00} \\
          \hline
        \end{tabular}
        }
    \end{subtable}

  \vspace{0.5em}

    \begin{subtable}[]{0.65\linewidth}
        \centering
        \caption{\small \textbf{ResNet-152.} Avg rel improve: 498.41\%. Avg $\beta$: 0.98.}
        \label{tab:rob-ResNet-152-full}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
          \hline
          \multirow{2}{*}{\diagbox[]{Dataset}{Attack}} & \multicolumn{3}{c|}{$\ell_2$} 
            & \multicolumn{3}{c|}{$\ell_\infty$} & \multicolumn{3}{c|}{Corruption} \\
          \cline{2-10}
          & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} 
          & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} \\
          \hline
          CIFAR-10     & 56.27 & 56.27 & \textcolor{gray}{1.00} 
                       & 11.47 & \textbf{15.00} & \textcolor{gray}{0.99} 
                       & 78.82 & \textbf{78.83} & \textcolor{gray}{0.99} \\
          \hline
          CIFAR-100    & 27.90 & \textbf{28.23} & \textcolor{gray}{0.98} 
                       & 5.40 & \textbf{7.70} & \textcolor{gray}{0.99} 
                       & 56.12 & 56.12 & \textcolor{gray}{1.00} \\
          \hline
          ImageNet     & 42.50 & 42.50 & \textcolor{gray}{1.00} 
                       & 0.30 & \textbf{13.53} & \textcolor{gray}{0.97} 
                       & 45.47 & 45.47 & \textcolor{gray}{0.99} \\
          \hline
        \end{tabular}
        }
    \end{subtable}
    \vskip -0.1in
\end{table*}
In this subsection, we demonstrate that CT enhances model robustness using RobustBench \cite{croce2020robustbench}, a standardized benchmark for evaluating model robustness. RobustBench includes both adversarial examples, such as $\ell_2$ and $\ell_\infty$-norm bounded perturbations, which measure a model’s resistance to adversarial changes, and naturally corrupted examples \cite{hendrycks2019corruption}, such as noise and fog, which assess model's robustness to real-world data distribution shifts.

To assess CT’s impact, we apply it to ImageNet-pretrained ResNet-18, ResNet-50, and ResNet-152 and evaluate their robustness on CIFAR-10, CIFAR-100, and ImageNet under both adversarial attacks ($\ell_2$ and $\ell_\infty$) and corruption-based distortions. For each dataset, we sample 1,000 instances for evaluation. More detailed settings are provided in \cref{app:exp-rob}.

As summarized in \cref{tab:rob-full}, CT consistently improves model robustness across the tested scenarios, achieving average relative improvements in robust accuracy\footnote{Cases where the ReLU baseline has zero robust accuracy are excluded from computation.} of 11.76\%, 348.44\%, and 498.41\% for ResNet-18, ResNet-50, and ResNet-152, respectively. Notably, the trend of increasing improvements as model size grows suggests that CT has the potential to be even more effective for larger models.

Moreover, the average $\beta$ values are even closer to 1 compared to the generalization experiments, with values of 0.92, 0.95, and 0.98 for the three models, highlighting CT’s efficiency in optimizing robustness.

These results demonstrate that CT effectively enhances the robustness of pretrained models against both adversarial perturbations and common corruptions, further reinforcing its practical benefits. Having demonstrated CT's impact on generalization and robustness in models that fully comply with the max-affine spline framework, where curvature tuning is provable from an end-to-end perspective, we now show that CT also works for transformers, where curvature tuning is only provable from a layer-wise perspective.



\subsection{Improving Generalization of Transformers}\label{sec:exp-transformer}
In this subsection, we demonstrate that CT is effective for transformers. Unlike models such as ResNets, where all layers adhere to the max-affine spline framework, transformers include attention layers that do not fit directly within this framework. Consequently, we lose strict theoretical guarantees when considering the end-to-end mapping of transformers. However, from a layer-wise perspective, the feed-forward layer combined with the activation function (if convex and piecewise-affine like ReLU) retains partial theoretical guarantees.  

To show that CT remains effective even in cases with only partial theoretical guarantees, we modify the Swin Transformer \cite{liu2021swin} by replacing all GELU activations following the feed-forward layers with ReLU, enabling CT to be applied. The network is then pretrained on Imagenette \cite{imagenette} which is a subset of 10 easily classified classes from Imagenet and transferred to the 9 downstream datasets used in the \textbf{ImageNet-to-Multiple-Datasets Transfer} experiment in \cref{sec:exp-natural-img}. Further details on the experimental setup are provided in \cref{app:exp-transformer}.  

As shown in \cref{tab:gen-swin}, CT improves the generalization of transformers even when partial theoretical guarantees are available. Specifically, CT achieves relative improvements of 2.43\% on Swin-T and 3.33\% on Swin-S, with average \(\beta\) values of 0.92 and 0.94, respectively, demonstrating CT's effectiveness and efficiency once again.  

\begin{table}[h]
\caption{\small Accuracy of Imagenette-pretrained Swin-T and Swin-S (ReLU-based) when transferred to 9 downstream datasets (\textbf{bold} entries indicate improvement with CT). \textbf{CT consistently enhances generalization across models and datasets, with $\beta$ values close to 1, demonstrating its effectiveness even with partial theoretical guarantees.} Reported values are means over three runs; the complete results, including standard deviations, are provided in \cref{tab:gen-swin-full}.}
\label{tab:gen-swin}
\begin{center}
\begin{small}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\diagbox[]{Dataset}{Model}} & \multicolumn{3}{c|}{Swin-T} & \multicolumn{3}{c|}{Swin-S} \\ \cline{2-7} 
& ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} & ReLU (\%) & + CT (\%) & \textcolor{gray}{($\beta$)} \\ \hline
Arabic Characters & 43.08 & \textbf{45.14} & \textcolor{gray}{0.92}
                  & 43.90 & \textbf{44.70} & \textcolor{gray}{0.97} \\ \hline
Arabic Digits     & 90.38 & \textbf{91.46} & \textcolor{gray}{0.86} 
                  & 88.74 & \textbf{89.15} & \textcolor{gray}{0.95}  \\ \hline
Beans             & 75.00 & \textbf{82.03} & \textcolor{gray}{0.85} 
                  & 66.41 & \textbf{71.09} & \textcolor{gray}{0.83}  \\ \hline
CUB-200-2011      & 6.97 & \textbf{7.02} & \textcolor{gray}{0.93} 
                  & 6.40 & \textbf{6.70} & \textcolor{gray}{0.94}  \\ \hline
DTD               & 21.51 & \textbf{21.70} & \textcolor{gray}{0.93} 
                  & 20.59 & \textbf{21.28} & \textcolor{gray}{0.94} \\ \hline
Fashion MNIST     & 78.61 & \textbf{79.08} & \textcolor{gray}{0.92}
                  & 77.48  & \textbf{77.64} & \textcolor{gray}{0.95} \\ \hline
FGVC-Aircraft     & 8.13 & \textbf{8.31} & \textcolor{gray}{0.98} 
                  & 7.12 & \textbf{7.70} & \textcolor{gray}{0.96} \\ \hline
Flowers102        & 23.77 & \textbf{24.19} & \textcolor{gray}{0.94}
                  & 22.29 & \textbf{23.01} & \textcolor{gray}{0.95} \\ \hline
Food101           & 17.35 & \textbf{17.41} & \textcolor{gray}{0.98} 
                  & 17.11 & \textbf{17.29} & \textcolor{gray}{0.95}  \\ \hline
\multirow{2}{*}{Avg Rel Improve \& $\beta$} & \multicolumn{2}{c|}{Rel Improve (\%)} & \textcolor{gray}{($\beta$)} & \multicolumn{2}{c|}{Rel Improve (\%)} & \textcolor{gray}{($\beta$)} \\ \cline{2-7} 
                  & \multicolumn{2}{c|}{2.43} 
                  & \textcolor{gray}{0.92}
                  & \multicolumn{2}{c|}{3.33} 
                  & \textcolor{gray}{0.94} \\ \hline
\end{tabular}
}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

With CT's effectiveness on improving the generalization of transformers demonstrated, even with partial theoretical guarantees, we now proceed to ablation studies to validate CT's implementation.

\subsection{Ablation Studies}\label{sec:exp-abl}
To assess the impact of CT’s formulation, we conduct ablation studies on the two smoothing mechanisms introduced in \cref{sec:beta-vq}: reparameterized Swish and SoftPlus. While each independently improves generalization and robustness, their combination proves the most effective.

We evaluate the effect of using only Swish or only SoftPlus in CT on the generalization experiment (transferring ImageNet-pretrained ResNets to 13 downstream tasks) and the robustness experiment (evaluating on RobustBench). The results in \cref{tab:gen-abl-full} and \cref{tab:rob-abl-full} show that while both individual components provide improvements (0.23\% and 2.96\% for Swish and SoftPlus in generalization; 8.06\% and 9.91\% in robustness), the full CT formulation achieves the highest gains (3.46\% in generalization; 11.76\% in robustness). These findings align with our theoretical insights in \cref{sec:proof}, which show that combining both functions helps mitigate decision boundary drift.

In summary, we demonstrate the effectiveness and efficiency of CT in improving model generalization and robustness. Furthermore, our ablation studies empirically validate the theoretical findings in \cref{sec:proof}. Next, we summarize our results and discuss future directions.