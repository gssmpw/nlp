In this section, we introduce our proposed method, Curvature Tuning (CT). We first dive into its motivation and construction in \cref{sec:beta-vq,sec:proof}, followed by implementation details in \cref{sec:ct-implement}. Readers focused on practical applications of CT will find our experiments in \cref{sec:exp}.


\subsection{The $\beta$-VQ Inference Framework}\label{sec:beta-vq}
To understand the motivation behind CT, we first conduct an in-depth study of the max-affine spline formulation from \cref{eq:maso}.

By inspecting \cref{eq:maso}, we observe that the mapping remains affine within each (implicitly defined) region where the pointwise maximum does not change. Specifically, for any input $\mathbf{x}$ where
\begin{align*}
    \arg\max_{r=1 \dots R} \big( \langle \mathbf{A}_{r,\cdot}, \mathbf{x} \rangle + \mathbf{b}_r \big),
\end{align*}
remains constant, all such inputs belong to the same region, as they share the same affine mapping. The nonlinearity of the function arises when transitioning between these regions. 


For instance, in the case of ReLU, where $R=2, \mathbf{A}_{2,\cdot}=\mathbf{0},$ and $\mathbf{b}_{2}=0$, the nonlinearity occurs along a hyperplane in the input space. Intuitively, a ReLU activation, which maps $\mathbb{R}\mapsto \mathbb{R}$, is nonlinear at $0$. When preceded by an affine transformation (such as linear or convolutional layer) with input dimension $D$, this nonlinearity occurs along a hyperplane of dimension $D-1$, defined by the layerâ€™s parameters mapping the input space to that unit.

{\bf Smoothing the nonlinearity by smoothing the spline region assignment process.}~Instead of going from one affine mapping to another in an abrupt fashion (whenever crossing that hyperplane), one may consider a smoother transition. As far as we are aware, there are two common practices to achieve that goal. To get into details into each of them, we must first provide a slightly different formulation of the max-affine spline mapping.

We know that each unit of a layer is a max-affine spline. The inference process of each unit can thus be decomposed into two steps:
\begin{enumerate}
    \item \textbf{VQ Inference Step (region selection)}: Determine the affine transformation that maximizes the output, which can be viewed as a vector quantization process. The decision is encoded in a selection variable $\mathbf{t} \in \mathbb{R}^R$, where $R$ is the number of input region partitions of the max-affine spline function. In a MASO setting, the selection variable $\mathbf{t}$ is a one-hot vector with the $r^*$-th entry set to 1, where:
    \begin{equation}
        r^* = \argmax_{r \in \{1, \dots, R\}} \big( \langle \mathbf{A}_{r,\cdot}, \mathbf{x} \rangle + \mathbf{b}_r \big).
    \end{equation}
    \item \textbf{Computation Step (affine transformation)}: Compute the output of the neuron based on the selection variable \( \mathbf{t} \):
    \begin{equation}\label{eq:computation}
        f(x) = \sum_{r=1}^R \mathbf{t}_r \cdot \big( \langle \mathbf{A}_{r,\cdot}, \mathbf{x} \rangle + \mathbf{b}_r \big).
    \end{equation}
\end{enumerate}
As discussed, the affine transformation is selected in a "hard" manner where only the transformation that maximizes the output is chosen. Alternatively, a "soft" approach can be employed in which the selection variable $\mathbf{t}$ is no longer a one-hot vector but is inferred in a probabilistic manner. To see that, we follow the probabilistic formulation from \cite{balestriero2018hard} as introduce the following regularize region selection problem
\begin{align}\label{eq:svq}
        \mathbf{t}_{\beta} =& \argmax_{\mathbf{t} \in \Delta_R}\beta \sum_{r=1}^R \mathbf{t}_r \cdot \big( \langle \mathbf{A}_{r,\cdot}, \mathbf{x} \rangle + \mathbf{b}_r \big) + (1-\beta) H(\mathbf{t}),\\
        =&\frac{\mathrm{e}^{\beta(\langle \mathbf{A}_{r,\cdot}, \mathbf{x} \rangle + \mathbf{b}_r)/(1-\beta)}}{\sum_{i=1}^R \mathrm{e}^{\beta(\langle \mathbf{A}_{i,\cdot}, \mathbf{x} \rangle + \mathbf{b}_i)/(1-\beta)}}\nonumber
    \end{align}
    where $H(\mathbf{t})$ represents the Shannon entropy of the selection variable, and $\Delta_R$ is the simplex in $\mathbb{R}^R$. With the Computation Step in \cref{eq:computation} and using a ReLU activation function, switching from $\beta=1$ to $\beta=0.5$ is provably equivalent to replacing the ReLU with a sigmoid Gated Linear Unit. And in the limit of employing $\beta=0$, the activation function becomes linear--and so does the entire input-output mapping of the DN.

% \begin{proposition}
%     The MASO parameters $A$, $B$ that induce the ReLU activation under HVQ induce the SiLU under SVQ \cite{balestriero2018hard}.
% \end{proposition}




% the $\beta$-VQ inference framework, which is grounded in spline theory. Here, VQ refers to vector quantization. Interested readers can refer to \cite{balestriero2018hard} for a more detailed explanation. In essence, the $\beta$-VQ inference framework demonstrates that changing the activation function corresponds to altering the mechanism for selecting which affine transformations are applied to the input within the activation layer.

% More specifically, take a model that is stacked from layers of the type described in \cref{thm:spline}. From \cref{thm:spline} we know that each layer is a MASO, and we can view the forward process for each output neuron, which is a max-affine spline function $s[a, b]$, by decomposing it into two steps (taking the $k$-th neuron as an example here):



% Both HVQ and SVQ selection variables can be inferred in a monolithic manner by solving their respective optimization problems:
% \begin{proposition}
%     The selection variable in HVQ can be inferred by solving:
%     \begin{equation}\label{eq:hvq}
%         t = \argmax_{t \in \Delta_R}\sum_{r=1}^R t_r \cdot \big( \langle a_{r,\cdot}, x \rangle + b_r \big),
%     \end{equation}
%     where $\Delta_R$ is a $R$-simplex \cite{balestriero2018hard}.
% \end{proposition}
% \begin{proposition}
%     The selection variable in SVQ can be inferred by solving:
%     \begin{equation}\label{eq:svq}
%         t = \argmax_{t \in \Delta_R}\sum_{r=1}^R t_r \cdot \big( \langle a_{r,\cdot}, x \rangle + b_r \big) + H(t),
%     \end{equation}
%     where $H(t)$ represents the Shannon entropy of the selection variable \cite{balestriero2018hard}.
% \end{proposition}

% By combining the maximization problems in \cref{eq:hvq} and \cref{eq:svq}, we define the $\beta$-VQ inference:
% \begin{equation}\label{eq:bvq}
%     t = \argmax_{t \in \Delta_R} \beta \cdot \sum_{r=1}^R t_r \cdot \big( \langle a_{r,\cdot}, x \rangle + b_r \big) + (1 - \beta) \cdot H(t),
% \end{equation}
% where $\beta \in [0, 1]$ controls the way of inference. Specifically:
% \begin{itemize}
%     \item When $\beta=1$, $\beta$-VQ recovers HVQ.
%     \item When $\beta=0.5$, $\beta$-VQ recovers SVQ.
%     \item When $\beta=0$, $\beta$-VQ assigns equal weights to all affine transformations, such that $t_r = \frac{1}{R}, \forall r \in \{1, 2, \dots, R\}$, reducing the layer to a simple linear layer.
% \end{itemize}

% Similarly, switching from HVQ to $\beta$-VQ is equivalent to changing from a ReLU layer to a reparameterized Swish layer:
% \begin{proposition}
%     The MASO parameters $A$, $B$ that induce the ReLU activation under HVQ induce the reparameterized Swish function \( Swish(x) = \sigma(\frac{\beta}{1 - \beta} x) \cdot x \) under $\beta$-VQ \cite{balestriero2018hard}.
% \end{proposition}

% Thus, by tuning $\beta$ in the reparameterized Swish function, the VQ inference step interpolates between HVQ ($\beta = 1$), SVQ ($\beta = 0.5$) and uniform weighting ($( t_r = \frac{1}{R} \, \forall r \in \{1, 2, \dots, R\} $), which corresponds to a simple linear layer.

\begin{figure*}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{fig/activation_functions.pdf}}
\caption{\small Visualization of nonlinearity smoothing through region assignment smoothing, max smoothing, and their combination. \textbf{The combined approach mitigates the opposing biases introduced by the individual methods.}}
\label{fig:examples}
\end{center}
\vskip -0.1in
\end{figure*}

{\bf Smoothing the nonlinearity by smoothing the max.}~As previously mentioned, there is an alternative way to smooth the max-affine spline mapping from \cref{eq:maso}. Instead of relying on a soft region assignment, we can instead directly smooth the maximum function. It is already well known that smoothing the maximum operator leads to the log-sum-exp operator. Hence, the mapping from \cref{eq:maso} now becomes
\begin{align}
     (1-\beta)\log(\sum_{r=1}^{R}\exp^{(\langle \mathbf{A}_{r,\cdot}, \mathbf{x} \rangle + \mathbf{b}_r)/(1-\beta)}),\label{eq:lse}
\end{align}
where we parametrized the mapping so that its behavior is akin to \cref{eq:svq}, a value of $\beta \rightarrow 1$ recovers the original affine spline activation, e.g., ReLU.

The crucial observation that we make is that both parametrizations have a tendency to shift the mean of the output of the unit either by a negative factor (for \cref{eq:svq}) or by a positive factor (for \cref{eq:lse}). This means that in very deep models, varying $\beta$ with either parametrization produces a shift in the decision boundary or regression that can not be recovered unless the parameters are trained once again--which we are trying to avoid. As a result, and as will be thoroughly detailed in \cref{sec:ct-implement}, our implementation will leverage the average of the two parametrizations, mitigating that bias as depicted in \cref{fig:examples}.

\subsection{Provably Tuning Decision Curvature and Mitigating Drift}\label{sec:proof}


Prior to deriving our proposed CT methodology relying on the smoothness of activation functions derived in \cref{sec:beta-vq}, we propose some characterization of a model's curvature as a function of $\beta$.


We start by recalling the observations that for both parametrizations, we have that as $\beta \rightarrow 0$ as the activation becomes linear. Because all current DNs can be formulated as simple compositions of activation functions interleaved with affine operators, it is direct to see that the entire input-output mapping also becomes a simple affine mapping when $\beta \rightarrow 0$. In that setting, the curvature of the mapping--defined as the norm of the Hessian matrix of the mapping--will be $0$. As a result, we see that as we go from the original DN mapping ($\beta=1$) to the linear setting, we modulate the mapping curvature, and in particular we reduce it from its original value to $0$ in the limit. When considering a classification task, the output of the DN is processed by a linear classifier. However, it is clear that as the DN's mapping becomes more and more akin to a simple affine mapping, as the decision boundary also converges to being linear in the input space. This is exemplified in \cref{fig:CT}.


\subsection{Curvature Tuning (CT): Implementation}\label{sec:ct-implement}
The implementation of CT is straightforward building upon \cref{sec:beta-vq} (PyTorch implementation in \cref{app:code}). To apply CT, we replace all ReLU activation functions in the pretrained model with a custom activation function defined as:
\begin{equation}\label{eq:CT}
    f(x) = 0.5  \sigma\left(\frac{\beta x}{1 - \beta}\right) \cdot x + 0.5  \log_\mathrm{e}\left(1 + \mathrm{e}^{\frac{x}{1 - \beta}}\right) \cdot (1 - \beta),
\end{equation}
where  $\sigma(\cdot)$ represents the sigmoid function. 

This activation function is essentially a convex combination of a reparameterized Swish function and a reparameterized SoftPlus function, defined as:
\begin{equation}\label{eq:reparam-swish}
    \text{Swish}(x) = \sigma(\eta x) \cdot x, \quad \eta = \frac{\beta}{1 - \beta},
\end{equation}
\begin{equation}\label{eq:reparam-softplus}
    \text{SoftPlus}(x) = \frac{1}{\gamma} \cdot \log\left(1 + \mathrm{e}^{\gamma x}\right), \quad \gamma = \frac{1}{1 - \beta}.
\end{equation}

Next, we determine the optimal value of $\beta$ by performing forward passes on the test set, evaluating the model's performance for each candidate $\beta$ in a predefined range, and selecting the value that corresponds to the best-performing model. This process eliminates the need for additional training(i.e. backpropagation), making CT computationally efficient.