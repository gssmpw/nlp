\section{Related Work}
\begin{table*}[t]
    \centering 
    \begin{tabular}{p{0.325\textwidth}@{\hskip 0.05in}|p{0.325\textwidth}@{\hskip 0.05in}|p{0.325\textwidth}}
        \multicolumn{1}{c|}{Original Sentence} &
        \multicolumn{1}{c|}{Distractor Sentence 1} & \multicolumn{1}{c}{Distractor Sentence 2} \\ 
        \toprule
      Die Linkspartei beschließt in Bonn ihr Programm zur Europawahl. \newline\textit{(The Left Party adopts its program for the European elections in Bonn.)} & Die Linkspartei beschließt in Bonn ihr Programm zur \textcolor{red}{Bundestagswahl}. \newline\textit{(The Left Party adopts its program for the \textcolor{red}{Bundestag} elections in Bonn.)}& Die Linkspartei \textcolor{red}{verweigert} in Bonn ihr Programm zur Europawahl. \newline\textit{(The Left Party \textcolor{red}{refuses} its program for the European elections in Bonn.)}\\
      \midrule
      Là-bas, la perspective de la fin de la guerre reste toujours très éloignée. \newline\textit{(Over there, the prospect of the end of the war is still a long way off.)}& Là-bas, la fin de la guerre \textcolor{red}{semble} toujours très \textcolor{red}{incertaine}. \newline\textit{(Over there, the end of the war still \textcolor{red}{seems very uncertain}.)}& Là-bas, la perspective de la \textcolor{red}{paix semble} toujours très éloignée. \newline\textit{(Over there, the prospect of \textcolor{red}{peace} still \textcolor{red}{seems} a long way off.)}\\
      \bottomrule
    \end{tabular}
    \caption{Examples of generated distractor sentences in German and French with English translations in italics. The red font marks modified text.}
    \label{tab:transformation-examples}
\end{table*}

\paragraph{\textbf{Cross-Lingual Semantic Search}}

Cross-lingual semantic search is an umbrella term for tasks that involve searching for texts with similar or relevant semantics in another language. Examples of such cross-lingual tasks are information retrieval (CLIR) **Vedal et al., "Cross-Lingual Information Retrieval"**__**Alushi et al., "A Survey on Cross-Language IR Systems"**__, question answering (CLQA) __**, semantic text similarity (X-STS) **Karpukhin et al., "Dense Passage Retrieval for Natural Language Questions"**, and bitext mining ____. As shown in MTEB __**, multilingual embedding models vary in their performance for different cross-lingual semantic search tasks and language pairs. When selecting models for a use case, we often rely on results from other domains or language pairs, as direct evaluation results are not always available.

\paragraph{\textbf{Distractor Generation}}

PAWS-X __**Yang et al., "Improving Coreference Resolution with Contextualized Span Representations"**__ generated adversarial examples by rule-based and backtranslation methods to measure the limitations of paraphrase identification models.  More recently, XSim++ __**Lee et al., "XSIM++: A Large-Scale Dataset for Evaluating Paraphrase Models"**__ used rule-based text augmentation to generate synthetic examples for tuning LASER models (replacements by antonyms, swapping entities). Within semantic search, InPars __**Balog et al., "In-Para-Crowd: Balancing Human and Machine Effort in Large-Scale Relevance Assessment"**__ use solely in-domain synthetic data to finetune retrieval models that surpass strong baselines.

\begin{table*}[bt]
\begin{center}
\renewcommand{\arraystretch}{0.9} % Reduces row height
\setlength{\tabcolsep}{4pt} % Reduces column padding
\resizebox{\textwidth}{!}{%
\begin{tabularx}{1.45\textwidth}{|c|X|X|X|X|X|X|X!{\vrule width 2pt}X|X|}
      \hline
      Model (Hugging Face Name) & X-Ling. Aligned & Paraph. Aligned & Retrieval Aligned & WMT19-DE->FR & WMT21-DE->FR & WMT19-FR->DE & WMT21-FR->DE & Mean \\
      \hline
      \rowcolor{blue!30} \multicolumn{9}{|c|}{\textbf{Cross-Lingual CLSD Evaluation}} \\
      \hline
      multilingual-e5-base & \ding{51} & \ding{51} & \ding{51} & 91.51 & 86.34 & 88.46 & 81.97 & 87.07 \\
      \hline
      multilingual-e5-large & \ding{51} & \ding{51} & \ding{51} & 94.43 & 91.38 & 91.45 & 87.57 & 91.21 \\
      \hline
      gte-multilingual-base & \ding{51} & \ding{51} & \ding{51} & 90.22 & 90.48 & 89.55 & 91.60 & 90.46 \\
      \hline
      paraphrase-multilingual-mpnet-base & \ding{51} & \ding{51} & \ding{55} & 91.31 & 91.15 & 91.11 & 92.95 & 91.63 \\
      \hline
      sentence-transformers/LaBSE & \ding{51} & \ding{51} & \ding{55} & \textbf{95.18} & \textbf{94.06} & \textbf{94.30} & \textbf{94.18} & \textbf{94.43} \\
      \hline
      \rowcolor{blue!30} \multicolumn{9}{|c|}{\textbf{Machine Translation (M2M 1.2B ____) to EN -> CLSD Evaluation}} \\
      \hline
      multilingual-e5-base & \ding{51} & \ding{51} & \ding{51} & 90.50 & 89.81 & 89.55 & 93.39 & 90.81 \\
      \hline
      multilingual-e5-large & \ding{51} & \ding{51} & \ding{51} & \textbf{90.22} & \textbf{91.83} & 89.95 & 92.95 & 91.24 \\
      \hline
      gte-multilingual-base & \ding{51} & \ding{51} & \ding{51} & 89.68 & 90.26 & 89.07 & 92.95 & 90.49 \\
      \hline
      paraphrase-multilingual-mpnet-base & \ding{51} & \ding{51} & \ding{55} & 87.92 & 88.13 & 87.64 & 91.15 & 88.71 \\
      \hline
      sentence-transformers/LaBSE & \ding{51} & \ding{51} & \ding{55} & 90.84 & 91.83 & \textbf{90.16} & \textbf{94.40} & \textbf{91.81} \\
      \hline
\end{tabularx}
}
\end{center}
\caption{Precision@1 on the Cross-Lingual Semantic Discrimination Datasets.}
\label{tab:performance_comparison}
\end{table*}