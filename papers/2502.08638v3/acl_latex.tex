% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage{tabularx}

\usepackage{pifont} 
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{amsmath}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
\parbox{0.6\linewidth}{\centering
Andrianos Michail, Simon Clematide, Rico Sennrich}
\\
University of Zurich
\\
\texttt{\{andrianos.michail,simon.clematide,sennrich\}@cl.uzh.ch}}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
The evaluation of cross-lingual semantic search capabilities of models is often limited to existing datasets from tasks such as information retrieval and semantic textual similarity. To allow for domain-specific evaluation, we introduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual semantic search task that does not require a large evaluation corpus, only parallel sentences of the language pair of interest within the target domain. This task focuses on the ability of a model to cross-lingually rank the true parallel sentence higher than challenging distractors generated by a large language model. We create a case study of our introduced CLSD task for the language pair German-French in the news domain. Within this case study, we find that models that are also fine-tuned for retrieval tasks benefit from pivoting through English, while bitext mining models perform best directly cross-lingually. A fine-grained similarity analysis enabled by our distractor generation strategy indicate that different embedding models are sensitive to different types of perturbations.
\end{abstract}

\section{Introduction}

When considering which model to use to perform semantic search within a specific domain and language pair, standard benchmarks such as MTEB \citep{muennighoff-etal-2023-mteb} do not cover every domain within all language pairs. We posit that one should aim to directly evaluate semantic search for the target language pair within the domain of interest but the lack of dataset for such evaluation cases makes it difficult to take fundamental decisions for the application uses, such as the choice of an embedding model or whether to use English as a pivot language to perform the task.

We propose a new simple and efficient dataset generation and evaluation scenario that simulates real-world challenges of cross-lingual semantic retrieval in large text collections. Our approach involves the use of LLMs to create adversarial examples from parallel sentences that challenge the cross-lingual embedding capabilities of multilingual models. Concretely speaking, four distractor sentences are generated for each parallel target sentence. We require these sentences to be very similar to the original target sentence in terms of syntactic structure and surface word forms, but to be semantically dissimilar.  
Our proposed task, Cross-Lingual Semantic Discrimination (CLSD), involves identifying the correct target sentence from among four distractor sentences in the target language, given an original sentence in the source language.

To better understand how the introduced changes in the distractors influence the semantic similarities cross- and monolingually, we perform a fine-grained analysis and correlate the semantic changes with linguistic properties at the level of part-of-speech tags.

\paragraph{\textbf{Contributions}}
1.~Proposing a new task that emulates cross-lingual semantic search within large text collections, requiring only a set of parallel sentences for dataset creation.
2.~Publishing four such datasets within the news domain for the language pair German-French.
3.~Demonstrate that within the task, certain embedding models perform better in direct cross-lingual retrieval, while others benefit from using English as a pivot language.
4.~Providing insights into the cross- and monolingual semantic representation of multilingual models by a linguistically informed analysis.

\section{Related Work}

\begin{table*}[t]
    \centering 
    \begin{tabular}{p{0.325\textwidth}@{\hskip 0.05in}|p{0.325\textwidth}@{\hskip 0.05in}|p{0.325\textwidth}}
        \multicolumn{1}{c|}{Original Sentence} &
        \multicolumn{1}{c|}{Distractor Sentence 1} & \multicolumn{1}{c}{Distractor Sentence 2} \\ 
        \toprule
      Die Linkspartei beschließt in Bonn ihr Programm zur Europawahl. \newline\textit{(The Left Party adopts its program for the European elections in Bonn.)} & Die Linkspartei beschließt in Bonn ihr Programm zur \textcolor{red}{Bundestagswahl}. \newline\textit{(The Left Party adopts its program for the \textcolor{red}{Bundestag} elections in Bonn.)}& Die Linkspartei \textcolor{red}{verweigert} in Bonn ihr Programm zur Europawahl. \newline\textit{(The Left Party \textcolor{red}{refuses} its program for the European elections in Bonn.)}\\
      \midrule
      Là-bas, la perspective de la fin de la guerre reste toujours très éloignée. \newline\textit{(Over there, the prospect of the end of the war is still a long way off.)}& Là-bas, la fin de la guerre \textcolor{red}{semble} toujours très \textcolor{red}{incertaine}. \newline\textit{(Over there, the end of the war still \textcolor{red}{seems very uncertain}.)}& Là-bas, la perspective de la \textcolor{red}{paix semble} toujours très éloignée. \newline\textit{(Over there, the prospect of \textcolor{red}{peace} still \textcolor{red}{seems} a long way off.)}\\
      \bottomrule
    \end{tabular}
    \caption{Examples of generated distractor sentences in German and French with English translations in italics. The red font marks modified text.}
    \label{tab:transformation-examples}
\end{table*}

\paragraph{\textbf{Cross-Lingual Semantic Search}}

Cross-lingual semantic search is an umbrella term for tasks that involve searching for texts with similar or relevant semantics in another language. Examples of such cross-lingual tasks are information retrieval (CLIR) \citep{lawrie2023overviewtrec2022neuclir,lawrie2024overviewtrec2023neuclir}, question answering (CLQA) \citep{lewis-etal-2020-mlqa}, semantic text similarity (X-STS) \citep{cer2017semeval}, and bitext mining \citep{tatoeba, zweigenbaum2018overview}. As shown in MTEB \citep{muennighoff-etal-2023-mteb}, multilingual embedding models vary in their performance for different cross-lingual semantic search tasks and language pairs. When selecting models for a use case, we often rely on results from other domains or language pairs, as direct evaluation results are not always available.

\paragraph{\textbf{Distractor Generation}}

PAWS-X \citep{yang2019paws,zhang-etal-2019-paws} generated adversarial examples by rule-based and backtranslation methods to measure the limitations of paraphrase identification models.  More recently, XSim++ \citep{chen-etal-2023-xsim} used rule-based text augmentation to generate synthetic examples for tuning LASER models (replacements by antonyms, swapping entities). Within semantic search, InPars \citep{bonifacio2022mmarcomultilingualversionms} use solely in-domain synthetic data to finetune retrieval models that surpass strong baselines.


\begin{table*}[bt]
\begin{center}
\renewcommand{\arraystretch}{0.9} % Reduces row height
\setlength{\tabcolsep}{4pt} % Reduces column padding
\resizebox{\textwidth}{!}{%
\begin{tabularx}{1.45\textwidth}{|c|X|X|X|X|X|X|X!{\vrule width 2pt}X|X|}
      \hline
      Model (Hugging Face Name) & X-Ling. Aligned & Paraph. Aligned & Retrieval Aligned & WMT19-DE->FR & WMT21-DE->FR & WMT19-FR->DE & WMT21-FR->DE & Mean \\
      \hline
      \rowcolor{blue!30} \multicolumn{9}{|c|}{\textbf{Cross-Lingual CLSD Evaluation}} \\
      \hline
      multilingual-e5-base & \ding{51} & \ding{51} & \ding{51} & 91.51 & 86.34 & 88.46 & 81.97 & 87.07 \\
      \hline
      multilingual-e5-large & \ding{51} & \ding{51} & \ding{51} & 94.43 & 91.38 & 91.45 & 87.57 & 91.21 \\
      \hline
      gte-multilingual-base & \ding{51} & \ding{51} & \ding{51} & 90.22 & 90.48 & 89.55 & 91.60 & 90.46 \\
      \hline
      paraphrase-multilingual-mpnet-base & \ding{51} & \ding{51} & \ding{55} & 91.31 & 91.15 & 91.11 & 92.95 & 91.63 \\
      \hline
      sentence-transformers/LaBSE & \ding{51} & \ding{51} & \ding{55} & \textbf{95.18} & \textbf{94.06} & \textbf{94.30} & \textbf{94.18} & \textbf{94.43} \\
      \hline
      \rowcolor{blue!30} \multicolumn{9}{|c|}{\textbf{Machine Translation (M2M 1.2B \citep{JMLR:v22:20-1307}) to EN -> CLSD Evaluation}} \\
      \hline
      multilingual-e5-base & \ding{51} & \ding{51} & \ding{51} & 90.50 & 89.81 & 89.55 & 93.39 & 90.81 \\
      \hline
      multilingual-e5-large & \ding{51} & \ding{51} & \ding{51} & \textbf{90.22} & \textbf{91.83} & 89.95 & 92.95 & 91.24 \\
      \hline
      gte-multilingual-base & \ding{51} & \ding{51} & \ding{51} & 89.68 & 90.26 & 89.07 & 92.95 & 90.49 \\
      \hline
      paraphrase-multilingual-mpnet-base & \ding{51} & \ding{51} & \ding{55} & 87.92 & 88.13 & 87.64 & 91.15 & 88.71 \\
      \hline
      sentence-transformers/LaBSE & \ding{51} & \ding{51} & \ding{55} & 90.84 & 91.83 & \textbf{90.16} & \textbf{94.40} & \textbf{91.81} \\
      \hline
\end{tabularx}
}
\end{center}
\caption{Precision@1 on the Cross-Lingual Semantic Discrimination Datasets.}
\label{tab:performance_comparison}
\end{table*}

 \section{Experiments}

\subsection{Cross-Lingual Semantic Discrimination (CLSD)}
We propose a task that aims to evaluate multilingual embedding methods based on their capacity to find the closest cross-lingual semantic match amidst large bilingual text collections. The goal is to measure the model's ability to identify a parallel sentence as the most similar to the original sentence in the source language. To accomplish this task, a dataset of parallel sentences is required that will be enriched with $N$ distractor sentences in the target language.

\begin{figure}[t]
\begin{tcolorbox}[title={Distractors Generation Prompt},label={prompt},colback=white]
Can you provide me with four tricky sentences (numbered) that look structurally and lexically similar but don't have the same meaning. The sentences should be within similar topics and share commonalities with the original. Answer in \{French/German\}! \newline
\{Original Sentence in French/German\}
\end{tcolorbox}
\caption{GPT-4 prompt template used for the generation of distractors}
\label{fig:prompt}
\end{figure}


In this paper, we test the hypothesis that LLMs can generate difficult examples that look similar on the surface, but convey different meanings. We prompt  GPT-4 (gpt-4-0613) \citep{openai2024gpt4} to monolingually generate four distractors for each original sentence (see Figure \ref{fig:prompt}). The distractors are requested to be structurally and lexically similar, whilst being semantically dissimilar. For each language, we manually examine 200 distractors of the original sentences and find that over 98\% of the distractors meet these criteria. Example sentences are shown in Table~\ref{tab:transformation-examples}.

\paragraph{\textbf{CLSD Datasets}} We generate and validate (samples of) four datasets, consisting of both language directions of German/French for the WMT19 \citep{barrault2019findings} and  WMT21 \citep{akhbardeh2021findings} test sets.\footnote{The data is available at \href{https://shorturl.at/wzSRd}{\texttt{link}}}. In the 18,928 generated distractors, we observe that the LLM used different modification strategies such as single polarity inversion, entity replacement or even multiple swaps with word-level Jaccard similarity between originals and distractors at ($\mu = 0.47$, $\sigma = 0.17$). Table~\ref{tab:distractor_jaccard} illustrates samples of distractors sets.
%Figure~\ref{fig:kde_plot} in the Appendix illustrates a wide range of word-level Jaccard similarity between the distractors, where  values near zero indicate little lexical overlap, whilst values approaching one shows distractor generations becoming a single ``slot-filling'' task.Generations along with their similarities are showcased in Appendix (Table~\ref{tab:distractor_jaccard}).


\paragraph{\textbf{CLSD Evaluation}}
To evaluate a multilingual embedding model \( E \), we assess its ability to discriminate between original parallel sentence pairs and challenging distractors. For a given sentence \( S \) in the source language and its corresponding translation \( T \) in the target language, along with a set of \( N \) distractor sentences \( D \) in the target language, the model must produce an embedding of \( S \) that is  more similar to the embedding of \( T \) than to that of any distractor \( d_i \) in  \( D \).

To measure performance of embedding models, we use Precision@1 (P@1), defined as the proportion of samples in which the model ranks the true translation as more similar to the source sentence than the four distractors. For example, in Table~\ref{tab:performance_comparison}, the column WMT19-DE->FR reports the P@1 of identifying the original French target sentence as more similar than its French distractors.

\paragraph{\textbf{Models Evaluated}}

We examine the following five embedding models available through SentenceTransformers \citep{reimers2019sentence}.

\noindent \textbf{multilingual-e5-base/-large} (M-E5-b/l) \citet{wang2024textembeddingsweaklysupervisedcontrastive,wang2024multilingual} transform XLM-Ro\-BER\-Ta \citep{conneau2020unsupervised} into a bi-encoder through weakly supervised contrastive pre-training with multilingual data and a second step of supervised fine-tuning on retrieval datasets.

\noindent \textbf{gte-multilingual-base} (M-GTE) \citet{zhang-etal-2024-mgte} pre-train a multilingual long-context encoder (8192) and derive a bi-encoder by a process similar to multilingual-e5 that also includes contrastive training against hard negatives.

\noindent \textbf{paraphrase-multilingual-mpnet-base-v2} (M-MPNet)
\citet{reimers2020making} use knowledge distillation through cosine loss training on parallel sentences between an English paraphrase trained teacher model (paraphrase-mpnet-base-v2) and a multilingual student model (XLM-RoBERTa \citep{conneau2020unsupervised})   to teach the student to represent sentences closely across multiple languages.

\noindent \textbf{LaBSE} Language-agnostic BERT Sentence Embedding \citep{feng-etal-2022-language} is trained with translation ranking loss and negative samples.
\subsection{CLSD Results}
The results of the models on the CLSD datasets are shown in  Table~\ref{tab:performance_comparison}.
Regarding the direct cross-lingual evaluation, the Bitext Mining specialist LaBSE is the best model in this task with an average score of 94.43 (P@1), demonstrating its consistent ability to cross-lingually rank exact parallel text higher than the distractors. Another pattern that emerges is that models (M-E5-b/l, M-GTE) that were also trained on English-centric retrieval datasets, such as the MS Macro \citep{NguyenRSGTMD16}, perform worse within our stricter task.

In the lower half of the table, we evaluate the models using English as a pivot language on both sides, employing the M2M 1.2B translation model \citep{JMLR:v22:20-1307}. Both LaBSE and M-MPNet show a performance decrease of 2.6/2.8 points compared to direct cross-lingual evaluation. In contrast,  models with weaker performance in direct cross-lingual show only minor improvements, except the M-E5-b model, which gains 3.7 points.

We conduct a qualitative analysis of the 279 samples where LaBSE succeeds in the direct cross-lingual evaluation but fails in the pivot-through-English evaluation. Our analysis reveals that, in most cases, language-specific terms are translated into broader and common English words, reducing the distinction between the original texts and the distractors. Less frequently, we observe omitted details in the translations, hallucinations, and, rarely, poor quality translations.

% Results in Table~\ref{tab:performance_comparison} show that there is a discrepancy between STS (DE-FR)  performance to our proposed cross-lingual sentence retrieval task, specifically within larger models. 
%% The performance ranks of the more related BIM task overall align  better with CLSD results. However, the lower performance for CLSD indicate its difficulty. 
% Table~\ref{tab:succesful_distractors} in the Appendix shows examples that even substantial changes in semantics can still mislead the models.

% To assess whether the same distractors confused the models, we analysed the errors in the base models. For WMT19-DE->FR, only 19\% of the MPNET errors were also errors in E5. For WMT19-FR->DE this was 34\%.

%As a more detailed error analysis of the base models, results in WMT19-DE/FR->FR/DE-CLSD shows that only 19\% of the errors in French and 34\% in German are caused by the same distractor.  In the Appendix (Table~\ref{tab:succesful_distractors}) we illustrate some successful distractor-original pairs for different models, highlighting the importance of sensitivity to named entities.


\subsection{Fine-grained similarity analysis}
How do minor surface changes affect the semantic similarity between sentence pairs? 
Can linguistic analysis provide a more precise characterisation of the changes?
To address these questions, we pick original-distractor pairs  in which the distractor differs from the original by only a single word. This subset includes 523 pairs (5.5\%)  in German and 324 pairs (3.4\%) in French. Using Stanza (v 1.8) \citep{qi2020stanza} and further validation by a linguist annotator, we assign part-of-speech tags to the differing token. 
We measure  the cross-lingual semantic change by subtracting the cosine distance between the original source sentence and the target distractor from the cosine distance between the original parallel pair.  To improve comparability in our analysis across different models, we normalize\footnote{The average cosine similarity difference between parallel and unrelated cross-lingual sentence pairs in the original WMT19/21  datasets is the normalization factor in Figure~\ref{fig:crosslingual_shift}.} this score by dividing with {$\parallel$$-$$\nparallel _{\text{ DE } \leftrightarrow \text{ FR}}$ for the corresponding model. Formally, we measure:  $$ \frac{(1 - \cos(E(S), E(T))) - (1 - \cos(E(S), E(d))}{\|-\nparallel _{\text{ DE } \leftrightarrow \text{ FR}}} $$

\noindent Figure~\ref{fig:crosslingual_shift} presents model results, showing that the M-MPNet model experiences the largest overall shifts in cosine similarity when single-word modifications are made.  The column ``ANY'' reflects  changes across all items, facilitating within-model comparisons to determine which linguistically characterized subgroups exhibit more pronounced changes. Notably, LaBSE, the best performing model within CLSD,  represents these single-word changes with relatively uniform similarity shifts across linguistic categories, unlike the other models. Additionally,
we observe that replacing proper nouns consistently decreases semantic similarity more than replacing other parts of speech across all languages and models.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.91\columnwidth, keepaspectratio]{X-Change.pdf} 
    \caption{Cross-lingual cosine similarity change in original-distractor sentence pairs with exactly one token swapped, grouped by the PoS of the differing token.}
    \label{fig:crosslingual_shift}
\end{figure}

\paragraph{\textbf{Mono- and Cross-Lingual Change Correlations}}
How language-agnostic are the base models when dealing with distractors? Does it matter whether the source sentence is presented in the source or target language?
We  measure the monolingual similarity change of the original to the distractor by substituting $E(S)$ with $E(T)$ in the calculation. The corresponding monolingual semantic change plots can be found in Figure~\ref{fig:monolingual_shift} in the Appendix.
%
%:  $$ \frac{\cos\_sim(E(T), E(T)) - \cos\_sim(E(T), E(d))}{\| - \nparallel _{\text{ DE } \leftrightarrow \text{ FR}}} $$
%
By correlating the results of the cross- and monolingual change for all one-word change cases (i.e.\ ``ANY''):   The correlation between monolingual and cross-lingual changes is  strong for {M-MPNet} with 0.93 closely followed by M-GTE with 0.85 and LaBSE with 0.82. The M-E5-b/l shows  lower correlation with 0.79 and 0.71 respectively.
However, specific nuances depend on  the language, model, or type of replaced word. For example, M-GTE shows a low correlation of 0.64 for proper nouns between cross- and monolingual evaluations in French, compared to 0.77 for German.

\section{Conclusion}

This paper contributes to cross-lingual semantic search by identifying gaps in current evaluation practices and introducing adversarial examples to test multilingual embeddings. Our research, on the German-French case study, introduces four new adversarial cross-lingual evaluation datasets for the news domain. Our comparative evaluation of direct cross-lingual and pivot through English evaluation highlights the advantage of each method based on the embedding model used. The fine-grained analyses demonstrate how generated distractors offer insights into the cross-lingual capabilities of multilingual models and detail how specific small perturbations in sentences can change these models' produced semantic representations. Future work should extend these approaches to other language pairs and refine adversarial techniques to improve semantic search model evaluations.

\section*{Limitations}
We introduce a pipeline that allows cross-lingual evaluation of semantic search between any two languages that have an available set of parallel texts. However, we evaluate only a single language pair (of interest) and only a single model for distractor generation, whereas in reality much cheaper methods might suffice.
Automatic prediction of PoS tags and named entities in our analysis can result in introducing noise within the fine-grained analysis results. The performance of the stanza models and our added manual results validation minimises this risk. However, the capabilities of current models in the language of interest should be critically assessed.
Furthermore, it is important to note that the examination of only the few models is due to space constraints; however, it is worth considering that valuable insights can also be gained from smaller models.
We hope that larger future work could address these limitations and explore this approach more broadly, creating a large scale cross-lingual semantic search dataset parallel in many, including low-resource languages.

\section*{Acknowledgments}
This research is conducted under the project \textit{Impresso -- Media Monitoring of the Past II Beyond Borders: Connecting Historical Newspapers and Radio}. Impresso is a research project funded by the Swiss National Science Foundation (SNSF 213585) and the Luxembourg National Research Fund (17498891).

\bibliography{acl_latex}

\appendix

\section{Appendix}
\label{sec:appendix}

\begin{table}[h!]
\centering
\begin{tabular}{|l|r|}
\hline
\textbf{Model Parameter} & \textbf{Value} \\ \hline
Temperature              & 1.0           \\ \hline
Top-P                    & 1.0            \\  \hline
\end{tabular}
\caption{Parameters used for GPT-4 via chat completion for the distractor generation. Undefined parameters are the default.}
\label{tab:LLM_params}
\end{table}

\begin{figure*}[t]
\centering
\resizebox{\textwidth}{!}{\includegraphics{M-Change.pdf}}
\caption{Monolingual cosine similarity change in original-distractor sentence pairs with exactly one token swapped, grouped by the part of speech of the differing token.}
\label{fig:monolingual_shift}
\end{figure*}

\begin{table*}[ht!]
    \centering
    \begin{tabular}{p{13cm} r}
        \toprule
        Distractor & \small Intra-Distractor Jaccard Sim. \\
        \midrule
        \textit{Original: PARIS (Reuters) - Die New Yorker Börse schloss am Freitag mit 0,68 \% im Minus und beendete die Woche mit einem allgemeinen Rückgang, da sie von enttäuschenden Ergebnissen, einem Wiederanstieg der Coronavirus-Infektionen und geopolitischen Unsicherheiten beeinträchtigt wurde.} & / \\ \addlinespace
        Adv1: \textcolor{red}{LONDON} (Reuters) - Die \textcolor{red}{Londoner} Börse schloss am \textcolor{red}{Dienstag} mit 0,68 \% im \textcolor{red}{Plus} und \textcolor{red}{startete} die Woche mit \textcolor{red}{allgemeinem Wachstum}, da sie von \textcolor{red}{erfreulichen} Ergebnissen, einem \textcolor{red}{kontinuierlichen Rückgang} der Coronavirus-Infektionen und geopolitischen \textcolor{red}{Stabilitäten angetrieben} wurde. & 0.434 \\ \addlinespace
        Adv2: \textcolor{red}{BERLIN} (Reuters) - Die \textcolor{red}{Berliner} Börse schloss am \textcolor{red}{Donnerstag} mit 0,68 \% im Minus und beendete die Woche mit einem \textcolor{red}{insgesamt flachen Verlauf}, \textcolor{red}{das durch gemischte Ergebnisse}, \textcolor{red}{eine Stagnation} der Coronavirus-Infektionen und geopolitische \textcolor{red}{Spannungen beeinflusst} wurde. & 0.300 \\ \addlinespace
        Adv3: \textcolor{red}{TOKYO} (Reuters) - Die \textcolor{red}{Tokioter} Börse schloss am \textcolor{red}{Mittwoch} mit 0,68 \% im \textcolor{red}{Plus} und \textcolor{red}{begann} \textcolor{red}{den Monat} mit \textcolor{red}{allgemeiner Erholung}, da sie von \textcolor{red}{positiven} Ergebnissen, einem \textcolor{red}{eutlichen Rückgang} der Coronavirus-Infektionen und geopolitischer \textcolor{red}{Sicherheit profitierte}. & 0.359 \\ \addlinespace
        Adv4: \textcolor{red}{MADRID} (Reuters) - Die \textcolor{red}{Madrider} Börse schloss am \textcolor{red}{Montag} mit 0,68 \% im Minus und \textcolor{red}{startete} die Woche mit \textcolor{red}{allgemeinem Rückgang}, da sie von \textcolor{red}{enttäuschenden} Ergebnissen, einem Wiederanstieg der Coronavirus-Infektionen und geopolitischen Unsicherheiten beeinträchtigt wurde. & 0.438 \\ \addlinespace
        \hline
        \addlinespace
         \textit{Original: Der Nasdaq verzeichnete die schlechteste Woche der letzten vier.} & / \\
        Adv1: Der Nasdaq \textcolor{red}{hat} die \textcolor{red}{beste} Woche der letzten vier \textcolor{red}{verzeichnet}. & 0.630 \\
        Adv2: Der Nasdaq verzeichnete die \textcolor{red}{aktivste} Woche der letzten vier. & 0.623 \\
        Adv3: Der Nasdaq \textcolor{red}{hat} die \textcolor{red}{ruhigste} Woche der letzten vier \textcolor{red}{verzeichnet}. & 0.630 \\
        Adv4: Der Nasdaq verzeichnete die \textcolor{red}{turbulenteste} Woche der letzten vier. & 0.623 \\
        \addlinespace
        \hline
        \textit{Original: Die Beamten werden im Haushalt für 2021 jedoch nicht vergessen werden.} &  / \\
        Adv1: Die Beamten werden im Haushalt für 2021 jedoch nicht \textcolor{red}{besprochen} werden. & 0.818 \\
        Adv2: Die Beamten werden im Haushalt für 2021 jedoch nicht \textcolor{red}{entlassen} werden. & 0.818 \\
        Adv3: Die Beamten werden im Haushalt für 2021 jedoch nicht \textcolor{red}{befördert} werden. & 0.818 \\
        Adv4: Die Beamten werden im Haushalt für 2021 jedoch nicht \textcolor{red}{belastet} werden. & 0.818 \\
        \bottomrule
        
    \end{tabular}
    \caption{Original-Distractor sets with their Intra-Distractor Jaccard Similarity. The red font indicates modified text.}
    \label{tab:distractor_jaccard}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{|>{\raggedright\arraybackslash}p{7cm}|>{\raggedright\arraybackslash}p{7cm}|>{\centering\arraybackslash}p{2cm}|}
\hline
\textbf{Original} & \textbf{Distractor} & \textbf{\small{Tricked Model}} \\ \hline
\multicolumn{3}{|c|}{\textbf{WMT19-
DE->FR-CLSD}} \\ \hline


Kipping au congrès de die Linke sur l'Europe : l'Europe est depuis longtemps un continent d'immigration. & Kipping au congrès de die Linke sur \textcolor{red}{l'Asie  : l'Asie} est depuis longtemps un continent de \textcolor{red}{diversité}. & \textbf{MPNet-M} \\ \hline

Par exemple, nous allons à présent recevoir un quartier général pour les munitions de l'UE, à Bruxelles, ce que les Britanniques ont jusqu'ici toujours empêché. & 
Par exemple, nous \textcolor{red}{prévoyons de lancer un bureau principal pour les ressources} de l'UE, à Bruxelles, ce que les Français \textcolor{red}{ont toujours refusé jusqu'ici.}
& \textbf{MPNet-M} \\ \hline

L'appel de Macron: l'Europe est davantage qu'un "projet" & 
L'appel de Macron: l'Europe est \textcolor{red}{plus qu'une "idée".} & \textbf{M-E5-b} \\ \hline

Trois commissions ont accepté ma candidature, l'une d'elle était de justesse contre. &
Trois \textcolor{red}{comités ont rejeté ma candidature, l'un d'eux était presque en faveur}. & \textbf{M-E5-b} \\ \hline

\multicolumn{3}{|c|}{\textbf{WMT19-
FR->DE-CLSD}} \\ \hline

Würden sie dies tun, gäbe es niemanden in Europa, der sich dagegen wehren würde. &

Würden sie dies tun, gäbe es niemanden in Europa, der sich \textcolor{red}{ dafür aussprechen} würde. & \textbf{MPNet-M} \textbf{M-E5-b} \\
\hline

Um dem etwas entgegenzusetzen, ist die EU-Kommission auf Kooperation mit den Internetriesen angewiesen. &

Um \textcolor{red}{das zu fördern}, ist die EU-Kommission \textcolor{red}{auf Zusammenarbeit} mit den \textcolor{red}{Internetgiganten} 
angewiesen. & \textbf{MPNet-M} \textbf{M-E5-b} \\ \hline

Und in der Sozialpolitik sehe ich bei ihr ebenfalls keinen Rechtsschwenk. &
\textcolor{red}{Und bei ihr sehe ich in der Sozialpolitik ebenfalls keinen Linksschwenk.} & \textbf{MPNet-M} \\ \hline

Bohnentopf, Hähnchen mit Pommes oder Kalbfleisch mit Soße und Kartoffeln: In der Mensa-Eingangshalle der Universität Karlsruhe drängeln sich um die Mittagszeit die jungen Leute. &

\textcolor{red}{Pizzaschnitten, Hähnchen mit Reis oder Rinderbraten mit Gemüse und Kartoffeln: Um die Abendstunde füllt sich der Festsaal der Universität Stuttgart mit hungrigen Studenten.} & \textbf{MPNet-M} \\ \hline

Nein. Zurückweisungen gibt es ja auch in anderen EU-Ländern. &

Nein, \textcolor{red}{Zuwanderungen} gibt es ja auch in anderen EU-Ländern. & \textbf{M-E5-b} \\  \hline

Und es ist zugleich die Emanzipation der Linken von ihrem Ex-Vorsitzenden Oskar Lafontaine. &

Und es ist zugleich die \textcolor{red}{Spaltung} der Linken \textcolor{red}{wegen} ihrem Ex-Vorsitzenden Oskar Lafontaine. & \textbf{M-E5-b} \\ \hline
\end{tabular}
\caption{Randomly selected examples of successful distractors. The red font indicates modified text.}
\label{tab:succesful_distractors}
\end{table*}


\end{document}
