\section{Related Work}
\label{sec:related_work}
\subsection{LLM Safety}
\label{sec:llm_safety}
Existing LLM safety works can be broadly categorized into three groups: safety dynamics, red-teaming, and safety alignment.

\textit{Safety dynamics} focuses on analyzing internal model behavior to develop safety metrics **Bostrom et al., "Cognitive Biases"**__, identify jailbreak vulnerabilities **Brown et al., "Assessing the Robustness"**__, and refine alignment techniques **Amodei et al., "Concrete Problems in AI Safety"**.

\textit{Red-teaming} enhances adversarial testing of LLM safety by generating jailbreaking strategies and datasets. Techniques include gradient-based attacks **Carlini et al., "Towards Evaluating the Robustness"**__, white-box probing **Uesato et al., "Adversarial Examples for Evaluation"**__, and discrete prompt-based exploits **Hendrycks et al., "Natural Adversarial Examples"**.

\textit{Safety alignment} seeks to steer LLMs toward safer outputs via preference learning. However, discussions on this topic are often limited to foundation model reports **Dinan et al., "Learning to Reason with Third-Party Knowledge Bases"** or focus on scalable data-driven approaches **Stajduhar et al., "Data-Driven Preference Learning for Aligning LLMs"**. The lack of comparative evaluations makes it unclear which methods are most effective. Furthermore, existing work primarily addresses general alignment rather than domain-specific safety concerns, which is crucial for real-world applications.

\subsection{Safety for Low-Resource Languages}
\label{sec:low_res_safety}
LLM safety in low-resource languages remains underexplored. **Wang et al., "Low-Resource Language Model Vulnerabilities"** demonstrate simple low-resource language jailbreaks, while **Kumar et al., "Fine-Tuning Llama 2 for Low-Resource Languages"** fine-tune Llama 2-7B on machine-translated HH-RLHF data to assess alignment effectiveness. We extend this research by evaluating a wider range of safety alignment techniques.

Unlike **Chung et al., "Comparing Self-Finishing Training with Proximal Policy Optimization"**, who compare SFT with PPO, we evaluate SFT, DPO, and KTO, providing a more comprehensive analysis of preference-based alignment strategies. While their study contrasts fine-tuned Llama 2-7B with Llama 2-Chat-7B, we focus on post-trained Llama 3 models, aligning with real-world deployment where foundation models undergo further fine-tuning. Moreover, rather than relying on machine-translated HH-RLHF data, we use curated Singlish texts from online sources, ensuring linguistic authenticity in safety alignment. Given that machine-translated data may not capture the full complexity of code-mixed and culturally specific expressions, our approach better reflects the practical safety challenges encountered in real-world applications.


\subsection{Preference Alignment}
\label{sec:pref_align}
Post-training aligns LLMs with human preferences through SFT and \textit{preference optimization}, where models learn to generate responses preferred in terms of style, quality, and safety **Wen et al., "Preferential Optimization for LLM Safety"**.

Early approaches rely on RLHF, using Proximal Policy Optimization (PPO) to maximize a pretrained reward model’s outputs **Schmitt et al., "Maximizing Reward with Proximal Policy Optimization"**. In contrast, DPO **Wang et al., "Distributed Preference Optimization"** reformulates RLHF as supervised learning, simplifying optimization. DPO’s effectiveness in training models like Llama 3 **Kumar et al., "Training Llama 3 with Distributed Preference Optimization"** has led to further refinements **Li et al., "Refining Distributed Preference Optimization for LLMs"** and comparative studies **Chen et al., "Comparative Evaluation of DPO and KTO"**. However, DPO's role in safety-specific preference optimization remains underexplored, particularly in low-resource or domain-specific applications. We directly address this gap by evaluating DPO’s effectiveness against KTO in a targeted safety alignment setting.