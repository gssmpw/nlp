\section{Related Work}
\label{sec:related_work}
\subsection{LLM Safety}
\label{sec:llm_safety}
Existing LLM safety works can be broadly categorized into three groups: safety dynamics, red-teaming, and safety alignment.

\textit{Safety dynamics} focuses on analyzing internal model behavior to develop safety metrics \citep{peng2024navigatingsafetylandscapemeasuring}, identify jailbreak vulnerabilities \citep{arditi2024refusallanguagemodelsmediated, zhou2024emulateddisalignmentsafetyalignment}, and refine alignment techniques \citep{wei2023jailbrokendoesllmsafety, zhou2024alignmentjailbreakworkexplain}.

\textit{Red-teaming} enhances adversarial testing of LLM safety by generating jailbreaking strategies and datasets. Techniques include gradient-based attacks \citep{zou2023universaltransferableadversarialattacks}, white-box probing \citep{hartvigsen2022toxigenlargescalemachinegenerateddataset, arditi2024refusallanguagemodelsmediated}, and discrete prompt-based exploits \citep{perez2022redteaminglanguagemodels, mehrotra2024treeattacksjailbreakingblackbox}.

\textit{Safety alignment} seeks to steer LLMs toward safer outputs via preference learning. However, discussions on this topic are often limited to foundation model reports \citep{openai2024gpt4technicalreport, grattafiori2024llama3herdmodels, geminiteam2024geminifamilyhighlycapable} or focus on scalable data-driven approaches \citep{bai2022constitutionalaiharmlessnessai}. The lack of comparative evaluations makes it unclear which methods are most effective. Furthermore, existing work primarily addresses general alignment rather than domain-specific safety concerns, which is crucial for real-world applications.

\subsection{Safety for Low-Resource Languages}
\label{sec:low_res_safety}
LLM safety in low-resource languages remains underexplored. \citet{yong2024lowresourcelanguagesjailbreakgpt4} demonstrate simple low-resource language jailbreaks, while \citet{shen2024languagebarrierdissectingsafety} fine-tune Llama 2-7B on machine-translated HH-RLHF data to assess alignment effectiveness. We extend this research by evaluating a wider range of safety alignment techniques.

Unlike \citet{shen2024languagebarrierdissectingsafety}, who compare SFT with PPO, we evaluate SFT, DPO, and KTO, providing a more comprehensive analysis of preference-based alignment strategies. While their study contrasts fine-tuned Llama 2-7B with Llama 2-Chat-7B, we focus on post-trained Llama 3 models, aligning with real-world deployment where foundation models undergo further fine-tuning. Moreover, rather than relying on machine-translated HH-RLHF data, we use curated Singlish texts from online sources, ensuring linguistic authenticity in safety alignment. Given that machine-translated data may not capture the full complexity of code-mixed and culturally specific expressions, our approach better reflects the practical safety challenges encountered in real-world applications.


\subsection{Preference Alignment}
\label{sec:pref_align}
Post-training aligns LLMs with human preferences through SFT and \textit{preference optimization}, where models learn to generate responses preferred in terms of style, quality, and safety \citep{ziegler2020finetuninglanguagemodelshuman, bai2022traininghelpfulharmlessassistant}.

Early approaches rely on RLHF, using Proximal Policy Optimization (PPO) to maximize a pretrained reward model’s outputs \citep{ziegler2020finetuninglanguagemodelshuman, ouyang2022traininglanguagemodelsfollow, bai2022traininghelpfulharmlessassistant}. In contrast, DPO \citep{rafailov2024directpreferenceoptimizationlanguage} reformulates RLHF as supervised learning, simplifying optimization. DPO’s effectiveness in training models like Llama 3 \citep{grattafiori2024llama3herdmodels} has led to further refinements \citep{pang2024iterativereasoningpreferenceoptimization, ethayarajh2024ktomodelalignmentprospect, xu2024contrastivepreferenceoptimizationpushing, azar2023generaltheoreticalparadigmunderstand} and comparative studies \citep{xu2024dposuperiorppollm}. However, DPO's role in safety-specific preference optimization remains underexplored, particularly in low-resource or domain-specific applications. We directly address this gap by evaluating DPO’s effectiveness against KTO in a targeted safety alignment setting.