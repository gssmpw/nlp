\section{Related Work}
\label{sec:related_work}
\subsection{LLM Safety}
\label{sec:llm_safety}
Existing LLM safety works can be broadly categorized into three groups: safety dynamics, red-teaming, and safety alignment.

\textit{Safety dynamics} focuses on analyzing internal model behavior to develop safety metrics ____, identify jailbreak vulnerabilities ____, and refine alignment techniques ____.

\textit{Red-teaming} enhances adversarial testing of LLM safety by generating jailbreaking strategies and datasets. Techniques include gradient-based attacks ____, white-box probing ____, and discrete prompt-based exploits ____.

\textit{Safety alignment} seeks to steer LLMs toward safer outputs via preference learning. However, discussions on this topic are often limited to foundation model reports ____ or focus on scalable data-driven approaches ____. The lack of comparative evaluations makes it unclear which methods are most effective. Furthermore, existing work primarily addresses general alignment rather than domain-specific safety concerns, which is crucial for real-world applications.

\subsection{Safety for Low-Resource Languages}
\label{sec:low_res_safety}
LLM safety in low-resource languages remains underexplored. ____ demonstrate simple low-resource language jailbreaks, while ____ fine-tune Llama 2-7B on machine-translated HH-RLHF data to assess alignment effectiveness. We extend this research by evaluating a wider range of safety alignment techniques.

Unlike ____, who compare SFT with PPO, we evaluate SFT, DPO, and KTO, providing a more comprehensive analysis of preference-based alignment strategies. While their study contrasts fine-tuned Llama 2-7B with Llama 2-Chat-7B, we focus on post-trained Llama 3 models, aligning with real-world deployment where foundation models undergo further fine-tuning. Moreover, rather than relying on machine-translated HH-RLHF data, we use curated Singlish texts from online sources, ensuring linguistic authenticity in safety alignment. Given that machine-translated data may not capture the full complexity of code-mixed and culturally specific expressions, our approach better reflects the practical safety challenges encountered in real-world applications.


\subsection{Preference Alignment}
\label{sec:pref_align}
Post-training aligns LLMs with human preferences through SFT and \textit{preference optimization}, where models learn to generate responses preferred in terms of style, quality, and safety ____.

Early approaches rely on RLHF, using Proximal Policy Optimization (PPO) to maximize a pretrained reward model’s outputs ____. In contrast, DPO ____ reformulates RLHF as supervised learning, simplifying optimization. DPO’s effectiveness in training models like Llama 3 ____ has led to further refinements ____ and comparative studies ____. However, DPO's role in safety-specific preference optimization remains underexplored, particularly in low-resource or domain-specific applications. We directly address this gap by evaluating DPO’s effectiveness against KTO in a targeted safety alignment setting.