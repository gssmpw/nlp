\section{Related Work}
Our research aligns with two closely related areas: the utilization of Large Language Models (LLMs) or natural language processing (NLP) techniques for automated test case generation and the development of benchmark datasets to evaluate LLM performance in generating computing artifacts. 

\subsection{Test Cases Generation via LLMs}
Test cases must be generated from certain software artifacts, which specified the inputs or expected outputs of the test cases \cite{anand_2013_an}. Traditionally, according to the types of artifacts used, test case generation techniques fall into several categories, such as symbolic execution \cite{anand_2011_heap}, search-based and evolutionary approaches \cite{fraser_2011_evolutionary, fraser_2011_evosuite}. These approaches rely on static or dynamic analysis techniques to explore a program’s control and data flow paths. These typically result in test cases that are less readable and understandable than manually written test \cite{mmoeinalmasi_2017_an} and may either lack assertions or include only generic ones \cite{panichella_2020_replication}. With the advancement of LLMs, researchers have explored their potential to automate certain software artifacts like codes, which has sparked interest in integrating LLMs into software testing, particularly for test case generation. The growing attention on LLM-based test case generation aims to improve efficiency, enable self-learning in test creation, or produce more natural-looking tests \cite{schfer_2023_an}. 

One way to use LLMs in test case generation is by emphasizing prompt creation. Yuan et al. [12] proposed a framework, ChatTester, which iteratively refines test cases by asking ChatGPT to generate follow-up prompts. This method resulted in a 34.3\% increase in compilable tests and an 18.7\% improvement in tests with correct assertions, demonstrating the potential of LLMs in enhancing test generation. Schäfer et al. \cite{schfer_2023_an} introduced TestPilot, a tool automates unit test generation using a Javascript framework without additional trainings. TestPilot leveraged OpenAI's Codex to generate unit tests by con-structing prompts with analyzing function's signature, usage snippets, and documentation, then refining prompts to improve generation outcomes. Similarly, ChatUniTest, developed by Xie et al. \cite{xie_2023_chatunitest} , extracted raw information from JAVA codes, converted into an Abstract Syntax Tree (AST). It utilizes static analysis during the preparation stage and applies an adaptive focal context mechanism to capture relevant contextual information within prompts and follows a generation-validation-repair mechanism to refine and correct errors in generated unit tests.

Another area of focus is to use LLMs for property-based or specialized testing. Vikram et al. \cite{vikram_2023_can} used LLMs to generate property-based tests with API documentation. Koziolek et al. \cite{koziolek_2024_automated} studied the use of LLMs to automate test case generation for Programmable Logic Controller (PLC) and Distributed Control System (DCS) control logic. Plein et al. \cite{plein_2024_automatic}  investigated the use of LLMs, including ChatGPT and CodeGPT, for creating executable test cases from natural language bug reports. Their approach showed potential for enhancing tasks like fault localization and patch validation, with test cases successfully generated for up to 50\% of examined bugs. Wang et al. \cite{wang_2019_automatic} proposed UMTG, an NLP-based method to automate the generation of acceptance test cases from natural language use case specifications.


\subsection{Benchmark for Evaluating LLMs}

Research on creating benchmark datasets for LLMs to generate test cases is limited. One recent proposal, TESTEVAL \cite{wang_2024_testeval}, collected 210 Python programs from the online coding site LeetCodes. However, the benchmark is limited to the collected Python programs which restrict its generalization or extension to other programming languages or use patterns. To better understand how to create benchmarks for LLM’s capabilities in generating code-related artifacts, we explored two close fields: text-to-SQL translation and code generation. 

Li et al. \cite{li_2023_can} in 2023 introduced the BIRD dataset, a large-scale benchmark designed to assess LLMs' ability to parse text-to-SQL queries across diverse domains. Similarly, Wang et al. (2022) \cite{lan_2023_unite} developed the UNITE benchmark, consolidates 18 publicly available text-to-SQL datasets, creating a unified resource for evaluation. UNITE comprises over 120,000 examples from more than 12 domains, 29,000 databases, and 3,900 SQL query patterns. 

For benchmarking code generation and evaluation, HumanEval, proposed by Chen et al. in 2021 \cite{markichengchen_2021_evaluating}, was a carefully crafted benchmark consisting of 164 programming challenges. Each challenge includes a function signature, docstring, body, and multiple unit tests. Liu et al. \cite{liu_2023_is} addressed the limitations of existing benchmarks in evaluating the functional correctness of code generated by LLMs, such as ChatGPT. They identified that many benchmarks rely on insufficient and low-quality test cases, leading to an overestimation of LLM performance. To tackle this, they introduce EvalPlus, a framework that augments a significant number of additional test cases using both LLM-based and mutation-based strategies. Yan et al. \cite{yan_2023_codescope} presented CodeScope, a comprehensive benchmark for evaluating LLMs in code understanding and generation across multilingual, multi-task, and multidimensional scenarios. CodeScope covered 43 programming languages and eight tasks, including code summarization, repair, and optimization.

\subsection{Summary}

Past research highlights the importance of benchmark datasets for training and evaluating LLM’s abilities to generate computing artifacts. However, existing benchmark dataset for test case generation is highly limited, lacking flexibility for extension to other programming languages or use cases. Most existing LLM-based test generation techniques focus on prompt crafting and additional documents or data for generating tests. Additionally, they were also domain-specific and lack generalizable evaluation methods.

This raise to three key questions: \\
1. Can we develop a flexible benchmark dataset? \\
2. Can users only use a single prompt with merely source codes to LLM to generate test cases? \\
3. How well can LLMs generate test cases while inputting solely source codes?

To address these gaps, there is a need for a new, more adaptable dataset. To simulate real-world scenarios where users enter codes directly, without supplementary data like documentation, and request LLM’s test case generation though a single prompt. To achieve this, we propose GBCV, an approach designed to create is extendable benchmark dataset and facilitate studies on LLM’s test case generation ability with minimal input.