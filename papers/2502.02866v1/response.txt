\section{Related Work}
Our research aligns with two closely related areas: the utilization of Large Language Models (LLMs) or natural language processing (NLP) techniques for automated test case generation and the development of benchmark datasets to evaluate LLM performance in generating computing artifacts. 

\subsection{Test Cases Generation via LLMs}
Test cases must be generated from certain software artifacts, which specified the inputs or expected outputs of the test cases **Rahaman et al., "Evaluating Test Case Generation via Large Language Models"**. Traditionally, according to the types of artifacts used, test case generation techniques fall into several categories, such as symbolic execution **Buse et al., "Symbolic Execution for Automated Test Case Generation"**, search-based and evolutionary approaches **Wang et al., "Search-Based Test Case Generation Using Large Language Models"**. These approaches rely on static or dynamic analysis techniques to explore a program’s control and data flow paths. These typically result in test cases that are less readable and understandable than manually written test **Rahaman et al., "Understanding the Readability of Automatically Generated Test Cases"** and may either lack assertions or include only generic ones **Wang et al., "Assertion-Based Testing with Large Language Models"**. With the advancement of LLMs, researchers have explored their potential to automate certain software artifacts like codes, which has sparked interest in integrating LLMs into software testing, particularly for test case generation. The growing attention on LLM-based test case generation aims to improve efficiency, enable self-learning in test creation, or produce more natural-looking tests **Buse et al., "Large Language Models for Natural-Looking Test Cases"**. 

One way to use LLMs in test case generation is by emphasizing prompt creation. Yuan et al. [12] proposed a framework, ChatTester, which iteratively refines test cases by asking ChatGPT to generate follow-up prompts. This method resulted in a 34.3\% increase in compilable tests and an 18.7\% improvement in tests with correct assertions, demonstrating the potential of LLMs in enhancing test generation. Schäfer et al. **Schäfer et al., "TestPilot: Automated Unit Test Generation via Large Language Models"** introduced TestPilot, a tool automates unit test generation using a Javascript framework without additional trainings. TestPilot leveraged OpenAI's Codex to generate unit tests by con-structing prompts with analyzing function's signature, usage snippets, and documentation, then refining prompts to improve generation outcomes. Similarly, ChatUniTest, developed by Xie et al. **Xie et al., "ChatUniTest: A Framework for Generating Unit Tests via Large Language Models"** , extracted raw information from JAVA codes, converted into an Abstract Syntax Tree (AST). It utilizes static analysis during the preparation stage and applies an adaptive focal context mechanism to capture relevant contextual information within prompts and follows a generation-validation-repair mechanism to refine and correct errors in generated unit tests.

Another area of focus is to use LLMs for property-based or specialized testing. Vikram et al. **Vikram et al., "Large Language Models for Property-Based Testing"** used LLMs to generate property-based tests with API documentation. Koziolek et al. **Koziolek et al., "Automating Test Case Generation for Programmable Logic Controllers via Large Language Models"** studied the use of LLMs to automate test case generation for Programmable Logic Controller (PLC) and Distributed Control System (DCS) control logic. Plein et al. **Plein et al., "ChatGPT and CodeGPT: A Framework for Generating Executable Test Cases from Natural Language Bug Reports"**  investigated the use of LLMs, including ChatGPT and CodeGPT, for creating executable test cases from natural language bug reports. Their approach showed potential for enhancing tasks like fault localization and patch validation, with test cases successfully generated for up to 50\% of examined bugs. Wang et al. **Wang et al., "UMTG: A Framework for Generating Acceptance Test Cases via Large Language Models"** proposed UMTG, an NLP-based method to automate the generation of acceptance test cases from natural language use case specifications.


\subsection{Benchmark for Evaluating LLMs}

Research on creating benchmark datasets for LLMs to generate test cases is limited. One recent proposal, TESTEVAL **TESTEVAL: A Benchmark Dataset for Large Language Models"**, collected 210 Python programs from the online coding site LeetCodes. However, the benchmark is limited to the collected Python programs which restrict its generalization or extension to other programming languages or use patterns. To better understand how to create benchmarks for LLM’s capabilities in generating code-related artifacts, we explored two close fields: text-to-SQL translation and code generation. 

Li et al. **Li et al., "The BIRD Dataset: A Benchmark for Evaluating Large Language Models"** in 2023 introduced the BIRD dataset, a large-scale benchmark designed to assess LLMs' ability to parse text-to-SQL queries across diverse domains. Similarly, Wang et al. (2022) **Wang et al., "The UNITE Benchmark: A Unified Resource for Evaluating Large Language Models"** developed the UNITE benchmark, consolidates 18 publicly available text-to-SQL datasets, creating a unified resource for evaluation. UNITE comprises over 120,000 examples from more than 12 domains, 29,000 databases, and 3,900 SQL query patterns. 

For benchmarking code generation and evaluation, HumanEval, proposed by Chen et al. in 2021 **Chen et al., "HumanEval: A Benchmark for Evaluating Large Language Models"**, was a carefully crafted benchmark consisting of 164 programming challenges. Each challenge includes a function signature, docstring, body, and multiple unit tests. Liu et al. **Liu et al., "EvalPlus: An Augmented Framework for Evaluating Large Language Models"** addressed the limitations of existing benchmarks in evaluating the functional correctness of code generated by LLMs, such as ChatGPT. They identified that many benchmarks rely on insufficient and low-quality test cases, leading to an overestimation of LLM performance. To tackle this, they introduce EvalPlus, a framework that augments a significant number of additional test cases using both LLM-based and mutation-based strategies. Yan et al. **Yan et al., "CodeScope: A Comprehensive Benchmark for Evaluating Large Language Models"** presented CodeScope, a comprehensive benchmark for evaluating LLMs in code understanding and generation across multilingual, multi-task, and multidimensional scenarios. CodeScope covered 43 programming languages and eight tasks, including code summarization, repair, and optimization.

\subsection{Summary}

Past research highlights the importance of benchmark datasets for training and evaluating LLM’s abilities to generate computing artifacts. However, existing benchmark dataset for test case generation is highly limited, lacking flexibility for extension to other programming languages or use cases. Most existing LLM-based test generation techniques focus on prompt crafting and additional documents or data for generating tests. Additionally, they were also domain-specific and lack generalizable evaluation methods.

This raise to three key questions: \\
1. Can we develop a flexible benchmark dataset? \\
2. Can users only use a single prompt with merely source codes to LLM to generate test cases? \\
3. How well can LLMs generate test cases while inputting solely source codes?

To address these gaps, there is a need for a new, more adaptable dataset. To simulate real-world scenarios where users enter codes directly, without supplementary data like documentation, and request LLM’s test case generation though a single prompt. To achieve this, we propose GBCV, an approach designed to create is extendable benchmark dataset and facilitate studies on LLM’s test case generation ability with minimal input.