\section{Related Work}
\textbf{Adversarial attacks}, and specifically jailbreak attacks or automated red teaming, can be categorized into (1) optimization based attacks~\citep{wallace_universal_2021, shin_autoprompt_2020, guo_gradient-based_2021, zou_universal_2023, geisler_attacking_2024, guo_cold-attack_2024, wen_hard_2023, kumar_gradient-based_2022, hou_textgrad_2023,liu_autodan_2024,zhu_autodan_2023,andriushchenko_jailbreaking_2025,schwinn_soft_2024,hughes_best--n_2024,sadasivan_fast_2024,thompson_flrt_2024} or (2) attacks using generative models/LLMs~\citep{perez_red_2022, mehrotra_tree_2023, chao_jailbreaking_2023,liao_amplegcg_2024,chen_rl-jack_2024,jha_llmstinger_2024,lin_pathseeker_2024}. Our novel REINFORCE objective is focusing on approaches of category (1) that use gradient information~\citep{wallace_universal_2021, shin_autoprompt_2020, guo_gradient-based_2021, zou_universal_2023, geisler_attacking_2024, guo_cold-attack_2024, wen_hard_2023, kumar_gradient-based_2022, hou_textgrad_2023,zhu_autodan_2023}. Specifically, we extend two gradient-based attacks, namely GCG~\citep{zou_universal_2023} and PGD~\citep{geisler_attacking_2024}. While GCG builds on top of the language model attack AutoPromopt~\citep{shin_autoprompt_2020}, PGD relates to adversarial attacks on GNNs~\citep{xu_adversarial_2020, geisler_attacking_2021,gosch_adversarial_2023,foth_relaxing_2024}. 
While ours is not the first jailbreak attack that uses reinforcement learning (RL), usually RL-based approaches are of category (2) and train another LLM to generate prompts for jailbreaking the targeted LLM~\citep{perez_red_2022, mehrotra_tree_2023, chao_jailbreaking_2023, chen_rl-jack_2024,jha_llmstinger_2024, lin_pathseeker_2024}. 
Even though we target gradient-based optimization, our objective is also very effective for GCG with random mutations (see \autoref{fig:gcg_strategies}). Thus, approaches like \citet{andriushchenko_jailbreaking_2025,liu_autodan_2024} could also benefit from our advanced objective.

\citet{andriushchenko_jailbreaking_2025} explore adaptive attacks. In contrast to our work, they add certain (potentially model-specific) features to their attack that improve the attack success rates (e.g., self-transfer, prompt templates, restarts, etc.). Most of their strategies are orthogonal to ours and could further improve REINFORCE-GCG / -PGD. Nevertheless, it should be noted that they also rely on a non-consistent attack objective (e.g., log probability of ``Sure''). Our findings with adaptive attacks on the circuit breaker defenses align with embedding space attacks~\cite {schwinn_revisiting_2024}.

Adversarial attacks that capture rich semantics have been explored before~\citep{qiu_SemanticAdv_2020, geisler_generalization_2022, wang_semantic_2023, kollovieh_assessing_2024}. Notably, \citet{wichers_gradient-based_2024} also explore attacks on LLMs that leverage a judge. In contrast to our work, they relax the generation using Gumble softmax~\citep{jang_categorical_2016} and backpropagate through the attacked model and judge for a GBDA-based optimization~\citep{guo_gradient-based_2021}. 

\textbf{Affirmative response.} Even though virtually all of the aforementioned optimization-based jailbreak attacks use an affirmative response objective, there have been attempts to mitigate its limitation via varying the template~\citep{jia_improved_2024} or distilling responses from a modified model~\citep{thompson_flrt_2024}. Concurrently, AdvPrefix 
 of \citet{zhu_advprefix_2024} finds better response prefixes via high prefilling attack success rates and low negative log-likelihood. Similar to them, our REINFORCE objective also alleviates the issue that the affirmative responses are rather short~\citep{qi_safety_2025}. We demonstrate the complementary strengths of AdvPrefix and our objective in \autoref{app:advprefix}.

\textbf{Judges and evaluation.} Multiple judges have been introduced in the literature~\citep{shen_anything_2024, chao_jailbreaking_2023, bhatt_purple_2023}. We use the HarmBench~\citep{mazeika_harmbench_2024} judge since we evaluate on HarmBench. Nevertheless, we note the recent work by \citet{souly_strongreject_2024} as their judge achieves a slightly better alignment to human evaluation. Moreover, since current LLM-as-a-judge models for scoring harmfulness are all imperfect, one could augment their scores with frequent patterns for false positives (non-harmful prompts that are judged harmful) like~\citet{hughes_best--n_2024}. Next to a judge-based evaluation, one could also gain insights into our objective through (mechanistic) interpretability~\citep{arditi_refusal_2024, wollschlager_geometry_2025}.