
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[preprint]{some2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{pifont}
\usepackage{url}
\usepackage{nicefrac}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{dblfloatfix}
\usepackage{subfig}
\usepackage{longtable}


\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\input{math_commands.tex}

\renewcommand{\equationautorefname}{Eq.}
\renewcommand{\figureautorefname}{Fig.}
\newcommand{\subfigureautorefname}{Fig.}
\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}
\renewcommand{\subsubsectionautorefname}{\S}
\renewcommand{\appendixautorefname}{\S}

\newcommand{\algorithmautorefname}{Algo.}
\makeatletter
\renewcommand{\ALG@name}{Algo.} %
\makeatother

\newcommand{\todo}[1]{\textcolor{red}{(Todo: #1)}}


\def\reward{\operatorname{Reward}}
\def\harmful{\operatorname{Harmful}}
\def\harmfulness{\operatorname{Harmfulness}}
\def\tx{\tilde{\vx}}
\def\tX{\tilde{\mX}}
\def\model{f_\theta}
\def\Pm{P_{\model}}

\sometitlerunning{REINFORCE Adversarial Attacks on Large Language Models}

\begin{document}

\twocolumn[
\sometitle{REINFORCE Adversarial Attacks on Large Language Models:\\ An Adaptive, Distributional, and Semantic Objective}






\begin{someauthorlist}
\someauthor{\quad \quad}{}
\someauthor{Simon Geisler}{tum}
\someauthor{Tom Wollschl\"ager}{tum}
\someauthor{M. H. I. Abdalla}{tum}
\someauthor{Vincent Cohen-Addad}{google}
\someauthor{\quad \quad}{}
\someauthor{Johannes Gasteiger}{google,anthropic}
\someauthor{Stephan G\"unnemann}{tum}
\end{someauthorlist}


\someaffiliation{tum}{Department of Computer Science \& Munich Data Science Institute, Technical University of Munich}
\someaffiliation{google}{Google Research}
\someaffiliation{anthropic}{Now at Anthropic}

\somecorrespondingauthor{Simon Geisler}{s.geisler@tum.de}

\somekeywords{Machine Learning}

\vskip 0.3in
]



\printAffiliationsAndNotice{} %

\begin{abstract}
To circumvent the alignment of large language models (LLMs), current optimization-based adversarial attacks usually craft adversarial prompts by maximizing the likelihood of a so-called affirmative response. An affirmative response is a manually designed start of a harmful answer to an inappropriate request. While it is often easy to craft prompts that yield a substantial likelihood for the affirmative response, the attacked model frequently does not complete the response in a harmful manner. Moreover, the affirmative objective is usually not adapted to model-specific preferences and 
essentially ignores the fact that LLMs output a distribution over responses. If low attack success under such an objective is taken as a measure of robustness, the true robustness might be grossly overestimated. To alleviate these flaws, we propose an adaptive and semantic optimization problem over the population of responses. We derive a generally applicable objective via the REINFORCE policy-gradient formalism and demonstrate its efficacy with the state-of-the-art jailbreak algorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent (PGD). For example, our objective doubles the attack success rate (ASR) on Llama3 and increases the ASR from 2\% to 50\% with circuit breaker defense.\textsuperscript{1}

\end{abstract}

\section{Introduction}
\label{sec:intro}

Identifying model misbehavior in large language models (LLMs) can be tricky, especially since models advance and the performance on benchmarks saturates. Nevertheless, the absence of misbehavior on static benchmarks does not imply the absence of misbehavior in general. For example, aligned LLMs readily refuse to answer many inappropriate prompts where the factual answer, e.g., could cause harm. However, the jailbreaking literature~\citep{zou_universal_2023, perez_red_2022,wen_hard_2023,liu_autodan_2024,zhu_autodan_2023,geisler_attacking_2024,guo_cold-attack_2024} revealed that minor tweaks usually suffice to circumvent alignment's countermeasures. Yet, approaches that can automatically audit LLMs via crafting such adversarial prompts typically rely heavily on human-engineered objectives that do not adapt to the evaluated/attacked model. Specifically, adversarial prompts are usually chosen to maximize the likelihood of a \emph{fixed} affirmative response to an inappropriate prompt. %
Such an objective has multiple defects. For example, even if the attack crafts a prompt that makes the affirmative response likely, the attacked model often continues in a non-harmful manner
(see \autoref{fig:conversation:aff}). Instead, jailbreak attacks should optimize for an arbitrary harmful response that is reasonable to obtain with the attacked model. In \autoref{fig:sets_model_affirmative}, we illustrate which responses our REINFORCE objective targets and how this compares to the popular affirmative response objective. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{assets/sets_model_affirmative_harmful.pdf}
    \caption{Responses that our REINFORCE and the affirmative objective encourage. Due to the popularity of the affirmative objective, it is likely that eliminating the hatched region will be prioritized while developing future models.}
    \label{fig:sets_model_affirmative}
\end{figure}

Since improving the affirmative optimization objective does not necessarily lead to higher attack success, it is an \emph{inconsistent} objective. Such inconsistency has been recognized as a key obstacle to strong adaptive attacks~\cite{tramer_adaptive_2020}. While \emph{adaptive attacks} refine their strategy based on the specifics of the target model, the affirmative objective is typically not even tailored to the attacked model. Non-adaptive attacks are known to overestimate robustness~\citep{carlini_towards_2017, mujkanovic_are_2022}--a pattern we also observe in our empirical evaluation of LLMs.


\begin{figure}[t!]
    \centering
    \subfloat[\emph{Unsuccessful} attack via affirmative objective\label{fig:conversation:aff}]{\includegraphics[width=0.95\linewidth]{assets/ConversationAffirmative2.pdf}}\\
    \vspace{-5pt}
    \subfloat[\emph{Successful} attack via REINFORCE objective (ours)\label{fig:conversation:reinforce}]{\includegraphics[width=0.95\linewidth]{assets/ConversationAffirmative2REINFORCE.pdf}}
    \caption{Attack on Gemma 1.1 7B~\citep{deepmind_gemma_2024}. Even though GCG~\citep{zou_universal_2023} with (a) affirmative objective finds an adversarial suffix s.t.\ the model starts its response with target affirmation with \(>60\%\) chance, almost attaining an ideal outcome,
    the model completes the response harmlessly, almost mocking the attacker. (b) In contrast, our REINFORCE objective successfully disables the model's alignment. We denote redactions with ``*''.}%
    \label{fig:conversation}
\end{figure}

We revisit attacks on (conditional) generative models and specifically LLMs. We define adversarial attacks based on an adaptive and semantic objective that acknowledges that the model outputs a distribution over possible responses. By this, we formalize the foundations for adversarial attacks on generative models and set the basis for an adaptive yet automated search for model defects. Especially with the ever-emerging capabilities of large models, manually designing target responses appears to be a heavily involved arms race against model advancements. Instead, when using an appropriate reward signal, optimizing our objective directly maximizes the probability of obtaining the undesired behavior in the model's generations. Since we evaluate our adversarial attack formulation via jailbreaking LLMs, we use an LLM-as-a-judge to quantify how harmful the model's responses are and, therefore, optimize the probability of obtaining harmful responses.
By replacing the reward signal, our formalism and objective could also be used to evaluate other properties, including factuality or unlearning.

We implement our semantic objective via REINFORCE~\citep{williams_simple_1992}, where we optimize the prompt of an LLM w.r.t.\ to a reward signal. In reinforcement learning (RL) terms, the LLM is a probabilistic policy that we parametrize by its prompt to maximize the reward. In the context of jailbreaks, our REINFORCE objective guides the search for adversarial prompts with the goal of maximizing the probability of inappropriate/harmful generations. In \autoref{fig:conversation:reinforce}, we present an exemplary generation that results from a successful attack with our objective. 


We show that the state-of-the-art attacks Greedy Coordinate Gradient (GCG) of~\citet{zou_universal_2023} as well as the Projected Gradient Descent (PGD) of \citet{geisler_attacking_2024} immensely benefit from our formulation of the optimization problem, even when keeping all other parameters of the attacks constant. For example, GCG's attack success rate (ASR) on HarmBench's challenging standard behaviors~\citep{mazeika_harmbench_2024} roughly doubles on the safety-tuned Llama 2 7B and Llama 3 8B models if comparing our REINFORCE objective to the ordinary affirmative response objective. Moreover, we attack the state-of-the-art defense for Llama 3 based on circuit breaking~\citep{zou_improving_2024} with an attack success rate of 50\%, while GCG with affirmative response is virtually never successful.

\textbf{Contributions:} \textbf{[1]} We formally define an adversarial attack on conditional generative models that targets the semantics of the model's responses, acknowledges the distributional nature of generations, and is adaptive. With our formulation and given an appropriate reward signal, a jailbreak attack directly optimizes the probability of obtaining harmful responses. The resulting objective is (asymptotically) consistent, which is important for strong attacks. \textbf{[2]} We derive a widely applicable objective using REINFORCE and demonstrate its practicality as well as efficacy on state-of-the-art LLMs by extending the GCG and PGD attacks.

\section{Adaptive, Distributional, and Semantic Adversarial Attacks on Generative Models}\label{sec:attack}

\textbf{Adversarial attack.} We define an adversarial attack on the (conditional) generative model \(\Pm(Y|X) = \model(Y|X)\) as
\begin{equation}\label{eq:attack}
    \tilde{x}^* = \argmax_{\tilde{x} \in \Phi(x)} \mathbb{E}_{y \sim \Pm(Y|X = \tilde{x})}\left[\reward(y, \tilde{x}) \right]
\end{equation}
where we aim to find an adversarial input \(\tilde{x}^*\) among the set of admissible adversarial inputs \(\Phi(x)\) that maximizes the expected reward \(\mathbb{E}\left[\reward(y, \tilde{x}) \right]\) over the population of responses \(y \in \gY\). Since \autoref{eq:attack} describes a test time (evasion) attack, the parameters of the model \(\theta\) are kept constant, and solely the input \(x \in \gX\) to the model is changed. A constraint on \(\tilde{x}\) is not strictly required due to the reward and \(\Phi(x) = \gX\) is usually sensible. However, often the adversarial input \(\tilde{X}\) is chosen in the vicinity of clean inputs \(x\): \(\tilde{x} \in \Phi(x)\). For example, if appending some tokens to the prompt \(x\)%
, \(\Phi(x)\) is the set of possible suffixes appended to \(x\).

\textbf{Semantic.} Here, we use ``reward'' not because the targeted outcome is likely to be desirable but rather to streamline the terminology with our RL-based approach. By convention, we assume that higher values are better from the perspective of the optimization problem, i.e., the attacker. Since we implement this objective for jailbreaks in this work, the reward scores the harmfulness \(\reward(y, \tilde{x}) = \harmfulness(y, \tilde{x})\) of output \(y\), given clean input \(x\), perturbed input \(\tilde{x}\), or both.  Depending on the semantics that the reward captures, our definition covers a variety of problem settings, including factuality or unlearning. 

\textbf{Distributional.} In our distributional perspective on the model's outputs \(Y\) and the corresponding adversarial attack (\autoref{eq:attack}), we optimize the expectation since it yields a simple solution (see \autoref{sec:method}) and allows for a very natural interpretation, while other descriptive statistics would also be possible. Specifically, in the context of jailbreaks and assuming the judge is well-calibrated \(\reward(Y, X=\tilde{x}) = P(\harmful|Y=y, X=\tilde{x})\), the optimization problem of \autoref{eq:attack} maximizes the probability of harmful responses: 
\begin{equation}
\begin{aligned}
    &\mathbb{E}_{y \sim \Pm(Y|X = \tilde{x})}[\reward(Y=y, X=\tilde{x})] \\
    &= \int P(\harmful|Y,X=\tilde{x}) \Pm(Y|X=\tilde{x}) \mathop{}\!\mathrm{d} Y \\
    &= P(\harmful | X=\tilde{x}) \\
\end{aligned}
\end{equation}
\textbf{Adaptive.} Our distributional perspective stands in stark contrast to the prevalent optimization problem tackled for jailbreaking LLMs, where the likelihood of a single and fixed (partial) output \(y_{\text{affirmative}}\) is maximized:
\begin{equation}
    \tilde{x}^* = \argmax\nolimits_{\tilde{x} \in \Phi(x)} \Pm(Y=y_{\text{affirmative}}|X=\tilde{x})
\end{equation}
Note that the static choice of \(y_{\text{affirmative}}\) is usually not specific to model \(\model\). Adapting an attack, manually or automatically, to a specific model is one of the key properties for strong adversarial attacks in predictive tasks, as demonstrated by~\citet{tramer_adaptive_2020}. They further state that 
\vspace{3pt}\\
\begin{tabular}{|p{0.95\linewidth}}
[...] \emph{loss functions, the cornerstone of successful adaptive attacks,} [should be] \emph{consistent--so that higher loss values result in strictly stronger attacks}. 
\end{tabular}

\vspace{-2pt}
The affirmative objective is neither consistent w.r.t.\ the semantics (harmfulness) of generations nor does it adapt to the attacked model. Such non-consistent objectives are known to be a critical hindrance to successful attacks, especially if evaluating defenses~\citep{carlini_towards_2017, athalye_obfuscated_2018}. With a non-consistent objective, even an \emph{all-powerful} optimizer might end up with an unsuccessful attack. Conversely, optimizing the objective in \autoref{eq:attack} adapts to the model's output distributions and is (asymptotically) \emph{consistent}. This makes it appropriate for adaptive attacks. Unfortunately, we lack an efficient estimator--\emph{until now}.



\section{REINFORCE Attacks on LLMs}\label{sec:method}

LLMs autoregressively model \(\Pm(Y|X)\) as
\begin{equation}
\begin{aligned}
    \Pm(Y=\vy|X=\vx) &= \prod\nolimits_{t=1}^T \Pm(\evy_t | \vx \,\oplus\, \vy_{:t-1}) \\
\end{aligned}
\end{equation}
with concatenation \(\,\oplus\,\), generated token sequence \(\vy \in \gV^{T}\), generated token sequence until (exclusive) the \(t\)-th token \(\vy_{:t-1} \in \gV^{t-1}\), prompt \(\vx \in \gV^{T'}\), and set of tokens \(\gV\). For simplicity, we do not introduce an explicit notation for the so-called system tokens before \(\vx\) and between \(\vx\)/\(\vy\) that, among other things, indicate the start or end of the user prompt and start of the model response. 

\textbf{Markov decision process (MDP).} LLMs' autoregressive nature allows for an interpretation as a MDP. Specifically, the distribution of the next token \( \Pm(\evy_t | \vx \,\oplus\, \vy_{:t-1})\) can be understood as a probabilistic policy \(\pi_{\model}(X = \vx \,\oplus\, \vy_{:t-1}) = \Pm(Y = \evy_t | X = \vx \,\oplus\, \vy_{:t-1})\). %

With discount factor \(\gamma=1\) and solely a terminal reward 
\begin{equation}
    \reward(\vy_{:t}, \vx) =
    \begin{cases}
    \reward(\vy, \vx), & \text{if } \vy_{:t} = \vy \\
    0, & \text{otherwise}
    \end{cases}
\end{equation}
the value function \(V(Y, X)\) is equivalent to the expectation in the adversarial attack objective \autoref{eq:attack}:
\begin{equation}
\begin{aligned}
    V(Y, X=\vx) &= \mathbb{E}_{\vy \sim \pi_{\model}(Y|X=\vx)} \left[\sum_{t=1}^\infty \gamma^t \reward(\vy_{:t}, \vx)\right] \\
    &= \mathbb{E}_{\vy \sim \Pm(Y|X=\vx)}\left[\reward(\vy, \vx) \right]
\end{aligned}
\end{equation}
The biggest difference to the RL problem is that we do not parametrize the policy \(\pi_{\model}(X = \vx \,\oplus\, \vy_{:t-1}) = \Pm(\evy_t | \vx \,\oplus\, \vy_{:t-1})\) directly since the model parameters $\theta$ are constant at test time. Instead, \autoref{eq:attack} indirectly parametrizes a policy by choosing the initial state \(X=\tx\). However, besides this detail, \autoref{eq:attack} is equivalent to RL's maximization of the value function \(\argmax_{\tx} V(Y, X=\tx)\).%

\textbf{Gradient-based jailbreak attacks.} We aim to equip existing state-of-the-art optimization-based attacks with our objective. Some of the most effective optimization-based attacks~\citep{zou_universal_2023, geisler_attacking_2024, guo_cold-attack_2024, zhu_autodan_2023} rely on gradient information for the one-hot encoded input tokens \(\mX \in \{0, 1\}^{T' \times |\gV|}\).
That is, \(\mX\) contains only zeros except for a single one per row at the position of the tokens in \(\vx\). With the implicit relaxation \(\{0, 1\} \to [0,1]\) we can then use the gradients \(\nabla_{\tX}\) to aid the optimization over \(\tx\). We use \(\vx\) and \(\mX\) interchangeably.

\textbf{REINFORCE.} One common strategy to maximize the value function, and thereby the adversarial attack objective of \autoref{eq:attack}, is via REINFORCE~\citep{williams_simple_1992}. REINFORCE is a policy-gradient optimization algorithm using the gradient \(\nabla_{\tX} V(Y, X=\tX)=\nabla_{\tX} \mathbb{E}\left[\reward(\vy, \tX) \right]\) and a gradient ascent algorithm of choice. Via the use of the policy gradient theorem
\begin{equation}\label{eq:reinforce_gradient}
\begin{aligned}
    & \nabla_{\tX} \mathbb{E}_{\vy \sim \Pm(Y|X=\tX)}\big[\reward(\vy, \tX) \big] \\
    &\propto \mathbb{E}_{\vy \sim \Pm(Y|X=\tX)} \big[\reward(\vy, \tX) \nabla_{\tX} \log \Pm(\vy | \tX)  \big] \\
\end{aligned}
\end{equation}
REINFORCE uses a proportional proxy of the true gradient, which is easier to evaluate. Using this objective, gradient-based attacks become adaptive, distributional, and optimize w.r.t.\ the semantics of the generations. For jailbreaks and a well-calibrated reward, our objective allows such attacks to optimize \(\argmax_{\tilde{x}}  P(\harmful | X=\tilde{x})\).

\textbf{Baseline.} Usually the expectation in \autoref{eq:reinforce_gradient} is approximated via sampling \(\vy \sim \Pm(Y|X)\). To lower the variance it is common to introduce a \emph{baseline} \(b^{(i)}(\tX)\) for the \(K\) samples:
\begin{equation}\label{eq:baseline}
\begin{aligned}
    & \nabla_{\tX} \mathbb{E}_{\vy \sim \Pm(Y|X=\tX)}\left[\reward(\vy, \tX) \right] \\
    &\approx \sum_{i=1}^K \left[ \reward(\vy^{(i)}, \tX) - b^{(i)}(\tX) \right] \nabla_{\tX} \log \Pm(\vy^{(i)} | \tX)
\end{aligned}
\end{equation}
While there are many possibilities for designing the baseline (e.g., exponential moving averages or a learned value function for an actor-critic approach), we use the simple and parameter-free REINFORCE Leave-One-Out (RLOO) estimator, proposed by \citet{kool_buy_2019} that recently achieved promising results for RLHF~\citep{ahmadian_back_2024}. For \(K=1\), we resort to \autoref{eq:baseline} with \(b^{(i)}(\tX)=0\).
\begin{equation}\label{eq:loor}
\begin{aligned}
    \sum_{i=1}^K \left[ \reward(\vy^{(i)}, \tX) - \frac{1}{K - 1}\sum_{j \ne i} \reward(\vy^{(j)}, \tX) \right] \\ 
    \cdot\, \nabla_{\tX} \log \Pm(\vy^{(i)} | \tX)
\end{aligned}
\end{equation}
\textbf{Interpretation.} Our REINFORCE estimator equals a signed weighted average over cross entropy \(\operatorname{CE}(\vy | \tX)\) operations for samples \(\vy \sim Y\) that we obtain to approximate the current expected reward. For this, note that \(\log \Pm(Y=\vy | \tX) = - \operatorname{CE}(\vy | \tX) = -\sum_t^{T} \operatorname{CE}(\evy_t, \model(\tX\,\oplus\, \vy_{:t-1}))\). Due to the baseline and with slight simplification in this description, our objective aims to reduce the likelihood of below-average harmful generations (``away'' loss). Conversely, above-average harmful generations are encouraged to become more harmful (``towards'' loss). Hence, our objective adaptively steers the attack towards harmful behaviors and away from non-harmful behaviors relative to the current samples. However, the dynamic nature of the baselines may also be suboptimal when all generations are harmful or harmless. To stabilize the attack in this case, we add a small constant ($0.1$) to the baseline in \autoref{eq:loor} and increment $K$ by one. This way, our estimator behaves like an exclusive ``away'' loss if all generations are harmless and like an exclusive ``towards'' loss if all generations are harmful.

\textbf{Relation to affirmative objective.} Our objective contains the affirmative objective as a special case with \(\reward(\vy_{\text{affirmative}}) = 1\) and a biased sampler that only samples \(\vy_{\text{affirmative}}\). In other words, the affirmative objective is a biased estimator of \autoref{eq:attack} that is non-adaptive due to the lack of sampling and lack of adjustment towards the current generations given \(\tX\), including their rewards.

\textbf{Sampling strategy \(\Pm'(Y|X)\).} To estimate the expectation in \autoref{eq:reinforce_gradient} as efficiently as possible we use a random generation \(\vy_{\text{random}}\). For \(\vy_{\text{random}}\), we use sampling with low temperature~\citep{ackley_learning_1985}. The temperature allows the exploration of generations while controlling the style of LLM generations~\citep{ficler_controlling_2017} and also mimics the deployment of LLMs. While more random samples would be desirable, they increase the computational cost. With sample efficiency in mind, we instead include the greedy generation \(\vy_{\text{greedy}}\) since it approximates the output distribution's mode and is often (but not always) a good proxy for its mean behavior~\citep{scholten_probabilistic_2025}.

Unfortunately, on aligned LLMs and at the beginning of the attack, this sampling strategy is likely not very sample efficient since \(P(\harmful|X=\mX)\) is low. Fortunately, for jailbreaking LLMs, in contrast to many RL applications, we have access to promising policy roll-outs \(\vy_{\text{seed}}\). For example, one can re-use the affirmative response \(\vy_{\text{seed}}=\vy_{\text{affirmative}}\) or a generation of a previously successful attack \(\vy_{\text{seed}}=\vy_{\text{history}}\), potentially obtained from a different (variant of the) model.
Hence, we bias the sampling for \autoref{eq:loor} with the goal of improved sample efficiency. We argue that mixing in further roll-outs provides some so-called off-policy characteristics. In other words, to control the well-known exploration-exploitation tradeoff of RL, we employ a specialized sampling strategy \(\Pm'(Y|X)\). 
Additionally to \(\vy_{\text{seed}}\), we enhance the exploitation of harmful generations by including the most likely harmful generation \(\vy_{\text{harmful}}\), once the model produced a harmful generation during the attack. Further, to harmonize lengths between generations, we may greedily extend \(\vy_{\text{seed}}\). In summary, we use these (up to) four samples to approximate \autoref{eq:reinforce_gradient} via \autoref{eq:loor}: \((\vy_{\text{seed}}, \vy_{\text{random}}, \vy_{\text{greedy}}, \vy_{\text{harmful}}) \sim \Pm'\).

During the optimization, we truncate generations at 128 tokens for efficiency but evaluate with up to 512 tokens (see \autoref{sec:empirical}). We next detail how we integrate our REINFORCE objective into GCG and PGD and refer to \autoref{app:reinforce} for details as well as \autoref{app:gcg_vs_pgd} for the tradeoffs between GCG and PGD.

\subsection{REINFORCE-GCG}

Our REINFORCE-GCG can be decomposed into four steps: (1) \emph{Generate}; (2) \emph{Gradient} calculation; (3) \emph{Mutate} current prompt to obtain candidates; (4) \emph{Score} candidates and proceed with best. In our REINFORCE implementation of GCG~\citep{zou_universal_2023}, we keep all its key features unchanged, like only allowing perturbing tokens containing ASCII characters, but we omit such details here for presentation. We portray REINFORCE-GCG in \autoref{algo:gcg}.

We use GCG's strategy for sampling the candidate substitutions (\emph{Mutate}), which alter the previous prompt each by exactly one token. For the loss \(\ell\) we use the negative of estimator \autoref{eq:loor} by dropping \(\nabla_{\tX}\). We follow the above sampling strategy for the generations \(\hat{\gY} = \{\vy^{(1)}, \vy^{(2)}, \dots, \vy^{(K)}\}\). Our GCG definition degrades to the regular GCG of \citet{zou_universal_2023} if we use \(\ell = \operatorname{CE}\) and fix \(\hat{\gY} = \{\vy_{\text{affirmative}}\}\).

\textbf{Details.} We find that the reward for jailbreaking (with the HarmBench judges~\citep{mazeika_harmbench_2024}) behaves almost in a binary manner, since it is either close to 0 or close to 1 but rarely takes values in between. This binary behavior yields only little guidance for REINFORCE. However, we may use intermediate rewards to steer the attack towards harmful prefixes of the model's responses. Moreover, the reward calculation is fairly cheap in contrast to the \emph{Selection} (see \autoref{app:gcg_cost}). Specifically, we calculate the rewards also for prefixes of the generations of lengths 20, 40, 80, and clamp the closest value in this sequence to the original length of \(\vy_{\text{seed}}\). To lower the cost of the \emph{Selection}, we use these intermediate rewards (excluding \(\vy_{\text{seed}}\)) to determine the length of the generation for choosing the best mutation \(\argmin_{\tX \in \hat{\gX}} \ell(\hat{\gY}, \tX)\). We use generation length 40 if no generation is harmful. Beyond that, if, e.g., the generation at length 40 is harmful, we calculate the loss until length 80. Moreover, we exclude the randomly obtained sample \(\vy_{\text{random}}\) for the candidate selection, which also improves its stability due to the single random sample. Since GCG is not making progress monotonically (i.e., possibly \(\ell(\tX^{(i+1)}) > \ell(\tX^{(i)})\)), once the mode of the output distribution (approximated by the greedy generation) is harmful, we only accept a new candidate if the mode remains harmful. Moreover, instead of returning the final \(\tX^{(E+1)}\), we return the best generation according to a modified version of \(\ell\) that emphasizes the mode of the output distribution, to improve \emph{consistency} in a low-sample regime. See \autoref{app:gcg} for additional details on GCG and our REINFORCE-GCG. 

\begin{algorithm}[t]
  \small 
    \caption{REINFORCE Greedy Coordinate Gradient (GCG)}
    \label{algo:gcg}
    \begin{algorithmic}%
        \State \textbf{Input:} Initial prompt $\tX^{(1)}$, loss $\ell$, sampling strategy $\Pm'(Y|X = \tX)$, \# iterations $E$, \# samples $K$, search width $S$
        \For{\(i \in \{1,2, \dots, E\}\)}
        \State \(\hat{\gY} \leftarrow \{\vy^{(1)}, \dots, \vy^{(K)}\} \sim P'_{\model}(Y|X = \tX^{(i)})\) \Comment{Generate}
        \State \(\mG \leftarrow \nabla_{\tX^{(i)}}\ell(\hat{\gY}, \tX^{(i)})\) \Comment{Gradient}
        \State \(\hat{\gX} \leftarrow \{\hat{\mX}^{(1)}, \dots, \hat{\mX}^{(S)}\} \sim \operatorname{Mutate}_{S}(\mG, \tX^{(i)})\) \Comment{Mutate}
        \State \(\tX^{(i+1)} \leftarrow \argmin_{\tX \in \hat{\gX}} \ell(\hat{\gY}, \tX)\) \Comment{Selection}
        \EndFor
        \State \textbf{return} \(\tX^{(E+1)}\) 
    \end{algorithmic}
\end{algorithm}

\subsection{REINFORCE-PGD}

To obtain our REINFORCE-PGD from the version by \citet{geisler_attacking_2024}, we require similar changes as for GCG. Namely, we generate samples for the approximation of the expectation and replace the loss \(\ell\), following \autoref{eq:loor}. Hence, our PGD definition in \autoref{algo:pgd} degrades to the regular PGD attack if we use \(\ell = \operatorname{CE}\) and fix \(\hat{\gY} = \{\vy_{\text{affirmative}}\}\).

\textbf{Details.} The subsequent additions to/clarifications of \autoref{algo:pgd} follow \citet{geisler_attacking_2024}. We run PGD for a batch of behaviors/prompts in parallel, use Adam~\citep{kingma_adam_2015} instead of vanilla gradient descent, and reinitialize the attack to the best intermediate solution, \(\vx_{\text{best}}\), if a configurable number of iterations (patience=100) fails to improve. In case we run out of patience, we sample an effective prefix/suffix for a different prompt in the batch with a 50\% chance. The initial so-called entropy projection is linearly ramped up, followed by cosine annealing with warm restarts~\cite{loshchilov_sgdr_2017} for both the learning rate and entropy projection. See \autoref{app:pgd} for more explanations.


\begin{algorithm}[t]
    \small 
    \caption{REINFORCE Projected Gradient Descent (PGD)}
    \label{algo:pgd}
    \begin{algorithmic}%
        \State \textbf{Input:} Initial pro.\ $\tX^{(1)}$, loss $\ell$, sampling strategy $\Pm'(Y|X)$, discretization \smash{\(d(\tX)\)}, projection \(\Pi\), \# iterations $E$, \# samples $K$
        \For{\(i \in \{1,2, \dots, E\}\)}
        \State \(\hat{\gY} \leftarrow \{\vy^{(1)}, \dots, \vy^{(K)}\} \sim P'_{\model}(Y|d(\tX^{(i)}))\) \Comment{Generate}
        \State \(\mG \leftarrow \nabla_{\tX^{(i)}}\ell(\hat{\gY}, \tX^{(i)})\) \Comment{Gradient}
        \State \(\tX^{(i+1)} \leftarrow \Pi(\tX^{(i)} - \alpha \mG)\)  \Comment{Update}

        \State \(\tx^{(i+1)} \leftarrow d(\tX^{(i+1)})\) \Comment{Discretization}
        \State \(\tilde{\ell} \leftarrow \ell(\hat{\gY}, \tx^{(i+1)})\)
        \Comment{``Discretized loss''}
        \State \(\tx_{\text{best}} \leftarrow \tx^{(i+1)} \textbf{ if } \operatorname{is\_best}(\tilde{\ell}) \textbf{ else } \tx_{\text{best}}\) \Comment{Remember best}
    \EndFor
    \State {\bfseries return} \(\tx_{\text{best}}\)
    \end{algorithmic}
\end{algorithm}


\section{Experimental Evaluation}\label{sec:empirical}

We evaluate our REINFORCE-GCG (\autoref{algo:gcg}) and REINFORCE-PGD (\autoref{algo:pgd}) attacks that maximize \(P(\harmful | X=\tilde{x})\) on state-of-the-art aligned LLMs on standard behaviors from HarmBench~\citep{mazeika_harmbench_2024}. We contrast the results primarily to the affirmative-response variants of GCG~\citep{zou_universal_2023} and PGD~\citep{geisler_attacking_2024}. With minor exceptions that we explicitly state in the following, we use the respective attack's default settings. %

\subsection{Setup}

\textbf{Benchmark.} We use all 200 default behaviors from HarmBench's standard behaviors~\citep{mazeika_harmbench_2024}, which excludes the copyright and contextual behaviors. For the runtime comparisons in \autoref{fig:gcg_performance} and ablations in \autoref{sec:ablations}, we subsample 50 behaviors. We exclude them solely due to constraints on our computational budget. For the evaluation, we report the attack success rate (ASR) using HarmBench's judge, which is based on Llama 2 13B. Following HarmBench, we report the ASR usually with 512 tokens (ASR@512), where we generate 512 tokens with the target model and then right-truncate the generated string to 512 tokens using the judge's tokenizer. Note that during the attack we solely rely on the reward of up to 128 tokens. Following HarmBench, we only score the greedy generation, which can be seen as an estimate of \(P_\theta(Y|X=\tx)\)'s mode. We report a single trial for each prompt and execute all experiments on 80\,GB A100/H100 GPUs. We use bfloat16 for attacked models and judge and compile the generation.

\textbf{Models.} We attack Llama 2 7B~\citep{touvron_llama_2023}, Llama 3 8B~\citep{grattafiori_llama_2024}, Gemma 1.1 2B and 7B~\citep{deepmind_gemma_2024}, as well as Vicuna 1.5 7B~\citep{zheng_judging_2023}. Moreover, we study the state-of-the-art defense based on circuit breakers~\citep{zou_improving_2024} for Llama 3 8B.

\textbf{Attacks.} For our evaluation, we use HarmBench's implementation/configuration for Greedy Coordinate Gradient (GCG) from~\citet{zou_universal_2023}: search width \(S=512\), \(E=500\) iterations, and initialize the adversarial suffix with 20 ``!''. For REINFORCE-GCG we use HarmBench's Llama 2 13B judge as reward signal. For comparability, we stick to this default setup, although more elaborate templates for adversarial attacks have shown to be a promising direction for enhancing attack efficacy~\citep{andriushchenko_jailbreaking_2025}. For Projected Gradient Descent (PGD) from~\citet{geisler_attacking_2024}, we follow the original hyperparameters. We run PGD for $E=5,000$ iterations, use a learning rate of \(\alpha = 0.11\), entropy projection of $40\%$, and initialize prefix as well as suffix with 25 ``!''. One of the few differences to the setup of \citet{geisler_attacking_2024} is that we keep the prompt length fixed to avoid model-specific code changes in the used Huggingface's transformers library~\citep{wolf_huggingfaces_2020}. Due to the importance of batching for PGD's (amortized) cost, we use HarmBench's Mistral 7B judge instead of Llama 2 13B during the optimization. Due to the 80\,GB GPU RAM limitation, we set the batch size to 17 prompts instead of 30 to 50. Specifically, we attack 17 prompts in parallel and report amortized runtime (total time divided by number of prompts). For REINFORCE-GCG and -PGD, we keep the other hyperparameters of the base attack unchanged. We use \(\vy_{\text{seed}} = \vy_{\text{affirmative}}\) unless stated otherwise. %

\textbf{Reward.} Since we use HarmBench's judge, we design the reward in analogy to its intended use. That is, an attack is deemed successful if the judge's greedy generation is ``yes'' \emph{regardless of capitalization}. Since the greedy generation always returns the next token which is most likely, our reward definition hovers around the most likely ``yes'' \(p^* = \argmax_{p \text{ s.t.\ } \operatorname{l}(p) = "yes"} P_{\text{judge}}(p|\vy,\vx)\) and most likely non-``yes'' token \(n^* = \argmax_{n \text{ s.t.\ } \operatorname{l}(n) \ne "yes"} P_{\text{judge}}(n|\vy,\vx)\). \(\operatorname{l}(.)\) is short for \(\operatorname{lower}(.)\). Inspired by logistic regression, we then take their log-odds/logits \(\log(\nicefrac{P_{\text{judge}}(p^*|\vy,\vx)}{P_{\text{judge}}(n^*|\vy,\vx)})\) and transform them via sigmoid function \(\sigma\), which is the inverse of the (binary) logit function:
\begin{equation}
\begin{aligned}
    &\reward(y, \tilde{x})
    = \harmfulness(y, \tilde{x}) \\
    &= \sigma\big(\underbrace{\log P_{\text{judge}}(p^*|\vy,\vx)}_{\text{``yes'' log prob.}} - \underbrace{\log P_{\text{judge}}(n^*|\vy,\vx)}_{\text{most likely non-``yes'' log prob.}}\big)
\end{aligned}
\end{equation}
If \(\reward(y, \tilde{x}) > 0.5\) the target metric is true and vice versa.
Due to the sigmoid \(\reward(y, \tilde{x}) \in [0, 1]\). This is desired for equivalence interpretation of our REINFORCE objective to the maximization of \(P(\harmful | X=\tilde{x})\). 

\begin{table}[b!]
\centering
\caption{ASR@512 for GCG~\citep{zou_universal_2023}, i.e.\ attack success rate (ASR) with 512 generated/judge tokens on the 200 prompts in standard HarmBench~\citep{mazeika_harmbench_2024}.}
\label{tab:main_gcg}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
   & Affirmative & REINFORCE \textbf{(ours)} \\
\midrule
Gemma 1.1 2B & 0.57 & \textbf{0.88} \\
Gemma 1.1 7B & 0.63 & \textbf{0.87} \\
Llama 2 7B & 0.32 & \textbf{0.56} \\
Llama 3 8B & 0.35 & \textbf{0.73} \\
Vicuna 1.5 7B & 0.86 & \textbf{0.95} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[b!]
\centering
\caption{ASR@512 for PGD of \citet{geisler_attacking_2024}.}
\label{tab:main_pgd}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
   & Affirmative & REINFORCE \textbf{(ours)} \\
\midrule
Gemma 1.1 2B & 0.56 & \textbf{0.82} \\
Gemma 1.1 7B & 0.54 & \textbf{0.84} \\
Llama 2 7B & 0.17 & \textbf{0.22} \\
Llama 3 8B & 0.57 & \textbf{0.69} \\
Vicuna 1.5 7B & 0.87 & \textbf{0.94} \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Main Results}

We report the results for REINFORCE-GCG in \autoref{tab:main_gcg} and REINFORCE-PGD in \autoref{tab:main_pgd}. We observe consistent and substantial gains over the affirmative objective. For example, the ASR@512 of REINFORCE-GCG is more than double that of affirmative GCG on Llama 3 8B. The only exception is Llama 2 7B, where PGD's performance improves only marginally. Note that we neither tuned the hyperparameters of the base attacks nor do we use a model-specific configuration. We provide uniformly randomly sampled examples of successful GCG attacks and the models' responses in \autoref{app:exmpirical_examples} (truncated to the first six lines). These examples are further evidence that our objective successfully reveals harmful behaviors that are different to \(\vy_{\text{affirmative}}\). Interestingly, we observe that the models sometimes refuse to answer in the first sentences of their response but ultimately provide the answer (e.g., second example in \autoref{tab:examples_llama3}).

\begin{table}[t]
\caption{Attack success rate (ASR) with our REINFORCE-GCG for attacking Llama 3 and its circuit-breaker defended version~\citep{zou_improving_2024}. * denotes numbers on the subset of successful attacks on the base model with \(\vy_{\text{affirmative}}\).}
\label{tab:main_cb}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrrrr}
\toprule
   & \multicolumn{2}{c}{Affirmative} & \multicolumn{4}{c}{REINFORCE \textbf{(ours)}} \\
\multicolumn{1}{r}{\(\vy_{\text{seed}}=\)} & \multicolumn{2}{c}{-} & \multicolumn{2}{c}{\(\vy_{\text{affirmative}}\)} & \multicolumn{2}{c}{\(\vy_{\text{history}}\)} \\
\multicolumn{1}{r}{ASR@} & 128 & 512 & 128 & 512& 128 & 512 \\
\midrule
Llama 3 8B & 0.29 & 0.35 & \textbf{0.66} & \textbf{0.73} & - & - \\
+ Circuit breaker & 0.01 & 0.02 & 0.21 & 0.23 & \textbf{0.46}$^*$ & \textbf{0.50}$^*$ \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure}[b!]
    \centering
    \includegraphics[width=\linewidth]{assets/performance_tradeoff_legend.pdf}\\
    \vspace{-10pt}
    \subfloat[Gemma 1.1 2B\label{fig:gcg_performance:gemma_2b}]{\includegraphics[width=0.49\linewidth]{assets/performance_tradeoff_google_gemma-1.1-2b-it.pdf}}\hfill
    \subfloat[Llama 3 8B\label{fig:gcg_performance:llama3_8b}]{\includegraphics[width=0.49\linewidth]{assets/performance_tradeoff_meta-llama_Meta-Llama-3-8B-Instruct.pdf}}
    \caption{Our REINFORCE objective provides a good ASR@128/runtime tradeoff in contrast to GCG with its affirmative response objective. Here we show the reward of the judge used during the attack. Runtimes are for H100s.}
    \label{fig:gcg_performance}
\end{figure}

\textbf{Circuit breaker.} We also evaluate the efficacy of our REINFORCE objective on the state-of-the-art defense based on circuit breaking~\citep{zou_improving_2024} and report the results in \autoref{tab:main_cb}. We observe a substantial gain from 2\% ASR@512 with the affirmative objective to 23\% with REINFORCE. Motivated by the fact that \citet{zou_improving_2024} tuned their model to withstand harmful requests and considering that our experiments run on a hard compute constraint that hinders thorough exploration, we also investigate the impact of better seed responses \(\vy_{\text{seed}}\). Specifically, we take the most harmful generation we found via successful attacks on the base model Llama 3 8B, which itself is already safety-tuned. On this subset of the data, we achieve a strong ASR@512 of 50\%. However, it should be noted that the circuit-breaking defense often produces stutter-like behavior late in their generation. \autoref{app:exmpirical_circuit_breaker} contains randomly sampled examples of our attacks and the model's responses. Nevertheless, even though the circuit breaker defense almost always repels an affirmative GCG attack for an adversarial suffix of 20 tokens, our REINFORCE attack achieves a high success rate. This finding underlines the importance of an (asymptotically) consistent objective for adaptive attacks on LLMs.

\textbf{Compute performance tradeoff.} We next investigate the compute performance tradeoff in \autoref{fig:gcg_performance}. Specifically, we contrast the ASR@128 for GCG with affirmative response and $E=5,000$ attack iterations to our REINFORCE-GCG and REINFORCE-PGD for the light-weight Gemma 1.1 2B (\autoref{fig:gcg_performance:gemma_2b}) and Llama 3 8B (\autoref{fig:gcg_performance:llama3_8b}). For PGD, we report the judge used for reward calculation (Mistral). We observe that our REINFORCE optimization not only achieves strong terminal ASRs but also is computationally efficient. REINFORCE-PGD performs particularly strong for Gemma 1.1 2B (\autoref{fig:gcg_performance:gemma_2b}) and, for Llama 3 8B (\autoref{fig:gcg_performance:llama3_8b}), our REINFORCE jailbreak attacks outperform standard GCG on most time scales. Note that we did not specifically tune our attacks w.r.t.\ the runtime vs.\ performance tradeoff.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.97\linewidth]{assets/mutation_selection_strategies_Meta-Llama-3-8B.pdf}
    \caption{Ablations of search and selection strategies for GCG on Llama 3 8B and ASR@512. We either select the mutated candidates (a) randomly, (b) using the gradient of the affirmative response, (c) or REINFORCE. We select the best candidate either according to (1) the affirmative response or (2) REINFORCE.}
    \label{fig:gcg_strategies}
\end{figure}

\begin{figure*}[t!]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc}
    \subfloat[Terminal rewards]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_rewards.pdf}} &
    \subfloat[Cross entropies of gradient calculation]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_ces.pdf}} &
    \vbox{\hbox{\includegraphics[width=0.175\linewidth]{assets/insights/insights_legend.pdf}}\vspace{5pt}}
    \hfill\\
    \subfloat[Cross entropies of \(\vy_{\text{seed}}\) for mutations]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_hist_bsln.pdf}} &
    \subfloat[Cross entropies of \(\vy_{\text{greedy}}\) for mutations]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_hist_0.pdf}} &
    \subfloat[Cross entropies of \(\vy_{\text{harmful}}\) for mutations]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_hist_harm.pdf}} \\
    \end{tabular}
    }
    \caption{Random example for an attack on Llama 3 8B (first 50 steps). As we show in (a), in attack step 7, the model's random generation \(\vy_{\text{random}}\) is harmful, which we then include as \(\vy_{\text{harmful}}\). In step 17, also \(\vy_{\text{greedy}}\) becomes harmful. Thereafter, also the harmfulness of random generations rises (see moving average \(\operatorname{MA}\)). As shown in (b), already small changes in \(P_{\model}(\vy_{\text{seed}}) = - \operatorname{CE}(\vy_{\text{seed}})\) may suffice to obtain harmful generations. Specifically, \(\operatorname{CE}(\vy_{\text{seed}})\) decreases for the first iterations and increases again after the greedy generation becomes harmful. (c-e) show histograms of the mutation's \(\operatorname{CE}\)s.}
    \label{fig:gcg_insights}
\end{figure*}

\subsection{Ablations and Insights}\label{sec:ablations}

\begin{table}[t]
\centering
\caption{Ablation study of the main REINFORCE-specific design choice: the sampling strategy \(\Pm'(Y|X)\). We use GCG (first row of table) as the base attack on Llama 3 8B.}
\label{tab:reinforce_ablation}
\resizebox{0.575\linewidth}{!}{
\begin{tabular}{p{0.3cm}p{0.3cm}p{0.3cm}p{0.3cm}p{0.3cm}cp{0.45cm}p{0.75cm}}
\toprule
\rotatebox{60}{extend $\vy_{\text{seed}}$} & 
\rotatebox{60}{w/ $\vy_{\text{seed}}$} & \rotatebox{60}{w/ $\vy_{\text{random}}$} & \rotatebox{60}{w/ $\vy_{\text{harmful}}$} & \rotatebox{60}{w/ $\vy_{\text{greedy}}$} &  & \rotatebox{60}{ASR@128} & \rotatebox{60}{ASR@512} \\
\midrule
\ding{55} & \ding{51} & \ding{55} & \ding{55} & \ding{55} && 0.29 & 0.35 \\
\ding{55} & \ding{51} & \ding{51} & \ding{55} & \ding{55} && 0.22 & 0.36 \\
\ding{51} & \ding{51} & \ding{51} & \ding{55} & \ding{55} && 0.24 & 0.34 \\
\ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{55} && 0.42 & 0.56 \\
\ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{51} && 0.66 & 0.73 \\
\bottomrule
\end{tabular}
}
\end{table}


\textbf{REINFORCE ablations.} In \autoref{tab:reinforce_ablation}, we provide an ablation study for the different responses of our sampling strategy \(\Pm'(Y|X)\). Surprisingly, adding a random generation \(\vy_{\text{random}}\) to the common affirmative objective \(\vy_{\text{seed}} = \vy_{\text{affirmative}}\) does not yield improvement. On the contrary, it even seems to hurt performance. Extending \(\vy_{\text{seed}}\) greedily to 128 tokens to harmonize the lengths seems to improve performance marginally. However, adding \(\vy_{\text{harmful}}\) does yield substantial improvement. Last, our full sampling strategy that also includes \(\vy_{\text{greedy}}\) achieves the best performance.

\textbf{GCG-specific ablations.} In \autoref{fig:gcg_strategies}, we investigate the GCG-specific aspects of \emph{mutating} the prompt and \emph{selecting} the best candidate on Llama 3 8B. Additionally to the affirmative objective \(\log P_\theta(\vy_{\text{affirmative}}|\tx)\), we also study uniformly random mutations. For each trial that involves our REINFORCE objective, we also use it to select the best prompt \(\tX^{(i)}\) for the ASR evaluation. The random mutations (blue) are consistently the worst choice, which indicates that the gradient is somewhat informative for flipping entries in the one hot matrix \(\tX\). However, it should be noted that random mutations combined with our objective to select the best candidate (solid blue) outperform vanilla GCG by a large margin (hatched orange). This finding is likely tied to the success of so-called Best-of-N (BoN) attacks~\citep{hughes_best--n_2024}. Moreover, our REINFORCE gradient seems to help even if using the affirmative selection criterion (hatched green). The most surprising result is perhaps that the affirmative gradient suffices in combination with our REINFORCE objective for selection (solid orange). We hypothesize that this is due to the excessive search width of \(S=512\) and the specifics of GCG's mutation sampling. In each step, GCG samples a diverse set of candidates--even with affirmative gradient \(\nabla_{\tX} \log P_\theta(\vy_{\text{affirmative}}|\tX)\).


\textbf{Choice of \(\vy_{\text{seed}}\).} On Llama 3 8B (+ circuit breaker) we also run an experiment with the model-specific targets obtained via AdvPrefix~\citep{zhu_advprefix_2024}. We find that our REINFORCE objective is not overly sensitive to the specific choice of \(\vy_{\text{seed}}\); however, benefits from a sensible choice. Taking also the results from our experiments with \(\vy_{\text{history}}\) with circuit breakers (\autoref{tab:main_cb}) into account, it appears that making \(\vy_{\text{seed}}\) sufficiently likely must be sufficiently easy. %
Moreover, we see the advantage of a dynamic objective like our REINFORCE on the state-of-the-art defense via circuit breaking. For example, the affirmative objective with AdvPrefix achieves an ASR@512 of 0.14, while our REINFORCE achieves 0.5. We refer to \autoref{app:advprefix} for details.

\textbf{Dynamics.} In \autoref{fig:gcg_insights}, we plot the early rewards and the cross entropies. Here we show that a random generation \(\vy_{\text{random}}\) which is harmful may then be used as \(\vy_{\text{harmful}}\). A few steps later, also the greedy generation becomes harmful, and the harmfulness of random generations rises. For this, we do not even require \(\vy_{\text{seed}}\) to become very likely, and the attack may pursue generations that are very different. For more details and comments on consistency, see \autoref{app:gcg_insights}. Moreover, for a comparison of attack and evaluation reward see \autoref{app:generalization}.





\section{Related Work}

\textbf{Adversarial attacks}, and specifically jailbreak attacks or automated red teaming, can be categorized into (1) optimization based attacks~\citep{wallace_universal_2021, shin_autoprompt_2020, guo_gradient-based_2021, zou_universal_2023, geisler_attacking_2024, guo_cold-attack_2024, wen_hard_2023, kumar_gradient-based_2022, hou_textgrad_2023,liu_autodan_2024,zhu_autodan_2023,andriushchenko_jailbreaking_2025,schwinn_soft_2024,hughes_best--n_2024,sadasivan_fast_2024,thompson_flrt_2024} or (2) attacks using generative models/LLMs~\citep{perez_red_2022, mehrotra_tree_2023, chao_jailbreaking_2023,liao_amplegcg_2024,chen_rl-jack_2024,jha_llmstinger_2024,lin_pathseeker_2024}. Our novel REINFORCE objective is focusing on approaches of category (1) that use gradient information~\citep{wallace_universal_2021, shin_autoprompt_2020, guo_gradient-based_2021, zou_universal_2023, geisler_attacking_2024, guo_cold-attack_2024, wen_hard_2023, kumar_gradient-based_2022, hou_textgrad_2023,zhu_autodan_2023}. Specifically, we extend two gradient-based attacks, namely GCG~\citep{zou_universal_2023} and PGD~\citep{geisler_attacking_2024}. While GCG builds on top of the language model attack AutoPromopt~\citep{shin_autoprompt_2020}, PGD relates to adversarial attacks on GNNs~\citep{xu_adversarial_2020, geisler_attacking_2021,gosch_adversarial_2023,foth_relaxing_2024}. 
While ours is not the first jailbreak attack that uses reinforcement learning (RL), usually RL-based approaches are of category (2) and train another LLM to generate prompts for jailbreaking the targeted LLM~\citep{perez_red_2022, mehrotra_tree_2023, chao_jailbreaking_2023, chen_rl-jack_2024,jha_llmstinger_2024, lin_pathseeker_2024}. 
Even though we target gradient-based optimization, our objective is also very effective for GCG with random mutations (see \autoref{fig:gcg_strategies}). Thus, approaches like \citet{andriushchenko_jailbreaking_2025,liu_autodan_2024} could also benefit from our advanced objective.

\citet{andriushchenko_jailbreaking_2025} explore adaptive attacks. In contrast to our work, they add certain (potentially model-specific) features to their attack that improve the attack success rates (e.g., self-transfer, prompt templates, restarts, etc.). Most of their strategies are orthogonal to ours and could further improve REINFORCE-GCG / -PGD. Nevertheless, it should be noted that they also rely on a non-consistent attack objective (e.g., log probability of ``Sure''). Our findings with adaptive attacks on the circuit breaker defenses align with embedding space attacks~\cite {schwinn_revisiting_2024}.

Adversarial attacks that capture rich semantics have been explored before~\citep{qiu_SemanticAdv_2020, geisler_generalization_2022, wang_semantic_2023, kollovieh_assessing_2024}. Notably, \citet{wichers_gradient-based_2024} also explore attacks on LLMs that leverage a judge. In contrast to our work, they relax the generation using Gumble softmax~\citep{jang_categorical_2016} and backpropagate through the attacked model and judge for a GBDA-based optimization~\citep{guo_gradient-based_2021}. 

\textbf{Affirmative response.} Even though virtually all of the aforementioned optimization-based jailbreak attacks use an affirmative response objective, there have been attempts to mitigate its limitation via varying the template~\citep{jia_improved_2024} or distilling responses from a modified model~\citep{thompson_flrt_2024}. Concurrently, AdvPrefix 
 of \citet{zhu_advprefix_2024} finds better response prefixes via high prefilling attack success rates and low negative log-likelihood. Similar to them, our REINFORCE objective also alleviates the issue that the affirmative responses are rather short~\citep{qi_safety_2025}. We demonstrate the complementary strengths of AdvPrefix and our objective in \autoref{app:advprefix}.

\textbf{Judges and evaluation.} Multiple judges have been introduced in the literature~\citep{shen_anything_2024, chao_jailbreaking_2023, bhatt_purple_2023}. We use the HarmBench~\citep{mazeika_harmbench_2024} judge since we evaluate on HarmBench. Nevertheless, we note the recent work by \citet{souly_strongreject_2024} as their judge achieves a slightly better alignment to human evaluation. Moreover, since current LLM-as-a-judge models for scoring harmfulness are all imperfect, one could augment their scores with frequent patterns for false positives (non-harmful prompts that are judged harmful) like~\citet{hughes_best--n_2024}. Next to a judge-based evaluation, one could also gain insights into our objective through (mechanistic) interpretability~\citep{arditi_refusal_2024, wollschlager_geometry_2025}.

\section{Limitations}

Due to the use of an LLM-as-a-judge reward signal, the judges' shortcomings influence our REINFORCE objective. However, we do not observe systematic issues, as we demonstrate in the random examples in \autoref{app:exmpirical_examples}. While we leave improving judges for future work, extrapolating previous developments, one can expect that they will steadily advance over time, and along with the advancements of judges, our REINFORCE attack objective may steadily improve.


Our evaluation relies on the greedy generation for the evaluation. A harmful greedy generation neither implies that other (random) generation must be harmful~\citep{scholten_probabilistic_2025} nor is it a very stable choice in \(\tx\). Additionally, in practice, more sophisticated decoding schemes have been used that impact the model's alignment~\cite {balashankar_infalign_2024}. Importantly, we avoided design decisions as much as possible that solely help due to the greedy evaluation.



\section{Conclusion and Discussion}

We propose an adaptive, distributional, and semantic objective rooted in reinforcement learning that overcomes the static nature of current attack objectives for jailbreaking generative models/LLMs. We show that our objective aims to optimize the probability for the model to output a harmful response (assumptions apply), and we implement our objective via REINFORCE. We demonstrate the efficacy empirically by utilizing the state-of-the-art jailbreak attacks PGD and GCG for our adversarial optimization procedure to attack state-of-the-art aligned LLMs. 

While our approach yields more effective jailbreaks for current models, we argue that adaptive objectives will be a cornerstone for the rigorous evaluation of future models. With the rising capabilities of LLMs, it is unlikely that human-engineered inappropriate/incorrect responses to a given prompt are the way to go in identifying model misbehavior. Similarly to tasks like scalable oversight~\citep{bowman_measuring_2022}, we expect humans will have trouble evaluating sufficiently smart or sophisticated models. Hence, we believe that our REINFORCE objective and its theoretical underpinning will be an important framework for the (offline) assessment of such models' failure modes.

\section*{Acknowledgements}

This research was supported by the Center for AI Safety Compute Cluster. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. Further, this material is based on work partially funded by Google. 

We also thank Leo Schwinn and Tim Beyer for their feedback and the related discussions.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning, and specifically generative models, in terms of reliability. We mitigate the risk of adversarial use of our adversarial attack objective via studying the white-box scenario. That is, the attacker needs full knowledge about the model architecture, including implementation details, and the model parameters. Hence, the biggest risk arises for models that have been released openly. Nevertheless, from a cybersecurity perspective, it is usually best to reveal security risks s.t.\ future versions/models can deploy appropriate fixes. Hence, we believe that the availability of our method to model developers outweighs the risks of its adversarial use.



\bibliography{references}
\bibliographystyle{some2025}


\newpage
\appendix

\onecolumn


\section{Greedy Coordinate Gradient (GCG)}\label{app:gcg}

We next provide additional details on GCG and its parametrization. We adopt the implementation details and parametrization of \citet{zou_universal_2023} unless stated otherwise.

\textbf{Mutate.} The term ``Coordinate'' in GCG indicates that one dimension is optimized at a time. This ``Coordinate''-property is implemented via the mutation sampler \(\operatorname{Mutate}_{S}(\mG, \tx)\). We provide the inner workings of the mutation sampler according to HarmBench's GCG implementation in \author{algo:gcg_mutate}. Recall that $:i$ is right exclusive and $i:$ left inclusive (like in Python).

\textbf{Tokenization inconsistencies.} Additionally GCG filters out mutated token sequences for which \(\tx^{(i)} \ne \text{tokenizer.encode}(\text{tokenizer.decode}(\tx^{(i)}))\), unless this condition is true for all mutated strings.

\begin{algorithm}[H]
  \small 
  \caption{GCG's \(\operatorname{Mutate}_{S}(\mG, \tx)\)}
  \label{algo:gcg_mutate}
    \begin{algorithmic}[1]
    \State {\bfseries Input:} Gradient \(\mG \in \R^{T' \times |\gV|}\), current tokens \(\tx^{(i)})\), search width \(S\)
    \State {\bfseries Parameters:} \# top tokens based on gradient \(k\)
    \For{\(i \in \mathcal{I}\)}
        \State \(\gC_i \leftarrow \text { Top-} k\left(-\mG_{i, :}\right)\)
    \EndFor
    \State \(\hat{\gX} \leftarrow \{\}\)
    \For{\(j \in \{1,2,\dots,S\}\)}
        \State \(i \leftarrow j \mod T'\)
        \State \(\hat{\gX} \leftarrow \hat{\gX} \cup \{\tx_{:i} \odot [\operatorname{Uniform}(\gC_i)] \odot \tx_{i+1:}\}\)
    \EndFor
    \State \textbf{return} \(\hat{\gX}\)
  \end{algorithmic}
\end{algorithm}

\textbf{Early stopping.} \citet{zou_universal_2023} also implemented an early stopping criterion where the attack is halted once a certain loss value is surpassed. We do not experiment with such a criterion, although it would be straightforward to implement.

\textbf{Disallowed tokens.} We also do not allow non-ascii tokens in \(\tx\) and follow HarmBench's implementation for this criterion. We not only enforce this for GCG but for all attacks in every setting in this work. 

\textbf{No fluency loss.} For GCG we do not include any fluency loss.


\section{Projected Gradient Descent (PGD)}\label{app:pgd}

We next provide additional details on PGD and its parametrization. We fully adopt the implementation details and parametrization of \citet{geisler_attacking_2024}.

\textbf{Projection(s).} The projection \(\Pi(\tX)\) ensures (1) that \(\tX\) remains in \([0,1]^{T' \times |\gV|}\) after the gradient update and that it remains a row-stochastic matrix \(\tX \mathbf{1}_{|\gV|} = \mathbf{1}_{T'}\). Note that also the one-hot matrix has this property. Additionally, (2) the projection ensures that \(\tX\) remains ``close'' to a one-hot matrix by the so-called entropy projection. Hence, the projection \(\Pi(\tX) = \Pi_{\text{entropy}}(\Pi_{\text{simplex}}(\tX))\) is a composition of (1) the simplex projection \(\Pi_{\text{simplex}}(\tX)\) and (2) the entropy projection \(\Pi_{\text{entropy}}(\tX)\). We provide pseudo code in \autoref{algo:pgd_simplex} and \autoref{algo:pgd_entropy}, respectively. The projections are applied to each token independently.

\textbf{Gradient clipping.} Additionally, to the pseudo code we clip the L2 norm of the gradient for each token \(\mG_i\) to 20. This avoids that exploding gradients mess up the momentum terms in the used Adam optimizer.

\textbf{Discretization and tokenization inconsistencies.} We denote the discretization with \(\tx = d(\tX)\), which applies a row-wise argmax operation to obtain \(\tx\). Additionally, we use the attacked model's tokenizer to avoid encode-decode inconsistencies. Thus, the full discretization procedure is \(d(\tX) = \operatorname{tokenizer.encode}(\operatorname{tokenizer.decode}(\argmax(\tX, \operatorname{axis=-1})))\).


\begin{algorithm}[H]
  \small 
  \caption{Simplex Projection \(\Pi_{\text{simplex}}\)}
  \label{algo:pgd_simplex}
    \begin{algorithmic}[1]
    \State {\bfseries Input:} Updated token \(\vs \in \R^{|\sT|}\)
    \State Sort \(\vs\) into \(\mu_1 \ge \mu_2 \ge \dots \ge \mu_{|\sT|}\)
    \State \(\rho \leftarrow \sum_{i=1}^{|\sT|} \sI \left[ \{ \mu_i - \nicefrac{1}{i} (\sum_{j=1}^i \mu_j - 1) \} > 0 \right]\)
    \State \(\psi \leftarrow \nicefrac{1}{\rho} (\sum_{j = 1}^\rho \mu_j - 1)\)
    \State {\bfseries Return} \(\vp\) s.t.\ \(\evp_i = \max\{\evs_i - \psi, 0\}\)
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \small 
  \caption{Entropy Projection \(\Pi_{\text{entropy}}\)}
  \label{algo:pgd_entropy}
    \begin{algorithmic}[1]
    \State {\bfseries Input:} Rel.\ token \(\vs \in [0, 1]^{|\sT|}\), target entropy \(S_{q=2}\)
    \State Center \(\vc \leftarrow \nicefrac{\sI[\vs > 0]}{\sum_{i=1}^{|\sT|} \sI[\vs > 0]}\) with element-wise \(>\) and \(\sI\)
    \State Radius \(R \leftarrow \sqrt{1 - S_{q=2} - \nicefrac{1}{\sum_{i=1}^{|\sT|} \sI[\vs > 0]}}\)
    \If{\(R \ge \|\vs - \vc\|\)}
        \State {\bfseries Return} \(\vs\)
    \Else
        \State {\bfseries Return} \(\Pi_{\text{simplex}}(\nicefrac{R}{\|\vs - \vc\|} \cdot (\vs - \vc) + \vc)\)
    \EndIf
  \end{algorithmic}
\end{algorithm}


\textbf{Patience.} If the target metric \(\ell_{\text{metric}}\) (for REINFORCE see \autoref{app:reinforce}) does not improve for a predefined number of iterations (100) we reset to the best previously known state \(\tX^{(\text{best})}\) or with 50\% chance we use a promising adversarial prompt from a different optimization in the batch. In both cases, we reinitialize \(\tX^{(i)}\) with discretized \(d(\tX^{(\text{best})})\), which is the one-hot encoding of \(\tx^{(\text{best})}\). We sample prompts from a different prompt in the batch from \(\operatorname{Cat}(\operatorname{softmax}(-\ell_{\text{metric}} / 0.25))\).

\textbf{Learning rate scheduler.} We linearly ramp up the learning rate along with the entropy regularization for the first 100 iterations. Thereafter, we apply a cosine annealing scheduler with warm restarts~\citep{loshchilov_sgdr_2017} with 60 steps duration and a terminal learning rate of 0.325.

\textbf{Entropy projection strength.} The entropy projection strength is scaled together with the learning rate, where lower learning rates correspond to a weaker projection. Moreover, the entropy projection strength is coupled to the difference between the ``relaxed loss'' \(\ell(\tX)\) and ``discrete loss'' \(\ell(\tx)\).

\section{REINFORCE}\label{app:reinforce}

\textbf{Sampling strategy.} We design the sampling strategy \(\Pm'(Y|X = \tX)\) with the goal of obtaining a sample efficient yet effective estimator for the REINFORCE objective of \autoref{eq:reinforce_gradient}. Thus, we use the following samples/generations:
\begin{enumerate}
    \item The static \(\vy_{\text{seed}}\): which is usually equal to the \(\vy_{\text{affirmative}}\) but can also be chosen from a previously successful attack for the same behavior on a potentially different model. Alternatively, one may use advanced initialization strategies such as \(\vy_{\text{advprefix}}\) from \citet{zhu_advprefix_2024} or successful generations from a different model \(\vy_{\text{history}}\). To ensure that \(\vy_{\text{seed}}\) provides sufficient guidance regardless of the LLM-as-a-judge reward, we clip the reward of \(\vy_{\text{seed}}\) from \([0,1]\) to \([0.5, 1.0]\). If \(\vy_{\text{seed}}\) is shorter than 128 tokens, we greedily complete its generations. In this case, we use the aforementioned reward clipping only for the original prefix of \(\vy_{\text{seed}}\), before extending it. For GCG, we anyways calculate intermediate rewards to handle the extension properly. For PGD, the extension is efficient since we only need one forward and backward pass of \(\model\) for the \emph{extended} \(\vy_{\text{seed}}\).
    \item The greedy roll-out  \(\vy_{\text{greedy}}\) which autoregressively returns the most likely next token. This sample can be understood as a cheap estimate for the mode of the distribution
    \item One random sample \(\vy_{\text{random}}\) with temperature \(0.7\) among the 256 most likely next tokens. We also experimented with more samples; however, only with minor impact on the target metric if accounting for the higher computational cost.
    \item The most likely harmful generation  \(\vy_{\text{harmful}}\) that we observed throughout the optimization. Specifically, we include a harmful response as soon its reward surpasses 0.2. Until reward 0.75 we always take the most harmful response we found. Beyond 0.75 we favor longer responses or responses that have a lower average cross entropy.
\end{enumerate}

Even though it should be possible to (largely) de-bias our estimator, e.g., using importance sampling similar to~\citet{kool_buy_2019}, we decide to keep the estimator unchanged/simple as we found it to be effective. 

\textbf{Judge.} We use HarmBench's~\citep{mazeika_harmbench_2024} judge for evaluation and rewards. Following HarmBench's usage, we calculate the reward based on the clean prompt  \(\reward(\vy, \vx)\) and not the perturbed prompt \(\tx\). 

Since we usually calculate the rewards for a batch of generations, stability considerations are important to further consistency in the optimization. Due to the varying length of the generations and prompts, we have a varying number of padded tokens. Although the differences in the rewards are usually rather small, they do exist and introduce noise into \(\reward(\vy, \vx)\). For this reason, during the attack we always (right) pad to a total length of generated 128 tokens.


\textbf{Token-level weighting.} We employ two mechanisms that eventually yield a token-level weighting:
\begin{enumerate}
    \item Linearly decaying weight of token-importance. Since we are optimizing over a long generation, in comparison to the affirmative objective (\(\approx 20\) tokens), we also introduce a weighting of the tokens \(\vy\) according to their positions. Specifically, we give the first token a five times higher weight than the last (128th) token and distribute the weight linearly in between. This prioritizes changing tokens early in the sequence vs.\ later tokens. Even though the performance differences are not pivotal, we argue that prioritizing early tokens is particularly important for the optimization over long generations with a small number of sampled generations.
    \item To encourage exploiting harmful generations and making them more likely, we aggregate the rewards for matching prefixes of two samples at the token level and calculate the RLOO weights for each token individually. This is important, especially in cases where the model sometimes refuses to answer rather late in the sequence. If two generations/samples  \(\vy^{(1)}\), \(\vy^{(2)}\) match until token \(m\) (\(\vy^{(1)}_{:m+1} = \vy^{(2)}_{:m+1}\) and \(\vy^{(1)}_{:m+2} \ne \vy^{(2)}_{:m+2}\), recall that our \(:m\) notation is exclusive), we assign the first \(m\) tokens the higher reward \(\max(\reward(\vy^{(1)}, \tx), \reward(\vy^{(2)}, \tx))\). Arguably this aggregation has similarities to Monte Carlo tree search (MCTS) since matching prefixes in \(\vy\) correspond to matching nodes in a search tree. We also apply this strategy if calculating intermediate rewards for REINFORCE-GCG.
\end{enumerate}


\textbf{REINFORCE loss \(\ell\).} The REINFORCE Leave-One-Out (RLOO) loss we ultimately use (negative of \autoref{eq:loor} after dropping the gradient) resolves to:
\begin{equation}\label{eq:loor_loss}
\begin{aligned}
    \ell(\hat{\gY}, \tX)
    &= - \sum_{i=1}^K \left[ \reward(\vy^{(i)}, \tX) - \frac{b_{\text{static}}}{K} - \frac{1}{K}\sum_{j \ne i} \reward(\vy^{(j)}, \tX) \right] \log P(\vy^{(i)} | \tX) \\
     &= \sum_{i=1}^K \left[ \reward(\vy^{(i)}, \tX) - \frac{b_{\text{static}}}{K} - \frac{1}{K}\sum_{j \ne i} \reward(\vy^{(j)}, \tX) \right] \operatorname{CE}(\vy^{(i)} | \tX) \\
\end{aligned}
\end{equation}
with samples \(\hat{\gY} = \{\vy_{\text{seed}}, \vy_{\text{random}}, \vy_{\text{greedy}}, \vy_{\text{harmful}}\} \sim \Pm'(Y|X)\) and static \(b_{\text{static}}\). As detailed in the main part, we introduce \(b_{\text{static}}=0.1\) to stabilize the loss when all generations are harmless or harmful.

\textbf{Target metric \(\ell_{\text{metric}}\).} As the target metric, that we use to determine the most successful attack step, we largely follow the estimator according to \autoref{eq:loor}/\autoref{eq:loor_loss}, using our biased sampler to obtain up to \(K=3\) samples, where we exclude the random generation \(\vy_{\text{random}}\). We exclude \(\vy_{\text{random}}\) to avoid its randomness since we only generate a single random generation. Note that due to the most likely harmful sample, \(K=2\) is also possible. Unfortunately, due to the dynamic baseline (the average reward of other samples) and the low amount of samples, the loss is still not directly usable as a \emph{consistent} metric to measure overall progress spanning all attack iterations. We choose to add a large constant (10) if the greedy generation is not harmful. As soon as the greedy generation is harmful, we double its weight in the loss calculation.

\textbf{REINFORCE-PGD.} For PGD we directly apply the described approach to estimate the REINFORCE gradient. For memory reasons (80\,GB GPU RAM), we use HarmBench's validation judge that is based on Mistral 7B~\citep{jiang_mistral_2023}.

\textbf{REINFORCE-GCG.} In contrast to PGD, for GCG it is easier to accommodate the Llama 2 13B judge and, thus, we decide for it as it is the primary judge of HarmBench. For GCG we have two deviations from the general REINFORCE procedure: (1) we additionally calculate rewards for tokens at positions 20, 40, and 80; (2) we do not use the random generation (temperature 0.7) for the selection. This reduces the overall runtime almost by 25\% since the scoring of the candidates is by far the most expensive step (for generation length 128).

Similarly to the global selection of the best step, for REINFORCE-GCG we need to select the best candidate in each step. Here we can directly use \autoref{eq:loor_loss}. However, we exclude the only random generation to avoid diminishing progress based on a potentially unlikely generation. Specifically, we keep the generations/rewards (minus baselines) constant and solely calculate the likelihoods \(\log P(\vy | \tX)\) for each candidate in \(\argmin_{\tX \in \hat{\gX}} \ell(\hat{\gY}, \tX)\).

Given the intermediate rewards at positions 20, 40, 80, and the terminal reward at position 128, we progressively adapt the number of generated tokens for the calculation of \(\argmin_{\tX \in \hat{\gX}} \ell(\hat{\gY}, \tX)\). We set the harmfulness/reward threshold to 0.01 for the greedy generation and include one additional position. However, we at least score until the generation of 40 tokens. For example, (a) if the greedy reward is 0 for all positions, we calculate the loss using 40 tokens; (b) if the greedy reward exceeds 0.01 at position 40, we calculate the loss using 80 tokens; or (c) if the greedy reward at position 80/128 exceeds 0.01 we calculate the loss using all 128 tokens.

\section{GCG vs.\ PGD Tradeoffs}\label{app:gcg_vs_pgd}

From the side-by-side comparison of GCG (\autoref{algo:gcg}) and PGD (\autoref{algo:pgd}), we observe many similarities between both approaches. For example, both approaches continuously relax the one-hot encoded and rely on the gradient to determine their next step. However, both approaches rely on different design choices that yield different tradeoffs as we list in \autoref{tab:gcg_vs_pgd}.

\begin{table}[H]
    \centering
    \caption{Comparison of GCG's and PGD's different characteristics and the resulting tradeoffs.}
    \label{tab:gcg_vs_pgd}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcc}
    \toprule
       Characteristic & GCG (\autoref{algo:gcg}) & PGD (\autoref{algo:pgd}) \\
    \midrule
        \multirow{5}{*}{How to perturb:} & Discrete mutation & Gradient-based optimization over relaxed prompts \\
        & A guaranteed change of the prompt & Gradient update not necessarily alters the discretized prompt \\
        & Each mutation alters a single token & Every token is affected by gradient update \\
        & Sample multiple mutations randomly & A single gradient step without additional randomness \\
        \midrule
        Batching: & Batching over mutations & Batching over multiple prompts/attacks \\
        \midrule
        Under runtime constraints: & Few attack iterations & Many attack iterations \\
    \bottomrule
    \end{tabular}
    }
\end{table}

In summary, GCG~\citep{zou_universal_2023} uses batching over multiple randomly sampled mutations for a single prompt to maximize GPU utilization. In contrast, in PGD~\citep{geisler_attacking_2024} only a single ``mutation'' is generated in each step and batches over multiple prompts. If further budgeting runtime, for PGD, one may apply more attack iterations. Importantly, in the context of REINFORCE, due to the computational cost of generations, it might be computationally suboptimal to produce a single ``mutation'' as PGD is doing. In other words, with the use of a computationally more demanding attack objective, the runtime cost of PGD is more strongly impacted than the one of GCG. For example, averaged over the experiment in \autoref{fig:gcg_performance}, REINFORCE-GCG is 9.4 times more expensive than vanilla GCG per step (roughly 2 to 3 times more expensive if generations are harmless). In contrast, REINFORCE-PGD is 33.3 times more expensive than vanilla PGD per step. The more astounding it is that despite the order of magnitude higher step cost, REINFORCE-GCG and -PGD appear to strike a better attack success vs.\ runtime tradeoff than vanilla GCG, as we show in \autoref{fig:gcg_performance}. The next section gives a more detailed breakdown of the runtime cost of our REINFORCE-GCG.

\section{GCG Runtime Cost}\label{app:gcg_cost}

For a more detailed breakdown of the time cost, we analyzed an attack on Llama 3 (excluding compile times) using PyTorch 2.5.1 with Python 3.12, CUDA 12.4, and Huggingface's transformers library~\citep{wolf_huggingfaces_2020} of version 4.47.1 on an 80\,GB H100 GPU. Here, the affirmative response had a length of 26 tokens. Recall that we follow \citet{zou_universal_2023} and use a search width of 512 tokens for GCG. We report average times for each step. We next explain the runtime for the three different cases (a) vanilla GCG, (b) best-case REINFORCE-GCG, and (c) worst-case REINFORCE-GCG. (b) applies if the generations are harmless and short, while (c) applies if generations are harmful and long. In \autoref{tab:runtimes}, we provide an overview.

\textbf{(a) Vanilla GCG.} With the configuration, the vanilla GCG~\citep{zou_universal_2023} requires roughly 1.3\,s per step using HarmBench's implementation~\citep{mazeika_harmbench_2024} with prefix caching. Specifically, the gradient costs around 100\,ms and the selection of the best mutation costs around 1.2\,s.

\textbf{(b) REINFORCE-GCG best-case runtime cost.} In the best case, when all generations are harmless and are short (around 20 tokens), the greedy and random generation require 250\,ms each. If completion of \(\vy_{\text{seed}}\) terminates immediately with an end-of-sequence token, its costs are negligible. In this scenario, the gradient calculation takes around 200\,ms. Calculating the rewards for the three generations (\(\vy_{\text{seed}}, \vy_{\text{greedy}}, \vy_{\text{random}}\)) at a single length (at most 27 tokens long) takes around 200\,ms. Last, the selection of the best mutation takes 1.7\,s (27 tokens). Moreover, we assume that there is no \(\vy_{\text{harmful}}\). Since the other costs are negligible, the total step cost is around 2.6\,s, which is 2.2 times more expensive than vanilla GCG.

\textbf{(c) REINFORCE-GCG worst-case runtime cost.} When the generations are long and harmful, our REINFORCE-GCG encounters higher costs. The generation of two times 128 tokens (\(\vy_{\text{greedy}}, \vy_{\text{random}}\)) takes 1.2\,s each, and completing 108 tokens (\(\vy_{\text{seed}}\)) takes 1\,s. The gradient calculation takes around 300\,ms. The rewards for \(\vy_{\text{seed}}, \vy_{\text{greedy}}, \vy_{\text{random}}\) at lengths 26, 40, 80, 128 tokens takes 600\,ms. For \(\vy_{\text{harmful}}\), we can reuse the historical values. The selection of the best mutation with 128 tokens takes 8.9\,s. In total, REINFORCE-GCG may require a worst-case runtime of 13.2\,s, which is 11 times more expensive than vanilla GCG.

With longer generations (128 tokens) that are still harmless, the cost can increase to around 6.4\,s seconds. Here we assume the time costs for generations, gradient, and reward as in the worst case. The selection with 40 tokens takes 2.3\,s.

\begin{table}[H]
\centering
\caption{Summary of runtime cost for vanilla GCG, best-case REINFORCE-GCG, and worst-case REINFORCE-GCG for a target length of up to 128 tokens on an Nvidia H100 80\,GB. Steps that are not listed have negligible runtime costs. The best case applies when all generations are harmless and short. The worst-case applies when generations are harmful and long. \label{tab:runtimes}}
\begin{tabular}{lrrr}
\toprule
Step & Vanilla GCG & Best-case REINFORCE-GCG & Worst-case REINFORCE-GCG \\
\midrule
Generation & - & 0.5\,s & 3.4\,s \\
Gradient & 0.1\,s & 0.2\,s & 0.3\,s \\
Reward & - & 0.2\,s & 0.6\,s \\
Selection & 1.2\,s & 1.7\,s & 8.9\,s \\
\midrule
Total & 1.3\,s & 2.6\,s & 13.2\,s\\
\bottomrule
\end{tabular}
\end{table}

\section{Generalization from Attack to Eval Metric}\label{app:generalization}

In this section, we contrast the results of the attack objective vs.\ the test objective. For REINFORCE-GCG, we use HarmBench's test judge based on Llama 2 13B with 128 tokens, and for REINFORCE-PGD, we use HarmBench's validation judge based on Mistral 7B with 128 tokens. We report the results for REINFORCE-GCG in \autoref{tab:extra_gcg} and observe almost flawless generalization from 128 to 512 tokens. In contrast, the results for REINFORCE-PGD in \autoref{tab:extra_pgd} show that the attack objective (Mistral 7B) and test objective (Llama 2 13B) differ greatly at 128 tokens. However, the differences become smaller between the attack objective (Mistral 7B) with 128 tokens and the test objective (Llama 2 13B) with 512 tokens. 

\begin{table}[H]
\centering
\caption{ASRs for GCG, where we contrast the attack objective (128 tokens) with the test objective (512) tokens.}
\label{tab:extra_gcg}
\begin{tabular}{lcccc}
\toprule
   & \multicolumn{2}{c}{Affirmative} & \multicolumn{2}{c}{REINFORCE \textbf{(ours)}} \\
\multicolumn{1}{r}{ASR@} & 128 & 512 & 128 & 512 \\
\midrule
Gemma 1.1 2B & 0.54 & 0.57 & \textbf{0.90} & \textbf{0.88} \\
Gemma 1.1 7B & 0.51 & 0.63 & \textbf{0.86} & \textbf{0.87} \\
Llama 2 7B & 0.32 & 0.32 & \textbf{0.63} & \textbf{0.56} \\
Llama 3 8B & 0.29 & 0.35 & \textbf{0.66} & \textbf{0.73} \\
Vicuna 1.5 7B & 0.85 & 0.86 & \textbf{1.00} & \textbf{0.95} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{ASRs for PGD, where we contrast the attack objective (Mistral 7B, 128 tokens) with the test objective (Llama 2 13B, 128 \& 512) tokens. Here we often observe substantial differences between the attack and test objective.}
\label{tab:extra_pgd}
\begin{tabular}{lcccccc}
\toprule
   & \multicolumn{3}{c}{Affirmative} & \multicolumn{3}{c}{REINFORCE \textbf{(ours)}} \\
\multicolumn{1}{r}{ASR@} & 128 (attack) & 128 & 512 & 128 (attack) & 128 & 512 \\
\midrule
Gemma 1.1 2B & 0.66 & 0.58 & 0.56 & \textbf{0.96} & \textbf{0.70} & \textbf{0.82} \\
Gemma 1.1 7B & 0.52 & 0.54 & 0.54 & \textbf{0.93} & \textbf{0.77} & \textbf{0.84} \\
Llama 2 7B & 0.20 & 0.19 & 0.17 & \textbf{0.26} & \textbf{0.24} & \textbf{0.22} \\
Llama 3 8B & 0.59 & \textbf{0.55} & 0.57 & \textbf{0.60} & 0.48 & \textbf{0.69} \\
Vicuna 1.5 7B & 0.84 & 0.80 & 0.87 & \textbf{0.97} & \textbf{0.90} & \textbf{0.94} \\
\bottomrule
\end{tabular}
\end{table}

\section{AdvPrefix~\citep{zhu_advprefix_2024}}\label{app:advprefix}

In \autoref{tab:advprefix}, we show additional ablations/comparisons to AdvPrefix. AdvPrefix is a technique to generate alternative affirmative responses that are tailored to the attacked model. We generate AdvPrefix for Llama 3 8B. The AdvPrefix generation takes around 5 minutes per prompt with the code provided by the authors and using two H100 80\,GB. The key observations are: (1) our REINFORCE objective outperforms theirs on both Llama 3 8B and its defended variant (circuit breaker). The differences are particularly pronounced with circuit breaker defense; and (2) using the generations of previously successful attacks \(\vy_{\text{history}}\) as well as \(\vy_{\text{advprefix}}\) for \(\vy_{\text{seed}}\) are both effective strategies.

\begin{table}[H]
\centering
\caption{Study of the impact of AdvPrefix from \citet{zhu_advprefix_2024} generated for Llama 3 8B. Here, the ``Affirmative'' objective maximizes the log probability \(\log P(\vy_{\text{seed}} | \tilde{\vx})\). \(\vy_{\text{affirmative}}\) is the target of HarmBench, \(\vy_{\text{advprefix}}\) denotes the target obtained via AdvPrefix, and \(\vy_{\text{history}}\) denotes a successful response from an attack on Llama 3 8B. We bold the best and underline the runner-up. * denotes numbers on the subset of successful attacks on the base model with \(\vy_{\text{affirmative}}\).}
\label{tab:advprefix}
\begin{tabular}{lrrrrrrrrrr}
\toprule
& \multicolumn{4}{c}{Affirmative} & \multicolumn{6}{c}{REINFORCE \textbf{(ours)}} \\
\multicolumn{1}{r}{\(\vy_{\text{seed}}=\)} & \multicolumn{2}{c}{\(\vy_{\text{affirmative}}\)} & \multicolumn{2}{c}{\(\vy_{\text{advprefix}}\)} & \multicolumn{2}{c}{\(\vy_{\text{affirmative}}\)} & \multicolumn{2}{c}{\(\vy_{\text{advprefix}}\)} & \multicolumn{2}{c}{\(\vy_{\text{history}}\)} \\
\multicolumn{1}{r}{ASR@} & 128 & 512 & 128 & 512 & 128 & 512 & 128 & 512 & 128 & 512 \\
\midrule
Llama 3 8B & 0.29 & 0.35 & \underline{0.67} & 0.70 & 0.66 & \underline{0.73} & \textbf{0.81} & \textbf{0.81} & - & - \\
+ Circuit breaker & 0.01 & 0.02 & 0.12 & 0.14 & 0.21 & 0.23 & \textbf{0.47} & \underline{0.48} & \underline{0.46}$^*$ & \textbf{0.50}$^*$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Dynamics of REINFORCE-GCG}\label{app:gcg_insights}

\begin{figure}[H]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc}
    \subfloat[Terminal rewards\label{fig:gcg_insights_extended_t}]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_rewards.pdf}} &
    \subfloat[Cross entropies of gradient calculation\label{fig:gcg_insights_extended_ce}]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_ces.pdf}} &
    \vbox{\hbox{\includegraphics[width=0.175\linewidth]{assets/insights/insights_legend.pdf}}\vspace{5pt}}
    \hfill\\
    \subfloat[Intermediate reward\label{fig:gcg_insights_extended_s}]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_additional_rewards_22.pdf}} &
    \subfloat[Intermediate reward\label{fig:gcg_insights_extended_m}]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_additional_rewards_40.pdf}} &
    \subfloat[Intermediate reward\label{fig:gcg_insights_extended_l}]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_additional_rewards_80.pdf}} \\
    \subfloat[Cross entropies of \(\vy_{\text{seed}}\) for mutations\label{fig:gcg_insights_hist_ce_bsln}]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_hist_bsln.pdf}} &
    \subfloat[Cross entropies of \(\vy_{\text{greedy}}\) for mutations]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_hist_0.pdf}} &
    \subfloat[Cross entropies of \(\vy_{\text{harmful}}\) for mutations\label{fig:gcg_insights_hist_ce_harm}]{\includegraphics[width=0.325\linewidth]{assets/insights/insights_400_hist_harm.pdf}} \\
    \end{tabular}
    }
    \hfill
    \subfloat[RLOO loss of gradient calculation\label{fig:gcg_insights_extended_lg}]{\includegraphics[width=0.315\linewidth]{assets/insights/insights_400_reinforce_loss.pdf}} \hspace{2pt}
    \subfloat[RLOO loss for mutations\label{fig:gcg_insights_extended_lm}]{\includegraphics[width=0.315\linewidth]{assets/insights/insights_400_hist_reinforce_loss.pdf}} 
    \hfill \\
    \caption{Exemplary attack on Llama 3 8B (first 50 steps). This figure extends \autoref{fig:gcg_insights} with the intermediate rewards (c-e) and the REINFORCE Leave-One-Out (RLOO) losses (i-j).}
    \label{fig:gcg_insights_extended}
\end{figure}


We next complement \autoref{fig:gcg_insights} and its accompanying discussion. That is, in \autoref{fig:gcg_insights_extended}, we also plot the intermediate rewards at lengths 22, 40, and 80. As hinted in the main part, here we use length 22 instead of 20 since \(\vy_{\text{affirmative}}\) is 22 tokens long. Moreover, in \autoref{fig:gcg_insights_extended_lg} \& \ref{fig:gcg_insights_extended_lm}, we also plot the REINFORCE loss.

It is an interesting phenomenon in \autoref{fig:gcg_insights_hist_ce_bsln} to \ref{fig:gcg_insights_hist_ce_harm} that most of the mutations seem to be concentrated in a region with small entropy values, while there is a consistent fraction of outliers for which the cross entropies are much larger. However, for the REINFORCE loss (\autoref{fig:gcg_insights_extended_lm}), the fluctuations are much less pronounced. Perhaps the lower variations are due to the averaging over multiple generations and the emphasis on early tokens (see ``Token-level weighting'' in \autoref{app:reinforce}).

\textbf{Intermediate rewards.} We observe that the HarmBench judge often returns zero reward for short generations (see \autoref{fig:gcg_insights_extended_s}). We only observe a reward of 0.5 for \(\vy_{\text{seed}}\) since we clamp it to the range \([0.5, 1]\). While this is intended behavior to determine attack success rates (ASRs), for a reinforcement learning objective like ours it might be beneficial to obtain better intermediate rewards. For this reason, we actually use \(\max\{\reward(\vy_{:22}), \reward(\vy_{:40}), \reward(\vy_{:80}), \reward(\vy)\}\) instead of \(\reward(\vy_{:22})\) to calculate \( \ell(\hat{\gY}, \tX)\) (\autoref{eq:loor_loss}) for the first 22 tokens. For the tokens until position 40, we use \(\max\{\reward(\vy_{:40}), \reward(\vy_{:80}), \reward(\vy)\}\) instead of \(\reward(\vy_{:40})\). And so on. See the second point in ``token-level weighting'' in \autoref{app:reinforce}.

\textbf{Consistency over multiple attack iterations.} Due to the dynamic weighting via the baselines in \autoref{eq:loor}/\autoref{eq:loor_loss}, our biased sampling strategy \(\Pm'(Y|X)\), and the single random sample, our loss \(\ell(\hat{\gY}, \tX)\) is also not strictly consistent. We opt against obtaining many random samples due to the computational cost. Moreover, biasing the sampling strategy seems key for an efficient attack (number of attack steps). We argue that for candidate selection \emph{within a single attack step}, the dynamic weighting via the baselines does not pose an additional challenge. However, the dynamic weighting may impact the consistency of \(\ell(\hat{\gY}, \tX)\) between different attack iterations since \(\hat{\gY},\) changes.
Nevertheless, we find that despite the dynamic weighting \(\ell(\hat{\gY}, \tX)\) correlates well with attack progress. However, setting the baseline in \autoref{eq:baseline} to zero might improve the consistency. See also ``target metric'' in \autoref{app:reinforce}.

\clearpage
\section{Example REINFORCE-GCG Attacks and Generations}\label{app:exmpirical_examples}

\chead{\textbf{\textcolor{red}{ATTENTION! THE REMAINING APPENDIX MAY CONTAIN UPSETTING CONTENT!}}}

We next show a randomly chosen collection of successful REINFORCE-GCG attacks and the resulting generations. To determine if an attack was successful, we use HarmBench's LLama 2 13 B judge.

\begin{longtable}{p{\textwidth}}%
    \caption{Gemma 1.1 2B. Randomly picked harmful generations. To limit the amount of harmful/upsetting content in this paper, we limit the model generations to the first 6 lines.}
    \endfirsthead
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-2b-it_some25_gcg_reinforce_2/g2_101.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-2b-it_some25_gcg_reinforce_2/g2_110.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-2b-it_some25_gcg_reinforce_2/g2_130.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-2b-it_some25_gcg_reinforce_2/g2_16.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-2b-it_some25_gcg_reinforce_2/g2_29.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-2b-it_some25_gcg_reinforce_2/g2_43.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-2b-it_some25_gcg_reinforce_2/g2_47.pdf}\\
\end{longtable}

\begin{longtable}{p{\textwidth}}%
    \caption{Gemma 1.1 7B. Randomly picked harmful generations. To limit the amount of harmful/upsetting content in this paper, we limit the model generations to the first 6 lines.}
    \endfirsthead
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-7b-it_some25_gcg_reinforce_2/g7_11.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-7b-it_some25_gcg_reinforce_2/g7_111.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-7b-it_some25_gcg_reinforce_2/g7_114.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-7b-it_some25_gcg_reinforce_2/g7_122.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-7b-it_some25_gcg_reinforce_2/g7_131.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-7b-it_some25_gcg_reinforce_2/g7_132.pdf}\\
\includegraphics[width=\linewidth]{examples/google_gemma-1.1-7b-it_some25_gcg_reinforce_2/g7_19.pdf}\\
\end{longtable}

\begin{longtable}{p{\textwidth}}%
    \caption{Llama 2 7B. Randomly picked harmful generations. To limit the amount of harmful/upsetting content in this paper, we limit the model generations to the first 6 lines.}
    \endfirsthead
\includegraphics[width=\linewidth]{examples/meta-llama_Llama-2-7b-chat-hf_some25_gcg_reinforce_1/l2_102.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Llama-2-7b-chat-hf_some25_gcg_reinforce_1/l2_106.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Llama-2-7b-chat-hf_some25_gcg_reinforce_1/l2_139.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Llama-2-7b-chat-hf_some25_gcg_reinforce_1/l2_149.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Llama-2-7b-chat-hf_some25_gcg_reinforce_1/l2_176.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Llama-2-7b-chat-hf_some25_gcg_reinforce_1/l2_191.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Llama-2-7b-chat-hf_some25_gcg_reinforce_1/l2_21.pdf}\\
\end{longtable}

\begin{longtable}{p{\textwidth}}%
    \caption{Llama 3 8B. Randomly picked harmful generations. To limit the amount of harmful/upsetting content in this paper, we limit the model generations to the first 6 lines.\label{tab:examples_llama3}}
    \endfirsthead
\includegraphics[width=\linewidth]{examples/meta-llama_Meta-Llama-3-8B-Instruct_some25_gcg_reinforce_1/l3_149.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Meta-Llama-3-8B-Instruct_some25_gcg_reinforce_1/l3_17.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Meta-Llama-3-8B-Instruct_some25_gcg_reinforce_1/l3_179.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Meta-Llama-3-8B-Instruct_some25_gcg_reinforce_1/l3_184.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Meta-Llama-3-8B-Instruct_some25_gcg_reinforce_1/l3_186.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Meta-Llama-3-8B-Instruct_some25_gcg_reinforce_1/l3_36.pdf}\\
\includegraphics[width=\linewidth]{examples/meta-llama_Meta-Llama-3-8B-Instruct_some25_gcg_reinforce_1/l3_37.pdf}\\
\end{longtable}

\begin{longtable}{p{\textwidth}}%
    \caption{Vicuna 1.5 7B. Randomly picked harmful generations. To limit the amount of harmful/upsetting content in this paper, we limit the model generations to the first 6 lines.}
    \endfirsthead
\includegraphics[width=\linewidth]{examples/lmsys_vicuna-7b-v1.5_some25_gcg_reinforce_2/v_162.pdf}\\
\includegraphics[width=\linewidth]{examples/lmsys_vicuna-7b-v1.5_some25_gcg_reinforce_2/v_181.pdf}\\
\includegraphics[width=\linewidth]{examples/lmsys_vicuna-7b-v1.5_some25_gcg_reinforce_2/v_26.pdf}\\
\includegraphics[width=\linewidth]{examples/lmsys_vicuna-7b-v1.5_some25_gcg_reinforce_2/v_36.pdf}\\
\includegraphics[width=\linewidth]{examples/lmsys_vicuna-7b-v1.5_some25_gcg_reinforce_2/v_41.pdf}\\
\includegraphics[width=\linewidth]{examples/lmsys_vicuna-7b-v1.5_some25_gcg_reinforce_2/v_43.pdf}\\
\includegraphics[width=\linewidth]{examples/lmsys_vicuna-7b-v1.5_some25_gcg_reinforce_2/v_66.pdf}\\
\end{longtable}


\section{Example REINFORCE-GCG Attacks on Llama 3 8B with Circuit Breaker}\label{app:exmpirical_circuit_breaker}


\begin{longtable}{p{\textwidth}}%
    \caption{Llama 3 8B with circuit breaker, where \(\vy_{\text{seed}} = \vy_{\text{history}}\). Randomly picked harmful generations. To limit the amount of harmful/upsetting content in this paper, we limit the model generations to the first 6 lines.}
    \endfirsthead
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/111.pdf}\\
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/117.pdf}\\
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/12.pdf}\\
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/121.pdf}\\
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/136.pdf}\\
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/14.pdf}\\
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/144.pdf}\\
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/167.pdf}\\
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/22.pdf}\\
\includegraphics[width=\linewidth]{examples/GraySwanAI_Llama-3-8B-Instruct-RR_some25_gcg_reinforce_alttargets_2/6.pdf}\\
\end{longtable}

\end{document}


