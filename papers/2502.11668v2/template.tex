\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage[final]{graphicx}
\usepackage[numbers]{natbib}
\usepackage{doi}
\usepackage{multirow}
\usepackage{float}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage[table]{xcolor}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand\linesubsec[1]{\vspace{0.8mm}\noindent\textbf{#1 --- }}
\newcommand{\lesstiny}{\fontsize{7pt}{8pt}\selectfont}



\title{NablAFx: A Framework for Differentiable Black-box and Gray-box Modeling of Audio Effects}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
    Marco Comunità\\
    \texttt{m.comunita@qmul.ac.uk}
    \AND Christian J. Steinmetz \And Joshua D. Reiss
    \AND \\
    Centre for Digital Music\\
    Queen Mary University of London, UK\\
}

% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
We present NablAFx, an open-source framework developed to support research in differentiable black-box and gray-box modeling of audio effects. 
Built in PyTorch, NablAFx offers a versatile ecosystem to configure, train, evaluate, and compare various architectural approaches. 
It includes classes to manage model architectures, datasets, and training, along with features to compute and log losses, metrics and media, and plotting functions to facilitate detailed analysis. 
It incorporates implementations of established black-box architectures and conditioning methods, as well as differentiable DSP blocks and controllers, enabling the creation of both parametric and non-parametric gray-box signal chains. The code is accessible at \small{\url{https://github.com/mcomunita/nablafx}}.
\end{abstract}

\keywords{Audio Effects Modeling \and Black-box Modeling \and Gray-box Modeling \and Neural Networks \and Differentiable DSP}

% \vspace{-1mm}
\section{Introduction}
% \vspace{-1mm}
\label{intro}
Audio effects are central for engineers and musicians to shape timbre, dynamics, and spatialisation of sound~\cite{wilmering2020history}.
% For some instruments (e.g., electric guitar) the processing chain is part of the artist’s creative expression \cite{case2010recording} 
% and music genres and styles are also defined by the type of effects adopted, 
% \cite{blair2015southern, williams2012tubby}
% with renowned musicians adopting specific combinations to achieve a unique sound \cite{prown2003gear}.
Therefore, research related to audio effects, especially with the success of deep learning 
% \citep{bengio2017deep} 
and differentiable digital signal processing (DDSP) \citep{engel2020ddsp}, is a very active field \citep{comunita2024afxresearch}. 
This includes applications such as classification and identification 
\citep{
comunita2021guitar},
parameters estimation 
\citep{
colonel2022direct, 
mitcheltree2023modulation}, 
modeling 
\citep{
% martinez2020deep, 
comunita2023modelling,
simionato2024comparative},
% and removal \citep{imort2022distortion, rice2023general}, 
% processing graph estimation \citep{lee2023blind, lee2024searching} 
style transfer \citep{
steinmetz2022style, 
% vanka2024diff, 
steinmetz2024st}, 
automatic mixing 
\citep{
steinmetz2021automatic, 
% martinez2022automatic, 
sai2023adoption}.
% or reverse engineering of a mix 
% \citep{
% colonel2023music,
% colonel2022reverse}.
Audio effects modeling is one of the most active applications of differentiable approaches, 
% since audio effects are extensively used at every stage of audio and music production, 
with the majority of methods falling into black-box (i.e., neural networks) and gray-box (i.e., DDSP) paradigms. 
While black-box models achieve state-of-the-art accuracy 
\citep{
wright2019real, 
% martinez2020deep, 
steinmetz2022efficient, 
comunita2023modelling, 
yeh2024hyper} 
there is interest in gray-box ones 
\citep{
colonel2022reverse,
wright2022grey, 
carson2023differentiable, 
miklanek2023neural,
yeh2024ddsp} 
due to interpretability and potential for efficiency.

Comparing modeling paradigms remains challenging due to significant variations in training and evaluation methods. 
In addition, the lack of standardized implementations for models and DDSP blocks further impedes reproducibility and performance assessment.
There are a growing number of audio effect implementations available to researchers, however existing options remain limited in a number of ways (see Table~\ref{tab:frameworks}).

While the \texttt{Spotify Pedalboard}\footnote{\lesstiny{\url{github.com/spotify/pedalboard}}} library offers Python implementations of common audio effects and allows to define signal chains, these are not differentiable. 
% In \citep{engel2020ddsp}, the concept of DDSP is introduced, and implementations of some differentiable blocks are available\footnote{\url{https://github.com/magenta/ddsp}}, even if such blocks are not common and easily re-usable. 
DDSP, introduced in \citep{engel2020ddsp}, provides some differentiable blocks\footnote{\lesstiny{\url{github.com/magenta/ddsp}}}, though they are neither common nor easily reusable, since they are focused on specific applications within audio synthesis.
\texttt{dasp}\footnote{\lesstiny{\url{github.com/csteinmetz1/dasp-pytorch}}}\citep{steinmetz2022style} includes differentiable implementations of common processing and mixing blocks, and while useful when imported into larger projects, the library in not meant to define signal chains.
Interconnections of processors can be defined in \texttt{diffmoog}\footnote{\lesstiny{\url{github.com/aisynth/diffmoog}}}\citep{uzrad2024diffmoog}, although mainly focused on FM synthesis and not suitable for effects modeling.
% Also \texttt{GRAFX}\footnote{\url{https://github.com/sh-lee97/grafx}} \citep{lee2024grafx} allows to define complex interconnections of processing blocks, and while the internal parameters of each block are learnable, they cannot be controlled from an external source, preventing parametric, time-varying or (non-)periodically modulated signal chains, an important aspect for effects modeling.
Also \texttt{GRAFX}\footnote{\lesstiny{\url{github.com/sh-lee97/grafx}}}~\citep{lee2024grafx} enables complex interconnections, but lacks external control, limiting parametric, time-varying, and modulated signal chains for effects modeling.

\texttt{pyneuralfx}\footnote{\lesstiny{\url{github.com/ytsrt66589/pyneuralfx}}}\citep{yeh2024pyneuralfx} is the only framework designed for modeling and, while it includes state-of-the-art neural networks, it focuses only on black-box approaches and does not include time-varying models \citep{comunita2023modelling}. 
Even though it provides functions for inference-time analysis, it lacks logging and plotting features during training and testing. 
Also, experiment configurations are hard to modularize and adapt to different datasets, models, or training procedures, limiting repeatability and comparison.

To address these limitations and advance differentiable audio effects modeling, we propose \textbf{NablAFx}, which provides:
\begin{itemize} 
    \item Black-box architectures, gray-box processors, and controllers for parametric/non-parametric models.
    \item Modules to manage datasets, training, and loss functions.
    \item Tools to log metrics and media during training and testing.
    \item Plotting functions for analysis throughout training.
\end{itemize}

\input{TABLES/frameworks}

% ===
% \vspace{-1mm}
\section{Framework}
\label{sec:framework}
% \vspace{-1mm}

NablAFx is a framework for audio effects modeling that allows researchers to easily define, train, evaluate and compare differentiable black-box and gray-box models. 
As shown in Fig.~\ref{fig:framework}, it integrates models, datasets, trainers, loss functions, metrics, and logging/plotting tools. 
Built with PyTorch Lightning\footnote{\lesstiny{\url{lightning.ai/pytorch-lightning}}}, it leverages Weights\&Biases\footnote{\lesstiny{\url{wandb.ai/site}}} to log results and media.
% \\\linesubsec{Nomenclature}
% Throughout this paper and the code, we define a nomenclature for audio effects modeling systems. 
% A \textit{processor} processes signals, while a \textit{controller} generates signals to control processors.
% \textit{Controls} are tensors (normalized to [0,1]) representing user controls (e.g., knobs/sliders/switches).
% \textit{Control parameters} are tensors on [0,1] that map controls to a processor's internal parameters.
% \textit{Parameters} are tensors ($[-\infty,+\infty]$) that determine a processor's internal coefficients.
% \textit{Coefficients} are the internal real-/complex-valued coefficients of a differentiable processor or the weights of a neural network.
% For example, a \textit{Resonance} control ([1,10]) might set a \textit{Q} control parameter ([0,1]) of a specific section in a \textit{Parametric EQ} processor, which in turn might set the \textit{Q} parameter ([0.1,100]) of a specific biquad filter in that processor, determining the \textit{B} and \textit{A} coefficients ([-1,1]) of that same biquad.

\begin{figure*}[t]
    % \vspace{-5mm}
    \centering
    \includegraphics[width=1\linewidth]{FIGURES/Framework.pdf}
    % \vspace{-2mm}
    \caption{Overview of the NablAFx framework for audio effects modeling}
    \label{fig:framework}
    % \vspace{-5mm}
\end{figure*}

\begin{figure*}[t]
% \vspace{-5mm}
\centering
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[height=3.7cm]{FIGURES/Example_EQBlock_Response.png}
    \caption{Parametric EQ: frequency response}
    \label{fig:ex_eqblock_response}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[height=3.7cm]{FIGURES/Example_NonlinBlock_Response.png}
    \caption{Nonlinearity: amplitude response}
    \label{fig:ex_nonlinblock_response}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
    \centering
    \includegraphics[height=3.7cm]{FIGURES/Example_DCBlock_Response.png}
    \caption{DC offset: time response}
    \label{fig:ex_dcblock_response}
\end{subfigure}
% \vspace{-2mm}
\caption{Examples of plotting features included in NablAFx}
% \vspace{-5mm}
\end{figure*}

\linesubsec{System}
In NablAFx all necessary functionalities are contained in an audio effects modeling system class. 
% The \textit{BaseSystem} class is at the top of the hierarchy and it is used to define and initialize loss function(s), optimizers, learning rate scheduler and metrics. 
% It also includes all shared methods to compute and log loss, metrics, audio and frequency/phase response. 
The \textit{BaseSystem} class handles the initialization of loss functions, optimizers, learning rate scheduler, metrics, and includes shared methods to compute and log loss, metrics, audio and frequency/phase response.
% The \textit{BaseSystem} is than split into \textit{BlackBoxSystem} and \textit{GrayBoxSystem} subclasses which define and initialize the respective black-box or gray-box model. 
% They also implement train, validation and test steps for the two types of system. 
% The \textit{GrayBoxSystem} class also extends \textit{BaseSystem} with methods to log the audio output and to plot and log the frequency/time response and the parameters values for each stage of the gray-box signal processing chain. 
% Both \textit{BlackBoxSystem} and \textit{GrayBoxSystem} are extended with classes (\textit{BlackBoxSystemWithTBPTT}, \textit{GrayBoxSystemWithTBPTT}) that implement truncated back-propagation through time, a training paradigm often used to speed up training of recurrent networks \citep{wright2019real}.
The \textit{BaseSystem} is divided into \textit{BlackBoxSystem} and \textit{GrayBoxSystem}, which initialize black-box and gray-box models, respectively, and implement train, validation, and test steps. 
The \textit{GrayBoxSystem} adds methods to log audio output, plot/log frequency and time responses, and parameters values for each stage of the signal chain. 
Both systems are extended with \textit{WithTBPTT} classes, which implement truncated backpropagation through time to enable faster training of recurrent networks\citep{wright2019real}.

\linesubsec{Model}
% Depending on the system class, the model to be trained is either a black-box or a gray-box one.
% In NablAFx, a black-box model can be any type of neural network architecture of any complexity, and the output is a function of the input and controls: $y = f(x,c)$.
% The \textit{BlackBoxModel} class includes a \textit{Processor} class, which represents any one of the neural networks being trained.
% On the other hand, a gray-box model is defined as an interconnection of differentiable processing blocks, each one limited to a specific processing function, defining such a model as a composition of functions: $y = (f_{1} \circ f_{2} \circ \ldots \circ f_{N})(x,c)$.
In our framework, black-box models can be any neural network - with outputs defined as a function of input and controls $y = f(x,c)$ -  represented by the \textit{Processor} class in \textit{BlackBoxModel}. 
Gray-box models comprise interconnected differentiable blocks, forming a function composition: $y = (f_{1} \circ f_{2} \circ \ldots \circ f_{N})(x,c)$, and the \textit{Processor} class defines a chain of processors.
A \textit{Controller} class defines a chain of controllers, each associated with a processor, allowing the definition of parametric and time-varying models that are a function of both input audio and controls.
% In our framework, the \textit{GrayBoxModel} class includes a \textit{Processor} class which defines a chain of processors, and a \textit{Controller} class which defines a chain of controllers, each associated to one processor, allowing to define parametric and time-varying gray-box models that are a function of both input signal and controls.

\linesubsec{Data}
The \textit{DataModule} class takes care of initializing the dataset and dataloaders for train, validation and testing. \textit{AudioEffectDataset} and \textit{ParametricAudioEffectDataset} classes are used to manage data for non-parametric and parametric models. 

\linesubsec{Metrics}
% To compute the metrics we rely on several packages: \textit{torchmetrics}\footnote{\url{https://lightning.ai/docs/torchmetrics/stable/}} for mean absolute error (MAE), mean squared error (MSE) and mean absolute percentage error (MAPE); \textit{auraloss}\footnote{\url{https://github.com/csteinmetz1/auraloss}}\citep{steinmetz2020auraloss} for error-to-signal ratio (ESR) \citep{wright2020perceptual} and DC loss; while the Frechét Audio Distance (FAD) \citep{kilgour2018fr} is computed thanks to the \textit{frechet-audio-distance}\footnote{\url{https://github.com/gudgud96/frechet-audio-distance}} package. 
Metrics are computed with: 
\textit{torchmetrics}\footnote{\lesstiny{\url{lightning.ai/docs/torchmetrics/stable/}}} for mean absolute error (MAE), mean squared error (MSE) and mean absolute percentage error (MAPE); \textit{auraloss}\footnote{\lesstiny{\url{github.com/csteinmetz1/auraloss}}}\citep{steinmetz2020auraloss} for error-to-signal ratio (ESR) 
% \citep{wright2020perceptual} 
and DC loss; and the 
\textit{frechet-audio-distance}\footnote{\lesstiny{\url{github.com/gudgud96/frechet-audio-distance}}} package for Frechét Audio Distance (FAD) \citep{kilgour2018fr}.

\linesubsec{Plotting}
% Beside methods to log losses, metrics and audio examples, we define methods to plot and log frequency, phase and time response for the whole system, as well as frequency or time response and parameters values for each DDSP block in a gray-box system.
In addition to logging losses, metrics and audio examples, we provide methods to plot and log frequency/phase response for the whole system, as well as frequency/time response and parameters values for each DDSP block in a gray-box system.
% We define two methods to compute frequency/phase response. The first is based on measuring the system's response to an exponential sine sweep\footnote{\url{https://ant-novak.com/pages/sss/}} \citep{novak2009nonlinear}, which is only suitable for linear and slightly nonlinear systems.
% The second method is our own implementation and much more suitable for nonlinear systems, regardless of the amount of distortion. 
We offer two methods to compute the frequency and phase response: one using an exponential sine sweep\footnote{\lesstiny{\url{ant-novak.com/pages/sss/}}} \citep{novak2009nonlinear}, suitable for linear and mildly nonlinear systems, and a custom method designed for nonlinear systems.
The latter measures the system's response in steps, using sinusoidal inputs at exponentially spaced frequencies.
% This is based on measuring the system's response in steps, using sinusoidal inputs at different frequencies spaced exponentially along the frequency axis. 
% To obtain a reliable measurement each sinusoid is several seconds long - to allow the system to reach a steady state - and the magnitude/phase response is only computed on the very last segment of the signal:
To ensure reliable measurements, each sinusoid lasts several seconds for the system to reach steady state, with magnitude/phase response computed only from the final segment.
\begin{align*}
    x = x[-T* \lfloor f_{s}/f_{1} \rfloor:] \\ 
    y = y[-T* \lfloor f_{s}/f_{1} \rfloor:]
\end{align*}
where $T$ is the signal duration (e.g. 5~s), $f_{s}$ is the sample rate and $f_{1}$ is the minimum frequency of the stepped sweep (e.g., 10~Hz). 
% An example of magnitude/phase response computed with our implementation for different input amplitudes ([-80~dB, 0~dB]) is shown in Fig.~\ref{fig:ex_frequency_response}, and it can be noticed how the plots are completely free of the noise and distortions typical of the exponential sine sweep method when applied to nonlinear systems. 
% In Fig.~\ref{fig:ex_eqblock_response} we show an example of frequency response and parameters values of a Parametric EQ block; while in Fig.~\ref{fig:ex_nonlinblock_response} and \ref{fig:ex_dcblock_response} we show an example of, respectively, learned nonlinearity compared to a $tanh$ (light gray) and time-varying DC offset as a function of the input signal (light gray).
Fig.~\ref{fig:ex_eqblock_response} shows the frequency response of a Parametric EQ block, while Fig.~\ref{fig:ex_nonlinblock_response} and \ref{fig:ex_dcblock_response} display examples of learned nonlinearity (vs. $tanh$, light gray) and time-varying DC offset (vs. input signal, light gray).

\begin{figure*}[t]
% \vspace{-5mm}
\centering
\begin{subfigure}{.125\textwidth}
  \centering
  \includegraphics[width=2cm]{FIGURES/LSTM-block-diagram.pdf}
  \caption{LSTM}
  \label{fig:lstm-block-diag}
\end{subfigure}
\begin{subfigure}{.195\textwidth}
  \centering
  \includegraphics[width=2.5cm]{FIGURES/TCN-block-diagram.pdf}
  \caption{TCN}
  \label{fig:tcn-block-diag}
\end{subfigure}
\begin{subfigure}{.195\textwidth}
  \centering
  \includegraphics[width=3cm]{FIGURES/TCNBlock-block-diagram.pdf}
  \caption{TCN block}
  \label{fig:tcnblock-block-diag}
\end{subfigure}
\begin{subfigure}{.265\textwidth}
  \centering
  \includegraphics[width=3.8cm]{FIGURES/GCN-block-diagram.pdf}
  \caption{GCN}
  \label{fig:gcn-block-diag}
\end{subfigure}
\begin{subfigure}{.195\textwidth}
  \centering
  \includegraphics[width=3.05cm]{FIGURES/GCNBlock-block-diagram.pdf}
  \caption{GCN block}
  \label{fig:gcnblock-block-diag}
\end{subfigure}
% \vspace{-7mm}
\caption{Black-box architectures included in NablAFx}
\label{fig:architectures1}
% \vspace{-5mm}
\end{figure*}

% ===
% \vspace{-1mm}
\subsection{Differentiable Black-box Models}
% \vspace{-1mm}
\label{sec:bb-models}
% In this section we describe the architecture of each of the neural network and conditioning methods included in the framework, which represent the state-of-the-art for audio effects modeling.
This section provides an overview of state-of-the-art neural network architectures and conditioning methods included in NablAFx.

\linesubsec{LSTM}
% The recurrent neural network architecture considered here has been widely adopted in previous works on nonlinear effects like overdrive, distortion and guitar amps \citep{wright2019real, wright2020real}, nonlinear time-varying effects like fuzz and compressor \citep{steinmetz2022efficient, comunita2023modelling}, and modulation effects like phaser and flanger \cite{wright2021neural, mitcheltree2023modulation}.
% As shown in Fig.~\ref{fig:lstm-block-diag}, the architecture is very simple and is made of a single LSTM layer, followed by a linear layer and a $tanh$ activation.
% For parametric models we include a conditioning block that takes as inputs the controls values and optionally, the input sequence itself.
The recurrent neural network architecture we implement is widely adopted for nonlinear effects (e.g., overdrive, distortion, guitar amps) \citep{wright2019real, wright2020real}, nonlinear time-varying effects (e.g., fuzz, compressor) \citep{steinmetz2022efficient, comunita2023modelling}, and modulation effects (e.g., phaser, flanger) \citep{wright2021neural, mitcheltree2023modulation}. 
As shown in Fig.~\ref{fig:lstm-block-diag}, it consists of a single LSTM layer, a linear layer, and a $tanh$ activation. 
For parametric models, a conditioning block processes control values and optionally the input sequence.

\linesubsec{TCN}
% Temporal convolution networks (TCN) were introduced in \citep{lea2016temporal} as a general sequence modeling architecture and shown to outperform recurrent architectures on a variety of tasks \citep{bai2018empirical}.
% Based on these observations TCNs were proposed in \citep{steinmetz2020learning, stein2010automatic, steinmetz2022efficient} for audio effects modeling and applied to linear (EQ and reverb) and nonlinear time-varying (compressor) effects. 
Temporal Convolution Networks (TCNs), introduced in \citep{lea2016temporal} and shown to outperform recurrent architectures \citep{bai2018empirical} on a variety of tasks, were proposed for audio effects modeling \citep{steinmetz2020learning, stein2010automatic, steinmetz2022efficient} and applied to linear (EQ, reverb) and nonlinear time-varying (compressor) effects.
The architecture (Fig.~\ref{fig:tcn-block-diag}) consists of a series of residual blocks (Fig.~\ref{fig:tcnblock-block-diag}) made of 1-dimensional convolutions with increasing dilation factors, optionally followed by batch normalization and conditioning block, and an activation function (here $tanh$). 
% A linear layer reduces the number of output channels to the same number as the input.
A linear layer matches the output channels to the input size.

\linesubsec{GCN}
% Gated convolution networks (GCN), introduced in \citep{rethage2018wavenet} as a feed-forward WaveNet \citep{van2016wavenet} architecture, are a special case of TCN that uses gated convolutions.
% GCNs were adopted in \citep{damskagg2019deep, damskagg2019real, wright2020real} for non-linear audio effects modeling (guitar amplifier, overdrive and distortion) and in \citep{comunita2023modelling} for non-linear time-varying effects (compressor and fuzz).
Gated Convolution Networks (GCNs), introduced in \citep{rethage2018wavenet} as a feed-forward WaveNet, 
% \citep{van2016wavenet} 
are a special case of TCNs with gated convolutions. 
GCNs have been used in 
\citep{
% damskagg2019deep, 
damskagg2019real, 
wright2020real} 
for nonlinear audio effects (guitar amp, overdrive, distortion) and in \citep{comunita2023modelling} for nonlinear time-varying effects (compressor, fuzz).
% A GCN (Fig.~\ref{fig:gcn-block-diag}) consists of residual blocks (Fig.~\ref{fig:gcnblock-block-diag}) composed of 1-d convolutions with increasing dilation factors, optionally followed by batch normalization and conditioning block, and a gated activation function.
% Beside the activation function a GCN architecture differs from a TCN because the output is obtained as a linear combination of the activation features at each block.
Beside the activation function at each block (Fig.\ref{fig:gcnblock-block-diag}), a GCN (Fig.\ref{fig:gcn-block-diag}) differs from a TCN in that its output is a linear combination of the activation features at each block.

\linesubsec{S4}
Structured state space sequence models (S4) were introduced in \citep{gu2021efficiently} as a general sequence modeling architecture and shown to outperform recurrent, convolutional and Transformer architectures on a variety of tasks. 
% An S4 neural network layer is a differentiable implementation of an infinite impulse response system (IIR) in state-space form. 
% Similarly to recurrent networks, implementing an IIR system, S4 models have - theoretically - and arbitrary long receptive field.
An S4 layer is a differentiable implementation of an infinite impulse response (IIR) system in state-space form, with a theoretically infinite receptive field, similar to recurrent networks.
Based on these observations state-space models were adopted for non-linear time-varying (compressor) effects modeling~\citep{yin2024modeling, simionato2024modeling}.
% The architecture implemented in our framework is based on \citep{yin2024modeling} (see Fig.~\ref{fig:s4-block-diag}) and consists of a series of S4 blocks. 

The architecture in our framework, based on \citep{yin2024modeling} (Fig.~\ref{fig:s4-block-diag}), consists of S4 blocks.
Unlike standard convolutional ones, S4 layers are not combined or mixed across data channels, this explains the use of a linear layer and activation function ($\tanh$) at the input of each S4 block (Fig.~\ref{fig:s4block-block-diag}) for affine transformations along the channel dimension. 
These are followed by an S4D layer \citep{gupta2022diagonal}, which uses diagonal matrices for a parameter-efficient implementation, optional batch normalization and conditioning block, followed by an activation function ($tanh$ in this case).
% Linear layers are also used as first and last layers to increase/reduce the number of channels from/to the same number as the input data.
Linear layers are used at the start and end to adjust the channel count to match the input data.

\begin{figure*}[t]
% \vspace{-5mm}
\centering
\begin{subfigure}{.245\textwidth}
  \centering
  \includegraphics[width=2.6cm]{FIGURES/S4-block-diagram.pdf}
  \caption{S4}
  \label{fig:s4-block-diag}
\end{subfigure}
\begin{subfigure}{.245\textwidth}
  \centering
  \includegraphics[width=3.2cm]{FIGURES/S4Block-block-diagram.pdf}
  \caption{S4 block}
  \label{fig:s4block-block-diag}
\end{subfigure}
\begin{subfigure}{.245\textwidth}
  \centering
  \includegraphics[width=3cm]{FIGURES/FiLMctrl-block-diagram.pdf}
  \caption{FiLM controller}
  \label{fig:film_ctrl}
\end{subfigure}
\begin{subfigure}{.245\textwidth}
  \centering
  \includegraphics[width=4cm]{FIGURES/FiLMmod-block-diagram.pdf}
  \caption{FiLM modulator}
  \label{fig:film_mod}
\end{subfigure}
% \vspace{-7mm}
\caption{Black-box architectures and conditioning methods included in NablAFx}
\label{fig:architectures2}
% \vspace{-5mm}
\end{figure*}

% \vspace{-1mm}
\subsubsection{Conditioning for Black-box Models}
% \vspace{-1mm}
\label{sec:cond-bb-models}
Conditioning mechanisms for black-box models have been explored for different purposes: to include parametric control \citep{
steinmetz2022efficient, 
% yeh2024hyper, 
simionato2024comparative, 
yin2024modeling}, 
to capture long-range dependencies \citep{comunita2023modelling} or for modulation in LFO-driven effects \citep{mitcheltree2023modulation, wright2021neural}.
While concatenation and feature-wise linear modulation (FiLM) \citep{
steinmetz2022efficient, 
yin2024modeling, 
% yeh2024hyper, 
mitcheltree2023modulation, 
simionato2024comparative} 
remain the most common methods, 
temporal FiLM (TFiLM) has been adopted to capture time-varying behavior \citep{comunita2023modelling}.
Beside these established methods, in this work we also propose three further conditioning methods: time-varying concatenation (TVCond), tiny TFiLM (TTFiLM) and time-varying FiLM (TVFiLM), as efficient implementations of time-varying conditioning similar to TFiLM. 
% Concatenation of the control values ($\mathbf{c}$) to the input sequence ($\mathbf{x_{n}}$) along the channel dimension is a common conditioning method because of its straightforward implementation and parameter efficiency. 
% It is considered the baseline conditioning strategy for recurrent networks and it has been evaluated in previous literature when applying parametric control to dynamic range compression \citep{steinmetz2022efficient} and overdrive \citep{yeh2024hyper} modeling.
Concatenating control values ($\mathbf{c}$) to the input sequence ($\mathbf{x_{n}}$) along the channel dimension is a simple, parameter-efficient conditioning method. 
It serves as a baseline for recurrent networks and has been used for parametric control in, e.g., compression~\citep{steinmetz2022efficient} and overdrive~\citep{yeh2024hyper}.

Equally common is FiLM conditioning, mainly when using TCN \citep{steinmetz2022efficient, yeh2024hyper}, GCN \citep{yeh2024hyper} or S4 \citep{yin2024modeling, simionato2024modeling} backbones, with works adopting it for compressors \citep{steinmetz2022efficient, simionato2024modeling, simionato2024comparative} and overdrive \citep{yeh2024hyper, simionato2024comparative} modeling.
% Introduced in \citep{perez2018film} as a general-purpose conditioning method, FiLM operates on the intermediate features of a neural network as a function of conditioning signals. 
% Given a conditioning vector $\mathbf{c}$, FiLM learns two functions $f$ and $g$, which are used to map the conditioning vector to a set of scaling $\gamma_{k,c}=f(\mathbf{c})$ and bias $\beta_{k,c}=g(\mathbf{c})$ parameters for each layer $k$ and channel $c$ of the network. 
Introduced in \citep{perez2018film} as a general-purpose conditioning method, FiLM modulates a neural network's intermediate features using a conditioning vector $\mathbf{c}$. It learns functions $f$ and $g$ to generate scaling ($\gamma_{k,c}=f(\mathbf{c})$) and bias ($\beta_{k,c}=g(\mathbf{c})$) parameters for each layer $k$ and channel $c$, which are used to modulate the activations at each layer $\mathbf{h}_{k,c}$, via a feature-wise affine transformation:
\begin{equation}
    \text{FiLM}(\mathbf{h}_{k,c},\gamma_{k,c},\beta_{k,c})=\gamma_{k,c}\cdot\mathbf{h}_{k,c}+\beta_{k,c}.
\end{equation}
% In practice, $f$ and $g$ are implemented as a neural network (see Fig.~\ref{fig:film_ctrl}) which learns a latent representation $\mathbf{z}$ of the conditioning vector $\mathbf{c}$ - the control values in our case. 
% Using a linear layer, the latent representation is then used to generate scaling and bias parameters at each block of the main neural network (see Fig.~\ref{fig:film_mod}).
In practice, $f$ and $g$ are neural networks (Fig.~\ref{fig:film_ctrl}) that learn a latent representation $\mathbf{z}$ of the conditioning vector $\mathbf{c}$; then, a linear layer uses the latent representation to generate scaling and bias parameters for each block of the main network (Fig.~\ref{fig:film_mod}).

% TFiLM, introduced in \citep{birnbaum2019temporal}, extends the expressivity of the network by leveraging long-range dependencies in the conditioning signal to vary the modulation of intermediate features across time. 
% Using recurrent networks, TFiLM layers modulate the intermediate features of a model over time as a function of the activations at each layer $\mathbf{h}_{k}$ and optionally, a conditioning vector $\mathbf{c}$ (see Fig.~\ref{fig:tfilm}).
TFiLM \citep{birnbaum2019temporal} enhances network expressivity by using recurrent networks to modulate intermediate features over time as a function of layer activations $\mathbf{h}_{k}$ and optionally a conditioning vector $\mathbf{c}$ (Fig.~\ref{fig:tfilm}).
Given a sequence of activations $\mathbf{h}_k$ from the $k$-th block of a network, the sequence is split into $T$ blocks of $B$ samples $\mathbf{h}_{k,b_{1}:b_{T}}$ along the sequence dimension. 
For each block $\mathbf{h}_{k,b_{t}}$, 1-dimensional max pooling downsamples the signal by a factor of $B$. 
% To also include the conditioning vector $\mathbf{c}$ we simply repeat it $T$ times and concatenate it to the downsampled activations. 
To include the conditioning vector $\mathbf{c}$, it is repeated $T$ times and concatenated with the downsampled activations.
% Then an LSTM generates a sequence of scaling $\gamma_{k,b_{1}:b_{T},c}$ and bias $\beta_{k,b_{1}:b_{T},c}$ parameters for each channel $c$. 
% These scaling and bias parameters are then used to modulate each channel of the activations individually in each block by an affine transformation:
Then, an LSTM generates scaling $\gamma_{k,b_{1}:b_{T},c}$ and bias $\beta_{k,b_{1}:b_{T},c}$ parameters for each channel $c$, which are used to modulate the activations in each block via an affine transformation:
\begin{equation*}
    \begin{split}
        \text{TFiLM}(\mathbf{h}_{k,b_{1}:b_{T},c},\gamma_{k,b_{1}:b_{T},c},\beta_{k,b_{1}:b_{T},c})=\\
        \gamma_{k,b_{1}:b_{T},c}\cdot\mathbf{h}_{k,b_{1}:b_{T},c}+\beta_{k,b_{1}:b_{T},c}.
    \end{split}
\end{equation*}
% In its standard formulation, TFiLM conditioning requires to add a recurrent network for each block in the main neural network. 
% Since the main network might include many blocks - [5,10] being a typical range - and since the modulation parameters are separate for each channel $c$ - [16-32] being a typical range - TFiLM conditioning can significantly increase the number of parameters with respect to the backbone network. 
% To retain the expressivity of TFiLM conditioning while reducing the number of parameters and computational cost to values similar to FiLM conditioning 
% we propose two further methods: TTFiLM and TVFiLM.
In its standard formulation, TFiLM conditioning adds a recurrent network for each block in the main neural network, which can lead to a significant increase in parameters due to the number of blocks (typically 5-10) and channels (typically 16-32). 

To retain TFiLM's expressivity while reducing parameters and computational cost, we propose two methods: TTFiLM and TVFiLM.
% TTFiLM (Fig.\ref{fig:ttfilm}) is structurally similar to TFiLM. 
% The computational complexity is reduced by using a small number of channels for the recurrent network; this is obtained thanks to a linear layer before the recurrent network. 
% The output is then scaled up to the right number of scaling $\gamma_{k,b_{1}:b_{T},c}$ and bias $\beta_{k,b_{1}:b_{T},c}$ channels using a small MLP.
TTFiLM (Fig.\ref{fig:ttfilm}) is structurally similar to TFiLM, and reduces the computational complexity by using fewer channels in the recurrent network, achieved through a linear layer before it. The output is then scaled up to the required number of scaling $\gamma_{k,b_{1}:b_{T},c}$ and bias $\beta_{k,b_{1}:b_{T},c}$ channels using a small MLP.
TVFiLM is a time-varying extension of FiLM conditioning. 
% This is obtained by replacing the MLP in the FiLM controller (Fig.~\ref{fig:film_ctrl}) with a recurrent network (Fig.~\ref{fig:tvfilm_ctrl}), allowing to obtain a time dependent latent representation $\mathbf{z}_{n,b_{1}:b_{T}}$ that is shared across the main network's blocks.
% The modulation sequences are then obtained at each block by a simple linear layer (Fig.\ref{fig:tvfilm_mod}), similarly to a standard FiLM conditioning (Fig.\ref{fig:film_mod}).
It replaces the MLP in the FiLM controller (Fig.~\ref{fig:film_ctrl}) with a recurrent network (Fig.~\ref{fig:tvfilm_ctrl}), creating a time-dependent latent representation $\mathbf{z}_{n,b{1}:b_{T}}$ shared across the main network's blocks. 
Modulation sequences are then generated at each block via a linear layer (Fig.\ref{fig:tvfilm_mod}), similarly to standard FiLM (Fig.\ref{fig:film_mod}).

% Furthermore, we implement time-varying concatenation (TVCond) conditioning for recurrent models. 
% This is achieved by simply using a TVFiLM controller (Fig.\ref{fig:tvfilm_ctrl}) to obtain a time-dependent conditioning sequence that we concatenate to the input signal, allowing to increase the expressivity with respect to a standard concatenation of the controls vector $\mathbf{c}$.
We also implement time-varying concatenation (TVCond) for recurrent models by using a TVFiLM controller to generate a time-dependent conditioning sequence, which is concatenated to the input for greater expressivity compared to standard concatenation.

% ===
% \vspace{-1mm}
\subsection{Differentiable Gray-box Models}
% \vspace{-1mm}
\label{sec:gb-models}
As described in Sec.~\ref{sec:framework} we define a gray-box model as a sequence of differentiable processors, each with an associated controller which generates the control parameters that dictate the behavior of the processor. 
% In this section we detail the processors and controllers provided in NablAFx. 

\begin{figure*}[t]
% \vspace{-5mm}
\centering
\begin{subfigure}{.26\textwidth}
  \centering
  \includegraphics[height=5.5cm]{FIGURES/TFiLM-block-diagram.pdf}
  \caption{TFiLM}
  \label{fig:tfilm}
\end{subfigure}
\begin{subfigure}{.245\textwidth}
  \centering
  \includegraphics[width=4.2cm]{FIGURES/TTFiLM-block-diagram.pdf}
  \caption{TTFiLM}
  \label{fig:ttfilm}
\end{subfigure}
\begin{subfigure}{.235\textwidth}
  \centering
  \includegraphics[width=3.5cm]{FIGURES/TVFiLMctrl-block-diagram.pdf}
  \caption{TVFiLM controller}
  \label{fig:tvfilm_ctrl}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
  \centering
  \includegraphics[height=4.5cm]{FIGURES/TVFiLMmod-block-diagram.pdf}
  \caption{TVFiLM modulator}
  \label{fig:tvfilm_mod}
\end{subfigure}
% \vspace{-7mm}
\caption{Black-box architectures and conditioning methods included in NablAFx}
\label{fig:test}
% \vspace{-5mm}
\end{figure*}

% \vspace{-1mm}
\subsubsection{Differentiable Audio Processors}
% \vspace{-1mm}
\label{sec:ddsp-processors}
% At the time of writing we define 3 types of audio processors: basic (phase inversion, gain and dc offset), filters (parametric EQ, shelving, etc.) and nonlinearities (tanh, MLP etc.). 
% Apart from few exceptions, all processors can be controlled by any of the controllers described in Sec.~\ref{sec:ddsp-controllers}, making it possible to define processors as parametric and time-varying.
For our application, we define three types of audio processors: basic (e.g., phase inversion, gain), filters (e.g., EQ, shelving), and nonlinearities (e.g., tanh, MLP). 
Most processors can be controlled by any of the controllers in Sec.~\ref{sec:ddsp-controllers}, enabling parametric and time-varying configurations.
All filters are implemented as differentiable biquads \citep{nercessian2020neural},
which are defined by a second order difference equation:
\begin{equation*}
    \begin{split}
        y[n]=\frac{1}{a_{0}}(b_{0}x[n]+b_{1}x[n-1]+b_{2}x[n-2]+\\
        -a_{1}y[n-1]-a_{2}y[n-1])    
    \end{split}
\end{equation*}
with the transfer function:
\begin{equation*}
    H(z)=\frac{b_{0}+b_{1}z^{-1}+b_{2}z^{-2}}{a_{0}+a_{1}z^{-1}+a_{2}z^{-2}}.
\end{equation*}
% Biquad coefficients are computed as a function of center/cutoff frequency in Hz, gain in dB, and Q factor as defined in an online resource by Robert Bristow-Johnson\footnote{\url{https://www.musicdsp.org/en/latest/Filters/197-rbj-audio-eq-cookbook.html}}.
Biquad coefficients are calculated based on center/cutoff frequency (Hz), gain (dB), and Q factor, following Robert Bristow-Johnson's method\footnote{\lesstiny{\url{www.musicdsp.org/en/latest/Filters/197-rbj-audio-eq-cookbook.html}}}.
To implement \textit{N}\textsuperscript{th} order filters (e.g., Parametric EQ) we follow the common practice of using \textit{K} cascaded second order sections:
\begin{equation}\label{eq:hz}
    H(z)=\prod_{k=0}^{K}H_{k}(z)
\end{equation}
% The magnitude response of these filters can be calculated by evaluating the transfer function along the unit circle in the complex plane, and taking the magnitude:
The frequency response is computed evaluating the transfer function along the unit circle in the complex plane and taking the magnitude:
\begin{equation*}
    \left|H(e^{j\omega})\right|=\left|\prod_{k=0}^{K}H_{k}(e^{j\omega})\right|.
\end{equation*}
For efficiency, during training we adopt the frequency sampling method, which approximates a cascade of second order IIR filters by computing the frequency response as in Eq.~\ref{eq:hz}, applying the convolution in the frequency domain via multiplication and using the inverse FFT to transform the signal back to the time domain:
\begin{equation*}
    y[n] = F^{-1}[Y(z)] = F^{-1}[X(z)H(z)].
\end{equation*}
% Below we describe each processor. 
% In general, all processors' parameters can be function of the input $\mathbf{x}$ and/or the controls $\mathbf{c}$. 
% For simplicity, the notation used below does not show the processors' parameters as time dependent (e.g., $\textnormal{Gain}_\textnormal{{dB}}$ and not $\textnormal{Gain}_\textnormal{{dB}}[n]$), but with the controllers we define in Section~\ref{sec:ddsp-controllers}, these can be made to change over time. 
In the following paragraph we describe each processor. Generally, all processors' parameters can depend on the input $\mathbf{x}$ and/or controls $\mathbf{c}$. 
For simplicity, we omit time dependency in the notation (e.g., $\textnormal{Gain}_\textnormal{{dB}}$ instead of $\textnormal{Gain}_\textnormal{{dB}}[n]$), but with the controllers in Section~\ref{sec:ddsp-controllers}, these can be made to change over time.

\linesubsec{Phase Inversion} Invert the phase of the input. 
\begin{equation*}
    y[n]=-x[n].
\end{equation*}

\linesubsec{Gain} Multiply input by a gain value in dB.
\begin{equation*}
    y[n]=x[n]*10^{(\textnormal{Gain}_\textnormal{{dB}}/20)}.
\end{equation*}

\linesubsec{DC Offset} Add a constant value to the input.
\begin{equation*}
    y[n]=x[n]+O
\end{equation*}

\linesubsec{Lowpass/Highpass} Second order lowpass/highpass implemented as a single biquad section.
\begin{alignat*}{2}
    b_{0} &= (1 \mp \cos\omega_{0}) / 2  & \quad & a_{0} = 1 + \alpha \\
    b_{1} &= \pm (1 \mp \cos\omega_{0})  & \quad & a_{1} = -2 * \cos\omega_{0} \\
    b_{2} &= (1 \mp \cos\omega_{0}) / 2  & \quad & a_{2} = 1 - \alpha
\end{alignat*}
where $\omega_{0} = 2\pi(f_{0}/f_{s})$ with $f_{0}$ and $f_{s}$ cutoff/sampling frequency, and $\alpha = \sin\omega_{0}/(2Q)$. 
Each filter is defined by 2 parameters: cutoff frequency and Q factor.

\linesubsec{Low/High Shelf} Second order low/high shelving filter implemented as a single biquad section.
\begin{equation*}
    \begin{split}
        b_{0} &= A [(A + 1) \mp (A - 1) \cos\omega_{0} + 2 \sqrt{A} \alpha] \\    
        b_{1} &= \pm2A[(A - 1) \mp (A + 1) \cos\omega_{0}] \\
        b_{2} &= A[(A + 1) \mp (A - 1) \cos\omega_{0} \mp 2\sqrt{A}\alpha] \\
        a_{0} &= (A + 1) \pm (A - 1)\cos\omega_{0} + 2\sqrt{A}\alpha \\
        a_{1} &= \mp2[(A - 1) \pm (A + 1) \cos\omega_{0}] \\
        a_{2} &= (A + 1) \pm (A - 1) \cos\omega_{0} - 2\sqrt{A}\alpha
    \end{split}
\end{equation*}
where $A=10^{(Gain_{dB}/40)}$.
Each filter is defined by 3 parameters: gain, cutoff frequency, Q factor.

\linesubsec{Peak/Notch} Second order peak/notch filter implemented as a single biquad section.
\begin{alignat*}{2}
    b_{0} &= 1 + \alpha A       & \quad & a_{0} = 1 + \alpha / A \\
    b_{1} &= -2 \cos\omega_{0}  & \quad & a_{1} = -2\cos\omega_{0} \\
    b_{2} &= 1 - \alpha A       & \quad & a_{2} = 1 - \alpha / A
\end{alignat*}
Each filter is defined by 3 parameters: gain, center frequency, Q factor.

\linesubsec{Parametric EQ} We define a Parametric EQ as a chain of 5 filters: low shelf, three peak/notch filters and a high shelf. 
Each Parametric EQ has 15 parameters.

\linesubsec{Shelving EQ} We define a Shelving EQ as a chain of 4 filters: highpass, low shelf, high shelf, lowpass. 
Each Shelving EQ is defined by a total of 10 parameters.
% \\\linesubsec{Static FIR Filter} We define a static (i.e., not driven by a controller) FIR filter of any number of taps using a siren network \citep{sitzmann2020implicit} that stores the values for each tap of the impulse response. 
% The filter class has options to initialize the weights of the siren network to specific values or to a pre-trained impulse response (e.g., generic loudspeaker).

\linesubsec{Static FIR Filter} We define a Static FIR Filter using a SIREN layer~\citep{sitzmann2020implicit}, which stores the tap values of an $N^{th}$-order impulse response.
\begin{equation*}
    y[n]=b_{0}x[n]+b_{1}x[n-1]+\ldots+b_{N}x[n-N]
\end{equation*}
The network can be initialized with a pre-trained response (e.g., loudspeaker) and its hyperparameters (hidden dimension and layers count) to be customized.

\linesubsec{Tanh Nonlinearity} Standard hyperbolic tangent.

\linesubsec{Static MLP Nonlinearity} MLP Nonlinearity implemented with SIREN layer, initialized by default with a pre-trained model approximating a $tanh$.

\linesubsec{Static Rational Nonlinearity} 
% A Padé approximant \citep{baker1961pade} is the rational function of a given order that best approximates a function near a specific point. 
% Given a function $f(x)$ and two integers $m\geq0$ and $n\geq1$, the Padé approximant of order $[m/n]$ is the rational function: 
A Padé approximant \citep{baker1961pade} is a rational function of order $[m/n]$ that best approximates a function $f(x)$ near a specific point, with $m \geq 0$ and $n \geq 1$:
\begin{equation*}
    R(x)=\frac{a_{0}+a_{1}x+a_{2}x^{2}+\cdots+a_{m}x^{m}}{1+b_{1}x+b_{2}x^{2}+\cdots+b_{n}x^{n}}
\end{equation*}
which agrees with $f(x)$ to the highest possible order. 
which amounts to:
\begin{alignat*}{1}
    f(0) &= R(0) \\
    f'(0) &= R'(0) \\
    \vdots \\
    f^{(m+n)} &= R^{(m+n)}(0).
\end{alignat*}
% Learnable Padè approximants\footnote{\url{https://github.com/ml-research/rational\_activations}} were introduced in \citep{molina2019pad} as a way to learn rational activation functions that are flexible and require a very small number of weights, i.e., the numerator's and denominator's coefficients.
% Using a single rational activations layer we define a learnable static rational nonlinearity. 
% By default, our rational nonlinearity is initialized with a $\tanh$ approximation of order $[6,5]$ for the numerator and denominator.
Learnable Padé approximants\footnote{\lesstiny{\url{github.com/ml-research/rational_activations}}} \citep{molina2019pad} enable flexible rational activation functions with few weights (numerator and denominator coefficients). 
We define a learnable Static Rational Nonlinearity using a single rational activation layer, initialized by default to a $tanh$ approximation of order $[6,5]$.

% ===
% \vspace{-1mm}
\subsubsection{Differentiable Controllers}
% \vspace{-1mm}
\label{sec:ddsp-controllers}
% We define five types of differentiable controllers (see Fig.~\ref{fig:controllers}) that can be used to generate control parameters for each differentiable audio processor in a signal chain: dummy, static, static conditional, dynamic, dynamic conditional.
We define five types of differentiable controllers (Fig.~\ref{fig:controllers}) to generate control parameters for an audio processor. 
% dummy, static, static conditional, dynamic, and dynamic conditional.

\linesubsec{Dummy} 
% A dummy controller is an empty controller used as placeholder for each processor in a signal chain that does not require any control parameters (e.g., Phase Inversion, Static FIR Filter, Static Nonlinearity).
A Dummy controller is a placeholder for processors that don't require control parameters (e.g., Phase Inversion, Static FIR Filter, Static Nonlinearity).

\linesubsec{Static} A Static controller is a tensor of trainable controls $\mathbf{b}$ - one for each control parameter in the respective processor - followed by a sigmoid function to limit the output to the [0,1] range: $g = \sigma(\mathbf{b})$.
% \begin{equation*}
%     g = \sigma(\mathbf{b})
% \end{equation*}

\linesubsec{Static Conditional} 
% To change the control parameters of a processor as a function of the audio effects controls (or some other fixed values) we define a static conditional controller, which is made of an MLP followed by a sigmoid activation. 
% The hyperparameters are: number of input controls, number of output control parameters, number of layers and hidden dimensions of the MLP.
A Static Conditional controller uses an MLP with a sigmoid activation to adjust control parameters based on audio effects controls (or some other fixed values): $g = f(\mathbf{c})$. Hyperparameters include number of input controls and output control parameters, number of layers, and hidden dimensions.
% \begin{equation*}
%      g = f(\mathbf{c})
% \end{equation*}

\linesubsec{Dynamic} 
% To change the control parameters as a function of a signal (typically the input audio) we define a dynamic controller. 
% The control signal, usually at audio sample rate, is downsampled using a max pooling layer (default downsampling factor is 128), followed by an LSTM layer, a sigmoid activation, and an upsampling layer to obtain the output control parameters sequence $\mathbf{g}_{n}$ at the original rate. 
% Hyperparameters allow to choose the block size (i.e., downsampling factor) and the number of recurrent layers, while the hidden size is dictated by the number of control parameters for each processor.
A Dynamic controller is used to adjust control parameters over time based on another signal, oftentimes the input audio: $g[n]=f(x[n])$. 
The control signal is downsampled (default downsampling factor is 128), processed through an LSTM, a sigmoid activation, and upsampled to output a control parameters sequence $\mathbf{g}_{n}$ at the original rate. Hyperparameters include block size (i.e., downsampling factor) and number of recurrent layers, while the hidden size is set by the number of control parameters for each processor.
% \begin{equation*}
%     g[n]=f(x[n])
% \end{equation*}

\linesubsec{Dynamic Conditional} 
% We define a dynamic conditional controller as a way to change a processor's control parameters as a function of both the audio effect controls (or some other fixed values) and the input signal (or some other time-varying control signal). 
% As in the dynamic controller a max pooling layer is used to downsample the control signal while an upsampling layer takes care of upsampling the control values to the same rate. 
% An LSTM takes both inputs and, after sigmoid activation and upsampling layer, we obtain the control parameters sequence $\mathbf{g}_{n}$ at the original signal rate.
A Dynamic Conditional controller adjusts control parameters based on both fixed values (typ., audio effect controls) and a time-varying control (typ., input signal): $g[n]=f(x[n],c)$. The signal is downsampled while the controls are upsampled and concatenated, the sequence processed by an LSTM, and after sigmoid activation and upsampling, the control parameters sequence $\mathbf{g}_{n}$ is returned at the original rate.
% \begin{equation*}
%     g[n]=f(x[n],c)
% \end{equation*}

Although the control parameters sequences are at sampling rate, the block size (i.e., downsampling rate) is used internally in each processors to downsample the sequence so that the coefficients are recomputed once per block. 
This is not a limitation, as setting the block size to 1 provides sequences at audio rate. 
No interpolation methods have been implemented for smooth control sequences at the time of writing.
% This is not a limitation since one could set the block size to 1 to obtain sequences at audio rate. 
% At the moment of writing no interpolation methods have been implemented to obtain smooth control sequences.

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{FIGURES/Controllers.pdf}
    % \vspace{-7mm}
    \caption{Controllers included in NablAFx}
    \label{fig:controllers}
    % \vspace{-5mm}
\end{figure}

\input{TABLES/models}
\input{TABLES/results}

% \vspace{-1mm}
\section{Audio Effects Modeling}
% \vspace{-1mm}
To showcase our audio effects modeling framework and evaluate the proposed conditioning methods we train parametric black- and gray-box models of the Multidrive Pedal Pro F-Fuzz, a digital emulation of the Dallas Arbiter Fuzz Face.
% To showcase our framework for audio effects modeling applications and demonstrate the effectiveness of the conditioning methods we proposed in Sec.~\ref{sec:cond-bb-models} and the controllers of Sec.\ref{sec:ddsp-controllers}, we run a series of experiments.
% We train examples of black- and gray-box architectures to obtain a parametric model of the Multidrive Pedal Pro F-Fuzz, a digital emulation of the Dallas Arbiter Fuzz Face, which has Volume and Fuzz controls.
Table~\ref{tab:models} shows all models configurations. 
We select TCN and S4 models and evaluate all conditioning methods available (i.e., FiLM, TFiLM, TTFiLM, TVFiLM).
The table shows how TTFiLM and TVFiLM enable implementation of time-varying conditioning with a small overhead w.r.t. FiLM.
% The proposed TTFiLM and TVFiLM methods allow to implement time-varying conditioning with a small overhead w.r.t. standard FiLM, making them a more efficient alternative to TFiLM. 

We propose two gray-box architectures (GB-DIST and GB-FUZZ) that are extensions of the typical Weiner-Hammerstein model \citep{colonel2022reverse} adopted for distortion modeling, which includes a memoryless nonlinearity in between pre-emphasis and de-emphasis linear time-invariant filters.
% Our models include: Parametric EQ, Gain, Offset, Nonlinearity, Gain, Parametric EQ.
We test two nonlinearities: Static MLP (MLP) and Static Rational Nonlinearity (RNL).
While GB-DIST models only use Static Conditional controllers, in GB-FUZZ we opt for a Dynamic Conditional controller for the Offset block, to capture the characteristic dynamic bias shift of fuzz effects.
% For GB-DIST models we only use Static Conditional controllers, while for GB-FUZZ ones we opt for a Dynamic Conditional controller for the Offset block, which allows to capture the characteristic dynamic bias shift of fuzz effects as a function of input signal and controls.

Models are trained for a maximum of 15k steps using a weighted sum of $L1$ and MR-STFT \citep{steinmetz2020auraloss} losses and the results shown in Table~\ref{tab:results}.
For TCN models, TTF and TVF conditioning perform on par with TF; while for S4 models TTF and TVF outperform TF.
For GB models, regardless of the nonlinearity type, GB-FUZZ achieves better results than GB-DIST, proving the Dynamic controller useful.
Also, RNL in shown to be an effective and efficient alternative to the MLP nonlinearity.
% Also, RNL nonlinearity performs on par with MLP, demonstrating it to be an effective and efficient alternative.

% \vspace{-1mm}
\section{Conclusion}
% \vspace{-1mm}
In this work we presented NablAFx, an open-source framework developed to support research in differentiable black-box and gray-box audio effects modeling. 
Its modular design enables easy configuration of experiments with different architectures, datasets, training settings, and loss functions. 
With logging, plotting, and performance metrics, it simplifies experiment analysis and comparison.
% The logging a plotting features make it possible to analyze and compare the various experiments, and the included metrics to evaluate models performance.
We consider gray-box models as a series connection of DDSP blocks, but this could be generalized using a graph representation.
% Also, while we consider black-box models made of a single neural network, these could be equally extended to interconnections of networks.
While black-box models are currently single networks, they could be extended to interconnected networks.
% Furthermore, there is the potential to define hybrid models as a mix of black-box and gray-box processor, to allow for DDSP blocks where a known function is required together with general neural networks where the function is to be learned.
Hybrid models could be introduced to combine black- and gray-box processors, allowing DDSP blocks with known functions alongside neural networks for learning functions.
% Contributions from the community would also allow to extend our framework in many ways: architectures, loss functions, metrics and so on.
Moreover, community contributions could help expand our framework in various ways, including new architectures, loss functions, metrics, and more.

% The \textit{Processor} and \textit{Controller} paradigm could be also extended to the \textit{BlackBoxModel} class, allowing to define complex interconnections of black-box and gray-box processors, and to mix the two types within the same system, to allow for DDSP blocks where a known function is required together with generic neural networks where the function is to be learned.

% \vspace{-1mm}
\section{Acknowledgments}
% \vspace{-1mm}
Funded by UKRI and EPSRC as part of the ``UKRI CDT in Artificial Intelligence and Music'', under grant EP/S022694/1.



\bibliographystyle{unsrtnat}
\bibliography{references} 

\end{document}
