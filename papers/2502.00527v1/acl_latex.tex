% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{dblfloatfix}

\newcommand{\name}{PolarQuant}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{PolarQuant: Leveraging Polar Transformation for Efficient Key Cache Quantization and Decoding Acceleration}

\author{Songhao Wu$^{1}$\thanks{\ \ Equal contribution.
% The work was done during Songhao Wu's internship at Meituan.
}
\quad Ang Lv$^{1}$\footnotemark[1] \\ 
\textbf{Xiao Feng}$^{2}$ \quad \textbf{Yufei Zhang}$^{3}$ \quad \textbf{Xun Zhang}$^{3}$ \\
\textbf{Guojun Yin}$^{3}$\thanks{\ \ Corresponding authors.} \quad \textbf{Wei Lin}$^{3}$ \quad \textbf{Rui Yan}$^{1}$\footnotemark[2] \\
  $^1$Renmin University of China\quad
  $^2$ShanghaiTech University\quad$^3$Meituan\\
  \texttt{\{songhaowu, anglv, ruiyan\}@ruc.edu.cn} \\
  \texttt{fxiao369@gmail.com} \\
  \texttt{\{zhangyufei08, zhangxun12, yinguojun02, linwei31\}@meituan.com} \\
  \\
}


\begin{document}
\maketitle

\begin{abstract}
The KV cache in large language models is a dominant factor in memory usage, limiting their broader applicability. 
Quantizing the cache to lower bit widths is an effective way to reduce computational costs; however, previous methods struggle with quantizing key vectors due to outliers, resulting in excessive overhead.
We propose a novel quantization approach called \name, which efficiently addresses the outlier challenge. 
We observe that outliers typically appear in only one of two dimensions, which are rotated together by a specific angle when rotary position embeddings are applied.
When represented as two-dimensional vectors, these dimensions exhibit well-structured patterns, with radii and angles smoothly distributed in polar coordinates.
This alleviates the challenge of outliers on per-channel quantization, making them well-suited for quantization.
Thus, \name\ divides key vectors into groups of two-dimensional sub-vectors, encoding them as the corresponding quantized radius and the polar angle, rather than quantizing original key vectors directly.
\name\ achieves the superior efficiency in KV cache quantization and accelerates the decoding process by turning the query-key inner product into a table lookup, all while maintaining the downstream performance of full-precision models.
\end{abstract}

\section{Introduction}

Large language models (LLMs) have achieved remarkable success across a wide range of applications. 
As these models continue to advance, the demand for enhanced long-context capabilities also increases, encompassing tasks such as contextual retrieval in question answering~\cite{liu-etal-2024-lost} and long-context generation for deep reasoning and reflection~\cite{o12024}.
However, a significant challenge in developing long-context LLMs is the rising memory cost associated with increasing context lengths, which hinders both their practical deployment and further research.

The attention mechanism~\cite{bahdanau2016neuralmachinetranslationjointly} in LLMs\footnote{In this paper, we focus on decoder-only Transformer-based~\cite{transformer} LLMs using rotary position embedding (RoPE,~\citealp{rope}), which are the predominant implementation of advanced LLMs.} is a major contributor to memory consumption, as its memory requirements typically grow quadratically with context length. 
To mitigate this overhead, a common strategy is to cache the key and value vectors (known as the KV cache) from previous contexts, thereby avoiding the need to recompute the entire quadratic attention weight matrix. 
Nonetheless, in long-context scenarios, the memory required for the KV cache often exceed that consumed by the LLM's weights, making it the dominant factor in overall memory usage.

\begin{figure*}[!t]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{teaser_0131.pdf}}
\caption{
(a) Illustration of outliers in key vectors. We highlight two dimensions rotated together by RoPE that exhibit outliers (exemplified by Llama 3.1-8B-Instruct Layer 0 Head 0).
(b) When viewing these two dimensions in a two-dimensional plane, although the individual x- or y-axis may contain outliers, they collectively form stable circular patterns, making quantization of the original outliers easier. 
Each blue dot represents a mapped two-dimensional vector, with transparency indicating frequency.
(c) An example of \name\ using \(m=3\) bits to quantize polar angles and \(n=2\) bits to quantize radii. The colorful arrows indicate sub-vectors formed by pairs of dimensions in the keys; the quantized results are shown with colorful dashed arrows, and the quantization error is represented by the grey dashed arrow.}
\label{fig:teaser}
\end{center}
\end{figure*}

A series of solutions is proposed to reduce the memory cost associated with the KV cache. 
Some studies introduce memory-efficient attention modules, such as GQA~\cite{ainslie-etal-2023-gqa} and MLA~\cite{deepseekai2024deepseekv2strongeconomicalefficient}. 
While these methods show promise for training future LLMs from scratch, they cannot be applied to existing pre-trained LLMs, which limits their generalizability.
Another research direction focuses on reducing the size of the KV cache in a manner compatible with existing LLMs. 
This includes techniques like KV cache eviction~\cite{zhang2023h2oheavyhitteroracleefficient, li2024snapkv, cai2024pyramidkvdynamickvcache}, which removes key and value vectors of unimportant tokens from the cache, and quantization~\cite{kivi,kvquant,atom,gear}, which represents cached key and value vectors in low bits.


This paper focuses on the cache quantization.
In general, key and value vectors are quantized along different axes.
Although value caches can be quantized on a token-wise basis~\cite{kivi} (i.e., across dimensions within each token position), quantizing the key cache is more challenging due to its channel-wise distributed outliers~\cite{kivi,kvquant}, as shown in Figure~\ref{fig:teaser}(a) (i.e., some specific dimensions at each token position contain outliers).
\citet{kivi} reveal channel-wise outliers, divide tokens into groups, and quantize tokens in each group along the specific dimension where outliers occur.
\citet{kvquant} note that RoPE disrupts the magnitudes of outliers at certain token positions, and they proposed channel-wise quantization of key vectors before applying RoPE (pre-RoPE).
However, these existing methods face a dilemma: quantizing post-RoPE keys demands fine-grained token grouping, whereas quantizing and caching pre-RoPE keys requires on-the-fly dequantization when applying RoPE—both of which result in overhead.



In this paper, we propose a new perspective on handling outliers in the key cache, effectively addressing the quantization dilemma.
Recall that RoPE applies a rotation to every two-dimensional sub-vector of the key vector using orthogonal $2 \times 2$ rotary matrices. For readers unfamiliar with RoPE, please refer to Section~\ref{sec:background}.
When analyzed in 2D polar coordinates, these sub-vectors form well-defined circular patterns, as illustrated in Figure~\ref{fig:teaser}(b).
By encoding each sub-vector as its corresponding radius \(r\) and polar angle \(\theta\), we can represent the entire key vector using all radii and angles.
This perspective transformation effectively mitigates outliers, as both the radii and polar angles become smoothly distributed.
Building on this, we propose a novel quantization method, \name, under the rotation perspective, which significantly simplifies the quantization of the key cache.
\name\ reduces the problem of quantizing key vectors to asymmetrically quantizing $r$ and $\theta$ into an $n$-bit and an $m$-bit integer. 
Intuitively, \name\ defines \(2^{n+m}\) distinct regions based on \(2^{m}\) angles and \(2^{n}\) radii. 
Each sub-vector is then encoded by the index of the region it belongs to.
Figure~\ref{fig:teaser}(c) illustrates \name\ for \(m = 3\) and \(n = 2\).


\textbf{\name\ achieves superior quantization efficiency over previous methods} for three primary reasons:
(1) Unlike pre-RoPE quantization~\cite{kvquant}, which requires on-the-fly dequantization when applying RoPE on memory-bounded GPUs, \name\ eliminates this overhead entirely.  
(2) Smoother distributions of radii and angles facilitate downstream performance preservation, so \name\ does not require the token grouping used in previous post-RoPE quantization~\cite{kivi}.  
(3) \name\ also requires fewer quantization parameters, not only because it does not use grouping, but also because it leverages the non-negativity of the radii to avoid storing zero-points.

Additionally, \textbf{\name\ enables a novel decoding acceleration method.} 
In the attention mechanism, it replaces the standard query-key multiplication with inner products between two-dimensional query sub-vectors and a quantized polar coordinate representation of key sub-vectors, which have finite and deterministic states.
This transforms matrix multiplication to a table lookup, greatly speeding up attention computation.
Although this approach can be applied to previous post-RoPE quantization methods, the increased number of quantization states from token grouping negates any overall efficiency gain.

We implement Triton~\cite{triton} kernels for \name\ and our new decoding acceleration method. 
%With $n$ = 4-bit, we achieve memory savings of \textcolor{red}{XX}\% and a \textcolor{red}{XX}\% faster，
With $n=4$ bit, we achieve an up-to 1.27$\times$ speedup of query-key mulitplication on various open-source LLMs, while maintaining comparable downstream performance to previous competitive methods.

Our contributions are threefold: (1) introducing polar transformation for quantization for the first time and deriving \name, a novel and efficient post-RoPE quantization method; 
(2) reducing the number of quantization parameters, thereby lowering quantization costs; and (3) proposing a new decoding acceleration algorithm as a natural byproduct of \name.
We are committed to open source.


\section{Background}
\label{sec:background}

Consider a specific Transformer layer where the input hidden states to the attention block are denoted as \( \mathbf{X} \in \mathbb{R}^{T \times D} \), where \( T \) is the sequence length and \( D \) is the hidden state dimension. 
For an arbitrary attention head, the \( d \)-dimensional query, key, and value vectors are obtained by applying three linear transformations to \( \mathbf{X} \). 
Specifically, for each head \( h \), the corresponding computations are as follows:
\[
\mathbf{Q} = \mathbf{X} \mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X} \mathbf{W}_K, \quad \mathbf{V} = \mathbf{X} \mathbf{W}_V,
\]
where each \( \mathbf{W}_{*} \in \mathbb{R}^{D \times d} \), and the resulting variables have shapes of \( \mathbb{R}^{T \times d} \).

The query and key vectors are then applied with RoPE~\cite{rope} to incorporate positional information. 
For a query or key vector at position \( t \in [1, T] \), the corresponding rotary matrix \( \boldsymbol{R}_{t, \Phi} \in \mathbb{R}^{d \times d} \) is defined as:
\begin{equation}
\boldsymbol{R}_{t, \Phi} = \begin{bmatrix}
\boldsymbol{r}_{t, \phi_1} & \mathbf{O} & \cdots & \mathbf{O} \\
\mathbf{O} & \boldsymbol{r}_{t, \phi_2} & \cdots & \mathbf{O} \\
\mathbf{O} & \mathbf{O} & \cdots & \boldsymbol{r}_{t, \phi_{d/2}}
\end{bmatrix}
\label{eq:rope_matrix}
\end{equation}
where \( \mathbf{O} \) is a zero matrix, and each \( \boldsymbol{r}_{t, \phi_i} \) for \( i \in [1, \frac{d}{2}] \) is a \( 2 \times 2 \) orthogonal matrix:
\[
\boldsymbol{r}_{t,\phi_i} = \begin{bmatrix}
\cos(t \phi_i) & -\sin(t \phi_i) \\
\sin(t \phi_i) & \cos(t \phi_i)
\end{bmatrix}
\]
Here, \( \phi_i \) is typically defined as \( \phi_i = b^{-2(i-1)/d} \), where \( b \) is a hyperparameter. 
This formulation encodes the relative distance \( l - t \) between a query at position \( l > t \) and a key at position \( t \) into their inner product, as shown by:
\[
(\mathbf{Q}_l \boldsymbol{R}_{l, \Phi}) (\mathbf{K}_t \boldsymbol{R}_{t, \Phi})^{\top} = \mathbf{Q}_l \boldsymbol{R}_{t-l, \Phi} \mathbf{K}_t^{\top}
\]

In causal language models, such as generative LLMs, each token can only attend to preceding tokens. 
Therefore, the keys (after applying RoPE) and values of previous tokens—specifically \( \mathbf{K}_t \boldsymbol{R}_{t, \Phi} \) (which we abbreviate as \( \mathbf{\tilde{K}}_t \)) and \( \mathbf{V}_t \)—are unaffected by future tokens. 
These vectors are thus cached, known as the KV cache, to avoid redundant recomputation. 
However, the size of the KV cache in large language models can become prohibitively large, often exceeding the number of model parameters. 
This presents a significant challenge when processing long contexts, making it essential to quantize the KV cache for more broader usage.

A value vector \( \mathbf{V}_t \in \mathbb{R}^{d} \) is typically $b$-bit quantized per-token-wise, denoted as \( Q(\mathbf{V}_t) \).
For an arbitrary dimension $j \leq d$, we have:
\[
Q(\mathbf{V}_{t}[j]) = \texttt{Clamp}\left( \left\lfloor \frac{\mathbf{V}_t[j] - Z_t}{s_t} \right\rceil, 0, 2^b - 1 \right),
\]
where:
\[
Z_t = \min(\mathbf{V}_t[:]), s_t = \frac{\max(\mathbf{V}_t[:]) - \min(\mathbf{V}_t[:])}{2^b - 1}.
\]
Here, the colon denotes iteration over all dimensions, following Python indexing syntax. 
\( Z_t \) is the zero-point, and \( s_t \) is the scaling factor. 
The function \( \texttt{Clamp}(x, y, z) \) restricts the value of \( x \) to integers within the range \( [y, z] \).

Outliers in key vectors (both pre-RoPE and post-RoPE) make per-token quantization challenging, as we discussed earlier and illustrated in Figure~\ref{fig:teaser}(a).
To address this, previous approaches~\cite{kivi, kvquant} quantize key vectors channel-wise. 
For example, for a arbitrary dimension \( i \), a quantized pre-RoPE key vector \( Q(\mathbf{K}_t) \) is given by:
\[
Q(\mathbf{K}_{t}[j]) = \texttt{Clamp}\left( \left\lfloor \frac{\mathbf{K}_t[j] - Z_j}{s_j} \right\rceil, 0, 2^b - 1 \right),
\]
where the zero-point and scaling factor alternate as:
\[
Z_j = \min(\mathbf{K}_{[:]}[j]),
\]
\[
 s_j = \frac{\max(\mathbf{K}_{[:]}[j]) - \min(\mathbf{K}_{[:]}[j])}{2^b - 1}.
\]
Here, the colon in the subscript denotes iteration over all token positions.

Another challenge in quantizing key vectors is deciding whether to apply quantization before or after the RoPE. 
Quantizing the key vectors after applying RoPE faces a more complex outlier distribution, requiring tokens to be grouped for individual quantization, which adds additional overhead~\cite{kvquant}. 
In contrast, outliers in pre-RoPE key vectors are more structured, but quantization in this case necessitates dequantization after retrieving keys from the cache and applying RoPE, which also introduces computational overhead.
Addressing this dilemma is the central focus of this paper.

\section{Method}


We begin by presenting the key findings of outlier patterns in the key vectors (Section~\ref{sec:motivation}). 
These insights form the foundation for our novel and efficient quantization approach, \name\ (Section~\ref{sec:main_method}).

\subsection{Motivation}
\label{sec:motivation}
As discussed earlier, outliers in key vectors pose a dilemma for researchers due to the overhead introduced by both pre-RoPE and post-RoPE channel-wise quantization.
Our solution to this challenge arises from a key observation:

\textit{When mapping a paired dimension with outliers to polar coordinates, the outlier elements naturally form well-structured circular patterns, which simplify quantization.}

Recall that in key vectors, elements in certain dimensions are jointly rotated by the same rotary sub-matrix \(\boldsymbol{r}_{n,\phi_i}\). 
Our analysis shows that the most prominent outliers (highlighted in colors in Figure~\ref{fig:teaser}(a)) tend to occur in one of these dimension pairs.\footnote{For efficiency, the rotary matrix is typically applied in an element-wise multiplication manner~\cite{rope}. 
To simplify implementation, dimensions \(i\) and \(i + d/2\) are often rotated together, rather than $i$ and $i+1$.
This results in non-adjacent outliers in Figure~\ref{fig:teaser}(a), but it does not affect our analysis, which is based on the matrix multiplication formulation (Eq.~\ref{eq:rope_matrix}).}

Because items in such paired dimensions are treated as two-dimensional vectors and rotated jointly, we are motivated to analyze these outliers in a two-dimensional plane.
Figure~\ref{fig:teaser}(b) maps the paired dimensions from Figure~\ref{fig:teaser}(a) onto a 2D Cartesian coordinate system, where the x-axis represents the first dimension and the y-axis represents the second. 
Despite large variations in individual x and y values (which would indicate outliers in isolation), the mapped vectors form a well-structured pattern. 
In other words, when transformed into polar coordinates, the outliers are characterized by a smoothly distributed radial coordinate \(r\) and a polar angle \(\theta\). 
This structure significantly alleviates the quantization challenges faced by key caches.
% For added clarity, Figure~\ref{fig:motivation} provides a supplementary illustration of how this polar transformation improves quantization.

\subsection{\name: Polar-Coordinate-Based Quantization of Post-RoPE Key Vectors}
\label{sec:main_method}

Building on these insights, we propose a novel polar-coordinate-based quantization method, \name, designed for post-RoPE key vectors, which eliminates the need for token grouping. 
Because the advantages of using a polar-coordinate perspective to handle outliers were discussed in the previous subsection, here we focus on the implementation details.  

For a 2D subvector \([ \tilde{\mathbf{K}}_{t}[2j], \tilde{\mathbf{K}}_{t}[2j+1]]\) in a post-RoPE key vector at position \( t \), where \(0 \leq j < d/2 \), we interpret \( \tilde{\mathbf{K}}_{t}[2j] \) and \( \tilde{\mathbf{K}}_{t}[2j+1] \) as Cartesian coordinates in the \( xy \)-plane. 
This 2D vector is then converted to polar coordinates, where the radius \( r_t[j] \) is given by:
\[r_{t}[j] = \sqrt{\tilde{\mathbf{K}}_{t}[2j]^2 + \tilde{\mathbf{K}}_{t}[2j+1]^2}, \] 
and the polar angle is:
\[ 
\theta_{t}[j] = \texttt{atan2}\left({\tilde{\mathbf{K}}_{t}}[2j+1], {\tilde{\mathbf{K}}_{t}[2j]}\right) + \pi,
\]
where $\texttt{atan2}\left(y, x\right)$ returns the angle between the positive x-axis and the point $(x, y)$, with a range of $(-\pi, \pi)$.

We quantize $r_t$ and $\theta_t$ asymmetrically, using $n$-bit and $m$-bit precisions, respectively:
\[
Q(r_{t}[j]) = \texttt{Clamp}\left( \left\lfloor \frac{r_t[j]}{s_j} \right\rceil, 0, 2^n - 1 \right),
\]
\[
Q(\theta_{t}[j]) = \left\lfloor
        \frac{2^{m - 1} \theta_{t}[j]  }{\pi}
    \right\rceil \mod 2^{m},
\]
where:
\[
s_j = \frac{\max(r_{[:]}[j])}{2^n - 1}.
\]
Note that $r_t$ does not have a quantization zero point because $r_t$ is always greater than or equal to 0.


Intuitively, \name\ divides the two-dimensional plane into \( 2^{n+m} \) regions, spanned by \( 2^{n} \) radii and \( 2^{m} \) polar angles. 
A 2D sub-vector of the key vector is then represented by the region it locates at.




\subsection{Efficient Decoding with \name}
\label{sec:algorithm}

In this section, we introduce how \name\ performs decoding, with a particular focus on the query-key inner product step based on a quantized key cache. 
We highlight how it accelerates the decoding process. 

Let us first review the dequantization process in traditional quantization methods. 
During decoding, the cached key vectors must be dequantized before performing the inner product with the current query token at position \( t \). 
The dequantization process is formalized as follows, where \( \mathbf{K}^{'}_{t} \) represents the dequantized key in floating-point precision. 
For each dimension \( 0 \leq j < d \), we have:
\[
\mathbf{K}^{'}_{t}[j] = Q(\mathbf{K}_{t}[j]) \cdot s_{j} + Z_{j},
\]
This dequantization introduces additional computational overhead. 
The inner product is then computed as \( \mathbf{Q}_{t} \cdot \mathbf{K}^{\prime\top}_{[:]} \).

We argue that this overhead is redundant. 
At any dimension \( j \), the dequantized outcomes belong to a finite set of size \( 2^b \), where \( b \) is the quantization precision.
This set depends entirely on the precision, and is unaffected by the shape or values of the quantized key.
When the cache size far exceeds \( 2^b \), it is more efficient to pre-compute and pick values from a lookup table, where the dequantized results have been stored. This is the key insight behind how \name\ accelerates multiplication. 

Specifically, in \name, the lookup table is constructed by mapping quantized polar coordinates to Cartesian coordinates. 
The x- and y-axis values are then treated as separate elements in the key vector's specific dimensions. 
For a quantized representation \( (Q(r_t[j]), Q(\theta_{t}[j])) \), the corresponding Cartesian coordinates in the key vector at dimensions \( 2j \) and \( 2j+1 \) are calculated as:
\[
\begin{bmatrix}
\tilde{\mathbf{K}}_{t}[2j] \\
\tilde{\mathbf{K}}_{t}[2j+1]
\end{bmatrix}^{\top}
=
\begin{bmatrix}
\cos \left( \frac{\pi Q(\theta_t[j])}{2^{m-1}} \right) \cdot (Q(r_t[j]) \cdot s_j) \\
\sin \left( \frac{\pi Q(\theta_t[j])}{2^{m-1}} \right) \cdot (Q(r_t[j]) \cdot s_j)
\end{bmatrix}^{\top}
\]
Here, the sub-vector \( [\tilde{\mathbf{K}}_{t}[2j], \tilde{\mathbf{K}}_{t}[2j+1]] \) represents a state in the lookup table, which contains \( \frac{d}{2} \times 2^m \) states in total (where \( \frac{d}{2} \) is the number of sub-vectors in a key vector of dimension \( d \), and \( 2^m \) is the number of quantized polar angle states).
When computing the query-key inner product for the dimensions \( 2j \) and \( 2j+1 \), the result is:
\[
\mathbf{IP}_{2j, 2j+1} = \mathbf{Q}_{t}[2j] \cdot \tilde{\mathbf{K}}_{t}[2j] + \mathbf{Q}_{t}[2j+1] \cdot \tilde{\mathbf{K}}_{t}[2j+1],
\]
and the final inner product is the sum:
\[
\sum_{0 \leq j < \frac{d}{2}} \mathbf{IP}_{2j, 2j+1}.
\]

Previous methods cannot take advantage of this acceleration approach for several reasons. 
For methods based on pre-RoPE keys, dequantizing the key cache and performing multiplication with the RoPE matrix introduces pronunced overhead. 
In contrast, methods that focus on post-RoPE keys divide tokens into \( T/~g \) groups, with each group having \( g \) tokens, and perform channel-wise quantization on the keys within each group.
When \( g \) is small, it cannot exceed \( 2^b \) by much, leading to no efficiency gain, and possibly even slower inference. 
However, when \( g \) is large, inference performance is compromised.


\begin{table*}[t]
\caption{
Evaluating quantization methods in long-text scenarios. 
We present experimental results from a series of advanced LLMs across a wide range of tasks in the LongBench benchmark.
Actual bit widths for each method are estimated based on a context length of 12.2K tokens, which is the average input length of tokens on the LongBench. 
% We exclude the KVQuant results for Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct, as the KVQuant implementation is incompatible with the GQA setting.
}
\label{tab:longbench}
\vskip 0.1in
\resizebox{\textwidth}{!}
{
    \begin{tabular}{c|c|cccccccccccc|c}
    \toprule
     \multicolumn{2}{c}{ }  & \multicolumn{3}{c}{\textbf{Single Doc. QA}} &  \multicolumn{3}{c}{\textbf{Multi Doc. QA}} & \multicolumn{3}{c}{\textbf{Summarization}} &  \multicolumn{3}{c}{\textbf{Few-shot Learning}} &  \\
     \cmidrule(lr{2pt}){3-5} \cmidrule(lr{2pt}){6-8} \cmidrule(lr{2pt}){9-11}   \cmidrule(lr{2pt}){12-14} 
     Method & Actual Bit & NtrvQA & Qasper & MF-en & 2Wiki & Hotpot & Musique & GovRep & QMSum & MNews & TREC & TriviQA & SamSum & Avg.  \\
     \midrule
     \multicolumn{2}{c}{ } & \multicolumn{12}{c}{\textbf{\textit{Llama-2-7B-Chat~(4K)}}} \\
     \cmidrule(lr{1pt}){1-2} \cmidrule(lr{1pt}){3-14} \cmidrule(lr{1pt}){15-15}
      fp16 Baseline & 16 & 19.36 & 21.82 & 36.90 & 29.59 & 27.50 & 8.05 & 26.83 & 20.73 & 26.39 & 64.50 & 83.31 & 41.56 & 33.88 \\
      \midrule
      KVQuant-4bit-1\% & 4.32 & 18.88 & 17.60 & 32.98 & 30.79 & 24.74 & 8.40 & 25.64 & 20.67 & 25.96 & 64.50 & 83.61 & 41.29 & 32.92  \\
       KIVI-4-gs32-rs128 & 5.08 & 18.75 & 21.44 & 36.30 & 30.58 & 27.21 & 7.55 & 26.76 & 20.92 & 26.19 & 64.50 & 82.90 & 41.47 & 33.71 \\
       PolarQuant-m4n4 & 4.16 & 18.55 & 21.03 & 36.38 & 30.88 & 27.01 & 7.54 & 27.30 & 20.67 & 26.02 & 64.50 & 83.67 & 41.47 & 33.75 \\     
       \midrule
       \multicolumn{2}{c}{ } & \multicolumn{12}{c}{\textbf{\textit{Mistral-7B-Instruct-v0.2~(32K)}}} \\
     \cmidrule(lr{1pt}){1-2} \cmidrule(lr{1pt}){3-14} \cmidrule(lr{1pt}){15-15}
      fp16 Baseline & 16 & 26.73 & 33.01 & 49.34 & 27.35 & 43.02 & 18.78 & 32.81 & 24.15 & 27.09 &  68.50 & 86.48 & 41.12 & 39.87 \\
      \midrule
       KIVI-4-gs32-rs128 & 5.08 & 26.68 & 32.38 & 49.46 & 27.59 & 42.91 & 18.81 & 32.99 & 24.40 & 27.17 & 68.50 & 86.41 & 41.04 & 39.86 \\
       % KVQuant-4bit-1\% & & & & &  &  &  &  &  &  &  &  &  \\
       PolarQuant-m4n4 & 4.16 & 27.43 & 32.58 & 49.54 & 26.61 & 42.55 & 18.65 & 32.52 & 24.00 & 27.09 & 68.50 & 86.64 & 41.78 & 39.82 \\
       \midrule
     \multicolumn{2}{c}{ } & \multicolumn{12}{c}{\textbf{\textit{Llama-3.1-8B-Instruct~(128K)}}} \\
     \cmidrule(lr{1pt}){1-2} \cmidrule(lr{1pt}){3-14} \cmidrule(lr{1pt}){15-15}
      fp16 Baseline & 16 & 31.63 & 46.58 & 56.89 & 48.96 & 58.10 & 31.57 & 34.44 & 25.23 & 26.99 & 74.00 & 92.64 & 43.22 & 47.52 \\
      \midrule
       KIVI-4-gs32-rs128 & 5.08 & 30.97 & 46.43 & 56.66 & 49.13 & 58.02 & 31.46 & 34.52 & 25.36 & 27.29 & 74.00 & 92.46 & 43.57 & 47.49 \\
       % KVQuant-4bit-1\% &  &  &  &  &  &  &  &  &  &  &  &  &  \\
       PolarQuant-m4n4 & 4.16 & 32.50 & 46.16 & 56.73 & 48.97 & 57.91 & 31.67 & 34.04 & 25.47 & 26.76 & 74.00 & 92.86 & 43.07 & 47.51 \\
       \bottomrule
     \end{tabular}
}
\end{table*}


\section{Experiments}

We compare \name\ with competitive key-value quantization algorithms.
Since \name\ focuses on quantizing the key states, we first retain the value states in full precision (with fp16 as the default) to isolate and highlight the effectiveness of the quantized key cache (Section~\ref{sec:key-quant}).
Unless stated otherwise, the value states are kept in full precision throughout the following discussions.

\subsection{Experiments on Key Quantization}
\label{sec:key-quant}

We perform extensive experiments by quantizing various models and evaluating them across different tasks to demonstrate the effectiveness of \name.

\paragraph{Models}
We evaluate \name\ on a diverse set of advanced LLMs, including Llama-2-7B-Chat~\cite{touvron2023llama2}, Llama-3.1-8B-Instruct~\cite{grattafiori2024llama3}, and Mistral-7B-Instruct-v0.2~\cite{jiang2023mistral7b}. 
% We evaluate \name\ on a diverse set of advanced LLMs~\cite{touvron2023llama2, grattafiori2024llama3, jiang2023mistral7b},
These models span three distinct families and cover varying scales. 
Notably, Llama-2-7B-Chat uses a multi-head attention (MHA) architecture, while the others employ grouped query attention (GQA~\cite{ainslie-etal-2023-gqa}). 
Additionally, the models differ in their effective context lengths, ranging from 4k for Llama-2-7B to 128k for Llama-3.1-8B. 
% \name\ is implemented is built upon Huggingface Transformers framework~\cite{wolf-etal-2020-transformers}, using a plug-in approach to implement the quantization and dequantization of the key states.

\paragraph{Tasks}
% As shown in~\cite{kivi, kvquant}, key-value cache only place a burden on memory when the context input exceeds a certain length.
In this study, we primarily evaluate the performance of \name~in long-context scenarios.
Specifically, we compared \name~with other quantization methods on the LongBench dataset~\cite{bai2023longbench}, which is a widely used multitask benchmark for long-context understanding. 
% We report the results for the sub-tasks in LongBench, including single-document question-answering (QA), multi-doc QA, summarization, and few-shot learning. 
% The input sequence is truncated to the model's maximum context length to fully utilize the input context.
We also assess quantization methods with inputs of typical context length, using the MMLU~\cite{hendrycks2021mmlu} and GSM8K~\cite{cobbe2021gsm8k}.
% MMLU is designed to assess the model's parametric knowledge at both high school and college levels.
% GSM8K is a math reasoning benchmark that tests the model's ability to perform arithmetic reasoning and compose mathematical steps using language.
These two tasks are evaluated in a 5-shot in-context learning setup.
For GSM8K, the ICL demonstrations are formulated as chain-of-thought prompts.

\paragraph{Quantization Precision}
In our main experiments on both long-context and typical-length contexts, we adopt 4-bit precision because previous studies have shown that this precision can achieve performance comparable to full precision across multiple benchmarks. 
Note that group-wise and channel-wise quantization increase the number of quantization parameters, so the equivalent quantization bits are larger than 4 in practice. 
In these 4-bit precision experiments, we report the actual quantization bits for all methods.

Although lower-bit quantization inevitably impairs performance, we explore the performance of \name\ under a lower-bit quantization setup, which is discussed in Section~\ref{sec:3bit}.

\paragraph{Kernel Implementation}
We implement the efficient decoding algorithm outlined in Section~\ref{sec:algorithm} as a fused Triton~\cite{triton} kernel, significantly reducing the number of floating-point operations required for query-key multiplication. 
This optimization follows the same principles as other efficient attention implementations, such as FlashAttention~\cite{dao2022flashattention}, which improves the General Matrix-Vector Product by partitioning the input into blocks and performing multiplication within each block. 
This block-based approach minimizes I/O operations through multiple passes, thereby accelerating the attention calculation. 
Additionally, by fusing the lookup operation with matrix multiplication in each block, \name\ ensures compatibility with FlashAttention, the widely used efficient attention implementation in advanced LLMs.
A detailed analysis is provided in the ``Efficiency Comparison'' paragraph.


\begin{table*}[t]
    \small
    \caption{Performance comparisons between differenet cache quantization methods on MMLU and GSM8K, showing that our method remains competitive with the baselines on short contexts. 
    % The KVQuant results are excluded for Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Chat, as the KVQuant implementation is incompatible with the GQA setting. 
    }
    \label{tab:icl}
    \centering
    \vskip 0.1in
    \resizebox{0.9\textwidth}{!}{
        \begin{tabular}{l|c|cccc|c|c}
            \toprule
            \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{5}{c}{\textbf{MMLU}} & \multirow{2}{*}{\centering \textbf{GSM8k}} \\
            \cmidrule(lr{1pt}){3-7}
            & & \textbf{Humanities} & \textbf{Social Science} & \textbf{STEM} & \textbf{Other} & \textbf{Avg.} &  \\
            \midrule
            \multirow{4}{*}{\textbf{\textit{Llama-2-7b-Chat}}} & fp16 Baseline & 38.46 & 56.54 & 35.19 & 58.49 & 47.17 & 24.79  \\
            \cmidrule(lr{1pt}){2-8}
            & KIVI-4-gs32-rs128 & 38.53 & 55.39 & 35.45 & 59.26 & 47.16 & 23.96   \\
            & KVQuant-4bit-1\% & 38.85 & 57.52 & 34.39 & 57.52 & 47.07 & 22.67 \\
            & PolarQuant-m4n4 & 38.46 & 56.05 & 34.92 & 57.60 & 46.75 & 23.20  \\
            \midrule
            \multirow{3}{*}{\textbf{\textit{Mistral-7B-Instruct-v0.2}}} & fp16 Baseline & 48.57 & 68.46 & 45.50 & 68.45 & 57.75 & 39.42  \\
            \cmidrule(lr{1pt}){2-8}
            & KIVI-4-gs32-rs128 & 48.17 & 68.46 & 45.50 & 68.45 & 57.64 & 39.42  \\
            % & KVQuant-4bit-1\%  & & & & & &  \\
            & PolarQuant-m4n4 & 47.85 & 68.79 & 46.30 & 68.84 & 57.95 & 40.56  \\
            \midrule
            \multirow{3}{*}{\textbf{\textit{Llama-3.1-8B-Instruct}}} & fp16 Baseline & 53.98 & 78.76 & 55.82 & 73.95 & 65.63 & 78.47  \\
            \cmidrule(lr{1pt}){2-8}
            & KIVI-4-gs32-rs128 & 53.91 & 78.76 & 56.08 & 73.95 & 65.68 & 79.00  \\
            % & KVQuant-4bit-1\%  & & & & & &  \\
            & PolarQuant-m4n4 & 54.17 & 78.92 & 55.56 & 73.95 & 65.65 & 79.61  \\
            \bottomrule
        \end{tabular}
    }
\end{table*}

\begin{table}[htbp]
\centering
\caption{The statistical results of the quantization parameters for \name. We take fp16 as the default full precision. The average bit width is estimated with \( T = 12.2K \), \( d = 128 \), \( g = 32 \), and \( s = 128 \). 
Here, \( T \) represents the input sequence length, \( d \) is the dimension of the attention head, \( g \) is the group size, \( s \) is the residual length and \( \alpha \) is the ratio of outliers saved in full precision. 
To simplify the formulation, we omit the batch size \( b \) and the number of attention heads \( n \).}
\label{tab:para}
\vskip 0.1in
\resizebox{0.48\textwidth}{!}{
    \begin{tabular}{lcc|c}
        \toprule
        Method & Parameter Amount & \multicolumn{2}{c}{Actual Bit}   \\
        \midrule
        KIVI-4 & $32Td/~g+16sd/~2$ & $32/~g+8sd/~T$ & 5.08  \\
        KVQuant & $32\alpha\cdot Td$ & $32\alpha$ & 4.32 \\
        \midrule
        PolarQuant & $16d/~2+16sd$ & $8/~T+16sd/~T$ & 4.16 \\ 
        \bottomrule
    \end{tabular}
}
\end{table}



\paragraph{Results}
Table~\ref{tab:longbench} show the results of~\name~on substasks of LongBench.
We exclude the KVQuant results for Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct, as the KVQuant implementation is incompatible with the GQA setting. 
For KIVI, we use the default configuration with a group size of 32 and a residual length of 128.
For KVQuant, we adhere to the original 4-bit implementation as depicted in~\cite{kvquant}.
The experiments indicate that our method achieves comparable performance with existing 4-bit quantization baselines faced with challenging long context inputs.
Moreover, the consistent performance preservation of \name~across different models, scales, and attention architectures further emphasizes the robustness and generalizability of our approach in key states quantization.

\begin{table}[t]
\caption{
Tested on an NVIDIA A800-SXM4-80GB GPU, \name\ reduces the average latency (in microseconds) of query-key multiplication compared to both fp16 PyTorch matrix multiplication and other baselines' custom multiplication implementations. 
Here, query and key vectors are configured as \( \mathbf{Q} \in \mathbb{R}^{1\times128} \) and \( \mathbf{\tilde{K}} \in \mathbb{R}^{T\times128} \), following the dimension settings of Llama-3.1, with the input length \( T \) varying.
}    
\label{tab:speed}
\vskip 0.1in
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
    \toprule
  Operation & $T=4K$ & $T=8K$ & $T=32K$ & $T=128K$ \\
   \midrule
   FP16 Matmul & 60.98 & 74.20 & 183.99 & 668.91 \\
   \midrule
   KIVI & 97.08 & 133.92 & 427.24 & 1590.44 \\ 
   KVQuant & 94.79 & 121.42 & 423.93 & 1834.58\\
   PolarQuant & 50.04 & 71.69 &  163.42 & 526.65 \\
   \bottomrule
\end{tabular}
}
\end{table}


Additionally, we evaluate \name's performance with inputs of standard length. 
The results on MMLU and GSM8K are presented in Table~\ref{tab:icl}. 
Similarly, we exclude the KVQuant results for Mistral and Llama-3.1 based on GQA architectures.
We observe that our \name~maintains competitive performance, with no significant decline in its parametric knowledge ability (MMLU) or in its planning and reasoning capacity for mathematical tasks (GSM8K). This indicates that our method effectively processes both long and short context inputs, 
and the quantization operations introduced do not incur any notable loss in performance on knowledge-intensive and reasoning tasks.

\begin{table*}[t]
\caption{
We report experimental results for \name-m4n2 on subtasks of the LongBench benchmark. 
We take Llama-3.1-8B-Instruct as backbone, with a quantization configuration of group size 64 and residual length 64. 
We compare \name-m4n2 with the 2-bit KIVI method, which uses a group size of 32 and a residual length of 128. 
% The actual bit widths for each method are computed based on a context length of 12.2K tokens, the average input length on LongBench.
}
\label{tab:appendix}
\resizebox{\textwidth}{!}
{
    \begin{tabular}{c|c|cccccccc|c}
    \toprule
     \multicolumn{2}{c}{ }  & \multicolumn{3}{c}{\textbf{Single Doc. QA}} &  \multicolumn{3}{c}{\textbf{Multi Doc. QA}} &  \multicolumn{2}{c}{\textbf{Few-shot Learning}} &  \\
     \cmidrule(lr{2pt}){3-5} \cmidrule(lr{2pt}){6-8} \cmidrule(lr{2pt}){9-10}   
     Method & Actual Bit & NtrvQA & Qasper & MF-en & 2Wiki & Hotpot & Musique & TREC & TriviQA  & Avg.  \\
     \midrule
      FP16 Baseline & 16 & 31.63 & 46.58 & 56.89 & 48.96 & 58.10 & 31.57 & 74.00 & 92.64 &  55.05 \\  
      \midrule
      KIVI-2-gs32-rs128 & 3.08 & 31.80 & 45.28 & 56.74 & 49.80 & 57.60 & 31.18 & 73.50 & 92.56 & 54.80 \\
      PolarQuant-m4n2 & 3.29 & 31.59 & 45.77 & 55.85 & 49.62 & 58.15 & 30.98 & 74.00 & 92.76 & 54.84 \\
    \bottomrule
     \end{tabular}
}
\end{table*}


Noteably, the actual quantization bit of \name\ is lower than baselines, indicating more efficient quantization, which we discuss in next paragraph, along with the practical query-key multiplcation speed comparsion.

\paragraph{Efficiency Comparison}
We discuss the quantization parameters required by each method and compare their practical running speeds, respectively.





(1) \textit{Quantization Parameters Analysis.}
We analyze the quantization parameter costs (e.g., the parameter amount of zeropoints and scales) of various quantization methods. 
Table~\ref{tab:para} reports the average bit width estimated using LongBench, as it provides a more accurate reflection of the actual bit usage of the baseline methods in a long-context scenario.

In group-wise quantization methods such as KIVI, the fine-grained group partition introduces additional quantization overhead.
For each group in KIVI, both the zero-point and the scale individually require 16 bits to store. 
For each channel in a single head, there are $T/~g$ groups, so the total quantization parameters is $2 \times 16 \times T \times d / ~g$, where $d$ represents the number of channels (i.e., the dimension of the attention head). 
Consequently, the total number of parameters for a single head can reach up to $32Td/~g$, resulting in a memory growth of the order $O(T)$.


\name\ has lower quantization costs for three reasons:

\textit{(i)} \name\ does not need to group tokens before quantization, thus eliminating the cost of group-wise overhead.

\textit{(ii)} The radius $r$ only has $d/2$ channels to quantize.

\textit{(iii)} The radius $r$ is non-negative. 
We can always take zero as the zero point, so there are no zero points to store.

Note that KIVI retains a residual length \(s\) of locally relevant key states in full precision since this is crucial for challenging tasks such as reasoning~\cite{kivi}. The window size is expected to be \(s/2\), which is significantly smaller than the long context length \(T\). Following KIVI, \name\ also retains a residual length of key states in full precision. This portion of the parameters occupies a smaller proportion of the overall quantization storage; 
therefore, this overhead is ngeligible in long-context scenario.
% therefore, this overhead is negligible in the above analysis.




(2) \textit{Query-Key Product Latency.}
We evaluate the latency of query-key multiplication on an NVIDIA A800-SXM4-80GB GPU, demonstrating significant speedups with our kernel implementation compared to baseline methods.  

Specifically, we record the wall-clock time of our custom query-key multiplication kernel. 
We test vector multiplication with a dimension of 128, a typical setting for 7B-parameter LLMs. 
The input sequence length $T$ varies up to 128K tokens.
We benchmark runtime by summing across 1000 iterations of the multiplication operation.
Our implementation is compared against PyTorch's FP16 matrix multiplication and the batch matrix multiplications used in other key-cache quantization methods, such as KIVI and KVQuant.
As shown in Table~\ref{tab:speed}, \name\ consistently accelerates multiplication speed across different lengths, achieving up to a $1.27\times$ speedup over baseline methods, which can further enhance overall throughput.  


\subsection{Exploration under the 3-Bit Precision}
\label{sec:3bit}
In this section, we investigate the use of \name~ for low-bit key cache quantization. 
Specifically, we present a variant of \name, named \name-m4n2, in which a 4-bit integer quantizes the angles and a 2-bit integer quantizes the radii, yielding a quantization effect equivalent to a 3-bit approach. 
We compare \name-m4n2 with other low-bit quantization methods that share a similar number of quantization parameters. 
Experimental results, shown in Table~\ref{tab:appendix}, indicate that \name~ achieves competitive performance with other baseline methods at 3-bit. 
% Future work will explore additional approaches to \name~in low-bit quantization.
In future work, we will further explore low-bit quantization schemes based on \name~.

\section{Conclusion}
In this paper, we view the outliers in the key cache of LLMs from a novel polar-coordinate-based perspective, which provides an efficient and effective solution, \name, to reduce the complexity and quantization costs in previous methods.
\name\ well preserves downstream performance even in long-context scenarios, comparable to previous works under 4-bit precision while achieving superior efficiency.
We hope the polar coordinate view can inspire the community to advance low-bit precision quantization techniques.



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}


\end{document}
