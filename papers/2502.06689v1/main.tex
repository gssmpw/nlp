\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\input{preamble}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Neumann eigenmaps for landmark embedding\\
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Shashank Sule}
\IEEEauthorblockA{\textit{Department of Mathematics} \\
\textit{University of Maryland, College Park}\\
College Park, United States of America \\
ssule25@umd.edu}
\and
\IEEEauthorblockN{Wojciech Czaja}
\IEEEauthorblockA{\textit{Department of Mathematics} \\
\textit{University of Maryland, College Park}\\
College Park, United States of America \\
wojtek@math.umd.edu}

}

\maketitle

\begin{abstract}
We present Neumann eigenmaps (NeuMaps), a novel approach for enhancing the standard diffusion map embedding using \emph{landmarks}â€”distinguished samples within the dataset. By interpreting these landmarks as a subgraph of the larger data graph, NeuMaps are obtained via the eigendecomposition of a renormalized Neumann Laplacian. We show that NeuMaps offer two key advantages: (1) they provide a computationally efficient embedding that accurately recovers the diffusion distance associated with the reflecting random walk on the subgraph, and (2) they naturally incorporate the Nystr\"om extension within the diffusion map framework through the discrete Neumann boundary condition. Through examples in digit classification and molecular dynamics, we demonstrate that NeuMaps not only improve upon existing landmark-based embedding methods but also enhance the stability of diffusion map embeddings to the removal of highly significant points.
\end{abstract}

\begin{IEEEkeywords}
Manifold learning, Diffusion maps, Spectral graph theory.
\end{IEEEkeywords}

\section{Introduction}
% Manifold learning algorithms (such as PCA, LLE, Diffusion maps, and their variants) are an indispensable tool for unsupervised learning in complicated real world data sets. However, often due to increasing sample size, the low-dimensional embedding obtained in these algorithms can quickly become computationally intractable due to an eigendecomposition step or simultaneous alignment of different local views. To fix this problem, there is a rich array of \emph{landmarking} methods for alleviating the computational complexity and more generally, for integrating distinguished data into the unsupervised learning problem \cite{landmark1, landmark2, landmark3, landmark4, landmark5, landmark6, landmark7}. In landmark methods, a small number of landmarks $V_S$ is selected from the pointcloud $V_G \subseteq \mathbb{R}^{m}$ such that $|V_S| \ll |V_G| =: n$. These landmarks may be chosen randomly or may represent significant points in the context of the data. The relevant dimensionality reduction technique is then applied efficiently to $S$, obtaining an embedding $\psi: V_S \to \mathbb{R}^d$ where $d < m$. Finally, $\psi$ is extended to all of $V_G$ using a computationally cheap out-of-sample extension algorithm. However, in this workflow, the rest of the data $V_G\setminus V_S$ is left out of the dimensionality reduction process. This begs the question of how to incorporate $V_G \setminus V_S$ within the dimensionality reduction algorithm while retaining the numerical speedup of computing an embedding on $V_S$. 
Manifold learning algorithms, such as principal component analysis, locally linear embeddings, diffusion maps, and their variants, are essential tools for unsupervised learning in sophisticated real-world datasets. However, the low-dimensional embeddings produced by these algorithms can become computationally intractable due to an eigendecomposition or alignment step which scales poorly with sample complexity. To address this challenge, a variety of \emph{landmarking} methods have been developed to reduce computational load and to incorporate distinguished data points into the unsupervised learning process \cite{landmark1, landmark2, landmark3, landmark4, landmark5, landmark6}.  

Typically, in landmarking methods, a subset of landmarks $ V_S $ is selected from the dataset $V_G := \{x_i\}_{i=1}^{n} \subseteq \mathbb{R}^m $, where $|V_S| \ll |V_G| = n$. These landmarks can be chosen either randomly or based on their significance within the dataset. The dimensionality reduction technique is then efficiently applied to $ V_S $, resulting in an embedding $ \psi: V_S \to \mathbb{R}^d $, where $ d < m $. Subsequently, $\psi $ is extended to the full dataset $ V_G $ using a computationally inexpensive out-of-sample extension algorithm. This raises the natural question of \emph{how to integrate the rest of the data $\delta S:= V_G \setminus V_S$ into the dimensionality reduction process while retaining the computational speedup of focusing on $V_S$}. This question is relevant even when computational complexity is not an issue: in particular, embeddings of $V_S$ that account for $\delta S$ may provide advantages over embeddings that do not. In this context, we may even \emph{switch} the roles of the landmarks: for instance, in \cite{landmarkdmap2} it was suggested that landmarks be $\delta S$ instead and inform the embedding of the rest of the set $V_S$. 

In this paper, we address this question specifically for the diffusion map (Dmap) algorithm by introducing Neumann maps (NeuMaps). In particular, by envisioning the overall data $V_G$ as the vertices of a graph, we propose that the subset $V_S \subseteq V_G$ be embedded via the normalized Neumann eigenvectors of the subgraph $S$ induced by $V_S$. The Neumann boundary condition generates the reflected random walk on subgraphs where the random walker may diffuse between landmarks either directly or via a reflection off the boundary vertices in $\delta S = V_G \setminus V_S$. The diffusion distance so defined by this random walk is thus additionally robust to perturbations due to data subsampling and emphasizes cluster structure due to the added within-cluster diffusion probability via reflection. 

\subsection{Related work} The Neumann spectrum of subgraphs and the Neumann Laplacian were established in the seminal works by Chung, Graham, and Yau \cite{sgt1, sgt2, sgt3} and later chronicled in the monograph by Chung \cite{SGTbook}. Here we study the properties of these eigenvectors as landmark embeddings. Integrating landmarks within the diffusion maps framework--and in particular using the removed data to inform landmark embeddings--is an active topic of research in manifold learning \cite{landmark0, landmarkdmap1, landmarkdmap2, landmarkdmap3, landmarkdmap4, landmarkdmap5}. 

\subsection{Contributions and organization} In this paper, we demonstrate the utility of the normalized Neumann eigenvectors of subgraphs as a landmark-based manifold learning technique. To this end, in Section 2 we briefly review the theory of Neumann eigenvectors of subgraphs from \cite{SGTbook}. In Section 3 we define Neumann eigenmaps as the eigenvectors of the reflecting random walk transition kernel; this definition allows us to prove an isometric embedding result for recovering a probability distance metric on the data. In Section 4 we provide numerical examples that benchmark NeuMaps against \emph{Roseland} \cite{landmark2} and Dmaps. 

% to illustrate that NeuMaps exhibits competitive performance in digit classification against the well-established landmark method \emph{Roseland} \cite{landmark2}, and can recover the key collective variable in the butane molecule, the dihedral angle, more accurately than Dmaps when using a small number of landmarks as $\delta S$. 

% In a related result from manifold learning, if the entire data $V_G$ is sampled i.i.d under the uniform measure from a compact Riemannian manifold $\mathcal{M}$ with boundary $\partial \mathcal{M}$, then the random walk Laplacian eigenfunctiosn (i.e the diffusion map) converge to the Laplace-Beltrami eigenfunctions with Neumann boundary conditions. Therefore, if the chosen subset $S$ represents samples from a submanifold $\mathcal{S} \subseteq \mathcal{M}$ then by including the discrete Neumann boundary value condition, our method can be considered as a way of integrating diffusion maps on $\mathcal{S}$ with their limiting property. 

% In more routine applications, however, $S$ is usually a set of landmarks and is scattered across the data manifold $\mathcal{M}$. Dmap has been adapted in several ways to account for the rem have bee

% it identified that the Neumann boundary condition generates the reflecting random walk on $S$ where the walk can jump from $x$ to $y$ either through an edge connecting them or by ``reflecting off" a shared boundary point. Therefore, returning to the context where $S$ is obtained after removing landmarks , the underlying idea This random walk is also seen in the $\alpha = 1/2$ case of the Diffusion Map where the random walk approximates the one produced by Fokker-Planck diffusion. Unlike the $\alpha = 1/2$ case, however, we make explicit use of the boundary condition $\partial_n u = 0$. Thus, the Nemann Map can also be viewed as a ``finitary" Fokker-Planck diffusion map. 

% These notes are organized as follows: the underlying theory of Dirichlet and Neumann eigenfunctions on subgraphs is discussed in Section \ref{sec: Neumann and Dirichlet Spectra}. We then use the notation and terminology from this section to define Neumann maps in Section \ref{sec: Neumann maps}. If you want to skip all this and go straight to the algorithm and some runs on test data check out Section \ref{sec: Algorithm and Results}. I lay out a few pressing questions in Section \ref{sec: Further questions}. Sections \ref{sec: Neumann and Dirichlet Spectra} and \ref{sec: Neumann maps} appeared in my undergraduate thesis.

\section{Neumann spectra of subgraphs}\label{sec: Neumann and Dirichlet Spectra}

We will consider the standard setup in diffusion maps where the data $V_G$ is used to construct a weighted graph where the edge weights are described by an affinity kernel. We therefore switch to using terminology on graphs. Moreover, note that if $A \in \mathbb{R}^{n\times m}$ and $X \subseteq [n], Y\subseteq [m]$ then $A[X,Y]$ denotes the submatrix obtained by selecting rows w.r.t in $X$ and columns w.r.t indices in $Y$. If $X = \{x\}, Y = \{y\}$ then $A[X,Y] = A[x,y]$, the $xy$\,th entry in $A$. 

\begin{definition}\label{def: graph}
Let $G = (V_G, E_G, W_G)$ be a finite weighted graph with vertices $V, |V| = n$, an adjacency matrix $W_{G} \in \mathbb{R}^{n \times n}$. Given an enumeration of $V_G$ we identify it with $[n]$. Then the edges are defined as $E_{G} = \{\{{x,y}\} \mid x, y \in V_G, w(x,y): = W[x,y] > 0\}$. 
\end{definition}
\begin{definition}\label{def: degree and laplacian}
    Let $\mathbf{1}_{|V_G|}$ be the vector of 1's in $|V_G|$ dimensions. The degree matrix of $G$ is given by $D_G = \textsf{diag}(W_G \mathbf{1})$ and the graph Laplacian is given by $L_G = D_G - W_G$. For any $x \in V_G$, we denote $d(x) := D_G[x,x]$. 
\end{definition}
\begin{definition}\label{def: subgraph info}
    Let $V_S \subseteq V_G$. Identifying $V_S$ with the corresponding subset of $[n]$, the subgraph induced by $V_S$ is given by $S = (V_S, E_{S}, W_S)$.
\end{definition}
\begin{definition}\label{def: degree info}
Let $V_S \subseteq V_G$ and $S = (V_S, E_S, W_S)$. The \emph{graph degree matrix} of $S$ is given by $T_{S} = D_G[V_S,V_S]$. Note that for any $x \in V_S$, we have $T_S[x,x] = d(x)$. 
\end{definition}

\begin{definition}\label{def: boundary info}
Let $S$ be a subgraph of $G$. We define the boundary edges $\partial S$ and boundary vertices $\delta S$ as: 
\begin{align}
    \partial S &= \{\{x,y\} \mid x \in V_S, y \in  V_G \setminus V_S, w(x,y) > 0\}, \\
    \delta S &= \{y \in V_G \setminus V_S \mid \exists \: e \in \partial S \text{ s.t. } y \in e\}. 
\end{align}
Moreover let $S^* = \partial S \cup E_S$. 
\end{definition}
\begin{definition}\label{ch2_def: Neumann eigenvalues}
Let $S \subseteq G$. Then the \textit{Neumann eigenfunction} $f^{N}_{1}: S \cup \delta S \to \mathbb{R}$ and $\textit{Neumann eigenvalue}$ $\lambda_{1}^{N}$ of $S$ are defined as follows: 
\begin{align}
    \lambda^{N}_1 &= \underset{f \mid_S\perp T_{S}1}{\text{min}} \frac{\sum_{\{x,y\} \in S^*}w(x,y)(f(x) - f(y))^2}{\sum_{x \in V_S}(f(x))^2 d(x)}\label{eq: Neumann eigenvalue}.
\end{align}
The minimizer in \eqref{eq: Neumann eigenvalue} will be termed $f^{N}_1$. In general, we may sequentially generate the $i$th Neumann eigenpair by constraining the minimization problem \eqref{eq: Neumann eigenvalue} the orthogonal subspace of $\textsf{span}\{f_1, \ldots, f_{i-1} \}.$
\end{definition}
% \begin{definition}\label{ch2_def: Dirichlet eigenvalues}
% Let $S \subseteq G$. Then the \textit{Dirichlet eigenfunction} $f^{D}_1$ and $\textit{Dirichlet eigenvalue}$ $\lambda^{D}_1$ of $S$ are defined as follows: 
% \begin{align}
%     \lambda^{D}_1 &= \underset{f\mid_{\delta S} \equiv 0}{\text{min}} \frac{\sum_{(x,y) \in S^{*}}(f(x) - f(y))^2}{\sum_{x \in V_S}(f(x))^2 d_x}, \\
%     f^{D}_{1} &= \underset{f\mid_{\delta S} \equiv 0}{\text{argmin}} \frac{\sum_{(x,y) \in S^{*}}(f(x) - f(y))^2}{\sum_{x \in V_S}(f(x))^2 d_x}.
% \end{align}
% In general, given the subspace of the first $i-1$ Dirichlet eigenfunctions denoted $D_{i-1}$, the $i$th Dirichlet eigenfunction and eigenvalue are defined as follows: 
% \begin{align}
%     \lambda^{D}_i &= \underset{f \perp D_{i-1}}{\text{min}} \frac{\sum_{(x,y) \in S^{*}}(f(x) - f(y))^2}{\sum_{x \in V_S}(f(x))^2 d_x}, \\
%     f^{D}_i &= \underset{f \perp D_{i-1}}{\text{argmin}} \frac{\sum_{(x,y) \in S^{*}}(f(x) - f(y))^2}{\sum_{x \in V_S}(f(x))^2 d_x}. \label{ch2_eq: Dirichlet equation}
% \end{align}
% \end{definition}

% \begin{remark}\label{ch2_rem: Courant fischer justification}
% Note that these definitions directly replicate the minimax equations that characterize eigenpairs of the graph Laplacian.  The eigenvalues $\lambda_{i}^{N}$ are the Rayleigh quotients of another symmetric operator given in \cite[p. 125]{SGTbook}. 
% \end{remark}

% While we explicitly choose Dirichlet eigenfunctions to have vanishing boundary conditions, the definition for Neumann eigenfunctions does not a priori assert a vanishing normal derivative condition. 

The following result proves that the Neumann eigenfunction satisfies a vanishing discrete normal derivative on the boundary $\delta S$:

\begin{theorem}[Lemma 8.1 in \cite{SGTbook}]\label{thm: Neummann eigenfunction main theorem}
Let $V_S \subseteq V_G$ and $L_{V^*}$ the Laplacian of the graph generated by $V^*=V_S \cup \delta S$. The Neumann eigenfunction $f = f_{1}^{N}$ satisfies the following properties: 
\begin{enumerate}
    \item Fix $x \in V_S$. Then 
    \begin{equation}
        L_{V^*}f(x) = \lambda_{N}^{1}d(x)f(x).\label{eq: Eigenvalue equation Neumann}
    \end{equation}
    \item Fix $x \in \delta S$. Then 
    \begin{equation}
        \sum_{\{x,y\} \in \partial S}w(x,y)(f(x) - f(y)) = 0. \label{eq: Neumann condition}
    \end{equation}
     % \item Let $h: V_S \cup \delta S \mapsto \RR$. Then
     % \begin{equation}
     %     \sum_{x \in V_S}h(x)L_{S\cup\delta S}f(x) = \sum_{(x,y) \in S^{*}}(h(x) - h(y))(f(x) - f(y)). \label{ch2_eq: Neumann bilinear form}
     % \end{equation}
\end{enumerate}
\end{theorem}

% An analogous result holds for Dirichlet eigenfunctions.

% \begin{theorem}\label{ch2_thm: Dirichlet eigenfunction main theorem}
% Let $V_S \subseteq V(G)$ and $S$ the induced graph. The Dirichlet eigenfunction $f = f_{1}^{D}$ satisfies the following properties: 
% \begin{enumerate}
%     \item Fix $x \in V_S$. Then
%     \begin{equation}
%         \sum_{(x,y) \in S^{*}}(f(x) - f(y)) = \lambda_{N}^{1}d_{x}f(x).\label{ch2_eq: Eigenvalue equation Dirichlet}
%     \end{equation}
%      \item Let $h: V_S \cup \delta S \mapsto \RR$ and $h(x) = 0$ for every $x \in \delta S$. Then
%      \begin{equation}
%          \sum_{x \in V_S}h(x)L_{S\cup\delta S}f(x) = \sum_{(x,y) \in S^{*}}(h(x) - h(y))(f(x) - f(y)). \label{ch2_eq: Dirichlet bilinear form}
%      \end{equation}
% \end{enumerate}
% \end{theorem}

% \begin{proof}
% We get to use simpler variations as the constraint on the minimization problem is different. For part (1), we fix $x_0 \in S$ and use the following variation: 
% \begin{gather}
%     f_{\epsilon}(x) = \begin{cases} f(x) + \epsilon \text{ if } x = x_0, \\ f(x) \text{ otherwise.} \end{cases}
% \end{gather}
% $f_{\epsilon}$ satisfies the vanishing boundary condition so we can compute the given Dirichlet quotient and then apply the same method as the proof for part (1) of Theorem \ref{ch2_thm: Neummann eigenfunction main theorem}. For part (2), notice that
% \begin{align*}
%          \sum_{x \in V_S}h(x)L_{S}f(x) &- \sum_{(x,y) \in S^{*}}(h(x) - h(y))(f(x) - f(y))\\
%          &= \sum_{\substack{(x,y) \in \partial S\\ y \in \delta S}}h(y)(f(x) - f(y))=0. \qedhere
% \end{align*}\end{proof}
% \begin{remark}\label{ch2_rem: diffs between Neumann Dirichlet}
% The differences between Theorems \ref{ch2_thm: Neummann eigenfunction main theorem} and \ref{ch2_thm: Dirichlet eigenfunction main theorem} are subtle but important. The first important difference is that while for the Neumann result the bilinear form in part (3) is true for \textit{any} function $h$ while the one in part (2) of the Dirichlet theorem is true only for functions $h$ vanishing on $\delta S$. Secondly, the proof for the Neumann condition involves perturbing $f$ on the boundary; as a consequence, we cannot use the same technique for the Dirichlet eigenfunctions, which are fixed at $0$ on $\delta S$. Lastly, both Dirichlet and Neumann functions satisfy the eigenvalue equation for $\mathcal{L}_{S \cup \delta S}$ on $S$. This does not mean that they are Laplacian eigenfunctions on $S \cup \delta S$. 
% \end{remark}

% Although the function $f$ in Equation \ref{ch2_eq: Neumann equation} does not satisfy an eigenvalue equation, the function $g = T^{1/2}f$ on $S$ does. Additionally, the vanishing normal derivative condition $\ref{ch2_eq: Neumann condition}$ changes to 
% \begin{equation}
%     \frac{1}{\sqrt{d'_{x}}}\sum_{\substack{(x,y) \in \partial S \\ y \in S}} \frac{g(x)}{\sqrt{d'_{x}}} - \frac{g(y)}{\sqrt{d_y}} = 0.\label{ch2_eq: Normalized Laplacian equation}
% \end{equation}
% As a consequence $g$ is a true eigenfunction of the normalized Laplacian with boundary conditions. Here we suggest an alternative formulation of these boundary problems in the language of operators. For the following definitions, assume $S \subseteq G$

% \begin{definition}\label{ch2_def: Dirichlet op}
% The \textit{Dirichlet Operator} (resp. \textit{Normalized Dirichlet Operator}), $D_{S}$ (resp. $\mathcal{D}_S$) is the restriction of $L_{G}$ (resp $\mathcal{L}_{G}$ to the rows and columns of $S$). 
% \end{definition}

% \begin{definition}\label{ch2_def: Boundary op}
% The \textit{boundary matrix} $B$ is a $|\delta S| \times |S|$ matrix where $B_{ij}$ is 1 if the $ith$ boundary vertex is connected to the $j$th vertex in $S$. 
% \end{definition}

% \begin{definition}\label{ch2_def: Boundary degree op}
% The \textit{boundary degree matrix} is the diagonal matrix $T^{\delta}_{S}$ where the $i$th diagonal entry is the degree of the $i$th boundary point into $S$.
% \end{definition}

% \begin{definition}\label{ch2_def: Neumann op}
% The \textit{Neumann Operator} (resp. \textit{Normalized Neumann Operator}), $N_{S}$ (resp. $\mathcal{N}_S$) are the following operators on the space of functions $f: S \mapsto \RR$: 
% \begin{align*}
%     N_S &= D_S - B^{\top}(T^{\delta}_{S})^{-1}B, \label{ch2_eq: Neumann op} \\
%     \mathcal{N}_S &= T_{S}^{-1/2}NT_{S}^{-1/2}. 
% \end{align*}
% \end{definition}

% \begin{remark}\label{ch2_rem: Neumann/Dirichlet make sense}
% If $f$ satisfies the Dirichlet condition on the boundary of $S$ then taking the Laplacian on any vertex of $S$ we get that 
% \begin{equation}\label{ch2_eq: Laplace equation with boundary terms}
%     \Delta_{G}f(x) = d_x f(x) - \sum_{y \in S \cup \delta S}f(y) = d_x f(x) - \sum_{y \in S}f(y).
% \end{equation}
% As a consequence, we can represent the action of the Laplacian on $S$ on a function with Dirichlet conditions by a matrix equal to the rows and columns of $L_{G}$ indexed by $S$. The Neumann situation is a bit more complicated. Suppose $f$ satisfies the Neumann condition on the boundary. Then equation \ref{ch2_eq: Laplace equation with boundary terms} still holds but the boundary terms are not zero. Instead, if $y \in \delta S$ then from the Neumann condition we have that $f(y) = (1/d'_{y})\sum_{z \sim y}f(z)$. We can substitute this back into equation \ref{ch2_eq: Laplace equation with boundary terms} and get that 
% \begin{align}
%     &\Delta_{G}f(x) = d_x f(x) - \sum_{y \in S \cup \delta S}f(y) \\
%     &= \underbrace{d_x f(x) - \sum_{\substack{y \in S\\ y \sim x}}f(y)}_{\text{Represented by $D$}} - \underbrace{\sum_{\substack{y \in \delta S\\y \sim x}}\frac{1}{d'_{y}}\sum_{\substack{z \sim y\\z \in S}}f(z)}_{\text{Represented by $B^{\top}(T^{\delta}_{S})^{-1}B$}}. 
% \end{align}
% \end{remark}

\begin{remark}
The condition \eqref{eq: Eigenvalue equation Neumann} shows that $f^{N}_1$ satisfies an eigenvalue equation at $x \in V_S$ for the \emph{ambient graph} $G$, while not necessarily being either a Laplacian eigenvector of the graph $S$ or $G$. Moreover, condition \eqref{eq: Neumann condition} can be viewed as a vanishing discrete normal derivative condition. 
\end{remark}

\section{Neumann Maps}\label{sec: Neumann maps}
The quotient \eqref{eq: Neumann eigenvalue} modifies the usual normalized Rayleigh quotient of the graph Laplacian on $S$ by penalizing large variations between $V_S$ and $\delta S$, thus motivating the use of $f^{N}_i$ as feature maps specifically adapted for smooth extensions to $\delta S$. We now show that the minimizers of \eqref{eq: Neumann eigenvalue} can be computed via the eigenvectors of the \emph{Neumann Laplacian} $L^{N}_{S}$. First, we define additional terms below. 
\begin{definition}
    The \emph{boundary operator} is defined as $B_S = W_{G}[V_{G}\setminus V_S, V_S]$ and the boundary degree matrix is defined by $T^{\delta}_{S} = \textsf{diag}(B_{S}\mathbf{1}_{|V_S|}) \in \mathbb{R}^{|\delta S| \times |\delta S|}$. Here $\mathbf{1}_{|V_S|} \in \mathbb{R}^{|V_S|}$. 
\end{definition}
The boundary operator can be used to define a \emph{Neumann Laplacian}:
\begin{definition}
     Let $S= (V_S, E_S, W_S)$ be a subgraph of $G$ and $L^{D}_{S} := L_{G}[V_S,V_S]$. Moreover, let $B_S$ be the boundary matrix. Then the Neumann Laplacian, $L^{N}_{S}$, is defined as follows: 
     \begin{align}
          L^{N}_{S} := L^{D}_{S} - B^{\top}(T^{\delta}_{S})^{-1}B.
     \end{align}
\end{definition}
The following proposition demonstrates how the Neumann Laplacian reformulates conditions \eqref{eq: Neumann condition} and \eqref{eq: Eigenvalue equation Neumann} into one eigenproblem: 
\begin{proposition}
\label{prop: Neumann laplacian}
Let $v: V_S \cup \delta S \to \mathbb{R}$ satisfy the Neumann condition \eqref{eq: Neumann condition} on $\delta S$ and $u = v\mid_{V_S}$ be its restriction to $S$. Then $v$ satisfies the Laplacian eigenvalue condition \eqref{eq: Eigenvalue equation Neumann} if and only if $u$ satisfies: 
    \begin{align}
        \lambda_{1}^{N}T_{S}u = L^{N}_{S}u. \label{eq: Neumann Laplacian eigenproblem}
    \end{align}
\end{proposition}
One may thus obtain the Neumann eigenvectors by computing an eigenvector of $L^{N}_{S}$ and then extending it to $\delta S$ via the Neumann condition \eqref{eq: Neumann condition}. Then from the converse of Proposition \eqref{prop: Neumann laplacian} the resulting vector on $V_S \cup \delta S$ is a Neumann eigenfunction. More importantly, the renormalized Neumann Laplacian $\mathcal{N} := T_{S}^{-1/2}L^{N}_{S}T_{S}^{-1/2}$ is similar to a random walk matrix:

\begin{proposition}\label{prop: Neumann random walk matrix is a random walk matrix}
Let $R = T_{S}^{-1/2}(I - \mathcal{N})T_{S}^{1/2}$. Then $R\textbf{1} = \textbf{1}$ and the entries of $R$ are all non-negative. 
\end{proposition}

We can now define a Neumann map by diagonalizing $R$ via the spectral decomposition of $\mathcal{N}$:  $$R = T_{S}^{-1/2}(I - \mathcal{N})T_{S}^{1/2} = T_{S}^{-1/2}U\Sigma U^{\top}T_{S}^{1/2} = V\Sigma Y^{\top}$$
where $V = T_{S}^{-1/2}U$ and $Y = T_{S}^{1/2}U$. Since $Y^{\top}V = I$, for any $t \geq 0$ we have $$R^{t} = \sum_{i=1}^{n}\sigma_{i}^{t}v_{i}y^{\top}_{i}$$ so in particular expanding the column space of $R^t$ we get
\begin{align}
   R^{t}[:,x_i] = \sum_{j=1}^{n}\sigma_{j}^{t}v_j y_{j}^{\top}(x_i). \label{eq: column space decomposition} 
\end{align}

\begin{definition}\label{ch3_def: Neumann diffusions}
Consider the column space decomposition of $R^{t}[:,i]$ in \eqref{eq: column space decomposition}. The $d$-dimensional Neumann map of vertex $i \in S$ is the following point in $\RR^{d}$:
\begin{align*} g^{d}_{t}(x_i) = \begin{bmatrix}\sigma_{2}^{t}y_{2}^{\top}(x_i) \\ \vdots \\ \sigma_{d+1}^{t}y_{d+1}^{\top}(x_i)\end{bmatrix}.\end{align*}
\end{definition}

\begin{theorem}\label{thm: Main theorem of Neumann diffusions} 
Let $X_t$ be the random walk defined by $R$ and $p^{t}_{ji} := P(X(t) = i \mid X(0) = j)$ be the probability of walking to vertex $i$ after starting at vertex $j$ after $t$ steps. Then
\begin{align}
&\sum_{j=1}^{|S|}(p^{t}_{ji} - p^{t}_{jk})d(x_j)^{-1} = ||g^{|S|}_{t}(x_{i}) - g^{|S|}_{t}(x_{k})||^2 .
\end{align}

\end{theorem}

\begin{remark}\label{ch3_rem: reflecting walks are neumann walks} The matrix $R$ is the transition matrix of the reflecting random walk mentioned in \cite{SGTbook} wherein if $u,v \in S$ then one may walk from $u$ to $v$ either directly or via a reflection off some mutually adjacent $x \in \delta S$. This random walk may be contrasted to the one in Roseland which considers random walks between vertices in $S$ \emph{only} via walks through $\delta S$. Therefore, the choice to include within-$S$ diffusion defines the conceptual difference between NeuMaps and Roseland. Moreover, note that $L^{N}_{S}$ is a perturbation of $L^{D}_{S}$, the Dirichlet graph Laplacian (see \eqref{eq: Neumann Laplacian eigenproblem}). Therefore, in theory, alternate Neumann Laplacians could be obtained by replacing $L^{D}_{S}$ with any user-chosen operator such as the Schr\"odinger or transport operator on the subgraph $S$ \cite{landmark0, landmarkdmap4, landmarkdmap5}. 
\end{remark}

% \begin{remark}
% For graphs generated from data, the edge weights are described by a renormalized kernel $k_{\epsilon}(x,y)$. Here $\epsilon$ is the kernel bandwidth controlling the pairwise affinity kernel. This enables NeuMaps to be a straightforward modification to any variant of Dmap featuring kernel reweighting \cite{kernelreweighting1, kernelreweighting2, kernelreweighting3, kernelreweighting4}by introducing an additional Neumann boundary condition to the Laplacian eigenvectors of the subgraph. 
% \end{remark}

\begin{remark} The Neumann condition gives a natural way to extend functions from $V_S$ to $V_S \cup \delta S$. In particular, if $\mathcal{N}g = \lambda g$ then defining $f = T^{1/2}g$ we have $L^{N}_{S}g = \lambda T_{S}g$. To make $f$ a Neumann eigenvector, we set its discrete normal derivative to zero, which corresponds to setting $f(x) = d(x)^{-1}\sum_{y \in V_S}w(x,y)f(y)$ for $x \in \delta S$. By rescaling, we get the following formula for extending $g$: 
    \begin{align}
        g(x) = \sum_{y \in V_S}\frac{w(x,y)}{\sqrt{d(x)d(y)}}g(y). \label{eq: extension formula}
    \end{align}
The extension of $g$ via \eqref{eq: extension formula} is therefore a multiple of its Nystr\"om extension \cite{nystrom}. Consequently, the Neumann condition naturally accounts for Nystr\"om extension, giving an additional justification for using NeuMap. 
\end{remark}

% \begin{remark}
% In \cite{SGTbook} it is shown that the quadratic form \eqref{eq: Neumann eigenvalue} corresponds to the generalized Rayleigh quotient of another symmetric operator given by $N^{\top}L_{G}N$, where $N^{\top} = [I_{S}\: B^{\top}_S(T^{\delta}_{S})^{-1}]$. However, here we remark that this operator \emph{is not the same} as $L^{N}_{S}$.
% \end{remark}

\section{Numerical experiments}

% \subsubsection{Spectral partitioning vector}
% The spectral partitioning (SP) vector $v_{SP}$--the first non-trivial eigenfunction of the normalized graph Laplacian--represents a relaxation of the Normalized Cut (NCut) problem and is therefore used to detect nonlinear cluster structure in data modeled by a random geometric graph. In the simplest case, thresholding $v$ at some value $\eta$ gives a partitioning of the data and thus an unsupervised algorithm for learning a two-cluster structure. Here we demonstrate that the first nontrivial eigenfunction of the normalized Neumann Laplacian $v_{NSP}$, which we term the Neumann spectral partitioning (NSP) vector, can also be used to cluster data and, in particular, may be more favorable compared to the SP eigenvector given the removal of only a small number of landmarks. We illustrate this on a 2D imbalanced dataset modeled by two Gaussian clusters parameterized by $X_1 \sim \mathcal{N}(0, 1.5I_2)$ and $X_2 \sim \mathcal{N}(4, 0.1I_2)$. We sample 1000 points from $X_1$, 100 points from $X_2$ and choose 20 random points as labeled. Then we compute $v_{SP}$ and $v_{NSP}$ for the remaining 1080 points. For building the Gaussian kernel, we use $\epsilon = 1.0$. We summarize one such experiment in Figure \eqref{fig: gaussian clusters}, noting that the values of $v_{SP}$ and $v_{NSP}$ both display a bimodal structure and can be used to separate the original 2D data. However, in the bottom right panel of Figure \eqref{eq: gaussian clusters}, it is seen that the modes of $v_{NSP}$ are more well-separated than those of $v_{SP}$. This represents a decisive advantage of Neumann maps over Laplacian eigenmaps as a feature map since increased mode separation in feature space is linked to higher accuracy on tasks such as clustering or classification. 

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{Gaussian clusters.png}
%     \caption{Comparing spectral clustering using Laplacian vs Neumann eigenmaps. Top left: original data and labels. Top right: SP vector. Bottom left: NSP vector corresponding to the first non-trivial Neumann eigenvector. Bottom right: Density of values in the SP vs NSP vectors. Note that both SP vectors are bimodal as expected but the modes of the Fiedler Neumann vector are more well-separated than those of the Fiedler vector}
%     \label{fig: gaussian clusters}
% \end{figure}

% To probe the question of mode separation further, we undertake the following experiment. For a given landmark removal rate of $\alpha \in (0,0.5)$, we randomly sample a vector remove $\alpha n$ points from the original two-cluster dataset $V_G$, resulting in the landmarks $\delta S$ and the landmark-removed data denoted by $S$. Then we compute $v_{SP}$ and $v_{NSP}$ on $S$. Since we have true labels $y$ for this data, we may consider the empirical mean of the projected cluster $\mu^{SP}_1 = \text{mean}(\{v_{SP}(x_i)\}_{x_i \in C_1})$ and $\mu^{SP}_2 = \text{mean}(\{v_{SP}(x_i)\}_{x_i \in C_1})$. Then $D_{SP} := |\mu^{SP}_1 - \mu^{SP}_2|$ (resp. $D_{NSP}$), is a measure for how far separated the modes of the two clusters are after projecting down the one-dimensional feature space of the SP (resp. NSP) vector. Similarly, we can also consider the entropies of each projected cluster given by constructing a histogram of $\{v_{SP}(x_i)\}_{x_i \in C_k}$ (for $k=1,2$) and then computing the entropies $H_{k}^{SP}$ (resp. $H^{NSP}_{k}$) for $k=1,2$ of the resulting probability density from the histogram. Note that $H_{k}^{SP}$ (resp. $H^{NSP}_{k}$) describe the uncertainty within the projected cluster and can therefore be considered a measure of the cluster spread. Since the landmarks are chosen randomly, we consider a 100 trials of this same experiment and compute an average of cluster mean separation and cluster entropies. Ideally, better clustering algorithm should on average have higher cluster mean separation (clusters are farther apart) and lower cluster entropies (clusters are tighter). We see exactly this when comparing Neumann vs Laplacian eigenmaps in Figure \eqref{fig: cluster_stats}. In particular, as we assign more points to be landmarks, the cluster means separate further and cluster spread gets tighter for $v_{NSP}$ as compared to $v_{SP}$. Moreover, even at somewhat modest 5\% landmark label rate, the mode separation is much better for $v_{NSP}$. 

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{Cluster_stats.png}
%     \caption{Top: Tracking $\mathbb{E}_{\omega}[D_{SP}]$ vs $\mathbb{E}_{\omega}[D_{SP}]$ over labeling rate. Here $\omega$ is the random variable given by random selection of $\alpha n$ points as landmarks. Middle: Tracking $\mathbb{E}_{\omega}[H^{SP}_{1}]$ measuring the spread of the larger cluster. Bottom: Tracking $\mathbb{E}_{\omega}[H^{SP}_{2}]$ measuring the spread of the smaller cluster.}
%     \label{fig: cluster_stats}
% \end{figure}
In the examples below, we adopt the paradigm where the landmarks are $\delta S$ and the rest of the set $V_S$ is embedded using NeuMap. We remark that in both examples below, $|\delta S| \leq |V_G \setminus \delta S|$. Thus, the computational acceleration of using NeuMap instead of Dmap will be modest. However, here our goal is not necessarily to emphasize the computational efficiency but the more favourable geometric and structural properties of the NeuMap embedding. 
\subsubsection{Digit classification and Roseland}
We consider the UCI digits dataset of 1083 $16 \times 16$ images of handwritten digits from $0-6$. We randomly select 25\% of the data as landmarks. As a comparison, we use the Roseland algorithm. We tune the bandwidth $\epsilon$ according to the max-min criterion in \cite{datafold}. To keep the comparisons consistent, we use the same $\epsilon$ for computing Neumann maps. After projecting in 2D we assign clusters using $k$-means and measure the cluster assignments using the normalized mutual information (NMI) and  unsupervised clustering accuracy (ACC) against the true labels. For Roseland, the NMI and ACC were 0.71 and  84\% respectively while for Neumann maps the NMI and ACC were 0.85 and 93\% respectively. Thus, Neumann maps outperforms Roseland on this task. We further visualize our results in Figure \ref{fig: digits} where we observe that Roseland by and large successfully separates the data in 6 clusters. However, in Neumann maps the clusters are significantly more concentrated, likely contributing to the higher accuracy of $k$-means on this embedding. An interesting artefact of the Neumann embedding seems to be the presence of few but significant outliers which are rather far apart from the cluster mean. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{digits_embedding.png}
    \caption{Left: Embedding UCI digits with 25\% random landmarking via Roseland. Right: Embedding UCI digits with Neumann maps.}
    \label{fig: digits}
\end{figure*}

\subsubsection{Learning collective variables in molecular dynamics}

In the analysis of data from molecular dynamics (MD) simulations, a highly important task is the recovery of optimal \emph{collective variables}, i.e low dimensional features of the high dimensional atomic coordinates \cite{cvsimportant}. Criteria for optimality of such CVs specifically geared for MD include accurate representations of the dynamical properties of the system \cite{cvsdynamical1, cvsdynamical2, cvsdynamical3}, reproducibility of transition rates and reaction channels in low dimensions \cite{cvsrr1, cvsrr2}, and ability to drive enhanced sampling simulations. 

% MD-specific hindrances to recovering CVs include the presence of undersampled transition states \cite{transitionstate}, slow-and-fast timescales in the underlying dynamics \cite{slowfast}, presence of group invariance \cite{equivariance}, and anisotropic diffusion in the projected space \cite{anisotropic}. 
While several algorithms for learning CVs have been proposed \cite{cv1, cv2, cv3, cv4, cv5}, the Fokker-Planck eigenfunctions--computable via Dmap--stand out due to their simplicity and their optimal embedding properties in a probability distance metric \cite{fp1, fp2}. We now show that given certain marked points, the Neumann Fokker-Planck eigenfunctions can also be used to learn CVs and visualize MD data. In particular, we demonstrate that they may learn the underlying collective variable \emph{more accurately} than Fokker-Planck (FP) eigenfunctions using diffusion maps. We empirically illustrate this in the case of the butane ($C_4 H_{10}$) molecule, widely used as a toy model for configurational changes in small molecules. In particular, it is well-known that the dynamics of the butane molecule, while residing in a 42-dimensional space, are accurately coarse-grained by the one-dimensional \emph{dihedral angle} $\theta$ in its carbon backbone. The dihedral angle characterizes the two metastable states of the butane molecule: the anti configuration given by $\theta \approx \pi$ and the gauche configurations given by $\theta \approx \pi/3, 5\pi/3$. The atomic coordinates $X_t \in \mathbb{R}^{14 \times 3}$ largely inhabit the metastable states and transition rarely between them. Additional enhanced sampling algorithms such as metadynamics may be used to generate more samples in the transition regions. To this end, we simulate a metadynamics trajectory for $X_t$ in OpenMM using the Langevin integrator at 300K and collect $10^4$ points given by $\{X_{i\Delta t}\}_{i=1}^{10^4}$ where the integrator timestep $\Delta t = 0.04 $ps. In order to apply Neumann maps to this data, we consider the following ways to obtain marked and unmarked data: (1) Subsampling uniformly in time (i.e every 10th point is labeled as marked; the rest are unmarked), (2) Marking metastable states i.e points corresponding to $|\theta - \pi| \leq 0.2$ and $|\theta - \pi/3| < 0.1, |\theta - 5\pi/3| < 0.1$ are labeled as marked, and (3) Subsampling uniformly in ambient space, where we select a $\delta$-net such that all points are within distance $\delta$ of this net. In all three cases, approximately 10\% of the points are marked. We detail our results in Figure \ref{fig: butane}. In the top panel, we conisder the subsampling-uniformly-in-time scheme. The first FP eigenfunctions $\psi_1$ computed using Dmap and NeuMap both correlate significantly with $\theta$. To measure this correlation, we model $\theta$ as a linear function of $\psi_1$ and observe a standard error (SE) of $2.8 \times 10^{-4}$ for $\psi_1$ computed via NeuMap against a much higher SE of $1.1 \times 10^{-2}$ for $\psi_1$ computed via Dmap, thus showing that NeuMap recovers the dihedral angle more accurately. By removing metastable states (middle) and removing $\delta$-nets (bottom)--we find that the performance of the Dmap degrades significantly, thus revealing that these points were instrumental in producing the original embedding that recovered the dihedral angle. However, when computing with NeuMap--presumably due to the robustness provided by reflection against the removed points--the embedding stays relatively unperturbed. Therefore, by incorporating deleted points yet retaining the computational efficiency of using fewer samples, NeuMap makes Dmap stable to erasures. 

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{butane_angle_rec.png}
    \caption{Top to bottom: FP eigenfunction embeddings of data after removal of landmarks provided by subsampling uniformly in time, choosing metastable states, and uniformly subsampling in space via $\delta$-nets.}
    \label{fig: butane}
\end{figure}
\section{Conclusion}
We have introduced NeuMap which enhances the Dmap embedding with landmarks and the Neumann boundary condition. The embedding so obtained recovers the diffusion distance of the reflecting random walk, accelerates the Dmap process via the eigendecomposition of a smaller matrix, and exhibits favourable performance over Roseland and Dmap in proof-of-concept examples.
\clearpage
\input{references}

\section*{Appendix}
Here we state the proofs of the results stated in the main paper. 

\subsection{Proof of theorem \eqref{thm: Neummann eigenfunction main theorem}}
We prove parts (1) and (2) variationally. For part (1),  fix $x_0 \in V_S$, define $vol(S) := \sum_{x \in V_S}d(x)$ and let 
    \begin{align*}f_{\epsilon}(x) = \begin{cases}f(x_0) + \frac{\epsilon}{ d_{x_0}} \text{ if } x=x_0, \\ f(x) - \frac{\epsilon}{ vol(S) - d_{x_0}}. \end{cases}\end{align*}
    First observe that $f_{\epsilon}\mid_{S} \perp T1_S$ so the minimization problem is well-defined on $f_{\epsilon}$. Now we compute the quotient for $f_{\epsilon}$:
    \begin{gather*}
        R(\epsilon) = \frac{\sum\limits_{(x,y) \in S^{*}}w(x,y)(f_{\epsilon}(x) - f_{\epsilon}(y))^2}{\sum\limits_{x \in V_S}(f_{\epsilon}(x))^2 d_x}\\
        = \frac{\sum\limits_{(x,y) \in S^{*}, x \neq x_0}w(x,y)(f(x) - f(y))^2}{\sum\limits_{x \neq x_0}(f(x) - \frac{\epsilon}{\text{vol}(S) - d_{x_0}})^2d_{x_0} + (f(x_0) + \frac{\epsilon}{d_{x_0}})^2d_{x_0}}\\
        + \frac{\sum\limits_{(x_0,y) \in S^{*}}w(x_0,y)(f(x) +  \frac{\epsilon}{d_{x_0}} - f(y) + \frac{\epsilon}{\text{vol}(S) - d_{x_0}}))^2}{\sum\limits_{x \neq x_0}(f(x) - \frac{\epsilon}{\text{vol}(S) - d_{x_0}})^2d_{x_0} + (f(x_0) + \frac{\epsilon}{d_{x_0}})^2d_{x_0}} \\
         = \frac{\sum\limits_{(x,y) \in S^{*}}w(x,y)(f(x) - f(y))^2}{\sum\limits_{x \in V_S}(f(x))^2 d_{x} + \frac{2\epsilon f(x_0)d_{x_0}\text{vol}(S)}{d_{x_0}(vol(S) - d_{x_0})} + O(\epsilon^2)}. \\
        + \frac{ \frac{2\epsilon\text{vol}(S)}{d_{x_0}(\text{vol}(S) - d_{x_0})}\sum\limits_{(x_0,y) \in S^*}w(x_0,y)(f(x_0) - f(y)) + O(\epsilon^2)}{\sum\limits_{x \in V_S}(f(x))^2 d_{x} + \frac{2\epsilon f(x_0)d_{x_0}\text{vol}(S)}{d_{x_0}(vol(S) - d_{x_0})} + O(\epsilon^2)}.
    \end{gather*}
The second equality follows after simplifying the algebra and noting that $\sum\limits_{x \in V_S}f(x)d_{x} = 0$. We know that when $\epsilon = 0$, $f_{\epsilon} = f$, which also minimizes $R(\epsilon)$. Thus, $R'(0) = 0$ so computing the derivative via the quotient rule and setting the numerator at $\epsilon = 0$ to zero, we get that 
\begin{align}
    &\Big(\frac{2\text{vol}(S)}{d_{x_0}(\text{vol}(S) - d_{x_0})}\cdot \\
    &\sum\limits_{(x_0,y) \in S^*}w(x_0,y)(f(x_0) - f(y))\Big)\sum\limits_{x \in V_S}(f(x))^2 d_{x}\\
    &- \Big(\frac{2f(x_0)d_{x_0}\text{vol}(S)}{d_{x_0}(\text{vol}(S) - d_{x_0})}\Big)\sum\limits_{(x,y) \in S^{*}}w(x,y)(f(x) - f(y))^2 = 0.
\end{align}
Rearranging the equation, dividing through by $\sum\limits_{x \in V_S}(f(x))^2 d_{x}$ and noting that 
$$\frac{\sum\limits_{(x,y) \in S^{*}}(f(x) - f(y))^2}{\sum\limits_{x \in V_S}(f(x))^2 d_{x}} = \lambda_{1}^{N}$$

gives us part (1). For part (2), we adopt a similar strategy but the variation is simpler. Fix $x_0 \in \delta S$ and set 
\begin{align*} f_{\epsilon}(x) = \begin{cases} f(x) + \epsilon \text{ if } x = x_0, \\ f(x) \text{ otherwise. } \end{cases}\end{align*}
Now compute the Neumann quotient and observe that we can separate the sum in the numerator over edges that connect to $x_0$ and those that don't. By definition, the edges that connect with $x_0$ are contained in $\partial S$ so 
\begin{align*}
R(\epsilon) &= \frac{\sum\limits_{(x,y) \in S^{*}}w(x,y)(f_{\epsilon}(x) - f_{\epsilon}(y))^2}{\sum\limits_{x \in V_S}(f_{\epsilon}(x))^2 d_x} \\
&= \frac{\sum\limits_{(x,y) \in S^{*}}w(x,y)(f(x) - f(y))^2}{\sum\limits_{x \in V_S}(f(x))^2 d_x} \\
&+ \frac{2\epsilon\sum\limits_{(x_0,y) \in \partial S}w(x_0,y)(f(x_0) - f(y)) + O(\epsilon^2)}{\sum\limits_{x \in V_S}(f(x))^2 d_x}.
\end{align*}
Once again, taking the derivative with respect to $\epsilon$ and setting it $0$ at $\epsilon = 0$ yields (2). 
\subsection{Proof of proposition \ref{prop: Neumann laplacian}}
Note that $v$ satisfies the Neumann condition if and only if for every $y \in \delta S$, 
\begin{align}
    f(y) = \frac{1}{\partial d(y)}\sum_{z \in V_S}w(y,z)v(z). \label{eq: neumann condition rephrased as boundary average}
\end{align} Here $\partial d(y) = T^{\delta}_{S}[y,y]$. To prove the equivalence stated, we compute the action of $L_{V^*}$ on $v$: 
\begin{align}
    &L_{V^*}v(x) = d(x)v(x) - \sum_{y \in V^*}w(x,y)v(y) \\
    &= d(x)v(x) - \sum_{y \in V}w(x,y)v(y) - \sum_{y \in \delta S}w(x,y)v(y) \\
    &= d(x)v(x) - \sum_{y \in V}w(x,y)v(y) - \sum_{y \in \delta S}\frac{w(x,y)}{\partial d(y)}\sum_{z \in V_S}w(y,z)v(z) \\
    &= d(x)u(x) - \sum_{y \in V}w(x,y)u(y) - \sum_{y \in \delta S}\frac{w(x,y)}{\partial d(y)}\sum_{z \in V_S}w(y,z)u(z) \\
    &= L_{S}^{N}u(x). 
\end{align}
Here the third equality follows by plugging in \eqref{eq: neumann condition rephrased as boundary average} and the fourth equality follows by noting that $v\mid_{V_S} = u$. Clearly, $L_{V^*}v(x) = \lambda d(x)v(x)$ if and only if $L_{S}^{N}u(x) = \lambda d(x)v(x)$. This proves our assertion. 

\subsection{Proof of proposition \ref{prop: Neumann random walk matrix is a random walk matrix}}

This follows because $\mathcal{N}$ admits 
$T^{1/2}\textbf{1}$ as a zero-eigenvector:

\begin{align*}R\textbf{1} = T_{S}^{-1/2}(I - \mathcal{N})T_{S}^{1/2}\textbf{1} = I\textbf{1} - T_{S}^{-1/2}\mathcal{N}T_{S}^{1/2}\textbf{1} = \textbf{1}.\end{align*}
To see the non-negativity of the entries, we expand $N$ in terms of the Dirichlet and Boundary operators:
\begin{align*}
    R &= T_{S}^{-1/2}(I - \mathcal{N})T_{S}^{1/2}\\
    &= I - T_{S}^{-1/2}\mathcal{N}T_{S}^{1/2}\\
    &= I - T_{S}^{-1}(L_G[V_S, V_S] - B^{\top}(T^{\delta}_{S})^{-1}BT_{S}^{-1/2})\\
    &= I - T_{S}^{-1}L_G[V_S, V_S] + T_{S}^{-1}B^{\top}(T^{\delta}_{S})^{-1}B \\
    &= I - T_{S}^{-1}(T_{S} - W_G[V_S, V_S]) + T_{S}^{-1}B^{\top}(T^{\delta}_{S})^{-1}B \\
    &= T_{S}^{-1}W_G[V_S, V_S] + T_{S}^{-1}B^{\top}(T^{\delta}_{S})^{-1}B
\end{align*}
Clearly the above matrix has nonnegative entries. Thus $R$ is row-stochastic. 
\subsection{Proof of \ref{thm: Main theorem of Neumann diffusions}}
We express the left hand side in matrix form and then compute:
\begin{align*}
    &\sum_{j=1}^{n}(P(X(t) = j\mid X(0) = i) - P(X(t) = j \mid X(0)= k))^2\frac{1}{d_j}\\
    &= (R^t \delta_{i} - R^t \delta_{k})^{\top}T^{-1}_{S}(R^t \delta_{i} - R^t \delta_{k})\\
    &= (R^t(\delta_{i} - \delta_{k}))^{\top}T^{-1}_{S}(R^t(\delta_{i} - \delta_{k}))\\
    &= (\delta_{i} - \delta_{k})^{\top}(R^t)^{\top}T^{-1}_{S}R^t(\delta_{i} - \delta_{k})\\
    &= (\delta_{i} - \delta_{k})^{\top}(C\Sigma^t B^{\top})^{\top}T^{-1}_{S}C\Sigma^t B^{\top}(\delta_{i} - \delta_{k})\\
    &= (\delta_{i} - \delta_{k})^{\top}B\Sigma^t C^{\top}T^{-1}_{S}C\Sigma^t B^{\top}(\delta_{i} - \delta_{k})\\
    &= (\delta_{i} - \delta_{k})^{\top}B\Sigma^t {\underbrace{W^{\top}T^{1/2}T^{-1}T^{1/2}W^{\top}}_I}\Sigma^t\Psi^{\top}(\delta_{i} - \delta_{k})\\
    &= (\Sigma^t B^{\top}(\delta_{i} - \delta_{k})^{\top})^{\top}\Sigma^t B^{\top}(\delta_{i} - \delta_{k})\\
    &= ||g_{t}(i) - g_{t}(k)||^2. 
\end{align*}
\section{Data and code availability}
The data and code for our numerical experiments has been made available at \url{https://github.com/ShashankSule/Neumann_maps/tree/pub}. 
\end{document}
