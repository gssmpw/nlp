\subsection{Learning rate schedules}\label{subsec:lr_schedules}

LR schedules have a long history in stochastic optimization, and are
motivated by convergence bounds for stochastic gradient
methods~\citep{moulines2011non,bottou2018optimization}.  For example,
following \citet{andriushchenko2023why}, consider SGD for a convex
loss parameterized by $\theta$: with a constant LR $\eta$, the gap
between the optimum and current loss at step $t$ can be bounded by:
\begin{equation}\label{eqn:biasvar}
\E[\Ls(\theta_t) - \Ls(\theta_*)] \le
\textcolor{TealBlue}{(1 - \eta\mu)^t ||\theta_0 - \theta_*||^2 }
+ \textcolor{orange}{\eta \sigma^2}
\end{equation}
where $\theta_0$ are initial parameters, $\theta_*$ is the loss
minimizer, $\sigma^2$ is a bound on the variance of gradient noise,
and $\mu$ is a measure of the objective's curvature (see also
\citet[Theorem~4.6]{bottou2018optimization}).  A larger LR can
decrease dependence on initial conditions (the
\textcolor{TealBlue}{\emph{bias}} term), but also increase the effect
of gradient noise (the \textcolor{orange}{\emph{variance}} term).
%
As training progresses, bias decreases exponentially in $t$, and the
relative importance of variance increases.
\citet{bottou2018optimization} note this motivates a strategy where LR
is high initially (to mitigate bias) and lowered later (to minimize
variance).

In practice, when and how to reduce the LR is rarely informed by ML
theory.
%
Many LLMs simply follow the 10x cosine schedule, which is noted to
work slightly better than cosine $\dtoz$ in the influential work of
\citet{hoffmann2022empirical}.
%
It is also well established that using a portion of a longer (or
extending a shorter) schedule is suboptimal compared to using a
schedule that reaches its minimum only at the final training
step~\citep{li2019budgeted,hoffmann2022empirical,hu2024minicpm,hagele2024scaling}.
%
%\citet{hoffmann2022empirical} adopt cosine decay, and report that
%``the difference between decaying by 10 and decaying to 0.0 ... is
%small, though decaying by a factor of 10 to be slightly more
%performant,'' while ``decaying by less (5) is clearly worse.''
%
Linear decay after warmup~\citep{howard2018universal} has been used in
LLMs, typically also to 10\% of the
peak~\citep{henighan2020scaling,dey2023cerebras,sengupta2023jais}.
%
Dropping LR at specific milestones (\emph{step decay}) is popular in
vision models~\citep{he2016deep,zagoruyko2016wide,li2020reconciling},
but has also been used in LLMs~\citep{bi2024deepseek}.
% do drop the LR in stages.  Seems to be popularized with
% \citet{zagoruyko2016wide}.  Although \citet{andriushchenko2023why}
% (who use it) cite ``step-decay as LR schedule'' from .  ``Step
% Decay, one of the most commonly-used learning rate
% schedules''~\citep{li2020reconciling}

\citet{kaplan2020scaling} compared various decay functions and
concluded the specific schedule was unimportant given a high enough
average LR, although decaying to zero ``gives a fixed improvement
close to the end of training.''
%They also use $\dtoz$ in \citet{radford2018improving}.
%Aside from the $\dtoz$ bonus, they say it really just seems to depend
%on the total area, but their results are noisy.  In our work, we
%consistently find, e.g., Cosine is worse than Linear.
%
Few papers explicitly compare different LR schedules for large-scale
training, and when comparisons are
made~\citep{shallue2019measuring,kaplan2020scaling,schmidt2021descending,hoffmann2022empirical,yang2022mup},
they are not the primary focus.  So, while some insights are gained,
``comprehensive study'' is usually regarded as ``out of
scope''~\citep{alephalpha2024introducing}.
% Could be a footnote: this situation is not unique to LR schedules;
% given the huge costs of LLM training at
% scale~\citep{strubell2019energy,patterson2021carbon,bender2021dangers}
% "There is little room for hyperparameter tuning." (Du et al, Glam
% paper).
%
One exception is \citet{defazio2023when}, who found linear equals or
outperforms other common schedules, including cosine, across a range
of problems, including LLM training.  In addition,
\citeauthor{defazio2023when}\ develop convergence bounds that
theoretically motivate linear as the optimal schedule.  Unlike our
work, they do not evaluate LLMs with different peak LRs or decay
ratios.

Seeking to measure model quality at different training durations
without having to re-train separate models from
scratch~\citep{zhai2022scaling,hagele2024scaling}, researchers have
adopted various \emph{continuous} schedules, such as constant,
cyclic~\citep{smith2017cyclical}, etc.
%
Following optimization theory~\citep{moulines2011non,defazio2024road}
weight averaging provides an alternative to
decay~\citep{sandler2023training,sanyal2023early,busbridge2024scale},
although it is typically not as effective~\citep{hagele2024scaling},
and moreover may have hyperparameters that implicitly depend on
training duration~\citep{defazio2024road}.
%
\emph{Warmup-Stable-Decay} (WSD) approaches have also been used in LLM
training~\citep{hu2024minicpm,shen2024jetmoe,ibrahim2024simple,hagele2024scaling}.
These methods train at a constant LR, but decay from a checkpoint in a
separate process when an intermediate model is needed.
%
We show that the optimal constant LR of these methods implicitly
depends on training duration, rendering them not truly
\emph{schedule-free}.

\subsection{Maximal update parameterization ($\mup$)}\label{subsec:mup}

It is common to scale initial weights of neural networks such that
activations have unit
variance~\citep{glorot2010initialization,he2015delving}.  Yet weights
can become unstable after a few steps of updates, if layer-wise LRs
are imbalanced~\citep{yang2022mup}.
% Instabilities grow with width~\citep{yang2022mup}.
In contrast, $\mup$~\citep{yang2020feature,dey2024practitioner}
prescribes a \emph{re-parameterization} of initial weight variances
and LRs -- essentially, rules for scaling these values as model width
(i.e., $d_{model}$) scales -- so that activations and updates remain
stable.
%
$\mup$ also stabilizes embeddings, layer norms, and self-attention in
Transformers.
%~\cite[App. B.1]{yang2022mup}.
% It enables \emph{feature learning} (separation of concepts in
%embedding space) at the infinite-width limit, while such learning
%notably does not occur with the standard or
%NTK~\citep{jacot2018neural}
%parameterizations~\citep{yang2020feature,noci2024learning}.  The
%ability of $\mup$ to facilitate HP transfer was developed
%in~\citep{yang2022mup} and a spectral perspective is provided
%in~\citep{yang2023spectral}.
%
$\mup$ is seeing growing application in
LLMs~\citep{dey2023cerebras,shen2024power,hu2024minicpm}, where it
acts to stabilize training and to enable transfer of optimal
hyperparameters (HPs) across model scales.

With $\mup$, base HPs can be tuned on a small \emph{proxy}, or
\emph{base}, model, and transferred to larger models.  Given the width
of the proxy model, $d_p$, and width of the target, $d_t$, $\mup$
prescribes scaling factors to apply to HPs. The base LR $\hateta$ is
scaled down to $\eta = \rho\hateta$, where $\rho=\nicefrac{d_p}{d_t}$.
In terms of LR schedules, the base LR $\hateta_t$ is scaled at every
step to provide $\eta_t$.  $\mup$ is convenient in our study as we can
sweep the same \emph{base} peak LRs, $\hateta$, at each model size,
and observe trends that are scale-invariant.

\subsection{AdamW weights as an exponentially-weighted moving average (EMA)}\label{subsec:wang}

An AdamW update at a single training step, $t$, can be expressed as:
\begin{equation}
\theta_t = (1 - \eta\lambda)\theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\label{eqn:adamw}
\end{equation}
where $\eta$ is the learning rate, $\hat{m}_t$ and $\hat{v}_t$ are
(bias-corrected) running averages of the gradient and the squared
gradient, respectively, and $\epsilon$ is a small constant added to
prevent division by zero.  The weight decay value, $\lambda$, is
typically set to 0.1 in LLM
training~\citep{brown2020language,hoffmann2022empirical,almazrouei2023falcon,alephalpha2024introducing}.

While the running averages of $m$ and $v$ in
Adam~\citep{kingma2014adam} and AdamW are exponentially-weighted
moving averages (EMAs), \citet{wang2024how} recently showed that the
\emph{weights} generated by AdamW can also be understood as an EMA ---
of the weight \emph{updates}.  That is, a standard EMA, $y_t$, for a
time-varying quantity, $x_t$, can be written as:
\begin{equation}\label{eqn:ema}
y_t = (1 - \alpha)y_{t-1} + \alpha x_t
\end{equation}
where $\alpha$ is the \emph{smoothing parameter}.  AdamW in
\cref{eqn:adamw} can be seen as an EMA by letting:
\begin{equation}\label{eqn:adamwema}
y_t=\theta_t\mbox{, }\alpha=\eta\lambda\mbox{, and }
x_t=-\frac{1}{\lambda}\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{equation}
\citeauthor{wang2024how} note the quantity
$\tau$=$\nicefrac{1}{\alpha}$, i.e., $\nicefrac{1}{(\eta\lambda)}$,
provides a rough \emph{timescale} over which updates are averaged.
%
Weight decay can therefore be used to control the effective window
over which updates are combined (smaller values of $\lambda$ increase
$\tau$, increasing the role of earlier updates in $\theta_t$).
%
This perspective also motivates dynamic LR schedules.  Initial $\tau$
should be small (high $\eta_t$), to forget early updates, ``while the
final timescale is around the total number of epochs [low $\eta_t$],
to ensure averaging over all datapoints.''
%
In fact, we will show that contribution of weight updates, $x_0, x_1,
\ldots x_{t}$, to $\theta_t$ at a particular step, $t$, cannot be
determined by the \emph{instantaneous} value of $\eta_t\lambda$.
Rather, the contribution of any $x_i$ to $\theta_t$ requires looking
at the full LR schedule \emph{holistically} (\cref{subsec:convex}).

\citeauthor{wang2024how} also motivate scaling rules for optimal
$\lambda$ as model and dataset size vary.  If dataset size increases
by a factor of $K$ (i.e., $K\times$ as many steps), the EMA
perspective recommends scaling $\lambda$ by $\nicefrac{1}{K}$ in order
to expand $\tau$ proportional to $K$.
%
Moreover, with $\mup$, if model size increases and LR is scaled by
$\rho$ (\cref{subsec:mup}), the EMA perspective motivates scaling
$\lambda$ by $\nicefrac{1}{\rho}$ to keep $\tau$
constant.\footnote{While the EMA perspective does not account for the
updates $x_t$ themselves depending on parameters $y_{t-1}$, we find it
a useful part of our \emph{conceptual} model of training, in that it
helps predict experimental results.}
%
%Although not explored by \citeauthor{wang2024how}, analagous rules can
%be derived for batch size changes.
%
%Of course, in practice we typically scale batch and dataset size
%together with model size, so real application requires jointly
%accounting for multiple factors.
