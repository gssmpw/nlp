\begin{figure}
  \centering
  \scalebox{\onefigscale}{
  \includegraphics[width=\textwidth]{pdffigs/nanogpt.111M.pdf}
  }
  \mbox{}
  \vspace{-1mm}
  \mbox{}
  \caption{\textbf{NanoGPT results} (111M, 20~TPP): Validation loss
    for different LR and decay combinations, for a \emph{NanoGPT
    model}.  With \texttt{float16} precision, we were not able to
    train above $6$e-$04$ without instabilities (first point in
    curves).  Moreover, at $\hateta = 6$e-$04$, $\tenx$ performed
    better than $\dtoz$.  After switching to \texttt{float32}, we were
    able to train at higher $\hateta$ values, where $\dtoz$
    demonstrates its familiar superiority over
    $\tenx$.\label{fig:maxlr_nanogpt_curves}}
\end{figure}
