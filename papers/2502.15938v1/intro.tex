Learning rate (LR) schedules play an important role in training large
language models.  The original Transformers paper proposed a brief LR
warmup followed by decay proportional to the inverse square root of
the step number~\citep{vaswani2017attention}.
%
%This schedule, sometimes
%called \emph{NoamOpt}~\citep{rush2018annotated} and often used in work
%by Noam Shazeer and
%colleagues~\citep{shazeer2017outrageously,raffel2020exploring,lepikhin2020gshard,zoph2022st,chowdhery2022palm},
%
This schedule has the advantage of not requiring prior specification
of the total training steps.  However, cooling down to a specific
minimum LR is acknowledged to be ``preferable when one knows the
training duration in advance''~\citep{zhai2022scaling} as it produces
``slightly better results''~\citep{raffel2020exploring}.
%
In this paper, our main focus is finding LR schedules that achieve the
minimum loss given a pre-specified number of training tokens.

The ``predominant choice''~\citep{hu2024minicpm} in such training ---
the ``de-facto standard''~\citep{hagele2024scaling} ---
% This ``widely adopted'' schedule~\citep{ibrahim2024simple}
%
is warmup followed by cosine decay to 10\% of the maximum LR, an
approach used in GPT3~\citep{brown2020language},
Gopher~\citep{rae2022scaling},
Chinchilla~\citep{hoffmann2022empirical},
\textsc{bloom}~\citep{lescao2023bloom}, Llama~\citep{touvron2023llama},
Llama2~\citep{touvron2023llama2}, Falcon~\citep{almazrouei2023falcon},
Pythia~\citep{biderman2023pythia}, etc.
%
It is used ``following \citeauthor{hoffmann2022empirical}''~\citep{muennighoff2023scaling}, and is the default schedule in
LLM codebases~\citep{karpathy2024nanogpt}.
% NanoGPT: ``minimum learning rate, should be ~= learning\_rate/10 per
% Chinchilla''

\input{fig_train_loss_curves.tex}

We present a large-scale empirical study to determine which schedules
work best in which situations, and why.
%
We focus on both \emph{compute-efficient} and \emph{over-trained}
models.  According to Chinchilla scaling
laws \citep{hoffmann2022empirical}, the fewest FLOPs to achieve a
given loss is obtained when models are trained for around 20
tokens-per-parameter (TPP).  It is also common to train for more than
20~TPP because smaller, over-trained models are cheaper to
serve \citep{touvron2023llama}.
% We hypothesized that the optimal LR schedule may depend on the peak
% LR, and validated this empirically.
Our experiments (across various model scales, vocabulary sizes, and
dataset sources) reveal a consistent outcome: when all schedules use
their optimal peak LR, linear decay-to-zero ($\dtoz$) works best at
compute-optimal TPP\@.
%
Moreover, the relative benefit of $\dtoz$ over $\tenx$ (in terms of
training, validation and downstream loss) \emph{increases} with TPP\@.
%
Compute savings from $\dtoz$ can be substantial for over-trained
models (\cref{fig:train_loss}).
%
LLMs such as Llama2-7B~\citep{touvron2023llama2}, trained for 286~TPP
at $\tenx$ decay, could likely have saved most of their training
compute by switching to $\dtoz$.

\input{fig_background_ema_lr.tex}

To explain the success of $\linear$-$\dtoz$, we build on recent work
on the related topic of \emph{weight
decay}~\citep{andriushchenko2023why,wang2024how}.
%
First, decaying \emph{to zero} helps because compute-efficient
training includes a long phase in which gradient noise is the key
factor slowing the loss reduction; with higher noise resulting from
higher TPP \emph{or} smaller batches, a vanishing LR works best.
% result in less emphasis on recent weight updates, and,
%consequentially, worse model quality.
Second, we show that approaching zero \emph{linearly} is beneficial
via a novel interpretation of AdamW~\citep{loshchilov2017decoupled},
the primary optimizer in LLM pre-training.
% -- as a convex combination of weight updates
With AdamW, weights generated at each step are implicitly a weighted
combination of weight \emph{updates}.  The combination coefficients
depend on the learning rate schedule and weight decay settings
(\cref{subsec:convex}).
%
Analyzing this \emph{dual} of the LR schedule, we observe linear decay
to produce a favorable combination of prior weight updates
(\cref{fig:background_ema}).  When LR drops abruptly, e.g., via
step-decay or, to a lesser extent, cosine decay, later updates receive
less emphasis, leading to worse model quality.
%
The EMA dual perspective also reveals the implicit schedule-awareness
of recent ``schedule-free'' approaches such
as \emph{Warmup-Stable-Decay}
(WSD)~\citep{hu2024minicpm,bi2024deepseek,hagele2024scaling}.
%However, it also suggests a method for truly schedule-free training
%(\cref{subsec:rational}).
%
%Using the dual view of the LR schedule to interpret our experimental
%results, we develop a multi-part theory for the success of $\dtoz$.

Our empirical study encompasses hundreds of models trained over a grid
of schedules and peak LRs, with scales ranging from 111M to 1.7B
parameters, and datasets up to 137B tokens.  We also compare LR
schedules over the cross product of weight decay versus peak LR, and
peak LR versus batch size.
%
We confirm a slight, consistent advantage of linear $\dtoz$ over
cosine $\dtoz$ at 20~TPP\@.  We also demonstrate that linear $\dtoz$
improves over continuous schedules such as WSD\@.
%
Further, when dataset or batch size changes, optimal peak LRs are much
more \emph{stable} when using $\dtoz$ compared to using lesser decay.
This latter finding exposes LR decay ratio as an important confounder
in prior work studying optimal hyperparameter transfer with
$\mup$~\citep{yang2020feature,yang2022mup}.
