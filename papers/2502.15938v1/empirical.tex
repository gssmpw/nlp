\subsection{Experimental setup}

% Add this back in later: LM~\citep{dey2023btlm3b8k}
Experiments use a GPT-like LLM~\citep{radford2019gpt2}, with ALiBi
embeddings~\citep{press2022alibi} and SwiGLU~\citep{shazeer2020glu}.
Main paper models are trained on
SlimPajama~\citep{cerebras2023slimpajama} and evaluated over 1.1B
held-out tokens (regardless of training TPP).
%
Unless otherwise indicated, $\lambda$=$0.1$ is used.
%
Training runs use the same random seed, so all decay functions
($\linear$, $\cosine$, etc.) and ratios ($\constant$, $\tenx$,
$\dtoz$) have identical warmups, but note results are very consistent
across seeds at this scale (appendix \cref{fig:seeds}).
%
By default we use $\mup$ (standard parameterization results are in
\cref{subsec:sp,subsec:nanogpt}).  $\mup$ HPs are derived from a
smaller proxy model tuned using a $\linear$-$\tenx$ schedule.
%
%(results for $\dtoz$ could conceivably
%improve if we tuned for it instead).
%
Since we hypothesized that different LR schedules may enjoy their
optimal peak LR at different values, our experiments compare schedules
when each is tuned to its optimal peak LR\@; we sweep $\hateta$ by
factors of 2$\times$ around the $\mup$ proxy-tuned $\hateta = \maxlr$.
%
\cref{sec:experimental_details} provides further details on the model
architecture (\cref{tab:model_info}), dataset sizes
(\cref{tab:train_steps}), and compute resources.

\subsection{Results}

\input{tab_sched_compare.tex}

\finding{$\linear$-$\dtoz$ is the optimal LR schedule across virtually
  all peak learning rates.}

For 610M-parameter $\mup$ models trained to compute-optimal 20~TPP,
the optimal $\linear$-$\dtoz$ setting achieves 0.77\% lower loss than
the optimal $\linear$-$\tenx$ setting (\cref{tab:sched_compare}).
At smaller, suboptimal peak LRs, $\tenx$ can be better than $\dtoz$.
%
In appendix \cref{fig:maxlr_sp.curves}, we demonstrate very
similar results using the standard parameterization.
%
Regarding the decay function, gains from $\linear$-$\dtoz$ over
$\cosine$-$\dtoz$ are small, but perfectly consistent across peak LRs,
exactly in line with recent
work~\citep{defazio2023when,lingle2024large}.
%
Interestingly, $\cosine$-$\tenx$ is consistently slightly better than
$\linear$-$\tenx$, showing that $\linear$ itself is not always best,
rather $\linear$ \emph{plus} $\dtoz$ is needed.
%
Lacking a cooldown phase, inverse square root ($\invsqrt$) and
$\constant$ do not perform as well.
%
%\citep{lingle2024large} - if you look through his findings, you also
%see that $\linear$ slightly beats $\cosine$ (across all LRs), except at the
%very smallest size.  This wasn't the point of that work, but note it!

Appendix \cref{fig:maxlr_tpp:models} compares schedules as we sweep
peak LR for 111M, 610M, and 1.7B models.  For these and subsequent
experiments, we use a $\linear$ decay.

\input{fig_maxlr_tpp.111M.tex}

%\input{fig_more_tpp.tex}

\finding{As TPP increases, the relative improvement of $\dtoz$ over
  $\tenx$ also increases.}

For 111M models at only 2~TPP, $\dtoz$ performs \emph{worse} than
$\tenx$ across all peak LRs (\cref{fig:maxlr_tpp:111M}, left).  610M
results are similar (appendix \cref{fig:maxlr_tpp:610M}).  However, as
TPP increases, $\dtoz$ begins to outperform $\tenx$, exceeding the
best setting of $\tenx$ by 1.6\% at 200~TPP, and performing 2.8\%
better at the proxy-tuned $\hateta$=$\maxlr$ (marked in plots with a
red vertical line).
%
While we hypothesized $\dtoz$ would surpass $\tenx$ at \emph{some}
TPP, our key finding is that, at both compute-efficient (20~TPP) and
over-trained sizes, decaying to \emph{zero} is optimal.
%
Even at 2~TPP, some LR decay is beneficial ($\tenx$ improves over
$\constant$), but too much evidently hampers bias reduction.
%
As TPP increases, optimal LRs for $\tenx$ shift lower as variance
reduction gains in importance, but now it is the lower \emph{peak} LRs
that hamper bias reduction, and $\tenx$ begins to lag $\dtoz$.
%
By 20+~TPP, $\dtoz$ is consistently superior.
%
%Discuss this a bit:
%
%With $\constant$, the shifting emphasis as TPP increases can only be
%satisfied by lowering the fixed LR (explaining the shift in optimal
%LR, \cref{fig:maxlr_tpp:111M,fig:maxlr_tpp:610M}).

\cref{fig:more_tpp} plots validation loss of $\tenx$ and $\dtoz$ at
different TPP settings for different model sizes (all models are
trained using the proxy-tuned peak LR\@).
%
For the 610M model, $\dtoz$ is also initially worse than $\tenx$, but
begins to surpass it around 4~TPP, and by 200~TPP is 2.6\% better.  As
with training loss (\cref{fig:train_loss}), an 80~TPP $\dtoz$
model can surpass a 200~TPP $\tenx$ model in validation loss.
%
\textbf{Importantly, these trends also hold for \emph{downstream}
  evaluation of the models} (appendix \cref{tab:downstream1}).

\cref{fig:more_tpp} also shows the advantage of using $\dtoz$ with
\emph{1.7B models trained to 80 TPP}\@; $\dtoz$ achieves roughly
3.05\% lower loss than $\tenx$ at the proxy-tuned $\hateta$.  At this
scale, such gains provide real-world impact: we over-train models of
this size for use as proposal models in speculative
decoding~\citep{leviathan2023fast} and smaller models like
Gemma-2B~\citep{mesnard2024gemma} and Phi-1.5~\citep{li2023textbooks}
are used in many production settings.
%
%So, finding the right recipes
%for over-training models at this scale is an important research
%direction and a valuable supplementary contribution of this paper.

\begin{figure}
\noindent % Ensures the minipages align with the left margin
\begin{minipage}[b]{0.39\textwidth}
  \centering
  \input{fig_more_tpp.tex}
\end{minipage}%
\hfill % Use to fill space between the minipages
\begin{minipage}[b]{0.57\textwidth}
  \centering
  \input{fig_maxlr_isotppbatches.tex}
\end{minipage}
\end{figure}

\finding{Compared to $\constant$ and $\tenx$, optimal peak LRs are much
  more stable with $\dtoz$.\label{find:optimal_lr}}

\cref{fig:maxlr_tpp:111M} reveals different levels of hyperparameter
sensitivity when increasing TPP\@: optimal LR shifts substantially
lower for $\constant$, somewhat lower for $\tenx$, and hardly at all
for $\dtoz$.  The same trend holds at the 610M scale (appendix
\cref{fig:maxlr_tpp:610M}).
%
Moreover, with $\dtoz$, loss is less sensitive to a suboptimal LR
(bowls are flatter).
%
$\dtoz$ loss is also more stable (and lower) as we vary both
\emph{weight decay} (appendix
\cref{fig:maxlr_wdgroups:610M,fig:maxlr_wdgroups:111M}) and
\emph{batch size}, $B$.
%
For example, the optimal LR already shifts significantly for $\tenx$
models at B=126, while $\dtoz$ optima only begin to shift at B=63
(\cref{fig:maxlr_isotppbatches}).  The superiority of $\dtoz$ over
$\tenx$ across $B$ is more clearly observed in appendix
\cref{fig:maxlr_isotppbatches_sep_batches}.

Crucially, if we keep the number of optimization steps constant
(11752), but only vary $B$ (effectively increasing TPP proportional to
$B$), we find the optimal learning rate does not \emph{decrease}
(i.e., with TPP) but rather \emph{increases} (appendix
\cref{fig:maxlr_isostepbatches}).  Clearly, LR decay is primarily
beneficial as a noise-reduction mechanism: with larger batches, we
have less gradient noise, and can afford a larger LR\@.

\finding{$\linear$-$\dtoz$ works better than $\wsd$ and $\cyclic$
  (continuous) LR schedules.}

\begin{figure}
  \noindent
\begin{minipage}[b]{0.40\textwidth}
  \centering
  \input{fig_maxlr_infinite.tex}
\end{minipage}%
\hfill
\begin{minipage}[b]{0.58\textwidth}
  \centering
  \input{fig_trainevals.tex}
\end{minipage}
\end{figure}

In \cref{fig:maxLR_infinites}, we compare $\dtoz$ to $\tenx$, and to
two approaches designed for continuous pre-training: $\cyclic$, which
cycles LR up and down, and $\wsd$ (\cref{subsec:lr_schedules}).  For
$\wsd$, we simulate a model being retrieved after 80~TPP, and cool the
LR to zero for the final 22.5\% of steps (around the proportion
recommended by \citet{hagele2024scaling}, and equal to the cooldown
duration in our 20~TPP models).  Appendix \cref{fig:inf_ema_lrs}
provides full LR curves and update coefficients for all models in
\cref{fig:maxLR_infinites}.

At its optimal LR, $\wsd$ works better than $\tenx$, confirming
results in \citet{hagele2024scaling}.  However, note the optimal peak
LR shifts lower for both $\tenx$ and $\wsd$ at this TPP\@.
%
$\linear$-$\dtoz$ remains best here, around 0.84\% better than the
optimal $\wsd$.  Given the diminishing returns of high-TPP training
(\cref{fig:more_tpp}), $\wsd$ would require significantly more
training FLOPs to reach the level of $\dtoz$.

\finding{$\constant$ and $\tenx$, but not $\dtoz$, strongly overfit to
  the end of the training data.}

\cref{fig:trainevals} shows the loss of trained models (with frozen
weights) on the exactly-same ordered batches used during pre-training.
As predicted by the extended EMA perspective (\cref{sec:emas}), the
higher the LR, the more models fit to late-stage training batches.  It
is striking that both $\constant$ and $\tenx$, but not $\dtoz$,
\emph{overfit} to the very final portion.  Extra adaptation of
generative models to recent training sequences has long been
observed~\citep{graves2013generating}, but to our knowledge this is
the first evidence $\dtoz$ may help mitigate these effects.  Since
$\dtoz$ demonstrates the lowest loss on batches \emph{slightly before}
the final training phase, placing the highest-quality and most-recent
data in the \emph{very} final phase, while using $\dtoz$ (e.g., as
in~\citet{dubey2024llama}), may be suboptimal.
%
These findings also contradict \citet{biderman2023pythia}, who found
training order had little impact on memorization.
