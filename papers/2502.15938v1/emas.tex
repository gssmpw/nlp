We now present an extended EMA perspective on AdamW that accounts for
time-varying LRs.  We then introduce our conceptual model of LLM
training, and connect it to the EMA perspective.  Finally, we outline
the specific testable hypotheses that follow from our conceptual
model.

\subsection{AdamW as convex combination of weight updates, driven by LR schedule}\label{subsec:convex}

Consider a generic moving average, but now with time-varying
smoothing, $\alpha_t$, i.e., $y_t = (1 - \alpha_t)y_{t-1} + \alpha_t
x_t$.  If we let $\alpha_1=1$ (so that $y_1=x_1$), we can express
$y_t$ in terms of all inputs $x_t$:
\begin{eqnarray}\label{eqn:extended_ema}
  y_1 &=& \alpha_1 x_1  \notag, \\
  y_2 &=& (1 - \alpha_2) \alpha_1 x_1 + \alpha_2 x_2, \cdots  \notag \\
%  y_3 &=& (1 - \alpha_3) (1 - \alpha_2) \alpha_1 x_1 + (1 - \alpha_3) \alpha_2 x_2 + \alpha_3 x_3, \cdots \notag \\
%  &\cdots&  \notag \\
  y_t &=& \sum_{i=1}^t \left( \prod_{j=i+1}^{t} (1 - \alpha_j) \right) \alpha_i x_i \notag \\
      &=& \sum_{i=1}^t c_{t,i} x_i
\end{eqnarray}

Where $c_{t,i} = \left( \prod_{j=i+1}^{t} (1 - \alpha_j) \right) \alpha_i$.
%
%is the contribution of input $x_i$ to output $y_t$ at time $t$.
%such
%that
%
%$y_t = \sum_{i=1}^t c_{t,i} x_i$.
%
It can be shown
%
$\forall t, \sum_i c_{t,i} = 1$, and hence, as in a standard EMA, each
$y_t$ is a \emph{convex combination} of inputs $x_1 \ldots x_t$ (all
coefficients are non-negative and sum to 1).  However, with
time-varying $\alpha_t$, we are not restricted to
exponentially-decreasing $c_{t,i}$ as $i$ decreases.  Indeed, for
\emph{any} convex combination of inputs, there is a corresponding
smoothing schedule that generates the combination via the EMA\@.
%
In terms of LR schedules for AdamW training, $y_t = \theta_t$, while
$\alpha_t = \eta_t\lambda$ becomes the smoothing parameter at step $t$
(cf. \cref{eqn:ema,eqn:adamwema}).\footnote{When using a $\mup$ LR
scaling factor, $\rho$ (\cref{subsec:mup}), the smoothing parameter is
$\alpha = \eta\lambda = \rho\hateta\lambda$.}

\subsection{Conceptual model: bias and variance in LLM training}\label{subsec:biasvar}

\input{tikzfigs/fig_bias_var_all.tex}

Following \citet{andriushchenko2023why}, our main conceptual premise
is that in LLM training, there is an initial training phase focused on
movement from initial conditions (bias reduction), followed by a later
phase bottlenecked by gradient variance.
%
Furthermore, analogous to prior work optimizing the convex loss gap
via SGD (\cref{eqn:biasvar}), we argue the primary beneficial
mechanism of LR decay in LLM training is to reduce gradient noise
during later stages of training, while simultaneously minimizing the
impact on bias reduction.
%
The larger the dataset, the longer the proportion of training
bottlenecked by gradient noise, and the greater the benefit from $\dtoz$
(\cref{fig:bias_var_all}).

\paragraph{Variance reduction}

Per-step gradient variance is known to increase over the course of
training~\citep{mccandlish2018empirical}.
%showed that the ratio of
%gradient variance to gradient magnitude increases over the course of
%training, a ratio they equate to the \emph{critical batch size} (the
%batch size at which the benefits of parallelism rapidly decrease due
%to gradient redundancy).
%
Very recently, \citet{zhang2024how} showed that for an LLM of a fixed
size, training with larger datasets corresponds to larger
\emph{critical batch sizes}, which directly relates to larger
(aggregate) gradient variance via the gradient noise
scale~\citep{mccandlish2018empirical}.

\input{tikzfigs/fig_cartoon.tex}

In the EMA perspective, parameters $\theta_t$ are a combination of
prior weight updates (indexed by $i$).  The more variance at step $t$,
the more updates that should be combined in order to reduce it.
%
Here variance is reduced by combining updates \emph{across} steps,
rather than increasing batch size at a \emph{specific} step.
%
Now, with a constant LR, update coefficient $c_{t, i}$ decreases
exponentially in $(t - i)$: the further back the update from the
current step, the less it contributes.
%
But if LR $\eta_i$ \emph{decreases} over steps, each later coefficient
$c_{t, m}$ will be smaller due to a lower $\alpha_m = \eta_m\lambda$.
Yet since coefficients sum to one, \emph{earlier} coefficients will
correspondingly \emph{increase}: coefficients are basically flattened
and updates contribute more evenly across steps.
%
%corresponds to scaling
%\emph{up} all $c_{t, k}$ where $k < i$ (cf.~\cref{eqn:extended_ema}).
%This has the effect of flattening the coefficients, effectively
%averaging over more outputs.
%
The more we decay, the more updates we average over, and the more
variance is reduced.
%
%(see appendix \cref{fig:more_ema_lrs} for a visual
%contrast between $\constant$ and $\dtoz$ coefficients, on a log scale)

\paragraph{Bias reduction}
Higher decay is preferable to reducing \emph{peak} LR because we must
also minimize \emph{bias}, i.e., the contribution of the initial
random weights.
%
In the EMA perspective, the contribution of these weights to
$\theta_t$ is $c_{t,1} = \prod_{j=2}^{t} (1 - \alpha_j)$.  For a
decaying LR, and where $\alpha_j = \eta_j\lambda \ll 1$, coefficient
$c_{t,1}$ can be approximated (with equality when LR is constant,
\cref{sec:c_bias_derivation}):
\begin{equation}\label{eq:c_bias}
c_{t,1} \approx (1-\bar{\alpha})^{t-1}
\end{equation}
where $\bar{\alpha}$ is the average $\alpha_j$ over the schedule.
Exactly as in \cref{eqn:biasvar}, bias therefore decreases
exponentially in the \emph{absolute} number of steps, $t$, with a rate
of decrease depending on the LR\@.  Crucially, this means that as we
train for more total steps (i.e., a higher tokens-per-parameter; TPP),
there is a decrease in the \emph{fraction} of steps required for
$c_{t,1}$ to become negligible.  At higher TPP, bias reduction becomes
relatively less important than variance reduction
(\cref{fig:cartoon}).

\subsection{Experimental Hypotheses}

\hypothesis{As TPP increases, the relative benefit of $\dtoz$ over
  $\tenx$ decay will increase.}

This hypothesis follows from the premise that gradient variance plays
a larger role at higher TPP, and greater LR decay (as in $\dtoz$)
allows for more updates to be averaged, and thus greater variance
reduction.
%
A related hypothesis is that if we increase the batch size at each
step, gradient variance will decrease, and so the benefits of $\dtoz$
over $\tenx$ decay should \emph{diminish}.
%
Note our conceptual framework does not say precisely at which TPP or
batch size that $\dtoz$ will first prevail; here we will rely on our
empirical findings (\cref{sec:empirical}) to fill in the theoretical
gap.

\hypothesis{As TPP increases, the optimal peak LR will decrease, for
  all LR schedules.\label{hyp:optimal_lr}}

Tuning peak LR is about trading off movement from initial
conditions (requiring a high LR) and mitigating variance (requiring a
low LR).  As TPP increases, and bias reduction plays a smaller role,
optimal peak LR should decrease.  We hypothesize the decrease will
be greater with a $\constant$ schedule, as $\constant$ does not use
decay to balance the conflicting demands of bias and variance
(\cref{fig:bias_var_all}).
%
Moreover, optimal peak LR for other \emph{continuous} LR schedules,
like $\wsd$ and $\cyclic$, should also decrease with longer training
durations.  In this way, such ``schedule-free'' approaches are
\emph{not truly schedule-free}.  This dependence is also obvious when
plotting update coefficients for these schedules (appendix
\cref{fig:inf_ema_lrs}): the higher the LR, the more emphasis on
recent updates.

\hypothesis{$\linear$ $\dtoz$ will improve over $\cosine$ $\dtoz$.}

While LR decay allows averaging over more weight updates, if the LR
decreases too quickly, the final weight updates may not contribute to
the EMA\@.  From a loss surface perspective, as the LR approaches
zero, we take vanishingly small steps.  Since $\cosine$ reaches
smaller steps faster than $\linear$ (\cref{fig:background_ema}, left),
$\cosine$ will make less progress toward the optimum loss.
%
From the EMA perspective, this is equivalent to $\cosine$ having
smaller $c_{t,i}$ coefficients as $i$ approaches $t$
(\cref{fig:background_ema}, right).
%
Note this problem is unique to $\cosine$ $\dtoz$ and will not affect,
e.g., $\cosine$ $\tenx$ decay.

\hypothesis{A high LR (not weight decay) reduces bias and achieves optimal loss.\label{hyp:wdbias}}

Note that weight updates $x_t$ have a coefficient of
$\nicefrac{1}{\lambda}$ in \cref{eqn:adamwema}.  So, while $\eta_t$
and $\lambda$ contribute equally to $\alpha_j$, increasing $\lambda$
to reduce bias is counterproductive as weight updates will be scaled
down proportionally, reducing movement from initial conditions.
%This observation will be crucial for interpreting our experimental
%findings where we systematically vary LR and weight decay (e.g.,
%\cref{fig:maxlr_wd:617M}).
%
However, if LR is in a high enough range so that bias plays a minimal
factor, both LR and WD should equally affect variance reduction.

%Since weight decay does not impact bias reduction, then at high TPP,
%it should be more effective to reduce variance by lowering WD than by
%reducing the LR\@.
%%
%This stands in contrast to very recent work that recommends purely
%decreases in LR for high-TPP training~\citep{bjorck2024scaling}.
