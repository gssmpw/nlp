\paragraph{The confounding role of LR schedule}

We have shown that the more constant the LR schedule, the more
sensitive the optimal peak $\eta$.
%(and the more the model overfits the end of the training data.
%provides empirical evidence for this perspective; the highest peak
%LRs achieve lowest loss on the final (re-visited) training batches.
These findings help explain LR sensitivity in prior work.
%
For example, \citet{shen2024power} were puzzled by their observation
that ``although the $\wsd$ scheduler could, in theory, continue in the
stable phase forever \ldots the optimal LRs are different for different
amounts of training tokens.''
%
As noted under \cref{hyp:optimal_lr}, LR schedules with long constant
periods only appear ``schedule-free'' in the primal; from the dual
(EMA) perspective, the higher the LR, the more emphasis is placed on
recent updates (see appendix
\cref{fig:more_ema_lrs:constant} for $\constant$, 
\cref{fig:inf_ema_lrs:wsd} for $\wsd$).
%
\citet{filatov2024time} and \citet[Figure~19]{yang2022mup} also
observed major decreases in optimal LR with higher TPP; these results
were both obtained with a constant LR\@.
%
In contrast, \citet{bjorck2024scaling} observed less significant
decrease of optimal $\eta$ with TPP, but used $\tenx$ decay.

LR schedule may also affect the optimal $\eta$ as batch size varies
--- particularly when using $\mup$.
%
Some prior $\mup$
studies~\citep{yang2022mup,noci2024learning,shen2024power} observed
linear scaling of optimal LR with batch size, i.e., the
so-called \emph{linear scaling
rule}~\citep{krizhevsky2014one,chen2016revisiting,smith2018dont}.
Others~\citep{lingle2024large} have observed square-root scaling,
resonating with other prior
studies~\citep{hoffer2017train,you2019large,malladi2022sdes}.
% You should note that the original $\mutransfer$
%paper~\citet{yang2022mup} looked at transferability at iso-Step cases
%(footnote 20), whereas \citet{noci2024learning} tried iso-TPP, but I
%no longer think this is a key issue.
This discrepancy can be explained by noting the linear scaling results
were all found with a $\constant$ or $\wsd$ LR decay, while
square-root was observed with $\linear$ $\dtoz$, again underscoring
the greater stability of $\dtoz$.
%
%Other proposed scaling rules, e.g.,
%square-root scaling for weight decay~\citep{loshchilov2017decoupled},
%should also be re-evaluated to account for the LR schedule.
%
%\citet{shen2024power} - need a linear scaling rule for batch size, but
%with the WSD schedule.  Echos linear scaling rule in prior work
%
%\citet{lingle2024large} - validated a rule where if you scale batches
%up 4x, you scale the LR 2x (i.e., found HP transfer with this).  This
%is the \emph{Adam Scaling Rule} from \citet{malladi2022sdes}, also
%used in \citep{hoffer2017train,you2019large}.  He does use linear decay
%to zero!  But no weight decay, so he was basically using Adam, not
%AdamW.
%
%\citet{shallue2019measuring} show ``learning rate scaling heuristics
%with the batch size do not hold across all problems or across all
%batch sizes'' as cited in \citet{you2019large}.
%
%Wow, seems like~\citet{shen2024power} found very similar things
%(regarding TPP and batch sizes), although they did not sweep different
%decay ratios!  But it helps to confirm that $\mup$ doesn't transfer as
%much, and now you can see that your paper extends this in another
%dimension, which is awesome.
%
%\citet{wortsman2023small} also look at how loss changes as you vary
%the LR across scales.  They did not consider how the minimum affects
%sensitivity.  ``We use a linear schedule for warmup and and a
%cosine-decay [32] schedule for the remainder, with minimum learning
%rate 1e-5.''  Actually, since they used a fixed minimum LR, but varied
%the maximum LR, you might view that as a bit of a confounder?
%
%If the GNS paper established that rule, what does it mean for
%something to become linear before something else?  It means something
%about NOISE!  One thing can handle noise better.  Check with
%Joel/Gavia on this.

\paragraph{The confounding role of training duration}\label{subsec:confounding}

While LR schedules have confounded studies varying TPP, TPP has
analogously confounded studies evaluating LR schedules.
%
Recall \citet{kaplan2020scaling} also saw a benefit from $\dtoz$.  In
contrast to Chinchilla scaling~\citep{hoffmann2022empirical}, in
the \citeauthor{kaplan2020scaling} perspective, small models should be
trained to high TPP (while larger models should be trained less).  It
is therefore not surprising \citeauthor{kaplan2020scaling} saw
benefits testing $\dtoz$ with small models; as we have shown, $\dtoz$
is especially effective at high TPP\@.
%
In contrast, in Figure~4 of \citet{yang2022mup}, $\linear$
is \emph{worst} of all schedules, and the gap between it and
$\constant$ and $\invsqrt$ grows with model width.  But here, since
training data is small --- and \emph{fixed}, then TPP
\emph{decreases} as width increases.  Thus training is in a phase
where bias reduction is paramount and $\dtoz$ is not effective.
%
Notably, in their LLM training experiments at higher TPP,
\citeauthor{yang2022mup}\ do report linear $\dtoz$ to work best.
%
%This is the right time to revisit these:
%\citet{shallue2019measuring}
%compared many different LR schedules (cosine, linear, piecewise,
%polynomial, etc.) on ResNETs.  They ultimately recommended linear
%decay as a simple approach that works as well as the best alternative,
%and requiring fewer hyperparameters to tune.
%
%\citet{hoffmann2022empirical} adopt cosine decay, and report that
%``the difference between decaying by 10 and decaying to 0.0 ... is
%small, though decaying by a factor of 10 to be slightly more
%performant,'' while ``decaying by less (5) is clearly worse.''

With this context, we speculate on why $\dtoz$ has not been adopted
more widely in LLM pre-training:
\begin{itemize}[leftmargin=*]
  \item First, it is common to evaluate hyperparameters on smaller
    training runs; unfortunately, with limited training data (low
    TPP), $\dtoz$ misleadingly under-performs.
% , or partway through training (e.g., \citet{almazrouei2023falcon}
%choose their peak LR based on loss rankings at the end of the warmup
%period).  ``Loss rankings at the end of learning rate warm-up broadly
%reflects rankings at the end of training, enabling us to search for
%optimal learning rates efficiently''~ â€“ i.e., evaluate them only at
%end of warm-up phase.
\item Secondly, coupling between LR schedule and optimal peak LR is
  problematic: with $\tenx$ decay, we may find a lower peak LR is
  optimal; if we then test $\dtoz$ with the same LR, it may be
  suboptimal for $\dtoz$.  Not seeing a benefit at this LR, we may
  conclude $\dtoz$ is inferior in general.
\item Finally, poorly controlled training dynamics may prevent
  $\dtoz$ models from training with their (higher) optimal LR.
  Indeed, when we initially compared $\dtoz$ and $\tenx$ using NanoGPT
  (\cref{subsec:nanogpt}), $\tenx$ performed better at the default
  LR\@.  Raising the LR resulted in training divergence.  Only after
  switching the numerical precision from \verb|bfloat16|
  to \verb|float32| could $\dtoz$ succeed --- and the model reach
  optimal loss.
\end{itemize}

%
%Things that enable stability, see~\citet{wortsman2023small} -- e.g.,
%longer warmup.  QK-layernorm, z-loss regularization, decoupled weight
%decay, width vs. depth, mup.
%
%``We briefly revisited the decay
%schedule (keeping the decay style constant to a cosine decay),
%ablating decaying to 0 vs. 10\% of the learning rate. Decaying to 0
%yielded slightly better downstream scores, so we chose this value.''
%
%However, it seems recently, the merits of $\dtoz$ are becoming more
%widely appreciated, and being used more in SOTA frontier
%models~\citet{alephalpha2024introducing,dubey2024llama}.
%%
%We hope it may also become the reference for schedules for continuous
%pre-training~\citep{hagele2024scaling}.

%Basically we must acknowledge that linear D2Z is starting to catch on.
%LLaMA3, Alepha Alpha.  WSD usually goes to zero.  But the baselines
%should also D2Z.  Plus growing theory supporting D2Z.

%In fact, I now see it as almost a diagnostic whether D2Z helps.  If it
%doesn't, your peak LR is likely lower than optimal, and you need to
%work on stabilizing training so you can push it higher.  But when you
%do, D2Z will be there, waiting for you.

\paragraph{The special benefit of $\dtoz$}

%Thus far, the dual view of the LR schedule has provided some theory
%explaining the slight, consistent benefits of $\linear$ over
%$\cosine$ (\cref{fig:background_ema},
%\cref{tab:sched_compare})... Going back to its origination
%in~\citet{loshchilov2016sgdr}, we are not aware of any theory
%supporting the use of $\cosine$.  You can't just drop the LR at the
%end (Step decay).  It explains the dependence of $\wsd$ and
%$\constant$ on the peak LR.  The fact the different curves hit their
%minimum at the same $\alpha$, suggests this is an important quantity.
%But the fact they are separated based on their $\hateta$ is also
%crucial.  And the fact there's a cooldown at the end, that's not
%really accounted for.
%People are putting their high-quality and recent data at the end of
%training (LLaMA3).  That makes sense then.  Actually, it was observed
%that the final data you train on influences the character of generated
%text as far back as Alex Graves's work.

Our results suggest a low $\alpha$ later in training can expand the
timescale over which weight updates are combined, reducing noise in a
similar way to increasing batch size.
%
However, there is apparently a separate, independent benefit from a
vanishing LR\@.
% but only when the models are sufficiently far from the initial
% conditions
Indeed, looking at appendix \cref{fig:inf_ema_lrs} for the 80~TPP
comparison to continuous schedules, we see $\tenx$ coefficients are
quite similar to the $\dtoz$ curve, apart from missing the final drop.
Moreover, they are flatter than the $\wsd$ EMA coefficients for the
same peak LR, suggesting better integration of prior updates.  Yet
$\wsd$ performs \emph{better} than $\tenx$ at all LR settings
(\cref{fig:maxLR_infinites}).
%
In contrast, at 2~TPP (\cref{fig:maxlr_tpp:111M}, left), $\tenx$
performs better than $\dtoz$ at every LR setting.
%
Prior work has shown large LRs allow exploration of the loss surface
at a height ``above the valley floor''~\citep{xing2018walk}, while LR
cooldown phases descend into a local
minimum~\citep{hu2024minicpm,hagele2024scaling}.
%
It appears that descending into these minima is beneficial only after
sufficient exploration of the loss surface.

\section{Limitations and Further Experiments}

While our findings strongly support linear $\dtoz$ being optimal in
our specific context, there are some limitations to keep in mind.
%
First, LR schedules like $\dtoz$ require prior knowledge of the total
number of training steps.  But it is worth reiterating that even
nominally schedule-free approaches such as $\constant$ also require
this knowledge in order to optimally set the LR value.  In contrast,
the extended EMA perspective of LR schedules enables derivation of a
truly schedule-free schedule, which we introduce
in \cref{subsec:rational}.
%
Second, our focus in this paper was specifically LLM training at
compute-optimal and overtrained dataset sizes.  For ML problems with
limited access to training data, $\dtoz$ is likely not the best
strategy.
%
Third, our work focuses on AdamW (the standard optimizer in LLM
pre-training).
%
While the extended EMA perspective of LR schedules will likely apply
to other optimizers that use decoupled weight decay (as noted
by~\citet{wang2024how}), it may not apply to approximate second order
methods, such as Shampoo~\citep{gupta2018shampoo}.
%
Finally, for LLMs with unstable training dynamics that cannot tolerate
high LRs, $\dtoz$ may not be beneficial.  We experienced this
first-hand when we initially trained NanoGPT~(\cref{subsec:nanogpt}).

However, we also note the remarkable consistency of $\dtoz$'s success.
%
While results in the main paper used the SlimPajama dataset,
consistent results were found with OpenWebText (\cref{subsec:nanogpt})
and a multilingual dataset (with a larger vocabulary)
(\cref{subsec:scaling}).
%Understanding if there is a relationship between dataset quality and
%the optimal decay schedule is an avenue for future research.  with
%downstream evaluations (\cref{subsec:downstream}),
We also saw consistent findings with different parameterizations
(\cref{subsec:sp,subsec:nanogpt}), architectures
(\cref{subsec:scaling}), weight sparsity settings
(\cref{subsec:sparsity}), and training frameworks
(\cref{subsec:nanogpt}).
%
\cref{subsec:scaling} describes scaling laws fit to models trained
%
with $\dtoz$ and $\tenx$, at model sizes up to 2.75B; results indicate
a growing performance gap as scale increases.
%
Finally, \cref{subsec:wd} demonstrates that the benefits
of \emph{weight decay} are observed primarily when using LR $\dtoz$,
where raising weight decay can fine-tune the EMA update coefficients
without affecting training stability.
%
%
%, training
%durations (\cref{subsec:tpp}), batch sizes
%(\cref{subsec:batches}), weight decay settings
%(\cref{subsec:wd}), datasets
%(\cref{subsec:nanogpt,subsec:scaling}),

%In \cref{subsec:rational}, we also derive a schedule that has
%completely flat EMA coefficients at every step; designing such
%schedules is a promising direction for future work in continuous
%pre-training.

%\inlinehighlight{Need to finalize this after we finalize the appendices, maybe}
%
%Not many people have the resources, to, e.g., train hundreds of LLMs
%with a constant LR :) So hard to reproduce?
%
%In this paper, we focused on explaining and unifying these
%observations, as opposed to remediations.  Some applications of our
%work would include improved modeling of loss with different schedules
%\citep{tissue2024scaling}, and better prediction of optimal HPs as the
%amount of training data, batch size, weight decay, and model size
%change~\citep{shen2024power}.
%
%Like, power scheduler doesn't try to FIX this, they just try to cope
%with it by adjusting the batch size accordingly.
%%
%D2Z is one mechanism that would greatly help them
%cope!!!!!!!!!!!!!!!!!
%%
%Well, if they want infinite LRs, then they'd need your rational
%schedule.
%
%The weight updates themselves are EMAs, so you implicitly include info
%from a longer time frame... what's the effect of THAT?  You know, it
%wouldn't be too hard to work this out.  If not analytically, you could
%feed in gradients that are unit vectors and compute their contribution
%to the output at each step via backprop.  It seems that maybe we can
%get rid of some hyperparameters this way, maybe?
%
%Will $\linear$-$\dtoz$ be best across optimizers, and does your
%analysis extend to other optimizers?
%
%Regarding our analysis, as \citet{wang2024how} note: ``There are other
%update rules such as Lion (Chen et al., 2023) and Sophia (Liu et al.,
%2024) that use decoupled weight decay. We expect that our
%recommendations will transfer to those settings, though we have not
%investigated this explicitly as AdamW is by far and away the most
%popular method at present''

%\citet{defazio2023when} - ``the linear-decay schedule matches or
%outperforms all commonly used default schedules including cosine
%annealing,''
%
%\citet{shallue2019measuring} - ``We chose linear decay because it
%performed at least as well as all other schedules we tried, while also
%being the simplest and requiring only two additional metaparameters''.
%This was with Nesterov momentum.
