Linear decay-to-zero is the optimal LR schedule for LLM training using
AdamW\@.
%
To be clear, less decay is beneficial for low tokens-per-parameter
training, but there is no practical reason to perform such training
with LLMs, since the same FLOPs could be used to train a smaller
model, over more tokens, to a lower loss -- using $\dtoz$.
%
The superiority of $\dtoz$ was validated across a range of
experimental conditions.  Results suggest its relative benefit will
increase as models increase in scale.  Moreover, when using $\dtoz$
and $\mup$, the optimal peak LR is less sensitive to changes in weight
decay, dataset size, and batch size, i.e., there is better
hyperparameter \emph{transfer}.

%Varying the decay schedule has proven to be a useful tool for
%stress-testing LLMs and developing insights into training.  Here,
%
Our analysis was aided by our interpretation of AdamW's output as a
convex combination of prior weight updates.
%
$\dtoz$ overfits less the final training sequences, and is especially
beneficial when gradient noise dominates training.
%
As we enter a phase of applied ML where inference efficiency is a
primary concern, there is strong motivation to study high-TPP
training, where gradient noise is the bottleneck.
%
While our results indicate that $\dtoz$ is a key component of the
solution here, further investigation is required, including into how
and when to adjust hyperparameters such as weight decay, batch size,
and learning rate, in the high-TPP context.
