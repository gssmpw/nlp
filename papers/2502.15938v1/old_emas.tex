This formulation enables proactive analysis of existing LR schedules
--- by calculating coefficients and judging how updates are integrated
over training --- without needing to actually train.  For example,
Figure~\ref{fig:background_ema} (Section~\ref{sec:intro}) compares the
effect of $\linear$, $\cosine$, and $\step$ decay on the output at
step $t$ of a 111M-parameter model trained for 200 TPP (2D plots of
$c_{t,i}$ over all $t$ and $i$ are in appendix
Figure~\ref{fig:img_emas}; see also Figures~\ref{fig:more_ema_lrs}
and~\ref{fig:inf_ema_lrs}).
%
Moreover, we can also design novel LR schedules to have desirable
blends of weight updates (Section~\ref{subsec:rational}).  Optimizing
these coefficients thus provides a \emph{dual} to optimizing the LR
decay function, and we therefore refer to these coefficients as the
\emph{dual coefficients} of the LR schedule.

%To what extent do the bias and variance terms play a role in modern
%LLM training?  We hypothesize that the answer to this question is
%\emph{scale-invariant} given a fixed training tokens-per-parameter
%(TPP).  We know that, regardless of scale, models tend to reach the
%compute-efficient-training frontier at around
%20~TPP~\citep{hoffmann2022empirical}.  It seems unlikely that at some
%scales, models train efficiently to 20~TPP by mostly minimizing bias,
%while at others they train efficiently to 20~TPP by mostly grappling
%with variance.  It is more likely that 20~TPP is consistently
%efficient across scales precisely \emph{because} it corresponds to a
%balanced combination of bias and variance.
% models are trained in a compute-efficient manner.  That is, provided
%we train models so that either training FLOPs or inference FLOPs are
%minimized (corrsponding to 20~TPP following Chinchilla scaling
%laws~\citep{hoffmann2022empirical}),

\subsection{Truly schedule-free LR schedules}\label{subsec:rational}

$\constant$ schedules, or schedules with a long constant phase such as
$\wsd$, are not truly ``schedule-free'' because their max LR setting
affects the $(1 - \alpha_j)$ terms in the dual.  Different LRs will
correspond to different effective timescales over updates, and thus
different LRs will be optimal for different training durations (see
appendix Figure~\ref{fig:more_ema_lrs:constant} for $\constant$ and
Figure~\ref{fig:inf_ema_lrs:wsd} for $\wsd$).

In contrast, we now derive a schedule such that coefficients are
always weighted equally, at every training step.  First, it can be
shown that:
\begin{eqnarray}
    \frac{c_{t,i+1}}{c_{t,i}} = \frac{\alpha_{i+1}}{(1-\alpha_{i+1})\alpha_i}
\end{eqnarray}
%
For the coefficients to be uniform at any step $t$, we require this
ratio to be 1, which implies that smoothing evolves $\alpha_{i+1}$ =
$\frac{\alpha_i}{(1+\alpha_i)}$.  Assuming a fixed weight decay (so
$\alpha_i = \eta_i\lambda$), coefficients will be equal if the LR
evolves:
\begin{eqnarray}\label{eqn:rational}
  \eta_{i+1} = \frac{\eta_i}{(1+\eta_i\lambda)}
\end{eqnarray}
We can initialize $\eta_0$ to some value and iterate
Eq.~\ref{eqn:rational} to generate the full LR schedule.  We call this
the $\rational$ schedule since it is both a rational expression of
$\eta_i$ and a very reasonable approach: regardless of how long we
train, all weight updates contribute equally.  At each step we
effectively decrease all prior coefficients such that they now equal
the coefficient of the current update, $\alpha_t$.
%
% This may minimize catastrophic forgeting.

With $\lambda = 1$ and no warmup, this schedule evolves like
$\nicefrac{1}{n}$.  However, in order to distance ourselves from the
initial conditions, we can warmup the LR in the usual manner to the
desired max LR, then switch on rational decay, ensuring equal
coefficients going forward (depicted in appendix
Figure~\ref{fig:more_ema_lrs:rational}).
%
However, such a schedule will almost surely not work as effectively as
$\linear$ $\dtoz$ for fixed-duration training, since it lacks the
cooldown phase where gradient noise is minimized.  Indeed,
$\nicefrac{1}{n}$ has performed relatively poorly in prior
studies~\citep{ge2019step,defazio2023when}.
%
However, we introduce it here as a promising schedule for
\emph{continuous} pretraining.\footnote{In fact, \nicefrac{1}{n}
decay has long been regarded as optimal for strongly-convex
loss~\citep{robbins1951stochastic}, and optimal in other contexts when
combined with \emph{averaging}~\citep{defazio2023when}.  Since
averaging is an alternative to cooldown~\citep{hagele2024scaling},
$\rational$ plus averaging is a compelling schedule-free direction.}
%
%The implication is: no matter how long we train for, we will always
%incorporate all the knowledge gained previously.  No catastrophic
%forgetting.

%Fascinating: ``The Robbins-Monro conditions are the foundation of
%early learning rate theory (Robbins and Monro, 1951).  They advocated
%for step size sequences where ... of the schedules satisfying these
%conditions, they advocated for schedules with 1/t decay as they are
%asymptotically optimal for twice continuously differentiable and
%strongly convex functions. This schedule was later shown to be optimal
%(even non-asymptotically) for strongly convex G-Lipschitz stochastic
%optimization when appropriate averaging is used (Shamir and Zhang,
%2013; Lacoste-Julien et al., 2012; Rakhlin et al., 2012).''  So
%averaging takes the place of cooldown, and this is apparently optimal
%-- this is exactly what you might have thought!
