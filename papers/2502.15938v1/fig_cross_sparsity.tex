\begin{figure}
  \centering
  \scalebox{\onefigscale}{
    \includegraphics[width=\textwidth]{pdffigs/cross_sparsities.pdf}
  }
  \mbox{}
  \vspace{-1mm}
  \mbox{}
  \caption{\textbf{Comparison of decay schedules when using
      \emph{varying} weight sparsity} (610M): Validation losses for
    models trained with $\supar$, as unstructured weight sparsity
    increases (density decreases).  All models trained with the same
    number of training tokens (11752 steps, equivalent to 20~TPP for
    the fully-dense models).  As sparsity increases, the number of
    trainable parameters decreases, and thus
    tokens-per-(non-zero)-parameter (and the benefit of $\dtoz$)
    increases.\label{fig:cross_sparsity}}
\end{figure}
