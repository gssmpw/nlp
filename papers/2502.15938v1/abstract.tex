%Many different learning rate (LR) schedules have been used to train
%LLMs.
%
%With little understanding of which schedules work best in which
%situations,
%
LLMs are commonly trained with a learning rate (LR) warmup, followed
by cosine decay to 10\% of the maximum ($\tenx$ decay).
%
In a large-scale empirical study, we show that under an optimal peak
LR, a simple linear decay-to-zero ($\dtoz$) schedule consistently
outperforms other schedules when training at compute-optimal dataset
sizes.
%
$\dtoz$ is superior across a range of model sizes, batch sizes,
datasets, and vocabularies.  Benefits increase as dataset size
increases.
%
Leveraging a novel interpretation of AdamW as an exponential moving
average of weight updates, we show how linear $\dtoz$ optimally
balances the demands of early training (moving away from initial
conditions) and late training (averaging over more updates in order to
mitigate gradient noise).
%
In experiments, a 610M-parameter model trained for 80
tokens-per-parameter (TPP) using $\dtoz$ achieves \emph{lower} loss
than when trained for 200~TPP using $\tenx$ decay, corresponding to an
astonishing 60\% compute savings.
%
Models such as Llama2-7B, trained for 286~TPP with $\tenx$ decay,
could likely have saved a majority of compute by training with
$\dtoz$.

%experimental settings. These observations prove robust
%across model sizes, batch sizes, architectures, weight-sparsity
%levels, and parameterizations.

%We explain the success of linear $\dtoz$ by interpreting AdamW as a
%convex combination of weight updates, with combination coefficients a
%function of the LR schedule; linear $\dtoz$ is best able to balance
%effective early training (combining a narrow window of updates in
%order to forget initial conditions) and reducing gradient noise later
%on (smoothing over many updates).
