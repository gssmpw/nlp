\section{Experimental Details}\label{sec:experimental_details}

\input{tab_model_info.tex}

\input{tab_train_steps.tex}

\cref{tab:model_info} provides details on model architecture and
hyperparameters for the main experiments (i.e., results presented in
the main paper).  \cref{tab:train_steps} provides information on
the training steps.  All the models in our main experiments were
trained on the SlimPajama dataset~\citep{cerebras2023slimpajama}, a
cleaned and deduplicated version of the RedPajama dataset.  We use the
GPT-2~\citep{radford2019gpt2} vocabulary of size 50257, and a context
length of 2048 tokens.
%
Unless otherwise noted, the weight decay value, $\lambda$, is by
default set to 0.1, following standard practice in LLM
pre-training~\citep{brown2020language,hoffmann2022empirical,almazrouei2023falcon,alephalpha2024introducing}.
%
Also following standard practice, we do not apply weight decay or bias
to LayerNorm layers.
%
Validation loss is always computed over 1.1B held-out tokens,
regardless of training TPP\@.
%
By default we parameterize with $\mup$ (further details below).

For a given TPP, all models have the exact same warmup phase: a linear
warmup of the LR from 0 to the peak value.
%
In all our runs, warmup was 10\% of the total steps.
%
LR warmup is standard practice in LLM pre-training.\footnote{While
prior work has suggested LR warmup is less valuable in modern Pre-LN
Transformers~\citep{xiong2020layer}, various other studies have shown
warmup leads to lower
loss~\citep{goyal2017imagenet1hour,liu2019variance,tissue2024scaling,kosson2024analyzing},
and may reduce sensitivity to peak LR~\citep{wortsman2023small}.  In
light of the similar benefits of $\dtoz$, it would be interesting to
investigate the value of warmup for models that are specifically
trained using $\dtoz$.}

All models in the main experiments were trained on a Cerebras CS-3
system.  610M-parameter, 20~TPP models take roughly 6 hours each to
train on a single CS-3.
%
If a training run did not complete due to numerical instabilities, the
values are left off our plots or marked as \texttt{NaN} in
\cref{tab:sched_compare}.
%
%Give a table with our $\mup$ changes.

% Tissue: Our pilot experiment (refer to Appendix A) shows that warmup
% indeed significantly accelerates convergence, a finding also noted
% by Liu et al. (2020); Kosson et al. (2024).
% Li: "The learning rate warmup heuristic achieves remarkable success in stabilizing training"
% Kosson: "This aids training"

% Ibrahim: Goyal et al. (2018) found that a gradual warm-up of LR
%early on in training can help overcome optimization challenges,
%particularly with large mini-batch sizes. Additionally, Popel & Bojar
%(2018) emphasized the importance of a warm-up stage when training
%Post-LN Transformers.  On the other hand, Xiong et al. (2020)
%discovered that Pre-LN Transformers are more stable and may not
%require a warm-up stage.
%Longer warmup reduces LR sensitivity!~\citep{wortsman2023small} -- why
%might that be?  I mean, it does move you away from the initial
%conditions... 
%
%Might be worth perusing: Justin Gilmer, Behrooz Ghorbani, Ankush Garg,
%Sneha Kudugunta, Behnam Neyshabur, David Cardoze, George Dahl, Zachary
%Nado, and Orhan Firat. A loss curvature perspective on training
%instability in deep learning. arXiv preprint arXiv:2110.04369,
%2021. -- something about warmups helping.  But warmup isn't the point
%of your work.

\paragraph{Proxy model hyperparameter tuning}\label{sec:proxy_tuning}

\input{tab_mup_hps.tex}

To find the optimal $\mup$ hyperparameters (HPs), we trained a
39M-parameter proxy model using a width $d_{\text{model}}$=$d_p$ of
256, with 24 layers and a head size of 64.  We trained this proxy
model on 800M tokens with a batch size of 256 and context length 2048,
using $\tenx$ decay.  We randomly sampled 350 configurations of base
learning rates, base initialization standard deviation, and embedding
and output logits scaling factors, and used the top-performing values
as our tuned HPs (\cref{tab:mup_hps}).

\section{Derivations}\label{sec:derivations}

\subsection{Derivation of \cref{eq:c_bias}}\label{sec:c_bias_derivation}

The initial coefficient is $c_{t,1} = \prod_{j=2}^{t} (1 - \alpha_j)$.
Clearly, if $\alpha_j = \alpha$ is constant, we have $c_{t,1} = (1 - \alpha)^{t-1}$.
%
Otherwise, given $\alpha_j$ is small, we can use a first-order Taylor expansion,
$e^{-\alpha_j} \approx 1 - \alpha_j$, and therefore:
\begin{align*}
  c_{t,1} &\approx \prod_{j=2}^{t} e^{-\alpha_j} \\
  &= e^{\sum_{j=2}^{t} -\alpha_j} \\
  &= e^{- \sum_{j=2}^{t} \alpha_j}. \\
\end{align*}
Assuming we only know the average value, $\baralpha = \frac{1}{t-1} \sum_{j=2}^{t} \alpha_j$,
  we have:
\begin{align*}
  c_{t,1} &\approx e^{-\baralpha(t-1)} \\
          &= (e^{-\baralpha})^{t-1}. \\
\end{align*}
Given $\baralpha$ is also small, we reverse the earlier Taylor
expansion, but now $e^{-\baralpha} \approx 1 - \baralpha$, and:
\begin{align*}
  c_{t,1} &\approx (1 - \baralpha)^{t-1}. \\
\end{align*}

That is, with a small time-varying smoothing parameter, the initial
coefficient in an EMA, $c_{t,1}$, also decreases exponentially with
the number of steps.

\section{Additional experimental results}

In this section, we include some additional results to support the
findings in the main paper.  All validation losses reported in this
section are from models trained with $\linear$ decay.

\subsection{Downstream evaluations}\label{subsec:downstream}

\input{tab_downstream1.tex}

\finding{Trends in validation loss also hold for \underline{downstream} evaluation.}

\cref{tab:downstream1} presents a variety of downstream
evaluations of the four models presented in
\cref{fig:train_loss}.
%
Differences between the models here are largely consistent with the
differences in training and validation loss, showing that $\dtoz$ is
meaningful not just for the autoregressive training objective, but for
real-world applications.

\subsection{Standard parameterization}\label{subsec:sp}

\input{fig_maxlr_SP.curves.tex}

\finding{Similar trends hold for the standard parameterization.}

\cref{fig:maxlr_sp.curves} presents results for a 610M-parameter
model trained with the standard parameterization.  Here $\hateta$ is
therefore not a $\mup$-corrected base LR, but rather a LR that we
swept directly for this model scale.
%
Results are obviously quite similar to results using $\mup$,
suggesting the benefits of $\dtoz$ are not $\mup$-specific.  Further
results using the standard parameterization, but for NanoGPT models,
are in \cref{subsec:nanogpt} below.

\subsection{Model sizes}\label{subsec:model_sizes}

\input{fig_maxlr_models.tex}

\finding{Improvement of $\dtoz$ over $\tenx$ is greater at 1.7B
  scale.}

\cref{fig:maxlr_tpp:models} presents results across 111M, 610M,
and 1.7B model sizes, all trained to 20 TPP\@.  Note the absence of
results for the highest LR setting at the 1.7B-scale; at the very
highest LR, numerical instabilities led to failed training runs.
Otherwise, results are fairly similar across model sizes.  At the
proxy-tuned peak LR, the gap between $\dtoz$ and $\tenx$ is 0.81\%,
0.67\%, and 1.56\%, at the 111M, 610M, and 1.7B scales, respectively.
We further investigate the issue of whether the gap between $\dtoz$
and $\tenx$ varies with model size as part of our scaling law
experiments below (\cref{subsec:scaling}).

\subsection{TPP}\label{subsec:tpp}

\input{fig_maxlr_tpp.617M.tex}

\finding{Improvement of $\dtoz$ over $\tenx$ grows with TPP (610M scale).}

As we vary TPP, we consistently see increasing gains with $\dtoz$.
Here we plot the results for the 610M-scale models in
\cref{fig:maxlr_tpp:610M}, as a counterpart to main
\cref{fig:maxlr_tpp:111M}.

\subsection{Batch sizes}\label{subsec:batches}

\paragraph{Batch size setup}

Default batch sizes of 192 and 504 (\cref{tab:model_info}) were
selected for 111M and 610M scales.  These specific values were based
on an internal scaling law for optimal batch size as a function of
compute FLOPs (similar to those in~\citet{bi2024deepseek}
and~\citet{porian2024resolving}).  Batch size of 504 was then re-used
for 1.7B training (later testing confirmed this to be a good setting
at 1.7B for 20~TPP training).  In our experiments varying batch size,
we swept $B$ by factors of 2 around these initial settings.

\subsubsection{Iso-TPP batch size experiments}

\input{fig_maxlr_isotppbatches.sep_batches.tex}

\finding{Improvement of $\dtoz$ over $\tenx$ grows as batch size decreases (at constant TPP).}

In main paper \cref{fig:maxlr_isotppbatches}, we presented results
where all models train for the same number of total \emph{tokens}
(20~TPP), while only the batch size, $B$, varies.
%
\cref{fig:maxlr_isotppbatches_sep_batches} presents the same data, but
with each $B$ separated into a separate subplot; this lets us better
observe how the differences between $\dtoz$ and $\tenx$ evolve as $B$
changes.
%
We see clearly that for smaller $B$, as gradient noise increases, the
differences between $\dtoz$ and $\tenx$ also increase.  This again
demonstrates that LR decay is beneficial as a noise reduction
mechanism.

We now discuss the related observation that the optimal LR decreases
as $B$ decreases.  Note the EMA perspective of \citet{wang2024how}
would predict linear scaling of $\hateta$ with $B$.  This is because
the total number of iterations scales with $1/B$.  Therefore, in order
to keep the timescale constant (as a fraction of the total
iterations), we would need to scale $\hateta$ (or $\lambda$)
proportional to any change in $B$.
%
Although we do see roughly linear scaling in the case of $\tenx$
models, the optimal $\hateta$ is more stable with $\dtoz$.
%
In our conceptual model, the purpose of the timescale is to optimize
variance reduction.
%
But note that lowering $\hateta$ also has an effect on bias reduction.
For $\tenx$, as $B$ changes, it seems paying the cost of lower bias
reduction is worth the benefits in variance reduction.  Since using
$\dtoz$ already enables better variance reduction, it seems, for
$\dtoz$, lowering $\hateta$ has less value in the trade-off, so
optimal $\hateta$ lowers to a lesser extent.

\subsubsection{Iso-step batch size experiments} 

\input{fig_maxlr_isostepbatches.tex}

\finding{Optimal LR \underline{increases} with batch size when
  training over the same number of batches/steps.}

In the above batch size tests, we kept the dataset size constant
(iso-TPP).  Now consider the following two mechanisms for increasing
the dataset size (TPP) of a training run:
\begin{enumerate}
\item Fix the batch size, but increase the number of steps.
\item Fix the number of steps, but increase the batch size.
\end{enumerate}
Approach (1) was taken in our experiments increasing TPP\@.  In that
case, we found optimal LR to decrease, as a mechanism for coping with
greater gradient noise, which grows with TPP\@.
%
We now discuss Approach (2): fixing the number of steps, but
increasing the batch size (iso-step).

In \cref{fig:maxlr_isostepbatches}, we keep the number of \emph{steps}
constant to 11752, so each model will see the same total number of
batches, and we increase $B$ by factors of two.\footnote{Note, for
purposes of scale, results at $B$=$504$ are the same in
\cref{fig:maxlr_isotppbatches_sep_batches} and
\cref{fig:maxlr_isostepbatches}; the latter just has a larger range on
the y-axis.}
%
Since number of steps does not change, \citet{wang2024how} do not
prescribe any adjustments to the timescale, and hence no adjustment is
needed to LR (or weight decay).
%
But we clearly see that optimal LR increases as $B$ increases
(like a rolling wheel, LR bowls rotate clockwise as we move to the
right).
%
This is evidently because increasing $B$ decreases gradient noise;
with less noise, we can afford a larger LR throughout training.
%
More precisely, increasing $\hateta$ decreases the number of AdamW
weight updates that are combined.  Therefore, an increase in $\hateta$
can be viewed as a way to reduce the effective number of batches that
are combined, basically compensating for the fact per-step $B$ is
larger.

As $B$ increases, gradient noise decreases, and we would expect
benefits of LR decay to also diminish; differences between $\tenx$ and
$\dtoz$ do seem to decrease.
%
This aligns with prior work, e.g., \citet{you2019how} compared SGD to
full-batch GD (with WideResNets for image classification), and showed
that when gradient noise is eliminated by using full GD, LR decay is
no longer beneficial.

%The deep connection optimal batch sizes of course also deeply depend on
%gradient noise~\citep{mccandlish2018empirical}.
%, it has a similar effect to decreasing the
%batch size

%Not to be confused with increasing the batch size and decreasing the
%number of steps, where \citet{wang2024how} would prescribe decreasing
%the timescale, so increasing the LR proportionally.  In this case, the
%number of iterations is constant, so \citeauthor{wang2024how} wouldn't
%advise doing anything specifically.
%
%Our results clarify that reducing $\alpha=\eta\lambda$ is primarily
%beneficial as a noise-reduction mechanism.
%
%Recall that in the view of \citet{wang2024how}, the benefit of
%$\alpha$ is to optimize the timescale of \emph{data}; if we double
%the amount of data, we should double the steps that we smooth over,
%e.g., \emph{decrease} $\eta$ or $\lambda$ by a factor of 2.
%%
%But consider \cref{fig:maxlr_isostepbatches}: as we double the amount
%of data (via doubling of the batch size and keeping the same number of
%steps), the optimal LR actually \emph{increases} rather than
%decreases.

\subsection{Weight Decay}\label{subsec:wd}

\subsubsection{Weight decay: results}
 
\finding{Benefits of weight decay are observed primarily when using LR
  $\dtoz$.}

\input{fig_maxlr_wd.617M.tex}

\cref{fig:maxlr_wd:610M} analyzes the interaction between \emph{weight
decay} (WD) and LR for 610M models.
%
Here we plot with the x-axis set to the (peak) smoothing parameter,
$\alpha$=$\eta\lambda$ (on a log scale), and y-axis set to validation
loss.  Note that for a given decay ratio and fixed $\alpha$, EMA
update coefficients (\cref{sec:emas}) are identical (i.e., specific
values of $\eta$ and $\lambda$ do not affect coefficients, only their
product).
%
Thus it supports the extended EMA perspective to note that regardless
of peak LR, $\hateta$, models tend to reach lowest loss at around the
same $\alpha$ (\cref{fig:maxlr_wd:610M}); that is, the same EMA
timescale, $\nicefrac{1}{\eta \lambda}$, is optimal across different
LRs.  However, to reach optimal loss, models require $\hateta$ to be
high enough, but not too high --- at $\hateta$=$6.5$e-$02$ and above,
we consistently encounter instabilities in training.

\input{fig_maxlr_wd.111M.tex}

When increasing $\lambda$ with a given peak LR (moving left-to-right
along curves), note behavior differs depending on LR schedule:
$\constant$ models perform worse, $\linear$-$\tenx$ sees marginal
gains, and $\linear$-$\dtoz$ benefits significantly.  Results are
similar for 111M models (\cref{fig:maxlr_wd:111M}).

\input{fig_maxlr_wd.617M.wd_groups.tex}
\input{fig_maxlr_wd.111M.wd_groups.tex}

\cref{fig:maxlr_wdgroups:610M,fig:maxlr_wdgroups:111M} depict the same
data, but with points now grouped by $\lambda$.\footnote{Note
\cref{fig:maxlr_wdgroups:610M} excludes some points from
\cref{fig:maxlr_wd:610M}: \cref{fig:maxlr_wd:610M} includes some
additional $\lambda$ settings specifically for $\hateta$=$\maxlr$, and
we leave those off this figure in order to reduce clutter.} For higher
or lower $\lambda$, we cannot reach the optimal timescale without
taking $\hateta$ out of its comfort zone.  Therefore, with our 20 TPP
models, $\lambda$=0.1 emerges as good default setting because, in
synergy with a comfortable LR, $\lambda$=0.1 corresponds to an
appropriate EMA timescale.

Similar to the findings in \citet{andriushchenko2023why}, we also
confirm weight decay does not act like a traditional regularizer here:
beneficial settings of weight decay always improve both validation
\emph{and} training loss.

%TODO: note Loshchilov results here too.
%
% ``weight decay does not provide benefits when training with constant
% LRs as shown in Fig. 5 (right) emphasizing the importance of its
% interaction with LR decay'' -- I think we reach a similar
% explanation for this.

% although these works don't also optimize the peak LR.  Also,
% Andriushchenko believes Chinchilla compared WD to no WD, but I'm
% personally not so sure.  It just says they compare Adam to AdamW.
% But Adam *could* have a WD value.

%We also provide additional weight decay results.
%\cref{fig:maxlr_wdgroups:610M} is the same data as in
%\cref{fig:maxlr_wd:610M}, except we now group the points by weight
%decay $\lambda$ rather than peak LR $\hateta$ 

%\cref{fig:maxlr_wd:111M,fig:maxlr_wdgroups:111M}
%provide the 111M counterparts to the 610M-scale weight decay plots.
%Results are broadly similar.

\subsubsection{Weight decay: further discussion}

\paragraph{Learning rates reduce bias, weight decay best controls variance}

Recall that \cref{hyp:wdbias} made the observation that while LR
$\eta$ and weight decay $\lambda$ can both equally change the update
coefficients, only $\eta$ is effective in moving the model away from
initial conditions (due to the $\nicefrac{1}{\lambda}$ factor applied
to the updates).
%
This is confirmed by \cref{fig:maxlr_wd:610M,fig:maxlr_wd:111M}: only
when $\eta$ is sufficiently high do models achieve optimal loss.
%
Given such a high-enough $\eta$, the primary benefit of weight decay
is evidently to adjust $\alpha$=$\eta\lambda$ (and hence the timescale
$\nicefrac{1}{\alpha}$), to a setting that synergizes best with the
decay schedule (for 610M models, around $4$e-$04$).
%
Weight decay is specifically useful in this context because training
instabilities prevent reaching this optimal $\alpha$ purely through
increasing $\eta$.
% Just for your own sanity: you can't take a lower LR and increase WD,
% or you won't make initial progress.  You can't take a higher WD and
% decrease WD, as it would be unstable.
%
However, given $\eta$ and $\lambda$ both affect the timescale, $\alpha$=$\eta\lambda$
can be viewed as the \emph{effective} or
\emph{intrinsic} LR~\citep{li2020reconciling,wang2024how}, but only
later in training.

Since weight decay does not impact bias reduction, it follows that,
when the timescale needs to be adjusted at \emph{high TPP} due to the
greater gradient noise, it should be more effective to lower WD than
to lower LR, exactly as prescribed by \citet{wang2024how}.
%
In ongoing concurrent work, we do observe lower loss from tuning WD
compared to tuning LR\@.
%
This stands in contrast to other recent work that recommends purely
decreases in LR for high-TPP
training~\citep{shen2024power,bjorck2024scaling}.

\paragraph{Why weight decay is only beneficial when using LR decay}

Our observation that weight decay is only beneficial when using a LR
(decay) schedule aligns with both \citet{loshchilov2017decoupled} and
\citet{andriushchenko2023why} who also observed no benefit from WD
when using constant LRs.
%
In contrast, \citet{alephalpha2024introducing} recently reported
$\lambda$=$0.1$ to improve over $\lambda$=$0.0$ in LLM training;
notably, they train with $\dtoz$.
%
We believe the reason for these observations can also be explained by
the bias/variance trade-off, and by the fact, noted above, that weight
decay primarily plays a role later in training.
%
Since its LR is fixed, $\constant$ negotiates the bias/variance
trade-off by using a LR that is suboptimally \emph{low} early and
suboptimally \emph{high} later on (refer back to
\cref{fig:bias_var_all} in the main paper for a visualization).
Increasing $\lambda$ raises the already-too-high (late) $\alpha$ even
higher, further decreasing the timescale $\nicefrac{1}{\alpha}$ and
hurting the model.
%
In contrast, when using a LR schedule, weight decay can play its
beneficial variance reduction role, as discussed above.

It is also worth observing that if one is using $\dtoz$ with a LR that
is suboptimally low (e.g., if increasing it any further caused
numerical stability issues, as we observed with NanoGPT in
\cref{subsec:nanogpt}), then increasing weight decay can be very
beneficial.  For example, note the large improvements in loss as
weight decay is increased for the $\hateta$=$4.0$e-$3$ and
$\hateta$=$2.0$e-$3$ curves in \cref{fig:maxlr_wd:610M}.

\paragraph{Stability of $\dtoz$ loss with high $\lambda$}

%Why is LR more stable?
%
While $\constant$ and $\tenx$ are much worse at higher $\lambda$
values, $\dtoz$ performs reasonably well even at $\lambda$=$1.0$
(particularly at the 610M scale).  Since increasing $\lambda$
effectively decreases the timescale $\nicefrac{1}{\alpha}$ over which
weight updates are integrated, it increases the variance over weight
updates.  Therefore, setting $\lambda$ too high hurts all models, but
since $\dtoz$ has lower $\eta$ later in training, it somewhat
counterbalances the increase in $\lambda$ in the product
$\alpha$=$\eta\lambda$.  Another view of this is that as fewer updates
are combined, noise increases; $\dtoz$ is evidently best at mitigating
the negative effects of such noise through its maximal LR decay.

\subsection{Step decay}

\input{fig_maxlr_step.curves.tex}

\finding{Step decay helps $\constant$, but does not provide benefits
  if already decaying the LR.}

In \cref{fig:maxlr_step.curves}, we present results investigating the
impact of $\step$ decay on model training.  Here, for 111M-parameter
models, $\step$ decay can improve the loss versus keeping the LR
\underline{constant} (left panel), but the resulting losses are still
much worse than those obtained with $\dtoz$ or $\tenx$ decay.  We also
tried applying a $\step$ decay to a LR that had been following a
$\tenx$ (middle panel) or $\dtoz$ trajectory (right panel); this
approach always led to inferior results.

While it is likely possible to improve the quality of $\step$ decay by
tuning the positioning of the drop, we hypothesize that these efforts
will not surpass $\dtoz$, since dropping the LR will fundamentally
always result in an over-emphasis of earlier updates, as shown in
\cref{fig:background_ema} and \cref{fig:img_emas}.  Moreover,
introducing additional tunable hyperparameters (i.e., when and how
much to decay) is a further drawback of the $\step$ schedule.

%\paragraph{Inverse square root decay variant}

%Note that we always performed $\invsqr$ decay after performing our
%standard linear increase in the LR warmup.  This follows the approach
%in \citet{vaswani2017attention} and also
%\href{https://huggingface.co/docs/transformers/en/main\_classes/optimizer\_schedules\#transformers.get\_inverse\_sqrt\_schedule}{some
%  LLM libraries}).  Practitioners should be aware that warming up at a
%constant LR has also been explored~\citep{raffel2020exploring}, and is
%the approach used in
%\href{https://docs.cerebras.net/en/rel-2.3.0/wsc/api/cerebras\_pytorch/optim.html\#cerebras.pytorch.optim.lr\_scheduler.InverseSquareRootDecayLR}{other
%  libraries}).
%
%We also tried another kind of inverse square root decay, where the
%step is reset to zero for the decay phase, but this one drops too
%quickly.  This leads us to conclude that $\invsqrt$ is sensitive to
%the length of warmup (as we show in
%\cref{fig:more_ema_lrs}), which is not widely appreciated.

\subsection{Weight sparsity}\label{subsec:sparsity}

\finding{Relative improvement of $\dtoz$ over $\tenx$ \emph{increases} with weight sparsity.}

\paragraph{Weight sparsity setup}

In this section, we investigate the role of $\linear$-$\dtoz$ in the
context of models trained with \emph{unstructured weight sparsity}, a
promising direction for improving the efficiency of large neural
networks~\citep{hoefler2021sparsity}.
%
We parameterize with $\mup$'s sparse extension,
$\supar$~\citep{dey2024sparse}.  $\supar$ allows us to use the same
$\mup$ hyperparameters as with dense models, except we must now apply
corrections due to both model scaling (i.e., $\rho$,
\cref{subsec:mup}) and layer sparsity.
%
For these experiments, we sparsified all non-embedding layers of our
610M-parameter dense models by randomly fixing certain weights to zero
for the duration of training.
%
We trained all models for 11752 total steps using a batch size of 504,
i.e., the same amount of training data as we used for training the
corresponding 610M-parameter dense models to 20~TPP\@
(\cref{tab:model_info,tab:train_steps}).

%% Skip:
%%\input{fig_maxlr_s=0.9375.heatmap.tex}
\input{fig_maxlr_s=0.9375.curves.tex}

At 93.75\% sparsity ($\nicefrac{1}{16}$ density), the optimal $\dtoz$
model improves by 1.64\% over the optimal $\tenx$ model, with a clear
trend of optimal LRs shifting lower and loss becoming worse as we go
from $\tenx$ to $3\times$ to $\constant$ decay
(\cref{fig:maxlr_s=0.9375.curves}).

\begin{figure}
\noindent % Ensures the minipages align with the left margin
\begin{minipage}[b]{0.49\textwidth}
  \centering
\input{fig_different_ratios.tex}
\end{minipage}%
\hfill % Use to fill space between the minipages
\begin{minipage}[b]{0.49\textwidth}
  \centering
  \input{fig_different_ratios_new.tex}
\end{minipage}
\end{figure}

At the proxy-tuned peak LR of $\maxlr$, $\dtoz$ is 2.15\% better than
$\tenx$.  We also trained 93.75\% sparse models with a variety of
other decay ratios between $\tenx$ and $\dtoz$, and present these
results in \cref{fig:different_ratios,fig:different_ratios_new}.
%
Here we see a largely linear decrease in loss with a linear increase
in the decay ratio (i.e., a linear decrease in the minimum LR).
%
These are encouraging findings in the sense that $\dtoz$ can seemingly
be used directly on a range of problems, without having to worry about
tuning a problem-specific LR decay ratio (e.g., 50$\times$ or
100$\times$).

\input{fig_cross_sparsity.tex}

In \cref{fig:cross_sparsity}, we investigate the gap between
$\dtoz$ and $\tenx$ at the proxy-tuned peak LR across different sparsity
levels.  Note that increasing sparsity effectively leads to a
corresponding decrease in the number of trainable parameters.  Since
we use a fixed number of training tokens in each case, as the number
of parameters decreases, the number of tokens-per-parameter (TPP)
\emph{increases}.
%
In this sense, we note the relative differences between $\dtoz$ and
$\tenx$ are consistent with our results in \cref{fig:more_tpp}
--- as TPP (and gradient noise) increases, $\dtoz$ performs relatively
better.

\subsection{Error bars}

\finding{Run-to-run variation in results is low.}

\input{fig_seeds.tex}

Taken as a whole, our results are remarkably stable: empirical results
for different model scales, training durations, batch sizes, weight
decays, and weight sparsity settings largely behave as predicted by
theory.  Since all validation runs are performed on 1.1B tokens, any
significant run-to-run variance must arise during training.
%
To quantify this variance, we repeated 111M-model 20~TPP training four
additional times, resulting in 5 total validation loss results for
each original training run.  \cref{fig:seeds} confirms that
run-to-run variance is remarkably low.  The only significant variance
arises in $\constant$ results at the highest learning rate.  Here, the
training loss sometimes spikes at various points during training, and
the final validation loss can be significantly higher for models that
cannot recover sufficiently following the spike.

\subsection{NanoGPT experiments}\label{subsec:nanogpt}

\finding{Improvement of $\dtoz$ over $\tenx$ persists in NanoGPT
  models --- when numerical issues are solved.}

We also compared $\tenx$ versus $\dtoz$ by training NanoGPT models
using the NanoGPT codebase \citep{karpathy2024nanogpt}.
%
NanoGPT uses the standard parameterization.  We configured these
models to be largely similar to our 111M-parameter models, also using
a weight decay of 0.1, a context length of 2048, and the GPT-2 vocab
size of 50257.  Key differences here are that we do not include bias
weights, and we trained on the OpenWebText dataset
\citep{gokaslan2019owt}.
%
Experiments are run on Nvidia A10 GPUs.

As mentioned in \cref{subsec:confounding}, we initially tested
NanoGPT in \verb|bfloat16| precision.  Here, at the default NanoGPT
learning rate of 6e-4, $\tenx$ performed slightly better than $\dtoz$.
As we pushed the LR 50\% higher (to 9e-4), both $\tenx$ and $\dtoz$
had higher loss, and by 1.2e-3, the loss from $\tenx$ doubled.

\input{fig_maxlr_nanogpt.curves.tex}

We suspected that numerical issues may be causing the instabilities,
and repeated our experiments in \verb|float32|.  At this precision, we
were able to successfully increase the LR, by factors of two, up to 32
times the default.
%
At these levels, we do see the familiar gains of $\dtoz$ over $\tenx$
(\cref{fig:maxlr_nanogpt_curves}).
%
We note there is nothing fundamentally limiting about \verb|float16|
precision itself - indeed all our main experiments were done using
this precision.  Rather, \verb|float16| is simply problematic at a
high learning rate in the NanoGPT codebase (see \citet[Appendix
  C.2]{gray2024normalization} for a potential root cause of this
instability).

These experiments demonstrate that a comparison between $\dtoz$ and
$\tenx$ may serve as a kind of \emph{diagnostic} of whether a model is
being trained at optimal peak LRs: if a 100M+ model trained to 20~TPP
does not see roughly 1\% gains from using $\dtoz$, it is likely the LR
is not high enough.  In order to raise the LR further, efforts to
stabilize the model, perhaps including $\mup$ or other
techniques~\citep{wortsman2023small}, may be warranted.

\subsection{Scaling law experiments}\label{subsec:scaling}

\finding{Improvement of $\dtoz$ over $\tenx$ grows across model scales, for a different model and training setup.}

Encouraged by the results of $\dtoz$ at smaller scales, we began
testing $\dtoz$ in some of our frontier model efforts.  Frontier
models do not provide scope for hyperparameter tuning at scale.  Thus
it becomes important to derive scaling laws to forecast loss at larger
scales, based on the loss with a sequence of smaller models.

For this set of experiments, we tested
Llama-style~\citep{touvron2023llama} architectures, except using
LayerNorm instead of RMSNorm, and multi-head attention instead of
group-query attention.  We use $\mup$ here as well, and a batch size
scaling law to determine an optimal batch size for each model scale.
%
These models also use ALiBi embeddings~\citep{press2022alibi} and
SwiGLU~\citep{shazeer2020glu}.  Here, the context length is 8192
tokens, and we use tied embeddings.
%
We also re-tuned our $\mup$ proxy-model hyperparameters.

We used a bilingual data mix of English, Arabic and source code
samples mixed in a 2:1:0.4 mix ratio. English data is from
the~Pile~\citep{gao2020pile}, Arabic uses a proprietary
dataset~\citep{sengupta2023jais}, and the source code comes from the
GitHub portion of the Pile.

\input{tab_model_info_scaling.tex}

To derive scaling laws to compare $\dtoz$ and $\tenx$ models, we used
the power law functional form $y = c x^m$, where $x$ is the
pre-training FLOPs, $y$ is the loss on the Pile validation set, and
$c$ and $m$ are parameters to be fit.
%
To fit these parameters, we transformed our Loss-to-FLOPs equation, $y
= c x^m$, to a logarithmic form, $log(y)=log(c)+m log(x)$, and fit the
slope and intercept parameters of this line using a least-squares
linear regression.
%
We trained models at four sizes to compute-optimal 20~TPP
(\cref{tab:model_info_scaling}), and computed total FLOPs spent
as well as validation loss on the Pile.
%
We then fit the power law free parameters to obtain our scaling laws.

\input{fig_pile_scaling.tex}

Encouragingly, here we find the scaling law slope of $\dtoz$ is
roughly 2.5\% better than $\tenx$ decay
(\cref{fig:pile_scaling}).
%
This translates to an improvement of roughly 1\% at 1.3B and 2.7B
scales, broadly similar to our earlier results at 1.7B scale.
%
Projecting our scaling law to a 70B model trained to a compute optimal
20~TPP, $\dtoz$ would achieve a roughly 2\% loss improvement over
$\tenx$ decay.
% If we trained such a model to higher TPP in order to enable
% efficient inference, we suspect gains would multiply

\section{AdamW as convex combination of weight updates, driven by LR schedule: Further details}

We have begun using plots of the weight update coefficients internally
to proactively analyze LR schedules.  These plots give us insight into
the timescale over which training data is integrated into the model
parameters, and can be obtained without actually performing any
training.
%
Moreover, we can also use the extended EMA perspective in order to
design novel LR schedules that have desirable blends of weight
updates.  Optimizing these coefficients thus provides a dual to
optimizing the LR decay function, which is why we refer to these
coefficients as the \emph{dual coefficients} of the LR schedule.
%
In this section, we first describe the design of one such LR schedule,
and then we follow with example plots for this and other schedules.
%
%This formulation enables proactive analysis of existing LR schedules
%--- by calculating coefficients and judging how updates are integrated
%over training --- without needing to actually train.  For example,
%Figure~\ref{fig:background_ema} (Section~\ref{sec:intro}) compares the
%effect of $\linear$, $\cosine$, and $\step$ decay on the output at
%step $t$ of a 111M-parameter model trained for 200 TPP (2D plots of
%$c_{t,i}$ over all $t$ and $i$ are in appendix
%Figure~\ref{fig:img_emas}; see also Figures~\ref{fig:more_ema_lrs}
%and~\ref{fig:inf_ema_lrs}).

%To what extent do the bias and variance terms play a role in modern
%LLM training?  We hypothesize that the answer to this question is
%\emph{scale-invariant} given a fixed training tokens-per-parameter
%(TPP).  We know that, regardless of scale, models tend to reach the
%compute-efficient-training frontier at around
%20~TPP~\citep{hoffmann2022empirical}.  It seems unlikely that at some
%scales, models train efficiently to 20~TPP by mostly minimizing bias,
%while at others they train efficiently to 20~TPP by mostly grappling
%with variance.  It is more likely that 20~TPP is consistently
%efficient across scales precisely \emph{because} it corresponds to a
%balanced combination of bias and variance.
% models are trained in a compute-efficient manner.  That is, provided
%we train models so that either training FLOPs or inference FLOPs are
%minimized (corrsponding to 20~TPP following Chinchilla scaling
%laws~\citep{hoffmann2022empirical}),

\subsection{Truly schedule-free LR schedules}\label{subsec:rational}

Recall that, for a moving average with time-varying smoothing,
$y_t = (1 - \alpha_t)y_{t-1} + \alpha_t x_t$,
we use
$c_{t,i} = \left( \prod_{j=i+1}^{t} (1 - \alpha_j) \right) \alpha_i$
to denote the dual coefficients,
i.e., the contribution of input $x_i$ to output $y_t$ at time $t$.
%
$\constant$ schedules, or schedules with a long constant phase such as
$\wsd$, are not truly ``schedule-free'' because their peak LR setting
affects the $(1 - \alpha_j)$ terms in the dual.  Different LRs will
correspond to different effective timescales over updates, and thus
different LRs will be optimal for different training
durations.\footnote{The different shapes of the coefficients for
different LRs can be seen in \cref{fig:more_ema_lrs:constant} for
$\constant$ and \cref{fig:inf_ema_lrs:wsd} for $\wsd$.}

In contrast, we now derive a schedule such that coefficients are
always weighted equally, at every training step.  First, it can be
shown that:
\begin{equation}
    \frac{c_{t,i+1}}{c_{t,i}} = \frac{\alpha_{i+1}}{(1-\alpha_{i+1})\alpha_i}.
\end{equation}
%
For the coefficients to be uniform at any step $t$, we require this
ratio to be 1.  Unity of this ratio implies that smoothing evolves
$\alpha_{i+1}$ = $\frac{\alpha_i}{(1+\alpha_i)}$.  Assuming a fixed
weight decay (so $\alpha_i = \eta_i\lambda$), coefficients will be
equal if the LR evolves:
\begin{equation}\label{eqn:rational}
  \eta_{i+1} = \frac{\eta_i}{(1+\eta_i\lambda)}
\end{equation}
We can initialize $\eta_0$ to some value and iterate
\cref{eqn:rational} to generate the full LR schedule.  We call this
the $\rational$ schedule since it is both a rational expression of
$\eta_i$ and a very reasonable approach: regardless of how long we
train, all weight updates contribute equally.  At each step we
effectively decrease all prior coefficients such that they now equal
the coefficient of the current update, $\alpha_t$.
%
% This may minimize catastrophic forgeting.

With $\lambda$=$1$ and no warmup, this schedule evolves like
$\nicefrac{1}{n}$.  However, in order to distance ourselves from the
initial conditions, we can warmup the LR in the usual manner to the
desired peak LR, then switch on rational decay, ensuring equal
coefficients going forward.  The full schedule is depicted in
\cref{fig:more_ema_lrs:rational}.
%
However, such a schedule will almost surely not work as effectively as
$\linear$ $\dtoz$ for fixed-duration training, since it lacks the
cooldown phase where gradient noise is minimized.  Indeed,
$\nicefrac{1}{n}$ has performed relatively poorly in prior
studies~\citep{ge2019step,defazio2023when}.
%
However, we introduce it here as a promising schedule for
\emph{continuous} pre-training.\footnote{In fact, \nicefrac{1}{n}
decay has long been regarded as optimal for strongly-convex
loss~\citep{robbins1951stochastic}, and optimal in other contexts when
combined with \emph{averaging}~\citep{defazio2023when}.  Since
averaging is an alternative to cooldown~\citep{hagele2024scaling},
$\rational$ plus averaging is a compelling schedule-free direction.}
%
%The implication is: no matter how long we train for, we will always
%incorporate all the knowledge gained previously.  No catastrophic
%forgetting.

%Fascinating: ``The Robbins-Monro conditions are the foundation of
%early learning rate theory (Robbins and Monro, 1951).  They advocated
%for step size sequences where ... of the schedules satisfying these
%conditions, they advocated for schedules with 1/t decay as they are
%asymptotically optimal for twice continuously differentiable and
%strongly convex functions. This schedule was later shown to be optimal
%(even non-asymptotically) for strongly convex G-Lipschitz stochastic
%optimization when appropriate averaging is used (Shamir and Zhang,
%2013; Lacoste-Julien et al., 2012; Rakhlin et al., 2012).''  So
%averaging takes the place of cooldown, and this is apparently optimal
%-- this is exactly what you might have thought!

\subsection{LR curves and dual coefficients}

\input{fig_img_emas.tex}

\input{fig_more_ema_lrs.tex}

\input{fig_inf_ema_lrs.tex}

In this section, we provide some extra figures that were referenced in
the main paper.  \cref{fig:img_emas} shows the dual coefficients
at every step of training, using color to indicate the coefficient
value (log-scale).  Every horizontal row/step of
\cref{fig:img_emas} reflects the coefficients at that step,
essentially providing a version of \cref{fig:background_ema} but
at each step.
%
\cref{fig:more_ema_lrs} provides the LR schedules and dual
coefficients for some of the schedules discussed in the main paper, as
well as our proposed $\rational$ schedule, which combines all the
prior weight updates (after warmup) equally at every step.
%
Finally, \cref{fig:inf_ema_lrs} gives the LR schedules and dual
coefficients for the comparison to $\wsd$ and $\cyclic$ in
\cref{fig:maxLR_infinites}.

It is worth re-iterating that the dual coefficients can be computed
separately from any actual training.  They are mathematically
equivalent to the LR schedule itself and simply provide a perspective
on how the weight updates combine to form parameters, when using the
AdamW optimizer.
%
Furthermore, it is also worth noting the vertical bar at step 1 in the
dual coefficient plots; this bar reflects the coefficient on the
initial, random weights.
%
To some extent, this $c_{1,t}$ value can serve as an indicator of how
far the model has moved from initial conditions, i.e., the extent to
which the model has reduced the bias.  In general, if $c_{1,t}$ is too
high (e.g., when it outweighs the sum of the other coefficients), then
bias is likely significantly hindering learning.
%
Two effective ways to reduce $c_{1,t}$ and thus reduce bias are to
(1)~raise the peak LR, and (2)~train for more TPP\@.
%
In contrast to these two methods, raising weight decay, $\lambda$, can
also decrease $c_{1,t}$, but is counterproductive for reducing bias
because it also reduces the scale of weight updates, as noted in
\cref{hyp:wdbias}.  Likewise, using smaller batches also reduces
$c_{1,t}$, but is likewise counterproductive if the batches become too
small, to the extent that gradient noise increases.  However, $\dtoz$
is more robust to such noise than $\tenx$ or $\constant$ decay.
Further investigating the interplay of $\lambda$, batch size, and
peak learning rate is important future work.
% Make the point that WSD is unfair: -- UPDATE: this is not the point
%of the paper!  - It seems like WSD is unfair because if you *did*
%want intermediate checkpoints, say, 4 or 5 during the full run, from
%which you could make a scaling law, and you needed 20\% extra compute
%each time, well, you'd basically have to almost double the total
%amount of compute!  But cyclic doesn't have this!  - If you counted
%the decay part as part of your total budget, you'd end up training
%for fewer tokens.  So it's serious!  Cyclic has much more going for
%it.  - So is WSD-Cyclic actually better?
