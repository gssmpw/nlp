\vskip -1em
\section{Preliminary Studies}
\label{sec:preliminaries}
\begin{figure*}[t!]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/result_fail_ratio.pdf}
        \vskip -1em
        \caption{Win ratio (\%) of MLP v.s. teacher models across datasets under input-720-predict-96 setting. The win ratio is generally large (average: 49.92\%, median: 49.96\%), indicating MLP and teacher models excel on different samples with minimal overlap.}
        \label{fig:result_cv}
    \end{minipage}
    \hfill
    \begin{minipage}{0.315\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/result_multi_scale.pdf}
        \vskip -1em
        \caption{Visualization of model predictions on different downsampled scales of ECL dataset. MLP consistently shows poor performance at multiple scales, while other models perform well, highlighting the importance of capturing multi-scale patterns.}
        \label{fig:result_multi_scale}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/results_multi_period.pdf}
        \vskip -1em
       \caption{Prediction spectrograms of various models on ECL dataset against the ground truth. MLP fails to match the amplitudes of several main frequencies in the ground truth, with red numbers indicating amplitude differences for the most significant frequency.}
        \label{fig:result_multi_period}
    \end{minipage}
    \vskip -1.5em
\end{figure*}

In this section, we explore the reasons behind adopting distillation (\textit{\textbf{What motivates distillation?}}) and investigate the specific time series information to distill into the MLP model (\textit{\textbf{What should we distill?}}). 
%Before exploring these subsections, we first introduce key notations and concepts.
We first introduce key notations. 
For multivariate long-term time series forecasting, given an input time series \( \mathbf{X} \in \mathbb{R}^{T \times C} \), where \( T \) represents the length of the look-back window and \( C \) represents the number of variables, the goal is to predict the future \( S \) time steps \( \mathbf{Y} \in \mathbb{R}^{S \times C} \). 
% The multivariate time series can also be viewed as a set of individual time series: \( \mathbf{X} = \{x^1, \dots, x^C\} \) and \( \mathbf{Y} = \{y^1, \dots, y^C\} \), where \( x^i \in \mathbb{R}^T \) and \( y^i \in \mathbb{R}^S \).

% We denote the model as \( f \). The channel-independent strategy predicts each channel independently, defined as:$\hat{Y} = [f(x^1), \dots, f(x^C)]$, where $\hat{Y} = [\hat{y}^1, \dots, \hat{y}^C]$ denotes the model prediction. In contrast, the channel-dependent strategy considers the combination of all channels simultaneously, defined as: $\hat{Y} = f(x^1, \dots, x^C)$.
% In this paper, for MLP, we adopt the channel-independent strategy because it eliminates the need to model inter-channel relationships~\cite{han2024capacity}, which reduces model complexity and results in a more lightweight model, which is especially advantageous when dealing with a large number of variables. 

\subsection{What Motivates Distillation?}
\label{sec:why_distill}
% \begin{table}[t]
%     \centering
%     \caption{Coefficient of variation between the difference of prediction errors of MLP and teacher models on multiple datasets. The description of datasets can be found in Appendix.\juntong{color highlight}}
%     \label{tab:cv}
%     \resizebox{0.45\textwidth}{!}{%
%     \begin{tabular}{lcccc}
%         \toprule
%         Models & iTransformer & ModernTCN & PatchTST\\
%         \midrule
%         ECL &  7.69 & 5.48 & 2.92   \\
%         ETTh1 &  3.27 & 1.99 & 8.79  \\
%         ETTh2 &  10.36 & 2.05 & 6.10  \\
%         ETTm1 &  21.58 & 10.24 & 11.27 \\
%         ETTm2 &  22.14 & 23.33 & 15.94  \\
%         Solar &  10.24 & 53.34 & 20.75  \\
%         Traffic &  1.82 & 0.55 &  51.13&  \\
%         Weather &  2.15 & 29.24 & 4.09 &  \\
%         \bottomrule
%     \end{tabular}%
%     }
% \end{table}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/result_fail_ratio.pdf}
%     \vskip -1.7em
%     \caption{Win ratio (\%) of MLP compared to teacher models on multiple datasets under the input-720-predict-96 setting. }
%     % The win ratio is generally above 30\% though MLP underperforms teachers, which indicates that MLP and teacher models exhibit significantly different prediction patterns on different samples. The description of datasets can be found in Appendix~\ref{app:implementation_details}.
%     \label{fig:result_cv}
%     \vskip -1.5em
% \end{figure}


% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/result_multi_scale.pdf}
%     \vskip -1.5em
%     \caption{Visualization of model predictions across different downsampled scales of ECL dataset. MLP consistently shows poor performance at multiple scales, while other models perform well, highlighting the importance of capturing multi-scale patterns.}
%     \label{fig:result_multi_scale}
%     \vskip -1.5em
% \end{figure}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/results_multi_period.pdf}
%     \vskip -1.5em
%     \caption{Prediction spectrograms of various models from the ECL dataset compared to the ground truth. The MLP model fails to match the amplitude of several main frequencies in the ground truth, with the red numbers indicating the amplitude differences for the most significant frequency.}
%     \label{fig:result_multi_period}
%     \vskip -1.5em
% \end{figure}

While MLP excel in efficiency, they often lag in performance compared to Transformer and CNN models or achieve only similar results (Figure~\ref{fig:result_rader}). However, even if MLP underperforms its teachers overall, it may excel on specific samples. To investigate this, we compare prediction errors of MLP's prediction $\mathbf{\hat{Y}}_s$ and teacher model's prediction $\mathbf{\hat{Y}}_t$ for $N$ samples, defined as 
$
e_s = \{\text{MSE}(\mathbf{\hat{Y}}_s^1, \mathbf{Y}^1), \cdots, \text{MSE}(\mathbf{\hat{Y}}_s^N, \mathbf{Y}^N)\}
$
and 
$
e_t = \{\text{MSE}(\mathbf{\hat{Y}}_t^1, \mathbf{Y}^1), \cdots, \text{MSE}(\mathbf{\hat{Y}}_t^N, \mathbf{Y}^N)\}.
$
We calculate the win ratio, indicating where MLP outperforms teachers:
\begin{equation}
    \text{Win Ratio} = {\sum \mathbbm{1}(e_s < e_t)}/{N},
\end{equation}
where $\mathbbm{1}(\cdot)$ equals 1 if MLP outperforms the teacher, otherwise 0. As shown in Figure~\ref{fig:result_cv}, the win ratio is high (average: 49.92\%, median: 49.96\%), despite MLP underperforming teacher models overall. For example, on the Traffic dataset, MLP lags behind ModernTCN but wins on 81.19\% of the samples, indicating differing strengths on distinct subsets. \textbf{This highlights the potential of distilling the complementary knowledge from the teacher model into MLP.}



% We identify two key observations that motivate our approach. 
% \textbf{First}, while MLPs excel in efficiency, they often lag in performance compared to Transformer and CNN models or achieve only similar results (Figure~\ref{fig:result_rader}). Cross-architecture \textit{KD} offers a solution to bridge this gap.
% % \textbf{Second}, even when MLP underperforms teachers, this does not necessarily mean they perform badly on all samples. We aim to examine whether the samples they are good at forecasting differ. Thus, we further measure the prediction error differences between MLP and teacher models on different samples. We denote the predictions of MLP and teacher as $\hat{Y}_s \in \mathbb{R}^{S \times C}$ and $\hat{Y}_t \in \mathbb{R}^{S \times C} $, respectively. 
% % % Then, the prediction errors can be written as $e_s = (\hat{Y}_s - Y)^2 \in \mathbb{R}^N$ and $e_t = (\hat{Y}_t - Y)^2 \in \mathbb{R}^N$, where $N$ denotes the number of samples. 
% % Then, the prediction errors of $N$ samples can be written as $e_s = \{\text{MSE}(\hat{Y}_s^1, Y^1),\cdots, \text{MSE}(\hat{Y}_s^N, Y^N)\} \in \mathbb{R}^N$ and $e_t = \{\text{MSE}(\hat{Y}_t^1, Y^1),\cdots, \text{MSE}(\hat{Y}_t^N, Y^N)\} \in \mathbb{R}^N$.
% % We then use the win ratio to assess where the student MLP outperforms the teachers:
% % \begin{equation}
% %     \text{Win Ratio} = {\textstyle\sum \mathbbm{1}(e_s < e_t)}/{N},
% % \end{equation}
% % where $\mathbbm{1}(\cdot)$ is an indicator function that equals 1 if the condition inside holds true, and 0 otherwise.
% \textbf{Second}, even if MLP underperforms its teachers overall, it may excel on specific samples. To investigate this, we compare prediction errors of MLP ($\hat{Y}_s$) and teacher models ($\hat{Y}_t$) for $N$ samples, defined as 
% $
% e_s = \{\text{MSE}(\hat{Y}_s^1, Y^1), \cdots, \text{MSE}(\hat{Y}_s^N, Y^N)\}
% $
% and 
% $
% e_t = \{\text{MSE}(\hat{Y}_t^1, Y^1), \cdots, \text{MSE}(\hat{Y}_t^N, Y^N)\}.
% $
% We calculate the win ratio, indicating where MLP outperforms teachers:
% \[
% \text{Win Ratio} = \frac{\sum \mathbbm{1}(e_s < e_t)}{N},
% \]
% where $\mathbbm{1}(\cdot)$ equals 1 if the condition is true, otherwise 0.

% The differences in prediction errors can be written as $e = | e_s - e_t | \in \mathbb{R}^N$. 
% \wei{give a brief explanation on the errors here. Positive values indicate xxxx; negative values indicate xxx } 
% A small $e$ indicates that both the MLP and the teacher model make similar prediction errors on a sample, suggesting that neither has a significant advantage. Conversely, a large $e$ indicates a notable discrepancy in prediction errors between the MLP and the teacher model, highlighting samples where one model significantly outperforms the other.
% We can use the coefficient of variation (CV)~\cite{shazeer2017outrageously, abdi2010coefficient, brown1998coefficient} to measure the differences of prediction error:
% \begin{equation}
%     \text{CV} = \frac{\text{Standard\_Deviation}(e)}{\text{Mean}(e)},
% \end{equation}
% where $\text{CV} > 1$ is considered high variance; that is, the MLP and teacher models produce significantly different predictions at many time series. As shown in Figure~\ref{fig:result_cv}, across multiple datasets, CV is consistently larger than 1, 
% As shown in Figure~\ref{fig:result_cv}, the win ratio is generally large (average: 49.92\%, median: 49.96\%). However, the MLP either underperforms or only achieves comparable performance to the teacher models as shown in Figure~\ref{fig:result_rader}, indicating that the MLP and teacher models demonstrate significantly different prediction patterns and excel on different samples with minimal overlap. For instance, on the Traffic dataset, while the MLP lags behind ModernTCN in Figure~\ref{fig:result_rader}, it wins on 81.19\% of the samples. This suggests that ModernTCN exhibits substantial advantages on a different subset of samples. \textbf{Thus, the complementary knowledge arising from these differing prediction patterns highlights the potential for the MLP to learn from the teacher models}.



% An intuitive way to distill knowledge from teachers to MLP is to directly match the predictions. However, we argue that this approach may be less effective for the following reasons. \textbf{First}, directly aligning final predictions can cause the MLP to overfit the noise in the teacher’s predictions, which does not provide stable or broadly applicable knowledge. \textbf{Second}, it is hard for the MLP to capture pattern-level knowledge present in the teacher’s predictions directly because the MLP has limited capacity to represent complex patterns. \textbf{Third}, focusing solely on prediction alignment may neglect the valuable knowledge embedded in intermediate features. Thus, what knowledge about time series should be distilled into MLP needs to be further investigated.

An intuitive strategy of \textit{KD} for MLP is to align predictions with teachers, but this faces limitations. \textbf{First}, it risks overfitting noise in the teacher’s predictions, leading to less stable knowledge~\cite{chen2017learning, takamoto2020efficient, gajbhiye2021knowledge}. \textbf{Second}, MLP may struggle to replicate the complex patterns such as seasonality, trends, and periodicity in teacher predictions directly due to their limited capacity. \textbf{Third}, this approach overlooks valuable knowledge from intermediate features of teacher model. Therefore, the specific knowledge to distill into MLP requires further exploration.

\subsection{What Should We Distill?}
\label{sec:what2distill}

% To investigate which complementary patterns of time series should be distilled into MLP, we further analyze their prediction patterns in both temporal and frequency domains. Real-world time series often show variations at multiple temporal scales~\cite{timemixer} and multiple periodicities in the frequency domain~\cite{timesnet}. 
% Thus, we conduct further analysis of these two patterns by presenting illustrative cases.

To investigate which complementary time series patterns to distill into MLP, we analyze prediction patterns in both temporal and frequency domains, considering real-world variations across temporal scales~\cite{timemixer} and periodicities~\cite{timesnet}. Thus, we conduct further analysis of these two patterns by presenting illustrative cases. The implementation is detailed in Appendix~\ref{app:implementation_preliminary}. 
% \textbf{(1) Multi-Scale Pattern:} As illustrated in Figure~\ref{fig:result_multi_scale}, we analyze the multi-scale pattern of the temporal domain by downsampling the original time series (\textit{Scale 0}) to multiple coarser scales (\textit{Scales 1, 2, and 3}). Models that perform well on the finest scale (\textit{Scale 0}) also tend to predict accurately across all scales, whereas MLP performs poorly at most scales. For instance, Transformer-based methods such as iTransformer~\cite{itransformer} and PatchTST~\cite{patchtst}, along with the CNN-based method ModernTCN~\cite{moderntcn}, closely match the ground truth on the coarsest scale (\textit{Scale 3}), demonstrating their strong ability to capture the underlying trends in time series data. In contrast, MLP exhibits significant deviations from the ground truth, indicating that their simple projections cannot effectively capture multi-scale patterns.
\textbf{(1) Multi-Scale Pattern:}  
% \wei{It is not clear how we exactly obtain these scales. please provide details like how we performed downsampling and whether this is from predictions or input.} 
By downsampling predictions \(\mathbf{\hat{Y}}\) using convolutional operations, we obtain their multi-scale representations. Figure~\ref{fig:result_multi_scale} shows the analysis of multi-scale temporal patterns by downsampling the time series (\textit{Scale 0}) to coarser scales (\textit{Scales 1–3}). Models excelling at \textit{Scale 0} generally perform well across all scales. The teacher models align closely with the ground truth at \textit{Scale 3}, capturing underlying trends effectively. In contrast, MLP significantly deviates, showing its limitations in handling multi-scale patterns.
% \textbf{(2) Multi-Period Pattern:} The periodicities in time series data can be readily observed in the frequency domain. Figure~\ref{fig:result_multi_period} presents the spectrograms of the predictions from different models alongside the ground truth. These spectrograms indicate that models with better performance (\textit{i.e.}, lower MSE) capture periodicities similar to those present in the ground truth. At the frequency with the largest amplitude in the ground truth, the discrepancy between the MLP's prediction and the ground truth is much greater than that of the other three models, indicating that MLP cannot effectively capture the periodicities in time series data. 
\textbf{(2) Multi-Period Pattern:} Periodicities in time series are visible in the frequency domain by transforming time series into spectrograms, with the x-axis representing frequency and periodicity calculated as the time series length $S$ divided by frequency. 
% \wei{still not clear what we mean by Length; I just noticed that you have defined $S$ before; think you can use $S$ but I don't know what we mean by frequency}. 
Figure~\ref{fig:result_multi_period} shows spectrograms of model predictions and the ground truth. Models with lower MSE capture periodicities more accurately, closely matching the ground truth at dominant frequencies. In contrast, MLP predictions show larger discrepancies, highlighting its inability to effectively capture periodic patterns.


% When MLP underperforms on certain samples where teacher models succeed, it may be attributed to challenges in effectively capturing both multi-scale and multi-period patterns. This inability causes MLP to perform worse than other models. Thus, capturing multi-scale and multi-period patterns in the temporal and frequency domain is crucial for effective time series forecasting, especially for MLP. Although different time series models target specific aspects of time series—for instance, iTransformer captures multi-variate correlations, PatchTST captures time series locality, and ModernTCN captures cross-time and cross-variable dependencies—they fundamentally leverage intrinsic time series characteristics to improve forecasting performance. Therefore, to enhance MLP, it is important to distill complementary multi-scale and multi-period time series patterns from teacher models. 

From our observations, we conjecture that MLP underperforms on certain samples due to their inability to capture essential multi-scale and multi-period patterns, which are essential for effective time series forecasting. To improve MLP, it is crucial to incorporate these complementary patterns from teacher models during distillation.
