\vspace{-2em}
\section{Introduction}

Forecasting is a notably critical problem in the time series analysis community, which aims to predict future time series based on historical time series records~\cite{wang2024deep}. It has broad practical applications such as climate modeling~\cite{wu2023interpretable}, traffic flow management~\cite{yin2021deep}, healthcare monitoring~\cite{kaushik2020ai} and finance analytics~\cite{granger2014forecasting}. 

Recently, there has been an ongoing debate over which deep learning architecture best suits time series forecasting. The 
% Recently, with the rising popularity of Transformer in natural language processing~\cite{DBLP:journals/corr/abs-1810-04805}, computer vision~\cite{dosovitskiy2020vit},
 

\begin{wrapfigure}[11]{r}{0.255\textwidth}
    \vskip -0.45em 
    \centering
    \includegraphics[width=1\linewidth]{figures/result_radar.pdf}
    \vspace{-3em}
    \caption{Performance comparison.}
    \label{fig:result_rader}
    \vspace{-2em}
\end{wrapfigure}
rise of Transformers in various domains~\cite{DBLP:journals/corr/abs-1810-04805,khan2022transformers} has led to their wide adoption in  time series forecasting~\cite{wen2022transformers, autoformer, fedformer, informer, patchtst, itransformer}, leveraging the strong capabilities of capturing pairwise dependencies and extracting multi-level representations within sequential data.
Similarly, CNN architectures have also proven effective by developing convolution blocks for time series~\cite{moderntcn, micn}.
However, despite the strong performance of Transformer-based and CNN-based models, they face significant challenges in large-scale industrial applications due to their relatively high computational demands, especially in latency-sensitive scenarios like financial prediction and healthcare monitoring~\cite{granger2014forecasting, kaushik2020ai}. In contrast, simpler linear or MLP models offer greater efficiency, although with lower performance~\cite{dlinear, sparsetsf}. These contrasting observations raises an intriguing question:
%Despite the strong performance of Transformer-based and CNN-based models, their application to large-scale industrial scenarios poses significant challenges. One key issue is the quadratic complexity inherent in modeling pairwise relationships between tokens or patches in Transformer and a large number of multiplications and additions in CNN, which can lead to substantial computational overhead.  This complexity poses challenges in deploying them in latency-sensitive applications that demand fast inference, such as financial prediction~\cite{granger2014forecasting} and healthcare monitoring~\cite{kaushik2020ai}. 
% On the other hand, researchers have found that a simple one-layer MLP model can significantly exceed Transformers-based and CNN-based models in efficiency, although it falls short of matching their performance~\cite{sparsetsf, dlinear, fits}. This observation leads us to a compelling question:

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.3\linewidth]{figures/result_radar.pdf}
%     \caption{Performance of TimeDistill. Average results (MSE) are reported following iTransformer~\cite{itransformer}.}
%     \label{fig:result_rader}
% \end{figure}

% \begin{center}
% \textit{\textbf{Can we combine MLP and Transformers (or other architectures) to create a powerful yet efficient model?}}  
% \end{center}
\begin{center}
\vspace{-0.7em}
\begin{tcolorbox}[colback=yellow!10!white, colframe=yellow!75!black, width=0.45\textwidth, boxrule=0.5mm, arc=5mm, auto outer arc]
\centering
\textit{\textbf{Can we combine MLP with other advanced architectures (e.g., Transformers and CNNs) to create a powerful yet efficient model?}}
\end{tcolorbox}
\vspace{-0.7em}
\end{center}

A promising approach to addressing this question is knowledge distillation (\textit{KD})~\cite{hinton2015distilling}, a technique that transfers knowledge from a larger and more complex model (\textit{teacher}) to a smaller and simpler one (\textit{student}) while maintaining comparable performance. In this work, we pioneer \textit{cross-architecture KD} in time series forecasting, with MLP as the \textit{student} and other advanced architectures (e.g., Transformers and CNNs) as the \textit{teacher}. However, designing such a framework is non-trivial, as it remains unclear what ``knowledge'' should be distilled into MLP.


% Our findings reveal that different models demonstrate strengths on distinct subsets of the data, even in cases where some models (e.g. MLP) exhibit lower overall performance (see Sec.~\ref{sec:why_distill}). This highlights the potential of cross-architecture KD to leverage their \textit{complementary capabilities}. 
To investigate this potential, we conduct a comparative analysis of prediction patterns between MLP and other time series models.  Our findings reveal that MLP still excels on some data subsets despite its overall lower performance (Sec.~\ref{sec:why_distill}), which highlights the value of harnessing the \textit{complementary capabilities} across different architectures. 
To further explore the specific properties to distill, we focus on two key time series patterns: multi-scale pattern in temporal domain and multi-period pattern in frequency domain, given that they are vital in capturing the complex structures typical of many time series.
% Multi-scale patterns are about time series variations at different granularities, while multi-period patterns are about identifying and modeling cyclic behaviors with overlapping or multiple periodicities. 
% We do not consider other patterns because these two are the most critical for accurately representing the complex scales and periodic structures inherent in many time series.
\textbf{(1)~Multi-Scale Pattern:} Real-world 
 time series often show variations at multiple temporal scales. For example, hourly recorded traffic flow data capture changes within each day, while daily sampled data lose fine-grained details but reveal patterns related to holidays~\cite{timemixer}. We observe that models that perform well on the finest scale also perform accurately on coarser scales, while MLP fails on most scales (Sec.~\ref{sec:what2distill}). 
\textbf{(2) Multi-Period Pattern:} Time series often exhibit multiple periodicities. For instance, weather measurements may have both daily and yearly cycles, while electricity consumption data may show weekly and quarterly cycles~\cite{timesnet}. We find that models that perform well can capture periodicities similar to those in the ground truth, but MLP fails to capture these periodicities (Sec.~\ref{sec:what2distill}).
Therefore, enhancing MLP requires distilling and integrating these multi-scale and multi-period patterns from teacher models. 

% We provide additional cases to highlight the importance of these two patterns for distillation in Sec.~\ref{sec:preliminaries}.
%\method{} transfers key time series-specific knowledge from a teacher model to a student MLP. % To achieve this, we first downsample the time series and perform matching across multiple scales in the temporal domain. Additionally, we applied the discrete Fourier transform (DFT) to the time series to extract the spectrum and align the period distribution in the frequency domain. \wei{performance loss? we actually improved the performance right?} Our key finding is that it is possible to distill knowledge from teacher models to MLP without significant performance loss while drastically reducing inference time for time series forecasting. 

Based on our observations, we propose a cross-architecture \textit{KD} framework named \method{} to bridge the performance and efficiency gap between complex teacher models and a simple MLP.  Instead of solely matching predictions in conventional \textit{KD}, \method{} focuses on aligning multi-scales and multi-period patterns between MLP and the teacher: we downsample the time series for temporal multi-scale alignment and apply Fast Fourier Transform (FFT) to align period distributions in the frequency domain. 
The \textit{KD} process can be conducted offline, shifting heavy computations from the latency-critical \textit{inference phase}, where millisecond matter, to the less time-sensitive \textit{training phase}, where longer processing time is acceptable. 
We validate the effectiveness of \method{} both theoretically and empirically and summarize our contributions as follows:
% The \textit{KD} process can be conducted offline and integrated with model training. In other words, a substantial part of the computation can be \textit{shifted from the latency-critical inference phase}, where millisecond reductions are impactful, to \textit{the less time-sensitive training phase}, where time costs of hours are more acceptable. 
% To thoroughly evaluate the effectiveness of our proposed framework, we conduct experiments on 8 well-known benchmarks. As shown in Figure~\ref{fig:result_rader}, \method{} consistently outperforms standalone MLP up to \textbf{18.6\%} and exceeds the performance of teacher models on \textbf{7 out of 8} datasets on MSE and \textbf{all} datasets on MAE. Our contributions are as follows:
% Moreover, MLP achieve significantly faster inference speeds compared to teacher models such as Transformer-based and CNN-based models. For instance, they are \textbf{3×-6×} faster on the ECL dataset and reduce memory footprint by \textbf{4×-13×}, as illustrated in Figure~\ref{fig:result_efficiency}.

\begin{figure}[t]
    \centering
    
    \includegraphics[width=0.4\textwidth]{figures/result_efficiency.pdf}
    \vskip -1.5em
    \caption{Model efficiency comparison averaged across all prediction lengths (96, 192, 336, 720) for the ECL dataset. Full results on more datasets are listed in Appendix~\ref{app:efficiency}.}
    \label{fig:result_efficiency}
    \vskip -2em
\end{figure}

% In summary, our contributions are as follows:
% \wei{think we can merge (a) and (b) and add a contribution to highlight the theoretical understanding}

\begin{compactenum}[(a)] 

\item We present \textit{the first cross-architecture KD} framework \method{} tailored for efficient and effective time series forecasting via MLP, which is supported by our preliminary studies examining multi-scale and multi-period patterns in time series.

% Our design is supported by our preliminary studies that demonstrate the complementary knowledge embedded within different time series models and analyze the multi-scale and multi-period prediction behaviors of MLPs and their teacher models. 

\item We provide theoretical insights into the benefits of \method{}, illustrating that the proposed distillation process can be viewed as a form of data augmentation through a special \textit{mixup} strategy.

% \item We provide theoretical interpretations on \method{} to demonstrate the distillation loss can be interpreted as training with augmented samples derived from a special \textit{mixup} strategy and point out several benefits. 

% \item We propose a flexible cross-architecture \textit{KD} framework, \method{}, for time series forecasting, which distills key multi-scale and multi-period time series patterns into a lightweight MLP. We also conduct a preliminary study to highlight the different multi-scale and multi-period prediction patterns of MLP and teachers.

% \item We show that \method{} achieves state-of-the-art performance while enjoying up to $6\times$ faster inference and up to $13\times$ smaller parameters than teacher models, as shown in Figure~\ref{fig:result_efficiency}. 

\item We show that \method{} is both effective and efficient, consistently outperforming standalone MLP by up to \textbf{18.6\%} and surpassing teacher models in nearly all cases (see Figure~\ref{fig:result_rader}). Additionally, it achieves up to \textbf{7x} faster inference and requires up to \textbf{130×} fewer parameters compared to teacher models (see Figure~\ref{fig:result_efficiency}).

% \item We demonstrate that \method{} is highly adaptable, generalizing effectively across various teacher/student models. Additionally, we highlight the transformative adjustments made by the distillation process in both the temporal and frequency domains.

\item We conduct deeper analyses of \method{}, exploring its adaptability across various teacher/student models and highlighting the distillation impacts it brings to the temporal and frequency domains.

% \item We demonstrate that \method{} is highly adaptable, generalizing effectively across various teacher/student models. Additionally, we highlight the transformative adjustments made by the distillation process in both the temporal and frequency domains.

% \item  We conduct a comprehensive analysis of the distillation effects of \method{}. Our work provides an in-depth investigation into its generality across various teacher/student models and sheds light on the transformative adjustments introduced by the distillation process in both the temporal and frequency domains.


% \item Deeper analysis of the distillation outcomes from \method{} are examined comprehensively by its generality to different teacher models and conducting a deep analysis of the distillation framework in both temporal and frequency domains.


% \wei{list the advantages: (1) flexible/versatile (or model-agnostic) KD framework; (2) efficient (highlight how many times faster (up to xxx times compared to xxx..) and MSE improvement... (3) }

\end{compactenum}





