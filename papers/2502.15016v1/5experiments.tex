\vskip -2em
\section{Experiment}
\begin{table*}[t]
\centering
\vskip -1.5em
\caption{Long-term time series forecasting results with prediction lengths $S~\in\{96, 192, 336, 720\}$. A lower MSE or MAE indicates a better prediction. For consistency, we maintain a fixed input length of 720 throughout all the experiments. Results are averaged from all prediction lengths. The best performance is highlighted in \textcolor{red}{\textbf{red}}, and the second-best is \textcolor{blue}{\underline{underlined}}. Full results are listed in Appendix~\ref{app:full_main_results}.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c lr|lr|lr|lr|lr|lr|lr|lr|lr}
\toprule
\multirow{2}{*}{Models} & \multicolumn{2}{c}{\textbf{\method{}}} & \multicolumn{2}{c}{iTranformer} & \multicolumn{2}{c}{ModernTCN} & \multicolumn{2}{c}{TimeMixer}& \multicolumn{2}{c}{PatchTST}  & \multicolumn{2}{c}{MICN} & \multicolumn{2}{c}{FEDformer} & \multicolumn{2}{c}{TimesNet} & \multicolumn{2}{c}{Autoformer}  \\ 
& \multicolumn{2}{c}{(\textbf{Ours})} & \multicolumn{2}{c}{(\citeyear{itransformer})} & \multicolumn{2}{c}{(\citeyear{moderntcn})} & \multicolumn{2}{c}{(\citeyear{timemixer})} &\multicolumn{2}{c}{(\citeyear{patchtst})} & \multicolumn{2}{c}{(\citeyear{micn})} & \multicolumn{2}{c}{(\citeyear{fedformer})} & \multicolumn{2}{c}{(\citeyear{timesnet})} &  \multicolumn{2}{c}{(\citeyear{autoformer})} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17} \cmidrule(lr){18-19}
Metric & \multicolumn{1}{|c}{MSE} & MAE & \multicolumn{1}{|c}{MSE} & MAE & \multicolumn{1}{|c}{MSE} & MAE & \multicolumn{1}{|c}{MSE} & MAE & \multicolumn{1}{|c}{MSE} & MAE & \multicolumn{1}{|c}{MSE} & MAE & \multicolumn{1}{|c}{MSE} & MAE & \multicolumn{1}{|c}{MSE} & MAE & \multicolumn{1}{|c}{MSE} & MAE \\
\midrule
ECL &\multicolumn{1}{|c}{\textcolor{red}{\textbf{0.157}}}	&\textcolor{red}{\textbf{0.254}}	&\textcolor{blue}{\underline{0.163}}	&0.259	&0.167	&0.262	&0.165	&\textcolor{blue}{\underline{0.259}} &0.165	&0.266	&0.181	&0.293	&0.274	&0.376	&0.250	&0.347	&0.238	&0.347 \\ \midrule
ETTh1 &\multicolumn{1}{|c}{\textcolor{red}{\textbf{0.429}}}	&\textcolor{red}{\textbf{0.441}}	&0.468	&0.476	&0.469	&0.465 &\textcolor{blue}{\underline{0.459}}	&\textcolor{blue}{\underline{0.465}}	&0.498	&0.490	&0.739	&0.631	&0.527	&0.524	&0.507	&0.500	&0.731	&0.659 \\ \midrule
ETTh2	&\multicolumn{1}{|c}{\textcolor{red}{\textbf{0.345}}}	&\textcolor{red}{\textbf{0.395}}	&0.398	&0.426	&\textcolor{blue}{\underline{0.357}}	&\textcolor{blue}{\underline{0.403}}	&0.422	&0.444 &0.444	&0.443	&1.078	&0.736	&0.456	&0.485	&0.419	&0.446	&1.594	&0.940 \\ \midrule
ETTm1	&\multicolumn{1}{|c}{\textcolor{red}{\textbf{0.348}}}	&\textcolor{red}{\textbf{0.379}}	&0.372	&0.402	&0.390	&0.410	&\textcolor{blue}{\underline{0.367}}	&\textcolor{blue}{\underline{0.388}} &0.383	&0.412	&0.439	&0.461	&0.423	&0.451	&0.398	&0.419	&0.570	&0.526 \\ \midrule
ETTm2	&\multicolumn{1}{|c}{\textcolor{red}{\textbf{0.244}}}	&\textcolor{red}{\textbf{0.311}}	&0.276	&0.337	&\textcolor{blue}{\underline{0.267}}	&\textcolor{blue}{\underline{0.330}}	&0.279	&0.339 &0.274	&0.335	&0.348	&0.404	&0.359	&0.401	&0.291	&0.349	&0.420	&0.448 \\ \midrule
Solar	&\multicolumn{1}{|c}{\textcolor{red}{\textbf{0.184}}}	&\textcolor{red}{\textbf{0.241}}	&0.214	&0.270	&\textcolor{blue}{\underline{0.191}}	&\textcolor{blue}{\underline{0.243}}	&0.238	&0.288 &0.210	&0.257	&0.213	&0.277	&0.300	&0.383	&0.196	&0.262	&1.037	&0.742 \\ \midrule
Traffic	&\multicolumn{1}{|c}{\textcolor{blue}{\underline{0.387}}}	&\textcolor{red}{\textbf{0.271}}	&\textcolor{red}{\textbf{0.379}}	&\textcolor{blue}{\underline{0.271}}	&0.413	&0.284	&0.391	&0.275 &0.402	&0.284	&0.500	&0.316	&0.629	&0.388	&0.693	&0.399	&0.696	&0.427 \\ \midrule
Weather	&\multicolumn{1}{|c}{\textcolor{red}{\textbf{0.220}}}	&\textcolor{red}{\textbf{0.269}}	&0.259	&0.290	&0.238	&0.277	&\textcolor{blue}{\underline{0.230}}	&\textcolor{blue}{\underline{0.271}} &0.246	&0.283	&0.240	&0.292	&0.355	&0.398	&0.257	&0.294	&0.471	&0.465 \\
\bottomrule

\end{tabular}%
}
% \vskip -1.5em
\label{tab:main}
\end{table*}


\subsection{Experimental Setup}

\textbf{Datasets and Baselines.} We run experiments to evaluate the performance and efficiency of \method{} on 8 widely used benchmarks: Electricity (ECL), the ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2), Solar, Traffic, and Weather, following~\cite{itransformer, timemixer, moderntcn}.  % More details are summarized in Appendix~\ref{app:implementation_details}.
To examine the effectiveness of our method across diverse tasks, we compare \method{} with 8 baseline models that cover a range of architectures. Specifically, we use \textit{Transformer-based models}: iTransformer~\cite{itransformer}, PatchTST~\cite{patchtst}, FEDformer~\cite{fedformer}, Autoformer~\cite{autoformer}; \textit{CNN-based models}: ModernTCN~\cite{moderntcn}, MICN~\cite{micn}, TimesNet~\cite{timesnet}; and an \textit{MLPs-based model}: TimeMixer~\cite{timemixer}.  More details can be found in Appendix~\ref{app:implementation_details}.

% We refer readers to Appendix~\ref{app:implementation_details} for more implementation details.

\subsection{Main Results}

% \wei{experimental setting? which teacher we are using? In addition to the empirical observations, we also provide more analysis, e.g., what does \method{} compare with the teacher performance , and the student performance? Right now we only showed that \method{} achieves new SOTA but readers may wanna see more discussions on the distillation..}  

Table~\ref{tab:main} presents the long-term time series forecasting performance of the proposed \method{} compared with previous state-of-the-art baselines. By default, \method{} uses ModernTCN as the teacher, though results with alternative teachers are provided in Sec.~\ref{sec:versatility}. Notably, \method{} outperforms the baselines on \textbf{7 out of 8 }datasets on MSE and \textbf{all} datasets on MAE. Furthermore, \method{} consistently exceeds the performance of its teacher (ModernTCN) by up to \textbf{5.37\%} and improves over vanilla MLP by up to \textbf{13.87\%}, as shown in Table~\ref{tab:different_teacher}. These results highlight the effectiveness of our multi-scale and multi-period distillation approach in transferring knowledge for enhanced forecasting performance.

\textbf{Efficiency Analysis.}
% Inference speed per batch (compared to transformer-based methods, also compare with other efficient TSF models like SparseTSF?)
Beyond its strong predictive performance, another notable advantage of \method{} is its extremely lightweight architecture, as it is simply an MLP. Figure~\ref{fig:result_efficiency} in the Introduction section shows the trade-off between inference time, memory footprint, and performance. We can observe that \method{} can achieve up to \textbf{7×} speedup and up to \textbf{130×} fewer parameters compared with baselines. This property makes \method{} suitable for deployment on devices with limited computational resources and in latency-sensitive applications that require fast inference. Compared to previous Transformer-based method Autoformer, we achieve \textbf{196×} speedup as shown in Table~\ref{tab:app_efficiency}. We list full results of efficiency analysis in Appendix~\ref{app:efficiency}.


\subsection{Versatility of \method{}}
\label{sec:versatility}

\begin{table}[ht!]
\centering
\vskip -1.5em
\caption{Performance improvement by \textbf{\method{}} with \textbf{different teachers}. $\Delta_{MLP}$, $\Delta_{Teacher}$ indicate the improvement of \textbf{MLP+\method{}} over a trained MLP and Teacher, respectively. We report average MSE across all prediction lengths. Full results are in Appendix~\ref{app:full_different_teacher_results}.}
\label{tab:different_teacher}
\small
\resizebox{0.48\textwidth}{!}{%
\begin{tabular}{cc c|c|c|c}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Teacher Models}} & \multicolumn{1}{c}{iTranformer} & \multicolumn{1}{c}{ModernTCN} & \multicolumn{1}{c}{TimeMixer} & \multicolumn{1}{c}{PatchTST} \\ 
& & (\citeyear{itransformer}) & (\citeyear{moderntcn}) & (\citeyear{timemixer}) & (\citeyear{patchtst}) \\
\midrule
\multirow{5}{*}{ECL} 
& Teacher         & 0.163 & 0.167 & 0.165 & 0.165 \\ 
& MLP             & 0.173 & 0.173 & 0.173 & 0.173 \\
& \textbf{+\method{} }   & \textbf{0.157} & \textbf{0.157} & \textbf{0.159} & \textbf{0.159} \\
\cmidrule(lr){2-6} 
& $\Delta_{Teacher}$  & 3.68\% & 5.61\% & 3.31\% & 3.64\% \\
& $\Delta_{MLP}$      & 9.27\% & 9.09\% & 7.94\% & 8.11\% \\
\midrule
\multirow{5}{*}{ETT(avg)} 
& Teacher         & 0.379 & 0.371 & 0.382 & 0.400 \\ 
& MLP             & 0.397 & 0.397 & 0.397 & 0.397 \\
& \textbf{+\method{}}    & \textbf{0.345} & \textbf{0.342} & \textbf{0.353} & \textbf{0.358} \\
\cmidrule(lr){2-6} 
& $\Delta_{Teacher}$  & 8.92\% & 7.94\% & 7.46\% & 10.38\% \\
& $\Delta_{MLP}$      & 13.06\% & 13.87\% & 10.91\% & 9.65\% \\
\midrule
\multirow{5}{*}{Solar} 
& Teacher         & 0.214 & 0.191 & 0.288 & 0.210 \\ 
& MLP             & 0.194 & 0.194 & 0.194 & \textbf{0.194} \\
& \textbf{+\method{}}    & \textbf{0.185} & \textbf{0.184} & \textbf{0.187} & 0.204 \\
\cmidrule(lr){2-6} 
& $\Delta_{Teacher}$  & 13.55\% & 3.60\% & 21.41\% & 2.86\% \\
& $\Delta_{MLP}$      & 4.80\% & 5.14\% & 3.58\% & -4.98\% \\
\midrule
\multirow{5}{*}{Traffic} 
& Teacher         & \textbf{0.379} & 0.413 & 0.391 & 0.402 \\ 
& MLP             & 0.434 & 0.434 & 0.434 & 0.434 \\
& \textbf{+\method{}}    & 0.389 & \textbf{0.387} & \textbf{0.391} & \textbf{0.390} \\
\cmidrule(lr){2-6} 
& $\Delta_{Teacher}$  & -2.64\% & 6.32\% & -0.04\% & 2.99\% \\
& $\Delta_{MLP}$      & 10.30\% & 10.70\% & 9.76\% & 10.07\% \\
\midrule
\multirow{5}{*}{Weather} 
& Teacher         & 0.259 & 0.238 & 0.230 & 0.246 \\ 
& MLP             & 0.234 & 0.234 & 0.234 & 0.234 \\
& \textbf{+\method{}}    & \textbf{0.220} & \textbf{0.220} & \textbf{0.219} & \textbf{0.220} \\
\cmidrule(lr){2-6} 
& $\Delta_{Teacher}$  & 15.06\% & 7.37\% & 4.82\% & 10.57\% \\
& $\Delta_{MLP}$      & 5.83\% & 5.66\% & 6.16\% & 5.83\% \\
\bottomrule
\end{tabular}
}
\vskip -2em
\end{table}

\begin{figure}[t] \centering
\includegraphics[width=0.45\textwidth]{figures/result_look_back_window.pdf}
\vskip -1.5em
    \caption{Forecasting performance is evaluated with different look-back lengths of $T \in \{96, 192, 336, 720\}$, and the results are averaged across all prediction lengths $S \in \{96, 192, 336, 720\}$ on eight datasets. The complete results are provided in Appendix~\ref{app:full_diferent_lookback_window_results}.}
    \label{fig:result_look_back_window}
\vskip -1.5em
\end{figure}
% \wei{a short introduction for this subsection will be helpful for readers to follow. e.g., In this subsection, we further demonstrate the flexibility/versatility of the proposed \method{} for different xxx.xxx.}
In this subsection, we further demonstrate the versatility of the proposed \method{} by evaluating its performance under different configurations, including variations in teacher/student models and look-back window lengths.


% \wei{how about different students? can we empower other MLPs? this could also be an interesting study; we may choose one or two recent MLPs to study?}

\textbf{Different Teachers.}
We use ModernTCN as the teacher model for our main results. In Table~\ref{tab:different_teacher}, we present results using other teacher models, such as iTransformer, TimeMixer, and PatchTST. Results with additional teachers are provided in Appendix~\ref{app:full_different_teacher_results}. We observe that \method{} can effectively learn from various teachers, improving MLP by up to \textbf{13.87\%}. Furthermore, \method{} achieves significant performance improvements over the teachers themselves, with gains of up to \textbf{21.41\%}. We hypothesize two key reasons for these improvements. First, the student MLP model already demonstrates strong learning capabilities; for instance, on the Solar dataset, MLP outperforms most teachers. Second, the multi-scale and multi-period \textit{KD} approach delivers both temporal and frequency knowledge from teachers to MLP, offering additional valuable insights. Similar findings have been reported in recent \textit{KD} studies~\cite{allen2020towards, zhang2021graph, guo2023linkless}, which suggest that integrating diverse views from multiple models can enhance performance.


\textbf{Different Students.} 
To evaluate whether \method{} can enhance the performance of other lightweight models, we select two MLPs-based models, TSMixer~\cite{tsmixer} and LightTS~\cite{lightts}, as students. Unlike a simple MLP, these MLPs-based models consist of multiple MLPs and operate in a channel-dependent manner. Consequently, they are generally more complex than a standard MLP, which results in reduced efficiency. We use ModernTCN as the teacher model, consistent with other experiments. As shown in Table~\ref{tab:different_student}, \method{} consistently improves the performance of TSMixer and LightTS, achieving remarkable MSE reductions of \textbf{6.26\%} and \textbf{8.02\%}, respectively. These results demonstrate that \method{} is highly adaptable and can effectively enhance other lightweight methods. We also explore the influence of student MLP architecture (e.g. layer number and hidden dimension) on \method{} in Appendix~\ref{app:different_students}.

\begin{table}[t!]
\centering
\vskip -1.5em
\caption{Performance improvement of \textbf{\method{}} with \textbf{different students} on ETTh1, averaged across all prediction lengths. $\Delta_{Student}$ represents the improvement of \textbf{Student+\method{}} over the original student. Additional results are in Appendix~\ref{app:different_students}.}

\label{tab:different_student}
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{c cc|cc|cc}
\toprule
\multicolumn{1}{c}{\multirow{3}{*}{Student Models}} 
& \multicolumn{2}{c}{\multirow{2}{*}{MLP}}
& \multicolumn{2}{c}{LightTS} 
& \multicolumn{2}{c}{TSMixer} \\
& \multicolumn{2}{c}{} & \multicolumn{2}{c}{(\citeyear{lightts})} & \multicolumn{2}{c}{(\citeyear{tsmixer})} \\
& MSE & MAE & MSE & MAE & MSE & MAE \\
\midrule
% ---------------- ETTh1 ----------------
Student
& 0.502 & 0.489
& 0.465 & 0.471
& 0.471 & 0.474 \\
\textbf{+\method{}} 
& \textbf{0.428} & \textbf{0.445}
& \textbf{0.436} & \textbf{ 0.445}
& \textbf{0.433} & \textbf{0.446} \\
\cmidrule{1-7}
$\Delta_{Student}$
& 14.74\% & 9.00\%
& 6.26\%  & 5.57\%
& 8.02\%  & 5.92\% \\
\bottomrule
\end{tabular}%
}
\vskip -1em
\end{table}

\textbf{Different Look-Back Window Lengths.}
The length of the look-back window significantly influences forecasting accuracy, as it determines how much historical information can be utilized for learning. Figure~\ref{fig:result_look_back_window} presents the average MSE results across all eight datasets. Overall, the performance of all models, particularly MLP, improves as the look-back window size increases. Notably, \method{} consistently enhances the performance of MLP and outperforms the teacher models across all look-back window lengths.


% 3 Transferability analysis

\subsection{Deeper Analysis into Our Distillation Framework}

\begin{table}[t]
\centering
\vskip -1em
\caption{Ablation study measured by MSE on different components of \method{} (Teacher: ModernTCN). The best performance is in \textcolor{red}{\textbf{red}}, and the second-best is \textcolor{blue}{\underline{underlined}}. More ablation study results are listed in Appendix~\ref{app:full_ablation_results}.}
\label{tab:ablation}
% \small
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l c c c c c}
\toprule
Method & ECL & ETT(avg) & Solar & Traffic & Weather \\
\midrule
Teacher            & 0.167 & 0.371 & 0.191 & 0.413 & 0.238 \\
MLP                 & 0.173 & 0.397 & 0.194 & 0.434 & 0.234 \\
\midrule
\method{}         & \textcolor{red}{\textbf{0.157}} & \textcolor{red}{\textbf{0.342}} & \textcolor{red}{\textbf{0.184}} & \textcolor{red}{\textbf{0.387}} & \textcolor{red}{\textbf{0.220}} \\
w/o prediction level & \textcolor{blue}{\underline{0.157}} & 0.373 & \textcolor{blue}{\underline{0.184}} & 0.392 & \textcolor{blue}{\underline{0.221}} \\
w/o feature level    & 0.161 & 0.349 & 0.188 & 0.393 & 0.224 \\
w/o multi-scale      & 0.162 & 0.377 & 0.187 & 0.393 & 0.224 \\
w/o multi-period     & 0.157 & \textcolor{blue}{\underline{0.342}} &0.184 & \textcolor{blue}{\underline{0.391}} & 0.221 \\
w/o sup              & 0.165 & 0.344 & 0.192 & 0.506 & 0.225 \\
\bottomrule
\end{tabular}
}
% \vskip -2em
\end{table}

\textbf{Ablation study.} As our proposed \method{} incorporates two distillation strategies, \textit{multi-scale distillation} and \textit{multi-period distillation}, we assess their effectiveness by removing the corresponding losses, $\mathcal{L}_{\text{scale}}$ and $\mathcal{L}_{\text{period}}$, from \method{}. Additionally, we evaluate the impact of \textit{prediction-level} and \textit{feature-level distillation} by removing $\mathcal{L}^{\mathbf{H}}$ and $\mathcal{L}^{\mathbf{Y}}$, respectively. Furthermore, we test the model by removing the supervised loss $\mathcal{L}_{\text{sup}}$, using only the distillation losses as the overall loss for \method{}. 

Table~\ref{tab:ablation} presents the results of these ablations, compared with the full \method{}, a stand-alone MLP, and the teacher models. We draw the following observations: \textbf{First}, each loss term at both levels, when used individually, already outperforms the stand-alone MLP. \textbf{Second}, when combined, the losses complement each other, enabling \method{} to achieve the best performance, surpassing the teacher model. \textbf{Third}, notably, \method{} maintains superior performance over both the MLP and the teacher even without the supervised loss $\mathcal{L}_{\text{sup}}$. This can be attributed to two potential reasons: (1) ground truth may contain noise, making it more challenging to fit, whereas teacher provides simpler and more learnable knowledge; and (2) \textit{multi-scale} and \textit{multi-period} distillation processes effectively transfer complementary knowledge from teacher to MLP.

\begin{figure}[t]
    \centering    \includegraphics[width=0.45\textwidth]{figures/vis_multiscale_multiperiod_ETTh1.pdf}
    % \vskip -1em
    % \caption{ Visualization of the model's predictions on multiple scales in the temporal domain and the corresponding spectrograms (multi-period patterns) in the frequency domain under the ETTh1 dataset. For this case, the MSE values of the MLP, Teacher, and \method{} are 0.790, 0.365, and 0.366, respectively. The results demonstrate that \method{} enhances MLP performance by bridging the gap in both temporal and frequency domains through multi-scale and multi-period distillation. Teacher refers to ModernTCN~\cite{moderntcn}.  Additional cases are provided in Appendix~\ref{app:cases}.}
    \vskip -1em
    \caption{Prediction comparison across temporal scales and spectrograms before and after distillation on ETTh1. MSE for MLP, Teacher (ModernTCN), and \method{} are 0.790, 0.365, and 0.366, showing \method{} bridges temporal and frequency domain gaps via multi-scale and multi-period distillation. More cases in Appendix~\ref{app:cases}.}
    \label{fig:vis_multiscale_multiperiod_ETTh1}
    \vskip -2em
\end{figure}

% \textbf{Effectiveness Visualization of Multi-Scale and Multi-Period Distillation.} \wei{the title for this subsubsection should be more like a question, e.g., "What is indeed learned from distillation?", "Does KD really bridge the gap between student and teacher?"}

% \begin{figure}[t]
%     \centering    \includegraphics[width=0.45\textwidth]{figures/result_std_distill.pdf}
%     \caption{ }
%     \label{fig:result_std_distill}
% \end{figure}

\begin{table}[t]
\centering
\vskip -1em
\caption{Win ratio (\%) of MLP and \method{} vs. ModernTCN under input-720-predict-96 setting. \( U_{M} \) and \( U_{T} \) denote samples where MLP and \method{} outperform the teacher. \textit{Win Keep}, \( \frac{|U_{M} \cap U_{T}|}{|U_{M}|} \), shows \method{}’s retention of MLP’s wins.}
\label{tab:ratio_change}
% \small
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{l c c c}
\toprule
Dataset & MLP & \method{} & \textit{Win Keep} \\
\midrule
ECL            & 35.64\% & 59.69\% & 95.44\% \\
ETTh1          & 28.58\% & 58.35\% & 79.27\% \\
ETTh2          & 25.57\% & 57.63\% & 69.10\% \\
ETTm1          & 42.82\% & 63.12\% & 84.61\% \\
ETTm2          & 42.84\% & 57.99\% & 80.14\% \\
Solar          & 59.40\% & 61.02\% & 91.77\% \\
Traffic        & 81.19\% & 91.24\% & 99.46\% \\
Weather        & 49.74\% & 56.43\% & 76.61\% \\
\bottomrule
\end{tabular}
}
\vskip -1em
\end{table}

\textbf{Does \method{} Truly Enhance MLP's Learning from the Teacher?} 
As shown in Table~\ref{tab:ratio_change}, \method{} improves the win ratio of MLP over the teacher by an average of 17.46\% across eight datasets. To determine whether this improvement is due to \method{} succeeding on samples MLP previously failed, we present the \textit{Win Keep} results. A higher \textit{Win Keep} indicates that \method{}'s improvements come from previously failed samples. \textit{Win Keep} remains consistently high across datasets (above 76.61\%, with an average of 84.55\%), indicating that \method{} not only retains MLP's success on samples where it already outperformed the teacher but also allows MLP to outperform the teacher on many samples it previously struggled with. This demonstrates that \method{} effectively transfers knowledge from the teacher to student MLP, enabling MLP to perform better on challenging samples while maintaining its existing strengths.

\textbf{Does \method{} Effectively Bridge the Gap Between Student and Teacher?} 
We present a case visualization from the ETTh1 dataset in Figure~\ref{fig:vis_multiscale_multiperiod_ETTh1}. The figure shows that \method{} reduces the difference between the teacher model (ModernTCN) and the student model (MLP) at multiple scales, and makes the fine-level series prediction more precise by transferring trend patterns at coarser scales from the teacher. In addition, for the same case, we visualize the spectrogram of the prediction and find that \method{} also helps reduce the difference between the teacher and MLP in the frequency domain, enabling the MLP to learn more structured multi-period patterns. 
% We further explore if MLP can implicitly learn multi-variate correlation via \method{} in Appendix~\ref{app:multivariate}.


% \textbf{Visualization of Multi-Variate Correlation Distillation.}
% Does it can distill multivariate correlation from transformed without explicit modeling

% 5 How does MLP benefit from distillation? (distillation serves as a regularization, expect results: gap between training and validation loss is visibly larger for MLPs than \method{}, and MLPs show obvious overfitting trends)

% 6 Impact of the Hyperparameter

% \textbf{Hyperparameter Sensitivity.}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/vis_hyperparameter.pdf}
%     \caption{Hyperparameter sensitivity analysis. The averaged MSE results across all prediction lengths over different $\alpha$ and $\beta$ on the ETTm1 and ETTm2 datasets.}
%     \label{fig:vis_hyperparameter}
% \end{figure}
% % \wei{Since we have hyperparameters $\alpha$ and $\beta$, some people may question whether our model is sensitive to the values of these hyperparams. So we should also have a figure for parameter analysis...}
% In this subsection, we investigate the sensitivity of two crucial hyperparameters, $\alpha$ and $\beta$, in the final objective function of \method{}. The parameters $\alpha$ and $\beta$ control the contributions of the prediction-level and feature-level distillation loss terms, respectively. To assess their effects, we vary both $\alpha$ and $\beta$ over the set {0.1, 0.5, 1, 2} on the ETTm1 and ETTm2 datasets. Figure~\ref{fig:vis_hyperparameter} presents the results. We find that \method{} is not sensitive to the choice of $\beta$, while increasing $\alpha$ generally leads to better performance. Moreover, \method{} consistently outperforms the teacher ModernTCN and MLP models across all combinations of $\alpha$ and $\beta$, highlighting the effectiveness of our distillation framework.
% % \wei{In our appendix, we should include the sensitivity analysis for all datasets. In this section, we can focus on one or two datasets and emphasize that similar patterns or observations are consistent across the other datasets.}



