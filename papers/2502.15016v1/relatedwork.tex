\section{Related Work}
\label{app:related_work}
\subsection{Debate in Long-Term Time Series Forecasting}
The trade-off between performance and efficiency has prompted a long-standing debate between Transformer-based models and MLP in long-term time series forecasting~\cite{dlinear, lightts, sparsetsf}. Informer~\cite{informer}, Autoformer~\cite{autoformer}, and FEDformer~\cite{fedformer} were among the leading Transformer-based methods. However, recent findings show that a simple Linear or MLP model can achieve performance comparable to these complex Transformer models across various benchmarks while offering significantly better efficiency~\cite{dlinear}. This outcome has raised questions about the necessity of Transformers in time series forecasting. Following this, research has moved in two directions. One direction suggests that the issues with Transformer-based models arise from the way they are applied.  For example, PatchTST~\cite{patchtst} uses patching to preserve local information, and iTransformer~\cite{itransformer} focuses on capturing multivariate correlations. These approaches surpass the simple one-layer MLP and demonstrate that Transformers can still deliver strong results in time series forecasting if applied effectively. Meanwhile, CNN-based models have also shown strong performance similar to Transformer-based models. TimesNet~\cite{timesnet} transforms 1D time series into 2D variations and applies 2D CNN kernels, MICN~\cite{micn} adopt multi-scale convolution structures to capture local features and global correlations, and ModernTCN~\cite{moderntcn} proposes a framework with much larger receptive fields than prior CNN-based structures. Nevertheless, these powerful CNN-based models also face efficiency issues, which further broadens the scope of the debate between performance and efficiency. The other direction focuses on developing lightweight MLP, such as N-BEATS~\cite{nbeats}, N-hits~\cite{nhits}, LightTS~\cite{lightts}, FreTS~\cite{frets}, TSMixer~\cite{tsmixer}, TiDE~\cite{tide}, which offer improved efficiency. However, these models typically only match, rather than surpass, the performance of state-of-the-art Transformer-based methods and CNN-based methods. In summary, while Transformer-based and CNN-based models generally offer better performance, simple MLP is more efficient. Therefore, we managed to combine the performance of Transformer-based and CNN-based models with MLP to produce a powerful and efficient model by cross-architecture \textit{KD}.

\subsection{Knolwedge Distillation on Time Series}
Knowledge distillation (\textit{KD})~\cite{hinton2015distilling} transfers knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) while maintaining comparable performance. By aligning the output distributions of teacher and student models, \textit{KD} provides richer training signals than hard labels alone, enabling the student to capture subtle patterns that the teacher has learned. In the context of time series, CAKD~\cite{xu2022contrastive} introduces a two-stage distillation scheme that distills features using adversarial and contrastive learning and performs prediction-level distillation. LightTS~\cite{campos2023lightts} designs a \textit{KD} framework specifically for cases where the teacher is an ensemble classifier in time series classification tasks, which limits its applicability to teachers with other architectures. Both of these works do not incorporate time series-specific designs. In contrast, our framework emphasizes extracting essential time series patterns, including multi-scale and multi-period patterns, enabling more effective knowledge distillation. Additionally, we address architectural differences and are the first to explore cross-architecture \textit{KD}.