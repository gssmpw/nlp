@inproceedings{dlinear,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={9},
  pages={11121--11128},
  year={2023}
}

@inproceedings{
patchtst,
title={A Time Series is Worth 64 Words:  Long-term Forecasting with Transformers},
author={Yuqi Nie and Nam H Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}

@inproceedings{
itransformer,
title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
author={Yong Liu and Tengge Hu and Haoran Zhang and Haixu Wu and Shiyu Wang and Lintao Ma and Mingsheng Long},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@inproceedings{informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}

@article{autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@inproceedings{fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={International conference on machine learning},
  pages={27268--27286},
  year={2022},
  organization={PMLR}
}

@article{sparsetsf,
  title={SparseTSF: Modeling Long-term Time Series Forecasting with 1k Parameters},
  author={Lin, Shengsheng and Lin, Weiwei and Wu, Wentai and Chen, Haojun and Yang, Junjie},
  journal={arXiv preprint arXiv:2405.00946},
  year={2024}
}

@article{fits,
  title={FITS: Modeling time series with $10 k $ parameters},
  author={Xu, Zhijian and Zeng, Ailing and Xu, Qiang},
  journal={arXiv preprint arXiv:2307.03756},
  year={2023}
}
@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{moderntcn,
  title={Moderntcn: A modern pure convolution structure for general time series analysis},
  author={Luo, Donghao and Wang, Xue},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{wang2024deep,
  title={Deep time series models: A comprehensive survey and benchmark},
  author={Wang, Yuxuan and Wu, Haixu and Dong, Jiaxiang and Liu, Yong and Long, Mingsheng and Wang, Jianmin},
  journal={arXiv preprint arXiv:2407.13278},
  year={2024}
}

@article{wen2022transformers,
  title={Transformers in time series: A survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  journal={arXiv preprint arXiv:2202.07125},
  year={2022}
}

@article{DBLP:journals/corr/abs-2009-06732,
  author       = {Yi Tay and
                  Mostafa Dehghani and
                  Dara Bahri and
                  Donald Metzler},
  title        = {Efficient Transformers: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2009.06732},
  year         = {2020},
  eprinttype    = {arXiv},
  eprint       = {2009.06732},
  timestamp    = {Fri, 18 Sep 2020 15:17:35 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-06732.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{granger2014forecasting,
  title={Forecasting economic time series},
  author={Granger, Clive William John and Newbold, Paul},
  year={2014},
  publisher={Academic press}
}

@article{yin2021deep,
  title={Deep learning on traffic prediction: Methods, analysis, and future directions},
  author={Yin, Xueyan and Wu, Genze and Wei, Jinze and Shen, Yanming and Qi, Heng and Yin, Baocai},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={23},
  number={6},
  pages={4927--4943},
  year={2021},
  publisher={IEEE}
}

@article{wu2023interpretable,
  title={Interpretable weather forecasting for worldwide stations with a unified deep model},
  author={Wu, Haixu and Zhou, Hang and Long, Mingsheng and Wang, Jianmin},
  journal={Nature Machine Intelligence},
  volume={5},
  number={6},
  pages={602--611},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{kaushik2020ai,
  title={AI in healthcare: time-series forecasting using statistical, neural, and ensemble architectures},
  author={Kaushik, Shruti and Choudhury, Abhinav and Sheron, Pankaj Kumar and Dasgupta, Nataraj and Natarajan, Sayee and Pickett, Larry A and Dutt, Varun},
  journal={Frontiers in big data},
  volume={3},
  pages={4},
  year={2020},
  publisher={Frontiers Media SA}
}

@inproceedings{
micn,
title={{MICN}: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting},
author={Huiqiang Wang and Jian Peng and Feihu Huang and Jince Wang and Junhui Chen and Yifei Xiao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@inproceedings{dong2018speech,
  title={Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition},
  author={Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5884--5888},
  year={2018},
  organization={IEEE}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{timemixer,
  title={Timemixer: Decomposable multiscale mixing for time series forecasting},
  author={Wang, Shiyu and Wu, Haixu and Shi, Xiaoming and Hu, Tengge and Luo, Huakun and Ma, Lintao and Zhang, James Y and Zhou, Jun},
  journal={arXiv preprint arXiv:2405.14616},
  year={2024}
}

@article{timesnet,
  title={Timesnet: Temporal 2d-variation modeling for general time series analysis},
  author={Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2210.02186},
  year={2022},
  publisher={arXivpreprint}
}

@article{han2024capacity,
  title={The capacity and robustness trade-off: Revisiting the channel independent strategy for multivariate time series forecasting},
  author={Han, Lu and Ye, Han-Jia and Zhan, De-Chuan},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}

@article{abdi2010coefficient,
  title={Coefficient of variation},
  author={Abdi, Herv{\'e}},
  journal={Encyclopedia of research design},
  volume={1},
  number={5},
  pages={169--171},
  year={2010}
}

@incollection{brown1998coefficient,
  title={Coefficient of variation},
  author={Brown, Charles E},
  booktitle={Applied multivariate statistics in geohydrology and related sciences},
  pages={155--157},
  year={1998},
  publisher={Springer}
}

@article{xu2022contrastive,
  title={Contrastive adversarial knowledge distillation for deep model compression in time-series regression tasks},
  author={Xu, Qing and Chen, Zhenghua and Ragab, Mohamed and Wang, Chao and Wu, Min and Li, Xiaoli},
  journal={Neurocomputing},
  volume={485},
  pages={242--251},
  year={2022},
  publisher={Elsevier}
}

@article{campos2023lightts,
  title={LightTS: Lightweight time series classification with adaptive ensemble distillation},
  author={Campos, David and Zhang, Miao and Yang, Bin and Kieu, Tung and Guo, Chenjuan and Jensen, Christian S},
  journal={Proceedings of the ACM on Management of Data},
  volume={1},
  number={2},
  pages={1--27},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{guo2023linkless,
  title={Linkless link prediction via relational distillation},
  author={Guo, Zhichun and Shiao, William and Zhang, Shichang and Liu, Yozen and Chawla, Nitesh V and Shah, Neil and Zhao, Tong},
  booktitle={International Conference on Machine Learning},
  pages={12012--12033},
  year={2023},
  organization={PMLR}
}

@article{zhang2021graph,
  title={Graph-less neural networks: Teaching old mlps new tricks via distillation},
  author={Zhang, Shichang and Liu, Yozen and Sun, Yizhou and Shah, Neil},
  journal={arXiv preprint arXiv:2110.08727},
  year={2021}
}

@article{allen2020towards,
  title={Towards understanding ensemble, knowledge distillation and self-distillation in deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2012.09816},
  year={2020}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{crossformer,
title={Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting},
author={Yunhao Zhang and Junchi Yan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}


@inproceedings{frets,
title={Frequency-domain {MLP}s are More Effective Learners in Time Series Forecasting},
author={Kun Yi and Qi Zhang and Wei Fan and Shoujin Wang and Pengyang Wang and Hui He and Ning An and Defu Lian and Longbing Cao and Zhendong Niu},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023}
}

@inproceedings{nbeats,
title={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},
author={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2020}
}

@article{lightts,
  title={Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures},
  author={Zhang, Tianping and Zhang, Yizhuo and Cao, Wei and Bian, Jiang and Yi, Xiaohan and Zheng, Shun and Li, Jian},
  journal={arXiv preprint arXiv:2207.01186},
  year={2022}
}

@inproceedings{nhits,
  title={Nhits: Neural hierarchical interpolation for time series forecasting},
  author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Ramirez, Federico Garza and Canseco, Max Mergenthaler and Dubrawski, Artur},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={6},
  pages={6989--6997},
  year={2023}
}


@article{tide,
  title={Long-term forecasting with tide: Time-series dense encoder},
  author={Das, Abhimanyu and Kong, Weihao and Leach, Andrew and Mathur, Shaan and Sen, Rajat and Yu, Rose},
  journal={arXiv preprint arXiv:2304.08424},
  year={2023}
}

@article{tsmixer,
  title={Tsmixer: An all-mlp architecture for time series forecasting},
  author={Chen, Si-An and Li, Chun-Liang and Yoder, Nate and Arik, Sercan O and Pfister, Tomas},
  journal={arXiv preprint arXiv:2303.06053},
  year={2023}
}

@book{thomas2006elements,
  title={Elements of information theory},
  author={Thomas, MTCAJ and Joy, A Thomas},
  year={2006},
  publisher={Wiley-Interscience}
}

@article{mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@article{jensen1906fonctions,
  title={Sur les fonctions convexes et les in{\'e}galit{\'e}s entre les valeurs moyennes},
  author={Jensen, Johan Ludwig William Valdemar},
  journal={Acta mathematica},
  volume={30},
  number={1},
  pages={175--193},
  year={1906},
  publisher={Springer}
}

@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@article{chen2017learning,
  title={Learning efficient object detection models with knowledge distillation},
  author={Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{takamoto2020efficient,
  title={An efficient method of training small models for regression problems with knowledge distillation},
  author={Takamoto, Makoto and Morishita, Yusuke and Imaoka, Hitoshi},
  booktitle={2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
  pages={67--72},
  year={2020},
  organization={IEEE}
}

@article{gajbhiye2021knowledge,
  title={Knowledge distillation for quality estimation},
  author={Gajbhiye, Amit and Fomicheva, Marina and Alva-Manchego, Fernando and Blain, Fr{\'e}d{\'e}ric and Obamuyide, Abiola and Aletras, Nikolaos and Specia, Lucia},
  journal={arXiv preprint arXiv:2107.00411},
  year={2021}
}

@article{kim2021comparing,
  title={Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation},
  author={Kim, Taehyeon and Oh, Jaehoon and Kim, NakYil and Cho, Sangwook and Yun, Se-Young},
  journal={arXiv preprint arXiv:2105.08919},
  year={2021}
}

@inproceedings{revin,
  title     = {Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift},
  author    = {Kim, Taesung and 
               Kim, Jinhee and 
               Tae, Yunwon and 
               Park, Cheonbok and 
               Choi, Jang-Ho and 
               Choo, Jaegul},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}

@article{non-stationary,
  title={Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting},
  author={Liu, Yong and Wu, Haixu and Wang, Jianmin and Long, Mingsheng},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{bommasani2024foundation,
  title={The foundation model transparency index v1. 1: May 2024},
  author={Bommasani, Rishi and Klyman, Kevin and Kapoor, Sayash and Longpre, Shayne and Xiong, Betty and Maslej, Nestor and Liang, Percy},
  journal={arXiv preprint arXiv:2407.12929},
  year={2024}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{dodge2022measuring,
  title={Measuring the carbon intensity of ai in cloud instances},
  author={Dodge, Jesse and Prewitt, Taylor and Tachet des Combes, Remi and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A and DeCario, Nicole and Buchanan, Will},
  booktitle={Proceedings of the 2022 ACM conference on fairness, accountability, and transparency},
  pages={1877--1894},
  year={2022}
}

@article{goswami2024moment,
  title={Moment: A family of open time-series foundation models},
  author={Goswami, Mononito and Szafer, Konrad and Choudhry, Arjun and Cai, Yifu and Li, Shuo and Dubrawski, Artur},
  journal={arXiv preprint arXiv:2402.03885},
  year={2024}
}