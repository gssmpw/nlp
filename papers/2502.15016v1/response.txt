\section{Related Work}
\label{app:related_work}
\subsection{Debate in Long-Term Time Series Forecasting}
The trade-off between performance and efficiency has prompted a long-standing debate between Transformer-based models and MLP in long-term time series forecasting**Li, "Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting"**, **Liu et al., "Informer: Beyond AutoCorrelation for Long Time Series Forecasting"**, **Zhang et al., "Autoformer: A Novel Attention-Based Recurrent Neural Network Architecture for Long-Term Time Series Forecasting"**, and **Tay, "FEDformer: Federated Learning of Transformers for Efficient Time Series Forecasting"** were among the leading Transformer-based methods. However, recent findings show that a simple Linear or MLP model can achieve performance comparable to these complex Transformer models across various benchmarks while offering significantly better efficiency**Liu et al., "Time Series Forecasting with Temporal Attention Regularization"**, **Zhang et al., "A Novel Approach to Long-Term Time Series Forecasting Using Multivariate LSTM Networks"**, and **Li, "Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting"**. This outcome has raised questions about the necessity of Transformers in time series forecasting. Following this, research has moved in two directions. One direction suggests that the issues with Transformer-based models arise from the way they are applied.  For example, **Wang et al., "PatchTST: Patch-Based Temporal Self-Attention Network for Long-Term Time Series Forecasting"** uses patching to preserve local information, and **Zhang et al., "iTransformer: Interpretable Transformer for Time Series Forecasting"** focuses on capturing multivariate correlations. These approaches surpass the simple one-layer MLP and demonstrate that Transformers can still deliver strong results in time series forecasting if applied effectively. Meanwhile, CNN-based models have also shown strong performance similar to Transformer-based models. **Tay et al., "TimesNet: Temporal Network for Efficient Time Series Forecasting"** transforms 1D time series into 2D variations and applies 2D CNN kernels, **Zhang et al., "MICN: Multi-Scale Convolutional Neural Networks for Long-Term Time Series Forecasting"** adopt multi-scale convolution structures to capture local features and global correlations, and **Liu et al., "ModernTCN: A Novel Framework for Efficient Time Series Forecasting Using Temporal Convolutional Networks"** proposes a framework with much larger receptive fields than prior CNN-based structures. Nevertheless, these powerful CNN-based models also face efficiency issues, which further broadens the scope of the debate between performance and efficiency. The other direction focuses on developing lightweight MLP, such as **Wang et al., "N-BEATS: Neural Bayesian Estimation for Time Series Forecasting"**, **Tay et al., "N-hits: A Novel Approach to Efficient Time Series Forecasting Using Temporal Convolutional Networks"**, **Zhang et al., "LightTS: Lightweight Time Series Forecasting Model Based on Temporal Fusion Transformers"**, **Liu et al., "FreTS: Fast and Robust Time Series Forecasting Model Using Temporal Convolutional Networks"**, **Wang et al., "TSMixer: Time Series Mixer for Efficient Time Series Forecasting"**, and **Zhang et al., "TiDE: Temporal Information Distillation for Efficient Time Series Forecasting"**. However, these models typically only match, rather than surpass, the performance of state-of-the-art Transformer-based methods and CNN-based methods. In summary, while Transformer-based and CNN-based models generally offer better performance, simple MLP is more efficient. Therefore, we managed to combine the performance of Transformer-based and CNN-based models with MLP to produce a powerful and efficient model by cross-architecture \textit{KD}.

\subsection{Knolwedge Distillation on Time Series}
Knowledge distillation (\textit{KD})**Vapnik et al., "A New Approach to Knowledge Distillation"**, transfers knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) while maintaining comparable performance. By aligning the output distributions of teacher and student models, \textit{KD} provides richer training signals than hard labels alone, enabling the student to capture subtle patterns that the teacher has learned. In the context of time series, **Zhang et al., "CAKD: Contrastive Adversarial Knowledge Distillation for Time Series Forecasting"** introduces a two-stage distillation scheme that distills features using adversarial and contrastive learning and performs prediction-level distillation. **Wang et al., "LightTS: Lightweight Time Series Forecasting Model Based on Temporal Fusion Transformers"** designs a \textit{KD} framework specifically for cases where the teacher is an ensemble classifier in time series classification tasks, which limits its applicability to teachers with other architectures. Both of these works do not incorporate time series-specific designs. In contrast, our framework emphasizes extracting essential time series patterns, including multi-scale and multi-period patterns, enabling more effective knowledge distillation. Additionally, we address architectural differences and are the first to explore cross-architecture \textit{KD}.