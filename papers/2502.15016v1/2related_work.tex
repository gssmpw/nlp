% \section{Related Work}
% \wei{two short paragraphs should be fine but mention detailed related work is in appendix}
% \paragraph{Debate in Long-Term Time Series Forecasting.}
% The trade-off between performance and efficiency has prompted a long-standing debate between Transformer-based models and MLP in long-term time series forecasting~\cite{dlinear, lightts, sparsetsf}. Informer~\cite{informer}, Autoformer~\cite{autoformer}, and FEDformer~\cite{fedformer} were among the leading Transformer-based methods. However, recent findings show that a simple one-layer MLP without activation function (e.g., Linear) can achieve performance comparable to these complex Transformer models across various benchmarks while offering significantly better efficiency~\cite{dlinear}. This outcome has raised questions about the necessity of Transformers in time series forecasting. Following this, research has moved in two directions. One direction suggests that the issues with Transformer-based models arise from the way they are applied.  For example, PatchTST~\cite{patchtst} uses patching to preserve local information, and iTransformer~\cite{itransformer} focuses on capturing multivariate correlations. These approaches surpass the simple one-layer MLP and demonstrate that Transformers can still deliver strong results in time series forecasting if applied effectively. Meanwhile, CNN-based models have also shown strong performance similar to Transformer-based models. TimesNet~\cite{timesnet} transforms 1D time series into 2D variations and applies 2D CNN kernels, MICN~\cite{micn} adopt multi-scale convolution structures to capture local features and global correlations, and ModernTCN~\cite{moderntcn} proposes a framework with much larger receptive fields than prior CNN-based structures. Nevertheless, these powerful CNN-based models also face efficiency issues, which further broadens the scope of the debate between performance and efficiency. The other direction focuses on developing lightweight MLP, such as N-BEATS~\cite{nbeats}, N-hits~\cite{nhits}, LightTS~\cite{lightts}, FreTS~\cite{frets}, TSMixer~\cite{tsmixer}, TiDE~\cite{tide}, which offer improved efficiency. However, these models typically only match, rather than surpass, the performance of state-of-the-art Transformer-based methods and CNN-based methods. In summary, while Transformer-based and CNN-based models generally offer better performance, simple MLP is more efficient. Therefore, we managed to combine the performance of Transformer-based and CNN-based models with MLP to produce a powerful and efficient model by cross-architecture distillation.

\vskip -1em
\section{Related Work}
We present a brief summary of related work, with a detailed version provided in \underline{Appendix~\ref{app:related_work}}. \textit{A Debate in Long-Term Time Series Forecasting:}
The trade-off between performance and efficiency has sparked debate between advanced architecture and MLP in long-term time series forecasting~\cite{dlinear, lightts, sparsetsf}. Transformers~\cite{informer,autoformer,fedformer, patchtst, itransformer} or CNNs~\cite{moderntcn,timesnet, micn} excel but face competition from a simple Linear or MLP model, which achieves comparable performance with greater efficiency~\cite{dlinear, nbeats,nhits,lightts,tsmixer}. To combine these strengths, we propose cross-architecture knowledge distillation (\textit{KD}) to achieve both high performance and efficiency.
% \paragraph{Knolwedge Distillation in Time Series.}
% Knowledge distillation (\textit{KD})~\cite{hinton2015distilling} transfers knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) while maintaining comparable performance. By aligning the output distributions of teacher and student models, \textit{KD} provides richer training signals than hard labels alone, enabling the student to capture subtle patterns that the teacher has learned. In the context of time series, CAKD~\cite{xu2022contrastive} introduces a two-stage distillation scheme that distills features using adversarial and contrastive learning and performs prediction-level distillation. LightTS~\cite{campos2023lightts} designs a \textit{KD} framework specifically for cases where the teacher is an ensemble classifier in time series classification tasks, which limits its applicability to teachers with other architectures. Both of these works do not incorporate time series-specific designs. In contrast, our framework emphasizes extracting essential time series patterns, including multi-scale and multi-period patterns, enabling more effective knowledge distillation. Additionally, we address architectural differences and are the first to explore cross-architecture \textit{KD}. 
\textit{\textit{KD} in Time Series:} 
% \wei{given limited space, we do not need the first sentence; just introducing works of KD in time series}
% \textit{KD}~\cite{hinton2015distilling} transfers knowledge from a larger, complex model (teacher) to a smaller model (student) while maintaining comparable performance. 
% In the time series domain, 
CAKD~\cite{xu2022contrastive} uses adversarial and contrastive learning for feature distillation without specific design for time series, while LightTS~\cite{campos2023lightts} designs a \textit{KD} framework for ensemble classifiers, limiting its generality. Unlike these, our framework targets time series-specific patterns, such as multi-scale and multi-period, pioneering cross-architecture \textit{KD}. 
