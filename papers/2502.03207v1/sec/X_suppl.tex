\maketitlesupplementary

\section{Object Trajectory Plotting Module}
\subsection{Details of Trajectory Plotting}
Inspired by AppAgent \cite{zhang2023appagent}, we replace direct trajectory generation with grid selection based on grid numbers. Specifically, we divide the given image into $N \times M$ grids, breaking it down into small square areas. Each area is labeled with an integer in the top-left corner and subdivided into nine subareas. Based on the previous step, we identify the starting point of the trajectory using the detection result and plot this starting point on the image, represented by a circle. Then, we provide the image overlaid with the grids and starting point as input to the agent.

\begin{figure}[b]
\centering
\includegraphics[width=0.99\columnwidth]{fig/Trajectory.pdf}
\caption{Details of trajectory plotting: The grid divides the image into small square areas. Each area is labeled with an integer in the top-left corner and is further subdivided into nine subareas.}
\label{fig:trajectory}
\end{figure}

We define the following functions: \textit{Set\_*\_Points (start: int, string; mid\_*: int, string; end: int, string)} for the agent. Here, \textit{*} is an integer ranging from $1-4$, used to achieve varying lengths and curvature in trajectory plotting. The parameters \textit{start}, \textit{mid\_*}, and \textit{end} include an integer label assigned to the grid area and a string representing the exact location within the grid area. The string can take one of the following nine values: center, top-left, top, top-right, left, right, bottom-left, bottom, and bottom-right.

A simple use case is \textit{Set\_2\_Points (start: 143, top-right; end: 33, bottom-right)}, which sets the starting point of the trajectory at the top-right of grid area 143 and the endpoint at the bottom-right of grid area 33. As illustrated in Figure \ref{fig:trajectory}, this function represents a simple linear trajectory.

Once we obtain the complete object trajectory, we use interpolation to determine the position in the trajectory for each frame. Then, these interpolated positions are input into the subsequent model to calculate dense optical flow.

\label{subsec: ablation}
\begin{table}[t]
  \centering
  \resizebox{0.80\columnwidth}{!} % 调整宽度
  {
  \begin{tabular}{cc|ccc}
    \toprule
    Offset & Grid & Object Movement & Dynamic\\
     Generation & Selection & Q\&A & Degree\\
     \midrule
    \checkmark & & 29.28 & 7.63 \\
    & \checkmark & 45.69 & 32.11 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Ablation study of trajectory plotting module (all values are in percentage).}
    \label{tab:sup_ab}
\end{table}

\subsection{More Ablation Study}
Here, we conduct an ablation study on different approaches for trajectory plotting. We use direct offset generation as the baseline module instead of grid selection. Specifically, we provide the agent with the starting point location based on the detection results and ask the agent to directly generate offsets from the starting point to define the object trajectory.

As shown in Table \ref{tab:sup_ab}, the grid selection approach shows better evaluation results in both metrics. The grid selection approach gives the agent an overall understanding of the image layout, and it is easier for the agent to use than direct offset generation. As for the dynamic degree metrics, the offset generation approach cannot output a suitable trajectory length, which may lead to a lower dynamic degree.

\begin{figure}[h]
\centering
\includegraphics[width=0.99\columnwidth]{fig/opticalflow.pdf}
\caption{Intermediate representations generation by motion field agent, including object trajectory, camera extrinsics, and unified optical flow maps.}
\label{fig:optical}
\end{figure}

\section{Training Data Preparation}
To eliminate the domain gap between the unified and real optical flow maps, we propose fine-tuning the optical flow adapter module, which maintains the generation capabilities of the base I2V diffusion model. For each video used for training, we first utilize a binary segmentation model, BiRefNet \cite{zheng2024birefnet}, to decompose the foreground and background. We then remove the dynamic foreground based on the binary segmentation mask. Next, we adopt an SLAM method, DROID-SLAM \cite{teed2021droid}, to compute the camera extrinsics $\hat{E}$ for each frame based on the masked video. Additionally, for the original video, we use an optical flow model, Unimatch \cite{xu2023unifying}, to estimate the real optical flow $\hat{F}$.

Next, we explain how to estimate the optical flow caused by object movement based on the camera extrinsics $\hat{E}$ and real optical flow $\hat{F}$. We define $I^{0}$ as the pixel position in the first frame. According to the predicted real optical flow $\hat{F}$, we compute the corresponding pixel position in the following frames, which can be formulated as:

\begin{equation} 
    \hat{I} = I^{0} + \hat{F}. 
\end{equation}

We reproject the pixel position in the following frames back to the first frame based on the predicted camera extrinsics $\hat{E}$. This can be computed by:

\begin{equation} 
    \hat{I}^{0} = \Pi(\hat{E}^{-1}\Pi^{-1}(\hat{I})), 
\end{equation}

where $\Pi$ and $\Pi^{-1}$ are the projection and unprojection operations, respectively. $\hat{I}^{0}$ indicates the pixel position in the first frame that contains only object movement. Finally, we compute the optical flow caused by object movement:

\begin{equation} 
    \hat{F}_{obj} = \hat{I}^{0} - I^{0}. 
\end{equation}

 We perform sampling \cite{zhan2019self} on these optical flow maps of object movement to obtain sparse object trajectories. Subsequently, we reuse the proposed analytical composition method to calculate the unified optical flow maps $\bar{F}$, which are utilized as input to fine-tune the optical flow adapter. Additionally, we calculate the error between the unified optical flow maps $\bar{F}$ and the real optical flow $\hat{F}$. If the error exceeds a threshold, we replace the unified optical flow maps $\bar{F}$ with the real optical flow $\hat{F}$ for training.

\section{Qualitative Results}

\subsection{Visualization of Intermediate Representations}

As illustrated in Figure \ref{fig:optical}, we show the intermediate representations generation by the motion field agent.

\subsection{More Comparison Results}
In Figure \ref{fig:comparison_sup}, we show more results compared to DynamiCrafter \cite{xing2025dynamicrafter}, CogVideoX \cite{yang2024cogvideox} and Pyramid Flow \cite{jin2024pyramidal}.

\section{User Study Interface}
Figure \ref{fig:interface} shows the designed user study interface. For each question, we randomly shuffle four videos generated by our method and the other three methods. We then ask participants to rank the videos from highest to lowest twice based on specific requirements. After the user study, we calculate the mean ranking for each method across different evaluation dimensions.

\begin{figure*}[h]
\centering
\includegraphics[width=0.99\linewidth]{fig/comparison_sup_1.pdf}
\caption{More comparison results of controllable I2V generation on our benchmark. The motion described in the text is in \textbf{bold}.}
\label{fig:comparison_sup}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.67\linewidth]{fig/interface.pdf}
\caption{User study interface. Each participant is required to evaluate 30 groups of videos and respond to two corresponding
sub-questions for each group. Due to the page limit, only one group of videos and two sub-questions are shown here.}
\label{fig:interface}
\end{figure*}

