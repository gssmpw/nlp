\section{Conclusion}
\label{sec:conclusion}
This work aims to address the challenge of precisely controlling I2V generation using only text input. We first propose a motion field agent that converts the motion information in the text into object trajectories and camera extrinsics. Then, we design an analytical optical flow composition module to integrate the explicit motion representations into unified optical flow maps. Next, we fine-tune an optical flow adapter to achieve controllable I2V generation. In the experiments, we construct a new video motion benchmark, and the evaluation results demonstrate that our method significantly outperforms other text-guided I2V generation models.

\noindent{\textbf{Limitation.} Due to the limited reasoning capabilities of LLMs, our method may fail to understand particularly complex motions. In these cases, the object trajectories and camera extrinsics generated by the motion field agent may be unreasonable. In future work, we can address these problems by constructing text-motion pairs and fine-tuning the LLMs to enhance their ability to understand particularly complex motions.}