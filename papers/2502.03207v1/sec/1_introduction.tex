\section{Introduction}
\label{sec:introduction}

\begin{figure}[t]
\centering
\includegraphics[width=0.99\columnwidth]{fig/framework.pdf}
\caption{Different frameworks of I2V generation models. (a) Controllable I2V generation via text encoder. (b) Controllable I2V generation via special control module. (c) Our method, controllable I2V generation via motion field agent.}
\label{fig:framework}
\end{figure}

Recently, image-to-video (I2V) generation models \cite{chen2023videocrafter1,dai2023animateanything,zhang2023i2vgen,guo2024i2v,ren2024consisti2v,xing2025dynamicrafter,zhang2024moonshot,ma2024cinemo,blattmann2023stable,ma2024follow,jin2024pyramidal,yang2024cogvideox} develop rapidly. These models bring images to life, making the visual content more dynamic and vivid. Compared to text-to-video (T2V) generation models \cite{ho2022video,ho2022imagen,singer2022make,ge2023preserve,mei2023vidm,he2022latent,an2023latent,blattmann2023align,guo2023animatediff}, I2V models use an image as the reference, which further constrains the content and reduces the uncertainty of the generated video. Most existing I2V generation models already achieve high-quality video generation, allowing them to preserve the visual details of the input image while generating stable results. However, precise control over the video using only text input remains a field worth exploring.

Some I2V generation models \cite{guo2024i2v,blattmann2023stable} randomly make the input image dynamic, resulting in an uncontrollable generated result. Other I2V models \cite{xing2025dynamicrafter,zhang2023i2vgen,ren2024consisti2v,dai2023animateanything,zhang2024moonshot,ma2024cinemo,ma2024follow} support simple control through text input. As shown in Figure \ref{fig:framework}(a), they adopt a text encoder to inject control information into visual features. They can perform overall control, but it is difficult to achieve fine-grained control over each element of the video. Additionally, training an I2V generation model with high alignment between motion information in the text and the video relies heavily on the quality of training data \cite{bain2021frozen,xue2022advancing,wang2023internvid,chen2024panda,tan2024vidgen}, and obtaining high-quality training data is also labor-intensive.

As illustrated in Figure \ref{fig:framework}(b), some controllable I2V generation models adopt specially designed modules for controlling different motion types, such as object movement \cite{wang2024motionctrl,yin2023dragnuwa,wu2025draganything,zhou2024trackgo,zhang2024tora,niu2024mofa,wu2024motionbooth} or camera motion \cite{he2024cameractrl,xu2024cavia,xu2024camco}. These modules are proposed for individual control conditions, making it difficult to control multiple motion types simultaneously. Additionally, inputting these control conditions may require expert knowledge, which creates a barrier for non-professional users. These methods are less flexible than those that directly control video motion through text input.

To this end, we propose MotionAgent, which enables fine-grained control for text-guided I2V generation. The key design is the motion field agent that converts the motion information in the text prompts into explicit motion representations, providing precise motion guidance. As shown in Figure \ref{fig:framework}(c), our framework directly takes the text as input. First, the agent parses and converts the motion information in the text into object trajectories and camera extrinsics, which explicitly represent object movement and camera motion, respectively. Then, these two intermediate representations are integrated in the 3D space and projected into unified optical flow maps through the proposed analytical optical flow composition module. Finally, we tune an optical flow adapter, using the unified flow as conditions, to control a base I2V diffusion model for video generation.

In the experiments, we first evaluate our method on a public I2V generation benchmark \cite{huang2024vbench}. Evaluation results demonstrate that our approach significantly improves the control accuracy of camera motion and achieves comparable video quality with other advanced models. Furthermore, we construct a subset of VBench for assessing the alignment of motion information in the text and the generated video. The results illustrate that our method achieves the best motion accuracy. We also conduct a user study, which shows that our generated videos align better with the motion information in the text and maintain high quality. Finally, we conduct ablation studies to verify the effectiveness of the proposed components. Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose a novel I2V generation pipeline via a motion field agent, which enables fine-grained motion control for text-guided image-to-video generation.
    \item We construct a new video motion benchmark, a subset of VBench, to assess the alignment of fine-grained motion information in the text and the generated video.
    \item Extensive experiments demonstrate the effectiveness of our proposed method which achieves the most accurate motion control in I2V generation with only text input.
\end{itemize}
