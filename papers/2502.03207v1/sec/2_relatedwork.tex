\section{Related Work}
\label{sec:relatedwork}

\subsection{Video Generation via Agent}
The development of ChatGPT \cite{Chatgpt} and GPT-4 \cite{achiam2023gpt} represents a crucial advancement in natural language processing. These large language models (LLMs) \cite{touvron2023llama,zeng2022glm,taori2023stanford,liu2024visual,zhang2023video} enable multi-round conversations and have the impressive ability to follow complex instructions. Furthermore, the integration of vision capabilities in GPT-4V \cite{yang2023dawn} is a milestone, enabling LLMs to process and interpret multimodal data. The vision capabilities in LLMs pave the way for the development of multimodal LLM-powered agents. These agents increasingly gain attention for their ability to effectively handle multimodal inputs and achieve complex tasks.

A series of studies propose to exploit the knowledge of agents for achieving controllable generation \cite{lian2023llm,feng2024layoutgpt,lin2023videodirectorgpt}, zero-shot generation \cite{huang2024free,lu2023flowzero,hong2023direct2v,oh2023mtvg}, or long video generation \cite{zhuang2024vlogger,tian2024videotetris}. Anim-Director \cite{li2024anim} builds an autonomous animation-making agent for generating contextually coherent animation videos. DreamFactory \cite{xie2024dreamfactory} leverages multi-agent collaboration principles and a keyframe iteration method to ensure consistency and style across long videos. VIDEOAGENT \cite{soni2024videoagent} aims to improve video plan generation based on a feedback pipeline. ChatCam \cite{liu2024chatcam} introduces a system that navigates camera motion through conversations with users and mimics a professional cinematographer’s workflow.

Our research contributes to this area by proposing a motion field agent to decompose and convert the motion information in the text into explicit intermediate representations of object movement and camera motion to guide precise control over video generation.

\subsection{Text-guided I2V Generation}
Generally, most I2V generation models \cite{dai2023animateanything,zhang2024moonshot,ma2024cinemo,jin2024pyramidal} support text and image input simultaneously. The input image guides the visual content of the video, while the text indicates the potential motion. VideoCrafter \cite{chen2023videocrafter1}, Dynamicrafter \cite{xing2025dynamicrafter}, ConsistI2V \cite{ren2024consisti2v}, and I2VGen-XL \cite{zhang2023i2vgen} are all based on the U-Net \cite{ronneberger2015u} backbone and use a text encoder to inject text embeddings into visual features, enabling text control over video content. CogVideoX \cite{yang2024cogvideox} and Open-Sora \cite{OpenSora}, based on the DIT \cite{peebles2023scalable} architecture, gain control capability by jointly learning text tokens and visual tokens. These methods, which directly learn the similarity between text embeddings and visual features, often highly depend on the quality of training data. However, there is a lack of large high-quality datasets with text labels that describe video motion. The text labels in most video datasets \cite{bain2021frozen,xue2022advancing,wang2023internvid,chen2024panda,tan2024vidgen} describe the content of a frame without much motion information and are not suitable for training I2V generation models with strong motion control capability, which leads to poor performance of these methods in achieving precise motion control through direct text input.

\subsection{I2V Generation via Motion Control Module}
With the development of video generation models, controllable video generation gradually attracts more attention. Existing methods \cite{zhang2024tora,xu2024cavia,xu2024camco} propose designing specialized modules for certain motion types, such as object movement or camera motion. DragNUWA \cite{yin2023dragnuwa} encodes sparse point trajectories into dense features as guidance information, which is then injected into the diffusion model to control object motion. DragAnything \cite{wu2025draganything} uses masks to identify a central point and subsequently generates a Gaussian map to track this center, providing a guiding trajectory for object-controllable I2V generation. TrackGo \cite{zhou2024trackgo} introduces a TrackAdapter, which encodes object trajectories into the network through a dual attention mechanism to control object movement in the generated videos. Other video generation methods achieve control of camera motion. MotionCtrl \cite{wang2024motionctrl} directly encodes camera extrinsics as features and inputs them to the model. IMAGE CONDUCTOR \cite{li2024image} and CameraCtrl \cite{he2024cameractrl} both convert sparse camera extrinsics into dense feature maps using Plücker embeddings and employ these dense embeddings to control camera motion.

In addition to encoding control conditions into deep features, some methods generate explicit intermediate representations, such as optical flow, to control the generated video. Motion-I2V \cite{shi2024motion} factorizes I2V generation into two stages: It first generates optical flow maps using a flow diffusion model and then uses the optical flow maps as explicit motion guidance for video generation. MOFA-Video \cite{niu2024mofa} proposes a generative motion field adapter. They first convert sparse point trajectories into dense optical flow through a sparse-to-dense network and then use an optical flow adapter to control video generation. Although these models achieve control over the videos via a motion control module, and their control capabilities are stronger than those of text-based models, they often require specific knowledge, such as providing object trajectories or camera extrinsics, which creates a barrier for users. Moreover, most controllable I2V generation methods can only control a single motion type at a time and struggle to achieve simultaneous control of all types.

Our method leverages the optical flow adapter proposed by MOFA-Video \cite{niu2024mofa} to control the base I2V diffusion model. Unlike MOFA-Video, our approach has two differences: 1) We propose a motion field agent that allows direct control of the generated video through text input rather than manual trajectory plotting. 2) We introduce an analytical optical flow composition module, which enables our method to simultaneously control both object movement and camera motion.