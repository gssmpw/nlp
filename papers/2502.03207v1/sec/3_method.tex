\section{Method}
\label{sec:method}

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\linewidth]{fig/agent.pdf}
\caption{Pipeline of Motion Field Agent. \textbf{Step 1:} The agent first parses the input text, dividing the motion information into two parts that respectively describe object movement and camera motion. \textbf{Step 2:} The agent draws the object trajectories according to the text of object movement. \textbf{Step 3:} The agent directly generates the camera extrinsics based on the text of camera motion.}
\label{fig:agent}
\end{figure*}

Our method aims to achieve fine-grained controllable I2V generation through text input, allowing precise control of both object movement and camera motion. Specifically, a motion field agent first converts the motion information in the text into object trajectories and camera extrinsics. These two explicit intermediate representations are then fed into an improved controllable I2V generation model to guide the video generation. Within the controllable I2V generation model, the object trajectories and camera extrinsics are integrated by an analytical optical flow composition module to compute unified optical flow maps. Subsequently, we employ Stable Video Diffusion (SVD) \cite{blattmann2023stable} as the base I2V diffusion model and utilize an optical flow adapter \cite{niu2024mofa} as the motion control module to generate the final video.

\subsection{Motion Field Agent}
As for the motion field of a video, we generally decompose it into object movement within the frame and overall camera motion. Based on the above decomposition, our agent performs two key tasks: object trajectory plotting and camera extrinsics generation. The details of the designed agent can be found in Figure \ref{fig:agent}.

\subsubsection{Video Motion Decomposition}
The agent analyzes motion information in the text and splits it into parts that respectively describe object movement and camera motion. This step decouples the complex motion information, allowing for precise and independent control of each motion type.

\subsubsection{Object Trajectory Plotting}
The object trajectory plotting step can be divided into two sub-tasks: object identification and trajectory plotting.

\noindent{\textbf{Object Identification}:
(1) The agent first splits the text of object movement into independent descriptions of each object shown in the given image. (2) Then, the agent further finds the dynamic object described in the text and feeds it into an open-world object detection algorithm. (3) The Grounded-SAM \cite{ren2024grounded} model is called for auxiliary detection and segmentation, facilitating the agent in identifying the corresponding object in the input images. All detection and segmentation results are plotted on the image as semi-transparent masks, which are fed back into the agent for identifying the object.}

\noindent{\textbf{Trajectory Plotting}:
(4) The agent locates the object in the image based on the detection and segmentation results, thereby determining the starting point of the trajectory. (5) The agent plots the trajectory with varying lengths and curvature according to the complexity of the text description.}

To enhance the capabilities of trajectory plotting, we replace direct trajectory generation with a grid selection approach \cite{zhang2023appagent}. An image is divided into $N \times M$ grids, with each grid labeled by an integer. The agent determines each point of the trajectory by selecting grid numbers, and the trajectory is formed by sequentially connecting all the points. More details of trajectory plotting can be found in the supplementary material.

\subsubsection{Camera Extrinsics Generation}
Agent directly generates camera extrinsics $E$ based on the text of camera motion and the input image. The text description specifies the motion type, such as zoom in/out, pan left/right, and tilt up/down. The image helps the agent determine the magnitude of the camera motion. For instance, a wide landscape scene may require intensive camera motion, while a narrow close-up shot may only need slight adjustments of camera location. It is worth noting that we constrain the translation $T$ of the camera extrinsics to $(-1, 1)$. In the next analytical optical flow composition module, we rescale the translation $T$ back to a reasonable range based on the estimated depth map.

\begin{figure}[t]
\centering
\includegraphics[width=0.99\columnwidth]{fig/I2V_Model.pdf}
\caption{Controllable I2V Generation Model. (a) The analytical optical flow composition module calculates unified optical flow maps based on the object trajectories and camera extrinsics. (b) The unified flow maps are fed into a fine-tuned optical flow adapter as the control condition. Then, we generate precisely controlled video results based on a base I2V diffusion model.}
\label{fig:i2v_model}
\end{figure}

\subsection{Controllable I2V Generation}

After the agent converts the motion information in the text into object trajectories and camera extrinsics, our improved controllable I2V generation model uses these two intermediate representations as inputs, enabling fine-grained control of object movement and camera motion in the video.

\subsubsection{Analytical Optical Flow Composition}
As shown in Figure \ref{fig:i2v_model}(a), this module employs optical flow as a proxy to geometrically compose object movement and camera motion, enabling our I2V generation model to accurately control each motion type in a unified manner.

We first use Metric3D \cite{yin2023metric3d} to estimate the depth map $D$ of the input image and unproject each pixel $I^{0}$ in the image to obtain the 3D location $P$.

CMP \cite{zhan2019self} is utilized to estimate the dense optical flow of object movement $F_{obj}$ according to the sparse object trajectories. Based on the estimated optical flow $F_{obj}$ and the depth map $D$, we calculate the 3D location offset $O$ caused by object movement. Note that we assume a large focal length for all images, thereby neglecting the offset on the z-axis, where $o_{z}=0$. We simply assume that the 3D location offset of the $i$-th pixel on the x/y-axis $o^{i}_{xy}$ is proportional to its depth $d^{i}$, with the same direction as the corresponding optical flow $f^{i}_{obj}$. It can be formulated as,
\begin{equation}
    o^{i}_{xy} = K^{-1}\cdot{f^{i}_{obj}}\cdot{d^{i}},
\end{equation}
where $K$ is the camera intrinsics.

Then, we apply the offset on the x/y-axis $o^{i}_{xy}$ to the 3D location of $i$-th pixel $p^{I}_{xy}$, which can be computed as,
\begin{equation}
    \hat{p}^{i}_{xy} = p^{I}_{xy} + o^{i}_{xy},
\end{equation}
where $\hat{p}^{i}_{xy}$ is the new 3D location of $i$-th pixel on the x/y-axis, and $\hat{p}^{i}_{z} = p^{i}_{z}$.

We rescale the translation $T$ of the camera extrinsics $E$ according to the maximum depth range. Subsequently, we reproject these moved points $\hat{P}$ into the corresponding image coordinate systems based on the camera extrinsics $E^{t}$ to calculate the new pixel position $I^{t}$ in the $t$-th frame. It can be calculated as,
\begin{equation}
    I^{t} = \Pi(E^{t}\hat{P}),
\end{equation}
where, $E^{t}$ is the camera extrinsics of the $t$-th frame, and $\Pi$ is the projection operation. We assume that the camera coordinate of the first frame serves as the world coordinate.

Finally, we calculate the pixel offsets of the corresponding pixel points, which can be noted as,
\begin{equation}
    \bar{F}^{t} = I^{t} - I^{0},
\end{equation}
where $\bar{F}$ are named unified optical flow maps. These flow maps contain motion information of both object movement and camera motion.

Figure \ref{fig:i2v_model}(b) illustrates the process of controllable I2V generation according to the unified optical flow maps. By adopting the unified optical flow maps as the control conditions, we leverage the optical flow adapter proposed by MOFA-Video \cite{niu2024mofa} and the frozen I2V diffusion model SVD \cite{blattmann2023stable} to achieve controllable I2V generation.

\subsubsection{Optical Flow Adapter Tuning}
Since the unified optical flow maps involve many simplifications and approximations, they contain geometrically unrealistic regions compared to the true optical flow maps. Directly using these flow maps as input for the optical flow adapter, which is trained on real optical flow maps, may lead to degraded results in the generated videos. We propose fine-tuning the optical flow adapter on the unified optical flow maps.

For each training video, we first use an optical flow model, Unimatch \cite{xu2023unifying}, to estimate the real optical flow. Then we employ a SLAM method, DROID-SLAM \cite{teed2021droid}, to compute the camera extrinsics for each frame. Next, we eliminate the optical flow caused by camera motion based on the estimated camera extrinsics, and approximately obtain the optical flow caused by object movement. We perform sparse sampling \cite{zhan2019self} on these optical flow maps to obtain sparse object trajectories. More details of data preparation can be found in the supplementary material. Subsequently, we reuse the proposed analytical composition method to calculate the unified optical flow maps, which are utilized as input to fine-tune the optical flow adapter. This step eliminates the domain gap between real and unified flow fields and facilitates high-quality video results.
