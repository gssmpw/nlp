\section{Experiments}
\label{sec:experiments}
We provide extensive comparisons to evaluate our models on controllable I2V generation tasks. First, we introduce the implementation details and evaluation metrics (Sec. \ref{subsec: implementation}). Importantly, we show the results on the general I2V generation benchmark (Sec. \ref{subsubsec: general}), controllable I2V generation benchmark (Sec. \ref{subsubsec: controllable}), and user study (Sec. \ref{subsubsec: user}). Lastly, we conduct ablation studies to demonstrate the effectiveness of each module in our MotionAgent (Sec. \ref{subsec: ablation}).

\subsection{Implementation Details}
\label{subsec: implementation}
For the motion field agent, we adopt GPT-4o \cite{achiam2023gpt} as the LLM. We input the image with a resolution of $2560\times1600$ to the agent. In the trajectory plotting module, we divide the image into $20\times10$ grids. For controllable I2V generation, we tune the optical flow adapter module on 32 NVIDIA A800 GPUs. The frozen base I2V generation model is SVD \cite{blattmann2023stable}. We use AdamW as an optimizer. During training, we randomly sample 24 video frames with a stride of 4. The learning rate is set to $2\times10^{-5}$ with a resolution of $512\times512$.

\textbf{Metrics.} In the comparison experiments of the general I2V generation task, we adopt the same evaluation metrics introduced by VBench \cite{huang2024vbench}. In the comparison of controllable I2V generation, we report Object Movement Q\&A, Complex Camera Motion, and Overall Score.

I2V Score reports the overall score of I2V generation metrics. Video-Text Camera Motion assesses the consistency between camera motion and the input text, such as zoom in/out. Video-Image Subject Consistency assesses whether the appearance of the subject remains consistent throughout the entire video compared to the input image. Video-Image Background Consistency evaluates the temporal consistency of background scenes with the input image. Subject Consistency assesses whether the subject’s appearance remains consistent throughout the entire video. Background Consistency evaluates the temporal consistency of the background scenes across frames. Motion Smoothness evaluates whether the motion in the generated video is smooth and follows the physical laws of the real world. Aesthetic Quality evaluates the artistic and aesthetic value perceived by humans towards each video frame. Dynamic Degree evaluates the level of dynamics generated by each model. Object Movement Q\&A assesses the consistency between the text description and object movement in the video. Complex Camera Motion evaluates the consistency between complex camera movements in the generated video and the input text description. Total Score is the overall metric of controllable I2V generation. All evaluation metrics are higher-the-better.

\subsection{Comparison with other SOTA Methods}
\subsubsection{General I2V Generation}
\label{subsubsec: general}
\begin{table*}[ht]
  \centering
  \resizebox{1.0\linewidth}{!}
  {
  \begin{tabular}{lcccccccccc}
    \toprule
    \multirow{2}{*}{Method} & I2V & Video-Text & Video-Image & Video-Image & Subject & Background & Motion & Aesthetic & Dynamic\\
     & Score & Camera Motion & Subject Consistency & Background Consistency & Consistency & Consistency & Smoothness & Quality & Degree\\
    \midrule
    VideoCrafter \cite{chen2023videocrafter1} & 88.95 & 33.60 & 91.17 & 91.31 & \underline{97.86} & \textbf{98.79} & 98.00 & 60.78 & 22.60 \\
    ConsistI2V \cite{ren2024consisti2v} & 94.81 & \underline{\underline{33.92}} & 95.82 & 95.95 & 95.27 & \underline{98.28} & 97.38 & 59.00 & 18.62 \\
    SEINE \cite{chen2023seine} & 96.26 & 20.97 & 97.15 & 96.94 & 95.28 & 97.12 & 97.12 & 64.55 & \underline{\underline{27.07}} \\
    I2VGen-XL \cite{zhang2023i2vgen} & 96.98 & 13.00 & 97.52 & 97.68 & \underline{\underline{96.36}} & 97.93 & \underline{\underline{98.31}} & \underline{\underline{65.33}} & 24.96 \\
    Animate-Anything \cite{dai2023animateanything} & \textbf{98.31} & 13.08 & \textbf{98.76} & \underline{98.58} & \textbf{98.90} & \underline{\underline{98.19}} & \underline{98.61} & \textbf{67.12} & 2.68 \\
    DynamiCrafter \cite{xing2025dynamicrafter} & \underline{97.98} & \underline{35.81} & \underline{98.17} & \textbf{98.60} & 95.69 & 97.38 & 97.38 & \underline{66.46} & \textbf{47.40} \\
    SVD \cite{blattmann2023stable} & 96.93 & -- & 97.51 & 97.62 & 95.42 & 96.77 & 98.12 & 60.23 & \underline{43.17} \\
    \textbf{MotionAgent} (Ours) & \underline{\underline{97.51}} & \textbf{81.91} & \underline{\underline{98.06}} & \underline{\underline{98.00}} & 96.10 & 96.76 & \textbf{98.93} & 64.48 & 16.67 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Evaluation results of general I2V generation on VBench \cite{huang2024vbench} (all values are in percentage). The best result is indicated in \textbf{bold}, the second-best result is indicated with \underline{underlines}, and the third-best result is indicated with \underline{\underline{double underlines}}.}
    \label{tab:vbench}
\end{table*}

We evaluate the general I2V generation capabilities on a public video generation benchmark, VBench \cite{huang2024vbench}. We compare our model with existing I2V generation models, including VideoCrafter \cite{chen2023videocrafter1}, ConsistI2V \cite{ren2024consisti2v}, SEINE \cite{chen2023seine}, I2VGen-XL \cite{zhang2023i2vgen}, Animate-Anything \cite{dai2023animateanything}, DynamiCrafter \cite{xing2025dynamicrafter}, and our base I2V diffusion model SVD \cite{blattmann2023stable}. To ensure a fair comparison of general I2V generation, we directly use the images and original text prompts provided by VBench as inputs for our model. The evaluation results of other methods are taken directly from the leaderboard on the official website \cite{Vbench}.

As shown in Table \ref{tab:vbench}, our method maintains high-quality I2V generation capabilities. Our pipeline ranks among the top for most metrics and even achieves the best results for some. These results demonstrate that our model can be compatible with general text prompts that may do not include motion information.

Compared with the base model SVD \cite{blattmann2023stable}, our method shows better results in consistency, smoothness, and aesthetic quality metrics. These improvements are attributed to the motion field agent, which can reason and generate suitable object trajectories and camera motion even when there is no motion information included in the text prompts. Additionally, the tuning of the optical flow adapter also improves video quality by constraining the generated video with optical flow and eliminating unreasonable motion. The dynamic degree metric of our method decreases due to the precise control we implement over the generated video. The objects mentioned in the text move accurately, while those not mentioned remain as static as possible, which leads to a lower dynamic degree score. When our model is given a detailed text description with more motion information, it can generate videos with a higher dynamic degree.

Notably, our method achieves $81.91\%$ for the Video-Text Camera Motion metric, which is significantly higher than other methods. This result demonstrates that our method achieves precise control over camera motion according to the text input. The base model SVD does not support text input, while our approach enables SVD to control generated video through text input.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{fig/comparison.pdf}
\caption{Comparison results of controllable I2V generation on our benchmark. The motion described in the text is in \textbf{bold}.}
\label{fig:comparison}
\end{figure*}

\subsubsection{Controllable I2V Generation}
\label{subsubsec: controllable}
To further evaluate the control capabilities of I2V generation models through direct text input, we conduct additional assessments. Currently, existing video generation benchmarks do not specifically evaluate the alignment of motion information in the text prompts and the video, so we introduce a new benchmark to evaluate the semantic alignment between videos and input text.

We reuse images from VBench \cite{huang2024vbench} and add more motion information to the original text prompts. To evaluate the control capabilities of object movement, we design new text prompts for 83 images from VBench, providing detailed object movement descriptions. For camera motion, we modify the simple camera motion prompts of 109 images to more complex ones, such as changing “zoom in” to “first zoom in then zoom out” or “zoom in incrementally.”

For each text prompt describing object movement, we design corresponding questions. We utilize a multimodal LLM model, GPT-4o \cite{Chatgpt}, to score the semantic alignment between the object movement in the videos and the input text through a question-and-answer (Q\&A) approach. To evaluate complex camera motions, we adopt the evaluation method introduced by VBench. The compared methods include five UNet-based models: VideoCrafter \cite{chen2023videocrafter1}, ConsistI2V \cite{ren2024consisti2v}, SEINE \cite{chen2023seine}, Motion-I2V \cite{shi2024motion}, and DynamiCrafter \cite{xing2025dynamicrafter}; and two DIT-based models: CogVideoX \cite{yang2024cogvideox} and Pyramid Flow \cite{jin2024pyramidal}.

\begin{table}[t]
  \centering
  \resizebox{0.95\columnwidth}{!} % 调整宽度
  {
  \begin{tabular}{lccc}
    \toprule
    \multirow{2}{*}{Method} & Object Movement & Complex & Total\\
     & Q\&A & Camera Motion & Scores\\
    \midrule
    VideoCrafter \cite{chen2023videocrafter1} & 13.67 & 6.64 & 9.42 \\
    ConsistI2V \cite{ren2024consisti2v} & 24.57 & 6.03 & 13.35\\
    SEINE \cite{chen2023seine} & 21.99 & 1.66 & 9.69\\
    Motion-I2V \cite{shi2024motion} & 28.76 & 7.09 & 15.65\\
    DynamiCrafter \cite{xing2025dynamicrafter} & \underline{\underline{29.38}} & \underline{\underline{8.22}} & \underline{\underline{16.58}}\\
    CogVideoX \cite{yang2024cogvideox} & 26.47 & \underline{20.62} & \underline{22.93}\\
    Pyramid Flow \cite{jin2024pyramidal} & \underline{30.96} & 6.18 & 15.97\\
    \textbf{MotionAgent} (Ours) & \textbf{45.69} & \textbf{77.76} & \textbf{65.10}\\
    \bottomrule
    \end{tabular}
    }
    \caption{Evaluation results of controllable I2V generation on our benchmark (all values are in percentage). The best result is indicated in \textbf{bold}, the second-best result is indicated with \underline{underlines}, and the third-best result is indicated with \underline{\underline{double underlines}}.}
    \label{tab:control}
\end{table}

As shown in Table \ref{tab:control}, our method achieves the best results compared to other methods in controllable video generation tasks. For the Object Movement Q\&A metric, our method demonstrates stronger control over object movement, with higher accuracy scored by GPT-4o \cite{Chatgpt}. It is worth noting that current multimodal LLMs have limited reasoning capabilities, which may lead to some inaccurate judgments and responses. However, the numerical results still indicate that our method achieves the most accurate control over object movement. For the Complex Camera Motion metric, our method significantly outperforms the other methods. Compared to Motion-I2V \cite{shi2024motion}, the optical flow generated by the motion field agent represents the motion information more precisely and results in a higher total score. Moreover, our method maintains consistent performance regardless of whether the camera motion is simple or complex, as illustrated by the camera motion metrics in Tables \ref{tab:vbench} and \ref{tab:control}. Some qualitative comparison results are shown in Figure \ref{fig:comparison}, where our method showcases more precise control over generated videos. 

Our method directly converts motion information in the text prompts into the intermediate motion representations through the motion field agent, and then uses optical flow to achieve controllable video generation. Without needing high-quality video-image-text pair training data, our approach achieves fine-grained controllable I2V generation.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{fig/user_study.pdf}
\caption{User study results of controllable I2V generation on video quality and semantic alignment of the motion information in text prompt and video.}
\label{fig:us}
\end{figure}

\subsubsection{User Study}
\label{subsubsec: user}
We collect 30 images from VBench \cite{huang2024vbench} and expand the prompts by adding detailed descriptions of object movement and camera motion. We then generate videos using the official codes of DynamiCrafter \cite{xing2025dynamicrafter}, CogVideoX \cite{yang2024cogvideox}, and Pyramid Flow \cite{jin2024pyramidal}. The user study is expected to be completed in 10-20 minutes. For each case, the user study interface shows the videos generated by four methods, and participants are instructed to evaluate the videos from two dimensions: (i) “Please sort the videos by visual quality from best to worst.”; (ii) “Please sort the videos from high to low based on the semantic alignment of the motion information in the text prompt and the video.”. Finally, we receive 50 valid responses from the participants.

As illustrated in Figure \ref{fig:us}, our method achieves the top rank in both dimensions. These results demonstrate that our method generates high-quality videos with the best alignment of motion information in the text.

\subsection{Ablation Study}
\label{subsec: ablation}
\begin{table}[t]
  \centering
  \resizebox{0.99\columnwidth}{!} % 调整宽度
  {
  \begin{tabular}{cccc|ccc}
    \toprule
    Detection & Object & Camera & Flow & Object Movement & Complex & Dynamic\\
     Tool & Movement & Motion & Composition & Q\&A & Camera Motion & Degree\\
     \midrule
    & \checkmark & \checkmark & \checkmark & 34.33 & 75.11 & 20.53 \\
    \checkmark & \checkmark & & \checkmark & 38.20 & 0.30 & 29.95 \\
    \checkmark & & \checkmark & \checkmark & 10.51 & 75.95 & 8.42 \\
    \checkmark & \checkmark & \checkmark & & 30.07 & 64.92 & 27.89 \\
    \checkmark & \checkmark & \checkmark & \checkmark & 45.69 & 77.76 & 32.11 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Ablation study of object identification module and optical flow composition module on our benchmark (all values are in percentage).}
    \label{tab:ab}
\end{table}

\subsubsection{Object Identification}
We conduct an ablation study on the approach to identify the objects described in the text. In our method, we use Grounded-SAM \cite{ren2024grounded} as an auxiliary detection algorithm to help the agent locate the corresponding objects. Here, we also propose a detection-free approach to achieve object identification through multiple rounds of dialogue.

Specifically, we first ask the agent to determine an initial position, which represents the object described in the text. Then, we plot this initial position directly on the image and input it again along with the text description to the agent. If the agent considers the current initial position correct, it returns the current position directly. If the agent considers the current position incorrect, it returns a new position. The dialogue loop continues until the agent confirms the correctness of the last selected position.

We compare the two approaches for identifying the objects described in the text. As shown in Table \ref{tab:ab}, the results using Grounded-SAM (full model) are significantly better than those using the multi-round dialogue (without a detection tool). The multi-round dialogue leads to some incorrect judgments, resulting in a lower score on the Object Movement Q\&A metric. Additionally, our method decomposes object movement and camera motion, so the degradation caused by incorrect object identification does not significantly affect the camera motion in the generated video. Moreover, the misjudgments in object identification lead to the degradation of the dynamic degree metric.

\subsubsection{Optical Composition}
Then, we conduct an ablation study on the analytical optical flow composition module and use the original generative motion field adapter proposed by MOFA-Video \cite{niu2024mofa} as a baseline. The baseline can only control object movement or camera motion independently, so we define two models: “without camera motion” and “without object movement,” which represent models that can only control either object movement or camera motion independently.

In our proposed module, we design an approach to compose optical flow using an analytical method. To verify the effectiveness of this approach, we design a mechanism that directly adds the optical flow of object movement and the optical flow caused by camera motion, which we call “without flow composition.”

As shown in Table \ref{tab:ab}, the baseline models can only control object movement or camera motion independently. Compared to “without flow composition,” our proposed analytical optical flow composition module achieves higher scores on both the Object Movement Q\&A and Complex Camera Motion metrics. Directly adding optical flows results in many unreasonable areas in the added flow maps, and the object optical flow may be overshadowed by the camera optical flow. For the dynamic degree metric, our full module also achieves the highest scores. Moreover, the dynamic degree results are higher than those acquired from the original prompts in VBench \cite{huang2024vbench}. This showcases that our method generates videos with a higher dynamic degree when there is more detailed motion information in the given text, and follows the text instructions accurately.
