\section{Introduction}

Vision-language models (VLMs) like Contrastive Language-Image Pretraining (CLIP) \cite{radford2021learning} have achieved widespread adoption due to their shared embedding space for text and image modalities, which enables strong performance on various downstream tasks. However, a fundamental limitation has emerged: CLIP often struggles with compositionality \cite{thrush2022winoground}, specifically the ability to bind attributes to corresponding objects in complex scenes \cite{Tang2023, Lewis2024, Yuksekgonul2023}. Compositionality is essential for VLMs, as it allows models to generalize effectively by combining simpler concepts and understanding their relations.
\begin{figure}[h]
  \centering
   \includegraphics[width=0.48\textwidth]{figures/drawing_darina_aligned.pdf}
   \caption{\small\textbf{LABCLIP mitigates the BoW behavior of CLIP.} (1) It has been reported that CLIP behaves like a BoW model with weak attribute-object binding. (2) We discover that embeddings of individual image and text modalities already contain the attribute-object binding information; this suggests that the cross-modal BoWness stems from the lack of alignment across the modalities. (3) A simple linear transformation of the text modality effectively mitigates the BoWness of CLIP.}   
    \label{fig:bow_illustration}
\end{figure}

Recent studies \cite{Yuksekgonul2023} have shown that CLIP frequently behaves like a bag-of-words (BoW) model, failing to bind attributes to corresponding objects. For instance, when presented with an image of ``an orange square and a blue triangle'' as in Figure \ref{fig:bow_illustration}, CLIP often matches the image to a caption ``a blue square and an orange triangle''. It is often unable to distinguish the structural difference. We refer to this phenomenon as \textbf{BoWness}, indicating the model's treatment of each data point as an unordered set of concepts. The BoWness significantly limits CLIP's compositional understanding. Previous research has evaluated this limitation by jointly considering the image and text embeddings. However, there has been little investigation into the source of the inability. In particular, we do not know whether the BoWness arises (1) from a lack of attribute-object binding information in the individual text and image embeddings or (2) from a mere lack of cross-modal alignment, achieved through cosine similarity.




In this work, we provide an answer to this question by measuring the presence of attribute-object binding information in individual image and text embeddings of CLIP. We train linear probes designed to extract attribute information for particular objects of interest. We then show that individual modalities linearly separate the attributes for each object present in the image or caption in almost any case. We thus verify that the embeddings of individual modalities contain the right attribute-object binding.

Building on this insight, we hypothesize that the CLIP's BoWness is attributable to superficial misalignment stemming from the cosine similarity computation. To validate this, we propose Linear Attribute Binding CLIP, or \textbf{LABCLIP}, to align the text and image modalities with a simple linear transformation \( \mathbf{A} \). The linear transformation is trained with synthetic negative samples obtained by permuting attribute-object associations in text captions in image-caption datasets. The CLIP encoders remain frozen. Empirically, we demonstrate significant improvements in cross-modal attribute-object binding in multiple benchmarks including ARO, SugarCrepe, and COCO. 


    












\section{Related work}



\noindent
\textbf{Limitations of CLIP's Encoders.} Numerous studies have highlighted weaknesses in both CLIP's vision and text encoders. CLIP's visual encoder tends to prioritize high-level understanding, often missing finer details crucial for distinguishing objects \cite{tong2024eyes}. Meanwhile, its text encoder struggles with tasks involving negations, spatial and numerical reasoning, and nuanced attribute distinctions \cite{tong2024mass, kamath2023text}. These limitations affect performance in downstream applications \cite{parashar2024neglected, tong2024eyes, tong2024mass}. Several efforts aim to interpret CLIP’s representations to understand these limitations better \cite{esfandiarpoor2024if, yun2023do, bhalla2024interpreting}.  In contrast, our work specifically focuses on attribute-object binding in scenarios with multiple objects and attributes, providing a targeted analysis of its compositional capabilities.

\noindent
\textbf{Compositional Reasoning and Alignment.} Compositional reasoning, critical for understanding complex scenes, has been studied extensively in neural networks \cite{hupkes2020compositionality, greff2020binding}. Prior work on CLIP investigates its ability to handle novel object-attribute combinations \cite{abbasi2024deciphering, bao2023prompting} and attributes its failures to weak compositional reasoning \cite{Lewis2024, Tang2023}. Some research employs controlled setups with two objects and distinct attributes \cite{Lewis2024, Tang2023}, providing initial insights into the attribute-object binding problem. Unlike these studies, our work examines whether CLIP’s architecture inherently limits its ability to bind attributes to objects by evaluating its capacity for capturing binding information within and across modalities.

\noindent
\textbf{Benchmarks for Compositionality.} A growing number of benchmarks evaluate compositionality in vision-language models, often using hard negatives or fine-grained distractors to challenge CLIP’s compositional reasoning \cite{zhao2022vl, ma2023crepe, ray2024cola, Yuksekgonul2023, hsieh2024sugarcrepe, thrush2022winoground}. These benchmarks vary in focus: some test fine-grained distinctions \cite{awal2024vismin}, others use hard positive pairs for reasoning \cite{kamath2024the}, and some target specific challenges like counting \cite{paiss2023teaching} or spatial reasoning \cite{kamath2023up}. Additionally, synthetic benchmarks provide controlled environments to test compositional reasoning with targeted scenarios often missing in real-world datasets \cite{Bordes2024, Johnson2017}. Our work extends these efforts by contributing a synthetic, controlled dataset with greater variation, specifically designed to evaluate attribute-object binding.























\definecolor{headerblue}{HTML}{D3E7E4} %
\definecolor{rowgrey}{HTML}{F2F2F2}    %





\section{The binding problem}
 

Ideally, vision-language models like CLIP need to capture the compositional structure of real-world scenes. A necessary condition for compositional understanding is the ability to accurately bind attributes to corresponding objects in scenes with multiple objects. Prior research suggests that CLIP's attribute-object binding is often arbitrary to the degree that the model can effectively be thought of as a bag-of-words (BoW) extractor that treats objects and attributes in image and text as an unordered collection of concepts, completely ignoring the order and structure therein \cite{Yuksekgonul2023}. 



In this work, we define \textbf{BoWness} of a vision-language model as the general tendency in models to treat inputs (image or text) as unordered sets of concepts. In contrast, we say that a model has a \textbf{binding ability} when it can link attributes correctly to the corresponding objects.



\subsection{Preliminaries} 

A CLIP model includes two feature extractors: $f_{\text{image}} : \cI \rightarrow \R^D$ for images and $f_{\text{text}} : \cT \rightarrow \R^D$ for texts, where $D$ is the dimensionality of CLIP encoders. For an image $\mathbf{x}^{\text{img}} \in \cI$ and a text sequence $\mathbf{x}^{\text{txt}} \in \cT$, CLIP embeds both inputs independently into a shared vision-language space. We are interested in the behavior and information content in embeddings $f_{\text{text}}(\mathbf{x}^{\text{txt}} )$ and $f_{\text{image}}(\mathbf{x}^{\text{img}})$ for a language-image pair $(\mathbf{x}^{\text{img}} , \mathbf{x}^{\text{txt}} )$.

We consider a paired image-text dataset $\mathcal{D} = \{(\mathbf{x}_{i}^{\text{img}} \mathbf{x}_{i}^{\text{txt}} )\}_{i=1}^N$ of $N$ samples, where each sample consists of a text sequence $\mathbf{x}_{i}^{\text{txt}} \in \cT$ and a corresponding image $\mathbf{x}_{i}^{\text{img}} \in \cI$. Such a dataset typically provides abundant \textit{positive pairs} with correct associations of attributes in the text captions to objects in the corresponding images. We refer to \textit{negative pairs} as synthetic pairs where the text captions in positive pairs are modified in a way that the attribute-object association is artificially broken through a permutation. For example, given a positive pair with the caption ``a red cube and a blue sphere'', we create a negative pair by keeping the image intact and modifying the caption to ``a blue cube and a red sphere''. %



The creation and usage of such synthetic negative pairs have been an established strategy in the assessment of CLIP’s ability to differentiate correct and incorrect attribute-object binding, as seen in previous benchmarks \cite{Yuksekgonul2023, thrush2022winoground, hsieh2024sugarcrepe}. Other researchers have considered incorporating the negative pairs in training or fine-tuning CLIP to further equip the model with improved attribute-object binding \cite{Yuksekgonul2023, patel2024tripletclip}. Our work, likewise, employs negative pairs for assessing and improving CLIP.













\noindent
\subsection{Datasets}
\label{sec:datasets}

In evaluating CLIP’s ability to perform attribute-object binding, we start by considering established real-world benchmarks that are commonly used to test compositional understanding in vision-language models.

    \textbf{ARO} \cite{Yuksekgonul2023} is a benchmark that tests compositionality in VLMs using real-world images, specifically evaluating their ability to understand different types of relationships, attributes, and order information.

    \textbf{SugarCrepe} \cite{hsieh2024sugarcrepe} is a benchmark for evaluating compositional understanding in vision-language models by generating fluent and sensical hard negatives.

    \textbf{COCO} \cite{lin2014coco} is a large-scale dataset containing diverse scenes and object categories used to evaluate object recognition, segmentation, and image captioning in complex real-world scenarios.


While these benchmarks offer insights into cross-modal alignment, their complexity and large variety of objects make them unsuitable for systematic evaluation of uni-modal attribute-object binding. Cross-modal binding only requires matching images to text, making it manageable despite the variability. However, for uni-modal binding, we need a controlled set of attributes and objects with consistent examples to assess how well the model links attributes to objects within a single modality. Due to this lack of control, real-world datasets are inadequate for testing uni-modal binding and are primarily suited to cross-modal alignment.

To overcome this, we use synthetic datasets. They provide exact control over objects and their attributes. This control allows for more targeted assessments of compositional behavior.

\textbf{CLEVR} \cite{Johnson2017} is a synthetic dataset featuring simple shapes and colors, offering a highly controlled environment to examine attribute-object binding without the complexities of natural scenes.

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{figures/datasets.pdf}
   \caption{\textbf{Comparison of examples from PUG:SPAR \cite{Bordes2024} and PUG:SPARE.} In PUG:SPAR, attributes correlated with object positions: objects on the left are linked to ``blue'' or ``grass'' and objects on the right are ``red'' or ``stone''. Our dataset PUG:SPARE de-correlates the potential shortcut.}
   \label{fig:pug}
\end{figure}

\textbf{PUG:SPAR} \cite{Bordes2024} is a synthetic dataset that extends the idea of controlled settings by using animal figures in varied scenes. PUG
aims to introduce a more realistic context while retaining control over attributes. SPAR stands for four variations present in the dataset: Scene, Position, Attribute, and Relation. See Figure \ref{fig:pug} for examples.


A remaining limitation of PUG:SPAR for the analysis of attribute-object binding is the presence of positional biases, where specific attributes are consistently correlated with the object positions (e.g., objects on the left are always blue or grass, and those on the right are red or stone). This opens up shortcuts through which models may achieve superficial attribute-object binding, while not genuinely understanding the structure.

\textbf{PUG:SPARE} is our modification of PUG:SPAR that mitigates the positional shortcut (Figure \ref{fig:pug}). `E' stands for `Extended'. PUG:SPARE randomizes the attribute-object associations across positions, ensuring that models may not exploit spatial bias. This modification allows for a more accurate assessment of CLIP’s compositional capabilities. The details about the dataset generation are provided in the Appendix. 

We use ARO, SugarCrepe and COCO in Section~\ref{sec:cross_modal_binding}. We use CLEVR, PUG:SPAR and PUG:SPARE in Sections~\ref{sec:status_quo}, \ref{sec:uni-modal-binding}, and \ref{sec:cross_modal_binding}.














\subsection{CLIP is a bag-of-words cross-modally}
\label{sec:status_quo}
In this section, we explain how previous approaches demonstrated the bag-of-words nature of CLIP. We reproduce prior results and confirm that CLIP is BoW cross-modally.

\noindent
\textbf{Previous approach demonstrating BoWness.}
A common approach to demonstrate CLIP’s BoWness behavior is by comparing the embeddings of an image with permutations of its caption \cite{Yuksekgonul2023}. For example, given an image of an orange square and a blue triangle, possible captions could be ``an orange square and a blue triangle'' and ``an orange triangle and a blue square''. The task is to identify the correct caption from these options, where one has correct color-object associations and the other has swapped associations. CLIP’s prediction is based on which caption embedding has a higher cosine similarity with the image embedding.


Ideally, CLIP should exhibit \textbf{cross-modal binding}, or an accurate association of attribute-object pairs across modalities. However, CLIP has been reported to be \textbf{cross-modally BoW}, treating the concepts in inputs as an unordered collection. These two opposite cases manifest as the correctness of the ranking of cosine similarities for pairs of inputs, as shown in the figure below. 

\begin{figure}[h!]
  \centering
  \vspace{-1em}
   \includegraphics[width=\linewidth]{figures/cross_modal_c.pdf}
   \vspace{-2em}
\end{figure}

\noindent
\textbf{Replicating BoWness results.}   Using this approach on the datasets discussed previously, \textit{our results confirm prior findings}, with accuracy levels close to chance. Specifically, we observe \textit{0.56 accuracy on CLEVR, 0.51 on PUG:SPAR, and 0.50 on PUG:SPARE}, indicating that CLIP’s performance is virtually at the level of random guessing. These results strongly suggest that CLIP cannot distinguish between correct and permuted attribute-object bindings. CLIP is indeed a bag-of-words model.









\subsection{CLIP binds concepts unimodally} \label{sec:uni-modal-binding}

Prior works evaluating CLIP’s BoW tendencies have relied on assessments that combine both text and image modalities. This approach has a key limitation: it does not separate the encoding of attribute-object binding within each modality from the cross-modal matching step. As a result, it remains unclear whether CLIP’s BoW behavior stems from limitations in the embeddings themselves or from issues in cross-modal alignment. To address this, we examine the \textbf{uni-modal binding}, referring to CLIP’s ability to encode attribute-object relationships independently within each modality. By evaluating text and image embeddings separately, we aim to clarify whether each modality alone captures sufficient binding information, shedding light on the roots of CLIP's BoW behavior.



\begin{figure*}[ht]
  \centering
   \includegraphics[width=\linewidth]{figures/linear_probe.pdf}
   \caption{\textbf{Uni-modal attribute-object binding.} (a) we train a linear probe per object to distinguish its color within image and text modality separately. (b) the linear probe establishes decision boundaries in CLIP’s representation space that differentiate between various attribute-object associations.}
   \label{fig:probing}
\end{figure*}
\noindent
\textbf{Linear probing for uni-modal binding.} To evaluate if CLIP encodes binding information within each modality, we use linear probing \cite{Alain2016}. By training linear classifiers for the information we care about on top of frozen representations, we assess the existence of information we seek in the representation. In this case, we seek the attribute-object binding information in each modality; we train linear classifiers separating attributes for each object present in the image or text inputs.

Given a dataset $\mathcal{D} = \{(\mathbf{x}_{i}^{\text{img}} , \mathbf{x}_{i}^{\text{txt}} )\}_{i=1}^N$, we define $\mathcal{O}$ as the set of all objects and $\mathcal{A}$ as the set of all attributes in the dataset. For each object $o \in \mathcal{O}$, we train two separate classifiers, one for images and one for text as shown in Fig.~\ref{fig:probing}. These classifiers are trained using embeddings extracted by the respective CLIP encoders, which remain frozen throughout the training process. We denote object-specific probes as $\texttt{image-probe}_o$ and $\texttt{text-probe}_o$, each predicting the attribute $a \in \mathcal{A}$ for the object $o$ based on either image or text embeddings:
\begin{nalign}
    \texttt{image-probe}_o & : f_{\text{image}}(\mathbf{x}_{i}^{\text{img}}) \mapsto a \\ 
    \texttt{text-probe}_o & : f_{\text{text}}(\mathbf{x}_{i}^{\text{txt}}) \mapsto a.
\end{nalign} 

For example, as illustrated in Fig.~\ref{fig:probing}(a), given an image or text describing a scene with ``a blue cylinder and red cube'' we extract corresponding embeddings using the CLIP encoders. We then train a linear classifier to recognize the attribute (e.g. $a=$``red'') of a specific object (e.g. $o=$``cube''). This process allows us to isolate and probe attribute recognition for each object individually within each modality.

We measure the amount of attribute-object binding information in each modality by evaluating the validation-set accuracy of the linear probes for the attribute prediction task. We measure the average attribute prediction accuracy across all objects $o\in \mathcal{O}$. It essentially measures the linear separability of the attributes for each object in the CLIP representations for each modality.
To contextualize the information content in pre-trained CLIP representations, we provide accuracies of random baseline and CLIP encoders fine-tuned for the attribute prediction task. They provide the reference points for no binding information and maximal binding information in the encoders, respectively.
See Appendix for further details.





\begin{table}
  \centering
    \small
  \setlength{\tabcolsep}{.8em}
  \begin{tabularx}{\columnwidth}{ll*{5}{c}}
    \toprule
    \multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Image}} & \multicolumn{2}{c}{\textbf{Text}} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6}
    Dataset & Encoder & Train & Test & Train & Test \\ \midrule
     & Random & 0.12 & 0.12 & 0.12 & 0.12 \\
    CLEVR & CLIP & 1.00 & 0.96 & 1.00 & 1.00 \\
     & CLIP (ft) & 1.00 & 0.99 & 1.00 & 1.00 \\ \midrule
     & Random & 0.25 & 0.25 & 0.25 & 0.25 \\
    PUG:SPAR & CLIP & 1.00 & 0.99 & 1.00 & 0.99 \\
     & CLIP (ft) & 1.00 & 0.98 & 1.00 & 1.00 \\ \midrule
     & Random & 0.12 & 0.12 & 0.12 & 0.12 \\
    PUG:SPARE & CLIP & 0.99 & 0.95 & 1.00 & 1.00 \\
     & CLIP (ft) & 1.00 & 1.00 & 1.00 & 1.00 \\ 
    \bottomrule
  \end{tabularx}
  \caption{\textbf{CLIP is not BoW uni-modally.} We measure the amount of attribute-object binding information in image and text representations for CLIP. Accuracies of linear probes classifying attributes for each object of interest are shown. Performances based on random baseline and fine-tuned (ft) CLIP encoders provide reference points for minimal and maximal binding information, respectively. We observe that pre-trained CLIP encoders have sufficient attribute-object binding information in respective modalities.}%
  \label{tab:probing}
  \vspace{-1em}
\end{table}

\noindent
\noindent
\textbf{Results.} Table~\ref{tab:probing} presents the attribute classification accuracies for the linear probes on text and image representations for datasets CLEVR, PUG:SPAR, and PUG:SPARE. The linear probes trained on pre-trained CLIP embeddings achieve accuracies far beyond the random baseline for both image (e.g. 0.96 on image-test set compared to 0.12 for CLEVR) and text (e.g. 1.00 on text-test vs to 0.12 for random) modalities. The accuracies are close to the maximal information bound given by the fine-tuned CLIP encoders. 

\noindent
\textbf{Sanity check: BoW models lack discriminative signal.} As a sanity check, we confirm that a truly BoW representation lacks the structure necessary for attribute-object binding. We train randomly initialized CLIP encoders under a BoW constraint, ensuring they recognize all attributes in an input without the constraint to associate them with objects. This is implemented by training both encoders to predict the presence of each attribute using soft label cross-entropy loss. We perform this for two-object case on CLEVR.

Applying linear probing to the resulting embeddings reveals significantly worse accuracy compared to the original CLIP embeddings (0.66 for images, 0.85 for text vs. 0.96 for CLIP). This confirms that a BoW model does not retain meaningful discriminative signals, illustrating that high-dimensional representations by themselves may fail to encode binding information in the absence of object-specific information.





We conclude that CLIP is already aware of attribute-object bindings in individual modalities.
We hypothesize that CLIP's BoW behavior \cite{Tang2023, Lewis2024} stems from the poor cross-modal alignment that takes place after the encoding stage. We verify the hypothesis in the next section.































\section{Improving cross-modal binding}
\label{sec:cross_modal_binding}

We have observed in the previous section that CLIP is not a bag-of-words (BoW) model uni-modally, suggesting the existence of the necessary attribute-object binding information in individual modalities. We thus narrow down the root cause of the previously observed cross-modal BoWness to a poor cross-modal alignment in the representation space. In this section, we verify that this is indeed the case by proposing a simple alignment strategy that recovers the attribute-object binding information across the modalities. Specifically, we propose to apply a simple linear transformation on the embeddings of one of the modalities (e.g. text) to ensure that the cosine similarities retrieve pairs with correct binding first. We refer to this method as LABCLIP.

\subsection{Linear Attribute Binding CLIP}


Linear Attribute Binding CLIP (LABCLIP) trains a linear transformation to better align the text and image embeddings. Instead of the standard image-text matching in CLIP, 
$$\langle f_{\text{image}}(\mathbf{x}^{\text{img}}), f_{\text{text}}(\mathbf{x}^{\text{txt}}) \rangle,$$
LABCLIP applies a transformation matrix \( \mathbf{A} \in \mathbb{R}^{D \times D} \) on the text embeddings before the inner product computation:
$$\langle f_{\text{image}}(\mathbf{x}^{\text{img}}), \mathbf{A} f_{\text{text}}(\mathbf{x}^{\text{txt}}) \rangle.$$

\noindent
\textbf{Training}. We train the linear transformation matrix $ \mathbf{A}$ contrastively, mirroring CLIP’s original training setup. Two approaches are explored: \textbf{Standard Batch (SB)}, which uses a batch of \( B \) positive image-text pairs; and \textbf{Hard Negative Batch (HNB)}, which includes negative text samples within the batch, following \cite{Yuksekgonul2023}. Negative samples are created by permuting the concepts in the original captions. For example, we transform ``a photo of a red cube and a blue sphere'' into ``a photo of a blue cube and a red sphere'' without changing the image part. Such negative samples can be generated without extra annotation costs. We denote a function performing such permutations as 
$$\text{permute} : \cT \rightarrow \cT.$$
The captions in COCO are not structured in a straightforward ``attribute-object'' format, making it challenging to isolate and swap attributes and objects directly. Because of this, we use the NegCLIP strategy from \cite{Yuksekgonul2023}, which shuffles nouns and adjectives to create negative samples.
In training, including negative text samples results in a \( B \times 2B \) batch, where the transformation is used to minimize the similarity with mismatched attribute-object pairs. The training process of LABCLIP is illustrated in Algorithm \ref{algo:labclip}.

CLIP’s weights remain frozen for LABCLIP. For a reference point, we also provide results with a fine-tuned CLIP model, denoted CLIP-FT. For all the considered methods and their variations, we perform only minimal hyperparameter searches and select models based on the validation set results at the last epoch. Unless stated otherwise, we use the approach with Hard Negative Batch as the default version of LABCLIP. See Appendix for a detailed description of the experimental setup.






\begin{algorithm}
\small 
\caption{\small Training algorithm for Linear Attribute Binding CLIP (LABCLIP)}
\begin{algorithmic}[1]
\State Initialize transformation matrix $\mathbf{A} \in \mathbb{R}^{D \times D}$
\State \textbf{Precompute} $\mathbf{i}_i = f_{\text{image}}(\mathbf{x}_i^{\text{img}})$ and $\mathbf{t}_i = f_{\text{text}}(\mathbf{x}_i^{\text{txt}})$ for all $i$ (CLIP encoders frozen)
\For{epoch $= 1$ to $N_{\text{epochs}}$}
  \For{each batch $\{(\mathbf{i}_i, \mathbf{t}_i)\}_{i=1}^B$}
    \State $\mathbf{t}_{i,\text{pos}} = \mathbf{A} \mathbf{t}_i$
    \State Compute positive scores $s_{i,i} = \langle \mathbf{i}_i, \mathbf{t}_{i,\text{pos}} \rangle$
    
    \If{negative sampling}
      \State Generate negatives: $\mathbf{t}_{j,\text{neg}} = \mathbf{A} \, f_{\text{text}}(\text{permute}(\mathbf{x}_j^{\text{txt}}))$
      \State Compute negative scores $s_{i,j} = \langle \mathbf{i}_i, \mathbf{t}_{j,\text{neg}} \rangle$ for $i \neq j$
      \State \textbf{Effective batch size}: $B \times 2B$
    \EndIf

    \State Compute $\mathcal{L}_{\text{img-to-txt}} = \text{CE}(\{s_{i,i}\}, \{s_{i,j}\})$
    \State Compute $\mathcal{L}_{\text{txt-to-img}} = \text{CE}(\{s_{i,i}\}, \{s_{j,i}\})$
    \State $\mathcal{L} = \mathcal{L}_{\text{img-to-txt}} + \mathcal{L}_{\text{txt-to-img}}$
    
    \State Update $\mathbf{A}$ to minimize $\mathcal{L}$
  \EndFor
\EndFor
\State \textbf{Return} learned transformation matrix $\mathbf{A}$
\end{algorithmic}\label{algo:labclip}
\end{algorithm}









\subsection{Results}

\begin{table}[h]
  \centering
  \small
  \renewcommand{\arraystretch}{1} %
  \begin{tabularx}{\columnwidth}{l *{4}{>{\centering\arraybackslash}X}}
    \toprule
    & \multicolumn{2}{c}{{Accuracy}} & \multicolumn{2}{c}{Recall@1} \\ 
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    & Train & Test & Train & Test \\ 
    \midrule
    \rowcolor{rowgrey} CLEVR & & & & \\ %
    \hspace{1em}- Random & 0.50 & 0.50 & 0.01 & 0.01  \\
    \hspace{1em}- CLIP & 0.49 & 0.58 & 0.25 & 0.36  \\
    \hspace{1em}- LABCLIP-SB & 1.00 & 0.95 & 1.00 & 0.94 \\
    \hspace{1em}- LABCLIP-HNB & 1.00 & 0.95 & 1.00 & 0.93  \\ 
    \hspace{1em}- CLIP-FT & 1.00 & 1.00 & 0.99 & 0.97  \\ 

    \rowcolor{rowgrey} PUG:SPAR & & & & \\ %
    \hspace{1em}- Random & 0.50 & 0.50 & 0.00 & 0.00  \\
    \hspace{1em}- CLIP & 0.52 & 0.53 & 0.08 & 0.09  \\
    \hspace{1em}- LABCLIP-SB & 1.00 & 0.97 & 0.98 & 0.90  \\ 
    \hspace{1em}- LABCLIP-HNB & 1.00 & 0.97 & 0.98 & 0.91 \\ 
    \hspace{1em}- CLIP-FT & 1.00 & 1.00 & 1.00 & 0.99 \\ 
    
    \rowcolor{rowgrey} PUG:SPARE & & & & \\ %
    \hspace{1em}- Random & 0.50 & 0.50 & 0.00 & 0.00  \\
    \hspace{1em}- CLIP & 0.50 & 0.50 & 0.06 & 0.06 \\
    \hspace{1em}- LABCLIP-SB & 0.94 & 0.90 & 0.90 & 0.86 \\ 
    \hspace{1em}- LABCLIP-HNB & 0.98 & 0.94 & 0.95 & 0.90 \\ 
    \hspace{1em}- CLIP-FT & 1.00 & 1.00 & 1.00 & 1.00 \\ 
    \bottomrule
  \end{tabularx}
  \caption{\textbf{LABCLIP recovers cross-modal binding.} We measure CLIP's ability to rank correctly attribute-object relationships higher. Compared to the baseline CLIP, which exhibits close-to-random accuracies and R@1 performances, LABCLIP demonstrates a superior attribute-object binding by employing a single linear transformation on the text embeddings.}
  \label{tab:transform}
\end{table}





Table~\ref{tab:transform} provides the cross-modal binding results on CLEVR, PUG:SPAR, and PUG:SPARE. The first column indicates the accuracy of a model in matching an image to one of two options: the correct caption or the caption with permuted attributes. Recall@1 measures the model's ability to retrieve the correct caption for a given image from all possible captions in the dataset.

Without training, the original CLIP results are at the random chance level, as discussed in Section~\ref{sec:status_quo}: CLIP cannot differentiate between correct and permuted captions cross-modally. Fine-tuning with negative samples enables perfect accuracy and near-perfect retrieval on these datasets, providing an upper bound on the possible attribute-object binding performance.


We observe that both alignment with the standard batch of samples (LABCLIP-SB) and with additional negative samples (LABCLIP-HNB) significantly improve performance compared to CLIP. On CLEVR, for example, both LABCLIP-SB and LABCLIP-HNB achieve an accuracy of 0.95, significantly greater than CLIP's 0.58. The results suggest that better cross-modal binding can be achieved by linearly transforming one of the embedding spaces—text in our case—without requiring extensive computations, complex methodologies, or \textit{any change to CLIP parameters}. This further corroborates our previous findings that all the ingredients and information for attribute-object binding are already present in the pre-trained CLIP models.

    
     

\begin{table*}[htb]
  \centering
  \small
  \begin{tabularx}{\textwidth}{l*{10}{>{\centering\arraybackslash}X}}
    \toprule
    & & \multicolumn{4}{c}{\textbf{ARO}} & \multicolumn{3}{c}{\textbf{SugarCrepe}} & \textbf{COCO} \\ 
    \cmidrule(lr){3-6} \cmidrule(lr){7-9} \cmidrule(lr){10-10}
    \textbf{Model} & \textbf{Backbone} & VG-A & VG-R & Flickr-PRC & COCO-PRC & Add & Replace & Swap & Recall@1 \\ 
    \midrule
    CLIP & ViT-B/32 & 0.63 & 0.63 & 0.60 & 0.48 & 0.73 & 0.80 & 0.62 & 0.30 \\
    NegCLIP \cite{Yuksekgonul2023} & ViT-B/32 & 0.71 & 0.81 & 0.91 & 0.86 & 0.87 & 0.85 & 0.75 & 0.41 \\ 
    LABCLIP-SB & ViT-B/32 & 0.64 & 0.59 & 0.42 & 0.32 & 0.83 & 0.83 & 0.69 & 0.41 \\ 
    LABCLIP-HNB & ViT-B/32 & 0.69 & 0.82 & 0.84 & 0.81 & 0.81 & 0.82 & 0.74 & 0.41 \\ \midrule
    CLIP & ViT-B/16 & 0.62 & 0.56 & 0.58 & 0.50 & 0.73 & 0.80 & 0.62 & 0.33 \\
    LABCLIP-SB & ViT-B/16 & 0.60 & 0.57 & 0.41 & 0.32 & 0.84 & 0.84 & 0.67 & 0.44 \\
    LABCLIP-HNB & ViT-B/16 & 0.60 & 0.71 & 0.87 & 0.84 & 0.83 & 0.84 & 0.73 & 0.44 \\ \midrule
    CLIP & ViT-L/14 & 0.63 & 0.64 & 0.55 & 0.47 & 0.75 & 0.79 & 0.61 & 0.37 \\
    LABCLIP-SB & ViT-L/14 & 0.62 & 0.60 & 0.44 & 0.31 & 0.85 & 0.84 & 0.64 & 0.46 \\
    LABCLIP-HNB & ViT-L/14 & 0.67 & 0.80 & 0.88 & 0.87 & 0.83 & 0.84 & 0.70 & 0.47 \\
    \bottomrule
  \end{tabularx}
  \caption{\textbf{LABCLIP enhances compositional reasoning on real-world benchmarks.} We compare the performance of our method with baseline CLIP and fine-tuned CLIP with negative examples (NegCLIP) on compositional benchmarks ARO and SugarCrepe, as well as retrieval performance on COCO. The simple linear alignment matrix effectively transforms the CLIP representation space, improving attribute-object binding and compositional understanding.}
\label{tab:aro_sugarcrepe_results}
\end{table*}

  





Table~\ref{tab:aro_sugarcrepe_results} demonstrates the performance of our method on the real-world data benchmarks, ARO~\cite{Yuksekgonul2023} and SugarCrepe~\cite{hsieh2024sugarcrepe}, when trained on the COCO dataset \cite{lin2014coco}. Our method significantly outperforms the standard CLIP model, indicating better understanding of attributes, relations, and word order, which leads to improved compositional reasoning.

We compare these results to NegCLIP, a fine-tuned CLIP model with negative image and text samples from COCO \cite{Yuksekgonul2023}. It should be noted that, in contrast to NegCLIP, LABCLIP does not include \textit{hard negative image data} in the batches and only uses one type of perturbation: shuffling of adjectives and nouns. Most importantly, our approach achieves comparable results to NegCLIP without requiring a computationally expensive fine-tuning process.


As discussed in \cite{Yuksekgonul2023}, it has been suggested that CLIP’s design may lack the ability to differentiate between complex scenes and their subtle variations. Its bag-of-words behavior was sufficient for high retrieval performance without strong reasoning. However, our results indicate that CLIP’s representations might already contain information about attribute-object bindings. By training an additional linear layer contrastively, we show that this information can be aligned more effectively. This supports the idea that CLIP’s embeddings capture more compositional details than previously thought.



     


\section{Analysis}

In this section, we analyze the robustness of uni-modal binding with the increasing number of objects, the representational similarities before and after alignment, and the effect of alignment on the modality gap.






\subsection{Uni-modal binding with more objects}
\label{sec:more_objects}

To evaluate the robustness of CLIP’s uni-modal attribute-object binding capabilities, we extend the experiments in Section~\ref{sec:uni-modal-binding} by varying the number of objects in each scene. In this setup, we increase the object count within the CLEVR dataset and measure the attribute probing accuracy of linear classifiers trained to identify object-specific attributes in both image and text representations. The goal is to assess how robust CLIP’s attribute-object binding remains as the number of objects in the scene increases.

 
\begin{figure}[h] 
  \centering
  \vspace{-1em}
   \includegraphics[]{figures/object_ablation2.pdf}
   \vspace{-2em}
   \caption{\textbf{Image and text embeddings effectively encode multiple objects.} We show the average linear probing accuracy on CLEVR as the number of objects increases. While performance slightly decreases, it remains relatively robust.} %
   \label{fig:object_ablation}
\end{figure}


Figure~\ref{fig:object_ablation} shows the attribute probing accuracy as a function of object count for both text and image modalities. The text modality maintains high accuracy, consistently above 0.8 across increasing object counts, indicating stable uni-modal binding for text representations. In contrast, the image modality’s accuracy declines with more objects: training accuracy drops from nearly 1.0 to around 0.75, and test accuracy falls from about 0.9 to 0.6 as the object count increases. These results suggest that while CLIP’s text embeddings capture attribute-object bindings robustly, binding information in image embeddings degrades as scenes become more complex. 




\subsection{Representational similarities and alignment} 


To gain deeper insights into the impact of our alignment transformation, we analyze cosine similarity distributions between positive and negative pairs before and after applying the alignment matrix \( \mathbf{A} \). For a text sequence \( \mathbf{x}^{\text{txt}} \), the aligned text representation is given by \( \mathbf{A} f_{\text{text}}(\mathbf{x}^{\text{txt}}) \), where \( \mathbf{A} \) is the alignment transformation applied to the original CLIP text embedding \( f_{\text{text}}(\mathbf{x}^{\text{txt}}) \). 

\noindent
\textbf{Text-to-text similarity.} First, we consider cosine similarities between positive and negative text representations before alignment, \(\langle f_{\text{text}}(\mathbf{x}_i^{\text{txt}}), f_{\text{text}}(\text{permute}(\mathbf{x}_i^{\text{txt}})) \rangle_{i=1}^N\), and after alignment, \(\langle \mathbf{A} f_{\text{text}}(\mathbf{x}_i^{\text{txt}}), \mathbf{A} f_{\text{text}}(\text{permute}(\mathbf{x}_i^{\text{txt}})) \rangle_{i=1}^N\). 

\begin{figure}
  \centering
   \includegraphics[]{figures/text_similarity.pdf}
   \vspace{-2.3em}
   \caption{\textbf{Alignment reduces the similarity between permuted text pairs.} We show the distributions of cosine similarity between original and permuted text, before and after alignment.}%
   \label{fig:similarity_analysis_t2t}
\end{figure}

The distributions are depicted in Fig.~\ref{fig:similarity_analysis_t2t}. We observe that, before alignment, the similarities are higher, indicating that image embeddings may be incorrectly matched with permuted text embeddings. After alignment, positive and negative text pairs become more dissimilar, potentially making it easier to distinguish between permuted text pairs.






\noindent
\textbf{Image-to-text similarity.} We analyze cross-modal similarities by comparing image embeddings to both positive and negative text embeddings, as shown in Fig.~\ref{fig:similarity_analysis_i2t}. The solid bars represent similarities between image embeddings and text embeddings for positive pairs \( \langle f_{\text{image}}(\mathbf{x}^{\text{img}}_i), f_{\text{text}}(\mathbf{x}^{\text{txt}}_i) \rangle_{i=1}^N \) and negative pairs \( \langle f_{\text{image}}(\mathbf{x}^{\text{img}}_i), f_{\text{text}}(\text{permute}(\mathbf{x}^{\text{txt}}_i)) \rangle_{i=1}^N \) before alignment.

The results indicate no distinction between positive and negative pairs before alignment, as the solid bars for both are at the same height. However, after alignment (dashed bars), the similarity to positive text embeddings \( \langle f_{\text{image}}(\mathbf{x}^{\text{img}}_i), f_{\text{text}}(\mathbf{A} \mathbf{x}^{\text{txt}}_i) \rangle_{i=1}^N \) is notably higher than to permuted text \( \langle f_{\text{image}}(\mathbf{x}^{\text{img}}_i), f_{\text{text}}(\mathbf{A} \text{permute}(\mathbf{x}^{\text{txt}}_i)) \rangle_{i=1}^N \). This demonstrates that alignment enables better differentiation, allowing the model to match images with the correct text rather than the permuted text.


\begin{figure}
  \centering
   \includegraphics[]{figures/image2text_similarity_with_vertical_gaps.pdf}
   \vspace{-2em}
   \caption{\textbf{LABCLIP enhances the similarity between image representation and correct text representations} We show the similarity distributions between image representations to positive and negative text representations before and after alignment.} %
   \label{fig:similarity_analysis_i2t}
\end{figure}


 \subsection{Implications to modality gap}


A key challenge in multimodal models like CLIP is the modality gap—a discrepancy between vision and text embeddings \cite{liang2022mind}. Previous studies \cite{schrodi2024two} suggest that reducing this gap improves alignment and interaction between modalities. Motivated by this, we measure the Euclidean distance between mean embeddings $\mathbf{x}$ and $\mathbf{y}$ from the vision and text representations before and after alignment. Specifically, we define $\mathbf{x} := \frac{1}{N} \sum_{i=1}^N f_{\text{image}}(\mathbf{x}_i)$ and $\mathbf{y} := \frac{1}{N} \sum_{i=1}^N f_{\text{text}}(\mathbf{y}_i)$, where $f_{\text{image}}$ and $f_{\text{text}}$ denote the encoders, and $N$ is the sample size. We then compute $\|\mathbf{x} - \mathbf{y}\|_2$ for original embeddings and $\|\mathbf{x} - \mathbf{A}\mathbf{y}\|_2$ for aligned embeddings, where $\mathbf{A}$ is a contrastively trained alignment matrix using LABCLIP.

\begin{figure}[ht]
  \centering
      \includegraphics[]{figures/modality_gap_final.pdf}
  \vspace{-0.8em}
  \caption{\textbf{Modality gap decreases after alignment.} Left: Modality gap between image and text representations before and after alignment. Right: UMAP visualization of COCO test set representations with text representations before and after alignment.}
  \label{fig:modality_gap}
\end{figure}

Our experiments reveal that the modality gap decreases substantially across COCO, PUG:SPAR, and PUG:SPARE datasets after alignment, while CLEVR shows a slight increase (Fig. \ref{fig:modality_gap}, left). Additionally, we provide a qualitative illustration of the modality gap (Fig. \ref{fig:modality_gap}, right) using UMAP \cite{mcinnes2018umap} showing COCO test set representations before and after alignment. In this visualization, the aligned text representations (green) move closer to the image representations (blue), indicating reduced modality gap after alignment. 

This suggests that alignment effectiveness may vary across datasets, with our approach successfully enhancing cross-modal compatibility in most cases.











\section{Conclusion}
In this study, we investigated the underlying reasons for CLIP's bag-of-words behavior. We focused specifically on attribute-object binding. Our findings demonstrate that a substantial amount of attribute-object binding information is already present in CLIP’s text and image embeddings. The issue, instead, lies in the cross-modal alignment that relies on cosine similarity. To address this, we introduced LABCLIP, a method that applies a linear transformation to text embeddings. LABCLIP significantly improves attribute-object binding during cross-modal matching, enhancing compositional understanding without requiring modifications to the CLIP encoders. Our work motivates further exploration into contrastive training objectives, alignment processes, and the strategic use of hard negatives to enable CLIP to capture compositional relationships more robustly.


\section{Acknowledgments}

The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Arnas Uselis. This work was supported by the Tübingen AI Center.

