\clearpage
\setcounter{page}{1}
\maketitlesupplementary

























\section{Datasets}


\label{sec:datasets_extra}




In this section, we provide details for the datasets introduced in Section~\ref{sec:datasets}. A summary of the key dataset characteristics for the CLEVR, PUG:SPAR, and PUG:SPARE is presented in Table~\ref{tab:dataset_summary}.
\begin{table*}[]
\centering
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccccc}
\hline
 & \textbf{CLEVR} & \textbf{PUG:SPAR} & \textbf{PUG:SPARE} \\ 
\hline
\#images & 5000 & 19840 & 88704 \\
\#attribute-objects combinations & 192 & 1984 & 3696 \\
\#objects & 3 & 32 & 12 \\
\#attributes & 8 & 4 & 8 \\
\#backgrounds & 1 & 10 & 4 \\
positions & random & \{left, right\} & \{left, right\} $\times$ \{front, back, equal\} \\
\hline
\end{tabular*}
\caption{\textbf{Specifications for the datasets used to test attribute-object binding in a controlled setting.} For CLEVR, the number of attribute-object combinations only reflects the two-object case. For PUG:SPAR, the numbers represent the filtered dataset used in our experiments.}
\label{tab:dataset_summary}
\end{table*}

\subsection{CLEVR}

\begin{figure*}[]
  \centering
   \includegraphics[width=\textwidth]{figures/clevr_generation.pdf}
   \caption{\textbf{CLEVR dataset generation process.} From a set of objects and attributes, we randomly generated combinations of $M$ objects per scene. Each combination was rendered with Blender, and a caption was generated by concatenating attributes and objects to match the image.}
   \label{fig:clevr_generation}
\end{figure*}

\begin{figure*}[ht]
  \centering
   \includegraphics[width=\textwidth]{figures/CLEVR_example_c.pdf}
   \caption{\textbf{Scene complexity increases with more objects.} We show examples from the CLEVR dataset with scenes containing different numbers of objects to highlight how image and text complexity changes with object count.}
   \label{fig:clevr_example}
\end{figure*}

\textbf{Generation}. Following the CLEVR dataset introduced in \cite{Johnson2017}, we generate new images using the 3D modeling software Blender~\cite{blender}. The dataset contains images with $M$ colored objects and corresponding captions. The set of objects is $\mathcal{O} = \{\text{cube}, \text{sphere}, \text{cylinder}\}$, and the attributes are selected from the set of eight colors: $\mathcal{A} = \{\text{blue}, \text{red}, \text{purple}, \text{cyan}, \text{gray}, \text{brown}, \text{green}, \text{yellow}\}$.

The data generation process is shown in Fig.~\ref{fig:clevr_generation}. We randomly sample objects and attributes to create combinations of $M$ objects of various attributes. For example, an attribute-object combination of 5 objects is 2 blue cubes, 1 green sphere, 1 purple sphere, and 1 blue cylinder. For scenes with two objects, we enforce that the objects are distinct. This results in 192 unique combinations of two objects: 3 choices for the first object, 2 for the second, 8 color options per object (colors can repeat), divided by 2 to ignore object order: $3 \times 2 \times 8 \times 8 \times 0.5 = 192$


Using the original CLEVR image generation pipeline, we construct a scene in Blender based on the sampled combinations. Objects are placed randomly on a neutral background with variations in material and size. The rendered images have dimensions of $320 \times 240$. For captions, we concatenate the attributes and objects in the format: ``$a_1 \ o_1 \ \textit{and} \ a_2 \ o_2 \ \textit{and ... and} \ a_M \ o_M$" where $a_j \in \mathcal{A}$ and $o_j \in \mathcal{O}$ for $j \in {1, ..., M}$. 

We generate $N=5000$ samples for each $M$-object configuration. For the experiments in Sections~\ref{sec:uni-modal-binding} and \ref{sec:cross_modal_binding}, we use $M = 2$. For testing uni-modal binding with an increasing number of objects in Section \ref{sec:more_objects}, we consider $M$ ranging from 2 to 10. Sample images and captions are shown in Fig.~\ref{fig:clevr_example}.


\noindent
\textbf{Train/test split}. For each $M$-object setting, we divide the dataset into training, validation, and test sets in a 90/10/10 ratio based on attribute-object combinations. For instance, if there are 192 attribute-object combinations for the two-object setting, 19 combinations are assigned to the validation set, 19 to the test set, and the remaining combinations to the training set. This ensures that the same combination does not appear in both the training and test sets.

\subsection{PUG:SPAR}

\textbf{Description}. PUG:SPAR is a synthetically generated dataset in Unreal Engine~\cite{Bordes2024}, featuring animal figures on various backgrounds. The animals can have their natural colors or attributes such as red, blue, grass, and stone. For our project, which tests attribute-object relationships in a controlled environment, we filter the dataset to include scenes with two animals and annotated attributes. This leaves us with images of two objects either in a blue/red or grass/stone attribute setting, with one animal on the left and the other on the right. However, the attributes are fixed to positions: the left objects are always blue or grass, and the right objects are always red or stone. This results in \( 32 \times 31 \times 2 = 1984 \) attribute-object combinations and a total of $19840$ images.

As discussed in Section~\ref{sec:datasets}, the fixed relationship between attributes and positions could lead to a shortcut strategy for the linear classifier: first identifying the blue/red or grass/stone setting, and then determining if the target object is on the left or right. To address this, we create PUG:SPARE, a dataset with an extended set of attributes that are independent of object positions.

\noindent
\textbf{Train/test split}. Similar to the CLEVR dataset, we split the dataset into training, validations, and test sets based on attribute-object combinations in a 90/10/10 ratio.


\subsection{PUG:SPARE}
\label{sec:pugspare}

\textbf{Generation}. Similar to PUG:SPAR, we generate photorealistic images of two animals on different backgrounds. Our dataset includes 12 possible animal objects and 8 possible colors for these animals. The objects appear in 4 different environments, creating varied backgrounds and lighting conditions. The relative positions between the two objects also change: the left object in front and the right object in the back, the left object in the back and the right object in front, and both objects at the same distance. Animals and attributes do not repeat. For example, ``red zebra and blue lion" is valid. However, ``red zebra and red lion" or ``red zebra and blue zebra" are not. We generate all possible two-object combinations with these conditions. This results in $12$ choices for the first object, $11$ choices for the second object, $8$ colors for the first object, $7$ colors for the second object, divided by $2$ to ignore the order: $12\times11\times8\times7\times0.5=3696$ combinations.

The images for these combinations are rendered in Unreal Engine. The image dimensions are $512 \times 512$. Captions follow the form "$a_1 \ o_1 \ \textit{and} \ a_2 \ o_2$", where $a_j \in \mathcal{A}$ and $o_j \in \mathcal{O}$. The examples are shown in Fig.~\ref{fig:pug_spare_example}.
\begin{figure*}[]
  \centering
   \includegraphics[width=\textwidth]{figures/pug_spare.pdf}
   \caption{\textbf{PUG:SPARE dataset examples.} PUG:SPARE offers all possible configurations of two objects from the set of 12 objects, 8 attributes, 4 backgrounds, and 3 position configurations (front/back, back/front, at the same distance). This allows comprehensive testing of attribute-object binding.}
   \label{fig:pug_spare_example}
\end{figure*}

\noindent
\textbf{Train/test split}. Similar to the CLEVR dataset, we divide the dataset into train, validation, and test splits based on the attribute-object combinations.

\subsection{COCO, ARO, SugarCrepe}

We evaluate the cross-modal binding performance of our method on real-world datasets by training on COCO~\cite{lin2014coco}. Following the protocol in \cite{Yuksekgonul2023}, we apply the Karpathy splits to divide the dataset into train, validation, and test sets.

The trained models are then assessed on compositional benchmarks, ARO~\cite{Yuksekgonul2023} and SugarCrepe~\cite{hsieh2024sugarcrepe}. Examples from these benchmarks are shown in Fig.~\ref{fig:aro_sg_examples}.
\begin{figure*}[]
  \centering
   \includegraphics[]{figures/aro_sg_examples_c.pdf}
   \caption{\textbf{Examples from the compositional benchmarks ARO and SugarCrepe.} These datasets are designed to test VLMs' ability to accurately bind attributes and objects in complex, real-world scenarios. The images illustrate varied compositions of objects, attributes, and their relationships, challenging a model's compositional understanding.}
   \label{fig:aro_sg_examples}
\end{figure*}

\section{Discussion on establishing BoWness}

Previous studies evaluating CLIP's Bag-of-Words (BoW) behavior have typically combined both text and image modalities, following the standard zero-shot learning approach. These assessments generally fall into two categories. The first approach, as demonstrated in \cite{Lewis2024, Tang2023}, involves comparing images to captions that describe only a single object in the scene. For example, given an image with a yellow sphere and a red cube, the text prompts might be: `a photo of \{yellow sphere, yellow cube, red sphere, blue cube, purple cylinder\}' (Fig.~\ref{fig:bow_ex}(a)). We argue that focusing on single-object descriptions does not fully capture CLIP’s capability for compositional reasoning in multi-object contexts. Since CLIP is trained to match an image to the caption that best aligns with its content, `yellow cube' would be a better match than `yellow sphere' in this example, as it provides a more accurate representation of the concepts in the scene. Limiting the evaluation to single-object descriptions, therefore, does not fairly test its ability to understand attribute-object associations.

The second approach, seen in studies like \cite{Yuksekgonul2023}, provides a more robust evaluation by comparing a query image to permutations of a more complete description, effectively testing BoWness. We adopt this methodology for our experiments in Section~\ref{sec:status_quo}. Specifically, we measure CLIP's accuracy in choosing between correct and permuted captions. For CLEVR, if the correct caption is `yellow sphere and red cube', we compare it to its permutation, `red sphere and yellow cube'(Fig.~\ref{fig:bow_ex}(b)). Similarly, for the PUG:SPAR and PUG:SPARE datasets, we test the model's ability to distinguish between captions like `blue elephant and red lion' versus `red elephant and blue lion' (Fig.~\ref{fig:bow_ex}(c),(d)).
\begin{figure}[]
  \centering
   \includegraphics[]{figures/bow_ex.pdf}
   \caption{\textbf{Illustrating BoWness.} The figure presents examples from CLEVR, PUG:SPAR, and PUG:SPARE datasets, used to demonstrate BoWness. The highlighted example in red shows a case where an image is compared to captions describing only a single object, which can lead to inaccurate assessments due to incomplete scene information.}
   \label{fig:bow_ex}
\end{figure}

\section{Details for uni-modal binding}
\label{sec:probing_details}

In this section, we provide details about the experiments conducted in Section~\ref{sec:uni-modal-binding}. 

Since each linear probe is object-specific, we filter the dataset to include only examples containing the target object. These examples are split into training, validation, and test sets with a 90/10/10 ratio based on the attribute-object combinations that include the target object.

We use OpenAI's CLIP model for all experiments \cite{radford2021learning}. The main results presented in Table~\ref{tab:probing} are based on the ViT-L/14 model, while additional results with the ViT-B/32 and ViT-B/16 models are shown in Table~\ref{tab:probing_extra}. During linear probing, the model weights are frozen. For upper-bound results derived with fine-tuning, we unfreeze the relevant image or text encoder weights and allow them to update during training. All models are trained on a single Nvidia A100 GPU.

We do not normalize the image or text embeddings before passing them to the linear classifiers. The linear classifiers are implemented in PyTorch using cross-entropy loss. Accuracy is measured by predicting attributes and averaging the results across all object-specific classifiers. In the two-object case, each object has only one possible color, and the task is to predict a single color per object. In the multi-object case, where multiple instances of the target object may appear, a prediction is considered correct only if all colors for the given target object are accurate.

We use both manual and random searches for hyperparameter tuning. A batch size of 32 consistently performs well across all datasets. For CLEVR, linear probing without fine-tuning requires learning rates of \{0.1, 0.01, 0.001\} and training for 1000 to 5000 epochs for images or 200 to 1000 epochs for text. For fine-tuning, we reduce the number of epochs to a range of 5 to 20. For the PUG datasets, we maintain a batch size of 32, with a learning rate of 0.1, and train for 50 to 200 epochs when not fine-tuning. For fine-tuning, the learning rates range between 0.001 and 0.01, with 5 to 50 epochs. Both training regimes utilize the SGD optimizer. The optimal configurations are selected based on validation set accuracy.



\begin{table}
  \centering
    \small
  \setlength{\tabcolsep}{.8em}
  \begin{tabularx}{\columnwidth}{ll*{5}{c}}
    \toprule
    \multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Image}} & \multicolumn{2}{c}{\textbf{Text}} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6}
    Dataset & Encoder & Train & Test & Train & Test \\ \midrule
    CLEVR & ViT-B/32 & 1.0 & 0.91 & 1.0 & 0.99 \\
         & ViT-B/16 & 1.0 & 0.90 & 1.0 & 0.99 \\ \midrule
    PUG:SPAR & ViT-B/32 & 1.0 & 0.97 & 1.0 & 0.98 \\
         & ViT-B/16 & 1.0 & 0.98 & 1.0 & 0.99 \\ \midrule
    PUG:SPARE & ViT-B/32 & 0.96 & 0.90 & 1.0 & 0.99 \\
         & ViT-B/16 & 0.96 & 0.92 & 1.0 & 0.99 \\
    \bottomrule
  \end{tabularx}
  \caption{\textbf{CLIP is not BoW uni-modally.} We evaluate the attribute-object binding information in CLIP's image and text embeddings. The table shows the accuracies of linear probes classifying attributes for each target object, averaged across all objects. The table extends Table~\ref{tab:probing} with results for ViT-B/32 and ViT-B/16 backbones. Pre-trained CLIP encoders contain sufficient binding information within each modality.}
  \label{tab:probing_extra}
  \vspace{-1em}
\end{table}

\noindent
\textbf{Details on training a BoW model.}
We simulate a BoW model by training CLIP encoders to recognize all attributes in the input while ignoring binding to objects. 

We attach a linear layer to CLIP’s image or text encoder that maps to attribute classes, similar to linear probes. We then reinitialize the CLIP encoders randomly and train the model to predict all attributes in the input with soft label cross-entropy loss. The soft labels correspond to the normalized count of attributes in the input. This ensures that the model behaves as a BoW because it is tasked to predict attributes without having to link them to specific objects.
We use these newly trained CLIP embeddings and apply linear probing to evaluate the presence of attribute-object binding information.

As explained in the main text, such a BoW model does not achieve high linear probing accuracy. On CLEVR, the average test accuracies of the linear probes are {0.66} for images and {0.85} for text, significantly worse than the probing performance on the actual CLIP embeddings (0.96). This reinforces the idea that BoW models do not contain features that are useful for binding.




\section{Cross-modal binding details}
\label{sec:labclip_details}
\begin{table}[h]
  \centering
  \small
  \renewcommand{\arraystretch}{1} %
  \begin{tabularx}{\columnwidth}{l l *{4}{>{\centering\arraybackslash}X}}
    \toprule
    \textbf{Model} & \textbf{Backbone} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multicolumn{2}{c}{\textbf{Recall@1}} \\ 
    \cmidrule(lr){3-4} \cmidrule(lr){5-6}
    & & Train & Test & Train & Test \\ 
    \midrule
    \rowcolor{rowgrey} CLEVR & & & & & \\ \midrule%
    CLIP & ViT-B/32 & 0.52 & 0.49 & 0.33 & 0.34 \\
    LABCLIP-SB & ViT-B/32 & 0.99 & 0.85 & 0.99 & 0.83 \\
    LABCLIP-HNB & ViT-B/32 & 0.99 & 0.83 & 0.99 & 0.81 \\ \midrule
    CLIP & ViT-B/16 & 0.50 & 0.54 & 0.31 & 0.38 \\
    LABCLIP-SB & ViT-B/16 & 1.00 & 0.93 & 1.00 & 0.92 \\
    LABCLIP-HNB & ViT-B/16 & 1.00 & 0.93 & 1.00 & 0.92 \\ \midrule
    
    \rowcolor{rowgrey} PUG:SPAR & & & & & \\ \midrule%
    CLIP & ViT-B/32 & 0.51 & 0.51 & 0.02 & 0.02 \\
    LABCLIP-SB & ViT-B/32 & 0.99 & 0.97 & 0.93 & 0.83 \\ 
    LABCLIP-HNB & ViT-B/32 & 0.99 & 0.98 & 0.93 & 0.84 \\ \midrule
    CLIP & ViT-B/16 & 0.52 & 0.53 & 0.04 & 0.04 \\
    LABCLIP-SB & ViT-B/16 & 0.99 & 0.97 & 0.94 & 0.88 \\ 
    LABCLIP-HNB & ViT-B/16 & 1.00 & 0.98 & 0.95 & 0.89 \\ \midrule
    
    \rowcolor{rowgrey} PUG:SPARE & & & & & \\ \midrule%
    CLIP & ViT-B/32 & 0.51 & 0.51 & 0.01 & 0.01 \\
    LABCLIP-SB & ViT-B/32 & 0.91 & 0.89 & 0.73 & 0.69 \\ 
    LABCLIP-HNB & ViT-B/32 & 0.95 & 0.93 & 0.77 & 0.73 \\ \midrule
    CLIP & ViT-B/16 & 0.51 & 0.50 & 0.03 & 0.03 \\
    LABCLIP-SB & ViT-B/16 & 0.91 & 0.87 & 0.84 & 0.80 \\ 
    LABCLIP-HNB & ViT-B/16 & 0.96 & 0.92 & 0.88 & 0.84 \\ 
    \bottomrule
  \end{tabularx}
  \caption{\textbf{LABCLIP enhances cross-modal binding.} These results extend the findings shown in Table~\ref{tab:transform} for ViT-B/32 and ViT-B/16 backbones.}
  \label{tab:transform_extra}
\end{table}

In this section, we provide additional details about LABCLIP, discussed in Section~\ref{sec:cross_modal_binding}. 

We use OpenAI's CLIP for all experiments, specifically the L/14 model for CLEVR, PUG:SPAR, and PUG:SPARE, as shown in Table~\ref{tab:transform}. Additional results for the B/32 and B/16 models are provided in Table~\ref{tab:transform_extra}. All models were trained using a single Nvidia A100 GPU.

The LABCLIP method involves adding an additional linear layer to the text encoder while keeping the CLIP weights frozen. For the upper bound results with fine-tuned CLIP, we unfreeze both encoder weights. The additional linear layer has the same dimension as the network output, equivalent to multiplying by a $D \times D$ matrix. We initialize this matrix as an identity matrix, which corresponds to the case when no transformation is applied. 

We use the same contrastive loss as in the original CLIP. The temperature parameter in the contrastive loss is a learnable parameter, initialized to 0. The \textbf{Standard Batch} contains only the corresponding images and text in each batch, while the \textbf{Hard Negative Batch} also includes hard negative captions. For synthetic datasets, we obtain hard negatives by swapping attributes. For COCO, we utilize the attribute and noun shuffling method introduced in NegCLIP to create negative captions \cite{Yuksekgonul2023}. For example, in COCO-PRC example shown in Fig.~\ref{fig:aro_sg_examples}, when the correct caption is `a man with a red helmet on a small moped on a dirt road', a possible negative caption is `a dirt with a small road on a red moped on a helmet man'. There are no negative images in our approach.

We employ a combination of manual and random searches for hyperparameter tuning, with the batch size ranging from 32 to 2048, epochs from 5 to 50, and learning rates between 0.0001 and 0.01. We use the Adam optimizer for LABCLIP but SGD when fine-tuning on the CLEVR and PUG datasets. The optimal hyperparameters are selected based on the final epoch performance on the validation set.
