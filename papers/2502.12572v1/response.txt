\section{Related Works}
\subsection{Singing Voice Synthesis}

Singing Voice Synthesis (SVS) has advanced significantly with deep learning, aiming to generate high-quality singing from musical scores and lyrics. Early models like **Zhang, "XiaoiceSing"** and **Chen et al., "DeepSinger"** utilize non-autoregressive and feed-forward transformers to synthesize singing voice. VISinger  employs the VITS **Kim et al., "VITS"** architecture for end-to-end SVS. GANs have also been used for high-fidelity voice synthesis **Ledig et al., "Generative Adversarial Networks in Neural Style Transfer and Image Synthesis"**, and DiffSinger **Hoogeboom et al., "DiffSinger: Unsupervised Singing Voice Synthesis with a Diffusion Model"** introduces diffusion for improved mel-spectrogram generation.
Despite these advancements, precise control over singing techniques remains a challenge, which is essential for enhancing artistic expressiveness. Controllable SVS focuses on managing aspects like timbre, emotion, style, and techniques. Existing works often target specific controls, such as Muse-SVS **Zhang et al., "Muse-SVS: Multimodal Singing Voice Synthesis with Emotion Control"** for pitch and emotion, StyleSinger **Pang et al., "StyleSinger: Style Transfer for Singing Voice Synthesis"** and TCSinger **Chen et al., "TCSinger: Task-Conditioned Singing Voice Synthesis with Conditional Flow"** for style transfer, and models for vibrato control **. However, we advance technique controllable SVS by enabling control over seven techniques across five languages.

\subsection{Prompt-guided Voice Generation}
In terms of voice generation, previous controls rely on texts, scores, and feature labels. Prompt-based control is emerging as a simpler, more intuitive alternative and has achieved great success in text, image, and audio generation tasks **Radford et al., "Learning to Generate Long-term Future Predictions via Temporal Consistency Adversarial Networks"**.
In speech generation, PromptTTS  and InstructTTS  use text descriptions to guide synthesis, offering precise control over style and content. 
In singing voice generation, Prompt-Singer  uses natural language prompts to control attributes like the singer's gender and volume but lacks advanced technique control. This paper addresses this gap by integrating multiple techniques into prompt-based control, allowing for more sophisticated and expressive singing voice generation.

\subsection{Flow Matching Generative Models}

Flow matching  is an advanced generative modeling technique that optimizes the mapping between noise distributions and data samples by ensuring a smooth transport path, reducing sampling complexity. It has significantly improved audio generation tasks.
Voicebox  uses flow matching for high-quality text-to-speech synthesis, noise removal, and content editing.
Audiobox  leverages flow matching to enhance multi-modal audio generation with better controllability and efficiency. 
Matcha-TTS  applies optimal-transport conditional flow matching for high-quality, fast, and memory-efficient text-to-speech synthesis. 
VoiceFlow  utilizes rectified flow matching to generate superior mel-spectrograms with fewer steps.
Inspired by these successes, we use flow matching for controllable singing voice synthesis to boost quality and efficiency.