[
  {
    "index": 0,
    "papers": [
      {
        "key": "lu2020xiaoicesing",
        "author": "Lu, Peiling and Wu, Jie and Luan, Jian and Tan, Xu and Zhou, Li",
        "title": "Xiaoicesing: A high-quality and integrated singing voice synthesis system"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ren2020deepsinger",
        "author": "Ren, Yi and Tan, Xu and Qin, Tao and Luan, Jian and Zhao, Zhou and Liu, Tie-Yan",
        "title": "Deepsinger: Singing voice synthesis with data mined from the web"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhang2022visinger",
        "author": "Zhang, Yongmao and Cong, Jian and Xue, Heyang and Xie, Lei and Zhu, Pengcheng and Bi, Mengxiao",
        "title": "Visinger: Variational inference with adversarial learning for end-to-end singing voice synthesis"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kim2021conditional",
        "author": "Kim, Jaehyeon and Kong, Jungil and Son, Juhee",
        "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wu2020adversarially",
        "author": "Wu, Jie and Luan, Jian",
        "title": "Adversarially trained multi-singer sequence-to-sequence singing synthesizer"
      },
      {
        "key": "huang2022singgan",
        "author": "Huang, Rongjie and Cui, Chenye and Chen, Feiyang and Ren, Yi and Liu, Jinglin and Zhao, Zhou and Huai, Baoxing and Wang, Zhefeng",
        "title": "Singgan: Generative adversarial network for high-fidelity singing voice generation"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2022diffsinger",
        "author": "Liu, Jinglin and Li, Chengxi and Ren, Yi and Chen, Feiyang and Zhao, Zhou",
        "title": "Diffsinger: Singing voice synthesis via shallow diffusion mechanism"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kim2023muse",
        "author": "Kim, Sungjae and Kim, Yewon and Jun, Jewoo and Kim, Injung",
        "title": "MuSE-SVS: Multi-Singer Emotional Singing Voice Synthesizer that Controls Emotional Intensity"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2024stylesinger",
        "author": "Zhang, Yu and Huang, Rongjie and Li, Ruiqi and He, JinZheng and Xia, Yan and Chen, Feiyang and Duan, Xinyu and Huai, Baoxing and Zhao, Zhou",
        "title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2024tcsinger",
        "author": "Zhang, Yu and Jiang, Ziyue and Li, Ruiqi and Pan, Changhao and He, Jinzheng and Huang, Rongjie and Wang, Chuxin and Zhao, Zhou",
        "title": "TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2021vibrato",
        "author": "Liu, Ruolan and Wen, Xue and Lu, Chunhui and Song, Liming and Sung, June Sig",
        "title": "Vibrato learning in multi-singer singing voice synthesis"
      },
      {
        "key": "song2022singing",
        "author": "Song, Yingjie and Song, Wei and Zhang, Wei and Zhang, Zhengchen and Zeng, Dan and Liu, Zhi and Yu, Yang",
        "title": "Singing voice synthesis with vibrato modeling and latent energy representation"
      },
      {
        "key": "ikemiya2014transferring",
        "author": "Ikemiya, Yukara and Itoyama, Katsutoshi and Okuno, Hiroshi G",
        "title": "Transferring vocal expression of f0 contour using singing voice synthesizer"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "ramesh2021zero",
        "author": "Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya",
        "title": "Zero-shot text-to-image generation"
      },
      {
        "key": "kreuk2022audiogen",
        "author": "Kreuk, Felix and Synnaeve, Gabriel and Polyak, Adam and Singer, Uriel and D{\\'e}fossez, Alexandre and Copet, Jade and Parikh, Devi and Taigman, Yaniv and Adi, Yossi",
        "title": "Audiogen: Textually guided audio generation"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "guo2023prompttts",
        "author": "Guo, Zhifang and Leng, Yichong and Wu, Yihan and Zhao, Sheng and Tan, Xu",
        "title": "PromptTTS: Controllable text-to-speech with text descriptions"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "yang2023instructtts",
        "author": "Yang, Dongchao and Liu, Songxiang and Huang, Rongjie and Lei, Guangzhi and Weng, Chao and Meng, Helen and Yu, Dong",
        "title": "Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wang2024prompt",
        "author": "Wang, Yongqi and Hu, Ruofan and Huang, Rongjie and Hong, Zhiqing and Li, Ruiqi and Liu, Wenrui and You, Fuming and Jin, Tao and Zhao, Zhou",
        "title": "Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "lipman2022flow",
        "author": "Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matthew",
        "title": "Flow Matching for Generative Modeling"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "le2024voicebox",
        "author": "Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and others",
        "title": "Voicebox: Text-guided multilingual universal speech generation at scale"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "vyas2023audiobox",
        "author": "Vyas, Apoorv and Shi, Bowen and Le, Matthew and Tjandra, Andros and Wu, Yi-Chiao and Guo, Baishan and Zhang, Jiemin and Zhang, Xinyue and Adkins, Robert and Ngan, William and others",
        "title": "Audiobox: Unified audio generation with natural language prompts"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "mehta2024matcha",
        "author": "Mehta, Shivam and Tu, Ruibo and Beskow, Jonas and Sz{\\'e}kely, {\\'E}va and Henter, Gustav Eje",
        "title": "Matcha-TTS: A fast TTS architecture with conditional flow matching"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "guo2024voiceflow",
        "author": "Guo, Yiwei and Du, Chenpeng and Ma, Ziyang and Chen, Xie and Yu, Kai",
        "title": "VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching"
      }
    ]
  }
]