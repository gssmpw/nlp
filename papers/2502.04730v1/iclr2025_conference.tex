\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{aligned-overset}
\usepackage{tikz}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{subfigure}
\usepackage{bbm}
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{caption}
\usetikzlibrary{positioning, bayesnet}
\usepackage[ruled]{algorithm2e}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{lipsum} 

% \usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\newcommand*\rot{\rotatebox{90}}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[]{hyperref}       % hyperlinks
\hypersetup{colorlinks=false}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}
\makeatother
 
\newreptheorem{theorem}{Theorem}
\newreptheorem{lemma}{Lemma}


\title{PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational Autoencoders}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{
Tianyu Xie$^{1}$, Harry Richman$^{3}$, Jiansi Gao$^{3}$, Frederick A. Matsen IV$^{3,4}$, Cheng Zhang$^{1,2,}$\thanks{Corresponding author.}\\
$^{1}$ School of Mathematical Sciences, Peking University\\
$^{2}$ Center for Statistical Science, Peking University\\
$^{3}$ Computational Biology Program, Fred Hutchinson Cancer Research Center\\
$^{4}$ Howard Hughes Medical Institute \\
\texttt{tianyuxie@pku.edu.cn,}
\texttt{\{hrichman,jsigao,matsen\}@fredhutch.org,}\\
\texttt{chengzhang@math.pku.edu.cn}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships.
Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and may lack sufficient resolution. 
In this paper, we introduce \emph{phylogenetic variational autoencoders} (PhyloVAEs), an unsupervised learning framework designed for representation learning and generative modeling of tree topologies.
Leveraging an efficient encoding mechanism inspired by autoregressive tree topology generation, we develop a deep latent-variable generative model that facilitates fast, parallelized topology generation.
PhyloVAE combines this generative model with a collaborative inference model based on learnable topological features, allowing for high-resolution representations of phylogenetic tree samples.
Extensive experiments demonstrate PhyloVAE's robust representation learning capabilities and fast generation of phylogenetic tree topologies.
\end{abstract}

\section{Introduction}\label{sec:introduction}
Phylogenetic trees are the foundational structure for describing the evolutionary processes among individuals or groups of biological entities.
Reconstructing these trees based on collected biological sequences (e.g., DNA, RNA, protein) from observed species, also known as phylogenetic inference \citep{felsenstein2004inferring}, is an essential discipline of computational biology~\citep{fitch1971parsimony, Felsenstein81, yang1997bayesian,ronquist2012mrbayes}.

Large collections of trees obtained from these approaches (e.g., posterior samples from MCMC runs \citep{ronquist2012mrbayes}), however,
are often difficult to summarize or visualize due to the discrete and non-Euclidean nature of the tree topology space\footnote{In phylogenetic terminology, a \emph{tree topology} is just the discrete graph-theoretic component of the tree without additional information such as edge lengths.}.
Yet, the importance of being able to do so in practice has spurred substantial research in this direction.
The classical approach to visualize and analyze distributions of phylogenetic trees is to calculate pairwise distances between the trees and project them into a plane using multidimensional scaling (MDS) \citep{amenta2002,hillis2005analysis,Jombart2017-qw}.
However, these approaches have the shortcoming that one can not map an arbitrary point in the visualization to a tree, and therefore do not form an actual visualization of the relevant tree space.
Furthermore, their effectiveness heavily depends on the choice of distance metric and can sometimes exhibit counterintuitive behaviors \citep{kuhner2015practical}, and the visualizations can suffer from poor resolution, where distinct sets of trees overlap within the same regions \citep{hillis2005analysis,Whidden2014QuantifyingME} (see Figure \ref{fig:real-phylogeny} in Section \ref{sec:real-phylogeny} for an example).
Recently, several vector representation methods have been developed for tree topologies \citep{liu2021, penn2023phylo2vec}.
However, these representations only provide raw features whose dimension scales to the tree topology size and hence may fail to deliver concise and useful information of tree topologies.
In general, finding good representations of tree topologies that preserve essential information for downstream tasks (e.g., comparison, visualization, clustering) remains challenging.

On the other end of the spectrum lie much more recent methods to perform density estimation on sets of phylogenetic trees \citep{Larget2013-et,Zhang2018SBN,xie2024artree}.
These methods are very high resolution, as can be seen by the excellent fit that they offer to empirical distributions of phylogenetic trees.
However, these methods do not lend themselves to representation learning and visualization, and it is difficult to understand what they are telling us about the structure of the phylogenetic tree shape distribution.


In this paper, we introduce \emph{phylogenetic variational autoencoders} (PhyloVAEs), which is an unsupervised learning framework that for the first time allows both representation learning and generative modeling of phylogenetic tree topologies in a satisfying and useful way. 
Inspired by the tree topology generating process outlined in ARTree \citep{xie2024artree}, we first encode a tree topology into an integer vector representing the corresponding edge decisions, through a linear-time algorithm.
Based on this encoding mechanism, we develop a deep latent-variable generative model for tree topologies, together with an inference model for the posterior distribution of the latent variable given the tree topology using learnable topological features \citep{Zhang2023VBPIGNN}.
In this way, PhyloVAE provides a latent space representation that can be easily visualized like the previous MDS method, but at the same time, it is a probabilistic model that gives a high-resolution representation of the tree topology distribution.
Although the main purpose of PhyloVAE is representation learning, we want to emphasize that it is the generative modeling objective that forces PhyloVAE to learn high-resolution representations to retain more distributional information.
We summarize our main contributions as follows:
\vspace{-0.2cm}
\begin{itemize}[leftmargin=0.5cm]
\setlength\itemsep{0.0cm}
\item We propose the first representation learning framework for phylogenetic tree topologies with deep models, which has more capacity to distinguish different shapes of tree topologies compared to traditional distance-based methods (see Section \ref{sec:real-phylogeny} for an example). Moreover, the generative nature of PhyloVAE also allows us to map an arbitrary point in the latent space to a tree topology (Figure \ref{fig:simulated-data}), which is impossible for current methods.
    \item In addition to providing a high-resolution representation of the tree topologies, PhyloVAE, as a non-autoregressive model,  enjoys much faster training/generation than a previous autoregressive model ARTree \citep{xie2024artree}, while maintaining the approximation performance.
    \item Extensive and practical experiments demonstrate the robust representation ability and generative modeling efficiency of PhyloVAE for phylogenetic tree topologies.
\end{itemize}



\section{Background}\label{sec:background}

\begin{wrapfigure}[12]{r}{.4\textwidth}
\vspace{-0.8cm}
\includegraphics[width=\linewidth]{figures/tree-example.pdf}
\caption{An example of a tree topology with six leaf nodes. The labels of leaf nodes are $\{\textrm{A,B,C,D,E,F}\}$.
}
\label{fig:tree}
\end{wrapfigure}

\paragraph{Phylogenetic trees}
The common structure for describing evolutionary history is a phylogenetic tree, which consists of a tree topology $\tau$ and the associated non-negative edge lengths on $\tau$. The tree topology $\tau$ is a bifurcating tree graph $\left(V, E\right)$, where $V$ and $E$ represent the sets of nodes and edges, respectively. Tree topologies can be either rooted or unrooted.
While our focus in this paper is on unrooted tree topologies, our proposed method can easily accommodate rooted ones as well.
We will refer to unrooted tree topologies as ``tree topologies'' unless specified otherwise.
In a tree topology $\tau$, the edges in $E$ are undirected, and the nodes in $V$ can have a degree of either 3 or 1.
Degree 3 nodes are internal nodes representing unobserved ancestor species, while degree 1 nodes are leaf nodes representing observed species labelled with corresponding species names.
An edge connecting a leaf node to an internal node is called a pendant edge.
The goal of phylogenetic inference is to reconstruct the underlying phylogenetic trees from the sequence data (e.g., DNA, RNA, protein) of observed species \citep{felsenstein2004inferring}, following various criteria such as maximum likelihood \citep{Felsenstein81}, maximum parsimony \citep{fitch1971parsimony}, and Bayesian approaches \citep{yang1997bayesian, Mau99, Larget1999MarkovCM, ronquist2012mrbayes}. 
With $N$ leaf nodes, the number of possible tree topologies explodes combinatorially ($(2N-5)!!$), posing significant challenges to phylogenetic inference and related representation learning tasks. 
Further details on phylogenetic models can be found in Appendix \ref{app:phyloinfer}.

\vspace{-0.2cm}
\paragraph{ARTree for tree topology modeling}
In ARTree \citep{xie2024artree}, an autoregressive probabilistic model for tree topologies, a tree topology is decomposed into a sequence of leaf node addition actions, and the associated conditional probabilities are modeled using learnable topological features via graph neural networks (GNNs) \citep{Zhang2023VBPIGNN}.
The corresponding tree topology generating process can be described as follows.
Let $\mathcal{X}=\{x_1,\ldots,x_N\}$ denote the set of leaf nodes with a pre-defined order.
The generating process starts from the simplest and unique tree topology $\tau_3=(V_3,E_3)$ that contains the first three leaf nodes ${x_1,x_2,x_3}$, and keeps adding new leaf node as follows.
Assume an intermediate tree topology $\tau_n=(V_n,E_n)$ with the first $n<N$ leaf nodes in $\mathcal{X}$, termed an \emph{ordinal tree topology} of rank $n$ as defined in \citet{xie2024artree}.
Then, an edge $e_n\in E_n$ is selected according to a conditional distribution $Q(e_n|e_{<n})$ computed by GNNs ($e_{<n}$ is the set of previously selected edges), and $\tau_n$ is then extended to $\tau_{n+1}$ by attaching the next leaf node $x_{n+1}$ to $e_n$.
This process is repeated until all leaf nodes in $\mathcal{X}$ are attached.
This way, a tree topology $\tau=\tau_N$ is uniquely transformed into an edge decision sequence $D=(e_3,\ldots,e_{N-1})$, and the corresponding ARTree-based probability is $Q(\tau)=Q(D)=\prod_{n=3}^{N-1}Q(e_n|e_{<n})$
where $e_{<3}=\emptyset$.
While effective, the repetitive computation of topological node embeddings and multi-round message passing steps in GNNs also add to the computational cost of ARTree.
More details are deferred to Appendix \ref{app:artree}.

\vspace{-0.2cm}
\paragraph{Variational autoencoder}
The variational autoencoder (VAE) \citep{VAE} assumes a generative model $p_{\bm{\theta}}(\bm{y},\bm{z})=p_{\bm{\theta}}(\bm{y}|\bm{z})p(\bm{z})$, where $\bm{z}\in\mathbb{R}^d$ is a latent variable with a prior distribution $p(\bm{z})$, and an inference model $q_{\bm{\phi}}(\bm{z}|\bm{y})$ as an approximation for the intractable posterior $p_{\bm{\theta}}(\bm{z}|\bm{y})$.
Given an observed data set $\{\bm{y}_1,\ldots,\bm{y}_{M}\}$, the generative model and inference model can be jointly learned by maximizing the following evidence lower bound (ELBO) 
\begin{equation}\label{eq:elbo}
L(\bm{y};\bm{\theta},\bm{\phi})
=\mathbb{E}_{q_{\bm{\phi}}(\bm{z}|\bm{y})}\log\left(\frac{p_{\bm{\theta}}(\bm{y},\bm{z})}{q_{\bm{\phi}}(\bm{z}|\bm{y})}\right)
=\log p_{\bm{\theta}}(\bm{y}) - D_\mathrm{KL}\left(q_{\bm{\phi}}(\bm{z}|\bm{y})\|p_{\bm{\theta}}(\bm{z}|\bm{y})\right)
\leq \log p_{\bm{\theta}}(\bm{y})
\end{equation}
for all data points $\{\bm{y}_i: i=1\ldots, M\}$.
Here, $p_{\bm{\theta}}(\bm{y})=\int_{\mathbb{R}^d}p_{\bm{\theta}}(\bm{y},\bm{z})\mathrm{d}\bm{z}$ is the marginal likelihood of $\bm{y}$ and $D_{\mathrm{KL}}$ is the Kullback-Leibler (KL) divergence.
To remedy the approximation of the latent variable posterior and achieve a more flexible generative model, the importance weighted autoencoder (IWAE) \citep{burda2016IWAE} instead uses the multi-sample lower bound
\begin{equation}\label{eq:mlb}
L_K(\bm{y};\bm{\theta},\bm{\phi})
=\mathbb{E}_{q_{\bm{\phi}}(\bm{z}^1|\bm{y})\cdots q_{\bm{\phi}}(\bm{z}^K|\bm{y})}\log\left(\frac{1}{K}\sum_{i=1}^K\frac{p_{\bm{\theta}}(\bm{y},\bm{z}^i)}{q_{\bm{\phi}}(\bm{z}^i|\bm{y})}\right)
\leq \log p_{\bm{\theta}}(\bm{y}).
\end{equation}
The equalities in equation (\ref{eq:elbo}) and (\ref{eq:mlb}) hold if and only if $q_{\bm{\phi}}(\bm{z}|\bm{y})=p_{\bm{\theta}}(\bm{z}|\bm{y})$.
While VAEs have been effectively used in graph representation learning \citep{kipf2016VGAE, GraphVAE, winter2021permutation, zahirnia2022micro}, they typically require transforming graphs into numerical encodings, such as adjacency matrices, and then working in the encoding space.
However, the specific bifurcating structure of phylogenetic tree topologies imposes unique constraints on these adjacency matrices, posing challenges for applying these methods directly.

\section{Proposed method}
In this section, we introduce phylogenetic variational autoencoders (PhyloVAEs), an unsupervised learning framework designed specifically for phylogenetic tree topologies. We begin with a concise overview of the problem setting and the fundamental components of PhyloVAE in Section \ref{sec:treevae-overview}. 
We then develop an encoding mechanism that bijectively maps tree topologies to encoding vectors in Section \ref{sec:encoding}.
Finally, Section \ref{sec:treevae-components} elucidates how to utilize this encoding mechanism and learnable topological features to establish the generative and inference models within the PhyloVAE framework.
We emphasize that the input to our algorithms is a collection of phylogenetic tree topologies
\footnote{These collections often come from phylogenetic analysis softwares such as MrBayes \citep{ronquist2012mrbayes} and BEAST \citep{drummond2007beast}, but they can also come from observations, open-source data sets, and other biological softwares, as long as they are of scientific interest.}.
Our goal is to build a probabilistic model and to learn useful representations of this collection, not to infer those trees directly from sequence data, which is a separate and intensively studied problem.

\subsection{PhyloVAE: an overview}\label{sec:treevae-overview}
Let $\mathcal{T} = \{(\tau^{i},w^i)\}_{i=1}^M$ be a collection of tree topologies\footnote{The weights are formed by merging replicate tree topologies in the data set for a compact form of $\mathcal{T}$.}, where $w^i$ is the weight for the tree topology $\tau^{i}$ and $\sum_{i=1}^M w^i=1$.
All tree topologies in $\mathcal{T}$ have the same leaf nodes $\mathcal{X}=\{x_1,x_2,\ldots,x_N\}$ with a pre-selected order.
For example, $\mathcal{T}$ can be a sample of tree topologies produced by some phylogenetic inference software, such as from a posterior sample \citep{ronquist2012mrbayes, Suchard2018-eo} or the bootstrap \citep{felsenstein1985bootstrap, Minh2013UltrafastAF}, {where the weight can equal to the frequency of a tree topology among these samples}.
Given the observed data set $\mathcal{T}$, the data distribution is defined as $p_{\textrm{data}}(\tau)=\sum_{i=1}^Mw^i\delta_{\tau^i}(\tau)$ where $\delta$ is a Kronecker delta function that satisfies $\delta_{\tau^i}(\tau)=1$ if $\tau=\tau^i$ and $\delta_{\tau^i}(\tau)=0$ elsewhere.

Similar to standard VAEs, PhyloVAE consists of a generative model and an inference model.
Let $\bm{z}\in \mathbb{R}^d$ be a latent variable with a prior distribution $p(\bm{z})$ and $p_{\bm{\theta}}(\tau|\bm{z})$ be a probabilistic model that defines the probability of generating $\tau$ conditioned on the latent variable $\bm{z}$.
The marginal probability of $\tau$ is given by
$p_{\bm{\theta}}(\tau) = \int_{\mathbb{R}^d}p_{\bm{\theta}}(\tau|\bm{z})p(\bm{z})\mathrm{d}\bm{z}.$
The prior distribution $p(\bm{z})$ is required to be analytic and easy to sample from.
In this paper, we will use a standard Gaussian prior distribution, i.e., $p(\bm{z})=\mathcal{N}(\bm{z};\bm{0}_d,\bm{I}_d)$.
With an inference model $q_{\bm{\phi}}(\bm{z}|\tau)$ that approximates the posterior $p_{\bm{\theta}}(\bm{z}|\tau)$, the multi-sample lower bound on $\tau$ takes the form
\begin{equation}\label{eq:treevae-mlb}
L_K(\tau;\bm{\theta},\bm{\phi})
=\mathbb{E}_{q_{\bm{\phi}}(\bm{z}^1|\tau)\cdots q_{\bm{\phi}}(\bm{z}^K|\tau)}\log\left(\frac{1}{K}\sum_{i=1}^K\frac{p_{\bm{\theta}}(\tau,\bm{z}^i)}{q_{\bm{\phi}}(\bm{z}^i|\tau)}\right)
\leq \log p_{\bm{\theta}}(\tau),
\end{equation}
which reduces to the standard ELBO for $\log p_{\bm{\theta}}(\tau)$ when $K=1$.
The overall multi-sample lower bound on $\mathcal{T}$ is defined as $L_K(\mathcal{T};\bm{\theta},\bm{\phi})=\mathbb{E}_{p_{\textrm{data}}(\tau)}L_K(\tau;\bm{\theta},\bm{\phi})$, which serves as the objective function for training PhyloVAE.
Unlike standard VAEs, the discrete nature of $\tau$ makes it challenging to construct the generative model and inference model.
In what follows, we describe how this is done using an encoding mechanism and learnable topological features respectively.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/encoding-v2.pdf}
    \caption{The decomposition loop and reconstruction loop for encoding the tree topology with leaf nodes $\mathcal{X}=\{\textrm{A,B,C,D,E,F}\}$ in Figure \ref{fig:tree}. Starting from the tree topology in the upper left, we remove the pendant edges $f_6, f_5,f_4$ (associated with the leaf nodes F, E, and D) sequentially, and record edge decision $e_5,e_4,e_3$. Then starting from the three-leaf tree topology in the lower right, we add back $f_4,f_5,f_6$ and index these nodes (except for the root) sequentially.
    The resulting encoding vector is $(3,7,5)$, which are the indexes associated with $e_3,e_4,e_5$.}
    \label{fig:encoding}
\end{figure}

\subsection{An encoding mechanism for tree topologies}\label{sec:encoding}
Recall that in the generating process of ARTree, there exists a map between a tree topology $\tau$ and the edge decision sequence $D=(e_3,\ldots,e_{n-1})$ (see Section \ref{sec:background} and Appendix \ref{app:artree} for more details). We can further encode this sequence into an integer-valued vector via the following procedure.

\paragraph{Decomposition loop}
In this loop, we repetitively remove the leaf nodes from $\tau_N=\tau$, an ordinal tree topology of rank $N$, starting from the last added node $x_N$ to the first added node $x_{4}$.
Suppose now we have $\tau_n=(V_n,E_n)$ in hand, a pendant edge $f_n=({t_n},x_n)$ is firstly removed, where ${t_n}$ is the internal node connected to $x_n$, resulting in an ordinal tree topology of rank $n-1$, $\tau_{n-1}$.
Let the two neighbors of ${ t_n}$ in $\tau_n$ be $u_{n-1}$ and $v_{n-1}$ (in addition to $x_n$).
The edge $e_{n-1}=(u_{n-1},v_{n-1})$, therefore, is the corresponding edge decision for $x_n$ on $\tau_{n-1}$ which we save for the reconstruction loop.
This procedure continues until a tree topology $\tau_3$ with the first three leaf nodes is reached.
See the top row in Figure \ref{fig:encoding} for an illustration.

\paragraph{Reconstruction loop}
In this loop, we start from $\tau_3$ and reconstruct $\tau_N$ by adding back the leaf nodes from $x_4$ to $x_N$. 
At the beginning, we index the leaf nodes on $\tau_3$ by setting $\mathrm{Index}(x_1)=1, \mathrm{Index}(x_2)=2$, $\mathrm{Index}(x_3)=3$, and denote the unique internal node in $\tau_3$ as $r$. 
Suppose now we have an ordinal tree topology $\tau_n=(V_n,E_n)$ of rank $n$. We first locate the edge decision $e_n=(u_n,v_n)$ on $\tau_n$ (without loss of generality, $e_n$ is assumed to be directed away from $r$ so that it can be identified via $v_n$ on $\tau_n$) and then attach the pendant edge $f_{n+1}=({ t_{n+1}},x_{n+1})$ to $e_n$. Finally, we set $\mathrm{Index}({ t_{n+1}})=N+n-2$ and $\mathrm{Index}(x_{n+1})=n+1$.
In this way, the next tree topology $\mathrm{\tau}_{n+1}=(V_{n+1},E_{n+1})$ is constructed and all the nodes except $r$ in $V_{n+1}$ are indexed.
This process ends upon the full reconstruction of $\tau_N$.
See the bottom row in Figure \ref{fig:encoding} for an illustration.

After the above two loops, the encoding vector $\bm{s}(\tau)$ for a tree topology $\tau$ takes the form
\begin{equation}
\bm{s}(\tau) = \left[s_3,s_4,\cdots,s_{n-1}\right]'=\left[\mathrm{Index}(v_3), \mathrm{Index}(v_4),\cdots, \mathrm{Index}(v_{n-1})\right]'\in\mathbb{N}^{n-3}.
\end{equation}
This encoding mechanism is summarized in Algorithm \ref{alg:edge-indexing}.
Theorem \ref{thm:encoding-linear-time} (proof in Appendix \ref{sec:app-linear-time-proof}) shows that this mechanism has linear time complexity, which is crucial to the efficient training of PhyloVAE.

\begin{theorem}\label{thm:encoding-linear-time}
Given a tree topology $\tau$ with $N$ leaf nodes, the time complexity of computing its encoding vector $\bm{s}(\tau)$ is $O(N)$.
\end{theorem}
Conversely, when given an encoding vector $\bm{s}$, one can simply follow the reconstruction loop to obtain the corresponding tree topology $\tau$ in linear time (deduced from Theorem \ref{thm:encoding-linear-time}).
This enables the fast generation of samples from PhyloVAE (details are deferred to Appendix \ref{sec:app-encoding-generative}).
We note that a similar encoding strategy has been proposed in Phylo2Vec \citep{penn2023phylo2vec}.
However, their approach has quadratic time complexity for vector encoding of tree topologies with unlabelled internal nodes.
Our method achieves faster processing by employing a smart indexing strategy, eliminating the need for repetitive relabelling of edges during the reconstruction loop.
\begin{algorithm}[t]
\caption{A linear-time algorithm for tree topology encoding}
\label{alg:edge-indexing}
\KwIn{A tree topology $\tau$ with $N$ leaf nodes.}
\KwOut{An encoding vector $\bm{s}(\tau)=(s_3,\ldots,s_{N-1})\in \mathbb{N}^{N-3}$ corresponding to $\tau$.}
$\tau_N\leftarrow \tau$\;
\For{$n=N,\ldots,4$}{
Remove the pendant edge $f_n=({t_n},x_n)$ from $\tau_n$ and obtain $\tau_{n-1}$\;
Record the edge decision $e_{n-1}=(u_{n-1},v_{n-1})$ on $\tau_{n-1}$\;
}
$\mathrm{Index}(x_1)\leftarrow 1; \mathrm{Index}(x_2)\leftarrow 2; \mathrm{Index}(x_3)\leftarrow 3$; $r\leftarrow$ the unique internal node of $\tau_3$\;
\For{$n=3,\ldots,N-1$}{
Attach the pendant edge $f_{n+1}$ to $e_n$ (assume $e_n=(u_n,v_n)$ is directed away from $r$) and obtain $\tau_{n+1}$\;
$\mathrm{Index}({ t_{n+1}})\leftarrow N+n-2$; $\mathrm{Index}(x_{n+1})\leftarrow n+1$\;
$s_n\leftarrow \mathrm{Index}(v_n)$\;
}
\end{algorithm}

\subsection{Generative model and inference model in PhyloVAE}\label{sec:treevae-components}
\paragraph{Generative model}
The encoding mechanism in Section \ref{sec:encoding} allows us to define $p_{\bm{\theta}}(\tau|\bm{z})$ through $p_{\bm{\theta}}(\bm{s}(\tau)|\bm{z})$, where $\bm{s}(\tau)=(s_3,\ldots,s_{N-1})$ is the encoding vector for $\tau$.
Similar to the diagonal Gaussian distribution used in standard VAEs, we assume the elements in $\bm{s}(\tau)$ are conditionally independent given $\bm{z}$, i.e.,
\begin{equation}\label{eq:treevae-generative}
p_{\bm{\theta}}(\tau|\bm{z}) = p_{\bm{\theta}}(\bm{s}(\tau)|\bm{z}) = \prod_{n=3}^{N-1}p_{\bm{\theta}}(s_n|\bm{z}).
\end{equation}
The factorized form of equation (\ref{eq:treevae-generative}) enables parallel computation of the probabilities $p_{\bm{\theta}}(s_n|\bm{z})$.
Our experiments show that this non-autoregressive structure substantially reduces the computational cost compared to autoregressive models such as ARTree.

Let $S_n:=\{i\in\mathbb{N}: 1\leq i\leq n \text{ or } N+1\leq i\leq  N+n-3\}$ be the set of allowable indices for the edges $\{\mathrm{Index}(v_n): e_n=(u_n,v_n)\in E_n\}$.
For all $3\leq n< N$, $p_{\bm{\theta}}(s_n|\bm{z})$ takes the form
\begin{subequations}
\arraycolsep=1.8pt
\def\arraystretch{1.5}
\begin{align}
p_{\bm{\theta}}(s_{n}|\bm{z})&=\mathrm{Discrete}\left[\mathrm{softmax}(\bm{m}_n\odot \bm{\phi}_n(\bm{z}))\right],\label{eq:edge-dist}\\
\bm{\Phi}(\bm{z}) &= [\bm{\phi}_3(\bm{z}),\ldots,\bm{\phi}_{N-1}(\bm{z})]'=\mathrm{MLP}_{\mathrm{enc}}(\bm{z})\in\mathbb{R}^{(N-3)\times(2N-3)},\label{eq:logits-MLP}
\end{align}
\end{subequations}
where $\mathrm{MLP}_{\mathrm{enc}}$ is a multi-layer perceptron, $\odot$ is elementwise multiplication, and $\bm{m}_n$ is defined as
\begin{equation}
(\bm{m}_{n}\odot \bm{\alpha})_i=\left\{
\begin{array}{ll}
\bm{\alpha}_i,   &  i\in S_n,\\
-\infty,   & \textrm{elsewhere}.
\end{array}
\right.
\end{equation}
This mask vector is introduced to ensure that the generated encoding vector $\bm{s}$ is always valid for representing a tree topology.  
\paragraph{Inference model}
The inference model $q_{\bm{\phi}}(\bm{z}|\tau)$ is built on top of learnable topological features as follows.
Firstly, we compute the topological node embeddings $\{\bm{f}_u^0: u\in V\}$ for $\tau=(V,E)$ by minimizing the following Dirichlet energy 
$\mathcal{E}(\tau) = \sum_{(u,v)\in E} \|\bm{f}_u^0-\bm{f}_v^0\|^2$
using the efficient two-pass algorithm described in \citet{Zhang2023VBPIGNN}.
These topological node embeddings are then fed into a gated message-passing block \citep{li2015gated} implemented as 
\begin{equation}
\bm{f}^{l+1}_{u} = \mathrm{GRU}\left(\bm{f}^{l}_{u}, \sum_{v\in \mathcal{N}(u)} \bm{W}_{\mathrm{msg}}^l\bm{f}^l_v\right), \ u \in V,
\end{equation}
where $\mathcal{N}(u)$ is the neighborhood of $u$, $\bm{W}_{\mathrm{msg}}^l$ is a learnable message matrix that aggregate the information from $N(u)$, and $\mathrm{GRU}$ is a gated recurrent unit \citep{cho2014learning}.
After $L$ rounds of message passing, the graph embedding vector $\bm{f}_\tau$ is computed by a sum-pooling layer, i.e., $\bm{f}_\tau = \sum_{u\in V} \bm{f}^L_u$.
Finally, we use a diagonal normal distribution for the conditional distribution of the latent variable $\bm{z}$ whose mean and standard deviation are defined based on $\bm{f}_\tau$ as follows
\begin{equation}
q_{\bm{\phi}}(\bm{z}|\tau) = \mathcal{N}\left(\bm{z}; \bm{\mu}_\tau, \mathrm{diag}(\bm{\sigma}^2_{\tau})\right), \ \bm{\mu}_\tau=\mathrm{MLP}_{\mu}(\bm{f}_\tau),\ \log\bm{\sigma}_\tau=\mathrm{MLP}_{\sigma}(\bm{f}_\tau),
\end{equation}
where $\mathrm{MLP}_{\mu}$ and $\mathrm{MLP}_{\sigma}$ are two multi-layer perceptrons, and $\bm{\phi}$ are the learnable parameters.
The mean of the inference model, $\bm{\mu}_\tau\in\mathbb{R}^d$, is a deterministic low-dimensional representation of $\tau$, and we will show its representation power in our experiments.
Although this inference model is built on top of \citet{Zhang2023VBPIGNN}, \citet{Zhang2023VBPIGNN} only provides a deep model architecture for extracting graph features and does not use it for representation learning.
This representation also induces a pairwise distance between tree topologies.
For two tree topologies $\tau_1$, $\tau_2$, we define the $L^p$ distance between them as $D_{L^p}(\tau_1,\tau_2)=\|\bm{\mu}_{\tau_1}-\bm{\mu}_{\tau_2}\|_p$, where $\|\cdot\|_p$ is the $p$-norm in the Euclidean space.
Note that the generative model also allows us to map an arbitrary point in the latent space to a tree topology, which is impossible for previous representation methods.

\paragraph{Optimization} Thanks to the Gaussian inference model, the gradient $\nabla_{\bm{\theta},\bm{\phi}}L_K(\tau;\bm{\theta},\bm{\phi})$ can be derived using the reparameterization trick \citep{VAE} as follows
\begin{equation}\label{eq:reparam}
\nabla_{\bm{\theta},\bm{\phi}}L_K(\tau;\bm{\theta},\bm{\phi})=\mathbb{E}_{\bm{\varepsilon}^1,\ldots,\bm{\varepsilon}^K\sim \mathcal{N}(\cdot;\bm{0},\bm{I})}\nabla_{\bm{\theta},\bm{\phi}}\log\left(\frac{1}{K}\sum_{i=1}^K\frac{p_{\bm{\theta}}(\tau,\bm{\mu}_\tau+\bm{\sigma}_\tau\odot\bm{\varepsilon}^i)}{q_{\bm{\phi}}(\bm{\mu}_\tau+\bm{\sigma}_\tau\odot\bm{\varepsilon}^i|\tau)}\right).
\end{equation}
During training, parameters of the generative model and inference model are updated along the gradient direction $\nabla_{\bm{\theta},\bm{\phi}}L_K(\mathcal{T};\bm{\theta},\bm{\phi})=\mathbb{E}_{p_{\textrm{data}}(\tau)}\nabla_{\bm{\theta},\bm{\phi}}L_K(\tau;\bm{\theta},\bm{\phi})$, using Monte Carlo gradient estimators.

\section{Related works}
For harnessing the latent-variable structure to accelerate autoregressive models, \citet{gu2018nonar} proposed non-autoregressive machine translation by defining a factorizable distribution for the output sequence conditioned on the input sequence and latent fertility variable. 
\citet{kaiser2018fast} extended this to discrete latent variables that summarize the input information.
This approach was also integrated with normalizing flows by \citet{ma2019flowseq}.

Previous VAE frameworks for graph representation learning often encode a graph to its adjacency matrix and then define the generative models for matrices \citep{kipf2016VGAE, GraphVAE, winter2021permutation, zahirnia2022micro}.
However, the bifurcating structure and the unlabelled internal nodes of tree topologies put special constraints on adjacency matrices, which may hinder the application of previous works to representation learning of phylogenetic trees.

The most popular means of learning an embedding of a collection of phylogenetic trees is to calculate pairwise distances in some way and project to a Euclidean space using multidimensional scaling \citep{amenta2002,hillis2005analysis,Jombart2017-qw}.
More recently, \citet{penn2023phylo2vec} proposed an encoding strategy that relies on tree topology branching patterns, with representation dimensions scaling according to tree size.

{ Some previous works integrated trees with VAEs \citep{shin2017treestructured, Vikram2018loracs, manduchi2023treevae}. However, they all consider a tree-shaped prior distribution or hierarchical latent variable structure for enhanced interpretability and generative quality. These papers do not consider modeling any graph or tree objects and thus are clearly distinct from our PhyloVAE.}


\section{Experiments}
In this section, we evaluate the performance of PhyloVAE for structural representation on simulated data sets (Section \ref{sec:simulation}) and real phylogenies (Section \ref{sec:real-phylogeny}), and generative modeling on benchmark data sets (Section \ref{sec:approximation-capacity}).
We set the latent dimension $d=2$ for better visualization of the latent representations in Section \ref{sec:simulation} and \ref{sec:real-phylogeny}.
For all experiments, the number of particles is set to $K=32$ and the inference model employs $L = 2$ rounds of message passing.
Leaf nodes are ordered lexicographically based on the corresponding species names.
We designed our experiments with the goals of (i) verifying the effectiveness of PhyloVAE for representation learning of tree topologies and (ii) examining the generative modeling performance of PhyloVAE, with an emphasis on the generation speed.
Results are gathered after 200,000 iterations with a batch size of 10.
Further details can be found in Appendix \ref{sec:app-experimental-details}.
Our code is released at \href{https://github.com/tyuxie/PhyloVAE}{\texttt{https://github.com/tyuxie/PhyloVAE}}.

\subsection{Representation learning on simulated data sets}\label{sec:simulation}

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{figures/simulation-plot.pdf}
    \caption{Performance of PhyloVAE for structural representation on simulated data sets.
    \textbf{Left}: A visualization of the 2D latent manifold for the data set of five-leaf tree topologies.
    $\Phi(\cdot)$ refers to the cumulative density function of the one-dimensional standard Gaussian distribution.
    Different colors represent the first edge decision and different transparencies of each color represent the second edge decision.
    \textbf{Middle}: Representation vectors of all the eight-leaf tree topologies. 
    The scatter size is proportional to the probability of the corresponding tree topology.
    \textbf{Right}: Pairwise scatter plot, linear regression, and Pearson correlation coefficients between different distance metrics of tree topologies.
    $L^2$ = Euclidean distance in PhyloVAE latent space, RF = Robinson-Foulds, PD = Path difference.
    }
    \label{fig:simulated-data}
    \vspace{-0.3cm}
\end{figure}


\paragraph{Five-leaf tree topologies}
In this experiment, the training set consists of all the 15 tree topologies with five leaves and a randomly generated weight vector $\bm{w}\sim\textrm{Dirichlet}(\beta\bm{1})\in\mathbb{R}^{15}$ for these tree topologies with $\beta=0.75$. Figure \ref{fig:simulated-data} (left) depicts the tree topologies as partition representation (see Figure \ref{fig:five-leaf-illustration} in Appendix \ref{sec:app-setting-five-leaf} for an illustration), generated by a trained PhyloVAE conditioned on the Gaussian quantiles. 
To ease presentation, we choose the argmax index of the multinomial probability in equation (\ref{eq:edge-dist}) instead of randomly sampling.
We see that the generated tree topology exhibits nice continuity as the latent variable varies.

\vspace{-0.1cm}
\paragraph{Eight-leaf tree topologies}
We begin by constructing the training set, which consists of a mixture of three peaked distributions comprising all 10,395 tree topologies with eight leaves.
Each peaked distribution is derived as follows: we select a ground truth tree topology and simulate DNA sequences of length 500 for the eight leaf nodes using the Jukes-Cantor (JC) substitution model \citep{jukes1969evolution} with the edge lengths fixed at 1.
We then compute the posterior distribution of tree topologies given the simulated DNA sequences with a uniform prior over tree topologies and the edge lengths fixed at 1.
This posterior distribution is used as the peaked distribution as it concentrates around the selected ground truth tree topology.
The same procedure is repeated three times (see Figure \ref{fig:8taxa-first}, \ref{fig:8taxa-second} and \ref{fig:8taxa-third} for the selected ground truth tree topologies), leading to three peaked distributions which are then evenly mixed to form the training data set.

The latent representations of eight-leaf tree topologies are visualized in Figure \ref{fig:simulated-data} (middle), where tree topologies on different peaks are clearly separated, demonstrating the effectiveness of PhyloVAE for representation learning.
Following \citet{kendall2016mapping}, we compare $L^2$ distance to Robinson-Foulds (RF) distance \citep{robinson1981comparison} and path difference (PD) distance \citep{steel1993pathdifference} in Figure \ref{fig:simulated-data} (right), where $L^2$ distance shows a positive correlation with RF/PD distance.

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{figures/real-phylogeny-combination.pdf}
    \caption{Performances of PhyloVAE and MDS plot for representing real phylogenies.
    \textbf{Left/Middle}: Latent representations of the posterior mammal gene trees for five genes with different lengths.
    The scatter size is proportional to the probability of the tree topology.
    \textbf{Right}: Latent representations of samples of tree topologies from five independent BEAST runs on the rabies data set.
    }
    \label{fig:real-phylogeny}
    \vspace{-0.3cm}
\end{figure}

\subsection{Representation learning on real phylogenies}\label{sec:real-phylogeny}

\paragraph{Gene trees and sequence lengths}
In this experiment, PhyloVAE is employed to analyze phylogenetic inference results obtained with different genes (different genes evolve under different evolutionary models) and sequence lengths.
Following \citet{hillis2005analysis}, we select five genes and the ground truth phylogenetic tree (Figure \ref{fig:murphytree}; 44 leaves) from the early placental mammal evolution analysis in \citet{murphy2001}.
For each gene, we simulate the DNA sequences with a fixed length along the ground truth tree using the corresponding evolutionary model, run a MrBayes chain \citep{ronquist2012mrbayes} for one million iterations, and sample per 100 iterations in the last 100,000 iterations, to gather the posterior samples, {as done in \citet{hillis2005analysis}}.
{These one million iterations are enough for the MrBayes run to converge.}
These 5,000 tree topologies with uniform weights constitute the training set of PhyloVAE.

The upper left and upper middle plots of Figure \ref{fig:real-phylogeny} depict the learned latent representations of the posterior tree topologies for different genes obtained by PhyloVAE. 
We see that different groups are clearly separated.
With a sequence length of 1000, the inferred posteriors from ADORA3 and APP are close to the ground truth tree topology, while those from IRBP, mtRNA, and ZFX show deviations.
When the sequence length is increased to 2000, the inferred posterior from ADORA3 becomes more concentrated around the ground truth tree topology, while the inferred posteriors from the other genes remain diffuse.
Figure \ref{fig:mammal-gene-trees-5000} in Appendix \ref{sec:app-results-gene-trees} shows that all five genes discover the ground truth tree topology with a sequence length of 5000.
For baseline visualization methods, the lower left and lower middle plots of Figure \ref{fig:real-phylogeny} show the multidimensional scaling (MDS) plots \citep{hamer2013multidimensional} of mammal gene trees, where we find that different groups tend to concentrate towards the origins and overlap with each other, while our PhyloVAE provides more clear representations of different groups.


\paragraph{Multiple phylogenetic analyses comparison}
In this experiment, we use PhyloVAE to compare multiple phylogenetic analyses and assess convergence.
The sequence alignment under consideration comprises 290 rabies genomes \citep{viana2023rabies}.
We conduct 5 independent BEAST \citep{Suchard2018-eo} analyses for 400 million iterations, and sample every 100,000 iterations in the last 200 million iterations.
Afterward, the rooted posterior tree topologies sampled by BEAST are unrooted.
The resulting 10,000 tree topologies with uniform weights form the training set of PhyloVAE.

In Figure \ref{fig:real-phylogeny} (right), tree topologies from five independent BEAST runs form five non-overlapping groups, with 2-3 sub-groups within each group, indicating the divergence of these BEAST runs.
Notably, the maximum clade credibility (MCC) tree from each run resides within the correct high-density region in the latent space.
An example showcasing the convergence of independent phylogenetic analyses is provided in Appendix \ref{sec:app-results-multiple-analyses}.



\newcommand*{\vcenteredhbox}[1]{\begingroup
\setbox0=\hbox{#1}\parbox{\wd0}{\box0}\endgroup}

\begin{table}[t]
    \caption{KL divergences to the ground truth of different methods across eight benchmark data sets.
    ``PhyloVAE ($d$)'' means PhyloVAE with latent dimension $d$.
    The ``\# Training set'' and ``\# Ground truth'' columns show the number of unique tree topologies in the training sets and ground truth respectively. The results are averaged over 10 replicate training sets. The tree topology probability of PhyloVAE is estimated using importance sampling with 1000 particles. The results of SBN-EM, SBN-EM-$\alpha$ are from \citet{Zhang2018SBN}, and the results of ARTree are from \citet{xie2024artree}.
    For each data set, the best result is marked in \textbf{black bold font} and the second best result is marked in \textbf{\color{brown}{brown bold font}}.
    }
    \label{tab:tde}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccccc}
    \toprule
    \multirow{2}{*}{Sequence set} &\multirow{2}{*}{\# Leaves}  &\multirow{2}{*}{\# Training set}&\multirow{2}{*}{\# Ground truth} &\multicolumn{5}{c}{KL divergence to ground truth} \\
\cmidrule(l){5-9}
       &&&&  SBN-EM & SBN-EM-$\alpha$ &  ARTree & PhyloVAE (2) & PhyloVAE (10) \\
       \midrule
       DS1 & 27 & 1228&2784  & 0.0136 & \textbf{\color{brown}{0.0130}}  & \textbf{0.0045} & 0.0273& 0.0189\\
       DS2 & 29 & 7 &42 & 0.0199 & 0.0128 & \textbf{0.0097}&0.0100&\textbf{\color{brown}{0.0098}} \\
       DS3 & 36 & 43 &351 & 0.1243 & 0.0882 &0.0548&\textbf{\color{brown}{0.0529}}&\textbf{0.0477} \\
       DS4 & 41 & 828& 11505 & 0.0763&0.0637 & \textbf{0.0299}&0.0619&\textbf{\color{brown}{0.0469}} \\
       DS5 & 50 &33752& 1516877  &0.8599&0.8218 & \textbf{\color{brown}{0.6266}}& 0.7985&\textbf{0.5744}\\
       DS6 & 50&35407& 809765&0.3016&0.2786&\textbf{\color{brown}{0.2360}}&0.3241&\textbf{0.2207}\\
       DS7 & 59&1125&11525 &0.0483&0.0399 &\textbf{0.0191}&0.0591&\textbf{\color{brown}{0.0370}}\\
       DS8 & 64&3067&82162 &0.1415&0.1236 & \textbf{0.0741}&0.1372&\textbf{\color{brown}{0.1061}}\\
    \bottomrule
    \end{tabular}
    }
\vspace{-0.5cm}
\end{table}

\subsection{Generative modeling on benchmark data sets}\label{sec:approximation-capacity}
Finally, we assess the generative modeling performance of PhyloVAE on eight benchmark sequence sets, DS1-8, which contain biological sequences from 27 to 64 eukaryote species and are commonly considered for benchmarking tree topology density estimation and Bayesian phylogenetic inference tasks in previous works \citep{Zhang2018SBN, Zhang2019VBPI, VBPI-JMLR, Zhang2020VBPINF, mimori2023geophy, Zhou2023PhyloGFN, xie2024artree, xie2024vbpisibranch,xie2024improving, vbpimixture, bbvimixture}. 
{These eight data sets cover comprehensive posterior patterns \citep{Whidden2014QuantifyingME} and are considered good representative cases.}
The training sets and ground truths for PhyloVAE are obtained the same way as in \citet{Zhang2018SBN} (see more details in Appendix \ref{sec:app-setting-benchmark}).
We consider SBN-EM, SBN-EM-$\alpha$, and ARTree as baselines, and use the same experimental settings in the original papers \citep{Zhang2018SBN, xie2024artree}.
For a fair comparison, we uses the same batch size and number of iterations for PhyloVAE as in ARTree.

\begin{wrapfigure}[17]{r}{.38\textwidth}
\vspace{-0.5cm}
\includegraphics[width=\linewidth]{figures/tde-time.pdf}
\caption{Runtime comparison between ARTree and PhyloVAE ($d=10$) with $K=32$ particles.
Training means 10 training iterations.
Generation means generating 100 tree topologies.
}
\label{fig:tde-time}
\end{wrapfigure}
Table \ref{tab:tde} reports the approximation accuracies measured by KL divergence obtained by different methods.
We see that PhyloVAE with a latent dimension of 10 achieves comparable (if not better) results to ARTree.
Although PhyloVAE with a latent dimension of 2 may have reduced capacity, we find it works fairly well in DS1-8, validating the reliability of the two-dimensional representations in Sections \ref{sec:simulation} and \ref{sec:real-phylogeny}.
Figure \ref{fig:tde-time} shows the runtime comparison between ARTree and PhyloVAE ($d=10$) with $K=32$ particles.
We see that both the training time and generation time of PhyloVAE are significantly reduced compared to ARTree, even though multiple particles are used in the multi-sample lower bound (\ref{eq:treevae-mlb}).
This is due to the non-autoregressive nature of PhyloVAEs, as inherited from VAEs.
Note that PhyloVAE achieves these while providing useful high-resolution representations of phylogenetic tree topologies (see Figure \ref{fig:DS-trees-representation} in Appendix \ref{sec:app-DS}), which is impossible for the other baseline methods that are merely designed for tree density estimation.


\section{Conclusion}
This paper proposes PhyloVAE, an unsupervised learning framework designed for representation learning and generative modeling of phylogenetic tree topologies.
By incorporating an efficient encoding mechanism of tree topologies and leveraging a latent-variable architecture, PhyloVAE facilitates fast training and generation using non-autoregressive generative models.
The learned inference model also provides informative, high-resolution representations for tree topologies, as demonstrated in our experiments.
PhyloVAE thus offers a latent space representation that can be easily visualized like the previous MDS method, while also functioning as a probabilistic model that provides a high-resolution view of tree topology distributions.
Further applications of PhyloVAE to practical tasks in phylogenetics (e.g., phylogenetic placement, Bayesian phylogenetic inference, etc.) and extending PhyloVAE for tree topologies with different leaves would be interesting future directions.

\section*{Acknowledgements}
This work was supported by National Natural Science Foundation of China (grant no. 12201014, grant no. 12292980 and grant no. 12292983), as well as National
Institutes of Health grant AI162611.
The research of Cheng Zhang was support in part by National Engineering Laboratory for Big Data Analysis and Applications, the Key Laboratory of Mathematics and Its Applications (LMAM) and the Key Laboratory of Mathematical Economics and Quantitative Finance (LMEQF) of Peking University.
Frederick Matsen is an investigator of the Howard Hughes Medical Institute.
The authors appreciate Dave Rich for carefully validating the codebase.
The authors are grateful for the computational resources provided by the High-performance Computing Platform of Peking University.
The authors appreciate the anonymous ICLR reviewers for their constructive feedback.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

\section{Phylogenetic trees and phylogenetic inference}\label{app:phyloinfer}
The common structure for describing evolutionary history is a phylogenetic tree, which consists of a bifurcating tree topology $\tau$ and the associated non-negative edge lengths $\bm{q}$ (see Figure \ref{fig:murphytree} for a real example).
The tree topology $\tau$ is a bifurcating tree graph $\left(V, E\right)$, where $V$ and $E$ are the set of nodes and edges respectively.
We will be consistent with the main text and use ``tree topology'' for unrooted tree topology.
For a tree topology $\tau$, the edges in $E$ are undirected and the nodes in $V$ can have either 3 degrees or 1 degree.
Those with 3 degrees are called internal nodes, representing unobserved ancestor species without labels, and those with 1 degree are called leaf nodes, representing the observed existing species with labels corresponding to the species names.
An edge is called a pendant edge if it connects one leaf node to an internal node.
For each edge $e\in E$, there exists a corresponding non-negative edge length $q_e$, which quantifies the evolutionary intensity between two neighboring species.
The set of edge lengths on $\tau$ is given by $\bm{q}=\{q_e:e\in E\}$.

Each leaf node in $V$ corresponds to a species with an observed biological sequence (e.g., DNA, RNA, protein). Let $\bm{Y}=\{Y_1,\ldots,Y_M\}\in \Omega^{N\times M}$ denote the observed sequences (with characters in $\Omega$) of $M$ sites across $N$ species. The transition probabilities of the characters along the edges of a phylogenetic tree are commonly modeled using a continuous-time Markov chain \citep{felsenstein2004inferring}.
With the common assumption that different sites evolve independently and identically, given a phylogenetic tree $(\tau,\bm{q})$, the likelihood of observing $\bm{Y}$ is
\begin{equation}\label{eq:likelihood}
p(\bm{Y}|\tau,\bm{q}) =\prod_{i=1}^M \sum_{a^i}\eta(a^i_r)\prod_{(u,v)\in E}P_{a^i_u a^i_v}(q_{uv}),
\end{equation}
where $a^i$ ranges over all extensions of $Y_i$ to the internal nodes with $a^i_u$ being the character assignment of node $u$ ($r$ represents an arbitrary internal node as the virtual root node), $E$ is the set of edges of $\tau$,
$q_{uv}$ is the branch length of the edge $(u,v)\in E$, $P_{jk}(q)$ is the transition probability from character $j$ to $k$ through an edge of length $q$, and $\eta$ is the stationary distribution of the Markov chain. 
Parameters in equation (\ref{eq:likelihood}) are called an \emph{evolutionary model}. 
Assuming a prior distribution $p(\tau,\bm{q})$ on phylogenetic trees, Bayesian phylogenetic inference aims at properly estimating the posterior distribution
\begin{equation}\label{eq:posterior}
p(\tau, \bm{q}|\bm{Y}) = \frac{p(\bm{Y}|\tau,\bm{q})p(\tau,\bm{q})}{p(\bm{Y})}\propto p(\bm{Y}|\tau,\bm{q})p(\tau,\bm{q}).
\end{equation}
There are several common phylogenetic analysis software for Bayesian phylogenetic inference such as MrBayes \citep{ronquist2012mrbayes}, BEAST \citep{drummond2007beast}, etc., which implement Markov chain Monte Carlo (MCMC) runs to samples from the phylogenetic posterior.


\section{ARTree}\label{app:artree}
ARTree \citep{xie2024artree} generates a tree topology in an autoregressive way. The sequential generating process in ARTree facilitates a probabilistic model over tree topologies which archives leading results in phylogenetic inference.
We introduce the tree topology generating process of ARTree as below and most of the statements are adapted from \citet{xie2024artree}.

Let $\tau_n = (V_n, E_n)$ be a tree topology with $n$ leaf nodes, where $V_n$ and $E_n$ represent the sets of nodes and edges, respectively.
A predetermined order, also known as the taxa order, is assumed for the leaf nodes $\mathcal{X} = \{x_1, \ldots, x_N\}$. We begin by providing the definition of ordinal tree topologies.


\begin{definition}[Ordinal Tree Topology]
Let $\mathcal{X}=\{x_1,\ldots,x_N\}$ be a set of $N(N\geq 3)$ leaf nodes. 
Let $\tau_n=(V_n,E_n)$ be a tree topology with $n$ $(n\leq N)$ leaf nodes in $\mathcal{X}$. 
We say $\tau_n$ is an ordinal tree topology of rank $n$, if its leaf nodes are the first $n$ elements of $\mathcal{X}$, i.e., $V_n\cap \mathcal{X}= \{x_1,\ldots,x_n\}$. 
\end{definition}

The tree topology generation process begins with $\tau_3$ (the unique ordinal tree topology of rank 3). At the $n$-th step, assume we have an ordinal tree topology $\tau_n = (V_n, E_n)$ of rank $n$. The following steps are conducted to add the leaf node $x_{n+1}$ into $\tau_n$:
i) Select an edge $e_n = (u, v) \in E_n$, which is then removed from $E_n$. ii) Add a new node $w$ and two additional edges, $(u, w)$ and $(w, v)$, to the tree topology $\tau_n$. iii)Insert the new leaf node $x_{n+1}$ and add an additional edge $(w, x_{n+1})$ to the tree topology $\tau_n$.
Moreover, \citet{xie2024artree} provides theoretical guarantees on the bijectiveness between the tree topology and the decision sequence $D=(e_3,\ldots,e_{N-1})$.
Thanks for this bijectiveness, the distribution $Q(\tau)$ over tree topologies is modelled by $Q(D)$ over decision sequences, i.e.
\begin{equation}\label{eq-prob-decomp}
Q(\tau) = Q(D) = \prod_{n=3}^{N-1}Q(e_n|e_{<n}),
\end{equation}
where $e_{<n}=(e_3,\ldots,e_{n-1})$ and $e_{<3}=\emptyset$. 
We provide the illustration and algorithm for the generating process in Figure \ref{fig:artreeplot} and Algorithm \ref{alg:generation}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/gnnplot.pdf}
    \caption{An illustration of building tree topology autoregressive using ARTree. This figure is from \citet{xie2024artree}.}
    \label{fig:artreeplot}
\end{figure}

\begin{algorithm}[t]
\caption{ARTree for tree topology modeling \citep{xie2024artree}}
\label{alg:generation}
\KwIn{A set $\mathcal{X}=\{x_1,\ldots,x_N\}$ of leaf nodes.}
\KwOut{An ordinal tree topology $\tau$ of rank $N$; the ARTree probability $Q(\tau)$ of $\tau$.}
$\tau_3=(V_3,E_3) \leftarrow$ the unique ordinal tree topology of rank $3$\;
   \For{$n=3,\ldots,N-1$}{
Let $f_n(u) = c_u f_n(\pi_u)+d_u$ where $\pi_u$ is the parent of $u$\;
Calculate the probability vector $q_n\in \mathbb{R}^{|E_n|}$ using the current GNN model\;
Sample an edge decision $e_n$ from $\mathrm{\sc Discrete}\left(q_n\right)$ and assume $e_{n}=(u,v)$\;
Create a new node $w$\;
$E_{n+1} \leftarrow \left(E_n\backslash \{e_{n}\}\right)\cup \{(u,w), (w,v), (w,x_{n+1})\}$\;
$V_{n+1} \leftarrow V_n\cup \{w,x_{n+1}\}$\;
$\tau_{n+1}\leftarrow (V_{n+1}, E_{n+1})$\;
}
 $\tau\leftarrow \tau_N$\;
 $Q(\tau)\leftarrow q_3(e_3)q_4(e_4)\cdots q_{N-1}(e_{N-1}).$
\end{algorithm}



\section{Details of the encoding mechanism}\label{sec:app-encoding}
\subsection{Proof of Theorem \ref{thm:encoding-linear-time}}\label{sec:app-linear-time-proof}
\begin{reptheorem}{thm:encoding-linear-time}
Given a tree topology $\tau$ with $N$ leaf nodes, the time complexity of computing its encoding vector $\bm{s}(\tau)$ is $O(N)$.
\end{reptheorem}

\paragraph{Proof of Theorem \ref{thm:encoding-linear-time}}
Assume the tree topology $\tau=(V,E)$ is stored in the binary tree data structure, where each node other than the root node also has a parent node pointer (the root node is arbitrarily selected from the internal nodes for the storage aim).
Before computing the encoding of $\tau$, we first perform a postorder traversal over $\tau$ and construct an index dictionary $D$ of the mappings $\{(n,x_n)\}_{n=1}^N \cup \{(k,v)\}_{N<k<2N-1,v\in V\backslash\mathcal{X}}$, whose time complexity is $O(N)$.

In the $n$-indexed step of the decomposition loop, we want to remove the leaf node $x_n$, which can be located by indexing the dictionary $D$ in $O(1)$ time.
Following the $x_n$'s parent node pointer, we reach $x_n$'s parent ${t_n}$.
We then detach $x_n, {t_n}$ from the tree and connect ${t_n}$'s remaining two neighbors $u_n,v_n$ whose indexes are $k_n^1, k_n^2$.
Since all these local modifications of tree topologies can be done in $O(1)$ time, the time complexity of the decomposition loop is $O(N)$.

In the $n$-indexed step of the reconstruction loop, we have to locate the edge $(u_n,v_n)$ to add the node $x_n$.
This can be done in $O(1)$ time by indexing $k_n^1, k_n^2$ in $D$.
Afterwards, we de-connect $u_n,v_n$ and add ${t_n}$, $x_n$ to the graph.
Since all these local modifications of tree topologies can be done in $O(1)$ time, the time complexity of the reconstruction loop is $O(N)$.
Please note that the indexes in dictionary $D$ are different from the $\mathrm{Index}(\cdot)$ function.

Therefore, the time complexity of computing the encoding vector $\bm{s}(\tau)$ is $O(N)$.

\subsection{From encodings to tree topologies}\label{sec:app-encoding-generative}
The encoding mechanism also provides a mapping from encodings to tree topologies, which is summarized in Algorithm \ref{alg:inverse-encoding}.
Algorithm \ref{alg:inverse-encoding} is similar to the reconstruction loop in Algorithm \ref{alg:edge-indexing}, and has time complexity $O(N)$.

\begin{algorithm}[t]
\caption{Converting encoding vectors to tree topologies}
\label{alg:inverse-encoding}
\KwIn{A sequence $\bm{s}=(s_3,\ldots,s_{N-1})\in \mathbb{N}^{N-3}$.}
\KwOut{A tree topology $\tau$ with $N$ leaf nodes.}
$\tau_3\leftarrow$ the unique tree topology with the first three leaf nodes $\{x_1,x_2,x_3\}$\;
$\mathrm{Index}(x_1)\leftarrow 1; \mathrm{Index}(x_2)\leftarrow 2; \mathrm{Index}(x_3)\leftarrow 3$; $r\leftarrow$ the unique internal node of $\tau_3$\;
\For{$n=3,\ldots,N-1$}{
$v_n\leftarrow$ the node in $\tau_n$ that satisfies $\mathrm{Index}(v_n)=s_n$\;
$u_n\leftarrow$ the neighbor node of $v_n$ (towards the direction of $r$) in $\tau_n$\;
Create an node ${ t_{n+1}}$\;
Attach the pendant edge $({ t_{n+1}},x_{n+1})$ to $(u_n,v_n)$\;
$\mathrm{Index}({ t_{n+1}})\leftarrow N+n-1$; $\mathrm{Index}(x_{n+1})\leftarrow n+1$\;
}
\end{algorithm}


\section{Experimental details}\label{sec:app-experimental-details}
For all experiments, PhyloVAE is implemented in PyTorch \citep{Paszke2019PyTorchAI}. The optimizer is Adam \citep{ADAM} with parameters $(\beta_1,\beta_2)=(0.9, 0.999)$ and $\texttt{weight\_decay}=0.0$.
The results are collected after 200000 iterations with batch size $B=10$.

\subsection{Five-leaf tree topologies}\label{sec:app-setting-five-leaf}
\begin{wrapfigure}[7]{r}{.3\textwidth}
\vspace{-1cm}
\includegraphics[width=\linewidth]{figures/five-leaf-illustration.pdf}
\caption{An illustration of the partition presentation (left) of a tree topology (right) containing five leaves \{A,B,C,D,E\}.
}
\label{fig:five-leaf-illustration}
\end{wrapfigure}
The dimension of the latent space is set to $d=2$. The generative model is a three-layer MLP with 512 hidden units and a ResNet architecture.
For the inference model, the number of message passing rounds is $L=2$, and both $\mathrm{MLP}_{\mu}$ and $\mathrm{MLP}_{\sigma}$ are composed of a two-layer MLP with 100 hidden units.
The number of particles in the multi-sample lower bound (\ref{eq:treevae-mlb}) is $K=32$.
The learning rate is set to 0.0003 at the beginning and anneals according to a cosine schedule.
The experiments are run on a single 2.4 GHz CPU.

\subsection{Eight-leaf tree topologies}
The dimension of the latent space is set to $d=2$. The generative model is a three-layer MLP with 512 hidden units and a ResNet architecture.
For the inference model, the number of message passing rounds is $L=2$, and both $\mathrm{MLP}_{\mu}$ and $\mathrm{MLP}_{\sigma}$ are composed of a two-layer MLP with 100 hidden units.
The number of particles in the multi-sample lower bound (\ref{eq:treevae-mlb}) is $K=32$.
The learning rate is set to 0.0003 at the beginning and anneals according to a cosine schedule.
The experiments are run on a single 2.4 GHz CPU.

The three pre-selected tree topologies for constructing the three peaked distributions are plotted in Figure \ref{fig:8taxa-gttree}.
Here are definitions of the Robinson-Foulds (RF) distance and the path difference (PD) distance between two tree topologies (the edge lengths are not considered).

\begin{definition}[Robinson-Foulds distance; \citet{robinson1981comparison}]
\label{def:rf-distance}
For a tree topology $\tau=(V,E)$ with leaf nodes $\mathcal{X}$, an edge $e\in E$ divides the leaf nodes into two parts $(\mathcal{X}_1,\mathcal{X}_2)$, according to the closeness between a leaf node and the right/left terminal node of $e$.
We define the unordered partition as $P_e:=(\mathcal{X}_1,\mathcal{X}_2)$ and all the partitions on $\tau$ as $\mathcal{P}_{\tau}=\{P_e:e\in E\}$.
The the Robinson-Foulds distance between two tree topologies $\tau_1$ and $\tau_2$ is defined as
\[
D_{\textrm{Robinson-Foulds}}(\tau_1,\tau_2) := |\mathcal{P}_{\tau_1}\Delta\mathcal{P}_{\tau_2}|=|\mathcal{P}_{\tau_1}\backslash\mathcal{P}_{\tau_2}|+|\mathcal{P}_{\tau_2}\backslash\mathcal{P}_{\tau_1}|.
\]
\end{definition}
\begin{definition}[Path difference distance; \citet{steel1993pathdifference}]
\label{def:pl-distance}
Let $\tau$ be a tree topology with $N$ leaf nodes $\mathcal{X}$.
The path length $l_{x_i,x_j}(\tau)$ between two leaf nodes $x_i,x_j$ is defined as the minimum number of edges on $\tau$ for connecting $x_i$ and $x_j$.
The the path length difference distance between $\tau_1$ and $\tau_2$ is
\[
D_{\textrm{path-difference}}(\tau_1,\tau_2) := \sum_{1\leq i< j \leq N}|l_{x_i,x_j}(\tau_1)-l_{x_i,x_j}(\tau_2)|.
\]
\end{definition}

\begin{figure}[t]
\centering
\subfigure[The pre-selected tree topology for the first peak.]{
    \includegraphics[width=0.3\textwidth]{figures/8taxatree1.pdf}
    \label{fig:8taxa-first}
}
\subfigure[The pre-selected tree topology for the second peak.]{
    \includegraphics[width=0.3\textwidth]{figures/8taxatree2.pdf}
    \label{fig:8taxa-second}
}
\subfigure[The pre-selected tree topology for the third peak.]{
    \includegraphics[width=0.3\textwidth]{figures/8taxatree3.pdf}
    \label{fig:8taxa-third}
}
\caption{Pre-selected tree topologies for constructing the three-peak distribution on the 10395 tree topologies with 8 leave nodes.}
\label{fig:8taxa-gttree}
\end{figure}

\begin{figure}[ht]
\centering
    \includegraphics[width=0.8\linewidth]{figures/murphytree.pdf}
    \caption{The ground truth phylogenetic tree for early mammal evolution \citep{murphy2001}.
    }
    \label{fig:murphytree}
\end{figure}

\subsection{Gene trees and sequence lengths}\label{sec:app-setting-gene-tree}
The dimension of the latent space is set to $d=2$. The generative model is a three-layer MLP with 512 hidden units and a ResNet architecture.
For the inference model, the number of message passing rounds is $L=2$, and both $\mathrm{MLP}_{\mu}$ and $\mathrm{MLP}_{\sigma}$ are composed of a two-layer MLP with 100 hidden units.
The number of particles in the multi-sample lower bound (\ref{eq:treevae-mlb}) is $K=32$.
The learning rate is set to 0.0003 at the beginning and anneals according to a cosine schedule.
The experiments are run on a single 2.3 GHz CPU.

\begin{table}[t]
    \caption{The evolutionary model for each gene in the early mammal evolution analysis \citep{murphy2001, hillis2005analysis}.
    }
    \label{tab:gene-evolutionary-model}
    \vskip0.5em
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccccccccccc}
    \toprule
    \multirow{2}{*}{Gene}&\multirow{2}{*}{Preferred model} & \multicolumn{4}{c}{Base frequencies} & \multicolumn{6}{c}{Relative substitution rates} 
 &\multirow{2}{*}{Pinv} & \multirow{2}{*}{Alpha} \\
\cmidrule(l){3-6}\cmidrule(l){7-12}
       &&A&C&G&T& AC&AG&AT&CG&CT&CG&& \\
       \midrule
ADORA3&K2P&0.25&0.25&0.25&0.25& 1&3&1&1&3&1&0 &-\\
APP&GTR + I + $\Gamma$&0.25&0.24&0.18&0.33&1.6&3.66&0.47&0.72&2.65&1&0&0.78\\
ZFX&HKY + I + $\Gamma$&0.35&0.23&0.18&0.23&1&7.94&1&1&7.94&1&0.49&1.24\\
IRBP&GTR + I + $\Gamma$&0.21&0.3&0.3&0.18&1.5&4.91&1.34&0.83&5.8&1&0.18&1.04\\
mtRNA&GTR + I + $\Gamma$&0.34&0.2&0.21&0.25&5.86&14&3.85&0.58&29.3&1&0.41&0.53\\
    \bottomrule
    \end{tabular}
    }
\end{table}

The evolutionary models of the five genes of the early mammal evolutionary analysis are in Table \ref{tab:gene-evolutionary-model}.
For each gene, we simulate the DNA sequences along the ground truth phylogenetic tree in Figure \ref{fig:murphytree}.
We also give the Newick representation of this ground truth tree (from \citet{hillis2005analysis}) for reproducing the results:

\texttt{
((Opossum: 0.072454, Diprotodontian: 0.061694):0, ((((Sloth:
0.056950, Anteater: 0.061637):0.009169, Armadillo: 0.056660):0.032179,
((((Hedgehog: 0.137379, Shrew: 0.124147):0.011789, Mole: 0.086828):
0.011954, (((Phyllostomid: 0.093178, Free tailed bat: 0.046665):0.011564,
(False vampire bat: 0.062583, (Flying Fox: 0.018553, Rousette Fruitbat:
0.018931):0.036729):0.004788):0.016400, ((((((Whale: 0.013788, Dolphin:
0.021978):0.019568, Hippo: 0.039894):0.004885, Ruminant: 0.073210):
0.008450, Pig: 0.067448):0.005893, Llama: 0.061851):0.027757, ((Horse:
0.043682, (Rhino: 0.028867, Tapir: 0.028638):0.005116):0.020583, ((Cat:
0.046372, Caniform: 0.055840):0.023068, Pangolin: 0.075956):0.003871):
0.001685):0.001155):0.002432):0.011058, (((Sciurid: 0.083962, ((Mouse:
0.042059, Rat: 0.045451):0.122018, (Hystricid: 0.074622, Caviomorph:
0.086677):0.062121):0.005432):0.011864, (Rabbit: 0.057873, Pika:
0.108683):0.043771):0.005743, ((Flying Lemur: 0.061380, Tree Shrew:
0.101818):0.003958, (Strepsirrhine: 0.076186, Human: 0.065099):
0.009553):0.001707):0.007711):0.009175):0.005977, ((((Tenrecid: 0.142758,
Golden Mole: 0.067180):0.009411, (Short Eared Elephant Shrew:
0.039055, Long Eared Elephant Shrew: 0.036033):0.088816):
0.002240, Aardvark: 0.068518):0.003248, ((Sirenian: 0.038154, Hyrax:
0.089482):0.002916, Elephant: 0.050883):0.014801):0.025967):0.284326)
}

\subsection{Multiple phylogenetic analyses comparison}\label{sec:app-setting-multiple-analyses}
The dimension of the latent space is set to $d=2$. The generative model is a five-layer MLP with 512 hidden units and a ResNet architecture.
For the inference model, the number of message passing rounds is $L=2$, and both $\mathrm{MLP}_{\mu}$ and $\mathrm{MLP}_{\sigma}$ are composed of a two-layer MLP with 512 hidden units.
The number of particles in the multi-sample lower bound (\ref{eq:treevae-mlb}) is $K=32$.
The learning rate is set to 0.00001 at the beginning and anneals according to a cosine schedule.
The experiments are run on a single 2.3 GHz CPU.

\subsection{Generative modeling capacity on benchmark data sets}\label{sec:app-setting-benchmark}
Following \citet{Zhang2018SBN, xie2024artree}, we construct the training data set and the ground truth as follows: (i) for each sequence set, there are 10 replicate training sets of tree topologies which are gathered from 10 independent MrBayes runs until the runs have ASDSF (the standard convergence criteria used in MrBayes) less than 0.01 or a maximum of 100 million iterations (tree topologies are sampled every 100 iterations with the first 25\% iterations discarded);
(ii) for each sequence set, the ground truth of tree topologies is gathered from 10 single-chain MrBayes for one billion iterations (tree topologies are sampled every 1000 iterations with the first 25\% iterations discarded).
The numbers of unique tree topologies in the training set and the ground truth for each sequence set are reported in Table \ref{tab:tde}.

We use the same model architecture and training strategy on all DS1-8. The generative model is a five-layer MLP with 512 hidden units and a ResNet architecture.
For the inference model, the number of message passing rounds is $L=2$, and both $\mathrm{MLP}_{\mu}$ and $\mathrm{MLP}_{\sigma}$ are composed of a two-layer MLP with 100 hidden units.
The number of particles in the multi-sample lower bound (\ref{eq:treevae-mlb}) is $K=32$ in Table \ref{tab:tde} and Figure \ref{fig:tde-time}.
The learning rate is set to 0.0003 at the beginning and anneals according to a cosine schedule.
The experiments are run on a single NVIDIA RTX 2080Ti GPU.

\section{Additional experimental results}

\subsection{Gene trees and sequence lengths}\label{sec:app-results-gene-trees}
Figure \ref{fig:mammal-gene-trees-5000} depicts the latent representations of the mammal gene trees inferred from DNA sequences with a length of 5000. We see that compared to shorter DNA sequences (Figure \ref{fig:real-phylogeny}), phylogenetic inference with longer DNA sequences better discovers the ground truth tree.
Again, the ADORA3 gene performs best for phylogenetic discovery, as the gene trees collapse to the ground truth tree.
The MDS plots in the same setting are shown in Figure \ref{fig:mds-mammal-gene-trees-5000}, where we see again that the tree topologies within a group tend to collapse towards the origins and are not sufficiently distinguished.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/mammal-gene-trees-5000.pdf}
\caption{Latent representations obtained by PhyloVAE for the mammal gene trees inferred from DNA sequences with a length of 5000.
}
\label{fig:mammal-gene-trees-5000}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/mds-mammal-gene-trees-5000.pdf}
    \caption{MDS plots for the mammal gene trees inferred from DNA sequences with a length of 5000.}
    \label{fig:mds-mammal-gene-trees-5000}
\end{figure}

\subsection{Multiple phylogenetic analyses comparison}\label{sec:app-results-multiple-analyses}
The right plot of Figure \ref{fig:real-phylogeny} provides an example that multiple phylogenetic analyses do not converge.
Here we provide an example that multiple phylogenetic analyses quickly converge.

We consider the fossil data sets \citep{wright2017fossil} which contain 42 sequences (i.e., 42 leaf nodes) with a length of 87, which describes the biological characters of 42 types of Paleozoic crinoids.
We assume the JC evolutionary model, implement two independent MrBayes \citep{ronquist2012mrbayes} runs for 10 million iterations, and gather the tree samples per 1000 iterations from the last 3 million iterations.
These 6000 tree topologies with uniform weights constitute the training set of PhyloVAE.

We find that 6000 tree topologies in the training set are distinct, which reflects the diffuse posterior of fossil phylogenies.
Despite this, the latent representations of tree topologies from the two independent MrBayes runs seem to have the same distribution.
This implies the underlying convergence of the two MrBayes runs.

\begin{figure}[ht]
\centering
    \includegraphics[width=0.65\linewidth]{figures/fossil-trees.pdf}
    \caption{Latent representation of two groups of tree topologies gathered from independent MrBayes runs on the fossil data sets.
    }
    \label{fig:fossil-trees}
\end{figure}


\subsection{Generative modeling on benchmark data sets}\label{sec:app-DS}


\begin{table}[t]
\centering
\begin{minipage}{0.49\linewidth}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc}
\toprule
KL divergence & $K=1$ & $K=16$ & $K=32$ & $K=64$ \\
\midrule
$d=2$ & 0.1275& 0.0308& 0.0273 &0.0264 \\
$d=5$ & 0.0951& 0.0182&0.0177&0.0166\\
$d=10$& 0.0997& 0.0230 & 0.0189 &0.0175\\
\bottomrule
\end{tabular}
}
\vspace{-0.5em}
\caption*{(a) DS1}
\vspace{1em}
\end{minipage}
\begin{minipage}{0.49\linewidth}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc}
\toprule
KL divergence & $K=1$ & $K=16$ & $K=32$ & $K=64$ \\
\midrule
$d=2$ &0.0202&0.0097&0.0100&0.0097 \\
$d=5$ &0.0202&0.0103&0.0099&0.0107 \\
$d=10$&0.0202&0.0107&0.0098&0.0103 \\
\bottomrule
\end{tabular}
}
\vspace{-0.5em}
\caption*{(b) DS2}
\vspace{1em}
\end{minipage}

\begin{minipage}{0.49\linewidth}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc}
\toprule
KL divergence & $K=1$ & $K=16$ & $K=32$ & $K=64$ \\
\midrule
$d=2$ &0.0674&0.0482&0.0529&0.0559 \\
$d=5$ &0.1397&0.0461&0.0502&0.0532  \\
$d=10$&0.0980&0.0453&0.0477&0.0515 \\
\bottomrule
\end{tabular}
}
\vspace{-0.5em}
\caption*{(c) DS3}
\end{minipage}
\begin{minipage}{0.49\linewidth}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc}
\toprule
KL divergence & $K=1$ & $K=16$ & $K=32$ & $K=64$ \\
\midrule
$d=2$ &0.1038&0.0646&0.0619&0.0607 \\
$d=5$ &0.0995&0.0470&0.0467&0.0471 \\
$d=10$&0.1082&0.0470&0.0469&0.0460\\
\bottomrule
\end{tabular}
}
\vspace{-0.5em}
\caption*{(d) DS4}
\end{minipage}
\caption{KL divergence of PhyloVAE to the ground truth on DS1-4 with varing $d$ and $K$.}
\label{tab:ablation}
\end{table}

\begin{figure}[ht]
\centering
    \includegraphics[width=0.65\linewidth]{figures/DS1KL.pdf}
    \caption{Tree probability estimates obtained by PhyloVAE v.s. ground truth on DS1.
    }
    \label{fig:DS1-KL}
\end{figure}

{Table \ref{tab:ablation} reports the KL divergence obtained by different choices of $d$ and $K$ on DS1-4.
We see that increasing $K$ can generally improve the approximation accuracy, while sometimes a large $d$ may increase the training difficulty and lead to overfitting.}

{Figure \ref{fig:DS1-KL} compares the probability estimates and the ground truth. We see that PhyloVAE provides reliable probability estimation.}

Figure \ref{fig:DS-trees-representation} shows latent representations of the tree topologies in the training set (repository 1) of DS1-8 produced by PhyloVAE.
\begin{figure}[ht]
\centering
    \includegraphics[width=\linewidth]{figures/DS-trees-representation.png}
    \caption{Latent representation of the tree topologies in the training set (repository 1) of DS1-8.
    The scatter size is proportional to the probability of the tree topology.
    }
    \label{fig:DS-trees-representation}
\end{figure}

To more comprehensively evaluate the efficiency of PhyloVAE, we report the memory usage of it and the baseline method ARTree in Table \ref{tab:memory}.

\begin{table}[ht]
\caption{Memory usage (MB) of running ARTree and PhyloVAE ($d=10, K=32$) on DS1-8.}
\label{tab:memory}
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
Data set & DS1&DS2&DS3&DS4&DS5&DS6&DS7&DS8\\
\midrule
ARTree&400.37&428.79&518.84&600.50&784.32&778.05&932.56&1067.93\\
PhyloVAE ($d=10,K=32$)&635.67&637.72&655.69&785.75&689.15&701.30&784.41&818.72\\ 
\bottomrule
\end{tabular}
}
\end{table}


\end{document}
