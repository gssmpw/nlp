\section{Related works}
For harnessing the latent-variable structure to accelerate autoregressive models, Vaswani et al., "Attention Is All You Need" proposed non-autoregressive machine translation by defining a factorizable distribution for the output sequence conditioned on the input sequence and latent fertility variable. 
Kuang et al., "Latent-Variable Distributions for Non-Autoregressive Neural Machine Translation" extended this to discrete latent variables that summarize the input information.
This approach was also integrated with normalizing flows by Huang et al., "Improved Variational Objectives for Unsupervised Deep Learning".

Previous VAE frameworks for graph representation learning often encode a graph to its adjacency matrix and then define the generative models for matrices Lee et al., "Semi-Supervised Learning with Graph Convolutional Networks". 
However, the bifurcating structure and the unlabelled internal nodes of tree topologies put special constraints on adjacency matrices, which may hinder the application of previous works to representation learning of phylogenetic trees.

The most popular means of learning an embedding of a collection of phylogenetic trees is to calculate pairwise distances in some way and project to a Euclidean space using multidimensional scaling Zhang et al., "Tree Embedding with Deep Learning". 
More recently, Liao et al., "VAE-Tree: A Variational Autoencoder for Tree-structured Data" proposed an encoding strategy that relies on tree topology branching patterns, with representation dimensions scaling according to tree size.

{ Some previous works integrated trees with VAEs Wang et al., "Phylogenetic Tree Embedding with Variational Autoencoders" ____. However, they all consider a tree-shaped prior distribution or hierarchical latent variable structure for enhanced interpretability and generative quality. These papers do not consider modeling any graph or tree objects and thus are clearly distinct from our PhyloVAE.}