\section{Related work}
\subsection{LLM for Mental Health}
Recent advancements in Large Language Models have significantly enhanced mental health applications \cite{related1, related2} by enabling automated psychological assessment \cite{related3}, real-time emotional support \cite{related4}, and crisis intervention. LLM-powered chatbots, such as Woebot \cite{related11} and Wysa\cite{related12}, provide cognitive behavioral therapy (CBT)-based dialogues, offering users coping strategies and emotional validation. Research has shown that LLMs can analyze text-based markers of mental distress \cite{related14}, detect early signs of depression or anxiety \cite{related15}, and assist in suicide prevention by identifying high-risk individuals in social media posts \cite{related13}. Additionally, they support clinicians by summarizing therapy sessions, extracting patient insights, and assisting in treatment planning \cite{related16}. While these applications show promise, challenges remain in ensuring model reliability, reducing biases, and addressing ethical concerns \cite{related17} in AI-driven mental health care.

\subsection{Measuring Suicidality via Natural Language Processing}
Suicide prevention is a critical mental health challenge, and recent advancements in natural language processing and large language models have opened new possibilities for measuring suicidality. Transformer-based models like BERT and GPT \cite{related5,related6} have been used to classify suicide-related content, with some incorporating structured risk assessment scales such as the Columbia-Suicide Severity Rating Scale (C-SSRS) to improve detection accuracy. Beyond detection, LLMs have been applied in clinical decision support \cite{related7}, extracting insights from psychiatric evaluations \cite{related9} and unstructured medical notes \cite{related8} to assist in suicide risk assessment. Additionally, LLMs hold promise in synthetic data generation \cite{ghanadian2024socially}, which helps researchers overcome the challenge of low base-rate events like suicide by increasing sample sizes and statistical power. However, ethical challenges \cite{related10} such as privacy risks, misinterpretation, and safety concerns highlight the need for careful regulation and human-in-the-loop frameworks to ensure responsible deployment in clinical and research settings. 