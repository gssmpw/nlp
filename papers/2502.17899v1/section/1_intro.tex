\section{Introduction}
As large language models (LLMs) and LLM-powered artificial intelligence systems become increasingly integrated into daily life, people are developing emotional connections with these technologies. Such connections are formed across various contexts, including casual conversation~\cite{conversation, conversation2}, mental health support~\cite{mentalhealth}, and companionship~\cite{mentalhealth2}. Undoubtedly, LLMs are transforming the way people conduct psychological research and seek emotional support, comfort, and assistance~\cite{lai2023psy,stade2023artificial}. However, although LLMs can show a sense of responsiveness and understanding towards humans~\cite{li2023large,s24155045}, significant challenges remain to apply them to these sensitive scenarios, as issues such as hallucinations, biases, and a lack of alignment with human values continue to hinder their reliability~\cite{chung2023challenges}.

Suicide remains one of the most pressing public health challenges worldwide, with particularly devastating impacts on adolescents and young adults~\cite{MartinezAles2020}. As prevention efforts evolve in the digital age, the increasing integration of LLMs into mental health support systems raises critical questions about their role and impact on \textit{suicide prevention and intervention}. A key concern is whether individuals experiencing suicidal ideation who seek support from LLMs might not only fail to receive appropriate intervention but could potentially face an increased self-harm risk, as exemplified by a tragic case that a 14-year-old teenager committed suicide, allegedly influenced by a chatbot character on an LLM platform.\footnote{ See \href{https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html}{New York Times article}.}

\input{figures/intro_example}

Despite recent advances in the ability of LLMs to detect explicit suicide-related content on social media posts~\cite{nguyen2024leveraging,Levkovich2024}, identifying real-world suicidal ideation remains challenging. \citet{suicide1} show that individuals experiencing suicidal thoughts often express their distress through subtle, abstract, and metaphorical expressions rather than direct statements. Existing benchmarks for identifying suicidal ideation, which mainly rely on predefined danger signals and specific social contexts~\cite{ATMAKURU2025102673}, often fail to capture these nuanced expressions. As a result, the ability of LLMs to detect suicide risk remains inadequately assessed. For instance, as illustrated in Figure~\ref{fig:introexample}, while LLMs can appropriately respond to \textit{explicit} suicide-related posts, they frequently fail to identify these subtle \textit{implicit} suicidal ideation and worsen the situation or even unintentionally encourage individuals toward self-harm.

In this work, we aim to comprehensively evaluate LLMs' capabilities in two critical aspects of suicide prevention: (i) the \textit{\textbf{I}dentification of \textbf{I}mplicit \textbf{S}uicidal ideation} (\textbf{$\text{IIS}$}) across diverse expressions and contexts, and (ii) \textit{the \textbf{P}rovision of \textbf{A}ppropriate \textbf{S}upportive responses} (\textbf{$\text{PAS}$}) that demonstrate empathy and encourage help-seeking behaviors. To achieve this, we introduce \ourdata, a novel dataset designed to assess both $\text{IIS}$ and $\text{PAS}$ of LLMs. Different from existing benchmarks only cover social media posts or electronic health record~\cite{redditdata,rawat-etal-2022-scan}, which are insufficient for evaluating the LLMs' ability in implicit expressions, we establish our dataset based on Death/Suicide Implicit Association Test (\textbf{D/S-IAT})~\cite{test4,test} and Negative Automatic Thinking~\cite{Batmaz2015}. These foundational studies from human assessment demonstrate that suicidal intent often manifests at a subconscious level through specific \textit{cognitive patterns}, such as associations between self-related and death-related descriptions, as well as distinct \textit{thinking patterns} like All-or-Nothing Thinking, where individuals perceive situations in absolute, binary terms. Leveraging these patterns alongside psychosocial stressors described by \citeauthor{ghanadian2024socially}, such as financial crises and relationship problems, and real-world scenarios extracted from the Reddit posts dataset~\cite{redditdata}, we developed \ourdata, which comprises 1308 implicit suicidal test cases. 

Also, contrary to previous work, we not only evaluate the LLMs' ability to identify Suicidal Ideation but also present a novel evaluation framework for assessing whether LLMs can provide appropriate supportive responses that demonstrate understanding and guidance to individuals exhibiting suicidal tendencies. Based on our \ourdata and evaluation pipeline, we conduct a comprehensive empirical study involving 8 widely-used LLMs under two experimental settings: (1) ``No warning,'' where the LLM has no prior knowledge of the user's suicidal ideation, and (2) ``Warning,'' where the LLM is informed of potential suicidal tendencies. Our data construction and evaluation methodology are illustrated in Figure~\ref{fig:overview}. The extensive results reveal that LLMs face significant challenges in accurately identifying implicit suicidal ideation.

% Overall, our contributions are concluded as follows:
% \begin{itemize}
%     \item We construct a rich and diverse implicit suicidal ideation dataset \ourdata c
%     omprising approximately 1,308 texts generated through a structured framework (DeepSuiMind) that integrates cognitive patterns (D/S-IAT), thinking patterns (ANT), and a wide range of real-world scenarios, psychological disorders, and sociopsychological triggers. Our dataset serves as a benchmark for evaluating the performance of AI models in understanding and responding to subtle, metaphorical, or context-dependent suicidal thoughts.
    

%     \item We propose a comprehensive evaluation framework grounded in suicide intervention guidelines, including the assessment of response quality based on scoring of suicide risk signal detection and intervention criteria and a comparative analysis of baseline and context-aware responses (with additional warnings) and human review for the evaluation. This ensures a robust assessment of model performance in understanding and responding to suicidal ideation text.
%     \item 
% \end{itemize}



% concerns \cite{risk} arise about LLMs' ability to interpret implicit emotional signals, particularly when negative emotions are conveyed through abstract or metaphorical expressions.
% Unlike humans, who can intuitively grasp subtle shifts in tone, indirect phrasing, and cultural nuances, AI models rely on explicit linguistic cues and statistical patterns \cite{suspect,suspect2}, making them prone to misinterpretation. 


% This limitation is particularly concerning in high-risk and sensitive mental health contexts, such as suicide prevention and early intervention for psychological distress. The tragic incident \cite{test3} that occurred in 2023 has already sounded the alarm, where an AI chatbot misinterpreted and provided responses that inadvertently reinforced a teenager's suicidal ideation. Research \cite{suicide1} has shown that individuals with suicidal ideation often express distress in subtle, abstract, and metaphorical ways. Cognitive network analyses of suicide notes reveal that suicidal individuals exhibit emotional compartmentalization and different emotional language structures from non-suicidal individuals. Additionally, Implicit Association Tests (IAT) \cite{test,test4}have demonstrated that suicidal intent often operates at a subconscious level, manifesting through implicit associations between self-identity and suicide-related concepts \cite{test2}.

% However, existing work on AI-driven suicide detection might be inherently limited, because it has primarily focused on supervised learning approaches trained on explicitly suicide-related content, often grounded on specific social backgrounds and danger signals. Also without annotations that differentiate overt suicidal statements from implicit ideation, limiting their applicability to capture implicit distress signals, such as metaphorical language, fragmented thoughts, or indirect expressions of hopelessness. In addition, some studies have attempted synthetic data augmentation, but they mainly focus on simulating stress-related social issues and ignore the implicit cognitive complexity that reflects real-world suicidal discourse. To address this gap, we introduce a novel implicitly suicidal discourse dataset. This dataset serves as a crucial benchmark for evaluating AI performance in recognizing subtle, abstract, and cognitively embedded distress signals.  

% To address this gap, we introduce a novel implicitly suicidal discourse dataset. This dataset serves as a crucial benchmark for evaluating AI performance in recognizing subtle, abstract, and cognitively embedded distress signals. 


