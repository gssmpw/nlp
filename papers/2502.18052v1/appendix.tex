% \section{Broader Implications}
\section{Proofs} \label{appx:proofs}
\paragraph{Notations.}
In the proofs below, we may use the multiple notations for \emph{partial discrepancy} interchangeably, namely $\delta_{ij} = \delta_{h_i}(h_j)$.
This is for readability and ease of portrayal.
Similarly, in place of $\mu$ (market share), sometimes there can be written $U$, for utility. These terms are also interchangeable. Lastly, the terms \quot{player} and \quot{provider} are interchangeable as well, since both refer to the learner.
\\

Before delving in to the proofs of the individual statements from the main paper, we will start with showing a helpful observation that sits at the crux of competitive dynamics, and will help with many of the proofs below.
\begin{observation}
\label{obs:rel_between_acc_and_deltas}
    For any 2 classifiers $h_i,h_j$ with accuracies $a_i, a_j$, it stands: $ a_i - \delta_{ij} = a_j - \delta_{ji}$.
\end{observation}
\begin{proof}
    Visual proof by  building a confusion matrix split up by accuracy on the label:
    \renewcommand{\arraystretch}{2}
    \begin{table}[h]
    %\caption{Distribution of predictions between 2 classifiers}
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{\blue{$h_j = y$}}& \multicolumn{1}{c}{\blue{$h_j \ne y$}}&  \multicolumn{1}{c}{}  \\
     \cline{2-3}
    \red{$h_i=y$} & $a_i-\delta_{ij}$ & $\delta_{ij}$ & \green{$ =a_i$}\\
    \cline{2-3}
    \red{$h_i \ne y$} & $\delta_{ji}$ & $1- a_i-\delta_{ji}$ & \green{$=1-a_i$} \\
     \cline{2-3}
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{\green{$=a_j$}}& \multicolumn{1}{c}{\green{$=1-a_j$}}&  \multicolumn{1}{c}{}
\end{tabular}
    \end{center}
    \label{tab:dist_of_preds}
\end{table}

Explanation of the table:
\begin{itemize}
     \item the cells are partitioned by buckets according to the correctness of the classsifiers. This helps us understand the discrepancy and overlap of accurate predictions between the classifiers.
    \item  We can immmediately observe that the 1st row sums to $a_i$, and the 1st column sums to $a_j$. Similarly, the 2nd row and 2nd column sum up to $1-a_i, 1- a_j$, repsectively. So too, $d_{ij}$ and $\delta_{ji}$ are in the top right and bottom left cell by definition. 
\end{itemize}
From the sums of the rows and columns, we observe the following equality from the top-left cell:
    \begin{equation}
    \label{eq:tying_accuracies_and_deltas}
        a_i - \delta_{ij} = a_j - \delta_{ji}
    \end{equation}
\end{proof}

\paragraph{\Cref{prop:utils_of_players} (Market Share).}
\begin{proof}
    from the definition of our problem setting, classifier $h_j$ gets: 
    \begin{itemize}[leftmargin=0.5cm]
        \item Full market share on the points that only $h_j$ is correct on: $\frac{1}{n} \sum_{i=1}^n \one{h_j(x) = y_i \land h_i(x) \ne y_i}$
        \item Half market share on the points that they are both correct on: $\frac{1}{2}\cdot\frac{1}{n} \sum_{i=1}^n \one{h_j(x) = y_i \land h_i(x) = y_i}$
    \end{itemize}
    By the definition of partial discrepancy (Eq.~\ref{eq:discrepancy}), the first bullet is exactly $\delta_{h_j}(h_i)$, and the second bullet is equal to $\frac{1}{2}(a_j - \delta_{h_j}(h_i))$.
    
    We then receive: $U(h_j|h_i) = \delta_{h_j}(h_i) + \frac{1}{2}(a_j - \delta_{h_j}(h_i)) = \frac{1}{2}(a_j + \delta_{h_j}(h_i))$  
\end{proof}

\paragraph{\Cref{prop:acc_iff_ms} (Market share $\leftrightarrow$ accuracy).}
We will first prove a helpful claim:
\begin{helpful_claim}
\label{hc:higher_acc_is_higher_partial_delta}
    For any 2 classifiers $h_1,h_2$ i with accuracies $a_1,a_2$, respectively, it stands that 
    $\delta_{12} > \delta_{21} \Leftrightarrow a_1 > a_2$.
\end{helpful_claim}
\begin{proof}
     From \cref{obs:rel_between_acc_and_deltas}, 
    $a_{1} -\delta_{12} = a_2  - \delta_{21}$, meaning: $ a_{1}  = a_2 + \delta_{12} - \delta_{21} $.
    Therefore:
    \begin{equation*}
    \begin{split}
        a_1 > a_2 \Leftrightarrow & \  a_1  - a_2 \ge 0 \\ 
         \Leftrightarrow & \  a_2 + \delta_{12} - \delta_{21} - a_2 \ge 0 \\ 
        \Leftrightarrow & \ \delta_{12} - \delta_{21} \ge 0 \\
        \Leftrightarrow & \ \delta_{12} \ge \delta_{21} 
    \end{split}
    \end{equation*}
    Where the 2nd inequality comes from substituting $a_1 =  a_2 + \delta_{12} - \delta_{21} $
\end{proof}

We will now prove \cref{prop:acc_iff_ms}:
\begin{proof}
    From \cref{prop:utils_of_players}, We know:
    $\mu(h_1|h_2) = a_{1} + \delta_{12}$, and 
    $\mu(h_2|h_{1}) = a_2 + \delta_{21}$.

    \begin{equation}
    \begin{aligned}        
     \quad & \mu(h_1 \mid h_2) > \mu(h_2 \mid h_1) 
     \\
     \Leftrightarrow &
     \quad a_{1} + \delta_{12} > a_2 + \delta_{21} 
    \\
    \Leftrightarrow &
    \quad a_1 > a_2 
    \end{aligned}
    \end{equation}

    Where the last inequality follows from \cref{hc:higher_acc_is_higher_partial_delta}.
\end{proof}

\paragraph{\Cref{lem:2x2_pne}.}
We will start with a helpful claim:
\begin{helpful_claim}
\label{hc:pne_inequalities}
    Let $h_1,h_2 \in \mathcal{H}$.
    Then: 
    \begin{itemize}[leftmargin=0.5cm]
        \item $ U(h_1|h_2) > U(h_2|h_2) \Leftrightarrow \delta_{h_1} > \frac{1}{2}\delta_{h_2} $ 
        \item $ U(h_2|h_1) > U(h_1|h_1) \Leftrightarrow \delta_{h_2} > \frac{1}{2}\delta_{h_1} $ 
    \end{itemize}
    Additionally, the above inequalities hold \textbf{if and only if} $|\delta_{h_1} - \delta_{h_2}| < \frac{1}{3}(\delta_{12}+\delta_{21})$.   
\end{helpful_claim}

\begin{proof}
    From \cref{prop:utils_of_players} we know that $U(h_1|h_1) = \frac{1}{2}a_1,U(h_2|h_2) = \frac{1}{2}a_2, U(h_i|h_j) = \frac{1}{2}(a_i + \delta_{ij}) $.
Therefore,
    \begin{equation}
        \begin{aligned}
            U(h_1|h_2) > U(h_2|h_2) &\Leftrightarrow \frac{1}{2}(a_1 + \delta_{12}) > \frac{1}{2}(a_2) \\
            &\Leftrightarrow a_1 + \delta_{h_1} > a_1 + \delta_{h_2} - \delta_{h_1} \\
            &\Leftrightarrow  \delta_{h_1} > \delta_{h_2} - \delta_{h_1} \\
            &\Leftrightarrow  2\delta_{h_1} > \delta_{h_2} \\
            &\Leftrightarrow  \delta_{h_1} > \frac{1}{2}\delta_{h_2} \\
        \end{aligned}
    \end{equation}
    Where the substitution of the RHS in the 2nd line comes from \cref{obs:rel_between_acc_and_deltas}.
    
    Similarly, 
    \begin{equation}
        \begin{aligned}
            U(h_2|h_1) > U(h_1|h_1) &\Leftrightarrow a_2 + \delta_{h_2} > a_1 \\
            &\Leftrightarrow a_2 + \delta_{h_2} > a_2 + \delta_{h_1} - \delta_{h_2} \\
            &\Leftrightarrow  \delta_{h_2} > \delta_{h_1} - \delta_{h_2} \\
            &\Leftrightarrow  2\delta_{h_2} > \delta_{h_1} \\
            &\Leftrightarrow  \delta_{h_2} > \frac{1}{2}\delta_{h_1} \\
        \end{aligned}
    \end{equation}


Now assume that the inequalities hold, meaning $\delta_{h_1} > \frac{1}{2}\delta_{h_2}$ and $\delta_{h_2} > \frac{1}{2}\delta_{h_1}$. Then,

\begin{equation}
    \begin{aligned}[b]
        \delta_{h_1} > \frac{1}{2}\delta_{h_2} &\rightarrow  \delta_{h_1} + \delta_{h_2} > \frac{3}{2}\delta_{h_2} \\
        &\rightarrow  \delta_{h_2} < \frac{2}{3}(\delta_{h_1} + \delta_{h_2}) 
    \end{aligned}
\end{equation}
Similarly, $\delta_{h_1} < \frac{2}{3}(\delta_{h_1} + \delta_{h_2})$. This means that $\maxx{\delta_{h_1},}{\delta_{h_2}} < \frac{2}{3}(\delta_{h_1} + \delta_{h_2})$, and therefore $|\delta_{h_1} - \delta_{h_2} |< \frac{1}{3}(\delta_{h_1} + \delta_{h_2})$.

[Note that all derivations apply both ways, meaning if $|\delta_{h_1} - \delta_{h_2} |< \frac{1}{3}(\delta_{h_1} + \delta_{h_2})$, then  $\delta_{h_1} > \frac{1}{2}\delta_{h_2}$ and $\delta_{h_2} > \frac{1}{2}\delta_{h_1}$.]

\end{proof}

Using \cref{hc:pne_inequalities}, the proof of \cref{lem:2x2_pne} is almost immediate:
from \cref{obs:rel_between_acc_and_deltas} we know that $a_1 - a_2 = \delta_{12} - \delta_{21}$.

The 2 players will choose differing strategies in the 2x2 game if and only if
$U(h_1|h_2) > U(h_2|h_2) \ \textbf{and} \ U(h_2|h_1) > U(h_1|h_1)$, which holds if and only if $\delta_{h_1} > \frac{1}{2}\delta_{h_2} \textbf{and} \ \delta_{h_2} > \frac{1}{2}\delta_{h_1}$, which holds if and only if $|\delta_{h_1} - \delta_{h_2}| < \frac{1}{3}(\delta_{12}+\delta_{21})$.
Since we know $a_1 - a_2 = \delta_{12} - \delta_{21}$, this proves \cref{lem:2x2_pne}.

\paragraph{\Cref{thm:2x2_pne} (2x2 PNEs.)}
From the inequalities in \cref{lem:2x2_pne}, we receive the conditions for which the providers play anti-coordinated strategies.

If one of these inequalities doesn't hold, meaning either $U(h_1|h_2) < U(h_2|h_2)$ or $U(h_2|h_1) < U(h_1|h_1)$, then the dominant strategy is $h_1,h_2$, respectively, depending on which inequality does not hold.
\paragraph{\Cref{thm:mlr_br} (Threshold best-responses under MLR).}
\begin{proof}
    Firstly,we note that since $g$ is continuous\footnote{We note that the theorem holds for categorical distributions as well, where the provider will simply go to the nearest point $x: g(x) \ge \frac{1}{2}$ if to the left, or $x: g(x) \le 2$ if doing a best-response to the right.} and increasing strongly in $[a,b]$, $g^{-1}$ is well defined.
  
    We will split $[a,b]$ into sub-intervals $[a,h] , [h,b]$ and calculate the best response in each interval:

    Let $h$ be the strategy we are responding to.
    
    % The best response in  interval $[a,h]$:

    It is clear that for all strategies in $[a,h]$, the utility on all points in $[h,b]$ is constant, since the classification on those points is the same. 
    So within the interval $[a,h]$, the player is looking to maximize $U_{[a,h]}$.
    Similarly, when considering the best response in interval $[h,b]$,  we need only maximize $U_{[h,b]}$, since for all points in the interval $U_{[a,h]}$ is the same.
    
    
    Let $ U_{[a,b]}(h|h) = c$. 
    
    It stands that $\forall h_1 \in [a,h]$: $$U_{[a,b]}(h_1|h) -  c = \int_{h_1}^h f_1(x) - \frac{1}{2}f_0(x) dx$$
    
    This is a straightforward expression of the utility. Any points outside $[h_1,h]$ have an identical classification for both $h$ and $h_1$, and so the only difference in utility is found inside the interval $[h_1,h]$, which $h_1$ classifies as positive and $h$ classifies as negative. 
    Therefore the gain in utility is $\int_{h_1}^h f_1(x) dx$, but the loss on the negative points is $\frac{1}{2}\int_{h_1}^h 
     f_0(x) dx$, since the utility on those points would be otherwise shared with $h$.
     
    Similarly, $\forall h_1 \in [h,b]$: $$U_{[a,b]}(h_1|h) -  c = \int_{h}^{h_1} f_0(x) - \frac{1}{2}f_1(x) dx$$
    

    Therefore,  $BR_{[a,h]}(h) = \underset{h_1 \in [a,h]}{\argmax} \ U_{[a,b]}(h_1|h) =  \underset{h_1 \in [a,h]}{\argmax} \  \int_{h_1}^h f_1(x) - \frac{1}{2}f_0(x) dx $.
    
    And $BR_{[h,b]}(h) = \underset{h_1 \in [h,b]}{\argmax} \ U_{[a,b]}(h_1|h) =  \underset{h_1 \in [h,b]}{\argmax} \  \int_{h_1}^h f_0(x) - \frac{1}{2}f_1(x) dx $.

    We will divide into cases based on the value of $g(h)$:

    \underline{\textbf{Case 1: $1/2 \le g(h) \le 2$:}}

    We will calculate $BR_{[a,h]}(h)$.
    Since $g$ is strictly increasing in $[a,b]$, the value $f_0(x) - \frac{1}{2}f_1(x)$ is positive for all points $x$ where $g(x) \ge \frac{1}{2}$. 
    
    Therefore, $\forall h_1 < h_2 \in [a,h]$, if $g(h_1) \ge \frac{1}{2}$  then $ U_{[a,h]}(h_1|h) \ge U_{[a,h]}(h_2|h)$
    
    In this case: $BR_{[a,h]}(h)= \max\left(a,g^{-1}\left(1/2 \right)\right)$, since in the case where  $\forall h_1 \in [a,h], g(h_1) > \frac{1}{2}$, then the maximum utility is found at $a$. 
    
    Similarly, the same argument holds to derive that: $BR_{[h,b]}(h) = \max\left(b,g^{-1}\left(2 \right)\right) $
    
    
    \underline{\textbf{Case 2: $g(h) > 2 $:}}
    
    % In this case the best response in this interval is $h$
    % because the integral for any $h_1>h$ will always be decreasing.
    In this case, $\forall h_1 > h \ \text{it stands that} \ U_{[h,b]}(h_1|h) < U_{[h,b]}(h|h)$, since the value $f_0(x) - \frac{1}{2}f_1(x)$ is negative for all points in $[h,b]$.

    Therefore the best-response is found in the interval $[a,h]$, in which case the derivation from the previous case holds, and 
    so $BR_{[a,h]}(h)= \max\left(a,g^{-1}\left(1/2 \right)\right)$.
    
    \underline{\textbf{Case 3: $g(h) < 1/2$:}}
    
    % In this case the best response in this interval is $h$
    % because the integral for any $h_1<h$ will always be decreasing.
    In this case, $\forall h_1 < h \ \text{it stands that} \  U_{[a,h]}(h_1|h) < U_{[a,h]}(h|h)$, since the value $f_0(x) - \frac{1}{2}f_1(x)$ is negative for all points in $[a,h]$.

    Therefore the best-response is found in the interval $[h,b]$, in which case the derivation from the previous case holds, and 
    so $BR_{[h,b]}(h) = \max\left(b,g^{-1}\left(2 \right)\right) $.

    So across all cases, we find that in the interval $[a,b]$, there are only 2 possible best responses: 
    
    $\max\left(b,g^{-1}\left(2 \right)\right)$, and $\min\left(a,g^{-1}\left(1/2 \right)\right) $. 

   
\end{proof}

\paragraph{\Cref{corr:thresh-2x2_reduction}.}
\begin{proof}
    Immediate from the fact that the strategy space of both players can be reduced to the 2 candidate best-responses stipulated in \cref{thm:mlr_br}.
\end{proof}
\paragraph{\Cref{thm:generalized_br_1d} (General threshold best-responses).}
\begin{proof}
As in the proof of \cref{thm:mlr_br}, We will split $[a,b]$ into sub-intervals $[a,h] , [h,b]$ and calculate the set of possible best responses for each interval:

Let $h$ be the strategy/threshold we are responding to.
We will rewrite the explicit forms for $U_{[a,b]}$:

$\forall h_1 \in [a,h]$: $$U_{[a,b]}(h_1|h) -  c = \int_{h_1}^h f_1(x) - \frac{1}{2}f_0(x) dx$$

$\forall h_1 \in [h,b]$: $$U_{[a,b]}(h_1|h) -  c = \int_{h_1}^h f_0(x) - \frac{1}{2}f_1(x) dx$$
Where $c  = U_{[a,b]}(h|h)$.

As explained in the proof of \cref{thm:mlr_br}, to calculate the best response in $[a,h]$,it is sufficient to maximize $U_{[a,h]}$; and to calculate the best-response in $[h,b]$, it is sufficient to maximize $U_{[h,b]}$.

We will calculate the best response in $[a,h]$:
\begin{helpful_claim}
\label{hc:outside_interval_br_left}
    Let $(h_1, h_2)$ be any open interval in $[a,h]$ such that $\forall x \in (h_1,h_2) \ \text{it holds that} \ g(x) > \frac{1}{2}$.
    
    Then $\forall x \in (h_1, h_2] \to U_{[a,h]}(h_1|h) > U_{[a,h]}(x|h)$.
\end{helpful_claim}
 \begin{proof}
    Let  $x \in (h_1, h_2]$.
    
     Using the derivations above of relative market shares between thresholds, we will compare the market shares of $x$ and $h_1$:

     $U_{[a,h]}(h_1|h) - U_{[a,h]}(x|h) = \int_{h_1}^x f_1(u) - \frac{1}{2}f_0(u) du$.

    Since we are given that in the segment $[h_1,x], \ g > \frac{1}{2}$, then it folows that $\forall u, f_1(u) - \frac{1}{2}f_0(u) > 0$, and therefore the integral must be positive and hence $U_{[a,h]}(h_1|h) - U_{[a,h]}(x|h) > 0$.
     
 \end{proof}
\begin{helpful_claim}
\label{hc:inside_interval_br_left}
    Let $(h_1, h_2)$ be any interval in $[a,h]$ such that $\forall x \in (h_1,h_2) \ \text{it holds that} \ g(x) < \frac{1}{2}$.
        
    Then $\forall x \in [h_1, h_2) \to U_{[a,h]}(h_2|h) > U_{[a,h]}(x|h)$.
\end{helpful_claim}
\begin{proof}
     We will show that $U_{[a,h]}(h_2|h) - U_{[a,h]}(x|h) > 0$, in a similar manner to \cref{hc:outside_interval_br_left}:
     
    Let  $x \in (h_1, h_2]$.
    We will compare the market shares of $x$ and $h_1$:

     $U_{[a,h]}(h_2|h) - U_{[a,h]}(x|h) = \int_{x}^{h_1} f_0(u) - \frac{1}{2}f_1(u) du$.

    Since we are given that in the segment $[x,h_2], \ g < \frac{1}{2}$, then it folows that $\forall u, f_0(u) - \frac{1}{2}f_1(u) > 0$, and therefore the integral must be positive and hence $U_{[a,h]}(h_2|h) - U_{[a,h]}(x|h) > 0$.
     
\end{proof}
Using the helpful claims, we can see that $BR_{[a,h]}(h) \in \{a,h\} \cup  P_+^{-1}(1/2)$:

Let $h_1 \notin \{a\, , h\} \cup  P_+^{-1}(1/2)$.

\underline{Case 1 - $g(h_1) > \frac{1}{2}$:}

Let $(x,y) \subseteq [a,h]$ be the largest consecutive interval that includes $h_1$ such that $\forall h' \in (x,y) \to g(h') > \frac{1}{2}$.
Since $f_0,f_1$ are continuous, then we know $g$ is continuous.
Therefore, either $g(x) = \frac{1}{2}$ and $g'(x)>0$, or $x = a$.
From \cref{hc:outside_interval_br_left}, we receive that $U_{[a,h]}(x|h) > U_{[a,h]}(h_1|h)$, and therefore $h_1$ cannot be a best response.

\underline{Case 2 - $g(h_1) < \frac{1}{2}$:}

Let $(x,y) \subseteq [a,h]$ be the largest consecutive interval that includes $h_1$ such that $\forall h' \in (x,y) \to g(h') < \frac{1}{2}$.
Since $f_0,f_1$ are continuous, then we know $g$ is continuous.
Therefore, either $g(y) = \frac{1}{2}$ and $g'(x)>0$, or $y = h$.
From \cref{hc:inside_interval_br_left}, we receive that $U_{[a,h]}(y|h) > U_{[a,h]}(h_1|h)$, and therefore $h_1$ cannot be a best response.


We define similar claims to calculate the set of possible best responses in $[h,b]$:
\begin{helpful_claim}
\label{hc:outside_interval_br_right}
    Let $(h_1, h_2)$ be any open interval in $[h,b]$ such that $\forall x \in (h_1,h_2) \ \text{it holds that} \ g(x) > 2$.
    
    Then $\forall x \in (h_1, h_2] \to U_{[h,b]}(h_1|h) > U_{[a,h]}(x|h)$.
\end{helpful_claim}
\begin{proof}
    We will show that $U_{[h,b]}(h_1|h) - U_{[h,b]}(x|h) > 0$:
     
    Let  $x \in (h_1, h_2]$.
    We will compare the market shares of $x$ and $h_1$:

     $U_{[h,b]}(h_1|h) - U_{[h,b]}(x|h) = \int_{h_1}^{x} \frac{1}{2}f_1(u) - f_0(u) du$.

    Since we are given that in the segment $[x,h_2], \ g >2$, then it follows that $\forall u, \frac{1}{2}f_1(u) - f_0(u) > 0$, and therefore the integral must be positive and hence $U_{[h,b]}(h_1|h) - U_{[h,b]}(x|h) > 0$.
\end{proof}

\begin{helpful_claim}
\label{hc:inside_interval_br_right}
    Let $(h_1, h_2)$ be any open interval in $[h,b]$ such that $\forall x \in (h_1,h_2) \ \text{it holds that} \ g(x) < 2$.
        
    Then $\forall x \in [h_1, h_2) \to U_{[a,h]}(h_2|h) > U_{[a,h]}(x|h)$.
\end{helpful_claim}
\begin{proof}
    We will show that $U_{[h,b]}(h_2|h) - U_{[h,b]}(x|h) > 0$:
     
    Let  $x \in (h_1, h_2]$.
    We will compare the market shares of $x$ and $h_1$:

     $U_{[h,b]}(h_2|h) - U_{[h,b]}(x|h) = \int_{x}^{h_2} f_0(u)- \frac{1}{2}f_1(u) du$.

    Since we are given that in the segment $[x,h_2], \ g < 2$, then it follows that $\forall u, f_0(u)- \frac{1}{2}f_1(u) > 0$, and therefore the integral must be positive and hence $U_{[h,b]}(h_2|h) - U_{[h,b]}(x|h) > 0$.

\end{proof}
Using the helpful claims, we can see that $BR_{[h,b]} \in \{h,b\} \cup  P_+^{-1}(2)$:

Let $h_1 \notin \{b\} \cup  P_+^{-1}(2)$.

\underline{Case 1 - $g(h_1) > 2$:}

Let $(x,y) \subseteq [a,b]$ be the largest consecutive interval that includes $h_1$ such that $\forall h' \in (x,y) \to g(h') > 2$.
Since $f_0,f_1$ are continuous, then we know $g$ is continuous.
Therefore, either $g(x) = 2$ and $g'(x)>0$, or $x = a$.
From \cref{hc:outside_interval_br_right}, we receive that $U_{[a,h]}(x|h) > U_{[a,h]}(h_1|h)$, and therefore $h_1$ cannot be a best response.

\underline{Case 2 - $g(h_1) < 2$:}

Let $(x,y) \subseteq [h,b]$ be the largest consecutive interval that includes $h_1$ such that $\forall h' \in (x,y) \to g(h') < 2$.
Since $f_0,f_1$ are continuous, then we know $g$ is continuous.
Therefore, either $g(y) = 2$ and $g'(x)>0$, or $y = b$.
From \cref{hc:outside_interval_br_right}, we receive that $U_{[a,h]}(y|h) > U_{[a,h]}(h_1|h)$, and therefore $h_1$ cannot be a best response.

\end{proof}
\paragraph{\Cref{prop:gen-br-1rd} 
(Convergence after 1 round).}

\begin{proof}
    Let $h$ be any starting classifier.

    At timestep $t=0$, we asume both players are at $h$.
    
    At timestep $t=1$, player i plays $h_i^1 = BR(h)$,  and player j plays $h_j^1 = BR(h_1^1)$.

    Let $h_{min} = \minn{h_i^1, }{h_j^1}, h_{max} = \maxx{h_i^1, }{h_j^1}$.
    
    Firstly, we will argue that there exists an optimal-accuracy classifier $h_{opt}$ such that $h_{opt} \in [h_{min}, h_{max}]$:

    Assume for the sake of contradiction that this isn't the case. Then there must exist some $h_{opt}$ either to the left of $h_{min}$ or to the right of $h_{max}$. Let's assume w.l.o.g that there exists some $h_{opt} > h_{max}$.
    Then $\mu(h_{opt}|h_{min})>\mu(h_{max}|h_{min})$: $a_{opt} > a_{max}$ by definition, and $\delta_{opt, min} > \delta_{max,min}$, since when $h_{min} < h_{max} < h_{opt}$, then $\delta_{max,min} \subset \delta_{opt,min}$.\footnote{containment here refers to the points that contribute to the values of $\delta$} 

    Therefore, exists some $h_{opt} \in [h_{min}, h_{max}]$.
    
    We now argue that $(h_i^1, h_j^1)$ is a PNE.

    
    Assume without loss of generality $h_i^1 < h_{opt}$. This generalization is without loss since we are proving a best-response equilibrium symmetrically for both thresholds, so it does not matter which player is on which side of $h_{opt}$.
    
    From the proof of \cref{thm:generalized_br_1d}, we know that $h_i^1 = \underset{h \in \{a,h_{opt}, P_+^{-1}(1/2)\}}{\argmax} U(h|h_{opt})$.

    [if $h_i^1 = h_{opt}$ we are done.]

    We know then that $h_j^1 \ge h_{opt}$.
    
     Assume for the sake of contradiction that $h_j^1 < h_{opt}$ -
     
     Then  $\delta_{h_{opt}}(h_i^1) > \delta_{h_j^1}(h_i^1)$ , and from the optimality of $h_{opt}$: $a_{opt} \ge a_{h_j^1}$, and therefore $U(h_{opt}|h_i^1) > U(h_j^1|h_i^1) $, contradiction to $h_j^1$ being a best-response.

     Now,  $h_j^1 \ge h_{opt} > h_i^1$. 

     We will argue $h_i^1$ is a best-response to $h_j^1$:

     $\forall h \in (h_{opt}, h_j^1]$, the utility of $h_{opt}$ is greater, similarly to how was argued above.
     
     $ \forall h < h_{opt}$, if $h_i^1$ is a best-response to $h_{opt}$, then it must also be a best-response to $h_j^1$, since the accuracy stays the same and the discrepancy grows in an equal amount for all classifiers $h<h_{opt}$. 
     
     Therefore, both classifiers $h_i^1, h_j^1$ are best responses to each other and therefore are a PNE. 
\end{proof}

\paragraph{\Cref{prop:i-improve-you-improve} (\quot{I improve, you improve}).}
\begin{proof}
From the proof of convergence in \cref{prop:gen-br-1rd}, we receive that in all threshold games, the players go to either side of an optimal classifier $h_{opt}$.

Assume that player i moved to as an initial best-response $h_i^1$ to some $h^{opt}$.
Then player j's best-response $h_j^1$  is such that $h_{opt}$ is between $h_i^1$ and $h_j^1$.
Since $h_j^1$is a BR, we know the market share of player $j$ increases (weakly).

For player $i$, from \cref{prop:utils_of_players}, $\mu_i = \frac{1}{2}(a_i +\delta_{ij})$.

$a_i$ remains the same, but $\delta_{ij}$ increase because $h_j^1$ went further away to the other side of $h_{opt}$, and as explained in the proof of \cref{prop:gen-br-1rd}, $\delta_{h_i^1, opt} \subset \delta_{h_i^1, h_j^1}$.

Therefore $\mu_i$ increases as well.

\end{proof}

\paragraph{\Cref{prop:ms_increases} (Market share increases during competition).}
\begin{proof}
Assume the players started from $h^0$:

    $\mu(h^0|h^0) = \frac{1}{2}a^0$

Let $(h_1, h_2)$ be anyt equilibrium.

    $\mu(h_1| h_2) = \frac{1}{2}(a_1 + \delta_{12})$

    We will prove $a_1 + \delta_{12} \ge a^0$:

    Assume $a_1 + \delta_{12} < a^0$.

    Then $a ^0 + \delta_{h^0,2} > a_1 + \delta_{12} $, contradiction to $h_1$ being a best-response to $h_2$.
\end{proof}

\paragraph{\Cref{corr:welfare} (Welfare increases during competition).}

This is immediate from \cref{prop:ms_increases} since $SW = \sum_i \mu_i$.

\paragraph{\Cref{prop:bounded_market_concentration}}
\begin{proof}
    Let $h_1,h_2$ be any PNE.
    
    Assume for the sake of contradiction that $\mu(h_1|h_2) > 2\cdot \mu(h_2|h_1)$. 

    From \cref{prop:utils_of_players} we receive that $\mu(h_2|h_1) = \frac{1}{2}(a_2 + \delta_{21})$.

    We will show that $\mu(h_1|h_1) = \frac{1}{2}a_1 > \frac{1}{2}(a_2 + \delta_{21})= \mu(h_2|h_1)$:

    W know that $\mu(h_1|h_2) > 2\cdot \mu(h_2|h_1) \Rightarrow a_1 + \delta_{12} > 2\cdot(a_2 + \delta_{21})$.

    We also know $\delta_{12} \le a_1$, by definition of  partial discrepancy.

    Therefore $a_1 + a_1 \ge a_1 + \delta_{12} > 2\cdot(a_2 + \delta_{21})$

    And so: $a_1 > a_2 + \delta_{21} \Rightarrow \mu(h_1|h_1) > \mu(h_2|h_1)$, contradiction to  $(h_1,h_2)$ being a PNE.

    
\end{proof}


%========================================================================
\section{Additional theoretical results} \label{appx:add_theory}

\subsection{Characterization of our problem setting as a congestion game.} \label{appx:congestion}
In \cref{sec:setup}, we mentioned that our problem setting is proven to have a PNE, a result shown by \citep{ben2019regression} through the use of an exact potential function.
Additionaly, \citep{monderer1996potential} show that every potential game is isomorphic to some congestion game; this connection however is not always readily evident.
We show here the exact reduction of our problem setting to a congestion game, and highlight that the cost function is negative, which may be counterintuitive to more classic settings of congestion games.
\begin{observation}
\label{obs:congestion_game}
    Our problem setting is reduced to the congestion game $(N,M,(H_i)_{i\in N} ,(c_j)_{j\in M})$
    Where:
    \begin{itemize}
        \item N is the number of players
        \item M is the samples in the training set upon which the players want to gain market share
        \item $H_i$ is the hypothesis class available to player $i$
        \item $c_j(k) = -\frac{1}{k}$ is the cost function assigned to each sample, where $k$ is the number of companies accurate on consumer $j$
    \end{itemize}
\end{observation}

\begin{proof}
    Firstly, we will show that the game that is defined above is indeed a congestion game.
    We can observe this almost immediately, as the cost function (while negative) is monotone increasing with $n_j$, and the cost is per-sample (equal for each player).
    
    Additionally, the hypothesis class $H_i$ has a one-to-one function $a:H \to \mathbb{P}(M)$ which is $a(h) = \{(x,y) \in M: h(x) = y\}$. Therefore, each strategy $h$ is equivalent to the strategy $a(h)$ and this is a subset of the facilities $M$.
    
    From the game that is defined, we receive a potential function $\Phi$ such that $ \forall i, \ \Delta \Phi = \Delta C_i$.

    The potential function is: $\Phi(\vec{h}) = \sum_{j=1}^m \sum_{k=1}^{n_j} c_j(k)$

    (For each sample, we take the sum of $c(1),\dots, c(n_j)$, and since $c$ is monotone increasing, minimizing the potential means minimizing both players being accurate for the same classifier)
    
    Now, we will show that our problem setting reduces to this game by showing the equivalence between maximizing the player utility in the problem setting and minimizing the player cost in the above congestion game, meaning,  $\forall i,\  \Delta U_i = - \Delta C_i $.
    
    Let $s_i^k$ be the number of samples that player $i$ is accurate on along with $k-1$ other players.
    
    We observe that $$C_i(\vec{h}) =\sum_{j \in a_i(h_i)} c_j(n_j(\vec{h})) = \sum_{k=1}^n s_i^k \cdot c(k) = -\sum_{k=1}^n s_i^k \cdot \frac{1}{k} = -U_i(\vec{h})$$
     (where the middle equality comes from rearranging the samples in bins of how many other players were accurate, and and then the cost is constant in that bin).

     
\end{proof} 

\extended{
\subsection{CORRECTNESS ADVANTAGE MODEL}
\todo{...}
}

%========================================================================
\section{Experimental details} \label{appx:exp_details}
\subsection{Data details}

\label{appx:exp_details:data}
All of the experiments on real data were studied on 3 datasets: \feature{compas-arrest, compas-violent}, and \feature{adult}.
The \feature{compas} datasets originated from studies of recidivism in the United States \citep{angwin2016machine}, and are used to predict if a criminal will be rearrested for general crimes and violent crimes,  respectively.
The \feature{adult} dataset is used to predict whether the an individual's income exceeds \$50K.
\paragraph{Preprocessing Details:}
\begin{itemize}[leftmargin=0.3cm]
    \item \textbf{Adult:}
    The adult dataset was imported in python through the \ModelName{uciml} library. All of the categorical features were one-hot encoded, and numerical features remain unprocesssed. 
    To enable a balanced learning task, SMOTE resampling was applied from the \feature{imblearn} package to attain a 50\% positive class ratio. 
    After the above preprocessing, 10,000 samples were chosen randomly, resulting in a dataset with $n = 10,000$ samples and $d = 100$ features.
    \item \textbf{COMPAS-Arrest/Violent:}
    The COMPAS-Arrest dataset was preprocessed for analysis by \citet{marx2020predictive}, and a copy of their csv files are included in their code. The csv files can be found at :\\
\url{https://github.com/charliemarx/pmtools/tree/master/data}.\\
    Both datasets contain $d=21$ preprocessed binary (previously one-hot encoded) features.
    The COMPAS-Arrest dataset contains $n=6,172$ samples and has a positive class ratio of $45.5\%$.\\
    The COMPAS-Violent dataset also originally had $6,172$ samples, however the positive ratio was 88.8\%. Therefore SMOTE upsampling was applied to the negative class to bring the positive ratio to  50\%. The total number of samples for which we use COMPAS-Violent is then $n= 10,960$.
\end{itemize}

\subsection{Model Class details}
\label{appx:exp_details:models}

For our empirical anlysis, we analyzed results of the experiments with 3 model class variants that were used as the effective strategy space of the service providers. We note that since the objective of this work is to understand the ability of providers to learn based on the importance of the \emph{samples}, we kept the hyperparameter tuning minimal, so as not to forcefully overfit the data.
\begin{enumerate}
    \item \textbf{Linear SVM:}
    \begin{itemize}
       
        \item Hyperparameters: The regularization parameter $C=1.0$. Other hyperparameters were left as default.
        \item Hyperparameter tuning was performed on the values of $C$, but we observed no significant difference in the ability of providers to best-respond.
        \item The model was implemented using the \feature{LinearSVC} class from the \feature{sklearn} package.
        \item sample weights from our method were passed using the \emph{sample weight} parameter of the \emph{fit} method.
    \end{itemize}
    \item \textbf{XGboost:}
    \begin{itemize}
        \item Hyperparameters:
    \begin{itemize}
        \item Learning rate: $0.3$
        \item Max tree depth: $6$
        \item all other hyperparameters remained the default, in particular performing row and column subsampling of 1.
    \end{itemize}
        \item the loss metric used for boosting is \emph{log-loss}
        \item the model was implemented using the \feature{XGBClassifier} class from the \feature{xgboost} package
        \item We note that some basic hyperparameter tuning was performed using a grid search, but default values yielded satisfactory results.
    \end{itemize}
    \item \textbf{Random Forest:}
    \begin{itemize}
        \item Hyperparameters:
    \begin{itemize}
        \item Number of estimators: $10$
        \item Max tree depth: the default, meaning all nodes were expanded until all of the leaves are pure or contain a single sample.
        \item all other hyperparameters remained the default.
    \end{itemize}
        \item the loss metric used for boosting is \emph{log-loss}
        \item the model was implemented using the \feature{RandomForestClassifier} class from the \feature{sklearn} package
        \item Hyperparameters were minimally tuned, and the default values were primarily used.
    \end{itemize}
    
\end{enumerate}

\subsection{General implementation details}

\label{appx:exp_details:general}
\paragraph{Test and validation set.}
For all experiments, the dataset was split into training, validation, and test sets. The test set comprised 20\% of the data and was held out for final performance evaluation. The validation set, also comprising 20\% of the data, was used for hyperparameter tuning when applicable. In cases where no hyperparameter tuning was performed, the validation set was not utilized, and so only the training and test sets were used.
\paragraph{Experiment Splits.}
To ensure integrity and mitigate the effect of random variations in the data, each experiment was conducted over 10 random splits of the dataset. For each split, the data was shuffled and divided into training, validation, and test sets according to the above proportions. The reported results in the following sections include standard errors calculated across these 10 splits, providing an estimate of variability in the model performance.
    
\paragraph{Code.} All of our code is implemented in Python. All of our experiments are reproducible and attached as supplementary material.
\paragraph{Hardware.} All experiments were run in the PyCharm IDE on a single Macbook Pro laptop, with 16GB of RAM, and M2 processor, and with no GPU support. However, the experiments to create the table metrics  were cumbersome on the IDE, and so the PyCharm heap size was raised to 8K MegaBytes in order to enlarge the stack.
The total runtime for all the results takes roughly 12 minutes.

%=========================================================================================
\section{Additional experimental results} \label{appx:add_exps}

\subsection{Main results for additional settings} \label{appx:tables}

In this appendix we showcase additional insights from our main results when tested on additional model classes.

\paragraph{XGboost}
\input{tables/table_metrics_xgb}
\Cref{tbl:app_xgb} shows the learning performance of the service providers when using XGBoost as the model class for training and inference.
The details of the Xgboost implementation and hyperparameters can be found in \Cref{appx:exp_details:models}.
A comparison of interest is the general improvements of the players relative to the Linear model class. We can notice that the welfare improvement, which is equal to the total market share of all players, is significantly lower for both the Adult and COMPAS-ARREST dataset.
This does not indicate that the total welfare is lower, but rather that due to the increased expressiveness of the XGboost model, the starting welfare began at a higher value.

Another point of interest is how the HHI (the measure of imbalance in market share between the providers) persists across model classes, regardless of expressivity. This further highlights the importance of taking into account the market dynamics and the order of play when competing with other providers.


\paragraph{Random Forest}
\input{tables/table_metrics_rf}
In a similar manner, \Cref{tbl:app_rf} presents the results on competition where the providers are employing a Random Forest model, whose details can be found in \cref{appx:exp_details:models}.
We can observe that the results resemble those of the XGboost model class, which can be expected due to the similarity in nature of all Decision Tree models. 

\extended{
\paragraph{Comparison across model classes}
\todo{OPTIONAL}
\todo{make a table for arrest/violent/adult where we compare the  }
}

\paragraph{Standard errors of experiments}
As mentioned in \cref{appx:exp_details}, each experiment was run over 10 train-test splits and the metric values were averaged out over those splits. 
\Cref{tbl:app_stderrs} portrays, for each metric, the maximum variation of the standard error among the three model classes analyzed.
We note that all of the errors are below 5\%, and most of the errors are well below 3\%.

\input{tables/stderrs_table}

\subsection{Competition Dynamics}
%%%========================================
%======== UTILITIES FIGURE ================
\begin{figure}[h!]
    \centering
\includegraphics[width=0.8\textwidth]{graphics/27-01-2025_utilities_competition_metrics_experiment_XGboost.pdf}
    \caption{Competition Dynamics with XGboost models on the COMPAS-Arrest dataset. Utilities of individual players are shown for Train (Left) and Test (Right), in markets involving 2,3,4,5,and 6 players (Top to Bottom)}. 
    \label{fig:cmpt_utils_xgb}
\end{figure}
%=============================================
%=============================================
\paragraph{Market Share}
In order to provide a more comprehensive analysis of how the market shares of the providers move through the best responses in the competition,  \Cref{fig:cmpt_utils_xgb} shows, for each number of players competing for market share, the shifts in market share of each provider based on their positions in the game (order of play). 

We can observe a few key points:
\begin{enumerate}
    \item \textbf{Market Stability.} Across all variations of the number of players, the market shares of the player converge almost immediately to their final respective values. This convergence occurs even before every player offered a single best-response, i.e, by the end of Round 1. 
    % An explanation for this is alluded to in \cref{sec:exp:real}, where the play of the first player already influences the strategy of the next players.
    % It can then be
    \item \textbf{Order of play.} In line with what is expressed in \cref{sec:exp:real}, The order of play when offering a best-response is important, and varies as a function of the number of players.
    For $n=2$ providers,  playing second can offer a significant competitive advantage.
    When shifting to markets with $n\ge 3$ providers, however, we observe a significant advantage to the 1st mover, as discussed in \cref{sec:exp:real}.
    \item \textbf{General market share trend.}
    Regardless of the order of play, and as alluded to in \cref{tbl:app_xgb}, the market share rises for all players, which supports the empirical claim that providers that are competing are in a way \emph{collaborating} to figure out how optimally divide the market.
    \item \textbf{Generalization to an unseen test set.}
    The performance on the unseen test set closely mirrors that observed during training, highlighting the robustness of the competition method. The similarity in performance demonstrates that the model can calculate a best-response without overfitting, and underscores the ability of the framework to maintain accuracy in line with that expected from traditional ML methods.    
    
\end{enumerate} 
% \paragraph{Discrepancy of classification}

% \subsection{Order of play} 
% \label{appx:chicken}
% An outstanding characteristic of the competition setting is the influence that the order of play has on the final market shares at equilibrium. 
% To quantify this phenomenon, and assert its robustness across methods,  we ran experiments for every combination of datasets and model classes that are listed in Appendices \ref{appx:exp_details:data} and \ref{appx:exp_details:models}. 
% For each experiment, namely for each combination of model class and dataset, the final market shares were calculated for each provider along with his/her relative position of play, i.e., at what position did the provider perform a best-response. Additionally, each experiment was performed for competitions with 2,3,4,5, and 6  players, so that we can compare dynamics across different market saturations.

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.75\linewidth]{graphics/25_01_27_chicken_order_of_play_comparison4.pdf}
%     \caption{Influence of order of play on market share at equilibrium. The term $\mu_i, \forall i,$ represents the market share of the provider who played a best-response in position $i$. The Orange densities measure the difference in market share between the provider that moved 2nd and the provider that moved 1st, whereas the Blue densities measure the difference in market share between the provider that moved 2nd and the provider that moved 3rd, when applicable. The Dashed lines inside the densities represent the 25\%, 50\%, and 75\% quantiles of values across experiments, respectively, from bottom to top.}
%     \label{fig:chicken_deltas}
% \end{figure}

% \Cref{fig:chicken_deltas} shows the order of play comparisons, averaged out across all of the experiments that were described above. 
% \paragraph{2 providers.} When the competition game is played with $n=2$ providers, we see a clear preference to be the provider that moves last, characterized by the market share term $\mu_1$.
% The vast majority of experiments showed a significant gain in market share, as seen by the fact that the 25\% quantile of values already shows a net-positive gain from moving last. We also note that the expected (mean) competitive advantage of moving last is 3.6\% and the median is 4.8\%.
% Given that for 2 players with equal model classes, the market share of a single provider will never exceed $\frac{2}{3} = 0.67$ (see \cref{prop:bounded_market_concentration}), then a $4.8\%$ difference in market share is  quite significant.
% \paragraph{3 or more providers.} When the competition game is played with $n\ge3$ providers, however,  we observe an entirely different dynamic.
% For $n >2$ providers, \cref{fig:chicken_deltas} provides two densities. The left density is the difference in market share between the player who moved 2nd versus the player who moved 1st ($\mu_1 = \mu_0$), and is represented in Orange. The Right density is the difference in market share between the player who moved 2nd versus the player who moved 3rd ($\mu_1 = \mu_2$), and is represented in Blue.
% We find here that the ratios have switched: it is in fact more advantageous to be the provider that moves 1st, as evidenced by the negative orientation of the LHS density plot (in Orange). The next interesting thing that we note is the seeming insignificance of order-of-play beyond the first two positions, as we can observe that the RHS density (in Blue) hovers around 0 with relatively low variance. This alludes to the premise put forth in \cref{sec:exp:real} that in competitive settings with 3 or more players, the person who moves 1st is in essence \quot{grabbing his territory}, and then induces all of the other players to play amongst themselves for the other resources, i.e consumers.


\subsection{Welfare} \label{appx:welfare}
\paragraph{Welfare behaviors in additional settings.}
\cref{fig:sw_across_models} shows  the social welfares across the model classes described in \cref{appx:exp_details:models}, namely \feature{LinearSVC, XGBoost}, and \feature{RandomForest}. The test welfares are shown for the COMPAS-arrest dataset, and are portrayed for each model class and each game varying the nubmer of players.
We can see that, as in \cref{sec:exp:real}, the welfare gets mazimized very early for the \feature{Linear} model class, but hits a non-maximal plateau for the Decision Trees. This is another example of the non-monotonic nature of social welfare: in many cases providers having less expressiveness in their models will in fact benefit the consumers.
%==========================================================
%======== SOCIAL WELFARES
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{graphics/27-01-2025_social_welfares_COMPAS_arrest.pdf}
    \caption{Social Welfares across model classes on the test set for the COMPAS arrest dataset. Each plot calculates the social welfare at each timestep and for each experiment that varies the number of players.}
    \label{fig:sw_across_models}
\end{figure}
\paragraph{Welfare across asymmetry in data.}
The social welfare trends across different levels of data representations can also behave in a surprising manner, as shown in \cref{sec:exp:real} and explained in great detail by \cite{jagadeesan2024improved}. 
As a portrayal of this phenomenon across different competition settings, \cref{fig:sw_across_number_features} plots the social welfares across experiments of 2,3, and 4 players, respectively.
We can observe, that while no as blatant as with $n=2$ providers, the general trend across all player formats is that the social welfare increases as an inverse proportion to the richness of the data representations.

%========================================================
%====== SW when varying data features ===================
\begin{figure}[b!]
    \centering
    \includegraphics[width=\linewidth]{graphics/compas_arrest_welfares_across_n_features_x_rounds_players_2_3_4.pdf}
    \caption{Comparison of social welfares on the COMPAS arrest test set when varying the number of features available to use for model training (and inference). }
    \label{fig:sw_across_number_features}
\end{figure}

\subsection{Synthetic Data}
\label{app:add_exps:synth}

In \cref{sec:exp:synth}, we showed an example of the chicken dynamic between asymmetrical class-conditioned gaussians. Additionally, we performed an overlap analysis between symmetric gaussians, where for each measure of distance between the means, or overlap, we calculated the best-response thresholds and performance metrics.
\Cref{fig:gauss_asymm_tipping_pt} (Left) shows the above analysis on asymmetric gaussians, namely $\sigma_{-1} = 2, \sigma_{+1} =1$.
As in \cref{sec:exp:synth}, at each overlap the thresholds of the players are initialized at $h_1^0 = h_2^0 = h_{opt}$, where $h_{opt}$ is the naively optimal classifier that maximizes accuracy on the distributions.
Here too, and as is guaranteed by \cref{thm:generalized_br_1d}, the best-response dynamics converge after just one round. 
In this case of asymmetry between the class-conditioned gaussians, we notice a distinctly different behavior.
while $h_1$ remains in the same proximity to $h_{opt}$ as in the symmetric case, threshold $h_2$ has a \quot{tipping point}, where it suddenly jumps to the far end of both distributions.
From the accuracy graph we can also see a sudden drop in accuracy coming fromm  model $h_2$. In regards to the market share, however, the provider that gets a spike in market share is in fact the one who played $h_1$ and remains close to the optimal, while the market share of $h_2$ simply shows a gradual increase, with no reference to a tipping point.

This phenomenon is understandable when we look at \cref{thm:generalized_br_1d}, that states that the best-response may be at the far end of the distributions. This occurs when either of the values $g^{-1}(1/2), g^-1(2)$ stops existing, where $g(x) = \frac{f_1(x)}{f_0(x)}$ is the MLR function. In these cases, the left or right threshold (depending on which value of $g^{-1}$ disappears) will continue to gain by moving to the far end of the interval. 
This is precisely what is shown in \cref{fig:gauss_asymm_tipping_pt} (Right); at a certain overlap ($\sim-1$), there is no threshold $h$ for which $g(h)=2$, and so the gain in discrepancy for $h_2$ will continue to outweigh the loss in accuracy, and $h_2$ ends up at the far right end. 
The market share of $h_1$ is then suddenly increased, since while its accuracy remains the same, the discrepancy from $h_2$ is now significantly greater. 
%===============================
%===--GAUSSIAN TIPPING POINT====
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{graphics/27_01_25_1d_gaussian_asymm_tipping_point.pdf}
    \caption{Dynamics of the best-response game on threshold classsifiers with asymmetric gaussians. The metrics at each level of proximity between the gaussians are shown (Left). The thresholds at the \quot{tipping point} (Right) can be seen to be very far apart. The hatches represent the sectors of exclusivity. }
    \label{fig:gauss_asymm_tipping_pt}
\end{figure}
%===============================
%===============================
\extended{
\paragraph{Four squares example} \todo{... OPTIONAL}
}
\subsection{Asymmetrical power between players}
Another interesting question we can ask from the perspective of the learners/providers is, if a provider were to invest cost and effort to gain better data, how would this help them in competition? In \naive 
\ settings, i.e. single-provider markets, the answer to this is straightforward: more data = better accuracy. In accuracy markets, however, the specialization needs to be considered as well, and as we have seen (\cref{sec:exp:real}), the behavior of the market when altering the data quality can be counter-intuitive.
To measure the gain to be had when improving data, we ran experiments where one of the providers has access to more features than its counterpart/s.
For each of 2 possible move positions (1st or 2nd), two metrics were considered:

1) The provider's gain in market share over himself if he weren't to improve his data (i.e. the data is identical for all parties), which measures the provider's marginal gain from improving  data

2) The provider's gain in $\Delta \mu$ from the next-best provider versus the setting where he didn't improve his data, which measures the impact of the data investment on the concentration of the market as a whole.
\input{tables/asymm_table_trial}
\Cref{tab:asymm_pwr} shows the above metrics for markets with 2,3, and 4 providers, and for various possibilities of data improvement. 

We can observe a few interesting trends:
\begin{enumerate}
    \item \textbf{$\Delta\mu$ from regular setting.} One of our central insights from  \cref{sec:exp:real} is that when $n=2$ providers, moving second is beneficial, and when $n>2$, the opposite is true, and moving first is better. In the case of investing in better data, and when comparing the provider's market gain vs. himself in a regular setting, we see an inverse effect.
    
    For $n=2$, better data creates market gains only when you are the first mover, as in certain lopsided markets the gain is $>12\%$. For example, in the case where the better-data provider has 15 features, and the other providers have 3 features, we can observe a 12\% gain, which measures the benefit of investing in the additional 12 features.  

    When the provider moves second, however, investing in more data does not translate to higher market share, in fact the provider loses significant market share, and would have been better off retaining the same primitive data as the other competitors.
    This phenomenom gets exacerbated further the more the provider invests in better data; For example, if one were to utilize all 21 features of the dataset when the competitors have access to only 9 features, the advatnaged provider would see a -12.35\% loss in market share.

    For markets where $n>2$, it is the other way around. When the advantaged provider moves first, he may see a decrease in market share from the regular setting where he didn't gain extra data; When moving 2nd, the data gain proves helpful. This stands in polar contrast to the case where $n=2$, and perhaps understandably so: It seems that wherever the providers have an initial advantage when the data is symmetrical, they would lose that advantage when investing in more data, perhaps hinting at the idea of decreasing marginal returns in investments.

    \item \textbf{$\Delta\mu$ from the next-best provider.} 
    When comparing the difference across data-variation experiments in market shares between providers, we notice that the trend behaves similarly to how we have seen in the order-of-play results of  \cref{sec:exp:real}. We can observe that,  interestingly, if a provider improved his absolute market-share relative to himself, this doesn't translate to the provider improving his market share relative to others. Take for example the cases where $n>2$ and the provider moves 2nd. As stated above and as can be seen in \cref{tab:asymm_pwr}, the added data advantage in this setting helps the provider gain in absolute market share.  The market gain relative to the other providers, however, has an inverse result, and in many cases the competitors end up with a better overall utility. This tells us that in these settings, when the advantaged provider goes second, the total welfare (as the sum of individual market shares) increases.
    
\end{enumerate}

\extended{
\todo{fix up table and make nicer}
\todo{Portray the metrics as a heat map on lower diagonal}
\todo{explain the results}
}


