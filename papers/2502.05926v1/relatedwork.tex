\section{Related Work}
\subsection{Large Language Models}
The development of large language models (LLMs) has significantly advanced the field of natural language processing (NLP). These models, primarily based on transformer architectures, have demonstrated remarkable capabilities in tasks such as text generation, translation, and summarization \cite{zhou2023style,zhou2021improving,zhou2021modeling}. Recent efforts have extended these capabilities beyond text, enabling the integration of multimodal data, including visual and auditory inputs, to solve more complex tasks \cite{zhou2023improving}.

Several studies have explored the application of LLMs to various domains, including bioinformatics and medical imaging. LLMs such as GPT-3 and BERT have been foundational in demonstrating the potential of language models for understanding and generating human language. These models have also been adapted for multimodal applications, combining text with other forms of data, such as images, to tackle more complex problems in fields like medical imaging \cite{ratzlaff2024training}, bioinformatics \cite{garg2023multimodal}, and artificial general intelligence (AGI) \cite{xu2024introspection,zhou2024visual}.

In the context of medical imaging, large language models have shown great promise for enhancing the understanding and generation of chest X-rays (CXR). However, challenges remain in effectively aligning visual features with textual information to maintain the integrity of both modalities. Tokenization methods, such as VQ-GAN, have been enhanced with domain-specific training processes to preserve crucial diagnostic features like lesion boundaries and textural details \cite{bucciarelli2024personalizing}. Additionally, existing models for CXR interpretation often rely on separate adapter networks to bridge the gap between visual and textual data, which can introduce bottlenecks in the processing pipeline \cite{xu2024introspection}.

Multimodal models like PaLM-E and Flamingo have demonstrated the ability to integrate multiple data types, including visual, auditory, and textual, to improve the generalization of AI systems across tasks \cite{song2024exploring}. These models show that LLMs, when trained on diverse multimodal data, can achieve more robust performance across a variety of cognitive and clinical tasks, from generating medical reports to answering clinical questions based on image inputs.

Despite these advancements, the integration of LLMs with medical imaging continues to face significant challenges, particularly in ensuring that fine-grained details in medical images are not lost during the interaction between vision and language models. The difficulty of training LLMs to handle domain-specific languages and medical terminologies also requires ongoing research to improve model robustness and accuracy \cite{huang2024video}. 

While the generalization of large language models from weak to strong \cite{zhou2025weak} also represents the potential of large medical models, their application to multimodal tasks such as medical imaging requires overcoming challenges in model design, tokenization, and the preservation of diagnostic information, areas that continue to be actively explored in the literature.

\subsection{CXR Image Understanding and Generation}

Chest X-ray (CXR) image understanding and generation have become a critical area of research in the intersection of medical imaging and artificial intelligence. Recent advancements in multimodal learning and large language models (LLMs) have paved the way for innovative approaches that address the unique challenges of CXR analysis, including the need for fine-grained reasoning and effective vision-language alignment.

Several methods focus on enhancing CXR-to-report generation by leveraging advanced vision-language frameworks. These models integrate text and image modalities to generate clinically accurate radiology reports. Early works employed encoder-decoder architectures, but recent studies have utilized multimodal LLMs to improve both textual fluency and clinical relevance \cite{kang2024wolf, kang2024wolf, lee2023llmcxr}. These models often employ vision encoders combined with pre-trained language models, enabling them to generate detailed reports by understanding visual features and mapping them to appropriate medical terminology.

In the realm of report-to-CXR generation, diffusion-based models have shown significant promise. These models aim to generate high-fidelity CXR images conditioned on textual descriptions, ensuring consistency with clinical semantics. By incorporating domain-specific knowledge and adaptive learning strategies, such approaches can achieve realistic image synthesis that aligns with diagnostic requirements \cite{huang2024diffcxr, han2024advancing}.

Another notable direction is multimodal fusion, where additional data sources such as electronic health records (EHR) are combined with CXR images to improve predictive capabilities. By addressing temporal asynchronicity and leveraging latent representations, these methods enable personalized and dynamic imaging-based predictions \cite{yao2024addressing}.

To further enhance CXR understanding, several studies have introduced frameworks designed to reduce biases and optimize data preprocessing. These frameworks aim to improve generalizability across diverse datasets by addressing confounding factors and ensuring that models focus on anatomically relevant features \cite{aslani2022optimising, castro2024padchestgr}. Additionally, generative adversarial networks (GANs) and reinforcement learning have been employed to improve specific aspects of CXR image quality, such as rib suppression or domain-specific posture and pathological realism \cite{han2021ganbased, chen2023finematching}.

Finally, explainable frameworks have gained traction, with efforts to align image regions and text tokens in a bidirectional manner. These models not only improve the interpretability of predictions but also enable cyclic training strategies, ensuring that the generated reports and images are mutually consistent \cite{chen2023finematching}.