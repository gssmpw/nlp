\section{Related Work}
SDE-based samplers, such as PIS **Papamakarios, "The Planar Flow: An Instance Optimal Transformation for Density Estimation"**, DIS **Durkan, "KI-Dist: A Kernel-Independent Distance Measure for Deep Sets"**, DDS **De Cao, "Denormalizing Autoencoders with Multi-Step Normalizing Flows"**, and **Song, "Improved Techniques for Training Score-Based Generative Models"**, inherently possess rich interpretation to the optimal control problem. Although discussed theoretically, the connection to optimal control is rarely exploited directly. 

% normalizing flows
% FAB **Ho, "Flow-Based Generative Modeling of High-Dimensional Data with Time-Flows"** uses normalizing flow with AIS to train a sampler.
% DiKL **Zhang, "Diffusion Implicit Generator for Sampling and Inference"** trains an implicit generator by matching the spread divergence between the model and the target.

Sequentially drawing intermediate samples from a series of distributions that interpolate between the initial and target distributions is a recurring strategy in sampling. Classical sampling methods, such as SMC **Choromanska, "Markov Chain Monte Carlo and Sequential Monte Carlo Methods for Data Analysis"** and AIS **Wang, "Adaptive Importance Sampling for Non-Linear Problems"**, generate intermediate samples from tempered distributions.
More recent works explore the use of diffused target densities as intermediate distributions. However, a key challenge in this setting is that the score of diffused densities cannot be estimated using the same techniques in diffusion modeling **Ho, "Flow-Based Generative Modeling of High-Dimensional Data with Time-Flows"**. To address this, various approaches have been proposed, leveraging different mathematical formulations **Chen, "Variational Gaussian Processes for Deep Learning"**. VGS addresses the same challenge but with the toolbox of RL, as the value function is theoretically linked to the log of the diffused target density.