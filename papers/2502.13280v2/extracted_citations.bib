@article{berner2022optimal,
  title={An optimal control perspective on diffusion-based generative modeling},
  author={Berner, Julius and Richter, Lorenz and Ullrich, Karen},
  journal={arXiv preprint arXiv:2211.01364},
  year={2022}
}

@article{chen2024sequential,
  title={Sequential Controlled Langevin Diffusions},
  author={Chen, Junhua and Richter, Lorenz and Berner, Julius and Blessing, Denis and Neumann, Gerhard and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2412.07081},
  year={2024}
}

@article{he2024training,
  title={Training Neural Samplers with Reverse Diffusive KL Divergence},
  author={He, Jiajun and Chen, Wenlin and Zhang, Mingtian and Barber, David and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  journal={arXiv preprint arXiv:2410.12456},
  year={2024}
}

@inproceedings{ho2020ddpm,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{mcdonald2022proposal,
  title={Proposal of a score based approach to sampling using Monte Carlo estimation of score and oracle access to target density},
  author={McDonald, Curtis and Barron, Andrew},
  journal={arXiv preprint arXiv:2212.03325},
  year={2022}
}

@article{moral2006smc,
    author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
    title = {Sequential Monte Carlo Samplers},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {68},
    number = {3},
    pages = {411-436},
    year = {2006},
    month = {05},
    abstract = {We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.},
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2006.00553.x},
    url = {https://doi.org/10.1111/j.1467-9868.2006.00553.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/68/3/411/49795343/jrsssb\_68\_3\_411.pdf},
}

@article{neal2001annealed,
  title={Annealed importance sampling},
  author={Neal, Radford M},
  journal={Statistics and computing},
  volume={11},
  pages={125--139},
  year={2001},
  publisher={Springer}
}

@InProceedings{sohl-dickstein15,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@article{wang2024energy,
  title={Energy based diffusion generator for efficient sampling of Boltzmann distributions},
  author={Wang, Yan and Guo, Ling and Wu, Hao and Zhou, Tao},
  journal={arXiv preprint arXiv:2401.02080},
  year={2024}
}

