@article{santambrogio2015optimal,
  title={Optimal transport for applied mathematicians},
  author={Santambrogio, Filippo},
  journal={Birk{\"a}user, NY},
  volume={55},
  number={58-63},
  pages={94},
  year={2015},
  publisher={Springer}
}

@book{villani2021topics,
  title={Topics in optimal transportation},
  author={Villani, C{\'e}dric},
  volume={58},
  year={2021},
  publisher={American Mathematical Soc.}
}

@article{lin1992,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, L.-J},
  journal={Machine Learning},
  volume={8},
  pages={293--321},
  year={1992},
  publisher={Springer}
}


@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  volume={14},
  number={8},
  pages={1771--1800},
  year={2002},
  publisher={MIT Press}
}


@inproceedings{Nijkamp2019nonconvergent,
 author = {Nijkamp, Erik and Hill, Mitch and Zhu, Song-Chun and Wu, Ying Nian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {5232--5242},
 publisher = {Curran Associates, Inc.},
 title = {Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model},
 url = {https://proceedings.neurips.cc/paper/2019/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf},
 volume = {32},
 year = {2019}
}


@incollection{du2019,
title = {Implicit Generation and Modeling with Energy Based Models},
author = {Du, Yilun and Mordatch, Igor},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alche-Buc and E. Fox and R. Garnett},
pages = {3608--3618},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@inproceedings{tieleman2008training,
  title={Training restricted Boltzmann machines using approximations to the likelihood gradient},
  author={Tieleman, Tijmen},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1064--1071},
  year={2008}
}

@inproceedings{lyu2011Unifying,
 author = {Lyu, Siwei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf},
 volume = {24},
 year = {2011}
}

@inproceedings{
zhang2022path,
title={Path Integral Sampler: A Stochastic Control Approach For Sampling},
author={Qinsheng Zhang and Yongxin Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=_uCb2ynRu7Y}
}


@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Reinforcement learning},
  pages={5--32},
  year={1992},
  publisher={Springer}
}

@InProceedings{fan2023optimizing,
  title = 	 {Optimizing {DDPM} Sampling with Shortcut Fine-Tuning},
  author =       {Fan, Ying and Lee, Kangwook},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {9623--9639},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/fan23b/fan23b.pdf},
  url = 	 {https://proceedings.mlr.press/v202/fan23b.html},
  abstract = 	 {In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting in sample quality comparable to or even surpassing that of the full-step model across various datasets.}
}


@inproceedings{
Qiu2020Unbiased,
title={Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models},
author={Yixuan Qiu and Lingsong Zhang and Xiao Wang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1eyceSYPr}
}


@InProceedings{gutmann2010noise,
  title = 	 {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author = 	 {Gutmann, Michael and Hyvärinen, Aapo},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {297--304},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/gutmann10a.html},
  abstract = 	 {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.}
}


@article{hynarinen2005score,
author = {Hyv\"{a}rinen, Aapo},
title = {Estimation of Non-Normalized Statistical Models by Score Matching},
year = {2005},
issue_date = {12/1/2005},
publisher = {JMLR.org},
volume = {6},
issn = {1532-4435},
abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {695–709},
numpages = {15}
}

@article{song2021train,
  title={How to Train Your Energy-Based Models},
  author={Song, Yang and Kingma, Diederik P},
  journal={arXiv preprint arXiv:2101.03288},
  year={2021}
}


@inproceedings{carreira2005contrastive,
  title={On contrastive divergence learning},
  author={Carreira-Perpinan, Miguel A and Hinton, Geoffrey},
  booktitle={International workshop on artificial intelligence and statistics},
  pages={33--40},
  year={2005},
  organization={PMLR}
}

@inproceedings{
yair2021contrastive,
title={Contrastive Divergence Learning is a Time Reversal Adversarial Game},
author={Omer Yair and Tomer Michaeli},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=MLSvqIHRidA}
}


@InProceedings{sutskever10convergence,
  title = 	 {On the Convergence Properties of Contrastive Divergence},
  author = 	 {Sutskever, Ilya and Tieleman, Tijmen},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {789--795},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/sutskever10a/sutskever10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/sutskever10a.html},
  abstract = 	 {Contrastive Divergence (CD) is a popular method for estimating the parameters of Markov Random Fields (MRFs) by rapidly approximating an intractable term in the gradient of the log probability. Despite CD’s empirical success, little is known about its theoretical convergence properties. In this paper, we analyze the CD$_1$ update rule for Restricted Boltzmann Machines (RBMs) with binary variables. We show that this update is not the gradient of any function, and construct a counterintuitive “regularization function” that causes CD learning to cycle indefinitely.  Nonetheless, we show that the regularized CD update has a fixed point for a large class of regularization functions using Brower’s fixed point theorem.}
}



@InProceedings{ceylan18conditional,
  title = 	 {Conditional Noise-Contrastive Estimation of Unnormalised Models},
  author =       {Ceylan, Ciwan and Gutmann, Michael U.},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {726--734},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/ceylan18a/ceylan18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/ceylan18a.html},
  abstract = 	 {Many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter estimation difficult. Examples of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsupervised deep learning. In previous work, the estimation principle called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. An open question is how to best choose the auxiliary noise distribution. We here propose a new method that addresses this issue. The proposed method shares with NCE the idea of formulating density estimation as a supervised learning problem but in contrast to NCE, the proposed method leverages the observed data when generating noise samples. The noise can thus be generated in a semi-automated manner. We first present the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can expect an improved performance compared to NCE when the data lie in a lower-dimensional manifold. Then we demonstrate its applicability in unsupervised deep learning by estimating a four-layer neural image model.}
}


@article{bengio2009justifying,
    author = {Bengio, Yoshua and Delalleau, Olivier},
    title = "{Justifying and Generalizing Contrastive Divergence}",
    journal = {Neural Computation},
    volume = {21},
    number = {6},
    pages = {1601-1621},
    year = {2009},
    month = {06},
    abstract = "{We study an expansion of the log likelihood in undirected graphical models such as the restricted Boltzmann machine (RBM), where each term in the expansion is associated with a sample in a Gibbs chain alternating between two random variables (the visible vector and the hidden vector in RBMs). We are particularly interested in estimators of the gradient of the log likelihood obtained through this expansion. We show that its residual term converges to zero, justifying the use of a truncation—running only a short Gibbs chain, which is the main idea behind the contrastive divergence (CD) estimator of the log-likelihood gradient. By truncating even more, we obtain a stochastic reconstruction error, related through a mean-field approximation to the reconstruction error often used to train autoassociators and stacked autoassociators. The derivation is not specific to the particular parametric forms used in RBMs and requires only convergence of the Gibbs chain. We present theoretical and empirical evidence linking the number of Gibbs steps k and the magnitude of the RBM parameters to the bias in the CD estimator. These experiments also suggest that the sign of the CD estimator is correct most of the time, even when the bias is large, so that CD-k is a good descent direction even for small k.}",
    issn = {0899-7667},
    doi = {10.1162/neco.2008.11-07-647},
    url = {https://doi.org/10.1162/neco.2008.11-07-647},
    eprint = {https://direct.mit.edu/neco/article-pdf/21/6/1601/823317/neco.2008.11-07-647.pdf},
}




@inproceedings{du2021improved,
                  title={Improved Contrastive Divergence Training of Energy Based Models},
                  author={Du, Yilun and Li, Shuang and Tenenbaum, B. Joshua and Mordatch, Igor},
                  booktitle={Proceedings of the 38th International Conference on Machine
                Learning (ICML-21)},
                  year={2021}
                }


@inproceedings{
lee2023guiding,
title={Guiding Energy-based Models via Contrastive Latent Variables},
author={Hankook Lee and Jongheon Jeong and Sejun Park and Jinwoo Shin},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=CZmHHj9MgkP}
}

@inproceedings{
xiao2021vaebm,
title={{\{}VAEBM{\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},
author={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=5m3SEczOV8L}
}



@inproceedings{bengio2013,
 author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generalized Denoising Auto-Encoders as Generative Models},
 url = {https://proceedings.neurips.cc/paper/2013/file/559cb990c9dffd8675f6bc2186971dc2-Paper.pdf},
 volume = {26},
 year = {2013}
}


@inproceedings{
grathwohl2021no,
title={No {\{}MCMC{\}} for me: Amortized sampling for fast and stable training of energy-based models},
author={Will Sussman Grathwohl and Jacob Jin Kelly and Milad Hashemi and Mohammad Norouzi and Kevin Swersky and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=ixpSxO9flk3}
}


@InProceedings{grathwohl20stein,
  title = 	 {Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling},
  author =       {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Joern-Henrik and Duvenaud, David and Zemel, Richard},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3732--3747},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/grathwohl20a/grathwohl20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/grathwohl20a.html},
  abstract = 	 {We present a new method for evaluating and training unnormalized density models. Our approach only requires access to the gradient of the unnormalized model’s log-density. We estimate the Stein discrepancy between the data density p(x) and the model density q(x) based on a vector function of the data. We parameterize this function with a neural network and fit its parameters to maximize this discrepancy. This yields a novel goodness-of-fit test which outperforms existing methods on high dimensional data. Furthermore, optimizing q(x) to minimize this discrepancy produces a novel method for training unnormalized models. This training method can fit large unnormalized models faster than existing approaches. The ability to both learn and compare models is a unique feature of the proposed method.}
}

@InProceedings{Han_2019_CVPR,
author = {Han, Tian and Nijkamp, Erik and Fang, Xiaolin and Hill, Mitch and Zhu, Song-Chun and Wu, Ying Nian},
title = {Divergence Triangle for Joint Training of Generator Model, Energy-Based Model, and Inferential Model},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{Han_2020_CVPR,
author = {Han, Tian and Nijkamp, Erik and Zhou, Linqi and Pang, Bo and Zhu, Song-Chun and Wu, Ying Nian},
title = {Joint Training of Variational Auto-Encoder and Latent Energy-Based Model},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}


@inproceedings{gao2020flow,
  title={Flow contrastive estimation of energy-based models},
  author={Gao, Ruiqi and Nijkamp, Erik and Kingma, Diederik P and Xu, Zhen and Dai, Andrew M and Wu, Ying Nian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7518--7528},
  year={2020}
}


@inproceedings{
nijkamp2022mcmc,
title={{MCMC} Should Mix: Learning Energy-Based Model with Neural Transport Latent Space {MCMC}},
author={Erik Nijkamp and Ruiqi Gao and Pavel Sountsov and Srinivas Vasudevan and Bo Pang and Song-Chun Zhu and Ying Nian Wu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=4C93Qvn-tz}
}


@inproceedings{
xie2022flow,
title={A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model},
author={Jianwen Xie and Yaxuan Zhu and Jun Li and Ping Li},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=31d5RLCUuXC}
}


@inproceedings{
arbel2021generalized,
title={Generalized Energy Based Models},
author={Michael Arbel and Liang Zhou and Arthur Gretton},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=0PtUPB9z6qK}
}


@inproceedings{
yu2021pseudospherical,
title={Pseudo-Spherical Contrastive Divergence},
author={Lantao Yu and Jiaming Song and Yang Song and Stefano Ermon},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=8qa6hkGYDJk}
}


@InProceedings{yu2020trainingf,
  title = 	 {Training Deep Energy-Based Models with f-Divergence Minimization},
  author =       {Yu, Lantao and Song, Yang and Song, Jiaming and Ermon, Stefano},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10957--10967},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/yu20g/yu20g.pdf},
  url = 	 {https://proceedings.mlr.press/v119/yu20g.html},
  abstract = 	 {Deep energy-based models (EBMs) are very flexible in distribution parametrization but computationally challenging because of the intractable partition function. They are typically trained via maximum likelihood, using contrastive divergence to approximate the gradient of the KL divergence between data and model distribution. While KL divergence has many desirable properties, other f-divergences have shown advantages in training implicit density generative models such as generative adversarial networks. In this paper, we propose a general variational framework termed f-EBM to train EBMs using any desired f-divergence. We introduce a corresponding optimization algorithm and prove its local convergence property with non-linear dynamical systems theory. Experimental results demonstrate the superiority of f-EBM over contrastive divergence, as well as the benefits of training EBMs using f-divergences other than KL.}
}


@InProceedings{yoon2021autoencoding,
  title = 	 {Autoencoding Under Normalization Constraints},
  author =       {Yoon, Sangwoong and Noh, Yung-Kyun and Park, Frank},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12087--12097},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},}


@inproceedings{che2020yourgan,
 author = {Che, Tong and ZHANG, Ruixiang and Sohl-Dickstein, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {12275--12287},
 publisher = {Curran Associates, Inc.},
 title = {Your GAN is Secretly an Energy-based Model and You Should Use Discriminator Driven Latent Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/90525e70b7842930586545c6f1c9310c-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{ho2020ddpm,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}


@article{ipm,
 ISSN = {00018678},
 URL = {http://www.jstor.org/stable/1428011},
 author = {Alfred Müller},
 journal = {Advances in Applied Probability},
 number = {2},
 pages = {429--443},
 publisher = {Applied Probability Trust},
 title = {Integral Probability Metrics and Their Generating Classes of Functions},
 urldate = {2023-08-20},
 volume = {29},
 year = {1997}
}



@InProceedings{lin20gda,
  title = 	 {On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems},
  author =       {Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6083--6093},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/lin20a/lin20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/lin20a.html},
}


@inproceedings{
nalisnick2018do,
title={Do Deep Generative Models Know What They Don't Know? },
author={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1xwNhCcYm},
}


@inproceedings{
Serra2020Input,
title={Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models},
author={Joan Serrà and David Álvarez and Vicenç Gómez and Olga Slizovskaia and José F. Núñez and Jordi Luque},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SyxIWpVYvr}
}







@article{fan1952minimax,
author = {Ky Fan },
title = {Fixed-point and Minimax Theorems in Locally Convex Topological Linear Spaces},
journal = {Proceedings of the National Academy of Sciences},
volume = {38},
number = {2},
pages = {121-126},
year = {1952},
doi = {10.1073/pnas.38.2.121},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.38.2.121},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.38.2.121}}


@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={AAAI},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@inproceedings{ziebart2010modeling,
author = {Ziebart, Brian D. and Bagnell, J. Andrew and Dey, Anind K.},
title = {Modeling Interaction via the Principle of Maximum Causal Entropy},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {1255–1262},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@article{wulfmeier2015maximum,
  title={Maximum entropy deep inverse reinforcement learning},
  author={Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar},
  journal={arXiv preprint arXiv:1507.04888},
  year={2015}
}

@article{arora2021survey,
  title={A survey of inverse reinforcement learning: Challenges, methods and progress},
  author={Arora, Saurabh and Doshi, Prashant},
  journal={Artificial Intelligence},
  volume={297},
  pages={103500},
  year={2021},
  publisher={Elsevier}
}


@InProceedings{finn2016guided,
  title = 	 {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization},
  author = 	 {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {49--58},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/finn16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/finn16.html},
  abstract = 	 {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.}
}


@article{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@InProceedings{arjovsky17wasserstein,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author =       {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {214--223},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/arjovsky17a.html},
  abstract = 	 {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.}
}


@inproceedings{kwon2022score,
 author = {Kwon, Dohyun and Fan, Ying and Lee, Kangwook},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {20205--20217},
 publisher = {Curran Associates, Inc.},
 title = {Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/7f52f6b8f107931127eefe15429ee278-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}



@InProceedings{sohl-dickstein15,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}


@article{kong2021fast,
  title={On fast sampling of diffusion probabilistic models},
  author={Kong, Zhifeng and Ping, Wei},
  journal={arXiv preprint arXiv:2106.00132},
  year={2021}
}

@inproceedings{song2021maximum,
 author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1415--1428},
 publisher = {Curran Associates, Inc.},
 title = {Maximum Likelihood Training of Score-Based Diffusion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0a9fdbb17feb6ccb7ec405cfb85222c4-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{
chen2023sampling,
title={Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
author={Sitan Chen and Sinho Chewi and Jerry Li and Yuanzhi Li and Adil Salim and Anru Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=zyLVMgsZ0U_}
}


@inproceedings{
gao2021learning,
title={Learning Energy-Based Models by Diffusion Recovery Likelihood},
author={Ruiqi Gao and Yang Song and Ben Poole and Ying Nian Wu and Diederik P Kingma},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=v_1Soh8QUNc}
}


@article{kozachenko1987sample,
  title={Sample estimate of the entropy of a random vector},
  author={Kozachenko, Lyudmyla F and Leonenko, Nikolai N},
  journal={Problemy Peredachi Informatsii},
  volume={23},
  number={2},
  pages={9--16},
  year={1987},
  publisher={Russian Academy of Sciences, Branch of Informatics, Computer Equipment and~…}
}

@inproceedings{dhariwal2021diffusion,
 author = {Dhariwal, Prafulla and Nichol, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8780--8794},
 publisher = {Curran Associates, Inc.},
 title = {Diffusion Models Beat GANs on Image Synthesis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
 volume = {34},
 year = {2021}
}

@book{ryu2022large,
  title={Large-scale convex optimization: algorithms \& analyses via monotone operators},
  author={Ryu, Ernest K and Yin, Wotao},
  year={2022},
  publisher={Cambridge University Press}
}

@inproceedings{
fan2023reinforcement,
title={Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models},
author={Ying Fan and Olivia Watkins and Yuqing Du and Hao Liu and Moonkyung Ryu and Craig Boutilier and Pieter Abbeel and Mohammad Ghavamzadeh and Kangwook Lee and Kimin Lee},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=8OTPepXzeh}
}

@inproceedings{ouyang2022,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@article{clark2023directly,
  title={Directly fine-tuning diffusion models on differentiable rewards},
  author={Clark, Kevin and Vicol, Paul and Swersky, Kevin and Fleet, David J},
  journal={arXiv preprint arXiv:2309.17400},
  year={2023}
}

@article{zhang2023hive,
  title={HIVE: Harnessing Human Feedback for Instructional Visual Editing},
  author={Zhang, Shu and Yang, Xinyi and Feng, Yihao and Qin, Can and Chen, Chia-Chih and Yu, Ning and Chen, Zeyuan and Wang, Huan and Savarese, Silvio and Ermon, Stefano and Xiong, Caiming and Xu, Ran},
  journal={arXiv preprint arXiv:2303.09618},
  year={2023}
}

@article{black2023training,
  title={Training diffusion models with reinforcement learning},
  author={Black, Kevin and Janner, Michael and Du, Yilun and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2305.13301},
  year={2023}
}

@inproceedings{
yoon2023energybased,
title={Energy-Based Models for Anomaly Detection: A Manifold Diffusion Recovery Approach},
author={Sangwoong Yoon and Young-Uk Jin and Yung-Kyun Noh and Frank C. Park},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=4nSDDokpfK}
}

@inproceedings{du2020compositional,
 author = {Du, Yilun and Li, Shuang and Mordatch, Igor},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6637--6647},
 publisher = {Curran Associates, Inc.},
 title = {Compositional Visual Generation with Energy Based Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/49856ed476ad01fcff881d57e161d73f-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{luo2023training,
  title={Training Energy-Based Models with Diffusion Contrastive Divergences},
  author={Luo, Weijian and Jiang, Hao and Hu, Tianyang and Sun, Jiacheng and Li, Zhenguo and Zhang, Zhihua},
  journal={arXiv preprint arXiv:2307.01668},
  year={2023}
}

@article{mokrov2023energy,
  title={Energy-guided Entropic Neural Optimal Transport},
  author={Mokrov, Petr and Korotin, Alexander and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2304.06094},
  year={2023}
}

@inproceedings{cuturi2013sinkhorn,
 author = {Cuturi, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},
 volume = {26},
 year = {2013}
}


@article{greensmith2004variance,
  title={Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning.},
  author={Greensmith, Evan and Bartlett, Peter L and Baxter, Jonathan},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={9},
  year={2004}
}

@article{kumar2019maximum,
  title={Maximum entropy generators for energy-based models},
  author={Kumar, Rithesh and Ozair, Sherjil and Goyal, Anirudh and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1901.08508},
  year={2019}
}

@InProceedings{Abbasnejad_2019_CVPR,
author = {Abbasnejad, M. Ehsan and Shi, Qinfeng and Hengel, Anton van den and Liu, Lingqiao},
title = {A Generative Adversarial Density Estimator},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{BoDai2019,
 author = {Dai, Bo and Liu, Zhen and Dai, Hanjun and He, Niao and Gretton, Arthur and Song, Le and Schuurmans, Dale},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Exponential Family Estimation via Adversarial Dynamics Embedding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/767d01b4bac1a1e8824c9b9f7cc79a04-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{
dai2017calibrating,
title={Calibrating Energy-based Generative Adversarial Networks},
author={Zihang Dai and Amjad Almahairi and Philip Bachman and Eduard Hovy and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SyxeqhP9ll}
}

@inproceedings{
salimans2022progressive,
title={Progressive Distillation for Fast Sampling of Diffusion Models},
author={Tim Salimans and Jonathan Ho},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TIdIXIpzhoI}
}

@article{xu2023ufogen,
  title={Ufogen: You forward once large scale text-to-image generation via diffusion gans},
  author={Xu, Yanwu and Zhao, Yang and Xiao, Zhisheng and Hou, Tingbo},
  journal={arXiv preprint arXiv:2311.09257},
  year={2023}
}

@article{sauer2023adversarial,
  title={Adversarial diffusion distillation},
  author={Sauer, Axel and Lorenz, Dominik and Blattmann, Andreas and Rombach, Robin},
  journal={arXiv preprint arXiv:2311.17042},
  year={2023}
}

@article{san2021noise,
  title={Noise estimation for generative diffusion models},
  author={San-Roman, Robin and Nachmani, Eliya and Wolf, Lior},
  journal={arXiv preprint arXiv:2104.02600},
  year={2021}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for Inverse Reinforcement Learning},
  author={Ng, Andrew Y and Russell, Stuart J},
  booktitle={Proceedings of the Seventeenth International Conference on Machine Learning},
  pages={663--670},
  year={2000}
}

@article{finn2016connection,
  title={A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models},
  author={Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.03852},
  year={2016}
}

@article{neal2003funnel,
  title={Slice sampling. The Annals of Statistics},
  author={Neal, Radford M},
  journal={Project Euclid},
  url = {https://doi.org/10.1214/aos/1056562461},
  year={2003}
}

@InProceedings{haarnoja18soft,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/haarnoja18b.html},
  abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}

@inproceedings{pomerleau1988alvinn,
 author = {Pomerleau, Dean A.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {ALVINN: An Autonomous Land Vehicle in a Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},
 volume = {1},
 year = {1988}
}



@InProceedings{wallace2023diffusion,
    author    = {Wallace, Bram and Dang, Meihua and Rafailov, Rafael and Zhou, Linqi and Lou, Aaron and Purushwalkam, Senthil and Ermon, Stefano and Xiong, Caiming and Joty, Shafiq and Naik, Nikhil},
    title     = {Diffusion Model Alignment Using Direct Preference Optimization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {8228-8238}
}

@inproceedings{
song2021scorebased,
title={Score-Based Generative Modeling through Stochastic Differential Equations},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PxTIG12RRHS}
}


@inproceedings{you22uniad,
 author = {You, Zhiyuan and Cui, Lei and Shen, Yujun and Yang, Kai and Lu, Xin and Zheng, Yu and Le, Xinyi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {4571--4584},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Model for Multi-class Anomaly Detection},
 volume = {35},
 year = {2022}
}


@InProceedings{Zavrtanik_2021_ICCV,
    author    = {Zavrtanik, Vitjan and Kristan, Matej and Sko\v{c}aj, Danijel},
    title     = {DRAEM - A Discriminatively Trained Reconstruction Embedding for Surface Anomaly Detection},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {8330-8339}
}


@inproceedings{
song2021denoising,
title={Denoising Diffusion Implicit Models},
author={Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=St1giarCHLP}
}

@inproceedings{karras2022elucidating,
 author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {26565--26577},
 publisher = {Curran Associates, Inc.},
 title = {Elucidating the Design Space of Diffusion-Based Generative Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a98846e9d9cc01cfb87eb694d946ce6b-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{kingma2021variational,
 author = {Kingma, Diederik and Salimans, Tim and Poole, Ben and Ho, Jonathan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {21696--21707},
 publisher = {Curran Associates, Inc.},
 title = {Variational Diffusion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/b578f2a52a0229873fefc2a4b06377fa-Paper.pdf},
 volume = {34},
 year = {2021}
}



@InProceedings{nichol21improved,
  title = 	 {Improved Denoising Diffusion Probabilistic Models},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8162--8171},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nichol21a.html},
  abstract = 	 {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.}
}



@inproceedings{
bao2022analyticdpm,
title={Analytic-{DPM}: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models},
author={Fan Bao and Chongxuan Li and Jun Zhu and Bo Zhang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=0xiJLKH-ufZ}
}

@article{jolicoeur2021gotta,
  title={Gotta go fast when generating data with score-based models},
  author={Jolicoeur-Martineau, Alexia and Li, Ke and Pich{\'e}-Taillefer, R{\'e}mi and Kachman, Tal and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:2105.14080},
  year={2021}
}

@inproceedings{lu2022dpmsolver,
 author = {Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and LI, Chongxuan and Zhu, Jun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {5775--5787},
 publisher = {Curran Associates, Inc.},
 title = {DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/260a14acce2a89dad36adc8eefe7c59e-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{
zhang2023gddim,
title={g{DDIM}: Generalized denoising diffusion implicit models},
author={Qinsheng Zhang and Molei Tao and Yongxin Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=1hKE9qjvz-}
}

@inproceedings{
watson2022learning,
title={Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality},
author={Daniel Watson and William Chan and Jonathan Ho and Mohammad Norouzi},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=VFBjuF8HEp}
}

@inproceedings{
zhang2023fast,
title={Fast Sampling of Diffusion Models with Exponential Integrator},
author={Qinsheng Zhang and Yongxin Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Loek7hfb46P}
}

@article{berthelot2023tract,
  title={Tract: Denoising diffusion models with transitive closure time-distillation},
  author={Berthelot, David and Autef, Arnaud and Lin, Jierui and Yap, Dian Ang and Zhai, Shuangfei and Hu, Siyuan and Zheng, Daniel and Talbott, Walter and Gu, Eric},
  journal={arXiv preprint arXiv:2303.04248},
  year={2023}
}

@misc{luhman2021knowledge,
      title={Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed}, 
      author={Eric Luhman and Troy Luhman},
      year={2021},
      eprint={2101.02388},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{zheng2023fastsampling,
  title = 	 {Fast Sampling of Diffusion Models via Operator Learning},
  author =       {Zheng, Hongkai and Nie, Weili and Vahdat, Arash and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {42390--42402},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/zheng23d/zheng23d.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zheng23d.html},
  abstract = 	 {Diffusion models have found widespread adoption in various areas. However, their sampling process is slow because it requires hundreds to thousands of network evaluations to emulate a continuous process defined by differential equations. In this work, we use neural operators, an efficient method to solve the probability flow differential equations, to accelerate the sampling process of diffusion models. Compared to other fast sampling methods that have a sequential nature, we are the first to propose a parallel decoding method that generates images with only one model forward pass. We propose <em>diffusion model sampling with neural operator</em> (DSNO) that maps the initial condition, i.e., Gaussian distribution, to the continuous-time solution trajectory of the reverse diffusion process. To model the temporal correlations along the trajectory, we introduce temporal convolution layers that are parameterized in the Fourier space into the given diffusion model backbone. We show our method achieves state-of-the-art FID of 3.78 for CIFAR-10 and 7.83 for ImageNet-64 in the one-model-evaluation setting.}
}

@INPROCEEDINGS{sun2023accelerating,
  author={Sun, Wujie and Chen, Defang and Wang, Can and Ye, Deshi and Feng, Yan and Chen, Chun},
  booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Accelerating Diffusion Sampling with Classifier-based Feature Distillation}, 
  year={2023},
  volume={},
  number={},
  pages={810-815},
  keywords={Codes;diffusion model;knowledge distillation;image generation;fast sampling},
  doi={10.1109/ICME55011.2023.00144}}

@inproceedings{
liu2023flow,
title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
author={Xingchao Liu and Chengyue Gong and qiang liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=XVjTT1nw5z}
}


@InProceedings{song2023consistency,
  title = 	 {Consistency Models},
  author =       {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32211--32252},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/song23a/song23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/song23a.html},
  abstract = 	 {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.}
}



@InProceedings{liu2023i2sb,
  title = 	 {{I}$^2${SB}: Image-to-Image Schrödinger Bridge},
  author =       {Liu, Guan-Horng and Vahdat, Arash and Huang, De-An and Theodorou, Evangelos and Nie, Weili and Anandkumar, Anima},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {22042--22062},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/liu23ai/liu23ai.pdf},
  url = 	 {https://proceedings.mlr.press/v202/liu23ai.html},
  abstract = 	 {We propose Image-to-Image Schrödinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schrödinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256$\times$256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. Moreover, I$^2$SB matches the performance of inverse methods that additionally require the knowledge of the corruption operators. Our work opens up new algorithmic opportunities for developing efficient nonlinear diffusion models on a large scale. Project page and codes: https://i2sb.github.io/}
}


@inproceedings{
albergo2023building,
title={Building Normalizing Flows with Stochastic Interpolants},
author={Michael Samuel Albergo and Eric Vanden-Eijnden},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=li7qeBbCR1t}
}

@inproceedings{
su2023dual,
title={Dual Diffusion Implicit Bridges for Image-to-Image Translation},
author={Xuan Su and Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=5HLoTvVGDe}
}

@inproceedings{
xiao2022tackling,
title={Tackling the Generative Learning Trilemma with Denoising Diffusion {GAN}s},
author={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=JprM0p-q0Co}
}

@inproceedings{xu2023siddm,
 author = {xu, yanwu and Gong, Mingming and Xie, Shaoan and Wei, Wei and Grundmann, Matthias and Batmanghelich, Kayhan and Hou, Tingbo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {17383--17394},
 publisher = {Curran Associates, Inc.},
 title = {Semi-Implicit Denoising Diffusion Models (SIDDMs)},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3882ca2c952276247fe9a993193b00e4-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{geng2024improving,
  title={Improving Adversarial Energy-Based Model via Diffusion Process},
  author={Geng, Cong and Han, Tian and Jiang, Peng-Tao and Zhang, Hao and Chen, Jinwei and Hauberg, S{\o}ren and Li, Bo},
  journal={arXiv preprint arXiv:2403.01666},
  year={2024}
}

@inproceedings{yu2023learning,
 author = {Yu, Peiyu and Zhu, Yaxuan and Xie, Sirui and Ma, Xiaojian (Shawn) and Gao, Ruiqi and Zhu, Song-Chun and Wu, Ying Nian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {42717--42747},
 publisher = {Curran Associates, Inc.},
 title = {Learning Energy-Based Prior Model with Diffusion-Amortized MCMC},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/85381f4549b5ddf1d48e2e287d7d3d15-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@article{bergmann2021mvtec,
  title={The MVTec anomaly detection dataset: a comprehensive real-world dataset for unsupervised anomaly detection},
  author={Bergmann, Paul and Batzner, Kilian and Fauser, Michael and Sattlegger, David and Steger, Carsten},
  journal={International Journal of Computer Vision},
  volume={129},
  number={4},
  pages={1038--1059},
  year={2021},
  publisher={Springer}
}

@article{uehara2024fine,
  title={Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control},
  author={Uehara, Masatoshi and Zhao, Yulai and Black, Kevin and Hajiramezanali, Ehsan and Scalia, Gabriele and Diamant, Nathaniel Lee and Tseng, Alex M and Biancalani, Tommaso and Levine, Sergey},
  journal={arXiv preprint arXiv:2402.15194},
  year={2024}
}

@article{lee2024parrot,
  title={Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation},
  author={Lee, Seung Hyun and Li, Yinxiao and Ke, Junjie and Yoo, Innfarn and Zhang, Han and Yu, Jiahui and Wang, Qifei and Deng, Fei and Entis, Glenn and He, Junfeng and others},
  journal={arXiv preprint arXiv:2401.05675},
  year={2024}
}

@article{guo2024versat2i,
  title={VersaT2I: Improving Text-to-Image Models with Versatile Reward},
  author={Guo, Jianshu and Chai, Wenhao and Deng, Jie and Huang, Hsiang-Wei and Ye, Tian and Xu, Yichen and Zhang, Jiawei and Hwang, Jenq-Neng and Wang, Gaoang},
  journal={arXiv preprint arXiv:2403.18493},
  year={2024}
}

@inproceedings{goodfellow2014gan,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}


@inproceedings{
brock2018large,
title={Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
author={Andrew Brock and Jeff Donahue and Karen Simonyan},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1xsqj09Fm},
}

@article{krizhevsky2009learning,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Krizhevsky, A},
  journal={Master's thesis, University of Tronto},
  year={2009}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{
reddy2020sqil,
title={{\{}SQIL{\}}: Imitation Learning via Reinforcement Learning with Sparse Rewards},
author={Siddharth Reddy and Anca D. Dragan and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1xKd24twB}
}


@InProceedings{ross11reduction,
  title = 	 {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  author = 	 {Ross, Stephane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {627--635},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/ross11a/ross11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/ross11a.html},
  abstract = 	 {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.}
}

@article{berner2022optimal,
  title={An optimal control perspective on diffusion-based generative modeling},
  author={Berner, Julius and Richter, Lorenz and Ullrich, Karen},
  journal={arXiv preprint arXiv:2211.01364},
  year={2022}
}

@inproceedings{geng2021bounds,
 author = {Geng, Cong and Wang, Jia and Gao, Zhiyong and Frellsen, Jes and Hauberg, S\o ren},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {19808--19821},
 publisher = {Curran Associates, Inc.},
 title = {Bounds all around: training energy-based models with bidirectional bounds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/a4d8e2a7e0d0c102339f97716d2fdfb6-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{chen2018neural,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Ordinary Differential Equations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
 volume = {31},
 year = {2018}
}



@InProceedings{belghazi18mutual,
  title = 	 {Mutual Information Neural Estimation},
  author =       {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {531--540},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/belghazi18a.html},
  abstract = 	 {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.}
}

@article{nguyen2010estimating,
  title={Estimating divergence functionals and the likelihood ratio by convex risk minimization},
  author={Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael I},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={11},
  pages={5847--5861},
  year={2010},
  publisher={IEEE}
}

@inproceedings{sauer2022stylegan,
  title={Stylegan-xl: Scaling stylegan to large diverse datasets},
  author={Sauer, Axel and Schwarz, Katja and Geiger, Andreas},
  booktitle={ACM SIGGRAPH 2022 conference proceedings},
  pages={1--10},
  year={2022}
}

@inproceedings{Heusel2017gan,
 author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf},
 volume = {30},
 year = {2017}
}


@inproceedings{kyn2019improved,
 author = {Kynk\"{a}\"{a}nniemi, Tuomas and Karras, Tero and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Precision and Recall Metric for Assessing Generative Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/0234c510bc6d908b28c70ff313743079-Paper.pdf},
 volume = {32},
 year = {2019}
}



@InProceedings{kim2023refining,
  title = 	 {Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models},
  author =       {Kim, Dongjun and Kim, Yeongmin and Kwon, Se Jung and Kang, Wanmo and Moon, Il-Chul},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {16567--16598},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/kim23i/kim23i.pdf},
  url = 	 {https://proceedings.mlr.press/v202/kim23i.html},
  abstract = 	 {The proposed method, <b>Discriminator Guidance</b>, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data’s FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.}
}

@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@article{zeng2022maximum,
  title={Maximum-likelihood inverse reinforcement learning with finite-time guarantees},
  author={Zeng, Siliang and Li, Chenliang and Garcia, Alfredo and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10122--10135},
  year={2022}
}


@article{renard2024convergence,
  title={Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm},
  author={Renard, Titouan and Schlaginhaufen, Andreas and Ni, Tingting and Kamgarpour, Maryam},
  journal={arXiv preprint arXiv:2403.16829},
  year={2024}
}

@InProceedings{Karras_2020_CVPR,
author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
title = {Analyzing and Improving the Image Quality of StyleGAN},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@article{tang2024fine,
  title={Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond},
  author={Tang, Wenpin},
  journal={arXiv preprint arXiv:2403.06279},
  year={2024}
}

@article{uehara2024feedback,
  title={Feedback efficient online fine-tuning of diffusion models},
  author={Uehara, Masatoshi and Zhao, Yulai and Black, Kevin and Hajiramezanali, Ehsan and Scalia, Gabriele and Diamant, Nathaniel Lee and Tseng, Alex M and Levine, Sergey and Biancalani, Tommaso},
  journal={arXiv preprint arXiv:2402.16359},
  year={2024}
}

@article{uehara2024bridging,
  title={Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models},
  author={Uehara, Masatoshi and Zhao, Yulai and Hajiramezanali, Ehsan and Scalia, Gabriele and Eraslan, G{\"o}kcen and Lal, Avantika and Levine, Sergey and Biancalani, Tommaso},
  journal={arXiv preprint arXiv:2405.19673},
  year={2024}
}


@InProceedings{geist2019theory,
  title = 	 {A Theory of Regularized {M}arkov Decision Processes},
  author =       {Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2160--2169},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/geist19a/geist19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/geist19a.html},
  abstract = 	 {Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler divergence. We propose a general theory of regularized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modified policy iteration approach, encompassing both policy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman operator and the Legendre-Fenchel transform, a classical tool of convex optimization. This approach allows for error propagation analyses of general algorithmic schemes of which (possibly variants of) classical algorithms such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are special cases. This also draws connections to proximal convex optimization, especially to Mirror Descent.}
}

@article{yu2015lsun,
  title={Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop},
  author={Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
  journal={arXiv preprint arXiv:1506.03365},
  year={2015}
}

@inproceedings{
yoon2024maximum,
title={Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models},
author={Sangwoong Yoon and Himchan Hwang and Dohyun Kwon and Yung-Kyun Noh and Frank C. Park},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=V0oJaLqY4E}
}

@inproceedings{
vargas2023denoising,
title={Denoising Diffusion Samplers},
author={Francisco Vargas and Will Sussman Grathwohl and Arnaud Doucet},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8pvnfTAbu1f}
}

@inproceedings{
midgley2023flow,
title={Flow Annealed Importance Sampling Bootstrap},
author={Laurence Illing Midgley and Vincent Stimper and Gregor N. C. Simm and Bernhard Sch{\"o}lkopf and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=XCTVFJwS9LJ}
}

@inproceedings{
akhound-sadegh2024iterated,
title={Iterated Denoising Energy Matching for Sampling from Boltzmann Densities},
author={Tara Akhound-Sadegh and Jarrid Rector-Brooks and Joey Bose and Sarthak Mittal and Pablo Lemos and Cheng-Hao Liu and Marcin Sendera and Siamak Ravanbakhsh and Gauthier Gidel and Yoshua Bengio and Nikolay Malkin and Alexander Tong},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=gVjMwLDFoQ}
}

@article{he2024training,
  title={Training Neural Samplers with Reverse Diffusive KL Divergence},
  author={He, Jiajun and Chen, Wenlin and Zhang, Mingtian and Barber, David and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  journal={arXiv preprint arXiv:2410.12456},
  year={2024}
}

@article{fuchs2020se,
  title={Se (3)-transformers: 3d roto-translation equivariant attention networks},
  author={Fuchs, Fabian and Worrall, Daniel and Fischer, Volker and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1970--1981},
  year={2020}
}

@article{thomas2018tensor,
  title={Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds},
  author={Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  journal={arXiv preprint arXiv:1802.08219},
  year={2018}
}

@inproceedings{satorras2021n,
  title={E (n) equivariant graph neural networks},
  author={Satorras, V{\i}ctor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle={International conference on machine learning},
  pages={9323--9332},
  year={2021},
  organization={PMLR}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group UK London}
}


@inproceedings{
phillips2024particle,
title={Particle Denoising Diffusion Sampler},
author={Angus Phillips and Hai-Dang Dau and Michael John Hutchinson and Valentin De Bortoli and George Deligiannidis and Arnaud Doucet},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=vMUnnS4OWC}
}

@article{chen2024sequential,
  title={Sequential Controlled Langevin Diffusions},
  author={Chen, Junhua and Richter, Lorenz and Berner, Julius and Blessing, Denis and Neumann, Gerhard and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2412.07081},
  year={2024}
}

@article{wang2024energy,
  title={Energy based diffusion generator for efficient sampling of Boltzmann distributions},
  author={Wang, Yan and Guo, Ling and Wu, Hao and Zhou, Tao},
  journal={arXiv preprint arXiv:2401.02080},
  year={2024}
}

@Article{heimel2023madnis,
	title={{MadNIS - Neural multi-channel importance sampling}},
	author={Theo Heimel and Ramon Winterhalder and Anja Butter and Joshua Isaacson and Claudius Krause and Fabio Maltoni and Olivier Mattelaer and Tilman Plehn},
	journal={SciPost Phys.},
	volume={15},
	pages={141},
	year={2023},
	publisher={SciPost},
	doi={10.21468/SciPostPhys.15.4.141},
	url={https://scipost.org/10.21468/SciPostPhys.15.4.141},
}









@article{noe2019boltzmann,
author = {Frank Noé  and Simon Olsson  and Jonas Köhler  and Hao Wu },
title = {Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning},
journal = {Science},
volume = {365},
number = {6457},
pages = {eaaw1147},
year = {2019},
doi = {10.1126/science.aaw1147},
URL = {https://www.science.org/doi/abs/10.1126/science.aaw1147},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aaw1147},
abstract = {Molecular dynamics or Monte Carlo methods can be used to sample equilibrium states, but these methods become computationally expensive for complex systems, where the transition from one equilibrium state to another may only occur through rare events. Noé et al. used neural networks and deep learning to generate distributions of independent soft condensed-matter samples at equilibrium (see the Perspective by Tuckerman). Supervised training is used to construct invertible transformations between the coordinates of the complex system of interest and simple Gaussian coordinates of the same dimensionality. Thus, configurations can be sampled in this simpler coordinate system and then transformed back into the complex one using the correct statistical weighting. Science, this issue p. eaaw1147; see also p. 982 By combining deep learning and statistical mechanics, neural networks sample the equilibrium distribution of many-body systems. Computing equilibrium states in condensed-matter many-body systems, such as solvated proteins, is a long-standing challenge. Lacking methods for generating statistically independent equilibrium samples in “one shot,” vast computational effort is invested for simulating these systems in small steps, e.g., using molecular dynamics. Combining deep learning and statistical mechanics, we developed Boltzmann generators, which are shown to generate unbiased one-shot equilibrium samples of representative condensed-matter systems and proteins. Boltzmann generators use neural networks to learn a coordinate transformation of the complex configurational equilibrium distribution to a distribution that can be easily sampled. Accurate computation of free-energy differences and discovery of new configurations are demonstrated, providing a statistical mechanics tool that can avoid rare events during sampling without prior knowledge of reaction coordinates.}}



@book{leimkuhler2015molecular,
  title={Molecular Dynamics: With Deterministic and Stochastic Numerical Methods},
  author={Leimkuhler, Benedict and Matthews, Charles},
  year={2015},
  publisher={Springer},
  address={Cham, Switzerland},
  isbn={978-3-319-16374-1},
  doi={10.1007/978-3-319-16375-8},
  url={https://link.springer.com/book/10.1007/978-3-319-16375-8}
}


@InProceedings{meila2021neural,
  title = 	 {Neural SDEs as Infinite-Dimensional GANs},
  author =       {Kidger, Patrick and Foster, James and Li, Xuechen and Lyons, Terry J},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5453--5463},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kidger21b/kidger21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/kidger21b.html},
  abstract = 	 {Stochastic differential equations (SDEs) are a staple of mathematical modelling of temporal dynamics. However, a fundamental limitation has been that such models have typically been relatively inflexible, which recent work introducing Neural SDEs has sought to solve. Here, we show that the current classical approach to fitting SDEs may be approached as a special case of (Wasserstein) GANs, and in doing so the neural and classical regimes may be brought together. The input noise is Brownian motion, the output samples are time-evolving paths produced by a numerical solver, and by parameterising a discriminator as a Neural Controlled Differential Equation (CDE), we obtain Neural SDEs as (in modern machine learning parlance) continuous-time generative time series models. Unlike previous work on this problem, this is a direct extension of the classical approach without reference to either prespecified statistics or density functions. Arbitrary drift and diffusions are admissible, so as the Wasserstein loss has a unique global minima, in the infinite data limit \textit{any} SDE may be learnt.}
}

@article{tzen2019neural,
  title={Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit},
  author={Tzen, Belinda and Raginsky, Maxim},
  journal={arXiv preprint arXiv:1905.09883},
  year={2019}
}

@article{roberts1996exponential,
  title={Exponential convergence of Langevin distributions and their discrete approximations},
  author={Roberts, Gareth O and Tweedie, Richard L and others},
  journal={Bernoulli},
  volume={2},
  number={4},
  pages={341--363},
  year={1996},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@inproceedings{
huang2024reverse,
title={Reverse Diffusion Monte Carlo},
author={Xunpeng Huang and Hanze Dong and Yifan HAO and Yian Ma and Tong Zhang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=kIPEyMSdFV}
}

@article{mcdonald2022proposal,
  title={Proposal of a score based approach to sampling using Monte Carlo estimation of score and oracle access to target density},
  author={McDonald, Curtis and Barron, Andrew},
  journal={arXiv preprint arXiv:2212.03325},
  year={2022}
}

@inproceedings{hoogeboom2022equivariant,
  title={Equivariant diffusion for molecule generation in 3d},
  author={Hoogeboom, Emiel and Satorras, V{\i}ctor Garcia and Vignac, Cl{\'e}ment and Welling, Max},
  booktitle={International conference on machine learning},
  pages={8867--8887},
  year={2022},
  organization={PMLR}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@misc{neal1998annealedimportancesampling,
      title={Annealed Importance Sampling}, 
      author={Radford M. Neal},
      year={1998},
      eprint={physics/9803008},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph},
      url={https://arxiv.org/abs/physics/9803008}, 
}

@article{10.1111/j.1467-9868.2006.00553.x,
    author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
    title = {Sequential Monte Carlo Samplers},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {68},
    number = {3},
    pages = {411-436},
    year = {2006},
    month = {05},
    abstract = {We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.},
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2006.00553.x},
    url = {https://doi.org/10.1111/j.1467-9868.2006.00553.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/68/3/411/49795343/jrsssb\_68\_3\_411.pdf},
}

@article{Leimkuhler_2012,
   title={Rational Construction of Stochastic Numerical Methods for Molecular Sampling},
   ISSN={1687-1197},
   url={http://dx.doi.org/10.1093/amrx/abs010},
   DOI={10.1093/amrx/abs010},
   journal={Applied Mathematics Research eXpress},
   publisher={Oxford University Press (OUP)},
   author={Leimkuhler, B. and Matthews, C.},
   year={2012},
   month=jun }

%ö
@misc{kohler2020equivariantflowsexactlikelihood,
      title={Equivariant Flows: Exact Likelihood Generative Learning for Symmetric Densities}, 
      author = {K\"{o}hler, Jonas and Klein, Leon and No\'{e}, Frank},
      year={2020},
      eprint={2006.02425},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2006.02425}, 
}

@article{papamakarios2021normalizing,
  title={Normalizing flows for probabilistic modeling and inference},
  author={Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={57},
  pages={1--64},
  year={2021}
} 

@article{moral2006smc,
    author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
    title = {Sequential Monte Carlo Samplers},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {68},
    number = {3},
    pages = {411-436},
    year = {2006},
    month = {05},
    abstract = {We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.},
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2006.00553.x},
    url = {https://doi.org/10.1111/j.1467-9868.2006.00553.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/68/3/411/49795343/jrsssb\_68\_3\_411.pdf},
}

@article{neal2001annealed,
  title={Annealed importance sampling},
  author={Neal, Radford M},
  journal={Statistics and computing},
  volume={11},
  pages={125--139},
  year={2001},
  publisher={Springer}
}

@inproceedings{
richter2024improved,
title={Improved sampling via learned diffusions},
author={Lorenz Richter and Julius Berner},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=h4pNROsO06}
}

@inproceedings{kidger2021neuralsde,
 author = {Kidger, Patrick and Foster, James and Li, Xuechen (Chen) and Lyons, Terry},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {18747--18761},
 publisher = {Curran Associates, Inc.},
 title = {Efficient and Accurate Gradients for Neural SDEs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/9ba196c7a6e89eafd0954de80fc1b224-Paper.pdf},
 volume = {34},
 year = {2021}
}
