%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\RE}{\ensuremath{\mathbb{R}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2024}

\begin{document}

\twocolumn[
\icmltitle{Unused Text for VGS}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}

% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

% \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% \begin{abstract}
% Storage for unused texts.
% \end{abstract}


\onecolumn

\section{Mean value function}
In practice, we fix $\sigma_t = \alpha_t s_t$. Ignoring constant terms, the optimization with respect to $\mu_t$ becomes:
\begin{align}
    \min_{\mu_t} \mathbb{E}_{\epsilon} \left[
         V^{t+1}(\alpha_{t} \bx_{t}+\mu_t+\sigma_t\epsilon) 
     \bigg| \bx_t     \right] + \frac{\tau ||\mu_t||^2 }{2s_t^2\alpha_t^2}
\end{align}
Thus, for a fixed $\bx_t$, the optimal $\mu_t$ can be expressed as:
\begin{align}\label{eq:opt_orginalvalue}
    \mu_{t}^*= \argmin_{\mu_t} \mathbb{E}_{\epsilon} \left[
         V^{t+1}(\alpha_{t} \bx_{t}+\mu_t+\sigma_t\epsilon) 
    \right] + \frac{\tau ||\mu_t||^2 }{2s_t^2\alpha_t^2}
\end{align}
The value gradient sampler employs a first-order approximation of the objective to determine an approximate $\mu_t^*$. Using the mean value function, the original objective can be transformed into a more tractable form. The mean value function is defined as follows:
\begin{align}
    \overline{V^t}(\bx_t) \doteq \mathbb{E}_{\epsilon}[V^{t}(\bx_t + \sigma_{t-1}\epsilon)]
\end{align}
Substituting above into \cref{eq:opt_orginalvalue}, the optimization becomes:
\begin{align}\label{eq:opt_meanvalue}
    \mu^*_t = \argmin_{\mu_t} \overline{V^{t+1}}(\alpha_t\bx_t + \mu_t) + \frac{\tau ||\mu_t||^2 }{2s_t^2\alpha_t^2}
\end{align}
The first-order necessary condition for the above optimization problem can be derived analytically:
\begin{align}
    \mu^*_t + \frac{s_t^2\alpha_t^2}{\tau}\nabla\overline{V^{t+1}}(\alpha_t\bx_t + \mu^*_t) = 0
\end{align}
The solution to $\bx = F(\bx)$ can be obtained iteratively, starting from $\bx^0$ and applying $\bx^{k+1} = F(\bx^k)$ (Convergence guarantee however, requires Lipshitz constraints on $F$). Beginning with $\mu^0_t = 0$, we compute $\mu^1_t = -\frac{s_t^2 \alpha_t^2}{\tau} \nabla \overline{V^{t+1}}(\alpha_t \bx_t)$, which is identical to the original value gradient sampler, except that $V^{t+1}$ is replaced with $\overline{V^{t+1}}$.

\paragraph{Learning the Mean Value Function} To train the mean value function, we collect the mean predictions $\alpha_{t-1} \bx_{t-1} + \mu_{t-1}$ and the corresponding samples $\bx_t$. We then minimize the mean squared error (MSE) between the mean value target, $\overline{V^{t+1}}(\alpha_t \bx_t + \mu_t) + \frac{\tau ||\mu_t||^2 }{2s_t^2 \alpha_t^2}$, and the current mean value prediction, $\overline{V^{t}}(\alpha_{t-1} \bx_{t-1} + \mu_{t-1})$, for $t = 1, \dots, T-1$. For $t = T$, we minimize the MSE between the target $E(\bx_T)$ and the current prediction $\overline{V^{T-1}}(\alpha_{T-1} \bx_{T-1} + \mu_{T-1})$.




\section{Analysis of value gradient sampler - VE $\tilde{q}$}\label{appendix:ve_anal}
In this session, we evaluate the general performance and prove the asymptotic optimality of our VG sampler with VE $\tilde{q}$. In VE $\tilde{q}$ setting, we consider $\tilde{q}(\bx_t|\bx_{t+1}) = \mathcal{N}(\bx_{t+1}, s_t^2I) $. 


In this case, our VG sampler can be expressed as:
\begin{align}
    \pi(\bx_{t+1}|\bx_t) = \mathcal{N}(\bx_{t} + \mu_t(\bx_t), \sigma_t^2I), \quad \mu_t = -s_t^2 \nabla V_{t+1}^{\pi}(\bx_t), \quad \sigma_t = s_t
\end{align}

\paragraph{Performance of VG sampler.}
We evaluate the performance of VG sampler through its the value function, $V_t^{\pi}$.
We derive the Bellman equation for $V_t^{\pi}$ as follows:
\begin{align}
    V_t^{\pi}(\bx_t) &= \mathbb{E}_{\pi(\bx_{t+1}|\bx_t)}\left[-\log{\tilde{q}(\bx_{t}|\bx_{t+1})} + \log{\pi(\bx_{t+1}|\bx_{t})} + V^{\pi}_{t+1}(\bx_{t+1}) \bigg| \bx_t\right] \\
    &= \mathbb{E}_{\bx_{t+1} = \bx_t + \mu_t + \sigma_t \epsilon}\left[\frac{||\bx_t - \bx_{t+1}||^2}{2s_t^2} + D\log{s_t} - D\log{\sigma_t} - \frac{D}{2} + V^{\pi}_{t+1}(\bx_{t+1}) \bigg| \bx_t\right]\\
    &= \frac{||\mu_t||^2}{2s_t^2} + D\frac{\sigma_t^2}{2s_t^2} - \frac{D}{2} + D\log{s_t} - D\log{\sigma_t} + \mathbb{E}_{\epsilon}\left[V_{t+1}^{\pi}(\bx_t + \mu_t + \sigma_t\epsilon)\right]\\
    &= \frac{s_t^2}{2}||\nabla V_{t+1}^{\pi}(\bx_t)||^2 + \mathbb{E}_{\epsilon}\left[V_{t+1}^{\pi}(\bx_t - s_t^2\nabla V_{t+1}^{\pi}(\bx_t) + s_t\epsilon)\right] \label{eq:ve_bellman}
\end{align}
Starting from $V_T^{\pi}(\bx_T) = E(\bx_T)$, we can recursively apply the Bellman equation to obtain the value function of the VG sampler $V_t^{\pi}$.

\paragraph{Asymptotic optimality of VG sampler.} For the analysis of limiting case $T\to \infty$, we view this problem as a discretization of $\bx(t)$ defined on the continuous time interval $\left[0, 1\right]$ such that $\bx_k = \bx\left(\frac{k}{T}\right) = \bx(k\Delta t)$ where $\Delta t = \frac{1}{T}$. Thus $t = k\Delta t$ relates the continuous time representation $t$ and discrete time representation $k$. To control the $s_k$'s in this limiting case, we define the cumulative variance function $v: \left[0, 1\right] \to \mathbb{R}^+$, as $v(K\Delta t) = \sum_{k=0}^{K-1}s_k^2$. As $T\to \infty, \Delta t \to 0, s_k \to 0$, the Bellman equation (Eq. \ref{eq:ve_bellman}) can be simplified as follows: 
\begin{align}
    V_k^{\pi}(\bx_k) &= \frac{s_k^2}{2}||\nabla V_{k+1}^{\pi}(\bx_k)||^2 + \mathbb{E}_{\epsilon}\left[V_{k+1}^{\pi}(\bx_k - s_k^2\nabla V_{k+1}^{\pi}(\bx_k) + s_k\epsilon)\right]\\
    &\approx \frac{s_k^2}{2}||\nabla V_{k+1}^{\pi}(\bx_k)||^2 + V_{k+1}^{\pi}(\bx_k) - s_k^2||\nabla V_{k+1}^{\pi}(\bx_k)||^2 +\frac{\nabla^2V_{k+1}^{\pi}(\bx_k)}{2} s_k^2
\end{align}
Here, $s_k^2$ terms higher than the second order were ignored. Dividing each side with $\Delta t$ and setting $\Delta t \to 0$, we obtain the following PDE for the value function. 
\begin{align}
    \frac{\partial V_t^{\pi}(\bx_t)}{\partial t} - \frac{v'(t)}{2}||\nabla V_{t}^{\pi}(\bx_t)||^2 + \frac{v'(t)}{2}\nabla^2V_{t}^{\pi}(\bx_t) = 0
\end{align}
Here, we used $\frac{s_k^2}{\Delta t} \to v'(k\Delta t) = v'(t)$. For convenience, we write $\bx_t$ instead of $\bx(t)$, with a slight abuse of notation.  


On the other hand, the SDE for the corresponding continuous time VE diffusion process $\tilde{q}(\bx_t)$ can be expressed as:
\begin{align}
    d\bx_{\bar{t}} = \sqrt{v'(\bar{t})}dB_{\bar{t}}
\end{align}
with reverse time $\Bar{t} = 1 - t$. From the Fokker-Planck equation, 
\begin{align}
    \frac{\partial \tilde{q}_{\Bar{t}}(\bx_{\Bar{t}})}{\partial \Bar{t}} = \frac{1}{2}v'(\Bar{t})\nabla^2 \tilde{q}_{\Bar{t}}(\bx_{\Bar{t}})
\end{align}
Substituting $\tilde{q}(\bx_{\bar{t}}) = \exp{-V_{\bar{t}}^*(\bx_{\bar{t}}})$,
\begin{align}
    \frac{\partial V_{\bar{t}}^*(\bx_{\bar{t}})}{\partial \bar{t}}  + \frac{v'(\bar{t})}{2}||\nabla V_{\bar{t}}^{*}(\bx_{\bar{t}})||^2 - \frac{v'(\bar{t})}{2}\nabla^2V_{\bar{t}}^*(\bx_{\bar{t}}) = 0
\end{align}
Rewriting the PDE with forward time $t = 1-\bar{t}$,
\begin{align}
    \frac{\partial V_{t}^*(\bx_{t})}{\partial t} - \frac{v'(t)}{2}||\nabla V_{t}^{*}(\bx_{t})||^2 + \frac{v'(t)}{2}\nabla^2V_{t}^*(\bx_{t}) = 0
\end{align}
We observe that $V_{t}^{\pi}(\bx_t)$ and $V_{t}^{*}(\bx_t)$ satisfy the same boundary condition $V_{1}^{\pi}(\bx_1) = V_{1}^{*}(\bx_1) = E(\bx_1)$ and the same PDE. Therefore, our value gradient sampler is asymptotically optimal for VE $\tilde{q}$, satisfying $V_{t}^{\pi}(\bx_t) = V_{t}^{*}(\bx_t)$.

\section{Analysis of value gradient sampler - VP $\tilde{q}$}\label{appendix:vp_anal}
In this session, we evaluate the general performance and prove the asymptotic optimality of our VG sampler with VP $\tilde{q}$. In VP $\tilde{q}$ setting, we consider $\tilde{q}(\bx_{t}|\bx_{t+1}) = \mathcal{N}(\sqrt{1-s_{t}^{2}} \bx_{t+1}, s_{t}^{2}I)$. 


In this case, our VG sampler can be expressed as:
\begin{align}
    \pi(\bx_{t+1}|\bx_{t}) = \mathcal{N}(\frac{1}{\sqrt{1-s_{t}^{2}}} \bx_{t} + \mu_{t}(x_{t}), \sigma_{t}^{2}I), \quad \mu_{t} = -\frac{s_{t}^{2}}{1-s_{t}^{2}} \nabla V^{t+1}(\frac{\bx_{t}}{\sqrt{1-s_{t}^{2}}}), \quad \sigma_{t} = \frac{s_{t}}{\sqrt{1-s_{t}^{2}}}
\end{align}

\paragraph{Performance of VG sampler.}
We evaluate the performance of the VG sampler through its value function, $V_t^{\pi}$.
We derive the Bellman equation for $V_t^{\pi}$ as follows:
\begin{align}
    V_t^{\pi}(\bx_t) &= \mathbb{E}_{\pi(\bx_{t+1}|\bx_t)}\left[-\log{\tilde{q}(\bx_{t}|\bx_{t+1})} + \log{\pi(\bx_{t+1}|\bx_{t})} + V^{\pi}_{t+1}(\bx_{t+1}) \bigg| \bx_t\right] \\
    &= \mathbb{E}_{\bx_{t+1} = \frac{\bx_{t}}{\sqrt{1-s_{t}^{2}}} + \mu_t + \sigma_t \epsilon}\left[\frac{||\bx_t - \sqrt{1-s_{t}^{2}} \bx_{t+1}||^2}{2s_t^2} + D\log{s_t} - D\log{\sigma_t} - \frac{D}{2} + V^{\pi}_{t+1}(\bx_{t+1}) \bigg| \bx_t\right]\\
    &= \frac{(1-s_{t}^{2})||\mu_t||^2}{2s_t^2} + D\frac{(1-s_{t}^{2})\sigma_t^2}{2s_t^2} - \frac{D}{2} + D\log{s_t} - D\log{\sigma_t} + \mathbb{E}_{\epsilon}\left[V_{t+1}^{\pi}(\frac{\bx_{t}}{\sqrt{1-s_{t}^{2}}} + \mu_t + \sigma_t\epsilon)\right]\\
    &= \frac{s_t^2}{2(1-s_{t}^{2})}||\nabla V_{t+1}^{\pi}(\frac{\bx_{t}}{\sqrt{1-s_{t}^{2}}})||^2 \\
    &+ \mathbb{E}_{\epsilon}\left[V_{t+1}^{\pi}(\frac{\bx_{t}}{\sqrt{1-s_{t}^{2}}} -\frac{s_{t}^{2}}{1-s_{t}^{2}} \nabla V_{t+1}^{\pi}(\frac{\bx_{t}}{\sqrt{1-s_{t}^{2}}}) + \frac{s_{t}}{\sqrt{1-s_{t}^{2}}}\epsilon)\right] + \frac{D}{2}\log(1-s_{t}^{2})\nonumber\label{eq:vp_bellman}
\end{align}
Starting from $V_T^{\pi}(\bx_T) = E(\bx_T)$, we can recursively apply the Bellman equation to obtain the value function of the value gradient sampler $V_t^{\pi}$.

\paragraph{Asymptotic optimality of VG sampler.} We repeat the procedure in \ref{appendix:vp_anal}.
For the analysis of limiting case $T\to \infty$, we view this problem as a discretization of $\bx(t)$ defined on the continuous time interval $\left[0, 1\right]$ such that $\bx_k = \bx\left(\frac{k}{T}\right) = \bx(k\Delta t)$ where $\Delta t = \frac{1}{T}$. Thus $t = k\Delta t$ relates the continuous time representation $t$ and discrete time representation $k$. To control the $s_k$'s in this limiting case, we define the cumulative variance function $v: \left[0, 1\right] \to \mathbb{R}^+$, as $v(K\Delta t) = \sum_{k=0}^{K-1}s_k^2$. As $T\to \infty, \Delta t \to 0, s_k \to 0$, the Bellman equation (Eq. \ref{eq:vp_bellman}) can be simplified as follows: 
\begin{align}
    V_k^{\pi}(\bx_k) &= \frac{s_k^2}{2(1-s_{k}^{2})}||\nabla V_{k+1}^{\pi}(\frac{\bx_{k}}{\sqrt{1-s_{k}^{2}}})||^2 \\
    &+ \mathbb{E}_{\epsilon}\left[V_{k+1}^{\pi}(\frac{\bx_{k}}{\sqrt{1-s_{k}^{2}}} -\frac{s_{k}^{2}}{1-s_{k}^{2}} \nabla V_{k+1}^{\pi}(\frac{\bx_{k}}{\sqrt{1-s_{k}^{2}}}) + \frac{s_{k}}{\sqrt{1-s_{k}^{2}}}\epsilon)\right] + \frac{D}{2}\log(1-s_{k}^{2})\nonumber \\
    (\tiny{T\to \infty, \Delta t \to 0, s_k \to 0}) &\approx \frac{s_k^2}{2}||\nabla V_{k+1}^{\pi}(\bx_{k})||^2 + V_{k+1}^{\pi}(\bx_k) + (\frac{1}{2}s_k^2\bx_k - s_k^2\nabla V_{k+1}^{\pi}(\bx_k))^T \nabla V_{k+1}^{\pi}(\bx_k) \\
    & + \frac{\nabla^2V_{k+1}^{\pi}(\bx_k)}{2}s_k^2 - \frac{D}{2}s_k^2 \nonumber
\end{align}
Here, $s_k^2$ terms higher than the second order were ignored. Dividing each side with $\Delta t$ and setting $\Delta t \to 0$, we obtain the following PDE for the value function.
\begin{align}
    \frac{\partial V_t^{\pi}(\bx_t)}{\partial t} + \frac{v'(t)}{2}\bx_t^T \nabla V_{t}^{\pi}(\bx_t) - \frac{v'(t)}{2}||\nabla V_{t}^{\pi}(\bx_t)||^2 + \frac{v'(t)}{2}\nabla^2V_{t}^{\pi}(\bx_t)  - \frac{Dv'(t)}{2} = 0
\end{align}
Here, we used $\frac{s_k^2}{\Delta t} \to v'(k\Delta t) = v'(t)$. For convenience, we write $\bx_t$ instead of $\bx(t)$, with a slight abuse of notation.  


On the other hand, the SDE for the corresponding continuous time VP diffusion process $\tilde{q}(\bx_t)$ can be expressed as:
\begin{align}
    d\bx_{\bar{t}} = -\frac{1}{2}v'(\bar{t})\bx_{\bar{t}} d\bar{t} + \sqrt{v'(\bar{t})}dB_{\bar{t}}
\end{align}
with reverse time $\bar{t} = 1 - t$. From the Fokker-Plank equation, 
\begin{align}
    \frac{\partial \tilde{q}(\bx_{\bar{t}})}{\partial \bar{t}} = \frac{Dv'(\bar{t})}{2}\tilde{q}(\bx_{\bar{t}}) +  \frac{v'(\bar{t})}{2}\bx_{\bar{t}}^T\nabla\tilde{q}(\bx_{\bar{t}}) + \frac{v'(\bar{t})}{2}\nabla^2\tilde{q}(\bx_{\bar{t}})
\end{align}
Substituting $\tilde{q}(\bx_{\bar{t}}) = \exp{-V_{\bar{t}}^*(\bx_{\bar{t}}})$, 
\begin{align}
    \frac{\partial V_{\bar{t}}^*(\bx_{\bar{t}})}{\partial \bar{t}} +\frac{Dv'(\bar{t})}{2}  - \frac{v'(\bar{t})}{2}\bx_{\bar{t}}^T\nabla V_{\bar{t}}^*(\bx_{\bar{t}}) + \frac{v'(\bar{t})}{2}||\nabla V_{\bar{t}}^{*}(\bx_{\bar{t}})||^2 - \frac{v'(\bar{t})}{2}\nabla^2V_{\bar{t}}^*(\bx_{\bar{t}}) = 0
\end{align}
Rewriting the PDE with forward time $t = 1-\bar{t}$,
\begin{align}
    \frac{\partial V_{t}^*(\bx_{t})}{\partial t} + \frac{v'(t)}{2}\bx_{t}^T\nabla V_{t}^*(\bx_{t}) - \frac{v'(t)}{2}||\nabla V_{t}^{*}(\bx_{t})||^2 + \frac{v'(t)}{2}\nabla^2V_{t}^*(\bx_{t})-\frac{Dv'(t)}{2} = 0
\end{align}
We observe that $V_{t}^{\pi}(\bx_t)$ and $V_{t}^{*}(\bx_t)$ satisfy the same boundary condition $V_{1}^{\pi}(\bx_1) = V_{1}^{*}(\bx_1) = E(\bx_1)$ and the same PDE. Therefore, our value gradient sampler is asymptotically optimal for VP $\tilde{q}$, satisfying $V_{t}^{\pi}(\bx_t) = V_{t}^{*}(\bx_t)$.



\section{Drift in the Auxiliary Process}

Let us consider a case where the auxiliary distribution $\tilde{q}(\bx_t|\bx_{t+1})$ has a drift $\mathbf{u}_t$ in the ``reverse" direction. We will consider the following auxiliary distribution
\begin{align}
    \tilde{q}(\bx_t|\bx_{t+1}) = \frac{1}{\Omega(\bx_{t+1})}\exp\left(-\left\Vert \bx_t - \frac{1}{\alpha_t}(\bx_{t+1} + \mathbf{u}(\bx_{t})) \right\Vert^2/s_t^2 \right),
    % = \mathcal{N}(\bx_t ; \frac{1}{\alpha_{t}}\bx_{t+1} + u(\bx_{t+1}), s_t^2I)
\end{align}
where $\Omega(\bx_{t+1})$ is the normalization constant. Note that $\tilde{q}$ is no longer Gaussian, because $\mathbf{u}(\bx_t)$ depends on $\bx_t$ instead of $\bx_{t+1}$. Let us write $\mathbf{u}_t \doteq \mathbf{u}(\bx_t)$. As $\bx_{t+1}=\alpha_t\bx_t + \mu_t + \sigma_t \epsilon$,
\begin{align}
    \log \tilde{q}(\bx_t|\bx_{t+1}) = -\frac{1}{2s_t^2\alpha_t^2} \left\Vert \mu_t + \mathbf{u}_t + \sigma_t \epsilon  \right\Vert^2 - \log \Omega(\bx_{t+1}).
\end{align}
\Cref{eq:optimal_policy} is now modified as follows:
\begin{align}
    &\mathbb{E}_{\pi} \left[ V^{t+1}(\bx_{t+1})+ \tau \log \frac{\pi_{\phi}(\bx_{t+1}|\bx_{t})}{\tilde{q}(\bx_{t}|\bx_{t+1})} \bigg| \bx_{t}\right] =
    \mathbb{E}_{\pi}[V^{t+1}(\bx_{t+1})] + 
    \frac{\tau ||\mu_t + \mathbf{u}_t||^2 }{2s_t^2 \alpha_{t}^{2}} + \frac{\tau D\sigma_t^2}{2s_t^2 \alpha_{t}^{2}} + const\\
    &\approx V^{t+1}(\bx_t) + \mu_t\nabla_{\alpha_t\bx_t}V^{t+1}(\alpha_t\bx_t) + 
    \frac{\tau ||\mu_t + \mathbf{u}_t||^2 }{2s_t^2 \alpha_{t}^{2}} + \frac{\tau D\sigma_t^2}{2s_t^2 \alpha_{t}^{2}} + const.
\end{align}
The constant term is independent of $\bx_t$.
Taking the derivative with respect to $\mu_t$ and setting it to be zero, we obtain the expression for the optimal drift.
\begin{align}
    \mu_t^* = - \frac{s_t^2\alpha_t^2}{\tau}\nabla_{\alpha_t\bx_t}V_{\psi}^{t+1}(\alpha_t \bx_t) - \mathbf{u}_t.
\end{align}

\section{Other Ideas}

Additional discussions on techniques can be informative and interesting. Such as,
\begin{itemize}
    \item Additional use of training data, as expert demonstrations or offline data
    \item Temperature annealing: start learning from higher temperatures and then gradually cool down to a lower temperature
    \item Formulation when there is a reverse drift term.
    \item How can we optimize the objective function other than the reverse KL divergence?
    \item Adaptive method for tuning $s_t$?
    \item Incorporating symmetry in the value function.
    \item Incorporating the energy gradient into the value function. \\
    PIS and DDS use the following parameterization of the control $\mu_{\phi}$:
    \begin{align*}
        \mu_{\phi}(\bx, t) = \text{NN}_1(t, \bx) + \text{NN}_2(t)\nabla\log{q(\bx)}
    \end{align*}
    DIS use
    \begin{align*}
        \mu_{\phi}(\bx, t) = \text{NN}_1(t, \bx) + \text{NN}_2(t)s_t(\bx)
    \end{align*}
    where $s_t(\bx)$ is the linear interpolation of initial and terminal scores $\nabla\log{q(\bx)}$ and $\nabla\log{\pi(\bx_0)}$\\
    This corresponds to the following parametrization of our value function. 
    \begin{align*}
        V^{t}_{\psi}(\bx) = \text{NN}_1(t, \bx) + \text{NN}_2(t)E(\bx)
    \end{align*}
    or 
    \begin{align*}
        V^{t}_{\psi}(\bx) = \text{NN}_1(t, \bx) + \text{NN}_2(t)(
        \frac{t}{T}E(\bx) + (1-\frac{t}{T})\frac{1}{2}\bx^2)
    \end{align*}
    
    
\end{itemize}

\section{Value Gradient Sampling for control with multiplicative weight}
\label{appendix:vp}
\subsection{Hyeokju's}
We assume $\pi_{\phi}(\bx_{t+1}|\bx_t)=\mathcal{N}(\alpha_{t}\bx_t + \mu_t, \sigma_t^2 I)$. Then, finding sampler parameters becomes finding $(\mu_t, \sigma_t)$.
\begin{align}
    \bx_{t+1} = \alpha_{t} \bx_t + \mu_t + \sigma_t \epsilon, \quad \epsilon\sim\mathcal{N}(0,I).
\end{align}
Then the update equation becomes,
\begin{align}
    \min_{\mu_t, \sigma_t} \mathbb{E}_{\epsilon} \left[
         [V_{\psi}^{t+1}(\alpha_{t} \bx_{t}+\mu_t+\sigma_t\epsilon)]          - \tau D \log \sigma_t + \frac{\tau}{2s_t^2}||(\alpha_{t} -1)\bx_{t}+\mu_t  +\sigma_t\epsilon||^2
     \bigg| \bx_t     \right].
\end{align}
The expression can be further simplified into the following:
\begin{align}
    \min_{\mu_t, \sigma_t} 
    \mathbb{E}_{\epsilon}[V_{\psi}^{t+1}(\alpha_{t} \bx_{t}+\mu_t+\sigma_t\epsilon)] - \tau D \log \sigma_t +
    \frac{\tau ||\mu_t||^2 }{2s_t^2} + \frac{\tau D\sigma_t^2}{2s_t^2} + \frac{\tau (\alpha_{t}-1)^{2} || \bx_{t}||^{2}}{2s_{t}^{2}} + \frac{\tau (\alpha_{t}-1) \mu_{t}^{\top} \bx_{t}}{s_{t}^{2}}.
\end{align}
\paragraph{Second-Order Approximation.} The approximation of the value function using second-order information is as followed.

\begin{align}
    &\mathbb{E}_{\epsilon}[V_{\psi}^{t+1} (\alpha_{t}\bx_{t}+\mu_t+\sigma_t\epsilon)] \nonumber\\
    &\approx \mathbb{E}_{\epsilon}\left[
    \begin{array}{l}
    V_{\psi}^{t+1}(\bx_{t}) + \{(\alpha_{t}-1)\bx_{t}+\mu_t+\sigma_t\epsilon\}^\top \nabla_{\bx} V_\psi^{t+1}(\bx_t) \nonumber \\
     \quad +0.5 \{(\alpha_{t}-1)\bx_{t}+\mu_t+\sigma_t\epsilon\}^\top \nabla\nabla_{\bx} V_\psi^{t+1}(\bx_t)  \{(\alpha_{t}-1)\bx_{t}+\mu_t+\sigma_t\epsilon\}
     \end{array}
    \right]\\
    &= V_{\psi}^{t+1}(\bx_{t}) + \{(\alpha_{t}-1)\bx_{t}+\mu_t\}^\top \nabla_{\bx} V_\psi^{t+1}(\bx_t) \\
    &+ 0.5\{(\alpha_{t}-1)\bx_{t}+ \mu_t\}^\top \nabla\nabla_{\bx} V_\psi^{t+1}(\bx_t) \{(\alpha_{t}-1)\bx_{t}+ \mu_t \} + 0.5\sigma_t^2 \nabla^2_{\bx}V_\psi^{t+1}(\bx_{t}).
\end{align}

Plugging this back into the above equation and setting the derivatives with respect to $\mu_t$ and $\sigma_t$ to zero, we obtain the following.
\begin{align}
    &\alpha_{t}^{*} = 1- \left( \nabla\nabla_\bx V^{t+1}(\bx_t) + \frac{\tau}{s_t^2}I\right)^{-1}\left(\bx_{t}^{\top}\left(\nabla \nabla_{\bx} V_{\psi}^{t+1}(\bx_{t}) + \frac{\tau}{s_t^2}I\right)\mu_{t} + \bx_{t}^{\top} \nabla_{\bx} V_{\psi}^{t+1}(\bx_{t}) \right) \\
    &\mu_t^* = - \left( \nabla\nabla_\bx V^{t+1}(\bx_t) + \frac{\tau}{s_t^2}I\right)^{-1} \nabla_\bx V^{t+1}(\bx_t) - \left(\alpha_{t}-1\right)\bx_{t}\\
    &(\sigma_t^2)^*= \frac{s_t^2}{1 + s_t^2 \nabla_{\bx}^2V_{\psi}^{t+1}(\bx_{t})/(\tau D) }
\end{align}

\subsection{Sangwoong's}
Our sampler is $\pi_{\phi}(\bx_{t+1}|\bx_t)=\mathcal{N}(\alpha_{t}\bx_t + \mu_t, \sigma_t^2 I)$, where $\alpha_t>0$ is constant.
We set the auxiliary distribution $\tilde{q}(\bx_t|\bx_{t+1})=\mathcal{N}(\frac{1}{\alpha_t} \bx_{t+1}, s_t^2I)$. This choice of auxiliary distribution will give us a particularly simple solution.

Then, finding sampler parameters becomes finding $(\mu_t, \sigma_t)$.
\begin{align}
    \bx_{t+1} = \alpha_{t} \bx_t + \mu_t + \sigma_t \epsilon, \quad \epsilon\sim\mathcal{N}(0,I).
\end{align}
The density of the auxiliary distribution is evaluated as follows:
\begin{align}
    \log\tilde{q}(\bx_t|\bx_{t+1}) = -\frac{||\frac{1}{\alpha_t}(\alpha_t \bx_t +\mu_t + \sigma_t \epsilon) - \bx ||^2}{2s_t^2} - \frac{D}{2}\log (2\pi s_t^2)
\end{align}
Then the update equation becomes,
\begin{align}
    \min_{\mu_t, \sigma_t} \mathbb{E}_{\epsilon} \left[
         V_{\psi}^{t+1}(\alpha_{t} \bx_{t}+\mu_t+\sigma_t\epsilon)          - \tau D \log \sigma_t + \frac{\tau}{2s_t^2}||\frac{\mu_t}{\alpha_t}  +\frac{\sigma_t}{\alpha_t}\epsilon||^2
     \bigg| \bx_t     \right].
\end{align}
The expression can be further simplified into the following:
\begin{align}
    \min_{\mu_t, \sigma_t} 
    \mathbb{E}_{\epsilon}[V_{\psi}^{t+1}(\alpha_{t} \bx_{t}+\mu_t+\sigma_t\epsilon)] - \tau D \log \sigma_t +
    \frac{\tau ||\mu_t||^2 }{2s_t^2\alpha_t^2} + \frac{\tau D\sigma_t^2}{2s_t^2\alpha_t^2}.
\end{align}

\paragraph{Second-Order Approximation.} 

The approximation of the value function using second-order information is as follows. Here, we apply Taylor expansion around $\alpha_t\bx_t$ instead of $\bx_t$.

\begin{align}
    &\mathbb{E}_{\epsilon}[V_{\psi}^{t+1} (\alpha_{t}\bx_{t}+\mu_t+\sigma_t\epsilon)] \nonumber\\
    &\approx \mathbb{E}_{\epsilon}\left[
    % \begin{aligned}
    V_{\psi}^{t+1}(\alpha_t\bx_{t}) + (\mu_t+\sigma_t\epsilon)^\top \nabla_{\alpha_t\bx_t} V_\psi^{t+1}(\alpha_t\bx_t)
     +\frac{1}{2} (\mu_t+\sigma_t\epsilon)^\top \nabla\nabla_{\alpha_t\bx_t} V_\psi^{t+1}(\alpha_t\bx_t)  (\mu_t+\sigma_t\epsilon)
     % \end{aligned}
    \right]\\
    &= V_{\psi}^{t+1}(\alpha_t\bx_{t}) + \mu_t^\top \nabla_{\alpha_t\bx_t} V_\psi^{t+1}(\alpha_t\bx_t) + \frac{1}{2}\mu_t^\top \nabla\nabla_{\alpha_t\bx_t} V_\psi^{t+1}(\alpha_t\bx_t) \mu_t  + \frac{1}{2}\sigma_t^2 \nabla^2_{\bx}V_\psi^{t+1}(\bx_{t}).
\end{align}

Plugging this back into the above equation and setting the derivatives with respect to $\mu_t$ and $\sigma_t$ to zero, we obtain the following.
\begin{align}
    &\mu_t^* = - \left( \nabla\nabla_{\alpha_t\bx_t} V^{t+1}(\alpha_t\bx_t) + \frac{\tau}{s_t^2\alpha_t^2}I\right)^{-1} \nabla_{\alpha_t\bx_t} V^{t+1}(\alpha_t\bx_t) \\
    &(\sigma_t^2)^*= \frac{s_t^2\alpha_t^2}{1 + s_t^2 \alpha_t^2 \nabla_{\alpha_t\bx_t}^2V_{\psi}^{t+1}(\alpha_t\bx_{t})/(\tau D) }
\end{align}

\bibliography{ref}
\bibliographystyle{icml2025}



\end{document}



