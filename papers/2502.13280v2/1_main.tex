%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[table,dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{comment}
\usepackage{enumitem}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
% \usepackage{kotex}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025_preprint}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\RE}{\ensuremath{\mathbb{R}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Value Gradient Sampler}

\begin{document}

\twocolumn[
\icmltitle{Value Gradient Sampler: Sampling as Sequential Decision Making}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sangwoong Yoon}{equal,ucl}
\icmlauthor{Himchan Hwang}{equal,snu}
\icmlauthor{Hyeokju Jeong}{equal,snu}
\icmlauthor{Dong Kyu Shin}{equal,snu}
\icmlauthor{Che-Sang Park}{snu}
\icmlauthor{Sehee Kweon}{saige}
\icmlauthor{Frank Chongwoo Park}{snu,saige}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{ucl}{University College London, London, UK}
\icmlaffiliation{snu}{Seoul National University, Seoul, Republic of Korea}
\icmlaffiliation{saige}{Saige Co., Ltd, Seoul, Republic of Korea}

\icmlcorrespondingauthor{Sangwoong Yoon}{sangwoong.yoon@ucl.ac.uk}
\icmlcorrespondingauthor{Frank C. Park}{fcp@snu.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We propose the Value Gradient Sampler (VGS), a trainable sampler based on the interpretation of sampling as discrete-time sequential decision-making.
VGS generates samples from a given unnormalized density (i.e., energy) by drifting and diffusing randomly initialized particles.
In VGS, finding the optimal drift is equivalent to solving an optimal control problem where the cost is the upper bound of the KL divergence between the target density and the samples.
We employ value-based dynamic programming to solve this optimal control problem, which gives the gradient of the value function as the optimal drift vector.
The connection to sequential decision making allows VGS to leverage extensively studied techniques in reinforcement learning, making VGS a fast, adaptive, and accurate sampler that achieves competitive results in various sampling benchmarks.
Furthermore, VGS can replace MCMC in contrastive divergence training of energy-based models.
We demonstrate the effectiveness of VGS in training accurate energy-based models in industrial anomaly detection applications.
\end{abstract}


\section{Introduction}

Generating samples from an unnormalized target density function is a fundamental task in probabilistic inference, and its applications are encountered across diverse fields, including machine learning \cite{hinton2002training}, statistical mechanics \cite{noe2019boltzmann}, particle physics \cite{heimel2023madnis} and computational chemistry \cite{leimkuhler2015molecular}.
While Markov Chain Monte Carlo (MCMC) has been the standard approach for sampling, it often requires a very long chain to generate independent samples for high-dimensional multi-model densities. 
A neural network trained as a sampler serves as an efficient alternative to MCMC, enabling the fixed-time generation of new samples.

Viewing sampling as an optimal control problem is an emerging approach for training neural samplers \cite{zhang2022path,berner2022optimal,vargas2023denoising}.
In many conventional sampling methods, such as MCMC, a sample is generated through multiple displacement steps from a point drawn from a prior distribution. Therefore, it is natural to ask whether we can find a policy that optimally moves the particle.
Formulating a sampling problem as optimal control provides a systematic framework to build a novel class of samplers and opens up interesting connections to other important ideas in machine learning, such as diffusion modeling \cite{berner2022optimal,phillips2024particle,huang2024reverse,mcdonald2022proposal} and Schr\"odinger bridge \cite{vargas2023denoising}.

The existing optimal control samplers are typically formulated using stochastic differential equations (SDEs) and, therefore, defined in \emph{continuous-time}. While SDE-based formulations are elegant and provide strong mathematical claims, they face several challenges when deployed in practice.
First, generating a new sample requires many simulation steps and is thus slow. An accurate simulation of continuous-time dynamics requires fine-grained discretization with more than a hundred steps. 
Second, optimal control samplers rely on Neural SDEs \cite{meila2021neural,tzen2019neural}, whose stable and efficient optimization is non-trivial \cite{kidger2021neuralsde}.



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.90\linewidth]{fig/figure_2d_value.png}
    \vskip -0.5cm
    \caption{VGS trained on a mixture of 9 Gaussians with $T=10$. The value functions and corresponding samples at each time step are shown. Samples drift along the gradient of the next-step value function. The experimental setup and results are explained in \cref{sec:synthetic_exp}.}
    \label{fig:2d-vgs}
\end{figure*}

% value function. / contribution
In this paper, we demonstrate that directly solving optimal control in a \emph{discrete-time} setting can be more effective and practical. A discrete-time policy can be directly optimized for fewer time steps, minimizing the performance degradation from discretization. Furthermore, discrete-time optimal control can be written as a Markov Decision Process (MDP). This allows us to use well-established techniques in sequential decision making and reinforcement learning (RL) for effective and efficient optimization.


We propose the \textbf{Value Gradient Sampler} (VGS) as an instantiation of a sequential decision-making approach for sampling. Like Langevin Monte Carlo and diffusion models, VGS applies drift and diffusion processes $T$ times to a randomly initialized particle to generate a sample. The drift vector is determined by minimizing the upper bound of the KL divergence between the samples and the target density, which takes the form of a classical optimal control objective. 

We employ value-based dynamic programming to solve this optimal control problem, yielding the gradient of the value function as the optimal drift vector. The value function is represented by a neural network and trained using standard RL techniques, such as temporal difference learning.
We evaluate VGS on multiple sampling benchmarks, including sampling equilibrium states of a $n$-body system.
VGS demonstrates competitive performance, often outperforming SDE-based samplers which use significantly larger $T$. \looseness=-1

% energy-based model learning.
In addition to sampling, VGS can be utilized to train energy-based models (EBMs).
The maximum likelihood training of an EBM is known to be highly unstable and computationally intensive due to the need for MCMC as a subroutine \cite{hinton2002training,du2019,yoon2021autoencoding}.
We demonstrate that VGS can replace MCMC in EBM training, making the process more efficient and resulting energy more accurate energy estimates.  \looseness=-1
% When applied to an industrial anomaly detection task, an EBM powered by VGS achieves strong performance.

% contribution and paper structure
The contribution of the paper can be summarized as follows:
\begin{itemize}[leftmargin=1em,topsep=0pt,noitemsep]
    \item We propose the Value Gradient Sampler (VGS), a sampler trained via RL.
    \item We demonstrate that VGS can generate new samples faster than SDE-based samplers while maintaining the quality of samples.
    \item We show that replacing MCMC with VGS produces a more accurate energy function, improving the performance of energy-based anomaly detection.
\end{itemize}
Code is available at \href{https://github.com/swyoon/value-gradient-sampler/}{https://github.com/swyoon/value-gradient-sampler/}.

%%%%% 공간 부족하면 이 문단 삭제 예정
% The rest of the paper is organized as follows. 
% Section 2 provides the necessary preliminaries. VGS is introduced in Section 3, and advanced techniques for improving VGS are discussed in Section 4. Section 5 describes the application of VGS to EBM learning. Related works are reviewed in Section 5. Section 6 presents the experimental results, and Section 7 concludes the paper.


\section{Preliminaries}

\paragraph{Sampling Problems.}
We consider the problem of drawing independent samples from a target density function $q(\bx)$ defined in $\RE^D$. 
Assuming $q(\bx)>0$ for all $\bx$, the target density can be expressed as:
\begin{align}
    q(\bx) = \frac{1}{Z}\exp(-E(\bx)/\tau), \label{eq:ebm}
\end{align}
where $E:\RE^D \to \RE$ is called an energy function, $\tau>0$ is a temperature parameter, and $Z$ is the normalization constant. The function $E(\bx)$ is assumed to be differentiable.
The focus is on settings where the normalization constant $Z$ is unknown or difficult to compute, with access limited to $E(\bx)$ and $\tau$. 


One of the most widely adopted sampling methods is MCMC, which utilizes a Markov chain having $q(\bx)$ as the stationary distribution. However, obtaining a sample in MCMC is slow because a large number of iterations have to be conducted for every independent sample generation. 
% are required to often reach the stationary distribution.

% MCMC is well-known approach for sampling, but requires a large number of iterations to conver

% Review of SDE-based samplers, including PIS, DIS.


\paragraph{Training Parametric Samplers.}
We aim to train a parametric sampler $\pi_\phi(\bx)$ with parameter $\phi$ to generate approximate samples from the target density $q(\bx)$. 
A parametric sampler can be faster at generating new samples as computation can be amortized in the training phase.

A natural choice of the objective function is the KL divergence between $\pi_\phi(\bx)$ and $q(\bx)$:
\begin{align}
    \min_\phi KL(\pi_\phi(\bx)||q(\bx))=\min_\phi \mathbb{E}_{\pi_\phi}\left[\log \frac{\pi_\phi(\bx)}{q(\bx)}\right], \label{eq:kl-objective}
\end{align}
which does not require samples from $q(\bx)$ for training.


\paragraph{Sampler Model.}
In this work, the sampler distribution $\pi_\phi(\bx)$ is implicitly obtained by the result of the following iterative drift-diffusion process:
\begin{align}
    \bx_0 \sim \mathcal{N}(0,\sigma_{init}^2 I), \quad
    \bx_{t+1} = \alpha_t\bx_{t} + \mu_\phi^t(\bx_t) +\sigma_t\epsilon_t \label{eq:pls}
\end{align}
for $t=0,\ldots,T-1$. 
% a class of samplers, which we will refer to as \textbf{Parametric Langevin Samplers} (PLS).
% These samplers operate by initially sampling particles from gaussian distribution and then iteratively refining them through a series of drift steps.
The coefficients $\alpha_t$ and $\sigma_t$ are constants and $\epsilon_t \sim\mathcal{N}(0,I)$. The drift $\mu_\phi^t(\bx_t)$ is a function of $\bx_t$ and $t$, parametrized by $\phi$. We set $\pi_\phi(\bx)$ to refer to the distribution of the final particle $\bx_T$. We will often write the final particle $\bx_T$ as $\bx$, dropping the subscript. 

The drift-diffusion process in \Cref{eq:pls} is a Markov process. The transition dynamics becomes that of Langevin Monte Carlo \cite{roberts1996exponential} when $\alpha_t=1$ and $\mu_\phi^t(\bx_t)=(\sigma^2_t/2)\nabla_{\bx_t}\log q(\bx_t)$.
A diffusion model \cite{ho2020ddpm,song2021scorebased} can also be expressed in the same form, where the drift is the score of the diffused version of $q(\bx)$.
Finally, \Cref{eq:pls} can be viewed as the discretization of SDE via the Euler-Maruyama method.

However, since $\pi_\phi(\bx)$ is defined implicitly, training the distribution by $\min_\phi KL(\pi_\phi(\bx_T)||q(\bx_T))$ is not trivial due to two challenges. First, the entropy of the sample distribution $\pi_\phi(\bx)$ is not analytically computable. Second, the gradient must propagate through time, requiring significant memory and making it vulnerable to gradient explosion or vanishing.


\begin{algorithm}[tb]
   \caption{Value Gradient Sampling}
   \label{alg:vgs_sampling}
\begin{algorithmic}
   \STATE {\bf Input:} Value $V_{\phi}^t(\bx_t)$, $\{\alpha_t\}_{t=0}^{T-1}$, $\{s_t\}_{t=0}^{T-1}$, $\sigma_{init}$
   \STATE $x_0 \sim \mathcal{N}(0, \sigma_{init}^2I)$ \hfill // Initialize samples
   \FOR{$t=0$ {\bf to} $T-1$}
   \STATE $\mu_{\phi}^t(\bx_t)=- \frac{s_t^2 \alpha_{t}^{2}}{\tau} \nabla V^{t+1}_{\phi}(\alpha_{t}\bx_t),     \sigma_t=\alpha_{t}s_t$ \hfill// Eq. (\ref{eq:vgs_sampling})
   \STATE $\bx_{t+1} = \alpha_t\bx_{t} + \mu_\phi^t(\bx_t) +\sigma_t\epsilon_t$ \hfill //  Eq. (\ref{eq:pls})
   \ENDFOR
   \STATE {\bf Output:} $\{\bx_t\}_{t=0}^T$
\end{algorithmic}
\end{algorithm}


\section{Value Gradient Sampling}

Here, we propose the \textbf{Value Gradient Sampler} (VGS), a novel sampler that interprets the drift-diffusion process (\Cref{eq:pls}) as a sequential decision-making problem. \looseness=-1

\subsection{Sampling as Optimal Control}
Instead of solving $\min_\phi KL(\pi_\phi(\bx_T)||q(\bx_T))$ directly, we minimize its upper bound. The upper bound is obtained by the data processing inequality, allowing the incorporation of intermediate diffusion steps $\bx_{0:T-1}$ into the KL divergence: 
\begin{align}
    KL(\pi_\phi(\bx_T)||q(\bx_T)) 
    \leq KL(\pi_\phi(\bx_{0:T})||\tilde{q}(\bx_{0:T}))\label{eq:data-processing},
\end{align}
where we also introduce an auxiliary distribution $\tilde{q}(\bx_{0:T}) = q(\bx_T)\prod_{t=0}^{T-1}\tilde{q}(\bx_t|\bx_{t+1})$. Note that the data processing inequality holds for an arbitrary choice of $\tilde{q}(\bx_{t}|\bx_{t+1})$.


Now the right-hand side of \cref{eq:data-processing} can be minimized: $\min_\phi KL(\pi_\phi(\bx_{0:T})||q(\bx_{T}) \tilde{q}(\bx_{0:T-1}|\bx_T)).$
By plugging the definitions of each distribution, multiplying by $\tau$, and discarding all constants, following problem is obtained:
\begin{align}
\min_{\phi} \mathbb{E}_{\pi_\phi(\bx_{0:T})} \Bigg[
E(\bx_T) + \tau\sum_{t=0}^{T-1} \log \frac{\pi_\phi(\bx_{t+1}|\bx_t)}{\tilde{q}(\bx_t|\bx_{t+1})}
\Bigg], \label{eq:obj-opt-control}
\end{align}
which can be viewed as an optimal control problem. The controller $\pi_\phi(\cdot)$ is optimized to minimize the terminal cost $E(\bx_T)$ plus the running costs $ \log \pi_\phi(\bx_{t+1}|\bx_t)/\tilde{q}(\bx_t|\bx_{t+1})$ for each transition $(\bx_t, \bx_{t+1})$.
The temperature $\tau$ balances between the terminal and the running costs.
To solve this standard optimal control problem, we apply the classical value-based dynamic programming approach.


\paragraph{Value Function.}
A value function, or cost-to-go function, $V^{t}_{\pi}(\bx_t)$ is defined as the expected sum of the future costs starting from $\bx_t$, following $\pi$.
\begin{align}
    V^{t}_{\pi}(\bx_t)=\mathbb{E}_{\pi}\left[
        E(\bx_T) + \tau  \sum_{t'=t}^{T-1} \log \frac{\pi_\phi(\bx_{t'+1}|\bx_{t'})}{\tilde{q}(\bx_{t'} | \bx_{t'+1})} \bigg| \bx_t \right] \label{eq:value_function}
\end{align}
for $t=0,\ldots,T-1$. We set $V^{T}_{\pi}(\bx_T)=E(\bx_T)$.
The optimal value function $V_*^{t}(\bx_t)$ is a value function that satisfies the Bellman optimality equation (\Cref{appendix:bellman}).
In practice, a value function is approximated with a neural network that takes $(\bx_t, t)$ as input and returns a scalar.  


\begin{algorithm}[tb]
   \caption{Learning Value Functions in VGS}
   \label{alg:vgs_training}
\begin{algorithmic}
   \STATE {\bf Input:} Energy $E(\bx)$, value  $V_{\phi}^t(\bx_t)$, target value $V_{\phi^-}^t(\bx_t)$, hyperparameters $\alpha_t$, $s_t$, $\sigma_{init}$, $n_{update}$
   %\STATE {\bf Initialize:} Target Value network $V_{\phi^-}^t(\bx_t)$
   \REPEAT
   \STATE Sample $\{(\bx_{t}, \mu_{\phi^{-}}^{t}(\bx_t)) \}_{t=0}^{T}$ using $V_{\phi^-}$ \hfill // Alg. \ref{alg:vgs_sampling}
   \STATE Initialize replay buffer $\mathcal{D} = \{(\bx_{t}, \mu_{\phi^{-}}^{t}(\bx_t)) \}_{t=0}^{T}$
   \FOR{$i=1$ {\bfseries to} $n_{update}$}
   \FOR{$(\bx_t, \mu_{\phi^-}^t(\bx_t))$ {\bfseries in} $\mathcal{D}$}
   %\hfill // \cref{eq:td_error_minimization}
   \STATE $\bx_{t+1} = \alpha_t\bx_{t} + \mu_{\phi^-}^t(\bx_t) +\sigma_t\epsilon_t$ \hfill //  Eq. (\ref{eq:pls})
   \STATE Compute $TD(\bx_t, \bx_{t+1})$ \hfill // Eq. (\ref{eq:td_target_net})
   \STATE Optimize $\min_{\phi} ((V_{\phi}^{t}(\bx_{t}) - TD(\bx_{t}, \bx_{t+1}))^2$
   %\STATE $\phi \leftarrow$ Update($\phi$, $\nabla_{\phi}((V_{\phi}^{t}(\bx_{t}) - TD(\bx_{t}, \bx_{t+1}))^2$)
   %\STATE $\min_{\phi} (V_{\phi}^{t}(\bx_{t}) - TD(\bx_{t}, \bx_{t+1}))^2$
   \ENDFOR
   \ENDFOR
   \STATE $\phi^{-} \leftarrow \lambda \phi^- + (1-\lambda)\phi$
   \STATE update $\sigma_{init}$ \hfill // Eq. (\ref{eq:init_sigma_update})
   \UNTIL{Convergence}
   %\UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\subsection{Value Gradient Sampler}\label{sec:value_grad}
We present VGS, a sampler that (approximately) solves the optimal control objective (\cref{eq:obj-opt-control}) by drifting a particle along the gradient of the next-step value function.
First, it is assumed that our policy $\pi(\bx_{t+1}|\bx_{t})$ follows the drift-diffusion process in \Cref{eq:pls} such that
\begin{align}
    \pi(\bx_{t+1}|\bx_t) = \mathcal{N}(\bx_{t+1}; \alpha_t\bx_t+ \mu_t, \sigma_t^2 I).
\end{align}
We will find the drift vector $\mu_t$ and the noise magnitude $\sigma_t$ that solves \cref{eq:obj-opt-control} as optimally as possible.

For now, we assume that the value function at time $t+1$, $V_{\pi}^{t+1}(\bx_{t+1})$, is given, and the policy $\pi$ after $t+1$ is fixed. 
Note that the value function becomes the optimal value function when $\pi$ is the optimal policy.
Then, employing the dynamic programming approach, we focus on the subproblem of finding the optimal policy at time $t$.
% For now, we assume that the value function $V_{\pi}^{t}(\bx_t)$ is given. Then, employing the dynamic programming approach, we focus on the subproblem of finding the optimal policy at time $t$.
\begin{align}
    &\min_{\pi(\bx_{t+1}|\bx)}\mathbb{E}_{\pi}\left[
        E(\bx_T) + \tau  \sum_{t'=t}^{T-1} \log \frac{\pi(\bx_{t'+1}|\bx_{t'})}{\tilde{q}(\bx_{t'} | \bx_{t'+1})} \bigg| \bx_t \right]\\
        &=  \min_{\pi(\bx_{t+1}|\bx)} \mathbb{E}_{\pi} \left[ V^{t+1}_{\pi}(\bx_{t+1})+ \tau \log \frac{\pi(\bx_{t+1}|\bx_{t})}{\tilde{q}(\bx_{t}|\bx_{t+1})} \bigg| \bx_{t}\right] \nonumber
\end{align}
To fully specify the problem, the auxiliary distribution $\tilde{q}(\bx_t|\bx_{t+1})$ has to be given. We choose $\tilde{q}(\bx_t|\bx_{t+1})$ as the following distribution:
\begin{align}
    \tilde{q}(\bx_{t}|\bx_{t+1}) = \mathcal{N}\left(\frac{1}{\alpha_{t}}\bx_{t+1}, s_t^2 I\right), \quad s_t > 0, \label{eq:q_tilde_gaussian}
\end{align}
which later gives us a particularly simple solution and also has an interesting connection to both variance-exploding ($\alpha_t = 1$) and variance-preserving ($\alpha_t = 1/\sqrt{1-s_t^2}$) diffusion processes. Now we can write the optimization for $\mu_t$ and $\sigma_t$ as follows: \looseness=-1
\begin{align}
    & \min_{\mu_t, \sigma_t} 
    \mathbb{E}_{\epsilon}
         \left[V^{t+1}_{\pi}(\alpha_{t}\bx_{t}+\mu_t+\sigma_t\epsilon)\right] \nonumber \\
     &\hspace{4em} - \tau D \log \frac{\sigma_t}{s_{t}}
    +
    \frac{\tau ||\mu_t||^2 }{2s_t^2 \alpha_{t}^{2}} + \frac{\tau D\sigma_t^2}{2s_t^2 \alpha_{t}^{2}}-\tau\frac{D}{2},  \label{eq:optimal_policy}
\end{align}
% Next, we apply a dynamic programming approach to address our optimal control problem in \cref{eq:obj-opt-control}, focusing on the subproblem of determining the optimal policy at time $t$, $\pi_{\phi}^{*}(\bx_{t+1}|\bx_t)$. Since we use a sampler model following \cref{eq:pls} in this work, the subproblem reduces to finding the optimal ($\mu^*_{t}$, $\sigma^*_{t}$) that minimizes the expected sum of costs given the initial state $\bx_t$. Using the definition of the value function (\cref{eq:value_function}) and incorporating our choice of $\tilde{q}$ (\cref{eq:q_tilde_gaussian}), we derive the optimization problem for ($\mu_{t}$, $\sigma_{t}$) as:
% \begin{align}
%     & \min_{\pi_{\phi}(\bx_{t+1}|\bx_{t})} \mathbb{E}_{\pi}\left[
%         V^{t}_{\pi}(\bx_t) | \bx_t \right]  \nonumber \\ 
%         &=  \min_{\pi_{\phi}(\bx_{t+1}|\bx)} \mathbb{E}_{\pi} \left[ V^{t+1}_{\pi}(\bx_{t+1})+ \tau \log \frac{\pi_{\phi}(\bx_{t+1}|\bx_{t})}{\tilde{q}(\bx_{t}|\bx_{t+1})} \bigg| \bx_{t}\right] \nonumber \\
%     %&=
%     %\min_{\mu_t, \sigma_t} \mathbb{E}_{\epsilon} \left[
%     %\begin{array}{l}
%     %     [V^{t+1}_{\pi}(\alpha_{t}\bx_{t}+\mu_t+\sigma_t\epsilon)] \\
%     %     \quad -\tau D \log \sigma_{t} - \tau \frac{||\epsilon||^{2}}{2}\\
%     %     \quad + \tau D \log {s_{t}} + \frac{\tau}{2s_t^2}||\frac{\mu_t}{\alpha_{t}}  +\frac{\sigma_t}{\alpha_{t}}\epsilon||^2
%     %\end{array}
%     %\bigg| \bx_t     \right]  \nonumber \\
%     &= \min_{\mu_t, \sigma_t} 
%     \mathbb{E}_{\epsilon}
%          \left[V^{t+1}_{\pi}(\alpha_{t}\bx_{t}+\mu_t+\sigma_t\epsilon)\right] \nonumber \\
%      &\hspace{4em} - \tau D \log \frac{\sigma_t}{s_{t}}
%     +
%     \frac{\tau ||\mu_t||^2 }{2s_t^2 \alpha_{t}^{2}} + \frac{\tau D\sigma_t^2}{2s_t^2 \alpha_{t}^{2}}-\tau\frac{D}{2}.  \label{eq:optimal_policy}
%     \end{align}
where $\epsilon\sim\mathcal{N}(0,I)$. To handle the first term, we evaluate a first-order Taylor approximation of $V^{t+1}_{\pi}$ around $\alpha_{t}\bx_{t}$.
% Unfortunately, an analytical solution to this problem is not available. In search of an approximation for optimal $\mu_t$ and $\sigma_t$, we use a first-order Taylor expansion of $V^{t+1}_{\pi}$ with respect to $\alpha_{t}\bx_{t}$. 
\begin{align}
    &\mathbb{E}_{\epsilon}[V^{t+1}_{\pi}(\alpha_{t}\bx_{t}+\mu_t+\sigma_t\epsilon)] \nonumber \\
    %&\approx \mathbb{E}_{\epsilon}[V^{t+1}_{\pi}(\alpha_{t}\bx_{t}) + (\mu_t+\sigma_t\epsilon)^\top \nabla V^{t+1}_{\pi}(\alpha_{t}\bx_t) ] \nonumber\\
    &\approx V_{\pi}^{t+1}(\alpha_{t}\bx_{t}) + \mu_t^\top \nabla_{\alpha_{t}\bx_{t}} V^{t+1}_{\pi}(\alpha_{t}\bx_t). \label{eq:first_order}
\end{align}
Substituting the approximation into \cref{eq:optimal_policy} and setting the derivatives with respect to $\mu_t$ and $\sigma_t$ to zero, we obtain the analytic expressions for the optimal drift $\mu_t$ and the optimal noise magnitude $\sigma_t$ as follows:
% . By applying parametrization for the sampler to the derived optimal drift and diffusion, we introduce \textbf{VGS}:
\begin{align}
    \mu_t = - \frac{s_t^2 \alpha_{t}^{2}}{\tau} \nabla_{\alpha_{t}\bx_{t}} V^{t+1}_{\pi}(\alpha_{t}\bx_t), \quad    \sigma_t = \alpha_{t}s_t. \label{eq:vgs_sampling}
\end{align}
The detailed derivation, including a second-order Taylor expansion, is presented in \cref{appendix:second_order}.
Note that $\mu_t$ is now a function of $\bx_t$ and is fully determined by the value function.
In practice, we will use the parametrized value function $V_\phi^t(\bx_t)$ with parameter $\phi$ to approximate the value function and \Cref{eq:vgs_sampling} can also be used to sample using $V_\phi^t(\bx_t)$  as stated in \Cref{alg:vgs_sampling}. \looseness=-1
% Since the value function fully determines the sampler, the value function's parameter $\phi$ is also the sampler's parameter.


% We clarify that \cref{eq:vgs_sampling} provides only a near-optimal solution to the original objective, as it is derived using a first-order approximation. However, we show that \cref{eq:vgs_sampling} is asymptotically optimal for both VE and VP $\tilde{q}$ processes, in the sense that $V_{\pi}^t(\bx_t) \to V^{*}(\bx_t)$ as $T \to \infty$. (For further details, refer to \cref{appendix:ve_anal} and \cref{appendix:vp_anal}.)



\subsection{Training Value Functions Using RL}\label{sec:learn_value}

Since the optimal value function is not known in practice, we learn a value function through multiple evaluations of target energy $E(\bx)$. As described in \Cref{alg:vgs_training}, we follow the framework of generalized policy iteration, which iterates between policy evaluation and policy improvement. Since the sampling step in VGS is equivalent to policy improvement, we discuss methods for policy evaluation.


\textbf{Temporal Difference Learning.}\quad
We implement temporal difference (TD) learning to evaluate the value function given samples from a policy. 
The value function needs to satisfy the following recurrence relation:
\begin{align}
    V^{t}_{\pi}(\bx_t)=\mathop{\mathbb{E}}\limits_{\pi(\bx_{t+1}|\bx_{t})}\left[
        V^{t+1}_{\pi}(\bx_{t+1}) + \tau  \log \frac{\pi(\bx_{t+1}|\bx_{t})}{\tilde{q}(\bx_{t} | \bx_{t+1})} \bigg| \bx_t \right].  \nonumber
\end{align}
% \begin{align}
%     V^{t}_{\pi}(\bx_t) = & \frac{\tau ||\mu^{t}_{\phi}(\bx_t)||^2 }{2s_t^2 \alpha_{t}^{2}} -\tau D \log{\alpha_t}\nonumber\\& +\mathbb{E}_{\pi(\bx_{t+1}|\bx_t)}[V^{t+1}_{\pi}(\bx_{t+1})]. \end{align}
A parametric value function $V^{t}_{\phi}(\bx_t)$ is trained to minimize the discrepancy between the two sides of the recurrence relation. The TD target for $V^{t}_{\phi}(\bx_t)$ is expressed as:
\begin{align}
    TD(\bx_t, \bx_{t+1}) = V^{t+1}_{\phi}(\bx_{t+1})+ \tau  \log \frac{\pi_{\phi}(\bx_{t+1}|\bx_{t})}{\tilde{q}(\bx_{t} | \bx_{t+1})}.
    \label{eq:td_target_net}
\end{align}
% TD(\bx_t, \bx_{t+1}) = \frac{\tau ||\mu^{t}_{\phi^-}(\bx_t)||^2 }{2s_t^2 \alpha_{t}^{2}} -\tau D \log{\alpha_t} + V^{t+1}_{\phi^-}(\bx_{t+1}).
% The notation $V_{\phi^-}$ and $\mu_{\phi^-}$ indicate that the computations are carried out using the target network.
Then, the value function is trained by minimizing the TD error, the mean squared error to the TD target is:
\begin{align}
    \min_{\phi} \mathbb{E}_{\pi(\bx_{t+1}|\bx_t)}[(V_{\phi}^{t}(\bx_t) - TD(\bx_t, \bx_{t+1}))^2],
    \label{eq:td_error_minimization}
\end{align}
% \begin{align}
%     \min_{\phi} \mathbb{E}_{(\bx_t, \mu^t_{\phi^-}(\bx_t))\sim \mathcal{D}}[\mathbb{E}_{\pi(\bx_{t+1}|\bx_t)}(V_{\phi}^{t}(\bx_t) - TD(\bx_t, \bx_{t+1}))^2]].
%     \label{eq:td_error_minimization}
% \end{align}
% To improve the sample efficiency, we introduce replay buffer $\mathcal{D}$.
where $TD(\bx_t, \bx_{t+1})$ is treated to be independent of $\phi$.
Note that other value function learning methods, e.g., Monte Carlo prediction, are also applicable to VGS.

\textbf{Target Network.}\quad 
While \Cref{eq:td_error_minimization} works, often stability and performance can be improved by introducing a target network $V_{\phi^-}^t(\bx_t)$ to evaluate the TD target \cite{mnih2015human}. The target network parameter $\phi^-$ is obtained via exponential moving averaging (EMA) of $\phi$ from past iterations: $\phi^{-} = \lambda \phi^- + (1-\lambda)\phi$. When a target network is employed for VGS, samples are generated using $V_{\phi^-}^t(\bx_t)$. 

% In Equation (\ref{eq:td_target_net}), we use the target value network $V_{\phi^-}$ instead of $V_{\phi}$ to compute the TD target. $V_{\phi^-}$ has the same architecture as the original value network $V_{\phi}$ with frozen weights. The drift in  Equation (\ref{eq:td_target_net}) is also computed using the target network, denoted as $\mu_{\phi^-}$.
% We adopt the common practice in RL of utilizing a target network, as it is known to bring numerous advantages, including stabilized training \cite{mnih2015human}. 


\textbf{Replay Buffer.}\quad
For better sample efficiency, we utilize a replay buffer $\mathcal{D}$ to store $(\bx_t, \mu^t_{\phi^-}(\bx_t))$, which allows us to resample $\bx_{t+1}$ during each update \cite{lin1992}. Each sample is reused $n_{update}$ times during training.
% This approach improves sample efficiency and reduces the correlation between consecutive samples \cite{lin1992}. 
% The complete training process is summarized in \cref{alg:vgs_training}.

\textbf{Off-Policy Learning.}\quad
As in RL, off-policy data can be utilized in the value function learning. 
A particularly efficient and useful off-policy data is a trajectory collected from an amplified noise level $(\sigma_t)_{\text{off}}=\eta\alpha_t s_t$ for a noise scale $\eta >1$. Amplifying the noise roughly corresponds to sampling from a higher temperature and helps exploration.

% Off-policy learning refers to reinforcement algorithms where the learning is driven by data generated from a behavior policy that is different from the target policy being optimized. 
% We have implemented various off-policy learning methods, including the use of replay buffer and trajectory generation via increased noise levels during sampling. These techniques are aimed at increasing exploration and sample efficiency.

% Specifically, a behavior policy with amplified noise level dramatically enhances the exploration over search space. We define noise scale $\eta > 1$ to amplify the noise scale of the target policy. A simple modification of the noise level in Equation (\ref{eq:vgs_sampling}) from $\sigma_t=\alpha_t s_t$
% to $(\sigma_t)_{\text{off}}=\eta\alpha_t s_t$ gives our behavior policy for off policy learning.
 




% \paragraph{Previous content (Outdated)}
% Based on the optimal control formulation Eq. \ref{eq:obj-opt-control} and the auxiliary distribution in Eq. 
% \ref{eq:q_tilde_gaussian}, the running cost of a transition $(\bx_{t}, \bx_{t+1})$, $t \in \{0,...,T-1\}$ is given as the following:
% \begin{align}
%     &C_{t+1}(\bx_{t},\bx_{t+1}) \triangleq
% \tau\log \frac{\pi_\phi(\bx_{t+1}|\bx_t)}{\tilde{q}(\bx_t|\bx_{t+1})}
%     \nonumber\\&=-\frac{\tau}{2\sigma_{t}^2}||\bx_{t+1} - \alpha_t\bx_{t}-\mu_\phi^t(\bx_t)||^2 - \tau D \log(\sigma_t)\nonumber\\&+\frac{\tau}{2s_t^2\alpha_{t}^2}||\bx_{t+1} - \alpha_{t}\bx_{t}||^2 + \tau D\log(s_t) + const.
%     \label{eq:running cost}
% \end{align}
% where $C_{t+1}(\bx_{t},\bx_{t+1})$ indicates the cost of sampling $\mathbf{x}_{t+1}$ starting from $\mathbf{x}_{t}$.

% During policy evaluation, we estimate the value function for the sampler model by the temporal difference update. The TD-target of the update is given by the following:
% \begin{align}
%     TD(\bx_{t},\bx_{t+1}) =\textrm{sg}[V^{t+1}_\psi(\bx_{t+1})] +  C_{t+1}(\bx_{t},\bx_{t+1})
%     \label{eq:td_target}
% \end{align}
% where $\textrm{sg}[\cdot]$ denotes a stop-gradient operator indicating that gradient is not computed for the term. With Eq. \ref{eq:td_target}, a temporal difference update rule is obtained.
% \begin{align}
%     \min_{\psi} \mathbb{E}_{\bx_t, \bx_{t+1}\sim \pi}[&(TD(\bx_{t},\bx_{t+1}) - V^t_\psi(\bx_t))^2]
%     \label{eq:critic-update}
% \end{align}
% To improve the stability an the efficiency of training the value function, we introduce replay buffer for off-policy learning.

%  The replay buffer $\mathcal{D}$
% stores the past experiences. The buffer stores $\bx_{t}$, the state at time step $t$ , $C_{t+1}(\bx_{t},\bx_{t+1})$, the transition cost from time step $t$ to time step $t+1$ and  $\bx_{t+1}$, the next state reached after executing a drift.

%  At each training iteration, a random mini-batch of experiences is sampled from the buffer to update value function. The Eq. 13 is approximated with the sampled mini-batch.


\subsection{Theoretical Analyses}
In this section, we present two theoretical results regarding the optimal value function (\cref{eq:value_function}) and the optimal auxiliary distribution $\tilde{q}(\bx_{0:T})$.


The first theorem demonstrates that the optimal value function can be interpreted as the energy of marginal density $\tilde{q}(\bx_t)$, a diffused version of the target $q(\bx_T)$. 

\begin{theorem}[Optimal Value Function] \label{theorem:value}
    If the admissible set of policies $\pi(\bx_{t+1:T}|\bx_t)$ includes the auxiliary distribution $\tilde{q}(\bx_{t+1:T}|\bx_t)$, the optimal value function $V^t_*(\bx_t)$ (\cref{eq:opt_value_def}) satisfies:
    \begin{align}
        \tilde{q}(\bx_t) = \frac{1}{Z}\exp{(-V^t_*(\bx_t) / \tau)}.
    \end{align}
\end{theorem}
\begin{proof}
The result follows from the definition of the optimal value function and the theorem condition $\min_{\pi} KL(\pi(\bx_{t+1:T}|\bx_t) \|\tilde{q}(\bx_{t+1:T}|\bx_t)) = 0$. See \cref{appendix:opt_val} for details.
\end{proof}
% Under the same assumptions, we can further show that the minimum value of our joint KL divergence objective (\cref{eq:obj-opt-control}) reduces to the KL divergence between the initial distributions.

The second theorem provides a guideline for designing the auxiliary distribution $\tilde{q}(\bx_t)$. The result shows that it is beneficial to choose an auxiliary distribution that transforms the target distribution into a Gaussian prior $\pi(\bx_0)$.  This choice is identical to how the forward process is designed in diffusion models \cite{sohl-dickstein15,song2021scorebased}.

\begin{theorem}[Optimal Auxiliary Distribution]
\label{theorem:auxillary}
    Under the same assumptions as \cref{theorem:value}, the minimum value of our joint KL divergence objective is related to $\tilde{q}(\bx_0)$ as follows:
    \begin{align}
        \min_{\pi} KL(\pi(\bx_{0:T}) || \tilde{q}(\bx_{0:T})) = KL(\pi(\bx_0) || \tilde{q}(\bx_0)).
    \end{align}
    Therefore, to fully minimize the objective to zero, the auxiliary distribution $\tilde{q}(\bx_{0:T-1}|\bx_T)$ must satisfy:
    \begin{align}
        \tilde{q}(\bx_0) = \pi(\bx_0).
    \end{align}
\end{theorem}

\begin{proof}
    Using the definition of the optimal value function and the  result from \cref{theorem:value}, we obtain:
    \begin{align}
        &\min_{\pi} KL(\pi(\bx_{0:T}) || \tilde{q}(\bx_{0:T}))\nonumber \\
        &= \mathbb{E}_{\pi(\bx_0)}[ \log{\pi(\bx_0)} +V^0_*(\bx_0)/\tau + \log{Z}] \nonumber\\
        &=KL(\pi(\bx_0) || \tilde{q}(\bx_0)). \nonumber \hfill \qedhere
    \end{align} 
\end{proof}


% \subsection{Connection to Diffusion Models}


\section{Sampling $n$-Body Systems with VGS} \label{sec:n_body_VGS}
% Sampling n-body systems is a fundamental problem with various real-world applications, including protein structure prediction \cite{jumper2021highly} and molecule generation \cite{hoogeboom2022equivariant}. 
% In this section, we focus on formally state the problem and show how to incorporate the underlying symmetry of the n-body system into VGS.  
Building on the general sampling problem discussed in the previous section, we now focus on the sampling problem under symmetry constraints. We demonstrate that VGS can effectively leverage the symmetry of the $n$-body  system.


\textbf{Sampling the Equilibrium State of $n$-Body Systems} \quad Sampling $n$-body systems is a fundamental problem with various real-world applications \cite{jumper2021highly, hoogeboom2022equivariant}. We represent the configuration of an $n$-body system in $\mathbb{R}^m$ as $\bx \in \mathbb{R}^{n\times m}$. Since the choice of the reference frame is arbitrary, the energy of the system, $E(\bx)$, must remain invariant under translations, rotations, and reflections of the particles. Furthermore, we assume that the particles are indistinguishable, which introduces permutation invariance. Taken together, the energy function $E(\bx)$ must exhibit $\mathrm{E}(m) \times \mathbb{S}_n$-invariance under the trivial group action—that is, transformations by $\mathrm{E}(m)$ and the permutations of the particles. Recent studies in training neural samplers have incorporated the symmetry of $n$-body systems into their algorithms, achieving state-of-the-art performance \cite{akhound-sadegh2024iterated, he2024training}. 


% Related Works와 Intro에 적당히 들어가면 될것 같습니다
% 
% Traditional methods leverage Markov Chain Monte Carlo (MCMC) or molecular dynamics (MD) which is usually computationally expensive, thus is not scalable to high dimensional physical system. (\cite{neal1998annealedimportancesampling,10.1111/j.1467-9868.2006.00553.x,leimkuhler2015molecular}). 
% % Annealed Importance Sampling (AIS; \cite{neal1998annealedimportancesampling})
% % Sequential Monte Carlo(SMC; \cite{10.1111/j.1467-9868.2006.00553.x}) 
% % Simulating the actual (molecular) dynamics (MD;\cite{leimkuhler2015molecular}) 
%  To address this issue, neural samplers, such as Path Integral Sampler (PIS; \cite{zhang2022path}), Time-reversed Diffusion Sampler (DIS; \cite{berner2022optimal}), Denoising Diffusion Sampler (DDS; \cite{vargas2023denoising}), and Iterated De-noising Energy Matching(iDEM; \cite{akhound-sadegh2024iterated}) have been suggested as promising alternatives. These neural samplers are trained to efficiently sample from complex distributions, and various training strategies have been proposed to overcome the challenges associated with multi-modal and high-dimensional target distributions.

\textbf{Incorporating Symmetries in VGS} \quad We can leverage the symmetry of $n$-body systems by reducing the effective dimension from $D = nm$ to $D = (n-1)m$ and utilizing an $\mathrm{E}(m)\times \mathbb{S}_m$-invariant value network with $\alpha_t = 1$.  


The reduction in effective dimension arises from the fact that the system is translation-invariant, allowing us to project the set of particles onto a subspace that satisfies the zero-mean condition, i.e., $\mathcal{X} = \{\bx \in \mathbb{R}^{n\times m} \mid \sum_{i=1}^{n} x_i = 0\}$. This projection ensures that the Boltzmann distribution is well-defined on $\mathcal{X}$, as it is impossible to construct a translation-invariant measure on $\mathbb{R}^{n\times m}$. Consequently, we consider the case where $\mu_{\phi}^t$ and $\epsilon_t$ in \cref{eq:pls} are restricted to $\mathcal{X}$. Since $\mathcal{X}$ is defined by $m$ linear constraints, it is isomorphic to $\mathbb{R}^{(n-1)m}$. Therefore, the arguments in \cref{sec:value_grad} remain valid while accounting for the reduced effective dimension.
  

We use an $\mathrm{E}(m)\times \mathbb{S}_n$-invariant value network to reflect the invariance of the value function. The following theorem shows that, with $\alpha_t=1$, the value function of VGS retains $\mathrm{O}(m)\times \mathbb{S}_n$-invariance on $\mathcal{X}$.

\begin{theorem}[Invariance of the Value Function]
\label{theorem:invariance}
    Assume that the energy function is $\mathrm{O}(m)\times \mathbb{S}_n$-invariant under the trivial group action $\circ$, as follows: 
    \begin{align}
        E(g \circ \bx) =  E(\bx) \quad \forall \bx\in \mathcal{X}, g\in \mathrm{O}(m)\times \mathbb{S}_n.
    \end{align}
    If $\alpha_t = 1$, then the value function $V_{\pi}^t$ of VGS preserves $\mathrm{O}(m)\times \mathbb{S}_n$-invariance:
    \begin{align}
        V^{t}_{\pi}(g \circ \bx_t) =  V^t_{\pi}(\bx_t) \quad \forall \bx_t\in \mathcal{X}, g\in \mathrm{O}(m)\times \mathbb{S}_n.
    \end{align}
\end{theorem}

\begin{proof}
    The proof follows by mathematical induction from $t=T-1$ to $t=0$. At each step, the fact that the gradient of a $\mathrm{G}$-invariant function $f:\mathcal{X}\to\mathbb{R}$ is $\mathrm{G}$-equivariant is used, provided that $\mathrm{G}$ acts orthogonally on $\mathcal{X}$ \citep[Lemma 2]{papamakarios2021normalizing}. For details, see \cref{appendix:thm4.1}.
\end{proof} 

Instead of projecting samples onto $\mathcal{X}$ and applying an $\mathrm{O}(m)\times \mathbb{S}_n$-invariant transformation, we directly use an $\mathrm{E}(m)\times \mathbb{S}_n$-invariant value network. The invariant network is designed using the pairwise distances of particles as inputs (for details, see \cref{sec:particle_exp}). Our approach is simpler than constructing an equivariant network, as done in prior works \cite{akhound-sadegh2024iterated, he2024training}. Equivariant networks often rely on graph structures \cite{satorras2021n} or require complex components such as spherical harmonics \cite{fuchs2020se, thomas2018tensor}.


\begin{algorithm}[tb]
   \caption{Training EBM with VGS}
   \label{alg:ebm-vgs}
\begin{algorithmic}
   \STATE {\bfseries Input:} EBM $E_\theta(\bx)$, Values $V_\phi^t(\bx_t)$, Dataset $\mathcal{D}=\{\bx_i\}_{i=1}^{N}$, Hyperparameter $\gamma>0$, Regularizer functional $Reg_\theta$.
   % \REPEAT
   % Generate sample using Algorithm 1
   \FOR{each minibatch $\bx_i\sim\mathcal{D}$}
   \STATE Sample $\bx^-\sim\pi_\phi(\bx)$ by \Cref{alg:vgs_sampling}
   % update energy
   \STATE $\min_\theta E_\theta(\bx_i) - E_\theta(\bx^-) + \gamma Reg_\theta(\bx_i, \bx^-)$
   % update value using Algorithm 2
   \STATE Update $V_\phi^t$ using \Cref{alg:vgs_training}.
   \ENDFOR
   % \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\section{Training Energy-Based Models with Value Gradient Samplers}

This section describes how VGS can serve as an effective alternative to MCMC for EBM training. \looseness=-1

\textbf{MCMC in EBM Training.}\quad
An EBM has the same form as the target density considered in \cref{eq:ebm} but with the energy function $E_\theta(\bx)$ having a trainable parameter $\theta$, i.e., $q_\theta(\bx) = \frac{1}{Z_\theta}\exp(-E_\theta(\bx)/\tau)$. 
Training an EBM amounts to finding $\theta$ that makes $q_\theta(\bx)$ as close as possible to the data generating density $p(\bx)$ given only access to a set of training data $\{\bx_i\}_{i=1}^{N}\sim p(\bx)$. Due to the intractable $Z_\theta$, evaluating the likelihood gradient requires sampling from model $q_\theta(\bx)$ \cite{hinton2002training}:\looseness=-1
\begin{align}
    \nabla_\theta \log q_\theta(\bx) = -\nabla_\theta E_\theta(\bx) + \mathbb{E}_{\bx^-\sim q_\theta}[\nabla_\theta E_\theta(\bx^-)],\nonumber
\end{align}
where $\bx^-$ is often called ``negative samples." The EBM update usually involves a regularizer $Reg_\theta(\bx_i,\bx^-)$ for the energy. The negative sample $\bx^-\sim q_\theta(\bx)$ is simulated by MCMC, which has been the key bottleneck in EBM training regarding computational efficiency and stability.
Still, many implementations of deep EBMs use MCMC, such as Langevin Monte Carlo, to generate negative samples \cite{du2019,gao2021learning,yoon2021autoencoding,yoon2023energybased}. 


\textbf{Training EBMs with VGS.}\quad
Having a trainable neural sampler $\pi_\phi(\bx)$ to replace MCMC has been shown to greatly stabilize training and often produce a more accurate energy function \cite{Abbasnejad_2019_CVPR,BoDai2019,dai2017calibrating,Han_2020_CVPR,kumar2019maximum,geng2021bounds,geng2024improving,yoon2024maximum}. Many of the frameworks for incorporating $\pi_\phi(\bx)$ have an equivalent objective function, which can be succinctly written as the following: \looseness=-1
\begin{align}
    \min_{\theta} \max_{\phi} KL(p(\bx)||q_\theta(\bx)) - KL(\pi_\phi(\bx) ||q_\theta(\bx)). \label{eq:gcd}
\end{align}    
This minimax problem has the Nash equilibrium of $p(\bx)=q_\theta(\bx)=\pi_\phi(\bx)$ when the models are well-specified. In practice, the equilibrium can be found by updating EBM $q_\theta(\bx)$ and sampler $\pi_\phi(\bx)$ alternatingly.

Being an algorithm aiming to (approximately) solve $\min_\phi KL(\pi_\phi(\bx) ||q_\theta(\bx))$, VGS can be used as a drop-in replacement for $\pi_\phi(\bx)$ in \Cref{eq:gcd}.
Although VGS minimizes only the upper bound of the KL divergence, it shows promising empirical results.
The process of training EBM using VGS is described in \Cref{alg:ebm-vgs}.



 
\section{Related Work}

SDE-based samplers, such as PIS \cite{zhang2022path}, DIS \cite{berner2022optimal}, DDS \cite{vargas2023denoising}, and \citet{richter2024improved}, inherently possess rich interpretation to the optimal control problem. Although discussed theoretically, the connection to optimal control is rarely exploited directly. 

% normalizing flows
% FAB \cite{midgley2023flow} uses normalizing flow with AIS to train a sampler.
% DiKL \cite{he2024training} trains an implicit generator by matching the spread divergence between the model and the target.

Sequentially drawing intermediate samples from a series of distributions that interpolate between the initial and target distributions is a recurring strategy in sampling. Classical sampling methods, such as SMC \cite{moral2006smc} and AIS \cite{neal2001annealed}, generate intermediate samples from tempered distributions.
More recent works explore the use of diffused target densities as intermediate distributions. However, a key challenge in this setting is that the score of diffused densities cannot be estimated using the same techniques in diffusion modeling \cite{sohl-dickstein15,ho2020ddpm,song2021scorebased}. To address this, various approaches have been proposed, leveraging different mathematical formulations \cite{akhound-sadegh2024iterated,phillips2024particle,huang2024reverse,mcdonald2022proposal,wang2024energy,chen2024sequential}. VGS addresses the same challenge but with the toolbox of RL, as the value function is theoretically linked to the log of the diffused target density. 



\section{Experiments}

\subsection{Sampling from Synthetic Distributions}
\label{sec:synthetic_exp}

\textbf{Target Distributions.}\quad
We use two distributions as our sampling benchmarks: a 9-component Gaussian Mixture Model (GMM) illustrated in Figure \ref{fig:2d-vgs}. and a funnel distribution. Further details in \cref{appendix:target_dist}.

These benchmarks are chosen to demonstrate that our VGS produces results that are competitive with the baseline methods: PIS \cite{zhang2022path}, DIS \cite{berner2022optimal}, DDS \cite{vargas2023denoising}.


\textbf{Performance Metrics.}\quad
 The comparisons are made based on three metrics: the Sinkhorn Distance ($\mathcal{W}^2_{\gamma} \downarrow$), the total variation distance-energy (TVD-E $\downarrow$), and the error in estimating the average standard deviation across the marginal distributions ($\Delta$std $\downarrow$). 
 We also report the number of time steps to generate a sample ($T$). 
 Further details in \cref{appendix:metrics}.
 
  We report both $\mathcal{W}^2_{\gamma}$ and TVD-E metrics to complement each other. $\mathcal{W}^2_{\gamma}$ is less sensitive to noisy samples, whereas TVD-E is less sensitive to missing modes \cite{he2024training}. \looseness=-1

 % Comment on W-2 vs energy TVD written on DiKL paper.

% We report the Wasserstein-2 (W-2) distances between
% samples yielded by these methods and ground truth
% samples obtained by MCMC. We also evaluate the
% sample energy and report the total-variant distance
% (TVD) between the distribution of the energy of samples. We note that both metrics have their limitations:
% W-2 tends to be less sensitive to noisy samples, which
% can be particularly detrimental in some n-body systems. Conversely, the energy TVD is less sensitive to
% missing modes. To provide a comprehensive evaluation, we plot both metrics together in Figure 5, and
% also visualize the samples along two selected axes to
% assess mode coverage in Figure 4.
 
\textbf{VGS Implementation.}\quad
 The trainable value function of the VGS sampler $V^t_\phi:\mathbb{R}^d \times \mathbb{Z}^{+} \rightarrow \mathbb{R}$ can be trained using any neural network architecture that incorporates time step embedding. We use a Fourier MLP architecture, an MLP with sinusoidal time step embedding. Further details in \cref{appendix:exp_setup}. 

\textbf{Results.}\quad
\cref{tab:sampler-performance} demonstrates that VGS outperforms all baseline models across both target distributions, GMM and funnel. VGS achieved similar or superior performance compared to SDE-based samplers on $\mathcal{W}^2_{\gamma}$ and $\Delta$std while using a significantly smaller number of time steps $T$. 

% However, TVD-E metric is sub-optimal compared to SDE-based samplers. Monitoring both metrics simultaneously enriches the understanding of the learned distribution에 해당하는 result 내용 추가


\textbf{Effect of the Number of Time Steps.}\quad
We report the Sinkhorn distance $\mathcal{W}_{\gamma}^{2}$ to the number of time steps $T$ in GMM for our method and the baselines in \cref{fig:nfe_sinkhorn}. Our method shows smaller $\mathcal{W}_{\gamma}^{2}$ compared to DDS for all $T$ under 100. We also proved that the performance of VGS is robust to $T$, while baselines such as DDS show a clear decline in performance when $T$ is reduced due to their formulation in the continuous-time domain. Lastly, we proved that VGS's performance on large numbers of $T$ can be reproduced for small numbers of $T$ by scaling noise.
%at the cost of robustness of $T$. 
\begin{table}[t!]
    \centering
    \setlength{\tabcolsep}{2pt}
    \caption{Sampler performance benchmark for GMM ($d=2$) and Funnel ($d=10$). The reported metrics are the mean and standard deviation over five trials.}
    \vskip 0.5mm
    \small
    %\resizebox{\textwidth}{!}
    {\begin{tabular}{lcccc}
    \toprule
     & $T$ $\downarrow$ &$\mathcal{W}^2_\gamma \downarrow$ & TVD-E $\downarrow$ & $\Delta\operatorname{std} \downarrow$ \\
    \midrule
    
    \multicolumn{2}{l}{\textbf{GMM} $(d=2)$} \\
    DiKL & 1 &0.033\scriptsize{$\pm$0.001} & 0.320\scriptsize{$\pm$0.002} & 0.052\scriptsize{$\pm$0.000} \\
    iDEM & 1000 &0.036\scriptsize{$\pm$0.001} & 0.412\scriptsize{$\pm$0.005} & 1.377\scriptsize{$\pm$0.014} \\
    PIS & 200 &0.302\scriptsize{$\pm$0.158} & 0.016\scriptsize{$\pm$0.001} & 1.968\scriptsize{$\pm$0.378} \\
    % \cmidrule{2-6}
    DIS & 200 &0.054\scriptsize{$\pm$0.002} & 0.017\scriptsize{$\pm$0.002} & 2.400\scriptsize{$\pm$0.045}\\
    DDS & 256 &0.049\scriptsize{$\pm$0.003} & \textbf{0.015}\scriptsize{$\pm$0.001} & 2.273\scriptsize{$\pm$0.102}\\
    % \cmidrule{2-6}
    VGS (Ours) & 10 &\textbf{0.021}\scriptsize{$\pm$0.000} & 0.051\scriptsize{$\pm$0.002} & 0.062\scriptsize{$\pm$0.041}  \\
     & 20 & \textbf{0.021}\scriptsize{$\pm$0.001} & 0.062\scriptsize{$\pm$0.004} & \textbf{0.024}\scriptsize{$\pm$0.008} \\
    \midrule
    \multicolumn{2}{l}{\textbf{Funnel} $(d=10)$} \\
    PIS & 200 &7.968\scriptsize{$\pm$0.610} & 0.248\scriptsize{$\pm$0.015} & 5.363\scriptsize{$\pm$0.124} \\
    % \cmidrule{2-6}
    DIS & 200 &7.834\scriptsize{$\pm$0.739} & \textbf{0.245}\scriptsize{$\pm$0.012} & 5.174\scriptsize{$\pm$0.158} \\
    % \cmidrule{2-6}
    DDS & 256 &7.489\scriptsize{$\pm$0.448} & 0.256\scriptsize{$\pm$0.015} & 6.495\scriptsize{$\pm$0.230} \\
    % \cmidrule{2-6}
    VGS (Ours) & 10 &7.616\scriptsize{$\pm$0.556} & 0.618\scriptsize{$\pm$0.018} & 7.929\scriptsize{$\pm$0.056} \\
     & 30 & \textbf{7.272}\scriptsize{$\pm$0.674} & 0.377\scriptsize{$\pm$0.053} & \textbf{5.059}\scriptsize{$\pm$0.104} \\

    \bottomrule
    \end{tabular}}
    % \vspace{-4pt}
    \label{tab:sampler-performance}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/nfe_sinkhorn.pdf}
    \vskip -0.3cm
    \caption{Sinkhorn distance $\mathcal{W}_{\gamma}^{2}$ to the number of time steps $T$ on GMM. Performance of VGS shows robustness to the number of time steps $T$ and outperforms DDS on every $T<100$. Increasing the noise magnitude $s_t$ in VGS improves performance in the small $T$ regime but makes training divergent when $T$ is large.
    % was increased from low to high by doubling the value $s_{t}$ for each level with fixed start and end value for each experiment. Increasing noise level limits the training of VGS for large number of time steps $T$.
    }
    \vspace{-8pt}

    \label{fig:nfe_sinkhorn}
\end{figure}



\textbf{Training and Sampling Time.}\quad We report the training and sampling times of VGS and baseline models for synthetic distributions in \cref{tab:train_sample_time}. Given the comparable model size, VGS shows faster training and sampling time than all SDE-based samplers (e.g., PIS, DIS, and DDS). Although each VGS step is more expensive than a step of SDE, as computing the gradient is usually more expensive than a forward pass, VGS supports a significantly smaller number of time steps to be faster than SDE samplers. Details can be found in \cref{appendix:train_sample_time}. \looseness=-1


\subsection{Sampling $n$-Body Particle Systems} 
\label{sec:particle_exp}

\textbf{Target Distributions.} \quad 
We conduct experiments on two benchmark particle systems: a 4-particle double-well potential (DW-4) and a 13-particle Lennard-Jones potential (LJ-13). Both tasks were introduced in \cite{kohler2020equivariantflowsexactlikelihood} to evaluate sampler performance under invariant target distributions. Further details about the potentials are provided in \cref{appendix:target_dist}.

\textbf{Performance Metrics.} \quad
We evaluate performance using three metrics: the total variation distance of energy (TVD-E $\downarrow$), the total variation distance of interatomic distances (TVD-D $\downarrow$), and the Wasserstein-2 distance ($\mathcal{W}^2 \downarrow$) between the test and generated samples. The samples are normalized to zero mean when computing $\mathcal{W}^2$. Since small changes in particle positions can lead to significant energy variations, $\mathcal{W}^2$ may be a less meaningful metric in this experiment.

\textbf{Invariant Value Network.} \quad 
We leverage the symmetry of particle systems by using $\mathrm{E}(m) \times \mathbb{S}_n$-invariant value networks. To ensure $\mathrm{E}(m)$-invariance, we use the $n(n-1)/2$ pairwise distances of the particle system as inputs. This design is naturally invariant and retains all information, as the configuration of particles $\bx$ contains equivalent information to the pairwise distances \citep[Appendix E]{satorras2021n}. Additionally, we sort the pairwise distances in descending order to further enforce $\mathbb{S}_n$-invariance.

\textbf{Results.} \quad 
We compare the performance of VGS in sampling particle systems with state-of-the-art methods: FAB \cite{midgley2023flow}, iDEM \cite{akhound-sadegh2024iterated}, and DiKL \cite{he2024training}. The results are summarized in \cref{tab:particle_tab}. VGS outperforms other baseline methods based on the TVD-E and TVD-D metrics. We visualize the energy and interatomic distance histograms for DW-4 in \cref{fig:dw4_histogram}. As described in \cref{sec:n_body_VGS}, we set $D=(n-1)m$ and $\alpha_t = 1$ for all experiments. More experimental details are provided in \cref{appendix:particle_detail}.




\begin{table}[t!]
    \centering
    \caption{Results of the $n$-body systems experiment. Baseline results are referenced from \cite{he2024training}, and we report our results using the same evaluation process. For details, see \cref{appendix:particle_detail}.}
    \small
    {\begin{tabular}{lrrr}
    \toprule
      & TVD-E $\downarrow$ & TVD-D $\downarrow$ & $\mathcal{W}^2 \downarrow$\\
    \midrule
    \multicolumn{2}{l}{\textbf{DW-4} $(d=8)$} \\
    FAB & 0.224\scriptsize{$\pm$0.008} & 0.097\scriptsize{$\pm$0.005} &  \textbf{1.554}\scriptsize{$\pm$0.015}\\
    iDEM  & 0.197\scriptsize{$\pm$0.010} & 0.103\scriptsize{$\pm$0.005}&  1.593\scriptsize{$\pm$0.012}
    \\
    DiKL & 0.162\scriptsize{$\pm$0.016} & 0.146\scriptsize{$\pm$0.006}&  1.579\scriptsize{$\pm$0.019}\\
    VGS ($T$ = 30) & \textbf{0.066}\scriptsize{$\pm$0.009} &
    \textbf{0.057}\scriptsize{$\pm$0.004} &  1.610\scriptsize{$\pm$0.027}\\
    \midrule
    \multicolumn{2}{l}{\textbf{LJ-13} $(d=39)$} \\
    FAB & 0.902\scriptsize{$\pm$0.010} & 0.252\scriptsize{$\pm$0.002}  &  4.938\scriptsize{$\pm$0.009} \\
    iDEM & 0.306\scriptsize{$\pm$0.013} & 0.044\scriptsize{$\pm$0.001}&  4.172\scriptsize{$\pm$0.007} \\
    DiKL & 0.284\scriptsize{$\pm$0.011} & 0.046\scriptsize{$\pm$0.002}&  4.233\scriptsize{$\pm$0.006} \\
    VGS ($T$ = 100)&
    \textbf{0.112}\scriptsize{$\pm$0.009} &
    \textbf{0.014}\scriptsize{$\pm$0.002} & \textbf{4.167}\scriptsize{$\pm$0.005}\\
    \bottomrule
    \end{tabular}}\label{tab:particle_tab}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/dw4_histogram.png}
    \vskip -0.2cm
    \caption{Energy (Left) and Interatomic Distance (Right) histograms of DW-4 samples from test data, VGS, and VGS with a non-invariant network. VGS generates accurate samples by leveraging the symmetry of the system.}
    \label{fig:dw4_histogram}
    \vskip -0.2cm
\end{figure}

\subsection{Training Energy-Based Models}


\subsubsection{Synthetic Distribution}

We train an EBM using VGS with $T=10$ following \Cref{alg:ebm-vgs} on a 2D 8 Gaussians dataset. For comparison, another EBM is trained using Langevin Monte Carlo with the same number of time steps as VGS.
The squared energy regularizer is used: $Reg_\theta(\bx_i,\bx^-)=E_\theta(\bx_i)^2+E_\theta(\bx^-)^2$.
As shown in \Cref{fig:EBM_MCMC-vs-VGS}, VGS recovers the accurate boundaries of the distributions and generates high-quality samples. Meanwhile, the EBM trained with Langevin Monte Carlo fails to reflect the true energy, even though Langevin Monte Carlo has the same mathematical form as VGS (\Cref{eq:pls}).
Note that short-run MCMC is known to produce inaccurate energy estimates in EBM training \cite{Nijkamp2019nonconvergent,nijkamp2022mcmc}, although it may generate feasible samples. 
% Experimental details can be found in \Cref{app:8gaussians}.




\subsubsection{Energy-Based Anomaly Detection}


\begin{table}[t]
    \centering
    \small
    \caption{Anomaly detection (DET) and localization (LOC) on MVTec-AD. AUROC averaged over object categories is shown.} \label{tab:anomaly-mvtec}
    {\begin{tabular}{lrrr}
    \toprule
     Model & DET & LOC \\
    \midrule
     DRAEM \cite{Zavrtanik_2021_ICCV} & 88.1 & 87.2 \\
     MPDR \cite{yoon2023energybased} & 96.0 & 96.7  \\
     UniAD \cite{you22uniad}  & 96.5\scriptsize{$\pm$0.08} & 96.8\scriptsize{$\pm$0.02}\\
     EBM+VGS (Ours) & \textbf{97.0} \scriptsize{$\pm$0.09}  & \textbf{97.1}\scriptsize{$\pm$0.01}\\
     \bottomrule
    \end{tabular}}
\end{table}
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/figure_ebm_comparison.png}
    \vskip -0.35cm
    \caption{EBM training on 2D 8 Gaussians. The red shade depicts the energy, and the dots are the samples. VGS produces an accurate energy estimate, while the short-run MCMC does not.}
    \label{fig:EBM_MCMC-vs-VGS}
    \vskip -0.18cm
\end{figure}

An energy function that accurately captures the data distribution can be used in unsupervised anomaly detection, as an anomaly will be assigned a high energy value.
When applied to MVTec-AD, an industrial anomaly detection dataset \cite{bergmann2021mvtec}, the EBM trained with VGS outperforms MPDR \cite{yoon2023energybased}, an EBM trained with a sophisticated MCMC scheme, as well as UniAD \cite{you22uniad}, a non-EBM method.
We adopt the unified multi-class setup in \citet{you22uniad}, which treats 15 object categories as a single class, resulting in multi-modal data distribution.
The training set only consists of the normal class, and the test set contains the normal and the detective classes. All images are encoded into 272$\times$14$\times$14 vectors using EfficientNet-b4 \cite{tan2019efficientnet}. The energy function is defined on the 272-dimensional space and applied across the spatial dimensions.
The dataset supports detection and localization tasks, where the performance is measured by area under the receiver operating characteristic curve (AUROC) computed image-wise and pixel-wise, respectively. The summarized results are presented in Table \ref{tab:anomaly-mvtec}. 
The full result and experimental details are in \cref{app:mvtec}.

% The EBM trained with VGS outperforms an EBM trained with a sophisticated MCMC scheme

% which contains 224$\times$224 RGB images of 15 object categories. We follow the multi-class problem setup proposed by \cite{you22uniad}. The training dataset contains normal object images from 15 categories without any labels. The test set consists of both normal and defective object images, each provided with an anomaly label and a mask indicating the defect location. The goal is to detect and localize anomalies, with performance measured by AUC computed per object category. This setting is challenging because the energy function should reflect the multi-modal data distribution.
% Following the preprocessing protocol in \cite{you22uniad, yoon2023energybased}, each image is transformed into a 272$\times$14$\times$14 vector using a pre-trained EfficientNet-b4 \cite{tan2019efficientnet}. VGS is conducted in a 272-dimensional space, treating each spatial coordinate independently. With the trained energy function, we can evaluate the energy value of 14x14 spatial features and use max pooling and bilinear interpolation for anomaly detection and localization, respectively.






% \textbf{MVTec-AD.}
%  -> 세희
% \paragraph{ViSA}
%  -> 혁주님
% \paragraph{BTAD}
% -> 동규님

\section{Conclusion}

This paper has formulated the sampling problem as a discrete-time continuous-space sequential decision-making problem.
This fresh perspective allows us to employ the techniques from RL, which was previously considered disjoint from the statistical sampling problem.
We believe this connection is still in its early stages of exploration and has the potential to inspire the development of new algorithms through the synergy of RL and statistical machine learning.

% \paragraph{Discussion} Comparision to normalizing flows??

\textbf{Limitations.}\quad First, VGS cannot generate samples in a single step. However, applying existing diffusion distillation techniques to VGS could potentially enable single-step sampling. Second, VGS does not provide an exact likelihood estimation for the generated samples, which prevents the direct use of techniques such as importance sampling, which relies on precise likelihood values. Third, the error of VGS is not mathematically quantified. We leave rigorous mathematical analysis as future work. 

% \section*{Software and Data}
% submission할 때는 안 써도 될 것 같음

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}



\section*{Impact Statement}


This work presents a novel algorithm for addressing the statistical problem of sampling. While its societal impact is not immediately apparent, the algorithm's applications may have indirect effects, including advancing natural sciences and potentially enhancing malicious generative models. However, we do not identify any specific potential impacts requiring further discussion at this time.



% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.




\bibliography{ref}
\bibliographystyle{icml2025}


\input{2_appendix}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
