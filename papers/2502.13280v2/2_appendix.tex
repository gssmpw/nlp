%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Bellman Optimality Equation} \label{appendix:bellman}
The optimal value function $V^{t}_{*}(\bx_t)$ for our optimal control problem \Cref{eq:obj-opt-control} is defined as follows:
\begin{align}
    V^{t}_{*}(\bx_t)=\min_{\pi}\mathbb{E}_{\pi}\left[
        E(\bx_T) + \tau  \sum_{t'=t}^{T-1} \log \frac{\pi(\bx_{t'+1}|\bx_{t'})}{\tilde{q}(\bx_{t'} | \bx_{t'+1})} \bigg| \bx_t \right]. \label{eq:opt_value_def}
\end{align}
Bellman optimality equation is given as follows:
\begin{align}
    V^{t}_{*}(\bx_t)=\min_{\pi(\bx_{t+1}|\bx_t)}\mathbb{E}_{\pi(\bx_{t+1}|\bx_t)}\left[
        V^{t+1}_{*}(\bx_{t+1}) + \tau  \log \frac{\pi(\bx_{t+1}|\bx_{t})}{\tilde{q}(\bx_{t} | \bx_{t+1})} \bigg| \bx_t \right].
\end{align}

\section{Proof of Theorems}
\subsection{Proof of Theorem 3.1} \label{appendix:opt_val} 
\textbf{Theorem 3.1} (Optimal Value Function)  
If the admissible set of policies $\pi(\bx_{t+1:T}|\bx_t)$ includes the auxiliary distribution $\tilde{q}(\bx_{t+1:T}|\bx_t)$, the optimal value function $V^t_*(\bx_t)$ (\cref{eq:opt_value_def}) satisfies:
\begin{align}
    \tilde{q}(\bx_t) = \frac{1}{Z}\exp{(-V^t_*(\bx_t) / \tau)}.
\end{align}

\begin{proof}
By assumption, $\min_{\pi} KL(\pi(\bx_{t+1:T}|\bx_t) |\tilde{q}(\bx_{t+1:T}|\bx_t)) = 0$. Since KL divergence is always non-negative, this minimum is attained when $\pi(\bx_{t+1:T}|\bx_t) = \tilde{q}(\bx_{t+1:T}|\bx_t)$. From the definition of the optimal value function in \cref{eq:opt_value_def}, we obtain:
\begin{align}
    V^{t}_{*}(\bx_t) &= \min_{\pi}\mathbb{E}_{\pi}\left[
        E(\bx_T) + \tau  \sum_{t'=t}^{T-1} \log \frac{\pi(\bx_{t'+1}|\bx_{t'})}{\tilde{q}(\bx_{t'} | \bx_{t'+1})} \bigg| \bx_t \right] \nonumber \\
    &= \min_{\pi} \tau \mathbb{E}_{\pi(\bx_{t+1:T}|\bx_{t})}\left[-\log{\tilde{q}(\bx_{t:T})} + \log{\pi(\bx_{t+1:T}|\bx_t)} \bigg| \bx_{t}\right] - \tau\log{Z} \nonumber\\
    &= \min_{\pi} \tau \mathbb{E}_{\pi(\bx_{t+1:T}|\bx_{t})}\left[-\log{\tilde{q}(\bx_{t+1:T}|\bx_t)}  + \log{\pi(\bx_{t+1:T}|\bx_t)} \bigg| \bx_{t}\right] - \tau\log{\tilde{q} (\bx_t)}- \tau\log{Z} \nonumber\\
    &= \min_{\pi} \tau KL(\pi(\bx_{t+1:T}|\bx_t) \| \tilde{q}(\bx_{t+1:T}|\bx_t)) - \tau\log{\tilde{q} (\bx_t)}- \tau\log{Z} \nonumber\\
    &=  - \tau\log{\tilde{q} (\bx_t)}- \tau\log{Z}. \label{eq:opt_val}
\end{align}
Here, we slightly abuse notation by writing $\tilde{q}(\bx_{t+1:T})=\tilde{q}(\bx_{t+1:T-1}|\bx_T)q(\bx_T)$ and $\tilde{q}(\bx_{t+1:T}|\bx_t)=\tilde{q}(\bx_{t+1:T-1}|\bx_t,\bx_T)q(\bx_T)$. Rearranging \cref{eq:opt_val}, we obtain:
\begin{align}
    \tilde{q}(\bx_t) = \frac{1}{Z}\exp{(-V^{t}_{*}(\bx_t)/\tau)}.
\end{align}
\end{proof}




\subsection{Proof of Theorem 4.1}\label{appendix:thm4.1}
\textbf{Theorem 4.1} (Invariance of the Value Function) 
Assume that the energy function is $\mathrm{O}(m)\times \mathbb{S}_n$-invariant under the trivial group action $\circ$, as follows: 
\begin{align}
    E(g \circ \bx) =  E(\bx) \quad \forall \bx\in \mathcal{X}, g\in \mathrm{O}(m)\times \mathbb{S}_n.
\end{align}
If $\alpha_t = 1$, then the value function $V_{\pi}^t$ of VGS preserves $\mathrm{O}(m)\times \mathbb{S}_n$-invariance:
\begin{align}
    V^{t}_{\pi}(g \circ \bx_t) =  V^t_{\pi}(\bx_t) \quad \forall \bx_t\in \mathcal{X}, g\in \mathrm{O}(m)\times \mathbb{S}_n.
\end{align}
\begin{proof}
Substituting $\alpha_t=1$ and $\mu_{t}(\bx_t) = - \frac{s_t^2 \alpha_{t}^{2}}{\tau} \nabla_{\alpha_{t}\bx_{t}} V_{\pi}^{t+1}(\alpha_{t}\bx_t)$, $\sigma_t = \alpha_{t}s_t$ into \cref{eq:optimal_policy}, we derive the Bellman equation for VGS as follows:
\begin{align}
    V^{t}_{\pi}(\bx_t) = & \frac{s_t^2  ||\nabla_{\bx_{t}} V_{\pi}^{t+1}(\bx_t)||^2 }{2\tau} +\mathbb{E}_{\epsilon_t}[V^{t+1}_{\pi}(\bx_t  - \frac{s_t^2}{\tau} \nabla_{\bx_{t}} V_{\pi}^{t+1}(\bx_t) + s_t\epsilon_t)]. \label{eq:inv_proof}
\end{align}
Assuming that $V_{\pi}^{t+1}$ is $\mathrm{O}(m)\times \mathbb{S}_n$-invariant, we aim to show that $V_{\pi}^{t}$ is also $\mathrm{O}(m)\times \mathbb{S}_n$-invariant. Since $||g \circ \bx|| = ||\bx||$ for all $\bx\in \mathcal{X}$, $\circ$ is an orthogonal action. Therefore $\nabla V_{\pi}^{t+1}$ is equivariant under this action, implying that $\nabla_{g \circ \bx_{t+1}} V_{\pi}^{t+1}(g\circ\bx_{t+1}) = g \circ\nabla_{\bx_{t+1}} V_{\pi}^{t+1}(\bx_{t+1})$ \citep[Lemma 2]{papamakarios2021normalizing}. Using this property we obtain:
\begin{align}
    V^{t}_{\pi}(g \circ \bx_t) &= \frac{s_t^2  ||\nabla_{g \circ \bx_{t}} V_{\pi}^{t+1}(g\circ \bx_t)||^2 }{2\tau} +\mathbb{E}_{\epsilon_t}[V^{t+1}_{\pi}(g\circ\bx_t  - \frac{s_t^2}{\tau} \nabla_{g \circ \bx_{t}} V_{\pi}^{t+1}(g\circ\bx_t) + s_t\epsilon_t)] \\
    &= \frac{s_t^2  ||g\circ\nabla_{\bx_{t}} V_{\pi}^{t+1}( \bx_t)||^2 }{2\tau} +\mathbb{E}_{\epsilon_t}[V^{t+1}_{\pi}(g\circ(\bx_t  - \frac{s_t^2}{\tau} \nabla_{\bx_{t}} V_{\pi}^{t+1}(\bx_t) +  s_t (g^{-1}\circ\epsilon_t))] \\
    & = \frac{s_t^2  ||\nabla_{\bx_{t}} V_{\pi}^{t+1}( \bx_t)||^2 }{2\tau} +\mathbb{E}_{\epsilon_t' = g^{-1}\circ\epsilon_t}[V^{t+1}_{\pi}(g\circ(\bx_t  - \frac{s_t^2}{\tau} \nabla_{\bx_{t}} V_{\pi}^{t+1}(\bx_t) +  s_t\epsilon_t'))] \\
    & = \frac{s_t^2  ||\nabla_{\bx_{t}} V_{\pi}^{t+1}( \bx_t)||^2 }{2\tau} +\mathbb{E}_{\epsilon_t}[V^{t+1}_{\pi}(\bx_t  - \frac{s_t^2}{\tau} \nabla_{\bx_{t}} V_{\pi}^{t+1}(\bx_t) +  s_t\epsilon_t)] = V^{t}_{\pi}(\bx_t).
\end{align}
For the final equality, we used the fact that the probability density function of a Gaussian random variable is $\mathrm{O}(m)\times \mathbb{S}_n$-invariant, which implies $\epsilon_t' \sim \mathcal{N}(0, I)$.  


The theorem result follows by mathematical induction, as $V^{T}(\bx_T) = E(\bx_T)$ is $O(n) \times S_m$-invariant by the assumption of the theorem.
\end{proof}

% \subsection{Propagation of Suboptimality.}
% Additionally, we define the value function (with respect to the policy $\pi$) as:
% \begin{align}
%      V^{\pi}_t(\bx_t) &\doteq  \mathbb{E}_{\pi(\bx_{t+1:T}|\bx_{t})}\left[E(\bx_T) + \sum_{t'=t}^{T-1} -\log{\tilde{q}(\bx_{t'}|\bx_{t'+1}) + \log{\pi(\bx_{t'+1}|\bx_{t'})}}\bigg| \bx_t \right] \\
%      &= \mathbb{E}_{\pi(\bx_{t+1}|\bx_{t})}\left[-\log{\tilde{q}(\bx_{t}|\bx_{t+1})} + \log{\pi(\bx_{t+1}|\bx_{t})} + V^{\pi}_{t+1}(\bx_{t+1}) \bigg| \bx_t \right]\\
%      &= KL(\pi(\bx_{t+1}|\bx_t)||\tilde{q}(\bx_{t+1}|\bx_t)) + \mathbb{E}_{\pi(\bx_{t+1}|t)}\left[V^{\pi}_{t+1}(\bx_{t+1}) +\log{\tilde{q}(\bx_{t+1})}\bigg| \bx_t \right] -\log{\tilde{q}(\bx_{t})}\\
%     &= KL(\pi(\bx_{t+1}|\bx_t)||\tilde{q}(\bx_{t+1}|\bx_t)) + \mathbb{E}_{\pi(\bx_{t+1}|t)}\left[V^{\pi}_{t+1}(\bx_{t+1}) - V^{*}_{t+1}(\bx_{t+1}) \bigg| \bx_t \right] + V^*_t(\bx_t),
% \end{align}
% where Bayes' rule is used $\tilde{q}(\bx_{t}|\bx_{t+1})=\tilde{q}(\bx_{t+1}|\bx_t)\tilde{q}(\bx_t)/\tilde{q}(\bx_{t+1})$.
% From the definition, we can observe that the sub-optimality of the value function, $\delta_t(\bx_t) \doteq V^{\pi}_t(\bx_t) - V^{*}_t(\bx_t)$ propagates through time as follows:
% \begin{align*}
%     \delta_t(\bx_t) = KL(\pi(\bx_{t+1}|\bx_t)||\tilde{q}(\bx_{t+1}|\bx_t)) + \mathbb{E}_{\pi(\bx_{t+1|t})}\left[\delta_{t+1}(\bx_{t+1})\right].
% \end{align*}


% \subsection{Optimality and Error Bound}
% \begin{align}
%     &KL(\pi(\bx_{0:T})||q_\theta(\bx_{T}) \tilde{q}(\bx_{0:T-1}|\bx_T)) 
%     = KL(\pi(\bx_T)||q_\theta(\bx_T)) 
%     + \mathbb{E}_{\pi}[KL(\pi(\bx_{0:T-1}|\bx_T)||\tilde{q}(\bx_{0:T-1}|\bx_T))]
% \end{align}
% The second KL divergence term characterizes the discrepancy of our upper bound.
% \begin{align}
%     &\mathbb{E}_{\pi}[KL(\pi(\bx_{0:T-1}|\bx_T)||\tilde{q}(\bx_{0:T-1}|\bx_T))]
%     = \sum_{t=0}^{T-1} \mathbb{E}_{\pi}[KL(\pi(\bx_t|\bx_{t+1})||\tilde{q}(\bx_{t}|\bx_{t+1}))] 
% \end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Detailed Derivation of VGS}\label{appendix:second_order}
In this section we present a detailed derivation process of a more accurate approximation of optimal drift and diffusion for VGS. We begin from the optimization problem of \cref{eq:optimal_policy}. We apply second-order Taylor expansion of $V_{\pi}^{t+1}$ with respect to $\alpha_{t}\bx_{t}$.
\begin{align}
    &\mathbb{E}_{\epsilon}[V_{\pi}^{t+1}(\alpha_{t}\bx_{t} +\mu_{t}+\sigma_{t}\epsilon)] \nonumber \\
    &\approx \mathbb{E}_{\epsilon}\left[
    V_{\pi}^{t+1}(\alpha_t\bx_{t}) + (\mu_t+\sigma_t\epsilon)^\top \nabla_{\alpha_t\bx_t} V_{\pi}^{t+1}(\alpha_t\bx_t)
     +\frac{1}{2} (\mu_t+\sigma_t\epsilon)^\top \nabla\nabla_{\alpha_t\bx_t} V_{\pi}^{t+1}(\alpha_t\bx_t)  (\mu_t+\sigma_t\epsilon)
    \right] \nonumber \\
    &= V_{\pi}^{t+1}(\alpha_t\bx_{t}) + \mu_t^\top \nabla_{\alpha_t\bx_t} V_{\pi}^{t+1}(\alpha_t\bx_t) + \frac{1}{2}\mu_t^\top \nabla\nabla_{\alpha_t\bx_t} V_{\pi}^{t+1}(\alpha_t\bx_t) \mu_t  + \frac{1}{2}\sigma_t^2 \nabla^2_{\alpha_{t}\bx_{t}}V_{\pi}^{t+1}(\alpha_{t}\bx_{t}).
    \label{eq:second_order}
\end{align}
Substituting the approximation into \cref{eq:optimal_policy} and removing constant terms, we rewrite the optimization problem as follows:
\begin{align}
    \min_{\mu_t, \sigma_t} &
    V^{t+1}_{\pi}(\alpha_{t}\bx_{t}) + \mu_t^\top \nabla_{\alpha_{t}\bx_{t}} V^{t+1}_{\pi}(\alpha_{t}\bx_t) + \frac{1}{2}\mu_t^\top \nabla\nabla_{\alpha_t\bx_t} V_{\pi}^{t+1}(\alpha_t\bx_t) \mu_t + \frac{1}{2}\sigma_t^2 \nabla^2_{\alpha_{t}\bx_{t}}V_{\pi}^{t+1}(\alpha_{t}\bx_{t})  \nonumber \\
    &- \tau D \log \sigma_t + \frac{\tau ||\mu_t||^2 }{2s_t^2 \alpha_{t}^{2}} + \frac{\tau D\sigma_t^2}{2s_t^2 \alpha_{t}^{2}}.\label{eq:second_order_simplified_obj}
\end{align}
Setting the derivatives of \cref{eq:second_order_simplified_obj} with respect to $\mu_{t}$ and $\sigma_{t}$ to zero, we derive the analytic expressions of the optimal drift $\mu_{t}$ and optimal noise magnitude $\sigma_{t}$ as follows:

VGS with first-order approximation:
\begin{align}
    \mu_t = - \frac{s_t^2 \alpha_{t}^{2}}{\tau} \nabla_{\alpha_{t}\bx_{t}} V^{t+1}_{\pi}(\alpha_{t}\bx_t), \quad    \sigma_t = \alpha_{t}s_t.
\end{align}
VGS with second-order approximation:
\begin{align}
    \mu_{t} = -\left(\nabla \nabla_{\alpha_{t}\bx_{t}} V_{\pi}^{t+1}(\alpha_{t}\bx_{t}) + \frac{\tau}{\alpha_{t}^{2}s_{t}^{2}}I \right)^{-1} \nabla_{\alpha_{t}\bx_{t}}V_{\pi}^{t+1}(\alpha_{t}\bx_{t}),
    \quad \sigma_{t}= \frac{\alpha_{t}s_{t}}{\sqrt{1+\alpha_{t}^{2}s_{t}^{2}\nabla_{\alpha_{t}\bx_{t}}^{2}V_{\pi}^{t+1}(\alpha_{t}\bx_{t})/(\tau D)}}. \label{eq:vgs_second_order}
\end{align}


\section{Training the Initial Distribution}\label{appendix:training_init_sigma}
Along with the value function, we can jointly train the initial distribution of VGS by optimizing $\sigma_{init}$. We reformulate our joint KL divergence minimization objective (right-hand side of \cref{eq:data-processing}) with $\sigma_{init}$ as an optimization variable:
\begin{align}
\min_{\sigma_{init}}\ \mathbb{E}_{\pi_\phi(\bx_{0:T})} \Bigg[
E(\bx_T) + \tau\sum_{t=0}^{T-1} \log \frac{\pi_\phi(\bx_{t+1}|\bx_t)}{\tilde{q}(\bx_t|\bx_{t+1})}
 + \tau\log{\pi(\bx_0)}\Bigg].
\end{align}
Recall that $\bx_0 \sim \mathcal{N}(0, \sigma_{init}^2I)$, which implies that $
\mathbb{E}_{\pi_{\phi}(\bx_{0:T})}[\log{\pi(\bx_0)}] = -D\log{\sigma_{init}} + \text{Const}.$ Using the definition of the value function, we simplify the optimization problem as follows:
\begin{align}
\min_{\sigma_{init}}\ \mathbb{E}_{\pi(\bx_{0})}[V_{\pi}^0(\bx_0) -\tau D\log{\sigma_{init}}].
\end{align}
Thus, in our experiments, we update $\sigma_{init}$ according to:
\begin{align}
    \min_{\sigma_{init}} \mathbb{E}_{z\sim \mathcal{N}(0, I)}[V_{\phi^{-}}^{0}(\sigma_{init}z)-\tau D\log\sigma_{init}]. \label{eq:init_sigma_update}
\end{align}
We set the initial value of $\sigma_{init}$ as $\sigma_{init}^2 = (1 + \sum_{t=0}^{T-1}s_t^2)$ when using the variance-exploding process for $\tilde{q}(\bx_{t}|\bx_{t+1})$ and as $\sigma_{init}^2 = 1$ when using the variance-preserving process for $\tilde{q}(\bx_{t}|\bx_{t+1})$. This choice is motivated by the result of \cref{theorem:auxillary}, ensuring that $\pi(\bx_0) = \tilde{q}(\bx_0)$ when $q(\bx)$ is a unit Gaussian.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Additional Details of Experiments}

\subsection{Target Distributions}
\label{appendix:target_dist}
\textbf{GMM} \cite{berner2022optimal}: We use a 2-dimensional Gaussian Mixture Model, with its density given by $\rho(x) = \frac{1}{m} \sum_{i=1}^{m}\mathcal{N}(x;{\mu}_i,\Sigma_{i}).$, where $m = 9,({\mu}_i)_{i=1}^{9} = \{-5,0,5\}\times\{-5,0,5 \}   \subset \mathbb{R}^2$ and $(\Sigma_{i})_{i=1}^{9} = 0.3\mathbf{I} \subset \mathbb{R}^{2\times2}$. With these parameters, a 9-component Gaussian Mixture density with evenly separated modes is obtained. A well-trained sampler must be able to sample from all nine modes.

\textbf{Funnel} \cite{neal2003funnel}: We use a 10-dimensional funnel distribution with its density given by $\rho(x)=\mathcal{N}(x_1;{0},\sigma^2)\prod_{i=1}^{d}\mathcal{N}(x_i;{0},e^{x_1})$, where $x\in \mathbb{R}^d$. The specific parameters are $d = 10$ and $\sigma = 3$. 
The funnel distribution is known to be a  challenging distribution for testing MCMC methods, as the variance grows exponentially as the first dimension $x_1$ increases.

\textbf{DW-4} \cite{kohler2020equivariantflowsexactlikelihood}: This system consists of 4 particles in 2-dimensions. Each pair of particles interacts via a \textit{ double well} potential \( u^{\text{DW}}(x) = \frac{1}{2\tau} \sum_{i,j} \left[ a \left( d_{ij} - d_0 \right) + b \left( d_{ij} - d_0 \right)^2 + c \left( d_{ij} - d_0 \right)^4 \right] \), where \(d_{ij}\) represents the pairwise distance between two particles \(i\) and \(j\). These pairwise interactions produce multiple metastable states in the system. All parameters \( a, b, c, d_0\) and \( \tau \) are set to match those used in the experiment from DiKL\cite{he2024training}.

\textbf{LJ-13} \cite{kohler2020equivariantflowsexactlikelihood}: We employ a system of 13 particles in 3-dimensions with the \textit{Lennard-Jones}
(LJ) potential \( u^{\text{LJ}}(x) = \frac{\epsilon}{2\tau} \left[ \sum_{i,j} \left( \left( \frac{r_m}{d_{ij}} \right)^{12} - 2 \left( \frac{r_m}{d_{ij}} \right)^6 \right) \right] \), where \(d_{ij}\) represents the pairwise distance between two particles \(i\) and \(j\). In this system, the energy landscape is highly complex, making it difficult to find the energy minima. Therefore, this system is considered good benchmark for evaluating structure generation methods. The parameters \(\epsilon, r_m\) and \(\tau\) are chosen to be the same as those used in the experiment from DiKL\cite{he2024training}.


\subsection{Performance Metrics}
\label{appendix:metrics}

\textbf{Total Variation Distance - Energy, (Interatomic) Distance} (TVD-E, D): The general Total Variation Distance (TVD) measures the difference between two distributions as \( \text{TVD}(P, Q) = \frac{1}{2} \int_{\mathcal{X}} | P(x) - Q(x) | \,dx \). We compute a discrete approximation of TVD using histogram-based probability distributions, following DiKL\cite{he2024training}. TVD-Energy (TVD-E) measures the difference between the energy distributions of the generated samples and the validation set, while TVD-Distance (TVD-D) quantifies the difference in interatomic distance distributions between particles.

\textbf{Error for Estimating the Average
Standard Deviation Across the Marginals} 
 ($\Delta$std): We report the error for estimating the average
standard deviation across the marginal, following the method proposed in \cite{richter2024improved}. Given a learned distribution of the sampler  $p_{sampler}$ and a target distribution $p_{target}$ with $d$-dimensional data, we compute the metric $\Delta\text{std} = \frac{1}{d}\sum_{k=1}^{d}\left |\sqrt{\mathbb{V}[P_k]}-\sqrt{\mathbb{V}[G_k]}\right |,  \text{where  }P\sim p_{sampler} \text{ and }G\sim p_{target}$.

\textbf{Wasserstein-2 Distance ($\mathcal{W}^2$)}: The standard 2-Wasserstein distance between two distributions is defined as \( \mathcal{W}_2(P, Q) = \left( \inf_{\pi} \int \pi(x, y) d(x, y)^2 \,dx \,dy \right)^{\frac{1}{2}} \),
where \(\pi \) is the transport plan with marginals constrained to \(P\) and \(Q\) respectively \cite{akhound-sadegh2024iterated}. We use the Euclidean distance \(||x-y||_2\) for \(d(x,y)\) and calculate the 2-Wasserstein distance between generated samples from the sampler and the ground truth data.

\textbf{Sinkhorn Distance ($\mathcal{W}^2_{\gamma}$}): The Sinkhorn distance is a metric which approximates the optimal transport distance. It adds entropic regularization to the Wasserstein distance in solving optimal transport problem, which is aimed to penalize highly deterministic transport plans \cite{cuturi2013sinkhorn}. It is defined as $ \mathcal{W}^2_{\gamma}(P, Q) = \left( \inf_{\pi} \int \pi(x, y) d(x, y)^2 \,dx \,dy +\gamma H(\pi) \right)^{\frac{1}{2}}$ where \(\pi \) is the transport plan with marginals constrained to \(P\) and \(Q\), $\gamma$ is a parameter to balance between the Wasserstein term and the entropy term, and $H(\pi)$ is the entropy term of the $\pi$, given by $H(\pi)=-\int \pi(x, y) \log(\pi(x, y)) \,dx \,dy$. We utilize the Sinkhorn distance implementation in \url{https://github.com/fwilliams/scalable-pytorch-sinkhorn} for efficient computation.


\subsection{Experimental Details}
\label{appendix:exp_setup} 

\subsubsection{Sampling from Synthetic Distribution Experiments}
For the synthetic distribution experiments, we conducted 5 independent runs with different seeds for each experiment. All reported metrics are the averages of the 5 runs, along with their standard deviation. Every experiment is performed on a single NVIDIA RTX 3090 (24GB) GPU.

\textbf{GMM}: VGS uses a MLP architecture with sinusoidal time step embeddings with ReLU activation function. The MLP is composed of 3 layers with the hidden dimension of size 1024 and the time step embedding dimension of size 1024. We leverage the ReLU activation function to make the first-order Taylor expansion in Eq. (\ref{eq:first_order}) a more accurate approximation. The second derivative of the ReLU activation function is zero, which we expect will minimize the effect of second-order differential terms. 

For $T=20$, we use a quadratic scheduler for $s_t^2$ starting from $2e-1$ to $1e-1$. The value function is trained with $\lambda = 0.95$, $n_{\text{update}} = 1$, and $\eta = 1.2$. The learning rates for the value function and $\sigma_{init}$ are set to $1e-4$ and $1e-3$, respectively.

 The PIS, DIS and DDS is trained using the default setting used in \cite{berner2022optimal}. We use the code provided at \url{https://github.com/juliusberner/sde_sampler}. The Fourier MLP architecture \cite{zhang2022path} is used, which is fundamentally a MLP architecture with sinusoidal time step embeddings with GeLU activation function. The network is composed of 4 layers with the hidden dimension of size 64 and the time step embedding dimension of size 64. Our VGS algorithm uses KL-divergence based loss. Although both KL-divergence loss and log-variance loss \cite{richter2024improved} can be used to train PIS, DIS, and DDS, we chose to use KL-divergence loss to ensure compatibility with the results.

 To compute the metrics, we use $10e5$ randomly sampled data from the target GMM distribution and another $10e5$ randomly sampled data from a learned sampler.

\textbf{Funnel}: Similar to the training of GMM, VGS uses a MLP architecture with sinusoidal time step embeddings with ReLU activation function. The MLP is composed of 4 layers with the hidden dimension of size 1024 and the time step embedding dimension of size 1024. 

For $T=30$, we use a quadratic scheduler for $s_t^2$, starting from $5e-1$ to $5e-4$. The value function is trained with $\lambda = 0$, $n_{\text{update}} = 3$, and $\eta = 1.1$. The learning rates for the value function and $\sigma_{init}$ are set to $1e-5$ and $1e-3$, respectively.

The energy of the Funnel distribution diverges as the sampled value of the first dimension, $x_1$ increases. Extremely high energy values destabilize the training of the value function. To stabilize training, we clip the maximum energy value at 100.

The PIS, DIS and DDS is trained using the default setting used in \cite{berner2022optimal} with modification to its training time steps from 60,000 to 10,000 steps as the performance degrades with more steps. The network is composed of 4 layers with the hidden dimension of size 64 and the time step embeddings dimension of size 64.

 To compute the metrics, we use $10e3$ randomly sampled data from the target GMM distribution and another $10e3$ randomly sampled data from a learned sampler.

 \textbf{Effect of the Number of Time Steps}: We use the same MLP architecture for VGS with GMM experiment. Noise level of VGS is leveraged by $s_{t}^{2}$ which we use a quadratic scheduler starting from $5e-2$ to $5e-4$ invariant of time steps $T$ for low noise level. At each increase of noise level, $s_{t}^{2}$ is increased by doubling the starting point and end point of the $s_{t}^{2}$ scheduler. The value function is trained with $\lambda=1-0.005T$, $n_{update}=3$, and $\eta=1.2$ invariant of noise level. The learning rates for the value function and $\sigma_{init}$ are set to $1e-4$ and $1e-2$.

\subsubsection{Sampling from $n$-body System Experiments}\label{appendix:particle_detail}

For the $n$-body system experiments, we follow the evaluation process used in \cite{he2024training}. We use the test set and the evaluation code provided at \url{https://github.com/jiajunhe98/DiKL} for metric calculation. As in \cite{he2024training}, we incorporate early stopping during training, using 2000 validation samples, and save the checkpoint with the lowest TVD-E. For the validation set, we use the validation data from \cite{akhound-sadegh2024iterated}. Since the original validation dataset consists of 10k samples, we randomly select 2000 samples.  

For testing, the metrics are computed using 2000 samples from our model and the test dataset. We repeat this process 10 times to report the mean and standard deviation. We omit the diffusion step for the final samples and set $\sigma_{T-1} = 0$ during the evaluation.  

\textbf{DW-4 (T=30)}: For the value function, we use an MLP architecture with sinusoidal time embeddings. The state and time embedding vectors are mapped to a 512-dimensional vector using an MLP, and the concatenated vectors are then passed through a two-layer MLP with a hidden dimension of 512.  

For $s_t^2$, we use a quadratic scheduler starting from $1e-1$ to $1e-4$. We employ the Huber loss for TD updates, as it is known to be less sensitive to outliers. The value function is trained with batch size 512, $\lambda = 0$, $n_{\text{update}} = 3$, and $\eta = 1.2$. The learning rates for the value function and $\sigma_{init}$ are set to $1e-5$ and $1e-3$, respectively.  

\textbf{LJ-13 (T=100)}: For the value function, we use an MLP architecture with sinusoidal time embeddings. The state and time embedding vectors are mapped to a 512-dimensional vector using an MLP, and the concatenated vectors are then passed through a three-layer MLP with a hidden dimension of 1024. The final layer input is element-wise multiplied by a time feature vector, which is extracted from a separate network. Motivated by the form of LJ potential, we use the inverse of the pairwise distance as the network input instead of the distance itself. To prevent divergence at $d=0$, a small constant is added.

For $s_t^2$, we use a exponential scheduler starting from $0.05$ to $1e-4$. We employ the Huber loss for TD updates, as it is known to be less sensitive to outliers. The value function is trained with batch size $512$, $\lambda = 0.9$, $n_{\text{update}} = 3$, and $\eta = 1.2$.  

Since the potential of LJ-13 diverges dramatically when two particles are close, we clip the maximum energy value at 100. We also clip the gradient norm during parameter updates at 1.0 for robustness. The learning rates for the value function and $\sigma_{init}$ are set to $1e-5$ and $1e-4$, respectively. We observed that the LJ-13 potential is highly sensitive to noise and thus reduced the noise scale to $\eta = 0.9$ during evaluation. When evaluated with $\eta = 1.0$, the metrics reported in \cref{tab:particle_tab} change to $\text{TVD-E} = 0.294$, $\text{TVD-D} = 0.036$, $\mathcal{W}^{2} = 4.283$.


\subsection{Training and Sampling Time}\label{appendix:train_sample_time}
\cref{tab:train_sample_time} shows the training and sampling time of VGS and SDE-based samplers. Both VGS models show faster training time compared to SDE-based samplers. However VGS (1024) exhibits slower sampling time than SDE-based samplers due to the high computational cost associated with its large number of parameters. In order to make a fair comparison we conducted the same experiment on VGS (128) which has a comparable number of parameters with SDE-based sampler models. VGS (128) showed 4$\times$ faster sampling time compared to SDE-based samplers while maintaining high sampling performance with $\mathcal{W}_{\gamma}^{2}=0.021$, TVD-E $=0.049$, $\Delta$std $= 0.107$ for GMM and  $\mathcal{W}_{\gamma}^{2}=7.457$, TVD-E $=0.628$, $\Delta$std $= 7.658$ for Funnel.
\begin{table}[t!]
    \centering
    \caption{Training and Sampling time of VGS and baseline models. VGS (128) and VGS (1024) each represents VGS using a value function network with hidden layer dimension of 128 and 1024. VGS (1024) results were used for \cref{tab:sampler-performance}. Sampling time was measured for sampling $10^{5}$ samples.}
    \setlength{\tabcolsep}{2pt}
    %\resizebox{0.8\textwidth}{!}
    {\begin{tabular}{llccccc|ccccc}
    \toprule
     Target & & \multicolumn{5}{c}{\textbf{Training}} & \multicolumn{5}{c}{\textbf{Sampling}} \\
     & & PIS & DIS & DDS & VGS (128) & VGS (1024) & PIS & DIS & DDS & VGS(128) &  VGS(1024)\\
     \midrule
     \multirow{2}{*}{\textbf{GMM}}&Parameters & 38K& 38K& 38K& 99K & 6300K & -& -& -& -& -\\
     \cmidrule{2-12}
    & Time & 10h & 10h & 12h &  0.40h & \textbf{0.36h} & 0.92s & 0.94s & 1.20s & \textbf{0.09s} & 1.90s \\
     \midrule
    \multirow{2}{*}{\textbf{Funnel}}&Parameters & 39K & 39K & 39K & 100K & 6300K & -& -& -& -& -\\
    \cmidrule{2-12}
     & Time& 10h & 10h & 12h & \textbf{0.75h} & 1.5h & 8.1s &  8.3s & 10.2s & \textbf{2.3s} & 16.4s\\
    \bottomrule
    % DiKL and DEM results saved in case 1.5h & 5.0h, 4.07s & 0.01s
    
    \end{tabular}}
    
    \label{tab:train_sample_time}
\end{table}


\subsection{Anomaly Detection}

% \subsubsection{8 Gaussians} \label{app:8gaussians}

\subsubsection{MVTec-AD}  \label{app:mvtec}

We utilize separate networks for the energy function and the value function, prioritizing the accurate estimation of the energy function. To achieve this, we adopt an autoencoder-based architecture, where the reconstruction error of a sample serves as its energy \cite{yoon2021autoencoding}. The diffusion model and the value function are implemented as five-step MLPs with time step embeddings. VGS provides flexibility in selecting $\pi(\bx_0)$. We initialize the samplerâ€™s distribution by applying noise to the data distribution, enhancing the precision of energy estimation near the data distribution.

\begin{table}[h]
    \setlength\tabcolsep{2pt}
    \centering
    \caption{Total 10 experiments, all models trained for 100 epochs before evaluation. Baselines results are adopted from DxMI \cite{yoon2024maximum} paper. The largest value in a task is marked as boldface.}
    \label{tab:mvtec-uniad-full}
    \begin{footnotesize}
    \begin{tabular}{c|ccccc|ccccc}
    \toprule
     & \multicolumn{5}{c}{Detection}  & \multicolumn{5}{c}{Localization} \\
                  & Ours & DxMI & {UniAD} & {MPDR} & {DRAEM} & Ours & DxMI & {UniAD} & {MPDR} & {DRAEM} \\
    \midrule
    Bottle      & \textbf{100} \scriptsize{$\pm$0.00} & \textbf{100.0} \scriptsize{$\pm$0.00}& 99.7\scriptsize{$\pm$0.04} & \textbf{100.0} & 97.5
                & \textbf{98.5} \scriptsize{$\pm$0.02}& \textbf{98.5}\scriptsize{$\pm$0.03} & 98.1\scriptsize{$\pm$0.04} & \textbf{98.5} & 87.6 \\
    Cable       & \textbf{97.2} \scriptsize{$\pm$0.40} & 97.1\scriptsize{$\pm$ 0.37} & 95.2\scriptsize{$\pm$0.84}  & 95.5 & 57.8 
                & 96.7 \scriptsize{$\pm$0.06} & 96.6\scriptsize{$\pm$0.10} & \textbf{97.3}\scriptsize{$\pm$0.10} & 95.6 & 71.3 \\
    Capsule     & \textbf{90.8} \scriptsize{$\pm$0.37} & 89.8\scriptsize{$\pm$ 0.61} & 86.9\scriptsize{$\pm$0.73} & 86.4 & 65.3 
                & \textbf{98.5} \scriptsize{$\pm$0.01}& \textbf{98.5}\scriptsize{$\pm$0.03} & \textbf{98.5}\scriptsize{$\pm$0.01} & 98.2 & 50.5 \\
    Hazelnut    & \textbf{100} \scriptsize{$\pm$0.02} & \textbf{100.0}\scriptsize{$\pm$ 0.04} & 99.8\scriptsize{$\pm$0.10}  & 99.9 & 93.7 
                & 98.3 \scriptsize{$\pm$0.02} & \textbf{98.4}\scriptsize{$\pm$0.04} & 98.1\scriptsize{$\pm$0.10} & \textbf{98.4} & 96.9 \\
    Metal Nut   & \textbf{99.9} \scriptsize{$\pm$0.06} & \textbf{99.9}\scriptsize{$\pm$ 0.11} & 99.2\scriptsize{$\pm$0.09} & \textbf{99.9} & 72.8 
                & \textbf{95.6} \scriptsize{$\pm$0.04}& 95.5\scriptsize{$\pm$0.03} & 94.8\scriptsize{$\pm$0.09} & 94.5 & 62.2 \\
    Pill        & \textbf{95.7} \scriptsize{$\pm$0.37} & 95.4\scriptsize{$\pm$ 0.66} & 93.7\scriptsize{$\pm$0.65}  & 94.0 & 82.2 
                & \textbf{95.7} \scriptsize{$\pm$0.03} & 95.6\scriptsize{$\pm$0.07} & 95.0\scriptsize{$\pm$0.16} & 94.9 & 94.4 \\
    Screw       & 89.7 \scriptsize{$\pm$0.79} & 88.9\scriptsize{$\pm$ 0.51} & 87.5\scriptsize{$\pm$0.57}  & 85.9 & \textbf{92.0} 
                & \textbf{98.6} \scriptsize{$\pm$0.04} & \textbf{98.6}\scriptsize{$\pm$0.08} & 98.3\scriptsize{$\pm$0.08} & 98.1 & 95.5 \\
    Toothbrush  & 89.5 \scriptsize{$\pm$1.47} & 92.2\scriptsize{$\pm$1.46} & \textbf{94.2}\scriptsize{$\pm$0.20} & 89.6 & 90.6 
                & \textbf{98.8} \scriptsize{$\pm$0.02} & \textbf{98.8}\scriptsize{$\pm$0.04} & 98.4\scriptsize{$\pm$0.03} & 98.7 & 97.7 \\
    Transistor  & 99.1 \scriptsize{$\pm$0.37} & 99.2\scriptsize{$\pm$0.28} & \textbf{99.8}\scriptsize{$\pm$0.09}  & 98.3 & 74.8 
                & 96.2 \scriptsize{$\pm$0.04}& 96.0\scriptsize{$\pm$0.13} & \textbf{97.9}\scriptsize{$\pm$0.19} & 95.4 & 65.5 \\
    Zipper      & 96.3 \scriptsize{$\pm$0.43} & 96.3\scriptsize{$\pm$0.50} & 95.8\scriptsize{$\pm$0.51} & 95.3 & \textbf{98.8} 
                & 96.8 \scriptsize{$\pm$0.12} & 96.7\scriptsize{$\pm$0.08} & 96.8\scriptsize{$\pm$0.24} & 96.2 & \textbf{98.3} \\
    Carpet      & 99.8 \scriptsize{$\pm$0.07} & \textbf{99.9}\scriptsize{$\pm$0.04} & 99.8\scriptsize{$\pm$0.02} & \textbf{99.9} & 98.0 
                & 98.7 \scriptsize{$\pm$0.05}& \textbf{98.8}\scriptsize{$\pm$0.02} & 98.5\scriptsize{$\pm$0.01} & \textbf{98.8} & 98.6 \\
    Grid        & 98.5 \scriptsize{$\pm$0.30} & 98.6\scriptsize{$\pm$0.28} & 98.2\scriptsize{$\pm$0.26}  & 97.9 & \textbf{99.3} 
                & 97.1 \scriptsize{$\pm$0.05} & 97.0\scriptsize{$\pm$0.07} & 96.5\scriptsize{$\pm$0.04} & 96.9 & \textbf{98.7} \\
    Leather     & \textbf{100.0} \scriptsize{$\pm$0.00} & \textbf{100.0}\scriptsize{$\pm$0.00} & \textbf{100.0}\scriptsize{$\pm$0.00} & \textbf{100.0}  & 98.7 
                & 98.5 \scriptsize{$\pm$0.07} & 98.5\scriptsize{$\pm$0.03} & \textbf{98.8}\scriptsize{$\pm$0.03} & 98.5 & 97.3 \\
    Tile        & \textbf{100.0} \scriptsize{$\pm$0.00} & \textbf{100.0}\scriptsize{$\pm$0.00} & 99.3\scriptsize{$\pm$0.14} & \textbf{100.0} & 95.2 
                & 95.3 \scriptsize{$\pm$0.09} & 95.2 \scriptsize{$\pm$0.14}& 91.8\scriptsize{$\pm$0.10} & 94.6 & \textbf{98.0} \\
    Wood        & 98.4 \scriptsize{$\pm$0.29} & 98.3\scriptsize{$\pm$0.33} & 98.6\scriptsize{$\pm$0.08} & 97.9 & \textbf{99.8} 
                & 93.9 \scriptsize{$\pm$0.08} & 93.8\scriptsize{$\pm$0.07} & 93.2\scriptsize{$\pm$0.08} & 93.8 & \textbf{96.0} \\
    \midrule
    Mean        & \textbf{97.0} \scriptsize{$\pm$0.09} & \textbf{97.0}\scriptsize{$\pm$0.11} & 96.5\scriptsize{$\pm$0.08} & 96.0 & 88.1 
                & \textbf{97.1} \scriptsize{$\pm$0.01} & \textbf{97.1}\scriptsize{$\pm$0.02} & 96.8\scriptsize{$\pm$0.02} & 96.7 & 87.2 \\
    \bottomrule
    \end{tabular}
    \end{footnotesize}

\end{table}


% \textbf{ViSA}
% \begin{table*}[h!]
%     \centering
%     \caption{Anomaly detection results with AUROC metric on VisA.}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{llccccc}
%         \toprule
%         \textbf{Category} & \textbf{Object} & \textbf{UniAD}[\cite{you2022unified}] & \textbf{OmniAL} & \textbf{DiAD} & \textbf{HVQ-Trans}  & \textbf{Ours} \\ 
%         \midrule
%         \multirow{4}{*}{\textbf{Complex structure}} 
%         & PCB1     & 94.2 / 99.4 & 77.7 / 97.6 & 88.1 / 98.7 & 96.7 / 99.4 &  \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         & PCB2      & 93.3 / 97.8 & 81.0 / 93.9 & 91.4 / 95.2 & 93.4 / 98.0 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         & PCB3    & 87.2 / 98.2 & 88.1 / 94.7 & 86.2 / 96.7 & 92.0 / 98.3 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         & PCB4   & 99.2 / 97.8 & 95.3 / 97.1 & 99.6 / 97.0 & 99.5 / 97.7 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         \midrule
%         \multirow{4}{*}{\textbf{Multiple instances}} 
%         & Macaroni1     &  91.6 / 99.2 & 92.6 / 98.6 & 85.7 / 94.1 & 93.1 / 99.4 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         & Macaroni2       & 83.9 / 97.9 & 75.2 / 97.9 & 62.5 / 93.6 & 86.2 / 98.5 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         & Capsules    & 73.1 / 98.1 & 90.6 / 99.4 & 58.2 / 97.3 & 77.1 / 99.0 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         & Candle    & 96.9 / 99.1 & 86.8 / 95.8 & 92.8 / 97.3 & 96.8 / 99.2 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         \midrule
%         \multirow{4}{*}{\textbf{Single instance}} 
%         & Cashew     & 93.2 / 98.6 & 88.6 / 95.0 & 91.5 / 90.9 & 94.9 / 99.2 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         & Chewing gum & 99.0 / 99.1 & 96.4 / 99.0 & 99.1 / 94.7 & 99.4 / 98.8 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         & Fryum    & 89.3 / 97.5 & 94.6 / 92.1 & 89.8 / 97.6 & 90.4 / 97.7 &  \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         & Pipe fryum    & 97.3 / 99.1 & 86.1 / 98.2 & 96.2 / 99.4 & 98.5 / 99.4 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         \midrule
%         \textbf{Mean} &  & 91.5 / 98.5 & 87.8 / 96.6 & 86.8  / 96.0 & 93.2 / 98.7 & \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         \bottomrule
%     \end{tabular}
%     }
% \end{table*}

% \begin{table*}[h!]
%     \setlength\tabcolsep{2pt}
%     \centering
%     \caption{Anomaly detection results with AUROC metric on VisA-Similar style to table 1}
%     \label{tab:visa-uniad-full}
%     \begin{footnotesize}
%     \begin{tabular}{c|cccccc|cccccc}
%     \toprule
%      & \multicolumn{6}{c}{Detection}  & \multicolumn{6}{c}{Localization} \\
%                   & Ours & DxMI & {UniAD} & {OmniAL} & {DiAD} & HVQ-Trans & Ours & DxMI & {UniAD} & {OmniAL} & {DiAD} & {HVQ-Trans} \\
%     \midrule
%     PCB1      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 94.2 & 77.7 & 88.1 & 96.7
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 99.4 & 97.6 & 98.7 & 99.4 \\
%     PCB2      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 93.3 & 81.0 & 91.4 & 93.4
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 97.8 & 93.9 & 95.2 & 98.0 \\
%     PCB3      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 87.2 & 88.1 & 86.2 & 92.0
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 98.2 & 94.7 & 96.7 & 98.3 \\
%     PCB4      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 99.2 & 95.3 &99.6 & 99.5
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 97.8 & 97.1 & 97.0 & 97.7 \\
%     Macaroni1      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 91.6 & 92.6 & 85.7 & 93.1
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 99.2 & 98.6 & 94.1 & 99.4 \\
%     Macaroni2      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 83.9 & 75.2 & 62.5 & 86.2
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 97.9 & 97.9 & 93.6 & 98.5 \\
%     Capsules      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 73.1 & 90.6 & 58.2 & 77.1
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 98.1 & 99.4 & 97.3 & 99.0 \\
%     Candle      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 96.9 & 86.8 & 92.8 & 96.8
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 99.1 & 95.8 & 97.3 & 99.2 \\
%     Cashew      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 93.2 & 88.6 & 91.5 & 94.9
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 98.6 & 95.0 & 90.9 & 99.2 \\
%     Chewing gum      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 99.0 & 96.4 & 99.1 & 99.4
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 99.1 & 99.0 & 94.7 & 98.8 \\
%     Fryum      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 89.3 & 94.6 & 89.8 & 90.4
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 97.5 & 92.1 & 97.6 & 97.7 \\
%     Pipe fryum      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 97.3 & 86.1 & 96.2 & 98.5
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 99.1 & 98.2 & 99.4 & 99.4 \\
%     \midrule
%     Mean      & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 91.5 & 87.8 & 86.8 & 93.2
%                 & \textbf{xx} \scriptsize{$\pm$x.xx} & \textbf{xx.x} & 98.5 & 96.6 & 96.0 & 98.7 \\
%     \bottomrule
%     \end{tabular}
%     \end{footnotesize}

% \end{table*}
% \textbf{BTAD}
% \begin{table*}[h!]
%     \setlength\tabcolsep{2pt}
%     \centering
%     \caption{Anomaly detection results with AUROC metric on BTAD.}
%     \begin{tabular}{lcccccccc}
%         \toprule
%         \textbf{Object} & \textbf{VT-ADL} & \textbf{PatchSVDD} & \textbf{MultiPatch} & \textbf{PaDiM} & \textbf{CFlow-AD} & \textbf{DRAEM} & \textbf{AD-CLSCNFs} & \textbf{Ours} \\ 
%         \midrule
%         Product 1     & 97.6.x / 76.3 & 96.3 / 94.9 & 100.0 / 97.3 & xx.x / xx.x & xx.x / xx.x & xx.x / xx.x & xx.x / xx.x& \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         Product 2      & 71.0 / 88.9 & 70.3 / 92.7 & 85.3 / 96.8 & xx.x / xx.x & xx.x / xx.x & xx.x / xx.x & xx.x / xx.x& \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         Product 3    & 82.6 / 80.3 & 91.1 / 91.7 & 100.0 / 99.0 & xx.x / xx.x & xx.x / xx.x & xx.x / xx.x & xx.x / xx.x& \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         \midrule
%         \textbf{Mean} & 83.7 / 81.8 & 85.9 / 93.1 & 95.1 / 97.7 & 93.0 / 93.9 & 93.0 / 93.9 & 95.4 / 97.0 & 96.0 / 97.1& \textbf{xx.x} $\pm$ xx.x / xx.x \\ 
%         \bottomrule
%     \end{tabular}
% \end{table*}

% \begin{table*}[h!]
%     \setlength\tabcolsep{2pt}
%     \centering
%     \caption{Anomaly detection results with AUROC metric on BTAD. Similar style to Table 6.}
%     \label{tab:btad-uniad-full}
%     \begin{footnotesize}
%     \begin{tabular}{c|cccccccc|cccccccc}
%     \toprule
%      & \multicolumn{8}{c}{Detection}  & \multicolumn{8}{c}{Localization} \\
%                   & Ours & VT-ADL & PatchSVDD & MultiPatch & PaDiM & CFlow-AD & DRAEM & AD-CLSCNFs
%                   & Ours & VT-ADL & PatchSVDD & MultiPatch & PaDiM & CFlow-AD & DRAEM & AD-CLSCNFs \\
%     \midrule
%     Product 1      & \textbf{xx} \scriptsize{$\pm$x.xx} & 97.6.x & 96.3 & 100.0 & xx.x & xx.x & xx.x & xx.x
%                    & \textbf{xx} \scriptsize{$\pm$x.xx} & 76.3   & 94.9 & 97.3   & xx.x & xx.x & xx.x & xx.x \\
%     Product 2      & \textbf{xx} \scriptsize{$\pm$x.xx} & 71.0   & 70.3 & 85.3   & xx.x & xx.x & xx.x & xx.x
%                    & \textbf{xx} \scriptsize{$\pm$x.xx} & 88.9   & 92.7 & 96.8   & xx.x & xx.x & xx.x & xx.x \\
%     Product 3      & \textbf{xx} \scriptsize{$\pm$x.xx} & 82.6   & 91.1 & 100.0  & xx.x & xx.x & xx.x & xx.x
%                    & \textbf{xx} \scriptsize{$\pm$x.xx} & 80.3   & 91.7 & 99.0   & xx.x & xx.x & xx.x & xx.x \\
%     \midrule
%     Mean           & \textbf{xx} \scriptsize{$\pm$x.xx} & 83.7   & 85.9 & 95.1   & 93.0 & 93.0 & 95.4 & 96.0
%                    & \textbf{xx} \scriptsize{$\pm$x.xx} & 81.8   & 93.1 & 97.7   & 93.9 & 93.9 & 97.0 & 97.1 \\
%     \bottomrule
%     \end{tabular}
%     \end{footnotesize}
% \end{table*}

% \begin{table*}[h!]
%     \setlength\tabcolsep{2pt}
%     \centering
%     \caption{Anomaly detection results with AUROC metric on BTAD. Similar style to Table 6.}
%     \resizebox{\textwidth}{!}{
%     \begin{footnotesize} 
%     \begin{tabular}{c|cccccccc|cccccccc}
%     \toprule
%      & \multicolumn{8}{c}{\scriptsize Detection}  & \multicolumn{8}{c}{\scriptsize Localization} \\ 
%                   & {\scriptsize Ours} & {\scriptsize VT-ADL} & {\scriptsize PatchSVDD} & {\scriptsize MultiPatch} & {\scriptsize PaDiM} & {\scriptsize CFlow-AD} & {\scriptsize DRAEM} & {\scriptsize AD-CLSCNFs} 
%                   & {\scriptsize Ours} & {\scriptsize VT-ADL} & {\scriptsize PatchSVDD} & {\scriptsize MultiPatch} & {\scriptsize PaDiM} & {\scriptsize CFlow-AD} & {\scriptsize DRAEM} & {\scriptsize AD-CLSCNFs} \\
%     \midrule
%     Product 1      & \textbf{xx} \scriptsize{$\pm$x.xx} & 97.6.x & 96.3 & 100.0 & xx.x & xx.x & xx.x & xx.x
%                    & \textbf{xx} \scriptsize{$\pm$x.xx} & 76.3   & 94.9 & 97.3   & xx.x & xx.x & xx.x & xx.x \\
%     Product 2      & \textbf{xx} \scriptsize{$\pm$x.xx} & 71.0   & 70.3 & 85.3   & xx.x & xx.x & xx.x & xx.x
%                    & \textbf{xx} \scriptsize{$\pm$x.xx} & 88.9   & 92.7 & 96.8   & xx.x & xx.x & xx.x & xx.x \\
%     Product 3      & \textbf{xx} \scriptsize{$\pm$x.xx} & 82.6   & 91.1 & 100.0  & xx.x & xx.x & xx.x & xx.x
%                    & \textbf{xx} \scriptsize{$\pm$x.xx} & 80.3   & 91.7 & 99.0   & xx.x & xx.x & xx.x & xx.x \\
%     \midrule
%     Mean           & \textbf{xx} \scriptsize{$\pm$x.xx} & 83.7   & 85.9 & 95.1   & 93.0 & 93.0 & 95.4 & 96.0
%                    & \textbf{xx} \scriptsize{$\pm$x.xx} & 81.8   & 93.1 & 97.7   & 93.9 & 93.9 & 97.0 & 97.1 \\
%     \bottomrule
%     \end{tabular}
%     \end{footnotesize}
%     }
% \end{table*}