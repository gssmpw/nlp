@inproceedings{10.1609/aaai.v38i16.29710,
author = {Zhang, Ceyao and Yang, Kaijie and Hu, Siyi and Wang, Zihao and Li, Guanghe and Sun, Yihang and Zhang, Cheng and Zhang, Zhaowei and Liu, Anji and Zhu, Song-Chun and Chang, Xiaojun and Zhang, Junge and Yin, Feng and Liang, Yitao and Yang, Yaodong},
title = {ProAgent: building proactive cooperative agents with large language models},
year = {2025},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i16.29710},
doi = {10.1609/aaai.v38i16.29710},
abstract = {Building agents with adaptive behavior in cooperative tasks stands as a paramount goal in the realm of multi-agent systems. Current approaches to developing cooperative agents rely primarily on learning-based methods, whose policy generalization depends heavily on the diversity of teammates they interact with during the training phase. Such reliance, however, constrains the agents' capacity for strategic adaptation when cooperating with unfamiliar teammates, which becomes a significant challenge in zero-shot coordination scenarios. To address this challenge, we propose ProAgent, a novel framework that harnesses large language models (LLMs) to create proactive agents capable of dynamically adapting their behavior to enhance cooperation with teammates. ProAgent can analyze the present state, and infer the intentions of teammates from observations. It then updates its beliefs in alignment with the teammates' subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of modularity and interpretability, making it easily integrated into various of coordination scenarios. Experimental evaluations conducted within the Overcooked-AI environment unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training when cooperating with AI agents. Furthermore, in partnered with human proxy models, its performance exhibits an average improvement exceeding 10\% compared to the current state-of-the-art method. For more information about our project, please visit https://pku-proagent.github.io.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1962},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@inproceedings{10.5555/2283396.2283451,
author = {Pajarinen, Joni and Peltonen, Jaakko},
title = {Efficient planning for factored infinite-horizon DEC-POMDPs},
year = {2011},
isbn = {9781577355137},
publisher = {AAAI Press},
abstract = {Decentralized partially observable Markov decision processes (DEC-POMDPs) are used to plan policies for multiple agents that must maximize a joint reward function but do not communicate with each other. The agents act under uncertainty about each other and the environment. This planning task arises in optimization of wireless networks, and other scenarios where communication between agents is restricted by costs or physical limits. DEC-POMDPs are a promising solution, but optimizing policies quickly becomes computationally intractable when problem size grows. Factored DEC-POMDPs allow large problems to be described in compact form, but have the same worst case complexity as non-factored DEC-POMDPs. We propose an efficient optimization algorithm for large factored infinite-horizon DEC-POMDPs. We formulate expectation-maximization based optimization into a new form, where complexity can be kept tractable by factored approximations. Our method performs well, and it can solve problems with more agents and larger state spaces than state of the art DEC-POMDP methods. We give results for factored infinite-horizon DEC-POMDP problems with up to 10 agents.},
booktitle = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume One},
pages = {325–331},
numpages = {7},
location = {Barcelona, Catalonia, Spain},
series = {IJCAI'11}
}

@inproceedings{10.5555/3306127.3332052,
author = {Samvelyan, Mikayel and Rashid, Tabish and Schroeder de Witt, Christian and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
title = {The StarCraft Multi-Agent Challenge},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ_obZ0.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2186–2188},
numpages = {3},
keywords = {multi-agent learning, reinforcement learning, starcraft},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inbook{10.5555/3454287.3454971,
author = {Mahajan, Anuj and Rashid, Tabish and Samvelyan, Mikayel and Whiteson, Shimon},
title = {MAVEN: multi-agent variational exploration},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Centralised training with decentralised execution is an important setting for cooperative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We specifically focus on QMIX [40], the current state-of-the-art in this domain. We show that the representational constraints on the joint action-values introduced by QMIX and similar methods lead to provably poor exploration and suboptimality. Furthermore, we propose a novel approach called MAVEN that hybridises value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents condition their behaviour on the shared latent variable controlled by a hierarchical policy. This allows MAVEN to achieve committed, temporally extended exploration, which is key to solving complex multi-agent tasks. Our experimental results show that MAVEN achieves significant performance improvements on the challenging SMAC domain [43].},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {684},
numpages = {12}
}

@inproceedings{10.5555/3545946.3598961,
author = {Formanek, Claude and Jeewa, Asad and Shock, Jonathan and Pretorius, Arnu},
title = {Off-the-Grid MARL: Datasets and Baselines for Offline Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Being able to harness the power of large, static datasets for developing autonomous multi-agent systems could unlock enormous value for real-world applications. Many important industrial systems are multi-agent in nature and are difficult to model using bespoke simulators. However, in industry, distributed system processes can often be recorded during operation, and large quantities of demonstrative data can be stored. Offline multi-agent reinforcement learning (MARL) provides a promising paradigm for building effective online controllers from static datasets. However, offline MARL is still in its infancy, and, therefore, lacks standardised benchmarks, baselines and evaluation protocols typically found in more mature subfields of RL. This deficiency makes it difficult for the community to sensibly measure progress. In this work, we aim to fill this gap by releasing off-the-grid MARL (OG-MARL): a framework for generating offline MARL datasets and algorithms.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2442–2444},
numpages = {3},
keywords = {multi-agent reinforcement learning, offline reinforcement learning, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3600270.3601883,
author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
title = {Large language models are zero-shot reasoners},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large-scale InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1613},
numpages = {15},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{10.5555/3600270.3602070,
author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
title = {Chain-of-thought prompting elicits reasoning in large language models},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1800},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{10.5555/3666122.3667509,
author = {Zhao, Zirui and Lee, Wee Sun and Hsu, David},
title = {Large language models as commonsense knowledge for large-scale task planning},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Large-scale task planning is a major challenge. Recent work exploits large language models (LLMs) directly as a policy and shows surprisingly interesting results. This paper shows that LLMs provide a commonsense model of the world in addition to a policy that acts on it. The world model and the policy can be combined in a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task planning. In our new LLM-MCTS algorithm, the LLM-induced world model provides a commonsense prior belief for MCTS to achieve effective reasoning; the LLM-induced policy acts as a heuristic to guide the search, vastly improving search efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin for complex, novel tasks. Further experiments and analyses on multiple tasks—multiplication, travel planning, object rearrangement—suggest minimum description length (MDL) as a general guiding principle: if the description length of the world model is substantially smaller than that of the policy, using LLM as a world model for model-based planning is likely better than using LLM solely as a policy. The code and supplementary materials are available at https://llm-mcts.github.io.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1387},
numpages = {21},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{10.5555/3666122.3667756,
author = {Ellis, Benjamin and Cook, Jonathan and Moalla, Skander and Samvelyan, Mikayel and Sun, Mingfei and Mahajan, Anuj and Foerster, Jakob N. and Whiteson, Shimon},
title = {SMACv2: an improved benchmark for cooperative multi-agent reinforcement learning},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for the centralised training with decentralised execution paradigm. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex closed-loop policies (i.e., those that condition on the observation). In particular, we show that an open-loop policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings during evaluation. We show that these changes ensure the benchmark requires the use of closed-loop policies. We also introduce the extended partial observability challenge (EPO), which augments SMACv2 to ensure meaningful partial observability. We evaluate state-of-the-art algorithms on SMACv2 and show that it presents significant challenges not present in the original benchmark. Our analysis illustrates that SMACv2 addresses the discovered deficiencies of SMAC and can help benchmark the next generation of MARL methods. Videos of training are available on our website.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1634},
numpages = {27},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@INPROCEEDINGS{10610855,
  author={Mandi, Zhao and Jain, Shreeya and Song, Shuran},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={RoCo: Dialectic Multi-Robot Collaboration with Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={286-299},
  keywords={Trajectory planning;Robot kinematics;Large language models;Semantics;Collaboration;Benchmark testing;Human in the loop},
  doi={10.1109/ICRA57147.2024.10610855}}

@article{Besta_Blach_Kubicek_Gerstenberger_Podstawski_Gianinazzi_Gajda_Lehmann_Niewiadomski_Nyczyk_Hoefler_2024, title={Graph of Thoughts: Solving Elaborate Problems with Large Language Models}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29720}, DOI={10.1609/aaai.v38i16.29720}, abstractNote={We introduce Graph of Thoughts (GoT): a framework that
advances prompting capabilities in large language models
(LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (&quot;LLM thoughts&quot;) are vertices, and edges correspond
to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts,
or enhancing thoughts using feedback loops. We illustrate
that GoT offers advantages over state of the art on different
tasks, for example increasing the quality of sorting by 62%
over ToT, while simultaneously reducing costs by &gt;31%.
We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting
schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both
of which form complex networks}, number={16}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten}, year={2024}, month={Mar.}, pages={17682-17690} }

@article{JMLR:v21:20-081,
  author  = {Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson},
  title   = {Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {178},
  pages   = {1--51},
  url     = {http://jmlr.org/papers/v21/20-081.html}
}

@article{Li_Liu_Zhang_Wei_Niu_Yang_Liu_Ouyang_2023, title={ACE: Cooperative Multi-Agent Q-learning with Bidirectional Action-Dependency}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26028}, DOI={10.1609/aaai.v37i7.26028}, abstractNote={Multi-agent reinforcement learning (MARL) suffers from the non-stationarity problem, which is the ever-changing targets at every iteration when multiple agents update their policies at the same time. Starting from first principle, in this paper, we manage to solve the non-stationarity problem by proposing bidirectional action-dependent Q-learning (ACE). Central to the development of ACE is the sequential decision making process wherein only one agent is allowed to take action at one time. Within this process, each agent maximizes its value function given the actions taken by the preceding agents at the inference stage. In the learning phase, each agent minimizes the TD error that is dependent on how the subsequent agents have reacted to their chosen action. Given the design of bidirectional dependency, ACE effectively turns a multi-agent MDP into a single-agent MDP. We implement the ACE framework by identifying the proper network representation to formulate the action dependency, so that the sequential decision process is computed implicitly in one forward pass. To validate ACE, we compare it with strong baselines on two MARL benchmarks. Empirical experiments demonstrate that ACE outperforms the state-of-the-art algorithms on Google Research Football and StarCraft Multi-Agent Challenge by a large margin. In particular, on SMAC tasks, ACE achieves 100% success rate on almost all the hard and super hard maps. We further study extensive research problems regarding ACE, including extension, generalization and practicability.}, number={7}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Li, Chuming and Liu, Jie and Zhang, Yinmin and Wei, Yuhong and Niu, Yazhe and Yang, Yaodong and Liu, Yu and Ouyang, Wanli}, year={2023}, month={Jun.}, pages={8536-8544} }

@article{Li_Zhao_Wu_Pajarinen_2024, title={Backpropagation Through Agents}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29277}, DOI={10.1609/aaai.v38i12.29277}, abstractNote={A fundamental challenge in multi-agent reinforcement learning (MARL) is to learn the joint policy in an extremely large search space, which grows exponentially with the number of agents. Moreover, fully decentralized policy factorization significantly restricts the search space, which may lead to sub-optimal policies. In contrast, the auto-regressive joint policy can represent a much richer class of joint policies by factorizing the joint policy into the product of a series of conditional individual policies. While such factorization introduces the action dependency among agents explicitly in sequential execution, it does not take full advantage of the dependency during learning. In particular, the subsequent agents do not give the preceding agents feedback about their decisions. In this paper, we propose a new framework Back-Propagation Through Agents (BPTA) that directly accounts for both agents’ own policy updates and the learning of their dependent counterparts. This is achieved by propagating the feedback through action chains. With the proposed framework, our Bidirectional Proximal Policy Optimisation (BPPO) outperforms the state-of-the-art methods. Extensive experiments on matrix games, StarCraftII v2, Multi-agent MuJoCo, and Google Research Football demonstrate the effectiveness of the proposed method.}, number={12}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Li, Zhiyuan and Zhao, Wenshuai and Wu, Lijun and Pajarinen, Joni}, year={2024}, month={Mar.}, pages={13718-13726} }

@inproceedings{NEURIPS2022_69413f87,
 author = {Wen, Muning and Kuba, Jakub and Lin, Runji and Zhang, Weinan and Wen, Ying and Wang, Jun and Yang, Yaodong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {16509--16521},
 publisher = {Curran Associates, Inc.},
 title = {Multi-Agent Reinforcement Learning is a Sequence Modeling Problem},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/69413f87e5a34897cd010ca698097d0a-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{NEURIPS2022_9c1535a0,
 author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and WU, YI},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {24611--24624},
 publisher = {Curran Associates, Inc.},
 title = {The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9c1535a02f0ce079433344e14d910597-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{NEURIPS2023_1b44b878,
 author = {Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {8634--8652},
 publisher = {Curran Associates, Inc.},
 title = {Reflexion: language agents with verbal reinforcement learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{deng2024newapproachsolvingsmac,
      title={A New Approach to Solving SMAC Task: Generating Decision Tree Code from Large Language Models}, 
      author={Yue Deng and Weiyu Ma and Yuxin Fan and Yin Zhang and Haifeng Zhang and Jian Zhao},
      year={2024},
      eprint={2410.16024},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.16024}, 
}

@inproceedings{gong-etal-2024-mindagent,
    title = "{M}ind{A}gent: Emergent Gaming Interaction",
    author = "Gong, Ran  and
      Huang, Qiuyuan  and
      Ma, Xiaojian  and
      Noda, Yusuke  and
      Durante, Zane  and
      Zheng, Zilong  and
      Terzopoulos, Demetri  and
      Fei-Fei, Li  and
      Gao, Jianfeng  and
      Vo, Hoi",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.200/",
    doi = "10.18653/v1/2024.findings-naacl.200",
    pages = "3154--3183",
    abstract = "Large Foundation Models (LFMs) can perform complex scheduling in a multi-agent system and can coordinate agents to complete sophisticated tasks that require extensive collaboration.However, despite the introduction of numerous gaming frameworks, the community lacks adequate benchmarks that support the implementation of a general multi-agent infrastructure encompassing collaboration between LFMs and human-NPCs. We propose a novel infrastructure{---}Mindagent{---}for evaluating planning and coordination capabilities in the context of gaming interaction. In particular, our infrastructure leverages an existing gaming framework to (i) act as the coordinator for a multi-agent system, (ii) collaborate with human players via instructions, and (iii) enable in-context learning based on few-shot prompting with feedback.Furthermore, we introduce {\textquotedblleft}Cuisineworld{\textquotedblright}, a new gaming scenario and its related benchmark that supervises multiple agents playing the game simultaneously and measures multi-agent collaboration efficiency. We have conducted comprehensive evaluations with a new auto-metric Collaboration Score: CoS for assessing the collaboration efficiency. Finally, Mindagent can be deployed in real-world gaming scenarios in a customized VR version of Cuisineworld and adapted in the {\textquotedblleft}Minecraft{\textquotedblright} domain. Our work involving LFMs within our new infrastructure for general-purpose scheduling and coordination can elucidate how such skills may be obtained by learning from large language corpora."
}

@misc{kuba2022trustregionpolicyoptimisation,
      title={Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning}, 
      author={Jakub Grudzien Kuba and Ruiqing Chen and Muning Wen and Ying Wen and Fanglei Sun and Jun Wang and Yaodong Yang},
      year={2022},
      eprint={2109.11251},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2109.11251}, 
}

@misc{li2024agentmixermultiagentcorrelatedpolicy,
      title={AgentMixer: Multi-Agent Correlated Policy Factorization}, 
      author={Zhiyuan Li and Wenshuai Zhao and Lijun Wu and Joni Pajarinen},
      year={2024},
      eprint={2401.08728},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2401.08728}, 
}

@misc{ma2024largelanguagemodelsplay,
      title={Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach}, 
      author={Weiyu Ma and Qirui Mi and Yongcheng Zeng and Xue Yan and Yuqiao Wu and Runji Lin and Haifeng Zhang and Jun Wang},
      year={2024},
      eprint={2312.11865},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.11865}, 
}

@misc{mcclellan2024boostingsampleefficiencygeneralization,
      title={Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance}, 
      author={Joshua McClellan and Naveed Haghani and John Winder and Furong Huang and Pratap Tokekar},
      year={2024},
      eprint={2410.02581},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.02581}, 
}

@article{meng2023offline,
  title={Offline pre-trained multi-agent decision transformer},
  author={Meng, Linghui and Wen, Muning and Le, Chenyang and Li, Xiyun and Xing, Dengpeng and Zhang, Weinan and Wen, Ying and Zhang, Haifeng and Wang, Jun and Yang, Yaodong and others},
  journal={Machine Intelligence Research},
  volume={20},
  number={2},
  pages={233--248},
  year={2023},
  publisher={Springer}
}

@misc{nayak2025llamarlonghorizonplanningmultiagent,
      title={LLaMAR: Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments}, 
      author={Siddharth Nayak and Adelmo Morrison Orozco and Marina Ten Have and Vittal Thirumalai and Jackson Zhang and Darren Chen and Aditya Kapoor and Eric Robinson and Karthik Gopalakrishnan and James Harrison and Brian Ichter and Anuj Mahajan and Hamsa Balakrishnan},
      year={2025},
      eprint={2407.10031},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2407.10031}, 
}

@InProceedings{pmlr-v97-son19a,
  title = 	 {{QTRAN}: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning},
  author =       {Son, Kyunghwan and Kim, Daewoo and Kang, Wan Ju and Hostallero, David Earl and Yi, Yung},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5887--5896},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/son19a/son19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/son19a.html},
  abstract = 	 {We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN’s superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively.}
}

@misc{tan2024cradleempoweringfoundationagents,
      title={Cradle: Empowering Foundation Agents Towards General Computer Control}, 
      author={Weihao Tan and Wentao Zhang and Xinrun Xu and Haochong Xia and Ziluo Ding and Boyu Li and Bohan Zhou and Junpeng Yue and Jiechuan Jiang and Yewen Li and Ruyi An and Molei Qin and Chuqiao Zong and Longtao Zheng and Yujie Wu and Xiaoqiang Chai and Yifei Bi and Tianbao Xie and Pengjie Gu and Xiyun Li and Ceyao Zhang and Long Tian and Chaojie Wang and Xinrun Wang and Börje F. Karlsson and Bo An and Shuicheng Yan and Zongqing Lu},
      year={2024},
      eprint={2403.03186},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.03186}, 
}

@misc{vinyals2017starcraftiinewchallenge,
      title={StarCraft II: A New Challenge for Reinforcement Learning}, 
      author={Oriol Vinyals and Timo Ewalds and Sergey Bartunov and Petko Georgiev and Alexander Sasha Vezhnevets and Michelle Yeo and Alireza Makhzani and Heinrich Küttler and John Agapiou and Julian Schrittwieser and John Quan and Stephen Gaffney and Stig Petersen and Karen Simonyan and Tom Schaul and Hado van Hasselt and David Silver and Timothy Lillicrap and Kevin Calderone and Paul Keet and Anthony Brunasso and David Lawrence and Anders Ekermo and Jacob Repp and Rodney Tsing},
      year={2017},
      eprint={1708.04782},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1708.04782}, 
}

@misc{wang2024executablecodeactionselicit,
      title={Executable Code Actions Elicit Better LLM Agents}, 
      author={Xingyao Wang and Yangyi Chen and Lifan Yuan and Yizhe Zhang and Yunzhu Li and Hao Peng and Heng Ji},
      year={2024},
      eprint={2402.01030},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01030}, 
}

@inproceedings{zhang2022discovering,
  title={Discovering generalizable multi-agent coordination skills from multi-task offline data},
  author={Zhang, Fuxiang and Jia, Chengxing and Li, Yi-Chen and Yuan, Lei and Yu, Yang and Zhang, Zongzhang},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@misc{zhang2024buildingcooperativeembodiedagents,
      title={Building Cooperative Embodied Agents Modularly with Large Language Models}, 
      author={Hongxin Zhang and Weihua Du and Jiaming Shan and Qinhong Zhou and Yilun Du and Joshua B. Tenenbaum and Tianmin Shu and Chuang Gan},
      year={2024},
      eprint={2307.02485},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2307.02485}, 
}

@misc{zhu2025madiffofflinemultiagentlearning,
      title={MADiff: Offline Multi-agent Learning with Diffusion Models}, 
      author={Zhengbang Zhu and Minghuan Liu and Liyuan Mao and Bingyi Kang and Minkai Xu and Yong Yu and Stefano Ermon and Weinan Zhang},
      year={2025},
      eprint={2305.17330},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2305.17330}, 
}

