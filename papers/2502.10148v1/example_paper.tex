%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{tcolorbox}
\usepackage{listings}
\lstset{
    escapeinside={(*@}{@*)},
    captionpos=b,
    framesep=3pt,
    xleftmargin=15pt,
    xrightmargin=15pt,
    breakautoindent=true,
    breaklines=true,
    breakatwhitespace=false,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Cooperative Multi-Agent Planning with Adaptive Skill Synthesis}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zhiyuan Li}{aalto}
\icmlauthor{Wenshuai Zhao}{aalto}
\icmlauthor{Joni Pajarinen}{aalto}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{aalto}{Department of Electrical Engineering and Automation, Aalto University}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS achieves up to 30\% higher win rates than state-of-the-art MARL algorithms in symmetric scenarios.
%Despite much progress in training distributed artificial intelligence (AI) to model collective behaviors, building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Across extensive evaluations on the improved version of StarCraft Multi-Agent Challenge (SMACv2), COMPASS achieves a 30\% higher win rate than other state-of-the-art MARL algorithms while maintaining scalability and interpretability.
\end{abstract}

\section{Introduction}
\label{introduction}
A major long-term goal for the field of cooperative multi-agent systems (MAS), e.g. multi-robot control \cite{gao2024coohoilearningcooperativehumanobject,feng2024learningmultiagentlocomanipulationlonghorizon}, power management \cite{monroc2024wfcrl}, and multi-agent games \cite{10.5555/3306127.3332052,Kurach_Raichuk_Stańczyk_Zając_Bachem_Espeholt_Riquelme_Vincent_Michalski_Bousquet_Gelly_2020}, is to build a protocol of collaboration among the agents. Multi-agent reinforcement learning (MARL), proven as an advanced paradigm of distributed artificial intelligence (AI), holds promise for discovering collective behavior from interactions. One line of works follows centralized training decentralized execution (CTDE) paradigm \cite{JMLR:v21:20-081,NEURIPS2022_9c1535a0,liu2024maximum,10.5555/3295222.3295385,zhu2025madiffofflinemultiagentlearning,Li_Zhao_Wu_Pajarinen_2024}. CTDE assumes a central controller that exploits global information, while the individual policies are designed to allow for decentralized execution. However, in many real-world scenarios, the central controller becomes unfeasible due to the communication overhead that exponentially scales with the number of agents, thereby compromising the scalability of MAS. In contrast, the decentralized training decentralized execution paradigm (DTDE) discards this assumption and is scalable to large-scale systems \cite{su2024a,pmlr-v80-zhang18n,ma2024efficient}. However, DTDE requires complicated learning and planning under uncertainty, as partial observability magnifies the discrepancy between each agent’s local observation and global information. Although much progress has been made, MARL suffers from compromised sample efficiency, interpretability, and transferability.

The emergence of Large Language Models (LLMs) has revitalized this field. LLM-based multi-agents have been proposed to leverage their remarkable capacity to perform task-oriented collective behaviors \cite{10610855,zhang2024buildingcooperativeembodiedagents,gong-etal-2024-mindagent,10.1609/aaai.v38i16.29710,nayak2025llamarlonghorizonplanningmultiagent}. The LLMs are used for high-level planning to generate centralized \cite{gong-etal-2024-mindagent,nayak2025llamarlonghorizonplanningmultiagent,deng2024newapproachsolvingsmac} or decentralized plans \cite{10610855,zhang2024buildingcooperativeembodiedagents}, often adopting a hierarchical decision-making structure in conjunction with a pre-defined low-level controller. While these methods have succeeded in a set of multi-agent problems including Overcooked-AI \cite{10.5555/3454287.3454752}, SMAC \cite{10.5555/3306127.3332052}, and VirtualHome \cite{Puig_2018_CVPR}, heavy reliance on text-based observation prevents them from learning from multi-modal information. Moreover, they ignore the non-Markovian nature of MAS, where learning and planning necessitate a decentralized closed-loop solution.

In this paper, we mitigate the existing research limitations and advance general decision-making for cooperative multi-agent systems. At a high level, COMPASS combines a vision language models (VLMs)-based planner with a dynamic skill library for storing and retrieving complex behaviors, along with a structured communication protocol. A diagram of COMPASS is provided in Figure. \ref{fig:compass}. Inspired by Cradle \cite{tan2024cradleempoweringfoundationagents}, the VLM-based planner perceives the visual and textual observation and suggests the most suitable executable code from the skill library. We adopt the code-as-policy paradigm \cite{wang2024executablecodeactionselicit} instead of task-specific primitive actions, as it constrains generalizability and fails to fully leverage foundation models' extensive world knowledge and sophisticated reasoning capabilities. 

Traditional open-loop methods struggle to produce effective plans that adapt to dynamics in stochastic, partially observable environments. To address this challenge, the VLM-based planer attempts to solve challenging and ambiguous final tasks, such as "Defeat all enemy units in the StarCraft multi-agent combat scenario while coordinating with allied units", by progressively proposing a sequence of clear, manageable sub-tasks while incorporating environmental feedback and task progress. COMPASS generates Python scripts through LLMs as semantic-level skills to accomplish sub-tasks, incrementally building a skill library throughout the task progress. Each skill is indexed through its documentation embeddings, enabling retrieval based on task-skill relevance. However, developing the skill library from scratch requires extensive exploration to discover viable strategies. In contrast, we pre-collect demonstration videos and introduce a "warm start" by initializing the skill library with strategies derived from the expert-level dataset.

Moreover, building autonomous agents to cooperate in completing tasks under partial observation requires an efficient communication protocol. However, naive communication leads to the risks of hallucination caused by meaningless chatter between agents \cite{10.5555/3666122.3668386}. Inspired by entity-based MARL \cite{pmlr-v139-iqbal21a,pmlr-v202-ding23d}, we present a structured communication protocol to formulate the communication among agents along with a global memory that allows all agents to retrieve. The protocol incorporates a multi-hop propagation mechanism, enabling agents to infer information about entities beyond their field of view through information shared by teammates. Similar to previous approaches, each agent maintains a local memory to preserve current and historical experiences.

Empirically, COMPASS demonstrates effective adaptation and skill synthesis in cooperative multi-agent scenarios. Through its dynamic skill library, it creates reusable and interpretable code-based behaviors that evolve during task execution. We evaluate COMPASS systematically in the improved StarCraft Multi-Agent Challenge (SMACv2) using both open-source (Qwen2-VL-72B \cite{wang2024qwen2vlenhancingvisionlanguagemodels}) and closed-source (GPT-4o-mini\footnote{https://platform.openai.com/docs/models\#gpt-4o-mini}, Claude-3-Haiku\footnote{https://www.anthropic.com/news/claude-3-haiku}) VLMs. COMPASS achieves strong results in Protoss scenarios with a 57\% win rate, substantially outperforming state-of-the-art MARL algorithms, including QMIX \cite{JMLR:v21:20-081}, MAPPO \cite{NEURIPS2022_9c1535a0}, HAPPO \cite{kuba2022trustregionpolicyoptimisation}, and HASAC \cite{liu2024maximum}. COMPASS maintains moderate performance in Terran scenarios and handles asymmetric settings effectively, though showing limited success in Zerg task. We further demonstrate COMPASS's ability to bootstrap effective strategies from expert demonstrations.


\section{Related Work}
\label{related work}
\subsection{Agents in the StarCraft Multi-Agent Challenge}
SMAC \cite{10.5555/3306127.3332052}, a predominant cooperative MARL benchmark based on the StarCraft II real-time strategy game \cite{vinyals2017starcraftiinewchallenge}, focuses on decentralized micromanagement scenarios where each unit operates under decentralized execution with partial observability to defeat enemy units controlled by Starcraft II’s built-in AI opponent. Previous research in SMAC resort to MARL which can be divided into two categories: 1) Online MARL: One line of representative research is value decomposition (VD) \cite{JMLR:v21:20-081,wang2021qplex,pmlr-v97-son19a}, which decomposes the centralized action-value function into individual utility functions. On the other hand, multi-agent policy gradient (MAPG) methods \cite{NEURIPS2022_9c1535a0,kuba2022trustregionpolicyoptimisation,liu2024maximum,Li_Liu_Zhang_Wei_Niu_Yang_Liu_Ouyang_2023,NEURIPS2022_69413f87,hu2024learning,na2024efficient} extend single-agent policy gradient algorithm to multi-agent with coordination modeling. Researches such as MAPPO \cite{NEURIPS2022_9c1535a0}, HAPPO \cite{kuba2022trustregionpolicyoptimisation}, and HASAC \cite{liu2024maximum} combine trust region and maximum entropy with MARL in a non-trivial way respectively. To encourage coordination, communication methods \cite{hu2024learning,lo2024learning}, sequential modeling methods \cite{Li_Liu_Zhang_Wei_Niu_Yang_Liu_Ouyang_2023,NEURIPS2022_69413f87}, and cooperative exploration methods \cite{10.5555/3454287.3454971,na2024efficient} have been proposed. 2) Offline MARL: Recent efforts such as MADT \cite{meng2023offline}, ODIS \cite{zhang2022discovering}, and MADiff \cite{zhu2025madiffofflinemultiagentlearning} leverage data-driven training via pre-collected offline datasets to enhance policy training efficiency. However, the near-optimal performance of these existing approaches on SMAC highlights the benchmark's limited stochasticity and partial observability. To address these limitations, SMACv2 \cite{10.5555/3666122.3667756} introduces more complexity to necessitate decentralized closed-loop control policies. There have been some recent attempts \cite{Li_Zhao_Wu_Pajarinen_2024,mcclellan2024boostingsampleefficiencygeneralization,10.5555/3545946.3598961,li2024agentmixermultiagentcorrelatedpolicy} to evaluate MARL algorithms on SMACv2, and the results confirm the complexity. However, current learning-based multi-agent methods are computationally inefficient and non-interpretable. In the quest to find methods that are sample-efficient and interpretable, LLM-SMAC \cite{deng2024newapproachsolvingsmac} leverage LLMs to generate centralized decision tree code under global information in an open-loop framework. Unlike prior works, COMPASS integrates a Vision-Language Model (VLM) with each agent in a decentralized closed-loop manner under partial observability, improving both real-world applicability and scalability.

\subsection{LLM-based Multi-Agent System}
Based on the inspiring capabilities of LLMs, such as zero-shot planning and complex reasoning \cite{10.5555/3600270.3601883,10.5555/3666122.3667509,10.5555/3600270.3602070,Besta_Blach_Kubicek_Gerstenberger_Podstawski_Gianinazzi_Gajda_Lehmann_Niewiadomski_Nyczyk_Hoefler_2024}, embodied single-agent researches have demonstrated the effectiveness of LLMs in solving complex long-horizon tasks \cite{wang2024voyager,yao2023react,NEURIPS2023_1b44b878,ichter2022do,ma2024largelanguagemodelsplay,wang2024executablecodeactionselicit,tan2024cradleempoweringfoundationagents}. 
Despite significant advances in single-agent applications, developing real-world multi-agent systems with foundation models remains challenging, primarily due to the nature of decentralized control under partial observability in multi-agent settings \cite{10.5555/2283396.2283451}. 
Most prior efforts \cite{10610855,zhang2024buildingcooperativeembodiedagents,10.1609/aaai.v38i16.29710,nayak2025llamarlonghorizonplanningmultiagent,gong-etal-2024-mindagent} leverage a hierarchical framework with components like perception, communication, planning, execution, and memory to build multi-agent systems with collective behaviors. These approaches can be roughly classified into two groups. 1) Centralized plan: MindAgent \cite{gong-etal-2024-mindagent} adopts a centralized planning scheme with a pre-defined oracle in a fully observable multi-agent game. LLaMAR \cite{nayak2025llamarlonghorizonplanningmultiagent} employs LLMs to manage long-horizon tasks in partially observable environments without assumptions about access to perfect low-level policies. 2) Decentralized plan: ProAgent \cite{10.1609/aaai.v38i16.29710} introduces Theory of Mind (ToM), enabling agents to reason about others’ mental states. RoCo \cite{10610855} and CoELA \cite{zhang2024buildingcooperativeembodiedagents} assign separate LLMs to each embodied agent for collaboration with communication. However, RoCo and CoELA assume a skill library with a low-level heuristic controller, which is impractical in real-world applications. Moreover, RoCo's open-loop plan-and-execute paradigm fails to incorporate environmental feedback during decision-making. In contrast, our work does not assume any pre-defined low-level controller and generates code-based action through VLMs in a closed-loop manner.


\section{Preliminaries}
\label{background}
We model a fully cooperative multi-agent game with $N$ agents as a \textit{decentralized partially observable Markov decision process} (Dec-POMDP) \cite{10.5555/2967142}, which is formally defined as a tuple $\mathcal{G}=( \mathcal{N},\mathcal{S},\mathcal{O},\mathbb{O},\mathcal{B}, \mathcal{A},\mathcal{T},\Omega,R,\gamma ,\rho_0 )$. $\mathcal{N} = \{ 1,\ldots,N \}$ is a set of agents, $s \in \mathcal{S}$ denotes the state of the environment and $\rho_0$ is the distribution of the initial state. $\mathcal{A} = \prod_{i=1}^{N}A^i$ is the joint action space, $\mathbb{O}=\prod_{i=1}^{N}O^i$ is the set of joint observations. At time step $t$, each agent $i$ receives an individual partial observation $o^i_t \in O^i$ given by the observation function $\mathcal{O} : (a_t, s_{t+1}) \mapsto  P(o_{t+1}|a_t, s_{t+1}) $ where $a_t, s_{t+1}$ and $o_{t+1}$ are the joint actions, states and joint observations respectively. Each agent $i$ uses a stochastic policy $\pi^{i}(a^i_t | h^i_t,\omega^i_t)$ conditioned on its action-observation history $h^i_t=(o^i_0, a^i_0, \ldots , o^i_{t-1}, a^i_{t-1})$ and a random seed $\omega^{i}_{t} \in \Omega_{t}$ to choose an action $a^i_t \in A^i$. A belief state $b_t$ is a probability distribution over states at time $t$, where $b_t \in \mathcal{B}$, and $\mathcal{B}$ is the space of all probability distributions over the state space. Actions $a_t$ drawn from joint policy $\pi(a_t|s_t, \omega_t)$ conditioned on state $s_t$ and joint random seed $\omega_t=(\omega^1_t, \ldots, \omega^N_t)$ change the state according to transition function $\mathcal{T} : (s_{t}, a^1_t, \ldots, a^N_t) \mapsto  P(s_{t+1}|s_{t}, a^1_t, \ldots, a^N_t) $. All agents share the same reward $r_t=R(s_{t}, a^1_t, \ldots, a^N_t)$ based on $s_t$ and $a_t$. $\gamma$ is the discount factor for future rewards. Agents try to maximize the expected total reward, $\mathcal{J}(\pi) = \mathbb{E}_{s_{0},a_0,\dots }[ \sum_{t=0}^{\infty}\gamma ^{t}r_t ]$, where $s_0 \sim \rho _0 (s_0), a_t \sim \pi (a_t|s_t,\omega_t)$.

\begin{figure*}[ht!]
\centering
\includegraphics[width=.99\textwidth]{figs/compass.pdf}
\caption{Overview of the COMPASS architecture, a novel framework that advances cooperative multi-agent decision-making through three synergistic components: (1) A VLM-based closed-loop planner that enables decentralized control by continuously processing multi-modal feedback and adapting strategies, addressing the non-Markovian challenge of multi-agent systems; (2) A dynamic skill synthesis mechanism that combines demonstration bootstrapping with incremental skill generation, improving sample efficiency and interpretability; and (3) A structured communication protocol that facilitates efficient information sharing through entity-based multi-hop propagation, enhancing cooperative perception under partial observability.}
\label{fig:compass}
\end{figure*}

\section{Methods}
\label{methods}
COMPASS, illustrated in Figure \ref{fig:compass}, is a decentralized closed-loop framework for cooperative multi-agent systems that continuously incorporate environmental feedback for strategy refinement. The architecture comprises three core components: 1) a VLM-based closed-loop planner that iteratively perceives, reasons, reflects and acts to adaptively complete tasks (Sec. \ref{planner}); 2) an adaptive skill synthesis mechanism for generating executable codes tailored to proposed sub-tasks (Sec. \ref{skill-library}); and 3) a structured communication protocol that enables agents to share visible entity information under partial observability (Sec. \ref{communication}).

\subsection{VLM-based Closed-Loop Planner}
\label{planner}
Inspired by recent advances in cognitive architectures for autonomous systems \cite{tan2024cradleempoweringfoundationagents}, COMPASS implements a sophisticated modular planning framework that emulates key aspects of cognitive decision-making. The planner adopts a modular formulation, utilizing four specialized models: Perception, Task Reasoning, Self-Reflection, and Actor. Each model fulfills a distinct yet interconnected role in the decision-making process. The Perception model processes multi-modal inputs, integrating both visual and textual information to build comprehensive environmental understanding. The Task Reasoning model analyzes the perceived information to decompose complex objectives into manageable sub-tasks, ensuring systematic progress toward the final goal. The Self-Reflection model continuously evaluates task execution and outcome quality, enabling adaptive behavior refinement. The Actor model translates plans into actions by selecting and executing the most appropriate skills from the skill library. We next discuss the various components in detail:

\textbf{Perception} forms the foundation of COMPASS's decision-making capabilities by enabling robust multi-modal understanding of complex environments. Solving complex real-world tasks often involves data of multiple modalities \cite{10.1145/3613905.3651029}, each contributing unique and complementary information for decision-making. We leverage the VLMs' ability to fuse and analyze a broader spectrum of data, including text- and image-based environment feedback, to enable agents to sense the surrounding environment. The system's perception mechanism operates at two levels: direct observation processing and collaborative information synthesis. At the direct level, VLMs process raw inputs to extract meaningful features and relationships from both visual and textual data. At the collaborative level, COMPASS addresses the inherent challenge of partial observability in multi-agent systems through an innovative multi-hop communication protocol (detailed in Sec. \ref{communication}) that enables agents to construct a more holistic understanding of their environment by sharing and aggregating observations. This dual-level perception architecture ensures that each agent maintains both detailed local awareness and broader contextual understanding, essential for effective decision-making in complex cooperative tasks.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.9\columnwidth]{figs/task_reasoning.pdf}
        % \vspace{-1em}
        \caption{Visualization of COMPASS's dynamic task reasoning process in the StarCraft Multi-Agent Challenge (SMACv2) environment. The figure demonstrates how the VLM-based planner decomposes a complex final goal ("defeat all enemy units") into a sequence of concrete, executable sub-tasks that adapt to the changing battlefield conditions. This closed-loop task decomposition enables efficient coordination among multiple agents under partial observability, as each sub-task provides clear, actionable objectives that agents can execute while maintaining overall mission alignment.} 
        \label{fig:task_reasoning}
    \end{center}
\end{figure}

\textbf{Task Reasoning} enables COMPASS to systematically approach complex cooperative challenges through collective task decomposition. Given a simple general final task in the cooperative multi-agent setting, e.g., "defeat all enemy units", in order to complete the task more efficiently, agents are required to decompose it into multiple sub-tasks and figure out the right one to focus on, while considering alignment among others (See Figure. \ref{fig:task_reasoning}). COMPASS harnesses the power of VLMs to analyze high-level task instructions in conjunction with environmental feedback and team member objectives to generate tractable sub-tasks that collectively advance the overall mission. As agents act under stochastic, partially observable environments, the task reasoning model continuously adapts its plans, proposing and refining sub-tasks based on emerging situations and progress assessment. This dynamic approach enables COMPASS to maintain strategic coherence while adjusting tactical decisions in response to changing circumstances.

\textbf{Actor} serves as the critical bridge between high-level reasoning and concrete action execution. Building upon recent advances in code-writing language models for embodied control \cite{liang2023codepolicieslanguagemodel,wang2024executablecodeactionselicit}, the Actor leverages the skill library by first identifying relevant skills for the proposed sub-task, then synthesizes perception and self-reflection inputs to select the optimal skill for execution. This streamlined approach ensures efficient skill selection while maintaining task alignment.

\textbf{Self-Reflection} enables COMPASS to continuously evaluate and refine its decision-making processes through systematic performance analysis. COMPASS instantiates the Self-Reflection model as a VLM which takes a sequence of visual results from the last skill execution with corresponding descriptions as input to assess the quality of the decision produced by the Actor and whether the task was completed. Additionally, we also request the VLM to generate verbal self-reflections to provide valuable feedback on the completion of the task.

\subsection{Adaptive Skill Synthesis}
\label{skill-library}
COMPASS employs a dynamic skill library that maintains and evolves a collection of executable behaviors. Each skill is represented as an executable Python function with comprehensive documentation describing its functionality and corresponding embedding that enables semantic retrieval. This skill library undergoes continuous refinement through two complementary mechanisms: incremental synthesis, where new skills are generated and existing ones are refined during task execution, and demonstration-based bootstrapping, which initializes the library with behaviors extracted from expert demonstrations. 

\textbf{Incremental Synthesis}
With the Task Reasoning component consistently proposing sub-tasks, COMPASS first attempts to retrieve relevant skills from the library using semantic similarity between the sub-task description and skill documentation embeddings. If no suitable skill exists, or if existing skills prove inadequate, the VLM generates a new Python script specifically tailored to the sub-task.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.95\columnwidth]{figs/multi-hop.pdf}
        % \vspace{-1em}
        \caption{Illustration of COMPASS's structured multi-hop communication protocol that enables efficient information sharing under partial observability. The figure demonstrates how information about Enemy \#1 propagates to the Ego agent through a chain of allied units (Ally \#1, \#2, \#3), despite Enemy \#1 being outside Ego's sight range. Each dashed circle represents an agent's local observation field, while arrows indicate the flow of entity-based information sharing. This mechanism enables agents to build a more holistic understanding of the environment by propagating critical information (e.g., enemy positions, status) through intermediate allies, effectively addressing the partial observability challenge in decentralized multi-agent systems.} 
        \label{fig:multi-hop}
    \end{center}
\end{figure}

\textbf{Bootstrapping}
However, developing the skill library from scratch requires extensive interactions with environments, which potentially leads to inefficient learning in the early stages. Inspired by offline MARL approaches \cite{meng2023offline,zhang2022discovering,zhu2025madiffofflinemultiagentlearning}, which leverage pre-collected datasets to enhance sample efficiency, we leverage MAPPO as the behavior policy to collect experiences, which are recorded as video sequences. The VLMs then analyze these demonstrations through a multi-stage process: first identifying key strategic patterns and behavioral primitives, then translating these patterns into executable Python functions with appropriate documentation. This initialization methodology establishes a foundational set of validated skills, substantially reducing the exploration overhead typically required for discovering effective behaviors. The resulting baseline skill library enables efficient task execution from the onset while maintaining the flexibility to evolve through incremental synthesis.

\begin{figure*}[ht!]
\centering
\includegraphics[width=.99\textwidth]{figs/smac1.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{Focus Fire Logic Implementation. (a) VLM-generated Python code snippet implementing dynamic focus fire logic. The code prioritizes enemy units based on the number of allied attackers, scaling the attack bonus exponentially. (b–d) Visualizations of focus fire execution across Protoss, Terran, and Zerg.}
\label{fig:smac_focus_fire}
\end{figure*}

\subsection{Structured Communication Protocol}
\label{communication}
To facilitate effective collaboration under partial observability, recent LLM-based multi-agent work \cite{10.5555/3666122.3668386,zhang2024buildingcooperativeembodiedagents} employs conversational framework with unconstrained communication protocol. However, while natural language offers flexibility, unrestricted communication can lead to potential hallucinations caused by ambiguous or irrelevant messages between agents. Drawing from advances in structured communication frameworks \cite{hong2024metagpt} and entity-based MARL \cite{pmlr-v139-iqbal21a,pmlr-v202-ding23d}, COMPASS implements a hierarchical communication protocol that focuses on efficient entity-based information sharing and multi-hop propagation. Each agent maintains an observation buffer containing information about entities in its field of view. At each timestep, agents share their local observations, which are then aggregated into a global entity memory accessible to all. COMPASS employs a multi-hop communication mechanism to propagate information about distant entities, enabling agents to build a more holistic observation of the environment by leveraging the collective knowledge of the team.

\section{Experiments}
We conducted a comprehensive experimental evaluation of COMPASS to assess its performance and capabilities in complex multi-agent scenarios. Our evaluation focused on the improved StarCraft Multi-Agent Challenge (SMACv2) \cite{10.5555/3666122.3667756}, which provides an ideal testbed for examining cooperative behavior under partial observability and stochasticity. Through systematic experimentation, we investigated two fundamental questions: (1) How does COMPASS perform compared to state-of-the-art MARL methods? (2) What are the individual contributions of the adaptive skill synthesis mechanism? Experiments utilize both open-source (Qwen2-VL-72B) and closed-source VLMs (GPT-4o-mini, Claude-3-Haiku), with Jina AI embeddings for skill retrieval. All results are averaged over 5 seeds to account for environmental stochasticity.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.99\columnwidth]{figs/smac2.pdf}
        % \vspace{-1em}
        \caption{Illustration and implementation of Kitting logic.  (a)-(c) demonstrate progressive stages of the kitting tactic where allied units strategically maintain optimal attack range while retreating from melee enemies. (d) shows the corresponding Python code snippet generated by the VLMs.} 
        \label{fig:kitting}
    \end{center}
\end{figure}

\begin{figure}
    \begin{center}
        \includegraphics[width=0.99\columnwidth]{figs/smac3.pdf}
        % \vspace{-1em}
        \caption{Illustration of Isolating logic. (a) Allied units strategically assemble into a cohesive formation. (b) The assembled units execute a rapid engagement against an isolated enemy unit, eliminating it before reinforcements can arrive, thus creating a numerical advantage.} 
        \label{fig:isolate}
    \end{center}
\end{figure}

\label{experiments}
\subsection{Experimental Setup}
\textbf{Scenarios} Our evaluation scenarios span three distinct race matchups (Protoss, Terran, and Zerg) and two categories (symmetric and asymmetric), as detailed in Table \ref{tab:smacv2}. The symmetric scenarios (5v5) test coordination in balanced engagements, while asymmetric scenarios (5v6) evaluate adaptation to numerical disadvantages. Each race combination presents unique tactical challenges due to different unit abilities and constraints.
\begin{table}[ht]
\caption{SMACv2 task scenarios}
\label{tab:smacv2}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
Task & Scenarios & Categories\\
\midrule
Protoss & protoss 5 vs 5 & symmetric\\
  & protoss 5 vs 6 & asymmetric\\
Terran & terran 5 vs 5 & symmetric\\
  & terran 5 vs 6 & asymmetric\\
Zerg & zerg 5 vs 5 & symmetric\\
  & zerg 5 vs 6 & asymmetric\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{Baselines} We compared COMPASS against the state-of-the-art MARL algorithms representing both value-based and policy-gradient approaches:
\begin{itemize}
    \item Value-Based Methods: QMIX \cite{JMLR:v21:20-081} uses a mixing network architecture to decompose joint action-values while maintaining monotonicity constraints.
    \item Policy Gradient Methods: MAPPO \cite{NEURIPS2022_9c1535a0} extends PPO to multi-agent settings with the CTDE paradigm. HAPPO \cite{kuba2022trustregionpolicyoptimisation} performs sequential policy updates by utilizing other agents' newest policy under the CTDE framework and provably obtains the monotonic policy improvement guarantee. HASAC \cite{liu2024maximum} combines the maximum entropy framework with trust region optimization to enhance exploration and coordination.
\end{itemize}

\textbf{Datasets} To enable effective bootstrapping of the skill library, we constructed a comprehensive demonstration dataset capturing diverse multi-agent strategies and interactions. We employed MAPPO with original hyper-parameters as our behavior policy for data collection, leveraging its strong performance in cooperative multi-agent tasks. Our final dataset comprises over 300 complete game episodes, each recorded as a video sequence capturing the full state-action trajectory. These demonstrations span all symmetric scenario types described in Table \ref{tab:smacv2}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.99\columnwidth]{figs/smac4.pdf}
        % \vspace{-1em}
        \caption{Demonstration of area-of-effect (AOE) optimization for Baneling units in SMACv2. (a) The VLM-generated Python code calculates optimal detonation positions by analyzing enemy cluster density and positions. (b-c) Visual sequence showing Baneling execution, where the unit identifies a dense cluster of enemy units and detonates for maximum AOE damage.} 
        \label{fig:baneling}
    \end{center}
\end{figure}

\subsection{Main Results}
\begin{table*}
\caption{Comparative performance of COMPASS (with three VLM variants) and state-of-the-art MARL baselines on SMACv2. Median win rates (\%) and standard deviations (subscripts) are reported across Protoss, Terran, and Zerg scenarios in symmetric (5v5) and asymmetric (5v6) categories. Results are averaged over 5 seeds. Bold values denote the best performance in each scenario.}
\centering
\begin{tabular}{lccccccc}
\hline
& QMIX & MAPPO & HAPPO & HASAC & \multicolumn{3}{c}{COMPASS} \\
\cline{6-8}
& & & & & GPT-4o-mini & Claude-3-Haiku & Qwen2-VL-72B \\
\hline
\multicolumn{8}{c}{PROTOSS} \\
\hline
SYMMETRIC & $0.27_{0.03}$ & $0.32_{0.067}$ & $0.34_{0.07}$ & $0.20_{0.08}$ & $\mathbf{0.57_{0.08}}$ & $0.49_{0.06}$ & $0.45_{0.04}$ \\
ASYMMETRIC & $0.01_{0.01}$ & $0.04_{0.04}$ & $0.02_{0.03}$ & $0.01_{0.02}$ & $\mathbf{0.08_{0.04}}$ & $0.06_{0.05}$ & $0.06_{0.03}$ \\
\hline
\multicolumn{8}{c}{TERRAN} \\
\hline
SYMMETRIC & $0.38_{0.04}$ & $0.36_{0.1}$ & $0.35_{0.1}$ & $	0.29_{0.01}$ & $\mathbf{0.39_{0.01}}$ & $0.38_{0.05}$ & $0.31_{0.02}$ \\
ASYMMETRIC & $0.06_{0.02}$ & $0.07_{0.06}$ & $0.01_{0.03}$ & $0.05_{0.02}$ & $\mathbf{0.1_{0.03}}$ & $\mathbf{0.1_{0.01}}$ & $0.06_{0.03}$ \\
\hline
\multicolumn{8}{c}{ZERG} \\
\hline
SYMMETRIC & $0.21_{0.01}$ & $\mathbf{0.27_{0.04}}$ & $0.2_{0.11}$ & $0.24_{0.07}$ & $0.16_{0.07}$ & $0.18_{0.02}$ & $0.14_{0.03}$ \\
ASYMMETRIC & $\mathbf{0.18_{0.03}}$ & $0.13_{0.09}$ & $0.09_{0.02}$ & $0.08_{0.05}$ & $0.03_{0.01}$ & $0.04_{0.01}$ & $0.02_{0.01}$ \\
\hline
\end{tabular}
\label{tab:quantitative_results}
\end{table*}

\begin{table}
\caption{We report quantitative results on SMACv2 under sparse reward settings, excluding COMPASS due to its inherent insensitivity to reward sparsity.}
\centering
\begin{tabular}{lcccc}
\hline
& QMIX$_{s}$ & MAPPO$_{s}$ & HAPPO$_{s}$ & HASAC$_{s}$ \\
\hline
\multicolumn{5}{c}{PROTOSS} \\
\hline
5V5 & 0 & 0 & 0 & 0 \\
5V6 & 0 & 0 & 0 & 0 \\
\hline
\multicolumn{5}{c}{TERRAN} \\
\hline
5V5 & 0 & 0 & 0 & 0 \\
5V6 & 0 & 0 & 0 & 0 \\
\hline
\multicolumn{5}{c}{ZERG} \\
\hline
5V5 & $0.02_{0.01}$ & 0 & 0 & 0 \\
5V6 & 0 & 0 & 0 & 0 \\
\hline
\end{tabular}
\label{tab:quantitative_results_sparse}
\end{table}

\textbf{Performance} As shown in Table \ref{tab:quantitative_results}, COMPASS demonstrates significant performance advantages in SMACv2, particularly excelling in Protoss scenarios where it achieves a 57\% win rate in symmetric engagements using GPT-4o-mini, substantially outperforming traditional approaches like QMIX (27\%), MAPPO (32\%), and HAPPO (34\%). 

However, performance varies across race matchups. While maintaining strong results in Terran scenarios (39\% win rate), COMPASS shows limited effectiveness in Zerg scenarios (16\% win rate). This performance disparity can be attributed to the unique mechanics of Zerg combat units, which demand more fine-grained micromanagement due to their shorter attack ranges and reliance on swarm-based tactics.

In asymmetric scenarios (5v6), COMPASS consistently outperforms MARL baselines in Protoss and Terran matchups, demonstrating its ability to execute effective strategies despite being outnumbered. Its success in these settings suggests that COMPASS can adapt dynamically, using coordinated tactics and learned skills to counteract numerical disadvantages. The robust performance holds across different VLM implementations, with GPT-4o-mini consistently achieving the strongest results.

Moreover, COMPASS demonstrates particular advantages in scenarios with sparse reward, where traditional MARL approaches significantly underperform. As shown in Table \ref{tab:quantitative_results_sparse}, under sparse reward settings, baseline methods struggle to achieve any meaningful success.

\textbf{Skill Analysis} We now analyze COMPASS's capability to synthesize and execute diverse tactical behaviors. COMPASS develops four key tactical patterns: (1) An exponentially-scaled focus fire implementation that coordinates multiple units' target selection based on allied attacker density (Figure \ref{fig:smac_focus_fire}), (2) A position-aware kiting mechanism that maintains optimal engagement ranges while managing unit positioning relative to threats (Figure \ref{fig:kitting}), (3) A formation-based isolation tactic that enables systematic target elimination through coordinated unit movements (Figure \ref{fig:isolate}), and (4) An area-of-effect (AOE) tactic that maximizes splash damage through cluster density calculation (Figure \ref{fig:baneling}). These synthesized skills exhibit clear strategic intent while maintaining interpretability.

\subsection{Ablation Studies}
\begin{table}
\caption{Win rates of the initialized skill library (bootstrapped from expert demonstrations) on SMACv2.}
\centering
\begin{tabular}{lccc}
\hline
& PROTOSS & TERRAN & ZERG \\
\hline
5V5 & $0.35_{0.06}$ & $0.24_{0.04}$ & $0.06_{0.01}$ \\
5V6 & $0.04_{0.05}$ & $0.06_{0.02}$ & $0.02_{0.03}$ \\
\hline
\end{tabular}
\label{tab:initialized}
\end{table}
\textbf{The Effectiveness of Skill Initialization} To evaluate the impact of our skill initialization, we analyze the performance of COMPASS using only the initialized skill library derived from expert demonstrations. The results in Table \ref{tab:initialized} demonstrate that skill initialization alone achieves non-trivial performance across different scenarios, particularly in symmetric matchups. Moreover, the gap between initialized skills and COMPASS underscores the necessity of incremental skill synthesis. A script example for skill initialization is in Appendix.

\section{Conclusion}
\label{conclusion}
We present COMPASS, a novel framework for cooperative multi-agent systems that integrates vision-language models, a dynamic skill library, and structured communication. Through decentralized closed-loop planning, COMPASS enables agents to iteratively decompose tasks and adapt strategies via environmental feedback. Our skill library, initialized from expert demonstrations and refined through execution, provides interpretable code-based behaviors. Our hierarchical communication protocol enhances coordination under partial observability through entity-level information sharing. Evaluations on SMACv2 demonstrate COMPASS's effectiveness in several scenarios, particularly in Protoss, while highlighting areas for improvement in others like the Zerg setting. These results suggest that COMPASS provides a promising direction for developing interpretable and adaptable multi-agent systems suitable for real-world applications.

% We present COMPASS, a novel framework for cooperative multi-agent systems that addresses challenges in sample efficiency, interpretability, and partial observability by integrating vision-language models (VLMs), a dynamic skill library, and structured multi-hop communication. COMPASS leverages VLMs for decentralized closed-loop planning, enabling agents to iteratively decompose tasks, adapt strategies via environmental feedback, and generate executable code-based skills stored in an evolving library bootstrapped from demonstrations. A hierarchical communication protocol propagates entity-level information across agents, enhancing coordination under partial observability. Evaluations on SMACv2 demonstrate COMPASS outperforms state-of-the-art MARL methods by 30\% in win rate, while its modular design ensures scalability and interpretability for real-world applications like robotics and resource management.

\section*{Impact Statement}

We believe that the proposed work enhances the capacity for intelligent decision-making in complex and dynamic environments, and can have a positive impact on real-world multi-agent applications such as robotics, traffic management, and resource allocation. However, it is essential to consider potential concerns such as the discrepancy between the simulated environment and the real world. Another potential effect of directly implementing the derived policy is that it could lead to biased decision-making and privacy infringements. Mitigation strategies to address potential hazards could include the establishment of ethical guidelines and regulatory frameworks alongside the integration of transparency.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Prompts used in COMPASS}
\begin{tcolorbox}[colback=black!5,colframe=black!40!black,title=Prompt for Perception]
\small{
You are an AI assistant helping with academic research in the StarCraft II's SMAC (StarCraft Multi-Agent Challenge) environment, controlling a $<$unit\_type$>$ unit with ID $<$unit\_id$>$ in micromanagement scenarios $<$scenario\_name$>$ to help your team defeat the enemy forces. You operate under decentralized execution with partial observability, making decisions based only on local observations within your unit's field of view. Your advanced capabilities enable you to process and interpret gameplay screenshots and other relevant information.\\
I will give you the following information:\\
$<$few\_shots$>$\\
Reasoning for the last episode:\\
$<$last\_episode\_reasoning$>$\\
Strategic situation analysis:\\
$<$info\_summary$>$\\
Below is the current in-game screenshot and its description:\\
$<$image\_introduction$>$\\
Minimap information:\\
$<$ego\_minimap$>$\\
Current task:\\
$<$task\_description$>$\\
Tactics recommendation:\\
$<$web\_search$>$\\
Based on the above information, you should first analyze the current game situation by integrating the information from the in-game screenshot, its description, and other provided information.\\
Game situation:\\
You should think step by step and provide detailed reasoning to determine the current state of the game. You need to answer the following questions step by step:\\
   1. What is your unit\_id, unit\ type?\\
   2. What map borders are you near? Check which cardinal directions (N/S/E/W) have unavailable movement actions.\\
   3. What is the current health status of your unit? What is the current shield status of your unit?\\
   4. Are there any enemy units visible, either in observation or minimap?\\
   5. Are there any ally units visible, either observation or minimap?\\
   6. Are you positioned at the optimal attack range from enemies, or do you need to reposition based on the enemies' locations and directions?\\
Region of interest: \\
What unit or location should be interacted with to complete the task based on the current screenshot and the current task? You should obey the following rules:  \\
   1. If your chosen region of interest is a unit, format the output as "[Enemy/Ally] \#[target\_id]" (e.g., "Enemy \#0" for enemy unit with ID 0, "Ally \#1" for ally unit with ID 1)\\
   2. If your chosen region of interest is location, format the output as "Location: [direction]" where direction must be one of: "North", "Northeast", "East", "Southeast", "South", "Southwest", "West", "Northwest", "Center" (e.g., "Location: Northeast")\\
   3. If there are units visible, prioritize using unit as region of interest.\\
   4. If the target\_id is required, you MUST only use enemy/ally's unit\_ids that are currently visible in your shooting range.\\
   5. If your chosen region of interest is location, you MUST verify its availability.\\
   6. If shared minimap information reveals enemies outside your sight range, prioritize moving to those locations unless there are enemies within your current vision range.\\
   7. Your chosen region of interest should align with the current task description and ally's intentions.\\
   8. Your chosen region of interest should enable you to quickly engage in combat or efficiently achieve the task in cooperation with allies?\\
Reasoning of region of interest: \\
Why was this region of interest chosen? \\
You should only respond in the format described below with a line break after each section colon (\#\#Section\#\#:) and NOT output comments or other information:\\
\#\#Game\_situation\#\#:\\
1. ...\\
\#\#Region\_of\_interest\#\#:\\
region of interest\\
\#\#Reasoning\_of\_region\_of\_interest\#\#:\\
1. ...
}
\end{tcolorbox}

\begin{tcolorbox}[colback=black!5,colframe=black!40!black,title=Prompt for Task Reasoning]
\small{
You are an AI assistant helping with academic research in the StarCraft II's SMAC (StarCraft Multi-Agent Challenge) environment, controlling a $<$unit\_type$>$ unit with ID <$unit_id$> in micromanagement scenarios $<$scenario\_name$>$ to help your team defeat the enemy forces. You operate under decentralized execution with partial observability, making decisions based only on local observations within your unit's field of view. You will be sequentially given $<$event\_count$>$ screenshots and corresponding descriptions of recent events. You will also be given a summary of the history that happened before the last screenshot. By analyzing these inputs, you gain a comprehensive understanding of the current context and situation within the game. You should assist in summarizing the next immediate task to do in SMACv2. Your ultimate goal is to help your team defeat the enemy forces as quickly as possible.\\
I will give you the following information:\\
Reasoning for the last episode:\\
$<$last\_episode\_reasoning$>$\\
Cumulative reward for the executing skill:\\
$<$cumulative\_reward$>$\\
Current task:\\
$<$task\_description$>$\\
Ally's tasks:\\
$<$ally\_task$>$\\
Minimap information:\\
$<$ego\_minimap$>$\\
Current game situation:\\
$<$game\_situation$>$\\
Tactics recommendation:\\
$<$web\_search$>$\\
The following are successive screenshots:\\
$<$image\_introduction$>$\\
Skill set in Python format to select the next skill:\\
$<$skill\_library$>$\\
Current executing skill:\\
$<$previous\_action$>$\\
Implementation of the skill:\\
$<$action\_code$>$\\
Reasoning for the skill:\\
$<$previous\_reasoning$>$\\
Self-reflection for the last executed skill:\\
$<$previous\_self\_reflection\_reasoning$>$\\
Task\_guidance: \\
Based on the comprehensive game state analysis and team context, decompose the primary objective of "defeat all enemy units" into ONE specific tactical sub-task that enhances either target prioritization (score\_target) or behavior control (control\_logic). This sub-task should be concrete, implementable, and aligned with team coordination.
Consider the following in your task decomposition:\\
1. Final Objective: Defeat enemy forces while preserving allies\\
2. Team Context:\\
   - Your unit's current assigned task
   - Ally units' assigned tasks
   - Progress made on previous tasks
3. Tactical Layer:\\
   - Enemy unit compositions and strategies
   - Team formation and positioning
The task should follow one of these formats:\\
For target prioritization (score\_target):\\
"Adjust [scoring weight/multiplier/threshold] to [specific combat calculation] based on [unit composition + battle state] where [precise condition]"\\
For behavior control (control\_logic):\\
"Implement [unit movement pattern/formation/targeting] when [combat state + ally positions] satisfy [precise conditions]"
Task Requirements:\\
Specificity: Must define exact behavior modification
Measurability: Must have clear success criteria
Actionability: Must be achievable using available atomic actions
Coordination: Must support team tactical objectives
Adaptability: Must respond to changing battle conditions
If current task implementation remains unsuccessful, output 'null'.\\
Reasoning\_of\_task: \\
Why was this new task chosen, or why is there no need to propose a new task? \\
Skill\_guidance:\\
Based on the current executing skill and the proposed next task, evaluate if there is alignment between them. Output True if the current skill effectively supports the task requirements, or False if a new skill is needed.\\
Reasoning\_of\_skill: \\
Why was this decision chosen? \\
You should only respond in the format described below with a line break after each section colon (\#\#Section\#\#:) and NOT output comments or other information:\\
\#\#Task\_guidance\#\#:

[task guidance]

\#\#Skill\_guidance\#\#:

[True or False]
}
\end{tcolorbox}

\begin{tcolorbox}[colback=black!5,colframe=black!40!black,title=Prompt for Skill Generation]
\small{
You are an AI assistant helping with academic research in the StarCraft II's SMAC (StarCraft Multi-Agent Challenge) environment, controlling a $<$unit\_type$>$ unit with ID <$unit_id$> in micromanagement scenarios $<$scenario\_name$>$ to help your team defeat the enemy forces. You operate under decentralized execution with partial observability, making decisions based only on local observations within your unit's field of view. Your task is to enhance combat effectiveness:\\
Reasoning for the last episode:\\
$<$last\_episode\_reasoning$>$\\
Cumulative reward for the executing skill:\\
$<$cumulative\_reward$>$\\
Current task:\\
$<$task\_description$>$\\
Ally's tasks:\\
$<$ally\_task$>$\\
Minimap information:\\
$<$ego\_minimap$>$\\
Current game situation:\\
$<$game\_situation$>$\\
$<$image\_introduction$>$\\
Skill set in Python format to select the next skill:\\
$<$skill\_library$>$\\
Current executing skill:\\
$<$previous\_action$>$\\
Implementation of the skill:\\
$<$action\_code$>$\\
Reasoning for the skill:\\
$<$previous\_reasoning$>$\\
Self-reflection for the last executed skill:\\
$<$previous\_self\_reflection\_reasoning$>$\\
Combat Analysis Task:\\
1. Analyze the provided script's effectiveness\\
2. Analyze the score\_target(unit) function's effectiveness and weaknesses.\\
3. Analyze the control\_logic() function's effectiveness and weaknesses.\\
4. Based on the current executing skill, the existing skills in skill library, and current task, evaluate if there is alignment between them. \\
5. If a new skill is needed, design tactical improvements while maintaining code structure.\\
6. If the current skill or there is any skill in skill library effectively supports the task requirements, output 'null' to avoid unnecessary token consumption.\\
Identify critical function for improvement (choose ONE Prioritize score\_target(unit)):\\
1. score\_target(unit): Target priority and scoring system. (Preferred)\\
2. control\_logic(): Unit movement and attack decision making.\\
Skill\_generation: \\
If there is no enemies, only output 'null'.\\
If the current skill or there is any skill in skill library effectively supports the task requirements, only output 'null'. \\
Otherwise:\\
The content of the improved code should obey the following code rules:\\
    1. Output Format: Only provide the complete improved function (score\_target(unit) (Preferred) OR control\_logic()).\\
    2. If the improved function is score\_target(unit), there is exactly one parameter named "unit".\\
    3. If the improved function is control\_logic(), it should take no parameters.\\
    4. The code should be surrounded in the '```python' and '```' structure. \\

You should only respond in the format described below with a line break after each section colon (\#\#Section\#\#:) and NOT output comments or other information:\\

\#\#Skill\_generation\#\#:\\
```python

def [function\_name]([parameters]):

    [improved implementation]
```
}
\end{tcolorbox}

\begin{tcolorbox}[colback=black!5,colframe=black!40!black,title=Prompt for Actor]
\small{
You are an AI assistant helping with academic research in the StarCraft II's SMAC (StarCraft Multi-Agent Challenge) environment, controlling a $<$unit\_type$>$ unit with ID <$unit_id$> in micromanagement scenarios $<$scenario\_name$>$ to help your team defeat the enemy forces. You operate under decentralized execution with partial observability, making decisions based only on local observations within your unit's field of view. Utilizing this insight, you are tasked with identifying the most suitable skill to take next, given the current task. You control the game unit and can execute skills from the available skill set. Upon evaluating the provided information, your role is to articulate the precise skill you would deploy, considering the game's present circumstances, and specify any necessary parameters for implementing that skill:\\
$<$last\_episode\_reasoning$>$\\
Cumulative reward for the executing skill:\\
$<$cumulative\_reward$>$\\
Current task:\\
$<$task\_description$>$\\
Ally's tasks:\\
$<$ally\_task$>$\\
Minimap information:\\
$<$ego\_minimap$>$\\
Current game situation:\\
$<$game\_situation$>$\\
$<$image\_introduction$>$\\
Skill set in Python format to select the next skill:\\
$<$skill\_library$>$\\
Current executing skill:\\
$<$previous\_action$>$\\
Implementation of the skill:\\
$<$action\_code$>$\\
Reasoning for the skill:\\
$<$previous\_reasoning$>$\\
Self-reflection for the last executed skill:\\
$<$previous\_self\_reflection\_reasoning$>$\\
Skills: \\
The best skill to execute next to progress in achieving the goal. Pay attention to the names of the available skills and to the previous skills already executed, if any. You should also pay more attention to the following skill rules:\\
    1. ONLY choose skill in the provided skill set.\\
    2. Output skills in Python code format with required keyword parameters.\\
    3. The ONLY required keyword parameter is "obs: str" - you MUST include this parameter as "obs='current'" in every skill. The actual observation will be automatically injected at runtime.\\
    4. If there is summarization of history, consider this information when selecting the skill.\\
    5. If the error report indicates that the last skill was unavailable, you MUST select a different skill.\\
    6. Consider coordination with other units and choose skills that enhance team performance and cooperation.\\
    7. Avoid repeating the same skill as the last executed skill unless there is a compelling strategic reason.\\
You should only respond in the format described below with a line break after each section colon (\#\#Section\#\#:) and NOT output comments or other information:\\

\#\#Skills\#\#:\\
```python

    skill\_name(obs='current')
    
```
}
\end{tcolorbox}
\newpage
\begin{lstlisting}[caption={Example Script of Skill Initialzation},label=code:long]
def race_melee_ranged_medivac_navi_A_star_score_type_default_center(obs: str):
    """
    Zealot/Zergling/Baneling/Colossus/Stalker/Hydralisk/Marauder/Marine/Medivac Controls Logic:
        Medivac:
            - Heals allies below 100% HP
            - Maintains 0.75 sight range from enemies
            - Centers between allies when no healing targets

        Melee (Zealot/Zergling/Baneling):
            - Attacks highest threat target within 0.7 sight range
            - Pursues targets using A* pathfinding
            - Groups with allies at >0.7 distance threshold

        Ranged (Colossus/Stalker/Hydralisk/Marauder/Marine):
            - Kites melee units at max_range - 0.05 
            - Focus fires targets shared with 2+ allies
            - Maintains position behind melee allies

        Key Implementation:
        - Pathfinding: A* pathfinding in 32x32 grid with unit collision radius
        - Target scoring: [0-10] based on type (colossus 9 > baneling 8 > zealot 7 > stalker 6 > hydralisk 5 > marauder 4 > marine 3 > zergling 2 > medivac 1)/health (0-0.6)/distance (0-0.3)/last attacked (0.1)
        - Default action: Move to center of map, parse region of interest, random choice

    Args:
        obs (str): Observation string containing game state
    """
    import math
    # Parse observation
    obs_data = parse_obs(obs)
    # Get set of available actions
    valid_actions = obs_data.available_actions

    if 0 in valid_actions:
        return 0    
    
    def score_target(unit):
        """Enhanced target scoring with improved kiting and formation control"""
        if unit.health <= 0:
            return -1
                    
        score = 0
        
        # Refined unit type priorities with enhanced threat scaling
        unit_priorities = {
            'colossus': 35.0,  # Further increased priority
            'stalker': 30.0,   # Enhanced anti-armor focus
            'zealot': 45.0,    # Higher melee threat recognition

            'marine': 45.0,    # Balanced damage dealer priority
            'marauder': 35.0,  # Anti-armor specialist
            'medivac': 30.0,   # Support unit priority

            'hydralisk': 30.0,  # High priority for their sustained DPS
            'zergling': 35.0,   # Medium priority as swarm units
            'baneling': 45.0,   # Critical priority due to splash damage
        }
        
        # Dynamic matchup priorities with improved counter weighting
        unit_counters = {
            'colossus': {'colossus': 1.2, 'stalker': 1.0, 'zealot': 1.5},
            'stalker': {'colossus': 1.2, 'stalker': 1.0, 'zealot': 1.5},
            'zealot': {'colossus': 1.2, 'stalker': 1.0, 'zealot': 1.5},

            'marine': {'marine': 1.5, 'medivac': 1.0, 'marauder': 1.2},
            'marauder': {'marine': 1.5, 'medivac': 1.0, 'marauder': 1.2},
            'medivac': {'marine': 1.5, 'medivac': 1.0, 'marauder': 1.2},

            'hydralisk': {'hydralisk': 1.0, 'zergling': 1.2, 'baneling': 1.5},
            'zergling': {'hydralisk': 1.2, 'zergling': 1.5, 'baneling': 1.0},
            'baneling': {'hydralisk': 1.2, 'zergling': 1.5, 'baneling': 1.0},

        }
        
        base_priority = unit_priorities.get(unit.unit_type.lower(), 5.0)
        is_ranged = unit.unit_type.lower() not in ['zealot', 'zergling', 'baneling']
        own_is_ranged = obs_data.own_unit_type.lower() not in ['zealot', 'zergling', 'baneling']
        
        # Enhanced threat assessment with improved melee handling
        matchup_mult = unit_counters.get(obs_data.own_unit_type.lower(), {}).get(unit.unit_type.lower(), 1.0)
        base_priority *= matchup_mult
        
        if hasattr(unit, 'can_attack'):  # Enemy unit
            score = base_priority

            distance_factor = max((1 - unit.distance) + 1, 0.5)
            score *= distance_factor

            if not own_is_ranged:
                range_ally = [ally for ally in obs_data.allies if ally.unit_type.lower() not in ['zealot', 'zergling', 'baneling']]
                if range_ally:
                    ally_x = sum(ally.position[0] for ally in range_ally) / len(range_ally)
                    ally_y = sum(ally.position[1] for ally in range_ally) / len(range_ally)
                    ally_distance = ((ally_x - unit.position[0])**2 + (ally_y - unit.position[1])**2)**0.5
                    distance_factor = max((1 - ally_distance) + 1, 0.5)
                    score *= distance_factor
            
            # Enhanced Position Analysis with improved spacing
            position_x, position_y = unit.position
            
            def calculate_combat_power(units, radius=0.5):  # Further reduced for tighter control
                total_power = 0
                ranged_count = 0
                melee_count = 0
                unit_positions = []
                
                for u in units:
                    dist = ((u.position[0] - position_x)**2 + 
                        (u.position[1] - position_y)**2)**0.5
                    unit_positions.append(u.position)
                    
                    if dist <= radius:
                        base_power = unit_priorities.get(u.unit_type.lower(), 5.0)
                        
                        # Unit type specific power calculation
                        if u.unit_type.lower() in ['zealot', 'zergling', 'baneling']:
                            melee_count += 1
                            if melee_count >= 2:
                                base_power *= 1.4
                        else:
                            ranged_count += 1
                            base_power *= 1.3
                        
                        # Health-based power scaling
                        health_factor = 1.5 if u.health > 0.7 else 1.0 if u.health > 0.4 else 0.6
                        position_factor = 1.3 - (dist/radius)
                        
                        total_power += base_power * health_factor * position_factor
                
                # Enhanced formation cohesion calculation
                cohesion = 0
                if len(unit_positions) > 2:
                    center_x = sum(p[0] for p in unit_positions) / len(unit_positions)
                    center_y = sum(p[1] for p in unit_positions) / len(unit_positions)
                    avg_dist = sum(((p[0] - center_x)**2 + (p[1] - center_y)**2)**0.5 
                                for p in unit_positions) / len(unit_positions)
                    max_desired_dist = 0.3  # Tighter formation control
                    cohesion = 2.0 / (1.0 + (avg_dist / max_desired_dist))
                
                return total_power * (1 + cohesion), melee_count, ranged_count
            
            ally_power, ally_swarms, ally_ranged = calculate_combat_power(obs_data.allies)
            enemy_power, enemy_swarms, enemy_ranged = calculate_combat_power(obs_data.enemies)
            
            # Improved Focus Fire Logic with enhanced commitment
            num_attackers = sum(1 for ally in obs_data.allies 
                            if ally.last_action >= 6 and ally.last_action - 6 == unit.id)
            
            if unit.id == obs_data.last_action - 6:
                persistence_bonus = 2.0  # Stronger target commitment
                score *= persistence_bonus
            
            if num_attackers > 0:
                focus_bonus = 1.2 ** num_attackers  # Enhanced focus fire emphasis
                # if num_attackers too high, discourage prevent overcommitment
                if num_attackers >= 3 and unit.id != obs_data.last_action - 6 and obs_data.own_unit_type.lower() in ['zergling', 'baneling']:
                    focus_bonus = 0.5
                score *= focus_bonus
            
            # Improved Combat Advantage Factor
            advantage_factor = 1.0
            if ally_power > enemy_power * 1.3:
                advantage_factor = 1.2  # More aggressive advantage pursuit
                if ally_swarms >= 3:
                    advantage_factor *= 1.2
            
            # prioritize isolated enemies
            if (enemy_swarms + enemy_ranged) == 1:
                advantage_factor *= 2.0
            elif (ally_swarms + ally_ranged) > (enemy_swarms + enemy_ranged):
                advantage_factor *= 1.2

            score *= advantage_factor
            
            # Health factor
            health_factor = (1 - unit.health) + 1
            score *= health_factor
            
        else:  # Ally unit
            score = base_priority
            
            # Improved Support Priority
            health_factor = (1 - unit.health) + 1
            score *= health_factor
            
            distance_factor = max((1 - unit.distance) + 1, 0.5)
            score *= distance_factor
            
        return score

    def control_logic():
        # Medivac units control logic
        if obs_data.own_unit_type.lower() == 'medivac':
            attack_actions = [a for a in valid_actions if a >= 6]
            # If there are allies
            if obs_data.allies:
                lowest_health_ally = min(obs_data.allies, key=lambda x: x.health)
                # If there are both allies and enemies
                if obs_data.enemies:
                    enemy_in_range = [enemy for enemy in obs_data.enemies if enemy.distance < 1]
                    # Check if any melee ally, if so and last action is not attack, move to center of melee allies
                    melee_ally = [ally for ally in obs_data.allies if ally.unit_type.lower() in ['zealot', 'zergling', 'baneling']]
                    if melee_ally:
                        # Move to center of melee allies
                        ally_x = sum(ally.position[0] for ally in melee_ally) / len(melee_ally)
                        ally_y = sum(ally.position[1] for ally in melee_ally) / len(melee_ally)
                    else:
                        ally_x = sum(ally.position[0] for ally in obs_data.allies) / len(obs_data.allies)
                        ally_y = sum(ally.position[1] for ally in obs_data.allies) / len(obs_data.allies)
                    # Calculate retreat position
                    if len(enemy_in_range) > 0:
                        enemy_center = (sum(e.position[0] for e in enemy_in_range) / len(enemy_in_range),
                                    sum(e.position[1] for e in enemy_in_range) / len(enemy_in_range))
                    else:
                        enemy_center = (sum(e.position[0] for e in obs_data.enemies) / len(obs_data.enemies),
                                    sum(e.position[1] for e in obs_data.enemies) / len(obs_data.enemies))
                    
                    dx = ally_x - enemy_center[0]
                    dy = ally_y - enemy_center[1]

                    distance = (dx ** 2 + dy ** 2) ** 0.5

                    safe_x = ally_x + (dx / abs(dx)) * 2/obs_data.own_sight_range if dx != 0 else ally_x
                    safe_y = ally_y + (dy / abs(dy)) * 2/obs_data.own_sight_range if dy != 0 else ally_y

                    distance = (safe_x ** 2 + safe_y ** 2) ** 0.5
                    criterion = 5/obs_data.own_sight_range
                    if obs_data.last_action >= 6 or len(attack_actions) == 0:
                        target_angle = math.atan2(enemy_center[1], enemy_center[0])
                        safe_angle = math.atan2(ally_y, ally_x)
                        angle_diff = abs(target_angle - safe_angle)
                        if distance > criterion and (math.pi/9 < angle_diff < 17*math.pi/9):
                            path_action = find_path(obs_data, safe_x, safe_y)
                            if path_action:
                                return path_action
                target_scores = {ally.id: score_target(ally) for ally in obs_data.allies}
                # Check if there are same max score targets
                max_score = max(target_scores.values())
                max_score_target_ids = [target_id for target_id, score in target_scores.items() if score == max_score]
                max_score_targets = [ally for ally in obs_data.allies if ally.id in max_score_target_ids]
                closest_ally = min(obs_data.allies, key=lambda x: x.distance)
                if len(max_score_targets) > 1:
                    # Chose the closest target
                    best_target = min(max_score_targets, key=lambda x: x.distance)
                else:
                    best_target = max_score_targets[0]
                if (best_target.id + 6) in valid_actions and 0 < best_target.health < 0.9:
                    return heal(best_target.id)
                elif (closest_ally.id + 6) in valid_actions and 0 < closest_ally.health < 0.9:
                    return heal(closest_ally.id)
                elif (lowest_health_ally.id + 6) in valid_actions and 0 < lowest_health_ally.health < 0.9:
                    return heal(lowest_health_ally.id)
                else:
                    # Move to the target
                    dx = best_target.position[0]
                    dy = best_target.position[1]
                    path_action = find_path(obs_data, dx, dy, target_type=best_target.unit_type.lower())
                    if path_action:
                        return path_action
            # If there are no allies
            else:
                # If there are only enemies
                if obs_data.enemies:
                    enemy_x = sum(e.position[0] for e in obs_data.enemies) / len(obs_data.enemies)
                    enemy_y = sum(e.position[1] for e in obs_data.enemies) / len(obs_data.enemies)
                    target_x = - enemy_x
                    target_y = - enemy_y

                    g_x = target_x * obs_data.own_sight_range + (obs_data.own_position[0] * 32)
                    g_y = target_y * obs_data.own_sight_range + (obs_data.own_position[1] * 32)
                    if not (0 <= g_x <= 32 and 0 <= g_y <= 32):
                        target_x = (0.5 - obs_data.own_position[0]) * 32 / obs_data.own_sight_range
                        target_y = (0.5 - obs_data.own_position[1]) * 32 / obs_data.own_sight_range

                    path_action = find_path(obs_data, target_x, target_y)
                    if path_action:
                        return path_action
        # Melee units control logic
        elif obs_data.own_unit_type.lower() in ['zealot', 'zergling', 'baneling']:
            # If there are enemies
            if obs_data.enemies:
                enemy_in_range = [enemy for enemy in obs_data.enemies if enemy.distance < 1]
                attack_actions = [a for a in valid_actions if a >= 6]

                if len(enemy_in_range) > 0:
                    enemy_center = (sum(e.position[0] for e in enemy_in_range) / len(enemy_in_range),
                                sum(e.position[1] for e in enemy_in_range) / len(enemy_in_range))
                else:
                    enemy_center = (sum(e.position[0] for e in obs_data.enemies) / len(obs_data.enemies),
                                sum(e.position[1] for e in obs_data.enemies) / len(obs_data.enemies))
                if obs_data.allies:
                    # Check if any melee ally, if so and last action is not attack, move to center of melee allies
                    melee_ally = [ally for ally in obs_data.allies if ally.unit_type.lower() in ['zealot', 'zergling', 'baneling']]
                    if melee_ally:
                        # Move to center of melee allies
                        ally_x = sum(ally.position[0] for ally in melee_ally) / len(melee_ally) / 2
                        ally_y = sum(ally.position[1] for ally in melee_ally) / len(melee_ally) / 2

                        safe_x = ally_x 
                        safe_y = ally_y

                        distance = (safe_x ** 2 + safe_y ** 2) ** 0.5
                        criterion = 2/obs_data.own_sight_range
                        if len(attack_actions) == 0 or distance > 0.5:
                            target_angle = math.atan2(enemy_center[1], enemy_center[0])
                            safe_angle = math.atan2(ally_y, ally_x)
                            angle_diff = abs(target_angle - safe_angle)
                            if distance > criterion and (math.pi/9 < angle_diff < 17*math.pi/9 or distance > 0.5):
                                path_action = find_path(obs_data, safe_x, safe_y)
                                if path_action:
                                    return path_action
                            
                # Enhanced cluster detection with dynamic radius
                enemy_clusters = {}
                cluster_centers = {}
                for enemy in obs_data.enemies:
                    nearby_enemies = []
                    center_x, center_y = enemy.position[0], enemy.position[1]
                    
                    # Dynamic cluster radius based on unit type
                    cluster_radius = 0.3 if obs_data.own_unit_type.lower() == 'baneling' else 0.2
                    
                    for other in obs_data.enemies:
                        distance = ((other.position[0] - enemy.position[0])**2 + 
                                (other.position[1] - enemy.position[1])**2)**0.5
                        if distance <= cluster_radius:
                            nearby_enemies.append(other)
                            center_x += other.position[0]
                            center_y += other.position[1]
                    
                    if nearby_enemies:
                        center_x /= len(nearby_enemies)
                        center_y /= len(nearby_enemies)
                        
                    enemy_clusters[enemy.id] = len(nearby_enemies)
                    cluster_centers[enemy.id] = (center_x, center_y)

                # Enhanced target scoring with tactical considerations
                target_scores = {}
                for enemy in obs_data.enemies:
                    base_score = score_target(enemy)
                    
                    # Enhanced cluster bonus for splash damage
                    if obs_data.own_unit_type.lower() == 'baneling':
                        cluster_bonus = 1.5 ** enemy_clusters[enemy.id]
                    else:
                        cluster_bonus = 1.2 ** enemy_clusters[enemy.id]
                    # Calculate final score with all factors
                    target_scores[enemy.id] = (base_score + cluster_bonus)

                # Check if there are same max score targets
                max_score = max(target_scores.values())
                max_score_targets = [enemy for enemy in obs_data.enemies 
                           if target_scores[enemy.id] >= max_score]  # Allow for close scores
                if len(max_score_targets) > 1:
                    # Choose target balancing distance and cluster potential
                    best_target = min(max_score_targets, 
                            key=lambda x: x.distance)
                else:
                    best_target = max_score_targets[0]

                if best_target.can_attack:
                    return attack(best_target.id)
                else:
                    # Move to the target
                    dx = best_target.position[0]
                    dy = best_target.position[1]
                    path_action = find_path(obs_data, dx, dy, target_type=best_target.unit_type.lower())
                    if path_action:
                        return path_action
                    elif attack_actions:
                        attackable_enemies = [enemy for enemy in obs_data.enemies if enemy.can_attack]
                        if obs_data.last_action in attack_actions:
                            return obs_data.last_action
                        if attackable_enemies:
                            return attack(min(attackable_enemies, key=lambda e: e.distance).id)
                        return random.choice(attack_actions)

            # If there are no enemies
            else:
                # If there are only allies
                if obs_data.allies:
                    # Improved melee group formation
                    melee_allies = [ally for ally in obs_data.allies 
                                if ally.unit_type.lower() in ['zealot', 'zergling', 'baneling']]
                    if melee_allies:
                        spacing = 0.1 if obs_data.own_unit_type.lower() == 'baneling' else 0.05
                        # Dynamic group positioning
                        center_x = sum(ally.position[0] for ally in melee_allies) / len(melee_allies)
                        center_y = sum(ally.position[1] for ally in melee_allies) / len(melee_allies)
                        
                        # Calculate spread from center
                        max_spread = max(((ally.position[0] - center_x)**2 + 
                                        (ally.position[1] - center_y)**2)**0.5 
                                    for ally in melee_allies)
                        
                        own_distance = ((center_x)**2 + (center_y)**2)**0.5
                        
                        if own_distance > spacing or max_spread > 0.1:
                            # Move toward center while maintaining minimum spacing
                            adjusted_x = center_x * 0.85  # Slight offset to prevent overcrowding
                            adjusted_y = center_y * 0.85
                            path_action = find_path(obs_data, adjusted_x, adjusted_y)
                            if path_action:
                                return path_action
                    else:
                        ally_x = sum(ally.position[0] for ally in obs_data.allies) / len(obs_data.allies)
                        ally_y = sum(ally.position[1] for ally in obs_data.allies) / len(obs_data.allies)
                        distance = (ally_x ** 2 + ally_y ** 2) ** 0.5
                        if distance > 0.05:
                            dx = ally_x
                            dy = ally_y
                            path_action = find_path(obs_data, dx, dy)
                            if path_action:
                                return path_action
        # Ranged units control logic
        else:
            attack_actions = [a for a in valid_actions if a >= 6]
            # If there are enemies
            if obs_data.enemies:
                # If there are both allies and enemies
                # Calculate retreat position
                enemy_in_range = [enemy for enemy in obs_data.enemies if enemy.distance < 1]
                if len(enemy_in_range) > 0:
                    enemy_center = (sum(e.position[0] for e in enemy_in_range) / len(enemy_in_range),
                                sum(e.position[1] for e in enemy_in_range) / len(enemy_in_range))
                else:
                    enemy_center = (sum(e.position[0] for e in obs_data.enemies) / len(obs_data.enemies),
                                sum(e.position[1] for e in obs_data.enemies) / len(obs_data.enemies))
                if obs_data.allies:
                    # Check if any melee ally, if so and last action is not attack, move to center of melee allies
                    melee_ally = [ally for ally in obs_data.allies if ally.unit_type.lower() in ['zealot', 'zergling', 'baneling']]
                    melee_enemy = [enemy for enemy in enemy_in_range if enemy.unit_type.lower() in ['zealot', 'zergling', 'baneling']]
                    if melee_ally:
                        # Move to center of melee allies
                        ally_x = sum(ally.position[0] for ally in melee_ally) / len(melee_ally)
                        ally_y = sum(ally.position[1] for ally in melee_ally) / len(melee_ally)
                        
                    else:
                        ally_x = sum(ally.position[0] for ally in obs_data.allies) / len(obs_data.allies) / 2
                        ally_y = sum(ally.position[1] for ally in obs_data.allies) / len(obs_data.allies) / 2

                    dx = ally_x - enemy_center[0]
                    dy = ally_y - enemy_center[1]
                    safe_x = ally_x
                    safe_y = ally_y
                    if melee_ally:
                        safe_x = safe_x + (dx / abs(dx)) * 2/obs_data.own_sight_range if dx != 0 else safe_x
                        safe_y = safe_y + (dy / abs(dy)) * 2/obs_data.own_sight_range if dy != 0 else safe_y
                    melee_threaten = False
                    if melee_enemy:
                        closest_melee_enemy = min(melee_enemy, key=lambda x: x.distance)
                        if closest_melee_enemy.distance <= 4/obs_data.own_sight_range:
                            melee_threaten = True
                            dx = safe_x - closest_melee_enemy.position[0]
                            dy = safe_y - closest_melee_enemy.position[1]
                            safe_x = safe_x + (dx / abs(dx)) * 1/obs_data.own_sight_range if dx != 0 else safe_x
                            safe_y = safe_y + (dy / abs(dy)) * 1/obs_data.own_sight_range if dy != 0 else safe_y
                    
                    distance = (safe_x ** 2 + safe_y ** 2) ** 0.5
                    criterion = 4/obs_data.own_sight_range
                    if obs_data.last_action >= 6 or len(attack_actions) == 0 or distance > 0.9:
                        target_angle = math.atan2(enemy_center[1], enemy_center[0])
                        safe_angle = math.atan2(ally_y, ally_x)
                        angle_diff = abs(target_angle - safe_angle)
                        if distance > criterion and ((math.pi/9 < angle_diff < 17*math.pi/9) or melee_threaten or distance > 0.9):
                            path_action = find_path(obs_data, safe_x, safe_y)
                            if path_action:
                                return path_action
                    # Focus fire logic
                    # Count how many allies are attacking each enemy
                    target_counts = {}
                    for ally in obs_data.allies:
                        if ally.last_action >= 6:
                            target_id = ally.last_action - 6
                            target_counts[target_id] = target_counts.get(target_id, 0) + 1
                    # Distance to safe point of each enemy affect target choosing
                    enemy_safe_distance = {enemy.id: ((enemy.position[0] - safe_x) ** 2 + (enemy.position[1] - safe_y) ** 2) ** 0.5 for enemy in obs_data.enemies}
                    # Find best target combining focus fire and threat scoring
                    target_scores = {enemy.id: score_target(enemy) for enemy in obs_data.enemies}
                    for target_id, count in target_counts.items():
                        if target_id in target_scores:
                            target_scores[target_id] += count * 0.5
                    for target_id, scores in target_scores.items():
                        target_scores[target_id] = scores * (1 - enemy_safe_distance[target_id] * 0.3)
                    best_target_id = max(target_scores.items(), key=lambda x: x[1])[0]
                    best_target = next(enemy for enemy in obs_data.enemies if enemy.id == best_target_id)
                    if best_target.can_attack:
                        return attack(best_target_id)
                    else:
                        # Best target is not in shoot range, move to target
                        dx = best_target.position[0]
                        dy = best_target.position[1]

                        # Only move to target if its direction is not conflicting with the safe point
                        # Check if target direction aligns with safe point direction
                        target_angle = math.atan2(dy, dx)
                        safe_angle = math.atan2(ally_y, ally_x)
                        angle_diff = abs(target_angle - safe_angle)
                        # Only move if angle difference is less than 90 degrees
                        if angle_diff < math.pi/9 or angle_diff > 17*math.pi/9 or not melee_ally:
                            if best_target.distance > obs_data.own_shoot_range / obs_data.own_sight_range:
                                path_action = find_path(obs_data, dx, dy, target_type=best_target.unit_type.lower())
                                if path_action:
                                    return path_action
                        if attack_actions:
                            if obs_data.last_action in attack_actions:
                                return obs_data.last_action
                            attackable_enemies = [enemy for enemy in obs_data.enemies if enemy.can_attack]
                            closest_enemy = min(
                                [enemy for enemy in attackable_enemies],
                                key=lambda enemy: enemy.distance,
                                default=None,
                            )
                            if closest_enemy and closest_enemy.can_attack:
                                return attack(closest_enemy.id)
                            return random.choice(attack_actions)
                        else:
                            if distance > criterion:
                                path_action = find_path(obs_data, safe_x, safe_y)
                                if path_action:
                                    return path_action
                        
                # If there are only enemies
                else:
                    # Closest enemy as target
                    closest_enemy = min(
                        [enemy for enemy in obs_data.enemies],
                        key=lambda enemy: enemy.distance,
                        default=None,
                    )
                    # No allies, kitting melee enemies
                    if closest_enemy.unit_type.lower() in ['zealot', 'zergling', 'baneling']:
                        if closest_enemy.distance <= 4 / obs_data.own_sight_range and obs_data.last_action >= 6:
                            enemy_x = sum(e.position[0] for e in obs_data.enemies) / len(obs_data.enemies)
                            enemy_y = sum(e.position[1] for e in obs_data.enemies) / len(obs_data.enemies)
                            target_x = - enemy_x
                            target_y = - enemy_y

                            g_x = target_x * obs_data.own_sight_range + (obs_data.own_position[0] * 32)
                            g_y = target_y * obs_data.own_sight_range + (obs_data.own_position[1] * 32)
                            if not (0 <= g_x <= 32 and 0 <= g_y <= 32):
                                target_x = (0.5 - obs_data.own_position[0]) * 32 / obs_data.own_sight_range
                                target_y = (0.5 - obs_data.own_position[1]) * 32 / obs_data.own_sight_range

                            path_action = find_path(obs_data, target_x, target_y)
                            if path_action:
                                return path_action
                        if closest_enemy.can_attack:
                            return attack(closest_enemy.id)
                    else:
                        # No melee enemies, highest priority enemy as target
                        target_scores = {enemy.id: score_target(enemy) for enemy in obs_data.enemies}
                        # Check if there are same max score targets
                        max_score = max(target_scores.values())
                        max_score_target_ids = [target_id for target_id, score in target_scores.items() if score == max_score]
                        max_score_targets = [enemy for enemy in obs_data.enemies if enemy.id in max_score_target_ids]
                        if len(max_score_targets) > 1:
                            # Chose the closest target
                            best_target = min(max_score_targets, key=lambda x: x.distance)
                        else:
                            best_target = max_score_targets[0]
                        if best_target.can_attack:
                            return attack(best_target.id)
                        else:
                            # Best target is not in shoot range, move to target
                            dx = best_target.position[0]
                            dy = best_target.position[1]
                            if best_target.distance > obs_data.own_shoot_range / obs_data.own_sight_range:
                                path_action = find_path(obs_data, dx, dy, target_type=best_target.unit_type.lower())
                                if path_action:
                                    return path_action
                                elif attack_actions:
                                    if obs_data.last_action in attack_actions:
                                        return obs_data.last_action
                                    attackable_enemies = [enemy for enemy in obs_data.enemies if enemy.can_attack]
                                    closest_enemy = min(
                                        [enemy for enemy in attackable_enemies],
                                        key=lambda enemy: enemy.distance,
                                        default=None,
                                    )
                                    if closest_enemy and closest_enemy.can_attack:
                                        return attack(closest_enemy.id)
                                    return random.choice(attack_actions)
            # If there are no enemies
            else:
                # If there are only allies
                if obs_data.allies:
                    # Check if any melee ally
                    melee_ally = [ally for ally in obs_data.allies if ally.unit_type.lower() in ['zealot', 'zergling', 'baneling']]
                    if melee_ally:
                        # Move to center of melee allies
                        melee_ally_x = sum(ally.position[0] for ally in melee_ally) / len(melee_ally)
                        melee_ally_y = sum(ally.position[1] for ally in melee_ally) / len(melee_ally)
                        dx = melee_ally_x
                        dy = melee_ally_y
                        distance = (dx ** 2 + dy ** 2) ** 0.5
                        if distance > 0.05:
                            path_action = find_path(obs_data, dx, dy)
                            if path_action:
                                return path_action
                    else:
                        # No melee allies, move to target ally
                        ally_x = sum(ally.position[0] for ally in obs_data.allies) / len(obs_data.allies)
                        ally_y = sum(ally.position[1] for ally in obs_data.allies) / len(obs_data.allies)
                        distance = (ally_x ** 2 + ally_y ** 2) ** 0.5
                        if distance > 0.05:
                            dx = ally_x
                            dy = ally_y
                            path_action = find_path(obs_data, dx, dy)
                            if path_action:
                                return path_action
        return default_action(obs)

    return control_logic()
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
