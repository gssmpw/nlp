\section{Related Work}
\label{related work}
\subsection{Agents in the StarCraft Multi-Agent Challenge}
SMAC \cite{10.5555/3306127.3332052}, a predominant cooperative MARL benchmark based on the StarCraft II real-time strategy game \cite{vinyals2017starcraftiinewchallenge}, focuses on decentralized micromanagement scenarios where each unit operates under decentralized execution with partial observability to defeat enemy units controlled by Starcraft II’s built-in AI opponent. Previous research in SMAC resort to MARL which can be divided into two categories: 1) Online MARL: One line of representative research is value decomposition (VD) \cite{JMLR:v21:20-081,wang2021qplex,pmlr-v97-son19a}, which decomposes the centralized action-value function into individual utility functions. On the other hand, multi-agent policy gradient (MAPG) methods \cite{NEURIPS2022_9c1535a0,kuba2022trustregionpolicyoptimisation,liu2024maximum,Li_Liu_Zhang_Wei_Niu_Yang_Liu_Ouyang_2023,NEURIPS2022_69413f87,hu2024learning,na2024efficient} extend single-agent policy gradient algorithm to multi-agent with coordination modeling. Researches such as MAPPO \cite{NEURIPS2022_9c1535a0}, HAPPO \cite{kuba2022trustregionpolicyoptimisation}, and HASAC \cite{liu2024maximum} combine trust region and maximum entropy with MARL in a non-trivial way respectively. To encourage coordination, communication methods \cite{hu2024learning,lo2024learning}, sequential modeling methods \cite{Li_Liu_Zhang_Wei_Niu_Yang_Liu_Ouyang_2023,NEURIPS2022_69413f87}, and cooperative exploration methods \cite{10.5555/3454287.3454971,na2024efficient} have been proposed. 2) Offline MARL: Recent efforts such as MADT \cite{meng2023offline}, ODIS \cite{zhang2022discovering}, and MADiff \cite{zhu2025madiffofflinemultiagentlearning} leverage data-driven training via pre-collected offline datasets to enhance policy training efficiency. However, the near-optimal performance of these existing approaches on SMAC highlights the benchmark's limited stochasticity and partial observability. To address these limitations, SMACv2 \cite{10.5555/3666122.3667756} introduces more complexity to necessitate decentralized closed-loop control policies. There have been some recent attempts \cite{Li_Zhao_Wu_Pajarinen_2024,mcclellan2024boostingsampleefficiencygeneralization,10.5555/3545946.3598961,li2024agentmixermultiagentcorrelatedpolicy} to evaluate MARL algorithms on SMACv2, and the results confirm the complexity. However, current learning-based multi-agent methods are computationally inefficient and non-interpretable. In the quest to find methods that are sample-efficient and interpretable, LLM-SMAC \cite{deng2024newapproachsolvingsmac} leverage LLMs to generate centralized decision tree code under global information in an open-loop framework. Unlike prior works, COMPASS integrates a Vision-Language Model (VLM) with each agent in a decentralized closed-loop manner under partial observability, improving both real-world applicability and scalability.

\subsection{LLM-based Multi-Agent System}
Based on the inspiring capabilities of LLMs, such as zero-shot planning and complex reasoning \cite{10.5555/3600270.3601883,10.5555/3666122.3667509,10.5555/3600270.3602070,Besta_Blach_Kubicek_Gerstenberger_Podstawski_Gianinazzi_Gajda_Lehmann_Niewiadomski_Nyczyk_Hoefler_2024}, embodied single-agent researches have demonstrated the effectiveness of LLMs in solving complex long-horizon tasks \cite{wang2024voyager,yao2023react,NEURIPS2023_1b44b878,ichter2022do,ma2024largelanguagemodelsplay,wang2024executablecodeactionselicit,tan2024cradleempoweringfoundationagents}. 
Despite significant advances in single-agent applications, developing real-world multi-agent systems with foundation models remains challenging, primarily due to the nature of decentralized control under partial observability in multi-agent settings \cite{10.5555/2283396.2283451}. 
Most prior efforts \cite{10610855,zhang2024buildingcooperativeembodiedagents,10.1609/aaai.v38i16.29710,nayak2025llamarlonghorizonplanningmultiagent,gong-etal-2024-mindagent} leverage a hierarchical framework with components like perception, communication, planning, execution, and memory to build multi-agent systems with collective behaviors. These approaches can be roughly classified into two groups. 1) Centralized plan: MindAgent \cite{gong-etal-2024-mindagent} adopts a centralized planning scheme with a pre-defined oracle in a fully observable multi-agent game. LLaMAR \cite{nayak2025llamarlonghorizonplanningmultiagent} employs LLMs to manage long-horizon tasks in partially observable environments without assumptions about access to perfect low-level policies. 2) Decentralized plan: ProAgent \cite{10.1609/aaai.v38i16.29710} introduces Theory of Mind (ToM), enabling agents to reason about others’ mental states. RoCo \cite{10610855} and CoELA \cite{zhang2024buildingcooperativeembodiedagents} assign separate LLMs to each embodied agent for collaboration with communication. However, RoCo and CoELA assume a skill library with a low-level heuristic controller, which is impractical in real-world applications. Moreover, RoCo's open-loop plan-and-execute paradigm fails to incorporate environmental feedback during decision-making. In contrast, our work does not assume any pre-defined low-level controller and generates code-based action through VLMs in a closed-loop manner.