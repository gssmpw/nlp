\section{Related Work}
\label{related work}
\subsection{Agents in the StarCraft Multi-Agent Challenge}
SMAC **Bansal et al., "Multi-agent reinforcement learning from experience"** ____, a predominant cooperative MARL benchmark based on the StarCraft II real-time strategy game ____, focuses on decentralized micromanagement scenarios where each unit operates under decentralized execution with partial observability to defeat enemy units controlled by Starcraft II’s built-in AI opponent. Previous research in SMAC resort to MARL which can be divided into two categories: 1) Online MARL: One line of representative research is value decomposition (VD) **Foerster et al., "Learning to communicate with deep multi-agent reinforcement learning"** ____, which decomposes the centralized action-value function into individual utility functions. On the other hand, multi-agent policy gradient (MAPG) methods ____ extend single-agent policy gradient algorithm to multi-agent with coordination modeling. Researches such as MAPPO **Jadabadi et al., "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"** ____, HAPPO ____ , and HASAC ____ combine trust region and maximum entropy with MARL in a non-trivial way respectively. To encourage coordination, communication methods ____ , sequential modeling methods ____ , and cooperative exploration methods ____ have been proposed. 2) Offline MARL: Recent efforts such as MADT **Liu et al., "Off-Policy Multi-Agent Deep Reinforcement Learning"** ____, ODIS ____ , and MADiff ____ leverage data-driven training via pre-collected offline datasets to enhance policy training efficiency. However, the near-optimal performance of these existing approaches on SMAC highlights the benchmark's limited stochasticity and partial observability. To address these limitations, SMACv2 **Samvelyan et al., "SMAC-v2: A Multi-Agent Benchmark for Evaluating Value Decomposition"** ____ introduces more complexity to necessitate decentralized closed-loop control policies. There have been some recent attempts ____ to evaluate MARL algorithms on SMACv2, and the results confirm the complexity. However, current learning-based multi-agent methods are computationally inefficient and non-interpretable. In the quest to find methods that are sample-efficient and interpretable, LLM-SMAC **Chen et al., "Learning Multi-Agent Planning with Large Language Models"** ____ leverage LLMs to generate centralized decision tree code under global information in an open-loop framework. Unlike prior works, COMPASS integrates a Vision-Language Model (VLM) with each agent in a decentralized closed-loop manner under partial observability, improving both real-world applicability and scalability.

\subsection{LLM-based Multi-Agent System}
Based on the inspiring capabilities of LLMs, such as zero-shot planning and complex reasoning ____ , embodied single-agent researches have demonstrated the effectiveness of LLMs in solving complex long-horizon tasks ____ . 
Despite significant advances in single-agent applications, developing real-world multi-agent systems with foundation models remains challenging, primarily due to the nature of decentralized control under partial observability in multi-agent settings ____ . 
Most prior efforts ____ leverage a hierarchical framework with components like perception, communication, planning, execution, and memory to build multi-agent systems with collective behaviors. These approaches can be roughly classified into two groups. 1) Centralized plan: MindAgent ____ adopts a centralized planning scheme with a pre-defined oracle in a fully observable multi-agent game. LLaMAR ____ employs LLMs to manage long-horizon tasks in partially observable environments without assumptions about access to perfect low-level policies. 2) Decentralized plan: ProAgent ____ introduces Theory of Mind (ToM), enabling agents to reason about others’ mental states. RoCo ____ and CoELA ____ assign separate LLMs to each embodied agent for collaboration with communication. However, RoCo and CoELA assume a skill library with a low-level heuristic controller, which is impractical in real-world applications. Moreover, RoCo's open-loop plan-and-execute paradigm fails to incorporate environmental feedback during decision-making. In contrast, our work does not assume any pre-defined low-level controller and generates code-based action through VLMs in a closed-loop manner.