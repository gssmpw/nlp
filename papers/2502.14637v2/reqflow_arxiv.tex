\PassOptionsToPackage{dvipsnames,table,xcdraw}{xcolor}
\documentclass[11pt]{article}
% \documentclass{article} % For LaTeX2e
% \usepackage{iclr2023_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\usepackage[margin=1in,letterpaper]{geometry} % decreases margins
% \usepackage{times}
\usepackage{amsmath, amsfonts, bm}
\usepackage{algorithm, algorithmic}
% \input{math_commands.tex}
\usepackage[skip=6pt plus1pt]{parskip}
\setlength{\parindent}{0pt}


\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsopn}


\usepackage{booktabs}       % professional-quality tables



\usepackage{caption}
% \usepackage{color}

\usepackage{enumitem}
\usepackage{float}

\usepackage{graphicx}
%\usepackage{times}

\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{multirow}

\usepackage{rotating}
\usepackage{siunitx}
\usepackage{stfloats}
\usepackage{subcaption}


\usepackage{thmtools,thm-restate}
\usepackage{threeparttable}

\usepackage{verbatim}

%adding by myself
\usepackage{wrapfig}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
\usepackage{multirow}
% \usepackage{times}
\usepackage{soul}
\usepackage{tcolorbox}
% \usepackage{natbib}
\usepackage{threeparttable}
% reqflow
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{overpic} 
\usepackage{float}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{makecell}
\usepackage{pifont}
% \usepackage[table,xcdraw]{xcolor}
\usepackage[textsize=tiny]{todonotes}
\usepackage{makecell}
\usepackage{threeparttable}
% \usepackage[ruled,vlined,linesnumbered]{algorithm2e} 
%usepackage{algpseudocode}
\usepackage{hyperref}
% \makeatletter
% \renewcommand{\ref}[1]{\textup{(\ref{#1})}}
% \makeatother

% \bibpunct{}{}{,}{a}{}{,}
% \let\oldcite\cite
% \renewcommand{\cite}[1]{(\oldcite{#1})}

\title{ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Angxiao Yue$^{1}$\quad Zichong Wang$^{2}$ \quad Hongteng Xu$^{1,3}$\thanks{Hongteng Xu is the corresponding author of this work.} \\
$^1$Gaoling School of Artificial Intelligence, Renmin University of China\\
$^2$School of Statistics, Renmin University of China\\
$^3$Beijing Key Laboratory of Big Data Management and Analysis Methods\\
\texttt{hongtengxu@ruc.edu.cn}\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\xmark}{\ding{55}}
\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}
\definecolor{top1}{rgb}{0.68, 0.85, 0.9}
\definecolor{top2}{rgb}{0.78, 0.93, 0.96}
\definecolor{top3}{rgb}{0.88, 0.97, 0.99}
%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{abstract}
\noindent Protein backbone generation plays a central role in \emph{de novo} protein design and is significant for many biological and medical applications.
Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency.
In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. 
In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format.
We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. 
Experiments show that ReQFlow achieves state-of-the-art performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37$\times$ faster than RFDiffusion and 62$\times$ faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency. The code is available at \url{https://github.com/AngxiaoYue/ReQFlow}.
\end{abstract}

\section{Introduction}
\emph{De novo} protein design~\cite{ingraham2023illuminating, lin2023generating} aims to design rational proteins from scratch with specific properties or functions, which has many biological and medical applications, such as developing novel enzymes for biocatalysis~\cite{kelly2020transaminases} and discovering new drugs for diseases~\cite{teague2003implications,silva2019novo}.
This task is challenging due to the extremely huge design space of proteins.
For simplifying the task, the mainstream \emph{de novo} protein design strategy takes protein backbone generation (i.e., generating 3D protein structures without side chains) as the key step that largely determines the rationality and basic properties of designed proteins. 

% Recently, deep generative models have significantly advanced the progress of \emph{de novo} protein design~\cite{ingraham2023illuminating, lin2023generating}. 
% A widely adopted approach focuses on generating protein backbones, without sequences or side chains. 
% Among these backbone generators, diffusion~\cite{ho2020denoising, watson2023novo, lin2024out} and flow-based~\cite{yim2023se,yim2023fast, bose2023se, huguet2024sequence, lipman2022flow} generative models have emerged as prominent methodologies. 
% State-of-the-art methods typically represent a protein with $N$ residues as a collection of $N$ $\text{SE}(3)$ frames, where each frame encodes the translation and rotation of an individual residue. 
% In this approach, the entire protein backbone is described by the group product $\text{SE}(3)^N$. 

% However, the current methods often struggle to generate high-quality proteins and suffer from computational inefficiency. 
% In particular, 
% For diffusion-based models, FrameDiff~\cite{yim2023se} requires a large number of sampling steps yet demonstrates relatively poor designability. Genie2~\cite{lin2024out} and RFDiffusion~\cite{watson2023novo} produce highly designable proteins but the inference is prohibitively expensive. 
% Flow-based models~\cite{bose2023se,yim2023fast} remove stochasticity from the sampling path, resulting in a straighter one that requires fewer steps and thereby improves inference speed. 
% However, their performance remains inferior to Genie2 and RFDiffusion and we empirically find that the rotation matrix representation used in these flow-based methods is numerically unstable when handling large rotation angles. 
% Notably, all the aforementioned models struggle to generate long protein backbones with satisfactory designability.
\begin{figure}[t]
    \centering
    \includegraphics[height=7cm]{figures/scheme_reqflow.pdf}
    \caption{An illustration of our rectified quaternion flow matching method, in which each residue is represented as a frame associated with a local transformation.}\label{fig:scheme}
\end{figure}

Focusing on protein backbone generation, many deep generative models, especially those diffusion and flow-based models~\cite{ho2020denoising, watson2023novo,lin2024out,yim2023se,yim2023fast,bose2023se,huguet2024sequence, lipman2022flow}, have been proposed as potential solutions. 
However, these models often generate protein backbones with poor designability (the key metric indicating the quality of generated protein backbones), especially for proteins with long residue chains. 
\begin{wrapfigure}{r}{0.48\textwidth}
  \begin{center}
    \includegraphics[height=7.5cm]{figures/log_speed_10_v2.pdf}
  \end{center}
  \caption{Comparisons for each method. For each model, the size of its circle indicates the model size, and the location of the circle's centroid indicates the logarithm of the average inference time when generating a protein backbone with length $N=300$ and the Fraction score of designable protein backbones.
  For QFlow and ReQFlow, we set the sampling step $L\in\{20, 50, 500\}$, respectively.}\label{fig:cmp}
\end{wrapfigure}
In addition, diffusion or flow models often require many sampling steps to generate protein backbones, resulting in high computational complexity and long inference time.
As a result, the above drawbacks on generation quality and computational efficiency limit these models in practical large-scale applications.





To overcome the above challenges, we propose a novel rectified quaternion flow (ReQFlow) matching method, achieving fast and high-quality protein backbone generation. 
As illustrated in Figure~\ref{fig:scheme}, our method learns a model to generate a local 3D translation and a 3D rotation respectively from random noise for each residue in a protein chain. 
Different from existing models, the proposed model represents each rotation as a unit quaternion and constructs its quaternion flow in $\text{SO}(3)$ by spherical linear interpolation (SLERP) in an exponential format~\cite{sola2017quaternion}, which can be learned by our quaternion flow (QFlow) matching strategy.
Furthermore, given a trained QFlow model, we leverage the rectified flow technique in~\cite{liu2022rectified}, re-training the model based on the paired noise and protein backbones generated by the model itself.
The rectified QFlow (i.e., ReQFlow) model leads to non-crossing sampling paths in $\mathbb{R}^3$ and $\text{SO}(3)$, respectively, when generating translations and rotations. 
As a result, we can apply fewer sampling steps to accelerate the generation process significantly.

% which guarantees the numerical stability and computational efficiency of rotation generation. 

% and improves the designability of generated protein backbones significantly, especially for the proteins with long residue chains.
% This reflow operation straightens the sampling path in a non-crossing manner, resulting in accelerated inference speed and improved designability, especially for long protein backbone generation.

% In our work, we introduce a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. 
% We generate a local translation and a 3D rotation for each residue from noise distributions. 
% The rotation generator represents each 3D rotation as a unit quaternion and constructs the flow by spherical linear interpolation (SLERP) in an exponential format. 
% Additionally, we rectify the quaternion flow by leveraging the paired noises and samples generated by the model itself. 
% This reflow operation straightens the sampling path in a non-crossing manner, resulting in accelerated inference speed and improved designability, especially for long protein backbone generation.


% Our contributions can be summarized as: (1). We conduct an empirical analysis of the numerical instability issue in the rotation matrix representation, which limits the performance of existing flow-based models. (2). 
% We construct a quaternion flow with quaternion representation and spherical linear interpolation (SLERP) in an exponential format,  ensuring numerical stability. 
% (3). We rectify the quaternion flow and propose ReQFlow which is capable of generating highly designable proteins with lightspeed. 


We demonstrate the rationality and effectiveness of ReQFlow compared to existing diffusion and flow-based methods.
In particular, thanks to the exponential format SLERP, ReQFlow is learned and implemented with guaranteed numerical stability and computational efficiency, especially when the rotation angle is close to $0$ or $\pi$. 
Experimental results demonstrate that ReQFlow achieves state-of-the-art performance, generating high-quality protein backbones with significantly reduced inference time. 
Furthermore, ReQFlow consistently maintains effectiveness and efficiency in generating long-chain protein backbones (e.g., the protein backbones with over 500 residues), where all baseline models suffer severe performance degradation.
As shown in Figure~\ref{fig:cmp}, ReQFlow outperforms existing methods and generates high-quality protein backbones, whose designability Fraction score is 0.972 when sampling 500 steps and 0.912 when merely sampling 50 steps.
% It outperforms strong competitors like RFDiffusion and Genie2 while being 37$\times$ faster than RFDiffusion and 62$\times$ faster than Genie2. 

\section{Related Work and Preliminaries}
\subsection{Protein Backbone Generation}
Many diffusion and flow-based methods have been proposed to generate protein backbones. 
These methods often parameterize protein backbones like AlphaFold2~\cite{jumper2021highly} does, representing each protein's residues as a set of $\text{SE}(3)$ frames.
Accordingly, FrameDiff~\cite{yim2023se} generates protein backbones by two independent diffusion processes, generating the corresponding frames' local translations and rotations, respectively.
Following the same framework, flow-based methods like FrameFlow~\cite{yim2023fast} and FoldFlow~\cite{bose2023se} replace the stochastic diffusion processes with deterministic flows. 

For the above methods, many efforts have been made to modify their model architectures and improve data representations, e.g., the Clifford frame attention module in GAFL~\cite{wagner2024generating} and the asymmetric protein representation module in Genie~\cite{lin2023generating} and Genie2~\cite{lin2024out}. 
In addition, some methods leverage large-scale pre-trained models to improve generation quality.
For example, RFDiffusion~\cite{watson2023novo} utilizes the pre-trained RoseTTAFold~\cite{baek2021accurate} as the backbone model.
FoldFlow2~\cite{huguet2024sequence} improves FoldFlow by using a protein large language model for residue sequence encoding. Taking scaling further and adopting a different architectural approach, Prote\'{i}na~\cite{geffner2025proteina} developed a large-scale, flow-based generative model using a non-equivariant transformer operating directly on C-alpha coordinates.

Currently, the above methods often suffer the conflict on computational efficiency and generation quality.
The state-of-the-art methods like RFDiffusion~\cite{watson2023novo} and Genie2~\cite{lin2024out} need long inference time to generate protein backbones with reasonable quality.
FrameFlow~\cite{yim2023fast} and GAFL~\cite{wagner2024generating} significantly improves inference speed while lags behind RFDiffusion and Genie2 in protein backbone quality. 
Moreover, all the methods suffer severe performance degradation when generating long-chain protein backbones. 
These limitations motivate us to develop the proposed ReQFlow, improving the current flow-based methods and generating protein backbones efficiently with satisfactory designability.




% FrameDiff~\cite{yim2023se} defines the diffusion process over a set of $\text{SE}(3)$ frames and independently generates translations and rotations of each frame. 
% This decoupling method is widely adopted in the following works~\cite{yim2023fast, bose2023se, wagner2024generating}. 
% % However, FrameDiff requires a large number of sampling steps yet demonstrates relatively poor designability.
% RFDiffusion~\cite{watson2023novo} utilizes the pre-trained protein structure network RoseTTAFold~\cite{baek2021accurate} to generate high-quality protein backbones.
% Genie~\cite{lin2023generating} asymmetrically represents protein structures, using Gaussian noising forward and SE(3)-equivariant attention backward.
% Genie2~\cite{lin2024out} extends Genie through architectural innovations and massive data
% augmentation.

% Flow-based models~\cite{bose2023se,yim2023fast,wagner2024generating,huguet2024sequence} have gained popularity for simplifying the sampling path by removing stochasticity, which leads to faster inference and improved design quality.
% Frameflow~\cite{yim2023fast} and FoldFlow~\cite{bose2023se} extend FrameDiff to the flow matching framework. 
% FoldFlow2~\cite{huguet2024sequence} presents new network architectures including a protein large language model as a sequence encoder and is trained on a larger dataset containing high-quality synthetic structures. 
% GAFL~\cite{wagner2024generating} introduces an extension of the invariant point attention (IPA) architecture, representing residues in the projective geometric algebra.
% % However, their performance remains inferior to Genie2 and RFDiffusion and we empirically find that the rotation matrix representation used in these flow-based methods is numerically unstable when handling large rotation angles. 
% % Notably, all the aforementioned models struggle to generate long protein backbones with satisfactory designability.

% For the state-of-the-art diffusion and flow-based models, RFDiffusion~\cite{watson2023novo} and Genie2~\cite{lin2024out} produce high-quality protein backbones but the inference is prohibitively extensive.
% FrameFlow~\cite{yim2023fast} and GAFL~\cite{wagner2024generating} significantly improves inference speed while lags behind RFDiffusion and Genie2 in protein backbone quality. FoldFlow2~\cite{huguet2024sequence} achieves the best design performance and inference speed among these models but still struggles to generate long protein backbones with satisfactory designability.
% % ET-Flow~\cite{hassan2024flow} proposed a flow matching approach to generate low-energy molecular conformations. Compared to diffusion models~\cite{xu2022geodiff, jing2022torsional}, ET-Flow notably enhances precision and physical validity while achieving reduced model weight and accelerated inference speed. 
% % This demonstrates the potential of flow matching methods in 3D structure generation, applicable to both molecules and proteins.


\subsection{Quaternion Algebra and Its Applications}
The proposed ReQFlow is designed based on quaternion algebra~\cite{dam1998quaternions,zhu2018quaternion}.
Mathematically, quaternion is an extension of complex numbers into four-dimensional space, consists of one real component and three orthogonal imaginary components. 
A quaternion is formally expressed as $q = s + x\texttt{i} + y\texttt{j} + z\texttt{k} \in \mathbb{H}$, where $\mathbb{H}$ denotes the quaternion domain, and $s, x, y, z \in \mathbb{R}$. 
The imaginary components $\{\texttt{i}, \texttt{j}, \texttt{k}\}$ satisfy $\texttt{i}^2 = \texttt{j}^2 = \texttt{k}^2 = \texttt{ijk} = -1$. 
Each $q\in\mathbb{H}$ can be equivalently represented as a vector $\bm{q} = [s, \bm{u}^\top]^\top\in\mathbb{R}^4$, where $\bm{u}^\top = [x, y, z]^\top$. 
Given $\bm{q}_1 = [s_1, \bm{u}_1^\top]^\top$ and $\bm{q}_2 = [s_2, \bm{u}_2^\top]^\top$, their multiplication is achieved by Hamilton product, i.e., 
\begin{equation}\label{eq:hamilton}
\bm{q}_1 \otimes \bm{q}_2 = 
\begin{bmatrix} 
s_1s_2 - \bm{u}_1^{\top}\bm{u}_2 \\ 
s_1\bm{u}_2 + s_2\bm{u}_1 + \bm{u}_1 \times \bm{u}_2 
\end{bmatrix},
\end{equation}
where $\times$ denotes the cross product. 
% Note that, Hamilton product is non-commutative, i.e., $\bm{q}_1 \otimes \bm{q}_2 \neq \bm{q}_2 \otimes \bm{q}_1 $. 


Quaternion is a powerful tool to describe 3D rotations. 
For a 3D rotation in the axis-angle formulation, i.e., $\bm{\omega} = \phi \bm{u} \in \mathbb{R}^3$, where the unit vector $\bm{u}$ and the scalar $\phi$ denote the rotation axis and angle, respectively, we can convert it to a unit quaternion by an exponential map~\cite{sola2017quaternion}:
\begin{equation}\label{eq:exp_map}
    \bm{q} = \exp\Bigl(\frac{1}{2} \bm{\omega}\Bigr) = \Bigl[\cos\frac{\phi}{2}, \sin\frac{\phi}{2}\bm{u}^\top\Bigr]^\top\in\mathbb{S}^3,
\end{equation}
where $\mathbb{S}^3=\{\bm{q}\in\mathbb{R}^4~|~\|\bm{q}\|_2=1\}$ is the 4D hypersphere.
The conversion from a unit quaternion to an angle-axis representation is achieved by a logarithmic map:
\begin{equation}\label{eq:log_map}
    \bm{\omega} = 2\log(\bm{q}).
\end{equation}
Suppose that we rotate a point $\bm{v}_1\in\mathbb{R}^3$ to $\bm{v}_2$ by $\bm{\omega}$, we can equivalently implement the operation by
\begin{equation}\label{eq:rot_q}
    \bm{v}_2 = \text{Im}(\bm{q} \otimes [0, \bm{v}_1^\top]^\top \otimes \bm{q}^{-1}),
\end{equation}
where $\bm{q}^{-1}=[\cos\frac{\phi}{2}, -\sin\frac{\phi}{2}\bm{u}^\top]^{\top}$ is the inverse of $\bm{q}$ and ``$\text{Im}(\cdot)$'' denotes the imaginary components of a quaternion (i.e., the last three elements of the corresponding 4D vector).
The quaternion-based rotation representation in Eq.~\ref{eq:rot_q} offers several advantages, including compactness, computational efficiency, and avoidance of gimbal lock~\cite{hemingway2018perspectives}, which has been widely used in skeletal animation~\cite{shoemake1985animating}, robotics~\cite{pervin1982quaternions}, and virtual reality~\cite{kuipers1999quaternions}.

Besides computer graphics, some quaternion-based machine learning models have been proposed for other tasks, e.g., image processing~\cite{xu2015vector,zhu2018quaternion} and structured data (e.g., graphs and point clouds) analysis~\cite{zhang2020quaternion,zhao2020quaternion}.
% The representative works include quaternion sparse coding~\cite{xu2015vector} and quaternion convolution\cite{zhu2018quaternion} for image processing, quaternion neural network~\cite{zhang2020quaternion} and quaternion message passing~\cite{zhao2020quaternion} for graphs and point clouds modeling, and so on.
Recently, some quaternion-based models have been developed for scientific problems, e.g., the quaternion message passing~\cite{yue2024plug} for molecular conformation representation and the quaternion generative models for molecule generation~\cite{kohler2023rigid,guo2025assembleflow}.
However, the computational quaternion techniques are seldom considered in protein-related tasks.
Our work fill this blank, demonstrating the usefulness of quaternion algebra in protein backbone generation.

% AssembleFlow~\cite{guo2025assembleflow} represents rotations in the inertial frames for assembling molecules, it constructs the quaternion-based flow by using spherical linear interpolation (SLERP) in an additive form.

\section{Proposed Method}


\subsection{Protein Backbone Parameterization}\label{sec:backbone}
\label{sec:parameterization}
We parameterize the protein backbone following~\cite{jumper2021highly,yim2023se,yim2023fast,bose2023se}. 
As illustrated in Figure~\ref{fig:scheme}, each residue is represented as a frame, where the frame encodes a rigid transformation starting from the idealized coordinates of four heavy atoms: $[\mathrm{N}^{\ast}, \mathrm{C}_\alpha^{\ast}, \mathrm{C}^{\ast},\mathrm{O}^{\ast}] \in \mathbb{R}^{3\times 4}$. 
In this representation, $\mathrm{C}_\alpha^{\ast} = [0, 0, 0]^{\top}$ is placed at the origin, and the transformation incorporates experimental bond angles and lengths~\cite{engh2012structure}. 
We can derive each residue's frame by 
\begin{equation}\label{eq:frame}
    [\mathrm{N}^{i}, \mathrm{C}_\alpha^{i}, \mathrm{C}^{i}, \mathrm{O}^{i}] = \mathrm{T}^{i} \circ [\mathrm{N}^{\ast}, \mathrm{C}_\alpha^{\ast}, \mathrm{C}^{\ast}, \mathrm{O}^{\ast}],
\end{equation}
where $\mathrm{T}^{i} \in \text{SE}(3)$ is the local orientation-preserving rigid transformation mapping the idealized frame to the frame of the $i$-th residue. 
In this study, we represent $\mathrm{T}^i=(\bm{x}^i, \bm{q}^i)$, where $\bm{x}^i \in \mathbb{R}^3$ represents the 3D translation and a unit quaternion $\bm{q}^i \in \mathbb{S}^3$, which double-covers $\text{SO}(3)$, represents a 3D rotation. 
According to Eq.~\ref{eq:rot_q}, the action of $\mathrm{T}^i$ on a coordinate $\bm{v}\in\mathbb{R}^3$ can be implemented as
\begin{equation}
    \mathrm{T}^i \circ \bm{v} = \bm{x}^i + \text{Im}(\bm{q} \otimes [0, \bm{v}^\top]^\top \otimes \bm{q}^{-1}).
\end{equation}
Note that, for protein backbone generation, we can use the planar geometry of backbone to impute the coordinate of the oxygen atom $\mathrm{O}^{i}$~\cite{yim2023fast,watson2023novo}, so we do not need to parameterize the rotation angle of the bond ``$\mathrm{C}_\alpha-\mathrm{C}$''.
As a result, for a protein backbone with $N$ residues, we have a collection of $N$ frames, resulting in the parametrization set $\Theta = \{\mathrm{T}^{i}\}_{i=1}^{N}$.
Therefore, we can formulate the protein backbone generation problem as modeling and generating $\{\mathrm{T}^{i}\}_{i=1}^{N}$ automatically.
% Besides, to construct the backbone oxygen atom $\mathrm{O}^{i}$, we rotate the bond ``$\mathrm{C}_\alpha-\mathrm{C}$'' using an additional rotation angle $\psi^i$. 
% As a result, for a protein backbone with $N$ residues, we have a collection of $N$ frames, resulting in the parametrization set $\Theta = \{\mathrm{T}^{i}, \psi^i\}_{i=1}^{N}$. 
% Therefore, we can formulate the protein backbone generation problem as modeling and generating $\{\mathrm{T}^{i}, \psi^i\}_{i=1}^{N}$ automatically.

% This approach allows the independent treatment of $\text{E}(3)$ and $\text{SO}(3)$, offering simplicity and enabling the construction of easily optimized interpolation paths within their respective spaces. 

\subsection{Quaternion Flow Matching}\label{sec:qflow}

We decouple the translation and rotation of each frame, establishing two independent flows in $\mathbb{R}^3$ and $\text{SO}(3)$, respectively. 
Without the loss of generality, we define these two flows in the time interval $[0, 1]$. 
When $t=0$, we sample the starting points of the flows as random noise, i.e., $\mathrm{T}_0=(\bm{x}_0,\bm{q}_0)\sim \mathcal{T}_0\times \mathcal{Q}_0$, where $\mathcal{T}_{0}=\mathcal{N}(\mathbf{0},\mathbf{I}_3)$ is the Gaussian distribution for translations, and $\mathcal{Q}_0=\mathcal{IG}_{\text{SO}(3)}$ is the isotropic Gaussian distribution on $\text{SO}(3)$ for rotations~\cite{leach2022denoising}, corresponding to uniformly sampling rotation axis $\bm{u}\in\mathbb{S}^2$ and rotation angle $\phi \in [0, \pi]$.
Based on Eq.~\ref{eq:exp_map}, we convert the sampled axis and angle to $\bm{q}_0$. 
When $t=1$, the ending points of these two flows, denoted as $\mathrm{T}_1=(\bm{x}_1,\bm{q}_1)$, should be the transformation of a frame.
We denote the data distribution of $\mathrm{T}_1$ as $\mathcal{T}_1\times\mathcal{Q}_1$.

% The conditional flows are defined along geodesic paths, which are constructed respectively by Euclidean linear interpolation in $\mathbb{R}^3$ for the translations and spherical linear interpolation (SLERP) in $\text{SO}(3)$ for the rotations:  

\textbf{Linear Interpolation of Translation.} 
For $\bm{x}_0 \sim \mathcal{T}_0$ and $\bm{x}_1 \sim \mathcal{T}_1$, we can interpolate the trajectory between them linearly: for $t\in [0, 1]$, 
\begin{eqnarray}\label{eq:linear}
\begin{aligned}
    &\bm{x}_t = (1-t)\bm{x}_0 + t \bm{x}_1,\,\text{with constant translation velocity:~}\bm{v} = \bm{x}_1 - \bm{x}_0.
\end{aligned}
\end{eqnarray}

\textbf{SLERP of Rotation in Exponential Format.} 
For unit quaternions $\bm{q}_0 \sim \mathcal{Q}_0$ and $\bm{q}_1 \sim \mathcal{Q}_1$, we interpolate the trajectory between them via SLERP in an exponential format~\cite{sola2017quaternion}:
\begin{equation}\label{eq:slerp_e}
\begin{aligned}
    &\bm{q}_t = \bm{q}_0 \otimes \exp(t \log(\bm{q}_0^{-1} \otimes \bm{q}_1)), \, \text{with constant angular velocity:~}\bm{\omega} = \phi\bm{u}.
\end{aligned}
\end{equation}
Here, $\bm{q}_0^{-1} \otimes\bm{q}_1=[\cos\left(\phi/2\right), \sin\left(\phi/2\right)\bm{u}^{\top}]^{\top}$ and $\bm{\omega}=2\log(\bm{q}_0^{-1} \otimes \bm{q}_1)$. 
$\exp(\cdot)$ and $\log(\cdot)$ are exponential and logarithmic maps defined in Eq.~\ref{eq:exp_map} and Eq.~\ref{eq:log_map}, respectively.




\textbf{Training QFlow Model.}
In this study, we adopt the $\text{SE}(3)$-equivariant neural network in FrameFlow~\cite{yim2023fast}, denoted as $\mathcal{M}_{\theta}$, to model the flows.
Given the transformation at time $t$, i.e., $\mathrm{T}_t$, the model predicts the transformation at $t=1$: 
\begin{equation}\label{eq:model}
\mathrm{T}_{\theta,1}=(\bm{x}_{\theta, 1}, \bm{q}_{\theta,1}) = \mathcal{M}_{\theta}(\mathrm{T}_t, t).
\end{equation}

We train this model by the proposed quaternion flow (QFlow) matching method.
In particular, given the frame $\mathrm{T}_1=(\bm{x}_1,\bm{q}_1)$, we first sample a timestamp $t\sim\text{Uniform}([0,1])$ and random initial points $\mathrm{T}_0=(\bm{x}_0,\bm{q}_0)\sim\mathcal{T}_0\times\mathcal{Q}_0$.
Then, we derive obtain $(\bm{x}_t,\bm{v})$ and $(\bm{q}_t,\bm{\omega})$ via Eq.~\ref{eq:linear} and Eq.~\ref{eq:slerp_e}, respectively.
Passing $(\bm{x}_t,\bm{q}_t,t)$ through the model $\mathcal{M}_{\theta}$, we obtain $\bm{x}_{\theta,1}$ and $\bm{q}_{\theta,1}$, and derive the translation and angular velocities at time $t$ by 
\begin{equation}\label{eq:velocity}
    \bm{v}_{\theta, t} = \frac{\bm{x}_{\theta, 1} - \bm{x}_t}{1 - t}, \quad \bm{\omega}_{\theta, t} = \frac{2\log(\bm{q}_t^{-1} \otimes \bm{q}_{\theta,1})}{1-t}.
\end{equation}

Based on the constancy of the velocities, we train the model $\mathcal{M}_{\theta}$ by minimizing the following two objectives:
\begin{eqnarray}\label{eq:objs}
\begin{aligned}
\mathcal{L}_{\mathbb{R}^3} =  \mathbb{E}_{t,\mathcal{T}_0, \mathcal{T}_1} 
[ \| \bm{v} - \bm{v}_{\theta, t} \|^2],\quad\quad
\mathcal{L}_{\text{SO}(3)} = \mathbb{E}_{t,\mathcal{Q}_0, \mathcal{Q}_1} 
[ \| \bm{\omega} - \bm{\omega}_{\theta, t} \|^2 ].    
\end{aligned}   
\end{eqnarray}
Besides the above MSE losses, we further consider the auxiliary loss proposed in~\cite{yim2023se}, which discourages physical violations, e.g., chain breaks or steric clashes.
Therefore, we train the model by 
\begin{equation}
\label{eq:loss}
    \sideset{}{_{\theta}}\min \mathcal{L}_{\mathbb{R}^3} + \mathcal{L}_{\text{SO}(3)} + \alpha \cdot \mathbf{1}\{t < \epsilon\}\cdot \mathcal{L}_{\text{aux}},
\end{equation}
where $\alpha\geq 0$ is the weight of the auxiliary loss, $\mathbf{1}$ is an indicator, signifying that the auxiliary loss is applied only when $t$ is sampled below a predefined threshold $\epsilon$.

\textbf{Inference Based on QFlow.} 
Given a trained model, we can generate frames of residues from noise with the predicted velocities.
In particular, given initial $(\bm{x}_0,\bm{q}_0)\sim\mathcal{T}_0\times\mathcal{Q}_0$, the translation is generated by an Euler solver with $L$ steps: 
\begin{equation}\label{eq:euler_trans}
    \bm{x}_{t+\Delta t} = \bm{x}_t + \bm{v}_{\theta, t} \cdot \Delta t,
\end{equation}
where the step size $\Delta t=\frac{1}{L}$. 
The quaternion of rotation is generated with an exponential step size scheduler: We modify Eq.~\ref{eq:slerp_e}, interpolating $\bm{q}_t$ with an acceleration as
\begin{equation}
    \bm{q}_t = \bm{q}_0 \otimes \exp( (1- e^{-\gamma t}) \log(\bm{q}_0^{-1} \otimes \bm{q}_1)),
\end{equation}
where $\gamma$ controls the rotation accelerating, and we empirically set $\gamma = 10$. 
Then, the Euler solver becomes:
\begin{equation}\label{eq:infer_slerp}
    \bm{q}_{t + \Delta t} = \bm{q}_{t} \otimes \exp\Bigl(\frac{1}{2}\Delta t \cdot \gamma e^{-\gamma t} \bm{\omega}_{\theta, t}\Bigr),
\end{equation}
where $\gamma e^{-\gamma t} \bm{\omega}_{\theta, t}$ is the adjusted angular velocity.
Previous works~\cite{bose2023se,yim2023fast} have demonstrated that the exponential step size scheduler helps reduce sampling steps and enhance model performance. 

% \begin{eqnarray}\label{eq:infer_slerp}
% \begin{aligned}
%     \bm{q}_{t + \Delta t} &= \bm{q}_t \otimes \exp(\gamma \Delta t \cdot \log(\bm{q}_{t}^{-1} \otimes \bm{q}_{\theta, 1}))\\
%     &=\bm{q}_t \otimes \exp\Bigl(\frac{\gamma(1-t)\Delta t}{2}\bm{\omega}_{\theta,t}\Bigr),
% \end{aligned}
% \end{eqnarray}
% where $\gamma$ is a constant that controls the rotations accelerating and we empirically set $\gamma = 10$. 
% Previous works~\cite{bose2023se,yim2023fast} have demonstrated that the exponential step size scheduler leads to much fewer sampling steps and helps enhance the designability of generated proteins. 
% Essentially, applying this scheduler means adjusting the angular velocity as $\hat{\bm{\omega}}_{\theta, t} = \gamma e^{-\gamma t} \bm{\omega}_{\theta, t}$ and updating $\bm{q}_t$ as $\bm{q}_{t + \Delta t} = \bm{q}_{t} \otimes \exp(\frac{1}{2}\Delta t \cdot \hat{\bm{\omega}}_{\theta, t})$.
% According to this scheduler, the angular velocity is derived as:
% \begin{equation}
%     \hat{\bm{\omega}}_{\theta, t} = \gamma e^{-\gamma t} \bm{\omega}_{\theta, t},
% \end{equation}
% with $\hat{\bm{\omega}}_{\theta, t}$, Eq.~\ref{eq:infer_slerp} is equivalent to an Euler solver:
% \begin{equation}
%     \bm{q}_{t + \Delta t} = \bm{q}_{t} \otimes \exp\Bigl(\frac{1}{2}\Delta t \cdot \hat{\bm{\omega}}_{\theta, t}\Bigr).
% \end{equation}

% More details of our training and inference schemes can be found in Appendix~\ref{app:alg}. 


\subsection{Rectified Quaternion Flow}\label{sec:reflow}

Given the trained QFlow model $\mathcal{M}_{\theta}$, we can rewire the flows in $\mathbb{R}^3$ and $\text{SO}(3)$, respectively, with a non-crossing manner by the flow rectification method in~\cite{liu2022rectified}. 
In particular, we generate noisy $\mathrm{T}_{0}^{\prime} = \{\bm{x}_0^{\prime},\bm{q}_0^{\prime}\} \sim \mathcal{T}_0\times\mathcal{Q}_0$ and transfer to $\mathrm{T}_1^{\prime} = \{\bm{x}_1^{\prime},\bm{q}_1^{\prime}\}\sim\mathcal{T}_1\times\mathcal{Q}_1$ by $\mathcal{M}_{\theta}$. 
Taking $\mathcal{M}_{\theta}$ as the initialization, we use the noise-sample pairs, i.e., $\{\mathrm{T}_0^{\prime}, \mathrm{T}_1^{\prime}\}$, to train the model further by the same loss in Eq.~\ref{eq:loss} and derive the rectified QFlow (ReQFlow) model.

The work in~\cite{liu2022rectified} has demonstrated that the rectified flow of translation in $\mathbb{R}^3$ preserves the marginal law of the original translation flow and reduces the transport cost from the noise to the samples.
We find that these theoretical properties are also held by the rectified quaternion flow under mild assumptions.
Let $(\bm{q}_0,\bm{q}_1)\sim\mathcal{Q}_0\times\mathcal{Q}_1$ be the pair used to train QFlow, and $(\bm{q}_0^{\prime},\bm{q}_1^{\prime})$ be the pair induced from $(\bm{q}_0,\bm{q}_1)$ by flow rectification.
Then, we have
\begin{theorem}
\label{theo:marginal}
(\textbf{Marginal preserving property}). The pair $(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})$ is a coupling of $\mathcal{Q}_0$ and $\mathcal{Q}_1$. The marginal law of $\bm{q}_t^{\prime}$ equals that of $\bm{q}_t$ at everytime, that is $\text{Law}(\bm{q}_t^{\prime}) = \text{Law}(\bm{q}_t)$.
\end{theorem}
% The newly constructed pair $(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})$ preserves the original marginal distributions, i.e., the rectification step does not alter the distribution of orientations at any time $t$.
\begin{theorem}
\label{theo:cost}
(\textbf{Reducing transport costs}). The pair $(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})$ yields lower or equal convex transport costs than the input $(\bm{q}_0, \bm{q}_1)$. For any convex $c$: $\mathbb{R}^3 \rightarrow \mathbb{R}$, define the cost as $C(\bm{q}_0, \bm{q}_1) = c\left( \log(\bm{q}_0^{-1} \otimes \bm{q}_1)\right)$.
Then, we have $\mathbb{E}[C(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})] \leq \mathbb{E}[C(\bm{q}_0, \bm{q}_1)]$.
\end{theorem}
Theorem~\ref{theo:cost} shows that the coupling $(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})$ either achieves a strictly lower or the same convex transport cost compared to the original one, highlighting the advantage of the quaternion flow rectification in reducing the overall rotation displacement cost without compromising the marginal distribution constraints (Theorem~\ref{theo:marginal}).
In addition, we have
\begin{corollary}
\label{cor:nonconstant_speed}
(\textbf{Cost Reduction with Nonconstant Speed}). Suppose the geodesic interpolation $\bm{q}_t$ between $\bm{q}_0$ and $\bm{q}_1$ has a constant axis $\bm{u}$, but its speed is nonconstant in time, i.e, $\bm{\omega}_t = a(t)\bm{u}$.
% Then, for any \emph{homogeneous} convex cost \(c:\mathbb{R}^3\to \mathbb{R}\) of order \(m\in(0,1]\) (i.e.\ \(c(a\,\bm{\omega}) = |a|^{m}\,c(\bm{\omega})\)), 
The quaternion flow rectification still reduces or preserves the transport cost.
\end{corollary}
This corollary means that when applying the exponential step size scheduler (i.e., Eq.~\ref{eq:infer_slerp}), the rectification still reduces or preserves the transport cost.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.45\linewidth]{figures/small_angles.pdf}
%     \caption{Average number of small rotation angles when generating 10 backbones for each length with exponential scheduler.}
%     \label{fig:small_angles}
% \end{figure}

\subsection{Rationality Analysis}\label{sec:rational}
% \xu{This part should be section 3.4, called Rationality Analysis. The logic should be 1) compared to existing methods, the main difference of our method is using exp slerp. 2) introduce matrix geodesic method and highlight its difference to exp slerp.
% 3) introduce add. slerp and highlight its difference to exp slerp.
% 4) Using your analytic experiment, demonstrating the rationality of exp slerp in protein backbone generation.
% 5) Use Table 1 summarize the analysis.}

Most existing methods, like FrameFlow~\cite{yim2023fast} and FoldFlow~\cite{bose2023se}, represent rotations as $3\times3$ matrices. 
Given two rotation matrices $\bm{R}_0$ and $\bm{R}_1$, they construct a flow in $\text{SO}(3)$ with matrix geodesic interpolation:
\begin{equation}\label{eq:matrix_geo}
    \bm{R}_t = \bm{R}_0 \exp_M\left(t \log_M(\bm{R}_0^\top \bm{R}_1)\right),
\end{equation}
where $\exp_M(\cdot)$ and $\log_M(\cdot)$ denote the matrix exponential and logarithmic maps, respectively. 
The corresponding angular velocity $\bm{\Omega}=\log_M(\bm{R}_0^\top \bm{R}_1)$. 
Different from existing methods~\cite{yim2023fast,yim2023se,bose2023se}, our method applies quaternion-based rotation representation and achieves rotation interpolation by SLERP in an exponential format, which achieves superior numerical stability and thus benefits protein backbone generation.

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=5cm]{figures/rotation_error.pdf}
    \subcaption{}
    \label{fig:rotation_error}
  \end{subfigure}
  \begin{subfigure}[b]{0.36\textwidth}
    \centering
    \includegraphics[height=5cm]{figures/dataset_large_angles.pdf}
    \subcaption{}
    \label{fig:dataset_large_angles}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[height=5cm]{figures/small_angles.pdf}
    \subcaption{}
    \label{fig:small_angles}
  \end{subfigure}
  \caption{(a) Mean round-trip errors from $\pi - 10^{-1}$ to $\pi - 10^{-7}$. 
  (b) The frequency of suffering large rotation angles per protein when training on the two datasets. 
  (c) The average number of small rotation angles per protein when generating ten backbones for each length.}
  \label{fig:main}
\end{figure}


To verify this claim, we conduct a round-trip error experiment: given an rotation $\bm{\omega}$ in the axis-angle format, we convert it to a rotation matrix $\bm{R}$ and a quaternion $\bm{q}$, respectively, and convert it back to the axis-angle format, denoted as $\hat{\bm{\omega}}_R$ and $\hat{\bm{\omega}}_q$, respectively.
Figure~\ref{fig:rotation_error} shows the round-trip errors in $L_2$ norm for large rotation angles (e.g., $\phi\in[\pi-10^{-2}, \pi)$). 
Our quaternion-based method is numerically stable while the matrix-based representation suffers severe numerical errors.
When training a protein backbone generation model, the numerical stability for large rotation angles is important.
Given the frames in the Protein Data Bank (PDB)~\cite{burley2023rcsb} dataset and the SCOPe~\cite{chandonia2022scope} dataset, we sample a random noise for each frame and calculate the rotation angle between them. 
The histogram in Figure~\ref{fig:dataset_large_angles} shows that when training an arbitrary flow-based model, the probability of suffering at least one large angle per protein is 0.59 for PDB and 0.34 for SCOPe, respectively.
It means that the matrix-based representation may introduce undesired numerical errors that aggregate and propagate during training.


% We further analyze the rotation angles between randomly sampled noise-target pairs during one training epoch. As illustrated in Fig.~\ref{fig:dataset_large_angles}, 59\% of angles in the Protein Data Bank (PDB)~\cite{burley2023rcsb} dataset and 34\% in the SCOPe~\cite{chandonia2022scope} dataset exceed $\pi - 10^{-2}$ radians. These two numerical experiments suggest that our model's improved stability in handling large rotation angles confers advantages for protein backbone generation tasks.

In addition, a very recent work, AssembleFlow~\cite{guo2025assembleflow}, also applies quaternion-based rotation representation and SLERP when modeling 3D molecules. 
In particular, it applies SLERP in an additive format:
\begin{equation}\label{eq:slerp_a}
    \bm{q}_t = \frac{\sin((1 - t) \frac{\phi}{2})}{\sin (\frac{\phi}{2})} \bm{q}_0 + \frac{\sin(t \frac{\phi}{2})}{\sin(\frac{\phi}{2})} \bm{q}_1,
\end{equation}
and updates rotations linearly by the following Euler solver:
\begin{equation}\label{eq:euler_add}
    \bm{q}_{t + \Delta t} = \bm{q}_t + \Delta t \cdot \bm{\eta}_t.
\end{equation}
Here, $\bm{\eta}_t$ is the instantaneous velocity in the tangent space of $\bm{q}_t$, which is derived by the first-order derivative of Eq.~\ref{eq:slerp_a}.
However, this modeling strategy also suffers numerical issues.
Firstly, although the additive format SLERP can generate the same interpolation path as ours in theory, when rotation angle $\phi$ is small (e.g., $\phi\in [0, 10^{-6})$), Eq.~\ref{eq:slerp_a} often outputs ``NaN'' because the denominator $\sin(\frac{\phi}{2})$ tends to zero.
The exponential step size scheduler leads to rapid convergence when generating protein backbones, which frequently generates rotation angles below the threshold $10^{-6}$ (as shown in Figure~\ref{fig:small_angles}) and thus makes the additive format SLERP questionable in our task.
Secondly, the Euler step in Eq.~\ref{eq:euler_add} makes $\|\bm{q}_{t + \Delta t}\|_2\neq 1$, so that renormalization is required after each update.
Table~\ref{tb:rot_comparison} provides a comprehensive comparison for the three rotation interpolation methods, highlighting the advantages of our method.
% More details and analytic experiments are provided in Appendix~\ref{app:exp}.
% \textbf{Rotation conversion}. A rotation can be compactly represented using the angle-axis formulation, which specifies a rotation angle $\phi$ and a rotation axis $\bm{u}$, with $\bm{\omega} = \phi \bm{u} \in \mathbb{R}^3$. Alternatively, rotations can be described using a $3\times3$ rotation matrix $\bm{R}$ or a unit quaternion $\bm{q}$, and the transformations between them are common in numerous practical and theoretical applications.
% % both of which enable the application of advanced mathematical frameworks, such as $\mathfrak{so}(3)$ algebra~\cite{jacobson2013lie, gallo2022so} and quaternion algebra~\cite{voight2021quaternion}. These representations are interconvertible, and the transformations between the angle-axis representation, rotation matrices, and quaternions are common in numerous practical and theoretical applications.
% \begin{itemize}
%     \item \textbf{Exponential map}. The angle-axis formulation can be converted into a rotation matrix or a unit quaternion through the exponential map. For a unit quaternion $\bm{q}$, see Eq.~\ref{eq:exp_map}. For a rotation matrix $\bm{R}$, the mapping is implemented using Rodrigues' rotation formula::
%     \begin{equation}
%         \bm{R} = \exp(\bm{\Omega})= \bm{I} + \sin(\phi) \frac{\bm{\Omega}}{\phi} + \left(1 - \cos\phi\right) \frac{\bm{\Omega}^2}{\phi^2},
%     \end{equation}
%     where $\bm{\Omega} = \lfloor \bm{\omega} \rfloor_\times \in \mathfrak{so}(3)$ is the skew-symmetric matrix representation.
%     \item \textbf{Logarithmical map}. We can reversely convert a unit quaternion $\bm{q}$ or rotation matrix $\bm{R}$ to an axis-angle representation. Eq.~\ref{eq:log_map} is the logarithmical map for the quaternion $\bm{q}$. For the rotation matrix $\bm{R}$, the corresponding mapping is:
%     \begin{equation}
%     \bm{\Omega} = \log(\bm{R}) = \frac{\phi}{2\sin\phi}(\bm{R} - \bm{R}^\top),
%     \end{equation}
%     where the rotation angle $\phi$ is computed by $\phi= \arccos\left(\mathrm{tr}(\frac{\bm{R} - 1}{2})\right)$.
% \end{itemize}

% \textbf{Interpolation}.
% When representing rotations using the rotation matrices or quaternions, smooth interpolation between rotations can be achieved through geodesic interpolation~\cite{dam1998quaternions,wu2019robust}. Within the Lie group $\text{SO}(3)$, which defines the manifold of 3D rotations, this geodesic path corresponds to the shortest trajectory between two rotations on the manifold.

% \begin{theorem}{(\textbf{Rotation Matrix Geodesic interpolation in $\text{SO}(3)$})}
% Let $\text{SO}(3)$ be endowed with the bi-invariant Riemannian metric induced by the Frobenius inner product $\langle \bm{A}, \bm{B} \rangle = \frac{1}{2}\text{Tr}(\bm{A}^\top \bm{B})$. For any two rotation matrices $\bm{R}_0, \bm{R}_1 \in \text{SO}(3)$, the geodesic $R(t)$ connecting $\bm{R}_0$ and $\bm{R}_1$ is uniquely parameterized as:
% \begin{equation}
% R(t) = \bm{R}_0 \exp\left(t \log(\bm{R}_0^\top \bm{R}_1)\right), \quad t \in [0,1],
% \end{equation}
% % where $\exp: \mathfrak{so}(3) \to \text{SO}(3)$ is the matrix exponential map, and $\log: \text{SO}(3) \to \mathfrak{so}(3)$ is its inverse.
% \label{theo:matrix_geodesic}
% \end{theorem}
% \begin{theorem}{(\textbf{SLERP in an Exponential Format as Geodesic Interpolation in $\text{SO}(3)$})}
% Let $\bm{q}_0, \bm{q}_1 \in \mathbb{S}^3$ be unit quaternions representing rotations in $\text{SO}(3)$. The spherical linear interpolation (SLERP) path in an exponential format can be expressed as:
% \begin{equation}
% q(t) = \bm{q}_0 \otimes \exp(t \log(\bm{q}_0^{-1} \otimes \bm{q}_1)), \quad t \in [0,1],
% \end{equation}
% Under the double cover $\pi: \mathbb{S}^3 \to \text{SO}(3)$, this path projects to a geodesic in $\text{SO}(3)$:
% \begin{equation}
%     \pi(q(t)) = \gamma(t).
% \end{equation}
% \label{theo:slerp_geodesic}
% \end{theorem}
% \vspace{-20pt} 

% Theo.~\ref{theo:matrix_geodesic} presents the closed-form expression for geodesic interpolation of rotation matrices in $\text{SO}(3)$. In Theo.~\ref{theo:slerp_geodesic}, The map $\pi: \mathbb{S}^3 \to \text{SO}(3)$ is a double covering, meaning each rotation $\bm{R} \in \text{SO}(3)$ corresponds to exactly two antipodal quaternions $\{\bm{q}, -\bm{q}\} \in \mathbb{S}^3$. The geodesic $q(t)$ on $\mathbb{S}^3$ projects to $R(t)$ in $\text{SO}(3)$ via $\pi$, preserving the geodesic property under the bi-invariant metric.
% % The geodesic path can be generated via the exponential and logarithmic maps. For rotation matrix representation, the geodesic interpolation is:
% % \begin{equation}
% %     \bm{r}_t = \exp_{{r}_0}\left( t \log_{\bm{r}_0}(\bm{r}_1) \right).
% % \end{equation}
% % Here, the subscript $ \bm{r}_0 $ indicates that this is the rotation of $\bm{r}_1 $ relative to $\bm{r}_0$.
% % As for quaternions, we have:
% % \begin{equation}\label{eq:slerp-e}
% %     \bm{q}_t = \bm{q}_0 \otimes \exp(t \log(\bm{q}_0^{-1} \otimes \bm{q}_1)),
% % \end{equation}
% % where $\bm{q}_0^{-1} \otimes \bm{q}_1 = [\cos\left(\phi/2\right), \sin\left(\phi/2\right)\bm{u}^{\top}]^{\top}$ represents the relative rotation from $\bm{q}_0$ to $\bm{q}_1$. 


% Obviously, for the geodesic interpolation defined by exponential and logarithmic mappings, \textbf{determining the rotation matrix or quaternion corresponding to an arbitrary time step $t$ requires conversions to and from the axis-angle representation}. Through numerical experiments in Sec.~\ref{sec:numerical}, we observe that rotation matrices exhibit numerical instability when handling large rotation angles, whereas quaternion representations consistently remain numerically stable.

% Particularly, the geodesic interpolation of quaternions is commonly referred to as the spherical linear interpolation (SLERP), and Eq.~\ref{eq:slerp-e} is an exponential form. The addictive form is:
% \begin{equation}
% \bm{q}_t = \frac{\sin((1 - t) \tau)}{\sin \tau} \bm{q}_0 + \frac{\sin(t \tau)}{\sin(\tau)} \bm{q}_1,
% \end{equation}
% where $\tau = \arccos(\langle\bm{q}_0, \bm{q}_1 \rangle) = 1/2 \phi$. They are equivalent and generate the same interpolation path.
% %  the first-order derivative of SLERP-D describes the instantaneous tangential velocity along the spherical path:
% % \begin{align*}
% % \bm{\nu}_t &= 
% % \frac{\mathrm{d}}{\mathrm{d} t} \text{SLERP}(\bm{q}_1, \bm{q}_2, t) \\ 
% % &= \frac{\phi}{\sin(\phi)} \left( -\cos((1 - t) \phi) \bm{q}_0 + \cos(t \phi) \bm{q}_1 \right)
% % \end{align*}
% \textbf{ODE Euler solvers}
% In scientific computation and numerical simulation scenarios~\cite{deuflhard2012scientific}, ODE solvers are commonly employed to reconstruct trajectories when rotations are represented as rotation matrices or quaternions. The initial rotations are denoted by $\bm{r}_0$ or $\bm{q}_0$, and the time step is $\Delta t$. Here, $\bm{\omega}^\wedge$ and $\bm{\omega}$ represent the constant angular velocity fields used for geodesic interpolation of rotation matrices and quaternions, respectively.
%  We have:
%  \begin{equation}
%     \bm{r}_{t + \Delta t} = \exp_{\bm{r}_t}(\Delta t \cdot \bm{\omega}^{\wedge}),
% \end{equation}
% \begin{equation}\label{eq:euler_q_e}
%     \bm{q}_{t + \Delta t} = \bm{q}_{t} \otimes \exp(\frac{1}{2}\Delta t \cdot \bm{\omega}).
% \end{equation}
% For standard SLERP, the first-order derivative describes the instantaneous tangential velocity $\bm{\eta}_t$ along the geodesic path:
% \begin{equation}
%     \bm{\eta}_t = \frac{\tau}{\sin \tau} \left( -\cos((1 - t) \tau) \bm{q}_0 + \cos(t \tau) \bm{q}_1 \right),
% \end{equation}
% at each step:
% \begin{equation}
%     \bm{q}_{t + \Delta t} = \bm{q}_t + \Delta t \cdot \bm{\eta}_t.
% \end{equation}
% Since $\bm{\eta}_t$ represents the instantaneous tangential velocity, applying the Euler method to solve it compromises the unit-norm property of the quaternion representation. Consequently, renormalization is required after each update, which inevitably introduces errors. This issue is validated in Sec.~\ref{sec:numerical}.

\begin{table}[t]
    \centering
    \caption{Comparisons for various rotation interpolation methods.}
    \label{tb:rot_comparison}
    % \footnotesize 
    \scriptsize
    \setlength{\tabcolsep}{2.5pt}
    \begin{tabular}{@{}c|c|ccc@{}}
        \toprule
        \multicolumn{2}{c|}{Method}  & \textbf{Matrix Geodesic} & \textbf{SLERP (Add. Format)} & \textbf{SLERP (Exp. Format)} \\ 
        \midrule
        
        \multirow{2}{*}{\makecell[c]{\textbf{Interpolation}}} 
        & Formula & 
        $\bm{R}_0 \exp_M\left(t \log_M(\bm{R}_0^\top \bm{R}_1)\right)$ & 
        $\frac{\sin((1\!-\!t)\frac{\phi}{2})}{\sin\frac{\phi}{2}}\bm{q}_0 \!+\! \frac{\sin(t\frac{\phi}{2})}{\sin\frac{\phi}{2}}\bm{q}_1$ & 
        $\bm{q}_0 \otimes \exp(t\log(\bm{q}_0^{-1}\!\otimes\!\bm{q}_1))$ \\ 
        \cmidrule(lr){2-5}
        
        & Velocity & 
        $\bm{\Omega}=\log_M(\bm{R}_0^\top \bm{R}_1)$ & 
        $\bm{\eta}_t= \frac{\phi(\cos( \frac{t\phi}{2}) \bm{q}_1-\cos((1 - t) \frac{\phi}{2}) \bm{q}_0)}{2\sin\frac{\phi}{2}}$ & 
        $\bm{\omega}=2\log(\bm{q}_0^{-1}\!\otimes\!\bm{q}_1)$ \\
        \midrule
        
        \multirow{2}{*}{\makecell[c]{\textbf{Euler}\\ \textbf{Solver}}} 
        & Update & 
        $\bm{R}_{t+\Delta t} = \bm{R}_t \exp_M(\Delta t\cdot\bm{\Omega})$ & 
        $\bm{q}_{t+\Delta t} = \bm{q}_t + \Delta t\cdot \bm{\eta}_t$ & 
        $\bm{q}_{t+\Delta t} = \bm{q}_t \otimes \exp(\frac{1}{2}\Delta t \cdot\bm{\omega})$ \\
        \cmidrule(lr){2-5}
        & No Renomalization & \checkmark & \ding{55} & \checkmark \\
        \midrule
        
        \multirow{2}{*}{\makecell[c]{\textbf{Numerical}\\ \textbf{Stability}}} 
        & $\phi \geq \pi - 10^{-2}$ & \ding{55} & \checkmark  & \checkmark  \\
        \cmidrule(lr){2-5}
        & $\phi \leq 10^{-6}$ & \checkmark & \ding{55} & \checkmark \\
        \midrule
        
        \multicolumn{2}{c|}{\textbf{Application Scenarios}} &  
        \makecell[c]{\footnotesize FrameFlow~\cite{yim2023fast},\\ \footnotesize FoldFlow~\cite{bose2023se}} &  
        \makecell[c]{\footnotesize AssembleFlow~\cite{guo2025assembleflow}}
         & 
        \makecell[c]{\footnotesize QFlow (\textbf{Ours}),\\ \footnotesize ReQFlow (\textbf{Ours})} \\
        \bottomrule
    \end{tabular}
\end{table}


\section{Experiment}
To demonstrate the effectiveness and efficiency of our methods (QFlow and ReQFlow), we conduct comprehensive experiments to compare them with state-of-the-art protein backbone generation methods.
In addition, we conduct ablation studies to verify the usefulness of the flow rectification strategy and the impact of sampling steps on model performance.
All the experiments are implemented on four NVIDIA A100 80G GPUs. 
Implementation details and experimental results are shown in this section and Appendix~\ref{app:exp}.


\subsection{Experimental Setup}
\textbf{Datasets.}
We apply two commonly used datasets in our experiments. 
The first is the 23,366 protein backbones collected from Protein Data Bank (PDB)~\cite{burley2023rcsb}, whose lengths range from 60 to 512.
The second is the SCOPe dataset~\cite{chandonia2022scope} pre-processed by FrameFlow~\cite{yim2023fast}, which contains 3,673 protein backbones with lengths ranging from 60 to 128. 
% Note that, SCOPe is notably smaller than PDB, which allows us to conduct more analytic experiments in-depth, e.g., verifying the advantage of the exponential format SLERP and the universality of the rectified flow strategy.


% We train QFlow on PDB~\cite{burley2023rcsb} and SCOPe~\cite{chandonia2022scope} datasets. 
% Following FrameDiff~\cite{yim2023se}, we download protein backbone data from Protein Data Bank (PDB) on December 17th, 2024. 
% After filtering, 23,366 samples whose length ranging from 60 to 512 were used for training. 
% We also use SCOPe dataset pre-processed by FrameFlow, which consists 3,673 samples with length ranging from 60 to 128. 
% QFlow and ReQFlow are mainly based on FrameFlow. 


\noindent\textbf{Baselines.} The baselines of our methods include diffusion-based methods (FrameDiff~\cite{yim2023se}, RFDiffusion~\cite{watson2023novo}, and Genie2~\cite{lin2024out}) and flow-based methods (FrameFlow~\cite{yim2023fast}, FoldFlow~\cite{bose2023se}, and FoldFlow2~\cite{huguet2024sequence}).
In addition, we rectify FrameFlow by our method (i.e., re-training FrameFlow based on the paired data generated by itself) and consider the rectified FrameFlow (ReFrameFlow) as a baseline as well.








% We compare our models (QFlow and ReQFlow) with several baselines, including RFDiffusion~\cite{watson2023novo}, Genie2~\cite{lin2024out}, FrameFlow~\cite{yim2023fast}, FrameDiff~\cite{yim2023se}, FoldFlow (including its three variants)~\cite{bose2023se}, and FoldFlow2~\cite{huguet2024sequence} on the PDB dataset. 
% Additionally, we compare our models with FrameFlow and Rectified FrameFlow (ReFrameFlow) on the Scope dataset. 
% For the PDB experiments, we utilize the checkpoints provided by the original papers. 
% In contrast, for the Scope experiments, we train all models from scratch.

\begin{table}[H]
    \centering
    % \vspace{-5pt}
    \caption{Comparisons for various models on PDB.
    For each designability metric, we bold the best result and show the top-3 results with a blue background.
    In the same way, we indicate the best and top-3 diversity and novelty results among the rows with Fraction $> 0.8$.
    The inference time corresponds to generating a protein backbone with length $N=300$.}
    \small
    \begin{tabular}{lccccccc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c}{\textbf{Efficiency}} & \multicolumn{2}{c}{\textbf{Designability}} & \multicolumn{1}{c}{\textbf{Diversity}} & \multicolumn{1}{c}{\textbf{Novelty}}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7}
        & Step
        & Time(s)
        & Fraction$\uparrow$
        & scRMSD$\downarrow$
        & TM$\downarrow$
        & TM$\downarrow$\\
        \midrule
        RFDiffusion & 50 &  66.23 & 0.904 &\cellcolor{top3}1.102$_{\pm\text{1.617}}$ & 0.382 & 0.822\\
        \midrule
        Genie2 & 1000 & 112.93& 0.908 & 1.132$_{\pm\text{1.389}}$ & 0.370 & \cellcolor{top1}\textbf{0.759} \\
               & 500 & 55.86 & 0.000 & 18.169$_{\pm\text{5.963}}$ & - & 0.115\\
        \midrule
        FrameDiff & 500 & 48.12 & 0.564 & 2.936$_{\pm\text{3.093}}$ & 0.441 & 0.799 \\
        \midrule
        FoldFlow$_{\text{Base}}$ & 500 & 43.52 & 0.624 & 3.080$_{\pm\text{3.449}}$ & 0.469 & 0.870 \\
        FoldFlow$_{\text{SFM}}$ & 500 & 43.63 & 0.636 & 3.031$_{\pm\text{3.589}}$ & 0.411 & 0.848\\
        FoldFlow$_{\text{OT}}$ & 500 & 43.35 & 0.852 & 1.760$_{\pm\text{2.593}}$ & 0.434 & 0.857\\
        \midrule
        FoldFlow2 & 50 & 6.35 & \cellcolor{top2}{0.952} & \cellcolor{top2}{1.083$_{\pm\text{1.308}}$} & 0.373  & 0.813 \\
                  & 20 & 2.63& 0.644& 3.060$_{\pm\text{3.210}}$& 0.339 & 0.736 \\
        \midrule
        FrameFlow & 500 & 20.72 & 0.872 & 1.380$_{\pm\text{1.392}}$ &\cellcolor{top3} 0.346 & 0.803\\
         & 200 & 8.69 & 0.864 & 1.542$_{\pm\text{1.889}}$ & 0.348 & 0.809 \\
         & 100 & 4.20 & 0.708 & 2.167$_{\pm\text{2.373}}$ & 0.332 & 0.806 \\
         & 50 & 2.23 & 0.704 & 2.639$_{\pm\text{3.079}}$ & 0.334 & 0.791 \\
         & 20 & 0.84 & 0.436 & 4.652$_{\pm\text{4.390}}$ & 0.319 & 0.772 \\
         & 10 & 0.47 & 0.180 & 7.343$_{\pm\text{5.125}}$ & 0.317 & 0.762 \\
        \midrule
        % QFlow-v1-Base & 500 & & 0.772 & 2.025 ($\pm$2.590) & 0.452 & 0.404 & 0.875 ($\pm$0.047) & 0.921 & 0.000 \\
        % QFlow-v1-Rectified & & & & & & & & & \\
        QFlow & 500 & 17.52 & \cellcolor{top3}0.936 & 1.163$_{\pm\text{0.938}}$ & 0.356 & 0.821 \\
         & 200 & 6.85 & 0.864 & 1.400$_{\pm\text{1.259}}$ &\cellcolor{top1} \textbf{0.344} & 0.807 \\
         & 100 & 3.45 & 0.916 & 1.342$_{\pm\text{1.364}}$ & 0.348 & 0.809 \\
         & 50 & 1.87 & 0.812 & 1.785$_{\pm\text{2.151}}$ &\cellcolor{top1} \textbf{0.344} & \cellcolor{top2} 0.784 \\
         & 20 & 0.81 & 0.604 & 3.090$_{\pm\text{3.374}}$ & 0.325 & 0.758 \\
         & 10 & 0.45 & 0.332 & 5.032$_{\pm\text{4.303}}$ & 0.313 & 0.715 \\
        \midrule
        ReQFlow & 500 & 17.29 & \cellcolor{top1}\textbf{0.972} & \cellcolor{top1} \textbf{1.071$_{\pm\text{0.482}}$} & 0.377 & 0.828 \\
         & 200 & 7.44 & 0.932 & 1.160$_{\pm\text{0.782}}$ & 0.384 & 0.826 \\
         & 100 & {3.62} & 0.928 & 1.245$_{\pm\text{1.059}}$ & 0.369 & 0.819 \\
         & 50 & {1.81} & 0.912 & 1.254$_{\pm\text{0.915}}$ & 0.369 & 0.810 \\
         & 20 & 0.80 & 0.872 & 1.418$_{\pm\text{0.998}}$ & 0.355 & \cellcolor{top3}0.791 \\
         & 10 & 0.45 & 0.676 & 2.443$_{\pm\text{2.382}}$ & 0.337 & 0.760 \\
        \bottomrule
    \end{tabular}
    \label{tab:PDB main results}
\end{table}


\noindent\textbf{Implementation Details.}
For the PDB dataset, we utilize the checkpoints of baselines and reproduce the results shown in their papers.
Given the QFlow trained on PDB, we generate 7,653 protein backbones with lengths in $[60, 512]$ from noise and then train ReQFlow based on these noise-backbone pairs.
For the SCOPe dataset, we train all the models from scratch.
Given the QFlow trained on SCOPe, we generate 3,167 protein backbones with lengths in $[60, 128]$ from noise and then train ReQFlow based on these noise-backbone pairs.
When training ReQFlow, we apply structural data filtering, selecting training samples based on scRMSD ($\leq$2\AA) and TM-score ($\geq$0.9 for long-chain proteins) and removing proteins with excessive loops ($>$50\%) or abnormally large radius of gyration (top 4\%).
ReFrameFlow is trained in the same way.

% RFDiffusion, Genie2 and FrameDiff are diffusion based models. 
% RFDiffusion is fine-tuned from RoseTTAFold(RF) structure prediction network, consisting 60M parameters, and utilized the whole PDB dataset. 
% It is the current gold standard.
% Genie2 is trained on a subset of AlphaFold Database(AFDB), consisting 590K structures with a maximum sequence length of 256 residues. 
% The model has total 15.7M parameters. 
% We are following the data preprocessing method of FrameDiff, thus its dataset is the subset of PDB, consisting 23K structures.

% FoldFlow series and FrameFlow are flow based models. 
% FoldFlow and FrameFlow are trained on the same dataset as FrameDiff (which is also QFlow training dataset). 
% Notably, FoldFlow2 utilized sequence information, and uses a pretrained protein language model (ESM2-650M) to encode sequences, thus having a much larger training set than other flow-based methods.


% QFlow PDB is trained on 4 NVIDIA A100 80G GPUs for 8.5 days.
% We generated 20 proteins with lengths ranging from 60 to 512, using 7653 sample-noise pairs for reflow, which takes about 2 hours to converge. 
% After distilling the model, we found it underperformed in protein generation.

% QFlow SCOPe is trained for 36 hours on the same hardware. 
% We generated 50 proteins with lengths from 60 to 128, with a final training set of 3167 sample-noise pairs. 
% Reflow training took about 1 hour. More details are listed in Appendix\ref{app:exp}

\noindent\textbf{Evaluation Metrics.}
\label{sec:metrics} 
Following previous works, we evaluate each method in the following four aspects: 

\noindent\textbf{1) Designability:} As the most critical metric, designability reflects the possibility that a generated protein backbone can be realized by folding the corresponding amino acid sequence. 
It is assessed by the RMSD of $C_\alpha$ (i.e., scRMSD) between the generated protein backbone and the backbone predicted by ESMFold~\cite{lin2023evolutionary}. 
Given a set of generated backbones, we calculate the proportion of the backbones whose $\text{scRMSD}\leq\text{2\AA}$ (denoted as Fraction).  

% It is assessed by generating sequences with ProteinMPNN~\cite{dauparas2022robust} and predicting their structures using ESMFold~\cite{lin2023evolutionary}, with structural fidelity evaluated through \textbf{TM-score} and $C_\alpha$RMSD (\textbf{scRMSD}). 
% In general, a protein backbone is considered designable if scRMSD$\leq$2\AA.

\noindent\textbf{2) Diversity:}  
% We use the average pairwise TM-score of \textit{designable} generated samples averaged across lengths and proportion of unique clusters by MaxCluster as our diversity metric. 
% We exclude non-designable structures as we do not expect them to be realized and including them would thus inflate diversity. 
% The lower mean TM-score indicates the proteins are distributed diverse. 
Given designable protein backbones, whose $\text{scRMSD}\leq\text{2\AA}$, we quantify structural diversity by averaging the mean pairwise TM-scores computed for each backbone length.
% We use the average pairwise \textbf{TM} score of \textit{designable} generated samples across lengths as our diversity metric, excluding non-designable structures to avoid inflating diversity.
% A lower mean TM-score indicates greater diversity in the distribution of the proteins.

\noindent\textbf{3) Novelty:} 
For each designable protein backbone, we compute its maximum TM-score to the data in PDB using Foldseek~\cite{van2022foldseek}. 
The average of the scores reflect the novelty of the generated protein backbones.

\noindent\textbf{4) Efficiency:} We assess the computational efficiency of each method by the number of sampling steps and the inference time for generating 50 proteins at two lengths: 300 residues for PDB and 128 residues for SCOPe. 


\subsection{Comparison Experiments on PDB}


\textbf{Generation Quality.}
Given the models trained on PDB, we set the length of backbone $N\in\{100, 150, 200, 250, 300\}$, and generate 50 protein backbones for each length.
Table~\ref{tab:PDB main results} shows that ReQFlow achieves state-of-the-art performance in designability, achieving the highest Fraction (0.972) among all models, significantly outperforming strong competitors such as Genie2 (0.908) and RFDiffusion (0.904). 
Additionally, it achieves the lowest scRMSD (1.071$\pm$0.482), with a notably smaller variance compared to the other methods, highlighting the model's consistency and reliability in generating high-quality protein backbones. 
Meanwhile, ReQFlow maintains competitive performance in diversity and novelty (0.828), comparable to state-of-the-art baselines. 

\noindent\textbf{Computational Efficiency.} 
Moreover, ReQFlow achieves ultra-fast protein backbone generation.
Typically, ReQFlow achieves a high Fraction score (0.912) with merely 50 steps and 1.81s, outperforming RFDiffusion and Genie2 with 37$\times$ and 62$\times$ acceleration, respectively.
The state-of-the-art methods like Genie2 and FoldFlow2 suffer severe performance degradation in designability when the number of steps is halved, while ReQFlow performs stably even reducing the number of steps from 500 to 20. 
In addition, even if using the same model architecture and inference setting, ReQFlow can be $\sim$10\% faster than FrameFlow because of utilizing the quaternion-based computation.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/combined_helix_strand_percent_heatmap.pdf}
    \caption{The distribution of protein backbones with respect to the percentages of their secondary structure.}
    \label{fig:distribution}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.52\textwidth}
    \centering
    \includegraphics[height=6cm]{figures/longchain_pointplot.pdf}
    \subcaption{Fraction}
    \label{fig:long_chain_designability}
    \end{subfigure}
    \begin{subfigure}[b]{0.44\textwidth}
    \centering
    \includegraphics[height=6cm]{figures/rmsd_box_plot_v2.pdf}
    \subcaption{scRMSD}
    \label{fig:long_chain_rmsd}
    \end{subfigure}
    \begin{subfigure}[b]{0.96\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/protein_arxiv_horizontal.pdf}
    \subcaption{Samples generated by ReQFlow}
    \label{fig:example}
    \end{subfigure}
    \caption{The comparison for various methods on the designability of generated long-chain protein backbones.}\label{fig:long_chain}
\end{figure}

\noindent\textbf{Fitness of Data Distribution.} 
Given generated protein backbones, we record the percentages of helix and strand, respectively, for each backbone, and visualize the distribution of the backbones with respect to the percentages in Figure~\ref{fig:distribution}.
The protein backbones generated by ReQFlow have a reasonable distribution, which is similar to those of RFDiffusion and FrameFlow and comparable to that of the PDB dataset.
However, the distribution of FoldFlow is significantly different from the data distribution and indicates a mode collapse risk --- the protein backbones generated by FoldFlow are always dominated by helix structures.
That is why FoldFlow is inferior to the other methods in diversity and novelty, as shown in Table~\ref{tab:PDB main results}.

\noindent\textbf{Effectiveness on Long Chain Generation.}
Notably, ReQFlow demonstrates exceptional performance in generating long-chain protein backbones (e.g., $N>300$). 
As shown in Figures~\ref{fig:long_chain_designability} and~\ref{fig:long_chain_rmsd}, ReQFlow outperforms all baselines on generating long protein backbones and shows remarkable robustness.
Especially, when the length $N>500$, which is out of the length range of PDB data, all the baselines fail to maintain high designability while ReQFlow still achieves promising performance in Fraction score and scRMSD and generates reasonable protein backbones, as shown in Figure~\ref{fig:example}.
This generalization ability beyond the training data distribution underscores ReQFlow’s potential for real-world applications requiring robust long-chain protein design.

\noindent\textbf{Ablation Study.}
We conduct an ablation study to evaluate the impact of different components in the ReQFlow model. 
The results in Table~\ref{tab:ablation} reveal that similar to existing methods~\cite{yim2023fast,bose2023se,huguet2024sequence}, the exponential step size scheduler is important for ReQFlow, helping generate designable protein backbones with relatively few steps (e.g., $N\leq 500$). 
Additionally, the data filter is necessary for making flow rectification work.
In particular, rectifying QFlow based on low-quality data leads to a substantial degradation in model performance.
In contrast, after filtering out noisy and irrelevant data, rectifying QFlow based on the high-quality data boosts the model performance significantly.



\subsection{Analytic Experiments on SCOPe}

\textbf{Universality of Flow Rectification.}
Note that, the flow rectification method used in our work is universal for various models.
As shown in Table~\ref{tab:SCOPe table mini} and Figure~\ref{fig:SCOPe fig}, applying flow rectification, we can improve the efficiency and effectiveness of FrameFlow as well.
This result highlights the broad utility of flow rectification as an operation that can enhance the performance of flow models on SO(3) spaces.
% We retrained FrameFlow and Rectified FrameFlow (ReFrameFlow) from scratch, and the comparison of results is shown in Table~\ref{tab:SCOPe table mini} and Figure~\ref{fig:SCOPe fig}. 
% Furthermore, the Reflow operation demonstrates its versatility and applicability across different flow models. 
% Both QFlow and FrameFlow benefit from the Reflow operation, which enhances performance, confirming that Reflow is not only effective with quaternion-based models but also provides significant improvements to matrix-based models like FrameFlow. 
% This highlights the broad utility of Reflow as an operation that can enhance the performance of flow models on SO(3) spaces.



\begin{table}[t]
\centering
\vspace{-5pt}
\caption{The Fraction scores of ReQFlow under different settings when generating backbones with length $N\in\{100, 150, 200, 250, 300\}$ by 500 steps.}
\small{
\tabcolsep=4pt
\begin{tabular}{ccc|ccc}
\toprule
Exponential& Flow& Data&\multicolumn{3}{c}{Sampling Steps}\\
Scheduler& Rectification & Filtering & 500 & 50 & 10\\
\midrule
\ding{55} & \ding{55} & \ding{55} & 0.040& 0.004&0.004 \\
\checkmark & \ding{55} & \ding{55} & 0.936 &0.812 & 0.332  \\
\checkmark & \checkmark &\ding{55} &0.716  & 0.704 &0.624 \\
\checkmark &\checkmark &\checkmark & 0.972 &0.912& 0.676  \\
\bottomrule
\label{tab:ablation}
\end{tabular}
}
\end{table}

\begin{table}[t]
    \centering
    \vspace{-5pt}
    \caption{Comparisons for various models on SCOPe. 
    For each metric of generation quality, we indicate the best and top-3 results in the same way as Table~\ref{tab:PDB main results} does.
    The inference time corresponds to generating a backbone with length $N=128$.}
    \small
    \begin{tabular}{lccccccc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c}{\textbf{Effciency}} & \multicolumn{2}{c}{\textbf{Designability}} & \multicolumn{1}{c}{\textbf{Diversity}} & \multicolumn{1}{c}{\textbf{Novelty}}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7}
        & Step
        & Time(s)
        & Fraction$\uparrow$
        & scRMSD$\downarrow$
        & TM$\downarrow$
        & TM$\downarrow$\\
        \midrule
        FrameFlow & 500 & 16.18  & 0.849 & 1.448$_{\pm\text{1.114}}$ & 0.397 & 0.858 \\
         & 50 & 1.69 & 0.820 & 1.546$_{\pm\text{1.316}}$ & \cellcolor{top2}0.379 &\cellcolor{top1} 0.836 \\
         & 20 & 0.69 & 0.713 & 1.918$_{\pm\text{1.495}}$  & 0.362 & 0.803 \\
         \midrule
        ReFrameFlow & 500 & 16.24 & 0.897 & 1.368$_{\pm\text{1.412}}$ & 0.403 & 0.857 \\
         & 50 & 1.65 & 0.903 & 1.291$_{\pm\text{0.763}}$ & 0.400 & 0.850 \\
         & 20 & 0.68 & 0.871 & 1.416$_{\pm\text{0.880}}$  & 0.401 & \cellcolor{top3}0.846 \\
         \midrule
        QFlow & 500 & 12.22  & 0.907 & 1.263$_{\pm\text{1.334}}$ & \cellcolor{top3}0.389 & 0.868 \\
         & 50 & 1.33 & 0.872 & 1.389$_{\pm\text{1.314}}$ & \cellcolor{top1}0.371 & 0.863 \\
         & 20 & 0.56 & 0.764 & 1.764$_{\pm\text{1.529}}$ & 0.367 & 0.814 \\
         \midrule
        ReQFlow & 500 & 12.18 &\cellcolor{top1} \textbf{0.972} & \cellcolor{top1}\textbf{1.043}$_{\pm\text{0.416}}$ & 0.416 & 0.868 \\
         & 50 & 1.27 & \cellcolor{top2}0.932 & \cellcolor{top2}1.162$_{\pm\text{0.812}}$ & 0.415 & 0.855 \\
         & 20 & 0.51 &\cellcolor{top3} 0.929 & \cellcolor{top3}1.214$_{\pm\text{0.633}}$ & 0.404 & \cellcolor{top2}0.844 \\
        \bottomrule
    \end{tabular}
    \label{tab:SCOPe table mini}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/designability_plot.pdf}
    \caption{A comparison for various methods on their designability with the reduction of sampling steps. Original data is in Table~\ref{tab:SCOPe table}.}
    \label{fig:SCOPe fig}
\end{figure}

\noindent\textbf{Superiority of Exponential-Format SLERP.}
The results in Table~\ref{tab:SCOPe table mini} and Figure~\ref{fig:SCOPe fig} indicate that QFlow and ReQFlow outperform their corresponding counterparts (FrameFlow and ReFrameFlow) in terms of designability across all sampling steps. 
In addition, QFlow methods are approximately 25\% faster than FrameFlow methods at each sampling step, demonstrating a significant speed advantage.
As we analyzed in Section~\ref{sec:rational}, the superiority of our models can be attributed to the better numerical stability and computational efficiency of quaternion calculations compared to the traditional matrix geodesic method. 


% \begin{table*}[htb]
%     \centering
%     \vspace{-5pt}
%     \caption{Unconditional protein backbone generation performance for 50 samples each length in $\{100, 150, 200, 250, 300\}$. We report the metrics from Section~\ref{sec:metrics}.}
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lccccccccc}
%         \toprule
%         \multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{Effciency}} & \multicolumn{2}{c}{\textbf{Designability}} & \multicolumn{2}{c}{\textbf{Diversity}} & \multicolumn{1}{c}{\textbf{Novelty}} & \multicolumn{2}{c}{\textbf{Sec. Struct.}}\\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-8} \cmidrule(lr){9-10}
%         & NFE
%         & Seconds ($\downarrow$)
%         & Fraction ($\uparrow$)
%         & scRMSD ($\downarrow$) 
%         & TM ($\downarrow$)
%         & Cluster ($\uparrow$)
%         & avg. max TM ($\downarrow$)
%         & Helix
%         & Strand\\
%         \midrule
%         PDB Dataset & & - & - & - & - & - & - & 0.390 & 0.230 \\
%         \midrule
%         RFDiffusion & 50 & & 0.908 & 1.047 ($\pm$1.497) & 0.369 & 0.877 & 0.823 ($\pm$0.069) & & \\
%         % Genie & & & & & & & & & \\
%         Genie2 & 1000 & & 0.908 & 1.132 ($\pm$1.389) & 0.370 & 0.948 & 0.759 ($\pm$0.084) & - & - \\
%         % Chroma & & & & & & & & \\
%         \midrule
%         FrameDiff-ICML & 500 & & 0.384 & 4.152 ($\pm$3.668) & 0.422 & 0.552 & 0.823 ($\pm$0.059) & 0.627 & 0.119 \\
%         FrameDiff-Improved & 500 & & 0.564 & 2.936 ($\pm$3.039) & 0.441 & 0.518 & 0.799 ($\pm$0.052) & 0.517 & 0.190 \\
%         FoldFlow-Base & 500 & & 0.624 & 3.080 ($\pm$3.449) & 0.469 & 0.429 & 0.870 ($\pm$0.038) & 0.914 & 0.000 \\
%         FoldFlow-SFM & 500 & & 0.636 & 3.031 ($\pm$3.589) & 0.411 & 0.566 & 0.848 ($\pm$0.050) & 0.921 & 0.005 \\
%         FoldFlow-OT & 500 & & 0.836 & 1.697 ($\pm$2.342) & 0.434 & 0.512 & 0.857 ($\pm$0.050) & 0.898 & 0.000 \\
%         FrameFlow & 500 & & 0.872 & 1.380 ($\pm$1.392) & 0.346 & 0.798 & 0.803 ($\pm$0.055) & 0.571 & 0.201\\
%         \midrule
%         % QFlow-v1-Base & 500 & & 0.772 & 2.025 ($\pm$2.590) & 0.452 & 0.404 & 0.875 ($\pm$0.047) & 0.921 & 0.000 \\
%         % QFlow-v1-Rectified & & & & & & & & & \\
%         QFlow & 500 & & 0.912 & 1.252($\pm$0.933) & 0.348 & & & 0.559 & 0.187\\
%         % Rectified QFlow & 500 & & & & & & & & \\
%         %  & 200 & & & & & & & & \\
%         %  & 10 & & & & & & & & \\
%         Rectified QFlow & 500 & & & & & & & & \\
%          & 100 & & & & & & & & \\
%          & 10 & & & & & & & & \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table*}



\section{Conclusion and Future Work}
In this study, we propose a rectified quaternion flow matching method for efficient and high-quality protein backbone generation. 
Leveraging quaternion-based representation and flow rectification, our method achieves state-of-the-art performance and significantly reduces inference time.
% \yue{In the near future, we plan to improve our method for generating high-quality long-chain protein backbones, including constructing a larger training dataset (RFDiffusion~\cite{watson2023novo}, Genie2~\cite{lin2024out} and Prote\'{i}na~\cite{geffner2025proteina} used much larger dataset), improving our model architecture (Prote\'{i}na~\cite{geffner2025proteina} was implemented with non-equivariant design and scaled to more than 400M parameters), and leveraging the knowledge within large-scale pre-training models (\yue{FoldFLow2~\cite{huguet2024sequence} utilized ESM2~\cite{lin2023evolutionary} to combine sequence information}).}
In the near future, we plan to improve our method for generating high-quality long-chain protein backbones. This will involve constructing a larger training dataset, building on approaches such as RFDiffusion~\cite{watson2023novo}, Genie2~\cite{lin2024out} and Prote\'{i}na~\cite{geffner2025proteina}. Additionally, we plan to refine our model architecture through two key strategies: increasing model capacity via parameter scaling and exploring non-equivariant design, drawing inspiration from the architecture of Prote\'{i}na~\cite{geffner2025proteina}. Furthermore, we intend to leverage the knowledge embedded in large-scale pre-training models, such as FoldFlow2~\cite{huguet2024sequence}, which incorporated sequence information using ESM2~\cite{lin2023evolutionary}.
As long-term goals, we will extend our method to conditional protein backbone generation for controllable protein design and explore its applications in side chain generation and full-atom protein generation.

\bibliographystyle{plain}
\bibliography{reqflow_arxiv}


% \clearpage
% \newpage
% \input{supp_arxiv.tex}
\newpage
\appendix
\onecolumn
\section{Proofs of Key Theoretical Results}
\subsection{The Angular Velocity under Exponential Scheduler}
\begin{proposition}
For spherical linear interpolation (SLERP) with angular velocity $\bm{\omega}$, when applying an exponential scheduler during inference:
\begin{equation}
    \bm{q}_t = \bm{q}_0 \otimes \exp\left( (1- e^{-\gamma t}) \log(\bm{q}_0^{-1} \otimes \bm{q}_1)\right),
\end{equation}
the resulting angular velocity evolves as $\hat{\bm{\omega}}_t = \gamma e^{-\gamma t} \bm{\omega}$.
\end{proposition}
\begin{proof}
The standard SLERP formulation in exponential form is:
\begin{equation}
    \bm{q}_t = \bm{q}_0 \otimes \exp\left( t \log(\bm{q}_0^{-1} \otimes \bm{q}_1)\right),
\end{equation}
where the relative rotation $\bm{q}_{\text{rel}} = \bm{q}_0^{-1} \otimes \bm{q}_1$ has logarithm map $\log(\bm{q}_{\text{rel}}) = \frac{1}{2}\phi\bm{u}$. The angular velocity is:
\begin{equation}\label{eq:app_omega}
    \bm{\omega} = 2 \cdot \log(\bm{q}_{\text{rel}}) = \phi\bm{u}.
\end{equation}
Introducing an exponential scheduler $\kappa(t) = 1 - e^{-\gamma t}$ with derivative $\kappa'(t) = \gamma e^{-\gamma t}$, the modified SLERP becomes:
\begin{equation}
    \bm{q}_t = \bm{q}_0 \otimes \exp\left( \kappa(t) \log(\bm{q}_{\text{rel}})\right).
\end{equation}
Differentiating with respect to time using the chain rule:
\begin{eqnarray}
\begin{aligned}
    \dot{\bm{q}}_t &= \bm{q}_0 \otimes \frac{d}{dt}\exp\left( \kappa(t)\log(\bm{q}_{\text{rel}})\right) \\
    &= \gamma e^{-\gamma t} \log(\bm{q}_{\text{rel}}) \otimes \bm{q}_0 \otimes \exp\left( \kappa(t)\log(\bm{q}_{\text{rel}})\right) \\
    &= \gamma e^{-\gamma t} \log(\bm{q}_{\text{rel}}) \otimes \bm{q}_t.
\end{aligned}
\end{eqnarray}
Applying the quaternion kinematics equation $\dot{\bm{q}} = \frac{1}{2}[0, \bm{\omega}^\top]^\top \otimes \bm{q}$ \cite{sola2017quaternion}, we solve for the effective angular velocity:
\begin{eqnarray}
\begin{aligned}
    [0, \hat{\bm{\omega}}_t^\top]^\top&= 2\dot{\bm{q}}_t \otimes \bm{q}_t^{-1} \\
    &= 2\gamma e^{-\gamma t} \log(\bm{q}_{\text{rel}}) \otimes \bm{q}_t \otimes \bm{q}_t^{-1} \\
    &= 2\gamma e^{-\gamma t} \log(\bm{q}_{\text{rel}}).
\end{aligned}
\end{eqnarray}
Substituting the angular velocity from Eq.~\ref{eq:app_omega} yields:
\begin{equation}
    \hat{\bm{\omega}}_t = \gamma e^{-\gamma t} \bm{\omega}.
\end{equation}
\end{proof}

\subsection{Proofs of The Theorems in Section~\ref{sec:reflow}}
% \xu{Because the proofs yield the same pipeline of~\cite{liu2022rectified}. You should first claim that the proofs are inspired by that work and derived based on the same techniques. What we did is extending and specifying the theoretical results in~\cite{liu2022rectified} for $\mathbb{S}^3$.}\yue{done.}
Our proofs yield the same pipeline used in~\cite{liu2022rectified}. 
The proofs are inspired by that work and derived based on the same techniques. 
What we did is extending and specifying the theoretical results in~\cite{liu2022rectified} for $\mathbb{S}^3$.
The original rotation process is $\{\bm{q}_t\}_{t\in[0,1]}$, where each $\bm{q}_t$ is a unit quaternion representating a rotation in $\text{SO}(3)$, $\bm{\omega}_t \in \mathbb{R}^3$ is the angular velocity at time $t$. The quaternion dynamics are given by
\begin{equation}\label{eq:app_dynamics}
    \dot{\bm{q}}_t = \frac{1}{2} [0, \bm{\omega}_t^\top]^\top \otimes \bm{q}_t \in T_{\bm{q}_t}(\mathbb{S}^3),
\end{equation}
where $T_{\bm{q}_t}(\mathbb{S}^3)$ is the tangent space at $\bm{q}_t$. We write $\bm{q}_0 \sim \mathcal{Q}_0$, $\bm{q}_1 \sim \mathcal{Q}_1$ for the initial and target distributions. For a given input coupling $(\bm{q}_0, \bm{q}_1)$, the exact minimum of $\mathcal{L}_{\text{SO}(3)}$ in Eq.~\ref{eq:objs} is achieved if
\begin{equation}\label{eq:app_conditional}
    \tilde{\bm{\omega}}_{\theta, t} = \tilde{\bm{\omega}}_t(\bm{q}, t) =\mathbb{E}[\bm{\omega}_t|\bm{q}_t = \bm{q}] \in \mathbb{R}^3,
\end{equation}
which is the expected angular velocity at point $\bm{q}$, time $t$.
We now define the rectified process $\{\bm{q}_t^{\prime}\}_{t\in[0,1]}$ by
\begin{equation}\label{eq:app_dynamics_rectified}
    \dot{\bm{q}}_t^{\prime} = \frac{1}{2} [0, \tilde{\bm{\omega}}_t(\bm{q}_t^{\prime}, t)^\top]^\top \otimes \bm{q}_t^{\prime}, \quad \bm{q}_0^{\prime} \sim \mathcal{Q}_0,
\end{equation}
\subsubsection{Proof of Theorems~\ref{theo:marginal}}
\begin{proof}
Consider any smooth test function $h: \mathbb{S}^3 \rightarrow \mathbb{R}$. By chain rule:
\begin{equation}
\frac{d}{dt}\mathbb{E}[h(\mathbf{q}_t)] =\mathbb{E}\bigl[\nabla_{\mathbb{S}^3}h(\bm{q}_t)\cdot\dot{\bm{q}}_t\bigr],
\end{equation}
where $\nabla_{\mathbb{S}^3}h$ is the gradient on the manifold. From the definition in Eq.~\ref{eq:app_dynamics}, since $\bm{\omega}_t$ is random, we rewrite inside the expectation by conditioning on $\bm{q}_t$:
\begin{equation}
\mathbb{E}\bigl[\nabla_{\mathbb{S}^3}h(\bm{q}_t)\cdot\dot{\bm{q}}_t\bigr]=
\mathbb{E} \Bigl[\nabla_{\mathbb{S}^3}h(\bm{q}_t) \cdot \tfrac12\bigl[0,\mathbb{E}(\bm{\omega}_t |\bm{q}_t)^\top\bigr]^\top\otimes\bm{q}_t \Bigr],
\end{equation}
because $\bm{\omega}_t | (\bm{q}_t = \bm{q}) $ has conditional mean $\tilde{\bm{\omega}}_t(\bm{q}, t)$, 
\begin{equation}
\frac{d}{dt}\,\mathbb{E}[h(\bm{q}_t)]=
\mathbb{E}\bigl[\nabla_{\mathbb{S}^3}h(\bm{q}_t)\,\cdot \tfrac12 [0,\tilde{\bm{\omega}}_t(\bm{q}_t, t)^\top]^\top\otimes\bm{q}_t\bigr].
\end{equation}
This evolution is exactly the \emph{weak (distributional) form} of the continuity equation:
\begin{equation}
 \partial_t\,\mu_t+
 \nabla \cdot \bigl(\tfrac12[0,\tilde{\bm{\omega}}_t(\bm{q}, t)^\top]^\top \otimes \bm{q} \cdot \mu_t\bigr)=0,
\end{equation}
where $\mu_t = \text{Law}(\bm{q}_t)$. According to Eq.~\ref{eq:app_dynamics_rectified}, That is exactly the same weak‐form evolution equation satisfied by the $\bm{q}_t^{\prime}$ process, where $\bm{\omega}$ is simply replaced by $\tilde{\bm{\omega}}_t$. If we let $\nu_t \mathrel{\mathop:}=\text{Law}(\bm{q}_t^\prime)$, it solves the same continuity equation with the same initial data $\nu_0 = \mu_0$. On a compact manifold like $\text{SO}(3)$, the continuity equation has a unique solution given an initial distribution. Hence $\mu_t=\nu_t$ at all times $t$. That is,
\begin{equation}
 \mathrm{Law}(\bm{q}_t^\prime)=
 \mathrm{Law}(\bm{q}_t),
 \quad
 \text{for all }t\in [0,1].
\end{equation}
\end{proof}

\subsubsection{Proof of Theorems~\ref{theo:cost}}
\begin{proof}
The net rotation from $\bm{q}_0$ to $\bm{q}_1$ can be given by integrating the angular velocity $\bm{\omega}_t \in \mathbb{R}^3$.
\begin{equation}
\log\bigl(\bm{q}_0^{-1}\otimes \bm{q}_1\bigr) = \frac{1}{2}\int_{0}^{1} \bm{\omega}_t\,dt,
\end{equation}
and similarly,
\begin{equation}
\log\bigl(\bm{q}_0^{\prime-1}\otimes \bm{q}_1^\prime\bigr) = \frac{1}{2}\int_{0}^{1} \tilde{{\bm{\omega}}}_t(\bm{q}_t^{\prime}, t)\,dt,
\end{equation}
Strictly speaking, one must keep track of the axis direction to ensure consistency, but the geodesic assumption here handles that. 
%Combined with a convex cost function $c: \mathbb{R}^{3} \rightarrow \mathbb{R}$, we have:
% \begin{equation}
% c\left(\log\bigl(\bm{q}_0^{\prime-1}\otimes \bm{q}_1^\prime\bigr)\right) = c\left(\int_{0}^{1} \tilde{{\bm{\omega}}}_t(\bm{q}_t^{\prime}, t)\,dt \right).
% \end{equation}
The rectified angular velocity $\tilde{\bm{\omega}}_t = \mathbb{E}[\bm{\omega}_t | \bm{q}_t]$ implies that the total rotation in the rectified process is a conditional expectation of the original rotation:
\begin{equation}\label{eq:app_conditional_expectation}
\log\left(\bm{q}_0^{\prime-1} \otimes \bm{q}_1^\prime\right) = \frac{1}{2}\int_0^1 \tilde{\bm{\omega}}_t \, dt = \frac{1}{2}\mathbb{E}\left[\int_0^1 \bm{\omega}_t \, dt \,\bigg|\, \{\bm{q_t^{\prime}}\}\right].
\end{equation}

Applying Jensen's inequality to the convex cost \(c\) over this conditional expectation:
\begin{equation}
c\left(\log\left(\bm{q}_0^{\prime-1} \otimes \bm{q}_1^\prime\right)\right) = c\left(\frac{1}{2}\mathbb{E}\left[\int_0^1 \bm{\omega}_t \, dt \,\bigg|\, \{\bm{q}_t^{\prime}\}\right]\right) \leq \mathbb{E}\left[\frac{1}{2}c\left(\int_0^1 \bm{\omega}_t \, dt\right) \,\bigg|\, \{\bm{q^{\prime}}_t\}\right].
\end{equation}
Taking the total expectation on both sides:
\begin{equation}
\mathbb{E}\left[C(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})\right] \leq \mathbb{E}\left[ \frac{1}{2}c\left(\int_0^1 \bm{\omega}_t \, dt\right)\right] = \mathbb{E}\left[C(\bm{q}_0, \bm{q}_1)\right].
\end{equation}
This final inequality establishes that the rectified coupling $(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})$ achieves equal or lower expected transport cost than the original coupling $(\bm{q}_0, \bm{q}_1)$.

% Taking the expectation on both sides:
% \begin{eqnarray}
%     \begin{aligned}
%         \mathbb{E}\Bigl[c(\log(\bm{q}_0^{\prime -1} \otimes\bm{q}_1^{\prime}))\Bigr] & \leq  \mathbb{E}\Bigl[c \int_{0}^{1}  \left(\tilde{{\bm{\omega}}}_t (\bm{q}_t^{\prime}, t)\right)\,dt\Bigr] \\
%         & \leq  \mathbb{E}\Bigl[ \int_{0}^{1}  c\left(\tilde{{\bm{\omega}}}_t (\bm{q}_t^{\prime}, t)\right)\,dt\Bigr], \quad \text{convexity of $c$, Jensen’s inequality}\\
%         & = \mathbb{E}\Bigl[\int_{0}^{1} c \left({\tilde{\bm{\omega}}}_t(\bm{q}_t, t) \right)\Bigr]\,dt, \quad \text{Theorem~\ref{theo:marginal}, the Marginal Preserving Property} \\
%         & = \mathbb{E}\Bigl[\int_{0}^{1} c \left( \mathbb{E}\bigl[\bm{\omega}_t|\bm{q}_t\bigr] \right)\Bigr]\,dt, \quad \text{Eq.~\ref{eq:app_conditional}.}\\
%         & \leq \mathbb{E}\Bigl[\int_{0}^{1}  \left( \mathbb{E}\bigl[c(\bm{\omega}_t)|\bm{q}_t\bigr] \right)\Bigr]\,dt, \quad \text{convexity of c, Jensen’s inequality} \\
%         &  = \int_{0}^{1}  \left( \mathbb{E}\bigl[c(\bm{\omega}_t)\bigr] \right)\,dt\\
%         & = \mathbb{E}\Bigl[c \int_0^1(\bm{\omega}_t)\, dt\Bigr] = \mathbb{E}\Bigl[c(\log(\bm{q}_0^{ -1} \otimes\bm{q}_1))\Bigr] 
%     \end{aligned}
% \end{eqnarray}
% Because $C(\bm{q}_0, \bm{q}_1) = c\left( \log(\bm{q}_0^{-1} \otimes \bm{q}_1)\right)$.
% Then, we have $\mathbb{E}[C(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})] \leq \mathbb{E}[C(\bm{q}_0, \bm{q}_1)]$.
% By the convexity of $c$, 
% \begin{equation}
% c\left(\int_{0}^{1} \tilde{{\bm{\omega}}}_t(\bm{q}_t^{\prime}, t)\,dt \right) \leq \int_{0}^{1} c\left(\tilde{{\bm{\omega}}}_t(\bm{q}_t^{\prime}, t) \right)\,dt. 
% \end{equation}
% Taking the expectation on both sides:
% \begin{eqnarray}
%     \begin{aligned}
%         \mathbb{E}\Bigl[c(\log(\bm{q}_0^{\prime -1} \otimes\bm{q}_1^{\prime}))\Bigr] & \leq \mathbb{E}\Bigl[ \int_{0}^{1} c \left(\tilde{{\bm{\omega}}}_t (\bm{q}_t^{\prime}, t)\right)\,dt\Bigr] \\
%         & = \int_{0}^{1} \mathbb{E}\bigl[c \left(\tilde{{\bm{\omega}}}_t (\bm{q}_t^{\prime}, t) \right)\bigr]\,dt \\
%         & = \int_{0}^{1} \mathbb{E}\bigl[c \left({\tilde{\bm{\omega}}}_t(\bm{q}_t, t) \right)\bigr]\,dt, \quad \text{Theorem~\ref{theo:marginal}, the Marginal Preserving Property} \\
%     \end{aligned}
% \end{eqnarray}
% Express $\tilde{\bm{\omega}}_t$ via conditional expectation:
% \begin{equation}
%     \tilde{\bm{\omega}}_t(\bm{q}_t, t) = \mathbb{E}[\bm{\omega}_t | \bm{q}_t],
% \end{equation}
% We have:
% \begin{equation}
%     \mathbb{E}\Bigl[c(\log(\bm{q}_0^{\prime -1} \otimes\bm{q}_1^{\prime}))\Bigr] \leq \int_{0}^{1} \mathbb{E}\bigl[c \left({\tilde{\bm{\omega}}}_t(\bm{q}_t, t) \right)\bigr]\,dt
% \end{equation}
% Taking the expectation on both sides:
% \begin{eqnarray}
%     \begin{aligned}
%         \mathbb{E}\Bigl[c(\log(\bm{q}_0^{\prime -1} \otimes\bm{q}_1^{\prime}))\Bigr] & \leq \mathbb{E}\Bigl[ \int_{0}^{1} c \left(\tilde{{\bm{\omega}}}_t (\bm{q}_t^{\prime}, t)\right)\,dt\Bigr] \\
%         & = \int_{0}^{1} \mathbb{E}\bigl[c \left(\tilde{{\bm{\omega}}}_t (\bm{q}_t^{\prime}, t) \right)\bigr]\,dt \\
%         & = \int_{0}^{1} \mathbb{E}\bigl[c \left({\tilde{\bm{\omega}}}_t(\bm{q}_t, t) \right)\bigr]\,dt, \quad \text{Theorem~\ref{theo:marginal}, the Marginal Preserving Property} \\
%     \end{aligned}
% \end{eqnarray}
% Express $\tilde{\bm{\omega}}_t$ via conditional expectation:
% \begin{equation}
%     \tilde{\bm{\omega}}_t(\bm{q}_t, t) = \mathbb{E}[\bm{\omega}_t | \bm{q}_t],
% \end{equation}
% Thus,
% \begin{eqnarray}
%     \begin{aligned}
%     c(\tilde{\bm{\omega}}_t(\bm{q}_t, t)) &= c(\mathbb{E}[\bm{\omega}_t | \bm{q}_t])\\
%     &\leq \mathbb{E}[c(\bm{\omega}_t)|\bm{q}_t], \quad \text{Jensen Inequality.}
%     \end{aligned}
% \end{eqnarray}

\end{proof}

\subsubsection{Proof of Corollary~\ref{cor:nonconstant_speed}}
\begin{proof}
Suppose the original process has the nonconstant angular velocity $\bm{\omega}_t = a(t)\bm{u}$ (fixed axis), with $\tau = \frac{1}{2}\int_0^1 a(t) dt$.
\begin{equation}
\log\bigl(\bm{q}_0^{-1}\otimes \bm{q}_1\bigr) = \frac{1}{2}\int_{0}^{1} \bm{\omega}_t\,dt = \frac{1}{2}\bm{u} \int_0^1a(t)\,dt = \tau \bm{u}
\end{equation}
Recall that the rectified angular velocity is:
\begin{equation}
    \tilde{\bm{\omega}}_t(\bm{q}, t) = \mathbb{E}[\bm{\omega}_t | \bm{q}_t]
\end{equation}
Since $\bm{\omega}_t = a(t)\bm{u}$, we simply get:
\begin{equation}
\tilde{\bm{\omega}}_t(\bm{q}, t) = \mathbb{E}[a(t) | \bm{q}_t] \bm{u}
\end{equation}
The total rotation from $\bm{q}_0^{\prime}$ to the $\bm{q}_1^{\prime}$ in the rectified process satisfies:
\begin{equation}
\log(\bm{q}_0^{\prime-1} \otimes \bm{q}_1^\prime) = \frac{1}{2}\int_0^1 \tilde{\bm{\omega}}_t(\bm{q}_t^{\prime}, t) dt = \frac{1}{2}\left(\int_0^1 \mathbb{E}[a(t) \,|\, \bm{q}_t^{\prime}] dt\right)\bm{u}.
\end{equation}
Let $\tau' = \frac{1}{2}\int_0^1 \mathbb{E}[a(t) \,|\, \bm{q}_t^{\prime}] dt$. Thus,
\begin{equation}
\log(\bm{q}_0^{\prime-1} \otimes \bm{q}_1^\prime) = \tau' \bm{u}
\end{equation}
Because $\tau = \frac{1}{2}\int_0^1 a(t) dt$, $\tau' = \frac{1}{2}\int_0^1 \mathbb{E}[a(t) \,|\, \bm{q}_t] dt$, and Eq.~\ref{eq:app_conditional_expectation} in Theorem~\ref{theo:cost}, we note
\begin{equation}
    \tau' \bm{u} = \frac{1}{2} \bm{u}\left(\int_0^1 \mathbb{E}[a(t) \,|\, \bm{q}_t^{\prime}] dt\right) =\frac{1}{2}\mathbb{E}\left[\int_0^1 a(t) \bm{u} \, dt \,\bigg|\, \{\bm{q_t^{\prime}}\}\right] = \mathbb{E}[\tau\bm{u}|\{\bm{q_t^{\prime}}\}]
\end{equation}
For the coupling $(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})$, the cost is:
\begin{equation}
    C(\bm{q}_0^{\prime}, \bm{q}_1^{\prime}) = c(\tau'\bm{u}).
\end{equation}
Since $\tau' \bm{u} = \mathbb{E}[\tau\bm{u}|\{\bm{q_t^{\prime}}\}]$, convexity of $c$ implies Jensen’s inequality in conditional form:
\begin{equation}
    c(\tau'\bm{u}) = c(\mathbb{E}[\tau\bm{u}|\{\bm{q_t^{\prime}}\}]) \leq \mathbb{E}[c(\tau\bm{u})|\{\bm{q_t^{\prime}}\}]
\end{equation}
Next, take unconditional expectation on both sides. By the law of total expectation (tower property),
\begin{equation}
    \mathbb{E}[c(\tau'\bm{u})] \leq \mathbb{E}[c(\tau\bm{u})].
\end{equation}
Since $c(\tau \bm{u}) = c(\log\bigl(\bm{q}_0^{-1}\otimes \bm{q}_1\bigr)) = C(\bm{q}_0, \bm{q}_1)$ and $c(\tau' \bm{u})=C(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})$. Therefore,
\begin{equation}
    \mathbb{E}[C(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})] \leq \mathbb{E}[C(\bm{q}_0, \bm{q}_1)].
\end{equation}

% Given the cost $ C(\bm{q}_0, \bm{q}_1) = c\left(\log(\bm{q}_0^{-1} \otimes \bm{q}_1)\right) $, where $ c $ is homogeneous of order $ m \in (0,1] $, we have:
% \begin{equation}
%     c(a \bm{u}) = |a|^m c(\bm{u}), \quad \forall a \in \mathbb{R}, \, \bm{u} \in \mathbb{R}^3.
% \end{equation}
% For a fixed axis $\bm{u}$ (unit vector), the half of the total rotation angle is $\tau = \|\log(\bm{q}_0^{-1} \otimes \bm{q}_1)\|$, so:
% \begin{equation}
% C(\bm{q}_0, \bm{q}_1) = c(\tau \bm{u}) = \tau^m c(\bm{u}).
% \end{equation}
% Suppose the original process has the nonconstant angular velocity $\bm{\omega}_t = a(t)\bm{u}$ (fixed axis), with $\tau = \frac{1}{2}\int_0^1 a(t) dt$. The rectified angular velocity is \(\tilde{\bm{\omega}}_t = \mathbb{E}[a(t) \bm{u} \,|\, \bm{q}_t] = \mathbb{E}[a(t) \,|\, \bm{q}_t] \bm{u}\), implying the half of the rotation displacement is:
% \begin{equation}
% \log(\bm{q}_0^{\prime-1} \otimes \bm{q}_1^\prime) = \frac{1}{2}\int_0^1 \tilde{\bm{\omega}}_t(\bm{q}_t^{\prime}, t) dt = \frac{1}{2}\left(\int_0^1 \mathbb{E}[a(t) \,|\, \bm{q}_t] dt\right)\bm{u}.
% \end{equation}

% Let $\tau' = \frac{1}{2}\int_0^1 \mathbb{E}[a(t) \,|\, \bm{q}_t] dt$. 
% Since the axis $\bm{u}$ is preserved, the rectified cost becomes:
% \begin{equation}
% C(\bm{q}_0^{\prime}, \bm{q}_1^{\prime}) = (\tau')^m c(\bm{u}).
% \end{equation}
% By Eq.~\ref{eq:app_conditional_expectation} in Theorem~\ref{theo:cost}, the rotation displacement satisfies:
% \begin{equation}
% \tau' \bm{u} = \frac{1}{2}\mathbb{E}\left[\int_0^1 a(t) \bm{u} dt \,\bigg|\, \{\bm{q}_t\}\right] = \mathbb{E}[\tau \bm{u} \,|\, \{\bm{q}_t\}\}.
% \end{equation}

% Thus, $\tau' = \mathbb{E}[\tau \,|\, \{\bm{q}_t\}\}$. Applying Jensen’s inequality to the convex function $f(x) = x^m$ for $m \in (0,1]$, which is concave for $m<1$ and linear for $m=1$:
% \begin{equation}
% \mathbb{E}\left[(\tau')^m\right] \leq \mathbb{E}\left[\mathbb{E}[\tau \,|\, \{\bm{q}_t\}]^m\right] \leq \mathbb{E}[\tau^m].
% \end{equation}

% Here, the first inequality uses the tower property of expectation, and the second uses the concavity of \(f(x) = x^m\) for \(m \in (0,1)\). However, since \(c(\bm{u})\) is constant for a fixed axis, the total expected cost reduction follows:
% \begin{equation}
% \mathbb{E}[C(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})] = \mathbb{E}[(\tau')^m c(\bm{u})] \leq \mathbb{E}[\tau^m c(\bm{u})] = \mathbb{E}[C(\bm{q}_0, \bm{q}_1)].
% \end{equation}

% Even with nonconstant speed along a fixed axis, the rectified coupling $(\bm{q}_0^{\prime}, \bm{q}_1^{\prime})$ reduces the expected transport cost due to the concave transformation of the angle $\tau$ (for $m < 1$) or preserves it (for $m = 1$), aligning with Theorem~\ref{theo:cost}.
\end{proof}




\section{Implementation Details}
\subsection{Ensuring The Shortest Geodesic Path on \text{SO}(3)}
When we interpolate two quaternions by using SLERP in an exponential format (Eq.~\ref{eq:slerp_e}), 
due to the double-cover property of quaternions (where every 3D rotation is represented by two antipodal unit quaternions), 
it is possible that the inner product $\langle \bm{q}_0, \bm{q}_1\rangle < 0$, which means that $\bm{q}_0$ and $\bm{q}_1$ lie in opposite hemispheres.
In such a situation, we apply $-\bm{q}_1$ in Eq.~\ref{eq:slerp_e}, ensuring the shortest geodesic path on $\text{SO}(3)$. 

%  following the density:
% \begin{equation}
% f = \frac{1 - \cos \omega}{\pi} \sum_{l=0}^\infty (2l + 1) e^{-l(l+1)e^2} \frac{\sin\left(\left(l + \frac{1}{2}\right)\omega\right)}{\sin(\frac{\omega}{2})}.
% \end{equation}
\subsection{Auxiliary Loss}\label{ap:aux loss}
We adopt the auxiliary loss from~\cite{yim2023se} to discourage physical violations such as chain breaks or steric clashes. Let $\mathcal{A} = [\mathrm{N}, \mathrm{C}_\alpha, \mathrm{C}, \mathrm{O}]$ be the collection of backbone atoms. The first term penalizes deviations in backbone atom coordinates:
\begin{equation}
\mathcal{L}_{\text{bb}} = \frac{1}{4N} \sum_{n=1}^N \sum_{a \in \mathcal{A}} \left\| a_n - \hat{a}_n \right\|^2,
\end{equation}
where $a_n$ is the ground-truth atom position, $\hat{a}_n$ is our predicted position, $N$ represents the number of residues.
The second loss is a local neighborhood loss on pairwise atomic distances,
\begin{equation}
\mathcal{L}_{\text{dis}} = \frac{1}{Z} \sum_{n,m=1}^N \sum_{a,b \in \mathcal{A}} \mathbf{1}\{d_{ab}^{nm} < 0.6\} \|d_{ab}^{nm} - \hat{d}_{ab}^{nm}\|^2,
\end{equation}
\begin{equation}
Z = \left(\sum_{n,m=1}^N \sum_{a,b \in \mathcal{A}} \mathbf{1}\{d_{ab}^{nm} < 0.6\}\right) - N,
\end{equation}
% where $d_{ab}^{nm} = \|a_n - b_m\|$ is the true atomic distance between atoms $a, b \in \mathcal{A}$ for residue $n$ and $m$. The predicted pairwise atomic distance is $\hat{d}_{ab}^{nm} = \|\hat{a}_n - \hat{b}_m\|$, 
where $d_{ab}^{nm} = \|a_n - b_m\|$ and $\hat{d}_{ab}^{nm} = \|\hat{a}_n - \hat{b}_m\|$ represent true and predicted inter-atomic distances between atoms $a, b \in \mathcal{A}$ for residue $n$ and $m$.
$\mathbf{1}$ is an indicator, signifying that only penalize atoms within 0.6nm($6\text{\AA}$). The full auxiliary loss can be written as
\begin{equation}
\mathcal{L}_{\text{aux}} = \mathcal{L}_{\text{bb}} + \mathcal{L}_{\text{dis}}.
\end{equation}

\subsection{The Schemes of Training and Inference Algorithms}\label{app:alg}
The schemes of our training and inference algorithms are shown below.

\begin{algorithm}
\caption{Training Procedure of QFlow}
\label{alg:qflow_training}
\begin{algorithmic}[1]
\setlength{\baselineskip}{1.10\baselineskip} % 行间距调整为1.3倍
\REQUIRE Training dataset $\mathrm{T}_1^{\mathcal{D}} = \bigl\{\{\mathrm{T^{j}_1 = (\bm{x}_1^j, \bm{q}_1^j)}\}_{j=1}^{N_i}\bigr\}_{i=1}^D$, model $\mathcal{M}_\theta$, number of epochs $N$
\STATE Initialize model parameters $\theta$
\FOR{epoch $= 1$ to $N$}
    \FOR{each mini-batch $\mathrm{T}_1^\mathcal{B} \subset \mathrm{T}_1^\mathcal{D}$}
        \STATE Sample $t^{\mathcal{B}}\sim \mathcal{U}[0, 1]$ , $\mathrm{T}_0^{\mathcal{B}} \sim \mathcal{T}_0 \times \mathcal{Q}_0$
        \STATE Interpolate translations: $\bm{x}_t^{\mathcal{B}} = \text{Linear}(\bm{x}_0^{\mathcal{B}} , \bm{x}_1^{\mathcal{B}}, t^{\mathcal{B}})$ \hfill Eq.~\ref{eq:linear}
        \STATE Interpolate rotations: $\bm{q}_t^{\mathcal{B}} = \text{SLERP-Exp}(\bm{q}_0^\mathcal{B},\bm{q}_1^\mathcal{B}, t^{\mathcal{B}})$ \hfill Eq.~\ref{eq:slerp_e}
        \STATE Predict targets: $\bm{x}_{\theta, 1}^{\mathcal{B}}, \bm{q}_{\theta, 1}^{\mathcal{B}} = \mathcal{M}_{\theta}(\mathrm{T}_t^{\mathcal{B}}, t^{\mathcal{B}})$
        \STATE Compute loss $\mathcal{L}(\theta; \bm{x}_t^{\mathcal{B}}, \bm{q}_t^{\mathcal{B}}, \bm{x}_{\theta, 1}^{\mathcal{B}}, \bm{q}_{\theta,1}^{\mathcal{B}})$ \hfill Eq.~\ref{eq:loss}
        \STATE Compute gradient $\nabla_\theta \mathcal{L}$
        \STATE Update parameters: $\theta \gets \theta - \eta \nabla_\theta \mathcal{L}$
    \ENDFOR
\ENDFOR
\STATE\textbf{Return:} Trained model parameters $\theta^*$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Inference}
\label{alg:inference}
\begin{algorithmic}[1]
\setlength{\baselineskip}{1.10\baselineskip} % 行间距调整为1.3倍
\REQUIRE Trained model $\mathcal{M}_\theta$, random noise $\mathrm{T}_0 \sim \mathcal{T}_0 \times \mathcal{Q}_0$, number of steps $L$, rotation acceleration constant $\gamma$
\STATE Initialize $t = 0$, $\Delta t = \frac{1}{L}$
\FOR{step $= 1$ to $L$}
    % \STATE Interpolate translations: $\bm{x}_t = \text{Linear}(\bm{x}_0 , \bm{x}_1, t)$ \hfill Eq.~\ref{eq:linear}
    % \STATE Interpolate rotations: $\bm{q}_t = \text{SLERP-Exp}(\bm{q}_0,\bm{q}_1, t)$ \hfill Eq.~\ref{eq:slerp_e}
    \STATE Predict targets: $\bm{x}_{\theta, 1}, \bm{q}_{\theta, 1} = \mathcal{M}_{\theta}(\mathrm{T}_t, t)$
    \STATE Compute velocity: $\bm{v}_{\theta, t}$, $\bm{\omega}_{\theta, t}$ \hfill Eq.~\ref{eq:velocity}
    \STATE Update translations: $\bm{x}_{t + \Delta t} \leftarrow \bm{x_t} + \bm{v}_{\theta, t} \cdot \Delta t$ \hfill Eq.~\ref{eq:euler_trans}
    \STATE Update rotations: $  \bm{q}_{t + \Delta t} \leftarrow \bm{q}_{t} \otimes \exp\Bigl(\frac{1}{2}\Delta t \cdot \gamma e^{-\gamma t} \bm{\omega}_{\theta, t}\Bigr)$ \hfill Eq.~\ref{eq:infer_slerp}
    \STATE Update states: $t \leftarrow t + \Delta t$, $\mathrm{T}_t \leftarrow \mathrm{T}_{t + \Delta t}$
\ENDFOR
\STATE\textbf{Return:} generated backbone frame $\mathrm{T}_1$
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{Training Procedure of ReQFlow}
\label{alg:recqflow_training}
\begin{algorithmic}[1]
\setlength{\baselineskip}{1.10\baselineskip} % 行间距调整为1.3倍
\REQUIRE Trained QFlow model $\mathcal{M}_\theta$, number of epochs $N$
\STATE Sample noise $\mathrm{T}_0^{\prime\mathcal{D}} \sim \mathcal{T}_0 \times \mathcal{Q}_0$
\STATE Create flow rectification pairs: $(\mathrm{T_0^{\prime }, \mathrm{T}_1^{\prime}})^{\mathcal{D}}$ \hfill Alg.~\ref{alg:inference}
\FOR{epoch $= 1$ to $N$}
    \FOR{each mini-batch $(\mathrm{T_0^{\prime }, \mathrm{T}_1^{\prime}})^{\mathcal{B}} \subset (\mathrm{T_0^{\prime }, \mathrm{T}_1^{\prime}})^{\mathcal{D}}$}
        \STATE Sample $t^{\mathcal{B}}\sim \mathcal{U}[0, 1]$
        \STATE Interpolate translations: $\bm{x}_t^{\mathcal{\prime B}} = \text{Linear}(\bm{x}_0^{\mathcal{\prime B}} , \bm{x}_1^{\mathcal{\prime B}}, t^{\mathcal{B}})$ \hfill Eq.~\ref{eq:linear}
        \STATE Interpolate rotations: $\bm{q}_t^{\mathcal{\prime B}} = \text{SLERP-Exp}(\bm{q}_0^\mathcal{\prime B},\bm{q}_1^\mathcal{\prime B}, t^{\mathcal{B}})$ \hfill Eq.~\ref{eq:slerp_e}
        \STATE Predict targets: $\bm{x}_{\theta, 1}^{\mathcal{\prime B}}, \bm{q}_{\theta, 1}^{\mathcal{\prime B}} = \mathcal{M}_{\theta}(\mathrm{T}_t^{\mathcal{\prime B}}, t^{\mathcal{B}})$
        \STATE Compute loss $\mathcal{L}(\theta; \bm{x}_t^{\prime \mathcal{B}}, \bm{q}_t^{\mathcal{\prime B}}, \bm{x}_{\theta, 1}^{\mathcal{\prime B}}, \bm{q}_{\theta,1}^{\mathcal{\prime B}})$ \hfill Eq.~\ref{eq:loss}
        \STATE Compute gradient $\nabla_\theta \mathcal{L}$
        \STATE Update parameters: $\theta \gets \theta - \eta \nabla_\theta \mathcal{L}$
    \ENDFOR
\ENDFOR
\STATE\textbf{Return:} Trained model parameters $\theta^*$
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}[t]  % [t] positions the algorithm at the top (optional placement)
% \caption{Training}  
% \label{alg:your_algorithm}  % For cross-referencing

% \begin{algorithmic}[1]  % [1] enables line numbering
% \REQUIRE Input parameters (e.g., training data $X$, learning rate $\eta$)
% \STATE Initialize parameters $\theta \sim \mathcal{N}(0, 1)$  
% \FOR{iteration $t = 1$ to $T$}  
%     \STATE Sample mini-batch $\{(x_i, y_i)\}_{i=1}^B \gets X$  
%     \STATE Forward pass: $z \gets f_\theta(x)$  
%     \STATE Compute loss: $\mathcal{L} \gets \frac{1}{B}\sum_{i=1}^B \ell(y_i, z_i)$  
    
%     \IF{using gradient clipping}  
%         \STATE Compute gradient: $g \gets \nabla_\theta \mathcal{L}$  
%         \STATE Clip gradient: $g \gets \text{clip}(g, \gamma)$  
%     \ELSE  
%         \STATE $g \gets \nabla_\theta \mathcal{L}$  
%     \ENDIF  
    
%     \STATE Update parameters: $\theta \gets \theta - \eta g$  
% \ENDFOR  

% \WHILE{not converged}  
%     \STATE Perform operation...  
% \ENDWHILE  

% \RETURN Model parameters $\theta$
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[H]
% \caption{Training} % 算法标题
% \label{alg:training} % 用于引用的标签
% \KwIn{Training dataset $\mathcal{D}$, learning rate $\eta$, epochs $T$}
% \KwOut{Trained model parameters $\theta^*$}
% Initialize parameters $\theta_0$\;
% \For{epoch $t \leftarrow 1$ \KwTo $T$}{
%     Shuffle training data $\mathcal{D}$\;
%     \For{batch $B_i \in \mathcal{D}$}{
%         Compute loss $\mathcal{L}(\theta_t, B_i)$\;
%         Compute gradients $\nabla_\theta \mathcal{L}$\;
%         Update parameters: $\theta_{t+1} \leftarrow \theta_t - \eta \nabla_\theta \mathcal{L}$\;
%     }
% }
% \Return{Best parameters $\theta^*$}\;
% \end{algorithm}

% \begin{algorithm}[H]
% \caption{Inference}
% \label{alg:inference}
% \KwIn{Input data $x$, trained parameters $\theta^*$}
% \KwOut{Prediction $y$}
% Compute hidden representation: $h \leftarrow f_\theta(x)$\;\\
% Apply activation: $a \leftarrow \sigma(h)$\;
% Compute output: $y \leftarrow \text{softmax}(a)$\;
% \Return{Prediction $y$}\;
% \end{algorithm}

% \textbf{Inference annealing}
% Following ~\cite{bose2023se}, we adapt infernece annealing to improve the stability and designability of generated protein structures by regulating velocity during inference. Instead of following standard ODE/SDE formulations, it applies a time-dependent scaling factor, typically $i(t) = ct$, to prevent excessive velocity growth in later stages. This adjustment ensures smoother generation, leading to more stable and diverse protein backbones.



\subsection{Data Statistics and Hyperparameter Settings}

We follow~\cite{yim2023se} to construct PDB dataset. The dataset was downloaded on December 17, 2024. We then applied a length filter (60–512 residues) and a resolution filter ($<$5\AA) to select high-quality structures. To further refine the dataset, we processed each monomer using DSSP~\cite{kabsch1983dictionary}, removing those with more than 50\% loops to ensure high secondary structure content. After filtering, 23,366 proteins remained for training.
We directly use the SCOPe dataset preprocessed by~\cite{yim2023fast} for training, which consists of 3,673 proteins after filtering. The distribution of dataset length is shown on Figure~\ref{fig:dataset distribution}.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/dataset_sequence_length_distribution.pdf}
    \caption{The length distribution of PDB and SCOPe dataset we use for training.}
    \label{fig:dataset distribution}
\end{figure}


When conducting reflow, we first generated a large amount of data to create the training dataset and then applied filtering to refine it. The filtering criteria were as follows: for proteins with lengths $\leq$ 400, we selected samples with scRMSD $\leq$ 2; for proteins with lengths $\geq$ 400, we included samples with either scRMSD $\leq$ 2 or TM-score $\geq$ 0.9. 
We also remove those with more than 50\% loop and those with max 4\% radius gyration. For the PDB dataset, we generated 20 proteins for each length in $\{60, 61, \dots, 512\}$, resulting in a reflow dataset containing 7,653 sample-noise pairs. For the SCOPe dataset, we generated 50 proteins for each length in $\{60, 61, \dots, 128\}$, producing a reflow dataset with 3,167 sample-noise pairs.

We set the batch size to 128 for an 80G GPU and use a learning rate of 0.0001. 
During inference, if exponential schedule is applied, the rate is set to 10.

\subsection{Baselines}
We compare our work with state-of-the-art methods in the community, including \href{https://github.com/aqlaboratory/genie2}{\textbf{Genie2}}, \href{https://github.com/RosettaCommons/RFdiffusion}{\textbf{RFdiffusion}}, \href{https://github.com/DreamFold/FoldFlow}{\textbf{FoldFlow/FoldFlow2}},  \href{https://github.com/microsoft/protein-frame-flow}{\textbf{FrameFlow}}, and \href{https://github.com/jasonkyuyim/se3_diffusion}{\textbf{FrameDiff}}. 
We use the default checkpoints and parameters provided in these methods' repositories for our comparisons. 

\subsection{Metrics}
% We list the implementation details of metrics as follows.

% \textbf{Designability}. A protein backbone is considered designable if at least one amino acid sequence can fold into its structure. Our evaluation follows the methodology described in~\cite{yim2023se}. Specifically, for each generated backbone, we sample eight sequences using ProteinMPNN~\cite{dauparas2022robust} with a temperature of 0.1. The folded structures of these sequences are then predicted using ESMFold~\cite{lin2023evolutionary}, and their root mean square deviation (RMSD) is computed against the original backbone. A sample is classified as designable if its lowest RMSD—referred to as self-consistency RMSD (scRMSD)—is below 2 \AA. The overall designability of a model is quantified as the fraction of samples that satisfy this condition.

Following existing work~\cite{geffner2025proteina,yim2023se,yim2023fast, bose2023se, huguet2024sequence}, we apply the metrics below to evaluate various methods.

\textbf{Designability}. We use this metric to evaluate whether a protein backbone can be formed by folding an amino acid chain. For each backbone, we generate 8 sequences with ProteinMPNN~\cite{dauparas2022robust} at temperature 0.1, and predict their corresponding structures using ESMFold~\cite{lin2023evolutionary}. Then we compute the minimum RMSD (known as scRMSD) between the predicted structures and the backbone sampled by the model. The designability score (denoted as ``fraction'' in this work) is the percentage of samples satisfying scRMSD$\leq$2\AA.

% The concept of \textbf{Designability} addresses whether a proposed protein backbone can be formed by folding some amino acid chain. 
% % Our assessment follows~\cite{yim2023se}. 
% For each generated backbone, we first derive eight sequences via ProteinMPNN~\cite{dauparas2022robust} (temperature=0.1). 
% We then predict the structures these sequences would form using ESMFold~\cite{lin2023evolutionary}. 
% Structural similarity to the original backbone is measured using RMSD. 
% A sequence sample passes the designability check if its best-matching predicted structure (lowest RMSD, known as scRMSD) is within 2\AA~of the target backbone. 
% The model's overall designability score (denoted as ``fraction'' in this work) is the percentage of samples satisfying scRMSD$\leq$2\AA.

% \textbf{Diversity}. The measure of diversity we report follows the methodology from ~\cite{bose2023se}.For each protein length specified above, we compute the average pairwise TM-score among designable samples, and then aggregate these averages across lengths. Since TM-scores range from zero to one, where higher scores indicate greater similarity, lower scores are preferable for this metric.
\textbf{Diversity}. This metric quantifies the diversity of the generated backbones. This involves calculating the average pairwise structural similarity among designable samples, broken down by protein length. Specifically, for each length $L$ under consideration, let $S_L$ be the set of designable structures. We compute $\text{TM}(b_i, b_j)$ for all distinct pairs $(b_i, b_j)$ within $S_L$. The mean of these TM-scores represents the diversity for length $L$. The final diversity score is the average of these means across all tested lengths $L$. Since TM-scores closer to 1 indicate higher similarity, superior diversity is reflected by lower values of this aggregated score.


% To assess \textbf{Diversity}, we employ the metric and methodology proposed by~\cite{bose2023se}. This involves calculating the average pairwise structural similarity among designable samples, broken down by protein length. Specifically, for each length $L$ under consideration, let $S_L$ be the set of designable structures. We compute $\text{TM}(b_i, b_j)$ for all distinct pairs $(b_i, b_j)$ within $S_L$. The mean of these TM-scores represents the diversity for length $L$. The final diversity score is the average of these means across all tested lengths $L$. Since TM-scores closer to 1 indicate higher similarity, superior diversity is reflected by lower values of this aggregated score.



\begin{table}[h!]
    \centering
    % \vspace{-5pt}
    \caption{Unconditional protein backbone generation performance for 10 samples each length in $\{60, 61, \cdots, 128\}$. We report the metrics from Section~\ref{sec:metrics} and we indicate the best and top-3 results in the same way as Table~\ref{tab:PDB main results} does.}
    \setlength{\tabcolsep}{3pt} 
    \small{
    \begin{tabular}{lcccccccc}
        \toprule
        \multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{Effciency}} & \multicolumn{2}{c}{\textbf{Designability}} & \multicolumn{1}{c}{\textbf{Diversity}} & \multicolumn{1}{c}{\textbf{Novelty}} & \multicolumn{2}{c}{\textbf{Sec. Struct.}}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-9}
        & Step
        & Time(s)
        & Fraction$\uparrow$
        & scRMSD$\downarrow$ 
        & TM$\downarrow$
        & TM$\downarrow$
        & Helix
        & Strand\\
        \midrule
        Scope Dataset & & - & - & - & - & - & 0.330 & 0.260 \\
        \midrule
        FrameFlow & 500 &16.18  & 0.849 & 1.448 ($\pm$1.114) & 0.397 &0.858 ($\pm$0.059) & 0.439 &0.236 \\
         & 400 &13.43 &0.864 &1.353 ($\pm$0.890) &0.380 &0.859 ($\pm$0.067) &0.452 &0.229 \\
         & 300 &9.80 & 0.861&1.422 ($\pm$1.178)  &0.383 &0.870 ($\pm$0.062) &0.449 &0.230 \\
         & 200 & 6.61&0.842  &1.496 ($\pm$1.411)  &\cellcolor{top3}0.378 &0.854 ($\pm$0.062) &0.437 &0.237 \\
         & 100 &3.19 & 0.823 &1.517 ($\pm$1.228)  &\cellcolor{top3}0.378 &0.848 ($\pm$0.061) &0.426 &0.238 \\
         & 50 &1.69 & 0.820 & 1.546 ($\pm$1.316) &0.379 &\cellcolor{top3}0.836 ($\pm$0.064) &0.441 &0.228 \\
         & 20 &0.69 & 0.713 & 1.918 ($\pm$1.495)  &0.362 &0.803 ($\pm$0.071) &0.416 &0.219 \\
         & 10 &0.35 &0.504 &2.924 ($\pm$2.362) &0.344 &0.782 ($\pm$0.084)&0.363 &0.213 \\
         \midrule
        ReFrameFlow & 500 &16.24 &0.897 &1.368 ($\pm$1.412) &0.403 &0.857  ($\pm$0.052) &0.501 &0.187 \\
         & 400 & 13.29&0.893 &1.328  ($\pm$0.763) &0.402 &0.858  ($\pm$0.052) &0.489 &0.202 \\
         & 300 & 10.27&0.888 &1.313  ($\pm$0.686) &0.401 &0.860  ($\pm$0.047) &0.485 &0.199 \\
         & 200 & 6.60&0.907 &1.326  ($\pm$0.761) &0.403 &0.852  ($\pm$0.051) &0.482 &0.206 \\
         & 100 & 3.65&0.886 &1.322 ($\pm$0.804)&0.408 &0.853 ($\pm$0.057) &0.499 &0.201 \\
         & 50 & 1.65&0.903 &1.291  ($\pm$0.763) &0.400 &0.850  ($\pm$0.053) &0.504 &0.202 \\
         & 20 & 0.68&0.871 &1.416 ($\pm$0.880)  &0.401 &0.846 ($\pm$0.050) &0.528 &0.190 \\
         & 10 & 0.33 &0.806 &1.696 ($\pm$1.093) &0.390 &\cellcolor{top1}\textbf{0.814 ($\pm$0.056)} &0.496 &0.192 \\
        \midrule
        QFlow & 500 &12.22  & 0.907 & 1.263 ($\pm$1.334) & 0.389 & 0.868 ($\pm$0.057) & 0.498 & 0.214 \\
         & 400  &10.11 & 0.907 & 1.199 ($\pm$0.847)&0.390 & 0.873 ($\pm$0.060)&0.476 &0.223\\
         & 300 &7.25 & 0.910 &1.243 ($\pm$1.027) &0.393 & 0.876 ($\pm$0.056) &0.503 &0.209 \\
         & 200 &4.78 & 0.877 &1.309 ($\pm$1.208) &0.389 &0.864 ($\pm$0.068) &0.481 &0.224 \\
         & 100 &2.48 & 0.903 &1.283 ($\pm$1.027) &0.385 &0.884 ($\pm$0.052) &0.476 &0.225 \\
         & 50 &1.33 & 0.872 &1.389 ($\pm$1.314) &\cellcolor{top1}\textbf{0.371} &0.863 ($\pm$0.064) &0.491 &0.206 \\
         & 20 & 0.56& 0.764 &1.764 ($\pm$1.529) &0.367 &0.814 ($\pm$0.071) &0.492 &0.192 \\
         & 10 & 0.29& 0.565 &2.589 ($\pm$2.216) &0.348 &0.772($\pm$0.081) &0.467 &0.167 \\
         \midrule
        ReQFlow & 500 & 12.18&\cellcolor{top1}\textbf{0.972} & \cellcolor{top1}\textbf{1.043 ($\pm$0.416)} &0.416 &0.868 ($\pm$0.046) &0.507 &0.228 \\
         & 400 &10.01 &\cellcolor{top3}0.962 &\cellcolor{top3}1.050 ($\pm$0.445)  &0.416 &0.864 ($\pm$0.053) &0.523 &0.212 \\
         & 300 & 7.13&\cellcolor{top3}0.962 &\cellcolor{top3}1.076 ($\pm$0.518)  &0.415 &0.864 ($\pm$0.050) &0.498 &0.233 \\
         & 200 & 4.80&0.948 &1.084 ($\pm$0.509) &0.406 &0.862 ($\pm$0.050)  &0.513 &0.218 \\
         & 100 &2.43 &0.933&1.123 ($\pm$0.669) &0.420 &0.861 ($\pm$0.053)  &0.514 & 0.310\\
         & 50 &1.27 & 0.932&1.162 ($\pm$0.812) &0.415 &0.855 ($\pm$0.053) &0.491 &0.237 \\
         & 20 & 0.51&0.929 & 1.214 ($\pm$0.633)&0.404& 0.844 ($\pm$0.053)  &0.514 & 0.307\\
         & 10 & 0.26 & 0.848 & 1.546 ($\pm$0.944) &0.403 &\cellcolor{top2}0.827 ($\pm$0.058)  &0.518 &0.195 \\
        \bottomrule
    \end{tabular}
    \label{tab:SCOPe table}
    }
\end{table}

% \textbf{Novelty}. We use foldseek~\cite{van2022foldseek} to calculate the max TM-score between the target protein and those in the database. We use the whole PDB dataset as our database, and the command is 

% % \texttt{foldseek easy-search <pdb\_path> <database> <aln\_file> <tmp\_folder>  
% % --alignment-type 1 \
% %         --exhaustive-search \
% %         --max-seqs 10000000000 \
% %         --tmscore-threshold 0.0 \
% %         --format-output query,target,alntmscore,lddt}
% \begin{verbatim}
% foldseek easy-search <pdb_path> <database> <aln_file> <tmp_folder>  
% --alignment-type 1 \
% --exhaustive-search \
% --max-seqs 10000000000 \
% --tmscore-threshold 0.0 \
% --format-output query,target,alntmscore,lddt
% \end{verbatim}

% where \texttt{<pdb\_path>}is the path of the generated structure as a PDB file,  \texttt{<database>} is the path to foldseek database, \texttt{<aln\_file>} and \texttt{<tmp\_folder>} specify the output file and directory for temporary files.

\textbf{Novelty}. We evaluate the structural novelty by finding the maximum TM-score between a generated structure and any structure in the Protein Data Bank (PDB), using Foldseek~\cite{van2022foldseek}. A lower resulting maximum TM-score signifies a more novel structure. The command~\cite{geffner2025proteina} utilized for this Foldseek search is configured as follows:

% Structural \textbf{Novelty} is assessed by finding the maximum TM-score between a generated structure and any structure in the Protein Data Bank (PDB), using Foldseek~\cite{van2022foldseek}. A lower resulting maximum TM-score signifies a more novel structure. The command~\cite{geffner2025proteina} utilized for this Foldseek search is configured as follows:

\begin{tcolorbox}[
    colframe=black,    % Color of the frame/edges
    colback=white,     % Color of the background
    % coltext=black,   % This is usually the default, so often not needed
    boxrule=0.5pt,     % Thickness of the frame line (adjust as needed)
    arc=0mm,           % Make corners sharp (optional, default is slightly rounded)
    boxsep=5pt,        % Padding between text and box edge (optional)
    verbatim           % Crucial: treat the content inside literally
                       % Preserves spaces, line breaks, special characters
]
\begin{verbatim}
foldseek easy-search <pdb_path> <database> <aln_file> <tmp_folder> 
--alignment-type 1 \
--exhaustive-search \
--max-seqs 10000000000 \
--tmscore-threshold 0.0 \
--format-output query,target,alntmscore,lddt
\end{verbatim}
\end{tcolorbox}

\textbf{Efficiency}. To ensure fairness, we measure inference time on idle GPU and CPU systems. For PDB-based models, we sampled 50 proteins of length 300 and reported the mean sampling time. Similarly, for SCOPe-based models, we sampled 50 proteins of length 128 and reported the mean sampling time. File saving and self-consistency calculations were excluded from the timing.




% \textbf{Genie2} We use the code from \href{https://github.com/aqlaboratory/genie2}{Genie2 public repository}. We loaded the base checkpoint trained for 40 epochs, and used the default sampling temperature (0.6).

% \textbf{RFdiffusion} We use the code from 
% \href{https://github.com/RosettaCommons/RFdiffusion}{RFdiffusion public repository}. Default configuration of the repository was used for sampling.

% \textbf{FoldFlow} We use the code from \href{https://github.com/DreamFold/FoldFlow}{FoldFlow public repository}, which contains both FoldFlow and FoldFlow2. We set \texttt{noise\_scale} to 0.1 and \texttt{flow\_matcher.so3.inference\_scaling} to 10 to achieve best performance.

% \textbf{FrameFlow} We install FrameFlow from its \href{https://github.com/microsoft/protein-frame-flow}{public repository}. Model weights are downloaded from \href{https://zenodo.org/records/12776473?token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6Ijg2MDUzYjUzLTkzMmYtNDRhYi1iZjdlLTZlMzk0MmNjOGM3NSIsImRhdGEiOnt9LCJyYW5kb20iOiIwNjExMjEwNGJkMDJjYzRjNGRmNzNmZWJjMWU4OGU2ZSJ9.Jo_xXr6-PpOzJHUEAuSmQJK72TMTcI49SStlAVdOHoI2wi1i59FeXnogHvcNioBjGiJtJN7UAxc6Ihuf1d7_eA}{here}.

% \textbf{FrameDiff} We used the code from \href{https://github.com/jasonkyuyim/se3_diffusion}{FrameDiff public repository}. The checkpoint we use is located in \texttt{./weights/paper\_weights.pth}. We use the provided conda environment and default configuration for sampling.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{More Experimental Results}\label{app:exp}


\subsection{Detailed Comparisons Based on SCOPe}
Table~\ref{tab:SCOPe table} presents comprehensive results from the SCOPe experiment, further demonstrating the superiority of the QFlow model and the reflow operation, especially in the context of ReQFlow. Notably, even with a generation process as concise as 10 steps, ReQFlow achieves a designable fraction of 0.848, while having an impressively fast inference time of just 0.26 seconds per protein. This highlights the efficiency and effectiveness of ReQFlow in generating feasible protein structures within a minimal timeframe. Additionally, both QFlow and ReQFlow models produce proteins with reasonable secondary structure distributions, indicating their capability to generate structurally plausible proteins. These findings underscore the potential of these models to significantly advance the field of protein design by balancing computational efficiency with structural accuracy.


\subsection{Comparisons on Model Size and Training Data Size}
The comparison of model size and training dataset size is listed in Table~\ref{tab:model_and_dataset_sizes}. Model sizes in the table refer to the number of \textit{total} parameter. FoldFlow2 utilizes a pre-trained model, thus having 672M parameters in total. The number of trainable parameters is 21M.

\begin{table}[t]
    \centering
    \caption{Model Sizes and Training Dataset Sizes}
    \small
    \begin{threeparttable}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Training Dataset Size} & \textbf{Model Size (M)} \\
        \midrule
        RFDiffusion & $>$208K & 59.8 \\
        Genie2 & 590K & 15.7 \\
        FrameDiff & 23K & 16.7 \\
        FoldFlow(Base,OT,SFM) & 23K & 17.5 \\
        FoldFlow2 & $\sim$160K & 672 \\
        FrameFlow & 23K & 16.7 \\
        \midrule
        QFlow & 23K & 16.7 \\
        ReQFlow & 23K+7K & 16.7 \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item[1] When training ReQFlow, we first apply the 23K samples of PDB to train QFlow, and then we use additional 7K samples generated by QFlow in the flow rectification phase.
    \end{tablenotes}
    \end{threeparttable}
    \label{tab:model_and_dataset_sizes}
\end{table}

\begin{table}[htb]
    \centering
    \caption{Comparisions for various methods on their performance (Fraction Score) in long backbone generation. The lengths of the generated backbones range from 300 to 600. We generate 50 samples for each length. We bold the best result and show the top-3 results with a blue background.}
    \small  % 使用更小的字号
    %\setlength{\tabcolsep}{3.5pt} % 减少列间距
    \begin{tabular}{l|ccccccc}
        \toprule
        Length $N$ & 300 & 350 & 400 & 450 & 500 & 550 & 600 \\
        \midrule
        RFDiffusion & 0.76 &0.70 &0.46 &0.36 &0.20 &\cellcolor{top3}0.20 &\cellcolor{top3}0.10 \\
        Genie2 & 0.86 &\cellcolor{top2}0.90 &\cellcolor{top2}0.74 &\cellcolor{top2}0.58 &0.28 &0.12 &\cellcolor{top3}0.10 \\
        FoldFlow2 & \cellcolor{top1}0.96 &\cellcolor{top3}0.88 &\cellcolor{top3}0.70 &\cellcolor{top3}0.56 &\cellcolor{top2}0.60 &\cellcolor{top2}0.26 &\cellcolor{top1}\textbf{0.16} \\
        FrameDiff &0.24 &0.18  &0.00  &0.00 &0.00 &0.00&0.00  \\
        FoldFlow-OT &0.62 &0.48 &0.30 &0.10 &0.04 &0.00 &0.00 \\
        FrameFlow & 0.72 &0.74 &0.48 &0.28 &0.24 &0.10 & 0.00\\       
        \midrule
        QFlow  & \cellcolor{top3}0.88 &0.78 &0.54 &0.50 &\cellcolor{top3}0.30 &0.02 &0.00 \\
        ReQFlow & \cellcolor{top1}\textbf{0.98} &\cellcolor{top1}\textbf{0.96} &\cellcolor{top1}\textbf{0.78} &\cellcolor{top1}\textbf{0.76} &\cellcolor{top1}\textbf{0.7} &\cellcolor{top1}\textbf{0.56} &\cellcolor{top3}0.10 \\
        \bottomrule
    \end{tabular}
    \label{tab:longchain}
\end{table}






\subsection{Visualization Results}
We use \textit{Mol Viewer}~\cite{sehnal2021mol} to visualize protein structures generated by different models, as shown in Figure~\ref{fig:methods visualization 1} and Figure~\ref{fig:methods visualization 2}. 
In Figure~\ref{fig:methods visualization 1}, all proteins originate from the same noise initialization generated by QFlow, whereas in Figure~\ref{fig:methods visualization 2}, the initialization is generated by FoldFlow. 
Each method follows its own denoising trajectory, leading to distinct structural outputs. 
FoldFlow2 adopts a default sampling step of 50, while all other methods use 500 steps. Due to architectural differences, the final structures vary across models, but within the same model, different sampling steps generally yield similar structures. 
Notably, the noise distributions produced by QFlow and FoldFlow exhibit slight discrepancies, and models generally perform better on its own noise.

Among all models, ReQFlow exhibits the most stable and robust performance, maintaining low RMSD and variance across different sampling steps while demonstrating resilience to varying noise inputs. In contrast, other methods show significant limitations. Although FoldFlow2 achieves low RMSD at 50 and 100 steps, it lacks diversity and novelty, as reflected in Table~\ref{tab:PDB main results}. 
FoldFlow-OT is highly sensitive to initial noise, displaying drastically different performance in Figure~\ref{fig:methods visualization 1} and Figure~\ref{fig:methods visualization 2}—evidenced by substantial variance across sampling steps when using QFlow noise. 
Moreover, FoldFlow-OT tends to overproduce $\alpha$-helices—coiled, spiral-like structures—resulting in high designability scores but deviating from realistic protein distributions. Conversely, ReQFlow and QFlow generate a higher proportion of $\beta$-strands, which appear as extended, ribbon-like structures, indicating a closer alignment with natural protein distributions.
This pattern suggests a high risk of mode collapse, where the model predominantly learns a specific subset of protein structures, leading to a lack of diversity and novelty. 
Furthermore, as sampling steps decrease, most baseline models experience a sharp deterioration in quality: RMSD values increase, rendering the structures non-designable. In extreme cases, some samples exhibit severe fragmentation or disconnected backbones (e.g., the dashed regions in FoldFlow2 at 20 steps, Figure~\ref{fig:methods visualization 1}), highlighting instabilities in their sampling dynamics.

% Among all models, ReQFlow demonstrates the best and most stable performance, exhibiting low RMSD and variance across different sampling steps, and shows robustness to different noises. Other methods suffer from other problems. FoldFlow generates an excessive number of $\alpha$-helices—coiled, spiral-like structures—resulting in high designability scores but deviating from realistic protein distributions. In contrast, ReQFlow and QFlow produce a higher proportion of $\beta$-strands, which appear as extended, ribbon-like structures, indicating a closer alignment with real-world protein distributions. This senario indicates high mode collapse risks, that the model only learns a certain kind of protein structure, leading to lack of diversity and novelty. Additionally, as sampling steps decrease, most baseline models experience a sharp decline in quality. 
% High RMSD values emerge, making the structures non-designable. In some extreme cases, samples shows severe fragmentation or disconnected backbones (e.g., the dashed regions in FoldFlow2 at 20 steps, Figure~\ref{fig:methods visualization 1}), suggesting instability in their sampling dynamics.





\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth, trim=0 150 0 50, clip]{figures/protein1.pdf}
    \caption{Visualization of different methods on length 300. Sampling start with a \textit{same} noise generated by QFlow.}
    \label{fig:methods visualization 1}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth, trim=0 150 0 50, clip]{figures/protein2.pdf}
    \caption{Visualization of different methods on length 300. Sampling start with a \textit{same} noise generated by FoldFlow.}
    \label{fig:methods visualization 2}
\end{figure}
\end{document}