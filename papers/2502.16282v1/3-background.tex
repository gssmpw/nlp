\section{\mbox{Research Questions and Experimental Setup}}

The recent line of work on the emergence of alignment across independently pre-trained unimodal models raises fundamental questions regarding the emergence of aligned representations and their implications on multimodal learning. Our research seeks to understand (1) when and why alignment emerges implicitly, and (2) whether alignment is a reliable indicator of performance. To reliably and comprehensively study these questions across all types of multimodal data, we use two principle dimensions to taxonomize multimodal data: \textit{interactions} and \textit{heterogeneity}~\citep{baltruvsaitis2018multimodal,liang2024foundations,tian2020makes}. Interactions measure the information shared between two modalities for a task, from more redundant (e.g., images and corresponding captions) to more unique (e.g., sensor placement). We expect alignment to emerge more easily between redundant modalities. Heterogeneity measures the degree of similarity across two data modalities independent of the task, from more similar (e.g., two languages) to more different (e.g., text and video). We expect alignment to emerge more easily between similar modalities. 
Our experiments aim to study the emergence of alignment and its relationship to downstream task performance by systematically varying the interactions and heterogeneity in multimodal data. To summarize, our fundamental guiding questions are:
\begin{enumerate}[noitemsep,topsep=0pt,nosep,leftmargin=*,parsep=0pt,partopsep=0pt]
    \item Does alignment emerge when uniqueness and heterogeneity increase?
    \item Does higher alignment always predict better performance when uniqueness is present?
    \item How can we characterize datasets through the correlation between performance and alignment?
\end{enumerate}
Based on these questions, we define our problem setting.

\subsection{Problem Setting}

We focus on a simplified setting with two modalities and an associated label, the generalization is straightforward. Concretely, we consider a scenario where we sample multimodal data and labels \(x_1, x_2, y \sim \mathbb{P}(X_1, X_2, Y)\) from a data distribution \(\mathbb{P}(X_1, X_2, Y)\). \(X_i\) represents the random variable for the $i$-th modality and \(Y\) for the task. Based on the relationships between $X_1$, $X_2$, and $Y$, these modalities can exhibit different degrees of interactions and heterogeneity.

\textbf{Interactions} measure the information shared between two modalities for a task, from more redundant to more unique. Redundancy \(R\) represents the shared information between the two modalities and the task (\(Y\)), such as between images and captions that describe the image~\citep{radford2021learning}. Uniqueness in modality 1 (\(U_1\)) quantifies the amount of information present in the first modality absent in the second but critical for the downstream task (and likewise for \(U_2\)). For example, feature selection is often optimized to provide new unique information and minimize redundancy to previous ones~\citep{peng2005feature}.


To investigate how alignment and performance change with respect to different interactions, we need synthetic controllable datasets and real-world multimodal benchmarks with different interactions. For constructing synthetic data, we assume that the task-relevant information (for a particular label \(y\)) can be decomposed into \(x_r, x_{u_1}, x_{u_2}\), where \(x_r\) denotes the common or redundant information, \(x_{u_1}\) represents information unique to the first modality, and \(x_{u_2}\) captures information unique to the second modality.

We construct the input data as \(x_1 = [x_r, x_{u_1}]\) and \(x_2 = [x_r, x_{u_2}]\). An overview of the data generation process is shown in Figure~\ref{fig:data_generation}. By selecting specific features to compute the label, we control the levels of redundancy and uniqueness. Specifically, \(Y\) is a nonlinear function of a subset of features, \(\mathcal{S} \subseteq [x_r, x_{u_1}, x_{u_2}]\). This enables us to control \(R\) as the number of features in \(\mathcal{S}\) that come from \(x_r\), and \(U_i\) as the number of features that come from \(x_{u_i}\) for $i \in \{1,2\}$. We denote the total uniqueness \(U = |\mathcal{S}| - R\). By keeping \(|\mathcal{S}|\) fixed while varying \(U\), we generate datasets with different proportions of redundant versus non-redundant information.

\textbf{Heterogeneity}. Different modalities often exhibit distinct structures, qualities, and representations~\cite{liang2024foundations}. For example, when one modality is a time series and another is a static image, differences in their vocabulary tokens, and different noise or distribution shifts in each modality. We aim to investigate how alignment and performance change with different degrees of heterogeneity, from more similar (e.g., two languages) to more different (e.g., text and video).

To generate synthetic datasets with varying heterogeneity, we start with the case where both modalities are redundant, meaning \(Y\) (the labels) is a nonlinear function of \(x_r\). Specifically, let \(x_1 = x_r\) and \(x_2 = \phi(x_r)\), where \(\phi(\cdot)\) is a nonlinear bijective function, as shown in Figure~\ref{fig:data_generation}. 
In this setting, heterogeneity is defined as the number of nonlinear transformations involved in \(\phi(\cdot)\). Concretely, if \(\phi(\cdot)\) is modeled as a multilayer perceptron (MLP), the number of layers \(D_{\phi}\) quantifies the level of heterogeneity between the two modalities. We extend this definition to cases where the modalities contain unique information. Let \(x_r\) represent the shared information between the two modalities, and \(x_{u_1}\) and \(x_{u_2}\) denote the unique information in each modality. In this scenario, \(X_1 = [x_r, x_{u_1}]\), and a modality that is heterogeneous with respect to \(X_1\) is defined as \(X_2 = \phi([x_r, x_{u_2}])\). Here, we assume \(\phi\) is a bijection, ensuring that the information content of \(X_2\) and \([x_r, x_{u_2}]\) remains unchanged. 

\begin{figure}[t!]
    \centering{
    \hspace{-3mm}
    \includegraphics[width=1.02\linewidth]{figures/data_generation_final.pdf}}
    \caption{\textbf{Synthetic data generation and training.} We generate synthetic data with varying levels of uniqueness and heterogeneity. The building blocks are the redundant and unique components \([x_r, x_{u_1}, x_{u_2}]\), where $[x_r, x_{u_1}]$ are used in creating $X_1$ and \([x_r, x_{u_2}]\) are for \(X_2\). The level of uniqueness is determined by the number of features from $x_r$ that are used to compute the labels $Y$, given that the total number of features used for label computation is held constant. $X_2$ is transformed into a heterogeneous modality using a transformation network $\phi$.  In our experiments, we compute alignment between unimodal encoders $E_1$, $E_2$ trained on $X_1$, $X_2$ respectively. 
    \vspace{-0.5em}
    }
    \label{fig:data_generation}
\end{figure}



\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.01\linewidth]{figures/unbiased_cka_best_pairwise_fixed_1_syn-depth_syn_align_scatter.pdf}
    \vspace{-2em}
    \caption{\textbf{Alignment vs uniqueness on synthetic datasets.} The alignment is computed between unimodal encoders trained on datasets with different levels of informational uniqueness. Each dot is an independent run on a different model size on a given dataset. We see that the maximum level of achievable level of alignment decreases as the level of uniqueness increases. Five different figures shows the different levels of nonlinear transformation we apply to the original data.}
    \label{fig:align_unique}
\end{figure*}
\xhdr{Experimental setup for synthetic datasets} 
We evaluate how uniqueness, redundancy, and heterogeneity influence the emergence of alignment by training encoders independently on each modality and measuring the alignment between their learned representations. Specifically, we train a single-layer encoder on the first modality, denoted as \(E_1\). For the second modality, which is transformed by the nonlinear function \(\phi(\cdot)\) with varying depths (\(D_\phi\)), we train a series of encoders denoted as \(E_{2,D_{Enc}}\), where \(D_{Enc}\) represents the depth of the encoder trained on the second modality and varies as \(D_{Enc} \in \{1, \ldots, 10\}\).

\xhdr{Experimental setup for real benchmarks}
In addition to experiments on synthetic data, we conduct analogous experiments on vision-language models. We use the same dataset and models as ~\citet{huh_platonic_2024}, which evaluates alignment on the Wikipedia caption dataset~\citep{srinivasan_wit_2021} with naturally co-occurring text and images. This dataset is inherently heterogeneous (text and images are different) with high redundancy due to overlapping semantic information. To vary the amount of unique information, we introduce a preprocessing procedure that perturbs the data in each modality. Specifically, for text, we randomly delete a percentage of characters, reducing the amount of information available in the textual representation. For image data, we replace a certain percentage of pixels with values drawn from a Gaussian distribution that matches the mean and variance of the original image.




Finally, we experiment with MultiBench~\citep{liang_multibench_2021} which collects a diverse range of real-world multimodal datasets: MOSEI ~\citep{bagher_zadeh_multimodal_2018}, a dataset for predicting emotions from videos (vision, audio, language); MOSI ~\citep{zadeh_mosi_2016}, a dataset for predicting sentiment from videos (vision, audio, language), URFUNNY ~\citep{hasan_ur-funny_2019}, a humor detection dataset from videos (vision, audio, language); MUStARD ~\citep{castro_towards_2019}, a sarcasm detection dataset from TV shows (vision, audio, language); and AVMNIST~\citep{perez-rua_mfas_2019}, a dataset for digit classification from paired images and spoken digits. While we cannot explicitly vary the information content, past work has collected human annotations of the levels of redundancy and uniqueness in these datasets, showing that most multimodal datasets have a significant amount of uniqueness~\citep{liang2023quantifying}. 

\paragraph{Computing Alignment.} For models trained on synthetic data, we evaluate alignment using Centered Kernel Alignment (CKA)~\citep{kornblith_similarity_2019}. Following the methodology outlined in \citet{huh_platonic_2024}, we evaluate alignment using mutual KNN, a variant of CKA. See Appendix~\ref{app:sec:alignment_computation} for more details.