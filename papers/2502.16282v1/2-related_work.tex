\section{Representation Alignment}

In this section, we review the concept of representation alignment through prior work on measuring alignment, methods to explicitly align representations, and observations regarding the emergence of alignment.

\xhdr{Measuring Alignment}  Measuring alignment between neural network representations is a widely used approach in the research community to analyze and improve training dynamics~\citep{huh_platonic_2024,klabunde_similarity_2024,kornblith_similarity_2019}. A prominent class of alignment metrics is based on canonical correlation analysis (CCA), a statistical technique for comparing two subspaces~\citep{thompsonCanonicalCorrelationAnalysis2005,golub1995canonical}, along with its nonlinear extensions using kernels~\cite{lai2000kernel,raghu_svcca_2017} and neural networks~\cite{andrew2013deep,wang2015deep,morcos_insights_2018}.  Among these methods, \citet{kornblith_similarity_2019} highlight the advantages of Centered Kernel Alignment (CKA), particularly its invariance to orthogonal transformations and isotropic scaling. Given mean-centered feature sets of \(n\) samples, \(Z_1, Z_2 \in \mathbb{R}^{n \times d}\), from two modalities \(X_1\) and \(X_2\), the CKA metric with a linear kernel is:

\begin{align*}
    \mathsf{CKA}(Z_1, Z_2) = \frac{\mathsf{ALIGN}(Z_1, Z_2)}{\sqrt{\mathsf{ALIGN}(Z_1, Z_1) \cdot \mathsf{ALIGN}(Z_2, Z_2)}}
\end{align*}
where \(\mathsf{ALIGN(Z_1, Z_2)}\) denotes $\mathsf{HSIC(Z_1Z_1^T, Z_2Z_2^T)}$ with $\mathsf{HSIC}$ denoting an empirical estimator of the Hilbert-Schmidt Independence Criterion \cite{gretton2005measuring}. Intuitively, CKA quantifies alignment by comparing the covariance structures of two feature sets, capturing whether their representations encode similar relationships. This property ensures that even if \(Z_1\) and \(Z_2\) undergo arbitrary rotations (e.g., due to different initialization schemes), the CKA metric remains consistent, making it a robust choice for assessing representation alignment. For large vision-language models, given the high dimensionality and large embedding sizes of learned representations, we employ a computationally efficient variation of CKA â€” the mutual k-nearest neighbors (mutual KNN) method~\cite{huh_platonic_2024}. Instead of directly comparing full covariance structures, this approach measures similarity by analyzing the overlap between the k-nearest neighbor sets of embeddings, improving scalability. Details are provided in Appendix \ref{app:sec:alignment_computation}.



\xhdr{Explicit Alignment}  In addition to research on measuring alignment in neural networks, another line of work focuses on explicitly aligning representations, a widely used technique for handling heterogeneous modalities~\citep{liang2024foundations}. A popular approach in this domain is multimodal contrastive learning, where representations of the same concept across different modalities (i.e., positive pairs) are brought closer together, while representations of different concepts (i.e., negative pairs) are pushed apart~\citep{frome2013devise,jia2021scaling,radford2021learning}. The coordination distance in contrastive learning is typically measured using cosine distance~\citep{mekhaldi2007multimodal} or max-margin losses~\citep{hu2019deep}. Theoretical results demonstrate that contrastive learning effectively captures redundant information shared between modalities~\citep{tian2020makes,tosh2021contrastive}. More recent extensions have been proposed to also capture unique and synergistic information, further refining multimodal representation learning~\citep{dufumier_what_2024,liang2023factorized}. 

\xhdr{Emergence of Implicit Alignment} In contrast to explicit alignment methods, recent findings suggest that alignment can emerge implicitly, even when neural networks differ in training objectives, datasets, and architectures~\citep{li_convergent_2015, raghu_svcca_2017, lenc_understanding_2019, barannikov_representation_2022, bonheme_how_2022}. Notably, this similarity becomes more pronounced in larger and wider networks~\citep{raghu_svcca_2017, morcos_insights_2018, kornblith_similarity_2019}.  Building on the observation that latent spaces are inherently comparable, a line of research explores composing components of different models with minimal or no additional training. \citet{lenc_understanding_2019} demonstrate that latent spaces can be stitched together using trainable stitching layers, while subsequent studies~\citep{bansal_revisiting_2021, csiszarik_similarity_2021} show that better-performing models tend to learn more similar representations when stitched.  More recently, the Platonic Representation Hypothesis~\citep{huh_platonic_2024} suggests that as vision and language models scale in capacity and performance, independently trained models exhibit increasing alignment. This finding implies that models are converging toward modality-agnostic representations, reinforcing the idea that alignment may emerge naturally as a byproduct of model scaling. However, if alignment continues to emerge, there would be no need for any of the explicit alignment methods described above. That explicit alignment has consistently been helpful implies either emergent alignment is not sufficient or emergent alignment does not always lead to improved performance. This discrepancy between the possibility of emergent alignment and the need for explicit alignment methods calls for a systematic exploration of the role of alignment and its downstream relationship to performance in multimodal learning.