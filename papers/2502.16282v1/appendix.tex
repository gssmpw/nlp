\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}



\section{Experimental Details}\label{app:experiment_details}

\subsection{Synthetic Data Experiments}
On the synthetic dataset, we train MLPs with the AdamW optimizer with the number of hidden dimensions kept the same as the number of input features, 12. For a given level of uniqueuness, we choose suitable hyperparameters across different model depths and transformation depths. Specifically, we tune the learning rate in the range $\{ 1e-1, 1e-2, 1e-3, 1e-4\}$ and weight decay in the range $\{0, 1e-1, 1e-2, 1e-3, 1e-4\}$ for each modality. The depth 1 MLP for the untransformed modality were trained for 50 epochs and the models for the transformed modality were trained for 300 epochs. To ensure robustness, we report results with five different random seeds for each dataset.

\subsection{Vision-Language Alignment}
% TODO: Find citations

We evaluate alignment using the same set of language and vision models as~\citet{huh_platonic_2024}. The language model families considered are BLOOM~\cite{workshop_bloom_2023}, OpenLLaMA~\cite{geng_openllama_2023}, and LLaMA~\cite{touvron_llama_2023} downloaded from HuggingFace~\cite{wolf_transformers_2020}. The vision models are vision transformer models of various sizes trained on various data and objectives. These include classification on ImageNet-21K~\cite{russakovsky_imagenet_2015}, MAE~\cite{heMaskedAutoencodersAre2022}, DINOv2~\cite{oquab_dinov2_2023}, CLIP~\cite{radford_learning_2021}, and CLIP finetuned on ImageNet-12K. These
models were downloaded from PyTorch Image Models ~\cite{wightman_pytorch_2019}.

% Due to the large dimensionality and embedding sizes of the learned representations, we adopt a computationally efficient variation of CKA, specifically the mutual k-nearest neighbors (mutual KNN) method. This approach measures the similarity between the k-nearest neighbor sets of the embeddings instead of directly comparing the full covariance structures, making it more scalable.  Following~\citet{huh_platonic_2024}, we measure alignment using mutual-KNN, . $k=10$ nearest neighbors over 1024 samples from WIT. For the vision model, the class token of each layer is used, and for the language model, the embeddings of a given layer are average pooled to a single token. $l_2$ normalization is applied to the features and elements in the features that are above the 95-th percentile are truncated.

\subsection{MultiBench Experiments}
% On the transformers trained on MultiBench datasets, we use a single head for self-attention and set the embedding size to the input dimension. For each dataset, we choose suitable hyperparameters across different model depths and tune the learning rate in the range $\{1e-3, 5e-4, 1e-4, 5e-5, 1e-5 \} $ and and weight decay in the range $\{0, 1e-1, 1e-2, 1e-3, 1e-4\}$.

We train transformers on the pre-extracted video, audio, and text features for the affective computing datasets and the audio modality of AVMNIST, and vision transformers on the AVMNIST digit images. For each modality, we vary the depth of the transformers in the range $\{1, \ldots, 10\}$. We use a single head for self-attention and set the embedding size to the input dimension. For classification tasks, we append a \texttt{[cls]} token to the sequence with a learnable embedding. The embedding of this token is used to compute alignment between layers; otherwise, we do average pooling over the input sequence. We use the AdamW optimizer. For each dataset, we choose suitable hyperparameters across different model depths and tune the learning rate in the range $\{1e-3, 5e-4, 1e-4, 5e-5, 1e-5 \} $ and and weight decay in the range $\{0, 1e-1, 1e-2, 1e-3, 1e-4\}$.

To ensure robustness, we train each architecture across 3 different seeds, providing 30 alignment-performance data points. To get the alignment-performance correlation given modalities 1 and 2, the alignment of every modality 2 model is computed with respect to the modality 1 model with the highest validation score across the different seeds, and we report the correlation of these alignment scores with the performance of the modality 2 models.    

\section{Alignment Computation}
\label{app:sec:alignment_computation}


Given mean-centered feature sets of \(n\) samples, \(Z_1, Z_2 \in \mathbb{R}^{n \times d}\), from two modalities \(X_1\) and \(X_2\), we first compute the covariances of these two different feature sets, and then compute the empirical estimator of the Hilbert-Schmidt Independence Criterion \cite{gretton2005measuring} using a linear kernel. Hence,

\begin{align}
\mathsf{HSIC}(Z_1Z_1^T,Z_2Z_2^T) = \frac{1}{(n - 1)^2} \text{Tr}(Z_1Z_1^T Z_2Z_2^T) = \frac{1}{(n - 1)^2} \|Z_1^TZ_2\|_F^2
\end{align}

The Centered Kernel Alignment (CKA)~\cite{kornblith_similarity_2019} is then obtained by normalizing HSIC to ensure scale invariance and comparability across different feature sets:

\begin{align}
\mathsf{CKA}(Z_1, Z_2) = \frac{\mathsf{HSIC}(Z_1Z_1^T, Z_2Z_2^T)}{\sqrt{\mathsf{HSIC}(Z_1Z_1^T, Z_1Z_1^T) \mathsf{HSIC}(Z_2Z_2^T, Z_2Z_2^T)}}
\end{align}

As demonstrated in \cite{huh_platonic_2024}, the definition of alignment can be adjusted to limit the cross-covariance measurement to only those samples identified as nearest neighbors of the current sample \(i\). This modification prioritizes similarity over dissimilarity, thereby emphasizing local alignment:

\begin{align}
\mathsf{ALIGN}_{\mathsf{MKNN}}(Z_1,Z_2) &= \sum_i \sum_j \alpha(i, j)) \\
\text{where, }\alpha(i, j) &= \mathbf{1}[Z_{1, j} \in knn(Z_{1, i}) \wedge Z_{2, j} \in knn(Z_{2, i}) \wedge i \neq j]
\end{align}

Here, \(Z_{1, k}\) and \(Z_{2, k}\) refer to the \(k\)\textsuperscript{th} row of \(Z_1\) and \(Z_2\), respectively, while \(\mathsf{MKNN}\) denotes Mutual KNN.

Thus, Mutual-KNN \(\mathsf{MKNN}\) is defined as:

\begin{align}
    \mathsf{MKNN}(Z_1, Z_2) = \frac{\mathsf{ALIGN}_{\mathsf{MKNN}}(Z_1,Z_2)}{\sqrt{\mathsf{ALIGN}_{\mathsf{MKNN}}(Z_1,Z_1) \cdot \mathsf{ALIGN}_{\mathsf{MKNN}}(Z_2,Z_2)}}
\end{align}

Following~\citet{huh_platonic_2024}, we use $k=10$ nearest neighbors over 1024 samples from the Wikipedia caption dataset. For the vision model, the class token of each layer is used, and for the language model, the embeddings of a given layer are average pooled to a single token. $l_2$ normalization is applied to the features and elements in the features that are above the 95-th percentile are truncated.

After computing the alignment between all pairs of layers between \(E_1\) and \(E_{2,d}\) using CKA or mutual KNN, we report the best alignment score across all layer pairs \cite{schrimpf_brain-score_2018}.

\section{Dataset Details}\label{app:dataset_details}

\subsection{Synthetic Data}\label{app:synthetic}

% [TODO lots of repeat between this and section 3, need to merge and shorten, and use figures to draw the data generation process. also explain the real datasets as early as possible don't want readers to think we only have synthetic data]

% \note{C: We may need to decide whether we want to use \(x_c\) or \(x_r\). The dual notation can get confusing. We also need to fix if the subscripts are capitalized or not. }
% \note{M: Yes, we can change $x_c$ to $x_r$}

We discuss in detail how we construct a synthetic dataset with two modalities to analyze how uniqueness, redundancy, and heterogeneity influence the emergence of alignment. Let \(x_1 = [x_r, x_{u_1}]\) and \(x_2 = [x_r, x_{u_2}]\). Here, \(x_r \in \mathbb{R}^{n_R}\) represents the redundant information shared between the two modalities, while \(x_{u_1}, x_{u_2} \in \mathbb{R}^{n_U}\) denote the unique information for each modality. Both \(x_1\) and \(x_2\) represent arbitrary data samples.

For each data sample, we generate \(x_r\), \(x_{u_1}\), and \(x_{u_2}\) by sampling binary vectors from a uniform distribution. Specifically, \(
x_r \sim \operatorname{Uniform}(\{0, 1\}^{n_R}), x_{u_1} \sim \operatorname{Uniform}(\{0, 1\}^{n_U}), \text{ and }x_{u_2} \sim \operatorname{Uniform}(\{0, 1\}^{n_U}).
\)

To define the labels for this dataset, we introduce task masks \(M_{R} \in \mathbb{R}^{n_R}\) and \(M_{U_1}, M_{U_2} \in \mathbb{R}^{n_U}\), which determine the features used in computing the output labels. These masks indicate whether a particular feature contributes to the label-generation process. Specifically, the task masks are defined as follows, where the subscript \(i\) refers to the \(i\)\textsuperscript{th} entry of the respective mask vector:
\begin{align}
    M_{R_i} &=
        \begin{cases} 
        1 & \text{if } 0 \leq i < n_R, \\
        0 & \text{otherwise.}
        \end{cases} \\
    M_{U_{1_i}} & = \begin{cases} 
        1 & \text{if } 0 \leq i < \lceil \frac{n_{U}}{2} \rceil, \\
        0 & \text{otherwise.}
        \end{cases} \\
    M_{U_{2_i}} & = \begin{cases} 
        1 & \text{if } 0 \leq i <  \lfloor \frac{n_{U}}{2} \rfloor, \\
        0 & \text{otherwise.}
        \end{cases} 
\end{align}
We define the task label \(y\) as a function \(\psi_{Y}(\cdot)\) of the masked components \(x_r \odot M_R\), \(x_{u_1} \odot M_{U_1}\), and \(x_{u_2} \odot M_{U_2}\), such that:
\begin{align}
    y = \psi_{Y}(x_r \odot M_R, x_{u_1} \odot M_{U_1}, x_{u_2} \odot M_{U_2}), 
\end{align}
where \(x_r \odot M_R\) captures the task-relevant redundant information, \(x_{u_1} \odot M_{U_1}\) captures the task-relevant unique information from modality 1 and \(x_{u_2} \odot M_{U_2}\) captures the task-relevant unique information from modality 2.
 
 
Here, \(\odot\) denotes the element-wise (Hadamard) product. Intuitively, the task masks \(M_R\), \(M_{U_1}\), and \(M_{U_2}\) are essential for controlling the relative contributions of the redundant (\(x_r\)) and unique (\(x_{u_1}\), \(x_{u_2}\)) components to the label generation process.

In our synthetic experiments, we assume the joint distribution of the components as follows:
\begin{align}
    \mathbb{P}(X_C, X_{U_1}, X_{U_2}) &= \mathbb{P}(X_C)\mathbb{P}(X_{U_1})\mathbb{P}(X_{U_2}) \\
    \text{Where, }\mathbb{P}(X_C) &= \operatorname{Uniform}(\{0, 1\}^{n_R}) \\
    \mathbb{P}(X_{U_1}) &= \operatorname{Uniform}(\{0, 1\}^{n_U}) \\
    \mathbb{P}(X_{U_2}) &= \operatorname{Uniform}(\{0, 1\}^{n_U})
\end{align}
This formulation assumes that the redundant information (\(x_r\)) and the unique components (\(x_{u_1}\), \(x_{u_2}\)) are all independently distributed. 

The task masks play a critical role in modulating which features are used to compute the labels, thereby allowing precise control over the relative importance of shared and unique information in the synthetic dataset. This design facilitates the study of how these components influence alignment and downstream task performance.

In our experiments, we fix \(n_Y\) (the number of features relevant for the task) while varying \(n_R\), \(n_{U}\) to explore different dataset configurations. Concretely, \(n_Y = n_R + n_{U}\).

When \(n_y = n_R\), both modalities have equal amounts of task-relevant information, allowing them to perform equally well on the downstream classification task. However, when \(n_R < n_Y\), the shared information \(x_c\) becomes insufficient to fully capture the task-relevant features. By adjusting the proportion of \(\frac{n_R}{n_Y}\), we heuristically vary the amount of task-specific shared information. This allows us to explore how the balance of redundant and unique information impacts alignment and downstream performance.

An example of this is when $n_Y=2$,  \(
x_r \sim \operatorname{Uniform}(\{0, 1\}), x_{u_1} \sim \operatorname{Uniform}(\{0, 1\}),x_{u_2} \sim \operatorname{Uniform}(\{0, 1\})\). Given that the label function is an OR function of $[x_r, x_{u_1}]$, where $n_U=1$, we can see how the label predictions would differ based on $x_1 = [x_r, x_{u_1}]$ and $x_2 = [x_r, x_{u_2}]$ in Table \ref{tab:u=1}.
\begin{table}[ht!]
    \centering
    \begin{tabular}{c|c|c|c|c}
       $x_1$  & $x_2$  & $\hat{y}_1$ & $\hat{y}_2$ & y\\
       \hline
        00 & 00 & 0 & 0 & 0 \\
        01 & 00 & 1 & 0 & 1\\
        00 & 01 & 0 & 0 & 0 \\ 
        01 & 01 & 1 & 0 & 1\\ 
        10 & 10 & 1 & 1 & 1\\ 
        11 & 10 & 1 & 1 & 1 \\ 
        10 & 11 & 1 & 1 & 1 \\ 
        11 & 11 & 1 & 1 & 1\\
        
    \end{tabular}
    \caption{\textbf{$n_U=1$ Predictions.} $\hat{y}_2$ is incorrect for 2 examples due to lacking the unique information.}
    \label{tab:u=1}
\end{table}

In contrast, when the labels are an OR of $[x_{u_1}, x_{u_2}]$, where $n_U=2$, we can see how the label predictions would differ based on $x_1 = [x_r, x_{u_1}]$ and $x_2 = [x_r, x_{u_2}]$ in Table \ref{tab:u=2}.
\begin{table}[ht!]
    \centering
    \begin{tabular}{c|c|c|c|c}
       $x_1$  & $x_2$  & $\hat{y}_1$ & $\hat{y}_2$ & y\\
       \hline
        00 & 00 & 0 & 0 & 0 \\
        01 & 00 & 1 & 0 & 1\\
        00 & 01 & 0 & 1 & 1\\ 
        01 & 01 & 1 & 1 & 1\\ 
        10 & 10 & 0 & 0 & 0\\ 
        11 & 10 & 1 & 0 & 1 \\ 
        10 & 11 & 0 & 1 & 1\\ 
        11 & 11 & 1 & 1 & 1\\
        
    \end{tabular}
    \caption{\textbf{$n_U=2$ Predictions.} Both $\hat{y}_1$ and $\hat{y}_2$ are incorrect for 2 examples due to lacking unique information in the other modality.}
    \label{tab:u=2}
\end{table}

% \note{C: @Megan please fill in the example when you get some time, thanks.}

To incorporate heterogeneity into the setup, we transform the second modality (\(x_2\)) using a nonlinear function \(\phi(\cdot)\). Specifically, \(\phi(\cdot)\) is modeled as a multilayer perceptron (MLP), where the number of layers (\(D_{\phi}\)), also referred to as transformation depth, serves as a heuristic measure of heterogeneity. A higher \(D_{\phi}\) implies a more complex transformation, thereby increasing the heterogeneity between the two modalities. Hence, concretely, \(x_{2, \phi} = \phi([x_c, x_{u_2}])\).

In all our experiments, we fix \(n_Y = 8\), which represents the total number of task-relevant features. We vary \(n_R \in \{0, \ldots, 8\}\), thereby controlling the amount of redundant (shared) task-specific information. Consequently, the amount of unique task-specific information is determined as \(n_U = n_Y - n_R\). We refer to the level of unique information as $U = n_U$.

% We evaluate how uniqueness, redundancy, and heterogeneity influence the emergence of alignment by training encoders independently on each modality and measuring the alignment between their learned representations. Specifically, we train a single-layer encoder on the first modality, denoted as \(E_1\). For the second modality, which is transformed by the nonlinear function \(\phi(\cdot)\) with varying depths (\(D_\phi\)), we train a series of encoders denoted as \(E_{2,D_{Enc}}\), where \(D_{Enc}\) represents the depth of the encoder trained on the second modality and varies as \(D_{Enc} \in \{1, \ldots, 10\}\)

% For each combination of uniqueness, redundancy, heterogeneity, and encoder depth, we compute the alignment between all pairs of layers between \(E_1\) and \(E_{2,d}\) using the CKA metric. We report the best alignment score across all layer pairs \cite{huh_platonic_2024}. 

% \note{Following, add reference. Additionally, include per-layer alignment results in the appendix.}


% In addition to experiments on synthetic data, we conduct analogous experiments on vision-language models, following the Platonic Representation Hypothesis \note{Add ref}. We use the same datasets evaluated in \note{Add ref}, where the text and image data are paired. This pairing inherently introduces a high degree of redundancy between the information contained in the images and the text. However, it is less straightforward to control the amount of unique information present in either modality within these datasets.

% To address this, we apply a simple preprocessing procedure to introduce perturbations and vary the level of unique information. Specifically, for the text data, we delete a random percentage of characters. For the image data, we replace a certain percentage of pixels with values drawn from a Gaussian distribution with the same mean and variance as the original image.

% We vary the percentage of deleted characters in the text and the  percentage of replaced pixels in the images in range $\{5, 10, 15, 25, 30, 35, 45, 50\}$. The level of uniqueness $U$ refers to the percentage of perturbation applied to both modalities. This allows us to control the degree of uniqueness in each modality even in a real world dataset, in a heuristic manner.

% %and the percentage of replaced pixels in the images between the ranges $\{5, 10, 15, 25, 30, 35, 45, 50\}$

% Following the methodology outlined in \cite{huh_platonic_2024}, we evaluate alignment across different model sizes and training schemes for vision-language models. Due to the large dimensionality and embedding sizes of the learned representations, we adopt a computationally efficient variation of CKA, specifically the mutual k-nearest neighbors (mutual KNN) method. This approach measures the similarity between the k-nearest neighbor sets of the embeddings instead of directly comparing the full covariance structures, making it more scalable.


\subsection{MultiBench Dataset}\label{app:multibench}
Below, we discuss the MultiBench datasets in more detail.

\begin{itemize}
    \item \textbf{MUStARD}~\citep{castro_towards_2019} is a dataset for automated sarcasm discovery, compiled from popular TV shows, including Friends, The Golden Girls, The Big Bang Theory, and Sarcasmaholics Anonymous. There are 414, 138, and 138 video segments in the training, validation, and testing data, which gives a total of 690 data points.
    \item \textbf{MOSI}~\citep{zadeh_mosi_2016} is a dataset for sentiment analysis consisting of 2,199 opinion video clips. Each video is further split into short segments (roughly 10-20 seconds) that are annotated, resulting in 1284, 229, 686 segments in the train, validation, and testing sets. As the annotations are sentiment intensity, which ranges from [-3, 3], we train our models on the continuous labels with L1 loss and evaluate positive-negative classification accuracy.
    \item \textbf{UR-FUNNY}~\citep{hasan_ur-funny_2019}  is a large-scale dataset for humor detection in human speech, consisting of more than 16000 video samples ( from TED talks collected from 1866 videos. There are a total of 10,598, 2,626, and 3,290 segments in the train, validation, and testing sets. Humor is annotated as either positive or negative, with a homogeneous 50\% split in the dataset. 
    \item \textbf{MOSEI} ~\citep{bagher_zadeh_multimodal_2018} is a large-scale dataset for sentence-level sentiment analysis and emotion recognition from real-world online videos, containing more than 65 hours of annotated video from more than 1,000 speakers and 250 topics. There are a total of 16,265, 1,869, and 4,643 segments in the train, validation, and testing sets, resulting in 22,777 data points. As in MOSI, we train our models on continuous sentiment intensity labels with L1 loss and evaluate positive-negative classification. 
    \item \textbf{AVMNIST}~\citep{perez-rua_mfas_2019} is a dataset created by pairing audio of a human reading digits from the FSDD dataset \cite{jackson_jakobovskifree-spoken-digit-dataset_2025} with written digits in the MNIST dataset \cite{lecun_gradient-based_1998} with a task to predict the digit into one of 10 classes (0-9). While common practice \cite{perez2019mfas} is to increase difficulty by removing 75\% of energy in the visual modality via PCA and adding noise from ESC-50 to the audio modality, we use the unnoised image and audio modalities in order to preserve the redundant information between modalities. An audio sample from FSDD with matching digit identity is paired with each image in MNIST, resulting in 55000, 5000, and 10000 examples in the train, validation, and test sets respectively. We train vision transformers on MNIST images that are converted to 4x4 patches with a sequence length of 49. We preprocess the raw FSDD audio into 36 MFCC coefficients with a maximum sequence length of 20 using librosa \cite{mcfee_librosa_2015}.
  
\end{itemize}

\section{Additional Figures}

\subsection{Additional Synthetic Data Results}\label{app:synthetic_align}

% \note{TODO: Show that the fixed depth yields similar results}

In Figures \ref{fig:align_het}, we plot the alignment between unimodal encoders over the levels of heterogeneity, demonstrating that the maximum alignment negatively correlates with heterogenity. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/unbiased_cka_best_fixed_1_pairwise_align_het_scatter.pdf}
    \caption{\textbf{Alignment vs. Heterogeneity.} The alignment is computed between unimodal encoders trained on datasets with different levels of heterogeneity. Each dot is an independent run on a different model size on a given dataset. We compute the Pearson correlation coefficient $r$ between heterogeneity and maximum alignment, and see that the maximum level of achievable level of alignment decreases as the level of uniqueness increases. 5 different figures shows 5 levels of informational uniqueness.}
    \label{fig:align_het}
\end{figure*}

In Figures \ref{fig:perf_align_3}, \ref{fig:perf_align_6}, and \ref{fig:perf_align_9}, we plot the relation between alignment and performance for individual synthetic datasets, which show that as uniqueness increases, alignment is no longer an indicator of performance. 

%%%%% Section 2 figures below


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/unbiased_cka_best_fixed_depth_1_syn_depth_3_scaled_pairwise_perf_vs_align.pdf}
    \caption{\textbf{Alignment vs Performance for $D_{\phi}=3$.} The alignment-performance trend is shown across different levels of uniqueness, with the Pearson's correlation coefficient $r$ reported for each plot.  }
    \label{fig:perf_align_3}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/unbiased_cka_best_fixed_depth_1_syn_depth_6_scaled_pairwise_perf_vs_align.pdf}
    \caption{\textbf{Alignment vs Performance for $D_{\phi}=6$.} The alignment-performance trend is shown across different levels of uniqueness, with the Pearson's correlation coefficient $r$ reported for each plot. }
    \label{fig:perf_align_6}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/unbiased_cka_best_fixed_depth_1_syn_depth_9_scaled_pairwise_perf_vs_align.pdf}
    \caption{\textbf{Alignment vs Performance for $D_{\phi}=9$.} The alignment-performance trend is shown across different levels of uniqueness, with the Pearson's correlation coefficient $r$ reported for each plot. }
    \label{fig:perf_align_9}
\end{figure}


\subsection{Vision-Language Alignment vs Unique}\label{app:vision_language_align_unique}

In Figures \ref{fig:augreg_align_unique}, \ref{fig:clip_align_unique}, \ref{fig:ft_in12k_align_unique}, and \ref{fig:mae_align_unique}, we plot the relation between vision-language alignment and uniqueness, which show that the maximum alignment decreases with uniqueness.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/augreg_align_unique.pdf}
    \caption{\textbf{ImageNet21k Alignment vs Uniqueness.} The alignment is computed between vision models trained on ImageNet21k and large language models. We compute the Pearson correlation coefficient $r$ between the maximum alignment and uniqueness.}
    \label{fig:augreg_align_unique}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/clip_align_unique.pdf}
    \caption{\textbf{CLIP Alignment vs Uniqueness.} The alignment is computed between CLIP vision models and large language models. We compute the Pearson correlation coefficient $r$ between the maximum alignment and uniqueness.}
    \label{fig:clip_align_unique}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/ft_in12k_align_unique.pdf}
    \caption{\textbf{CLIP Finetuned on ImageNet21k Alignment vs Uniqueness.} The alignment is computed between vision models trained with CLIP and finetuned on ImageNet21k and large language models. We compute the Pearson correlation coefficient $r$ between the maximum alignment and uniqueness.}
    \label{fig:ft_in12k_align_unique}
\end{figure*}
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/mae_align_unique.pdf}
    \caption{\textbf{MAE Alignment vs Uniqueness.} The alignment is computed between MAE vision models and large language models. We compute the Pearson correlation coefficient $r$ between the maximum alignment and uniqueness.}
    \label{fig:mae_align_unique}
\end{figure}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/augreg_align_perf_unique_labeled.pdf}
%     \caption{Caption}
%     \label{fig:augreg_align_perf_unique_labeled}
% \end{figure*}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/ft_in12k_align_perf_unique_labeled.pdf}
%     \caption{Caption}
%     \label{fig:ft_in12k_align_perf_unique_labeled}
% \end{figure*}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/mae_align_perf_unique_labeled.pdf}
%     \caption{Caption}
%     \label{fig:mae_align_perf_unique_labeled}
% \end{figure*}
\subsection{Vision-Language Alignment vs Performance}\label{app:vision_language_align_perf}

In Figures \ref{fig:dino_align_perf_unique}, \ref{fig:augreg_align_perf_unique}, \ref{fig:clip_align_perf_unique}, \ref{fig:ft_in12k_align_perf_unique}, and \ref{fig:mae_align_perf_unique}, we plot the relation between vision-language alignment and performance. 



% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/ft_in12k_align_unique.pdf}
%     \caption{Caption}
%     \label{fig:in12k_align_unique}
% \end{figure}


% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/mae_align_unique.pdf}
%     \caption{Caption}
%     \label{fig:mae_align_unique}
% \end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/dino_align_perf_unique.pdf}
    \caption{\textbf{DINOv2 Vision-Language Alignment vs Performance.} We plot the vision-language alignment using DINOv2 vision models  with respect to language model performance, measured using \texttt{bits-per-byte-loss} and show individual best fit lines for each size of vision model as well as the average Pearson correlation coefficient $r$. }
    \label{fig:dino_align_perf_unique}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/augreg_align_perf_unique.pdf}
    \caption{\textbf{ImageNet21K Vision-Language Alignment vs Performance.} We plot the vision-language alignment using vision models trained on ImageNet21K classification with respect to language model performance, measured using \texttt{bits-per-byte-loss} and show individual best fit lines for each size of vision model as well as the average Pearson correlation coefficient $r$. }
    \label{fig:augreg_align_perf_unique}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/clip_align_perf_unique.pdf}
    \caption{\textbf{CLIP Vision-Language Alignment vs Performance.} We plot the vision-language alignment using CLIP vision models with respect to language model performance, measured using \texttt{bits-per-byte-loss} and show individual best fit lines for each size of vision model as well as the average Pearson correlation coefficient $r$. }
    \label{fig:clip_align_perf_unique}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/ft_in12k_align_perf_unique.pdf}
    \caption{\textbf{CLIP finetuned on ImageNet-21k Vision-Language Alignment vs Performance.} We plot the vision-language alignment using CLIP finetuned on ImageNet-21k vision models with respect to language model performance, measured using \texttt{bits-per-byte-loss} and show individual best fit lines for each size of vision model as well as the average Pearson correlation coefficient $r$.}
    \label{fig:ft_in12k_align_perf_unique}
\end{figure*}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figures/mae_align_perf_unique.pdf}
    \caption{\textbf{MAE Vision-Language Alignment vs Performance.} We plot the vision-language alignment using MAE vision models with respect to language model performance, measured using \texttt{bits-per-byte-loss} and show individual best fit lines for each size of vision model as well as the average Pearson correlation coefficient $r$.}
    \label{fig:mae_align_perf_unique}
\end{figure*}


% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/clip_align_perf_unique_labeled.pdf}
%     \caption{Caption}
%     \label{fig:clip_align_perf_unique_labeled}
% \end{figure*}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/augreg_align_perf_unique_labeled.pdf}
%     \caption{Caption}
%     \label{fig:augreg_align_perf_unique_labeled}
% \end{figure*}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/ft_in12k_align_perf_unique_labeled.pdf}
%     \caption{Caption}
%     \label{fig:ft_in12k_align_perf_unique_labeled}
% \end{figure*}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/mae_align_perf_unique_labeled.pdf}
%     \caption{Caption}
%     \label{fig:mae_align_perf_unique_labeled}
% \end{figure*}