% \section{Experiments and Results}
\input{TABLES/main_results}


In this section, we perform comprehensive experiments to evaluate our proposed method to answer the following research questions:
\begin{itemize}[leftmargin=*]
\item \textbf{Q1:} Can a fully discretized EEG tokenization framework outperform baseline methods that rely on continuous embeddings? How crucial is joint frequency-temporal modeling in EEG analysis and tokenization? 

% \item \textbf{Q2:} How crucial is the joint modeling of frequency and temporal features for EEG analysis? 

\item\textbf{Q2:} How effectively do our learned tokens represent EEG features and capture class-specific characteristics? Do the generated tokens capture relevant frequency information?

% How well do our learned tokens represent EEG features, and how effective are they in capturing class-specific characteristics? Do the tokens generated by our \tokenizer capture good frequency features?

% \item\textbf{Q3:} How well do our learned tokens represent EEG features, and how effective are they in capturing class-specific characteristics?

% effectively capture both frequency and temporal features,along with class-specific characteristics, through unsupervised pretraining?}
% \askFillIn{Putting token analysis on here.}

% \item \textbf{Q4:} Do the tokens generated by our \tokenizer capture good frequency features?

% \item \askFillIn{\textbf{Q4: Do the tokens generated by our \tokenizer capture good frequency features? }}\askFillIn{Can we learn good frequency features?
% How can we learn frequencies? We can provide some in-depth frequency response analysis here.}

% \item \textbf{Q3:}  Do the tokens produced by our \tokenizer capture class-specific characteristics?
\item \textbf{Q3:}  Are the tokens generated by our \tokenizer scalable, and can they enhance the performance of existing EEG foundation models such as LaBraM?
% \askFillIn{Putting scalability analysis on here.}

\item \textbf{Q4:}  Do tokens produced by \tokenizer offer interpretability, and do they correspond to distinct, recognizable EEG patterns?
\end{itemize}

% \subsection{Datasets}
\subsection{Experiment Setup}
\subsubsection{Datasets:} We conducted our evaluation on four EEG datasets:
\begin{itemize}[leftmargin=*]
\item\textbf{TUH EEG Events (TUEV)} \cite{harati2015improved}: TUEV is a subset of the TUH EEG Corpus \cite{obeid2016temple}, which comprises clinical EEG recordings collected at Temple University Hospital between 2002 and 2017. The dataset is annotated for six EEG event types: spike and sharp wave (SPSW), generalized periodic epileptiform discharges (GPED), periodic lateralized epileptiform discharges (PLED), eye movement (EYEM), artifact (ARTF), and background (BCKG). 

\item\textbf{TUH Abnormal EEG Corpus (TUAB)} \cite{lopez2015automated}: TUAB comprises EEG recordings collected at Temple University Hospital, which are labeled for normal and abnormal EEG activity. 


% TUEV contains $11,914$ recordings, yielding a total of $112,491$ annotated samples, each with a duration of 5 seconds.

% Each sample is a 5-second recording from 16 channels, derived using the 16 bipolar montage based on the international 10–20 system.

\item\textbf{IIIC Seizure} \cite{jing2023development,ge2021deep}: The IIIC Seizure dataset is curated for the detection of six distinct ictal–interictal–injury continuum (IIIC) patterns and is sourced from \cite{jing2023development,ge2021deep}. The annotations include: (1) others (OTH), (2) seizure types (ESZ), (3) lateralized periodic discharge (LPD), (4) generalized periodic discharge (GPD), (5) lateralized rhythmic delta activity (LRDA), and (6) generalized rhythmic delta activity (GRDA). 


% The dataset is from \cite{jing2023development,ge2021deep} and comprises $2,689$ recordings, yielding a total of $135,096$ samples. Each sample is a 10-second EEG segment recorded from 16 channels.

\item\textbf{CHB-MIT} \cite{shoeb2009application}: The CHB-MIT dataset is a widely used benchmark for epilepsy seizure detection. It comprises EEG recordings from 23 pediatric subjects with intractable seizures. 

% The dataset includes $686$ recordings, yielding a total of $326,993$ EEG samples, each with a duration of 10 seconds.


% It contains $2,339$ recordings, yielding a total of $409,455$ samples, each with a duration of 10 seconds.

\end{itemize}



\subsubsection{Preprocessing:} We follow the preprocessing setup of BIOT \cite{yang2024biot}. Unlike LaBraM \cite{jiang2024large}, which utilized 23 channels in the TUEV and TUAB datasets, we adhere to the 16-channel bipolar montage from the international 10–20 system, as used in \cite{yang2024biot}. All EEG recordings are resampled to 200 Hz. For TUEV and TUAB, we apply a bandpass filter ($0.1$–$75$ Hz) and a notch filter (50 Hz), following the preprocessing pipeline of LaBraM \cite{jiang2024large}. STFT computation of the signals is performed using PyTorch, with detailed parameters provided in Appendix~\ref{app:stft_params}. For training, validation, and test splits, we follow the recommendations from \cite{yang2024biot}. Additional details on dataset statistics and splits are provided in Appendix~\ref{app:dataset_splits}.


\subsubsection{Baselines and Metrics:} We evaluated our approach against the baselines from \cite{yang2024biot} as well as the current state-of-the-art methods, including BIOT \cite{yang2024biot} and LaBraM \cite{jiang2024large}. All baselines were reproduced using their respective open-source GitHub repositories. To ensure a fair comparison, our experiments follow a single-dataset setting for all the baselines. Specifically for BIOT, we conducted their proposed unsupervised pretraining followed by fine-tuning on the same dataset. Similarly, for LaBraM, we used their base model and conducted neural tokenizer training, masked EEG modeling, and fine-tuning within the same dataset. For performance evaluation, we used balanced accuracy, Cohen's Kappa coefficient, and weighted-F1 score for multi-class classification tasks, while balanced accuracy, AUC-PR, and AUROC were used for binary classification tasks. For TUAB, we used binary cross-entropy loss for fine-tuning, while the cross-entropy loss was applied to the TUEV and IIIC datasets. Given the class imbalance in the CHB-MIT dataset, we employed focal loss for all experiments. All experiments were conducted using five different random seeds, and we report the mean and standard deviation for each metric. Appendix~\ref{app:experiment_details} provides additional details on the experiment settings.



% We reproduced the baselines from \cite{yang2024biot} borrowing their codebase. Additionally we compare with the current SOTA methods including BIOT\cite{yang2024biot} and LaBraM\cite{jiang2024large}, which we reproduced using their official GitHub repositories. In our study we conducted all the experiments in single dataset setting, as our current focus is to build and evaluate a good tokenizer. For BIOT we conducted their proposed unsupervised pretraining and fine-tuning on the same dataset. Similarly, for LaBraM, we utilized their base model and conducted neural tokenizer training, masked EEG modeling and fine-tuning on the same dataset for fair comparisons. For multi-class classification experiments (TUEV, IIIC) we utilized balanced accuracy, Cohen's Kappa coefficient and weighted-F1 score as metrics. Similarly for binary classification tasks, we utilized balanced accuracy, AUC-PR and AUROC metrics. For TUAB we used binary-cross entropy loss for fine-tuning and cross-entropy loss for TUEV and IIIC datasets. Due to the imbalance nature of CHB-MIT dataset, we used focal loss for all our experiments similar to past methods\cite{yang2024biot}. We used five different random seeds for all our experiments and report the mean and standard deviation for each metrics. 



\subsection{Q1: Performance and Importance of Joint Frequency-Temporal Modeling}
% \subsection{Q1: Performance Comparison with SOTA Continuous Embedding Based Methods}
% compare the scores. 
% talk on the parameters
% talk on the neural tokanizer param and ours and even with low params we are more efficient.
\noindent{\textbf{Q1.1 - Performance Evaluation:}} 
% In this section, we evaluate the performance of our \method framework on EEG-related tasks, comparing it against state-of-the-art (SOTA) methods based on continuous embeddings. 
Table~\ref{tab:eeg_classification} presents EEG event classification results on TUEV and abnormal detection performance on TUAB. Our \method consistently outperforms all baselines across all metrics. For event-type classification on TUEV, \method achieves a $5\%$ increase in balanced accuracy over BIOT ($0.4679\rightarrow0.4943$) and LaBraM ($0.4682\rightarrow0.4943$). In Cohen's Kappa, \method improves by $9\%$ over BIOT ($0.4890\rightarrow0.5337$) and $5\%$ over LaBraM ($0.5067\rightarrow0.5337$). In abnormal detection on TUAB, \method achieves a $5\%$ improvement across all metrics compared to LaBraM. This highlights that our fully discrete tokenization approach surpasses the performance of existing continuous embedding-based approaches. 
% The results also suggest that our tokenizer captures EEG representations more effectively than the neural tokenizer used in LaBraM for model pretraining. 
Another advantage of \method is its reduced model footprint. As shown in Table~\ref{tab:eeg_classification}, \method achieves better results with significantly fewer parameters—a 3-fold reduction compared to LaBraM (5.8M $\rightarrow$ 1.9M) and a 1.5-fold reduction compared to BIOT (3.2M $\rightarrow$ 1.9M). This reduction can be attributed to discrete tokenization approach, which compresses EEG into a token sequence, thereby reducing data complexity.


\noindent{\textbf{Q1.2 - Importance of Joint Frequency and Temporal Modeling:}} To evaluate the importance of joint frequency-temporal modeling, we conducted an ablation study comparing three tokenization variants: (1) \method-Raw Signal Only (\method-R), which uses only raw EEG patches $\{x_i\}_{i=1}^N$ to predict the spectrum $\mathbf{S}$, (2) \method-STFT Only (\method-S), and (3) \method, which jointly models both temporal and frequency features. Masked modeling was applied for token learning in the latter two, with consistent \encoder training across all variants. As shown in Table~\ref{tab:eeg_classification}, all three variants outperform existing baselines. In event classification, \method-S improves Cohen’s Kappa over \method-R ($0.5194 \rightarrow 0.5275$). However, in abnormal detection, \method-R achieves a higher AUC-PR ($0.8814 \rightarrow 0.8908$). These results indicate that different EEG tasks rely on distinct feature domains, underscoring the necessity of joint modeling. The primary \method consistently outperforms both single-domain approaches across all settings, further underscoring the importance of joint modeling.


% In the latter two variants, we employ masked modeling for token learning, while the subsequent \encoder training remains consistent across all variants. As shown in Table~\ref{tab:eeg_classification}, all three variants of \method consistently outperform existing baselines.  In event classification task \method-S achieves better performance compared to \method-R ($0.5194\rightarrow0.5275$) in Cohen's kappa. However, \method-R outperforms \method-S ($0.8814\rightarrow0.8908$) in AUC-PR score in abnormal detection tasks. This suggests that for different EEG tasks essential features comes from different domains highlighting the need of joint modeling. The main variant \method outperfoms both other single domain variants across all metrics and tasks. 

% the performance of our \method framework on EEG-related tasks and compare it against existing state-of-the-art (SOTA) methods that rely on continuous embeddings. Table~\ref{tab:eeg_classification} presents the results of EEG event type classification on TUEV and abnormal detection on TUAB datasets. Our \method outperforms all baselines across all metrics consistently. For event-type classification tasks using 5-second EEG signals, \method achieves an $5\%$ increase in balanced accuracy compared to existing state of the art methods BIOT($0.4679 \rightarrow 0.4943$) and LaBraM ($0.4682\rightarrow 0.4943$). In Cohen's Kappa \method shows an increase of $9\%$ compared to BIOT ($0.4890 \rightarrow 0.5337$) and $5\%$ increase compared to LaBraM ($0.5067 \rightarrow 0.5337$). In abnormal detection tasks given a 10-second sample, \method achieves an increase of approximately $5\%$ across all metrics compared to LaBraM ($0.7720\rightarrow 0.8152$, $0.8498 \rightarrow 0.8946$, $0.8534\rightarrow 0.8897$). This comparison highlights that our fully discrete tokenization approach achieves significant improvements compared to existing continuous embedding based models such as BIOT and hybrid method which somewhat rely on discrete tokens such as LaBraM. Another key advantage of the \method is its smaller model footprint. As shown in Table~\ref{tab:eeg_classification}, our \method archives better results with significantly  less number of parameters, with 3-fold reduction compared to LaBraM(5.8M$\rightarrow$1.9M) and around 1.5 fold reduction compared to BIOT(3.2M$\rightarrow$1.9M). This reduction in model complexity can be attributed to our discrete tokenization approach, which compresses EEG data into a sequence of tokens, thereby reducing overall data complexity.



% compares various models on the TUEV and TUAB datasets. The results demonstrate that \method consistently outperforms all baselines across all metrics. 


% Our results demonstrate that TFM-Token consistently outperforms all baselines across all metrics, highlighting the effectiveness of our tokenization method in EEG classification tasks. Another key advantage of the \method is its smaller model footprint. As shown in Table~\ref{tab:eeg_classification}, our framework performs better despite using fewer parameters than most baselines. For instance, while LaBraM-Base has 5.8M parameters and BIOT has 3.2M parameters, TFM-Token achieves superior results with only 1.9M parameters. This reduction in model complexity can be attributed to our discrete tokenization approach, which compresses EEG data into a sequence of tokens, thereby reducing overall data complexity while preserving essential information. Overall, these findings validate that discretized EEG tokenization not only matches but surpasses the performance of existing continuous embedding-based approaches while offering greater computational efficiency during inference. 

% Notably, TUAB presents a particularly challenging setting where LaBraM and BIOT exhibit lower performance than other baselines despite their pretraining and fine-tuning strategies. In contrast, TFM-Token significantly outperforms these models, which indicates that our \tokenizer learns highly informative discrete tokens. 




 





% \subsection{Q2: Importance of Joint Frequency and Temporal Modeling}
% % mention the three variants
% % mention that all our variants are better than baselines
% % highlight the best performing model
% % mention that in the next section we further deeply analyze these tokens
% To evaluate the effectiveness of joint frequency-temporal modeling, we conducted an ablation study comparing three variants of our tokenization framework: (1) TFM-Token-Raw Signal Only, which uses only raw EEG patches $\{x_i\}_{i=1}^N$ to predict the STFT transform $\mathbf{S}$, (2) TFM-Token-STFT Only, and (3) TFM-Token, which jointly models both temporal and frequency features. The latter two variants employ masked modeling for token learning, while the subsequent \encoder training remains consistent across all variants.

% As shown in Table~\ref{tab:eeg_classification}, not only does the TFM-Token variant with joint modeling consistently outperform both single-domain approaches, but notably, all three variants demonstrate superior performance compared to existing baselines. These results demonstrate the complementary nature of temporal features (capturing long-range dependencies and transient events) and frequency features (detecting characteristic oscillatory patterns) in EEG analysis. These results validate our approach of incorporating both domains to learn more informative EEG tokens, which we further analyze in detail in the following Sections~\ref{sec:token_quality_analysis} and \ref{sec:freq_learning}.




\subsection{Q2: EEG Token Quality Analysis and Frequency Learning}
\label{sec:token_quality_analysis}

\input{TABLES/token_utilization}

% \begin{figure}[t]
%     \centering
%     % \rule{0.8\linewidth}{0.5\linewidth} 
%     % \includegraphics[width=\linewidth]{Figures/Story_Overview_Fig.pdf}
%     \includegraphics[width=\linewidth]{FIG/retrieval_test_2.pdf}
%     % \includegraphics[width=\linewidth]{Figures/retrieval_test.pdf}
%     \caption{Overview of the token quality analysis study. (a) Class-Token Uniqueness Score of the tokenizers across different classes. (b) Precision scores of tokenizers in EEG is similar to the retrieval class.}
%     \label{fig:retrieval_test}
% \end{figure}
% % talk on token utilization and impact of PE.. Mention results in appendix. 
% % talk on classwise token uniqueness.. explain the experiment and how the scores are obtained.
% % talk on the data mining test. 
% In this section, we study the quality of the EEG tokens learned by our \tokenizer by analyzing three key aspects: (1) token utilization, (2) class-specific distinctiveness, and (3) semantic coherence. We conducted our analysis using all three \tokenizer variants and the neural tokenizer from LaBraM\cite{jiang2024large}, testing them on the test splits of both the TUEV and IIIC datasets, which have multi-classes. For consistency and fair comparison, all tokenizers employed a standardized vocabulary size of $8,192$ tokens.



% talk on token utilization and impact of PE.. Mention results in appendix. 
% talk on classwise token uniqueness.. explain the experiment and how the scores are obtained.
% talk on the data mining test. 
We study the quality of the EEG tokens learned by our \tokenizer by analyzing four key aspects: (1) token utilization, (2) class-specific distinctiveness, (3) similar class retrieval, and (4) frequency learning capability. We conducted our analysis using all three \tokenizer variants and the neural tokenizer from LaBraM \cite{jiang2024large}, testing them on the test splits of both the TUEV and IIIC datasets, which have multiple classes. All tokenizers employed a fixed vocabulary size of $8,192$ tokens for consistency and fair comparison.

\noindent{\textbf{Q2.1 - Token utilization and Class uniqueness:}} Token utilization ($\%$) score was calculated as the percentage of unique tokens activated from the total available vocabulary size. To quantify whether the tokenizers capture class-distinctive representations, we introduce the Class-Token Uniqueness Score, defined as:
$$
\text{Class-Token Uniqueness \%} = \frac{\text{\# Unique Tokens in Class} }{\text{\# Tokens Utilized by Class}}\times 100 %
$$
Figure~\ref{fig:retrieval_test}a visualizes the class-token uniqueness scores for each class in both datasets. A robust tokenizer should capture class-distinctive tokens across all dataset classes through unsupervised pretraining. To assess this, we computed the geometric mean (GM) of class-token uniqueness scores, as shown in Table~\ref{tab:token_utilization}. Our \tokenizer reduces token utilization by more than two-fold compared to the neural tokenizer on TUEV ($21.13\% \rightarrow 9.78\%$) and nearly two-fold on IIIC ($15.25\% \rightarrow 8.26\%$). 
It also significantly improves learning of class-unique tokens compared to neural tokenizer ($0.034\% \rightarrow 2.14\%$on TUEV, $0.0\% \rightarrow 1.429\%$ on IIIC). These results demonstrate that the \tokenizer captures more compact and useful tokens than the neural tokenizer. Additionally, \tokenizer achieves a higher class-token uniqueness score across all classes compared to \tokenizer-R ($0.0\% \rightarrow 1.429\%$ on IIIC) and \tokenizer-S  ($0.619\% \rightarrow 1.429\%$ on IIIC), as depicted in Figure~\ref{fig:retrieval_test}a. This further validates joint frequency-temporal modeling in EEG analysis.


% \tokenizer also achieves better class-token uniqueness score across all classes compared to to \tokenizer-R ($0.0\% \rightarrow 1.429\%$ on IIIC) and \tokenizer-S ($0.619\% \rightarrow 1.429\%$ on IIIC) variants, which can be observed in Figure~\ref{fig:retrieval_test}a. This also highlights the importance of joint frequency-temporal modeling. 



% Our \tokenizer shows more than two-fold reduction in token utilization compared to neural tokenizer on TUEV ($21.13\% \rightarrow 9.78\%$) and approximately two-fold reduction on IIIC($15.25\% \rightarrow 8.26\%$) datasets. Also, our \tokenizer has a significant increase in learning class-unique tokens compared to neural tokenizer ($0.034\% \rightarrow 2.14\%$ and $0.0\% \rightarrow 1.429\%)$. Both of these results suggests that our \tokenizer captures more useful and compact tokens compared to neural tokenizer. Additionally these tokens are more effective given its performance on EEG related tasks. 



% Our \tokenizer shows lower token utilization compared to neural tokenizer ($21.13\% \rightarrow 9.78\%$, $15.25\% \rightarrow 8.78\%$,


% % In order to evaluate the tokenizers' capacity to acquire class-specific unique tokens across all classes via unsupervised pretraining, we calculated the geometric mean (GM) of the class-token uniqueness scores. Table~\ref{tab:token_utilization} presents the token utilization alongside the GM of class-token uniqueness scores across various tokenization methods. 


% Furthermore, Figure~\ref{fig:retrieval_test}a visualizes the class-token uniqueness scores for each class in both datasets. 

% Compared to the neural tokenizer, all \tokenizer variants exhibit a significantly lower token utilization percentage. However, our proposed \tokenizer, which integrates both frequency and temporal domains, achieves a higher GM of class-token uniqueness, indicating a more representative yet compact vocabulary. Figure~\ref{fig:retrieval_test}a clearly illustrates our tokenizer's effectiveness across all classes in both TUEV and IIIC datasets. Tokenizers based solely on raw signals struggle to capture class-unique tokens, underscoring the importance of joint temporal-frequency modeling. Despite having significantly fewer parameters than the neural tokenizer, our \tokenizer captures more informative and class-distinctive tokens, reflected in its superior classification performance. 

\noindent{\textbf{Q2.2 - Tokens for Similar-Class Sample Mining:}} We conducted an EEG signal mining experiment based on similar-class sample retrieval. Given a multi-channel EEG sample, we first obtain its discrete token representation. Using the Jaccard similarity score, we then retrieve the top $K$ most similar samples from the dataset and compute the precision score for correctly retrieving samples of the same class. For this study, we constructed a balanced subset from the IIIC and TUEV datasets and tested all four tokenization methods. The retrieval performance, illustrated in Figure~\ref{fig:retrieval_test}b, shows that all \tokenizer variants significantly outperform neural tokenizer. Notably, \tokenizer-S and \tokenizer achieve nearly $60\%$ precision on the TUEV for $K=1$. While the Jaccard similarity measure demonstrates initial feasibility, further research is needed to identify optimal metrics for token-based EEG retrieval. 

% A promising future direction lies in leveraging these discrete tokens for identifying positive pairs within datasets, potentially enabling novel contrastive learning frameworks for EEG analysis.

\begin{figure}[t]
    \centering
    % \rule{0.8\linewidth}{0.5\linewidth} 
    % \includegraphics[width=\linewidth]{Figures/Story_Overview_Fig.pdf}
    \includegraphics[width=\linewidth]{FIG/retrieval_test_2_new.pdf}
    % \includegraphics[width=\linewidth]{Figures/retrieval_test.pdf}
    \caption{Analysis of token quality across three \tokenizer variants and the neural tokenizer. (a) Comparison of class-token uniqueness scores across all classes. (b) Retrieval performance comparison of tokenizers in a similar-class sample mining task.}
    
    % \caption{Overview of the token quality analysis study. (a) Class-Token Uniqueness Score of the tokenizers across different classes. (b) Precision scores of tokenizers in EEG is similar to the retrieval class.}
    \label{fig:retrieval_test}
    % \vspace{-0.2cm}
\end{figure}
\noindent{\textbf{Q2.3 - Evaluating the Frequency Learning of \tokenizer Tokens:}}
\label{sec:freq_learning}
% In this experiment, we compare frequency-domain and temporal-domain Transformer encoders to evaluate their ability to capture diverse frequency features in EEG signals. 
% We first apply a discrete Fourier transform to each token to decompose it into its constituent frequency components. 
% Then, we compute spectral entropy from all amplitude values as a quantitative measure of how uniformly the energy is distributed across the frequency spectrum. 
% In our framework, a higher spectral entropy value indicates that the energy is more evenly spread among different frequency components, implying that the model has learned a broader range of frequency features.
% Figure \ref{fig:frequeny_ana} shows the results on the TUEV, TUAB, and CHBMIT datasets,
% showing that the frequency encoder produces tokens with higher spectral entropy than the temporal encoder.
% For instance, on the TUEV dataset, the frequency encoder achieved an average spectral entropy of 0.26 compared to 0.14 for the temporal encoder. 
% This result suggests that the frequency encoder is more effective at extracting diverse frequency characteristics from the raw EEG data, which may enhance performance on downstream tasks such as classification.
In this experiment, we compare the frequency and temporal-domain encoders of the \tokenizer to evaluate their ability to capture diverse frequency features in EEG signals. 
Specifically, we arrange all tokens in temporal order and perform a discrete Fourier transform on the token sequence. 
This process decomposes the tokens into frequencies, where each frequency reflects the degree of change between tokens at various scales. 
Larger changes indicate more diverse token representations.
%—that is, the model has learned clear differences between tokens.
Then, we compute spectral entropy, defined as the normalized Shannon entropy of the amplitude values, to quantify how energy is distributed across the spectrum. 
Higher spectral entropy means that the model has learned a broader range of frequency features, capturing differences from both large-scale trends and fine details. 
Figure \ref{fig:frequeny_ana} shows that on the TUEV, TUAB, and CHBMIT datasets, the frequency encoder produces tokens with significantly higher spectral entropy than the temporal encoder. 
For example, on the TUEV dataset, the frequency encoder achieved an average spectral entropy of 0.26, while the temporal encoder reached only 0.14. 
This multi-scale sensitivity benefits downstream tasks such as classification, where learning detailed differences in EEG tokens can improve performance.








% \subsection{Q4: Evaluating the Frequency Learning of \tokenizer Tokens}
% \label{sec:freq_learning}

% In this experiment, we compare the frequency and temporal Transformer encoders to assess how well they capture multiple EEG signal frequencies. 
% We apply a Fourier transform to break each token into its frequency components, then use spectral entropy to measure how diverse these components are.
% % Specifically, a higher entropy value means that more frequency components are represented in the token. 
% Specifically, a higher entropy value indicates that the energy in the frequency spectrum is distributed more uniformly, meaning that the model has learned representations from a wider range of frequency components.
% Our tests on the TUEV, TUAB, and CHBMIT datasets consistently show that the frequency encoder produces tokens with higher spectral entropy, proving that it captures a broader range of EEG frequency features.

% In this experiment, we compare frequency-domain and temporal-domain Transformer encoders to evaluate their ability to capture diverse frequency features in EEG signals. 
% We first apply a discrete Fourier transform to each token to decompose it into its constituent frequency components. 
% Then, we compute spectral entropy from all amplitude values as a quantitative measure of how uniformly the energy is distributed across the frequency spectrum. 
% In our framework, a higher spectral entropy value indicates that the energy is more evenly spread among different frequency components, implying that the model has learned a broader range of frequency features.
% Figure \ref{fig:frequeny_ana} shows the results on the TUEV, TUAB, and CHBMIT datasets,
% showing that the frequency encoder produces tokens with higher spectral entropy than the temporal encoder.
% For instance, on the TUEV dataset, the frequency encoder achieved an average spectral entropy of 0.26 compared to 0.14 for the temporal encoder. 
% This result suggests that the frequency encoder is more effective at extracting diverse frequency characteristics from the raw EEG data, which may enhance performance on downstream tasks such as classification.


\begin{figure}[t]
    \centering
    % \rule{0.8\linewidth}{0.5\linewidth} 
    % \includegraphics[width=\linewidth]{Figures/Story_Overview_Fig.pdf}
    \includegraphics[width=0.98\linewidth]{FIG/frequency_ana.pdf}
    % \includegraphics[width=\linewidth]{Figures/retrieval_test.pdf}
    \caption{
    %5 samples randomly selected from the TUEV, TUAB, and CHBMIT datasets to analyze the frequency complexity of the token sequences learned by both the temporal and frequency encoders.
    An analysis of how the proposed frequency and temporal-domain encoders capture frequency features, by using the spectral entropy of the learned token sequences from randomly selected samples. 
    Higher values indicate that the tokens contain richer frequency information.}
    \label{fig:frequeny_ana}
    % \vspace{-0.2cm}
\end{figure}



% \begin{figure}[t]
%     \centering
%     % \rule{0.8\linewidth}{0.5\linewidth} 
%     % \includegraphics[width=\linewidth]{Figures/Story_Overview_Fig.pdf}
%     \includegraphics[width=\linewidth]{FIG/Labram_tfm.pdf}
%     % \includegraphics[width=\linewidth]{Figures/retrieval_test.pdf}
%     \caption{Performance Comparison of LaBraM with their neural tokenizer vs TFM-Tokenizer}
%     \label{fig:labram_tfm}
% \end{figure}

% \subsection{Q3: Scalability of \tokenizer}
\subsection{Q3: Does \tokenizer Enhance LaBraM?}
\input{TABLES/labram_with_TFM_tokens}
% explain the experiment then the results
% mention the size of both tokenizers.
To assess the scalability of \tokenizer, we investigated its ability to enhance an existing EEG foundation model. We selected LaBraM \cite{jiang2024large}, which employs a neural tokenizer solely for pretraining. 
This setup makes it an ideal candidate for this study. We replaced LaBraM neural tokenizer with \tokenizer during the masked EEG modeling stage and evaluated its performance on TUEV and TUAB, presented in Table~\ref{tab:LaBraM_with_TFM}. On TUEV, LaBraM with \tokenizer achieves a $9\%$ increase in balanced accuracy ($0.4682 \rightarrow 0.5147$) and a $3\%$ increase in Cohen's Kappa ($0.5067 \rightarrow 0.5220$). 
On TUAB, \tokenizer consistently outperforms the neural tokenizer. 
These results confirm the capability of \method in enhancing the performance of EEG foundation models. The increase in balanced accuracy suggests that our tokenizer learns more class-discriminative tokens than the neural tokenizer.


% These validate that our \tokenizer is capable of existing EEG foundation models. Also improvement in balanced accuracy indicates that \tokenizer's tokens learns better class related tokens compared to neural tokenizer. 


% To evaluate the scalability of our \tokenizer, we investigated whether it can enhance the performance of an existing state-of-the-art EEG foundation model. For this study, we selected LaBraM \cite{jiang2024large}, as its training scheme includes a neural tokenizer used solely for pretraining the final model. This makes it an ideal candidate for assessing the adaptability of our tokenizer. We replaced the neural tokenizer in LaBraM with our \tokenizer during the masked EEG modeling stage and evaluated its performance on the TUEV and TUAB datasets. Table~\ref{tab:LaBraM_with_TFM} presents the performance comparison between LaBraM with its original neural tokenizer and LaBraM using our \tokenizer. The results clearly demonstrate that LaBraM with \tokenizer outperforms the original LaBraM model across all metrics, indicating that \tokenizer generates higher-quality tokens compared to the neural tokenizer. Improvement in balanced accuracy across both datasets indicates that \tokenizer learns tokens with better class discriminability compared to the neural tokenizer. These results validate the scalability of our \tokenizer, as it can be integrated into different EEG foundation models and further enhance their performance.



% \subsection{Token quality analysis}

% TODO:
% \begin{itemize}
%     \item do utilization analysis and unique token analysis
%     \item gets the unique tokens and creates some figures. Share it with Brandon.
% \end{itemize}
\begin{figure}[t]
    \centering
    % \rule{0.8\linewidth}{0.5\linewidth} 
    % \includegraphics[width=\linewidth]{Figures/Story_Overview_Fig.pdf}
    \includegraphics[width=0.98\linewidth]{FIG/token_interpret_TUEV_1_new3.pdf}
    % \includegraphics[width=\linewidth]{Figures/retrieval_test.pdf}
    \caption{Overview of motifs captured by \tokenizer on the TUEV dataset: (a) presents three samples from the PLED class, while (b) displays three samples from the GPED class.
    % {\color{red}
    % font is too small.
    % }
    }
    %\vspace{-0.5cm}
    \label{fig:interpret_TUEV_1}
\end{figure}
\subsection{Q4: Interpretability of Learned Tokens}
\label{sec:Q6}
We conducted a visual inspection study to determine whether the learned tokens by our \tokenizer capture and represent distinct time-frequency motifs in EEG signals. Our findings are presented in Figure~\ref{fig:interpret_TUEV_1}, which illustrates the unique tokens identified by \tokenizer on the TUEV dataset. 
% Each column in the Figure presents three different samples from the PLED and GPED classes. 
Each token represents a spectral window and its corresponding raw EEG patch (1s window with 0.5s overlap). For more precise visualization, we highlight each class's most frequently occurring tokens, assigning different colors to each token. This analysis confirms that our \tokenizer effectively captures class-specific distinct EEG patterns, encoding them into discrete tokens. For instance, token $4035$ in the PLED class consistently captures a characteristic drop followed by a rise in EEG signals, maintaining its structure across different samples despite variations in noise, amplitude, and minor shifts within the window. Similarly, tokens such as $5096$ and $3751$ in the GPED class demonstrate the advantages of combining frequency and temporal features, as they remain robust to minor temporal shifts and warping within a window due to emphasizing spectral patterns. However, we also identified certain limitations associated with using fixed windowing and overlapping segments for tokenization. Specifically, when large shifts cause a distinct EEG pattern to be split across two adjacent windows, the model may assign them separate tokens, potentially treating them as distinct patterns. 

% Addressing this issue could be an important direction for future research, enabling the learning of shift-invariant tokens. 


% to further enhance the learning. Overall, these findings highlight the potential of discretized tokenization for EEG, as it reduces data complexity by mitigating the effects of noise and amplitude variations, while also enabling a structured and interpretable representation of EEG patterns.





% \subsection{Discrete tokens for EEG Data Mining}
% Experiment, using the data from test set, retrieve top 10 similar samples from training and use that to assign labels for each test sample. 
% study the confusion matrix for each class for their retrieval.
% TODO:
% \begin{itemize}
%     \item Do retrieval test for the new variants of TFM-TOKEN on TUEV and IIIC
%     \item Do continuous token retrieval experiments using BIOT
% \end{itemize}


% \subsection{Does TFM-TOKEN capture time-frequency information better?}
% In this section focus on comparing TFM-TOKEN stft reconstruction with the LaBRAM frequency reconstruction and discuss. (Park)



% \subsection{Positional Encoding ablation in the tokenizer}

% \subsection{Different Setting Ablation}
% for LaBraM using the pretrained model run experiment for 8 channel only in Finetuning. Same for ours. Dataset TUEV.


% \subsection{Importance of Capturing Frequency Band Information for EEG}
% This section includes an ablation on various masking methods and compares their results on TUEV. 
%     1. Random frequency masking
%     2. Frequency band masking
% Also, an ablation of a model variant without a freq encoder will be conducted, and the results will be provided.

% \subsection{Importance of raw signal in EEG analysis}
% Provide the token utilization plots to show that including the raw signals improves the learning. Additionally provide a table to compare with and without raw signal ablation of the model. (We already have this result)




% \subsection{Is TFM-TOKEN generalizable to other biosignals?}
% add single dataset pretraining and fine-tuning results here. or recreate the BIOT setting for ECG and HAR, provide the results. 


% \subsection{TFM Tokenizer scaling experiments}

% \subsection{TFM Classifier scaling experiments}





 



