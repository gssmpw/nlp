% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.

We introduce \tokenizer, a novel tokenization framework tailored for EEG analysis that transforms continuous, noisy brain signals into a sequence of discrete, well-represented tokens for various EEG tasks. Conventional approaches typically rely on continuous embeddings and inter-channel dependencies, which are limited in capturing inherent EEG features such as temporally unpredictable patterns and diverse oscillatory waveforms. In contrast, we hypothesize that critical time-frequency features can be effectively captured from a single channel. By learning tokens that encapsulate these intrinsic patterns within a single channel, our approach yields a scalable tokenizer adaptable across diverse EEG settings. We integrate the \tokenizer with a transformer-based \encoder, leveraging established pretraining techniques from natural language processing—such as masked token prediction—followed by downstream fine-tuning for various EEG tasks. Experiments across four EEG datasets show that \method outperforms state-of-the-art methods. On TUEV, our approach improves balanced accuracy and Cohen's Kappa by $5\%$ over baselines. Comprehensive analysis of the learned tokens demonstrates their ability to capture class-distinctive features, enhance frequency representation, and ability to encode time-frequency motifs into distinct tokens, improving interpretability. 

% Code is available at \url{https://anonymous.4open.science/r/TFM-Token-FE33}.

% \url{https://github.com/Jathurshan0330/TFM-Token}.

% 



% Additionally, comprehensive studies on the learned tokens reveal their quality, evidenced by enhanced EEG signal mining capabilities, including similar class sample retrieval and the ability to generate distinct, class-specific tokens. Code for \method is available at \url{https://anonymous.4open.science/r/TFM-Token-FE33}.
