We introduced \method, a fully discrete tokenization framework consisting of \tokenizer and \encoder modules. \tokenizer effectively encodes time-frequency motifs from single-channel EEG into distinct tokens, which \encoder utilizes for downstream tasks. Comprehensive evaluations across multiple datasets demonstrate that \method outperforms state-of-the-art models while using fewer parameters. Our analysis confirms the robustness, class distinctiveness, and interpretability of learned tokens, providing deeper insights into EEG tokenization. 
These findings lay a strong foundation for future research in developing tokens that extend beyond fixed window segments and enhance capturing temporal-frequency information.


% We introduced \method, a fully discrete tokenization framework comprised of \tokenizer and \encoder modules. Our \tokenier effectively captures time-frequency motifs from single-channel EEG into distinct tokens, which are then used by \encoder for downstream tasks. Through comprehensive evaluations across multiple datasets, our approach demonstrated superior performance compared to state-of-the-art models while using fewer parameters. Our analysis further confirmed the learned tokens' robustness, class-distinctiveness, and interpretability, providing more insights into EEG tokenization. The ability of \method to generate robust, interpretable tokens while seamlessly integrating with existing EEG models lays a strong foundation for future research, particularly in developing tokens that extend beyond fixed window segments and enhance capturing temporal-frequency information.



% a fully discrete tokenization framework for EEG signals that captures time-frequency motifs from single-channel recordings. Our approach addresses key limitations of existing methods by providing an efficient, interpretable discrete representation that does not rely on continuous embeddings or inter-channel dependencies. Experimental evaluations across multiple datasets demonstrated state-of-the-art performance, surpassing leading models such as LaBraM and BIOT while using fewer parameters. The joint modeling of temporal and frequency features proved instrumental in enhancing representation quality, and the tokenizer's seamless integration with existing EEG models underscores its practical utility. Our analysis further confirmed the robustness, class-distinctiveness, and interpretability of the learned tokens. These findings establish promising direction for EEG tokenization where future directions could include capturing tokens not limited by window segments, further reducing redundencies and better tokens to capture temporal and frequency information.