\section{Related works: Categorization of Tokenization Approaches}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{FIG/simplified_related_works.png}
%     % \vspace{-.5cm}
%     \caption{EEG Tokenization Approaches - Can be moved to supplementary}
%     \label{fig:eeg_tokenization}
% \end{figure}

% \subsection{Symbolic Representation for Time-series}

% {\color{blue}I think this paper is not related to symbolic representation, it would be better to remove this subsection.
% But you can merge useful text to other subsections.}

% % Symbolic tokenization has long been explored to represent time-series data. One of the earliest approaches, Symbolic Aggregate approXimation (SAX) ____, discretizes time-series data into symbolic representations, enabling dimensionality reduction and pattern recognition. SAX first transforms raw time-series data into a Piecewise Aggregate Approximation (PAA) representation and then converts it into discrete symbolic strings. This representation facillated indexing, clustering and classification related tasks in time-series datamining. With the right token representation for EEG, we can adapt such tasks in EEG domain but the effective method to capture and represent EEG signals into a distinct tokens remains a challenge. 

% Symbolic tokenization has long been explored for representing time-series data, with Symbolic Aggregate approXimation (SAX) ____ being one of the earliest approaches. SAX discretizes time-series data by first transforming it into a Piecewise Aggregate Approximation representation and then converting it into discrete symbolic strings. This method facilitates tasks like indexing, clustering, and classification in time-series data mining. Adapting such techniques to EEG could unlock similar benefits. However, effectively capturing and representing EEG signals as distinct, meaningful tokens remains a significant challenge.

% Building on the success of tokenization strategies in NLP, recent time-series foundation models, including Chronos ____, Lag-Llama ____,  have adopted tokenization as a core mechanism for their representation learning. Chronos, for instance, tokenizes time series into bins using mean scaling and uniform binning____, while Lag-Llama concatenates all past time points within a specific window into a single vector as tokens____. While it's effective for simpler tasks, these methods do not scale well with high-frequency EEG signals. UniTS____ employs a complex tokenization strategy incorporating special tokens, including prompt tokens, sequence tokens, and task tokens. Despite the method's performance and generalizability, the effectiveness of this tokenization strategy in capturing complex temporal and frequency patterns in EEG signals remains an open question.