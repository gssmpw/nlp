\begin{table}[thpb]
\centering
\caption{Ablation on with and without masked token prediction pretraining on TUEV dataset.}
\label{tab:masked_token_predition_ablation}
\resizebox{\linewidth}{!}{%
% \begin{tabular}{l p{2.2cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm}} %{lccccccc}
\begin{tabular}{lccc}
\toprule
\textbf{Models}  &  \multicolumn{3}{c}{\textbf{TUEV (event type classification) }} \\
\cmidrule(lr){2-4} 
 &  \textbf{Balanced Acc.} & \textbf{Cohen's Kappa} & \textbf{Weighted F1} \\
\midrule


\textbf{TFM-Token}-R & $0.4898\pm 0.0105$ & $0.5194 \pm 0.0195$ & $0.7518 \pm 0.0095$ \\

\textbf{TFM-Token}-S & $0.4708 \pm 0.0339$ & $0.5275\pm 0.0314$ & $0.7538\pm 0.0152$ \\

\textbf{TFM-Token} &   $0.4943 \pm 0.0516$  &  $0.5337 \pm 0.0306$  &  $0.7570 \pm0.0163$ \\

% & $0.5487\pm0.0038$ &	$0.4589\pm0.0045$ &	$0.5549\pm0.0044$  \\

\bottomrule

\end{tabular}
}
% \multicolumn{8}{l}{1. The number of parameters for LaBraM is only considering their classifier model. The size of their neural tokenizer used for masked EEG modeling training was 8.6M.} \\
\end{table}