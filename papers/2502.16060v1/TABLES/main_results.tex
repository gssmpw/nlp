% \setlength{\tabcolsep}{6pt}
\begin{table*}[ht]
\centering
\caption{EEG classification performance comparison on TUEV and TUAB datasets.}
\label{tab:eeg_classification}
\resizebox{\textwidth}{!}{%
% \begin{tabular}{l p{2.2cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm}} %{lccccccc}
\begin{tabular}{lccccccc}
\toprule
\textbf{Models} & \textbf{Number} & \multicolumn{3}{c}{\textbf{TUEV (event type classification)}} & \multicolumn{3}{c}{\textbf{TUAB (abnormal detection)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
 &\textbf{of Params} & \textbf{Balanced Acc.} & \textbf{Cohen's Kappa} & \textbf{Weighted F1} & \textbf{Balanced Acc.} & \textbf{AUC-PR} & \textbf{AUROC} \\
\midrule

SPaRCNet\cite{jing2023development} & 0.79M & $0.4161 \pm 0.0262$  &  $0.4233 \pm 0.0181$  &  $0.7024 \pm 0.0104$  & $0.7885\pm0.0205$ &	$0.8782\pm0.0127$& $0.8804\pm0.0071$  \\

ContraWR\cite{yang2021self} & 1.6M &  $0.4384 \pm 0.0349$  &  $0.3912 \pm 0.0237$  &  $0.6893 \pm 0.0136$  & $0.7851\pm0.0050$ &	$0.8719\pm0.0076$ &	$0.8743\pm0.0042$ \\

CNN-Transformer\cite{peh2022transformer} & 3.2M &  $0.4087 \pm 0.0161$  &  $0.3815 \pm 0.0134$  &  $0.6854 \pm 0.0293$  & $0.7898\pm0.0062$ & $0.8781\pm0.0053$	&$0.8781\pm0.0067$\\

FFCL\cite{li2022motor} & 2.4M &  $0.3979 \pm 0.0104$  &  $0.3732 \pm 0.0188$  &  $0.6783 \pm 0.0120$  & $0.7869\pm0.0054$ &	$0.8694\pm0.0091$&	$0.8761\pm0.0044$\\

ST-Transformer\cite{song2021transformer} & 3.5M &  $0.3984 \pm 0.0228$  &  $0.3765 \pm 0.0306$  &  $0.6823 \pm 0.0190$  & $0.8004\pm0.0037$ & $0.8737\pm0.0069$ &	$0.8798\pm0.0029$\\

BIOT\cite{yang2024biot} & 3.2M &  $0.4679 \pm 0.0354$  &  $0.4890 \pm 0.0407$  &  $0.7352 \pm 0.0236$  & $0.7955\pm0.0047$&	$0.8819\pm0.0046$& $0.8834\pm0.0041$ \\

LaBraM-Base\cite{jiang2024large} & 5.8M &  $0.4682 \pm 0.0856$  &  $0.5067 \pm 0.0413$  &  $0.7466 \pm 0.0202$  & $0.7720\pm0.0046$ &$0.8498\pm0.0036$ &	$0.8534\pm0.0027$ \\

\midrule

\textbf{\method-R} & 1.8M & $\underline{0.4898} \pm 0.0105$ & $0.5194 \pm 0.0195$ & $0.7518 \pm 0.0095$ & $\underline{0.8033}\pm0.0021$ & $\underline{0.8908}\pm0.0027$ & $\underline{0.8849}\pm0.0024$  \\

\textbf{\method-S} & 1.9M& $0.4708 \pm 0.0339$ & $\underline{0.5275} \pm 0.0314$ & $\underline{0.7538} \pm 0.0152$ &  $0.7927\pm0.0044$	& $0.8814\pm0.0095$	&$0.8836\pm0.0052$\\

\textbf{\method} & 1.9M &  \textbf{0.4943 $\pm$ 0.0516}  &  \textbf{0.5337 $\pm$ 0.0306}  &  \textbf{0.7570 $\pm$ 0.0163} & \textbf{0.8152 $\pm$ 0.0014}	&   \textbf{0.8946 $\pm$ 0.0008}	&\textbf{0.8897 $\pm$ 0.0008}\\

% & $0.5487\pm0.0038$ &	$0.4589\pm0.0045$ &	$0.5549\pm0.0044$  \\

\bottomrule

\end{tabular}
}
\begin{flushleft} 
\footnotesize{1. The best results are \textbf{bolded}, while the second-best are \underline{underlined}. 2. The number of parameters for LaBraM is only considering their classifier model. The size of their neural tokenizer was 8.6M.
3. Results on CHB-MIT and IIIC Seizure  are provided in Table~\ref{tab:seizure_detection_CHBMIT} and \ref{tab:eeg_classification_IIIC} in Appendix~\ref{app:chbmit_iiic_comparison} }
\end{flushleft}
% \multicolumn{8}{l}{1. The number of parameters for LaBraM is only considering their classifier model. The size of their neural tokenizer used for masked EEG modeling training was 8.6M.} \\
\end{table*}


\begin{comment}
\begin{table*}[ht]
\centering
\caption{EEG classification performance comparison on TUEV and IIIC Seizure datasets. \textcolor{red}{Results on CHB-MIT and TUAB are provided in Table~\ref{} in Appendix~\ref{}}}
\label{tab:eeg_classification}
\resizebox{\textwidth}{!}{%
% \begin{tabular}{l p{2.2cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm}} %{lccccccc}
\begin{tabular}{lccccccc}
\toprule
\textbf{Models} & \textbf{Number} & \multicolumn{3}{c}{\textbf{TUEV (event type classification)}} & \multicolumn{3}{c}{\textbf{IIIC Seizure (seizure type classification)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
 &\textbf{of Params} & \textbf{Balanced Acc.} & \textbf{Cohen's Kappa} & \textbf{Weighted F1} & \textbf{Balanced Acc.} & \textbf{Cohen's Kappa} & \textbf{Weighted F1} \\
\midrule

SPaRCNet\cite{jing2023development} & 0.79M & $0.4161 \pm 0.0262$  &  $0.4233 \pm 0.0181$  &  $0.7024 \pm 0.0104$  & $0.5011 \pm 0.0286$ & $0.4115 \pm 0.0297$ & $0.4996 \pm 0.0262$  \\

ContraWR\cite{yang2021self} & 1.6M &  $0.4384 \pm 0.0349$  &  $0.3912 \pm 0.0237$  &  $0.6893 \pm 0.0136$  & $0.5421 \pm 0.0123$ & $0.4549 \pm 0.0166$ & $0.5387 \pm 0.0138$  \\

CNN-Transformer\cite{peh2022transformer} & 3.2M &  $0.4087 \pm 0.0161$  &  $0.3815 \pm 0.0134$  &  $0.6854 \pm 0.0293$  & $0.5395 \pm 0.0144$ & $0.4500 \pm 0.0165$ & $0.5413 \pm 0.0176$ \\

FFCL\cite{li2022motor} & 2.4M &  $0.3979 \pm 0.0104$  &  $0.3732 \pm 0.0188$  &  $0.6783 \pm 0.0120$  & $0.5309 \pm 0.0217$ & $0.4412 \pm 0.0253$ & $0.5315 \pm 0.0277$ \\

ST-Transformer\cite{song2021transformer} & 3.5M &  $0.3984 \pm 0.0228$  &  $0.3765 \pm 0.0306$  &  $0.6823 \pm 0.0190$  & $0.5093 \pm 0.0122$ & $0.4217 \pm 0.0151$ & $0.5217 \pm 0.0110$ \\

BIOT\cite{yang2024biot} & 3.2M &  $0.4679 \pm 0.0354$  &  $0.4890 \pm 0.0407$  &  $0.7352 \pm 0.0236$  & $0.4458\pm0.0183$  & $0.3418\pm0.0228$  &  $0.4511\pm0.0207$ \\

LaBraM-Base\cite{jiang2024large} & 5.8M &  $0.4682 \pm 0.0856$  &  $0.5067 \pm 0.0413$  &  $0.7466 \pm 0.0202$  & $0.4736 \pm 0.0101$ & $0.3716 \pm 0.0128$ & $0.4765 \pm 0.0097$ \\

\midrule

\textbf{TFM-Token-Raw Signal Only} & 1.8M & $0.4898 \pm 0.0105$ & $0.5194 \pm 0.0195$ & $0.7518 \pm 0.0095$ & $0.5812 \pm 0.0029$ &	$0.5051 \pm 0.0048$ &	$0.5874 \pm 0.0029$ \\

\textbf{TFM-Token-STFT Only} & 1.9M& $0.4708 \pm 0.0339$ & $0.5275 \pm 0.0314$ & $0.7538 \pm 0.0152$ &  $0.5709 \pm 0.0059$	&$0.4904 \pm 0.0065$	& $0.5799 \pm 0.0059$\\

\textbf{TFM-Token} & 1.9M &  \textbf{$0.4943 \pm 0.0516$}  &  \textbf{$0.5337 \pm 0.0306$}  &  \textbf{$0.7570 \pm 0.0163$} & & &\\

% & $0.5487\pm0.0038$ &	$0.4589\pm0.0045$ &	$0.5549\pm0.0044$  \\

\bottomrule

\end{tabular}
}
\begin{flushleft} 
\footnotesize{1. The number of parameters for LaBraM is only considering their classifier model. The size of their neural tokenizer used for masked EEG modeling training was 8.6M.}
\end{flushleft}
% \multicolumn{8}{l}{1. The number of parameters for LaBraM is only considering their classifier model. The size of their neural tokenizer used for masked EEG modeling training was 8.6M.} \\
\end{table*}
\end{comment}

            
\begin{comment}
    
\begin{table*}[ht]
\centering
\caption{\textcolor{red}{Comparison}}
\label{tab:eeg_classification}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\textbf{Models} & \textbf{Number} & \multicolumn{3}{c}{\textbf{TUEV (event type classification)}} & \multicolumn{3}{c}{\textbf{IIIC Seizure (seizure type classification)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
 &\textbf{of Params} & \textbf{Balanced Acc.} & \textbf{Cohen's Kappa} & \textbf{Weighted F1} & \textbf{Balanced Acc.} & \textbf{Cohen's Kappa} & \textbf{Weighted F1} \\
\midrule

SPaRCNet \cite{} & 0.79M & 0.4161$\pm$0.0262  &  0.4233$\pm$0.0181  &  0.7024$\pm$0.0104  & 0.5011$\pm$0.0286 &	0.4115$\pm$0.0297	& 0.4996$\pm$0.0262  \\


ContraWR \cite{} & 1.6M &  0.4384$\pm$0.0349  &  0.3912$\pm$0.0237  &  0.6893$\pm$0.0136  & 0.5421$\pm$0.0123	&0.4549$\pm$0.0166	&0.5387$\pm$0.0138  \\

CNN-Transformer \cite{} & 3.2M&  0.4087$\pm$0.0161  &  0.3815$\pm$0.0134  &  0.6854$\pm$0.0293  & 0.5395$\pm$0.0144 & 0.4500$\pm$0.0165 & 0.5413$\pm$0.0176 \\

FFCL \cite{} & 2.4M &  0.3979$\pm$0.0104  &  0.3732$\pm$0.0188  &  0.6783$\pm$0.0120  & 0.5309$\pm$0.0217 &	0.4412$\pm$0.0253 & 0.5315$\pm$0.0277 \\

ST-Transformer \cite{}& 3.5M &  0.3984$\pm$0.0228  &  0.3765$\pm$0.0306  &  0.6823$\pm$0.0190  & 0.5093$\pm$0.0122 &	0.4217$\pm$0.0151 & 0.5217$\pm$0.0110 \\

% (Vanilla) BIOT \cite{}& 3.2M &  0.4682$\pm$0.0125  &  0.4482$\pm$0.0285  &  0.7085$\pm$0.0184  & $\pm$  &  $\pm$  &  $\pm$  \\

BIOT^{*}\cite{}& 3.2M &  0.4679$\pm$0.0354  &  0.4890$\pm$0.0407  &  0.7352$\pm$0.0236  & $\pm$  &  $\pm$  &  $\pm$  \\


LaBraM-Base \cite{}& 5.8M &  0.4682$\pm$0.0856  &  0.5067$\pm$0.0413  &  0.7466$\pm$0.0202  & 0.4736$\pm$0.0101 &	0.3716$\pm$0.0128& 0.4765$\pm$0.0097 \\

\midrule


% prev results
% \textbf{TM-Token-Raw Signal Only}& 2.1M&  0.4134$\pm$0.0289	&0.5021$\pm$0.0404&	0.7363$\pm$0.0213  &  $\pm$  &  $\pm$  \\

% \textbf{TFM-Token-STFT Only}& 2.3M&  0.4232$\pm$0.0240	&0.5432$\pm$0.0340 &	0.7605$\pm$0.0162  &  $\pm$  &  $\pm$  \\

% \textbf{TFM-Token}& 2.3M&  \textbf{0.4712}\pm0.0230  &  \textbf{0.5450}\pm0.0170  &  \textbf{0.7612}\pm0.0073 & $\pm$  &  $\pm$  &  $\pm$  \\

\textbf{TM-Token-Raw Signal Only}& 1.1M &  	&&	 &  $\pm$  &  $\pm$  \\

\textbf{TFM-Token-STFT Only}& &  	& &	  &  $\pm$  &  $\pm$  \\

\textbf{TFM-Token}& 1.9M&  \textbf{0.4943}$\pm$0.0516  &  \textbf{0.5337}$\pm$0.0306  &  \textbf{0.7570}$\pm$0.0163 & $\pm$  &  $\pm$  &  $\pm$  \\

% \hline\\
% BIOT(6 EEG datasets) \cite{}&  &  $\pm$  &  $\pm$  &  $\pm$  & $\pm$  &  $\pm$  &  $\pm$  \\
% LaBraM-Base (6 EEG datasets) \cite{}&  &  $\pm$  &  $\pm$  &  $\pm$  & $\pm$  &  $\pm$  &  $\pm$  \\
% % \hline\\
% TFM-Token (6 EEG datasets)&  &  $\pm$  &  $\pm$  &  $\pm$  & $\pm$  &  $\pm$  &  $\pm$  \\


\bottomrule
\multicolumn{7}{l}{1. The number of parameters for LaBraM is only considering their classifier model. The size of their neural tokenizer used for masked EEG modeling training was 8.6M.} \\
% \multicolumn{7}{l}{2. \textbf{Bold} for the best model (trained from scratch) and \fbox{box} for the best pre-trained models. Running time comparison is in Appendix C.4.} \\
\end{tabular}
}
\end{table*}
\end{comment}