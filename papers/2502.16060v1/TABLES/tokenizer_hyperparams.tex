\begin{table}[thpb]
    \centering
    \caption{Hyperparameters for \tokenizer unsupervised pretraining on single-channel setting}
    % \resizebox{\linewidth}{!}{%
    \begin{tabular}{lc}
        \hline
        \textbf{Hyperparameter}& \textbf{Values}  \\
        \hline
          Batch size & 256 \\
          Optimizer & AdamW \\
          Weight decay & 0.00001 \\
          $\beta_1$ & 0.9\\
          $\beta_2$ & 0.99\\
          Learning rate scheduler & Cosine\\
          Minimal Learning rate & 0.001 \\
          Peak Learning rate & 0.005 \\
          \# of Warmup Epochs & 10 \\
          \# of Pretraining Epochs & 100\\

          
       
        \hline
    \end{tabular}%}
    \label{tab:tfm_tokenizer_training_params}
\end{table}

\begin{table}[thpb]
    \centering
    \caption{Hyperparameters for \tokenizer}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lccc}
        \hline
        \multicolumn{3}{c}{\textbf{Hyperparameter}} & \textbf{Values}  \\
        \hline
        \multirow{10}{*}{Temporal Encoder} & \multirow{4}{*}{Convolution layer 1} & Input Channels & 1 \\
                        &           & Output Dimension & 64 \\
                        &           & Kernel Size & 200 \\
                        &           & Stride       & 100 \\
                        & \multirow{3}{*}{Convolution layer 2} & Output Dimension & 64 \\
                        &           & Kernel Size & 1 \\
                        &           & Stride       & 1 \\
                        & \multirow{3}{*}{Convolution layer 3} & Output Dimension & 32 \\
                        &           & Kernel Size & 1 \\
                        &           & Stride       & 1 \\   
        \hline
        \multirow{10}{*}{Frequency Patch Encoder} & \multirow{4}{*}{Convolution layer 1} & Input Channels & 1 \\
                        &           & Output Dimension & 64 \\
                        &           & Kernel Size & 5 \\
                        &           & Stride       & 5 \\
                        & \multirow{3}{*}{Convolution layer 2} & Output Dimension & 64 \\
                        &           & Kernel Size & 1 \\
                        &           & Stride       & 1 \\
                        & \multirow{3}{*}{Convolution layer 3} & Output Dimension & 64 \\
                        &           & Kernel Size & 1 \\
                        &           & Stride       & 1 \\   
        \hline
        \multirow{4}{*}{Frequency Transformer} &  & Transformer Encoder Layers & 2 \\
                        &           & Embedding Dimension & 64 \\
                        &           & Number of Heads & 8 \\
        \hline
        \multirow{3}{*}{Gated Patchwise Aggregation} &  & Output Dimension & 32 \\
                                    &           & Kernel Size & 5 \\
                                    &           & Stride       & 5 \\

        \hline
        \multirow{4}{*}{Temporal Transformer} &  & Transformer Encoder Layers & 2 \\
                        &           & Embedding Dimension & 64 \\
                        &           & Number of Heads & 8 \\
        
        % \hline
        \multicolumn{3}{c}{Token vocabulary (Codebook size)} & 8192 \\
        \hline
        \multirow{4}{*}{Transformer Decoder} &  & Transformer Encoder Layers & 8 \\
                        &           & Embedding Dimension & 64 \\
                        &           & Number of Heads & 8 \\
                        Linear Decoder & & & 100 \\
        \hline
    \end{tabular}}
    \label{tab:tfm_tokenizer_params}
\end{table}

