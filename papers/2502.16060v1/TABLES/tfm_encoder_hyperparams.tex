\begin{table}[thpb]
    \centering
    \caption{Hyperparameters for \encoder, its masked token prediction pretraining and downstream finetuning}
    \resizebox{0.8\linewidth}{!}{%
    \begin{tabular}{lc}
        \hline
        \textbf{Hyperparameter} & \textbf{Values}  \\
        \hline
         Transformer Encoder Layers & 4 \\
         Embedding Dimension & 64 \\
         Number of Heads & 8 \\
        \midrule
        \multicolumn{2}{c}{\textbf{Masked Token Prediction Pretraining}}\\
        \midrule
      Batch size & 512 \\
      Optimizer & AdamW \\
      Weight decay & 0.00001 \\
      $\beta_1$ & 0.9\\
      $\beta_2$ & 0.99\\
      Learning rate scheduler & Cosine\\
      Minimal Learning rate & 0.001 \\
      Peak Learning rate & 0.005 \\
      \# of Warmup Epochs & 5 \\
      \# of training Epochs & 50\\

      \midrule
        \multicolumn{2}{c}{\textbf{Finetuning}}\\
        \midrule
    \multicolumn{2}{c}{Other parameters are the same as above except:}\\
      $\beta_2$ & 0.999\\
      label smoothing (multi-class) & 0.1 \\
        
        \hline
    \end{tabular}}
    \label{tab:tfm_encoder_params}
\end{table}


          

          
