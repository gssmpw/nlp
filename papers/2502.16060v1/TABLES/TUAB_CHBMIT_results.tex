\begin{table}[thpb]
\centering
\caption{Seizure detection performance comparison on the CHB-MIT dataset}
\label{tab:seizure_detection_CHBMIT}
\resizebox{\linewidth}{!}{%
% \begin{tabular}{l p{2.2cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm}} %{lccccccc}
\begin{tabular}{lcccc}
\toprule
\textbf{Models} & \textbf{Number} &  \multicolumn{3}{c}{\textbf{CHB-MIT (seizure detection)}} \\
\cmidrule(lr){3-5} 
 &\textbf{of Params} &  \textbf{Balanced Acc.} & \textbf{AUC-PR} & \textbf{AUROC} \\
\midrule

SPaRCNet\cite{jing2023development} & 0.79M & $0.5483\pm0.0164$ &	$0.1264\pm0.0112$	&$0.7953\pm0.0402$ \\

ContraWR\cite{yang2021self} & 1.6M &  $0.6479\pm0.0423$ &	$0.2312\pm0.0316$	&$0.8198\pm0.0197$\\

CNN-Transformer\cite{peh2022transformer} & 3.2M & $0.5900\pm0.0429$	& $0.2250\pm0.0512$	&$0.8333\pm0.0326$\\

FFCL\cite{li2022motor} & 2.4M &  $0.6565\pm0.0260$ &	$0.2115\pm0.0286$	&$0.8071\pm0.0117$ \\

ST-Transformer\cite{song2021transformer} & 3.5M & $0.6717\pm0.0271$	&$0.2133\pm0.0148$	&$0.8657\pm0.0139$ \\

BIOT\cite{yang2024biot} & 3.2M &  $0.6582\pm0.0896$&	$0.3127\pm0.0890$ &	$0.8456\pm0.0333$\\

LaBraM-Base\cite{jiang2024large} & 5.8M & $0.5035\pm0.0078$ &	$0.0959\pm0.0742$& $0.6624\pm0.1050$  \\

\midrule

% \textbf{TFM-Token-Raw Signal Only} & 1.8M & $0.5812 \pm 0.0029$ &	$0.5051 \pm 0.0048$ &	$0.5874 \pm 0.0029$ \\

% \textbf{TFM-Token-STFT Only} & 1.9M&  $0.7538 \pm 0.0152$ &  $0.5709 \pm 0.0059$	&$0.4904 \pm 0.0065$	& $0.5799 \pm 0.0059$\\

\textbf{\method} & 1.9M & \textbf{0.6750 $\pm$ 0.0392} &	\textbf{0.3379 $\pm$ 0.0515}	& \textbf{0.8839 $\pm$ 0.0173} \\

% & $0.5487\pm0.0038$ &	$0.4589\pm0.0045$ &	$0.5549\pm0.0044$  \\

\bottomrule
\end{tabular}
}
% \multicolumn{8}{l}{1. The number of parameters for LaBraM is only considering their classifier model. The size of their neural tokenizer used for masked EEG modeling training was 8.6M.} \\
\end{table}









\begin{table}[t]
\centering
\caption{Seizure type classification performance comparison on the IIIC Seizure dataset}
\label{tab:eeg_classification_IIIC}
\resizebox{\linewidth}{!}{%
% \begin{tabular}{l p{2.2cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm}} %{lccccccc}
\begin{tabular}{lcccc}
\toprule
\textbf{Models} & \textbf{Number} &  \multicolumn{3}{c}{\textbf{IIIC Seizure (seizure type classification)}} \\
\cmidrule(lr){3-5} 
 &\textbf{of Params} &  \textbf{Balanced Acc.} & \textbf{Cohen's Kappa} & \textbf{Weighted F1} \\
\midrule

SPaRCNet\cite{jing2023development} & 0.79M &  $0.5011 \pm 0.0286$ & $0.4115 \pm 0.0297$ & $0.4996 \pm 0.0262$  \\

ContraWR\cite{yang2021self} & 1.6M &  $0.5421 \pm 0.0123$ & $0.4549 \pm 0.0166$ & $0.5387 \pm 0.0138$  \\

CNN-Transformer\cite{peh2022transformer} & 3.2M &  $0.5395 \pm 0.0144$ & $0.4500 \pm 0.0165$ & $0.5413 \pm 0.0176$ \\

FFCL\cite{li2022motor} & 2.4M &  $0.5309 \pm 0.0217$ & $0.4412 \pm 0.0253$ & $0.5315 \pm 0.0277$ \\

ST-Transformer\cite{song2021transformer} & 3.5M &  $0.5093 \pm 0.0122$ & $0.4217 \pm 0.0151$ & $0.5217 \pm 0.0110$ \\

BIOT\cite{yang2024biot} & 3.2M &  $0.4458\pm0.0183$  & $0.3418\pm0.0228$  &  $0.4511\pm0.0207$ \\

LaBraM-Base\cite{jiang2024large} & 5.8M &   $0.4736 \pm 0.0101$ & $0.3716 \pm 0.0128$ & $0.4765 \pm 0.0097$ \\

\midrule

% \textbf{TFM-Token-Raw Signal Only} & 1.8M & $0.5812 \pm 0.0029$ &	$0.5051 \pm 0.0048$ &	$0.5874 \pm 0.0029$ \\

% \textbf{TFM-Token-STFT Only} & 1.9M&  $0.7538 \pm 0.0152$ &  $0.5709 \pm 0.0059$	&$0.4904 \pm 0.0065$	& $0.5799 \pm 0.0059$\\

\textbf{\method} & 1.9M &  \textbf{0.5775 $\pm$ 0.0042} &	\textbf{0.4985 $\pm$ 0.0039} &	\textbf{0.5847 $\pm$ 0.0050}\\

% & $0.5487\pm0.0038$ &	$0.4589\pm0.0045$ &	$0.5549\pm0.0044$  \\

\bottomrule

\end{tabular}
}
% \multicolumn{8}{l}{1. The number of parameters for LaBraM is only considering their classifier model. The size of their neural tokenizer used for masked EEG modeling training was 8.6M.} \\
\end{table}






% % \setlength{\tabcolsep}{6pt}
% \begin{table}[thpb]
% \centering
% \caption{EEG abnormal detection performance comparison on TUAB}
% \label{tab:tuab_chb_results}
% \resizebox{\linewidth}{!}{%
% % \begin{tabular}{l p{2.2cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm}} %{lccccccc}
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Models} & \textbf{Number} & \multicolumn{3}{c}{\textbf{TUAB (abnormal detection)}} \\
% \cmidrule(lr){3-5} 
%  &\textbf{of Params} & \textbf{Balanced Acc.} & \textbf{AUC-PR} & \textbf{AUROC} \\
% \midrule

% SPaRCNet \cite{} & 0.79M & $0.7885\pm0.0205$	&$0.8804\pm0.0071$	&$0.8782\pm0.0127$ \\

% ContraWR \cite{} & 1.6M &  $0.7851\pm0.0050$&	$0.8743\pm0.0042$	&$0.8719\pm0.0076$ \\

% CNN-Transformer \cite{} & 3.2M &  $0.7898\pm0.0062$&	$0.8781\pm0.0067$&	$0.8781\pm0.0053$  \\

% FFCL \cite{} & 2.4M &  $0.7869\pm0.0054$ &$0.8761\pm0.0044$ &	$0.8694\pm0.0091$\\

% ST-Transformer \cite{} & 3.5M &  $0.8004\pm0.0037$&	$0.8798\pm0.0029$	&$0.8737\pm0.0069$  \\

% BIOT\cite{} & 3.2M &  $0.7955\pm0.0047$	& $0.8834\pm0.0041$& 	$0.8819\pm0.0046$ \\

% LaBraM-Base \cite{} & 5.8M & $0.7720\pm0.0046$ &	$0.8534\pm0.0027$& $0.8498\pm0.0036$  \\

% \midrule

% \textbf{TFM-Token-Raw Signal Only} & 1.8M & $0.8033\pm0.0021$ & $0.8849\pm0.0024$ & $0.8908\pm0.0027$ \\

% \textbf{TFM-Token-STFT Only} & 1.9M& & & \\

% \textbf{TFM-Token} & 1.9M &  \textbf{0.8152}$\pm0.0014$	& \textbf{0.8897}$\pm0.0008$	& \textbf{0.8946}$\pm0.0008$\\

% % & $0.5487\pm0.0038$ &	$0.4589\pm0.0045$ &	$0.5549\pm0.0044$  \\

% \bottomrule

% \end{tabular}
% }
% % \multicolumn{8}{l}{1. The number of parameters for LaBraM is only considering their classifier model. The size of their neural tokenizer used for masked EEG modeling training was 8.6M.} \\
% \end{table}


 