% \section{\method: Time-Frequency Motif Learning-based EEG Tokenization}

\method comprises two components: (1) \textbf{\tokenizer:} addresses problem statement 1 by converting continuous EEG signals into discrete tokens, capturing key time-frequency features, and (2) \textbf{\encoder:} tackles problem statement 2 by leveraging these tokens for downstream EEG tasks, such as classification. To mitigate the quadratic complexity of standard Transformers \cite{vaswani2017attention}, we employ a linear attention mechanism \cite{katharopoulos2020transformers, wang2020linformer} across all transformer modules.
% Figure~\ref{fig:tfm_token}a shows an architectural overview and feedward process.
Given input EEGs, \tokenizer independently tokenizes each channel into discrete token sequences. 
The \encoder then processes these tokens to perform classification.
For training \method, we first conduct an unsupervised pretraining of \tokenizer in a single-channel setting to encode time-frequency motifs into discrete tokens (see Figure~\ref{fig:tfm_token}a, \underline{Sec~\ref{sec:tfm_tokenizer}}). 
The tokenizer is then frozen, and \encoder undergoes masked token prediction pretraining (see Figure~\ref{fig:tfm_token}b, \underline{Sec~\ref{sec:tfm_encoder}}), followed by fine-tuning for downstream EEG tasks.
% During inference, given a multi-channel EEG input $\mathbf{X}$, \tokenizer independently tokenizes each channel into discrete token sequences. The \encoder then processes these tokens to perform classification.

% \noindent\textbf{\method Inference Overview:} During inference (see Figure~\ref{fig:tfm_token}a), given a multi-channel EEG input $\mathbf{X}$, the \tokenizer independently tokenizes each channel into discrete token sequences. The \encoder then processes these tokens to generate predictions for downstream EEG tasks.




\noindent\textbf{Time-Frequency discretization. }
\label{subsec:descretization}
The input to \method consists of discrete time and frequency patches, applied to both raw EEG data and its corresponding spectrogram.
This approach addresses the transient nature of EEG waveforms, which may decompose the entangled patterns and is foundational for motif learning.
% This process is foundational for motif learning.
% which aims to identify and utilize recurrent, meaningful patterns within the data.
Specifically, we segment $x$ into time-domain length patches $L$ with the same hop size of $H$. This yields a sequence of patches $\{x_i\}_{i=1}^{N}$, where $N = \lfloor (T-L)/H \rfloor + 1$ is the total number of patches. 
Using the same $L$ and $H$ for both STFT and temporal segmentation, we ensure that each patch $x_i$ corresponds one-to-one with a spectral window $\mathbf{S}_i$. In practice, we obtain the raw EEG patches by configuring the convolutional layers with a kernel size and stride that match the STFT window length $L$ and hop size $H$. 
Additionally, we denote embeddings and masks as $\mathbf{E}$ and $\mathbf{M}$, respectively.


\subsection{Single Channel \tokenizer}
\label{sec:tfm_tokenizer}
%In this section, describe the TFM tokenizer learning method. TFM tokenizer consists of three main modules: 1. Frequency Encoder 2. Temporal Encoder and 3. Task Decoder.
We introduce the \tokenizer, a scalable module for tokenizing single-channel EEG signals 
$x$ by effectively capturing both their temporal and frequency dynamics. Our design is inspired by the Vector-Quantized Variational Autoencoder (VQ-VAE)\cite{van2017neural}, which has been widely adopted for tokenization efforts in other domains such as video processing \cite{agarwal2025cosmos}. As illustrated in Figure~\ref{fig:tfm_token}a, the \tokenizer comprises three primary components: (1) Localized Spectral Window Encoder, (2) Temporal Encoder, and (3) Temporal Transformer. At a high level, \tokenizer adopts a \emph{frequency-then-time} paradigm. First, we isolate each spectral window $\mathbf{S}_i$ (for $i=1,…, N$) to capture intra-frequency band dependencies, yielding an aggregated representation for each window (Localized Spectral Window Encoder, see Figure~\ref{fig:tfm_token}d). Next, these frequency-based representations are combined with features extracted from the corresponding raw EEG patches $\{x_i\}_{i=1}^N$ (Temporal Encoder). Finally, the combined features are passed through the Temporal Transformer to capture long-range temporal dependencies, and the resulting embeddings are quantized into discrete tokens.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{KDD2025_EEG_Tokenization/FIG/Frequency_encoder.pdf}
%     % \vspace{-.5cm}
%     \caption{\method: Model Overview}
%     \label{fig:freq_encoder}
% \end{figure}



\noindent\textbf{Localized Spectral Window Encoder. } Capturing frequency-band characteristics or compositions is crucial for EEG analysis, as the data often contains oscillatory components (e.g., alpha, beta) with varying amplitudes and time.  
Unlike previous studies that project an entire spectral window with a single linear layer \cite{yang2024biot}, we propose to patch along the frequency axis, enabling better modeling of cross-frequency dependencies. In detail, this encoder consists of three main steps (see Figure~\ref{fig:tfm_token}d):
\begin{itemize}[left=0pt]
    \item \textbf{Frequency Patch Encoder. } Given a set of spectral windows $\{\mathbf{S}_i\}_{i=1}^N$, we isolate and divide each spectral window $\mathbf{S}_i$ into $P$ non-overlapping patches $\{\mathbf{S}_{(i,p)}\}_{p=1}^P$, each spanning $\Delta f$ frequency bins such that $P.\Delta f = F$. We then project each frequency patch into a latent space:
    $$
    e_{(i,p)} = \text{GroupNorm}\left(\text{GeLU}\left(\mathbf{W}_{p}\mathbf{S}_{(i,p)}\right)\right)
    $$
    where $\mathbf{W}_{p}\in \mathbb{R}^{D\times \Delta f}$ is a learnable matrix.
    
    \item \textbf{Frequency Transformer. } In order to capture the interactions among the frequency patch embeddings $e_{(i,p)}$, we apply a frequency transformer that operates along the frequency axis within each spectral window $\mathbf{S}_i$. This enables the capturing of intra-frequency band dependencies (e.g., interactions between low and high-frequency bands).
    
    \item \textbf{Gated Patchwise Aggregation:} In many EEG scenarios, large portions of the frequency spectrum can be irrelevant.
    For instance, tasks related to sleep primarily focus on frequency bands up to approximately 32 Hz \cite{Chen2023TNSRE}. 
    Also, the frequencies of interest vary across conditions and tasks. 
    To emphasize important frequency patches and suppress the rest, we adopt a gated aggregation mechanism. 
    Formally, we first group frequency patch embeddings, then apply gated weighting before concatenating them into a single embedding $\mathbf{E}^F_i$
    $$\mathbf{E}^F_i = \text{Concat}\left[\sigma\left(\mathbf{W_{g1}e_{(i,p)}}\right)\mathbf{W_{g2}e_{(i,p)}}\right]$$
    where $\mathbf{W_{g1}},\mathbf{W_{g2}}$ are trainable parameters and $\sigma(\cdot)$ is the element-wise sigmoid function.
\end{itemize}

\noindent\textbf{Temporal Encoder. } To capture temporal dynamics from the raw EEG patches $\{x_i\}_{i=1}^{N}$, we perform a linear projection of the patch followed by GELU\cite{hendrycks2016gaussian} activation and group normalization, producing temporal embeddings $\{\mathbf{E}^T_i\}_{i=1}^N$. \\

% Notably, including the raw signal alongside the STFT introduces beneficial stochasticity, thereby enhancing token learning\cite{}.


\noindent\textbf{Temporal Transformer. } We form a unified time-frequency embedding by combining each aggregated frequency embedding $\mathbf{E}_i^F$ with its corresponding temporal embedding $\mathbf{E}_i^T$ and feed the resulting vectors into a temporal transformer. This module attends jointly to time and frequency features across all windows, capturing long-range dependencies. The output is then quantized into discrete tokens $\{v_i\}_{i=1}^N$ using a learnable codebook $\mathcal{V}^k$. Notably, we do not add any positional encoding as EEG signals are inherently non-stationary and often exhibit chaotic behavior; thus, our goal is to capture their distinctive features without enforcing positional constraints. 
We further provide an ablation study (Appendix~\ref{app:with_wo_pe_ablation}) showing the effect of omitting positional encodings.\\




% \noindent\textbf{$\rm(\hspace{.18em}2\hspace{.18em})$ Temporal Encoder:} To capture temporal dynamics from the raw EEG patches $\{x_i\}_{i=1}^{N}$, we perform a linear projection followed by GELU\cite{} activation and group normalization. For simplicity, we implement patching and embedding in a single 1D convolutional layer of kernel size $L$ and stride $H$, which directly encodes each patch into a temporal feature space resulting $\{E^i_T\}_{i=1}^N$. Notably, including the raw signal alongside the STFT introduces beneficial stochasticity, thereby enhancing token learning\cite{}.\\

% \textbf{Frequency Transformer:} We next learn the dependencies among the  $P$ frequency patches $\{e_f^{(i,p)}\}_{p=1}^P$ within each spectral window. Treating these $P$ patches as a sequence, the frequency transformer operates along the frequency axis to capture intra-window interactions (e.g., interactions between low and high-frequency bands), thereby refining the patch embeddings $\{e_f^{(i,p)}\}$.

% \textbf{Frequency Patch Encoder:} Let $\{X_F^i\}_{i=1}^N$ be a set of spectral windows of $X_F$, where $X_F^i\in\mathbb{R}^F$ denotes the STFT magnitude at time step $i$ (i.e., the $i$-th column of $X_F$). Each $X_F^i$ thus captures the frequency-domain information for a particular time window. To process each spectral window $X_F^i$ in a patchwise manner, we divide it into $P$ non-overlapping frequency patches $\{X_F^{(i,p)}\}_{p=1}^P$. Each patch $X_F^{(i,p)}$ covers $\Delta f$ frequency bins such that $P.\Delta f = F$. We project each patch into a latent embedding $e_f^{(i,p)}\in\mathbb{R}^{d}$ as follows:

% \begin{equation}
%     e^{(i,p)} = \text{GroupNorm}\left(\text{GeLU}\left(\mathbf{W}_{fp}X_F^{(i,p)}\right)\right)
% \end{equation}

% where $\mathbf{W}_{fp}\in \mathbb{R}^{d\times \Delta f}$ is a learnable parameter and $d$ is the embedding dimension. This frequency patch encoder thus produces a set of patchwise embeddings: $\{e_f^{(i,p)}\}_{p=1}^P$ 
% By splitting each spectral window into smaller frequency patches, we can better capture and model cross-frequency dependencies. After extracting frequency features within each spectral window, we can concatenate these features across time to form a sequence that reflects temporal variations within each frequency band. To extract these dynamics, our Localized Spectral Window Encoder comprises three main components (see Figure~\ref{fig:freq_encoder}): (1) Frequency Patch Encoder, (2) Frequency Transformer, and (3) Gated Patchwise Aggregation. 

% In this section, we present our proposed \tokenizer inspired by a Vector-Quantized Variational Autoencoder\cite{van2017neural}. To simplify notation, we focus on a single EEG channel and, therefore, omit the channel index $c$; the same procedure applies to each channel in parallel. As illustrated in Figure~\ref{fig:tfm_token}b, the \tokenizer comprises three main components: (1)Localized Spectral Window Encoder, (2)Temporal Encoder, and (3)Temporal Transformer. Following these modules, we quantize the learned embeddings and feed the resulting tokens into a task decoder. The entire module is trained with a masked frequency band and temporal reconstruction objective, facilitating the tokenizer to capture time-frequency motifs in EEG signals. 

% \noindent\textbf{Time-Frequency Decomposition and Segmentation:}
% We decompose the signal into frequency and temporal components to capture complementary structures—such as oscillatory patterns and transient events—in EEG data. Our \tokenizer learns time-frequency motifs from two transformations of the signal: (1) Time-frequency representation $(X_F)$ and (2) Raw EEG patches$\left(\{x_i\}_{i=1}^{N}\right)$. To obtain $X_F$, we compute a Short-Time Fourier Transform (STFT) of the single channel EEG $x$ using a windowing function $w(.)$ of length $L$ and a hop size $H$:
% \begin{equation}
%     X_F(\omega,\tau) = \left|\sum_{l=0}^{L-1}x(\tau H +l)w(l)e^{\frac{-j2\pi\omega l}{L}} \right|
% \end{equation}
% Where $\omega$ indexes the discrete frequencies and $\tau$ indexes the time segments (i.e., time windows shifted by $H$). We retain only the magnitude $|.|$, which is input to our feature encoder.

% In parallel, we segment the same EEG signal $x$ into time-domain patches of length $L$ with the same hop size of $H$. This yields  patches $\{x_i\}_{i=1}^{N}$, where $N = \lfloor (T-L)/H \rfloor + 1$ is the total number of patches. By aligning the patch length and hop size with those used for the STFT, we preserve one-to-one correspondence between each patch and its corresponding spectral window. These patches are then fed into the temporal encoder to capture temporal dynamics.\\




% $\mathbf{E}_f^i = \left[e_f^{(i,1)},e_f^{(i,2)},...,e_f^{(i,P)}\right]$.



% \textbf{Gated Patchwise Aggregation:} In many EEG scenarios, large portions of the frequency spectrum are irrelevant, and crucial frequency bands can vary widely across different conditions and tasks. Consequently, identifying and emphasizing relevant frequency bins while suppressing uninformative ones is essential. Inspired by weakly supervised strategies used in gigapixel pathology images\cite{lu2021data}, we employ a gated bin-wise aggregation mechanism to selectively weight frequency patches. Specifically, we first group frequency patch embeddings $\{e_f^{(i,p)}\}_{p=1}^{P}$ for each spectral window $X^i_F$ and then apply gated weighting before concatenating them into a single embedding $E_F^i$.
% % % To distill features from the most salient frequency patches, we employ a gated patchwise aggregation strategy. 
% % Specifically, we first group frequency patch embeddings, then apply a gated weighting to emphasize important patches and suppress uninformative ones. By concatenating the gated embeddings, we obtain a single frequency embedding $E^i_F$ that summarizes the entire spectral window $X_F^i$ as follows:
% \begin{equation}
%     E^i_F = \text{Concat}\left[\sigma\left(\mathbf{W_{g1}e_f^{(i,p)}}\right)\mathbf{W_{g2}e_f^{(i,p)}}\right]
% \end{equation}

% where $\mathbf{W_{g1}},\mathbf{W_{g2}}$ are learnable parameter matrices and $\sigma(.)$ is the element-wise sigmoid function. \\
% \noindent\textbf{$\rm(\hspace{.18em}3\hspace{.18em})$ Temporal Transformer:} We form a unified time-frequency embedding by concatenating each aggregated frequency embedding $\{E^i_F\}_{i=1}^N$ with the corresponding temporal embedding $\{E^i_T\}_{i=1}^N$. To provide ordering information, we add positional embeddings before feeding the concatenated vectors into the Temporal Transformer, which jointly attends to temporal and frequency features. The resulting embeddings are then quantized into discrete tokens $v^i$ using a learnable codebook $\mathcal{V}^k$.

% In order to enable the model to focus on frequency band dynamics and capture temporal variation we propose a combination of frequency band and temporal masking strategy to train the tokenizer. 

% \subsubsection{Temporal Encoder and Transformer}
% Focus on the importance of the raw signal and how we combine both raw signals with frequency temporal dynamics.

\noindent\textbf{Tokenizer Codebook. } 
Our tokenizer is designed to capture temporal frequency motifs by applying vector quantization along the time axis, that is, individual patches.
This is different from conventional quantization methods in computer vision, which typically operate on the embedding dimension of a patch \cite{van2017neural, esser2020taming}.
As a result, each token in our vocabulary directly represents a waveform within a specific short duration and can facilitate the retrieval of the timestamp of input EEGs to understand the composition of an EEG.
This enhances interpretability; for instance, specific codes in the vocabulary are activated in response to the occurrence of particular temporal or frequency patterns.
To validate this, we conducted a visual inspection study, with results presented in Section~\ref{sec:Q6}. \\




\noindent\textbf{Frequency Making Prediction for Tokenizer Learning. }
The input to the tokenizer is the full frequency band of a temporal patch. 
To facilitate effective frequency learning, we employ a frequency-band and temporal masking strategy during \tokenizer training.
% This masked learning strategy highlights frequency-band dynamics, which are crucial in EEG analysis. 
We partition $\mathbf{S}$ into $N_F = \lfloor\frac{F}{\delta_f}\rfloor$ frequency groups of size $\delta_f$ and randomly mask a subset based on a predefined mask ratio (e.g., 0.5), forming a frequency-band mask $M_F$. Simultaneously, a temporal mask $M_T$ is applied along the time axis. The final mask, $M = M_F \land M_T$, is used to mask the STFT representation, $S^M = M \odot S$, where $\odot$ denotes element-wise multiplication. 
For data augmentation and efficient training, we adopt a symmetric masking approach from \cite{jiang2024large} by defining $M_{\text{sym}} = 1-M$. 
Given $\mathbf{S}^M$ and $x$, our \tokenizer first quantized them into a sequence of distinct tokens, which is then passed through a transformer decoder followed by a linear decoder that predicts the masked regions. The tokenizer is trained using a masked reconstruction loss:

% Although quantization is applied along the time axis, i.e., a full band 
% rather than the frequency axis, our tokenizer is designed to capture both temporal and frequency-domain features. To achieve this, we employ a frequency band and temporal masking strategy. This masked learning strategy highlights frequency-band dynamics, which are crucial in EEG analysis. We partition $\mathbf{S}$ into $N_F = \lfloor\frac{F}{\delta_f}\rfloor$ frequency groups of size $\delta_f$ and randomly mask a subset based on a predefined mask ratio (e.g., 0.5), forming a frequency-band mask $M_F$. Simultaneously, a temporal mask $M_T$ is applied along the time axis. The final mask, $M = M_F \land M_T$, is used to mask the STFT representation, $S^M = M \odot S$, where $\odot$ denotes element-wise multiplication. For data augmentation and efficient training, we adopt a symmetric masking approach from\cite{jiang2024large} by defining $M_{\text{sym}} = 1-M$. Given $\mathbf{S}^M$ and $x$, our \tokenizer first quantized them into a sequence of distinct tokens, which is then passed through a transformer decoder followed by a linear decoder that predicts the masked regions. The tokenizer is trained using a masked reconstruction loss:
$$
\mathcal{L}_{\mathrm{rec}}=
\sum_{(f,t)\in\mathcal{I}_{\mathrm{mask}}}
\!\bigl\|\mathbf{S}(f,t)-\hat{\mathbf{S}}(f,t)\bigr\|_2^2
$$
where $(f,t)\in\mathcal{I}_{\text{mask}}$ represents masked frequency--time indices and $\hat{\mathbf{S}}$ is the reconstruction of $\mathbf{S}$. Alongside the masked reconstruction loss, we incorporate the codebook and commitment losses \cite{van2017neural}, yielding the total loss:   

$$
\mathcal{L}_{\mathrm{token}}=\mathcal{L}_{\mathrm{recon}} + \alpha \;\sum_{i}
\bigl\|\mathrm{sg}[E_i]\;-\;v_i\bigr\|_2^2 + \beta \;\sum_{i}
\bigl\|E_i\;-\;\mathrm{sg}[v_i]\bigr\|_2^2
$$
where, $\mathrm{sg}[\cdot]$ is the stop-gradient operator, and $\alpha,\beta$ are hyperparameters. We also employ exponential moving average strategy\cite{van2017neural} for stable codebook updates. 




% Eventhough we apply quantization along the time axis not in the frequency axis, we want to enforce the tokenizer to capture features from time and frequency domain. To achieve that we employ a frequency band and temporal masking strategy. Capturingfrequency band wise features are important for EEG analysis and we achieve that by masking a group of freuqncy bins in $\mathbf{S}$. This masked learning strategy highlights frequency-band dynamics, which are crucial in EEG analysis. We partition $X$ into $N_F = \lfloor\frac{F}{\delta_f}\rfloor$  frequency groups of size $\delta f$. A mask ratio (e.g., 0.5) determines which groups are randomly masked to form a frequency-band mask $M_F$. Simultaneously, a temporal mask $M_T$ is created along the time axis. Their logical AND yields the resulting mask $M = M_F\land M_T$, which masks the STFT $X^M=M\odot X$. $\odot$ denotes element-wise product. For data augmentation and efficient training, we adopt a symmetric masking approach from\cite{jiang2024large} by defining $M_{\text{sym}} = 1-M$. 




% We enforce a frequency-band and temporal masked reconstruction objective to train our \tokenizer, enabling it to learn discrete time-frequency representations from single-channel EEG signals. This masked learning strategy highlights frequency-band dynamics, which are crucial in EEG analysis. We partition $X$ into $N_F = \lfloor\frac{F}{\delta_f}\rfloor$  frequency groups of size $\delta f$. A mask ratio (e.g., 0.5) determines which groups are randomly masked to form a frequency-band mask $M_F$. Simultaneously, a temporal mask $M_T$ is created along the time axis. Their logical AND yields the resulting mask $M = M_F\land M_T$, which masks the STFT $X^M=M\odot X$. $\odot$ denotes element-wise product. For data augmentation and efficient training, we adopt a symmetric masking approach from\cite{jiang2024large} by defining $M_{\text{sym}} = 1-M$. Given $X^M$ and $x$, our \tokenizer first quantized them into a sequence of distinct tokens, which is then passed through a transformer decoder followed by a linear decoder, which predicts the masked regions. Alongside the masked reconstruction loss, we incorporate the codebook and commitment losses from VQ-VAE\cite{van2017neural}, yielding the total loss:   
% \begin{align}
% \mathcal{L}_{\mathrm{total}}
% &=\;
% \sum_{(f,t)\in\mathcal{I}_{\mathrm{mask}}}
% \!\bigl\|X(f,t)-\hat{X}(f,t)\bigr\|_2^2%}_\text{Masked Recon. Loss}
% \\[6pt]
% &\quad+\;
% \alpha \;\sum_{i}
% \bigl\|\mathrm{sg}[E_i]\;-\;v_i\bigr\|_2^2%_\text{Codebook Loss}
% \\[6pt]
% &\quad+\;
% \beta \;\sum_{i}
% \bigl\|E_i\;-\;\mathrm{sg}[v_i]\bigr\|_2^2%}_\text{Commitment Loss},
% \end{align}
% \noindent
% where $(f,t)\in\mathcal{I}_{\text{mask}}$ represents masked frequency--time indices, $\hat{X}$ is the reconstruction of $X$, $\mathrm{sg}[\cdot]$ is the stop-gradient operator, and $\alpha,\beta$ are hyperparameters. We also employ exponential moving average strategy\cite{van2017neural} for stable codebook updates. 



% \subsection{Single Channel Token Learning}
% We pretrain the \tokenizer with a frequency-band and temporal masked reconstruction objective, enabling it to learn discrete time-frequency representations from single-channel EEG signals. This masked learning strategy highlights frequency-band dynamics, which are crucial in EEG analysis, and encourages the tokenizer to capture these patterns.
% \noindent\textbf{Frequency Band and Temporal Masking:} Let $\delta_f$ be the frequency band size, so we can partition the STFT $X_F\in\mathbb{R}^{F\times T}$ into $N_F = \lfloor\frac{F}{\delta_f}\rfloor$ frequency groups. With a mask ratio (e.g.,0.5), we randomly mask these groups to create a frequency-band mask $M_F$. Similarly, we create a temporal mask $M_T$ along the time axis. The combined mask is given as $M = M_F\land M_T$, where $\land$ denotes logical AND, resulting in the masked input $X_F^\mathcal{M}=M\odot X_F$. $\odot$ denotes element-wise product. To increase data samples and efficient training, we adopt a symmetric masking strategy\cite{jiang2024large} by defining $M_{sym} = 1-M$.
% \noindent\textbf{Loss Function:} Given the masked STFT $X_F^\mathcal{M}$ and the raw EEG $x$, our \tokenizer first encodes and quantizes them into a sequence of discrete tokens $v^i$. These token embeddings are then passed into a Transformer decoder followed by a linear decoder to predict the masked regions. Alongside the masked reconstruction loss, we incorporate the codebook and commitment losses from VQ-VAE\cite{van2017neural}, yielding the total loss:   




\subsection{Token-Wise \encoder}
\label{sec:tfm_encoder}
%In this section, we describe how to extend the learned \tokenizer to multi-channel EEG classification via our \encoder. 
The \encoder is designed to aggregate tokenized representations across channels and perform downstream classification. It consists of two main components: (1) a token-embedding lookup table and (2) linear attention transformer layers. Given a multi-channel recording $\mathbf{X}\in\mathbb{R}^{C\times T}$, we apply the fixed (pretrained) \tokenizer to each channel independently, obtaining discrete token sequences $\Bigl\{\{v_i^c\}_{i=1}^N\Bigr\}_{c=1}^C$. Each token is mapped to its embedding via a lookup table initialized with the \tokenizer’s learned codebook weights. 
We flatten the embeddings across channels and incorporate channel and position embeddings to encode respective information. 
We then prepend a [CLS] token \cite{devlin2018bert}, the output of which is designed to capture the overall representation of the sequence for downstream tasks. 
The resulting embeddings are then processed by linear attention transformers in \encoder.

\noindent\textbf{Token Prediction for \encoder Learning. }
We adopt a strategy akin to masked language modeling. 
We randomly mask tokens across multiple channels and time steps and train the model to predict these masked tokens via a cross-entropy loss. Along with representation learning, this approach enhances robustness to missing or corrupted data, common in real-world EEG systems where channels or time segments may be dropped or noisy. Finally, the \encoder is finetuned to downstream tasks.




