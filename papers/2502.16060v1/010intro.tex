Electroencephalograms (EEGs) reveal real-time neuronal activity with millisecond precision, reflecting the responses to various event stimuli. 
This makes EEGs essential not only for fundamental research \cite{OxfordBrain2019,NatNeuroscience2021} but also for diverse applications such as sleep staging \cite{phan2022sleeptransformer,yang2021self,pradeepkumar2024towards}, neurological disease detection \cite{kotoge2024splitsee,afzal2024ICMLrest}, and brain-computer interfaces \cite{ICML2012Precup_850,Lukas2022ICML,NEURIPS2023_AMAG}.
% EEG data is noisy and redundant, while lacking recognizable characteristics of general time series, such as trends or periodicity \cite{kotoge2024splitsee}.
Practical EEG analysis is a complex, multi-step process involving filtering, feature engineering, pattern recognition, annotation, etc.
Deep learning (DL) models have shown remarkable success in automating EEG analysis across various tasks \cite{sharma2022evolutionary,GNN_ICLR22,Chen2023TNSRE}.
These achievements stem from their representation learning capability, projecting noisy EEGs onto a discriminative feature space that captures associations with discrete neurophysiological events.
Recent neuroscientific literature reports that DL methods successfully match the reliability and accuracy of 20 clinical experts in identifying seizure-related events \cite{jing2023development}.

 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{FIG/story_fig_3.pdf}
    \caption{\tokenizer tokenizes EEG signals by encoding time-frequency motifs as discrete tokens. The neural tokenizer used for LaBraM \cite{jiang2024large}, which is a raw signal-based tokenizer, fails to capture time-frequency variations. Our \tokenizer effectively preserves this information.}
    % \vspace{-0.4cm}
    \label{fig:story_fig}
\end{figure}

Despite their success, effectively representing EEGs remains a primary focus in the research community.
Real-world EEG recordings are acquired under diverse scenarios, using different devices, and even for the same task, data formats often vary due to mismatched channels and variable lengths  \cite{yang2024biot}. 
Unfortunately, most existing methods typically learn representations on a case-by-case basis with specific architectures or fixed channel settings.
These methods exhibit limited generalization across tasks and poor scalability to different data formats.
For example, a sleep apnea model is not readily applicable to a sleep staging task, since the former aims to capture anomaly patterns while the latter is to learn discriminative representations \cite{pradeepkumar2024towards}.
Likewise, a sleep staging model trained on central region EEGs cannot directly generalize to frontal EEG data.
As a result, retraining, elaborate fine-tuning, or model adjustments are often required \cite{yang2024biot}, escalating practical costs and further restricting applicability to advanced devices or non-standard data, such as ECoG \cite{pradeepkumar2021decoding}.
There is thus an urgent need to develop an EEG analysis method that serves broader research objectives.

% For instance, a sleep apnea model cannot extend to sleep staging task, since former is somehow anormaly detection whereas the later focus on classification, or a sleep staging model trained on central region EEGs cannot directly generalize to frontal EEG data.
% % Or, multi-channel modeling on the standard 10-20 system {\color{blue}Cite} is not easily scalable to fewer channels.
% As a result, retraining or elaborate fine-tuning and task-oriented model adjustments are often required {\color{blue}Cite}, increasing practical costs and further limiting the applicability to advanced devices or non-standard data, such as ECoG {\color{blue}Cite of ECoG}.


Recently, the transformative impact of large foundation models\cite{touvron2023llama,achiam2023gpt} has inspired and elevated EEG representation learning to new heights.
Several foundation EEG models have been proposed \cite{duan2023dewave,yi2024learning,mohammadi2024eeg2rep,yang2024biot}, demonstrating both enhanced performance and generalization.
Researchers often \textit{tokenize EEGs} into discrete, short-duration snapshots across different data formats and model their dependencies leveraging foundational yet powerful Transformers.
By leveraging the paradigm of pretraining on large-scale, cross-task datasets, these models can learn universal representation, enabling effective adaptation to downstream tasks without requiring task-specific architectures \cite{kotoge2024splitsee,mohammadi2024eeg2rep}.
Thanks to unified tokenization and the automated input adaptation of the Transformer, such methods also address challenges such as complex data format reconfigurations \cite{yang2024biot} and labeled data scarcity \cite{jiang2024large}.
However, this direction remains nascent, and several limitations remain:


\begin{itemize}[left=0pt]
    \item \textbf{Inappropriate Tokenization Representation. } One reason large language models (LLMs) succeed is their effective tokenization and similar benefits have been demonstrated in image~\cite{van2017neural} and video~\cite{wang2024larp,agarwal2025cosmos} tokenization. However, existing foundation EEG models generally do not adopt a discrete tokenization paradigm. Although some methods claim to provide an EEG “tokenizer,” they typically lack a discrete approach similar to NLP. For instance, LaBraM~\cite{jiang2024large} employs a neural tokenizer only during pretraining but relies on raw EEG signals during inference.
    % failing to effectively capture key temporal and frequency dynamics (see Figure~\ref{fig:story_fig}. 
    BIOT~\cite{yang2024biot} suggests a “biosignal tokenizer” that remains continuous rather than discretized. Although time-series tokenization methods have shown promise~\cite{ansari2024chronos,rasul2023lag}, they do not scale well to EEG’s higher sampling rates and other artifacts. 
    
    % Such limitations hamper interpretability and degrade representation quality, suggesting that fully discrete tokenization could reduce data complexity and improve robustness to noise and other variations, thereby enabling LLM-level capabilities and interpretability for EEG.
    
    
    % \askFillIn{
    % Pls fill in:
    % One reason LLMs are so successful is that they have effective tokenizers, and even in computer vision, tokenizing images has shown great success. 
    % Existing foundation EEG models do not incorporate the tokenization paradigm. 
    % Even some methods claim they have tokenizer, but they are not true tokenizers, and why ((We shall generally introduce every line of work but immediately criticize them on their disadvantages).
    % Without proper tokenization, what issues arise? Why do they happen? What are the bad consequences?
    % }
     \item \textbf{Insufficient Frequency Representation. }
     % Key EEG features, termed distinct waveforms, are typically characterized by varying frequencies.
     Capturing eventful EEG features, which are characterized by distinct frequencies, is a primary focus of EEG analysis.
     However, tokenizing and modeling raw EEGs often lead to a loss of frequency diversity.
     This frequency representation collapse is a common issue in time-series modeling, as low-frequency components typically dominate the EEG data, biasing models toward lower frequencies while overlooking critical high-frequency features, such as spikes. Although LaBraM has recently incorporated frequency reconstruction into its objective function, it still relies on raw EEG signals, inevitably resulting in sub-optimal representation, as shown in Figure~\ref{fig:story_fig}.
     
     \item \textbf{Scalability and Generalization. } 
     EEG-related tasks vary significantly in channel configurations. 
     For example, seizure detection typically utilizes 16 channels, whereas sleep studies often require only 1–2 channels. 
     However, existing models are primarily designed for multi-channel settings, heavily relying on cross-channel prediction. 
     This design limits their scalability and adaptability to configurations with fewer or even single channels, as well as to varying acquisition setups.

    
\end{itemize}


Therefore, in this paper, we propose \method, an effective, fully discretized EEG tokenization framework that captures time-frequency motifs from single-channel EEG signals into distinct tokens. 
%These tokens can be directly utilized to train an EEG foundation model. 
Technically, our contributions are as follows:





% Electroencephalograms (EEG), as a staple brain imaging tool, reveal the real-time activity of millions of neurons with millisecond precision, reflecting various physiological states or event stimuli. This capability makes EEG essential not only for fundamental research into pathophysiology, neuroplasticity, and brain-computer interfaces but also for practical applications, such as sleep staging \cite{iber2007aasm}, diagnosing neurological disorders \cite{noachtar2009role}, and emotion analysis \cite{niemic2004studies}.
% Practical EEG analyses focus on capturing representative neural oscillations, or waveforms, characterized by varying frequency and amplitude, which correspond to specific brain states and functions. These oscillations include Delta (0.5–4 Hz), Theta (4–7 Hz), Alpha (8–12 Hz), and Beta (13–30 Hz), as well as distinctive patterns like sleep spindles and K-complexes. 
% However, these waveforms are often transient, noisy, and embedded within a complex temporal and frequency landscape, as shown in Figure~\ref{fig:story_fig}, making them challenging to interpret and learn effectively. 
% Moreover, the scarcity of labeled EEG data further makes it difficult to learn these intricate patterns effectively. As a result, developing robust methods for EEG analysis has been difficult. Traditional signal processing methods, like independent component analysis, wavelet transforms, and Fourier transforms, decompose EEG into clearer frequency or spatial representations but often rely on domain expertise and may lack generalizability across subjects or conditions. 



% Over the past decade, deep learning (DL) models have proven to be a powerful method in automating EEG analysis, achieving impressive performances across various tasks, such as seizure detection \cite{jing2023development,yang2024biot}, sleep stage classification\cite{yang2021self,phan2022sleeptransformer,pradeepkumar2024towards}, motor imagery classification\cite{amin2019deep}, stress assessment\cite{sharma2022evolutionary}, and emotion recognition\cite{zhang2020investigation}.  More recently, the paradigm of large language models (LLMs) and their groundbreaking success in natural language processing (NLP) has inspired and elevated EEG representation learning to new heights. This is largely driven by the label-free nature of pretraining in LLMs, which enables the models to leverage vast amounts of data, mitigate the challenge of label scarcity, and learn robust, generalized representations across diverse data domains\cite{devlin2018bert,achiam2023gpt}. These advantages make the LLM paradigm highly beneficial for time-series and EEG representation learning. Several works in EEG have been proposed employing methods such as contrastive learning\cite{kotoge2024splitsee,weng2024self} or masked prediction strategies\cite{mohammadi2024eeg2rep} inspired by time-series foundation models such as Chronos\cite{ansari2024chronos}, MOMENT\cite{goswami2024moment}, Lag-Llama\cite{rasul2023lag}, and MOIRAI\cite{woo2024unified}. These models also leverage tokenization strategies inspired by NLP to process and analyze time-series data\cite{ansari2024chronos,rasul2023lag}. However, several limitations and challenges still exist in EEG tokenization or representation learning:
% \begin{itemize}
%     \item \textbf{Complexity of temporal variations:} EEG signals exhibit various temporal variations, manifested as a mixture of low- and high-frequency components that co-occur and are intermixed in complex ways. Furthermore, key EEG features are transient, as shown in Figure~\ref{fig:story_fig}, occurring over short time intervals, and are inherently non-stationary. Accurately capturing these complex temporal patterns remains a significant challenge. Existing tokenization and representation learning methods often struggle to disentangle these variations, resulting in suboptimal feature extraction and performance.
    
%     \item \textbf{Frequency dominance:} EEG signals are often dominated by low-frequency components, causing models to primarily focus on these features while overlooking critical high-frequency information. In the EEG context, this is analogous to capturing only Delta waveforms while neglecting other essential frequencies. This limitation can be observed in the reconstructed Fourier spectrum shown in Figure~\ref{fig:story_fig}, generated by the neural tokenizer in the LaBraM\cite{jiang2024large} approach, the current state-of-the-art method. Accurately capturing a diverse range of frequencies, including both low- and high-frequency dynamics, is non-trivial and remains a significant challenge.
    
%     \item \textbf{Scalability and task generalization:} EEG-related tasks vary significantly in channel configurations—for example, seizure detection typically utilizes 16 channels, while sleep studies often require only 1–2 channels. This variability limits the scalability and adaptability of such models across diverse EEG tasks and acquisition configurations.

%     \item \textbf{Interpretability of representations:} While existing methods capture relevant information from EEG signals, the resulting learned tokens are often continuous, unbounded, and lack distinctiveness. Unlike the discrete, interpretable sub-word tokens used in NLP, these continuous representations are challenging to map to specific brain patterns, reducing their interpretability. 
% \end{itemize}



% To address the aforementioned challenges, we propose \method, a fully discretized tokenization framework that captures time-frequency motifs from single-channel EEG signals into distinct tokens. Directly processing raw EEG signals is suboptimal, as it fails to adequately represent key frequency-based features essential for many tasks. By tokenizing the signals, we reduce data complexity while improving the ability to capture meaningful features from both frequency and temporal axes. These tokens are then utilized to train an EEG foundation model. Our key contributions are summarized as follows:

\begin{itemize}[left=0pt]
    \item \textbf{\tokenizer and \encoder:} We introduce a scalable discrete tokenization framework for EEG, transforming single-channel signals into discrete token sequences akin to NLP models. TFM-Tokenizer converts EEG into discrete tokens, and \encoder utilizes them for downstream tasks.
    
    % We propose a scalable and discrete tokenization framework for EEG that captures time-frequency motifs from single-channel EEG signals, enabling symbolic tokenization akin to NLP models. The \tokenizer converts single-channel EEG signals into discrete token sequences. Using the learned tokens as input, we pretrain \encoder with a masked token prediction strategy, which is then fine-tuned for downstream tasks.

    \item \textbf{Joint Modeling of Frequency and Temporal Dynamics:} Our tokenizer integrates raw EEG patches with time-frequency representations, using frequency band and temporal masking to capture essential frequency patterns while disentangling temporal variations (Figure~\ref{fig:story_fig}). 
    % This mitigates the effect of frequency dominance and complex temporal variations. 
    
    % \item \textbf{Joint Modeling of Frequency and Temporal Dynamics: } By integrating raw EEG patches with their time-frequency representations, combined with frequency band and temporal masking strategy, our tokenizer effectively captures essential frequency information while disentangling complex temporal variations as shown in Figure~\ref{fig:story_fig}. This approach reduces the effect of frequency dominance and complex temporal variations. 

    \item \textbf{Scable tokenization:} Our single-channel approach enables flexible adaptation across EEG tasks and channel configurations. \tokenizer further enhances state-of-the-art EEG models, such as LaBraM \cite{jiang2024large}.

    % \item \textbf{Scable tokenization:} Our single-channel tokenization approach ensures scalability across diverse EEG tasks, enabling flexible adaptation to various channel configurations. The \tokenizer further demonstrates its scalability by enhancing existing state-of-the-art EEG foundation models such as LaBraM\cite{jiang2024large}

    \item \textbf{Empirical Validation and Token Quality Analysis:} We evaluate our framework on four EEG downstream tasks, demonstrating state-of-the-art performance. Beyond performance, we comprehensively analyze token quality, including token visualization, class-specific uniqueness, and frequency learning analysis, validating that our learned tokens are informative and interpretable.
    
    % \item \textbf{Effective capture of frequency band and temporal dynamics:} 
    % By jointly modeling frequency and temporal domains, from raw EEG patches and time-frequency representation of EEG signal, combined with masked prediction strategy, our tokenizer effectively captures essential frequency information while disentangling complex temporal variations as shown in Figure~\ref{fig:story_fig}. This approach reduces the effect of frequency dominance and complex temporal variations. 
    
    % By employing a combination of frequency band masking and temporal masking, the tokenizer effectively captures essential frequency information while disentangling complex temporal variations as shown in Figure~\ref{fig:story_fig}. This approach reduces the effect of frequency dominance and complex temporal variations. 
    
    % \item \textbf{Scable tokenization:} Our focus on single-channel tokenization ensures scalability across diverse tasks, accommodating various channel configurations without compromising performance.  
    
    
    
    % \item \textbf{Empirical Performance and Analysis:} The proposed framework is evaluated on seven EEG-related downstream tasks, comparing its performance with prior methods. Additionally, a detailed analysis is conducted to assess the quality of the learned tokens and their ability to capture time-frequency motifs.
\end{itemize}
 







% \chen{

% Paragraph-1: Background of EEGs, their characteristics, various waveforms, and the importance of capturing these waveforms.


% Electroencephalograms (EEG), as a staple brain imaging tool, reveal the activities of millions of neurons in real-time with millisecond precision, reflecting various body states or event stimuli.
% This makes EEG essential not only for fundamental research into pathophysiology, xxxx,  but also for practical applications, xxxx.
% Practical EEG analyses focus on capturing representative neural oscillations, termed waveforms, characterized by varying frequency and amplitude, which correspond to specific brain electrical activities.
% However, these waveforms are transient, noisy, and often label scarce, making them challenging to interpret and learn effectively. 
% As a result, xxxxxx.
% }




% \chen{
% Paragraph-2:
% Deep learning (DL) has proven to be a powerful method, and various DL-based approaches have demonstrated impressive achievements.
% In particular, the recent paradigm of large language models (LLMs) has elevated EEG representation learning to new heights. This is primarily due to the label-free nature of pre-training, which enables models to leverage vast unlabeled datasets, xxx, and xxx (talk about benefits of the LLM paradigm). These advantages make the LLM paradigm highly beneficial for time series and EEG learning.
% Several EEG-focused works have been proposed, with some following contrastive learning approaches, while others employ mask prediction strategies, inspired by time-series foundation models.
% However, several limitations still exist:
% 1, 2, 3 (could use itemizes)

% It would be better write the limitations from different perspective, for example: 

% [Complexity of Temporal Variations]

% A series of time observations is typically considered a complex set of signals or waves that varies over time. 
% Various temporal variations, manifested as different frequency waves, such as low-frequency long-term periodicity or high-frequency fluctuation, often co-occur and are intermixed in the real world. 
% While tokenizing a time series may provide fine-grained information for the model, the temporal variations in resulting tokens or sub-series are also entangled.
% This issue may complicate the feature extraction and forecasting performance.
% Provide a Story Figure to show this.

% [Frequency Dominance:]

% In many signals, low-frequency components tend to dominate. If models primarily capture these low-frequency components, they may overlook critical high-frequency information. 
% For example, focusing solely on low-frequency components is akin to capturing only ``Delta waveforms", while neglecting others. 
% Capturing a diverse range of frequencies is non-trivial and remains a significant challenge.



% Paragraph-3:
% Describe our methods and list our contributions.

% }

















%  1. Cross channel dependices only is sub optimal for foundation model. Because some EEG tasks requires different channel settings.
% 2. capturing intrinsic frequency information is important, where different frequency bands in EEG represents different waveforms such as (alpha, etc). Addition to this temporal raw signal is also important as since there are non-stationary, there will be time invariant EEG patterns in the signals. This shows the importance of capturing patterns from raw EEG signals.
% results story: Saw why frequency band information is important, through ablation on different freq masking methods. 
%  importance of raw signal along with the frequency information. % ablation on input
% show potential application results on ECG.


%% Simplify the fig 1 to show the difference. with the previous works.
% Differences between LaBRAM and us:
% Input, they are not really utilizing the tokens, frequency band learning.

% use the reference from park to show the importance of fourier features in the introduction and maybe for some theoretical analysis.
