% \subsection{EEG Representation Learning}
% \askFillIn{
%     We can first briefly introduce how different deep learning models can learn meaningful features from EEGs, such as 1D-CNNs for capturing temporal-invariant features and sequential models for modeling long-term dependencies.
%     Next, we can introduce general EEG representation learning approaches, categorized into contrastive learning methods (e.g., BIOT, SplitSEE, TS-TCC) and self-prediction methods.
%     }

\noindent\textbf{EEG Representation Learning. } 
% Deep learning has achieved remarkable success in EEG representation learning across multiple feature domains, using a variety of architectures. 
% These include 1D Convolutional Neural Networks (1D-CNNs)  \cite{jing2023development,dar2020cnn}, Recurrent Neural Networks (RNNs) \cite{xu2020one,jiang2024large}, Transformers \cite{Chen2023TNSRE,pradeepkumar2024towards}, Spiking Neural Networks \cite{chenIJCAI}, Graph models \cite{AAAI20_EvolveGCN,GNN_ICLR22} and Hybrids \cite{biswal2018expert}.
% . 1D Convolutional Neural Networks (1D-CNNs) excel at capturing temporal-invariant features from raw EEG signals \cite{jing2023development,dar2020cnn}, while sequential models like Recurrent Neural Networks (RNNs) and Transformers effectively model long-term dependencies in EEG data \cite{xu2020one,yang2024biot,jiang2024large}. Hybrid architectures combining these approaches have further enhanced EEG representation learning \cite{biswal2018expert}.
To learn general representations and address issues of label scarcity in EEG data, self-supervised learning (SSL) has emerged as a prominent paradigm, and existing works can be categorized into two main approaches: contrastive learning and self-prediction.
% From a training perspective, self-supervised learning (SSL) has emerged as a prominent paradigm for EEG representation learning, addressing the challenge of label scarcity through two primary approaches: contrastive learning and self-prediction. 
Contrastive learning methods, including TS-TCC \cite{eldele2021time}, TF-C \cite{zhang2022self} and SplitSEE \cite{kotoge2024splitsee}, leverage augmentation or transformation of EEG inputs to learn consistent representations. 
% TS-TCC focuses on temporal representation learning using strong and weak augmentations\cite{eldele2021time}. 
% TF-C extends this by separately learning temporal and frequency representations and enforcing consistency between them\cite{zhang2022self}. 
% In contrast, SplitSEE operates in a single-channel setting, adapting deep clustering for SSL to capture temporal-frequency representations\cite{kotoge2024splitsee}. 
In contrast, self-prediction methods, namely BENDR \cite{kostas2021bendr}, MAEEG \cite{chien2022maeeg}, BIOT \cite{yang2024biot} and EEG2REP \cite{mohammadi2024eeg2rep},  aim to accurately reconstruct masked or corrupted input.
% Namely, BENDR\cite{kostas2021bendr} and MAEEG\cite{chien2022maeeg} employ a masked autoencoder to predict masked regions in multi-channel EEG signals in the signal space. 
% Moreover, BIOT\cite{yang2024biot} and EEG2REP\cite{mohammadi2024eeg2rep} apply masked modeling in the feature space. 
% LaBraM\cite{jiang2024large} pretrains its encoder using masked token prediction given raw signal input, where a separate neural tokenizer generates soft token labels. LaBraM\cite{jiang2024large} pretrains its encoder using masked prediction, with token IDs for raw EEG inputs generated by a separate neural tokenizer, which is not part of the final inference model. However, these self-prediction models overlook frequency features. 
% In contrast, our \tokenizer employs frequency band and temporal masking to capture temporal and frequency features in tokens.
However, their learning objectives heavily rely on cross-channel prediction to focus on spatial characteristics.
In contrast, our method emphasizes inherent time-frequency features within a single-channel setting and can adapt to any channel configuration.




% 1D Convolutional Neural Networks (1D-CNNs) are commonly applied directly to raw EEG signals to extract temporal-invariant features \cite{jing2023development,dar2020cnn}. Similarly, 2D-CNNs are utilized on spectrogram representations of EEG to learn frequency domain features \cite{cui2020eeg}. Meanwhile, sequential models such as Recurrent Neural Networks (RNNs) and Transformers are well-suited for capturing long-term dependencies, making them effective for modeling complex temporal structures in EEG data \cite{yang2024biot,jiang2024large}. Additionally, hybrid architectures that combine CNNs with sequential models have been explored to enhance EEG representation learning\cite{biswal2018expert,jing2023development}. 


   
\noindent\textbf{Foundation EEG Models. }
Inspired by the success of foundation models in NLP, recent efforts have sought to develop foundation models for EEG analysis. 
These models can be categorized into decoding and encoder-based methods. 
Decoding-only methods focus on generative tasks like EEG-to-text translation, with representative works including DeWave \cite{duan2023dewave}, EEG2Text \cite{liu2024eeg2text}, and E2T-PTR \cite{wang2024enhancing}.
In contrast, encoder-only methods concentrate on fundamental EEG classification tasks and representation learning. 
Notable models include LaBraM \cite{jiang2024large}, BIOT \cite{yang2024biot}, BRANT \cite{zhang2024brant}, and MMM \cite{yi2024learning}.
Our work aligns with this latter category, focusing on enhancing the representation quality to improve classification performance. 


\noindent\textbf{EEG Tokenization. }
Tokenization has been instrumental in NLP, where discrete subword units have proven to reduce data complexity and improve model performance and interoperability. 
Existing attempts for EEGs include patch-based continuous tokenization, such as BIOT \cite{yang2024biot} and BRANT \cite{zhang2024brant}, and vector quantization (VQ)-based methods like DeWave \cite{duan2023dewave}.
Patch-based methods do not involve encoding or quantization, leading to unbounded and continuous representations that lack distinctiveness and interpretability.
In contrast, VQ-based tokenizers, traditionally successful in tokenizing continuous images \cite{esser2020taming}, have recently been adapted for EEG by LaBraM \cite{jiang2024large},
However, its tokenizer is utilized only during training and not during inference. 
Conceptually, its primary role is to pre-train classification layers, rather than encoding inputs and reducing data complexity.
Here, our method is explicitly VQ-based, treating the codebook as a real tokenizer for EEG data.
Moreover, we enforce each token to capture time-frequency motifs\cite{xu2023rebar} in EEG inputs, ensuring a more structured and interpretable representation.



% Several attempts have been made in EEG tokenization, including patch-based continuous tokenization and vector quantization (VQ)-based methods. 
% Existing attempts include patch-based continuous tokenization, like BIOT \cite{yang2024biot} and BRANT \cite{zhang2024brant} and vector quantization (VQ)-based methods, like DeWave\cite{duan2023dewave}. 
% % BIOT \cite{yang2024biot} and BRANT \cite{zhang2024brant} employ patch-based continuous tokenization, mapping EEG signals to continuous embeddings without encoding or quantization. 
% The patch-based methods without encoding or quantization. , unfortunately, the resulting representations are unbounded and continuous, lacking the distinctiveness and interpretability of sub-word tokens commonly used in NLP. 
% % Furthermore, they are susceptible to high data complexity and the inherent noise of EEG signals. 
% VQ-based methods, on the other hand, have been explored primarily for decoding tasks, such as DeWave\cite{duan2023dewave}.
% Recently, LaBraM has expanded the VQ tokenizers to encoder-only methods \cite{jiang2024large}.
% However, the tokenizer is utilized only during training and not during inference. 
% Conceptually, its primary role is to pre-train classification layers, rather than encoding EEGs and reducing data complexity.
% Here, our method is explicitly VQ-based, treating the codebook as a real tokenizer for EEG data. 
% A key distinction is that we enforce each token to capture time-frequency motifs in EEG inputs, ensuring a more structured and interpretable representation.

%LaBraM takes a hybrid approach, using a VQ-based neural tokenizer during training but not during inference. 
%In contrast, our method is explicitly VQ-based, treating the codebook as a real tokenizer for EEG segments. A key distinction is that we enforce each token to capture time-frequency motifs in the segments, ensuring a more structured and interpretable EEG representation.

% Given the success of foundation models (LLMs) in NLP, there have been several efforts to develop foundation models for EEG analysis. These include LaBraM\cite{jiang2024large}, BIOT\cite{yang2024biot}, BRANT\cite{zhang2024brant},MMM\cite{yi2024learning} and DeWave\cite{duan2023dewave}. These models can be broadly categorized in to decoding methods (e.g., EEG-to-text approaches) and encoder methods. LaBraM, BIOT,MMM and BRANT models belongs to the encoder methods and DeWave, \textcolor{blue}{add} belongs to decoding methods. Our approach can be classified as a encoder based methods.

% Tokenization has been one of the key factors to success in NLP, where each sub-words are represented as discrete tokens. There have been several efforts in EEG tokenizations and these include patch-based continuous tokenization approaches and vector quantization (VQ) based methods. BIOT and BRANT can be considered as patch-based continuous tokenization approach, as they map EEG signals to continuous embedding spaces without applying quantization.  While these approaches capture relevant features, the resulting representations are unbounded and continuous, lacking the distinctiveness and interpretability of sub-word tokens commonly used in NLP. Furthermore, they are prone to challenges such as high data complexity and the inherent noise of EEG signals. on the other hand vector quantization has been studies for decoding tasks such as EEG-to-text.  LaBraM can be considered a hybrid approach as they utilize a VQ based neural tokenizer to train their final model but not utilized during inference. Here, we position our method as VQ-based, but the key distinction with the prior works is that we treat the codebook as a real tokenizer of EEG segments, where we enforce each distinct token to capture time-frequency features. 


    
\noindent\textbf{Frequency Representation Collapse. }
Frequency domain analysis is crucial in EEG and general time series analysis \cite{almost_harmonics_2020, Autoformer, Timesnet, Etsformer}. 
In real-world signals, time-domain observations inherently mix multiple frequency components, and high-energy, low-frequency signals often dominate the spectrum \cite{time_frequency_decomposition_1998, long_and_short_SiGIR18}. 
As a result, these entangled frequency features makes it difficult for models to distinguish between them \cite{ICMLFedformer, Piao2024fredformer}.
%when raw time-domain signals are fed into models, the frequency components become entangled, making it difficult for the model to distinguish between them.
%This inherent imbalance makes it challenging for models to capture the full spectrum of frequency information from raw signals, leading them to overemphasize dominant high-energy frequencies while neglecting the lower-energy yet critical signals, known as frequency learning bias \cite{Zhi_Qin_John_Xu_2020_frequencyprinciple, Piao2024fredformer}.
%This imbalance can cause models to focus too much on these dominant low frequencies while ignoring the high-frequency details that are crucial for capturing fine-grained information \cite{Zhi_Qin_John_Xu_2020_frequencyprinciple, Piao2024fredformer}.
Recent studies have shown that these entangled signals can lead to a collapse in the learned frequency representations \cite{Zhi_Qin_John_Xu_2020_frequencyprinciple, Piao2024fredformer}. 
Models tend to overemphasize the dominant low-frequency features while neglecting the high-frequency details.
This issue can lead to a lack of capturing various EEG waveforms and degenerating data representation \cite{howTransWork_2022_ICLR}.
%Similar trends have been observed in Transformer architectures for NLP and computer vision, with deeper layers producing increasingly homogeneous token representations \cite{antioversmooth_2022_ICLR, howTransWork_2022_ICLR, NIPS2023scan}. 
%In contrast, our approach leverages frequency domain modeling—integrating frequency band and temporal masking—to explicitly guide the model in learning a richer and more balanced set of frequency features.
%, which is the primary focus of our tokenizer.
%producing increasingly similar token representations \cite{antioversmooth_2022_ICLR, howTransWork_2022_ICLR, NIPS2023scan}. 
%Repeated self-attention and feed-forward operations 
% Similar trends have been observed in Transformers for NLP and computer vision, with deeper layers gradually diminishing the fine-grained differences among token embeddings, resulting in a loss of discriminative power \cite{antioversmooth_2022_ICLR, NIPS2023scan, nguyen2023mitigating_oversmoothing, yin2024textgt_oversmoothing}. 
% This can be viewed as frequency learning bias, where the model overly focuses on high-energy signals while neglecting other frequency features \cite{howTransWork_2022_ICLR}.
%where deeper layers gradually diminish the fine-grained differences among token embeddings, resulting in restriction of expressiveness \cite{antioversmooth_2022_ICLR, NIPS2023scan, nguyen2023mitigating_oversmoothing, yin2024textgt_oversmoothing}. 
%Similar trends have also been observed in Transformers for NLP and computer vision, where the representations of tokens tend to become more similar in deeper layers \cite{antioversmooth_2022_ICLR, NIPS2023scan, nguyen2023mitigating_oversmoothing, yin2024textgt_oversmoothing}.
% A similar phenomenon is observed in various Transforme-based models in different studies \cite{antioversmooth_2022_ICLR, NIPS2023scan, nguyen2023mitigating_oversmoothing, yin2024textgt_oversmoothing}. 
% In these models, token representations gradually become more similar in deeper layers \cite{antioversmooth_2022_ICLR, NIPS2023scan, nguyen2023mitigating_oversmoothing, yin2024textgt_oversmoothing}. 
% This can be interpreted as a form of frequency bias, in which the model overly focuses on high-energy, low-frequency signals while neglecting the mid-to-high frequency components \cite{howTransWork_2022_ICLR, Piao2024fredformer}.
%This frequency bias occurs when the model emphasizes high-energy, low-frequency signals while neglecting mid-to-high frequency components. 
%This is because low-frequency signals capture common features across multiple tokens, whereas mid-to-high frequencies capture the fine-grained details that differentiate them \cite{howTransWork_2022_ICLR, Piao2024fredformer}. 
% This happens because models only focus on the low-frequency information that captures the common features shared among tokens, while neglecting the high-frequency information which holds the subtle differences that distinguish tokens \cite{howTransWork_2022_ICLR, Piao2024fredformer}.
%As a result, overreliance on low-frequency information leads to homogenized token representations with diminished discriminative power \cite{howTransWork_2022_ICLR, Piao2024fredformer}.
%Essentially, just as oversmoothing causes a collapse in feature diversity across layers, frequency bias 
%leading to an imbalanced representation of the underlying frequency characteristics of the data. 
%By incorporating frequency domain modeling techniques, such as frequency band masking into the tokenization, the model towards learning a richer and more balanced frequency features.
Motivated by these works, our paper focuses on developing methods to learn diverse, informative frequency features. 
In Section \ref{sec:freq_learning}, we provide an analysis of our proposed frequency-domain tokenizer and its impact on model performance.

\begin{comment}
    

----------
----------
\vspace{20pt}

Research in EEG representation learning and tokenization can be broadly categorized into two main approaches: non-discretized tokenization and hybrid tokenization. Non-discretized tokenization methods map EEG signals to continuous embedding spaces without applying quantization.  While these approaches capture relevant features, the resulting representations are unbounded and continuous, lacking the distinctiveness and interpretability of sub-word tokens commonly used in NLP. Furthermore, they are prone to challenges such as high data complexity and the inherent noise of EEG signals. Examples of such methods include BIOT\cite{yang2024biot}, EEG2REP\cite{mohammadi2024eeg2rep}, MMM\cite{yi2024learning}, and BRANT\cite{zhang2024brant}. BIOT\cite{yang2024biot} introduces a flexible biosignal encoder architecture, employing unsupervised pretraining on two large EEG datasets using masked prediction tasks. This is followed by supervised pretraining on four labeled EEG datasets and fine-tuning for specific downstream tasks. EEG2REP\cite{mohammadi2024eeg2rep} improves masked EEG reconstruction by shifting the task from raw signal space to a latent space, addressing noise and subject variability challenges in raw EEG signals. MMM\cite{yi2024learning} addresses varying channel configurations and spatial structures with a unified channel topology, multi-dimensional position encoding, multi-level channel hierarchy, and a multi-stage pretraining strategy for emotion recognition. Brant\cite{zhang2024brant} is a foundation model for intracranial recordings, pre-trained on a large dataset to learn long-term temporal dependencies and spatial correlations from neural signals in both time and frequency domains. 

Hybrid tokenization methods combine learned discrete tokens with raw signals in their workflows but do not rely on tokenized representations for downstream tasks.  An example is LaBRAM \cite{jiang2024large}, which adopts a two-stage process. In the first stage, LaBRAM trains a neural tokenizer using multi-channel raw EEG signals, predicting the Fourier spectrum's magnitude and phase. However, these tokens often fail to capture essential frequency and temporal variations, focusing instead on higher-power frequencies, which limits their effectiveness for EEG tasks as depicted in Figure~\ref{fig:story_fig}. The learned tokens are used only as masked labels in a pretraining task, where their classifier predicts the tokens corresponding to masked raw EEG segments. However, the final classifier relies on raw EEG signals for downstream fine-tuning. Relying solely on raw signals for frequency feature extraction is suboptimal due to the inherent noise and complexity of EEG signals.


To the best of our knowledge, no prior work has introduced a fully discretized tokenization framework for EEG signals akin to foundation models in NLP like BERT \cite{devlin2018bert} or GPT \cite{achiam2023gpt}. Such a tokenization approach offers several advantages over raw signal processing: (1) reduced data complexity, (2) improved noise resilience, as abstract features in the latent space are less affected by noise \cite{mohammadi2024eeg2rep}, and (3) enhanced interpretability through discrete, symbolic representations. Moreover, existing methods predominantly focus on multi-channel EEG\cite{yang2024biot,jiang2024large,zhang2024brant} and rely heavily on cross-channel dependencies. In contrast, our approach emphasizes learning time-frequency motifs from single-channel EEG, making it inherently scalable and adaptable to various downstream EEG settings and channel configurations.



\end{comment}