\section{Additional Results}

\subsection{Performance on CHB-MIT and IIIC Seizure}
\label{app:chbmit_iiic_comparison}

Table~\ref{tab:seizure_detection_CHBMIT} and \ref{tab:eeg_classification_IIIC} presents the performance comparison of \method with baselines on seizure detection (CHB-MIT) and seizure type classification (IIIC Seizure) tasks. \method outperforms all baselines across all metrics in both datasets. On the CHB-MIT dataset with a highly imbalanced binary classification task, BIOT is the only baseline with an AUC-PR above $0.25$.  However, \method surpasses BIOT, achieving an $8\%$ improvement in AUC-PR ($0.3127 \rightarrow 0.3379$) and a $4.5\%$ increase in AUROC ($0.8456 \rightarrow 0.8839$), demonstrating better robustness to class imbalance. For the IIIC Seizure dataset, where the task is to classify 10-second, 16-channel EEG segments into six classes, \method improves Cohenâ€™s Kappa by $9.5\%$ ($0.4549 \rightarrow 0.4985$) and Weighted F1 by $8.5\%$ ($0.5387\rightarrow 0.5847$) over ContraWR, which achieves second best results. 

The superior performance of \method across four EEG datasets shows the promise of a fully discretized framework that has the potential to enhance future EEG foundation models. These results also underscore the importance of capturing both temporal and frequency information, highlighting the critical role of frequency learning in EEG analysis.


% Table\ref{tab:seizure_detection_CHBMIT} and \ref{tab:eeg_classification_IIIC} present the performance comparison of our \method with baselines on seizure detection (CHB-MIT) and seizure type classification (IIIC Seizure) tasks. Our \method outperforms all baselines in all metrics in both datasets. CHB-MIT dataset posses a challenging binary classification task mainly due to its imbalanced settings. BIOT is the only baseline to achieves better performance in CHB-MIT datasets, where as all other baselines' AUC-PR scores are less than $0.25$. However, \method outperforms BIOT and achieves $8\%$ improvement over  BIOT in AUC-PR($0.3127 \rightarrow 0.3379$) and $4.5\%$ improvement in AUROC($0.8456 \rightarrow 0.8839$). This shows that our model is more robust to imbalance data compared to the baselines. 

% On the IIIC seizure dataset, the task is to classify each $10$ second and $16$ channel EEG segments into one of six classes. \method achieves $9.5\%$ increase in Cohen's Kappa($0.4549 \rightarrow 0.4985$) and $8.5\%$ increase in Weighted F1 ($0.5387 \rightarrow 0.5847$) compared to ContraWR, which achieves the second best performance on this dataset. Our \method's consistent superiors performance compared to other baselines on four EEG datasets suggests that fully discretization based framework would enable building better EEG foundation models in future. Additionaly these results highlight the importance of capturing both temporal and frequency information, where research on frequency learning is extremely important for EEG analysis. 

\input{TABLES/TUAB_CHBMIT_results}

\subsection{Effect of Masked Token Prediction in EEG Tokenization}
\label{app:masked_token_prediction_ablation}
% \input{TABLES/masked_token_prediction_ablation}

We conducted an ablation study on \encoder to assess the impact of masked token prediction pretraining in a fully discretized framework. Using a pretrained \tokenizer, we compared two approaches: (1) masked token prediction pretraining followed by fine-tuning and (2) direct fine-tuning without pretraining. This experiment was performed on the TUEV dataset across all three \tokenizer variants, with results summarized in Figure~\ref{fig:MTP_ablation}. While Cohen's Kappa and Weighted F1 showed no significant differences between the two approaches, masked token prediction pretraining significantly improved balanced accuracy across all \tokenizer variants. This suggests that pretraining enhances class-wise prediction consistency by capturing token dependencies and making \encoder more robust to missing channels or time segments, a common challenge in EEG analysis.
 
\begin{figure}[h]
    \centering
    % \rule{0.8\linewidth}{0.5\linewidth} 
    % \includegraphics[width=\linewidth]{Figures/Story_Overview_Fig.pdf}
    \includegraphics[width=\linewidth]{FIG/MTP_ablation.png}
    % \includegraphics[width=\linewidth]{Figures/retrieval_test.pdf}
    \caption{Masked Token Prediction Ablation}
    \label{fig:MTP_ablation}
\end{figure}


\subsection{Removing Position Embedding in \tokenizer Improves Token Learning}
\label{app:with_wo_pe_ablation}
\input{TABLES/with_wo_pe_ablation}

Through our empirical analysis, we found that the performance of our \method significantly improved when no position embedding was applied to the \tokenizer. EEG patterns are inherently chaotic and non-stationary, meaning similar motifs can occur at any position within the signal. An ideal tokenizer should be capable of capturing and representing such EEG motifs as distinct tokens without relying on positional information. 

We conducted an ablation study comparing the \tokenizer's performance with and without position embeddings to critically analyze this phenomenon. The results of this analysis, presented in Table~\ref{tab:with_wo_PE}, clearly show that the \tokenizer without position embedding achieves significantly better performance, with an increase of $4\%$ in Cohen's Kappa ($0.5119 \rightarrow 0.5337$).

We further studied the quality of the learned tokens in terms of token utilization and class-uniqueness scores. Token utilization decreased ($12.87\% \rightarrow 9.78\%$) when position embeddings were removed, while the class-token uniqueness score increased ($1.94\% \rightarrow 2.14\%$). This suggests that the \tokenizer, when using positional encoding, learns different tokens for the same motifs depending on their location in the signal, leading to redundancy. Removing the position embedding allows the \tokenizer to learn more compact and meaningful tokens without introducing unnecessary data complexities. This improvement is further illustrated in the motifs captured by the \tokenizer's tokens in Figure~\ref{fig:interpret_TUEV_1} in Section~\ref{sec:Q6}.




\subsection{Ablation on Token Vocabulary Size}
To evaluate the impact of token vocabulary size on performance and token learning, we conducted an ablation study by varying the vocabulary size from 256 to 8192 in powers of two. As shown in Figure~\ref{fig:codebook_ablation_metrics}, no monotonic trend was observed for Cohen's Kappa and Weighted F1 scores. However, balanced accuracy increased with larger vocabulary sizes. Further analysis of token utilization and class-token uniqueness scores is presented in Figure~\ref{fig:codebook_ablation_util_unique}. Notably, Figure~\ref{fig:codebook_ablation_util_unique}b shows that class-token uniqueness scores increase with vocabulary size, contributing to the improvement in balanced accuracy by enabling learning more unique class-specific tokens.
 


\begin{figure}[thpb]
    \centering
    % \rule{0.8\linewidth}{0.5\linewidth} 
    % \includegraphics[width=\linewidth]{Figures/Story_Overview_Fig.pdf}
    \includegraphics[width=\linewidth, trim=20 0 20 0, clip]{FIG/code_book_size_ablation.pdf}
    % \includegraphics[width=\linewidth]{Figures/retrieval_test.pdf}
    \caption{Token vocabulary size ablation with performance metrics}
    \label{fig:codebook_ablation_metrics}
    \vspace{-0.4cm}
\end{figure}

\begin{figure}[thpb]
    \centering
    % \rule{0.8\linewidth}{0.5\linewidth} 
    % \includegraphics[width=\linewidth]{Figures/Story_Overview_Fig.pdf}
    \includegraphics[width=\linewidth, trim=20 0 20 0, clip]{FIG/util_unique_code_book_size_ablation.pdf}
    % \includegraphics[width=\linewidth]{Figures/retrieval_test.pdf}
    \caption{Token vocabulary size ablation with token utilization and uniqueness}
    \label{fig:codebook_ablation_util_unique}
    \vspace{-0.4cm}
\end{figure}


% \subsection{Ablation on \tokenizer Modules}
\newpage
\subsection{Ablation on Masking}
\input{TABLES/masking_ablation}
We conducted an ablation study on masking strategies during \tokenizer pretraining to assess their impact on performance. Results shown in Table~\ref{tab:masking_ablation} indicate that random masking on the spectrogram $S$ performs poorly compared to other strategies, underscoring the need for effective masking to capture frequency and temporal features from EEG. Frequency bin masking significantly improves performance over random masking, with an $8\%$ increase in Cohen's Kappa ($0.4772 \rightarrow 0.5193$) and a $7\%$ increase in balanced accuracy ($0.4351 \rightarrow 0.4673$), highlighting the importance of modeling frequency band dynamics. The addition of temporal masking further boosts balanced accuracy by $5\%$ ($0.4673 \rightarrow 0.4946$), underscoring the importance of joint temporal-frequency modeling. However, temporal masking results in a decline in Cohen's Kappa and Weighted F1, which is then resolved by introducing symmetric masking, achieving the overall best performance.




% \subsection{Additional Interpretability Results}





% \newpage
\section{\method Implementation and Hyperparameter Tuning}
\label{app:tfmtoken_hyperparams}

\begin{figure}[thpb]
    \centering
    % \rule{0.8\linewidth}{0.5\linewidth} 
    % \includegraphics[width=\linewidth]{Figures/Story_Overview_Fig.pdf}
    \includegraphics[width=\linewidth]{FIG/model_overview_appendix.pdf}
    % \includegraphics[width=\linewidth]{Figures/retrieval_test.pdf}
    \caption{\method Overview}
    \label{fig:model_overview_appendix}
    % \vspace{-0.4cm}
\end{figure}

Figure~\ref{fig:model_overview_appendix} presents an overview of \method during inference. This section provides additional details on the implementation and training of the framework.

\subsection{Training Pipeline:} For all experiments, we follow a single-dataset setting, where all processes in each experiment are conducted within the same dataset. The training process of our framework is as follows: (1) \tokenizer unsupervised pretraining, (2) \encoder pretraining using masked token prediction, and finally (3) fine-tuning on the same dataset for downstream tasks.

\subsection{Hyperparameter Tuning of \method}
We employed a systematic approach to optimize the hyperparameters of both the \tokenizer and \encoder models using Ray Tune\footnote{https://docs.ray.io/en/latest/tune/} with the Optuna\footnote{https://optuna.org/}  search algorithm. Our optimization process followed a three-phase strategy. 

In the first phase, we optimized the \tokenizer architecture by tuning the depth and number of attention heads in the frequency transformer, temporal transformer, and transformer decoder modules to minimize the masked reconstruction loss $\mathcal{L}_{recon}$. This was followed by tuning the training optimizer's parameters, including learning rate and weight decay. The second phase focused on the \encoder optimization for the classification task, where we first tuned its architectural parameters (depth and number of heads), followed by training the optimizer's parameters while keeping the tokenizer frozen. The third phase focused on tuning optimizer parameters for the masked token prediction pretraining of the \encoder.

To ensure a fair comparison with LaBraM's neural tokenizer, we maintained a vocabulary size of $8,192$ and an embedding dimension of $64$. For our ablation studies involving raw signal-only and STFT-only variants, we doubled the embedding dimensions of the temporal encoder and frequency patch encoder to match the codebook dimension while maintaining all other parameters same. Detailed hyperparameter configurations for both \tokenizer and \encoder are provided in Appendices~\ref{app:tfmtokenizer_hyperparams} and \ref{app:encoder_hyperparams}, respectively.

% \newpage

\subsection{\tokenizer Hyperparameters}
\label{app:tfmtokenizer_hyperparams}
\input{TABLES/tokenizer_hyperparams}

\newpage
\subsection{\encoder Hyperparameters}
\label{app:encoder_hyperparams}
\input{TABLES/tfm_encoder_hyperparams}





\section{Additional Experiment Details}
\label{app:experiment_details}
\subsection{Dataset Statistics and Splits}
\label{app:dataset_splits}
\input{TABLES/dataset_statistics}
This section provides detailed information on the datasets used in our experiments and their respective splits. Table~\ref{tab:dataset_stats} summarizes key statistics, including the number of recordings, the total number of samples after preprocessing, their duration, and the corresponding downstream tasks. For TUEV and TUAB, we utilized the official training and test splits provided by the dataset and further divided the training splits into $80\%$ training and $20\%$ validation sets. We performed a subject-wise split into $60\%$ training, $20\%$ validation, and $20\%$ test on the IIIC Seizure dataset. In the CHB-MIT dataset, we used 1-19 subjects for training, 20-21 for validation, and 22-23 for testing.  




\subsection{STFT parameters}
\label{app:stft_params}
To extract frequency-domain representations of the EEG, we utilized the STFT function from PyTorch. The recommendations of \cite{yang2024biot} guided our parameter selection and empirical analysis of different configurations to optimize the time-frequency resolution tradeoff. The final parameters are as follows:

\begin{table}[thpb]
    \centering
    \caption{STFT parameters}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcc}
        \hline
        \textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
        \hline
        FFT size ($n_{\text{fft}},L$) & $200$  & Number of frequency bins (equal to resampling rate) \\
        Hop length $H$ & $100$ & Step size for sliding window ($50\%$ overlap) \\
        Window type & Hann & A smoothing window function to reduce spectral leakage \\
        Output representation & Magnitude & Only the absolute values of the STFT are retained \\
        Centering & False & The STFT is computed without implicit zero-padding \\
        One-sided output & True & Only the positive frequency components are kept \\
        \hline
    \end{tabular}}
    \label{tab:stft_params}
\end{table}



% \subsection{More Details on Datasets and Preprocessing}
% % loss function for pretraining fine-tuning chbmit

% \subsection{Environment Settings}






% \section{Related works: Categorization of Tokenization Approaches}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{FIG/simplified_related_works.png}
%     % \vspace{-.5cm}
%     \caption{EEG Tokenization Approaches - Can be moved to supplementary}
%     \label{fig:eeg_tokenization}
% \end{figure}

% \subsection{Symbolic Representation for Time-series}

% {\color{blue}I think this paper is not related to symbolic representation, it would be better to remove this subsection.
% But you can merge useful text to other subsections.}

% % Symbolic tokenization has long been explored to represent time-series data. One of the earliest approaches, Symbolic Aggregate approXimation (SAX) \cite{lin2003symbolic}, discretizes time-series data into symbolic representations, enabling dimensionality reduction and pattern recognition. SAX first transforms raw time-series data into a Piecewise Aggregate Approximation (PAA) representation and then converts it into discrete symbolic strings. This representation facillated indexing, clustering and classification related tasks in time-series datamining. With the right token representation for EEG, we can adapt such tasks in EEG domain but the effective method to capture and represent EEG signals into a distinct tokens remains a challenge. 

% Symbolic tokenization has long been explored for representing time-series data, with Symbolic Aggregate approXimation (SAX) \cite{lin2003symbolic} being one of the earliest approaches. SAX discretizes time-series data by first transforming it into a Piecewise Aggregate Approximation representation and then converting it into discrete symbolic strings. This method facilitates tasks like indexing, clustering, and classification in time-series data mining. Adapting such techniques to EEG could unlock similar benefits. However, effectively capturing and representing EEG signals as distinct, meaningful tokens remains a significant challenge.

% Building on the success of tokenization strategies in NLP, recent time-series foundation models, including Chronos \cite{ansari2024chronos}, Lag-Llama \cite{rasul2023lag},  have adopted tokenization as a core mechanism for their representation learning. Chronos, for instance, tokenizes time series into bins using mean scaling and uniform binning\cite{ansari2024chronos}, while Lag-Llama concatenates all past time points within a specific window into a single vector as tokens\cite{rasul2023lag}. While it's effective for simpler tasks, these methods do not scale well with high-frequency EEG signals. UniTS\cite{gao2024units} employs a complex tokenization strategy incorporating special tokens, including prompt tokens, sequence tokens, and task tokens. Despite the method's performance and generalizability, the effectiveness of this tokenization strategy in capturing complex temporal and frequency patterns in EEG signals remains an open question. 
