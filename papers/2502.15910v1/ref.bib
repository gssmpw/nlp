% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@article{liu2024protecting,
  title={Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench},
  author={Liu, Zheyuan and Dou, Guangyao and Jia, Mengzhao and Tan, Zhaoxuan and Zeng, Qingkai and Yuan, Yongle and Jiang, Meng},
  journal={arXiv preprint arXiv:2410.22108},
  year={2024}
}


@inproceedings{liu2024improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={CVPR},
  year={2024}
}

@article{laurenccon2024matters,
  title={What matters when building vision-language models?},
  author={Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Cord, Matthieu and Sanh, Victor},
  journal={arXiv preprint arXiv:2405.02246},
  year={2024}
}

@article{liu2024towards,
  title={Towards safer large language models through machine unlearning},
  author={Liu, Zheyuan and Dou, Guangyao and Tan, Zhaoxuan and Tian, Yijun and Jiang, Meng},
  journal={arXiv preprint arXiv:2402.10058},
  year={2024}
}

@inproceedings{thudi2022unrolling,
  title={Unrolling sgd: Understanding factors influencing machine unlearning},
  author={Thudi, Anvith and Deza, Gabriel and Chandrasekaran, Varun and Papernot, Nicolas},
  booktitle={EuroS\&P},
  year={2022},
}

@inproceedings{liu2022continual,
  title={Continual learning and private unlearning},
  author={Liu, Bo and Liu, Qiang and Stone, Peter},
  booktitle={CoLLAs},
  year={2022},
}

@article{nguyen2020variational,
  title={Variational bayesian unlearning},
  author={Nguyen, Quoc Phong and Low, Bryan Kian Hsiang and Jaillet, Patrick},
  journal={Neurips},
  year={2020}
}

@article{zhang2024negative,
  title={Negative preference optimization: From catastrophic collapse to effective unlearning},
  author={Zhang, Ruiqi and Lin, Licong and Bai, Yu and Mei, Song},
  journal={arXiv preprint arXiv:2404.05868},
  year={2024}
}

@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={CVPR},
  year={2024}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Neurips},
  year={2024}
}


@article{conmy2023towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Neurips},
  year={2023}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Neurips},
  year={2019}
}

@article{pochinkov2024dissecting,
  title={Dissecting Language Models: Machine Unlearning via Selective Pruning},
  author={Pochinkov, Nicholas and Schoots, Nandi},
  journal={arXiv preprint arXiv:2403.01267},
  year={2024}
}


@article{maini2024tofu,
  title={Tofu: A task of fictitious unlearning for llms},
  author={Maini, Pratyush and Feng, Zhili and Schwarzschild, Avi and Lipton, Zachary C and Kolter, J Zico},
  journal={arXiv preprint arXiv:2401.06121},
  year={2024}
}

@inproceedings{dang2021right,
  title={Right to be forgotten in the age of machine learning},
  author={Dang, Quang-Vinh},
  booktitle={Advances in Digital Science: ICADS 2021},
  year={2021},
}

@inproceedings{bourtoule2021machine,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  year={2021},
}

@article{ilharco2022editing,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal={arXiv preprint arXiv:2212.04089},
  year={2022}
}

@article{dou2024avoiding,
  title={Avoiding Copyright Infringement via Machine Unlearning},
  author={Dou, Guangyao and Liu, Zheyuan and Lyu, Qing and Ding, Kaize and Wong, Eric},
  journal={arXiv preprint arXiv:2406.10952},
  year={2024}
}

@article{yao2023large,
  title={Large language model unlearning},
  author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  journal={arXiv preprint arXiv:2310.10683},
  year={2023}
}

@article{liu2024machine,
  title={Machine unlearning in generative ai: A survey},
  author={Liu, Zheyuan and Dou, Guangyao and Tan, Zhaoxuan and Tian, Yijun and Jiang, Meng},
  journal={arXiv preprint arXiv:2407.20516},
  year={2024}
}

@article{liu2024shield,
  title={SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation},
  author={Liu, Xiaoze and Sun, Ting and Xu, Tianyang and Wu, Feijie and Wang, Cunxiang and Wang, Xiaoqian and Gao, Jing},
  journal={arXiv preprint arXiv:2406.12975},
  year={2024}
}

@article{zhang2023counterfactual,
  title={Counterfactual memorization in neural language models},
  author={Zhang, Chiyuan and Ippolito, Daphne and Lee, Katherine and Jagielski, Matthew and Tram{\`e}r, Florian and Carlini, Nicholas},
  journal={Neurips},
  year={2023}
}

@article{nasr2023scalable,
  title={Scalable extraction of training data from (production) language models},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023}
}

@article{wang2024large,
  title={Large Scale Knowledge Washing},
  author={Wang, Yu and Wu, Ruihan and He, Zexue and Chen, Xiusi and McAuley, Julian},
  journal={arXiv preprint arXiv:2405.16720},
  year={2024}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Neurips},
  year={2022}
}

@article{meng2022mass,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@article{alexey2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Alexey, Dosovitskiy},
  journal={arXiv preprint arXiv: 2010.11929},
  year={2020}
}

@article{ghiasi2022vision,
  title={What do vision transformers learn? a visual exploration},
  author={Ghiasi, Amin and Kazemi, Hamid and Borgnia, Eitan and Reich, Steven and Shu, Manli and Goldblum, Micah and Wilson, Andrew Gordon and Goldstein, Tom},
  journal={arXiv preprint arXiv:2212.06727},
  year={2022}
}

@article{zhang2021moefication,
  title={Moefication: Transformer feed-forward layers are mixtures of experts},
  author={Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  journal={arXiv preprint arXiv:2110.01786},
  year={2021}
}


@inproceedings{liu2023deja,
  title={Deja vu: Contextual sparsity for efficient llms at inference time},
  author={Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others},
  booktitle={ICML},
  year={2023},
}

@article{varley2023information,
  title={Information theory for complex systems scientists},
  author={Varley, Thomas F},
  journal={arXiv preprint arXiv:2304.12482},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Neurips},
  year={2020}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={JMLR},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{qin2023chatgpt,
  title={Is ChatGPT a general-purpose natural language processing task solver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  journal={arXiv preprint arXiv:2302.06476},
  year={2023}
}

@article{tan2024democratizing,
  title={Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning},
  author={Tan, Zhaoxuan and Zeng, Qingkai and Tian, Yijun and Liu, Zheyuan and Yin, Bing and Jiang, Meng},
  journal={arXiv preprint arXiv:2402.04401},
  year={2024}
}

@article{ye2023mplug,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}

@inproceedings{ye2024mplug,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Hu, Anwen and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei},
  booktitle={CVPR},
  year={2024}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{tan2024personalized,
  title={Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts},
  author={Tan, Zhaoxuan and Liu, Zheyuan and Jiang, Meng},
  journal={arXiv preprint arXiv:2406.10471},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Neurips},
  year={2022}
}


@article{nguyen2022survey,
  title={A survey of machine unlearning},
  author={Nguyen, Thanh Tam and Huynh, Thanh Trung and Ren, Zhao and Nguyen, Phi Le and Liew, Alan Wee-Chung and Yin, Hongzhi and Nguyen, Quoc Viet Hung},
  journal={arXiv preprint arXiv:2209.02299},
  year={2022}
}

@article{liu2024rethinking,
  title={Rethinking machine unlearning for large language models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Yao, Yuguang and Liu, Chris Yuhao and Xu, Xiaojun and Li, Hang and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}

@inproceedings{papantoniou2024arc2face,
  title={Arc2face: A foundation model for id-consistent human faces},
  author={Papantoniou, Foivos Paraperas and Lattas, Alexandros and Moschoglou, Stylianos and Deng, Jiankang and Kainz, Bernhard and Zafeiriou, Stefanos},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  year={2004}
}

@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}

@inproceedings{yu2024rlhf,
  title={Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},
  author={Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
  booktitle={CVPR},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Neurips},
  year={2023}
}

@article{duarte2024cop,
  title={De-cop: Detecting copyrighted content in language models training data},
  author={Duarte, Andr{\'e} V and Zhao, Xuandong and Oliveira, Arlindo L and Li, Lei},
  journal={arXiv preprint arXiv:2402.09910},
  year={2024}
}

@article{xie2017large,
  title={Large-scale cloze test dataset created by teachers},
  author={Xie, Qizhe and Lai, Guokun and Dai, Zihang and Hovy, Eduard},
  journal={arXiv preprint arXiv:1711.03225},
  year={2017}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={USENIX Security 21},
  year={2021}
}

@article{joshi2024towards,
  title={Towards Robust Evaluation of Unlearning in LLMs via Data Transformations},
  author={Joshi, Abhinav and Saha, Shaswati and Shukla, Divyaksh and Vema, Sriram and Jhamtani, Harsh and Gaur, Manas and Modi, Ashutosh},
  journal={arXiv preprint arXiv:2411.15477},
  year={2024}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Neurips},
  year={2024}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}


@article{huang2024commonsense,
  title={Commonsense Knowledge Editing Based on Free-Text in LLMs},
  author={Huang, Xiusheng and Wang, Yequan and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2410.23844},
  year={2024}
}

@article{ippolito2022preventing,
  title={Preventing verbatim memorization in language models gives a false sense of privacy},
  author={Ippolito, Daphne and Tram{\`e}r, Florian and Nasr, Milad and Zhang, Chiyuan and Jagielski, Matthew and Lee, Katherine and Choquette-Choo, Christopher A and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2210.17546},
  year={2022}
}

@article{lucki2024adversarial,
  title={An adversarial perspective on machine unlearning for ai safety},
  author={{\L}ucki, Jakub and Wei, Boyi and Huang, Yangsibo and Henderson, Peter and Tram{\`e}r, Florian and Rando, Javier},
  journal={arXiv preprint arXiv:2409.18025},
  year={2024}
}

@article{cooper2024machine,
  title={Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy, Research, and Practice},
  author={Cooper, A Feder and Choquette-Choo, Christopher A and Bogen, Miranda and Jagielski, Matthew and Filippova, Katja and Liu, Ken Ziyu and Chouldechova, Alexandra and Hayes, Jamie and Huang, Yangsibo and Mireshghallah, Niloofar and others},
  journal={arXiv preprint arXiv:2412.06966},
  year={2024}
}

@article{li2024wmdp,
  title={The wmdp benchmark: Measuring and reducing malicious use with unlearning},
  author={Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and others},
  journal={arXiv preprint arXiv:2403.03218},
  year={2024}
}


@article{ni2025towards,
  title={Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey},
  author={Ni, Bo and Liu, Zheyuan and Wang, Leyao and Lei, Yongjia and Zhao, Yuying and Cheng, Xueqi and Zeng, Qingkai and Dong, Luna and Xia, Yinglong and Kenthapadi, Krishnaram and others},
  journal={arXiv preprint arXiv:2502.06872},
  year={2025}
}

@inproceedings{zhang2025pretrained,
  title={Pretrained Image-Text Models are Secretly Video Captioners},
  author={Zhang, Chunhui and Jian, Yiren and Ouyang, Zhongyu and Vosoughi, Soroush},
  booktitle={Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2025},
}

@inproceedings{diao2024learning,
  title={Learning Musical Representations for Music Performance Question Answering},
  author={Diao, Xingjian and Zhang, Chunhui and Wu, Tingxuan and Cheng, Ming and Ouyang, Zhongyu and Wu, Weiyi and Gui, Jiang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  year={2024}
}

@article{zhang2024mopi,
  title={MOPI-HFRS: A Multi-objective Personalized Health-aware Food Recommendation System with LLM-enhanced Interpretation},
  author={Zhang, Zheyuan and Wang, Zehong and Ma, Tianyi and Taneja, Varun Sameer and Nelson, Sofia and Le, Nhi Ha Lan and Murugesan, Keerthiram and Ju, Mingxuan and Chawla, Nitesh V and Zhang, Chuxu and others},
  journal={arXiv preprint arXiv:2412.08847},
  year={2024}
}

@inproceedings{diao2025temporal,
  title={Temporal Working Memory: Query-Guided Segment Refinement for Enhanced Multimodal Understanding},
  author={Diao, Xingjian and Zhang, Chunhui and Wu, Weiyi and Ouyang, Zhongyu and Qing, Peijun and Cheng, Ming and Vosoughi, Soroush and Gui, Jiang},
  year={2025},
  booktitle={Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2025) Findings},
}


@article{fu2024amoeballm,
  title={AmoebaLLM: Constructing Any-Shape Large Language Models for Efficient and Instant Deployment},
  author={Fu, Yonggan and Yu, Zhongzhi and Li, Junwei and Qian, Jiayi and Zhang, Yongan and Yuan, Xiangchi and Shi, Dachuan and Yakunin, Roman and Lin, Yingyan Celine},
  journal={arXiv preprint arXiv:2411.10606},
  year={2024}
}