\section{Related Work}
\paragraph{MU for Generative Models.}
As LLMs and MLLMs memorize large amounts of sensitive knowledge during pre-training and fine-tuning, privacy concerns have grown with the rise of generative models ____. 
Machine Unlearning (MU) offers an efficient solution to selectively erase unwanted information while preserving overall model performance. ____ formalized unlearning objectives for LLMs, introducing a gradient-ascent-based approach to remove harmful knowledge. 
To address catastrophic forgetting, task vector-based approaches have been proposed ____. In response to the \textit{Right to Be Forgotten} ____, benchmarks like TOFU ____ and MLLMU-Bench ____ were developed using synthetic data, highlighting the need for privacy-preserving methods. However, existing unlearning algorithms are not explicitly designed for MLLMs to achieve comprehensive unlearning across modalities.

% To further advance unlearning in the context of the Right to Be Forgotten ____, works such as TOFU ____ and MLLMU-Bench ____ have presented benchmarks for unlearning in LLMs and MLLMs using synthetic data, emphasizing the need for privacy-preserving unlearning methods that remove sensitive information while maintaining model performance. Notably, MLLMU-Bench highlights that simply adapting unlearning methods from the textual modality does not yield optimal performance in the case of MLLMs. However, this phenomenon has not been sufficiently explored, leaving a gap in understanding the unique challenges associated with MLLM unlearning.

\paragraph{Model Pruning.}
Model pruning has proven to be an effective approach for removing redundant weights to enhance the performance and efficiency of a model. 
For example, ____ proposes a weight pruning-based technique to identify sub-circuits that contribute most to a specific dataset. Additionally, pruning can be used to preserve key model capabilities while reducing computational costs. 
For instance, ____ introduces a method to prune unused attention heads without impacting overall performance. 
____ shows that pruning can be used to unlearn specific behaviors of transformer models through a selective neuron approach. 
Additionally, it empirically demonstrates the effectiveness of neuron pruning over weight pruning. However, without a modality-specific pruning strategy, achieving thorough unlearning to remove target knowledge across different modalities remains challenging.