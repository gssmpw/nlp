\section{Related Work}
\paragraph{MU for Generative Models.}
As LLMs and MLLMs memorize large amounts of sensitive knowledge during pre-training and fine-tuning, privacy concerns have grown with the rise of generative models **Duchi, "Efficient Algorithms for Unlearning"**. 
Machine Unlearning (MU) offers an efficient solution to selectively erase unwanted information while preserving overall model performance. **Chang et al., "Gradient-Based Unlearning for Large-Scale Models"** formalized unlearning objectives for LLMs, introducing a gradient-ascent-based approach to remove harmful knowledge. 
To address catastrophic forgetting, task vector-based approaches have been proposed **Veness and Weber, "Catastrophic Forgetting in Neural Networks"**. In response to the \textit{Right to Be Forgotten} , benchmarks like TOFU  and MLLMU-Bench  were developed using synthetic data, highlighting the need for privacy-preserving methods. However, existing unlearning algorithms are not explicitly designed for MLLMs to achieve comprehensive unlearning across modalities.

% To further advance unlearning in the context of the Right to Be Forgotten , works such as TOFU  and MLLMU-Bench  have presented benchmarks for unlearning in LLMs and MLLMs using synthetic data, emphasizing the need for privacy-preserving unlearning methods that remove sensitive information while maintaining model performance. Notably, MLLMU-Bench highlights that simply adapting unlearning methods from the textual modality does not yield optimal performance in the case of MLLMs. However, this phenomenon has not been sufficiently explored, leaving a gap in understanding the unique challenges associated with MLLM unlearning.

\paragraph{Model Pruning.}
Model pruning has proven to be an effective approach for removing redundant weights to enhance the performance and efficiency of a model. 
For example, **Frankle and Carbin, "The Lottery Ticket Hypothesis"** proposes a weight pruning-based technique to identify sub-circuits that contribute most to a specific dataset. Additionally, pruning can be used to preserve key model capabilities while reducing computational costs. 
For instance,  introduces a method to prune unused attention heads without impacting overall performance. 
 shows that pruning can be used to unlearn specific behaviors of transformer models through a selective neuron approach. 
Additionally, it empirically demonstrates the effectiveness of neuron pruning over weight pruning. However, without a modality-specific pruning strategy, achieving thorough unlearning to remove target knowledge across different modalities remains challenging.