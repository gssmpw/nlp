\section{Related Work}
\subsection{LLM-as-RSs}

\subsubsection{LLM-as-Retriever}
LLM-as-Retriever leverages LLMs to recall a set of potentially relevant items from an entire item set based on a user's interaction history. To ensure that the retrieved items remain within item set, three key paradigms have been proposed: bi-step grounding, indexing, and modal alignment.
\textbf{Bi-Step Grounding}~\cite{lin2024bridging,gao2024sprec} retrieves items by measuring the similarity between the textual output of the LLM and the candidate set. A pioneering work in this area is BIGRec~\cite{bao2023bi}, which reformulates the recommendation task by first grounding the LLM’s output from the language space to the recommendation space, and subsequently aligning it with the actual item space.
\textbf{Indexing}~\cite{chen2024enhancing,li2024semantic} discretizes items into semantically meaningful tokens and employs beam search for retrieval. A notable example is LC-Rec~\cite{zheng2024adapting}, which aligns language tokens with item index tokens through task-specific optimizations, effectively bridging the gap between language representations and item indexing.
\textbf{Modal Alignment}~\cite{yu2024break,li2023e4srec,chen2024hllm,zhang2024recgpt} transforms the semantic vectors encoded by the Collaborative Retrieval Model (CRM) to align them with the semantic space of the LLM, replacing the traditional next-token prediction head with a next-item prediction head. This approach seamlessly integrates collaborative filtering information into the LLM, leading to significant improvements in retrieval performance.

\subsubsection{LLM-as-Ranker}
The LLM-as-Ranker paradigm~\cite{cao2024aligning,luo2024recranker} requires LLMs to either rank a set of candidates based on a user's interaction history (list-wise ranking) or predict the likelihood of user interaction with a specific item (point-wise ranking).
\textbf{Point-wise Ranking.} TALLRec~\cite{bao2023tallrec} represents a pioneering effort in this domain. Subsequent studies have introduced notable advancements, including integration with Click-Through Rate (CTR) models~\cite{lin2024clickprompt,lin2024rella}, optimization of user preference modeling~\cite{zheng2024harnessing}, and improvements in text-like encoding techniques~\cite{zhang2024text}.
\textbf{List-wise Ranking.} LLMRank~\cite{hou2024large} serves as a foundational work in this area. Building on this, later research has explored various extensions, such as interpretable cross-domain recommendation~\cite{petruzzelli2024instructing}, intent-driven session-based recommendation~\cite{sun2024large}, and comprehensive LLM-powered recommendation systems~\cite{kim2024large}. Additionally, researchers have sought to enhance ranking performance through fine-tuning strategies~\cite{yue2023llamarec,liao2024llara,chen2024softmax,liu2022parameter} and prompt optimization techniques~\cite{wang2024whole}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{4.jpg}
  \caption{User preference maintenance module.}
  \label{fig:8mke}
\end{figure}

\subsection{LLM-enhanced RSs}
LLM-enhanced recommender systems (RSs) leverage LLMs to enhance the capabilities of Collaborative Retrieval Models (CRM) during the training phase, while LLMs are not required during inference. Depending on the type of knowledge provided by the LLM, some studies utilize LLMs to construct or optimize graphs that encode structural knowledge for CRM~\cite{hu2024bridging, zhang2024finerec, sakurai2024llm, wang2024llmrg, yang2024sequential,liu2024filtering}. Others introduce interaction information into CRM by generating synthetic interactions~\cite{wang2024large, wei2024llmrec}. Additionally, certain works enhance CRM inputs by optimizing features~\cite{jia2024altfs, wang2024llms,liu2023triple} or generating textual content~\cite{zhang2024embsum, du2024enhancing, sun2024largecf, xi2024towards}. Furthermore, some approaches improve CRM’s ability to learn high-quality representations by leveraging embeddings~\cite{geng2024breaking, wang2024can, cui2024distillation, liu2024large, harte2023leveraging, zhang2024notellm, ren2024representation}.