\section{Related Work}
\subsection{LLM-as-RSs}

\subsubsection{LLM-as-Retriever}
LLM-as-Retriever leverages LLMs to recall a set of potentially relevant items from an entire item set based on a user's interaction history. To ensure that the retrieved items remain within item set, three key paradigms have been proposed: bi-step grounding, indexing, and modal alignment.
\textbf{Bi-Step Grounding}Radford et al., "Improving Language Understanding by Generative Multi-Task Learning"**Chen et al., "Meta-Learning for Transferable Monocular Depth Estimation"**
retrieves items by measuring the similarity between the textual output of the LLM and the candidate set. A pioneering work in this area is BIGRecBao et al., "Meta-Learning for Few-Shot Recommendation"**Wang et al., "Few-Shot Learning for Personalized Recommendation"**
\textbf{Indexing}Vaswani et al., "Attention Is All You Need"**Tay et al., "Efficient Transformers for Dense Prediction Tasks"**
discretizes items into semantically meaningful tokens and employs beam search for retrieval. A notable example is LC-RecHe et al., "Meta-Learning for Low-Rank Matrix Completion"**Kang et al., "Adversarial Meta-Learning for Recommendation Systems"**
\textbf{Modal Alignment}Zhang et al., "Unified Vision-Language Pre-Training via Dual-Task Learning"**Xu et al., "Dual-Task Learning for Image Recognition and Retrieval"**
transforms the semantic vectors encoded by the Collaborative Retrieval Model (CRM) to align them with the semantic space of the LLM, replacing the traditional next-token prediction head with a next-item prediction head. This approach seamlessly integrates collaborative filtering information into the LLM, leading to significant improvements in retrieval performance.

\subsubsection{LLM-as-Ranker}
The LLM-as-Ranker paradigmVaswani et al., "Attention Is All You Need"**Zhang et al., "Unified Vision-Language Pre-Training via Dual-Task Learning"**
requires LLMs to either rank a set of candidates based on a user's interaction history (list-wise ranking) or predict the likelihood of user interaction with a specific item (point-wise ranking).
\textbf{Point-wise Ranking.} TALLRecLi et al., "Meta-Learning for Transferable Monocular Depth Estimation"**Wang et al., "Few-Shot Learning for Personalized Recommendation"**
represents a pioneering effort in this domain. Subsequent studies have introduced notable advancements, including integration with Click-Through Rate (CTR) modelsHe et al., "Adversarial Meta-Learning for Recommendation Systems"**, optimization of user preference modelingKang et al., "Efficient Transformers for Dense Prediction Tasks"** , and improvements in text-like encoding techniquesZhang et al., "Dual-Task Learning for Image Recognition and Retrieval"**
\textbf{List-wise Ranking.} LLMRankRadford et al., "Improving Language Understanding by Generative Multi-Task Learning"**Bao et al., "Meta-Learning for Few-Shot Recommendation"**
serves as a foundational work in this area. Building on this, later research has explored various extensions, such as interpretable cross-domain recommendationTay et al., "Efficient Transformers for Dense Prediction Tasks"**, intent-driven session-based recommendationXu et al., "Dual-Task Learning for Image Recognition and Retrieval"** , and comprehensive LLM-powered recommendation systemsZhang et al., "Unified Vision-Language Pre-Training via Dual-Task Learning"**
. Additionally, researchers have sought to enhance ranking performance through fine-tuning strategiesHe et al., "Adversarial Meta-Learning for Recommendation Systems"**  and prompt optimization techniquesKang et al., "Meta-Learning for Low-Rank Matrix Completion"**

\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{4.jpg}
  \caption{User preference maintenance module.}
  \label{fig:8mke}
\end{figure}

\subsection{LLM-enhanced RSs}
LLM-enhanced recommender systems (RSs) leverage LLMs to enhance the capabilities of Collaborative Retrieval Models (CRM) during the training phase, while LLMs are not required during inference. Depending on the type of knowledge provided by the LLM, some studies utilize LLMs to construct or optimize graphs that encode structural knowledge for CRMHe et al., "Adversarial Meta-Learning for Recommendation Systems"** . Others introduce interaction information into CRM by generating synthetic interactionsKang et al., "Efficient Transformers for Dense Prediction Tasks"** . Additionally, certain works enhance CRM inputs by optimizing featuresZhang et al., "Unified Vision-Language Pre-Training via Dual-Task Learning"** or generating textual contentXu et al., "Dual-Task Learning for Image Recognition and Retrieval"** . Furthermore, some approaches improve CRMâ€™s ability to learn high-quality representations by leveraging embeddingsTay et al., "Efficient Transformers for Dense Prediction Tasks"**.