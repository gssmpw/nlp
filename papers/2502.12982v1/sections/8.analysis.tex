\section{Analysis}

This section presents our insights on building multilingual LLMs during both the continual pre-training and post-training stages. 
We also examine Sailor2's capabilities in translation and cultural understanding, two core components of practical multilingual applications.

\subsection{Effect of Model Expansion} 

For both Sailor and Sailor2, we adopt a continual pre-training (CPT) approach to efficiently develop multilingual LLMs by reusing computational resources. Unlike Sailor, Sailor2 incorporates model expansion, which creates additional capacity for learning new knowledge from the multilingual corpus.

We analyze the perplexity shift during CPT with and without model expansion, with detailed results shown in Table~\ref{tab:ppl_shift_sailor1_sailor2}. Experimental findings reveal that, compared to the Qwen1.5~→~Sailor transition, Qwen2.5~→~Sailor2 exhibits less degradation in English/Chinese and greater improvements in the target SEA languages during continual pre-training. Notably, even though Qwen2.5 is trained on 18T tokens versus Qwen1.5's 4T tokens, Sailor2 still achieves significant gains in SEA languages with only minor degradation in English.

We conduct a comprehensive examination across multiple languages in Figure~\ref{fig:ppl_shift}, which illustrates the PPL distribution shift for English, Chinese, and sixteen SEA languages. The results demonstrate that Sailor2 maintains its performance in English and Chinese while achieving significantly lower perplexity in SEA languages. Further discussions on future work in efficient CPT can be found in Section~\ref{sec:future_work_efficient_training}.

\input{tables/8_sailor_and_sailor2_ppl}



\subsection{Effect of Continual Pre-training}
Qwen2.5 models have already been trained on 18T tokens, meaning that many SEA tokens were likely seen during the pre-training stage. This raises the question of whether the expensive continual pre-training stage using an additional 400B SEA tokens is still necessary. To investigate, we conducted an ablation study with the following setup: post-training on both the vanilla Qwen2.5-7B base model and the Sailor2-8B base model, with the same post-training dataset and training steps. 
Detailed results are listed in Table~\ref{table:language_wise_reward}.
We could observe that CPT is essential, especially for low-resource languages like khm and lao.

\begin{table}[htbp]
\centering
\caption{Language-wise Score on SEA-WildBench between chat models trained using the Qwen and Sailor2 models. Note that Qwen2.5-7B-Chat is trained using the Sailor2 post-training pipeline.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Model} & \textbf{SWB Score} & \textbf{tha} & \textbf{vie} & \textbf{ind} & \textbf{tgl} & \textbf{zsm} & \textbf{khm} & \textbf{lao} & \textbf{mya} \\
\midrule
\midrule
Sailor2-8B-Chat & 0.43 & 0.44 & 0.40 & 0.40 & 0.39 & 0.36 & 0.43 & 0.56 & 0.44 \\
\midrule
Qwen2.5-7B-Chat (ours) & 0.25 & 0.31 & 0.34 & 0.33 & 0.21 & 0.31 & 0.11 & 0.21 & 0.08 \\
\bottomrule
\end{tabular}%
}
\label{table:language_wise_reward}
\end{table}



\subsection{Key Findings in Preference Data Construction}\label{sec.pdata}

We conduct a series of ablation studies to assess the impact of design choices in the preference data construction pipeline, as described in Sec.~\ref{sec.pt.data}.

\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sec8_3-on_off_policy.pdf}
        \caption{Length-controlled win rates comparison after different preference tuning stages on Sea-WB.}
        \label{fig:sec83_on_off}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sec8_3-lc.pdf}
        \caption{Effect of the language consistency verifier on the model performance. The results show a consistent performance gain across various experiment settings.}
        \label{fig:sec83_lc}
    \end{minipage}
\end{figure}

\paragraph{First Off-Policy, Then On-Policy Training Improves DPO Performance.}
While prior studies~\citep{guo2024direct,lambert2024t} have shown that on-policy training in DPO leads to greater performance improvements than off-policy training, our preliminary findings suggest that directly applying on-policy training to an SFT model yields limited gains. We hypothesize that this is due to the SFT model's insufficient ability to generate high-quality responses in chat tasks. To address this, we first perform off-policy training to initialize a stronger policy model before transitioning to on-policy training. In other words, our training pipeline consists of SFT, followed by off-policy DPO training, and then on-policy DPO training. As shown in Fig.~\ref{fig:sec83_on_off}, off-policy training on Sailor2-20B-SFT significantly enhances model performance, while subsequent on-policy training provides further improvements.

\paragraph{Language Consistency Verifier Improves Downstream DPO Performance.} 
Due to the lack of a reward model for SEA languages, we use Skywork-Reward-Gemma-2-27B~\citep{liu2024skywork} which is trained primarily on English data. To mitigate the RM's limitations in evaluating SEA language responses, we introduce a language consistency verifier. The verifier~\footnote{{We use \href{https://huggingface.co/facebook/fasttext-language-identification}{facebook/fasttext-language-identification} for language verification.}} labels a response as \texttt{true} if its language matches the prompt; otherwise, it is labeled \texttt{false}. If at least two responses are language-consistent, we select the winning and losing responses based on the RM's reward scores. If only one response is language-consistent, it is chosen as the winning response. If none are language-consistent, the prompt is discarded. 

We evaluate the verifier’s effectiveness under three settings: for the 8B model, we conduct on-policy training with $\beta\in\{0.1, 0.3\}$; for the 20B model, we use $\beta=0.1$. As shown in Fig.~\ref{fig:sec83_lc}, incorporating the verifier consistently improves performance compared to training without it. Moving forward, further improvements in preference tuning for low-resource languages could be explored, such as training an RM specifically for SEA languages and leveraging self-alignment techniques~\citep{chen2024self,chen2025bootstrapping,kim2025spread}.  

\begin{table}[h!]
\centering
\caption{Comparison of on-policy and distillation methods on Sailor2-8B after off-policy training. LC is short for the length-controlled win rate on Sea-WB.}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{LC} & \textbf{Avg. Length} \\
\midrule
On-policy & 0.49 & 2849 \\
Distillation & 0.48 & 2752 \\
\bottomrule
\end{tabular}
\label{tab.pdata.compare}
\end{table}

\paragraph{Distillation Reduces the Response Length while Maintaining Comparable Model Performance.}
We investigate the effectiveness of distillation within the same model family during DPO training given two practical considerations: (1) Distillation leverages the high-quality on-policy data used for training Sailor2-20B-Chat; (2) Reusing Sailor2-20B’s on-policy data significantly reduces the computational cost of data generation and reward evaluation. To assess its impact, we conduct a controlled experiment on our 8B model. Specifically, we conduct DPO with its own on-policy data and with the on-policy data from Sailor2-20B. Results in Table.~\ref{tab.pdata.compare} indicate that distillation maintains comparable model performance while reducing response length. 

\subsection{Cross-lingual Translation Ability of Sailor2}
\label{sec:analysis_translation}

We analyze the performance of Sailor2-20B on Flores Plus~\citep{nllb2022}, a translation dataset covering over 200 languages. Since our focus is primarily on SEA languages, we limit the scope to a subset of the dataset containing SEA languages, Chinese and English.
Table \ref{tab:flores_eng_xx_translation_performance} compares Sailor2 and three baseline models on English-centric translation pairs. 
Table \ref{tab:matrix-chrf-sailor2_20b} (and Table \ref{tab:matrix-chrf-qwen2_5_32b}, \ref{tab:matrix-chrf-qwen2_5_72b}, \ref{tab:matrix-chrf-llama3_1_70b} and \ref{tab:matrix-chrf-nllb_moe_54b}) shows performance of Sailor2-20B (and other baselines) between all language pairs in Flores Plus\footnote{The prediction results of Sailor2 and baseline models could be found in \url{https://huggingface.co/datasets/sailor2/Flores-Plus-Evaluation-Log-Preview-Cleaned}.}. We provide a visual comparison of Sailor2-20B against other baselines in Figure \ref{fig:flores_plus__qwen_32b}, \ref{fig:flores_plus__qwen_72b}, \ref{fig:flores_plus__llama_70b} and \ref{fig:flores_plus__nllb_moe_54b}.

\paragraph{Sailor2: Excelling in Low-Resource Translation}
Despite having significantly fewer parameters, Sailor2-20B demonstrates remarkable capabilities in low-resource language translation. 
The superior performance extends to approximately 80\% of low-resource language scenarios. 
As shown in Table \ref{tab:flores_eng_xx_translation_performance}, compared to Qwen2.5-32B and Qwen2.5-72B, Sailor2 consistently achieves higher chrF++ scores in low-resource language pairs, though showing slightly lower performance in English or Chinese translation. 
When compared with Llama3.1-70B, Sailor2-20B exhibits particular strengths in challenging low-resource scenarios, demonstrating significant advantages in Lao translation (7.3/37.6 vs 3.7/22.7), as well as when Burmese or Khmer is the target language.
The performance gap becomes even more pronounced when examining bidirectional translation capabilities - Sailor2-20B maintains relatively balanced performance in both directions for low-resource languages, while other models show significant degradation when translating into low-resource languages.
We also report the win rate of each model for each language pair in Figure \ref{fig:flores_plus_WR}. The win rate is defined as the percentage of times a model's output achieves the top-1 ChrF++ score.

\paragraph{Translation Patterns and Language Effects}
Besides the performance of Sailor2, analysis of Table \ref{tab:matrix-chrf-sailor2_20b} also reveals several important patterns in translation behavior:

(1) English demonstrates consistent superior performance across all language pairs, achieving higher chrF++ scores both as source and target language. This is clearly visible in Table \ref{tab:matrix-chrf-sailor2_20b}, where English-sourced translations consistently achieve scores above 50 chrF++ points for most target languages, significantly higher than other source languages. The performance advantage is particularly pronounced in the XX → English direction compared to English → XX translations, as evidenced by the consistently higher scores in both tables.

(2) We observe that translation quality appears more dependent on the target language than the source language, as shown in Table \ref{tab:matrix-chrf-sailor2_20b} where vertical columns (representing target languages) display more consistent score ranges compared to horizontal rows (source languages). For instance, translations into Vietnamese (vie) consistently fall within the 45-55 chrF++ range regardless of source language, while Vietnamese as a source language shows more variable performance depending on the target.

(3) Languages within the same family or system exhibit notably higher translation performance, as demonstrated in Table \ref{tab:matrix-chrf-sailor2_20b} by the Cebuano-Tagalog pair (ceb-fil: 55.5/53.3). This pattern suggests that linguistic similarity plays a crucial role in translation quality, potentially through the model's internal representation of language families. The table also reveals that geographically and culturally proximate languages, such as Indonesian-Malay pair (ind-zsm: 60.4/59.3), tend to achieve better bilateral translation performance compared to more distant language pairs.

\input{tables/8_flores_plus_eng_xx}

\input{tables/8_flores_chrf_sailor2_20b}


\subsection{SEA Culture Understanding Ability of Sailor2}
\label{sec:analysis_culture}


\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.75\textwidth]{figures/sec8_5_culture_eval_1.pdf}
        \caption{Results on BLEnD benchmark.}
        \label{fig:culture_blend}
    \end{subfigure}
    
    \vspace{0.5cm}

    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.75\textwidth]{figures/sec8_5_culture_eval_2.pdf}
        \caption{Results on CulturalBench benchmark.}
        \label{fig:culture_culturalbench}
    \end{subfigure}

    \caption{Performance comparison across models on BLEnD and CulturalBench benchmarks.}
    \label{fig:culture}
\end{figure}


Cultural understanding significantly influences the practical application and interaction quality of multilingual LLMs.
To assess the cultural understanding capabilities of the Sailor2 series models in Southeast Asian contexts, we employ CulturalBench~\citep{chiu2024culturalbench}, BLEND~\citep{myung2024blend}, and Global-MMLU~\citep{singh2024global} as evaluation benchmarks, covering a total of seven languages. 

Specifically, CulturalBench consists of single-choice and judgment questions in Filipino, Indonesian, Malaysian, Singaporean English, Thai, and Vietnamese.
BLEND includes question-and-answer tasks in Indonesian and West Java languages, while Global-MMLU comprises single-choice questions in Filipino, Indonesian, and Vietnamese.
Notably, BLEND and Global-MMLU are multilingual evaluation datasets. 
To further refine the measurement of the cultural understanding ability of LLMs, we translate CulturalBench into a multilingual version using Google Translate (from English to local languages).  

All evaluations are conducted using the 3-shot prompting approach. 
The experimental results are presented in Tables~\ref{tab:culturalbench_performance},~\ref{tab:blend_performance}, and~\ref{tab:global_mmlu_performance}.
We summarize our results in Figure~\ref{fig:culture} and find that, among models of similar size, Sailor2 has a better understanding of SEA culture, including its cuisine, traditions, geography, and more.
In Figure~\ref{fig:culturalbench_examples}, we present sample responses of the Sailor2-20B model on CulturalBench.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/sec8_5_culture_examples.pdf}
    \caption{Sample responses from CulturalBench.}
    \label{fig:culturalbench_examples}
\end{figure}


\paragraph{Indonesian Culture Understanding} 
Indonesian culture is uniquely rich and diverse, shaped by various ethnicities, languages, and historical influences. Evaluating this separately helps us assess the model’s ability to capture these distinct cultural nuances. 
We adopts two types of benchmark for evaluation: (1) General Knowledge, Local knowledge \& Reasoning: IndoMMLU~\citep{indommlu}, IndoCareer~\citep{indocareer}; (2) Cultural Reasoning: IndoCulture~\citep{koto2024indoculture}, MAPS~\citep{MAPS}, COPAL-ID~\citep{copalid}, IndoCloze~\citep{IndoCloze}.
As listed in Table~\ref{tab:8_indo_culture_eval}, Sailor2 models present the good performance in understanding Indonesian culture and knowledge.

\vspace{-2mm}
\input{tables/8_indo_culture_evaluation}

