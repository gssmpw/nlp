\section{Evaluation}

\subsection{Evaluation on Base Model}

For base model evaluation, we focus on the basic language understanding task like sentence classification, and language generation task like question answering and machine translation.
Specially, we evaluate Sailor2 on SailCompass~\citep{sailcompass} evaluation suite and FLoRes-200~\citep{nllb2022} translation suite.
To expand the evaluated language coverage, we choose the dataset in Indonesian, Thai, Vietnamese, Malay and Javanese. 

For Indonesian, we choose IndoCulture~\citep{koto2024indoculture}, TydiQA~\citep{tydiqa}, Belebele~\citep{bandarkar-etal-2024-belebele}.
For Thai, we choose MMLU~\citep{kydlicek2024finetasksmultilingualtasks}, M3Exam~\citep{zhang2023m3exam}\footnote{For Thai M3Exam, we adopt finetasks~\citep{kydlicek2024finetasksmultilingualtasks} codebase for evaluation.} and Belebele.
For Vietnamese, We choose VMLU~\footnote{VMLU:~\url{https://github.com/ZaloAI-Jaist/VMLU/}}, M3Exam and Belebele.
For Malay, we choose Tatabahasa~\citep{lovenia2024seacrowd}.
For Javanese, we choose M3Exam.
For all SEA languages, we choose FLoRes-200~\citep{nllb2022} and XCOPA~\citep{ponti2020xcopa}. 
We have more detailed comparison and analysis for translation in Section~\ref{sec:analysis_translation} and culture understanding in Section~\ref{sec:analysis_culture}. 

Detailed results are presented in Table~\ref{tab:base_model_pref_overview}. We observe that both Sailor2-8B and Sailor2-20B exhibit the highest average performance within their respective parameter groups. Notably, Sailor2-20B even outperforms larger models, including the three-times larger Llama3.1-70B.


\input{tables/7_base_model_evaluation}


\subsection{Evaluation on Chat Model}

We aim to comprehensively evaluate the performance of our Chat Model by using WildBench~\citep{lin2024wildbenchbenchmarkingllmschallenging} as the primary evaluation dataset. WildBench covers five tasks: Coding \& Debugging, Information Seeking, Math \& Data, Reasoning \& Planning, and Creative Tasks. We employ GPT-4o-0806 to translate WildBench into eight SEA languages (Thai, Vietnamese, Indonesian, Tagalog, Burmese, Khmer, Lao, and Malay), thereby creating a new benchmark named SEA-WildBench~(SWB).

Detailed results are presented in Table~\ref{tab:chat_perf_task_level} (task-level) and Table~\ref{tab:chat_perf_language_level} (language-level). We use the SWB Score as our evaluation metric, which is calculated based on the win-rate against GPT-4o-0806 (the same model serves as the judge). We selected the most representative open models, including both general-purpose and SEA language-optimized variants. For improved visualization, we use Llama-3.1-70B-Instruct as the baseline, with a SWB Score of 30. Our results indicate that both Sailor2-20B-Chat and Sailor2-8B-Chat achieve superior performance across various tasks and languages. 
As shown in Table~\ref{tab:chat_perf_language_level}, Sailor2 models excel in low-resource languages. Notably, Sailor2-20B-Chat achieves nearly a 50\% win rate against GPT-4o-0806 on SeaWildBench, demonstrating GPT-4o-level performance in local chat scenarios for Southeast Asian languages.


Note that the overall SWB Score can be higher than the scores for individual subsets. For example, although Llama-2-7B-Chat scores below 0.05 on each subset, its overall SWB Score is 0.05. 
We follow the WildBench score calculation~\footnote{\url{https://tinyurl.com/49en4cw6}}.
This method may overestimate scores in cases where parse errors occur.


\input{tables/8_swb_split_by_task}

\input{tables/8_swb_split_by_language}
