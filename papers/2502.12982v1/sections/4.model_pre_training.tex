\section{Model Continual Pre-Training}

\subsection{Model Expansion}

The Sailor2 model comes in three sizes, 1B, 8B, and 20B, which are expanded from the Qwen2.5 base models of 0.5B, 7B, and 14B, respectively. The decision was made to perform model expansion prior to continual pre-training in order to mitigate the potential for forgetting of English and Chinese language capabilities, while also enhancing the model’s capacity for further improvements in SEA languages.

In practice, the approach draws inspiration from LlamaPro~\citep{wu2024llama}, leveraging a block-expansion mechanism in the original Qwen2.5 model. This approach significantly enhances the model’s performance in SEA languages while maintaining stable capabilities in English and Chinese. By utilizing the strategy, the newly introduced layers are able to store the additional SEA knowledge from the continually pre-trained tokens, rather than overwriting the existing linguistic information of the other languages.



\subsection{Model Parallel Optimization}
We leverage key Megatron-LM optimizations \citep{narayanan2021megatron} to accelerate training.


\subsubsection{Zero Bubble Pipeline Parallelism} 

Zero Bubble Pipeline Parallelism \citep{qi2023zerobubble} minimizes GPU idle time by splitting the backward pass into input and weight components, prioritizing the former. While ZB-2P or ZBV \citep{qi2024pipeline} could fully eliminate pipeline bubbles for better throughput, we opt for the simpler ZB-H1 \citep{qi2023zerobubble}, which reduces bubbles to $\nicefrac{1}{3}$ with just ~80 lines of code changes in Megatron-LM \citep{narayanan2021megatron}.


\subsubsection{Large Vocabulary Optimization}

As vocabulary size increases, placing vocabulary layers in the first or last pipeline stage leads to imbalanced computation and memory usage. 
For Sailor2-8B, a single vocabulary layer is roughly equivalent to four transformer layers, increasing memory usage and GPU idle time, often resulting in out-of-memory (OOM) errors.
Moreover, Zero Bubble Pipeline Parallelism \citep{qi2023zerobubble} further exacerbates this by delaying weight gradient computation, making vocabulary activations long-lived and a memory bottleneck. 
While Vocabulary Parallelism proposed in \cite{yeung2024vocab} proposes a perfect balance, we take a simpler approach: redistributing transformer layers from the last stage to other stages (excluding the first) based on FLOP calculations, which also eliminates the last stage’s extra memory overhead.


\subsection{Intra-Document Training}

We employ intra-document masking to disable cross-document attention within a packed sequence. It has been shown in previous studies~\citep{zhao-etal-2024-analysing, llama3} that it improves pretraining compared to fully-open attention by a large margin, especially when the documents are randomly concatenated with each other. 
It has also been shown to be effective in large-scale pretraining. Specifically, during pretraining, we replace the attention module in Megatron with the \texttt{flash\_attn\_varlen}
 function and pass the length information of the documents in the pretraining corpus to ensure that attention is computed only within the same document, avoiding the calculation of cross-document scores.



\subsection{Two-Stage Continual Pre-Training}

We adopt a two-stage pre-training approach inspired by MiniCPM~\citep{hu2024minicpm}. 
In stage one, we train on comprehensive datasets at a high learning rate (1e-4) and 1,024 global batch size, introducing high-resource languages such as English, Chinese, Vietnamese, Indonesian, Thai, Malay, Burmese, Tagalog, and Khmer. 
In stage two, we shift to high-quality tokens with a lower learning rate (1e-5)  and 4,096 global batch size, and expand to include both high-resource and low-resource languages, adding Cebuano, Lao, Javanese, Waray, Sundanese, and Ilocano. 
This strategy automatically mixes data in stage one and seamlessly integrates high-quality low-resource tokens in stage two without adjusting mixing ratios.

\subsubsection{Stage 1: Pre-training with Balanced Data Mixture}
In stage 1, we select a subset of languages that could provide sufficiently enough tokens for Regmix data mixture optimization. 
After conducting 1,000 runs of data mixture optimization using 1M models, we observed a subtle shift from the original token distribution. Notably, the optimized data mixture resulted in upsampling languages like Khmer, Malay, Burmese, Thai, and Tagalog, while simultaneously downsampling Indonesian and Vietnamese. The final data mixture of Stage 1 is shown in Table~\ref{tab:effective_tokens_stage1}~(tokens counted in the tokenizer of Qwen2.5).

\vspace{-1mm}
\begin{table}[ht]
\centering
\caption{Effective Tokens by Language in Stage 1.}
\begin{tabular}{lc}
\toprule
\textbf{Language} & \textbf{Effective Tokens} \\ \midrule
Vietnamese & 102B \\
Indonesian & 94B \\
Thai       & 92B \\
English    & 51B \\
Chinese    & 50B \\
Burmese    & 23.5B \\
Malay      & 21B \\
Tagalog    & 10B \\
Khmer      & 6.5B \\ \midrule
\textbf{Stage 1 (Total)} & \textbf{450B} \\ \bottomrule
\end{tabular}
\label{tab:effective_tokens_stage1}
\end{table}
\vspace{-1mm}


\subsubsection{Stage 2: Annealing with High-Quality Tokens}
In stage 2, we lower the learning rate to 1e-5 (1/10 of the original learning rate), and take 20\% of the stage 1 dataset to make sure the model still behaves well on the original distribution. As for the remaining 80\% training budget, we allocate them to high-quality SEA tokens, where all low-resource languages are added, and the token distribution of high-resource languages is maintained as similar to the stage 1.
In addition, we also added some English instruction tuning datasets and some datasets contributed by the Sailor2 community.

\vspace{-1mm}
\begin{table}[ht]
\centering
\caption{Effective Tokens by Language in Stage 2.}
\begin{tabular}{lc}
\toprule
\textbf{Language} & \textbf{Effective Tokens} \\ \midrule
Stage 1 & 10B \\
English Instruction Tuning Dataset & 2.5B \\
Vietnamese (High-Quality) & 10.9B \\
Indonesian (High-Quality) & 12.8B \\
Thai (High-Quality) & 13.9B \\
Burmese (High-Quality) & 2.8B \\
Malay (High-Quality) & 1.3B \\
Tagalog (High-Quality) & 2.2B \\
Khmer (High-Quality) & 0.9B \\
Waray (High-Quality) & 0.02B \\
Ilocano (High-Quality) & 0.05B \\
Javanese (High-Quality) & 0.17B \\
Lao (High-Quality) & 0.33B \\
Cebuano (High-Quality) & 0.30B \\
Sundanese (High-Quality) & 0.09B \\ \midrule
\textbf{Stage 2 (Total)} & \textbf{60B} \\ \bottomrule
\end{tabular}
\label{tab:effective_tokens_stage2}
\end{table}
\vspace{-1mm}





