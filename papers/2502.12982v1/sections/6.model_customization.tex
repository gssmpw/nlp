\section{Model Customization}

\subsection{Long-Context Training}

A 128K token context window allows large language models (LLMs) to handle complex tasks such as multi-document question answering~\citep{wang2024leave}, repository-level code comprehension~\citep{jimenez2024swebench}, and many-shot learning by capturing long-range dependencies~\citep{agarwal2024many}, leading to more coherent and contextually relevant outputs~\citep{mazumder2022lifelong}.

The Sailor2 series employs AnchorAttention~\citep{wang2024precision} to extend its maximum context length from 4K to 128K\footnote{Long-context training codebase: \url{https://github.com/haonan3/AnchorContext}}. In particular, Sailor2 masks out cross-document attention to prevent the model from aggregating irrelevant information across irrelevant documents. Note, this strategy that aligns with the approach used during pretraining. By maintaining a consistent masking paradigm in both pretraining and long-context training, Sailor2 mitigates potential conflicts that could arise from shifting between different attention mechanisms.

Unlike approaches such as LLaMA3~\citep{llama3}, which rely solely on cross-document attention masking, Sailor2 introduces an anchor token that serves as a stable reference point. Specifically, the first token in each training sequence (the \texttt{<eos>} token of each sample) retains a fixed positional ID and is therefore visible to all documents within the training context. This design helps reduce numerical instability and provides the model with a consistent anchor across the extended sequence. Furthermore, instead of resetting the positional IDs to $0$ for each new document~\citep{zhao-etal-2024-analysing}, Sailor2 maintains continuous positional indexing across the entire sequence, allowing the model to fully utilize the entire position range in training.

With AnchorAttention, Sailor2 efficiently achieves long-context capabilities while training on a relatively small amount of data. Specifically, Sailor2 uses a total of 4 billion tokens in 1,000 steps (4 million tokens per step) at a learning rate of \(2 \times 10^{-5}\), with the first 200 steps designated as warm-up. Despite the limited token budget, Sailor2 effectively extends its context length, as demonstrated by the model’s performance on the RULER benchmark~\citep{ruler}, as shown in Table~\ref{tab:ruler}.

In the meanwhile, the short-context performance is kept and sometimes outperforms the pretrained one. 
The perplexity on different languages is in Table~\ref{tab:long_context_model_ppl}.
The performance of different tasks is in Table~\ref{tab:downstream_task_perf}.

\vspace{-2mm}
\input{tables/6_long_context_ruler}
\input{tables/6_long_context_ppl}
\input{tables/6_long_context_downstream_task}



\subsection{Speculative Decoding}
To accelerate model inference, we adopted speculative decoding, a technique designed to reduce the computational cost of autoregressive generation. Specifically, we customized a one-layer draft model, GliDe~\citep{du2024glide}\footnote{Speculative decoding codebase: \url{https://github.com/NonvolatileMemory/GliDe_with_a_CaPE_ICML_24}~(Training) and \url{https://github.com/penghui-yang/sailor-glide}~(Inference)}, for Sailor 8B and 20B.

\paragraph{Background.} GliDe is a draft model based on a transformer decoder-only architecture that retains standard components—self-attention, cross-attention, and feed-forward networks (FFNs). In GliDe, the conventional self-attention layer is applied first, where each token in the sequence attends only to its preceding tokens. This is immediately followed by a cross-attention layer, which reuses precomputed and cached cross-attention outputs from the target LLM instead of recomputing the keys and values for each draft token. This approach yields a more precise token representation while reducing redundant computations. Finally, the cross-attended outputs pass through position-wise FFNs to further refine token representations. The processing sequence follows: self-attention → cross-attention → FFN.

\paragraph{Implementation Details.} Unlike GliDe, we share the weights of the embedding layer and LM head between the target and draft models, significantly reducing memory consumption, especially for large-vocabulary LLMs. Moreover, to improve the stability and robustness of the draft model, we employed a flash noise training technique to replace the cape mask in the original GliDe, which can not only solve the problem of training-inference discrepancy but also be compatible with Flash Attention~\citep{dao2022flashattention}. Specifically, for the cross-attention query \(Q_t\) in the draft model, we can only ensure access to the corresponding key-value states \(K_{<t'}\), \(V_{<t'}\) that satisfy \( 1\le|t' - t|<\gamma \), where \(\gamma\) denotes the number of speculative steps. During training, we randomly shift the indices of queries and key-value states within the range \(1 \le j < \gamma\).  
If the sequence length is \(l\), we then compute  
\(
O_{\geq j} \;=\; \mathrm{flash\_attn}\bigl(Q_{\geq j}, \,K_{< l-j}, \,V_{< l-j}\bigr).
\)
This approach effectively enforces the same visibility constraints as those in the inference phase, \emph{i.e.}, \( 1\le|t' - t|<\gamma \), thereby ensuring that the training process aligns with inference behavior.



During speculative decoding inference, we first generate tokens autoregressively using the one-layer draft model, followed by parallel verification with Sailor 2. This approach effectively reduces the number of autoregressive steps required for decoding. Since tree-based speculative decoding is incompatible with Flash Attention, we opted for a straightforward sequential speculative decoding strategy. We set the speculation length $\gamma$ to 4 based on empirical observations. 

\paragraph{Performance.} The performance of our GliDe model is illustrated in Figure~\ref{fig:glide_accept_length} and Figure~\ref{fig:glide_speed}, demonstrating an approximate 2$\times$ acceleration. Notably, for Burmese (mya), our approach achieves an accept length exceeding 3 and a speedup of approximately 2.5$\times$. We attribute this improvement to the high tokenization granularity of Burmese, which provides a greater margin for speculative decoding to optimize token generation.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/sec6_2_accept_length_comparison.pdf}
    \caption{
    Comparison of GliDe Accept Length in Different Languages.
    }
    \label{fig:glide_accept_length}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/sec6_2_token_generation_speed.pdf}
    \caption{
    Comparison of GliDe Token Generation Speed in Different Languages.
    }
    \label{fig:glide_speed}
\end{figure}

\subsection{Model Pruning}

By leveraging existing pre-trained models, the pruning method enables the rapid generation of smaller-scale models, 
which avoids the high costs of training from scratch. In this study, we apply the Sheared LLaMA method\citep{xia2023sheared} to prune the Sailor2-20B and Sailor2-8B models, resulting in Sailor2-14B and Sailor2-3B respectively\footnote{Model pruning codebase: \url{https://github.com/princeton-nlp/LLM-Shearing }}. 
The pruning stage takes 6B tokens.
Subsequently, we performed continual training using a dataset of 180B tokens to recover the models' performance.

\paragraph{Background.} The Sheared LLaMA method focuses on structured pruning to produce smaller yet competitive models from pre-trained larger models. It employs two main techniques: targeted structured pruning and dynamic batch loading.  
Targeted structured pruning compresses a model into a target architecture via $L_0$-regularized binary mask learning.
Lagrange multipliers are applied to enforce constraints on the target architecture, ensuring the pruned model adheres to the desired configuration while optimizing performance. Dynamic batch loading adjusts training data batches based on domain-specific loss reduction rates, enhancing data efficiency and accelerating convergence.

\paragraph{Implementation Details.} In contrast to the Sheared LLaMA method, we introduce several optimizations in our approach. First, instead of pruning Multi-Head Attention as in the original method, we retain the Key and Value Heads in the Grouped Query Attention~\citep{ainslie2023gqa} structure, pruning only an equal number of Query Heads from each Query Head Group corresponding to the KV Heads. Second, we do not prune the layer dimension, as our preliminary experiments have shown that pruning the layer dimension leads to convergence difficulties. Instead, we focus on optimizing other dimensions (i.e., hidden dimension, head number, and intermediate dimension). Third, to maintain consistency between the hidden dimensions and the number of attention heads, the pruning options are limited. We recommend conducting ablation studies with minimal continual training to identify optimal configurations. Finally, during continual training,  we have not used the dynamic batch loading strategy, as it is complex to divide the pretraining data into several domains explicitly.
Instead, we directly sample from the Sailor2 Stage-2 training dataset, achieving promising results.
 
\paragraph{Performance.} To obtain the final chat models, we train two pruned models using the Sailor2 post-training pipeline, resulting in Sailor2-3B-Chat and Sailor2-14B-Chat. 
The experimental results in Table~\ref{table:pruning_swb_perf} demonstrate that these pruned models significantly outperform the baseline Qwen2.5 in low-resource languages such as Khmer and Lao.


\input{tables/6_pruning_swb_results}

