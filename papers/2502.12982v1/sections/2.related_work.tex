\section{Related Works}

\subsection{Open SEA Language Models}

Open science has gained increasing attention, particularly with the thriving efforts in developing open language models. 
While notable initiatives like OLMo~\citep{groeneveld2024olmo}, LLM360~\citep{liu2023llm360}, and MAP-Neo~\citep{zhang2024map} have made significant contributions, they primarily focus on dominant languages on the Internet, such as English and Chinese.
The Aya model~\citep{ustun-etal-2024-ayamodel} serves as a massively multilingual language model, supporting 101 languages, beats previous multilingual models such as BloomZ~\citep{muennighoff2022crosslingual}, yet not particularly expert in South-East Asian (SEA) languages.
Although there has been some recent progress in creating SEA language models, open initiatives such as the SeaLLM series~\citep{nguyen-etal-2024-seallms, zhang2024seallms} and Sea-LION series~\citep{sea_lion_2024} still fall short of achieving performance levels comparable to commercial models, such as GPT-4o~\citep{achiam2023gpt4}.

Starting in March 2024, we have continuously released both Sailor and Sailor2. 
We are committed to building a fully open pipeline for the entire LLM ecosystem while striving to achieve top-tier SEA language performance. 
In the future, we will continue refining the Sailor series models to advance open language models for more low-resource languages.


\subsection{Open SEA Language Resources}
\label{sec:related_work_resource}

Resources for SEA languages remain underdeveloped.
\textbf{Pre-training}: Even the recent Fineweb2 Dataset~\citep{penedo2024fineweb-2}, which scales the pre-training corpus to over 1,000 languages, provides a significantly smaller data volume for SEA languages compared to others, falling short of the 100B tokens. Moreover, directly translating English resources into local languages often leads to an overestimation of performance, as these translations typically lack culturally nuanced content~\citep{singh2024global}.
\textbf{Post-training}: The Aya dataset~\citep{singh2024aya} is the largest multilingual instruction fine-tuning resource, containing 513 million instances across 114 languages. It comprises mainly machine-translated data with a small, essential human-curated subset.
\textbf{Evaluation}: Although benchmarks such as SeaBench~\citep{liu2025seaexam}, SeaCrowd~\citep{lovenia2024seacrowd}, and SeaEval~\citep{SeaEval2023} have been introduced, they remain limited in either language coverage, primarily focusing on Thai, Indonesian, Vietnamese, and Malay, or in dataset quality due to reliance on machine translations.


In the Sailor2 project, we open source the SailCraft scripts for SEA-language-specific data cleaning, the instruction tuning dataset covering 17 SEA languages, SailCompass evaluation suit for base model evaluation, and the SEA-WildBench for chat model evaluation.


\subsection{Cookbook for Multilingual Language Models}
There have been swift advancements and many explorations in multilingual large language models.
FinGPT~\citep{luukkonen-etal-2023-fingpt} builds on BLOOM~\citep{Scao2022BLOOMA1} through continual pretraining (CPT), primarily targeting Finnish and other low-resource languages, while incorporating English data for optimization.
MAP-Neo~\citep{zhang2024map} is a recently released 7B bilingual Chinese-English Bilingual model, designed with a from-scratch approach. Notably, it offers full transparency, particularly in pretraining corpus collection, processing, and cleaning, providing detailed records and rigorous data curation rules.
Jais~\citep{sengupta2023jais}, an Arabic-centric multilingual model, is trained from scratch on Arabic and English data, with followup safety tuning, offering a structured guidance recipe for optimizing model safety.
BritLLM \citep{britllm} is a UK-centric LLM initiative, aiming to develop open pipelines tailored to UK-specific needs, including law, finance, healthcare, and multilingual diversity.

The Sailor2 project also actively explores multilingual LLM development, offering a cookbook while addressing key challenges such as English performance degradation, multilingual data collection and cleaning, optimal language mixing strategies, multi-stage training, post-training techniques, inference acceleration, and more.