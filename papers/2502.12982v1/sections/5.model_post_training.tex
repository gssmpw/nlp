\section{Model Post-Training}

Sailor2 employs the following post-training techniques: (1) two-stage instruction tuning using 4.8M examples from SEA-UltraChat, covering 14 SEA languages; and (2) two-stage preference tuning on both off-policy data from SEA-UltraFeedback and on-policy preference data. Table \ref{table:sft_data_distribution} summarizes the statistics for SEA-UltraChat and SEA-UltraFeedback.

\subsection{Instruction Tuning}

\subsubsection{SEA-UltraChat Construction}
As described in Section~\ref{sec:related_work_resource}, existing instruction tuning datasets for SEA languages are limited in both quality and quantity. To address this, we translate UltraChat~\citep{ding2023enhancing}, a high-quality and diverse English instruction dataset, into 15 SEA languages using GPT-4o-0803, resulting in 4.4 million multilingual examples. 
Translating code and math data into multiple languages remains particularly challenging~\citep{Huang2025BenchMAXAC}. To mitigate this, we developed a novel multi-round translation prompt\footnote{Inspired by \url{https://baoyu.io/blog/prompt-engineering/translator-gpt-prompt-v2}; see Box~\ref{box:trans_prompt} in Appendix for details.}.

\paragraph{Data Cleaning.}
The dataset is first partitioned by language, and each entry is assigned a MinHash signature \citep{Broder1997OnTR} using 128 permutations. These signatures are then compared using a Locality-Sensitive Hashing (LSH) index \citep{leskovec2014mmds} with a Jaccard similarity threshold of 0.8, enabling efficient identification of near-duplicate entries. 
The data entries are also verified against a strict message format specification: (1) a system prompt, if present, must appear as the first message; (2) user queries and assistant responses must strictly alternate, with the assistant's response being the final message; and (3) all messages must contain non-empty content. 
Through this process, the deduplication phase eliminated 1.4\% of the original data rows\footnote{Most deduplicated examples result primarily from translation errors.}, while the verification filtered out about 1K invalid samples. 
Finally, SEA-UltraChat comprises 4.8 million examples across 14 Southeast Asian languages, as detailed in Table~\ref{table:sft_data_distribution}.

\begin{figure*}
\centering
\begin{minipage}[b]{\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{figures/sec5_1_distribution_analysis.pdf}
\end{minipage}
\captionof{figure}{Distribution of categories and languages in SEA-UltraChat. Stage 2 data is carefully curated to ensure a balanced representation across both dimensions.}
\label{fig:sft_cate_distribution}
\end{figure*}
\vspace{-2mm}

% \subsubsection{SEA-UltraChat Categorization}
\paragraph{Data Categorization.}
Following the categories of WildBench \citep{lin2024wildbenchbenchmarkingllmschallenging}, we categorize the data into \textbf{5 main categories} encompassing \textit{11 subcategories}: \textbf{Coding \& Debugging} (\textit{Coding \& Debugging}), \textbf{Info Seeking} (\textit{Information Seeking, Advice Seeking}), \textbf{Math \& Data} (\textit{Math, Data Analysis}), \textbf{Reasoning \& Planning} (\textit{Reasoning, Planning}), and \textbf{Creative Tasks} (\textit{Creative Writing, Editing, Brainstorming, Role Playing}). 
To perform this categorization, we employ Qwen2.5-7B-Instruct to classify each data point based on the initial user query into one of the 11 subcategories, which are then consolidated into the 5 main categories.
The distribution of these categories is presented in Figure \ref{fig:sft_cate_distribution}. Notably, \textbf{Coding \& Debugging} and \textbf{Math \& Data} collectively constitute less than 12\% of the total dataset, revealing a significant category imbalance in the distribution.


\subsubsection{Two-Stage Instruction Tuning}

In developing multilingual models, maintaining balance across languages and domains is crucial. However, our supervised fine-tuning dataset exhibits significant imbalances in both dimensions, as shown in Figure~\ref{fig:sft_cate_distribution}: language distribution ranges from 34.8\% for English to merely 0.6\% for low-resource languages like Acehnese, while domain coverage shows a substantial difference in percentage, with creative tasks significantly greater than coding and mathematical content.

To address these imbalances, we employ the two-stage instruction tuning inspired by~\cite{Huang2024OpenCoderTO}. 
Stage 1 establishes a broad foundation by processing the bulk of the training data with a large batch size of 4096 over a single epoch. 
To optimize learning, the learning rate is gradually decreased from $7 \times 10^{-6}$ to $7 \times 10^{-7}$.
Building upon this base, Stage 2 then focuses on a carefully selected subset of data balanced across both languages and domains, employing a small batch size of 512 over 3 epochs. 
This strategic approach maximizes the use of instruction data while ensuring the model maintains balance across dimensions.


\subsubsection{Instruction Data Selection for Stage 2}
To select high-quality data for stage 2, we annotate each sample with two metrics: (1) a reward score from a reward model\footnote{We use \texttt{Skywork/Skywork-Reward-Llama-3.1-8B} from HuggingFace as the reward model.}, and (2) the perplexity computed by Sailor2-8B. 
Both metrics are normalized by computing their percentiles within each language and category. Figure~\ref{fig:sft_ppl_reward} displays the distribution of English instruction data in the Creative Task category. Our case study in Table~\ref{tab:corner_case_analysis} demonstrates that instruction data with both high reward scores and high perplexity are particularly valuable for stage 2 training. 
In general, a high reward score indicates a high-quality response, while high perplexity suggests that such responses are under-trained. Based on this analysis, we rank the instruction data using the harmonic mean (i.e., the product divided by the sum) of their reward and perplexity percentiles.

After ranking, we apply an embedding-based deduplication step to select a fixed number of final candidates for each category and language. Specifically, we utilize the \textit{jinaai/jina-embeddings-v3} model from HuggingFace to generate embeddings and filter out any data point whose cosine similarity with an already selected item exceeds 0.6.





\begin{figure*}
\centering
\begin{minipage}[b]{\textwidth}
\centering
\includegraphics[width=0.85\textwidth]{figures/sec5_1_sft_ppl_vs_reward.pdf}
\end{minipage}
\captionof{figure}{The PPL Percentile vs Reward Percentile distribution of English instruction data on Creative Tasks. We select High PPL High Reward candidates (top right) as stage 2 instruction data. We report corner cases highlighted in yellow in Table \ref{tab:corner_case_analysis}.} 
\label{fig:sft_ppl_reward}
\end{figure*}

\input{tables/5_sft_case_study}

\subsection{Preference Tuning}

In Sailor2, we perform the preference tuning to enhance model performance beyond supervised fine-tuning. This section first introduces the problem formulation of reinforcement learning from human feedback and the learning algorithms examined in this work (Sec.~\ref{sec.pt.background}). We then describe the pipeline for constructing preference data in SEA languages (Sec.~\ref{sec.pt.data}) and present the full recipe of the preference tuning (Sec.~\ref{sec.pt.recipe}). In addition, we provide extensive ablation study results on preference data construction in Sec.~\ref{sec.pdata}.

\subsubsection{Background}\label{sec.pt.background}
In preference tuning, the preference data typically takes the form of pairwise preferences. Each prompt $\mathbf{x}$ is paired with two possible responses, $\mathbf{y}_1$ and $\mathbf{y}_2$. The human annotator~\citep{christiano2017deep} or AI annotator~\citep{lee2023rlaif} provides the preference feedback $o(\mathbf{y}_1 \succ \mathbf{y}_2|\mathbf{x}) \in \{0,1\}$, indicating whether $\mathbf{y}_1$ is preferred over $\mathbf{y}_2$. The preferred response is denoted as $\mathbf{y}_w$, while the other is denoted as $\mathbf{y}_l$. 

\textbf{Policy optimization algorithms.} DPO~\citep{rafailov2024direct} is introduced to optimize the policy model in an offline manner. \cite{rafailov2024direct} demonstrates that it directly optimizes the RLHF objective using the following equivalent formulation:
\begin{align}
    \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(\mathbf{y}_w|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_w|\mathbf{x})} - \beta \log \frac{\pi_\theta(\mathbf{y}_l|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_l|\mathbf{x})} \right) \right].
\end{align}

Unlike the classic RLHF pipeline~\citep{ouyang2022training} which first trains a reward model and then optimizes the policy using the trained RM, DPO optimizes the policy while simultaneously training an implicit reward model. This approach allows DPO to directly optimize the policy using preference pairs, thereby simplifying the preference-tuning pipeline. Recently, many variants have been proposed to improve the vanilla DPO algorithm~\citep{meng2024simpo, mao2024simple, azar2024general, ethayarajhmodel}. In this work, we explored three promising approaches including SimPO~\citep{meng2024simpo}, length-normalized DPO (LN-DPO)~\citep{rafailov2024direct}, and length-regularized DPO (LR-DPO)~\citep{park2024disentangling}. Our experiment results indicate that LR-DPO achieves a favorable balance between performance and verbosity. The objective of LR-DPO is defined as follows: 

{\small
\begin{align}
    \!\!\mathcal{L}_{\text{LR-DPO}}(\pi_\theta; \pi_{\text{ref}})\!=\!-\mathbb{E}_{(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l) \sim D} \left[ \log \sigma \left(
    \beta \log \frac{\pi_\theta(\mathbf{y}_w|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_w|\mathbf{x})} -
    \beta \log \frac{\pi_\theta(\mathbf{y}_l|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_l|\mathbf{x})} +
    \alpha |\mathbf{y}_w| - \alpha |\mathbf{y}_l| \right) \right].\!\!
\end{align}
}

The additional length difference term serves as a regularizer, down-weighting the gradients of preference pairs in which the preferred response is longer, and vice versa. This mitigates length exploitation in preference tuning.


\subsubsection{Preference Data}\label{sec.pt.data}

Our preference tuning consists of two stages, training with off-policy responses generated by Llama-3-8B-Instruct and training with on-policy responses generated by Sailor2 suite. Additionally, we conduct the preference distillation from our 20B model to smaller models. 

\paragraph{Off-policy Data.} To construct the off-policy dataset, we first translate the UF-Llama3 preference dataset\footnote{\url{https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback-armorm}} into SEA languages. 
Low-quality translations are filtered based on perplexity scores obtained from the Sailor2-8B base. The resulting off-policy dataset is a mixture of SEA languages and English. 
Note that GPT-4o struggles with translating extremely low-resource languages such as Lao and Khmer, often producing outputs with excessive emojis and improper formatting. We find these cases using a simple script and translate them into the target language using Deepseek-V3~\citep{deepseekai2024deepseekv3technicalreport}, which has demonstrated superior performance as evaluated by~\cite{Huang2025BenchMAXAC}.

\paragraph{On-Policy Data.} 
At this stage, we use the prompts from the off-policy dataset to generate responses with the corresponding model. These responses are scored by the open-source reward model, Skywork-Reward-Gemma-2-27B~\citep{liu2024skywork}, selecting the highest as chosen and the lowest as rejected. 
We also apply a language consistency verifier to correct input-output language mismatches (excluding translation tasks).

\paragraph{Preference Distillation.}  
After off-policy training, our 1B–14B models are finetuned on the on-policy data from our 20B model, rather than using their own. This approach simplifies the training pipeline and reduces computational costs. Our ablation study (Sec.~\ref{sec.pdata}) shows that distillation yields comparable downstream DPO performance.

\paragraph{Probe Data.} 
During development, we observed unexpected model behaviors that were not captured by standard evaluation suites. For example, an early version of Sailor2 frequently included emojis in its responses—an undesired trait in many use cases. However, since the presence of emojis alone did not significantly affect reward model scores, this issue remained undetected. To address this, we introduce probe data, a set of prompts designed to elicit specific behaviors. These prompts were selected from AlpacaEval 2 by identifying cases where an early model version produced emoji-containing responses. Using this probe data, we assessed whether our interventions effectively reduced emoji overuse. 

More ablation studies in Sec.~\ref{sec.pdata} analyze the impact of design choices in the preference data construction pipeline.

\subsubsection{Preference Tuning Recipe}\label{sec.pt.recipe}

Due to the absence of a high-quality reward model for PPO-based algorithms, we explore different direct alignment algorithms, such as DPO and its variants (SimPO~\citep{meng2024simpo}, LN-DPO~\citep{rafailov2024direct}, LR-DPO~\citep{park2024disentangling}). 
LN-DPO optimizes the length-averaged log-probabilities, while LR-DPO explicitly introduces the response length as the regularizer in their objective. 
We extensively tuned hyperparameters and conducted ablation studies to optimize model performance. Table~\ref{tab.pt.hyper_search} summarizes the hyperparameter search space, and Table~\ref{tab.pt.hypers} lists the final preference tuning settings. LR-DPO, offering a good balance between performance and verbosity, was chosen to train our final models.

All experiments were conducted with the training framework, Oat~\citep{liu2024sample,liu2025oat}, which enables large-scale and flexible training.

\vspace{-2mm}
\begin{table}[h]
\centering
\caption{Hyperparameters of different algorithms for preference tuning. We explore hyperparameters suggested by prior work~\citep{meng2024simpo,lambert2024t}.}
\begin{NiceTabular}{lcccl}
\toprule
\textbf{Algorithm} & \textbf{LR} & $\beta$ & \textbf{Batch Size} & \textbf{Method Specific} \\
\midrule
SimPO & $\{5$e-$7$, $1$e-$6\}$ & $\{2.5, 10\}$ & $128$ & $\gamma\text{-}\beta$ ratio: $\{0.3, 0.5\}$ \\
LN-DPO & $5$e-$7$ & $\{5, 10, 15\}$ & $128$ & \\
DPO & $5$e-$7$ & $\{0.01, 0.1, 0.3\}$ & $128$ & \\
LR-DPO & $5$e-$7$ & $\{0.01, 0.1, 0.3\}$ & $128$ & $\alpha$: $[1$e-$5$,$1$e-$2]$ \\
\bottomrule
\end{NiceTabular}
\label{tab.pt.hyper_search}
\end{table}

\vspace{-2mm}

\begin{table}[h]
\centering
\caption{Final training hyperparameters for preference tuning. We utilize the length-regularized DPO proposed by~\cite{park2024disentangling}.}
\begin{NiceTabular}{l|ccccc}
\toprule
\multicolumn{1}{l}{\textbf{Hyperparams}} & \textbf{1B} & \textbf{3B} & \textbf{8B} & \textbf{14B} & \textbf{20B} \\
\midrule
Learning Rate & \multicolumn{5}{c}{$5$e-$7$} \\
Learning Rate Schedule & \multicolumn{5}{c}{cosine with min lr} \\
Batch Size & \multicolumn{5}{c}{$128$} \\
Max Response Token Length & \multicolumn{5}{c}{$2048$} \\
KL Coefficient $\beta$ & \multicolumn{5}{c}{$0.01$} \\
Warm Up Ratio & \multicolumn{5}{c}{$0.03$} \\
Number of Epochs & \multicolumn{5}{c}{$1$} \\
Length-Regularized Coef. $\alpha$ & $0.001$ & $0.0002$ & $0.01$ & $0.0$ & $0.003$ \\
\bottomrule
\end{NiceTabular}
\label{tab.pt.hypers}
\end{table}
\vspace{-2mm}