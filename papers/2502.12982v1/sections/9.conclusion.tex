\section{Conclusion and Future Work}

This report introduces the Sailor2 family of open models, designed to facilitate the development of large language models for Southeast Asian languages. 
We also summarize the key insights from our pipeline for building the Sailor2 model, covering data curation, continual pre-training, post-training, evaluation and advanced model customization. 
We hope this report will inspire the community to develop more inclusive and robust multilingual language models for underserved languages.

Looking ahead, we plan to expand our multilingual research to include a broader range of low-resource languages and explore more efficient model training approaches. 
The following sections detail our motivations and review the most relevant works in \textbf{data curation}, \textbf{model design}, and \textbf{model training}.

\subsection{Synthetic Data Curation for Supporting Low-resource Languages}
\label{sec:future_work_data_curation}

Apart from a few high-resource languages, most languages have a relatively scarce supply of training tokens. For example, in Sailor2, six languages contain fewer than 1 billion training tokens (See Table~\ref{tab:disk_size_training_token} for detailed statistics). 
For extremely low-resource languages like Minangkabau (spoken in Indonesia by approximately 6.5 million people) and Acehnese (spoken in Indonesia by around 3.5 million people), we were only able to mine fewer than one million tokens\footnote{See full list of undetermined (und) data in \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb-2} for more low-resource languages.}. 

One effective way to address this issue is to leverage translated synthetic data. For example, \citet{Wang2024MultilingualPU} translate high-quality documents (e.g., Fineweb-Edu~\citep{lozhkov2024fineweb-edu}) from English into medium-level languages such as French, German, and Spanish. Similarly, \citet{doshi-etal-2024-pretraining} adopt the Translationese dataset to extend coverage to additional low-resource languages, including Hindi, Gujarati, and Marathi.





\subsection{Tokenizer-Free Model for Open-Vocabulary Learning}
\label{sec:future_work_tokenizer_free_model}

Recent studies demonstrate that tokenizer-free language models can effectively process unseen languages and exhibit greater robustness against noise attacks compared to tokenizer-based models. 

One approach involves pixel-based language models~\citep{lotz-etal-2023-text, rust-etal-2023-pixel}, which treat text as images. This enables them to learn any script and achieve open-vocabulary language learning by exploiting visual similarities among characters and scripts through parameter sharing. In contrast, byte-level language models~\citep{evabyte, kallini2024mrt5, xue-etal-2022-byt5} bypass the tokenization step entirely by directly processing the raw character or byte stream as input.

These approaches offer significant benefits for both morphologically rich languages and languages that mix multiple scripts. 
For example, languages such as Turkish, Finnish, and Hungarian are known for their complex morphological structures, while Japanese (which combines Kanji, Hiragana, and Katakana) and Hindi (which often integrates Devanagari and Latin scripts) frequently mix scripts in everyday usage.

\subsection{Efficient Continual Pre-training for Multilingual Model}
\label{sec:future_work_efficient_training}

Continual pre-training is more efficient and cost-effective than training from scratch when building new multilingual language models for target languages. 
By leveraging an existing base model, its inherent capabilities are preserved while saving computational investment. 
For instance, developers can use Sailor2 as a foundation to build more powerful models for Southeast Asian languages with their in-house data. 
Moreover, both the developing infrastructure (e.g., Sailor2 open source every details) and the model tokenizer (the most critical component for multilingual as verified by \cite{tao2024scaling}) are mature.

However, our concern is that many existing open models might be over-trained, leaving little room for further fine-tuning. 
For example, Llama3.1 was trained on 15T tokens and Qwen2.5 on 18T tokens. Although Sailor2 employs model expansion to mitigate this issue, its approach remains inefficient due to the increased computational cost associated with a larger model size.

To achieve more efficient continual pre-training, we propose exploring the following directions:
(1) Employing an over-training indicator~\citep{ouyang2024low} to guide the selection of an appropriate base model.
(2) Enhancing model plasticity~\citep{Chen2023ImprovingLP} to enable the model to absorb additional multilingual knowledge.
(3) Leveraging insights from the lottery ticket hypothesis to update only the most essential parameters~\citep{Yuan2024KSLotteryFC}.
