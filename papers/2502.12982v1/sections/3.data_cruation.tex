\section{Data Curation}

Sailor2 showcases substantial improvements in pre-training data quality over its predecessor Sailor, driven by several key factors:

\begin{enumerate}
    \item \textbf{Better data sourcing.}
    \item \textbf{Better data filtering.}
    \item \textbf{Data recall for low-resource languages.}
    \item \textbf{Swift data-mixture in multilingual training.}
\end{enumerate}

With these enhancements, we have a larger and high-quality continual pre-training corpus, expanding from \textbf{150 Billon} SEA tokens in Sailor~\citep{dou-etal-2024-sailor} to \textbf{400 Billion} SEA tokens, covering \textbf{13 SEA languages} as listed in Table~\ref{tab:languages_family}.

\input{tables/3_language_family}

\subsection{Web Data Curation}
All the data used for Sailor2 is sourced from publicly available resources.

For the replay data employed during continual pre-training to prevent model degeneration, we select Fineweb-Pro~\citep{zhou2024programming} for English~\footnote{\url{https://huggingface.co/datasets/gair-prox/FineWeb-pro}}, Chinese-Fineweb-Edu~\citep{yu2025opencsgchinesecorpusseries} for Chinese~\footnote{\url{https://huggingface.co/datasets/opencsg/chinese-fineweb-edu}}, and Open-Web-Math-Pro~\citep{zhou2024programming} for math~\footnote{\url{https://huggingface.co/datasets/gair-prox/open-web-math-pro}}. 
Since our current focus is on general multilingual LLMs rather than coding models, we deliberately avoid including code data in the replay to safeguard multilingual performance.

For SEA language data that provide local text and knowledge, we extract content from 96 CommonCrawl snapshots spanning from summer 2013 to April 2024. Additionally, to extract high-quality and professional text, we also leverage publicly available PDFs.

For the bilingual data used to organize the code-switch dataset, we follow the Sailor~\citep{dou-etal-2024-sailor} approach by selecting Open Subtitles and open translation data\footnote{\url{https://opus.nlpl.eu/OpenSubtitles-v2018.php}}. 
Subtitles typically consist of brief, conversational sentences. To generate longer, more coherent documents, we employ a sliding window of 100 to concatenate adjacent subtitle segments.


\subsection{Synthetic Data Curation}\label{sec:synthetic_data_curation}

To address challenges in selecting high-quality datasets for low-resource languages, we leverage the NLLB-3.3B model to translate high-quality English documents into local languages. For each language, we train a FastText classifier following the approach of~\citet{li2024datacomplm} to identify high-quality text. Specifically, we generate a training set comprising 10,000 positive examples and 10,000 negative examples. The positive examples are obtained by machine-translating high-quality English datasets, 40\% from Cosmopedia~\citep{benallal2024cosmopedia}, 40\% from MADLAD~\citep{kudugunta2023madlad400}, and 20\% from UltraChat~\citep{ding2023enhancing}. The negative examples are randomly sampled from the CommonCrawl corpus for each language.
Once trained, the classifiers rank documents in the CommonCrawl corpus based on their likelihood of being a positive example. We then select the top 20\% as the high-quality subset for annealing.



\subsection{Data Cleaning}

We leverage SailCraft for comprehensive data processing consisting of six layers filtering\footnote{\url{https://github.com/sail-sg/sailcraft}}.
It employs rule-based cleaning, model-based filtering, near deduplication, exact deduplication, URL deduplication, and frequent line removal. 
During URL deduplication, we prioritize documents with more content, effectively reducing total tokens by nearly 50\%. 
As for the frequent line removal, following the Llama3~\citep{llama3} approach, we remove lines appearing more than 5 times in 10M document buckets, successfully eliminating nearly 5\% of total tokens, most of which were determined to be meaningless content.

Table~\ref{tab:disk_size_training_token} (with tokens counted using the Qwen2.5 tokenizer) presents the raw tokens used for Sailor2 training after data cleaning and deduplication. 
We subsequently downsample or upsample portions of this data to achieve a more balanced training set (see Section~\ref{sec:data_mixture}).

\begin{table}[ht]
\centering
\caption{Statistics of Raw Tokens Used in Sailor2 Continual Pre-training.}
\begin{tabular}{lccc}
\toprule
\textbf{Language} & \textbf{ISO Code} & \textbf{Disk size} & \textbf{Estimated Tokens} \\ \midrule
Vietnamese       & \texttt{vie}      & 1.9T               & 475B                    \\ 
Indonesian       & \texttt{ind}      & 1.3T               & 325B                    \\ 
Thai             & \texttt{tha}      & 242G               & 61B                     \\ 
Malay            & \texttt{zsm}      & 44G                & 11B                     \\ 
Burmese          & \texttt{mya}      & 25.8G              & 6.5B                    \\ 
Tagalog          & \texttt{tgl}      & 17.5G              & 4.4B                    \\ 
Khmer            & \texttt{khm}      & 6.9G               & 1.7B                    \\ 
Cebuano          & \texttt{ceb}     & 2.1G               & 0.5B                    \\ 
Lao              & \texttt{lao}      & 1.9G               & 0.5B                    \\ 
Javanese         & \texttt{jav}      & 1.2G               & 0.3B                    \\ 
Waray            & \texttt{war}     & 0.8G               & 0.2B                    \\ 
Sundanese        & \texttt{sun}      & 0.7G               & 0.2B                    \\ 
Ilocano          & \texttt{ilo}     & 0.2G               & 0.1B                    \\ \bottomrule
\end{tabular}
\label{tab:disk_size_training_token}
\end{table}


\subsection{Data Mixture}\label{sec:data_mixture}

We employe RegMix~\citep{liu2024regmix} to optimize the data mixture, with the primary objective of maximizing the log sum across all languages considered in stage 1.
Unlike our previous practices in Sailor~\citep{dou-etal-2024-sailor} that used 0.5B models as proxy models for data mixture, we follow RegMix and utilize 1M samll models as our proxy model, even for the scenario of continual pre-training. 
Our underlying assumption is that if a model can be trained over an extended period, the converged or equivalent data mixture should remain relatively consistent.
Please refer to RegMix for more implementation details.
