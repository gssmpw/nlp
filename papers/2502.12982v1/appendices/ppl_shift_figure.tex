% \section*{PPL Shifts in Different Languages}

\input{tables/5_1_post_training_data_distribution}

\begin{figure*}
\centering
\begin{minipage}[b]{\textwidth}
\centering
\includegraphics[width=0.95\textwidth]{figures/sec8_1_ppl_shift.pdf}
\end{minipage}
\captionof{figure}{Comparison of PPL distribution smoothed with Kernel Density Estimation (KDE). We compare Sailor2-8B, Qwen2.5-7B, Sailor1-7b and Qwen1.5-7B. Our results demonstrate that with extra 1B parameters, Sailor2-8B can preserve its English and Chinese capability, while achieving in much lower PPL in SEA languages.}
\label{fig:ppl_shift}
\end{figure*}

\begin{figure*}
\centering
\begin{minipage}[b]{\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/sec8_1_WR_compare.pdf}
\end{minipage}
\captionof{figure}{Comparison of win rate of four models based on their ChrF++ scores.
The shaded area in each cube represents the top-1 accuracy of each model across different translation directions in the Flores Plus Translation Dataset.
We observed that Sailor2 performs on par with, or even outperforms NLLB, a model optimized for translation tasks that excels at translating low-resource languages.
}
\label{fig:flores_plus_WR}
\end{figure*}

\begin{figure*}
\centering
\begin{minipage}[b]{\textwidth}
\centering
\includegraphics[width=0.95\textwidth]{figures/sec8_1_bleu_chrf_diff_vs_qwen2_5_32b.pdf}
\end{minipage}
\captionof{figure}{Comparison of BLEU and ChrF++ scores on the Flores Plus Translation Dataset across various source-target language pairs between Sailor2 20B and Qwen2.5-32B. BLEU Score Difference = Sailor2 BLEU - Qwen2.5 BLEU.}
\label{fig:flores_plus__qwen_32b}
\end{figure*}

\begin{figure*}
\centering
\begin{minipage}[b]{\textwidth}
\centering
\includegraphics[width=0.95\textwidth]{figures/sec8_1_bleu_chrf_diff_vs_qwen2_5_72b.pdf}
\end{minipage}
\captionof{figure}{Comparison of BLEU and ChrF++ scores on the Flores Plus Translation Dataset across various source-target language pairs between Sailor2 20B and Qwen2.5-72B. BLEU Score Difference = Sailor2 BLEU - Qwen2.5 BLEU.}
\label{fig:flores_plus__qwen_72b}
\end{figure*}

\begin{figure*}
\centering
\begin{minipage}[b]{\textwidth}
\centering
\includegraphics[width=0.95\textwidth]{figures/sec8_1_bleu_chrf_diff_vs_llama3_1_70b.pdf}
\end{minipage}
\captionof{figure}{Comparison of BLEU and ChrF++ scores on the Flores Plus Translation Dataset across various source-target language pairs between Sailor2 20B and Llama3.1-70B. BLEU Score Difference = Sailor2 BLEU - Llama3.1 BLEU.}
\label{fig:flores_plus__llama_70b}
\end{figure*}

\begin{figure*}
\centering
\begin{minipage}[b]{\textwidth}
\centering
\includegraphics[width=0.95\textwidth]{figures/sec8_1_bleu_chrf_diff_vs_nllb_moe_54b.pdf}
\end{minipage}
\captionof{figure}{Comparison of BLEU and ChrF++ scores on the Flores Plus Translation Dataset across various source-target language pairs between Sailor2 20B and NLLB-MoE-54B. BLEU Score Difference = Sailor2 BLEU - NLLB BLEU. We noticed NLLB failed to generate complete long Chinese sentences: \url{https://github.com/facebookresearch/fairseq/issues/5549}, and we also found many common Chinese characters and punctuations are tokenized to $<$unk$>$.}
\label{fig:flores_plus__nllb_moe_54b}
\end{figure*}

\input{tables/8_flores_chrf_qwen2_5_32b}
\input{tables/8_flores_chrf_qwen2_5_72b}
\input{tables/8_flores_chrf_llama3_1_70b}
\input{tables/8_flores_chrf_nllb_moe_54b}