\section{Evaluation}\label{sec-5-eval}


\subsection{Dataset and Settings}

%We create our own dataset which includes the \tbd{IMU, System, intercation and usage information data} needed to evaluate our framework.

\noindent\textbf{Data Collection.} Interactions were recorded using two smartphones (Pixel 6A running Android 13 and Samsung A54 running Android 14). We developed a data collection tool based on \sys to facilitate both data collection and labeling. The tool features an intuitive interface that allows participants to select a task type, after which the tool automatically begins collecting data in the background. The logger is power efficient, as it drains about 0.557\% battery during a 40-minute session with maximum sampling rate on Pixel (Around 130 Hz). We gathered usage data from 27 participants (18 male and 9 female).  All collection activities were approved by our the institutional review board (IRB) at our institution.

Each participant was instructed to interact with commonly used applications—Amazon, Gmail, Instagram, Slack, Spotify, and YouTube—to simulate everyday usage patterns (e.g., searching, changing passwords, or uploading media). Before starting, participants received a task list containing 44 actions (full list in appendix) designed to guide their interactions with these six applications. The task list (Table~\ref{tab:action_list} lists all the tasks) aimed to cover a comprehensive range of scenarios typical for smartphone users, including both benign actions (e.g., watching videos or listening to music) and potentially IPI-related actions (e.g., attempting to modify an account password). The list is randomized to enhance generalization and reduce potential bias in the collected data. Before each task, a team member sets up the appropriate label for the task, after which the participant begins completing it. Once the task is finished, the team member assigns the label for the next action, repeating this process until all tasks are completed.

We intentionally introduced flexibility into the task instructions to allow participants to complete tasks in their preferred ways to capture increased behavioral diversity. For example, we did not prescribe specific postures for using the phone or dictate the exact steps to complete a task. In the case of modifying a password, participants could either navigate through multiple settings pages or use the app's built-in assistant to reach the relevant page directly. %By providing this operational freedom, we aimed to increase behavioral diversity in the dataset. This diversity benefits both the User Identification module, which relies on training with a general user dataset, and the Behavior Classification module, which requires varied behavioral data to enhance generalization and scalability. 

  
\noindent\textbf{Dataset.} We collected the modalities listed in Table~\ref{tab:modalitycontent}. All data streams were gathered using standard Android APIs, which are compatible with Android smartphones running Android 9.0 or later, without requiring root access. As previously mentioned, this dataset includes usage data from 27 participants, including 9 pairs (e.g., couples or close friends). On average, each participant took 41.5 minutes to complete all tasks, resulting in a total of 18.7 hours of recorded data streams. %The data is organized in a tabular format, with each row representing all features collected at the same timestamp.


\noindent\textbf{Dataset processing.} To process collected data, we first apply min-max normalization to the raw data streams, scaling each feature to the range [0, 1]. Data streams are collected at the highest available sampling rate of up to 130 Hz. To analyze the impact of lower sampling rates, we downsample the data to various frequencies - [1 Hz, 2 Hz, 5 Hz, 10 Hz, 20 Hz]. Next, we apply a sliding window to extract windows of inputs into our models. To evaluate the effect of different window time spans on module performance, we experimented with varying window durations - [1s, 2s, 5s, 10s, 20s].

For the evaluation of the User Identification Module, we randomly select 95\% of the data from 22 users as the pretraining set for the AutoEncoder. The remaining 5\% is combined with data from 3 additional users, labeled as 'abuser' (positive sample, denoted as class 1), to form the fine-tuning set. Two users are designated as the owner and the abuser. From the owner, the first 5 minutes of data are added to the fine-tuning set (as described in Section~\ref{scalable}.), labeled as 'owner' (negative sample, denoted as class 0), while the owner's data after the initial 5 minutes and all of the abuser's data are used to form the test set. For the evaluation of the Behavior Classification Module, we use a leave-one-user-out scheme, where data from 27 users serves as the training set, and the remaining user is used as the test set. This approach assesses whether the learned patterns can generalize to classify the behaviors of unseen users. %For end-to-end analysis, we modify the Behavior Classification Module's training and test set split from 27:1 to 26:2, excluding and discarding the first 5 minutes of data from the test set. This adjustment aligns the test data size with that of the User Identification Module for consistency.


\noindent\textbf{Evaluation Settings.} We train and evaluate the models on a Linux server running on Ubuntu 22.04.5 LTS with an NVIDIA L40 GPU, using Tensorflow 2.17.0 and Python 3.12.7. 

For all results, we used K-fold validation to consistency of our results. This involved setting up multiple permutations of pretraining users, tuning users, and test user combinations, and averaging the results across these permutations. Specifically, for the User Identification Module, we tested 12 combinations of owner-abuser pairs: 9 involving the previously mentioned participant pairs (e.g., couples or close friends) and 3 involving strangers to increase test diversity. For the Behavior Classification Module, we conducted 12 iterations of leave-one-user-out evaluations and averaged the results.

The AutoEncoder for user identification is pretrained on a self-supervised reconstruction task for a maximum of 100 epochs with a batch size of 512, using the Adam optimizer and a learning rate of 1e-3. Training incorporates early stopping with a patience of 5 epochs and a minimum improvement threshold of 0.0001. Mean Squared Error (MSE) is used as the loss function to reconstruct the input data. Similarly, the Behavior Classification model is trained for 50 epochs, batch size = 512, Adam optimizer, learning rate = 1e-3, and is optimized with Categorical Crossentropy as the loss function.

\noindent\textbf{Evaluation Metrics.} The primary metrics used for evaluation include F1 score, recall, precision, false positive rate (FPR), false negative rate (FNR), and accuracy. F1 score balances precision and recall, recall measures the proportion of true positives correctly identified, and precision evaluates the reliability of positive predictions. False positive rate (FPR) is particularly critical in our context as it indicates the proportion of negative samples incorrectly classified as positive. A high FPR could lead to false alarms, which may cause unnecessary interventions or distress in IPV detection scenarios. FNR measures the rate of missed detections, while accuracy represents the overall proportion of correctly classified behaviors. %\textcolor{blue}{Stephen: Perhaps need to add equations, not entirely sure.}


\subsection{Effectiveness of User Identification}\label{Level 1 eval}

%\textcolor{blue}{Incorporate ablations where you try out different types of configurations and layers (e.g., swap out the LSTM for something else). Also include baseline comparisons against other methods.}

\begin{table}[htb]
        \centering\caption{F1 scores across different sampling rates and window time spans for Module-1: User Identification.}
        \label{tab:mod1f1}
        \begin{tabular}{cr|lllll|}
        \cline{3-7}
        \multicolumn{1}{l}{}                                      & \multicolumn{1}{l|}{} & \multicolumn{5}{c|}{\textbf{Window time span}}                                                                                                 \\ \cline{3-7} 
        \multicolumn{1}{l}{}                                      & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\textbf{1sec}} & \multicolumn{1}{l|}{\textbf{2sec}} & \multicolumn{1}{l|}{\textbf{5sec}} & \multicolumn{1}{l|}{\textbf{10sec}} & \textbf{20sec} \\ \hline
        \multicolumn{1}{|c|}{\multirow{5}{*}{\rotatebox{90}{\textbf{Frequency}}}} & \textbf{1Hz}                     & \multicolumn{1}{l|}{0.858}      & \multicolumn{1}{l|}{0.925}      & \multicolumn{1}{l|}{0.969}      & \multicolumn{1}{l|}{0.922}       & 0.917       \\ \cline{2-7} 
        \multicolumn{1}{|c|}{}                                    & \textbf{2Hz}                      & \multicolumn{1}{l|}{0.900}      & \multicolumn{1}{l|}{0.939}      & \multicolumn{1}{l|}{0.968}      & \multicolumn{1}{l|}{0.974}       & 0.944       \\ \cline{2-7} 
        \multicolumn{1}{|c|}{}                                    & \textbf{5Hz}              & \multicolumn{1}{l|}{0.980}      & \multicolumn{1}{l|}{0.973}      & \multicolumn{1}{l|}{0.962}      & \multicolumn{1}{l|}{0.975}       & 0.978       \\ \cline{2-7} 
        \multicolumn{1}{|c|}{}                                    & \textbf{10Hz}                     & \multicolumn{1}{l|}{0.981}      & \multicolumn{1}{l|}{0.980}      & \multicolumn{1}{l|}{0.965}      & \multicolumn{1}{l|}{0.961}       & 0.954       \\ \cline{2-7} 
        \multicolumn{1}{|c|}{}                                    & \textbf{20Hz}                     & \multicolumn{1}{l|}{0.978}      & \multicolumn{1}{l|}{0.980}      & \multicolumn{1}{l|}{0.980}      & \multicolumn{1}{l|}{0.976}       & 0.976       \\ \hline
        \end{tabular}
        \end{table}

\noindent\textbf{F1 score across varying sampling rates and window time spans} We trained, fine-tuned, and evaluated our owner identification module with different data sampling rates and input window sizes. Table~\ref{tab:mod1f1} shows the F1 score for a variety of configurations, which we found is highest (F1: 0.981) using a sampling rate of 10 Hz and a window duration of 1 second. Across all configurations, even with lower sampling rates and shorter window sizes, the F1 scores remain consistently high (>~0.85), demonstrating robustness and requiring less computation to process. 

Interestingly, we observe a plateau or slight performance drop with certain configurations, such as a 5 Hz sampling rate and a 5-second window size. We hypothesize that while a larger window size and sampling provides more information, it requires a larger and more expressive model to generalize well. Our model has the smallest number of parameters, compared to existing authentication works, which limits its capability of generalizing to higher dimensional inputs. Despite these limitations, our results suggest that a highly expressive model is not strictly necessary. The best configuration performs admirably even at moderately low sampling rates, making the system computationally efficient and power-friendly. Additionally, the stability of F1 scores across varying configurations highlights the model's adaptability, offering flexibility in deployment scenarios where power and processing constraints are critical.

We opt for the best configuration, i.e., a sampling rate of 10 Hz with a window time span of 1 second. Unless explicitly stated otherwise, all subsequent evaluations of the user identification module are based on this optimal configuration.

\begin{comment}
As shown in Figure \ref{fig:f1score} and \ref{fig:auc}, both F1 score and AUC are highest (F1 score: 0.93 and AUC: 0.96)
     when we use the model with sampling rate of 1 and 
    one second window, namely a single timestamp data analysis. Noticeably, for models running at frequencies of {1, 2} Hz, and with window size of {1, 2} seconds,
    we can get the satisfactory F1 scores and AUCs. This suggests that our model is capable of authenticating user
    in using low sampling rate and short window, which is helpful in reducing the computation time and workload.
     Generally, as the frequency and window size get larger, there is a drop
    in both F1 score and AUC. This may be due to the underfitting of the model since we adopt a relatively computationally-
    friendly model and only trained for \Le~epochs for quick training, so when the complexity of the input data increases, it would
    be more difficult for the model to learn the legitimate user's behavior pattern.
\end{comment}
    
%them using different frequencies and timespan to 
%understand the system performance. F1 score and Area Under Curve (AUC) are used as the metrics to determine the
%accuracy of each model configuration. We downsampled and generated time-series dataset
%for each pair configutation of fixed frequency and window size, which was then used for training
%and testing. For each configuration, we trained the model under each configuration for 10 epochs, with a batch size of \textcolor{red}{\Le (TBC...)}. 
%We calculate the AUC for each configuration and use the test set to determine the threshold to get the f1 score.

\begin{comment}
    As shown in Figure \ref{fig:f1score} and \ref{fig:auc}, both F1 score and AUC are highest (F1 score: 0.93 and AUC: 0.96)
     when we use the model with sampling rate of 1 and 
    one second window, namely a single timestamp data analysis. Noticeably, for models running at frequencies of {1, 2} Hz, and with window size of {1, 2} seconds,
    we can get the satisfactory F1 scores and AUCs. This suggests that our model is capable of authenticating user
    in using low sampling rate and short window, which is helpful in reducing the computation time and workload.
     Generally, as the frequency and window size get larger, there is a drop
    in both F1 score and AUC. This may be due to the underfitting of the model since we adopt a relatively computationally-
    friendly model and only trained for \Le~epochs for quick training, so when the complexity of the input data increases, it would
    be more difficult for the model to learn the legitimate user's behavior pattern.
\end{comment}


    

        \begin{table}[htbp!]
            \centering\caption{Comparison between \sys and baseline models for Module-1: User Identification.}
            \label{tab:baseline}
                \begin{tabular}{cccc}
                \toprule
                \textbf{Metrics}      & \textbf{AuthentiSense} & \textbf{KedyAuth} & \textbf{Ours} \\
                \midrule
                \textbf{F1 score}     & 0.922         & 0.875     & 0.981   \\
                \textbf{FPR}          & 0.081         & 0.331     & 0.052         \\
                \textbf{FNR}          & 0.084         & 0.007     & 0.001         \\
                \textbf{Model size}   & 5,373KB                & 1,186KB           & 214KB         \\
                \textbf{Parameter \#} & 685,221                & 268,843           & 28,130       \\
                \bottomrule
                \end{tabular}
                \end{table}


    \noindent\textbf{Comparison with baseline frameworks.} We compared our owner identification framework with KedyAuth~\cite{huh2023long} and AuthentiSense~\cite{fereidooni2023authentisense}, two state-of-art user authentication frameworks for mobile devices, as shown in Table~\ref{tab:baseline}. We implemented and trained both methods using the same datasets and setting as \sys. Shown in Table~\ref{tab:baseline}, We see that \sys's owner identification method outperforms both baselines, with a higher F1 score and lower FPR/FNR, with a model size that is an order of magnitude smaller.

    Compared to AuthentiSense, \sys achieves a 6.4\% improvement in F1 score. AuthentiSense employs a larger CNN-based siamese network trained using the triplet mining technique, which is highly demanding in terms of training data and prone to overfitting or inefficiencies. In contrast, \sys benefits from its much smaller model size (25$\times$ smaller) and a more direct and efficient training approach. By leveraging unsupervised training with an AutoEncoder, \sys learns feature representations directly from encoding and reconstructing raw data traces, leading to better generalization despite using significantly fewer parameters.

    Similarly, \sys outperforms KedyAuth, which also uses an unsupervised AutoEncoder and an SVM classifier. \sys achieves a 12.2\% improvement in F1 score and a 6$\times$ reduction in FPR, which is critical in IPV detection scenarios. KedyAuth's CNN-based model is optimized for high-frequency data at 100 Hz but struggles to effectively handle lower-frequency data. In contrast, \sys leverages a multi-head LSTM AutoEncoder, which excels at processing sequential data and extracting richer patterns from lower-frequency signals. Additionally, \sys incorporates a post-processing technique (Section~\ref{scalable}) that further smooths and stabilizes the results, reducing the FPR to just 5.2\%, which is significantly lower than KedyAuth's 33.1\%.
    
%\noindent\textbf{\tbd{Analyze couple and non-couple similarity.}} Shown in Table \ref{fig:effect}



            \begin{figure}[htbp!]
                \centering\includegraphics[width=1\columnwidth]{figure/effective_pretrain.png}%
            \caption{Impact of pretraining and fine-tuning scheme.}\label{fig:pretrain_effect}
            \end{figure}

    \noindent\textbf{Effects of pretraining and fine-tuning.} Here, we analyze the impact of pretraining and fine-tuning. As shown in Figure~\ref{fig:pretrain_effect}, we vary the number of samples used to fine-tune the base model to the phone's owner (x-axis), which is collected from the user during a short one-time 5-minute second calibration phase when the user initially installs \sys. We also ran experiments, with and without pretraining. Finally, we also looked at different selection schemes for choosing the windows used during fine-tuning. Ultimately, \sys adopts a random selection scheme (e.g., choose $n$ random windows of data provided by the user), which observed had the highest performance. We compared against selecting fine-tuning windows based on \textit{chronological time} and \textit{similarity}. 
    
    When selecting based on \textit{chronological time}, we simply use the first $n$ consecutive windows provided by the user. We believe that this scheme performed worse than random selection because the windows collected at a specific time is likely less diverse than randomly selecting windows across the entire calibration session, where users are likely performing a variety of different actions throughout.
    
    When selecting based on \textit{similarity}, we select windows from the owner and from other users that are similar to each other (measured by cosine similarity). The intuition is that it is more difficult to distinguish inputs that are more aligned, so incorporating them during the fine-tuning phase could potentially help the model distinguish these harder cases. To achieve a balance, we select windows in a specific ratio: 30\% hard (most similar), 50\% mid (moderately similar), and 20\% easy (least similar). However, these cases are likely not completely indicative of the general behavior of the owner and would require more typical examples to generalize well. Randomly sampling provides a high chance of selecting diverse samples that both embody a user's typical behavior and samples that may be difficult to distinguish from other users.

    As shown in Figure~\ref{fig:pretrain_effect}, the peak F1 score is achieved with our random selection method using 12 windows for fine-tuning and remains consistently high as the number of windows increases. In comparison, similarity-based selection exhibits a similar trend as the random method but plateaus at a lower F1 score. Furthermore, it shows instability as the training set size changes, fluctuating from an F1 score of 0.960 with 20 windows to 0.902 with 24 windows, demonstrating its less stable performance compared to random sampling and highlighting its difficulty in finding diverse yet representative windows for adaptation. Similarly, the time-based method, constrained by its lack of diversity, consistently results in lower F1 scores across different training window sizes.


        \begin{figure}[htbp!]
            \centering\includegraphics[width=1\columnwidth]{figure/mod2acc.png}%
        \caption{Top-k accuracy across granularity levels for Module-2: IPI behavior classification.}\label{fig:mod2acc}
        \end{figure}
        
\subsection{Effectiveness of Behavior Detection}
%\takeaway{Top-1 Accuracy, Top-2 Accuracy, F1-Score. Analysis of ambiguity and the role of Top-2 and Top-3 accuracy in addressing it. Also Incorporate ablations where you try out different types of configurations and layers (e.g., swap out the LSTM for something else).}

    \noindent\textbf{Classification accuracy.} Beyond performing user identification, \sys detects \ipvact at the three granularity levels according to our proposed taxonomy (Table~\ref{tab:actionsubaction}). The performance is summarized in Figure~\ref{fig:mod2acc}, where we report the top-k performance for k = 1, 2, 3, and 5 (e.g., the ground truth category, action, or subaction is in the list of top-k most probable behaviors). In the case of top-1, or logging only the most probable behavior, \sys performs significantly better than random guessing (F-1 score of around 0.6 to 0.7). This far from perfect performance illustrates the difficulty of distinguishing \ipvact, using only a limited set of data streams, constrained by visibility to abusers that may access the phone. 
    
    However, by slightly broadening our logging to the top-k, the rate we successfully ``detect'' or report the true behavior increases drastically. For instance, reporting the top-5 most probable \textit{subactions} yielded an F-1 score of 0.895 and reporting the top-3 \textit{actions} boosted the F-1 score to 0.923. These results highlight how certain \ipvact may exhibit similar patterns across multiple modalities, especially at finer-grained levels (e.g., subactions). This reinforces the need for systems like \sys to account for ambiguity by ranking behaviors, rather than solely relying on single-point predictions, which still offers investigators or automated systems actionable insights.%, even when exact predictions are challenging.

    %This suggests that while many of the behaviors exhibit similar patterns, that make it difficult to definitively pinpoint the exact behavior, \sys is expressive enough to rank the true behavior highly.

%In these scenarios, \sys logs the top k most likely behaviors exhibited by the non-owner. As such, as long as the true behavior falls within the top-k, \sys successfully logs the activity.

% Raw score vs. top-2, top-3, top-5, etc.. accuracy

    
        
    
    
        


        %\begin{figure}[htbp!]
           % \centering\includegraphics[width=0.9\columnwidth]{figure/placeholder-top2.png}%
        %\caption{Accuracy in 3 levels of labels}\label{fig: L2top2}
        %\end{figure}
    
    
        %\noindent\textbf{\tbd{Ambiguity in interaction behaviors.}} Ambiguity and case analysis in \ref{fig: L2top2}
    
    
    
        \begin{table}[htbp!]
                \centering\caption{\sys end-to-end performance at Action (9-class) level. }
                        \label{tab:e2e}
        \begin{tabular}{lccc}
        \toprule
        \multicolumn{1}{c}{\textbf{Accuracy}} & \textbf{F1} & \textbf{Precision}  & \textbf{FPR} \\
        \midrule
        \textbf{TOP-3}                    & 0.981 & 0.966  & 0.040  \\
        \textbf{TOP-3 (No Post Processing)} & 0.978 & 0.961  & 0.044  \\
        \textbf{TOP-2}                    & 0.973 & 0.952  & 0.055  \\
        \textbf{TOP-2 (No Post Processing)} & 0.969 & 0.944  & 0.063  \\
        \textbf{TOP-1}                    & 0.956 & 0.920  & 0.087  \\
        \textbf{TOP-1 (No Post Processing)} & 0.951 & 0.911  & 0.095  \\
        \bottomrule
        \end{tabular}
        \end{table}

\subsection{End to End System Performance}
%\takeaway{Combined Accuracy, Precision, Recall, Case Scenarios. Case studies showing combinations of Level 1 and Level 2 outputs (e.g., Genuine + Alter Account → Safe, Imposter + Alter Account → IPV Risk)}

    The evaluation of the end-to-end (e2e) system focuses on its ability to combine User Identification Module and Behavior Classification Module to produce final predictions aligned with the goals of IPI detection. The labeling process for e2e performance is derived by integrating the outputs of these two levels. Specifically, User Identification Module determines whether the user is the owner or abuser, while Behavior Classification Module classifies the corresponding behavior into categories such as benign or IPI-related actions. By combining these outputs, the system assigns a single label to each instance that encapsulates both the identity of the user and the behavioral context.

    For example, behaviors classified as "Alter Account" are considered safe when performed by the verified owner but are flagged as IPI risks when executed by an abuser. Similarly, benign behaviors by an intruder, such as general browsing, are marked as non-risk. This labeling framework ensures that the system captures the interplay between identity and behavior, allowing for nuanced and context-aware decision-making that is not possible by analyzing just one or the other.

    The rationale behind this labeling approach lies in its structured integration of user identity and behavioral context, achieved through explicit rule-based definitions. As mentioned, FPR is crucially important due to the nature of IPI events. Trivial false alarms could lead to significant negative consequences for victims, such as undue distress, mistrust in the system, or unintentional exposure to their abusers. So it is important to handle interventions with care to avoid exacerbating emotional distress, particularly for at-risk populations like IPV survivors. The e2e labeling framework minimizes such risks by ensuring that both user identity and behavioral context are integrated into the decision-making process.

    Table~\ref{tab:e2e} shows the e2e performance of \sys at Action level (5-class) behaviors in identifying both the correct user (owner vs. non-owner) and the IPI behavior. The best performance is achieved through top-3 behavior classification. With 0.981 F1 score and 0.040 FPR, the e2e system demonstrates great reliability when allowed to consider the top three ranked predictions, effectively minimizing false alarms. Narrowing the prediction scope to top-2 provides more confident results, with only a marginal trade-off in F1 score, precision, and FPR. Similarly, top-1 predictions achieve the highest certainty but at the cost of slightly higher FPR, underscoring the inherent trade-off between prediction completeness and certainty. This balance is especially critical in high-risk scenarios, where reducing false positives is paramount to avoid unnecessary distress or harm to victims, yet ensuring accurate identification of IPI behaviors remains equally important.

    Additionally, the application of post-processing consistently enhances system performance across all configurations. For instance, in the top-1 prediction, post-processing reduces FPR from 0.095 to 0.087 and improves F1 from 0.951 to 0.956, reinforcing the importance of refinement steps to mitigate prediction errors. These results validate the effectiveness of \sys in integrating user identity and behavioral context, demonstrating its capability to reliably classify behaviors at the action level.


\subsection{Ablation Study}
\label{subsec:ablation_study}

            \begin{figure}[htbp!]
                \centering\includegraphics[width=0.9\columnwidth]{figure/mod1ablation.png}%
            \caption{Module-1: User Identification ablation study.}\label{fig:mod1ablation}
            \end{figure}
    
    
\noindent\textbf{Module-1: user identification ablation.} We evaluate a range of classifiers to assess their performance for user identification. For a comprehensive analysis, we include traditional machine learning approaches, such as Support Vector Machine (SVM) and Random Forest (RF), as well as deep learning-based methods like Long Short-Term Memory (LSTM) and Dense Neural Networks. Additionally, we include XGBoost, a widely used ensemble learning algorithm. Specifically, we set the SVM with an RBF kernel, Random Forest with 100 estimators, and use a multi-head LSTM (each with 6 units) to mirror the AutoEncoder architecture. For the dense network, we adopt two dense layers connected to the AutoEncoder output for prediction. The XGBoost model is configured with a learning rate of 0.1 (commonly used to balance convergence speed and performance), a maximum tree depth of 6, 100 estimators, and the logloss evaluation metric to optimize classification performance.

Figure~\ref{fig:mod1ablation} shows the performance of \sys with different backbones for the classifier. We adopt SVM as the backbone of the classifier, since it has the highest performance, with 0.98 F1 score. Among the deep learning-based models, the Dense classifier demonstrates an F1 score of 0.969, which is slightly lower than SVM but higher than the LSTM classifier. Out of all architectures, SVM is the least complex, least expressive, but less susceptible to overfitting. It's high performance suggests that the encoder could extract highly relevant features, with little data, to distinguish between the phone owner and other users, which is helpful in our few-shot fine-tuning process. Other more expressive methods, such as Dense MLPs and LSTM, typically require more data, memory, and compute resources.

            \begin{table}[htbp!]
            \small
            \centering\caption{Module-2: Behavior Detection ablation study.}
                        \label{tab:mod2ablation}
            \begin{tabular}{ccccc}
            \toprule
            \textbf{Accuracy} & \textbf{\begin{tabular}[c]{@{}c@{}}LSTM\\ -\\ CNN\end{tabular}} & \textbf{LSTM} & \textbf{CNN} & \textbf{Transformer} \\
            \toprule
            \multicolumn{5}{c}{\textbf{Category (5-class)}}                 \\
            \midrule
            \textbf{Top3} & 0.963       & 0.963       & {\ul 0.965} & 0.960 \\
            \textbf{Top2} & 0.887       & {\ul 0.888} & 0.884       & 0.850 \\
            \textbf{Top1} & {\ul 0.674} & 0.666       & 0.667       & 0.630 \\
            \midrule
            \multicolumn{5}{c}{\textbf{Action (9-class)}}                   \\
            \midrule
            \textbf{Top3} & {\ul 0.923} & 0.904       & 0.917       & 0.882 \\
            \textbf{Top2} & {\ul 0.826} & 0.815       & 0.819       & 0.765 \\
            \textbf{Top1} & {\ul 0.642} & 0.638       & 0.623       & 0.591 \\
            \midrule
            \multicolumn{5}{c}{\textbf{Subaction   (28-class)}}             \\
            \midrule
            \textbf{Top3} & {\ul 0.813} & 0.783       & 0.800       & 0.769 \\
            \textbf{Top2} & {\ul 0.728} & 0.695       & 0.715       & 0.689 \\
            \textbf{Top1} & {\ul 0.567} & 0.545       & 0.558       & 0.527 \\
            \bottomrule
            \end{tabular}
            \end{table}
    
    
    \noindent\textbf{Module-2: behavior classification ablation.} Table~\ref{tab:mod2ablation} shows the performance of \sys in classifying \ipvact with different backbones architectures. Our hybrid LSTM-CNN architecture, that extracts both temporal (LSTM) and spatial (CNN) features, significantly outperforms single type architectures. Across all scenarios this hybrid architecture also outperforms a transformer architecture, which generally outperforms other architectures when data and model size is abundant. However, in our scenario where models are small (only hundreds of thousands of parameters), transformer performance typically suffers, which matches our observations. While LSTM-CNN did not outperform a purely CNN or LSTM approach in determining the category (5-class) of behavior, the performance is essentially equal, with less than $1\%$ difference.

            \begin{table}[htbp!]
            \small
            \centering\caption{Data modality ablation study for Module-1: User Identification.}
                        \label{tab:mod1modality}
            \begin{tabular}{ccccccc}
            \toprule
                              & \textbf{\begin{tabular}[c]{@{}c@{}}IMU\\ +\\ SYS\end{tabular}} & \textbf{IMU} & \textbf{SYS} & \textbf{INT} & \textbf{APP} & \textbf{ALL} \\
            \midrule
            \textbf{F1 score} & {\ul 0.974}      & 0.968        & 0.675        & 0.079          & 0.561        & 0.938        \\
            \bottomrule
            \end{tabular}
            \end{table}
    
            \begin{table}[htbp!]
            \small
            \centering\caption{Data modality ablation study for Module-2: Behavior Detection.}
                        \label{tab:mod2modality}
            \begin{tabular}{ccccccc}
            \toprule
            \textbf{Accuracy} & \textbf{\begin{tabular}[c]{@{}c@{}}INT\\ +\\ APP\end{tabular}} & \textbf{INT} & \textbf{APP} & \textbf{IMU} & \textbf{SYS} & \textbf{ALL} \\
            \toprule
            \multicolumn{7}{c}{\textbf{Category (5-class)}}                           \\
            \midrule
            \textbf{Top3} & {\ul 0.963} & {\ul 0.963} & 0.939 & 0.824 & 0.87  & 0.93  \\
            \textbf{Top2} & {\ul 0.887} & 0.881       & 0.79  & 0.645 & 0.688 & 0.806 \\
            \textbf{Top1} & {\ul 0.674} & 0.655       & 0.538 & 0.396 & 0.386 & 0.558 \\
            \midrule
            \multicolumn{7}{c}{\textbf{Action (9-class)}}                             \\
            \midrule
            \textbf{Top3} & {\ul 0.923} & 0.898       & 0.813 & 0.57  & 0.615 & 0.811 \\
            \textbf{Top2} & {\ul 0.826} & 0.787       & 0.666 & 0.436 & 0.483 & 0.681 \\
            \textbf{Top1} & {\ul 0.642} & 0.607       & 0.479 & 0.269 & 0.298 & 0.486 \\
            \midrule
            \multicolumn{7}{c}{\textbf{Subaction   (28-class)}}                       \\
            \midrule
            \textbf{Top3} & {\ul 0.813} & 0.786       & 0.629 & 0.248 & 0.316 & 0.657 \\
            \textbf{Top2} & {\ul 0.728} & 0.697       & 0.495 & 0.173 & 0.252 & 0.556 \\
            \textbf{Top1} & {\ul 0.567} & 0.533       & 0.35  & 0.101 & 0.148 & 0.392 \\
            \bottomrule
            \end{tabular}
            \end{table}
    
    
    \noindent\textbf{Modality ablations.} Tables~\ref{tab:mod1modality} and~\ref{tab:mod2modality} show the performance of user identification and behavior classification, varying the data streams used as input. We see that incorporating all streams of data does not yield the best performance and carefully selecting only the most relevant yields more promising results. Because leveraging 1) IMU and SYS data streams for user identification and 2) INT + APP data streams for behavior classification yielded the most promising performance, we adopt these two orthogonal data streams into their respective modules for \sys. 
    
            
    
    
    
    %\takeaway{IMU,SYS for level1, Interaction, APP usage for level2, which fulfills e2e balanced performance}
                        
                
            
    
    % \subsection{Power and Memory Analysis}\label{sec-5-sub-power}
    %     \begin{figure}[htbp]
    %         \centering\includegraphics[width=0.9\columnwidth]{figure/chart2.pdf}%
    %     \caption{Power Analysis}\label{fig:powerana}
    %     \end{figure}
    
    %     We next demonstrate the result of power analysis of its implementation on device. To understand the power consumpiton situation of the system, we conducted several running test on the implemented application with different settings, and at the same time we run YouTube playing videos in the foreground for comparison. For each setting, the test is conducted for 3 times, each time lasting for 5 minutes, and we acquire the consumption data from Andoird built-in analysis in Settings$\rightarrow$Battery$\rightarrow$Battery usage.
    
    %     As shown in Figure \ref{fig:powerana}, compared to YouTube, \sys~drains less battery with fully functioning at \textcolor{red}{50 Hz}, showing its usability on mobile device. It is noted that as the sampling frequency goes down, there is a significant decrease in the power consumption, from 16.75 mAh at \textcolor{red}{50Hz} to 7.85 mAh at 8Hz. This is crucial since it is feasible to set the system at a lower frequency mode to reach the efficiency between the power consumption and the accuracy of the task.
    
    %     Moreover, in each scenario, \sys~drains less batterylife compared with YouTube for the same running time. Even for scenario in which our system has the highest power consumption, it is still exceeded by YouTube. This proves our application's usability since we adopt lower sampling frequency in pratical use, which consumes obviously less power compared to the most-energy-intensive scenario.
