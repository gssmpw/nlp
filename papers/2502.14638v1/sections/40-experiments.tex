\section{How Well Does \modelname Reason Image Locations?}

We compare \modelname against prior state-of-the-art image \geoloc models and other baseline approaches (Section~\ref{sec:main_experiments}), ablate each module to evaluate their contributions (Section~\ref{sec:ablation_study}), and provide qualitative examples to highlight successful and challenging cases (Section~\ref{sec:qualitative_analysis}). 


\subsection{Experimental Setup}

\textbf{Implementation.} We use three open-source models in \modelname: \mbox{MiniCPM-V~\cite{yao2024minicpm}}, LLaVA~\cite{liu2024llavanext}, and \mbox{Qwen2-VL}~\cite{Qwen2VL}. These models serve as \textsc{vlm}s for \macname, \micname, and the \guessname components within the \modelname framework. (1)~For \macname, Low-Rank Adaptation (LoRA)~\cite{hu2021lora} fine-tunes models using \dataname. We use \mbox{\textit{minicpm-v-2.6}}, \mbox{\textit{llava-1.6-vicuna-7b}}, and \mbox{\textit{qwen2-vl-7b}} for their advanced performance and mid-range size, which align with our cost constraints.
(2)~For \micname, we select the top three cropped clues as the basis for generation (\textit{e.g.}, if multiple houses are cropped, only will the three with the highest similarity be analyzed). \textsc{clip}~\cite{radford2021learning} encodes guidebook images and query images, retrieving guidebook data by the Euclidean distance~\(d\) between image embeddings (\textsc{faiss}~\cite{johnson2019billion}), returning associated text if \(d\) is below a threshold~$d_t$ (set to 30). 
(3)~We prompt the \guessname to predict locations at the coordinates level. Training hyperparameters, model configurations, and prompts are in Appendix~\ref{apd:implementations}.


\begin{table*}[t]
\centering
\small
\begin{tabular}{l r r r r r r r}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Continent}\\ $2{,}500$ km\end{tabular}} & 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Country}\\ $750$ km\end{tabular}} & 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Region}\\ $200$ km\end{tabular}} & 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{City}\\ $25$ km\end{tabular}} & 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Street}\\ $1$ km\end{tabular}} & 
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance$\downarrow$\end{tabular}}} & 
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Score$\uparrow$\end{tabular}}}\\
\midrule
 $G^3$             & 50.9 & 14.6 &  2.3 &  0.1 & 0.0 & 4,341 & 1,304 \\
GeoCLIP           & 78.2 & 46.5 & 17.1 &  3.5 & 0.4 & 2,099 & 2,613 \\
StreetCLIP        & 79.4 & 43.4 & 13.4 &  1.7 & 0.3 & 2,060 & 2,543 \\
\midrule
MiniCPM-V         & 27.1 &  15.9 &  6.7 &  1.6 & 0.1 & 7,320 &   909 \\
 LLaVA             & 43.9 & 23.1 &  7.0 & 1.2 & 0.0 & 5,096 & 1,418 \\
Qwen2-VL          & 89.4 & 66.7 & 31.8 &  6.1 & 0.1 & 1,124 & 3,344 \\
\midrule
 \textit{\textbf{\modelname}} &&&&&&& \\
~~ - MiniCPM-V    & 71.5 & 44.1 & 16.9 &  3.5 & 0.3 & 2,956 & 2,413 \\
 ~~ - LLaVA        & 74.7  & 39.4 & 12.0 &  1.9 & 0.3 & 2,243 & 2,354 \\
~~ - Qwen2-VL     & \textbf{91.1} & \textbf{66.9} & \textbf{31.9} &  \textbf{6.7} & \textbf{0.7} & \textbf{965} & \textbf{3,389} \\
\bottomrule
\end{tabular}
\caption{
Accuracy and scores on GWS5k. The data from Continent to Street represents the accuracy (\%) at each level. The three sections are \geoloc models, \textsc{vlm}s, and \modelname. \textbf{Bold} font indicates the best performance. \modelname (\mbox{Qwen2-VL}) achieves the highest accuracy across all metrics. 
}
\label{tab:main_results}
\end{table*}


\textbf{Baselines.}
We compare \modelname with two baselines: (1) \textit{\geoloccap Models}: we select top-performing open-source models from prior research in image \geoloc: $G^3$~\cite{luo2022g}, GeoCLIP~\cite{vivanco2024geoclip}, and StreetCLIP~\cite{haas2023learning}. (2) \textit{Vision Language Models}: we select vanilla MiniCPM-V, LLaVA, Qwen2-VL as baselines, consistent with the backbone models used in \modelname. The prompts for these \textsc{vlm} baselines are identical to those in \modelname but lack analyses. We do not include commercial closed-source models (as discussed in Limitations).

\textbf{Dataset and Metrics.}
Following previous work~\cite{hays2008im2gps, astruc2024openstreetview, haas2024pigeon}, we evaluate our framework on two public datasets, including GWS5K sampled from GWS15K~\cite{clark2023we} due to cost constraints, and Im2GPS3k~\cite{hays2008im2gps}. We first computed the haversine distance between predicted and ground truth coordinates. For models limited to city level outputs, we use the coordinates of the predicted city as their predictions. Next, we evaluated the prediction accuracy---the percentage of guesses that fall within a distance threshold from the correct location---at five geographic levels: Street ($1$ km), City ($25$ km), Region ($200$ km), Country ($750$ km), and Continent ($2{,}500$ km). In addition, we calculated the average error distance and GeoGuessr Score, a metric from the original GeoGuessr game that quantifies guess accuracy, with a scoring range of $0$ to $5000$. Details about metric computation are in Appendix~\ref{apd:metrics}.

\subsection{Main Experiments}
\label{sec:main_experiments}


\begin{table}[t!]
\centering
\small
\begin{tabular}{l c c c }
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{\textsc{rouge} F1}} \\
 & \textbf{R1} & \textbf{R2} & \textbf{RL} \\
\midrule
 \textbf{\macname}~(MiniCPM-V)    & 51.0 & \textbf{14.8} & \textbf{24.6} \\
MiniCPM-V         & 46.4 & 12.6 & 22.1 \\
 \textbf{\macname}~(LLaVA)  & 49.8 & 13.9 & 24.0 \\
LLaVA       & 44.7 & 10.8 & 21.8 \\
 \textbf{\macname}~(Qwen2-VL)   & \textbf{51.4} & 14.6 & 24.3 \\
Qwen2-VL        & 45.2 & 12.3 & 22.1 \\
\bottomrule
\end{tabular}
\caption{\textsc{rouge} F1 scores for reasoning generated by models and humans (\%). \macname models reason more similarly to humans.}
\label{tab:rouge_f1_scores}
\end{table}


\textbf{Accuracy.} We compare \modelname with state-of-the-art image \geoloc models and Vision Language Models (GWS5k results in Table~\ref{tab:main_results}). (1)~\modelname(Qwen2\mbox{-}VL) has the highest accuracy across all metrics, beating specialized \geoloc models trained on domain-specific datasets, despite its relatively compact size of only 7 billion parameters. (2)~All \textsc{vlm}s generate effective analytical reasoning trained with only around $1{,}000$ samples and beat their vanilla models.
These findings underscore the quality of training data and the efficacy of \modelname. Similar results on Im2GPS3k are in Appendix~\ref{apd:supplementary_exps}.

\textbf{Reasoning.} We evaluate the linguistic reasoning quality generated by the model on a reserved test set of 50 games. To measure the alignment between model and human reasoning, we compute their \textsc{rouge} scores~\cite{lin2004rouge} which illustrate whether the model simulates human reasoning. \macname achieves higher \textsc{rouge} scores across all models and metrics after training (Table~\ref{tab:rouge_f1_scores}). 

We apply GPT-4o to label the granularity and accuracy (measured by exact match) of the reasoning by \modelname(Qwen2-VL) on GWS5k (Table~\ref{tab:reasoning}). \macname predicts country with an accuracy of 79.6\%, while it's challenging (3.0\%) when it makes finer-grained predictions (\textit{e.g.}, city, town, or street), as these predictions require additional information. For example, \macname outputs ``the combination of these elements point towards a city like Chaco, Argentina'' while the correct answer is ``Trelew, Argentina.'' This indicates the importance of \micname for precise predictions.

\begin{table}[t!]
\centering
\small
\begin{tabular}{l r r }
\toprule
\textbf{Level} & \textbf{Frequency} & \textbf{Accuracy} \\
\midrule
Country & 100.0 & 79.6 \\
Others & 50.7 & 3.0 \\
\bottomrule
\end{tabular}
\caption{Frequency and accuracy (exact match) of \macname. ``Others'' indicates more detailed predictions, which are challenging.}
\label{tab:reasoning}
\end{table}

\textbf{Comparison with Humans.} 
We also compare \modelname's performance against human players in fifty randomly sampled GeoGuessr games (collected in Appendix~\ref{apd:data}), focusing on common metrics for country, city, and street level predictions. Filtered human player data have time and access to additional knowledge resources (\textit{e.g.}, maps and guidebooks) for a fair comparison with \modelname. \modelname outperforms humans in overall scores (Table~\ref{tab:human_comparison}), although humans excel at finer-grained predictions by iteratively cross-referencing maps and comparing terrain and features within the game. This highlights a future direction to use non-textual features to refine map-based searches and enhance street-level accuracy of models.


\begin{table}[t!]
\centering
\small
\begin{tabular}{l r r r r}
\toprule
\textbf{Model} & \textbf{Country} & \textbf{City} & \textbf{Street} & \textbf{Score$\uparrow$} \\
\midrule
 \textit{\textbf{\modelname}} &&&& \\
~~ - MiniCPM-V & 56.0 & 18.0 & 0.0 & 2,863 \\
~~ - LLaVA & 48.0 & 14.0 & 0.0 & 2,690 \\
~~ - Qwen2-VL & \textbf{86.0} & 32.0 & 4.0 & \textbf{4,202}\\
\midrule
\textbf{Human Players} & 76.0 & \textbf{48.0} & \textbf{42.0} & 3,757\\
\bottomrule
\end{tabular}
\caption{Performance between humans and \modelname. The data from City to Street represents accuracy (\%). Our best model beats humans with a higher overall score but still struggles to achieve fine-grained accuracy.}
\label{tab:human_comparison}
\end{table}


\subsection{Ablation Study}
\label{sec:ablation_study}

To illustrate the contributions of each component in \modelname, we ablate the reasoning training, the impact of \macname, and \micname. Table~\ref{tab:ablation_results} presents the three \textsc{vlm}s' accuracy on GWS5k. In this setup, \modelname represents our framework, ``w/o training'' denotes results with the same prompt but without training on \dataname, ``w/o Macro'' and ``w/o Micro'' refer to the results without the \macname and \micname modules, respectively. 

\begin{table}[t!]
\centering
\small
\begin{tabular}{l r r r}
\toprule
\textbf{Model} & \textbf{Country} & \textbf{City} & \textbf{Street} \\
\midrule
 \textbf{\modelname}~(MiniCPM-V) & \textbf{44.1} &  \textbf{3.5} &  \textbf{0.3} \\
~~ - w/o training             &- 3.3 & - 0.4 & - 0.2 \\
 ~~ - w/o \macname                &- 10.2 & - 0.7 & - 0.0 \\
~~ - w/o \micname                & - 0.3 & - 0.3 & - 0.2 \\
 ~~ - MiniCPM-V                &- 14.9 & - 0.5 & - 0.2 \\
\midrule
 \textbf{\modelname}~(LLaVA) & 39.4 & \textbf{1.9} &  \textbf{0.3} \\
~~ - w/o training             &- 25.8 & - 1.2 & - 0.3 \\
 ~~ - w/o \macname                &- 20.2 & - 0.8 & - 0.0 \\
~~ - w/o \micname                & \textbf{+ 0.4} & - 0.2 & - 0.2 \\
 ~~ - LLaVA                    &- 16.3 & - 0.7 & - 0.3 \\
\midrule
 \textbf{\modelname}~(Qwen2-VL) & 66.9 &  \textbf{6.7} &  \textbf{0.7} \\
~~ - w/o training             & - 6.0 & - 0.9 & - 0.5 \\
 ~~ - w/o \macname                & - 4.0 & - 0.6 & - 0.2 \\
~~ - w/o \micname                & \textbf{+ 0.1} & - 0.9 & - 0.5 \\
 ~~ - Qwen2-VL                 & - 0.2 & - 0.6 & - 0.6 \\
\bottomrule
\end{tabular}
\caption{
Ablation results of \modelname on the GWS5k dataset. Each component contributes to model accuracy, with their removal leading to notable declines across Country, City, and Street levels.
% \jbgcomment{All figure captions should have takeaway,bingo}
}
\label{tab:ablation_results}
\end{table}


\textbf{Results.} (1)~Each module contributes to improving the model's accuracy. (2)~Surprisingly, when the model is prompted to zero-shot generate reasoning, it can be misleading and decrease final accuracy. This highlights the necessity of training with \dataname. (3)~\macname plays a critical role in coarse-grained localization, with improvements at the country level and decreases without it, as the reasoning in \dataname is limited to the country and city level. (4)~\micname substantially enhances fine-grained reasoning. Precise street-level localization on the GWS dataset is challenging, but the \micname narrows the scope within 1 km for images containing textual information using map searches (Table~\ref{tab:main_results}). Results on Im2GPS3k are in Appendix~\ref{apd:supplementary_exps}, which is consistent with GWS. 


\subsection{Qualitative Analysis}
\label{sec:qualitative_analysis}

This section examines how the analytical reasoning derived from images contributes to \modelname’s inference process. \modelname closely examines details within the image (\textit{e.g.}, climate, orientation, and ``Lower Mill'' in Figure~\ref{fig:cases} (top)) to determine the location. This detailed reasoning narrows down the possible range, while integration with OpenStreetMap data further aids the model in finding the restaurant, with an error distance of under $1$ meter, improving its estimate by $144$ km.

However, image elements can also mislead the model. In Figure~\ref{fig:cases} (middle), the model fixates on a shop name in the image, ``KLICK'', which can be interpreted as a German word. This leads the reasoning process astray, resulting in an incorrect localization. OpenStreetMap can also lead to false predictions when there are places with the same name, such as ``Bradesco'', a well-known Brazilian bank (Figure~\ref{fig:cases}, bottom). The reasoning makes image \geoloc models more interpretable by revealing how image elements influence decisions.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.827\linewidth]{images/cases.pdf} 
    \caption{\textbf{Top:} The model uses visual details and OpenStreetMap to accurately determine the location. \textbf{Middle:} The model is misled by linguistic elements---the shop name, resulting in an incorrect inference. \textbf{Bottom:} The model found a namesake when using OpenStreetMap.}
    \label{fig:cases}
\end{figure*}