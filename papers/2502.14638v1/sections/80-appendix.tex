\appendix

\section{Implementation Details}
\label{apd:implementations}

\subsection{Training Parameters}

We trained the \macname on Nvidia RTX 6000 Ada (48G), with CUDA 12.4, Transformers 4.45.1, and Pytorch 2.1.2.

\begin{table}[htbp]
\centering
\small
\begin{tabular}{l c}
\toprule
\textbf{Parameters} & \textbf{Value} \\
\midrule
Max Length & 2048 \\
LoRA Rank & 8 \\
LoRA Alpha & 32 \\
Optimizer & AdamW \\
Adam Beta1 & 0.9 \\
Adam Beta2 & 0.95 \\
Learning Rate & 1e-4 \\
Warmup Ratio & 0.05 \\
LR Scheduler Type & cosine \\
Batch Size & 1 \\
Weight Decay & 0.1 \\
\bottomrule
\end{tabular}
\caption{
Training parameters for \macname.
}
\label{tab:training_parameters}
\end{table}


\subsection{Other Parameters.}

For reproducibility, we also provide the parameters used in other modules and \textsc{vlm}s within our framework.

\textbf{GroundingDino.} We utilize GroundingDino to crop detailed information from the images, such as signs and houses. We observe variation in the features of images across different datasets. For instance, the GWS5k dataset focuses on street scenes, and other datasets contain considerable noise (\textit{e.g.}, animals). Consequently, to reduce noise that could potentially affect model performance, we empirically set the thresholds as follows: \textit{Box-Threshold} = 0.5 and \textit{Text-Threshold} = 0.5 for GWS5K, and \textit{Box-Threshold} = 0.8 and \textit{Text-Threshold} = 0.6 for Im2GPS3k.

\textbf{Retrieval-Augmented Generation.} We employ CLIP as the image encoder for guidebook clues, using ViT-B-32 as the vision encoder. The guidebook database is deployed with \textsc{faiss}, and similarity is calculated using Euclidean Distance. The number of most relevant retrieved images, $k$, is set to 3, with a similarity threshold of 30.

\textbf{OpenStreetMap.} We use the \textit{Nominatim Search API} to process map searches, which takes text queries, and return the most relevant results along with the place name, address, and coordinates.


\textbf{Vision Language Models.} We use Vision-Language Models in our framework for reasoning and location inference. The three models are \mbox{\textit{minicpm-v-2.6}}, \mbox{\textit{llava-1.6-vicuna-7b}}, and \mbox{\textit{qwen2-vl-7b}}. Each model is configured with a temperature of $0$ and an output length of $2048$.

\subsection{Prompts for \textsc{vlm}s.}

In Table~\ref{tab:data_prompts} and Table~\ref{tab:prompts}, we present the prompts used in \modelname for Vision Language Models. Four distinct prompts are employed: (1) the \textit{Data Processing Prompt}, which employed an answer guided reasoning generation method to prompt \textsc{vlm}s in extracting step-by-step reasoning from YouTube transcripts; (2) the \textit{\macname Prompt}, which is the same as the query in the training data, prompting \textsc{vlm}s to generate a coherent reasoning process to infer the location within an image; (3) the \textit{\micname Prompt}, which generates additional knowledge from image details, and (4) the \textit{\guessname Prompt}, which synthesizes all prior information to make a final prediction.

\begin{table}[htbp]
    \centering
    \small
    \newcolumntype{Y}{>{\arraybackslash}X}
    \begin{tabularx}{\linewidth}{Y}
    \toprule 
        \textit{\textbf{Data Processing Prompt}} \\
        <image> Given an image and the known location details (Country: {country}, Latitude: {lat}, Longitude: {lon}), and an expert's analysis of the location ({transcript}), craft a brief and cohesive reasoning path that deduces this location based on the visual clues present in the image. Begin your reasoning without revealing that you know the exact location, using a tone of exploration and inference. Carefully analyze and link observations of natural features (climate, vegetation, terrain), man-made structures (roads, buildings, signage), and distinct landmarks. Allow these observations to naturally lead you to the correct country, enhancing the accuracy of your deductions. Ensure that while the narrative seems to be guessing, it aligns with the known country, confirming the reliability of your reasoning without stating the specific coordinates. Start the reasoning without any intro, and make sure to make it brief. \\
    \bottomrule
    \end{tabularx}
    \caption{The prompts used in \modelname.}
    \label{tab:data_prompts}
\end{table}


\section{Data.}
\label{apd:data}

In this section, we present the data processing workflows and provide more detailed information on the various types of data used in the system.

\subsection{Data Processing.}

\textbf{YouTubers.} We utilized the scripts of five professional GeoGuessr players' YouTube videos as the starting data for our reasoning generation. We thank these five players for their contributions to knowledge dissemination and promotion of image \geoloc: zi8gzag, GeoWizard, GeoPeter, Geogasm, and RAINBOLT TWO.

\textbf{Data Processing.}
We used the Google Street View\footnote{https://www.google.com/streetview/} API to retrieve images for our dataset. We selected a resolution of 640×640 pixels (the maximum resolution accepted by \textsc{gsv}), a field of view (\textsc{fov}) of 90, and headings of 0, 90, 180, and 270 degrees to obtain four images. Stitching them together produces a complete street view image, providing the same amount of information that a GeoGuessr player would see.

Next, we split the videos for retrieving the transcripts or each round. After a player submits their final guess, the game reveals the distance between their guessed location and the actual coordinates, where the player can choose to either proceed to the next round or end the challenge. We use precise pixel coordinates in conjunction with \textsc{ocr} technology to detect the presence of the ``Next'' or ``End'' buttons and split the videos. We sample frames at a rate of 1/6 per second to ensure no scene is missed. Additionally, we extract the GeoGuessr Score displayed beside the button and collect human players' scores. Next, due to the noise in the data (with many informal language from players), we provide GPT-4o with the correct locations for paraphrasing and generating higher quality and more coherent data. 


\begin{table}[htbp]
    \centering
    \small
    \newcolumntype{Y}{>{\arraybackslash}X}
    \begin{tabularx}{\linewidth}{Y}
    \toprule 
        \textit{\textbf{\macname Prompt}} \\
        <image> Given an image, craft a brief and cohesive reasoning path that deduces this location based on the visual clues present in the image. Using a tone of exploration and inference. Carefully analyze and link observations of natural features (climate, vegetation, terrain), man-made structures (roads, buildings, signage), and distinct landmarks. Allow these observations to naturally lead you to the correct country, enhancing the accuracy of your deductions. Start the reasoning without any intro, and make sure to make it brief. \\
        \midrule
        \textit{\textbf{\micname Prompt}} \\
        <image> Analyze the \{item\} images to determine the region with the highest likelihood of finding this type of \{item\}. For each image, provide only the core reasoning in one sentence. Don't say you can't determine, try your best as it's a \geoloc game \\
        \midrule
        \textit{\textbf{\guessname Prompt}} \\
        <image> <information> Using the provided information as a reference, estimate the location depicted in the image with as much accuracy and precision as possible. Generally, you might use the reasoning to roughly locate the coarse-grained location, and use other information to help you decide more precisely. Use your own knowledge as well. Aim to deduce the exact coordinates whenever feasible. Format your response strictly as JSON in the following structure:\{``country'': ``<country\_name>'', ``city'': ``<city\_name>'', ``latitude'': <Latitude Coordinate>, ``longitude'': <Longitude Coordinate>\} Ensure the JSON output is correctly formatted. Provide a well-informed estimate for each value, avoiding any empty fields. Do not include additional information or commentary.\\
    \bottomrule
    \end{tabularx}
    \caption{The prompts used in \modelname.}
    \label{tab:prompts}
\end{table}

\subsection{Data Demonstration.}

In this section, we present examples and key statistics for both \dataname and guidebook datasets.


\begin{table*}[htbp]
\centering
\small
    \newcolumntype{Y}{>{\arraybackslash}X}
    \begin{tabularx}{\linewidth}{Y}
    \toprule 
        \textit{\textbf{Image}} \\
        \includegraphics[width=\linewidth]{images/chile.jpg} \\
        \midrule
        \textit{\textbf{Location}} \\
        Chile. -27.1265479, -109.2876917 \\
        \midrule
        \textit{\textbf{Reasoning}} \\
        These imposing stone figures are situated on a grassy hillside overlooking the ocean, suggesting a coastal location. The sparse vegetation and the presence of unique megalithic structures indicate a historical and possibly isolated environment. The statues, known as Moai, are characteristic of an island rich in archaeological history. Such a distinct culture is associated with an island famous for these figures, which is set far away from mainland destinations. This points toward the Polynesian island in the southeastern Pacific known for these world-renowned statues, aligning closely with a location associated with Chile. \\
    % \bottomrule
    \toprule 
        \textit{\textbf{Image}} \\
        \includegraphics[width=\linewidth]{images/china.jpg} \\
        \midrule
        \textit{\textbf{Location}} \\
        China. 22.27992557066081, 114.1648415981852 \\
        \midrule
        \textit{\textbf{Reasoning}} \\
        Tall, sleek skyscrapers dominate the cityscape, suggesting a prominent urban environment. The distinct yellow license plates on vehicles are a key hint, typically associated with regions influenced by British rule. The street signs feature both English and Chinese characters, which points to a bilingual city. The presence of a distinctive red double-decker bus is highly characteristic of a city with British influence. All these clues, combined with the modern architecture and dense urban vibes, strongly suggest this is Hong Kong, likely within its central business district. \\
    \toprule 
        \textit{\textbf{Image}} \\
        \includegraphics[width=\linewidth]{images/italy.jpg} \\
        \midrule
        \textit{\textbf{Location}} \\
        Italy. 42.71658113303754, 13.01051150781922 \\
        \midrule
        \textit{\textbf{Reasoning}} \\
        The landscape features rolling hills and a dense cover of greenery, indicative of a temperate climate typically found in parts of Southern Europe. The road layout with its narrow curve and the types of vehicles suggest a European setting. The presence of Italian-language signage such as Cascia and recognizable Italian road markers, like narrow front plates, strongly points to Italy. The architecture, with its rustic and modest buildings in the distance, complements the rural Italian countryside vibe. Hence, these visual cues collectively affirm the location as Italy, likely in a more central to southern region given the sign for Cascia. \\
    \bottomrule
\end{tabularx}
\caption{
Demonstration of \dataname.
}
\label{tab:geoclues}
\end{table*}

\textbf{\dataname.} 
Each data includes a panoramic image, the corresponding location, and a high-quality reasoning process that shows how geographical and cultural information is used to infer the location (Figure~\ref{tab:geoclues}). To reduce hallucination and bias, the model is not required to generate specific street-level locations or coordinates directly, but carefully analysis about image elements (\textit{e.g.}, climate) that collectively lead to the prediction. \dataname is geographically well-distributed, covering various countries across the globe (Figure~\ref{fig:location_distribution}).

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{images/location_distribution.png} 
    \caption{Location distribution of \dataname, covering a wide range of countries around the world.}
    \label{fig:location_distribution}
\end{figure}

\begin{table}[htbp]
\centering
\small
    \newcolumntype{Y}{>{\arraybackslash}X}
    \begin{tabularx}{\linewidth}{Y}
    \toprule 
        \textit{\textbf{Image}} \\
        \includegraphics[width=\linewidth]{images/france_house.png} \\
        \midrule
        \textit{\textbf{Text}} \\
        (Toptips) Houses in Brittany, a western region of France, are coloured white with dark roofs. \\
    \toprule 
        \textit{\textbf{Image}} \\
        \includegraphics[width=\linewidth]{images/tunisia_sign.jpg} \\
        \midrule
        \textit{\textbf{Text}} \\
        (Plonkit) Tunisia has a fairly unique stop sign with Arabic and Latin script. \\
    \bottomrule
\end{tabularx}
\caption{
Examples of clues in the two guidebooks, Toptips and Plonkit. The data is constructed as image-text pairs in the guidebooks.
}
\label{tab:guidebook}
\end{table}


\textbf{Guidebooks.} As shown in Table~\ref{tab:guidebook}, each guidebook entry consists of an image-text pair, where the text describes the location and features depicted in the image. We utilize \textsc{rag} in \modelname to identify the most similar image and use the accompanying text to support the reasoning process.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/data_distribution.pdf} 
    \caption{Distribution of Distance (left) and Length of Reasoning (right) in \dataname.}
    \label{fig:distribution}
\end{figure*}


\textbf{Statistical Results.} We present the statistical results of \dataname in Figure~\ref{fig:distribution}. Most distances are relatively small, indicating that players are often successful in deducing the correct location. The reasoning process averages 842 words in length, demonstrating high quality with extensive details.


\begin{table}[t!]
    \centering
    \small
    \newcolumntype{Y}{>{\arraybackslash}X}
    \begin{tabularx}{\linewidth}{Y}
    \toprule 
        \textit{\textbf{Image}} \\
        \includegraphics[width=\linewidth]{images/israel.jpg} \\
        \midrule
        \textit{\textbf{Location}} \\
        Israel, Ashkelon. 31.66671, 34.59127 \\
        \midrule
        \textit{\textbf{Query}} \\
        <image> Given an image, craft a brief and cohesive reasoning path that deduces this location based on the visual clues present in the image. Using a tone of exploration and inference. Carefully analyze and link observations of natural features (climate, vegetation, terrain), man-made structures (roads, buildings, signage), and distinct landmarks. Allow these observations to naturally lead you to the correct country, enhancing the accuracy of your deductions. Start the reasoning without any intro, and make sure to make it brief. \\
        \midrule
        \textit{\textbf{Response}} \\
        The scene reveals a blend of urban and natural features typical of \textcolor{climate}{\textbf{a Mediterranean climate}}. Notably, the presence of \textcolor{vegetation}{\textbf{palm trees}} suggests a \textcolor{climate}{\textbf{warm, temperate region}}. The architecture of the buildings, characterized by \textcolor{building}{\textbf{flat roofs and a modern, functional style}}, often indicates environments with minimal snowfall. Observing the infrastructure, \textcolor{infrastructure}{\textbf{the neatly paved roads and sidewalks with occasional dust}} align with urban areas in regions with dry summers. The use of earth-toned paint on buildings matches the aesthetic found in Middle Eastern locales. Further exploration of the layout reveals hints of both residential and possibly suburban planning, with open spaces that are common in balanced urban settings. Taking all these elements into consideration, the collective visual attributes and environmental clues point to a location consistent with Israel's landscape and architectural style.\\
    \bottomrule
    \end{tabularx}
    \caption{Fine-tuning data format. The reasoning process leverages visual information in the images to deduce the correct location, such as \textcolor{climate}{\textbf{climate}}, \textcolor{vegetation}{\textbf{vegetation}}, \textcolor{building}{\textbf{building}}, and \textcolor{infrastructure}{\textbf{infrastructure}}.}
    \label{tab:sft_data_full}
\end{table}


\begin{table*}[t]
\centering
\small
\begin{tabular}{l r r r r r r r}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Continent}\\ $2,500$ km\end{tabular}} & 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Country}\\ $750$ km\end{tabular}} & 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Region}\\ $200$ km\end{tabular}} & 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{City}\\ $25$ km\end{tabular}} & 
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Street}\\ $1$ km\end{tabular}} & 
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance$\downarrow$\end{tabular}}} & 
\multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Score$\uparrow$\end{tabular}}}\\
\midrule
$G^3$             & 47.3 & 23.9 &  6.0 &  1.6 &  0.0 & 4,938& 1,451\\
GeoCLIP           & 82.3 & 66.5 & 48.0 & \textbf{32.2} & \textbf{13.0} & 1,762& 3,402\\
StreetCLIP        & 68.2 & 51.2 & 29.6 & 19.0 &  4.2 & 3,161& 2,640\\
\midrule
MiniCPM-V         & 33.2 & 27.8 & 22.4 & 15.9 &  2.3 & 6,624& 1,433\\
LLaVA             & 61.2 & 43.2 & 25.9 & 16.5 &  2.6 & 3,387& 2,338\\
Qwen2-VL          & 75.0 & 65.0 & 48.9 & 29.9 &  5.3 & 2,483 & 3,237\\
\midrule
\textit{\textbf{\modelname}} &&&&&&& \\
~~ - MiniCPM-V    & 68.5& 51.7& 36.5& 23.1 &  3.0 & 3,149 & 2,726 \\
~~ - LLaVA        & 70.4 & 47.8 & 26.8 & 16.7 &  2.8 & 2,851 & 2,592 \\
~~ - Qwen2-VL     & \textbf{84.0} & \textbf{68.3} & \textbf{49.1} &  28.9 & 5.5 & \textbf{1,631} & \textbf{3,482} \\
\bottomrule
\end{tabular}
\caption{
Performance on Im2GPS3k.
}
\label{tab:main_results_im2gps}
\end{table*}

\section{Metrics Definition}
\label{apd:metrics}

\subsection{GeoGuessr Score}

The \textit{GeoGuessr Score} is a metrics introduced in the GeoGuessr game to quantify the accuracy of a guess. It is defined as:

% % \begin{small}
\begin{equation}
    \text{GeoGuessr Score} = 5000 \times e^{-\frac{\text{d}}{1492.7}}
\end{equation}
% \end{small}

where $d$ represents the distance between the guessed coordinates and the actual coordinates in kilometers. Therefore, a more accurate guess yields a \textit{GeoGuessr Score} closer to 5,000.

\subsection{Haversine Distance}
We calculate the Haversine Distance of the models with the following formulas: 

\begin{small}
\begin{equation}
\Delta = \sqrt{\sin^2\left(\frac{\delta_\text{lat}}{2}\right) + \cos(\text{lat}_\text{cor}) \cos(\text{lat}_\text{pred}) \sin^2\left(\frac{\delta_\text{lon}}{2}\right)}
\end{equation}

\begin{equation}
d = 2r \cdot \arcsin(\Delta)
\end{equation}
\end{small}

\text{where:}
\begin{itemize}
    \item \( r \) is the Earth’s radius, which we set as 6,371,
    \item \( \delta_\text{lat} \) is the difference in latitude between the true and predicted coordinates,
    \item \( \delta_\text{lon} \) is the difference in longitude between the true and predicted coordinates,
    \item \( \text{lat}_\text{cor} \) and \( \text{lon}_\text{cor} \) are the correct coordinates,
    \item \( \text{lat}_\text{pred} \) and \( \text{lon}_\text{pred} \) are the predicted coordinates.
\end{itemize}


\section{Supplementary Experiments}
\label{apd:supplementary_exps}

In this section, we present supplementary experiments, including results from the experiments on Im2GPS3k, and \micname details.

As shown in Table~\ref{tab:main_results_im2gps}, \modelname outperforms prior models on Im2GPS3k in terms of Average Distance and GeoGuessr Score. However, GeoCLIP achieves better performance at the City and Street level, likely due to its training on coordinates datasets. The ablation results demonstrated in Table~\ref{tab:ablation_results_im2gps} are consistent with those in Table~\ref{tab:ablation_results}.


\begin{table}[htbp]
\centering
\small
\begin{tabular}{l r r r}
\toprule
\textbf{Model} & \textbf{Country} & \textbf{City} & \textbf{Street} \\
\midrule
 \textbf{\modelname}~(MiniCPM-V) & \textbf{51.7} &  \textbf{23.1} &  \textbf{3.0} \\
~~ - w/o training             &- 1.6 & - 1.8 & - 0.1 \\
 ~~ - w/o \macname                &- 10.6 & - 3.5 & - 0.2 \\
~~ - w/o \micname                & - 0.3 & - 0.2 & - 0.0 \\
 ~~ - MiniCPM-V                &- 23.9 & - 7.2 & - 0.7 \\
\midrule
 \textbf{\modelname}~(LLaVA) & 47.8 & \textbf{16.7} & \textbf{ 2.8} \\
~~ - w/o training             &- 15.3 & - 4.7 & - 0.8 \\
 ~~ - w/o \macname                & - 8.1 & - 1.3 & - 0.1 \\
~~ - w/o \micname                & \textbf{+ 0.1} & - 0.2 & - 0.2 \\
 ~~ - LLaVA                    &- 4.5 & - 0.2 & - 0.1 \\
\midrule
 \textbf{\modelname}~(Qwen2-VL) & 68.3 &  28.9 &  \textbf{5.5} \\
~~ - w/o training             & - 4.3 & - 1.2 & - 0.3 \\
 ~~ - w/o \macname                & - 2.9 & + 0.5 & - 0.1 \\
~~ - w/o \micname                & \textbf{+ 0.1} & - 0.0 & - 0.2 \\
 ~~ - Qwen2-VL                 & - 3.3 & \textbf{+ 1.0} & - 0.2 \\
\bottomrule
\end{tabular}
\caption{
Ablation results on Im2GPS3k.
}
\label{tab:ablation_results_im2gps}
\end{table}

We also analyze the usage of each tool across the datasets and the number of grounding images. This analysis illustrates how frequently \modelname leverages each tool and image detail to deduce locations. As shown in Table~\ref{tab:grounding_distribution} and Table~\ref{tab:tool_usage}, houses are the most frequently identified items in the testing dataset, as images often contain multiple houses. In contrast, signs, though less common, play a critical role by generating queries for \textsc{osm}. The distribution of items directly influences the frequency of tool usage for knowledge retrieval.


\begin{table}[htbp]
\centering
\small
\begin{tabular}{l r r r}
\toprule
\textbf{Dataset} & \textbf{house} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{road} \\ \textbf{sign}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{building} \\ \textbf{sign}\end{tabular}} \\
\midrule
GWS5k           & 3,451 &  20 & 104 \\
Im2GPS3k        & 465   &  52 & 24 \\
\bottomrule
\end{tabular}
\caption{
The frequency of how each item is grounded.
}
\label{tab:grounding_distribution}
\end{table}


\begin{table}[htbp]
\centering
\small
\begin{tabular}{l c r r r}
\toprule
\textbf{Dataset} & $N$ & \textbf{RAG} & \textbf{MAP} & \textbf{\textsc{vlm}} \\
\midrule
GWS5k            & 5,000 & 128 & 70 & 1,978 \\
Im2GPS3k         & 2,997 &  213 & 21 & 493 \\
\bottomrule
\end{tabular}
\caption{
The usage of each tool in each dataset, where $N$ denotes the size of the dataset.
}
\label{tab:tool_usage}
\end{table}