\section{\modelname: Localizing Images with Reasoning and Tools}
\label{sec:method}

\begin{figure*}[t]
    \centering \includegraphics[width=\linewidth]{images/framework.pdf}

\caption{The
    framework of \modelname comprises three main components:
    the \macname, which handles general reasoning; the \micname, which
    leverage external knowledge for detail-specific analysis, and
    the \guessname, which combines outputs from both analyzers to
    generate predictions.}  \label{fig:framework}
\end{figure*}


This section presents \modelname: image \geoloc with reasoning
about cultural and geographical clues and using
external tools. YouTube experts both reason with image elements (\textit{the driving is on the left}) and interpret image details with guidebooks or maps (\textit{the Paria Main Road is in Toco}) to locate an image. Based on this observation, \modelname (Figure~\ref{fig:framework}) has three components: (1)~\textit{\macname}, which focuses on generating a
reasoning process that analyzes the general information present in the
image (Section~\ref{sec:macro}); (2)~\textit{\micname}, which uses
additional knowledge sources and tools to explore the details
(Section~\ref{sec:micro}); and (3)~\textit{\guessname}, which takes
the concatenated outputs from both analyzers to determine the final
location, which can be configured to any granularity of locations
(Section~\ref{sec:guesser}).

\definecolor{climate}{HTML}{10739E}
\definecolor{vegetation}{HTML}{AE4132}
\definecolor{building}{HTML}{56517E}
\definecolor{infrastructure}{HTML}{36393D}

\begin{table}[t!]
    \centering
    \small
    \newcolumntype{Y}{>{\arraybackslash}X}
    \begin{tabularx}{\linewidth}{Y}
    \toprule 
        \textit{\textbf{Image}} \\
        \includegraphics[width=\linewidth]{images/israel.jpg} \\
        \midrule
        \textit{\textbf{Location}} \\
        Israel, Ashkelon. 31.66671, 34.59127 \\
        \midrule
        \textit{\textbf{Query}} \\
        <image> Given an image, craft a brief and cohesive reasoning path that deduces this location based on the visual clues present in the image. Using a tone of exploration ... \\
        \midrule
        \textit{\textbf{Response}} \\
        The scene reveals a blend of urban and natural features typical of \textcolor{climate}{\textbf{a Mediterranean climate}}. Notably, the presence of \textcolor{vegetation}{\textbf{palm trees}} suggests a \textcolor{climate}{\textbf{warm, temperate region}}. The architecture of the buildings, characterized by \textcolor{building}{\textbf{flat roofs and a modern, functional style}}, often indicates environments with minimal snowfall. ... \textcolor{infrastructure}{\textbf{the neatly paved roads and sidewalks with occasional dust}} align with urban areas in regions with dry summers. ... the collective visual attributes and environmental clues point to a location consistent with Israel's landscape and architectural style.\\
    \bottomrule
    \end{tabularx}
    \caption{Fine-tuning data format. The reasoning process leverages visual information in the images to deduce the correct location, such as \textcolor{climate}{\textbf{climate}}, \textcolor{vegetation}{\textbf{vegetation}}, \textcolor{building}{\textbf{building}}, and \textcolor{infrastructure}{\textbf{infrastructure}}. Complete prompts and response are in the Appendix.}
    \label{tab:sft_data}
\end{table}


\subsection{Training \textsc{vlm}s to \textsc{Reason} about Image Locations}
\label{sec:macro}

Recent \textsc{vlm}s can---sometimes---reason about the location of an
image.~\cite{ligeoreasoner}. However, the reasoning is limited to only
a few words and does not help localization~\cite{zhang2024can}.
%
To enhance \textsc{vlm}s to reason location-relevant
information in images, we create \dataname and fine-tune \textsc{vlm}s using it to build \macname. The
reasoning includes geographical information such as
climate, vegetation, building, and infrastructure (Table~\ref{tab:sft_data}). This approach
enables models to deduce locations from geographically pertinent
details, expanding the depth and applicability. 

After training, \macname can generate a rationale for images, where given an image~\( I \), the fine-tuned \textsc{vlm} produces a reasoning~\( R \). However, as the reasoning relies solely on \textsc{vlm}s constrained by their parameterized knowledge, it lacks the information to understand specific details. For instance, human experts can search maps for text on buildings or road signs and consult guidebooks to identify the house style of a particular country, which goes beyond the intrinsic knowledge within \textsc{vlm}s. To emulate this process, an additional module, \micname, integrates external tools, enabling more accurate interpretation of nuanced details.


\subsection{\textsc{Searching} Image Details}
\label{sec:micro}

The \micname module extracts fine-grained details from images to enhance the reasoning by integrating relevant knowledge. It crops the image, generates queries, and retrieves external knowledge.

\textbf{Grounding Image Details.} As highlighted in Section~\ref{sec:data_analysis}, human experts often concentrate on specific elements in images, such as signs, houses, and roads, which provide crucial location-based clues. A precise grounding model generates high-quality queries: given an image~\( I \) and a predefined set of elements~\( E = \{e_1, e_2, \dots, e_n\} \), \micname uses GroundingDino~\cite{liu2023grounding} to crop the image according to \(E\). Since each image may contain multiple instances of an element, the cropped images is defined as \( C = \{ c_{i,j} \mid e_i \in E, j \in [1, m_i] \} \), where \( m_i \) is the count of element~\( e_i \) in \( I \). Specifically, we select \textit{road sign}, \textit{building sign}, and \textit{house} from Figure~\ref{fig:clue-type} as elements, which align well with GroundingDino, since alternatives could yield overly large figures or uninformative results. Each cropped image \( c_{i,j} \) is a query for specific tools. Additionally, if \( c_{i,j} \) is a \textit{sign} that contains text, text-based queries are generated with Optical Character Recognition (\textsc{ocr}) from Qwen2-VL~\cite{Qwen2VL}. Therefore, the query set~\( Q \) is:

\vspace{-5pt}
\[
Q = \bigcup_{i,j} \{ c_{i,j}, \text{OCR}(c_{i,j}) \text{ if } c_{i,j} \in \text{signs} \}
\]


\textbf{Tools.} The query set~\( Q \) is then fed into a Tool Set~\( T \), which retrieves relevant knowledge. We use three tools for information retrieval: 
(1) The \textit{GeoGuessr Guidebook} contains rich information for locating images (Section~\ref{sec:data_collection}). Following prior research~\cite{luo2022g, zhou2024img2loc}, we frame \textit{Guidebook} using as a Retrieval-Augmented Generation problem. Given an input image (\textit{e.g.}, a house as in Figure~\ref{fig:framework}), we retrieve the most similar images. 
(2) \textit{Map.} The map is a critical tool in image \geoloc: text in images (\textit{e.g.}, a name on a sign) can pinpoint a location. We use OpenStreetMap\footnote{\url{https://www.openstreetmap.org/}} for location retrieval, providing the top three search results, with the place name and multi-level location details.
(3) \textit{\textsc{vlm}.} We use an additional \textsc{vlm} as a tool by prompting it to identify details that might be overlooked in the \macname. The \textsc{vlm} generates descriptions for details to narrow down potential locations (Figure~\ref{fig:framework}). Each tool~\( t \) in the Tool Set~\( T \) contributes to the retrieval of additional knowledge~\( K \): 

\vspace{-5pt}
\[
K = \bigcup_{t \in T} t(Q)
\]
Further implementation details in the Appendix~\ref{apd:implementations}.

\subsection{\textsc{Guessing} the Final Location}
\label{sec:guesser}
The \guessname uses all prior information to generate the final prediction. It concatenates the reasoning $R$ from the \macname with the external knowledge $K$ retrieved by the \micname, forms them into a prompt template $p$ along with the image $i$, and makes the location prediction with a \textsc{vlm}:

\vspace{-5pt}
\[
\hat{y}_{\text{loc}} = \mathrm{\textsc{vlm}}_{p} \big( I, \mathrm{concat}(R, K) \big)
\]
where $\hat{y}_{\text{loc}}$ is the model's generated location. The prompt $p$ is configurable to flexibly adjust to specific output formats, such as various location levels (\textit{e.g.}, country, city, and coordinates).