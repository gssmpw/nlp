\section{Collecting \dataname: Linking Places to Images}

This section explains how we process the reasoning of GeoGuessr players to construct \dataname. In addition, we analyze their reasoning and identify fifteen key clues humans use in \geoloc.

\subsection{Data Collection}
\label{sec:data_collection}

Despite previous datasets containing image--location pairs~\cite{hays2008im2gps, vo2017revisiting, astruc2024openstreetview} and reasoning insights from guidebooks~\cite{luo2022g, ligeoreasoner}, there is still a lack of datasets that capture the analytical reasoning process used to deduce locations from image details. To train \modelname to generate reasoning (Section~\ref{sec:macro}), we use the data from GeoGuessr, a popular game where players infer locations from street views, which preserve experts' knowledge and strategies for image \geoloc. We mine game data from ``play along'' videos of five popular YouTubers, along with transcripts of their reasoning during gameplay.

\textbf{Data Mining.} In a typical GeoGuessr game, there are multiple rounds of guessing the location from a new image. To segment the video transcript, we identify the timestamps of each round's result pages by using Qwen-VL~\cite{bai2023qwen} to match the buttons and extract the corresponding scores. For images, we retrieve images from Google StreetView (\textsc{gsv}) API based on the coordinates of each round, omitting any unavailable ones.\footnote{As \textsc{gsv} updates the images from time to time, some coordinates are deleted.} Following~\citet{haas2024pigeon}, we capture images from four different directions and combine them to create $360$-degree panoramic views (Table~\ref{tab:sft_data}), which contain the same details as in the games. For reasoning data, we split the transcripts by round timestamps. The raw dataset contains $2637$ images and respective locations.

\textbf{Data Processing.} To ensure data quality, we apply several processing steps: (1) we manually review and remove games where the visual content differs between the players' view and the \textsc{gsv} data, such as games that allow movement or feature satellite view; (2) we exclude games with poor reasoning quality, including those with a time limit of less than $30$ seconds, transcripts shorter than $100$ words, or incorrect answers (where the GeoGuessr Score is lower than $3400$, approximately $575$ km); (3) answer-guided reasoning~\cite{mendes2024granular} generates the step-by-step reasoning process from both the transcripts and images, guided by the correct locations and details mentioned by humans.

After processing, our final dataset, \textbf{\dataname}, consists of $1120$ images, each associated with a corresponding location (continent, country, and coordinates), reasoning, and scores. \dataname is geographically well-distributed (Table~\ref{fig:distribution}), covering a diverse set of images in various countries to reduce potential data biases. \dataname is useful for analyzing human \geoloc strategies and training models for related tasks. Prompts, examples, and other details are in the Appendix~\ref{apd:data}.

\textbf{Guidebook Data.} For \micname (Section~\ref{sec:micro}), we collect guidebook data from two popular GeoGuessr community sources,\footnote{\url{https://somerandomstuff1.wordpress.com/2019/02/08/geoguessr-the-top-tips-tricks-and-techniques} and \url{https://www.plonkit.net}} which include tips from top players of GeoGuessr. Following~\citet{luo2022g} and~\citet{ligeoreasoner}, each instance in our dataset contains a clue (\textit{\textit{e.g.}, The chevrons are black with white arrows in the U.K.}) paired with an image. The final dataset contains $6227$ image--clue pairs that have distinguishable geographical features, which could be further integrated into \geoloc frameworks as additional knowledge (Examples are in Appendix~\ref{apd:data}).

\subsection{Data Analysis}
\label{sec:data_analysis}

We further analyze the reasoning of human players and investigate common patterns among human experts in \geoloc tasks. We use spaCy~\cite{honnibal2015improved} for noun extraction, allowing us to explore the specific information these experts focus on. We manually filter out irrelevant words and phrases, retaining only content pertinent to geographic reasoning.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{images/clues.pdf} 
    \caption{Top clues in human reasoning. Humans identify \textit{roads}, \textit{cars}, \textit{poles}, and linguistic clues---specifically the languages on \textit{plates}, \textit{signs} and \textit{houses}.}
    \label{fig:clue-type}
\end{figure}

This process identifies fifteen core clues frequently mentioned by experts reflecting common analytical patterns and reasoning strategies (Figure~\ref{fig:clue-type}). The keywords cover cultural clues (\textit{e.g.}, language, flag, road, house) and natural geographical features (\textit{e.g.}, mountain, island, tree). This distribution conforms to categories in the guidebooks and further guides the implementation of our framework in Section~\ref{sec:method}.