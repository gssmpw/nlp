\section{Related Work}
\textbf{Model interventions.} Model interventions are a family of approaches that manipulate model activations to control generations ____. ____ propose a method to identify neurons in pre-trained transformer models that are most predictive of a particular concept (\textit{expert neurons}) and show that setting the activations of these experts to their mean value can induce the presence of the target concept in model generations. ____ find the expert neurons for toxic language and steer the LLM to generate less toxic text by dampening these neurons, while ____ achieve detoxification by using a contrastive prompt. ____ propose a method to control generations by leveraging the differences in residual stream activations between pairs of positive and negative examples. In mLLMs,  ____ use this approach to produce more target language tokens in open-ended generation. However, prior work does not analyze the changes these interventions introduce in the representational space of mLLMs nor does it explore the impact of the interventions on cross-lingual alignment.


\paragraph{Aligning multilingual representations in mLLMs.} 
Research on LLM representation alignment falls into two broad categories:  1) Improving model performance on downstream tasks via post-training methods such as prompt-based techniques ____, fine-tuning, or continuous pre-training ____. 2)  Understanding where and how representation alignment is achieved in mLLMs. For example, ____ show that English-dominated mLLMs like Llama-2 use English as a pivot language and ____ systematically evaluate factor contributing to successful cross-lingual transfer in such models.