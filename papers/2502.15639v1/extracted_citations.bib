@article{Kojima_et_al,
      title={On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons}, 
      author={Takeshi Kojima and Itsuki Okimura and Yusuke Iwasawa and Hitomi Yanaka and Yutaka Matsuo},
      journal = {NAACL},
      year={2024},
      url={https://arxiv.org/abs/2404.02431}, 
}

@inproceedings{huang-etal-mprompt,
    title = "Not All Languages Are Created Equal in {LLM}s: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
    author = "Huang, Haoyang  and
      Tang, Tianyi  and
      Zhang, Dongdong  and
      Zhao, Xin  and
      Song, Ting  and
      Xia, Yan  and
      Wei, Furu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.826/",
    doi = "10.18653/v1/2023.findings-emnlp.826",
    pages = "12365--12394"
}

@article{li2024_align,
      title={Improving In-context Learning of Multilingual Generative Language Models with Cross-lingual Alignment}, 
      author={Chong Li and Shaonan Wang and Jiajun Zhang and Chengqing Zong},
      year={2024},
      eprint={2311.08089},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      journal={NAACL}
}
      url={https://arxiv.org/abs/2311.08089}, 
}

@misc{li_activation_steering,
      title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model}, 
      author={Kenneth Li and Oam Patel and Fernanda Vi√©gas and Hanspeter Pfister and Martin Wattenberg},
      year={2023},
        journal={NeurIPS}
}
      eprint={2306.03341},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.03341}, 
}

@InProceedings{pmlr-v162-cuadros22a,
  title = 	 {Self-conditioning Pre-Trained Language Models},
  author =       {Suau, Xavier and Zappella, Luca and Apostoloff, Nicholas},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {4455--4473},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/cuadros22a/cuadros22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/cuadros22a.html},
  abstract = 	 {In this paper we aim to investigate the mechanisms that guide text generation with pre-trained Transformer-based Language Models (TLMs). Grounded on the Product of Experts formulation by Hinton (1999), we describe a generative mechanism that exploits expert units which naturally exist in TLMs. Such units are responsible for detecting concepts in the input and conditioning text generation on such concepts. We describe how to identify expert units and how to activate them during inference in order to induce any desired concept in the generated output. We find that the activation of a surprisingly small amount of units is sufficient to steer text generation (as little as 3 units in a model with 345M parameters). While the objective of this work is to learn more about how TLMs work, we show that our method is effective for conditioning without fine-tuning or using extra parameters, even on fine-grained homograph concepts. Additionally, we show that our method can be used to correct gender bias present in the output of TLMs and achieves gender parity for all evaluated contexts. We compare our method with FUDGE and PPLM-BoW, and show that our approach is able to achieve gender parity at a lower perplexity and better Self-BLEU score. The proposed method is accessible to a wide audience thanks to its simplicity and minimal compute needs. The findings in this paper are a step forward in understanding the generative mechanisms of TLMs.}
}

@inproceedings{rimsky_steering,
    title = "Steering Llama 2 via Contrastive Activation Addition",
    author = "Rimsky, Nina  and
      Gabrieli, Nick  and
      Schulz, Julian  and
      Tong, Meg  and
      Hubinger, Evan  and
      Turner, Alexander",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.828/",
    doi = "10.18653/v1/2024.acl-long.828",
    pages = "15504--15522",

}

@article{rodriguez2024controlling,
  title={Controlling Language and Diffusion Models by Transporting Activations},
  author={Rodriguez, Pau and Blaas, Arno and Klein, Michal and Zappella, Luca and Apostoloff, Nicholas and Cuturi, Marco and Suau, Xavier},
  journal={arXiv preprint arXiv:2410.23054},
  year={2024}
}

@inproceedings{tanwar-prompt_multi,
    title = "Multilingual {LLM}s are Better Cross-lingual In-context Learners with Alignment",
    author = "Tanwar, Eshaan  and
      Dutta, Subhabrata  and
      Borthakur, Manish  and
      Chakraborty, Tanmoy",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.346/",
    doi = "10.18653/v1/2023.acl-long.346",
    pages = "6292--6307"
}

@misc{turner2024_steering,
      title={Steering Language Models With Activation Engineering}, 
      author={Alexander Matt Turner and Lisa Thiergart and Gavin Leech and David Udell and Juan J. Vazquez and Ulisse Mini and Monte MacDiarmid},
      year={2024},
      eprint={2308.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.10248}, 
}

@misc{wendler2024llamasworkenglishlatent,
      title={Do Llamas Work in English? On the Latent Language of Multilingual Transformers}, 
      author={Chris Wendler and Veniamin Veselovsky and Giovanni Monea and Robert West},
      year={2024},
      eprint={2402.10588},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.10588}, 
}

@misc{zhang2023_crosslingualalignment,
      title={BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models}, 
      author={Shaolei Zhang and Qingkai Fang and Zhuocheng Zhang and Zhengrui Ma and Yan Zhou and Langlin Huang and Mengyu Bu and Shangtong Gui and Yunji Chen and Xilin Chen and Yang Feng},
      year={2023},
      eprint={2306.10968},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.10968}, 
}

