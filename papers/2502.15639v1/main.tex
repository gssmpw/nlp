
% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{tablefootnote}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
%\usepackage{graphics}
\usepackage{float}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{multirow}
% \usepackage[table]{xcolor}
\usepackage{cleveref}

% \newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\nonsignificant}[1]{\color{black!30}#1}

\title{Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models}


\author{
 \textbf{\stepcounter{footnote}Anirudh Sundar\textsuperscript{1}\thanks{Work completed during an internship at Apple}} \thanks{Equal contribution} \qquad
 \textbf{Sinead Williamson \textsuperscript{2}} \qquad
 \textbf{Katherine Metcalf \textsuperscript{2}} \qquad \AND
 \textbf{Barry-John Theobald\textsuperscript{2}} \qquad
 \textbf{Skyler Seto \textsuperscript{2}} \qquad
 \textbf{Masha Fedzechkina \textsuperscript{2} \footnotemark[3]} 
 \\
\textsuperscript{1}AI Virtual Assistant Lab, Georgia Institute of Technology\\
\textsuperscript{2}Apple\\
%\textsuperscript{*}Equal contribution\\
\texttt{\small{asundar34@gatech.edu}, \texttt{{\{sa\_williamson, kmetcalf, bjtheobald,  sseto, mfedzechkina\}}@apple.com}
}}
      
         

\begin{document}
\maketitle

\begin{figure*}[h!]
     \centering
     \includegraphics[width=\textwidth]{latex/figures/multilingual_experts_intro_figure.pdf}
     \caption{When text from multiple languages is embedded using an LLM with an intervention on expert Spanish neurons,  the resulting text embeddings cluster more closely around the medoid of the embedding space (left). This modified model is better at matching Spanish queries to translated text compared to an unmodified model (right).}
     \hfill
     \label{fig:overview_intro_schematic}
\end{figure*}

\begin{abstract}

Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions --- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval. 

\end{abstract}



\section{Introduction}

Large language models (LLMs) exhibit impressive performance on a variety of tasks from text summarization to zero-shot common-sense reasoning \cite{raffel2020exploring, liu-lapata-2019-text,bosselut-etal-2019-comet,Richardson2023CommonsenseRF} and are increasingly deployed in a variety of fields ranging from health to entertainment. Despite these capabilities, to ensure that deployed LLMs align with human values, are non-toxic, and do not hallucinate, they often must be adapted post pre-training. 
 

Model interventions have emerged as data- and compute-efficient tools for model adaptation, whereby targeted updates are applied to model activations after pre-training \cite{rodriguez2024controlling, li_activation_steering, rimsky_steering}. One such method is~\emph{finding experts}~\cite{pmlr-v162-cuadros22a, suau2024whispering} which manipulates the activations of~\emph{expert}~neurons responsible for encoding a broadly defined concept (e.g., a word or style of text) to steer model generations into a desired direction. This approach has been successfully used  in a variety of domains, ranging from achieving gender parity \cite{pmlr-v162-cuadros22a}, reducing toxicity \cite{suau2024whispering},studying geopolitical biases \cite{geographic_experts} and multilingual capabilities \cite{Kojima_et_al} in mLLMs. 

While model interventions successfully control model generations, we do not fully understand their implications for model performance. Two observations are relevant: First, model intervention methods increase perplexity on a fixed dataset post-intervention \cite{suau2024whispering} meaning that the intervention introduces changes in how the model represents language. Second, work on mLLMs \cite{Kojima_et_al} has shown that intervening on experts for a given language increases the probability of generating that language in the model output (expected outcome) and improves prompt-based machine translation (surprising outcome). These performance gains suggest that the intervention may increase the alignment between representations of different languages.

In this work, we focus on representational changes in mLLMs with an emphasis on cross-lingual alignment for two reasons. First, gains in mLLM performance are largely attributed to better alignment of multilingual representations \cite{wu-rep_isomorphism, lample_unsupervised}. This has generated a lot of interest in improving multilingual alignment \cite{Chaudhary_dict, efimov_adjustment, lample_cross_lingual, liu-transliterations}. Second, datasets with the same text in multiple languages are available for a variety of tasks, which enables us to study the impact of the intervention in a controlled way across multiple languages. 

Specifically, we examine changes in the embedding space of mLLMs introduced by the finding experts intervention and link these changes to downstream task performance (see Fig. \ref{fig:overview_intro_schematic}). We hypothesize that this intervention increases cross-lingual alignment in mLLMs and present results supporting this hypothesis. We find that:
\begin{enumerate}
        \item  The intervention projects all languages into a new space with new properties in the mLLM .
        \item Some of the new properties are less desirable, as evidenced by an increase in perplexity post-intervention.  
        \item  Other properties of the new space are desirable. Specifically, the distances between the language embeddings are reduced in the new space, i.e., the cross-lingual representations are more aligned (Section \ref{sec:embedding_space}). This translates into a performance gain on cross-lingual retrieval with up to 2x improvement in top-1 accuracy (Section \ref{sec:cross-ling_retrieval}), while preserving within-language similarity (Section \ref{sec:paraphrase_retrieval}).
\end{enumerate}



\section{Related Work}

\textbf{Model interventions.} Model interventions are a family of approaches that manipulate model activations to control generations \cite{li_activation_steering, turner2024_steering, rodriguez2024controlling}. \citet{pmlr-v162-cuadros22a} propose a method to identify neurons in pre-trained transformer models that are most predictive of a particular concept (\textit{expert neurons}) and show that setting the activations of these experts to their mean value can induce the presence of the target concept in model generations. \citet{suau2024whispering} find the expert neurons for toxic language and steer the LLM to generate less toxic text by dampening these neurons, while \citet{turner2024_steering} achieve detoxification by using a contrastive prompt. \citet{rimsky_steering} propose a method to control generations by leveraging the differences in residual stream activations between pairs of positive and negative examples. In mLLMs,  \citet{Kojima_et_al} use this approach to produce more target language tokens in open-ended generation. However, prior work does not analyze the changes these interventions introduce in the representational space of mLLMs nor does it explore the impact of the interventions on cross-lingual alignment.


\paragraph{Aligning multilingual representations in mLLMs.} 
Research on LLM representation alignment falls into two broad categories:  1) Improving model performance on downstream tasks via post-training methods such as prompt-based techniques \cite{huang-etal-mprompt, tanwar-prompt_multi}, fine-tuning, or continuous pre-training \cite{zhang2023_crosslingualalignment, li2024_align}. 2)  Understanding where and how representation alignment is achieved in mLLMs. For example, \citet{wendler2024llamasworkenglishlatent} show that English-dominated mLLMs like Llama-2 use English as a pivot language and \citealt{zhao_llama} systematically evaluate factor contributing to successful cross-lingual transfer in such models. 


\section{Methods}
We seek to understand the impact of network interventions on the representational space of mLLMs with a focus on cross-lingual alignment. We consider three open-source mLLMs: Aya-8B (instruction fine-tuned) \cite{aya},  PolyLM-13B (chat version) \cite{polylm}, and Bloom-7B (base) \cite{workshop2022bloom}. Since our aim is to draw conclusions about cross-lingual alignment, we want to make sure that we know what languages were seen in pre-training and include mLLMs for which a detailed description of pre-training datasets is available, excluding LLMs such as Mistral \cite{jiang2023mistral}, Llama \cite{touvron2023llamaopenefficientfoundation}, and Gemma \cite{team2024gemma}. We begin by identifying and intervening on the language experts in the mLLMs and then study cross-lingual alignment in the embedding space and downstream task performance pre- and post-intervention.



\subsection{Probing dataset construction}  Following \citet{Kojima_et_al}, we use the Flores200 dataset \cite{flores200} to find the expert neurons for a particular target language (i.e., the language specifically targeted by the intervention). Flores200 is a machine translation dataset containing short paragraphs sampled from Wikimedia \footnote{\url{https://commons.wikimedia.org/wiki/Main_Page}} and subsequently translated into 204 languages by skilled human translators. We limit our investigations to the intervention on five target languages --- English, German, French, Spanish, and Japanese. These languages are well represented in pre-training data of the models we are considering, ensuring the existence of expert neurons. 

\subsection{Identifying expert neurons}
Expert neurons for a given language are identified following \citet{suau2024whispering}. This \emph{finding experts} approach consists of two steps -- first, expert neurons are identified for a particular concept of interest (in our case, a particular target language) and then an intervention is performed to change the activations of these neurons. Expert neurons are those that can classify sentences as being from the positive set (containing the target language) vs.\ the negative set (that does not contain the target language), as measured by the area under the ROC curve. For each of the five languages under consideration, we use the Flores200 dev split for the target language as the positive set, and the dev splits for the other four languages plus Chinese as the negative set. 
We include Chinese to increase variety in the character systems in the negative set but we do not consider it for the positive set.

\subsection{Intervening on expert neurons}
\label{sec:unit_selection}
For the intervention, we select the $k$ neurons with the highest expertise (i.e., highest AUROC). We select the value for $k$ that balances generating text in the target language with a low perplexity on the language-specific Wikipedia text (See Appendix \ref{app:selecting_neurons} for further details). The activation for these neurons is set to their respective mean value calculated over the positive sentences \cite{pmlr-v162-cuadros22a}. 




\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{latex/figures/PPL_LID_combined.png}
\caption{Language ID accuracy and Log perplexity for the intervention on five target languages. The x-axis shows the number of activated experts (0 indicates the original model).}
\label{fig:LID_PPL}
\end{figure*}


For almost all target languages, the probability of generating that language increases post-intervention (Fig.\ \ref{fig:LID_PPL}, top), suggesting that the intervention is successful. The only exception is English in the Aya-8B model, where the intervention reduces the likelihood of generating English. We believe that the intervention steers the model away from the default configuration, and English is the default language for that model. 

Interestingly, despite Bloom-7B's training set containing neither German nor Japanese, the intervention results in generating both languages with high probability. Our hypothesis is that the Bloom-7B pre-training data contains some amount of German and Japanese data that is large enough to enable expert discovery and controlled generation. 




While we are successfully able to increase the accuracy of target language generation through the interventions, consistent with prior work \cite{suau2024whispering}, we observe an increase in perplexity post-intervention as the number of activated neurons increases, see Fig.\ \ref{fig:LID_PPL} (bottom). 





For our analyses, we set $k$ to  $100$ experts for Bloom-7B and $2000$ for PolyLM-13B and Aya-8B. For brevity, we present the results for the intervention on Spanish (randomly chosen) in the main text. The results for the other languages are in the respective appendices. 


\section{The intervention shifts the embedding space increasing cross-lingual alignment}
\label{sec:embedding_space}
We begin our investigation by quantifying the differences induced by the intervention into the embedding space. 
For this analysis, we intervene on each of the five target languages discussed in Section \ref{sec:unit_selection} and examine the effect of the intervention on the representations of 22 languages (the union of all languages present in the pre-training across the three language models). We exclude Arabic and Chinese from consideration due to the lack of conformity in the scripts used\footnote{Arabic data are represented in both the Arabic and Latin scripts, while Chinese data are written using both Simplified and Traditional scripts. This decision is motivated by prior work showing that a discrepancy in the encoding can influence performance \cite{blaschke2025analyzing} and several models under consideration do not provide information on which encoding was used in the pre-training. Appendix \ref{app:lang_list} contains the full list of languages and the language codes. }. Note that not all of these languages are part of the pre-training for every model under consideration but we present them for consistency (clearly indicating in all figures if the languages were seen by the model during the pre-training). 


\begin{table*}[h]
    \centering
    \resizebox{\textwidth}{!}
    {
    \begin{tabular}{llcccccc}
         \hline
         \multirow{2}{*}{Model} & \multirow{2}{*}{Language} & \multicolumn{2}{c}{\underline{Distance (all languages)}} & \multicolumn{2}{c}{\underline{Distance to Medoid}} & \multicolumn{2}{c}{\underline{Distance to Target}}\\
         & & Pre & Post & Pre & Post & Pre & Post \\
         \hline
         \multirow{2}{*}{Aya-8B} & Target & -- & -- & $0.62_{\pm\text{0.03}}$ & $0.14_{\pm\text{0.03}}$ & -- & -- \\
          & Non-Target  & $0.72_{\pm\text{0.00}}$ & $0.19_{\pm\text{0.04}}$ & $0.58_{\pm\text{0.01}}$ & $0.12_{\pm\text{0.01}}$ & $0.77_{\pm\text{0.01}}$ & $0.2_{\pm\text{0.01}}$ \\ \hline
          \multirow{2}{*}{Bloom-7B} & Target & --  & --  & $0.60_{\pm\text{0.21}}$ & $0.04_{\pm\text{0.01}}$ & -- &  --\\
          & Non-Target  & $0.72_{\pm\text{0.00}}$ & $0.17_{\pm\text{0.06}}$ & $0.5_{\pm\text{0.03}}$ & $0.11_{\pm\text{0.01}}$ & $0.78_{\pm\text{0.03}}$ & 
          $0.13_{\pm\text{0.01}}$\\ \hline
          \multirow{2}{*}{PolyLM-13B} & Target & -- & -- & $0.72_{\pm\text{0.04}}$ & $0.43_{\pm\text{0.09}}$ & -- & -- \\
          & Non-Target  & $0.85_{\pm\text{0.00}}$ & $0.56_{\pm\text{0.09}}$& $0.72_{\pm\text{0.01}}$ & $0.43_{\pm\text{0.02}}$ & $0.86_{\pm\text{0.01}}$ & 
          $0.54_{\pm\text{0.02}}$\\ \hline
    \end{tabular}
    }
    \caption{Cosine distances between 22 languages under consideration, mean distance to the target of the intervention, and and the distance to the medoid of the embedding space are reduced post-intervention. Distance (all languages) refers to pairwise cosine distance between the embeddings of 22 languages; distance to target refers to the distance between the intervention target and the remaining 21 languages. Pre refers to pre-intervention and post to post-intervention. Distances are means and standard errors of the mean over the five intervention targets.}
    \label{tab:umap_intervention_movement}
\end{table*}







For each of the 22 languages, we embed the Flores200 test set (1012 sentences per language) with the original and intervened models' last layer. To characterize the changes in the embedding space, we calculate two types of distances: (1) the pairwise cosine distance between the embeddings of the 22 languages for the intervened and unintervened spaces and (2) the cosine distance between the mean of the embedding for each of the 22 languages and the medoid of each space (pre- and post-intervention) (Table \ref{tab:umap_intervention_movement}). We project the multidimensional embeddings into a two-dimensional space using UMAP \cite{umap} to visualize how the embedding space changes. 

Our findings are as follows. The intervention pulls the embeddings of all languages into a new space (see Fig.\ \ref{fig:umaps_es} for Spanish and Appendix \ref{sec:appendix_umaps} for other languages), rather than moving them closer to the embeddings of the target language in the unintervened space. The increase in perplexity post-intervention discussed in Section \ref{sec:unit_selection} also supports this finding. 

The post-intervention embeddings for the different languages are closer to each other compared to the pre-intervention embeddings, as indicated by the reduced pairwise cosine distances between the languages. Specifically, the distances are reduced because the post-intervention embeddings are pulled closer to the medoid of the embedding space. As a result of the shift, all languages are closer to the target post-intervention.

We notice that all distances under consideration are reduced less post-intervention for PolyLM-13B compared to the other models. We hypothesize that this relates to the specific data distribution and training procedure used for PolyLM-13B. Unfortunately, since we do not have access to the data the three models under consideration were trained on, we cannot test this hypothesis in this work. We return to this point in Section \ref{sec:limitations}.


Taken together, these findings suggest that the intervention projects language embeddings into a new space where they are more aligned. In the following sections, we explore if this change translates into downstream task performance.



 


\begin{figure*}[h]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_es_100.pdf}
         \caption{Bloom-7b}
         \label{fig:bloom_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_es_2000.pdf}
         \caption{Aya-8b}
         \label{fig:aya_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_es_2000.pdf}
         \caption{PolyLM-13b}
         \label{fig:polylm_umap}
     \end{subfigure}
        \caption{UMAP embeddings for 22 languages in the Bloom-7B model (left), Aya-8B (middle), and PolyLM-13B (right). The embeddings post-intervention are marked with `*' for each language. The dots represent individual sentences in the pre-intervention space; the crosses represent individual sentences in the post-intervention space. The languages that are not in the training set for a given model are marked in red.}
        \label{fig:umaps_es}
\end{figure*}



\section{Cross-lingual similarity is enhanced in the new space}
\label{sec:cross-ling_retrieval}

We now ask if the increased alignment post-intervention translates to downstream task performance. We use \textbf{cross-lingual} retrieval as our downstream task: Given a sentence (query) in one language (query language), and a set of sentences (candidates) in a different language (candidate language), which of the candidates is a translation (match)? Our main experiments are carried out on the Flores200 test split \cite{flores200} as it allows us to test cross-lingual retrieval across multiple combinations of query and candidate languages. As the dev split of the Flores200 dataset was used to identify language experts, we also present results on the validation split of Tatoeba \cite{totoeba} and the test split of BUCC-18 \cite{hu2020xtreme} for an independent validation of our findings (see Appendix \ref{sec:app_bucc_tatoeba}).

\begin{figure*}[h]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/bloom/dotplot_Bloom-7b_Spanish.png}
         \caption{Bloom-7b}
         \label{fig:bloom_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/cohere/dotplot_Aya-8b_Spanish.png}
         \caption{Aya-8b}
         \label{fig:aya_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/polylm/dotplot_PolyLM-13b_Spanish.png}
         \caption{PolyLM-13b}
         \label{fig:polylm_dotplot}
     \end{subfigure}
        \caption{Top-1 retrieval accuracy for the intervention on Spanish for 22 languages in the Bloom-7B model (left), Aya-8B (middle), and PolyLM-13B (right). The languages that are not in the training set for a given model are marked in red.}
        \label{fig:dotplots_es}
\end{figure*}

For each sentence, we compute pre- and post-intervention embeddings by averaging over the last hidden state of the mLLM, producing vectors with dimensions matching the model's hidden size. To identify the closest matching sentence, we compute inner products between the query (e.g., in Spanish), and all candidates (e.g., in French). We select the candidate with the highest inner product as the match, and then measure top-1 accuracy.


\begin{table*}[ht]
  \centering
  \resizebox{\textwidth}{!}
  {
  \small{
\begin{tabular}{cccccc}
\toprule
Model & Query language & $r(\text{acc}_{\text{\small{post}}} , d_{\text{\small{post}}})$ & $r(\text{acc}_{\text{\small{pre}}} , d_{\text{\small{pre}}})$ & $r(d_{\text{\small{post}}} , d_{\text{\small{pre}}})$ & $r(\Delta \text{acc}, d_{\text{\small{pre}}}-d_{\text{\small{post}}})$ \\
\midrule
\multirow{5}{*}{Aya-8B} & es & -0.51 [-0.88 -0.18] & -0.89 [-0.98 -0.55] & 0.48 [0.32 0.78] & 0.86 [0.49 0.96] \\
& fr & -0.64 [-0.91 -0.45] & -0.86 [-0.97 -0.57] & 0.51 [0.27 0.89] & 0.89 [0.84 0.97] \\
& en & -0.94 [-0.97 -0.85 ] & -0.80 [-0.96 -0.34] & 0.65 [0.44 0.92] & \nonsignificant{0.10 [-0.69  0.60]} \\
& de & -0.89 [-0.96 -0.75] & -0.87 [-0.98 -0.44] & 0.33 [-0.74  0.76] & 0.52 [0.12 0.95] \\
& jp & \nonsignificant{-0.02 [-0.62  0.34]} & -0.96 [-0.99  0.34] & \nonsignificant{0.27 [-0.18  0.62]} & 0.89 [0.30 0.99] \\
\midrule
\multirow{5}{*}{Bloom-7B} & es & -0.97 [-0.99 -0.95] & -0.83 [-0.99 -0.54] & 0.79 [0.71 0.98] & \nonsignificant{0.1 [-0.98  0.88]} \\
& fr & -0.98 [-0.99 -0.94] & -0.89 [-0.99 -0.38] & 0.75 [0.62  0.99] & \nonsignificant{0.23 [-0.98 0.83]} \\
& en & -0.89 [-0.99 -0.60] & -0.89 [-0.99 -0.44] & 0.97 [0.96 0.99] & \nonsignificant{0.23 [-0.90  0.86]} \\
& de & -0.90 [-0.99 -0.74] & \nonsignificant{-0.50 [-0.96  0.34]} & 0.95 [0.86 0.99] & \nonsignificant{-0.72 [-0.97  0.22]} \\
& jp & -0.90 [-0.99 -0.80] & NA\tablefootnote{The cosine distances between Japanese and other languages are identical in Bloom-7b in the unintervened space and thus the correlation coefficient is not defined. This is likely due to the fact that Japanese is not in Bloom 7b's pre-training.} & \nonsignificant{-0.48 [-0.90  0.97]} & \nonsignificant{0.64 [-0.70  0.93]} \\
\midrule
\multirow{5}{*}{PolyLM-13B} & es & -0.44 [-0.91 -0.38] & -0.84 [-0.96 -0.65] & 0.70 [0.44 0.91] & \nonsignificant{0.10 [-0.31  0.57]} \\
& fr & -0.44 [-0.82  -0.35] & -0.90 [-0.99 -0.62] & 0.66 [0.20 0.93] & \nonsignificant{0.30 [-0.18   0.82]} \\
& en & -0.86 [-0.98 -0.53] & -0.84 [-0.98 -0.52] & 0.99 [0.96 0.99] & \nonsignificant{0.28 [-0.33 0.62]} \\
& de & \nonsignificant{-0.01 [-0.51 0.81]} & -0.95 [-0.99  -0.57] & \nonsignificant{0.15 [-0.56  0.57]} & \nonsignificant{-0.04 [-0.71  0.34]} \\
& jp & \nonsignificant{-0.52 [-0.91  0.92]} & -0.96 [-0.99  0.00] & 0.73 [0.56 0.96] & \nonsignificant{0.25 [-0.25 0.57]} \\
\bottomrule
\end{tabular}
}
}
  \caption{Pearson correlations ($r$) between top-1 retrieval accuracy ($\text{acc}$) and mean pairwise cosine distance in the embedding space $d$. Subscripts indicate the space from which embeddings are sampled: pre = original model; post =  intervened model. Numbers in brackets represent bootstrapped 95\% confidence intervals. Correlations that are not statistically significant (p-values >0.05) are shown in gray.}
   \label{table:performance_corrs}
\end{table*}


\paragraph{Top-1 retrieval accuracy improves post-intervention for retrieval with the target language.} We first examine if the increased proximity to the target language in the post-intervention embedding space translates into top-1 retrieval accuracy improvement when the target is used as the retrieval query for the $22$ candidate languages under consideration.


We find that top-1 retrieval accuracy improves post-intervention when using the target as the query language (see Fig.\ \ref{fig:dotplots_es} for the Spanish intervention and Appendix \ref{sec:appendix_dotplots} for the remaining four languages). This finding is consistent across most target languages and models. Candidate languages present in the pre-training data generally demonstrate larger gains post-intervention. The pattern of improvement differs based on the model. Specifically, for Aya-8B a successful intervention results in consistent improvements in top-1 accuracy for the majority of candidate languages (median=32\%; max=74\%). For Bloom-7B, top-1 accuracy gains are large (up to 89\%) for a small number of candidate languages, with moderate improvements for other languages (median=14\%). For PolyLM-13B, the improvements are small (median=0.5\%; max=12\%).

To better understand how the increased alignment in the embedding space influences cross-lingual retrieval, we look at the mean pairwise cosine distances between the query and candidate languages and explore how this correlates with retrieval accuracy (Fig.~\ref{fig:dotplots_es}). Table~\ref{table:performance_corrs} shows average correlations between post-intervention top-1 retrieval accuracy ($\text{acc}_{\text{\small{post}}}$) and mean query-candidate language distance both pre- and post-intervention ($d_{\text{\small{pre}}}$, $d_{\text{\small{post}}}$), average correlations between $d_{\text{\small{pre}}}$ and $d_{\text{\small{post}}}$, and average correlations between improvement in accuracy ($\Delta\text{acc}=\text{acc}_{\text{\small{post}}} - \text{acc}_{\text{\small{pre}}}$) and change in distance between pre- and post-intervention embeddings ($d_{\text{\small{pre}}}-d_{\text{\small{post}}}$). When calculating averages, we only include candidate languages seen in pre-training for each model; we note that the general pattern stays the same but the correlations are somewhat weaker if all 22 languages are considered for all models.


We find that in this setting, when the query and intervention-target language are the same, the distance between query/target and match language is predictive of top-1 cross-lingual retrieval accuracy in both pre- and post-intervention spaces. 

As discussed in Section \ref{sec:embedding_space}, all language embeddings move closer to the target's embeddings post-intervention, which explains the gains in cross-lingual retrieval accuracy. The distances in the unintervened and intervened space are positively correlated---language embeddings that are closer to the target pre-intervention are also closer to the target post-intervention. However, the magnitude of the performance gain in the intervened space does not correlate with the reduction in distance between the match and target languages across the two spaces, suggesting that the increased alignment post-intervention cannot be simply explained by a reduction in distances. 


\begin{figure*}[h]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/bloom/heatmap_Bloom-7b_Spanish.png}
         \caption{Bloom-7b}
         \label{fig:bloom_heatmap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/cohere/heatmap_Aya-8b_Spanish.png}
         \caption{Aya-8b}
         \label{fig:aya_heatmap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/polylm/heatmap_PolyLM-13b_Spanish.png}
         \caption{PolyLM-13b}
         \label{fig:polylm_heatmap}
     \end{subfigure}
        \caption{$(\text{Top-1 accuracy}_{\text{post-intervention}} - \text{Top-1 accuracy}_ {\text{pre-intervention}})$ for $22$ languages after intervening on Spanish expert neurons in the Bloom-7B model (left), Aya-8B (middle), and PolyLM-13B (right). The languages that are not in the training set for a given model are marked in red.}
        \label{fig:heatmaps_es}
\end{figure*}


\paragraph{Top-1 retrieval accuracy improves post-intervention for retrieval with the non-target languages.} In Section~\ref{sec:embedding_space}, we found that the distances between almost all languages decrease post-intervention---not just the distances to the intervention target. We next examine if these reduced cosine distances between languages \textit{other than} the intervention target  translate into improved top-1 retrieval accuracy when using these languages as the query language.

For example, we study if intervening on Spanish experts improves Dutch-English retrieval (in this case, neither the query nor candidate language is the intervention-target language).

We find that, perhaps surprisingly,  improvements observed when the query language is the intervention target (see Fig.\ ~\ref{fig:dotplots_es} and Table~\ref{table:performance_corrs}) carry over to query languages other than the intervention-target language (see Fig.\ \ref{fig:heatmaps_es} for the Spanish intervention and Appendix \ref{sec:appendix_heatmaps} for the remaining four languages). For example, the intervention on Spanish expert neurons for Bloom-7B results in retrieval improvement when English, French, and Portuguese are the query language. The same intervention improves retrieval when querying Hebrew with Persian or when querying Czech with Greek in the Aya-8B model and when querying Russian with Portuguese in PolyLM-13B. These are examples of larger improvements, but many other languages follow the same pattern with smaller gains. Generally, the patterns in improvement are consistent with those seen when Spanish is the query language. Languages that are in the pre-training set have larger accuracy gains. Bloom-7B has large improvements for a small number of languages and no drops in performance. Aya-8B has relatively large improvements for a majority of languages but also has a drop in performance for some. As noted previously, PolyLM-13B performance is uneven---the improvement varies by language with languages in the pre-training set generally having larger improvements.


\section{Within-language similarity is preserved in the new space}
\label{sec:paraphrase_retrieval}

As observed in Section \ref{sec:embedding_space}, all languages move toward the medoid of the embedding space post-intervention, which raises the question of whether language-specific similarities preserved in the new space. To answer this question, we evaluate performance on a \textbf{paraphrase} retrieval task which tests whether a sentence in the intervened space can be matched with its paraphrase in the intervened space. We utilize the PAWS-X dataset \cite{hu2020xtreme}, which provides paired sentences across seven languages, including all five of our intervention targets. From the test split, we retain only the paraphrase pairs, excluding non-paraphrases and sentences from other languages. This transforms our evaluation into a within-language sentence retrieval task, where the goal is to match each sentence with its paraphrase from a pool of candidates for that language.

The paraphrase retrieval task reveals two key findings about embedding spaces before and after the intervention. First, the top-1 paraphrase retrieval accuracy remains largely unchanged after the intervention (see Table~\ref{tab:paraphrase_retrieval_spanish}), indicating that the new embedding space preserves within-language similarity. Second, when attempting retrieval between intervened and unintervened embeddings of the same language --- i.e., using the embeddings from the unintervened model as the query and the embeddings from the intervened model as candidate matches --- accuracy drops significantly. This decline supports the observation in Section \ref{sec:embedding_space} suggesting that the intervention projects embeddings into a distinctly different space from their original unintervened representations. This finding also aligns with the increase in perplexity observed post-intervention -- the intervened space of a given language is \emph{not} the same space as the original space of this language.


\begin{table}[t!]
  \centering
  \resizebox{0.49\textwidth}{!}{
\begin{tabular}{lrrr}
\toprule
 \textbf{Model} & \multicolumn{3}{c}{\textbf{Top-1 Accuracy}} \\
  & (Pre) & (Post) & (Mixed) \\
\midrule
 Bloom-7B  & 0.80 & 0.80
 & 0.33 \\
 Aya-8B  & 0.85 & 0.86 & 0.64  \\

 PolyLM-13B  & 0.52 & 0.56 & 0.41 \\
\bottomrule
\end{tabular}
}
  \caption{Top-1 accuracy results for the paraphrase retrieval task following the  intervention on Spanish. The results for other languages can be found in Appendix \ref{appendix:paraphrase_retrieval_all}.  Pre = both the query and candidate embeddings are from the original model; Post = both the query and the candidate embeddings are from the intervened model; Mixed = query is from the original model and candidates are from the intervened model.}
  \label{tab:paraphrase_retrieval_spanish}
  \vspace{-10pt}
\end{table}


\section{Intervention on random neurons does not provide an improvement on downstream tasks}
\label{sec:random_experts}

In our analyses so far, we have attributed the changes in the embedding space to the intervention on expert neurons. Before we conclude, we consider an alternative possibility --- that the expert neurons do not play a role in increasing alignment post-intervention, but instead alignment is achieved by fixing the activations of a number of neurons in a network. To address this, we assign the activation levels of the language expert neurons used in prior sections to the same number of neurons chosen randomly in the network and repeat our analyses on these models.

We find that intervening on random neurons produces markedly different results compared to activating language experts (see Appendix \ref{sec:appendix_random_models}). The embedding space after the intervention on random neurons does not have the same properties as described in Section \ref{sec:embedding_space}, which translates into the performance on downstream tasks for all models. Specifically, for the Aya-8B and PolyLM-13B top-1 cross-lingual retrieval accuracy drops for all languages post-intervention on random neurons compared to pre-intervention. Interestingly, for Bloom-7B, there is mostly no change for all target languages except French, which surprisingly improves post-intervention on random neurons. However, the gains are significantly smaller compared to those after intervening on French experts. Similar to the intervention on language experts, within-language paraphrase retrieval shows only small changes post-intervention. When they occur, these changes tend to be negative (i.e., the performance drops) after intervening on random neurons and positive after intervening on the actual language experts. 

\section{Conclusions}
We present a novel analysis of the impact of the finding-experts intervention on cross-lingual alignment in mLLMs. We find that intervening on language experts projects model embeddings into a new space where languages are more aligned than in the original space but still preserve within-language similarity. These findings provide an explanation for the increase in perplexity observed post-intervention in prior work \cite{pmlr-v162-cuadros22a}. The increase in cross-lingual alignment results in up to 2x improvement in top-1 retrieval accuracy. Additionally, we show that the correlation between cross-lingual alignment and cross-lingual retrieval is high and statistically significant. Importantly, these improvements are not restricted to the intervention-target language. For instance, an intervention on Japanese results in a large top-1 retrieval accuracy improvement for English-Portuguese for Bloom-7B. We find that the three models we study show markedly different patterns both in the changes to the embedding space and downstream tasks. We leave it to future to work to determine the causes of these differences, though we hypothesize that they are due to the pre-training differences.

\section{Limitations}
\label{sec:limitations}
There are several limitations that need to be considered when interpreting our results.

The major limitation is that we are working with pre-trained models and we have only limited information on training data and procedure. Specifically, for Bloom-7B and PolyLM-13B, we have the information on the proportion of each language in the pre-training set. For Aya-8B, only information on which languages were seen in the pre-training (but no proportions) is available. 

We observe different performance gains post-intervention for the three models under consideration and while we hypothesize that these differences are due to training data and/or procedure, we do not have enough information to test this hypothesis. Future work should explore the effect of intervention on alignment in a more controlled setting where the models are trained from scratch on a publicly available dataset manipulating language proportions in the training data to better understand what is driving the difference.

We have studied only one approach out of a family of approaches to controllable generations \cite{rimsky_steering, suau2024whispering, rodriguez2024controlling}. Each approach in the family comes with its differences -- in the way the neurons targeted by the intervention are discovered, how the changes are introduced to the activations, how many neurons are intervened on, etc. We do not fully understand how these design decisions impact the representation space. For example, it is possible that some of these approaches are more beneficial for alignment while others introduce changes that are more beneficial for other tasks (or not at all). The comparison of approaches is beyond the scope of current work and we leave it for future investigations.



\bibliography{main}

\appendix
% \documentclass[draft]{article}
\section{List of languages studied}
\label{app:lang_list}

The following languages are considered in this work:
\onecolumn
\begin{table*}[h!]
\centering 
\begin{tabular}{rllll}
\toprule
\# & Language & ISO 639-1 & ISO 639-3 \\
\midrule
1 & Thai & th & tha \\
2 & Czech & cs & ces \\
3 & German & de & deu \\
4 & Greek & el & ell \\
5 & English & en & eng \\
6 & French & fr & fra \\
7 & Hebrew & he & heb \\
8 & Hindi & hi & hin \\
9 & Indonesian & id & ind \\
10 & Italian & it & ita \\
11 & Japanese & ja & jpn \\
12 & Korean & ko & kor \\
13 & Dutch & nl & nld \\
14 & Persian & fa & pes \\
15 & Polish & pl & pol \\
16 & Portuguese & pt & por \\
17 & Romanian & ro & ron \\
18 & Russian & ru & rus \\
19 & Spanish & es & spa \\
20 & Turkish & tr & tur \\
21 & Ukrainian & uk & ukr \\
22 & Vietnamese & vi & vie \\
\bottomrule
\end{tabular}
\caption{ISO 639-1 and ISO 639-3 Language Codes}
\end{table*}


\clearpage

\newpage

\section{Selecting expert neurons}
\label{app:selecting_neurons}
The value of $k$ is determined as follows. For each of the five languages, we sweep over expert set sizes ranging from 100 to 5000. For each setting of language and number of experts, we run free-form generation to generate 256 sentences over eight random seeds (for a total of 2048 sentences) using the beginning of sentence (BOS) token as the prompt. We perform generation with temperature=1 and top\_p=0.9 \footnote{The other hyper-parameters are set to default for \texttt{transformers==4.41}}. We then use lang-id \cite{lui-baldwin-2012-langid} to measure the probability of the text generated in the target language. 

To calculate the perplexity of Wikipedia text in the target language for the original and intervened models, we use the Wikimedia dump from 2023-11-01 \footnote{https://huggingface.co/datasets/wikimedia/wikipedia}. Paragraphs of text shorter than 100 characters are removed and the remaining paragraphs are concatenated together. Finally, a corpus of 10 million tokens is selected from the concatenated paragraphs. The context length is set to the model's maximum input size (in tokens) and a stride (i.e., sliding the context window) of 512 tokens is used to speed up the perplexity measurement. 

\clearpage

\newpage

\section{UMAP embeddings for  four intervention-target languages}
\label{sec:appendix_umaps}
Note: The embeddings post-intervention are marked with `*' for each language. The languages that are not in the training set are marked in red.

\subsection{Bloom-7B}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_en_100.jpg}
         
         %\label{fig:bloom_en_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_en_500.jpg}
         
         %\label{fig:bloom_de_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_en_1000.jpg}
         
        % \label{fig:bloom_fr_umap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_en_2000.jpg}\label{fig:bloom_jp_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_en_5000.jpg}
         
         \label{fig:bloom_en_umap}
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on English expert neurons}
        \label{fig:bloom_en_umaps}
\end{figure*}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_es_100.jpg}
         
         %\label{fig:bloom_en_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_es_500.jpg}
         
         %\label{fig:bloom_de_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_es_1000.jpg}
         
        % \label{fig:bloom_fr_umap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_es_2000.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_es_5000.jpg}
         
       
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on Spanish expert neurons}
        \label{fig:bloom_es_umaps}
\end{figure*}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_de_100.jpg}
         
         %\label{fig:bloom_en_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_de_500.jpg}
         
         %\label{fig:bloom_de_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_de_1000.jpg}
         
        % \label{fig:bloom_fr_umap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_de_2000.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_de_5000.jpg}
         
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on German  expert neurons}
        \label{fig:bloom_de_umaps}
\end{figure*}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_fr_100.jpg}
         
         %\label{fig:bloom_en_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_fr_500.jpg}
         
         %\label{fig:bloom_de_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_fr_1000.jpg}
         
        % \label{fig:bloom_fr_umap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_fr_2000.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_fr_5000.jpg}
         
         \label{fig:bloom_fr_umap}
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on French  expert neurons}
        \label{fig:bloom_fr_umaps}
\end{figure*}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_jp_100.jpg}
         
         %\label{fig:bloom_en_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_jp_500.jpg}
         
         %\label{fig:bloom_de_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_jp_1000.jpg}
         
        % \label{fig:bloom_fr_umap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_jp_2000.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom/umap_Bloom-7b_jp_5000.jpg}
         
         
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on Japanese expert neurons}
        \label{fig:bloom_jp_umaps}
\end{figure*}

\clearpage
\newpage
\subsection{Aya-8B}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_en_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_en_500.jpg}
         
        
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_en_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_en_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_en_5000.jpg}
         
        
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on English expert neurons}
        \label{fig:aya_en_umaps}
\end{figure*}


\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_es_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_es_500.jpg}
         
        
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_es_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_es_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_es_5000.jpg}
         
        
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on Spanish expert neurons}
        \label{fig:aya_es_umaps}
\end{figure*}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_de_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_de_500.jpg}
         
        
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_de_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_de_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_de_5000.jpg}
         
        
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on German expert neurons}
        \label{fig:aya_de_umaps}
\end{figure*}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_fr_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_fr_500.jpg}
         
        
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_fr_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_fr_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_fr_5000.jpg}
         
        
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on French expert neurons}
        \label{fig:aya_fr_umaps}
\end{figure*}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_jp_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_jp_500.jpg}
         
        
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_jp_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_jp_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere/umap_Aya-8b_jp_5000.jpg}
         
        
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on Japanese expert neurons}
        \label{fig:aya_jp_umaps}
\end{figure*}
\clearpage
\newpage
\subsection{PolyLM-13B}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_en_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_en_500.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_en_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_en_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_en_5000.jpg}
         
         
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on English expert neurons}
        \label{fig:polylm_en_umaps}
\end{figure*}


\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_es_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_es_500.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_es_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_es_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_es_5000.jpg}
         
         
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on Spanish expert neurons}
        \label{fig:polylm_es_umaps}
\end{figure*}


\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_de_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_de_500.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_de_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_de_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_de_5000.jpg}
         
         
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on German expert neurons}
        \label{fig:polylm_de_umaps}
\end{figure*}


\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_fr_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_fr_500.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_fr_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_fr_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_fr_5000.jpg}
         
         
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on French expert neurons}
        \label{fig:polylm_fr_umaps}
\end{figure*}


\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_jp_100.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_jp_500.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_jp_1000.jpg}
         
         
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_jp_2000.jpg}
         
         
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm/umap_PolyLM-13b_jp_5000.jpg}
         
         
     \end{subfigure}
     
        \caption{UMAP embeddings for intervention on Japanese expert neurons}
        \label{fig:polylm_jp_umaps}
\end{figure*}

\clearpage
\newpage 
\section{Paraphrase retrieval accuracy for four intervention-target languages}
\label{appendix:paraphrase_retrieval_all}

\begin{table*}[h]
  \centering
  % \resizebox{\textwidth}{!}{
\begin{tabular}{llrrrr}
\toprule
\textbf{Model} & \textbf{Language} & \multicolumn{3}{c}{\textbf{Top-1 Accuracy}} \\
 & &  (Pre) & (Post) & (Mixed) \\
\midrule
\multirow{5}{*}{Bloom-7B } 
& en & 0.80 & 0.80 & 0.71 \\
& fr & 0.80 & 0.80 & 0.26 \\
& de & 0.72 & 0.75 & 0.22 \\
& ja & 0.47 & 0.59 & 0.07 \\
%& zh & 0.71 & 0.73 & 0.58 \\
\midrule
\multirow{5}{*}{Aya-8B }
& en & 0.87 & 0.87 & 0.56 \\
& fr & 0.83 & 0.83 & 0.75 \\
& de & 0.82 & 0.82 & 0.62 \\
& ja & 0.70 & 0.76 & 0.55 \\
%& zh & 0.82 & 0.84 & 0.74 \\
\midrule
\multirow{5}{*}{PolyLM-13B }
& en & 0.55 & 0.53 & 0.48 \\
& fr & 0.52 & 0.50 & 0.44 \\
& de & 0.50 & 0.55 & 0.39 \\
& ja & 0.57 & 0.57 & 0.32 \\
%& zh & 0.70 & 0.67 & 0.62 \\
\bottomrule
\end{tabular}
% }
\caption{Top-1 accuracy results for the paraphrase retrieval task for four intervention languages. Pre= both the query and the candidate embeddings are from the original unintervened model; Post= both the query and the candidate embeddings are from the intervened model; Mixed = query embedding is from the original model and the candidates are from the intervened model.}
  \label{tab:paraphrase_retrieval_all_appendix}
\end{table*}


\clearpage
\newpage 


\section{Top-1 cross-lingual retrieval accuracy for four intervention-target languages (query language is the same as the intervention target)}
\label{sec:appendix_dotplots}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/bloom/dotplot_Bloom-7b_English.png}
         
         \label{fig:bloom_en_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/bloom/dotplot_Bloom-7b_German.png}
         
         \label{fig:bloom_de_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/bloom/dotplot_Bloom-7b_French.png}
         
         \label{fig:bloom_fr_dotplot}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/bloom/dotplot_Bloom-7b_Japanese.png}
         
         \label{fig:bloom_jp_dotplot}
    
     \end{subfigure}
     
        \caption{Top-1 retrieval accuracy for $22$ languages in the Bloom-7B model. The language of the intervention is provided in the caption to each subfigure. The languages that are not in the training set for a given model are marked in red.}
        \label{fig:bloom_five_dotplots}
\end{figure*}

\begin{figure*}[hbt!]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/cohere/dotplot_Aya-8b_English.png}
         
         \label{fig:aya_en_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/cohere/dotplot_Aya-8b_German.png}
         
         \label{fig:aya_de_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/cohere/dotplot_Aya-8b_French.png}
         
         \label{fig:aya_fr_dotplot}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/cohere/dotplot_Aya-8b_Japanese.png}
         
         \label{fig:aya_jp_dotplot}
     
     \end{subfigure}
     
        \caption{Top-1 retrieval accuracy for $22$ languages in the Aya-8B model. The language of the intervention is provided in the caption to each subfigure. The languages that are not in the training set for a given model are marked in red.}
        \label{fig:aya_five_dotplots}
\end{figure*}



\begin{figure*}[hbt!]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/polylm/dotplot_PolyLM-13b_English.png}
         
         \label{fig:polylm_en_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/polylm/dotplot_PolyLM-13b_German.png}
         
         \label{fig:polylm_de_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/polylm/dotplot_PolyLM-13b_French.png}
         
         \label{fig:polylm_fr_dotplot}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/polylm/dotplot_PolyLM-13b_Japanese.png}
         
         \label{fig:polylm__dotplot}
     
     \end{subfigure}
     
        \caption{Top-1 retrieval accuracy for $22$ languages in the PolyLM-chat-13b model. The language of the intervention is provided in the caption to each subfigure. The languages that are not in the training set for a given model are marked in red.}
        \label{fig:polylm_five_dotplots}
\end{figure*}

\clearpage
\newpage

\section{Top-1 cross-lingual retrieval accuracy for four intervention-target languages (query language is different from the intervention target)}
\label{sec:appendix_heatmaps}


\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/bloom/heatmap_Bloom-7b_English.png}
         
         \label{fig:bloom_en_heatmap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/bloom/heatmap_Bloom-7b_German.png}
         
         \label{fig:bloom_de_heatmap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/bloom/heatmap_Bloom-7b_French.png}
         
         \label{fig:bloom_fr_heatmap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/bloom/heatmap_Bloom-7b_Japanese.png}
         
         \label{fig:bloom_jp_heatmap}
     
     \end{subfigure}
     
        \caption{$(\text{Top-1 accuracy}_{\text{post-intervention}} - \text{Top-1 accuracy}_ {\text{pre-intervention}})$ for Bloom-7B. The language of the intervention is provided in the caption to each subfigure. The languages that are not in the training set are marked in red.}
        \label{fig:bloom_five_heatmaps}
\end{figure*}

\begin{figure*}[hbt!]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/cohere/heatmap_Aya-8b_English.png}
         
         \label{fig:aya_en_heatmap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/cohere/heatmap_Aya-8b_German.png}
         
         \label{fig:aya_de_heatmap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/cohere/heatmap_Aya-8b_French.png}
         
         \label{fig:aya_fr_heatmap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/cohere/heatmap_Aya-8b_Japanese.png}
         
         \label{fig:aya_jp_heatmap}
     \end{subfigure}
     
     
        \caption{$(\text{Top-1 accuracy}_{\text{post-intervention}} - \text{Top-1 accuracy}_ {\text{pre-intervention}})$ for Aya-8B. The language of the intervention is provided in the caption to each subfigure. The languages that are not in the training set are marked in red.}
        \label{fig:aya_five_heatmaps}
\end{figure*}



\begin{figure*}[hbt!]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/polylm/heatmap_PolyLM-13b_English.png}
         
         \label{fig:polylm_en_heatmap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/polylm/heatmap_PolyLM-13b_German.png}
         
         \label{fig:polylm_de_heatmap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/polylm/heatmap_PolyLM-13b_French.png}
         
         \label{fig:polylm_fr_heatmap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/heatmaps/polylm/heatmap_PolyLM-13b_Japanese.png}
         
         \label{fig:polylm_jp_dotplot}
     \end{subfigure}
     
     
        \caption{$(\text{Top-1 accuracy}_{\text{post-intervention}} - \text{Top-1 accuracy}_ {\text{pre-intervention}})$ for PolyLM-13B. The language of the intervention is provided in the caption to each subfigure. The languages that are not in the training set are marked in red.}
        \label{fig:polylm_five_heatmaps}
\end{figure*}

\clearpage
\newpage

\section{Results for the interventions on random neurons}
\label{sec:appendix_random_models}

\subsection{Top-1 paraphrase retrieval accuracy after the intervention on random neurons}


\begin{table*}
  \centering
  \resizebox{\textwidth}{!}{
\begin{tabular}{llrrrr}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Language} & \multicolumn{3}{c}{Accuracy} \\
& & Pre & Post & mixed \\
\midrule
\multirow{5}{*}{Bloom-7B} 
& en & 0.80 & 0.80 & 0.78 \\
& es & 0.80 & 0.79 & 0.62 \\
& fr & 0.80 & 0.71 & 0.00 \\
& de & 0.72 & 0.72 & 0.62 \\
& ja & 0.47 & 0.45 & 0.35 \\
\midrule
\multirow{5}{*}{PolyLM-13B}
& en & 0.55 & 0.55 & 0.51 \\
& es & 0.53 & 0.53 & 0.48 \\
& fr & 0.53 & 0.54 & 0.50 \\
& de & 0.50 & 0.54 & 0.45 \\
& ja & 0.60 & 0.58 & 0.23 \\
\midrule
\multirow{5}{*}{Aya-8B}
& en & 0.87 & 0.81 & 0.00 \\
& es & 0.85 & 0.73 & 0.01 \\
& fr & 0.83 & 0.70 & 0.01 \\
& de & 0.82 & 0.70 & 0.00 \\
& ja & 0.70 & 0.44 & 0.00 \\
\bottomrule
\end{tabular}
}
\caption{Top-1 accuracy results for the paraphrase retrieval task for five intervention languages for the intervention on random neurons. Pre= both the query and the candidate embeddings are from the original unintervened model; Post= both the query and the candidate embeddings are from the intervened model; Mixed = query embedding is from the original model and the candidates are from the intervened model.}
  \label{tab:paraphrase_retrieval_random}
\end{table*}

\clearpage
\newpage

\subsection{UMAP embeddings for the interventions on random neurons}
\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom-random/umap_Bloom-7b_de_100.jpg}
         
         %\label{fig:bloom_en_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom-random/umap_Bloom-7b_en_100.jpg}
         
         %\label{fig:bloom_de_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom-random/umap_Bloom-7b_fr_100.jpg}
         
         %\label{fig:bloom_fr_umap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/bloom-random/umap_Bloom-7b_jp_100.jpg}
         %\label{fig:bloom_jp_umap}
     \end{subfigure}
     
     
        \caption{UMAP embeddings for interventions on random neurons for Bloom-7B}
        \label{fig:bloom-random_five_umaps}
\end{figure*}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere-random/umap_Aya-8b_de_2000.jpg}
         
         %\label{fig:aya_en_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere-random/umap_Aya-8b_en_2000.jpg}
         
         %\label{fig:aya_de_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere-random/umap_Aya-8b_fr_2000.jpg}
         
         %\label{fig:aya_fr_umap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/cohere-random/umap_Aya-8b_jp_2000.jpg}
         
         
     \end{subfigure}
     
        \caption{UMAP embeddings for interventions on random neurons for Aya-8B}
        \label{fig:aya-random_five_umaps}
\end{figure*}



\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm-random/umap_PolyLM-13b_de_2000.jpg}
         
         %\label{fig:polylm_en_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm-random/umap_PolyLM-13b_en_2000.jpg}
         
         %\label{fig:polylm_de_umap}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm-random/umap_PolyLM-13b_fr_2000.jpg}
         
        % \label{fig:polylm_fr_umap}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/umaps/polylm-random/umap_PolyLM-13b_jp_2000.jpg}
         
         
     \end{subfigure}
     
        \caption{UMAP embeddings for interventions on random neurons for PolyLM-13B}
        \label{fig:polylm-random_five_umaps}
\end{figure*}

\clearpage
\newpage

\subsection{Top-1 Retrieval accuracy for interventions on random neurons}

\begin{figure*}[h]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/bloom-random/dotplot_Bloom-7b_German.png}
         
         \label{fig:bloom-random_de_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/bloom-random/dotplot_Bloom-7b_English.png}
         
         \label{fig:bloom-random_en_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/bloom-random/dotplot_Bloom-7b_French.png}
         
         \label{fig:bloom-random_fr_dotplot}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/bloom-random/dotplot_Bloom-7b_Spanish.png}
         
         \label{fig:bloom-random_jp_dotplot}
    
     \end{subfigure}
     
        \caption{Top-1 retrieval accuracy for $22$ languages in the Bloom-7B model with the intervention on $100$ random neurons. The language of the intervention is provided in the caption to each subfigure. The languages that are not in the training set are marked in red.}
        \label{fig:bloom-random_five_dotplots}
\end{figure*}



\begin{figure*}[hbt!]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/cohere-random/dotplot_Aya-8b_German.png}
         
         \label{fig:aya-random_de_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/cohere-random/dotplot_Aya-8b_English.png}
         
         \label{fig:aya-random_en_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/cohere-random/dotplot_Aya-8b_French.png}
         
         \label{fig:aya-random_fr_dotplot}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/cohere-random/dotplot_Aya-8b_Japanese.png}
         
         \label{fig:aya___dotplot}
     
     \end{subfigure}
     
        \caption{Top-1 retrieval accuracy for $22$ languages in the Aya-8B model with the intervention on $2000$ random neurons. The language of the intervention is provided in the caption to each subfigure. The languages that are not in the training set are marked in red.}
        \label{fig:aya-random_five_dotplots}
\end{figure*}



\begin{figure*}[hbt!]
     \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/polylm-random/dotplot_PolyLM-13b_German.png}
         
         \label{fig:polylm-random_de_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/polylm-random/dotplot_PolyLM-13b_English.png}
         
         \label{fig:polylm-random_en_dotplot}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/polylm-random/dotplot_PolyLM-13b_French.png}
         
         \label{fig:polylm_random_dotplot}
     \end{subfigure}
        \hfill
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{latex/figures/dotplots/polylm-random/dotplot_PolyLM-13b_Japanese.png}
         
         \label{fig:polylm-random_jp_dotplot}
     
     \end{subfigure}
     
        \caption{Top-1 retrieval accuracy for $22$ languages in the PolyLM-13b-chat model with the intervention on $2000$ random neurons. The language of the intervention is provided in the caption to each subfigure. The languages that are not in the training are marked in red.}
        \label{fig:polylm-random_five_dotplots}
\end{figure*}

\clearpage
\newpage

\section{Cross-lingual retrieval results on BUCC-18 and Tatoeba}
\label{sec:app_bucc_tatoeba}
%The Tatoeba and BUCC-18 datasets \cite{hu2020xtreme} consist of sentences paired with their translations in English. For a given target language, we identify and set expert neurons for that language to their average activations based on the Flores-200 dataset. Following the expert activation, we perform cross-lingual retrieval between the target language and the English sentences. Given a sentence in French (say), the goal is to retrieve the corresponding English translation. We compute embeddings for all sentences and retrieve the translation embedding with the largest cosine similarity. Table \ref{tab:bucc18tatoeba} reports the Top-1 accuracy of this retrieval task. 

\begin{table}[h]
    \centering
    \begin{tabular}{l c c c}
        \toprule 
        Model & \multirow{2}*{Language} & \multicolumn{2}{c}{Top-1 Accuracy} \\
        & & Pre & Post \\
        \midrule 
        % \rowcolor{gray!20} 
        \multicolumn{4}{c}{\textbf{Tatoeba}} \\
        \midrule 
        \multirow{4}*{Aya-8B} & es & 0.114 & 0.415 \\
                                       & fr & 0.087 & 0.251 \\
                                       & de & 0.119 & 0.444 \\
                                       & jp & 0.034 & 0.307 \\
        \midrule 
        \multirow{4}*{Bloom-7B} & es & 0.008 & 0.551 \\
                                      & fr & 0.011 & 0.434 \\
                                      & de & 0.006 & 0.032 \\
                                      & jp & 0.002 & 0.043 \\
        \midrule
        \multirow{4}*{PolyLM-13B} & es & 0.082 & 0.178 \\
                                      & fr & 0.067 & 0.130 \\
                                      & de & 0.029 & 0.171 \\
                                      & jp & 0.000 & 0.040 \\
        \midrule 
        % \rowcolor{gray!20} 
        \multicolumn{4}{c}{\textbf{BUCC-18}} \\
        \midrule 
        \multirow{2}*{Aya-8B} & fr & 0.012 &  0.073 \\
                                       & de & 0.017 & 0.332 \\
        \midrule 
        \multirow{2}*{Bloom-7B} & fr & 0.000 &  0.287 \\
                                       & de & 0.000 & 0.02 \\
        \midrule 
        \multirow{2}*{PolyLM-13B} & fr & 0.006 &  0.286 \\
                                       & de & 0.008 & 0.281 \\
        \bottomrule
    \end{tabular}
    \caption{Top-1 retrieval for the intervetion on five target languages for Tatoeba and BUCC-18. Pre= original model; Post= intervened model.}
    \label{tab:bucc18tatoeba}
\end{table}


\section{Computational budget}
All experiments were run on 8 A100(80GB) GPUs. The total approximate running time for 90 GPU/hours Aya-8B, 120 GPU/hours for PolyLM-13B, and 110 GPU/hours for Bloom-7B.

\section{License and Attribution}
All datasets used in this work are supported by public licenses. PAWS-X, Tatoeba, BUCC are part of the XTREME benchmark licensed under Apache; Flores200 is licensed under Creative Commons. The pre-trained models used in this work are also supported by public licenses Bloom-7B (RAIL 1.0), Aya-8B (Creative Commons), and PolyLM-chat-13B (Apache).


\end{document}
