\section{Background}
\label{sec:background}

Shapley Additive exPlanations (SHAP) \cite{NIPS2017_Lundberg} is a technique for estimating each feature's contribution to the output $y$ of a deep learning model $f(x)$. Rooted in cooperative game theory, SHAP is based on Shapley values, introduced by Lloyd Shapley as a method for fairly distributing payouts among participants in cooperative games \cite{shapley:book1952}. SHAP values correspond to the Shapley values of a conditional expectation function derived from the model, capturing feature interactions and dependencies to provide robust explanations.

In practice, SHAP isolates the impact of individual features ($w_i \in x$) on the model's output while accounting for the influence of other features ($x \setminus w_i$). It calculates the average difference in predictions when a feature is included versus excluded, offering insights into how features influence the modelâ€™s decisions. SHAP is applicable across various models, including tree-based \cite{kumar_shapley_problems_2020} and neural network models \cite{ahn_shapley_www_2024}, enabling researchers to identify key predictors and analyze model behavior. Its flexibility and strong theoretical foundation make SHAP invaluable for post-hoc interpretability \cite{sundararajan_many_shapley_2020}, particularly in applications requiring both accuracy and interpretability, such as medical diagnostics \cite{stiglic_health_interpretability_2020}, financial risk assessment \cite{barnes_finance_interpret} and software engineering tasks, as explored in this study.


