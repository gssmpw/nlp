\begin{abstract}
Recent advancements in Large Language Models (\llms) have paved the way for Large Code Models (\lcms), enabling automation in complex software engineering tasks, such as code generation, software testing, and program comprehension, among others. Tools like GitHub Copilot and ChatGPT have shown substantial benefits in supporting developers across various practices. However, the ambition to scale these models to trillion-parameter sizes, exemplified by GPT-4, poses significant challenges that limit the usage of Artificial Intelligence (AI)-based systems powered by large Deep Learning (DL) models. These include rising computational demands for training and deployment and issues related to trustworthiness, bias, and interpretability. Such factors can make managing these models impractical for many organizations, while their ``black-box'' nature undermines key aspects, including transparency and accountability. In this paper, we question the prevailing assumption that increasing model parameters is always the optimal path forward, provided there is sufficient new data to learn additional patterns. In particular, we advocate 
for a \textbf{N}eurosymbolic research direction that combines the strengths of existing DL techniques (\eg LLMs) with traditional symbolic methods--renowned for their reliability, speed, and determinism. To this end, we outline the core features and present preliminary results for our envisioned approach, aimed at establishing the first \textbf{N}eurosymbolic \textbf{P}rogram \textbf{C}omprehension (\framework) framework to aid in identifying defective code components.

%Preliminary experiments in this area underscore the promise and feasibility of the proposed neuro-symbolic approach within the broader context of program comprehension, enhancing both its interpretability and transparency. This balance is achieved through a symbolic component that we shaped using interpretability methods. 


%\ALEJO{Recent advancements in Large Language Models (\llms) have paved the way for Large Code Models (\lcms), enabling automation in complex software engineering tasks, such as code generation, software testing, and program comprehension. However, the push to scale these models to trillion-parameter sizes, as seen with GPT-4, introduces significant adoption challenges. These include increased computational demands for training and deployment, along with concerns around trustworthiness, bias, and interpretability. For many organizations, these factors make managing such models impractical, while their “black-box” nature compromises essential including transparency and accountability. In this paper, we challenge the prevailing assumption that increasing model parameters is the optimal approach for enhancing \lcms performance, provided new data is available for further learning. In particular, we advocate for a NeuroSymbolic framework that combines the strengths of existing DL techniques, with traditional symbolic methods--renowned for their reliability, speed, and determinism. To this end, we outline the core components and present preliminary results for our envisioned approach, aimed at creating the first \textbf{N}euroSymbolic \textbf{P}rogram \textbf{C}omprehension (NPC) framework to aid in identifying defective code components. To the best of our knowledge, this is the first documented attempt toward embedding a symbolic layer into the probabilistic framework of \lcms for program comprehension, opening a promising new direction for automating software engineering practices with enhanced interpretability and transparency.}

\end{abstract}

\begin{IEEEkeywords}
Neuro-Symbolic AI, Vulnerability Detection, Program Comprehension, Interpretability.
\end{IEEEkeywords}