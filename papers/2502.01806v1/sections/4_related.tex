\section{Related Work}
\label{sec:related_work}


%%%%% Intro to related works %%%%%%%%%%% 

In this section, we present an overview of studies relevant to this paper, including (i) the interpretability of models for SE and; (ii) applications of neurosymbolic AI for SE. 


%%%%%%%%%%% Interpretability %%%%%%%%%%% 
\textbf{Interpretability of Models for SE:} Chen \etal \cite{chen_cat-probing_2022} introduce CAT-probing, a method to quantitatively interpret how pre-trained models (CodePTMs) for programming languages capture the structural properties of code. They highlight that the middle layers in models may significantly influence transfer of general structural knowledge, while later layers refine task-specific knowledge. Anand \etal \cite{anand_critical_2024} approach interpretability of code \llms (cLLMs) via attention analysis and show that attention maps of cLLMs fail to encode syntactic-identifier relations. Bui \etal \cite{bui_autofocus_2019} aim to enhance the interpretability of attention-based models for code by adapting code perturbations to evaluate the meaningful code elements. Other research works proposed interpretability techniques by applying the principles of information storage \cite{haider_looking_2024},  AST-probe \cite{majdinasab_deepcodeprobe_2024}, and syntactic structures combined with prediction confidence \cite{palacio_towards_2024}.

%%%%%%%%%%% Neurosymbolic %%%%%%%%%%% 
\textbf{Neurosymbolic AI in SE:} Princis \etal \cite{princis_sql_2024} integrate symbolic reasoning techniques into \llms to improve SQL query generation. This hybrid system leverages symbolic checks for query validation and repair during the generation process. To achieve this, the system employs partial query evaluation and early elimination of invalid queries, significantly improving runtime and accuracy. The study does not explore the interpretability of this hybrid system. 

Arakelyan \etal \cite{arakelyan_ns3_2022} combine neural and symbolic methods to improve the multi-step reasoning and compositional querying abilities of semantic code search (SCS) systems. The approach utilizes rule-based parsing of the natural language queries to identify matches between the parsed query components and code snippets. The rules, however, are manually created by the authors and might not generalize well for other natural and programming languages.

Jana \etal \cite{jana_cotran_2024} present CoTran, an LLM-based neurosymbolic system for translating code between programming languages. The proposed system leverages a \textit{symbolic execution feedback} to ensure functional equivalence of translated code. The code translation is available between Java and Python languages. Integration of the symbolic component improves the system's ability to maintain the original code's logic and leads to more robust and reliable translations. 


There are also works on the applications of neurosymbolic AI techniques in program synthesis \cite{parisotto_neuro-symbolic_2016} \cite{bosnjak_programming_2017}, representation learning \cite{allamanis_learning_2017}, error correction \cite{xue_interpretable_2024}, semantic code repair \cite{devlin_semantic_2017}, and bug fixing \cite{hu_fix_2022}.



%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{BACKGROUND - SHAP}

%SHapley Additive exPlanations (SHAP) values are widely used to interpret machine learning model predictions by attributing importance to individual features \cite{NIPS2017_Lundberg}. The method is rooted in Shapley values - developed from cooperative game theory and named after Lloyd Shapley \cite{shapley:book1952}. He introduced Shapley Values as a solution for fairly distributing payouts among participants in cooperative games. In the context of machine learning, SHAP assigns marginal contributions to features across all permutations of feature subsets, resulting in explanations that account for feature interactions and dependencies. In practice, SHAP values can be applied across a variety of models, including tree-based \cite{kumar_shapley_problems_2020} and neural network models \cite{ahn_shapley_www_2024}, allowing researchers to identify key predictors for each instance and evaluate model behavior across multiple instances. SHAPâ€™s adaptability and theoretical basis make it a valuable tool for post-hoc interpretability \cite{sundararajan_many_shapley_2020}. Using SHAP is advantageous for applications where both predictive accuracy and interpretability are essential, such as in medical diagnostics \cite{stiglic_health_interpretability_2020}, financial risk assessment \cite{barnes_finance_interpret}, and, as discussed here, software engineering tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%