\section{Methodology}
\label{sec:approach}



In this section, we introduce our \textbf{N}eurosymbolic \textbf{P}rogram \textbf{C}omprehension (\framework) framework, which leverages a feature-importance interpretability method to both interpret and guide model predictions, as illustrated in \figref{fig:npc_pipeline}. First, we describe SHAP, the interpretability method we use to generate local explanations for individual predictions. Next, we outline our approach for identifying patterns within the SHAP values computed for input features. Finally, we explain how these patterns are converted into symbolic rules to enhance model performance, particularly in cases with low prediction confidence.


%%%%%%%%%%%%%%%%%%% NPC PIPELINE  
\begin{figure}[ht]
		\centering
  \vspace{-1.5em}
  \includegraphics[width=0.4\textwidth]{images/pipeline.pdf}
		\caption{\framework depicted as a sequence of steps.}
    \label{fig:npc_pipeline}
    \vspace{-1em}
\end{figure}
%%%%%%%%%%%%%%%%%%

%%%%%%% RESULTS TABLE 
\input{tables/results}
%%%%%%%%


\subsection{SHAP values ($\phi$)}
%Shapley Additive exPlanations (SHAP) \REF is an additive feature importance technique that allows us to estimate each feature's contribution to the output $y$ of a deep learning model $f(x)$. SHAP is based on cooperative game theory, specifically the concept of Shapley values \REF. More precisely, SHAP values ($\phi$) correspond to the Shapley values of a conditional expectation function derived from the original model. To compute the SHAP value $\phi$ for a feature $w_i \in x$, we use \equref{eq:shapley_values}. In this equation, $W$ represents the set of input features in $x$, while $z'$ and $x'$ are binary vectors that indicate feature presence, with $z', x' \in {0,1}^W$. The expression $|z'|$ denotes the number of non-zero entries in $z'$, and $z' \subseteq x'$ indicates all vectors $z'$ where non-zero entries are a subset of those in $x'$.

Shapley Additive exPlanations (SHAP) \cite{NIPS2017_Lundberg} is an additive feature importance technique used to estimate each feature's contribution to the output $y$ of a deep learning model $f(x)$. Rooted in cooperative game theory, SHAP is based on Shapley values, introduced by Lloyd Shapley as a method for fairly distributing payouts among participants in cooperative games \cite{shapley:book1952}. SHAP values ($\phi$) correspond to the Shapley values of a conditional expectation function derived from the model. By assigning marginal contributions to features across all permutations of feature subsets, SHAP accounts for feature interactions and dependencies, providing robust, theoretically grounded explanations. To compute the SHAP value $\phi$ for a feature $w_i \in x$, we use \equref{eq:shapley_values}, where $W$ represents the set of input features in $x$, and $z'$ and $x'$ are binary vectors indicating feature presence, with $z', x' \in {0,1}^W$. The term $|z'|$ denotes the number of non-zero entries in $z'$, and $z' \subseteq x'$ indicates all vectors $z'$ where non-zero entries are a subset of those in $x'$. 

\begin{equation}
\label{eq:shapley_values}
    \phi_i = \sum_{z' \subseteq x'} \frac{|z'|! (|W| - |z'| - 1)!}{|W|!} \left[f_x(z') - f_x(z' \setminus w_i) \right]
\end{equation}


SHAP approximates \equref{eq:shapley_values} using the conditional expectation of the model: $f_x(z') \approx \mathbb{E}[f(z) \mid Z_S]$, where $S$ is the set of non-zero indices in $z'$. The features in $z$ are fixed to those in $S$ to isolate their impact on the output. The contribution of $w_i$ is determined by averaging the difference in the model output when $w_i$ is included versus excluded, across all subsets $S$ that do not contain $w_i$.

%To interpret model predictions, SHAP defines a surrogate model $g(x)$ as a linear combination of importance values assigned to each input feature: $g(x) = \phi_0 + \sum_{i=1}^{j}{\phi_i w_i}$, where $\phi_0 = f(\emptyset)$. In other words, each feature’s \textit{individual contribution} is represented by its corresponding SHAP value $\phi_i$. Summing the effects of all feature attributions closely approximates the original model's output $f(x)$. 

\subsection{Pattern Identification}
\label{sec:npc_pattern_identification}

In a classification task, given a set of inputs $\mathbb{X}$ that the \lcm predicts as belonging to a specific class $y \in Y$ (\eg Defect/Non-Defect), we compute SHAP values for each input $x \in \mathbb{X}$. These SHAP values are calculated relative to the expected predicted class: $y = \mathbb{E}[f(\mathbb{X})]$. For each token $w_i \in x$, we compute its SHAP value $\phi_i$ by solving \equref{eq:shapley_values}. Inspired by syntax decomposition \cite{syntax_capabilities, asttrust, docode}, we apply an alignment function $\delta(w_i): w_i \to \mu \in \mathbb{M}$ to tag tokens $w_i \in x$ with meaningful AST types $\mathbb{M}$, defined by the programming language grammar. This process produces a SHAP tensor for each target class in $Y$: ${(i, w_i, \phi_i, \mu_i)}$, where $i$ is the position, $w_i$ is the token, $\phi_i$ is the SHAP value, and $\mu_i$ is the associated AST type. The entire procedure is depicted in region \circled{1} of \figref{fig:npc_pipeline}.

After computing the SHAP tensors for each target class in $Y$, we merge them and group the $\phi$ values by the AST tag associated with their corresponding tokens. We define position ranges as $[a, b], \quad 0 \leq a \leq b \leq \max{|x|: x \in \mathbb{X}}$. For each range, we train a supervised model (\eg logistic regression, decision tree, random forest) to identify curves that best capture the relationship between $\phi$ values and feature positions. Curves with an accuracy exceeding $60\%$ and a well-defined decision boundary for the target class (\ie intersection with the x-axis) provide evidence of patterns in specific AST type positions where SHAP values influence the model's decisions. This process, depicted in region \circled{2} of \figref{fig:npc_pipeline}, takes inspiration from probing classifier techniques widely used in the context of NLP \cite{hewitt_designing_2019} and SE \cite{lopez_ast-probe_2022, troshin_probing_2022}. In these techniques, a supervised model (\eg classifier) is trained to predict a property of interest from the hidden representations of a neural network \cite{belinkov_probing_2021}.

The computed curves allow us to identify regions and position ranges where a feature’s $\phi$ value (\ie SHAP value corresponding to a specific AST node) consistently influences the overall prediction of the expected outputs either positively or negatively. 

\subsection{Neurosymbolic Rules}
From the identified patterns in SHAP value distributions, we derive Neurosymbolic rules encapsulating feature structures that align with expected model predictions. These rules consist of two parts: (i) configurations positively correlated with the predicted label, forming symbolic rules for correctly predicted patterns, and; (ii) complementary rules for configurations linked to lower prediction reliability, enabling targeted model adjustments in uncertain cases. We derive these rules by grouping SHAP-influential features within each type $\mu \in \mathbb{M}$ and formulating conditions based on both feature presence and SHAP value contributions. For instance, if a feature linked to an AST node consistently shows high SHAP values for defect code at the input's start, it may represent a necessary condition for a \textbf{\textit{defect}} prediction in the rule. As illustrated in region \circled{3} of \figref{fig:npc_pipeline}, the derived neurosymbolic rules can be applied during the post-training stage of an ML pipeline, for instance, in supervised fine-tuning and knowledge distillation.