\section{Methodology}
\label{sec:approach}

In this section, we present the \textbf{N}eurosymbolic \textbf{P}rogram \textbf{C}omprehension (\framework) framework, which leverages SHAP values (refer to \secref{sec:background}) to interpret and guide model predictions. We first describe our approach to identifying patterns in SHAP values for input features. Next, we explain how these patterns are transformed into symbolic rules to improve model performance, particularly in scenarios with low prediction confidence.

%%%%%%%%%%%%%%%%%%% NPC PIPELINE  
\begin{figure}[ht]
		\centering
  \vspace{-1.5em}
  \includegraphics[width=0.45\textwidth]{images/pipeline.pdf}
		\caption{Description of \framework framework as a sequence of steps.}
    \label{fig:npc_pipeline}
    \vspace{-1em}
\end{figure}
%%%%%%%%%%%%%%%%%%

%%%%%%% RESULTS TABLE 
\input{tables/results}
%%%%%%%%

\subsection{Pattern Identification}
\label{sec:npc_pattern_identification}

Drawing inspiration from probing classifier techniques widely used in NLP \cite{hewitt_designing_2019} and SE \cite{lopez_ast-probe_2022, troshin_probing_2022}, our framework leverages supervised machine learning techniques to identify patterns in the SHAP values computed for specific predictions in classification tasks. Probing techniques work by examining the latent representations of a model to determine the extent to which specific types of information are encoded. Specifically, a supervised model (\eg classifier) is trained to predict properties of interest from the neural network's hidden representations \cite{belinkov_probing_2021}. In the context of our framework, we propose training classifiers to predict target classes from SHAP value distributions enabling the formulation of symbolic rules, as illustrated in \figref{fig:npc_pipeline}.

First, given a set of inputs $\mathbb{X}$ that the \lcm predicts as belonging to a specific class $y \in Y$ (\eg Secure/Insecure), we compute SHAP values ($\phi$) for each input $x \in \mathbb{X}$. The SHAP values are calculated relative to the expected predicted class: $y = \mathbb{E}[f(\mathbb{X})]$. Inspired by syntax decomposition \cite{syntax_capabilities, palacio_towards_2024, docode}, we apply an alignment function $\delta(w_i): w_i \to \mu \in \mathbb{M}$ to tag tokens $w_i \in x$ with meaningful AST types $\mathbb{M}$, defined by the programming language grammar. This process produces a SHAP tensor for each target class: ${(i, w_i, \phi_i, \mu_i)}$, where $i$ is the position, $w_i$ is the token, $\phi_i$ is the SHAP value, and $\mu_i$ is the associated AST type. The entire process is depicted in region \circled{1} of \figref{fig:npc_pipeline}.

After computing the SHAP tensors for each target class in $Y$, we merge them and group the $\phi$ values by the AST tag associated with their corresponding tokens. We define position ranges as $[a, b], \quad 0 \leq a \leq b \leq \max{|x|: x \in \mathbb{X}}$. For each range, we train a supervised model (\eg logistic regression, decision tree, random forest) to identify curves that best capture the relationship between $\phi$ values and feature positions. Curves with an accuracy exceeding $60\%$ and a well-defined decision boundary for the target class (\ie intersection with the x-axis) provide evidence of patterns in specific AST type positions where SHAP values influence the model's decisions. The computed curves allow us to identify regions and position ranges where a featureâ€™s $\phi$ value (\ie SHAP value corresponding to a specific AST node) consistently influences the overall prediction of the expected outputs either positively or negatively.

\subsection{Symbolic Rules}
From the identified patterns in SHAP value distributions, we derive symbolic rules encapsulating feature structures that align with expected model predictions. These rules consist of two parts: (i) configurations positively correlated with the predicted label, forming symbolic rules for correctly predicted patterns, and; (ii) complementary rules for configurations linked to lower prediction reliability, enabling targeted model adjustments in uncertain cases. We derive these rules by grouping SHAP-influential features within each type $\mu \in \mathbb{M}$ and formulating conditions based on both feature presence and SHAP value contributions. For instance, if a feature linked to an AST node consistently shows high SHAP values for insecure code at the input's start, it may represent a necessary condition for an \textbf{\textit{insecure}} prediction in the rule. As illustrated in region \circled{3} of \figref{fig:npc_pipeline}, the derived symbolic rules can be applied during the post-training stage of an ML pipeline, for instance, in supervised fine-tuning and knowledge distillation to facilitate knowledge transfer between models.