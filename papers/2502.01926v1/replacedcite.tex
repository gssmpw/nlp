\section{Prior Work}
\label{sec:prior}

\textbf{Existing fairness benchmarks. }In predictive AI settings, theoretical and empirical studies have explicitly considered sensitive attributes, e.g., as an input feature, in achieving fairness____. However, in the generative AI setting, the importance of this explicit treatment seems to be forgotten.
Before July 30, 2024 we conducted a literature review of fairness benchmarks for language models by supplementing a Google Scholar search with four prior works' literature reviews:
102 datasets from ____, 21 datasets from ____, 8 datasets from ____, and 6 datasets from ____.
We reduced this to 37 benchmark datasets by selecting those that: a) focus on fairness, b) can be applied to generative language models, c) have sufficient documentation to determine how well a difference unaware model would perform. We considered out of scope coreference resolution and hate speech detection because they are often addressed by more narrow predictive models. 
% In Tbl.~\ref{tbl:litreview} we show our findings where our review reveals that only three out of 37 fair benchmarks are both appropriately specified (i.e., descriptive or normative rather than indicator) and definitively require a difference aware model. 
We find that 32 out of 37 prior benchmarks are based in difference unawareness (Tbl.~\ref{tbl:litreview}). Furthermore, we apply our categorization schema and see that more than half are \textit{correlation} benchmarks, where the descriptive or normative aim remains unspecified. This leaves only three of the 37 benchmarks that provide full specification and require a difference aware model.
% Most benchmarks are designed to be resolved by difference unaware models. 
We highlight representative statements from prominent benchmarks that illustrate the pattern of difference unawareness: HELM writes ``\textit{we explicitly define social bias as ‘a systematic asymmetry in language choice’}''____; BOLD describes, ``\textit{In each domain, some groups may be more frequently associated with negative emotions than others when an LM generates text}''____; DiscrimEval states that they ``\textit{measure discrimination in terms of differences in the probability of a yes decision across demographic attributes}''____. 
% All of these quotes demonstrate how undesirable bias is defined to be any difference between groups, whether in language asymmetry, negative emotion, or probability of a yes decision.
These quotes illustrate that undesirable bias is consistently characterized as any disparity between groups, whether in linguistic asymmetry, expressions of negative emotion, or the likelihood of a positive decision.

\setlength\arrayrulewidth{.8pt}
\begin{table*}[]
\caption{Literature review of 37 existing fairness benchmark papers for language models, with references listed in Appendix Tbl.~\ref{tbl:app_litreview}. Counts total 40 because some benchmarks contain multiple components. Blue cells indicate the type of benchmark we introduce in this work.}
\label{tbl:litreview}
    \fontsize{9.5pt}{10.8pt}\selectfont
\begin{tabular}{|p{1.8cm}|p{1.5cm}|p{.7cm}|p{10.1cm}|}
% \begin{tabular}{|p{2.cm}|p{1.8cm}|p{1.cm}|p{9cm}|}
\hline
Difference Treatment & Content Form & Count & Example Task \\ \hline
\multirow{3}{1.8cm}{} & Descriptive  & 7 &  Question answering task performance disparities when the mentioned demographic group is perturbed____.  \\ \cline{2-4} 
Difference \newline Unaware ($=$)& Normative                  & 6   & Hiring decision disparities in candidates who are equal except for age, gender, and race____.   \\ \cline{2-4}
& Correlation                 & 19  &  Disparities in occupations generated for ``The [woman/man] worked as...''____. \\ \hline
\multirow{3}{1.8cm}{\cellcolor{blue!25}} 
    & \cellcolor{blue!25}Descriptive    
    & 0 
    & Accuracy in recognizing which demographic groups are underrepresented in which occupations.  
    \\ \hhline{>{\arrayrulecolor{blue!25}}-|>{\arrayrulecolor{black}}-|--}

\cellcolor{blue!25}Difference Aware ($\neq$) 
    & \cellcolor{blue!25}Normative   
    & 3 
    & Recognizing that offensive statements can be more harmful towards certain groups than others____.  
    \\ \cline{2-4}
% \multirow{3}{1.8cm}{ {\cellcolor{blue!25}}}   & \rowcolor{blue!25}Descriptive    &   0 & \rowcolor{white}  Accuracy in recognizing which demographic groups are underrepresented in which occupations.  \\ \hhline{>{\arrayrulecolor{blue!25}}-|>{\arrayrulecolor{black}}-|*2{-}|} %\hhline{~|*3{-}|}
% {\cellcolor{blue!25}Difference Aware ($\neq$)}& \rowcolor{blue!25}Normative   &    3 &  \rowcolor{white} Recognizing that offensive statements which reinforce stereotypes may be more offensive when made towards certain demographic groups than others____. \\ \cline{2-4}
{\cellcolor{blue!25}}& Correlation                 & 4  & Amplification from societal rates in occupations generated for ``The [woman/man] worked as...''____. \\ \hline
Ambiguous & Descriptive & 1 & Assessing appropriate reactions by an LLM to gender disclosure____. \\ \hline
\end{tabular}
\end{table*}



\textbf{Definitions of bias. }
% Overall, these prior works enforce a notion of fairness that is grounded in difference unawareness. 
The issue of ``bias'' being poorly defined has been highlighted in prior work____, emphasizing the need to connect ``bias'' to specific harms____. When left unspecified, fairness is often implicitly framed as difference unaware treatment, which parallels the problem of poorly conceptualized and operationalized notions of stereotypes____. 

\textbf{Forms of difference awareness. }____ offers an insightful social analysis of algorithmic fairness's insufficient engagement with racial color-blindness. ____ discuss a similar tension between \textit{invariance} and \textit{adaptation} to identity-related language features. 
% They make the conceptual argument that invariance may be an easy solution because of its simplicity, where ``bias'' is measured as failing to produce the same output across groups. 
% The alternative of appropriate adaptation to group differences is a more open-ended challenge. 
Their notion of adaptation is slightly different from our concept of difference awareness; they focus on personalization (e.g., in an email reply) based on a recipient's social identity. ____ explore a related tension between personalization and stereotyping. ____ in their work takes ``the normative position that identical model behavior across target categories is insufficient,'' but their method of hand-labeling outputs for stereotypes is not scalable. In our work, we concretely take on these challenges and build a benchmark suite to measure difference awareness.




Other work has discussed difference unawareness in the context of demographic representation of text-to-image models____. In our work, we posit a much broader notion of difference unawareness, beyond simply demographic image representation, and describe the difference created by descriptive versus normative form.
% ____: when people measure "bias" they have not connected it to what the harm is makes a similar point
% Prior work has pointed out other weaknesses of existing bias benchmarks. ____ critique four existing NLP bias benchmarks for being poorly conceptualized and operationalized. For example, they note that comparing stereotypes between Scottish and American does not test a valid stereotype. Similar to how they see fallacies in existing fairness benchmarks, we have pointed out a different one. They offer fantastic critiques towards benchmarks of contrastive sentences, and we take their advice in trying to build benchmarks for a novel domain: when groups should be treated differently.