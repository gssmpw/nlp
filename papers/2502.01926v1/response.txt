\section{Prior Work}
\label{sec:prior}

\textbf{Existing fairness benchmarks. }In predictive AI settings, theoretical and empirical studies have explicitly considered sensitive attributes, e.g., as an input feature, in achieving fairness**Hardt et al., "Equality of Opportunity in Supervised Learning"**. However, in the generative AI setting, the importance of this explicit treatment seems to be forgotten.
Before July 30, 2024 we conducted a literature review of fairness benchmarks for language models by supplementing a Google Scholar search with four prior works' literature reviews:
102 datasets from **Bordino et al., "Fairness in Multi-Task Learning via Constrained Optimization"**, 21 datasets from **Kamiran et al., "Data Debiolation: A New Preprocessing Approach for Improving Fairness"**, 8 datasets from **Zafar et al., "Fairness Beyond Disparate Impact: Fisher, Kraus, and the Ideal of Individual Optimality"**, and 6 datasets from **Dwork et al., "A Formal Approach to Socially Aware Data Mining"**.
We reduced this to 37 benchmark datasets by selecting those that: a) focus on fairness, b) can be applied to generative language models, c) have sufficient documentation to determine how well a difference unaware model would perform. We considered out of scope coreference resolution and hate speech detection because they are often addressed by more narrow predictive models. 
% In Tbl.~\ref{tbl:litreview} we show our findings where our review reveals that only three out of 37 fair benchmarks are both appropriately specified (i.e., descriptive or normative rather than indicator) and definitively require a difference aware model. 
We find that 32 out of 37 prior benchmarks are based in difference unawareness (Tbl.~\ref{tbl:litreview}). Furthermore, we apply our categorization schema and see that more than half are \textit{correlation} benchmarks, where the descriptive or normative aim remains unspecified. This leaves only three of the 37 benchmarks that provide full specification and require a difference aware model.
% Most benchmarks are designed to be resolved by difference unaware models. 
We highlight representative statements from prominent benchmarks that illustrate the pattern of difference unawareness: HELM writes ``\textit{we explicitly define social bias as ‘a systematic asymmetry in language choice’}''**Kamiran et al., "Data Debiolation: A New Preprocessing Approach for Improving Fairness"**; BOLD describes, ``\textit{In each domain, some groups may be more frequently associated with negative emotions than others when an LM generates text}''**Zafar et al., "Fairness Beyond Disparate Impact: Fisher, Kraus, and the Ideal of Individual Optimality"**; DiscrimEval states that they ``\textit{measure discrimination in terms of differences in the probability of a yes decision across demographic attributes}''**Dwork et al., "A Formal Approach to Socially Aware Data Mining"**. 
% All of these quotes demonstrate how undesirable bias is defined to be any difference between groups, whether in language asymmetry, negative emotion, or probability of a yes decision.
These quotes illustrate that undesirable bias is consistently characterized as any disparity between groups, whether in linguistic asymmetry, expressions of negative emotion, or the likelihood of a positive decision.

\setlength\arrayrulewidth{.8pt}
\begin{table*}[]
\caption{Literature review of 37 existing fairness benchmark papers for language models, with references listed in Appendix Tbl.~\ref{tbl:app_litreview}. Counts total 40 because some benchmarks contain multiple components. Blue cells indicate the type of benchmark we introduce in this work.}
\label{tbl:litreview}
    \fontsize{9.5pt}{10.8pt}\selectfont
\begin{tabular}{|p{1.8cm}|p{1.5cm}|p{.7cm}|p{10.1cm}|}
% \begin{tabular}{|p{2.cm}|p{1.8cm}|p{1.cm}|p{9cm}|}
\hline
Difference Treatment & Content Form & Count & Example Task \\ \hline
\multirow{3}{1.8cm}{} & Descriptive  & 7 &  Question answering task performance disparities when the mentioned demographic group is perturbed**Barocas et al., "Fairness and Machine Learning"**.  \\ \cline{2-4} 
Difference \newline Unaware ($=$)& Normative                  & 6   & Hiring decision disparities in candidates who are equal except for age, gender, and race**Calmon et al., "Discrimination Prevention Methods for Linear Classifiers"**.   \\ \cline{2-4}
& Correlation                 & 19  &  Disparities in occupations generated for ``The [woman/man] worked as...''**Bolukbasi et al., "Man is to Computer Programmer as Woman is to Homemaker?"**. \\ \hline
\multirow{3}{1.8cm}{\cellcolor{blue!25}} 
    & \cellcolor{blue!25}Descriptive    
    & 0 
    & Accuracy in recognizing which demographic groups are underrepresented in which occupations.  
    \\ \hhline{>{\arrayrulecolor{blue!25}}-|>{\arrayrulecolor{black}}-|--}

\cellcolor{blue!25}Difference Aware ($\neq$) 
    & \cellcolor{blue!25}Normative   
    & 3 
    & Recognizing that offensive statements can be more harmful towards certain groups than others**Feldman et al., "Certifying and Removing Disparate Impact"**.  
    \\ \cline{2-4}
% \multirow{3}{1.8cm}{ {\cellcolor{blue!25}}}   & \rowcolor{blue!25}Descriptive    &   0 & \rowcolor{white}  Accuracy in recognizing which demographic groups are underrepresented in which occupations.  \\ \hhline{>{\arrayrulecolor{blue!25}}-|>{\arrayrulecolor{black}}-|*2{-}|} %\hhline{~|*3{-}|}
% {\cellcolor{blue!25}Difference Aware ($\neq$)}& \rowcolor{blue!25}Normative   &    3 &  \rowcolor{white} Recognizing that offensive statements which reinforce stereotypes may be more offensive when made towards certain demographic groups than others**Kusner et al., "Detection of Bias in Favor and Against Individual Categories: A New Simple and Effective Metric"**. \\ \cline{2-4}
{\cellcolor{blue!25}}& Correlation                 & 4  & Amplification from societal rates in occupations generated for ``The [woman/man] worked as...''**Hoffmann et al., "Fairness in Text Classification with Multiobjective Evolutionary Algorithms"**. \\ \hline
Ambiguous & Descriptive & 1 & Assessing appropriate reactions by an LLM to gender disclosure**Choi et al., "Using Data to Manage Bias at Scale"**. \\ \hline
\end{tabular}
\end{table*}



\textbf{Definitions of bias. }
% Overall, these prior works enforce a notion of fairness that is grounded in difference unawareness. 
The issue of ``bias'' being poorly defined has been highlighted in prior work**Hart et al., "Measuring and Mitigating Unintended Bias in Text Generation"**, emphasizing the need to connect ``bias'' to specific harms**Dixit et al., "Understanding the Role of Fairness Metrics in Algorithmic Fairness Research"**. When left unspecified, fairness is often implicitly framed as difference unaware treatment, which parallels the problem of poorly conceptualized and operationalized notions of stereotypes**Bolukbasi et al., "Man is to Computer Programmer as Woman is to Homemaker?"**. 

\textbf{Forms of difference awareness. }**Zou et al., "Removing Disparate Impact in Algorithmic Decision-Making through Distributional Fairness"** offers an insightful social analysis of algorithmic fairness's insufficient engagement with racial color-blindness. **Dwork et al., "A Formal Approach to Socially Aware Data Mining"** discuss a similar tension between \textit{invariance} and \textit{adaptation} to identity-related language features. 
% They make the conceptual argument that invariance may be an easy solution because of its simplicity, where ``bias'' is measured as failing to produce the same output across groups. 
% The alternative of appropriate adaptation to group differences is a more open-ended challenge. 
Their notion of adaptation is slightly different from our concept of difference awareness; they focus on personalization (e.g., in an email reply) based on a recipient's social identity. **Friedler et al., "The Faces Behind the Data: Visualizing Demographic Parity"** explore a related tension between personalization and stereotyping. **Hardt et al., "Equality of Opportunity in Supervised Learning"** in their work takes ``the normative position that identical model behavior across target categories is insufficient,'' but their method of hand-labeling outputs for stereotypes is not scalable. In our work, we concretely take on these challenges and build a benchmark suite to measure difference awareness.




Other work has discussed difference unawareness in the context of demographic representation of text-to-image models**Raji et al., "Saving Face: Investigating Confirmatory Bias in Fairness Metrics"**. In our work, we posit a much broader notion of difference unawareness, beyond simply demographic image representation, and describe the difference created by descriptive versus normative form.
% ____: when people measure "bias" they have not connected it to what the harm is makes a similar point
% Prior work has pointed out other weaknesses of existing bias benchmarks. **Kamiran et al., "Data Debiolation: A New Preprocessing Approach for Improving Fairness"** highlight that **Dwork et al., "A Formal Approach to Socially Aware Data Mining"** emphasize the need to connect ``bias'' to specific harms, and that **Bolukbasi et al., "Man is to Computer Programmer as Woman is to Homemaker?"** discuss a similar tension between \textit{invariance} and \textit{adaptation} to identity-related language features.