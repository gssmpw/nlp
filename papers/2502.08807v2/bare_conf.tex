%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.

\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{multirow}
\usepackage{balance}
\usepackage{hyperref}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
% \author{

% \IEEEauthorblockN{Zifan He}
% \IEEEauthorblockA{University of California, Los Angeles \\
% zifanhe1202@g.ucla.edu}
% \and
% \IEEEauthorblockN{Anderson Truong}
% \IEEEauthorblockA{University of California, Los Angeles \\
% andersonleetruong@gmail.com}
% \and
% \IEEEauthorblockN{Yingqi Cao}
% \IEEEauthorblockA{University of California, San Diego \\ yic033@ucsd.edu}
% \and
% \IEEEauthorblockN{Jason Cong}
% \IEEEauthorblockA{University of California, Los Angeles \\ cong@cs.ucla.edu}
% }

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
\author{
\IEEEauthorblockN{
Zifan He\IEEEauthorrefmark{1},
Anderson Truong\IEEEauthorrefmark{1},
Yingqi Cao\IEEEauthorrefmark{2} and
Jason Cong\IEEEauthorrefmark{1}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}University of California, Los Angeles}
\IEEEauthorblockA{\IEEEauthorrefmark{2}University of California, San Diego\\
Email: zifanhe1202@g.ucla.edu, andersonleetruong@gmail.com, yic033@ucsd.edu, cong@cs.ucla.edu}
}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
The rise of deep neural networks (DNNs) has driven an increased demand for computing power and memory. Modern DNNs exhibit high data volume variation (HDV) across tasks, which poses challenges for FPGA acceleration: conventional accelerators rely on fixed execution patterns (dataflow or sequential) that can lead to pipeline stalls or necessitate frequent off-chip memory accesses. To address these challenges, we introduce the Inter-Task Auto-Reconfigurable Accelerator (InTAR) \footnote{\url{https://github.com/OswaldHe/InTAR}}, a novel accelerator design methodology for HDV applications on FPGAs. InTAR combines the high computational efficiency of sequential execution with the reduced off-chip memory overhead of dataflow execution. It switches execution patterns automatically with a static schedule determined before circuit design based on resource constraints and problem sizes. Unlike previous reconfigurable accelerators, InTAR encodes reconfiguration schedules during circuit design, allowing model-specific optimizations that allocate only the necessary logic and interconnects. Thus, InTAR achieves a high clock frequency with fewer resources and low reconfiguration time. Furthermore, InTAR supports high-level tools such as HLS for fast design generation. We implement a set of multi-task HDV DNN kernels using InTAR. Compared with dataflow and sequential accelerators, InTAR exhibits $\mathbf{1.8\times}$ and $\mathbf{7.1 \times}$ speedups correspondingly. Moreover, we extend InTAR to GPT-2 medium as a more complex example, which is $\mathbf{3.65 \sim 39.14\times}$ faster and a $\mathbf{1.72 \sim 10.44\times}$ more DSP efficient than SoTA accelerators (Allo and DFX) on FPGAs. Additionally, this design demonstrates $\mathbf{1.66 \sim 7.17\times}$ better power efficiency than GPUs.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\section{Introduction} \label{sec:intro}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.85\columnwidth]{imgs/intra-arch.png}
%     \caption{Example timeline of dataflow and sequential execution patterns and \textsc{InTAR}. Intermediate data volumes between tasks are indicated as proportional to the area of the white rectangles. Only sequential execution requires off-chip memory access.}
%     \label{fig:intra-arch}
% \end{figure}

The development of deep neural networks (DNNs) has driven numerous breakthroughs in AI-assisted applications \cite{dong2019unified, dosovitskiy2020image} while intensifying demands for compute efficiency due to increasing network complexity and memory requirements. As DNN applications grow in size \cite{touvron2023llama, raiaan2024review}, larger data volumes generated between operations strain hardware resources during deployment. Consequently, a key challenge in designing DNN accelerators is: \textit{Given resource constraints imposed by physical hardware limitations or power considerations, how can execution scheduling be improved to minimize end-to-end latency while accounting for hardware overhead?}

A common strategy of DNN accelerator designs is grouping computations belonging to the same matrix/vector operation into a \textbf{task} and scheduling computations at the task level. For example, self-attention \cite{vaswani2017attention} of transformer models involves two tasks: $QK^T$ and softmax operation. Previous accelerators focus on two types of fixed task execution patterns: \textbf{dataflow} (also called spatial) \cite{chen2024allo, basalama2023flexcnn} or \textbf{sequential} (also called temporal) executions \cite{jouppi2023tpu, bai2023fet, liu2021hardware, zhang2020dnnexplorer}. For \textbf{dataflow} executions, each processing element (PE) specializes in a single task. Data is streamed through FIFOs connecting PEs for dependent computations with minimal on-chip buffer for data reuse. Depending on the dataflow graph, it includes both task-pipeline and task-parallel executions. \textbf{Task-pipeline} streams data between dependent tasks, and \textbf{task-parallel} executes independent tasks in parallel. For \textbf{sequential} executions, a versatile PE processes tasks serially and parallelizes the computations inside each task. Resources are reused across operators. Although both execution patterns exhibit strong performance, each of them faces a major challenge for large DNN applications \cite{chen2024understanding, zhuang2024ssr}:
\begin{itemize}[wide=0pt]
    \item \textbf{Dataflow: reduced computation resource efficiency}. Due to the coarse and fine-grained pipeline stalls from the data dependencies, some PEs in dataflow execution may remain idle, leading to low computation resource efficiency and high end-to-end latency. 
    \item \textbf{Sequential: high off-chip memory access overhead.} Output data from previous tasks will be stored and reused by the next tasks in sequential execution. Moreover, data may be cached and remain unused until the consumer tasks start executions. Due to the limited on-chip resources, it is often difficult to store all output data in SRAM. If storing data in the off-chip memory, the accelerator will suffer from high latency and energy consumption of off-chip memory access.  
\end{itemize}

\begin{table}[ht]
  \centering
  \caption{Minimum and Maximum Intermediate Data Size of Sample DNNs}
  \label{table:trans-cnn}
  \resizebox{0.8\columnwidth}{!}{
  \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Model} & \textbf{Type} & \textbf{Min Data Size} & \textbf{Max Data Size}\\
    \hline
    GPT 2 \cite{radford2019language} & Transformer & $64L$ & $\max(4096L, L^2)$ \\
    \hline
    Llama 2 7B \cite{touvron2023llama} & Transformer & $128L$ & $\max(22016L, L^2)$ \\
    \hline
    ResNet-50 \cite{he2016deep} & CNN & 151K & 802K \\
    \hline
    ResNet-152 & CNN & 151K & 802K \\
    \hline
    ResNext-101 \cite{xie2017aggregated} & CNN & 151K & 1.04M \\
    \hline
  \end{tabular}
  }
\end{table}

% With the rising awareness of power efficiency and privacy in language model inference \cite{stojkovic2024towards, yao2024survey}, researchers are searching for alternative solutions without power-hungry GPUs. The \textbf{flexible architecture} of FPGAs allows customized allocation of resources for different stages of computation as workload changes, preventing wasting power with idle compute units. Moreover, designers can freely distribute on-chip memory to align with the layout of computation resources, reducing memory access latency. With task-pipelining and data caching on abundant on-chip memory resources compared to GPUs \cite{Xilinx-versal-hbm, nvidia-v100}, FPGAs can achieve significantly \textbf{fewer off-chip data movement}. Multiple recent works in FPGAs have demonstrated $\mathbf{3 \sim 10 \times}$ \textbf{lower energy consumptions} than GPUs \cite{he2024levelst, song2023callipepla, chen2024understanding} and exhibits the potential for energy-efficient transformer inference.

% \begin{table}[ht]
%   \centering
%   \caption{Normalized compute efficiency and off-chip memory access of sequential and dataflow executions and \textsc{InTAR} on GPT-2 medium with 1024-token sequences. All accelerators use the same attention algorithm.}
%   \label{table:main}
%   \resizebox{\columnwidth}{!}{
%   \begin{tabular}{|l|c|c|c|}
%     \hline
%     \textbf{Execution} & \textbf{Accelerator} & \textbf{Norm. Compute} & \textbf{Norm. Off-chip}\\
%     \textbf{Pattern} & & \textbf{Efficiency ($\uparrow$)} & \textbf{Memory Access ($\downarrow$)} \\
%     \hline
%     Sequential & FQ-BERT \cite{liu2021hardware} & 1 & 1\\
%     \hline
%     Sequential & NVIDIA-A100 & 0.22 & 0.89\\
%     \hline
%     Sequential & NVIDIA-T4 & 0.39 & 1.24 \\
%     \hline
%     Dataflow & Allo \cite{chen2024allo} & 0.48 & 0.74\\
%     \hline
%     \textbf{\textsc{InTAR}} & - & \textbf{0.90} & \textbf{0.74}\\
%     \hline
%   \end{tabular}
%   }
% \end{table}

To address both challenges simultaneously, we observe another commonly neglected feature of DNNs to exploit: \textbf{high data-volume variation} (HDV). Table \ref{table:trans-cnn} lists a set of DNN applications and the minimum/maximum input/output data size between operations. $L$ is the sequence length for transformer models. These models offer both a large maximum data size (up to 88 MB with $L$ = 2048) and a significant data size variation ($5.3\times \sim 172\times$ from minimum to maximum data size). A potential solution to improve computation resource efficiency while circumventing off-chip memory access for intermediate data is to execute sequentially for tasks that produce small data within on-chip memory capacity, and stream data for tasks that produce large data.

Therefore, we propose a novel accelerator design paradigm on FPGAs: \textbf{inter-task auto-reconfigurable accelerator} (\textsc{InTAR}). \textsc{InTAR} can switch execution patterns automatically based on on-chip memory and computation resources. When a task produces large intermediate data, \textsc{InTAR} pipelines multiple tasks to avoid accessing off-chip memory for this data. Otherwise, \textsc{InTAR} will process tasks sequentially to maximize computational efficiency by eliminating pipeline stalls. Compared with other reconfigurable accelerators \cite{cai2023inter, cong2014fully, baek2020multi, liu2022overgen, kong2023hardware}, \textsc{InTAR} allows \textit{model-specific circuit optimization} that keeps only necessary control logics and interconnects for reconfiguration. Hence, InTAR requires fewer reconfiguration resources, achieves a high clock frequency, and has a low reconfiguration overhead (10 to 20 ns) (Section \ref{sec:intrra}). Moreover, since computations are reconfigured at the task level, \textsc{InTAR} is one of the first works regarding FPGA-based reconfigurable accelerators that support high-level hardware generation tools such as High-Level Synthesis (HLS) \cite{cong2022fpga} for fast accelerator design. Specifically, our contributions include:

\begin{itemize}[wide=0pt]
    \item We present \textsc{InTAR}, a novel accelerator design paradigm on FPGAs that balances the memory access and computational efficiency tradeoff for HDV applications (Section \ref{sec:intrra}). 

    \item We illustrate the techniques to design \textsc{InTAR} with HLS and important considerations in placement and routing for hardware implementation of \textsc{InTAR} on FPGAs (Section \ref{sec:design}).
    
    \item Evaluations. We implement \textsc{InTAR} for five multi-task kernels that broadly exist in DNN applications (Self-Attention, Multi-layer CNN, FFN layer, VAE, and Gating Network) to show its advantages. InTAR exhibits $\mathbf{1.8\times}$ and $\mathbf{7.1 \times}$ speedup compared with the corresponding dataflow and sequential accelerator. We further present InTAR on the GPT-2 medium model for a complete DNN example, which achieves a speedup of $\mathbf{3.65 \sim 39.14\times}$ and a $\mathbf{1.72 \sim 10.44\times}$ improvement in DSP efficiency compared to the SoTA accelerators (Allo \cite{chen2024allo} and DFX \cite{hong2022dfx}). Moreover, \textsc{InTAR} demonstrated $\mathbf{1.66 \sim 7.17\times}$ better power efficiency compared to GPUs.
\end{itemize}

% Resource constraints come from:
% 1. physical hardware constraints: you cannot use more than what the FPGA board supports.
% 2. Energy pre-design optimization: The designer may use tools like Xilinx power design manager to find out the resource configuration given a power constraint. Thus, we have to follow the new constraint when designing the accelerator.

\section{Background and Motivations}

\subsection{Dataflow and Sequential DNN Accelerators}

Dataflow accelerators allocate resources per DNN task and pipeline computations to conserve memory while preserving performance. For example, FlexCNN \cite{basalama2023flexcnn} composes CNN-based accelerators in HLS, GenGNN \cite{abi2022gengnn} accelerates GNNs, and Allo \cite{chen2024allo,chen2024understanding} designs FPGA accelerators by parsing Python scripts into HLS code for each operator, automatically combining layer optimizations with customization primitives. 

On the other hand, sequential accelerators process layers in DNN sequentially and attempt to utilize all available resources. GPUs and TPUs \cite{jouppi2023tpu} are considered instruction-based sequential accelerators that support various DNNs. ZyncNet \cite{gschwend2020zynqnet} is a CNN accelerator deployed on Zync SoC with a specialized topology. FlightLLM \cite{zeng2024flightllm} is a vector processor implemented on Alveo U280 FPGA for Llama model inference. 

\subsection{Hybrid Accelerators}

Some previous works attempt to mitigate the drawbacks of dataflow and sequential accelerators by having a hybrid execution pattern. A naive approach is allocating part of the resource for dataflow execution and the rest for sequential execution \cite{zhang2020dnnexplorer}. A more recent method is to allocate several PEs and schedule multiple tasks to the same PE. For instance, SSR \cite{zhuang2024ssr} is a hybrid accelerator implemented on AMD Xilinx Versal ACAP devices for vision transformers. By reserving two groups of AI engines for two types of matrix multiplies, SSR allows sequential executions of matrix multiplies (GEMM) to minimize latency and enables data forwarding between dependent GEMMs to reduce the memory cost. However, PE specializations of hybrid accelerators impede the flexibility of execution pattern switching, which may lead to stalls. Section \ref{sec:case} will further discuss the inefficiency of hybrid accelerators with an example.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/intra_attn_sample_v3.png}
    \caption{Example of mapping computations of attention and linear projection of value matrix to the dataflow, sequential, hybrid accelerators, and \textsc{InTAR}. The sequence length is 256, the hidden dimension is 1024, and the weights are $1024\times 1024$ matrices. Depending on the scheduling of the rest of the computations in the entire model, \textsc{InTAR} can choose between task-parallel (left) and sequential modes (right) for better locality. Both will have the same latency.}
    \label{fig:attn_sample}
\end{figure*}

\subsection{Reconfigurable Accelerators}

A reconfigurable accelerator can change its microarchitecture on the fly. A common implementation of such accelerators is the coarse-grain reconfigurable array (CGRA) \cite{liu2022overgen, cong2014fully, tanomoto2015cgra}, which compiles a dataflow graph into a configuration and modifies switches connecting PEs accordingly. However, CGRA has complex reconfiguration logic to ensure generalizability. This introduces substantial area overheads \cite{taras2019impact} and additional wiring that negatively affects the timing \cite{liu2022overgen}.

An alternative method to reconfigure resources is dynamic partial reconfiguration (DPR) \cite{vipin2018fpga}, which reserves regions on FPGAs to load partial bitstreams for runtime reconfiguration. While having a low resource overhead for the configuration controller, it has a higher reconfiguration overhead (10 to 100 ms) than FPGA-based CGRA (around 10 ns).

Other reconfigurable accelerators focus on optimization under only specific computation scenarios \cite{baek2020multi, cai2023inter, tong2024FEATHER}. For example, SET \cite{cai2023inter} performs inter-layer scheduling of convolution networks onto the tiled accelerators utilizing a time-space resource-allocation tree. FEATHER \cite{tong2024FEATHER} mainly resolves reduction efficiency in each CNN layer by reconfiguring the reduction dataflow.

Considering these limitations, existing reconfigurable accelerators can hardly apply to a broad range of modern DNNs, which necessitates the development of a new design paradigm. 

\subsection{Applications with High Data Volume Variation}

A multi-task application is defined as high data-volume variation (HDV) if the input/output data sizes between tasks are highly varied. Many of the DNNs are HDV for two reasons:
\begin{itemize}[wide=0pt]
    \item Modern DNNs have complex dependencies. Data is produced several layers in advance and can only be discarded after they are reused. Hence, the data production and elimination rates vary widely.
    \item The objectives of many DNN workloads involve feature extraction (e.g., classification) that reduces the data size, or data construction (e.g., image/text generation), which increases the data size. By the nature of these workloads, the data size will change during execution.
\end{itemize}

% Why do most data-volume-variant applications belong to the machine learning field?
% 1. modern machine learning applications have deep networks with complex dependencies. Data produced in previous layers may be reused in the following layers (e.g. skip connection, gating network). Data production/elimination rate can vary from layer to layer.
% 2. Nature of machine learning tasks: many ML tasks involve feature extraction (e.g. classification tasks) with down-sampling, data constructions (e.g., image/text generation) with up-sampling, or both (e.g., GAN, VAE, Transformers)

\subsection{A Motivating Case Study: Attention + Linear Projection} \label{sec:case}

We provide a case study to illustrate the advantages of inter-task reconfiguration for HDV DNNs: calculating the attention score and a linear projection of the value matrix. Figure \ref{fig:attn_sample} shows the dataflow graph of the attention layer and the timeline for each execution pattern. The direction of the arrows indicates the output production order, and the lengths represent the production rate. Here we focus on calculating $Q$, $K$, $V$, and $A$ in the dashed square. In this example, we assume that:
\begin{itemize}[wide=0pt]
    \item Matrix multiplies are weight-stationary as weights are already stored in on-chip memory. 
    \item Outputs ($A$ and $V$) will be written into off-chip memory.
    \item The rest of the on-chip memory size will only store either $Q$ or $K$, but not both. 
    \item Input sequence is short, which means $A$ can be stored in on-chip buffer with $K$ or $Q$.
\end{itemize}

The pattern of data size variation is \textit{first increase then decrease}. Dataflow execution instantiates PEs for each task. There are three ways to stream and cache data across tasks, illustrated in Figure \ref{fig:attn_sample}: outer product with aligned and unaligned production rates of $Q$ and $K$ (Figure \ref{fig:attn_sample}(a) and (b)), and inner product with unaligned production rates (Figure \ref{fig:attn_sample}(c)). Inner products with aligned production rates require caching of both $Q$ and $K$, which is infeasible. Each case incurs compute resource idling due to either varied workload size of $A$, back-pressure from streaming, or early completion of tasks.

Sequential execution instantiates a single PE to execute each task serially, as shown in Figure \ref{fig:attn_sample}(d). Due to insufficient on-chip memory capacity, it needs to read from and write to the off-chip memory, increasing the overall latency. 

Some hybrid accelerators \cite{zhuang2024ssr} can mitigate the issues by executing $Q$ and $V$ sequentially and pipeline $K$ and $A$. However, since PEs are specialized for either streaming data or cached buffer \cite{zhuang2024ssr}, computation of $V$ cannot be assigned to PE 2 and PE 3, and these two PEs are idle at the beginning, waiting for the arrival of data for $Q$.

For \textsc{InTAR} (Figure \ref{fig:attn_sample}(f)), PEs can be reconfigured to either sequential or dataflow execution at different times. Therefore, PEs can be efficiently allocated while off-chip memory access for intermediate data is avoided. Section \ref{sec:intrra} discusses how \textsc{InTAR} manages this in detail.

\section{Inter-Task Auto-Reconfigurable Accelerator} \label{sec:intrra}

To improve resource efficiency, \textsc{InTAR} minimizes off-chip memory access by employing dataflow execution when necessary while keeping the rest of the execution as sequential as possible. The finest granularity of execution pattern switching is the \textbf{task}, which represents a group of computations belonging to the same matrix or vector operation. Execution pattern switching is achieved through automatic reconfiguration based on instructions stored in the static configuration buffer (Section \ref{sec:templ}), which is hardened into the design. These instructions trigger computation behavior and data movement changes. 

Unlike other reconfigurable \cite{liu2022overgen, cong2014fully, tong2024FEATHER, cai2023inter, kong2023hardware} and overlay accelerators \cite{yu2019opu, yu2020light}, which prioritize abstraction and fast compilation, InTAR focuses on reconfiguration for optimization. The reconfiguration schedule in InTAR is determined at \textbf{circuit design time} rather than relying on post-design external commands. For every application, a \textbf{static schedule} is created based on the specific application and resource constraints, and the circuit is then tailored accordingly. This approach reduces reliance on general-purpose control units and cross-bars, retaining only essential reconfiguration logic and interconnects. The result is low resource utilization for reconfiguration controls (49\% LUT reduction for a design with $3\times$ throughput compared to \cite{liu2022overgen} for multiple GEMMs), high clock frequency ($2.41\times$ that of \cite{liu2022overgen}), and low reconfiguration latency ($10^6\times$ faster than DPR \cite{kong2023hardware}, and comparable to \cite{liu2022overgen}) compared with FPGA-based CGRA and DPR approaches. Additionally, InTAR handles reconfiguration at the task level, enabling rapid and convenient development with high-level tools like HLS. This makes InTAR well-suited for FPGAs to adapt quickly to new DNN applications. Lastly, InTAR's reconfiguration scope is not limited to specific computations. Table \ref{tab:reconfig_comp} compares InTAR with prior DNN accelerator works.

% In this section, we first illustrate \textsc{InTAR}'s execution modes and how \textsc{InTAR} achieves high resource efficiency in the case study by switching between modes. Then, we describe the architecture template of \textsc{InTAR}. 


% Difference between domain-specific CGRA/Overlay and InTAR and the benefits:
% 1. Interconnect and resource overhead when having fixed and generalized microarchitecture for various applications.
% 2. Focus on a single or a fixed set of applications with a single microarchitecture design. It cannot extend to future applications with more complicated dependencies and data volume variance.

% The convenience of implementing InTAR: since scheduling is handled at task level, high-level synthesis (HLS) is sufficient to implement a high-quality InTAR design, allowing hardware designers at the HLS level and compiler developer to leverage InTAR without touching RTL code (example HLS code after).

\subsection{Execution Modes of \textsc{InTAR}} \label{sec:comp-mode}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/intrra-arch-template.png}
    \caption{Left: Architecture template of \textsc{InTAR}. Compute cores (CC) compute linear operations (e.g., GEMM, ConvNet), and SFUs compute non-linear operations (e.g., softmax, GeLU). Each CC contains a scratchpad memory, a reconfigurable MAC unit array, a reduction unit, and a data movement control unit. Dashed lines indicate the candidates for connection between CCs and SFUs. Right: example architectures for each execution mode within the template, with $n,m=2$.  For a schedule, we will compose the architectures of the required modes to keep only the necessary interconnects and logic.}
    \label{fig:intrra-template}
\end{figure*}

\begin{table*}[ht]
    \centering
    \caption{Comparison between \textsc{InTAR} and other DNN acceleration works. The major reason for having idle compute resources is described in the parentheses for each work.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{Prior works} & \textbf{Platform} & \textbf{Contribution Type} & \textbf{Accelerator} & \textbf{Inter-task} & \textbf{Idle Compute} & \textbf{Intermediate} & \textbf{When Reconfig} & \textbf{Reconfiguration} &\textbf{Model-spec.} \\
        & & & \textbf{Category} & \textbf{Schedule} & \textbf{Resource Exist?} & \textbf{Data Movement} &  \textbf{Determined} & \textbf{Design Scope} &  \textbf{Design Opt.}\\
        \hline
        Allo \cite{chen2024allo} & FPGA & Domain-Spec. Lang. & Dataflow & \xmark & Yes (dataflow execution)& On-chip only & - & - & \xmark\\
        \hline
        FQ-BERT \cite{liu2021hardware}, DFX \cite{hong2022dfx} & FPGA & Accelerator & Sequential & \xmark & No & On-/Off-chip & - & - & \xmark \\
        \hline
        SSR \cite{zhuang2024ssr} & FPGA & Accelerator & Hybrid & \cmark & Yes (PE specialization) & On-chip only & - & - & \xmark\\
        \hline
        % Herald \cite{kwon2021heterogeneous} & ASIC & Hybrid & \cmark & \xmark & Yes (single DNN) & Off-chip & - & - & \xmark \\
        % \hline
        FPCA \cite{cong2014fully}, OverGen \cite{liu2022overgen} & FPGA & Architecture & Reconfigurable & \xmark & Yes (single DNN) & On-/Off-chip  & After circuit generation & General & \xmark \\
        \hline
        FEATHER \cite{tong2024FEATHER} & FPGA & Accelerator & Reconfigurable & \xmark & No & On-/Off-chip & After circuit generation & Reduction Network & \xmark \\
        \hline
        SET \cite{cai2023inter} & ASIC & Scheduler & Reconfigurable & \cmark & Yes (sub-opt. sched.) & On-/Off-chip & N/A & Tiled Accelerator & \xmark \\
        \hline
        \hline
        \textbf{\textsc{InTAR}} & FPGA & Design Paradigm & Reconfigurable & \cmark & No & On-chip only & Circuit design time & General & \cmark \\
        \hline
    \end{tabular}
    }
    \label{tab:reconfig_comp}
\end{table*}

Depending on the available resources on FPGAs and the properties of tasks, \textsc{InTAR} configures the resources into sequential, task-pipeline, or task-parallel mode, corresponding to the dataflow and sequential execution patterns mentioned in Section \ref{sec:intro}. 
\begin{itemize}[wide=0pt]
    \item \textbf{Sequential}: All resources perform a single task in parallel.
    \item \textbf{Task-pipeline}: Same as dataflow execution with pipelined dependent tasks. Computational resources are partitioned proportionally to the workloads. Data are streamed from one task to another using FIFOs. 
    \item \textbf{Task-parallel}: Same as dataflow execution with independent tasks running in parallel. The resource efficiency is the same as the sequential mode, but it may have a better locality for subsequent tasks (e.g., for the example in upper left of Figure \ref{fig:attn_sample}, if calculating $V'$ and $O$ are pipelined, then running $Q$ and $V$ in parallel and assigning $V$ to the PE that will compute $V'$ avoids data aggregation overhead from other PEs). 
\end{itemize}

Figure \ref{fig:attn_sample}(f) illustrates the task assignment and timeline of \textsc{InTAR} for the case study in Section \ref{sec:case}. PEs 1 and 2 handle both attention computation and linear projections. Depending on the subsequent operations, \textsc{InTAR} can choose either task-parallel or sequential mode for $Q$ and $V$. If the scheduler decides to serialize computing $V'$ and $O$, then the sequential mode is more suitable to allow PEs 1 and 2 to execute cooperatively. If the scheduler pipelines computations of $V'$ and $O$, then task-parallel mode is preferred to improve locality. Both choices will not affect the latency but will determine the data movement overhead of the following tasks. After calculating $Q$ and $V$, \textsc{InTAR} switches to task-pipeline mode to compute $K$ and $A$, since storing $Q$ consumes all scratchpad memory. 


% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=0.8\linewidth]{intra_csra_flow.png}
%     \caption{An overview of the CDS flow. Circles represent the tasks in a dataflow graph, or operations in DNNs. The same color indicates the same operations.}
%     \label{fig:csra}
% \end{figure*}


\subsection{Architecture Template} \label{sec:templ}

Figure \ref{fig:intrra-template} is the architecture template of \textsc{InTAR} for HDV DNN applications, which consists of a grid of compute cores (CC) computing linear operations and special function units (SFU) inserted in between. Each CC contains:
\begin{itemize}[wide=0pt]
    \item Reconfigurable PE Array (PEA): a multi-dimensional array of PEs that allows changing inputs and precisions. Data communication between PEs is either in a systolic array style or by broadcasting the data to all PEs. Each PE produces the complete or partial result of the linear operation.
    \item Scratchpad Memory: store intermediate and weight data.
    \item Reduction Unit: read the partial outputs from the PEA if needed and perform accumulations. 
    \item Data Movement Control Unit: determine the flow of data to the scratchpad memory and other CCs/SFUs.
\end{itemize}
When switching execution modes, a global instruction reader will read both the configuration instructions from a static buffer and the input size passed as the kernel argument. It will then modify part of each instruction related to the input size (e.g., the loop bounds) and send it to the CCs. The reconfigurable PEA and data movement control will read the instructions and propagate them among the CCs. The instruction sequence is determined by the static schedule of mode switching at the circuit design time. Each instruction contains a \textbf{stage index} and several\textbf{ loop bounds} (e.g., reconfiguring the three loop bounds for GEMMs). A stage refers to the execution of a single or a group of tasks that follows one of the execution modes, seen in Figure \ref{fig:attn_sample}(f), e.g., computing $Q$ and $V$ in stage 1 and $K$ and $A$ in stage 2. Each CC has a specified behavior for each stage, orchestrated by multiplexers with stage indices as the select signals. Notice that the stage-specific behaviors vary from one application to another. Thus, \textsc{InTAR} only needs to handle the reconfigurations defined in that application-specific schedule. The reconfiguration overhead consists of four parts: reading, modifying, sending, and decoding instructions to generate signals for multiplexers in the CCs. Each of these is done in a single cycle. Thus, reconfigurations have a 4-cycle overhead (10 to 20 ns, depending on frequency), which is negligible compared to modern DNN inference latency. 

The template has several design parameters for users to derive a specific architecture, including:
\begin{itemize}[wide=0pt]
    \item Number of rows $n$ and columns $m$ of the CCs. 
    \item Instantiation of the SFUs, which determine the positions and computations of each SFU.
    \item FIFO connections between the CCs and SFUs.
    \item Scratchpad memory size and number of memory ports.
    \item Reconfigurable PE array dimensions.
\end{itemize}
Depending on the schedule, we assign values to these parameters to perform the three execution modes mentioned in Section \ref{sec:comp-mode}. The right of Figure \ref{fig:intrra-template} depicts the example architectures of each execution mode derived from the template for $n,m=2$ for the case study in Section \ref{sec:case}. The colors of connections indicate the purpose of communications (read input and weights, perform reductions, redistribute output, or pipeline intermediate results). For a specific schedule, we compose the derived architectures of the execution modes utilized to get the final circuit design. \textbf{Common interconnections and control logic across execution modes will be merged, and unused interconnects will never be introduced.}

% \subsection{Challenges of \textsc{InTAR}}

% We identify two challenges that will impede the advocating of \textsc{InTAR} as an accelerator design paradigm on FPGAs:

% \noindent \textbf{Fast and convenient design flow.} Previous work in reconfigurable accelerator on FPGAs \cite{liu2022overgen, cong2014fully, tong2024FEATHER} relies on time-consuming RTL flow, but supports multiple applications with a single bitstream. Since \textsc{InTAR} requires design per application, it is crucial to have a faster and more convenient design flow rather than adopting the RTL flow. This will make designing large applications more manageable.

% \noindent \textbf{Complex logic required to repurpose resource on FPGAs.} To generate a place-able and routable design on FPGA with high clock frequency, having utilization under resource constraints is far from enough. Especially for designs with auto-reconfiguration, the control logic is usually complex and difficult to achieve timing closure.

\section{Designing Accelerators in \textsc{InTAR}} \label{sec:design}

The first step in designing \textsc{InTAR} accelerators is performing design space exploration (DSE). Given a dataflow graph (DFG) and device constraints, one searches through various combinations of architecture parameters and task execution modes (see Section \ref{sec:intrra}) to minimize total latency.

\subsection{Finding Schedule and Architecture Template Parameters}
In this work, we first topologically sort the tasks based on dependencies in the DFG and apply a heuristic-based DSE:
\begin{itemize}[wide=0pt]
    \item If the total size of output and previously cached data is larger than the memory constraint, then select the task-pipeline mode.
    \item Otherwise, if task-parallel mode can improve locality for the subsequent tasks as illustrated in Section \ref{sec:comp-mode}, then pick task-parallel mode. If not, then choose the sequential mode.
    \item Non-linear operations are pipelined with previous tasks.
\end{itemize}
Then, we analyze the generated schedule and the hardware configurations and determine architecture template parameters with the following heuristics: The schedule infers the instantiation of the SFUs and FIFO connections. The column count of CCs equals the maximum number of tasks of all stages, and the row count is the number of dies on the FPGA necessary to improve floorplanning. We compute the PEA dimensions to maximize utilization with limited cross-die communications and calculate the memory ports needed. Finally, the scratchpad memory is the maximum memory size required over all stages. For applications with variable input size, we schedule based on the maximum possible size and employ batching \cite{looks2017deep} to prevent inefficiency of the design for small data size. Although the heuristics are not guaranteed optimal, we show that our schedule results improved performance consistently (see Sections \ref{sec:testbench}, \ref{sec:gpt2}). We leave the development of an optimized DSE engine as future work.

\subsection{Designing Reconfigurable Modules: A Case Study in HLS}
In this work, we require the designer to manually write the reconfiguration designs based on the static schedule. Following the architecture template, there are three types of reconfigurations: data movement, compute, and control. All of them can be implemented in HLS efficiently with \textbf{conditional dataflow}, which utilizes if-else statements to declare the behavior changes and exploit HLS's resource binding pass to merge interconnects and logic as much as possible.

\noindent\textbf{Data Movement Reconfiguration}: a majority of auto-reconfigurations in \textsc{InTAR} are powered by data movement control. In different stages, each CC may load data from various sources (e.g., other CCs, on-chip buffers, registers, or constants) and send it to multiple destinations. Figure \ref{fig:hls_data_move} is an example of the data movement control reconfiguration in CC3: data flow from CC1 to buf 1 at stage 0 and flow from buf 2 to CC 2 at stage 1. The corresponding HLS code is a realization using the stage index as the condition to identify which flow the data movement control should adopt. Different stages can share data sources and destinations, which can potentially save interconnects. For instance, if the data flows from CC1 for each stage, the reader module can be reused.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\columnwidth]{imgs/data_move_repurpose.png}
    \caption{Example of data movement reconfiguration. Left: In different stages, PE 3 either read from PE 1 and write to on-chip buffer buf 1, or read from buf 2 and write to PE 2. Right: the corresponding HLS code.}
    \label{fig:hls_data_move}
\end{figure}

\noindent \textbf{Compute Reconfiguration}: Based on the definition of the architecture template, the reconfigurable PEA can change the input operands and the precision. Figure \ref{fig:hls_compute} illustrates a PEA switching the data source between stages, where each source is interpreted as a different precision. To change the input operands, we adopt a similar method of data movement reconfiguration to extract inputs from two source buffers conditioned on the stage index. Inputs are packed for data parallelism. Then, when computations start, each PE will select part of the input operand and pad with zeros to make sure the bitwidths are uniform across stages. This guides the HLS to bind compute resources between stages. The two steps cannot be merged, otherwise, the HLS will instantiate two PEAs with different precision specifications.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{imgs/compute_repurpose.png}
    \caption{Example of compute reconfiguration. Left: The reconfigurable PEA consumes different buffers at each stage and supports mixed precision of b-bit and c-bit calculations. Right: the corresponding HLS code.}
    \label{fig:hls_compute}
\end{figure}

% // dimension of PEA = M * N
% // some data read of buffers at the beginning
% config_inst inst = fifo_inst.read();
% const int stg_idx = inst.stg_idx;
% for(int i = 0; i < LEN; i++){
%   //step 1: extract inputs using data movement repurposing
%   ap_uint<WIDTH_A> op1_vec = buf_a[i]; //WIDTH_A = M * a
%   ap_uint<PACK_DATA_WIDTH> op2_vec; //PACK_DATA_WIDTH = max(b,c)*N
%   if(stg_idx == 0){
%     op2_vec = buf_b[i];
%   } else {
% 	op2_vec = buf_c[i];
%   }
%   //step 2: select right bitwidth during computation
%   for(int p = 0; p < M; p++){
%     #pragma HLS unroll
%     for(int q = 0; q < N; q++){
%       #pragma HLS unroll
%       ap_int<a> op1; ap_int<LARGE_BIT> op2;//LARGE_BIT = max(b,c)
%       op1 = op1_vec((p+1)*a-1, p*a);
%       if(stg_idx == 0){
%         // pad zero when LARGE_BIT > b
%         op2 = ap_int<LARGE_BIT>(op2_vec((q+1)*b-1, q*b));
%       } else {
%         op2 = ap_int<LARGE_BIT>(op2_vec((q+1)*c-1, q*c));
%       }
%       buf_res[p][q] += op1 * op2;
%     }
%   }
% }

\noindent \textbf{Control Reconfiguration}: In \textsc{InTAR}, there are control reconfigurations for loop bounds and data-dependent conditional dataflow. For loop-bound control, we can implement variable loop bounds for different stages of computations using the configuration instructions (left of Figure \ref{fig:hls_control}). For data-dependent conditional dataflow, when the behavior of executions depends on both stage index and other data sources, we can compose the conditions with logical operators (right of Figure \ref{fig:hls_control}). 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{imgs/control_repurpose.png}
    \caption{Example of control reconfigurations in HLS. Left: loop-bound control. Right: data-dependent conditional dataflow.}
    \label{fig:hls_control}
\end{figure}

All accelerators in the \textsc{InTAR} paradigm are covered by the compositions of the three types of reconfigurations mentioned above, indicating complete support in HLS.

\subsection{Important Considerations in Placement and Routing}

In order to achieve rapid development, we need to make sure that the generated design is both valid and performant. Following are several important considerations of implementing a placeable and routable \textsc{InTAR} design with high frequency.

\noindent\textbf{Distribute Independent Tasks Across Dies.} For multi-die FPGAs \cite{Xilinx, versal}, cross-die wires are limited and often negatively affect the timing. Thus, when encountering a design with resource utilization higher than the single-die capacity, cross-die wiring latency can be the bottleneck of low clock frequency. A simple heuristic is to place independent tasks (e.g., multi-head attentions) into different dies instead of executing with all resources to avoid cross-die communications. Since there is no data movement between these tasks, we also do not add FIFOs between these tasks, which further reduces the cross-die wiring. 

\noindent\textbf{Matching On-chip Memory Parallel Access Dimension.} To serve multiple MAC units in each PEA, scratchpad memory is partitioned by access patterns. In \textsc{InTAR}, the PEA in the CCs may require varying patterns between tasks. For example, Figure \ref{fig:mem_part} shows two dependent GEMMs: the first computes 2048 MACs simultaneously ($4 \times 8 \times 64$), and the second computes 1024 MACs ($4 \times 4 \times 64$). Since the PEA is reused across tasks, we aim to equalize the throughputs of the two matrix multiplies for efficiency, necessitating further partitioning of the second GEMM. This causes high-fanout nets at the write ports, increases cycle time, and reduces frequency. We address this by tiling operands so that output tiles have equal dimensions (e.g., $16\times16$), aligning parallel read/write dimensions even after matrix transpose. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\columnwidth]{imgs/reconfig_issue.png}
    \caption{Example of a design with high-fanout nets on write ports of on-chip memory banks due to unaligned parallel memory access dimensions.}
    \label{fig:mem_part}
\end{figure}

% \subsection{Challenges for \textsc{InTAR}}

% We identify three challenges of implementing \textsc{InTAR} on FPGAs.

% \noindent \textbf{Scheduling under resource constraints.} The ultimate goal of \textsc{InTAR} is to achieve high computational resource efficiency for lower latency within the various on-chip resource constraints of FPGAs, including memory capacity, memory ports, and DSP counts. However, deciding which tasks should run in pipeline or parallel is complex, since executing tasks in dataflow or sequentially can affect the available resources for remaining tasks.

% \noindent \textbf{Complex logic required to repurpose resource on FPGAs.} To generate a place-able and routable design on FPGA, having utilization under resource constraints is far from enough. Especially for designs with auto-reconfiguration, the control logic is usually complex and hard to route.

% \noindent \textbf{Increasing latency from cross-die wiring.} Many datacenter FPGA boards \cite{Xilinx, versal} have multiple dies in the package. While it provides more resources, a naive implementation can drastically increase the latency due to cross-die communications. 

% Section \ref{sec:sched} and \ref{sec:routing} will describe methods to resolve these challenges.


% \section{Scheduling and Resource Allocation} \label{sec:sched}

% This section will discuss one formulation to search for a scheduling and resource allocation of \textsc{InTAR} implementing the GPT-2 medium model as a showcase. 

% \subsection{Definitions} \label{sec:def}

% \begin{itemize}[wide=0pt]
%     \item \textbf{Local cut ($C^{(q)}_p$)}: A cut of an edge for a single branch of the dataflow graph, e.g., $C_0^{(0)}$ in Figure \ref{fig:dp_df_graph}. The super-script indicates the branch index and the subscript is the distance from the root node.
%     \item \textbf{Global cut ($G_i = \{C^{(q)}_p\}$)}: A set of local cuts that separate the network, e.g., $G_0$ and $G_1$ in Figure \ref{fig:dp_df_graph}.
%     \item  \textbf{Partition ($(G_i, G_j)$)}: The subgraph between two global cuts. During execution, tasks in the partition will be executed in the same \textbf{stage}, employing one of the execution modes.
%     \item \textbf{Bypass buffer ($S_{byp}$)}: data produced from previous partitions but not consumed yet in the current partition.
% \end{itemize}


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.55\columnwidth]{imgs/dp_graph.png}
%     \caption{Dataflow graph for GPT-2 models.}
%     \label{fig:dp_df_graph}
% \end{figure}

% \subsection{Problem Formulation}

% Figure \ref{fig:dp_df_graph} is the dataflow graph for GPT-2 models. We reduce multi-headed attention into a single head and scale the constraints correspondingly since we will distribute MHA computations across different SLRs (Section \ref{sec:routing}). For this scheduling problem, the \textbf{inputs} are the set of global cuts of the corresponding dataflow graph and the device constraints. The \textbf{output} is a subset of global cuts, representing the optimal partitioning. Pipelining all tasks in the dataflow graph is one schedule that minimizes the off-chip memory access but has maximum latency incurred by pipeline stalls. Hence, we set the \textbf{objective} of scheduling to \textbf{maximize latency reduction} by eliminating pipeline stalls using partitioning. Some partitioning of the dataflow graph allows us to reduce pipeline stalls by serializing task executions without the need to access off-chip memory, with no additional latency overhead since on-chip memory access overlaps with computation. For each global cut $G_i$, the function $F_{stall}(G_i)$ estimates latency reduction by removing pipeline stalls. We compute the latency reduction for tasks near the global cuts, referencing \cite{chen2024allo} for pipeline stall analysis by modeling the data production and consumption rates for dependent tasks.

% The global and local cuts have the following resource constraint functions:
% \begin{itemize}[wide=0pt]
%     \item $F_{scratch}(G_i, G_j, S_{byp})$: Required scratchpad buffer size between two global cuts, including the bypass buffer.
%     \item $F_{mac}(G_i, G_j)$: Maximum MAC operations required between two global cuts.
%     \item $F_{w}(G_i, G_j)$: Total size of weight loaded on-chip between two global cuts.
%     \item $F_{port}(G_i, G_j)$: Number of memory ports required between two global cuts to use all computational resources.
%     \item $F_{cross}(G_i, G_j)$: Maximum number of SLR-crossing wires of each SLR boundary when deploying tasks between two global cuts.
% \end{itemize}

% We impose two extra constraints in our formulation to guarantee high compute efficiency and low off-chip memory access in the final schedule:
% \begin{itemize}[wide=0pt]
%     \item \textbf{Non-GEMM tasks need to be pipelined with GEMM tasks}: Since the majority of the computations in transformers are GEMM, most computational resources are allocated for CCs. Since we do not share resources between CCs and SFUs, if a non-GEMM task is executed in serial, CCs will be idle, reducing resource efficiency.
%     \item \textbf{No off-chip memory access for intermediate data}: For transformers, each GEMM produces large intermediate data that are highly reused. Thus storing intermediate data off-chip will significantly increase off-chip memory access, increasing the total latency and energy consumption.
% \end{itemize}

% \subsection{Schedule by Dynamic Programming}

% To ensure a partition is valid, we utilize the constraint functions and the device resource constraints. There are three conditions for validating a partition: \ding{172} The required scratchpad memory and the bypass buffer should be smaller than the total on-chip memory. We assume ping-pong buffering to allow reading from and writing into memory simultaneously. \ding{173} Required MAC operations should be larger than the total available MAC units to avoid low MAC unit occupancy. \ding{174} The number of required memory ports should be smaller than the total number of available memory ports for parallel execution. \ding{175} The number of SLR-crossing wires should not exceed the maximum SLR-crossing capacity of devices. For routability and timing closure, we set the on-chip memory constraint to 40\% and the SLR-crossing constraint to 50\%. After checking the validity of a partition, we also get information about what data have not been consumed and update the bypass buffer correspondingly.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.6\columnwidth]{imgs/flowchart-schedule.png}
%     \caption{High-level flowchart of the maximum latency reduction partitioning algorithm for \textsc{InTAR}.}
%     \label{fig:flowchart}
% \end{figure}

% \begin{algorithm}
%     \small
%     \caption{Maximize Latency Reduction of Partitioning}
%     \label{alg:partition}
%     \begin{algorithmic}[1]
%         \REQUIRE A sorted list of available global cuts $GC = \{G_i\}$ of the dataflow graph from the root to the leaf. $G_L \in GC$ is the last global cut, nearest to the leaf node. $G_H \in GC$ is the first global cut, nearest to the root node.
%         \STATE Create map $M: G_i \times S_{byp} \rightarrow \{G_i\} \times \mathbb{N}$
%         \STATE $M(G_L, \emptyset) = (\emptyset, 0)$
%         \FOR{$G_S \in GC$ in reverse order}
%             \STATE $\text{cur} = 0$
%             \FOR{all possible $S_{byp}$ of $G_S$}
%                 \FOR{$G_T \in GC, G_T \ne G_S$ and $\forall C_p^{(q)} \in G_T, C_{x}^{(y)} \in G_S$, $p \ge x$}
%                     \IF{$\max{(M(G_T, \dots)[1])} + F_{stall}(G_S) \le \text{cur}$}
%                         \STATE continue
%                     \ENDIF
%                     \IF{There is no GEMM between $G_S$ and $G_T$}
%                         \STATE continue
%                     \ENDIF
%                     \STATE Valid, $S'_{byp}$ = Validate\_Partition($G_S$,$G_T$,$S_{byp}$)
%                     \IF{Valid and $M(G_T, S'_{byp})[1] + F_{stall}(G_S) > \text{cur}$}
%                         \STATE $M(G_S, S_{byp})[0] = M(G_T, S'_{byp})[0] \cup \{G_T\}$
%                         \STATE $M(G_S, S_{byp})[1] = M(G_T, S'_{byp})[1] + F_{stall}(G_S)$
%                         \STATE cur $= M(G_S, S_{byp})[1]$
%                     \ENDIF
%                 \ENDFOR
%             \ENDFOR
%         \ENDFOR
%         \STATE Call $F_{out}$, $F_w$, $F_{port}$ for all $G_i \in M(G_H, \emptyset)[0]$ and compute the maximum of each to get scratchpad size and scratchpad port number.
%         \STATE Return $M(G_H, \emptyset)$.
%     \end{algorithmic}
% \end{algorithm}

% With the validation algorithm, we formulate a 2D dynamic programming algorithm to find the maximum latency reduction of partitioning. Figure \ref{fig:flowchart} displays the high-level procedure of the algorithm. $M: G_i \times S_{byp} \rightarrow \{G_i\} \times \mathbb{N}$ is a 2D projection, with a global cut and $S_{byp}$ as the inputs and a set of global cuts (best partitioning from $G_i$ to $G_L$, the global cut nearest to the leaf node) and an integer (maximum latency reduction from the partitions). It is used to prune unnecessary calls of the validation algorithm and speed up execution. The algorithm will traverse from the global cut nearest to the leaf node ($\{C_{10}\}$ in Figure \ref{fig:dp_df_graph}) to the top global cut ($G_0$ in Figure \ref{fig:dp_df_graph}) and fill the content of $M$ gradually. For each global cut $G_S$ in this traversal, we find $G_T$, which is the global cuts closer to $G_L$ than $G_S$, and all possible bypass buffer configuration $S_{byp}$ corresponds to $G_S$. Before validating partition $(G_S, G_T)$, we incorporate two optimizations to skip partition validity checks: \ding{172} retrieving maximum latency reduction when having $G_T$ to know if it is possible to achieve higher latency reduction with $G_T$ and $G_S$, and \ding{173} skip global cuts that create partitions without GEMM. If both conditions are passed, the algorithm checks whether $(G_S, G_T)$ is a valid partition and whether it achieves better latency reduction than previous iterations, updating $M$ correspondingly. Once the algorithm finishes traversal, the optimal partition is stored in $M(G_H, \emptyset)$, where $G_H$ is the top-most global cut in the dataflow graph (i.e., $G_0$ in figure \ref{fig:dp_df_graph}). With the returned partitions, we can get the scratchpad memory and port number requirement by substituting all global cuts in the returned partitions into $F_{scratch}$, $F_w$, and $F_{port}$ constraint functions.

% The space and time complexity is $O(BD^B)$, where $B$ is the total number of branches and $D$ is the depth of the dataflow graph. For GPT-2 medium, $B=3$ and $D=10$. Furthermore, the algorithm is optimal. We will omit the detailed algorithm pseudocode and the optimality proofs for brevity.


% \noindent \textbf{Theorem}: Given the constraints in the formulation, Algorithm \ref{alg:partition} returns an optimal partition with maximum latency reduction.

% \noindent \textbf{Proof (Induction)}: The best partitioning after the last global cut of $GC$ is no partition since there is no task beyond this cut. Thus, $M(G_L, \emptyset) = (\emptyset, 0)$ is optimal.

% For $G_S \in GC$, $G_S \ne G_L$, algorithm \ref{alg:partition} traverses all $G_T$ between $G_S$ and $G_L$ (line 6). Assume $M(G_T, S_{byp})$ returns the optimal partition for any $S_{byp}$. Then, $M(G_T, S'_{byp})[1] + F_{stall}(G_S)$ is the maximum latency reduction when having $G_T$ and $G_S$ in the final partition, with the corresponding partitioning $M(G_T, S'_{byp}) \cup \{G_T\}$. Algorithm \ref{alg:partition} always keeps and assigns the maximum latency reduction to ``cur", which guarantees the optimality of $M(G_S, S_{byp})$ for any $S_{byp}$. If $\max{(M(G_T, \dots)[1])} + F_{stall}(G_S)$ is not larger than ``cur", any solution containing $G_T$ is worse than the currently saved solution for $M(G_S, S_{byp})$.

% By induction, we prove that $M(G_H, S_{byp})$ is optimal for any $S_{byp}$. For $G_H$, there is no bypass buffer as it is the first global cut. Thus $M(G_H, \emptyset)$ returns the optimal partitioning.

% \subsection{Heuristic: Multi-die Aware Pass}

% After generating a schedule with the resource-aware pass, we perform the multi-die aware pass to map each task in partitions with several heuristics. There are two objectives: reducing cross-die wiring to improve frequency and reducing cross-die communications to improve latency. 

% \noindent \textbf{Prioritize mapping IIS to different dies.} Since each IIS in the dataflow graph does not connect, there are no FIFO connections when mapping to the FPGA board. Thus, we prioritize mapping IIS to different dies to reduce cross-die wiring, such as computing each head of multi-headed attention in different dies. If there are $N$ IISs with identical dataflow, the scheduler distributes them to dies in a round-robin manner. If $N$ is not divisible by the number of dies $D$, the remaining IISs are mapped sequentially with all computational resources. The resource-aware pass ensures that each IIS can map to a single die or multiple dies.

% \noindent \textbf{Merge partitions for data locality.} For consecutive partitions with a single task, the multi-die aware pass will merge the partitions and apply task-parallel mode when: 
% \ding{172} there exists a subsequent partition with multiple pipelined tasks, and \ding{173} the data required by these tasks come from the partitions with a single task. Given the examples in Section \ref{sec:comp-mode}, this improves the data locality that reduces data aggregation overhead.

% \noindent \textbf{Cross-die wiring constraints.} Multi-die FPGAs have physical cross-die wiring constraints to follow. Having high cross-die wiring utilization can lead to high routing congestion and low clock frequency. Thus, CDS allows users to set the crossing limit between dies, which is a fraction indicating the percentage of cross-die wiring used. Accordingly, the allocated MAC units will be reduced to align with the wiring utilization for tasks that require data movement across dies.

% With the multi-die aware pass, the CDS flow can determine the number of rows and columns of CCs and SFUs. PE array allocation is also determined by the relative workload ratio between CCs and the total available MAC units.

% % \noindent \textbf{Switch between modes for independent partitions.} As illustrated in the example in Section \ref{sec:case}, the task-parallel mode is sometimes more efficient than the task-serial mode in data distribution latency. Hence, the pass will identify independent partitions and search if any partition with pipelined operations requires data from one of the independent partitions, and schedules partitions in multi-task parallel mode. In this case, PEs of the pipelined tasks can read the local scratchpad memory during computations.

% \subsection{Comparison to Other DNN Schedulers}

% Alpa \cite{zheng2022alpa} uses dynamic programming with a runtime latency model for inter-operator scheduling across multiple GPUs. Herald maps each DNN layer to the most suitable dataflow accelerator in the HDA and reduces latency by reordering computations. SET employs a simulated annealing algorithm to analyze estimated latency and energy cost. However, few of these approaches consider resource constraints like SRAM capacity, SRAM port number, and available compute resources, which are crucial for FPGAs. Table \ref{tab:sched} compares previous DNN schedulers with CDS flow.

% \begin{table}[ht]
%     \centering
%     \caption{Comparison between CDS of \textsc{InTAR} and prior works. SA means simulated annealing, and DP refers to dynamic programming.}
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{|c|c|c|c|c|c|}
%         \hline
%         \textbf{Prior} & \textbf{Target} & \textbf{Objective} & \textbf{Algorithm} & \textbf{Resource} & \textbf{Multi-Die}\\
%         \textbf{Works} & \textbf{Platform} &  & & \textbf{Aware} & \textbf{Aware} \\
%         \hline
%         SSR \cite{zhuang2024ssr} & FPGA & Min Latency & Genetic & \cmark & \xmark \\
%         \hline
%         SET \cite{cai2023inter} & Tile Acc. & Min Latency & SA & \xmark & \xmark \\
%         \hline
%         Alpa \cite{zheng2022alpa} & Multi GPU & Min Latency & DP & \xmark & \xmark \\
%         \hline
%         Herald \cite{kwon2021heterogeneous} & HDA & Min Latency & Heuristic & Mem-only & \xmark \\
%         \hline
%         \hline
%         \textbf{\textsc{InTAR}} & FPGA & Max Lat. Reduction & DP + Heuristic & \cmark & \cmark\\
%         \hline
%     \end{tabular}
%     }
%     \label{tab:sched}
% \end{table}

% \subsection{Future Work Beyond CDS flow}
% In CDS flow, we reduce the IIS and scale the constraints as a heuristic to exploit the parallelism and local data communication in each die. Although applying the resource-aware pass to the expanded dataflow graph can output better scheduling and resource allocation schemes, it can incur frequent cross-die data communication and high cross-die wiring utilizations. A future direction beyond the CDS flow can focus on performing resource and multi-die passes simultaneously to generate an optimal schedule that is floorplan-aware.

\begin{table*}[ht]
    \centering
    \caption{Descriptions of multi-task kernels with the corresponding parameters and data size constraints. The on-chip memory constraints are artificial to make the kernel HDV.}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{|p{0.09\textwidth}|>{\raggedright}p{0.23\textwidth}|>{\raggedright}p{0.18\textwidth}|>{\raggedright}p{0.17\textwidth}|p{0.2\textwidth}|p{0.17\textwidth}|}
        \hline
        \textbf{Kernel} & \textbf{Description} & \textbf{DNN Applications} & \textbf{Parameters} & \textbf{Data Size Constraints} & \textbf{Data Variation Pattern} \\
        \hline
        Self Attention (Attn) & A set of dependent matrix multiplications that extract the contextual relationships between tokens in a sequence. & Widely used in sequence modeling (Transformer \cite{vaswani2017attention}, ViT \cite{han2022survey}, GAT \cite{velivckovic2017graph}) & 256 token sequences. The hidden dimension is 1024. & Min data size: 0.5 MB, Max data size: 1.0 MB, On-chip memory constraint: 0.625 MB & Increase ($Q, K$) $\rightarrow$ Decrease ($Q, K \rightarrow A$) $\rightarrow$ Increase ($V$)$\rightarrow$ Decrease ($A, V \rightarrow V'$) \\
        \hline
        FFN Layer (FFN) & Three layers of linear projections for feature extractions. Input is a single vector. & Elementary block in many CV and LLM applications (ResNet \cite{he2016deep}, YoloV3 \cite{farhadi2018yolov3}, Transformer) & Input size = final output size = 256. First layer output size = 1024, second layer output size = 2048. & Min data size: 0.5 KB, Max data size: 2 KB, On-chip memory constraint: 1 KB & Increase $\rightarrow$ Increase $\rightarrow$ Decrease \\
        \hline
        Multi-layer CNN (M-CNN) & Four layers of convolutions with upsampling and pooling layers & Used in many CV applications (ResNet, VGG \cite{vedaldi2016vgg}) & Input size = 224, kernel size = 3, $2\times2$ upsampling and pooling size. & Min data size: 98 KB, Max data size: 1.57 MB, On-chip memory constraint: 125 KB. & Increase $\rightarrow$ Increase $\rightarrow$ Decrease $\rightarrow$ Decrease\\
        \hline
        Variational Autoencoder (VAE) & Contains an encoder in convolutions for the latent space distribution and a decoder in transpose convolutions. & Used for image compression and the generator model in GAN (VAE-GAN \cite{gao2020zero}) & Input size = 28, kernel sizes are 4 and 8, 2 channels and 2 filters & Min data size: 1.3 KB, Max data size: 3.1 KB, On-chip memory constraint: 1.5 KB &  Decrease (encoder 1) $\rightarrow$ Decrease (encoder 2) $\rightarrow$ Increase (decoder 1)$\rightarrow$ Increase (decoder 2) \\
        \hline
        Gating Network (GN) & Two parallel linear projections with element-wise product and a sequential linear projection. & Llama-family LLMs \cite{touvron2023llama} & Sequence length = 32, input dimension = 4096, hidden dimension = 11008 & Min data size: 0.25 MB, Max data size: 0.67 MB, On-chip memory constraint: 0.5 MB & Increase (up projections) $\rightarrow$ Decrease (element-wise product, down projection)\\
        \hline
    \end{tabular}
    }
    \label{tab:eval_benchmark}
\end{table*}

To evaluate \textsc{InTAR}, we will show its broadness in DNN accelerations (Section \ref{sec:testbench}) and its advantages when deploying more complex DNNs on different FPGAs (Section \ref{sec:gpt2}).

\section{Evaluation 1: Multi-Task Kernels in HDV DNNs} \label{sec:testbench}

\subsection{Experiment Setup}
To demonstrate the broadness of \textsc{InTAR} in HDV DNN accelerations, we construct a testbench with five multi-task kernels that can serve as a standalone DNN application (Multi-layer CNN, Variational Autoencoder) or participate in part of the DNN applications (Self Attention, FFN Layer, Gating Network). Table \ref{tab:eval_benchmark} illustrates these kernels in the evaluation. Unlike existing benchmarks \cite{zhou-rosetta-fpga2018, pouchet2012polybench} for FPGA-based accelerator evaluation, our testbench has three features crucial for HDV: \ding{172} it covers various DNN applications in computer vision, language modeling, graph processing, etc., displayed in the "DNN Applications" column; \ding{173} it adopts hyperparameters used in the real applications (e.g., Gating Network has the identical dimensionalities as Llama 2\cite{touvron2023llama}); and \ding{174} kernels are all HDV, and we impose a memory constraint between the minimum and maximum input/output data sizes so that \textsc{InTAR} can exploit its ability to switch execution modes. 

Using our testbench, we compare \textsc{InTAR} with human-optimized dataflow and sequential accelerators, each designed in four weeks. All designs are evaluated under identical resource constraints, including on-chip memory capacity, the number of DSPs, and off-chip memory bandwidth utilization. All designs are implemented utilizing Xilinx Vitis HLS 2021.2 with TAPA \cite{guo2023tapa} and evaluated on Alveo U280 FPGA board, with a clock frequency of 300 MHz. We chose the human-optimized designs as the baseline since: \ding{172} There is a lack of existing designs of the kernels in our testbench with the same model and hardware configurations, and \ding{173} existing frameworks \cite{Duarte:2018ite, basalama2023flexcnn, chen2024allo} for DNN application design generation on FPGAs do not handle off-chip memory communications or do not consider the extra resource constraints introduced to evaluate HDV DNNs. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{imgs/intrra-eval1-plot.png}
    \caption{Speedup and DSP efficiency of \textsc{InTAR} over dataflow and sequential execution for the five multi-task kernels in the testbench. Values are normalized from sequential accelerators.}
    \label{fig:intrra-eval1}
\end{figure}

\subsection{Analysis}

Figure \ref{fig:intrra-eval1} shows the speedup and DSP efficiency of InTAR over sequential and dataflow accelerators. Overall, InTAR achieves a speedup of $7.1 \times$ and $1.8\times$ over sequential and dataflow accelerators, along with $7.5 \times$ and $1.9\times$ higher DSP efficiency (GOP/s/DSP) in geometric mean. The performance boost of InTAR over sequential accelerators mainly comes from the reduction of off-chip memory access, where InTAR has 76\% lower off-chip memory access volume than the sequential accelerators in geomean over the five kernels. For dataflow accelerators, InTAR is significantly faster in self-attention and multi-layer CNN than the other three kernels. There are two explanations. First, the two kernels mentioned have a higher data reuse between dependent tasks for matrix multiplications and convolutions, which requires data buffering and preprocessing before continuing the computations (e.g., the pooling layer of the multi-layer CNN will wait until producing two rows of outputs from the previous layer to downsample the data). Since the selected kernels have a long chain of dependent tasks, pipeline stall latency may dominate the end-to-end latency. Second, InTAR can better reuse off-chip memory ports for these two kernels. For example, after loading inputs and finishing the first convolution layer in the multi-layer CNN, InTAR can utilize the memory ports allocated to the inputs to write outputs. Thus, InTAR has a higher effective off-chip read/write throughput than dataflow accelerators, which only use the dedicated memory port for input/output. For Gating Networks, InTAR cannot further reduce latency compared with dataflow accelerators since only input and output data sizes are within the memory constraint, and our schedule for InTAR is identical to pipelining all tasks. 

\section{Evaluation 2: GPT-2 Input Prefilling} \label{sec:gpt2}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\columnwidth]{imgs/intra_fpga_design_v2.png}
%     \caption{A sample design of InTAR mapped to Versal VPK180 FPGA for GPT-2 Medium model. (a) Overall architecture and parameters. Each SLR contains two CCs and one SFU. (b) The scheduled timeline. The color of the blocks corresponds to the PEs or SFUs shown in (a).}
%     \label{fig:u280-design}
% \end{figure}

\subsection{Experiment Setup}

We choose the GPT-2 medium input prefilling stage as a complex scenario to further evaluate \textsc{InTAR}, which is a frequently used benchmark by prior SoTA FPGA-based LLM accelerators \cite{chen2024allo, hong2022dfx}. We selected two platforms (Versal VPK180 \cite{versal-premium} and Alveo U280 \cite{Xilinx}), which result in different schedules and architecture template parameters based on their resource constraints. For instance, the design for VPK180 has four rows of CCs spreading out to four SLRs of the FPGA, while the design on U280 has 3 rows of CCs to fit its 3-SLR layout. Each SLR includes two CCs and an SFU for both platforms. Thanks to the grid structure of \textsc{InTAR}'s architecture template, we can adapt the design conveniently to other FPGA boards with only slight modifications. 

To generate the circuit design, we employ Xilinx HLS with the TAPA framework \cite{guo2023tapa} and prototype using the Vitis 2021.2 that performs well on older platforms such as U280, and Vitis 2024.1 optimized for VPK180. The floorplanning of the CCs and SFUs is predetermined based on the architectural template, while the remaining modules (e.g., auxiliary buffers) are managed by AutoBridge \cite{guo2021autobridge}. The designs are both placed and routed for U280 and VPK180. For comparison, we select Allo \cite{chen2024allo} and DFX \cite{hong2022dfx} as the SoTA accelerators, both of which utilize U280 as the compute platform; accordingly, we include InTAR implemented on U280.

% We also consider several other accelerators, but they are difficult to compare fairly. SSR \cite{zhuang2024ssr} targets the Versal AI engine and cannot deploy large transformer models due to limited on-chip memory. FEATHER's \cite{tong2024FEATHER} FPGA design does not contain the softmax module for the attention layer. FlightLLM \cite{zeng2024flightllm} only works for Llama models and has aggressive algorithmic optimizations, which are not comparable.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/compare-merged.png}
    \caption{Left: Latency and DSP efficiency of \textsc{InTAR} (U280, VPK180), Allo, and DFX. Both designs of \textsc{InTAR} are significantly more DSP efficient. Right: Latency and power efficiency of \textsc{InTAR} (U280, VPK180), Allo \cite{chen2024allo}, and GPU solutions for GPT-2 medium model. }
    \label{fig:compare-gpu}
\end{figure*}

\begin{table}[ht]
    \centering
    \caption{Resource utilization and frequency of FPGA-based solutions (Allo, DFX, and \textsc{InTAR}) for GPT-2 medium input prefilling task. Speedup is normalized based on DFX.}
    \resizebox{0.85\columnwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & Allo \cite{chen2024allo} & DFX \cite{hong2022dfx} & \textsc{InTAR} & \textsc{InTAR} \\
        \hline
        \textbf{Device} & U280 & U280 & U280 & VPK180 \\
        \hline
        \textbf{Frequency} & 247 MHz & 200 MHz & 224 MHz & 300 MHz \\
        \hline
        \textbf{BRAM} & 389 (19\%) & 1192 (59\%) &535 (27\%) & 700 (14\%) \\
        \hline
        \textbf{DSP} & 1780 (20\%) & 3533 (39.2\%) & 6727 (75\%) & 6726 (47\%) \\
        \hline
        \textbf{LUT} & 569K (44\%) & 520K (40\%) & 485K (37\%) & 1074K (32\%) \\
        \hline
        \textbf{FF} & 653K (25\%) & 959K (43\%) & 627K (25\%) & 1072K (16\%) \\
        \hline
        \textbf{URAM} & 111 (12\%) & 104 (11\%) & 336 (35\%) & 412 (16\%) \\
        \hline
        \textbf{Norm. Speedup} & $1.83 \times$ & $1.0 \times$ & $14.64 \times$ & $39.14 \times$ \\
        \hline
    \end{tabular}
    }
    \label{tab:accel}
\end{table}

Table \ref{tab:accel} lists the resource utilization and frequency. \textsc{InTAR} on VPK180 has lower DSP utilization than U280 since VPK180 has fewer cross-SLR wires than U280 and PEA sizes are highly correlated with cross-SLR communications. DFX is executed in FP16, and all other designs employ the W4A8 format. We scale the DSP efficiency for DFX to align the data type. For performance metrics, we use OpenCL profiling functions to get the latency and \verb|xbutil| to measure the power within 100 runs on U280. Due to a lack of access to a physical device, we employ QEMU and the Xilinx Power Design Manager to calculate the latency and estimate the power consumption of VPK180. DFX is not included in comparing power efficiency due to a lack of data in the original work.

We also compare InTAR with various GPUs shown in Table \ref{tab:device}. PyTorch profiler measures the latency and memory transactions. To measure the power consumption, we employ the NVIDIA management library to probe the power every 10 ms and calculate the average power when it is stable. All GPUs are inference in BFloat16 format, which is a commonly supported and optimized data format for GPUs.

\begin{table}[ht]
    \centering
    \caption{Hardware configurations of the FPGAs and GPUs}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
         & U280 & VPK180 & T4 & A100 & MI210 \\
        \hline
        \textbf{Frequency} & 224 MHz & 240 MHz & 585 MHz & 765 MHz & 1.7 GHz \\
        \hline
        \textbf{Bandwidth} & 460 GB/s & 52.1 GB/s & 320 GB/s & 1.56 TB/s & 1.64 TB/s \\
        \hline
        \textbf{Peak Power} & 75 W & 180 W & 70 W & 250 W & 300 W \\
        \hline
        \textbf{Peak Perf.} & 8.09 TOP/s & 20.7 TOP/s & 65.13 TOP/s & 311.84 TOP/s & 181 TOP/s \\
        \hline
        \textbf{Process Node} & TSMC 16nm & TSMC 7nm & TSMC 12nm & TSMC 7nm & TSMC 6nm \\
        \hline
    \end{tabular}
    }
    \label{tab:device}
\end{table}

\subsection{Analysis} \label{sec:eval}

\textit{Comparison with FPGA-based Accelerators.} Figure \ref{fig:compare-gpu} presents the latency, DSP efficiency, and power efficiency comparison between FPGA and GPU accelerators. Compared to Allo and DFX, \textsc{InTAR} on U280 is $7.99 \times$ and $14.64 \times$ faster, and $2.19 \times$ and $3.98 \times$ more DSP efficient, respectively. \textsc{InTAR} on VPK180 is $21.39 \times$ and $39.14\times$ faster, and $5.66 \times$ and $10.44 \times$ more DSP efficient than Allo and DFX. \textsc{InTAR} on VPK180 is more DSP efficient than U280 since DSP58 has a higher throughput than DSP48. Additionally, implementing DSPs in dot-product modules on VPK180 trims the logic and further reduces cycles, leading to more than $2\times$ boost in DSP efficiency. Furthermore, similar to Evaluation 1 in Section \ref{sec:testbench}, we compared InTAR with our manually implemented dataflow and sequential accelerators, achieving $1.8\times$ and $8.3\times$ speedup and $1.7\times$ and $8.2\times$ DSP efficiency boost.

% \begin{table}[ht]
%     \centering
%     \caption{Performance comparison between \textsc{InTAR}, Allo, and FQ-BERT in throughput and DSP efficiency. The result for FQ-BERT is estimated for GPT-2 medium.}
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{|c|c|c|c|}
%         \hline
%          & Allo \cite{chen2024allo} & FQ-BERT \cite{liu2021hardware} & \textsc{InTAR} (U280/VPK180) \\
%         \hline
%         \textbf{Model} & GPT-2 Medium & BERT-base & GPT-2 Medium \\
%         \hline
%         \textbf{Sequence Length} & 128 & 128 & 128 \\
%         \hline
%         \textbf{Latency (ms)} & 316.22 & 23.79 & 45.91 / 23.03 \\
%         \hline
%         \textbf{Throughput (GOP/s)} & 249.60 ($1.0\times$) & 939.47 ($3.8\times$) & 1719.23 ($6.9\times$) / 3427.26 ($13.7\times$) \\
%         \hline
%         \textbf{GOP/s/DSP} & 0.14 ($1.0\times$) & 0.29 ($2.1\times$) & 0.26 ($1.9\times$) / 0.51 ($3.6\times$) \\
%         \hline
%         \hline
%         \textbf{Model} & GPT-2 Medium & GPT-2 Medium$^*$ & GPT-2 Medium \\
%         \hline
%         \textbf{Sequence Length} & 512 & 512 & 512 \\
%         \hline
%         \textbf{Latency (ms)} & 1096.48 & 1152.65 & 184.01 / 73.65 \\
%         \hline
%         \textbf{Throughput (GOP/s)} & 305.57 ($1.0\times$) & 290.68 ($0.95 \times$) & 1820.88 ($6.0\times$) / 4549.36 ($14.9\times$) \\
%         \hline
%         \textbf{GOP/s/DSP} & 0.17 ($1.0\times$) & 0.09 $(0.53\times)$ & 0.28 ($1.6\times$) / 0.68 ($4.0\times$)\\
%         \hline
%     \end{tabular}
%     }
%     \label{tab:throughput}
% \end{table}

\textit{Comparison with GPUs.} As shown in Figure \ref{fig:compare-gpu}, \textsc{InTAR} on U280 achieves a $1.07\times$ speedup in geometric mean over T4 and $2.41\times$ speedup on VPK180. Especially for short sequences ($L \le 256$), InTARs performance surpasses both T4 and A100 GPUs. Moreover, with the same process node, \textsc{InTAR} on FPGAs attains higher power efficiency than GPUs. \textsc{InTAR} on U280 is $2.37 \times$ more efficient than NVIDIA T4. \textsc{InTAR} on VPK180 is $1.66\times \sim 7.17\times$ more efficient than T4, A100, MI210. One source of latency and power overhead for GPUs is the off-chip memory access. Since GPUs execute tasks sequentially and GPT-2 medium tends to generate large intermediate data, off-chip memory access is more intense on GPUs than \textsc{InTAR}-optimized FPGA designs. As a result, \textsc{InTAR} has a $20\% \sim 67\%$ lower off-chip memory access compared to T4, A100, and MI210.

% \begin{table}[ht]
%     \centering
%     \caption{Normalized off-chip memory access volume of GPUs with various attention algorithms and \textsc{InTAR}}
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{|c|c|c|c|}
%         \hline
%         & Algorithm & Norm. Off-chip Memory Access Volume \\
%         \hline
%         \textsc{InTAR} & Eager & 1 \\
%         \hline
%         NVIDIA-T4 & Eager & 1.67\\
%         \hline
%         NVIDIA-A100 & Eager & 1.20\\
%         \hline
%         NVIDIA-A100 & FlashAttention \cite{dao2022flashattention} & 1.01\\
%         \hline
%     \end{tabular}
%     }
%     \label{tab:mem_alg}
% \end{table}

% \section{Related Works}

% \subsection{Other DNN Accelerators on FPGAs}

% FlexCNN \cite{basalama2023flexcnn} is another framework specific for CNN model accelerator generation on FPGA. FET-OPU \cite{bai2023fet} is an FPGA-based overlay for transformer models, which employs an instruction control unit to dispatch executions of a systolic array and a special function unit. The design is only synthesized. DFX \cite{hong2022dfx} is another overlay design for single and multi-FPGA LLM applications. FlightLLM \cite{zeng2024flightllm} is a vector processor implemented on U280 for Llama model inference. DNNExplorer \cite{zhang2020dnnexplorer} is a hybrid accelerator that applies dataflow designs for the first few layers and executes the rest of the layers on a generic overlay. CHARM \cite{zhuang2023charm} is a framework to generate heterogeneous accelerator on Versal ACAP platform.

% \subsection{Reconfigurable Accelerators}

% Fully pipelined composable architecture (FPCA) \cite{cong2014fully} allows deploying and reconfiguring for multiple DNN tasks and pipelines the computations to increase resource utilization. Overgen \cite{liu2022overgen} is an overlay generator on FPGA for domain-specific applications. The compute engine is implemented in CGRA. Due to design complexity for generalizability, these accelerators have low clock frequency and low single-task resource efficiency. \cite{zhao2023token} deploys a reconfigurable systolic array to pipeline part of the attention layer. 

% SET \cite{cai2023inter} schedules on tiled accelerators using a time-space resource allocation tree. It assumes a very flexible NoC for communication and cannot optimally allocate tasks to every tile at every cycle. FEATHER \cite{tong2024FEATHER} focuses on intra-task reconfiguration for reduction operations in CNN, which is a local optimization.

\section{Conclusion and Future Work}

In this work, we propose InTAR, a novel accelerator design paradigm for DNNs with high data volume variation. InTAR enhances resource efficiency by switching execution patterns and performing model-specific optimizations. It surpasses SoTA FPGA accelerators in speed, DSP, and power efficiency, and, compared with GPUS, it reduces off-chip memory access.

Although focused on DNNs, InTAR may also enhance non-DNN HDV applications, which we plan to explore. Future work includes developing an optimized DSE engine to replace heuristic scheduling and automate InTAR design compilation.

\section*{Acknowledgement}

This work was supported in part by PRISM, one of the seven centers in the JUMP 2.0 program sponsored by SRC and DARPA. It is also supported by CDSC industrial partners and the AMD \footnote{J. Cong has a financial interest in AMD} HACC Program.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliography{references}
\bibliographystyle{IEEEtran}

% that's all folks
\end{document}


