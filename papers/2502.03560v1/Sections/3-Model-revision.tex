\begin{figure}[!t]
\centering
  \includegraphics[width=0.49\textwidth]{Images/model.png}
  \caption{
  a) An information-processing view of human error ~\cite{wickens2021engineering} assumes that a typing error can be produced by any step in a sequence of three: interpretation, intention, and execution. 
  Slips are incorrectly executed movements,
  lapses are incorrect commands, and mistakes emerge when misinterpretation of the typed text leads to inappropriate decisions about what to do.
  %
  b) ~\name extends the architecture that underpins \textsc{CRTypist}. With ~\name, the system models the cognitive processes that generate errors. Moreover, the supervisory controller can observe the consequences of errors~\cite{shi2024crtypist}.}
  \label{fig:model}
\end{figure}

\section{\name: The Modeling Principles and Design}

The primary goal for ~\name is to reproduce human-style typing errors, including the way people make corrections \cite{pinet2022correction}, without compromising the overall realism of the model's predictions relative to the previous state-of-the-art model~\cite{shi2024crtypist}. In addition, we wanted the model to be able to run directly on pixels as in previous work 
and to account for individual differences. To this end, our modeling approach employs three principles, discussed below.
The first marks the most significant advance, and the other two are assumptions that, while developed in prior work,
we have adapted to account for more types of errors. All three are drawn together into a single model.

\paragraph{Noisy cognitive capabilities.} 
We model cognitive resources as limited-capacity channels. 
When these resources are requested to be faster, they generate more errors.
While previous work has applied this principle to model motor control in typing, we here extend it to cover vision and working memory.
Specifically, \textsc{Vision} controls the gaze movement to observe the screen, processing pixels through foveated and peripheral views; \textsc{Working memory} holds information about what has been typed with a level of uncertainty.
An important feature of our model of these capabilities is that they contain theory-inspired empirical parameters that contain the level of noise. 
This allows us to simulate users with different abilities.

\paragraph{Hierarchical supervisory control.} 
We assume that there are two levels of control in typing: high and low.
Higher-level control takes a supervisory role~\cite{botvinick2012hierarchical, frank2012mechanisms}. 
It monitors what happens (based on its beliefs) and sets goals accordingly for low-level controllers.
At the low level, two motor systems are responsible for movement: one handles eye control, and the other controls the fingers.
These systems are given a goal (e.g., to press ``K''), which they try to reach in a way that factors in their own, limited abilities.
This hierarchical approach confers greater modeling power: we can now model these abilities independently of each other, as opposed to in an end-to-end manner.
It also gives a boost to training, because we can train the controllers separately.


\paragraph{Computational rationality.} 
The final assumption is that the (high- and low-level) controllers adjust their policy to maximize expected utility,
while optimality is bounded by the noisy cognitive abilities \cite{oulasvirta2022computational}. 
In practice, that entailed formulating typing as a partially observable Markov decision process (POMDP). This is consistent with prior work \cite{jokinen2017modelling,jokinen2021touchscreen},
but we added a new element by introducing error-producing mechanisms in the cognitive environment of the supervisory controller. 

\subsection{Noisy cognitive capabilities}
\label{sec:errors-generating}

\rv{
One key contribution of the model lies in covering diverse noisy cognitive capabilities in a unified model, whereas previous modeling studies considered only a subset of finger slips.
}
Slips, lapses, and mistakes are connected with different but partially overlapping generation mechanisms~\cite{reason1990human}. 
Slips are unintended and uncontrolled actions, lapses occur when people forget to do something, and mistakes are incorrect decisions that a person makes in the mistaken belief that this is the right thing to do. 

We took an information processing approach to categorizing human errors~\cite{wickens2021engineering}, then applied the resulting framework to touchscreen typing. 
Our mapping from the information processing perspective to transcription typing is illustrated in Figure~\ref{fig:model}~(a), where each component leads to one of the specific types of human error, which are interconnected into a complete process. 
%
When interacting with a touchscreen, individuals may gain inaccurate perceptions of the text typed, which lead to mistakes in their typing. Subsequently, they might forget to perform corrective typing actions, because of memory lapses. Finally, slips in motor control can cause them to execute finger movements incorrectly. 

Rather than list every possible error in each category, we adhered to Occam's razor and identified the major factors in the human errors that occur often in touchscreen typing. In our model, each latent mechanism at play in these errors is controlled by at least one error parameter, for factoring in the relevant capability. Through combining these mechanisms, the model can replicate diverse human errors.

\subsubsection{Slips}

Slips happen when there is a discrepancy between intention and execution. In touchscreen typing, slips are often caused by motor control errors due to physical limitations such as hand tremors or the fat finger problem.
The precision of fingertip movement depends on motor control noise, which varies with speed and distance ~\cite{fitts1954information}.

We simulate this underlying mechanism by using the Weighted Homographic (WHo) model~\cite{guiard2015mathematical}: $(y-y_0)^{1-k_\alpha}(x-x_0)^{k_\alpha} = F_K$,
In this model, $x$ represents the movement time of the finger, $y$ represents the standard deviation for the spread of the finger's endpoint, and $F_K$ is a parameter that controls finger capability â€“ a smaller $F_K$ value indicates more accurate movement. This motor control noise can lead to substitution errors (tapping a key adjacent to the intended one etc.) or omission errors (the finger not hitting any key). 

We simulate other types of slips also -- specifically, unintentional double taps and swapping of motor commands, which are influenced by finger movement speed: $P(v)=1 - e^{-k \cdot v}$. Higher typing speed can increase the likelihood of unintended insertions and transpositions.
In the case of double tapping, the finger makes a movement to the same key immediately, while swapping of motor commands can disturb the keystroke order when the finger is close to the key that should come \emph(after) the next one.

\subsubsection{Lapses}

In touchscreen typing, lapses occur when people forget to give a command to their fingers, such that steps in the process get skipped. These mistakes are often attributed to cognitive errors resulting from forgetfulness~\cite{nicolau2012elderly}.

\name simulates this latent mechanism by modeling the probability of forgetting to give a motor command to the fingers at character level. That is, we assume that, when people's memory of what has been typed is weak, they could forget to type what they intended to type next.
We simplify the likelihood of this by randomly forgetting a character to type, related to the time $t$ since the last proofreading, using exponential decay: $P(t) = 1 - e^{-kt}$, where $k$ is a free parameter that controls the likelihood of forgetting. With a lower $k$ value, fewer lapses occur during typing, with a minimum of $k = 0$, at which there are no lapses.


\subsubsection{Mistakes}

In touchscreen typing, mistakes can be attributed to incorrectly observing the touchscreen. This has two aspects: misreading already-typed text during proofreading and inaccurately observing the finger's position during visual guidance.
The first mechanism is related to the accuracy of proofreading. It is possible for a user focusing on the text field to overlook errors and perceive incorrect text as correct. This affects error handling. We model the mechanism by expressing the conditional probability of missing a typo during proofreading via the time-dependent function $P_{\text {obs--text}}= p_0 \cdot e^{-T}$, where longer-duration proofreading increases the likelihood of accuracy.

The second mechanism manifests itself during visual guidance when the gaze is on the finger. Occlusion may lead to inaccurate observation of the finger's position~\cite{baudisch2009back}; that is, a finger obstructing some part of the keyboard could make it difficult to determine the position accurately. We use a constant value $P_{\text {obs--finger}}$ to model the conditional probability of missing a finger slip caused by finger movement during visual guidance.

\begin{figure*}[!t]
\centering
  \includegraphics[width=\textwidth]{Images/fullcase.png}
  \caption{A simulation example involving multiple mechanisms that generate various text errors and corrections. In typing of ``welcome to chi'' with the Gboard interface, the following errors occur: 1) the model initially forgets to type the letter ``l'' (an omission error) though then quickly correcting it; 2) it accidentally types ``e'' instead of ``w'' (making a substitution error) although it corrects this mistake as well; 3) and, at the end of the sentence, it makes an insertion error by double tapping ``i'' -- with the model failing to detect this and submitting the text as-is.}
  \label{fig:errorcase}
\end{figure*}

\subsection{\rv{Hierarchical supervisory control}} 
\label{sec:supervisory-control}

People can strategically modulate the resources they allocate to precluding or correcting errors ~\cite{anderson2004integrated, fodor1983modularity}.
\rv{Our model's architecture design is anchored in that of the latest supervisory typing model, CRTypist~\cite{shi2024crtypist}, which models the supervisory control problem as deciding where to look and where to move the finger. Specifically, we built \name on the internal environment of CRTypist, which furnishes the interface between the control policy and the touchscreen. The three key components within this internal environment each have distinct abilities and limitations:}
\textsc{Vision} is responsible for moving the gaze to observe the screen from pixels via foveated and peripheral views; the \textsc{finger} decides on the finger movement for tapping on the touchscreen keyboard; and \textsc{working memory} holds both the information about what has been typed and the belief data. In general, the modeling for the first two of these is controlled by the supervisor in parallel in line with the belief from the memory.
\rv{
As is illustrated in Figure~\ref{fig:model}~(b), where our model diverges from the design of CRTypist is in integrating noisy cognitive capabilities into the supervisory control architecture by adding mistakes to the vision implementation for proofreading and visual guidance, adding lapses to the commands to the finger module, and adding slips to execution by the finger module â€“ with all these error mechanisms being parameterizable factors that affect the internal environment. The parameters' effects are reflected further in the belief tied to the observation retrieved from working memory.
}

Putting it all together, we model typing with errors as a POMDP.
The supervisory controller attempts to type a phrase given to it.
However, it has only partial access to the touchscreen through the internal environment. 
Moreover, the internal environment is stochastic, arising directly from the error-creating mechanisms we describe above.
The POMDP definition is as follows:
\begin{itemize}
    \item The full state, $\mathcal{S}$, includes all information about the screen, at pixel level. This cannot be directly observed.
    \item The observation space, $\mathcal{O}$, supplies the belief as to what has been typed and the probabilities for each error type: the probability of missing a typo when proofreading, the probability of missing a finger slip, the probability of forgetting a motor command, that of an unintentional double tap, the probability of unintentional swapping of motor commands, and finger motor control noise. The reason we include these error-related beliefs in our observations is to make sure the model is able to adjust its behavior to the error capacity.
    \item The action space, $\mathcal{A}$, dictates the goals for both finger and gaze movements. Specifically, the goal for the finger is to reach the next key to be typed, while the vision's focus is split between the key and the input field. Once the goals are set, the vision and finger modules within the internal environment execute the actual movements, using pre-trained models introduced in previous work ~\cite{shi2024crtypist}.
    \item The reward function, $\mathcal{R} = (1 - \textit{Err}^{\alpha}) - w \cdot t$, combines error rates and the time budget, where $\textit{Err}$ represents the error rate, $\alpha$ controls sensitivity to errors, $w$ is the weight assigned to time, and $t$ is the time taken. This formulation encourages a balance between speed and accuracy.
\end{itemize}


\subsection{\rv{Computational rationality}}

\label{sec:optimization}
We followed the main steps of workflows geared for building computationally rational models of human behavior~\cite{chandramouli2024workflow}, where the goal is to train an agent to replicate human decision-making processes as closely as possible. In our case, the agentâ€™s optimal policy for the supervisory controller is trained via reinforcement learning (RL) with Proximal Policy Optimization (PPO) from the \texttt{stable-baselines3} library~\cite{schulman2017proximal}, over the course of 5 million timesteps. During this training, the agent learns to predict human typing patterns by continually refining its policy in response to observed behaviors in the simulated environment. We chose PPO for its ability to effectively balance exploration and exploitation during training while also guaranteeing stability through its clipped objective function, which limits large, destabilizing policy updates \cite{schulman_proximal_2017}.

\rv{
Another improvement introduced by \name is parameter fitting for a computationally rational model through joint optimization. 
The goal is to achieve an optimal and stable policy within a large behavior space that accommodates diverse error-relevant behaviors.
Beyond the optimization of human parameters, the behavior of the model depends on the hyperparameters of the model's training. 
Careful selection of such parameters is essential, since they significantly influence the performance of RL agents ~\cite{andrychowicz_what_2020, paine_hyperparameter_2020, yang_efficient_2021}, and even small changes in the implementation of RL algorithms can affect their performance~\cite{engstrom_implementation_2020}.
}

To infer the optimal parameters, we used a two-loop optimization process to jointly optimize parameters. In the outer loop, the model is trained with a variety of human parameters, while in the inner loop, it identifies the optimal user group characteristics that the agent can model. 
This process seeks to pinpoint the best combination of model hyperparameters and human parameters, in order to optimize the typing model.

\begin{itemize}
    \item \textit{Outer loop optimization}: The outer loop focuses on optimizing key hyperparameters that influence the process for training the RL agent (e.g., the entropy coefficient and clipping range). Optimizing these hyperparameters is important because they directly affect how the agent interacts with its environment and learns from that interaction. In the outer loop, these hyperparameters are refined to minimize the difference between the agent's typing behavior and the target human typing behavior, measured in terms of the Jensen--Shannon divergence~\cite{shi2024crtypist}. This loop aims to find a general typing model that works well across the full range of user behaviors.
    \item \textit{Inner loop optimization}:  Within each iteration of the outer loop, the inner one optimizes the parameters that are essential for adapting the agent to distinct user groups. These parameters reflect variations in typing speed, accuracy, and style among users.
\end{itemize}

Both the outer and inner loops use a Bayesian optimization (BO) framework to guide the search for optimal parameters. We chose BO for this problem because it efficiently handles medium-dimensional and expensive-to-compute objective functions~\cite{gel_bayesian_2018}. The optimization process returns as its output the optimal general typing model and a set of human parameters, resulting in a robust typing model that performs well in various scenarios. Details of the parameters involved in the optimization can be found in the supplemental material.

\subsection{\rv{Simulation and visualization}}

% \subsection{Simulation Example}

Figure~\ref{fig:errorcase} gives an example of the simulation results from the model. It illustrates how errors arise and the coordination between the eyes and fingers in handling them.
Specifically, the figure depicts three sorts of text error (an omission, substitution, and insertion error), stemming from two mechanisms (lapses and slips). The first two errors have been corrected, while the third has been left uncorrected.
Such material attests that our model generates not only errors in text but also  moment-to-moment behavior in typing and fixing errors.

To help practitioners and researchers simulate behaviors, we developed a visualization tool shown in Figure ~\ref{fig:UI}. 
The interface comprises a parameter setting panel (on the left) and a behavior analysis one (on the right). From the parameter setting panel, users can input a target text phrase for typing, choose a keyboard layout, and set error parameters. Upon clicking of the ``Submit'' button, the model loads the specified parameters and simulates typing behaviors, consistent with the inputs. The typing behavior generated is represented through three types of visualization:  a) A trajectory view displays the spatial movements of both gaze and finger. b) A heatmap view shows the spatial distributions of the regions traversed by the finger (in blue) and gaze (in red). c) A time series view presents the key-by-key distances from the positions of gaze and finger to the next key to tap over time, indicating the temporal relationship between the finger and the gaze. This visualization-based exploration tool allows users to fine-tune the model manually, thereby simulating human error behaviors, ones that closely match specific user performance.

\begin{figure}[!t]
\centering
  \includegraphics[width=0.48\textwidth]{Images/UI.png}
  \caption{Visualization tool for exploring simulations. a) Via the settings panel, users can choose a target phrase for typing with the specified keyboard layout and adjust error parameters. b) The behavior analysis panel displays simulated gaze and finger movement to demonstrate the human error-linked behavior. To simulate different scenarios, the user can adjust parameters that affect the error-generating mechanisms in the model.}
  \label{fig:UI}
\end{figure}