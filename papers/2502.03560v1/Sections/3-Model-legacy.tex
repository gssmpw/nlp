\section{Typoist: Modeling Principles and Design}

\begin{figure}[!t]
\centering
  \includegraphics[width=0.9\textwidth]{Images/error-model.png}
  \caption{
  1) The errors-generating process is based on the information processing approach of human errors~\cite{wickens2021engineering}, where we include three categories errors in three steps of typing: Slips are the unintentional execution of finger movements; Lapses occur when intentions of commands to the finger are forgotten; Mistakes happen when the typed text is misinterpreted, leading to wrong decisions.
  2) The model implementation is based on CRTypist~\cite{shi2024crtypist}, where we extend sub-modules for errors in the internal environment. The supervisor control the vision and finger to handle errors based on the belief of typed text and finger position. The reward is defined with a speed–accuracy tradeoff: typing correct intended phrases as quickly as possible.}
  \label{fig:model}
\end{figure}

% \subsection{Modeling Overview}

Modeling human errors in touchscreen typing is not trivial, as it involves a complex interaction of perceptual, cognitive, and motor control processes.
We follow the workflow of building computationally rational models of human behavior~\cite{chandramouli2024workflow}.
In order to improve our modeling, we consider the following three modeling requirements.

First, we need to cover a range of different types of human errors and their causes. For example, errors in typing can stem from various reasons such as misobservation of the screen or mistyping an incorrect key adjacent to the correct one. To create a comprehensive model for human errors, we must integrate all of these error mechanisms to produce diverse errors that are more human-like.
To achieve this, we draw on the theory of human errors~\cite{reason1990human} and create an error-generating process (Sec.~\ref{sec:errors-generating}).

Next, our model should reflect the user's strategies for correcting errors. Error correction behavior is closely linked to errors, as humans have subconscious mechanisms for making corrections~\cite{pinet2022correction}. Human error correction involves not only finger movements for backspacing but also gaze movements for proofreading. By modeling these error correction behaviors, we can understand the coordination between gaze and hand movements during error correction, which shows the process of speed-error tradeoff. 
Additionally, the model should account for the impact of auto-correction on the correction behaviors, which could decrease the probability of manual correction. It should also reflect the changes in behavior for the speed-error tradeoff due to different typing conditions. We develop a supervisory control of vision and finger over errors (Sec.~\ref{sec:supervisory-control}) to address these aspects.

Last, our model needs to be adaptable to individual differences in typing behaviors. These differences are especially prominent among different user groups and are related to cognitive and physical abilities~\cite{sarcar2016towards, sarcar2018ability}. For instance, users with Parkinson's disease may make more insertion and substitution errors due to hand tremors~\cite{wang2021facilitating}, while elderly users may make more omission errors due to cognitive errors~\cite{nicolau2012elderly}. 
To support these special user groups, we allow for the adjustment of error parameters in the internal model, enabling immediate observation of the corresponding behaviors based on the trained model (Sec.~\ref{sec:exploration}). To ensure that our model captures general human behavior, we infer human parameters to achieve the optimal settings for the model (Sec.~\ref{sec:optimization}).

\subsection{Cognitive mechanisms that produce errors}
\label{sec:errors-generating}

The theory of human errors proposes three main categories of errors—slips, lapses, and mistakes—which are associated with different but partially overlapping generating mechanisms~\cite{reason1990human}. 
Slips are unintended and uncontrolled actions, lapses occur when people forget to do something, and mistakes are wrong decisions that people make while thinking they are doing the right thing. 
We adopt the information processing approach to categorize human errors~\cite{wickens2021engineering} and apply this framework to touchscreen typing. 
This mapping from the information processing approach to transcription typing is illustrated in Fig.~\ref{fig:model}-1, where each component leads to a specific type of human error, and they are interconnected as a complete process. 
Similar to the decision-making process, when interacting with a touchscreen, individuals may inaccurately perceive the typed text, leading to mistakes in their typing. Subsequently, they may forget to carry out typing actions due to memory lapses, and finally, they may execute finger movements incorrectly due to slips in motor control. Rather than listing all possible errors in each category, we adhere to Occam's razor and identify the major factors of human errors that frequently occur in touchscreen typing. Each latent mechanism in these errors is controlled by at least one error parameter to regulate the capability of this error. Through the combination of these mechanisms, the model can replicate diverse human errors.

\subsubsection{Slips}

Slips happen when there is a discrepancy between intention and execution. In touchscreen typing, slips are often caused by errors in motor control due to physical limitations such as hand tremors or the well-known ``fat finger problem''~\cite{siek2005fat}.
The precision of fingertip movement depends on motor control noise, which varies with speed and distance ~\cite{fitts1954information}.
We simulate this underlying mechanism using the Weighted Homographic model~\cite{guiard2015mathematical}: $(y-y_0)^{1-k_\alpha}(x-x_0)^{k_\alpha} = F_K$,
In this model, $x$ represents the movement time of the finger, $y$ represents the standard deviation for the spread of the finger's endpoint, and $F_K$ is a parameter that controls finger capability – a smaller $F_K$ value indicates more accurate movement. This motor control noise can lead to substitution errors – tapping the wrong adjacent keys – or omission errors when finger taps on no key. 
We also simulate other types of slips , such as unintentional double taps and swapping of motor commands, which are influenced by finger movement speed: $P(v)=1 - e^{-k \cdot v}$, which means that higher typing speed can increase the likelihood of unintended insertions and transpositions.
In the case of double tapping, the finger makes another movement to the same key immediately, while swapping of motor commands can disrupt the correct order of keystrokes when the finger is close to the key after next.

\subsubsection{Lapses}

Slips are incorrect actions that stem from the right intentions, while lapses refer to failures that involve not taking any action. In touchscreen typing, lapses occur when people forget to give a command to their fingers, leading to skipped steps in the process. These mistakes are often attributed to cognitive errors resulting from forgetfulness~\cite{nicolau2012elderly}.
In this module, we simulate this latent mechanism by modeling the probability of forgetting to give a motor command to the fingers at the character level. This means we assume that people might have a chance to forget to type what they intended to type next when their memory strength about what has been typed is low.
We simplify the probability by randomly forgetting a character to type next, related to the time $t$ from the last proofreading time, using exponential decay: $P(t) = 1 - e^{-kt}$, where $k$ is the free parameter that controls the likelihood of forgetting. A lower $k$ value means fewer lapses could happen during typing, and when $k = 0$, there will be no lapses.


\subsubsection{Mistakes}

Mistakes occur when there is a failure to accurately assess a situation, leading to wrongful intentions. In touchscreen typing, mistakes can be attributed to incorrectly observing the touchscreen, resulting in making the wrong decisions. This encompasses two aspects: misreading the typed text during proofreading and inaccurately observing the finger's position during visual guidance.
The first mechanism relates to the accuracy of proofreading. When the focus is on the text field, there is a possibility of overlooking errors and perceiving incorrect text as correct, impacting error handling. We model this mechanism using the conditional probability of missing a typo during proofreading as a time-dependent function $P_{\text {obs--text}}= p_0 \cdot e^{-T}$, where a longer proofreading time increases the likelihood of accuracy.
The second mechanism occurs during visual guidance when observing the finger. Inaccurate observation of the finger's position may result from content occlusion~\cite{baudisch2009back}, where the finger may obstruct a part of the keyboard, making it difficult to accurately determine its position. We use a constant value $P_{\text {obs--finger}}$ to model the conditional probability of missing a finger slip caused by finger movement during visual guidance.

\subsection{Supervisory control to handle errors}
\label{sec:supervisory-control}

Beyond the process of generating errors, it is more important to understand how people react to errors and deal with them. In nature, humans can strategically modulate the amount of resources they allocate to prevent errors or affect the probability of errors~\cite{anderson2004integrated, fodor1983modularity}.
Therefore, we model error-handling behaviors by controlling resources over errors, following the principles of computational rationality ~\cite{oulasvirta2022computational}. 
The control model design is based on the latest supervisory typing model for touchscreen typing~\cite{shi2024crtypist}, where the supervisory control problem is modeled as deciding where the vision should look and where the finger should move.
The supervisory controller relies on an internal environment to interact with the touchscreen, which involves cognitive and physical limitations.
There are three key modules in the internal environment: Vision is responsible for the gaze movement to observe the screen from pixels through foveated and peripheral view; Finger decides the finger movement to tap on the touchscreen keyboard; Working memory holds the information about what has been typed with a belief. Generally, the finger and vision models are controlled by the supervisor in parallel based on the information from working memory.

As illustrated in Fig.~\ref{fig:model}-2, the delta of our model is to integrate the error-generating process into this supervisory control architecture by adding mistakes into the vision for proofreading and visual guidance, adding lapses to the command to finger module, and adding slips to the execution of the finger module – all of these error mechanisms are with parameters affecting the internal environment. Additionally, the observation from the working memory involves the parameters of these error mechanisms as the belief.

% The cognitive decision rate of the supervisor is 50ms, which is less than 70ms cognitive decision rate in classic GOMS model. However, as vision and finger will not take commands from supervisor model when they are in action and their in-action time duration might be longer, so it will not affect the realistic decision time. A shorter decision cycle will give more precious simulation.

We model the typing with error as a Partially Observable Markov Decision Process (POMDP) since the supervisory controller only has partial access to the touchscreen through the internal environment. 
\begin{itemize}
    \item The full state, $\mathcal{S}$, includes the whole pixel-level information on the screen and cannot be directly observed.
    \item The observation space, $\mathcal{O}$, provides the belief of what has been typed and the probabilities of errors, including conditional probability of missing a typo when proofreading; conditional probability of missing a finger slip; probability of forgetting a motor command; probability of unintentional double tap; probability of unintentional swapping of motor commands; and finger motor control noise.
    \item The action space, $\mathcal{A}$, determines the goals for both finger and gaze movements. Specifically, the finger's goal is to reach the next key to be typed, while the vision focus is on both the key and the input field. Once the goals are set, the vision and finger modules within the internal environment execute the actual movements using pre-trained models that were introduced in the previous work.
    \item The reward function, $\mathcal{R} = (1 - \text{Err}^{\alpha}) - w \cdot t$, combines error rates and time budget, where $\text{Err}$ represents the error rate, $\alpha$ controls the sensitivity to errors, $w$ is the weight assigned to time, and $t$ is the time taken. This formulation encourages a balance between speed and accuracy."
\end{itemize}
The optimal policy of supervisory controller is trained via a reinforcement learning algorithm, using proximal policy optimization from the stable-baselines3 library~\cite{schulman2017proximal}.

\subsection{Exploration of model behaviors}
\label{sec:exploration}

The design of the internal environment based on computational rationality provides flexibility to adjust error parameters, which can lead to various behaviors.
To investigate diverse error-generating and error-handling behaviors, we have developed a model visualization interface for analyzing user behaviors in touchscreen typing, as depicted in Fig.~\ref{fig:UI}. The interface comprises a parameter setting panel on the left and a behavior analysis panel on the right. In the parameter setting panel, users can input a target text phrase for typing, choose a keyboard layout, and select error parameters. Upon clicking the ``Submit'' button, the model will load the specified parameters and simulate typing behaviors based on the inputs. The simulated typing behavior is represented through three types of visualizations:  a) A trajectory view displays the spatial movements of both gaze and finger; b) A heatmap view shows the spatial distributions of the regions traversed by finger (blue) and gaze (red); c) A time series view presents the key-by-key distances from the positions of gaze and finger to the next key to tap over time, indicating the temporal relationship between finger and gaze. This visualization-based exploration tool allows users to fine-tune the model manually, simulating human error behaviors to replicate specific user performance.

\begin{figure}[!h]
\centering
  \includegraphics[width=0.9\textwidth]{Images/UI.png}
  \caption{The visualization interface of the model allows users to explore various user behaviors on touchscreen keyboards. a) Users can choose a target phrase to type on a specific keyboard and adjust error parameters in the settings panel. b) Gaze and finger movement are simulated and displayed in the behavior analysis panel to demonstrate human error behavior. Users have the freedom to simulate different scenarios by adjusting error parameters.}
  \label{fig:UI}
\end{figure}

\subsection{Parameter inference}
\label{sec:optimization}

% Joint Optimization of Parameters

Beyond the human parameters in the internal environment, the behavior of the model depends on several factors, including the hyperparameters of the model training and the weights of the reward function used during training. Research has shown that careful selection of parameters can significantly improve the performance of RL agents~\cite{andrychowicz_what_2020, paine_hyperparameter_2020, yang_efficient_2021}, and even small changes in the implementation of these algorithms can affect their performance~\cite{engstrom_implementation_2020}. To optimize the typing model, our goal is to minimize the difference between the agent's predicted typing patterns and actual human typing patterns, measured using the Jensen-Shannon divergence. This ensures that the agent's typing behavior closely matches that of human users, leading to more realistic and effective interactions.

To infer the optimal parameters, we used a two-loop optimization process (see details in Appendix.~\ref{appendix:optimization}) to jointly optimize parameters. In the outer loop, the model is trained with a variety of human parameters, while in the inner loop, it identifies the optimal user group characteristics that the agent can model. This process aims to identify the best combination of reward, model, and human parameters in order to optimize the typing model.

\begin{itemize}
    \item \textit{Outer loop optimization}: The outer loop focuses on optimizing key parameters that influence both the reward system and the training process of the RL agent (e.g., entropy coefficient and clipping range). Optimizing these parameters is important because they directly affect how the agent interacts with its environment and learns from these interactions. In the outer loop, these parameters are refined to minimize the difference between the agent's typing behavior and the target human typing behavior. This loop aims to find a general typing model that works well for different user behaviors.
    \item \textit{Inner loop optimization}:  Inside each iteration of the outer loop, the inner loop optimizes the parameters that are essential for adapting the agent to different user groups. These parameters reflect variations in typing speed, accuracy, and style among users.
\end{itemize}

Both the outer and inner loops use a Bayesian Optimization (BO) framework to guide the search for optimal parameters. BO is chosen for this problem because it efficiently handles high-dimensional and expensive objective functions (Frazier, Powell, and Dayanik, 2018).  At the end of the optimization process, the optimal general typing model and a set of human parameters are returned, resulting in a robust typing model that performs well in various scenarios. The detailed parameters involved in the optimization can be found in the supplemental material.

% Inference Not Fitting!
% The draft's emphasis on the flexibility of the approach taken (computational rationality and RL) is a double edged sword. The upside is that such a "simple" model can describe the data sets (of transcription typing). The downside is the burden of convincing the reader -- me included - that the approach is not under-constrained enough that it can do just about any predictions given data (aka overfitting). So if possible the exposition of the work should be made tighter / more rigorous wherever possible.  

% The proposed model is defined as a Reinforcement Learning (RL) agent that learns to perform tasks through interactions with its environment. The performance of an RL agent depends on many factors, such as the RL algorithm, parameters, hyperparameters, and the reward system used during training. Research has shown that careful selection of parameters can significantly improve the performance of RL agents \cite{andrychowicz_what_2020, paine_hyperparameter_2020, yang_efficient_2021}, and that even small changes in the implementation of these algorithms can affect their performance \cite{engstrom_implementation_2020}. To optimize the typing model, our goal was to minimize the difference between the agent's predicted typing patterns and actual human typing patterns, measured using the Jensen-Shannon Divergence. This ensures that the agent's typing behavior closely matches that of human users, leading to more realistic and effective interactions.

% To achieve this optimization goal, we performed a joint optimization of parameters using a two-loop process. This process is designed to find the best set of reward, model, and human parameters to optimize the typing model, as described below:

% \begin{itemize}
%     \item \textbf{Outer Loop Optimization}: The outer loop focuses on optimizing key parameters that influence both the reward system and the training process of the RL agent. Optimizing these parameters is important because they directly affect how the agent interacts with its environment and learns from these interactions. In the outer loop, these parameters are refined to minimize the difference between the agent's typing behavior and the target human typing behavior. This loop aims to find a general typing model that works well for different user behaviors. The following parameters are optimized in the outer loop:
%     \begin{itemize}
%         \item \textbf{Reward Parameters}: These parameters control how rewards are given to the RL agent based on typing accuracy and time efficiency. The reward $r$ is defined as:
%         \begin{equation}
%         r = (1 - \text{cer}^{\alpha}) - w \cdot t,
%         \end{equation}
%         where:
%         \begin{itemize}
%             \item $\alpha$: exponential power constant that adjusts the sensitivity of the reward to the character error rate (cer).
%             \item $w$: weight for time in the reward calculation.
%             \item cer: character error rate, indicating the percentage of incorrectly typed characters.
%             \item $t$: typing time.
%         \end{itemize}
%         The parameters $w$, and $\alpha$ are the ones optimized in the outer loop.
%         \item \textbf{Training Parameters}: These parameters control various aspects of the agent's training process, such as exploration and learning from experiences. The parameters optimized include:
%         \begin{itemize}
%             \item Lambda parameter for Generalized Advantage Estimation (GAE), which affects the trade-off between bias and variance in advantage estimation during training.
%             \item Discount factor for future rewards, which determines how much the agent values immediate rewards compared to future rewards.
%             \item Learning rate, which defines the step size for the optimization algorithm and affects how quickly or slowly the model learns from new data.
%             \item Number of training epochs, which determines how many times the learning algorithm updates the model parameters per training iteration.
%             \item Entropy coefficient, which encourages exploration by penalizing deterministic policies, helping the agent explore a wider range of actions.
%             \item Clipping range for policy updates, which helps stabilize training by preventing large updates that could destabilize the policy.
%         \end{itemize}
%         \item \textbf{Neural Network Architecture Parameters}: These parameters define the architecture of the neural networks used by the RL agent, which affects its ability to learn and generalize from data. Specifically, the number of layers and the number of neurons in each layer of the policy network are optimized for the typing agent.
%     \end{itemize}
%     \item \textbf{Inner Loop Optimization}:  Inside each iteration of the outer loop, the inner loop optimizes the human parameters. These parameters are important for adapting the agent to different user groups, reflecting variations in typing speed, accuracy, and style among users. The parameters being optimized include:
%     \begin{itemize}
%         \item Memory retention: Reflects the user's ability to remember key placements and typing patterns.
%         \item Finger dexterity: Represents the user's typing speed and finger movement efficiency.
%         \item Vision acuity: Indicates the user's visual precision in locating keys.
%         \item Proofreading errors: Accounts for mistakes made during the proofreading process.
%         \item Guidance mistakes: Measures errors due to incorrect guidance or decision-making while typing.
%         \item Forgetting rate: Captures the rate at which the user forgets key placements or typed content. 
%         \item Bouncing rate: Represents the frequency of repeated key presses (e.g., double-pressing a key unintentionally).
%         \item Order errors: Reflects the occurrence of typing characters in the wrong order.
%     \end{itemize}
%     The inner loop fine-tunes these parameters to identify the specific user group characteristics that the current model can best replicate. This ensures that the agent is not only optimized for a general typing pattern but also capable of adjusting to individual user behaviors.
% \end{itemize}

% Both the outer and inner loops use a Bayesian Optimization (BO) framework to guide the search for optimal parameters. BO is useful for this problem because it efficiently handles high-dimensional and expensive objective functions. Manually searching for the optimal parameters would be impractical due to the large and complex parameter space, where interactions between parameters can have non-linear effects on the model's performance. BO addresses this challenge by using a probabilistic surrogate model to predict the performance of different parameter sets and strategically sampling from this model to find the optimum with fewer evaluations \cite{frazier_tutorial_2018}. This approach reduces the computational resources and time needed to identify high-performing parameters.

% By using this two-loop Bayesian Optimization approach, the model is trained in the outer loop with a range of human parameters, and in the inner loop, it finds the best user group characteristics that the agent can model. At the end of the optimization process, the best general typing model and a set of human parameters are returned, resulting in a robust typing model that performs well in various scenarios. The pseudo-code for the optimization algorithm can be found in the Supplemental material.