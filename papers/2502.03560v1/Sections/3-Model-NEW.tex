\begin{figure}[!t]
\centering
  \includegraphics[width=0.9\textwidth]{Images/error-model.png}
  \caption{
  a) The information-processing view of human error ~\cite{wickens2021engineering} assumes that a typing error can be produced by any step in a sequence of three steps: Interpretation, Intention, and Execution . 
  Starting from the right-hand side, Slips are incorrectly executed movements; 
  Lapses are incorrect commands; and Mistakes happen when the typed text is misinterpreted, leading to wrong decisions about what to do.
  %
  b) ~\name extends an architecture explored in \textsc{CRTypist}. In ~\name, the cognitive processes are modeled that generate errors. Moreover, the supervisory controller can observe the consequences of errors~\cite{shi2024crtypist}.}
  \label{fig:model}
\end{figure}

\section{Typoist: Modeling Principles and Design}

The primary goal of ~\name is to reproduce human-like typing errors, including the way they correct them \cite{pinet2022correction}, without compromising the overall realism of model predictions when compared to the previous state-of-the-art model~\cite{shi2024crtypist}. In addition, we want the model to be able to run directly on pixels as previous work,
and account for key individual differences. To this end, our modeling approach employs three principles. The first is the most significant advance over prior work. While the two latter assumptions have been developed in prior work,
we adapt them to account for more types of errors. Also, we unify all principles into a single model. 

\paragraph{Noisy cognitive capabilities.} 
We model cognitive resources as limited-capacity channels. 
When these resources are request to be faster, they generate more errors.
While previous work has applied this principle to model motor control in typing, we here extend it to cover vision and working memory.
Specifically, \textsc{Vision} controls the gaze movement to observe the screen, processing pixels through foveated and peripheral views; \textsc{Working memory} holds information about what has been typed with a level of uncertainty.
An important feature of our model of these capabilities is that they contain theory-inspired empirical parameters that contain the level of noise. 
This allows us to simulate users with different abilities.

\paragraph{Hierarchical supervisory control~\cite{botvinick2012hierarchical, frank2012mechanisms}.} 
We assume that there are two levels of control in typing: high and low.
The high level takes a supervisory role. 
It monitors what happens (based on its beliefs) and sets goals accordingly to lower-level controllers.
The low level consists of two motor systems responsible for movement: one controls the eye and the other controls the fingers. 
These systems are given a goal (e.g., press 'K') and they try to accomplish that in a way that factors in their own limited abilities.
This hierarchical approach gives us some modeling power: we can now model these abilities independently of each -- as opposed to an end-to-end approach.
It also makes it possible to boost training, because we can train the controllers separately.

\paragraph{Computational rationality.} 
The assumption here is that the (high and low level) controllers adapt their policy to maximize expected utility,
however optimality is bounded by the noisy cognitive abilities \cite{oulasvirta2022computational}. 
In practice, like prior work, we formulate typing as a partially observable Markov decision process \cite{jokinen2017modelling,jokinen2021touchscreen}.
However, the new element is the introduction of error-producing mechanisms in the cognitive environment of the supervisory controller. 

\subsection{Noisy cognitive capabilities}
\label{sec:errors-generating}

Slips, lapses, and mistakes are associated with different but partially overlapping generating mechanisms~\cite{reason1990human}. 
Slips are unintended and uncontrolled actions, lapses occur when people forget to do something, and mistakes are wrong decisions that people make while thinking they are doing the right thing. 
We adopt the information processing approach to categorize human errors~\cite{wickens2021engineering} and apply this framework to touchscreen typing. 
This mapping from the information processing approach to transcription typing is illustrated in Figure~\ref{fig:model}-a, where each component leads to a specific type of human error, and they are interconnected as a complete process. 
%
When interacting with a touchscreen, individuals may inaccurately perceive the typed text, leading to mistakes in their typing. Subsequently, they may forget to carry out typing actions due to memory lapses, and finally, they may execute finger movements incorrectly due to slips in motor control. Rather than listing all possible errors in each category, we adhere to Occam's razor and identify the major factors of human errors that frequently occur in touchscreen typing. Each latent mechanism in these errors is controlled by at least one error parameter to regulate the capability of this error. Through the combination of these mechanisms, the model can replicate diverse human errors.

\subsubsection{Slips}

Slips happen when there is a discrepancy between intention and execution. In touchscreen typing, slips are often caused by errors in motor control due to physical limitations such as hand tremors or the well-known ``fat finger problem''~\cite{siek2005fat}.
The precision of fingertip movement depends on motor control noise, which varies with speed and distance ~\cite{fitts1954information}.

We simulate this underlying mechanism using the Weighted Homographic (WHo) model~\cite{guiard2015mathematical}: $(y-y_0)^{1-k_\alpha}(x-x_0)^{k_\alpha} = F_K$,
In this model, $x$ represents the movement time of the finger, $y$ represents the standard deviation for the spread of the finger's endpoint, and $F_K$ is a parameter that controls finger capability – a smaller $F_K$ value indicates more accurate movement. This motor control noise can lead to substitution errors – tapping the wrong adjacent keys – or omission errors when finger taps on no key. 

We also simulate other types of slips, specifically  unintentional double taps and swapping of motor commands, which are influenced by finger movement speed: $P(v)=1 - e^{-k \cdot v}$. Higher typing speed can increase the likelihood of unintended insertions and transpositions.
In the case of double tapping, the finger makes another movement to the same key immediately, while swapping of motor commands can disrupt the correct order of keystrokes when the finger is close to the key after next.

\subsubsection{Lapses}

In touchscreen typing, lapses occur when people forget to give a command to their fingers, leading to skipped steps in the process. These mistakes are often attributed to cognitive errors resulting from forgetfulness~\cite{nicolau2012elderly}.

In this module, we simulate this latent mechanism by modeling the probability of forgetting to give a motor command to the fingers at the character level. This means we assume that people might have a chance to forget to type what they intended to type next when their memory strength about what has been typed is low.
We simplify the probability by randomly forgetting a character to type next, related to the time $t$ from the last proofreading time, using exponential decay: $P(t) = 1 - e^{-kt}$, where $k$ is the free parameter that controls the likelihood of forgetting. A lower $k$ value means fewer lapses could happen during typing, and when $k = 0$, there will be no lapses.


\subsubsection{Mistakes}

In touchscreen typing, mistakes can be attributed to incorrectly observing the touchscreen. This encompasses two aspects: misreading the typed text during proofreading and inaccurately observing the finger's position during visual guidance.
The first mechanism relates to the accuracy of proofreading. When the focus is on the text field, there is a possibility of overlooking errors and perceiving incorrect text as correct, impacting error handling. We model this mechanism using the conditional probability of missing a typo during proofreading as a time-dependent function $P_{\text {obs--text}}= p_0 \cdot e^{-T}$, where a longer proofreading time increases the likelihood of accuracy.

The second mechanism occurs during visual guidance when observing the finger. Inaccurate observation of the finger's position may result from content occlusion~\cite{baudisch2009back}, where the finger may obstruct a part of the keyboard, making it difficult to accurately determine its position. We use a constant value $P_{\text {obs--finger}}$ to model the conditional probability of missing a finger slip caused by finger movement during visual guidance.

\begin{figure}[!t]
\centering
  \includegraphics[width=\textwidth]{Images/fullcase.png}
  \caption{A simulation example involving multiple mechanisms that generate various text errors and corrections.  When typing ``welcome to chi'' on Gboard, the following errors occur: (1) the model initially forgets to type the letter ``l'' (omission error) but then quickly corrects it; (2) it accidentally types ``e'' instead of ``w'' (substitution error) but then corrects this mistake as well; (3) at the end of the sentence, it makes an insertion error by double tapping ``i'', but the model fails to detect this and submits the text directly.}
  \label{fig:errorcase}
\end{figure}

\subsection{Supervisory control} 
\label{sec:supervisory-control}

People can strategically modulate the amount of resources they allocate to prevent errors or affect the probability of errors~\cite{anderson2004integrated, fodor1983modularity}.
Our control model design is based on the latest supervisory typing model for touchscreen typing~\cite{shi2024crtypist}, where the supervisory control problem is modeled as deciding where the vision should look and where the finger should move.
The supervisory controller relies on an internal environment to interact with the touchscreen, which involves cognitive and physical limitations.
There are three key modules in the internal environment: \textsc{Vision} is responsible for the gaze movement to observe the screen from pixels through foveated and peripheral view; \textsc{Finger} decides the finger movement to tap on the touchscreen keyboard; \textsc{Working memory} holds the information about what has been typed with a belief. Generally, the \textsc{Finger} and \textsc{Vision} models are controlled by the supervisor in parallel based on the belief from \textsc{Working memory}.

%As illustrated in Figure~\ref{fig:model}-2, the delta of our model is to integrate the error-generating process into this supervisory control architecture by adding mistakes into the vision for proofreading and visual guidance, adding lapses to the command to finger module, and adding slips to the execution of the finger module – all of these error mechanisms are with parameters affecting the internal environment. Additionally, the observation from the working memory involves the parameters of these error mechanisms as the belief.

% The cognitive decision rate of the supervisor is 50ms, which is less than 70ms cognitive decision rate in classic GOMS model. However, as vision and finger will not take commands from supervisor model when they are in action and their in-action time duration might be longer, so it will not affect the realistic decision time. A shorter decision cycle will give more precious simulation.

Putting it all together, we model typing with errors as a \emph{Partially Observable Markov Decision Process} (POMDP).
The supervisor controller attempts to type a phrase given to it.
However, it has partial access to the touchscreen through the internal environment. 
Moreover, the internatl environment is stochastic, specifically due to the error-creating mechanisms we describe above.

The POMDP definition is as follows:
\begin{itemize}
    \item The full state, $\mathcal{S}$, includes the whole pixel-level information on the screen and cannot be directly observed.
    \item The observation space, $\mathcal{O}$, provides the belief of what has been typed and the probabilities of errors, including conditional probability of missing a typo when proofreading; conditional probability of missing a finger slip; probability of forgetting a motor command; probability of unintentional double tap; probability of unintentional swapping of motor commands; and finger motor control noise. The reason we include these error-related beliefs in our observations is to ensure that the model is able to adjust its behavior to its error capacity.
    \item The action space, $\mathcal{A}$, determines the goals for both finger and gaze movements. Specifically, the finger's goal is to reach the next key to be typed, while the vision focus is on both the key and the input field. Once the goals are set, the vision and finger modules within the internal environment execute the actual movements using pre-trained models that were introduced in previous work ~\cite{shi2024crtypist}.
    \item The reward function, $\mathcal{R} = (1 - \text{Err}^{\alpha}) - w \cdot t$, combines error rates and time budget, where $\text{Err}$ represents the error rate, $\alpha$ controls the sensitivity to errors, $w$ is the weight assigned to time, and $t$ is the time taken. This formulation encourages a balance between speed and accuracy."
\end{itemize}

\subsection{Simulation Example}

In Figure~\ref{fig:errorcase}, there is an example of the simulation results from the model. It illustrates how errors happen and the coordination between the eyes and fingers when handling errors.
Specifically, there are three text errors (omission, substitution, and insertion error) stemming from two mechanisms (lapses and slips). The first two errors have been corrected, while the third has been left uncorrected.
It shows that our model not only generates errors in text, but also produces moment-to-moment behavior of typing and fixing errors.

\subsection{Model training and parameter inference}
\label{sec:optimization}
We follow the main steps of workflow for building computationally rational models of human behavior~\cite{chandramouli2024workflow}, where the goal is to train an agent to replicate human decision-making processes as closely as possible. In our case, the agent’s optimal policy for the supervisory controller is trained using Reinforcement Learning (RL) with Proximal Policy Optimization (PPO) from the stable-baselines3 library~\cite{schulman2017proximal}, over the course of 10 million timesteps. During this training, the agent learns to predict human typing patterns by continually refining its policy in response to observed behaviors in the simulated environment. PPO was selected due to its ability to effectively balance exploration and exploitation during training, while also ensuring stability through its clipped objective function, which limits large, destabilizing policy updates \cite{schulman_proximal_2017}.

Beyond the human parameters in the internal environment, the behavior of the model depends on several factors, including the hyperparameters of the model training and the weights of the reward function used during training. Careful selection of such parameters is essential, as they significantly influence how well the agent can learn from its environment and adapt to human-like behaviors. Many studies have shown that this selection can significantly improve the performance of RL agents~\cite{andrychowicz_what_2020, paine_hyperparameter_2020, yang_efficient_2021}, and even small changes in the implementation of RL algorithms can affect their performance~\cite{engstrom_implementation_2020}. To optimize the typing model, our goal is to minimize the difference between the agent's predicted typing patterns and actual human typing patterns, measured using the Jensen-Shannon divergence. This ensures that the agent's typing behavior closely matches that of human users.

To infer the optimal parameters, we used a two-loop optimization process to jointly optimize parameters. In the outer loop, the model is trained with a variety of human parameters, while in the inner loop, it identifies the optimal user group characteristics that the agent can model. This process aims to identify the best combination of reward, model, and human parameters in order to optimize the typing model.

\begin{itemize}
    \item \textit{Outer loop optimization}: The outer loop focuses on optimizing key parameters that influence both the reward system and the training process of the RL agent (e.g., entropy coefficient and clipping range). Optimizing these parameters is important because they directly affect how the agent interacts with its environment and learns from these interactions. In the outer loop, these parameters are refined to minimize the difference between the agent's typing behavior and the target human typing behavior. This loop aims to find a general typing model that works well for different user behaviors.
    \item \textit{Inner loop optimization}:  Inside each iteration of the outer loop, the inner loop optimizes the parameters that are essential for adapting the agent to different user groups. These parameters reflect variations in typing speed, accuracy, and style among users.
\end{itemize}

Both the outer and inner loops use a Bayesian Optimization (BO) framework to guide the search for optimal parameters. BO is chosen for this problem because it efficiently handles high-dimensional and expensive objective functions~\cite{gel_bayesian_2018}. At the end of the optimization process, the optimal general typing model and a set of human parameters are returned, resulting in a robust typing model that performs well in various scenarios. The detailed parameters involved in the optimization can be found in the supplemental material.

\subsection{Visualization tool} 
\label{sec:exploration}

To help practitioners and researchers simulate behaviors, we developed a visualization tool shown in Figure ~\ref{fig:UI}. 
The interface comprises a parameter setting panel on the left and a behavior analysis panel on the right. In the parameter setting panel, users can input a target text phrase for typing, choose a keyboard layout, and select error parameters. Upon clicking the ``Submit'' button, the model will load the specified parameters and simulate typing behaviors based on the inputs. The simulated typing behavior is represented through three types of visualizations:  a) A trajectory view displays the spatial movements of both gaze and finger; b) A heatmap view shows the spatial distributions of the regions traversed by finger (blue) and gaze (red); c) A time series view presents the key-by-key distances from the positions of gaze and finger to the next key to tap over time, indicating the temporal relationship between finger and gaze. This visualization-based exploration tool allows users to fine-tune the model manually, simulating human error behaviors to replicate specific user performance.

\begin{figure}[!t]
\centering
  \includegraphics[width=0.75\textwidth]{Images/UI.png}
  \caption{We offer a tool for exploring simulations. a) Users can choose a target phrase to type on a specific keyboard and adjust error parameters in the settings panel. b) Gaze and finger movement are simulated and displayed in the behavior analysis panel to demonstrate human error behavior. Users can simulate different scenarios by adjusting parameters that affect the error-generating mechanisms in the model.}
  \label{fig:UI}
\end{figure}