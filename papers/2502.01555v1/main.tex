%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
%%\documentclass[manuscript]{acmart}
\usepackage{CJKutf8}
\usepackage{multirow}
\usepackage{graphicx}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{April 28--May 2,
%   2025}{Sydney, Australia}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}
\fancyhead{} % Clears the header

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Query Brand Entity Linking in E-Commerce Search}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Dong Liu}
\authornote{\textbf{Both authors contributed equally to this research.}}
\email{liuadong@amazon.com}
%% \authornotemark[1]
\affiliation{%
  \institution{Amazon}
  \country{Luxembourg}
}
\author{Sreyashi Nag}
%% \authornote{Both authors contributed equally to this research.}
\email{sreyanag@amazon.com}
\authornotemark[1]
\affiliation{%
  \institution{Amazon}
  \country{USA}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Dong et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  In this work, we address the brand entity linking problem for e-commerce search queries. The entity linking task is done by either i) a two-stage process consisting of entity mention detection followed by entity disambiguation or ii) an end-to-end linking approaches that directly fetch the target entity given the input text.
  The task presents unique challenges: queries are extremely short (averaging 2.4 words), lack natural language structure, and must handle a massive space of unique brands. We present a two-stage approach combining named-entity recognition with matching, and a novel end-to-end solution using extreme multi-class classification. We validate our solutions by both offline benchmarks and the impact of online A/B test.
  % Brand entity linking for e-commerce search queries is challenging due to the short text length (averaging about 2.4 words per query), lack of a typical natural language structure and the massive search space for unique brands.
  % In this paper, we first present our two-stage approach consisting of an named-entity recognition and a matching step. We then proposed a novel end-to-end approach using extreme multi-class classification approach. We conduct extensive offline experiment benchmarks to validate the solutions. Afterwards, we further validate our solution by reporting the numerical impact of the solution's online A/B test.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%% \begin{teaserfigure}
%%   \includegraphics[width=\textwidth]{sampleteaser}
%%   \caption{Seattle Mariners at Spring Training, 2010.}
%%   \Description{Enjoying the baseball game from the third-base
%%   seats. Ichiro Suzuki preparing to bat.}
%%   \label{fig:teaser}
%% \end{teaserfigure}

%% \received{20 February 2007}
%% \received[revised]{12 March 2009}
%% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.



\maketitle
\section{Introduction}
% Brand is the second most important attribute identified in search queries behind product-type. Correctly recognizing brands mentioned both explicitly (e.g., \textit{nike running shoes}) and implicitly (e.g., \textit{airpods pro}) in queries is an important component of search query understanding and is crucial to providing a good shopping experience to customers. Currently, brands are usually identified by their string names in search queries. This results in challenges in brand understanding such as (i) unifying brand name variants for global brands (e.g., \textit{Calvin Klein} and \textit{\begin{CJK}{UTF8}{min}カルバンクライン\end{CJK}}), (ii) different surface forms for the same brand (e.g., \textit{CK} and \textit{Calvin Klein}) and (iii) identifying brand relationships between parent and sub-brands (e.g., \textit{Nike} and \textit{Air Jordan}). 

Manufacturer identity is the second most important attribute identified in search queries behind product-type in e-commerce. Correctly recognizing brand names, whether mentioned directly (e.g., through explicit company names) or indirectly (e.g., through product-specific terminology), is an important component of search query understanding and is crucial to providing a good shopping experience to customers.
The current string-based approach to brand identification in search queries presents several challenges: (i) unifying brand name variants across different languages and regions (e.g., a Western brand name written in its original form versus its representation in Asian scripts), (ii) different surface forms for the same brand (e.g., abbreviations versus full names) and (iii) identifying brand relationships between parent and sub-brands (e.g., a parent company and its product line brands).
Therefore, in addition to recognizing the brand names mentioned in the query, it is also important to link them to the corresponding global brand entity.
It would be valuable to unify the concept of brand across different e-commercial stores in a single namespace, i.e., brand entity (identity to each brand itself).
Each brand entity is is unique across languages, stores and surface forms. As part of this effort, we aim to recognize the brand entity for branded search queries. We define branded search queries as those queries with clearly specified brand intents, e.g., \textit{<brand> running shoes}. Such queries have a single brand entity as its shopping intent. We view this as an entity linking problem where the brand needs to be identified and subsequently linked to a unique brand entity in our brand database.

Entity linking is the task of recognizing meaningful mentions in a textual context and linking them to a knowledge base~\citep{reinanda2020kg,sevgili2020neural}. In literature, entity linking sometimes assumes that the entity mentions are already given or detected, for example, using a named entity recognition (NER) system. However, in this paper, we define entity linking as the combined task of entity recognition and entity disambiguation that operates on the raw textual context. In the e-commerce domain, entity linking is especially important to understand search queries and the customer’s shopping intent. It is used to extract the important attributes in a search query, e.g., brand, product type etc. and link them to known entities in a knowledge base. The most common approach to entity extraction is a two-step process consisting of: i) entity recognition, and ii) entity disambiguation. The goal of entity recognition is to identify meaningful spans from the textual context.  When recognizing multiple entity types, this step also includes tagging the mentioned entity span with its entity type. The recognized spans are usually ambiguous in nature and the entity disambiguation step is then performed to link the span to a knowledge base. Often there are multiple candidate entities in the knowledge base that match the recognized entity. The entity linker generates and ranks these candidates. In two-step methods, when a mention span is identified via a Named Entity Recognition (NER) model, there are commonly three ways to generate entity candidates: i) surface form matching; ii) expansion using aliases; iii) prior probability matching. In surface form matching such as \citep{le2019distant, zwicklbauer2016robust, morene2017combine}, a candidate list is generated such each entity match at least a surface form of the mention. Matching may be done via either exact or fuzzy matching. For expansion with aliases \citep{zhang2019joint}, a dictionary of additional aliases is constructed to grouping known surface forms and aliases of a brand together. As for the prior probability method \citep{spitkovsky12a}, a pre-calculated prior probability of corresponding entities for given certain mentions is used to infer the possible entity candidates for a given query. After the candidate generation in the two-step methods, the entity candidates are usually ranked according to their relevance to the mention input, where the relevance can also take into account the context of the mention. Candidate ranking can be done via computing the Levenshtein distance or via vector product in a semantically enriched embedding space where both entity and mention are represented by semantic vectors \citep{logeswaran-etal-2019-zero,huang2015leverageing, cao-etal-2017-bridge}.

Recent approaches have also performed joint entity recognition and disambiguation. This includes approaches that perform both steps simultaneously \cite{peters2019knowledge, sorokin2018mixing}, allowing for interaction and shared knowledge between the two steps. Other approaches eliminate the two-step process entirely and tackle this problem as an end-to end task, directly predicting the entity from the input textual context. For example, \citep{kolitsas2018end} use BERT  to formulate the task as a sequence labeling problem, where each token in the text is assigned an entity link or a NIL class (a none class indicating no entity). Thus, they perform token classification over the entire entity vocabulary. Yet another approach retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context~\citep{cao2021autoregressive}.

The entity linking of e-commence queries is challenging since the query length is usually short (averaging about 2.4 words per query) and there is limited additional context. Unlike web search, such queries often lack the natural language structure and open source NLP models are unable to handle such query distributions well. Further, brands are a very important concept in e-commerce and there are hundreds of thousands of unique brands on such services with new being constantly added. In this work, we proposed different solutions to the brand entity linking problem. Our contributions can be summarized as follows:
\begin{enumerate}
\item We first build a two-stage entity linking model for e-commerce query brand entity prediction, consisting of an NER brand recognition model (we employ the pretrained NER model as shall be explained in \ref{sub-sec-ner}) and a surface form matching step.
\item We explore both lexical and semantic matching techniques for surface form matching and develop a product type-based filtering step for brand entity prediction refinement in massive brand output space.
\item We propose a novel end-to-end extreme multi-class classification model to directly takes the search query as input and predicts the relevant brand entity. The end-to-end model is then integrated with the two-stage model as a fused solution for brand entity linking problem. We conduct extensive offline experiment benchmarks and online A/B test to validate our solution.
\end{enumerate}

\section{Preliminaries}
We introduce three preliminaries models that are applied as part of our proposed methods. 
%The first two models, MetaTS-NER and Q2PT, are not re-trained and used off-the-shelf while the third model is re-trained for this task using a pre-existing framework.

\subsection{MetaTS-NER}\label{sub-sec-ner}
One of our baseline solution replies on named-entity recognition (NER). Therefore we introduce the NER model used in this work. We utilize the MetaTS-NER \citep{Li2021Meta} model for brand string detection. MetaTS-NER is a multilingual DistilmBERT-based for sequential labeling. The model is fitted with internal data to predict eight kinds of attribute type annotation, i.e., product line, media title, brand, gender, material, artist, color and size for product search queries. 
Our first two-stage framework of brand entity linking has a dependency on this model since it consumes the brand name annotations of queries. 

% It is built for eight kinds of attribute type annotation, i.e., product line, media title, brand, gender, material, artist, color and size for product search queries. After a two-phase training (pre-training with internal weak labeled data and finetuning with strong labeled dataset, named APPEN data), the trained model is used to parse the above-mentioned 8 types of query attributes reflecting the shopping intents. Our first two-stage framework has a dependency on this model since it consumes the brand name annotations of queries. Since
% this model is already available, we only need to develop the secondary phase for brand entity candidate matching and disambiguation.

\subsection{Q2PT}
Our solutions would rely on product type intents of search query to disambiguate the the brand entity linking. We thus need a product type classification model to parse a search query. The classification model that we leverage is an internal model, i.e. query-to-product-type model (Q2PT). The deep learning classification model is BERT-based search query classification model and fitted with internal dataset. We would use acronym PT for product type. 

% The Q2PT model identifies the intended product type (PT) for search queries. In the product catalog and search, the set of mutually exclusive PTs is defined and it aims to use them for improving customer experience. Q2PT is being used to move from lexical-based matching to attribute-based matching and to support various shopping pages CX features. This model is also used off-the-shelf.

\subsection{PECOS}

PECOS (Prediction in Enormous and Correlated Output Spaces)~\citep{yu2022pecos} is a general purpose tool aimed at solving extreme multi-class classification problems.
PECOS firstly representing each label by semantic vector (sparse or dense vector) and then runs repetitively clustering algorithm to build hierarchical label tree.   
Then a set of score functions are trained in supervised machine learning fashion to do inference on the hierarchical label tree. The solution allows to find best matches from a large search space effectively and efficiently.
PECOS has been applied in information retrieval track successfully for semantic matching that allows generating a semantic match-set beyond the limitation of conventional lexical matching. PECOS for semantic matching not only enjoys competitive prediction accuracy and also offers low inference latency thanks to the hierarchical clustering tree and doubly-sparse data structure~\citep{etter2021accelerating}. For example, the inference latency is less than 5 ms per query for generating predictions from a class space of 100 million products~\citep{chang2021extreme}. In this work, we would apply the framework provided by PECOS to train new classification model for our entity disambiguation task.

\section{Brand Entity Linking via Two-Stage Framework}\label{sec_two_stage_framework}
Following the typical two-stage approach in entity linking, i.e., a mention detection step followed by entity disambiguation, we utilize the MetaTS-NER model to build a two-stage framework for brand entity prediction. As shown in Figure~\ref{two-stage-framework} and Figure~\ref{ner-m2e-pecos}, in the first stage, we will extract the brand name from branded search queries using the NER model. In the second stage, we map the extracted brand name to the corresponding brand entity using a matcher followed by a filtering step. The two-stage solutions are detailed in the following sections.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{./figures/20241209/v1_model3.png}
  %% \vspace{-0.3cm}
  \caption{A NER based two-stage framework for brand entity linking}
  %% \vspace{-0.5cm}
  \label{two-stage-framework}
\end{figure}

\subsection{MetaTS-NER $+$ Exact Lexical Matching}\label{ner_exact_matching}

In our first benchmark, we implement the matcher as a static dictionary with the keys being brand names and the values being the brand entities. For an input query, MetaTS-NER detects the brand name in its string form. The output of MetaTS-NER is then fed to the matcher. The dictionary serves as an extract string matcher that performs a lookup based on this output. We construct this dictionary alike internal dataset from brand-name2entity dataset (detailed in Section~\ref{subsec:dataset}). 
In the dictionary dataset, each brand entity may have multiple textual representations (e.g., full names and their abbreviated forms). We collect all valid representations as potential keys in our dictionary. 
Since some brand name representations are store-specific, we append the store tag to the beginning of the dictionary key. For representations that are common across multiple stores, we duplicate the same brand name and add corresponding store tag as prefix. While each brand entity is unique, brand names on the other hand do not have this constraint. 

%Since there can be multiple brand entities sharing the same surface form, the dictionary can contain a one-to-many mapping. %For example, the brand name \textit{alpine} corresponds to both a cider brand (brand entity ID: \textit{483302}) and an electronics brand (brand entity: \textit{256820}) and will therefore have a value of \textit{'483302|256820'} as its dictionary value.

In building the matching mapping, there is another challenge that two brand entities may share the same brand name, i.e. a brand name may be mapped to more than one brand entities without further context. 
%For example, the brand name \textit{alpine} may correspond to a cider brand or an electronics brand.
To address the ambiguity of one-to-many mappings, we implement a filtering step to disambiguate between the multiple brand entity candidates based on the query context, i.e. the other side information in Figure~\ref{two-stage-framework}. We apply a product-type (PT) based filtering strategy that aims to match the intended product-type of the input query with the known product-types of the candidate brand entities. The product-type information for a brand entity is pre-computed based on the catalog data. Since we do not have an authoritative source for brand-PT relationships, we approximate this data indirectly from products stamped with the brand entities. For every unique brand entity, we sample and collect a list of impressed products (those shown as part of search results for any query) over a four month period, where their corresponding PTs are aggrated. Hence for each brand entities, we have a list of associated PTs. We note that this is not an exhaustive list of PTs and we are unable to collect this data for every brand entity. We use the Q2PT model to predict the product-type for the query. If the dictionary lexical matcher has mapped the brand surface form to multiple brand entities, we only select those brand entities that have PT matching the Q2PT prediction. We found that this filtering was able to narrow down the prediction to a single brand entity for over 50\% of multi-brand-entity prediction cases. We do not make any brand-entity prediction for the queries that still map to multiple brand entities to ensure high precision. The solution is illustrated in Figure~\ref{two-stage-framework}.

\subsection{NER $+$ PECOS Semantic Matching}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{./figures/20241209/m2e_pecos3.png}
  %\vspace{-0.3cm}
  \caption{A NER-and-PECOS-based two-stage framework for brand entity linking}
  %\vspace{-0.5cm}
  \label{ner-m2e-pecos}
\end{figure}
Simple string matching based on surface forms often fails to capture brand name variations. When different textual representations of the same brand (such as abbreviations and full names) are not all included in the dictionary, some valid mentions may be missed. To address this limitation, we explore semantic matching as an alternative retrieval approach for brand mention matching in entity linking.

% The lexical string matching based on brand name surface form can easily miss retrieving a relevant brand entity if the surface form contains some variations. For instance, CK and Calvin Klein represent the same brand but use different surface forms. If the dictionary does not cover both instances of the brand, one of them can be missed in mapping to its brand entity. Hence, we explore an alternate way of retrieving the brand entities through semantic matching.

We formulate the matching step as a eXtreme Multi-class Classification (XMC) problem. Given the MetaTS-NER brand output, a learning-based model aims to efficiently tag the brand name with its most relevant class (i.e., brand entity) from the large-scale output space (i.e., the brand entity space). The number of brand entities is quite large (around 600K) and many of them can be correlated to a certain degree. The large output space corresponds to a long-tailed label distribution, suggesting that many labels have very few relevant queries. These characteristics are commonly-shared in the XMC problem. From this perspective, we implement the matcher with the state-of-the-art XMC solver PECOS.

The minimum inference latency introduced by PECOS also makes it appealing as a plug-in component for existing real-time services in the query understanding engine. In Figure~\ref{ner-m2e-pecos}, we illustrate the Mention2Entity-PECOS (M2E-PECOS) method which replaces the lexical matcher by the semantic matcher powered by PECOS. Specifically, we use semantic matching for brand entity generation from a large output space to tackle the limitation in exact lexical string matching on surface forms.

\textbf{Model training and inference:} Unlike the lexical matcher which is parameter-free and does not need model training, PECOS requires training data to estimate model parameters. We replace the dictionary based exact lexical matcher with the PECOS semantic matcher to get the Mention2Entity-PECOS (M2E-PECOS) method. The input and output spaces remain the same as the lexical matching method. The model consumes the MetaTS-NER brand name prediction as its input and classifies it into one of the brand entities in the output space. 
The model is trained using the training data that shall be explained in section \ref{subsec:dataset}. 
We extract (\textit{brand name, brand entity}) pairs as relevant training instances. Each predicted brand entity comes with a relevance score, which is used in the sequential filtering stage. If the product type based filtering cannot disambiguate brand entities, we can rely on the relevance score and select the highest-scored brand entity. Both training and inference of M2E-PECOS have the vectorization step of projecting the textual brand names into numerical vector space (we use TF\_IDF vectorization for experiments). We train a single M2E-PECOS model that is capable of doing inference on multi-lingual brand names.


\section{Brand Entity Linking via Extreme Multi-Class Classification Framework}
The two-stage approaches have the following limitations: i) Since the two-stage approach relies on two independent steps, each can introduce errors, which means error can propagate from one step to the following step. ii) The first-stage NER casts a bottle neck for recall performance. If the MetaTS-NER model fails to recognize a brand, we are unable to retrieve a brand entity. The matcher can also fail to retrieve a relevant brand entity in some cases. 
Furthermore, string matching can only retrieve explicitly mentioned brands from queries. Brands can also be implied through product names. For instance, a product line name may indicate a specific manufacturer without explicitly stating it. The two-stage approach cannot retrieve such implicit brand references. Additionally, two-stage methods are more cumbersome and complex than end-to-end methods.
To tackle these limitations, we propose an end-to-end method for brand entity linking that directly predicts brand entities from input queries.

%Furthermore, this method can only retrieve explicit brands from queries. 
%Brands can also be implicitly mentioned in queries. For example, in \textit{airpods pro}, \textit{apple} is the implicit brand for this input. The two-stage approach will be unable to retrieve such brands. iii) Two-stage methods are more cumbersome and complex than end-to-end methods. To tackle these limitations, we propose an end-to-end method for brand entity linking that directly predicts the brand entities from the input queries.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{./figures/20241120/q2e_pecos_v2.png}
  %\vspace{-0.3cm}
  \caption{A \textbf{PECOS} based End-to-End framework for brand entity linking}
  %\vspace{-0.5cm}
  \label{pecos_end_to_end}
\end{figure}


% \subsection{An End-to-End solution with PECOS}

For the end-to-end model, the extreme multi-class classifier, PECOS is directly exposed to the input queries. This simplifies model training, deployment and inference significantly compared to the two-stage approach. Figure \ref{pecos_end_to_end} shows the framework. The PECOS model directly takes a query as input and predicts relevant brand entities. Afterwards, a filtering step is conducted that utilizes the PECOS relevance scores along with its predictions to post-process the predictions. Similar to the two-stage approaches, we utilize the PT filtering step to ensure a PT match with the query. We refer to this model as Q2E-PECOS. The label space for Q2E-PECOS is collected from the brand-name2entity dataset (see Section~\ref{subsec:dataset}) as the collection of all unique and valid brand entities. We have 61678 such brand entities as our classes.

An important point to note is that the majority of search queries do not contain a brand intent. The input to our brand entity linking model is all search queries, out of which branded queries are only a subset. To address this, we add a \textit{NO\_ENTITY} class to the brand entity space to allow our model to handle non-branded queries. In other words, we expect the Q2E-PECOS to predict \textit{NO\_ENTITY} label for non-branded queries.

\textbf{Model Training and Inference}: Q2E-PECOS model directly maps queries to brand entities that is different from two-stage framework in Section~\ref{sec_two_stage_framework}. %Q2E-PECOS can use both (\textit{brand name, brand entity}) from UBC as well as (\textit{query, brand entity}) and (\textit{query, NO\_ENTITY}) for training instances. 
The training data of Q2E-PECOS is multi-lingual and is explained in section~\ref{sec_training_data} in detail. The training data contains both valid brand entities and dummy entity (\textit{NO\_ENTITY}).
In the inference stage, when multiple classes are predicted for an input query, \textit{NO\_ENTITY} (if predicted) and valid brand entities are ranked according to their relevance scores. A simple way is to choose the highest-scored class on the basis of PT match for the input query, which is what we implement in this approach.

\section{Fusion of the Q2E-PECOS and Extract Lexical Match Prediction}\label{fusion}
\begin{figure}
  \centering
  \includegraphics[width=8cm]{./figures/20241209/brand_entity_fusion3.png}
  %\vspace{-0.3cm}
  \caption{Fusion of one-stage and two-stage into a single solution}
  % \vspace{-0.5cm}
  \label{fig:fusion-solution}
\end{figure}

The proposed Q2E-PECOS model as a one-stage solution is simpler and capable of covering more variety of brand names than extract-string matching based method. However, while Q2E-PECOS allows us to retrieve a wider variety of brands, it shows lower precision compared to extract-string matching based solution. To receive the benefits of both methods, we can run the two methods in parallel independently from each other and fuse their predictions as a post processing step to make the final prediction for a query. When both predictions are available for a query, we select the MetaTS-NER + exact lexical match prediction over Q2E-PECOS to opt for higher precision. The diagram of the fusion solution is shown in Figure~\ref{fig:fusion-solution}.

\section{Experiments}
%\vspace{-2mm}
% In this section, we would start witth explanation of datasets used in this work, followed by the metrics used for model comparisons. Numerical resutls are then reported.
\subsection{Datasets}
\label{subsec:dataset}
% Sreyashi: I am consolidating all dataset-related content in this section. It seems cleaner than mentioning it in the individual model sections.
%\vspace{-2mm}
The datasets for model training and evaluation are pruned from internal data. We explain them in the following sections.

\subsubsection{Training Datasets}\label{sec_training_data}
The training dataset consists of three folds and is explained as the following.

{Brand-name2entity dataset:}
Brand-name2entity dataset is internal dictionary-like dataset with each entry being \textit{brand name, brand entity} pair. The dataset has two user cases: i) It serves as part of the training data where a brand name is considered as a pseudo query. ii) it assists to the strongly-labeled dataset building as would be explained in the following section. As for why we use this as part of training data, we consider it as a form of data augmentation.  Similar augmentation has been explored in the extreme zero-shot text classification~\citep{xiong2022extreme} and large-scale retrieval of the search engine~\citep{tay2022transformer}. 

% In this case, we consider the brand name as a complete query for input to the model. Such technique can be viewed as a form of data augmentation, and has been explored in the extreme zero-shot text classification~\citep{xiong2022extreme} and large-scale retrieval of the search engine~\citep{tay2022transformer}. We construct 711964 such training pairs from the UBC dataset.

% The Append query dataset does not cover all unique brands at Amazon. In order to expose the model to additional brand data, we use the UBC snapshot containing (\textit{brand name, brand entity}) pairs to construct pseudo positive pairs. In this case, we consider the brand name as a complete query for input to the model. Such technique can be viewed as a form of data augmentation, and has been explored in the extreme zero-shot text classification~\citep{xiong2022extreme} and large-scale retrieval of the search engine~\citep{tay2022transformer}. We construct 711964 such training pairs from the UBC dataset.

{Strongly-labeled dataset:}
The srong-labeled dataset is a dataset derived from search queries annotated by human judges internally. The data contains both branded as well as non-branded queries. 
The internal annotations consist of top query attributes like \textit{brand}, \textit{color}, \textit{size} etc. The data set is used for NER model training.
For the purpose of our brand entity linking, we only utilize the \textit{brand} labels and mark all other attributes as $O$ (Other). 
% A brand label in text string form is given to a query if it contains a brand mention in the query, and "other" label is given for queries without brand mention. 
The dataset covers 13 languages: English, Spanish, German, French, Italian, Japanese, Dutch, Portuguese, Polish, Turkish, Swedish, Arabic and Czech. We collect 806972 training pairs from this dataset.
The dataset contains the brand name from the human annotated label. To build mapping from a search query to brand entity, we add an additional step of mapping the brand name to its corresponding brand entity through exact string matching by leveraging brand-name2entity dataset. This gives us the strongly-labeled dataset.
%Note that for our NER-based two-stage approaches, the NER model is trained with the brand name as the supervision. For the end-to-end PECOS model (Q2E-PECOS), we use the mapped brand entities as labels for training.



{Weakly-labeled query data from sampled search engagements:}
Although the brand-name2entity data source is rich in its brand distribution, the brand names alone do not match the query distribution. Since we ultimately wish to identify brand entities for search queries, we sampled and aggregated the historical search traffic to collect a weakly-labeled set of branded queries. We sampled query-product pairs that have a strong association through past search engagement aggregated over a time period. From the sampled data, we also collect the brand names for each of these products. We then check if the brand name is present in the associated query string as a substring. If it is present, we label the query with that brand name. Note that we rely on the strong query-product association to make this labeling and we do not verify these labels through human annotation. We build 1308816 such examples as part of training data.

\subsubsection{Test Dataset}
We use a held-out subset of the strongly-labeled dataset as our gold test dataset. As mentioned previously, the dataset contains queries with their annotated brand names and the associated entities. 
Since we add the corresponding brand entities through exact string matching using the brand name. The mapping process can sometimes lead to more than one brand entity being selected. Since we cannot be sure of the right one, we discard such examples for the purpose of precision and recall measurement. 
There are 28439 queries in total in the test dataset, out of which 24054 queries (84.5\%) are labeled with single brand entity whereas 4385 queries are multi-brand-entity labelled queries.

\subsection{Evaluation Metrics}
Following the typical Recall and Precision definition, we define our metrics as
\begin{small}
\begin{align}
  \text{Recall} &= \frac{\text{Number of queries getting correctly single-brand-entity prediction}}{\text{Number of queries having single-brand-entity label in test data}} \nonumber\\
  \text{Precision} &= \frac{\text{Number of queries getting correctly single-brand-entity prediction}}{\text{Number of queries having single-brand-entity prediction}}. \nonumber
\end{align}
\end{small}
Though there are some queries with multi-brand-entity labels. We believe they are queries with brand name intent which takes up about $15\%$ in our test data. We define a metric named Coverage to include these group of test queries by
\begin{small}
    \begin{align}
  \text{Coverage} = \frac{\text{Number of queries with single-brand-entity prediction}}{\text{Total number of query in test data}} \nonumber
\end{align}
\end{small}

The difference between the recall and coverage metrics is the following: i) recall only counts the correctly predicted single-brand-entity labelled query number whereas coverage includes also wrong single-brand-entity predicted cases as well. ii) As for denominators, recall counts the queries with single-brand-entity labels whereas coverage also counts the none-single-brand-entity labelled queries. We include the coverage metric due to our limitation of test data labeling.

\subsection{Numerical Results}
% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{table}[!ht]
%   \caption{Evaluation of the benchmark and proposed methods}
%   \label{eval-table}
%   \centering
%   \begin{tabular}{l|lll|l|lll|lll}
%      Method & \multicolumn{3}{c}{Training Data} & Fusion? & \multicolumn{3}{c}{TOP9\_AVG} & \multicolumn{3}{c}{TEN\_AVG} \\
%     \hline
%          {}& APPEN & UBC & Catalog Data & coverage & recall & precision & coverage & recall & precision  \\ \hline
%          {$+$ Exact Lexical Match}& Yes & Yes & No & No & 0.5828 & 0.64664 & 0.97218 & 0.70158 & 0.86211 & 0.99153 \\ \hline
%         \multirow{3}{*}{$+$ M2E-PECOS} & No & Yes & No & No & 0.71149 & 0.63876 & 0.88584 & 0.88213 & 0.81193 & 0.90852 \\ \hline
%         & Yes & Yes & No & No & 0.60863 & 0.59579 & 0.9523 & 0.75897 & 0.76526 & 0.9518 \\ \hline
%          & Yes & Yes & No & Yes & 0.67508 & 0.6764 & 0.96054 & 0.83311 & 0.88566 & 0.98729 \\ \hline
%         \multirow{3}{*}{Q2E-PECOS} & No & Yes & No & No & 0.93133 & 0.60962 & 0.65563 & 0.95938 & 0.73698 & 0.76744 \\ \hline
%         & Yes & Yes & No & No & 0.49683 & 0.50559 & 0.94892 & 0.63966 & 0.72404 & 0.97759 \\ \hline
%          & Yes & Yes & No & Yes & 0.70975 & 0.75258 & 0.96131 & 0.80768 & 0.94712 & 0.9892 \\ \hline
%         & Yes & Yes & Yes & No & 0.62495 & 0.62144 & 0.93457 & 0.73348 & 0.78759 & 0.96968 \\ \hline
%         & Yes & Yes & Yes & Yes & 0.75313 & 0.77346 & 0.94926 & 0.85093 & 0.94635 & 0.98548 \\ \hline
%     \end{tabular}
%   \end{table}
\begin{table*}
	\caption{Evaluation of the benchmark and proposed methods. All numbers are in percentage (\%).}
  	\label{eval-table}
   	\centering
	%\resizebox{\columnwidth}{!}{%
	\begin{tabular}{c|ccc|r|rrr|rrr}
      \toprule
     	Method & \multicolumn{3}{c}{Training Data} & Fusion & \multicolumn{3}{c}{Group-1\_AVG} & \multicolumn{3}{c}{Group-2\_AVG} \\
		{}& {\begin{tabular}[x]{@{}c@{}} Brand \\ name2entity \end{tabular}} & {\begin{tabular}[x]{@{}c@{}} Strongly \\ labeled \end{tabular}} & {\begin{tabular}[x]{@{}c@{}} Weakly \\ labeled \end{tabular}} & {}&coverage & recall & precision & coverage & recall & precision  \\
		\midrule
      	{\begin{tabular}[x]{@{}c@{}} NER $+$ Exact \\ Lexical Match\end{tabular}}&
       Yes & Yes & No & No & 58.28 & 64.66 & 97.22 & 70.16 & 86.21 & 99.15 \\
      	\midrule
		\multirow{4}{*}{\begin{tabular}[x]{@{}c@{}} NER $+$ \\ M2E-PECOS\end{tabular}} & Yes & No & No & No & 71.15 & 63.88 & 88.58 & 88.21 & 81.19 & 90.85 \\
            & Yes & Yes & No & No & 60.86 & 59.58 & 95.23 & 75.90 & 76.53 & 95.18 \\
               & Yes & Yes & No & Yes & 67.51 & 67.64 & 96.05 & 83.31 & 88.57 & 98.73 \\
          \midrule
        \multirow{7}{*}{Q2E-PECOS} & Yes & No & No & No & 93.13 & 60.96 & 65.56 & 95.94 & 73.70 & 76.74 \\
        	& Yes & Yes & No & No & 49.68 & 50.56 & 94.89 & 63.97 & 72.40 & 97.76 \\
        	& Yes & Yes & No & Yes & 70.98 & 75.26 & 96.13 & 80.77 & 94.71 & 98.92 \\
        	& Yes & Yes & Yes & No & 62.50 & 62.14 & 93.46 & 73.35 & 78.76 & 96.97 \\
        	& Yes & Yes & Yes & Yes & 75.31 & 77.35 & 94.93 & 85.09 & 94.64 & 98.55 \\
    	\bottomrule
    \end{tabular}
	%}
\end{table*}

We present our experimental results in Table \ref{eval-table}. We present multiple experiment runs depending on the training data and fusion configurations used. Here, the fusion parameter refers to post-processing fusion strategy mentioned in section \ref{fusion}.

We first measure precision, recall and coverage metrics for each online retail store. 
The evaluation is performed over two store clusters, i.e. Group-1 and Group-2. 
% We divide the set of stores worldwide into two groups. The Group-1 stores includes United States (US), United Kingdom (UK), Canada (CA), India (IN), Germany (DE), France (FR), Italy (IT), Spain (ES), and Japan (JP). These are the largest retail stores operating with larger traffic volume. The Group-2 consists of Transitioning, Emerging and New stores and includes Mexico (MX), Brazil (BR), Australia (AU), Turkey(TR), UAE (AE), Netherlands (NL), Saudi Arabia (SA), Singapore (SG), and Sweden (SE). 
The two groups differ greatly in size, evaluation data volume and search query pattern distributions. Hence, we average the metrics for both groups separately.

We first look at the two-stage methods, i.e. MetaTS-NER $+$ Exact Lexical Match and M2E-PECOS. With the fusion technique, it can be seen that semantic matching via PECOS does help to improve the coverage over the exact lexical matching technique. But we see a drop in both recall and precision when moving from exact lexical match to semantic match. While the semantic matching technique can retrieve more brand entities, it does so with a lower precision. When a fusion of the lexical and semantic matching techniques is applied, we see improvement in both coverage and recall, whereas the precision is slightly affected.

%Next, we compare the Q2E-PECOS model to the baseline model Metis-NER $+$ Exact Lexical Match. When only UBC data is used for its training, the coverage is significantly higher while recall and precision get worse. After we include the APPEN data into its training data, we are not only augmenting it training instance, but also including a group of queries that contains no brand names. We tag these queries with the label of none-GCORID-class. This means that we are actually augmenting the output space of brand entities by one, i.e., the none-GCORID entity. (Sreyashi: I don't see this run included in the table).

Next, we compare the Q2E-PECOS model to the baseline model MetaTS-NER $+$ Exact Lexical Match. We see that the training data choice has a significant impact in the performance of the Q2E-PECOS model. This is expected since the end-to-end model requires a lot more data to learn the task. When only using the brand-name2entity and the strongly-labeled datasets, Q2E-PECOS preforms worse than the MetaTS-NER + Exact Lexical Matching model across all metrics. Adding the fusion strategy improves the model performance significantly. This shows that with limited data, both models have complimentary effects. When we add in the weakly-labeled catalog data, we see the Q2E-PECOS model improve over our baselines even without the fusion mechanism. Finally, we get the best performance across all metrics when combining all training data sources and also including fusion mechanism.
%We see that the performance of Q2E-PECOS is now is aligned with exact lexical match method on the three metrics while suffers from some defects. But Q2E-PECOS is an end-to-end solution that is a much simpler and faster. Again, we can also apply the prediction fusion strategy between Metis-NER $+$ Extract Lexical Match and Q2E-PECOS. With fusion enabled, we can improve the coverage and recall significantly meanwhile maintaining the precision at the same high level. We trained another Q2E-PECOS model with further data augmentation of catalog data. As can be seen, the fused performance is even getting better in almost all demissions.


\begin{table}
  \caption{False Alarm Rate (\%) on Non-branded Queries}
  %\vspace{-0.2cm}
  \label{false-alarm}
  \centering
  %\begin{small}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{c|r|r|rr}
    \toprule
    {} & \multirow{2}{*}{\begin{tabular}[x]{@{}c@{}} NER $+$ Exact \\ Lexical Match \end{tabular}} & \multirow{2}{*}{\begin{tabular}[x]{@{}c@{}} NER $+$ \\ M2E-PECOS\end{tabular}} & \multirow{2}{*}{\begin{tabular}[x]{@{}c@{}} Q2E-PECOS without \\ Weakly Labeled Data\end{tabular}} & \multirow{2}{*}{\begin{tabular}[x]{@{}c@{}} Q2E-PECOS with \\ Weakly Labeled Data\end{tabular}} \\
    {} & {} & {} & {} & {} \\
    \midrule
     %{False Alarm} & {0.01177} & {0.03267} & {0.04037} & {0.0655}\\
     {False Alarm} & {1.177} & {3.267} & {4.037} & {6.550}\\
    \bottomrule
  \end{tabular}
  }
  %\vspace{-0.6cm}
  %\end{small}
\end{table}

\textbf{False alarm evaluation:} All metrics shown in Table \ref{eval-table} were measured on the gold test set containing only branded queries. In the real-world e-commerce services, we wish to deploy such a model for all queries and expect the model to be able to identify branded queries and the predict the corresponding brand entity. To study the impact of the model in such a setting, we collected ~85K non-branded queries and observe the predictions by the different models. We define the false alarm rate in this evaluation as the percentage of non-branded queries for which the model incorrectly predicts a brand entity. Table \ref{false-alarm} reports the rates for the four main models. As we can see, the exact lexical match based method receives the lowest false alarm. The M2E-PECOS has a slightly larger false alarm and Q2E-PECOS model further increases the rate by ~3\%. Although we see an increased false alarm rate with our best Q2E-PECOS model, we believe the increase is still acceptable given the overall performance gain of this model along with its more simplified implementation.

\section{Online Results from A/B Test}
%Recognizing brand entities is search queries is a fundamental component of query understanding and identifying the customers shopping intent. Brand-related signals are used is several steps in an e-commerce search engine including matching, ranking, navigation etc. Furthermore, recognizing brands as entities rather than simple strings is also important. What makes this problem challenging is the massive number of brands (internal and external) which are continuously changing.

In e-commerce, identifying the right brand and linking to right entity is crucial in ensuring the relevant products are retrieved and ranked for the customer. Failing to do so can result in a negative shopping experience. 
To assess comparative performance, we conducted A/B testing between our two proposed approaches under real-world conditions.
In the A/B testing, the NER followed with exact lexical match is the control solution while the fusion solution as shown in Figure~\ref{fig:fusion-solution} is the treatment. The fusion solution improves the brand entity recall by +11\% in Group-1 cluster stores and +5.44\% in Group-2 Cluster stores in online experiment. Furthermore,
the A/B test shows the fusion solution increases customer engagement by 0.02\% and
improves immediate contribution profit by 0.03\%. 

% The two-stage solution based on MetaTS-NER and a brand entity matcher was launched into production directly.
% Launching a baseline entity linking solution is required to enable brand entity linking for the query brand signal which will facilitate moving towards recognizing brands as entities instead of strings.
% We then carried out an online A/B test to study the impact of Q2E-PECOS entity linking in a fusion solution as shown in Figure~\ref{fig:fusion-solution}. 
% The A/B test studied the downstream impact of using a new end-to-end DL model for brand entity prediction to increase the coverage of the brand name signal.

%The online test is also to ensure that using the new model to augment brand entity linking does not result in negative customer engagement impact from downstream applications that have a dependency on the brand name signal, for example: ranking, restriction, sponsored brands, softlines etc.


 

\section{Conclusion}
In this work, we proposed a two-stage method and an end-to-end classification approach for brand entity linking for e-commerce search queries. We conduct extensive experiments using various modeling and data augmentation techniques and show improved performance compared to the more commonly used two-stage approaches to this problem. We also explore the feasibility of deploying such a model on real-world search query traffic. Given the result, we show that there is good scope for improvement in exploring end-to-end techniques for attribute extraction tasks. We also built a fusion solution including both the two-stage and one-stage models into one framework. The solution has been valided by an online A/B test.
% While our current work uses a simple post-processing strategy for the extreme multi-class classification model, future work can explore better post processing stratergies.

%In this work, we proposed two alternative solutions for brand entity linking of product search queries. We discussed and experimented different training of the proposed solutions. Model performance was measured through three four metrics and comparison between these models versus a two-stage benchmark model were conducted. According to the results, the proposed solutions are valid methods for product search query brand entity linking. With techniques such as training data augmentation and fusion of prediction, the proposed Q2E-PECOS is a promising solution according to our experiments. Our current work uses pretty simple post-processing strategy for the proposed semantic matching based models, i.e., simply choosing the prediction with highest relevance score among the multiple prediction candidates. It is worthy to look at more sophistic ranking or reranking methods for post-processing multiple brand entity candidates in finalizing model prediction.


\section*{Acknowledgement}

We would like to acknowledge our colleagues who have made concrete support to this project, and helped us to improve product search via this brand entity linking project. 
Thank \textbf{Wei-Cheng Chang}, \textbf{David Saile}, and \textbf{Rahul Goutam} for consistent support and help to remove the project obstacles. Thank \textbf{Gaurav Yengera} for support on the named entity recognition part.
We thank \textbf{David Basken} and \textbf{Ushah Kommi Reddi} for the support of our dataset building. We would also like to thank to \textbf{Marcello Federico} for helping reviewing and polishing the work. 


\bibliographystyle{ACM-Reference-Format}
\bibliography{mybib}

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
