\ifx\XeTeXrevision\undefined
\pdfoutput=1
\fi
\csname @namedef\endcsname{ver@natbib.sty}{9999/12/31}
\newcommand{\bibpunct}[6]{}
\PassOptionsToClass{cleveref}{jmlr}
\documentclass[preprint,12pt]{colt2025} 
\expandafter\let\csname ver@natbib.sty\endcsname\undefined
\let\bibpunct\undefined
\renewcommand{\eminnershape}{\fontfamily{lmr}\slshape\scshape\selectfont}
\csname g@addto@macro\endcsname{\appendix}{\crefalias{section}{appendix}\crefalias{subsection}{appendix}}

\jmlrvolume{}
\jmlryear{}
\jmlrproceedings{}{Preprint:}

\usepackage[centerfigures,commands,citations,authoryear,enumerate,environments]{AVT}
\crefname{equation}{}{}
\bibliography{Adversarial-TS}
\usepackage{thmtools}
\newtheorem{assumption}{Assumption}
\usepackage{framed}
\newenvironment{customleftbar}{\def\FrameCommand{{\color{gray}\vrule width 3pt}\hspace{\parindent-3pt}}\MakeFramed{\advance\hsize-\width \FrameRestore}\setlength{\parindent}{0pt}\slshape}{\endMakeFramed}

\title[An Adversarial Analysis of Thompson Sampling for Full-information Online Learning]{An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces\\[6pt]\normalfont\large\slshape Dedicated to the memory of David Draper}
\usepackage{times}

\author[Terenin and Negrea]{
\Name{Alexander Terenin} \\
\addr{Cornell University}
\AND
\Name{Jeffrey Negrea} \\
\addr{University of Waterloo and the Vector Institute}
}




\begin{document}

\maketitle

\begin{abstract}
We develop an analysis of Thompson sampling for online learning under full feedback---also known as prediction with expert advice---where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling with a certain Gaussian process prior widely-used in the Bayesian optimization literature has a $\mathcal{O}(\beta\sqrt{T\log(1+\lambda)})$ rate against a $\beta$-bounded $\lambda$-Lipschitz adversary.
\end{abstract}

\vspace{0.5\baselineskip}

\begin{keywords}\ignorespaces
Thompson sampling, online learning, prediction with expert advice, follow-the-perturbed-leader, follow-the-regularized-leader, non-discrete action spaces
\end{keywords}


\section{Introduction} 

Making predictions in the face of adversarial feedback is a central problem in learning theory, and is known by a number of names, including online learning, no-regret learning, and prediction with expert advice \cite{cesabianchi2006prediction}.
In this framework, at each time step $t$, the \emph{learner} must decide on which action $x_t \in X$ to take, while, simultaneously and with knowledge of the learner's strategy, an \emph{adversary} chooses a reward function $y_t : X \to \R$ from some space of reward functions $Y$.
The learner then receives a reward of $y_t(x_t)$, and the game is repeated.
The learner's goal is to minimize their expected regret with respect to the single best action in hindsight.

Online learning is of fundamental importance because many other learning problems can be solved by reduction to an online learning oracle.
For example, in algorithmic game theory, coarse correlated equilibria can be computed using \emph{no-regret dynamics} \cite{roughgarden16}, which essentially work by implementing a set of agents that each play online learning algorithms against each other.
This approach extends to stricter equilibrium concepts when modified appropriately.
As a second example, \textcite{foster21,foster22,foster23} have recently shown that various variants of \emph{decision-making with structured observations}, a generalization of multi-armed bandits where pulling an arm can additionally provide information about other arms, and which includes certain forms of reinforcement learning, can be efficiently solved by reduction to an online regression oracle---a special case of an online learning~algorithm.

The aforementioned reductions, in general, involve a specialized action space $X$.
Online learning, however, is best-understood when $X$ is a finite set, with exponential weights variants the most-common approach.
Motivated by a desire for principled equilibrium-computation approaches in non-discrete spaces of interest in domains like robotics and economics, in this work, we ask:

\begin{customleftbar}
Given a general, potentially-uncountable action space $X$, and a space $Y$ of reward functions with a given degree of regularity, what practically-implementable algorithm should the learner~use?
\end{customleftbar}

In this work, we take an initial step towards a framework which can answer this question.
Our starting point will be the work of \textcite{gravin2016towards}, who derived the Nash equilibrium of infinite-horizon discounted online learning with three experts, namely $X = \{1,2,3\}$ and $Y = \{y \in \ell^\infty(X) : \norm{y}_\infty \leq 1\}$---and discovered the learner's optimal strategy to be a form of Thompson sampling: the learner should choose arms with probability proportional to them being best-in-hindsight under a certain optimal Bayesian prior distribution over possible adversaries.
In this work, we:
\1 Develop a framework for deriving regret bounds for Thompson sampling algorithms akin to those of \textcite{gravin2016towards}, but which are finite-horizon and allow general priors over adversaries. We do so by showing that regret decomposes into the \emph{prior regret} the learner expects to incur, and \emph{excess regret} arising from the true adversary not matching the prior.
\2 Provide a recipe for constructing strong priors using regret lower bounds and Gaussian universality, generalizing the maximin duality view of \textcite{gravin2016towards} to the minimax regret setting. This gives an ansatz for our key question of what algorithm the learner should use, and can be interpreted as a recipe for transforming regret lower bounds into upper bounds.
\3 Develop an approach for bounding prior and excess regret, using stochastic process expected supremum bounds for the former, and showing that the latter is bounded by certain Bregman divergences, which are dual to those appearing in the \emph{local norm} analysis of \textcite{orabona2019modern},~and which mirror certain terms in the \emph{stochastic smoothing} formalism of \textcite{abernethy2016perturbation}.
\4 Show, in the classical finite-expert setting, that the aforementioned recipe works, giving optimal minimax rates. In doing so, we show that concrete terms arising from our analysis can be bounded using techniques similar to those of \textcite{abernethy2016perturbation}, leading to the same regret, but without learning-rate-type restrictions and with a different interpretation.
\5 Show, in the setting $X = [0,1]$ with uncountably-many experts and $Y$ the set of $\beta$-bounded $\lambda$-Lipschitz functions, that our ansatz leads to practically-implementable algorithm akin to Bayesian optimization variants in use \cite{frazier18,garnett23}, and achieves a~regret~of
\[
\c{O}\del{\beta \sqrt{T\log(1+\lambda)}}
.
\] 
\0 
In doing so, our work constitutes a proof-of-concept that demonstrates algorithms generalizing those of \textcite{gravin2016towards}, paired with an analysis that generalizes the approaches of \textcite{abernethy2016perturbation,orabona2019modern}, can work in non-discrete action spaces---thus, we view it as a first step towards developing a general theory of Thompson sampling for non-discrete online learning. 


Thompson sampling, named for the seminal work of \textcite{thompson1933likelihood,thompson1935theory}, is a special case of the \emph{follow-the-perturbed-leader} framework \cite{kalai2005efficient}, with perturbations given by the posterior variability of the best action.
The analysis of Thompson sampling in the stochastic bandit setting---for instance, following \textcite{russo2016information}---leverages the ability of the algorithm to borrow information across time to ensure posterior concentration, but not across arms.
In the adversarial setting, our work shows this is addressed by having the learner place their prior over the \emph{set of adversary's strategies}, rather than on mean rewards.
Posterior sampling has also been explored in general-feedback settings, for instance by \textcite{xu2023bayesian}, whose approach gives a variant of classical exponential weight algorithms in the full feedback setting we~study.

The algorithms we consider are can be viewed as a form of random rollouts.
From this view, our results are closely-related to the \emph{relax-and-randomize} perspective of \textcite{rakhlin2012relax}, but do not rely on Rademacher-complexity-theoretic assumptions on the perturbation distribution.
Similarly, our results can be seen as a step towards a constructive analog of the theory of \textcite{sridharan2010convex}, which derives fundamental complexity of more-general games where the learner's strategies lie in spaces generalizing the space of probability measures we consider.

In spite of its completely-different motivation, the terms in our framework mirror those arising in the \emph{stochastic smoothing} approach of \textcite{abernethy2016perturbation} for analyzing \emph{follow-the-perturbed leader} algorithms---and, are also dual, in the convex conjugate sense, to those appearing in the \emph{local norm} analysis of \textcite{orabona2019modern} for more-general \emph{follow-the-regularized-leader} algorithms.
Our analysis therefore connects these works with the game-theoretic perspective of \textcite{gravin2016towards}, and in doing so reveals that learning rate schedule restrictions, which are used in both approaches---and exclude Thompson-sampling-like algorithms which correspond to increasing sequences of learning rates---are not necessary for either analysis to work.

The algorithms we obtain from our \textcite{gravin2016towards}-inspired ansatz are practical by construction, and can be implemented numerically on infinite state spaces in a manner similar to standard implementations of Thompson sampling used in Bayesian optimization \cite{wilson20,wilson21}.
In addition to our motivations arising from computational game theory, we therefore also view our work as a first step towards integrating ideas from adversarial learning into Bayesian optimization.

\section{Problem Formulation}
\label{sec:preliminaries}
The variant of the \emph{online learning game} we analyze, first considered by \textcite{vovk1990aggregating,littlestone1994weighted} following work of \textcite{cover1966behavior}, is called \emph{prediction with expert advice}, and refers to a sequential two-player zero-sum game between a \emph{learner} and \emph{adversary}, both of which are allowed to randomize. 
At each time $t\in[T] = \cbr{1,..,T}$, the game proceeds as follows: 
\1 The learner picks an action $x_t\in X$, where $X$ is called the \emph{state space} or \emph{space of experts}.
\2 The adversary responds with $y_t : X \to \R$, assumed to lie in some space $Y$ of \emph{reward functions}.
\0
The game's expected value, from the learner's perspective at time $T$, is given by the \emph{expected regret}
\[
R(p,q) &= \E_{\substack{x_t\~p_t\\y_t\~q_t}} \sup_{x\in X} \sum_{t=1}^T y_t(x) - \sum_{t=1}^T y_t(x_t)
\]
where $p = (p_1,..,p_t)$ is the learner's strategy, and $q = (q_1,..,q_t)$ is the adversary's strategy, both of which may be chosen adaptively.
For a deterministic adversary, we write $R(p,y)$.
Using this formalism, the classical setting corresponds to taking $X = [N]$ to be a finite set, and $Y = \{y\in\ell^\infty(X;\R) : \norm{y}_\infty \leq 1\}$ to be an $\ell^\infty$ unit ball. 
In more general settings, to ensure the game's values are well-defined, we assume $X$ is second-countable compact Hausdorff, and $Y\subseteq C(X;\R)$ is a convex subset of continuous real-valued functions on $X$. 
For further details see~\Cref{apdx:technical}.

In this work, we seek algorithms whose regret rate, as a function of the time horizon $T$ and a suitable notion of the complexity of the adversary's decision space $Y$, matches the \emph{minimax regret} 
\[
\min_p \max_q R(p,q)
.
\]
For finite expert classes, the minimax expected regret is $\c{O}(\sqrt{T\log N})$. 
In this setting, the most well-known strategy achieving the minimax rate is the exponential weights algorithm \cite{littlestone1994weighted,vovk1990aggregating}, which processes the historical rewards in order to explicitly compute a mixed strategy, represented by a probability measure on experts.

To generalize this to infinite expert sets, one approach is to represent the learner's actions by a density with respect to some prior \cite{alquier21a,negrea2021minimax}: this typically yields policies for which sampling is intractable, with bounds on comparator-dependent regret in terms of divergence from the prior.
The other main approach focuses on proving minimax rates by reduction to the finite-expert setting via sequential covers \cite{rakhlin2015online}. 
This establishes information-theoretic limits of sequential prediction using metric entropy variants, but does not directly lead to practical algorithms because the cover sizes needed make computation infeasible.

Another class of algorithms known to achieve the minimax rate of expected regret in the finite-expert setting are follow-the-perturbed-leader strategies, such as \textcite{kalai2005efficient,devroye2013prediction,vanerven14,abernethy2016perturbation}. 
These operate implicitly: rather than an explicit formula to compute the probabilities of a mixed strategy, a sampling procedure is defined directly. 
We view such approaches as a promising avenue for constructing practical algorithms on infinite-expert sets which cannot efficiently be discretized: to achieve this, a key challenge is to understand how to construct good perturbation distributions for spaces of interest.


\section{Thompson Sampling for Adversarial Full-information Online Learning}
\label{sec:general-results}

In this work, we develop algorithms which are a form of \emph{Thompson sampling}---a Bayesian approach to decision-making under uncertainty widely-used in the stochastic bandit literature, which works by sampling each action according to the posterior probability it is optimal.
At first, one might not expect a Bayesian approach to make any sense for online learning: unlike in statistical learning, the reward functions $y_t$ in online learning vary arbitrarily at each round, so there is no well-specified likelihood one can use to build a Bayesian model.
Moreover, the most-common online learning algorithm---namely, exponential weights---uses an update rule which resembles, but does not correspond, to Bayesian learning: it includes a learning rate term, which makes it trust the data~less.

Despite this, \textcite{gravin2016towards} show that the exact Nash equilibrium of discounted online learning with three experts admits a Bayesian interpretation: the learner's algorithm is a form of Thompson sampling.
At each round, the learner plays each expert according to its conditional probability of being the best eventual expert, with the distribution over future experts given by the optimal adversary of the game's maximin dual.
The proof proceeds by calculating the maximin-optimal adversary explicitly, then using its form to derive the learner's strategy.

Since the maximin optimal adversary's actions over different rounds are not independent, such adversaries are intractable in more general settings.
However, the functional form of the learner's strategy motivates the following question: \emph{does Thompson sampling, with respect to a prior given by a strong adversary, admit a minimax regret bound?}
To study this, we begin by defining the~strategy.

\begin{restatable}[Thompson Sampling]{definition}{DefTS}
\label{def:ts}
Let $q^{(\gamma)}$ be a prior over the adversary's strategy.
Define the \emph{Thompson sampling} strategy for the learner $p_t$ by 
\[
\label{eqn:ts}
x_t &= \argmax_{x\in X} \sum_{\tau=1}^{t-1} y_\tau(x) + \sum_{\tau=t}^T \gamma_\tau(x)
&
(\gamma_t,..,\gamma_T) &\~ q^{(\gamma)} \given \gamma_1 = y_1, .., \gamma_{t-1} = y_{t-1}
\]
with ties broken according to a given tie-breaking rule.
\end{restatable}

To further understand \Cref{def:ts}, note that the conditional distribution of $\sum_{\tau=1}^T \gamma_\tau$ given $\gamma_\tau = y_\tau$, $\tau \in [t]$, is precisely the distribution of the sum inside the maximizer in \Cref{eqn:ts}.
This strategy can be implemented in practice, even in non-discrete settings: it is a special case of the \emph{follow-the-perturbed-leader (FTPL)} framework of \textcite{kalai2005efficient}, where the perturbation distribution and learning rate are set according to the prior over the adversary's future actions.
\Cref{def:ts} is therefore practical by construction, up to an optimization oracle.
To distinguish $q^{(\gamma)}$ from the true adversary, throughout this work we refer to it as the \emph{virtual adversary} under the prior.
We now study Thompson sampling's regret, and aim to develop principled choices of priors in adversarial settings.

\subsection{A Bayesian-type Decomposition of Minimax Regret: from lower to upper bounds}
\label{sec:excess-regret}

The Thompson sampling algorithm given in \Cref{def:ts} admits a natural Bayesian interpretation.
At the same time, it is an instance of the FTPL framework with a particular sequence of perturbations, and the regret analysis of FTPL is well-understood in the setting of finite expert classes.
Motivated by the usefulness of viewing statistical learning from Bayesian and minimax viewpoints simultaneously, we develop a Bayesian way of analyzing minimax regret.
Let $\Gamma_t$ be the FTRL regularizer of Thompson sampling: defined, following \textcite[Chapter 30.5]{lattimore20}, in terms of its convex conjugate $\Gamma^*_t(f) = \E \sup_{x\in X}\del{ f(x) + \gamma_{t:T}(x)}$, where we denote sums by $\gamma_{1:T} = \sum_{t=1}^T \gamma_t$ with the convention $\gamma_{T+1:T} = 0$. 
We also write $\pair{y_t}{p_t} = \E_{x\~p_t} y_t(x)$.
We now state our first result.
\begin{restatable}{proposition}{PropRegretDecomposition}
\label{prop:excess_regret}
We have
\[
R(p,y) = R(p,q^{(\gamma)}) + \ubr{\sum_{t=1}^T \Gamma^*_{t+1}(y_{1:t}) - \Gamma^*_t(y_{1:t-1}) - \pair{y_t}{p_t} + \E\pair[0]{\gamma_t}{p^{(\gamma)}_t}}{E_{q^{(\gamma)}}(p,y)}
\]
where $p^{(\gamma)}_t$ is a copy of the learner's strategy applied to simulated data $\gamma_{1:t-1}$ drawn from the prior $q^{(\gamma)}$ instead of the $y_{1:t-1}$. 
We call $ R(p,q^{(\gamma)})$ the \emph{prior regret} and $E_{q^{(\gamma)}}$ the \emph{excess regret}.
\end{restatable}


\begin{proof}
All proofs are in \Cref{apdx:technical}. 
A sketch is as follows: the idea is to apply a standard telescoping argument, similar to the more general follow-the-regularized-leader (FTRL) regret decomposition given by \textcite[Lemma 7.1]{orabona2019modern}, with two key modifications: we replace the FTRL regularizer with its convex conjugate, and additionally include the $\gamma_t$-terms given by the virtual adversary induced by the prior.
The claim follows by noting that the telescoping-sum boundary term, $\Gamma^*_t(0)$, is by definition the expected best-in-hindsight value under the virtual adversary, and~therefore its difference with $\pair[0]{\gamma_t}{p^{(\gamma)}_t}$ is equal to the expected regret incurred against the strategy $q^{(\gamma)}$.~
\end{proof}

The advantage of this decomposition is that (a) it is completely general, requiring no assumptions about the setting, virtual adversary distributions, or learning rates, and (b) even in non-discrete settings, it is not difficult to choose $q^{(\gamma)}$ in a way that makes $R(p,q^{(\gamma)})$ straightforward to upper-bound.
To see this, we now explore consequences of this decomposition: to do so, we introduce a criterion central to the work of \textcite{gravin2016towards} that a strong virtual adversary should satisfy.

\begin{restatable}{definition}{DefEqualizing}
We say $q^{(\gamma)}$ is \emph{equalizing} if, for every $t$, the conditional expectation of $\gamma_t(x)$ given the history up to time $t-1$ is almost surely constant in $x$, and call it \emph{centered} if it is equal to~zero.
\end{restatable}

\textcite{gravin2016towards} refer to equalizing adversaries as \emph{balanced}: we instead use the more-standard term from game theory---see \textcite[Chapter 11]{straffin2010game}.
To ease notation in what follows, we prove that if $q^{(\gamma)}$ is independent across time, we can take $q^{(\gamma)}$ to be centered without loss~of~generality.

\begin{restatable}{lemma}{LemCentering}
Given an online learning game and an equalizing adversary $q^{(\gamma)}$ supported on $Y$ which is independent across time, there exists an equivalent online learning game with centered equalizing adversary $q^{(\gamma')}$ supported on $Y'$.
\end{restatable}

If the adversary plays an equalizing strategy, the learner's expected regret does not depend on their choice of algorithm $p_t$.
Thus, the existence of an equalizing adversary $q^{(\gamma)}$ implies a regret lower bound---an observation we now formalize.

\begin{restatable}{proposition}{PropLowerBound}
\label{prop:lower_bound}
Let $q^{(\gamma)}$ be equalizing, centered, and supported on at most $Y$. Then 
\[
\label{eqn:lower_bound}
\E \sup_{x\in X}\sum_{t=1}^T \gamma_t(x) = R(\.,q^{(\gamma)}) \leq \min_p \max_q R(p,q)
.
\]
\end{restatable}
\noindent
This statement merits two observations:
\1 In choosing a prior for Thompson sampling, we should define a \emph{strong} adversary to be one which is equalizing, and for which the inequality in \Cref{eqn:lower_bound} is tight up to a constant.
\2 For a strong virtual adversary in this sense, if $E_{q^{(\gamma)}}(p,y)$ is of the same order as $R(p,q^{(\gamma)})$ for all $y$, then Thompson sampling achieves the minimax rate.
\0 
\Cref{prop:excess_regret,prop:lower_bound} therefore provide an ansatz for defining good Thompson sampling priors, and a blueprint for transforming regret lower bounds into upper bounds.
To verify that such a blueprint can work in principle, consider the discrete setting $X = [N]$ and $Y = \ell^\infty(X)$.
Taking $q^{(\gamma)}$ to consist of IID Rademacher random variables for each expert leads to $R(\.,q^{(\gamma)}) = \Theta(\sqrt{T\log N})$ by standard arguments.
Thus, if one can prove that $E_{q^{(\gamma)}}(p,y) \leq \c{O}(\sqrt{T\log N})$, the aforementioned lower bound transforms into a minimax-rate upper bound.

In both discrete and non-discrete settings, tight upper bounds on $R(\.,q^{(\gamma)})$ can be proven by chaining.
On the other hand, proving lower bounds on suprema of general random processes is more difficult.
For bounded $Y$, in most cases we can expect $\frac{1}{\sqrt{T}}\gamma_{1:T}$ to converge to a Gaussian process as $T\to\infty$.
Lower bounds on suprema of Gaussian processes, in turn, can in general be derived using Fernique--Sudakov minorization.
In cases where one expects a Gaussian adversary to have an expected supremum of the same order as a strong equalizing adversary, it thus also makes sense to consider Thompson sampling with $q^{(\gamma)}$ given by the aforementioned Gaussian adversary: our examples of \Cref{sec:finite-dimensions,sec:bounded-Lipschitz} will consist of such algorithms.

\subsection{A Bregman Divergence Bound on Excess Regret}

In the preceding section, we developed a general decomposition of Thompson sampling's minimax regret.
The terms in this decomposition can be interpreted as (i) the regret the learner expects to incur under the prior, and (ii) an excess regret term, which quantifies how much worse---or better---the learner will do when faced against the true adversary.
Bounding term (i) is straightforward, so we now focus on term (ii).
Taking inspiration from the finite-dimensional setting where FTPL---and hence Thompson sampling---can be understood as FTRL with regularizer sequence $\Gamma_t$, for which the respective Fenchel conjugates are the functions $\Gamma^*_t$ defined previously, we now prove that the excess regret can be bounded term-by-term by Bregman divergences $D_{\Gamma^*_t}$ defined with respect to $\Gamma^*_t$.

\begin{restatable}{proposition}{PropBregmanBound}
Assume $q^{(\gamma)}$ is equalizing, independent across time, and ties almost never occur.~Then
\[
E_{q^{(\gamma)}}(p,y) \leq \sum_{t=1}^T D_{\Gamma^*_t}(y_{1:t} \from y_{1:t-1})
.
\]
\end{restatable}

\begin{proof}
We provide a sketch, with a full proof deferred to \Cref{apdx:technical}.
The idea is to (a) add-subtract $\pair{y_t - \gamma_t}{p_{t+1}}$, (b) use the tower rule to write $\Gamma^*_t(\.) = \E_{\gamma_t\~q^{(\gamma)}_t} \Gamma^*_{t+1}(\. + \gamma_t)$ to ensure $\Gamma^*$-functions for the same summand have matching time indices, (c) use Bregman duality to rewrite the resulting terms as a Bregman divergence in primal form (up to technicalities), then (d) apply Young's inequality for the convex function $p \mapsto D_{\Gamma_{t+1}}(p \from p_{y_{1:t-1} + \gamma_t})$, thereby canceling out the primal Bregman divergence, which is otherwise difficult to understand because $\Gamma_t$ is only defined implicitly.
\end{proof}

The advantage of this bound is that it is completely general, requiring no assumptions about the setting, virtual adversaries (except for time-independence and not allowing ties), or learning rates.
The disadvantage is that it is non-negative, whereas the excess regret can be negative for a sequence of reward functions $y_t$ which is not worst-case.
We believe it might be possible~to~relax~the assumptions further, at the cost of a more-involved analysis: we omit this, given our aim of understanding non-discrete settings, and because the virtual adversaries we invoke satisfy our~assumption.

One can interpret the resulting Bregman divergences probabilistically by explicitly writing out the $\Gamma^*_t$-terms.
Introducing the notation $x^*_f = \argmax_{x\in X} f(x)$, with ties broken according to the tie-breaking rule chosen as part of Thompson sampling's definition, this gives
\[
\sum_{t=1}^T D_{\Gamma^*_t}(y_{1:t} \from y_{1:t-1}) = \E \sum_{t=1}^T \del{(y_{1:t} + \gamma_{t:T})(x^*_{y_{1:t} + \gamma_{t:T}}) - (y_{1:t} + \gamma_{t:T})(x^*_{y_{1:t-1} + \gamma_{t:T}})}
\]
where each summand is the difference between the posterior mean total reward of the random best expert, and posterior mean total reward of the best expert if $y_t$ were replaced with $0$---this is analogous to the comparison between the be-the-regularized-leader and follow-the-regularized leader interpretations of FTRL's regret bounds.
This expression should be contrasted with the excess regret itself, which can analogously be written
\[
E_{q^{(\gamma)}}(p,y) = \E \sum_{t=1}^T \del{(y_{1:t} + \gamma_{t+1:T})(x^*_{y_{1:t} + \gamma_{t+1:T}}) - (y_{1:t} + \gamma_{t:T})(x^*_{y_{1:t-1} + \gamma_{t:T}})}
\]
and whose interpretation is similar, but where the function being evaluated in the first term no longer matches that of the second term.
This seemingly-subtle difference, from a probabilistic view, has significant consequences from a convex-analytic one: using the Lagrange form of Taylor's Theorem, assuming it applies,\footnote{This derivative, understood as a positive distribution, always exists by convexity, though it may fail to be bilinear, continuous, or function-valued, in which case one needs to instead work with the integral form of Taylor's~Theorem.} one can write the resulting Bregman divergences as
\[
D_{\Gamma^*_t}(y_{1:t} \from y_{1:t-1}) = \frac{1}{2} \p^2_{y_t} \Gamma^*_t(y_{1:t-1} + \tl{y})
\]
where $\tl{y}$ lies on the line segment $[0,y_t]$, and $\p^2_{y_t}$ is the second Gâteaux derivative in the direction $y_t$.
This expression is similar to certain terms arising in the local norm analysis of \textcite{orabona2019modern}, with one key difference: our Hessians are defined with respect to the convex conjugate $\Gamma^*_t$ instead of the regularizer $\Gamma_t$.
To get an intuitive sense for the effect of this difference, consider what would occur for $\Gamma(p) = D_{\f{KL}}(p \from p_0)$.
Then, for most signed measures, the first Gâteaux derivative $\p_u \Gamma(p)$ is not always finite-valued, and one must restrict to signed measures with explicit density bounds.
In contrast, note that $\Gamma^*(f) = \log \E_{x\~p_0} \exp f(x)$, for which $\p_v \Gamma^*(f)$ is finite-valued for all bounded measurable $f,v$.
In practice, therefore, we expect Hessians of $\Gamma^*_t$ to be easier to work~with.


\section{Examples}

In the preceding section, we established a general framework for reasoning about the minimax regret of Thompson sampling, with a prior given by a general virtual adversary $q^{(\gamma)}$.
We now use this framework to analyze online learning in two settings: (a) the classical finite-expert setting, in order to verify that our analysis of Thompson sampling obtains the correct rates, and (b) a setting with uncountably many experts defined on a given interval, with an adversary restricted to bounded Lipschitz functions. 
For the latter setting, to our knowledge, no follow-the-perturbed-leader-like algorithm with a non-vacuous rate is known.

\subsection{Finitely Many Experts}
\label{sec:finite-dimensions}

We now apply our theory to the classical finite-expert setting, in order to verify that it produces a result which at least matches established theory.
Let $X = [N]$ and $Y = [-1,1]^N$.
By standard lower-bound theory \cite{cesabianchi2006prediction}, the equalizing adversary which at each time $t$ plays independent Rademacher random variables in each coordinate leads to a rate of $\Omega(\sqrt{T\log N})$. Since known algorithms---such as exponential weights and various FTPL variants---achieve this rate, it is the minimax rate, and therefore fits our criterion for a strong adversary.

Following \Cref{sec:excess-regret}, recall that we are not required to choose our virtual adversary to be a strong adversary supported on $Y$: we can instead choose a different, analytically-simpler virtual adversary, but should pick one which is also equalizing and has the same lower bound.
The Rademacher adversary achieves the same rate as an adversary which, at each time $t$, plays IID Gaussians with variance $\sigma^2$ in each coordinate---in fact, this comparison is used in the proof that the Rademacher adversary achieves the lower bound above.
We choose this to be our virtual adversary, and denote it by $q^{(\gamma)}$.
Our task is to verify that this virtual adversary leads to the minimax rate of regret.

We first analyze the prior regret term.
Since $\frac{1}{T} \sum_{t=1}^T \gamma_t \~[N](0, \sigma^2 I_N)$, using the standard maximal inequality for equal-variance Gaussians, we obtain
\[
R(\., q^{(\gamma)}) = \E \max_{x\in X}\sum_{t=1}^T \gamma_t(x) \leq \sigma\sqrt{2 T\log N}.
\]
which is sharp up to small constants.
Next, we focus on the excess regret. 
For $\Gamma^*_t$ defined with respect to a multivariate Gaussian virtual adversary, one can explicitly calculate the Hessian matrix which occurs in the Taylor form of the Bregman divergence. 
This calculation occurs as a step in the stochastic-smoothing-based approach to \textcite{abernethy2016perturbation}'s analysis of FTPL, and in other settings \cite{bhatnagar2007adaptive,bertsekas1973stochastic,nesterov2017random}. 
We now state the result.

\begin{lemma}[{\textcite[Lemma 7]{abernethy14}}]\label{lem:hessian-calculation}
If $\gamma_t\~[N](0,K)$ are IID across times $t\in[T]$, then
\[
\grad^2 \Gamma^*_t(f) = \frac{1}{T-t+1}\E \1_{i^*_{f + \gamma_{t:T}}} \gamma_{t:T}^T K^{-1}. 
\]
where $\displaystyle i^*_{f + \gamma_{t:T}} = \argmax_{x\in X} \del{f(x) + \gamma_{t:T}(x)}$, and $\gamma_{t:T}$ is reinterpreted as a random vector in $\R^N$.
\end{lemma}
Using this, the remaining argument is essentially a re-run of the proof of Lemma 1.8 of \textcite{abernethy2016perturbation}.
To contrast this with the non-discrete setting examined in the sequel, we reproduce the argument here.
Writing the respective Bregman divergence in Taylor form, we obtain
\begingroup
\allowdisplaybreaks
\[
D_{\Gamma^*_t}(y_{1:t} \from y_{1:t-1})
&= \frac{1}{2} y_t^T \grad^2 \Gamma^*_t(y_{1:t-1} + \tilde{y}) y_t = \frac{1}{2\sigma^2(T-t+1)} \E y_t \1_{i^*_{y_{1:t-1} + \tl{y} + \gamma_{t:T}}} \gamma_{t:T}^T y_t
\\
\overset{\t{(i)}}&\leq \frac{1}{2\sigma^2(T-t+1)} \norm{y_t}_\infty^2 \norm{\E \1_{i^*_{y_{1:t-1} + \tl{y} + \gamma_{t:T}}} \gamma_{t:T}^T }_{L(\infty,1)}
\\
\overset{\t{(ii)}}&\leq \frac{1}{\sigma^2(T-t+1)} \norm{y_t}_\infty^2 \tr \E \1_{i^*_{y_{1:t-1} + \tl{y} + \gamma_{t:T}}} \gamma_{t:T}^T 
\\
\overset{\t{(iii)}}&\leq \frac{1}{\sigma^2(T-t+1)}\norm{y_t}_\infty^2 \E \max_{s\in X}\gamma_{t:T}(x) \overset{\t{(iv)}}{\leq} \frac{1}{\sigma}
\sqrt{\frac{2\log N}{(T-t+1)}}\norm{y_t}_\infty^2 .
\]
\endgroup
where (i) is Hölder's inequality and an operator norm inequality, (ii) is the inequality
\[
\norm[1]{\E \1_{i^*_{y_{1:t-1} + \tl{y} + \gamma_{t:T}}} \gamma_{t:T}^T }_{L(\infty,1)} \leq 2 \tr \E \1_{i^*_{y_{1:t-1} + \tl{y} + \gamma_{t:T}}} \gamma_{t:T}^T,
\] 
proven for this specific class of Hessian matrices by \textcite{abernethy2016perturbation} using that the diagonal is positive and the rows sum to zero, (iii) follows by bounding the value of random vector an arbitrary coordinate by the value at its largest coordinate, and (iv) is the same Gaussian maximal inequality as was used for the prior regret term.
We conclude that the excess regret is bounded according to
\[
E_{q^{(\gamma)}}(p,y) \leq \sum_{t=1}^T \sqrt{\frac{2\log N}{(T-t+1)}}\norm{y_t}_\infty^2 = \frac{1}{\sigma}\sqrt{2\log N} \sum_{t=1}^T \frac{1}{\sqrt{t}} \leq \frac{2}{\sigma}\sqrt{2T \log N}
.
\]
Combining the prior and excess regret, and optimizing $\sigma = \frac{1}{\sqrt{2}}$, the overall regret is bounded by
\[
R(p,q)\leq 2\sqrt{T \log N}
\]
for all adversaries $q$, thus obtaining the desired minimax rate.

We conclude that, for $X = [N]$ and $Y = \cbr{y\in \ell^\infty(X) \ : \ \norm{y}_\infty\leq 1}$, the Hessian bounds of \textcite{abernethy2016perturbation} suffice to obtain a minimax rate.
From a technical standpoint, therefore, the main difference between their framework and ours is that we apply Young's inequality to obtain the Bregman divergence, whereas they obtain the same term by other means.
This seemingly-minor difference leads \textcite{abernethy2016perturbation} to impose learning rate restrictions and that the total perturbation is taken according to a scale family with non-decreasing-in-$t$ scale to control the terms which appear in their analysis---which, in particular, exclude the Thompson sampling variant analyzed here.
Though the quantities that appear are ultimately similar, applying Young's inequality therefore buys us more generality at no cost in terms of the obtained regret bound.

The other difference between our frameworks is their interpretation. \textcite{abernethy2016perturbation} focus on the convex-optimization-theoretic concept of stochastic smoothing. 
Our approach centers the game-theoretic concepts of \textcite{gravin2016towards}, albeit in a setting much broader than exact Nash equilibria.
Viewed in this way, our approach offers two chief contributions to the finite-expert setting. 
First, we show that both ways of thinking about online learning lead to the same quantities appearing in the analysis. 
Second, for Thompson-sampling-like FTPL strategies, regret upper bounds for the learner are paired with lower bounds on the minimax regret, provided the perturbations are supported on $Y$---or, with an additional argument, are universal scaling limits of such perturbations.
Thus, adversarial strategies designed to witness lower bounds, or their asymptotic analogs---such as Gaussian perturbations---serve as good ansatzes for selecting the Thompson sampling~prior.


\subsection{Experts in \texorpdfstring{$[0,1]$}{[0,1]} with a Lipschitz Adversary}
\label{sec:bounded-Lipschitz}

We now turn our attention to a prototype setting with uncountably many experts.
Our aim is to develop a proof-of-concept regret analysis which shows that the Bayesian approach to analyzing minimax regret developed in \Cref{sec:excess-regret} can lead to a non-vacuous guarantee, as a first step towards a comprehensive understanding of online learning in general state spaces.
For this, we consider the state space $X = [0,1]$ with an adversary playing reward functions that lie in the subset $Y = \{y\in\f{BL}(X;\R) : \norm{y}_\infty \leq \beta, \abs{y}_{\f{Lip}} \leq \lambda\}$ of the Banach space of bounded Lipschitz functions, where the adversary's allowed functions are $\beta$-bounded and $\lambda$-Lipschitz.

We work with a virtual adversary given by a Gaussian process $q^{(\gamma)}_t = \f{GP}(0,k)$  with kernel $k$.
With access to an optimization oracle, one could implement this form of Thompson sampling in practice---indeed, the resulting algorithm would be similar to well-developed variants of Bayesian optimization \cite{frazier18,garnett23}, where how to perform Thompson sampling numerically is well-understood \cite{wilson20,wilson21}.

What regret guarantees might Thompson sampling have in this setting? 
The general analysis of \Cref{sec:general-results} applies, so our task is to bound the prior and excess regret. 
If we take a centered Gaussian process prior, which is automatically equalizing, then the prior regret reduces to a Gaussian process expected supremum---an important and well-studied quantity in probability theory. 
Tight bounds on suprema of Gaussian processes can be established using chaining arguments \cite{talagrand2005generic}, among other techniques. 
The prior regret can immediately be bounded as follows for any $X$.

\begin{restatable}{lemma}{LemPrior}
For $q^{(\gamma)}_t = \f{GP}(0,k)$, assumed IID over time, let $M_{X,k} = \E\sup_{x\in X}\gamma_t(x)$.
Then
\[
R(\cdot, q^{(\gamma)}) = \E\sup_{x\in X}\sum_{t=1}^T \gamma(x) = \sqrt{T} M_{X,k}
.
\]
\end{restatable}

Returning to the setting where $X=[0,1]$, we study the excess regret, with the aim of also controlling it by $M_{X,k}$.
We must first pick a suitable Gaussian process prior based upon which we construct the virtual adversary. 
The natural choice, when the adversary plays bounded Lipschitz functions, is to take $k$ to be Matérn-1/2, also known as the exponential or stationary Ornstein--Uhlenbeck kernel.
In the same way a Brownian motion arises as a scaling limit of random walks with discrete steps of $\pm 1$---which correspond to our Lipschitz constraint---due to the connection between Brownian motion and the Ornstein--Uhlenbeck process, one can expect this covariance to arise as a scaling limit of a random walk which includes a suitable drift to ensure it is shift-invariant.
For this choice, with variance $\sigma^2$ and length scale $\kappa$, the covariance kernel is $k(x_1,x_2) = \sigma^2 \exp\del{-\frac{\abs{x_1-x_2}}{\kappa}}$. 
By \Cref{lem:ou-max-ineq} of \Cref{apdx:technical}, for this family of kernels on $X=[0,1]$ we have
\[
M_{X,k} \leq C \sigma \sqrt{\log\del{1+\frac{1}{\kappa}}}
\]
where $C$ is a universal constant.
In the aforementioned random-walk scaling limit, the asymptotic variance is $\Theta(\beta^2)$ and the asymptotic length scale is $\Theta(\lambda^{-1})$, suggesting a lower bound on the minimax regret of $\Omega(\beta \sqrt{\log(1+\lambda)})$.
In the interest of brevity, we omit a rigorous derivation of this lower bound, and instead view it a reasonable motivation for our choice of prior.

The next step involves passing to a cover in order to calculate the Hessian terms that arise.
Covering-based techniques are common in regret analyses---for instance, by \textcite{rakhlin2015online}---with a key difference being that in our case the cover is not used by the algorithm---rather, it is purely a technical tool appearing in the analysis.
We pass to the cover as follows.

\begin{restatable}{lemma}{LemCover}
\label{lem:cover}
Let $k$ be Matérn-1/2 and let $X_h \subseteq X$ be a cover with radius $h$.
Then
\[
D_{\Gamma^*_t}(y_{1:t} \from y_{1:t-1}) & \leq  D_{\Gamma_{h,t}^*}(y_{h,1:t} \from y_{h,1:t-1}) + \c{O}\del{\sqrt{h\log\del{\frac{1}{h}}}}
\]
where $y_{h,1:t} = y_{1:t}(X_h)$, and $\displaystyle\Gamma^*_{h,t}(f) = \E \max_{x\in X_h} \del{f(x) + \gamma_{t:T}(x)}$.
\end{restatable}

Using this, we regain the ability to apply \Cref{lem:hessian-calculation} without the need to make technical sense of what the infinite-dimensional analog of the respective Hessian actually is, and avoiding the need to handle regularity questions---at least for the present setting.
Rewriting the Bregman divergence on the cover with the Lagrange form of Taylor's Theorem, we get
\[
D_{\Gamma_{h,t}^*}(y_{h,1:t} \from y_{h,1:t-1}) = \frac{1}{2(T-t+1)}\E y_{h,t}^T \1_{i^*_{y_{h,1:t-1} + \tl{y}_h + \gamma_{h,t:T}}} \gamma_{h,t:T}^T K_h^{-1} y_{h,t} 
\]
where $K_h$ is the kernel matrix, defined by $(K_h)_{ij} = k(x_i, x_j)$, for all pairs of points ($x_i, x_j)$ on the cover.
We now cannot apply the argument of \textcite{abernethy2016perturbation} directly, due to the presence of aforementioned kernel matrix inverse: the simplest approach would be to apply minimum eigenvalue bounds from the kernel interpolation literature \cite{wendland04,schaback11} and then optimize the choice of cover, but these lead, at best, to a vacuous rate of $\c{O}(T)$.

To overcome this, the key idea will be to keep the terms $K^{-1}_h y_{h,t}$ together, as they encode how our choice of kernel used to define the prior and implement the Thompson sampling algorithm interacts with the assumed bounded Lipschitz regularity of the adversary.
To analyze this, we choose the cover $X_h$ to consist of evenly-spaced points.
With this choice, the matrix $K_h$ is a Kac--Murdock--Szegő matrix, whose inverse has an explicit form---it essentially computes finite difference quotients, which one can use to obtain the following.

\begin{restatable}{lemma}{LemKernelMatrixInvNorm}
\label{lem:kernel_matrix_inv_norm}
For any evenly-spaced set of points $X_h \subseteq [0,1]$, where $0<h<1$ is the spacing, and any bounded Lipschitz $y: X\to \R$, letting $y_h = y(X_h)$, we have
\[
\norm{K^{-1}_h y_h}_\infty \leq \frac{1}{\sigma^2} \norm{y_h}_\infty + \frac{\kappa}{\sigma^2} \abs{y_h}_{\f{Lip}} = \frac{1}{\sigma^2}\max_{x\in X_h} \abs{y_h(x)} + \frac{\kappa }{\sigma^2}\max_{x,x' \in X_h} \frac{|y_h(x) - y_h(x')|}{|x-x'|}
.
\]
\end{restatable}

We now generalize the argument of \Cref{sec:finite-dimensions}. 
For the Bregman divergence on the~cover,~write 
\[
&D_{\Gamma_{h,t}^*}(y_{h,1:t} \from y_{h,1:t-1}) = \frac{1}{2(T-t+1)} \E y_{h,t} \1_{i^*_{y_{h,1:t-1} + \tl{y}_h + \gamma_{h,t:T}}} \gamma_{h,t:T}^T K^{-1}_h y_{h,t}
\\
&\quad\leq \frac{1}{2(T-t+1)} \norm{y_{h,t}}_\infty \norm{K^{-1}_h y_{h,t}}_\infty \norm{\E \1_{i^*_{y_{h,1:t-1} + \tl{y}_h + \gamma_{h,t:T}}} \gamma_{h,t:T}^T }_{L(\infty,1)}
\\
&\quad\leq \frac{1}{(T-t+1)}\norm{y_{h,t}}_\infty \del{\frac{1}{\sigma^2}\norm{y_{h,t}}_\infty + \frac{\kappa}{\sigma^2} \abs{y_{h,t}}_{\f{Lip}}} \E \max_{x\in X_h}\gamma_{t:T}(x) \\
& \quad\leq     \frac{C  \E \max_{x\in X_h}\gamma_{t:T}(x)}{\sqrt{T-t+1}}\del{\frac{1}{\sigma^2}\norm{y_{h,t}}_\infty + \frac{\kappa}{\sigma^2} \abs{y_{h,t}}_{\f{Lip}}}\norm{y_{h,t}}_\infty. 
\]
where the steps are similar to before---but with the key modification that the $K^{-1}_h y_{h,t}$ is always kept together.
Applying this with \Cref{lem:cover} and taking a limit as $h\to 0$, we obtain 
\[
D_{\Gamma^*_t}(y_{1:t} \from y_{1:t-1}) \leq \frac{C \sqrt{\log(1+\frac{1}{\kappa})}}{\sqrt{T-t+1}}\del{\frac{\norm{y}_\infty^2}{\sigma} + \frac{\norm{y}_\infty}{\sigma} \frac{\abs{y}_{\f{Lip}}}{1/\kappa}}.
\]
Summing this up over time, and adding the prior regret gives a regret bound, which we now state.

\begin{restatable}{theorem}{ThmLipschitzTS}
\label{thm:lipschitz-ts}
Let $X = [0,1]$, and $Y = \{y \in \f{BL}(X;\R) : \norm{y}_\infty \leq \beta, \abs{y}_{\f{Lip}} \leq \lambda\}$.
Then Thompson sampling, where $q^{(\gamma)}$ is a Gaussian process with Matérn-1/2 kernel, admits a regret bound of
\[
R(p,q) \leq  C \sigma \sqrt{T\log\del{1+\frac{1}{\kappa}}} + 2C \sqrt{T\log\del{1+\frac{1}{\kappa}}}\del{\frac{\beta^2}{\sigma} + \frac{\beta}{\sigma} \frac{\lambda}{1/\kappa}}
.
\]
Taking $\sigma = \sqrt{2} \beta$ and $\kappa = \frac{1}{\lambda}$, we obtain
\[
R(p,q) \leq  C (2\beta+1) \sqrt{2T\log\del{1+\lambda}}
.
\]
\end{restatable}
This matches the lower bound we expect to follow from the Gaussian universality mentioned above even in infinite-expert, non-$\ell^\infty$-like settings far beyond those considered by \textcite{gravin2016towards}. 
We can intuitively sanity-check this bound by embedding the finite expert setting with $N$ experts into $(\beta,\lambda)=(1,N)$: the bound would then achieve the minimax rate for the embedded problem.
To conclude, note that we would not have been able to achieve this by directly applying the results of \textcite{abernethy14,abernethy2016perturbation}, rather than the infinite-dimensional analogues derived here---we would have no way to relate the regret of Thompson sampling on the infinite domain to its behavior restricted to a cover.
Thus, though our results for the finite-expert setting appear similar, taking a general-state-space approach from the outset is needed to carry out the analysis.


\section{Discussion} 

In this paper, we developed a framework for deriving minimax regret bounds for Thompson sampling algorithms akin to the Nash equilibrium strategy of \textcite{gravin2016towards} for the three-expert setting, but which allow a general, potentially-uncountable number of experts.
The framework decomposes regret into the \emph{prior regret} the learner expects to incur, and an \emph{excess regret} term describing how much additional regret can be incurred due to the true adversary not matching the~prior.

Using this decomposition, we introduced an ansatz for constructing priors that one can expect lead to good minimax performance, by demonstrating that equalizing adversaries which lead to meaningful regret lower bounds are good candidates for Thompson sampling priors---thereby, deriving a general proof strategy for transforming regret lower bounds into regret upper bounds.

We carried out this this routine using a practical---but, we expect, universal---simplification, where the prior is Gaussian, thereby (i) obtaining minimax rates in the classical discrete setting, and (ii) obtaining a regret bound one can reasonably expect to be minimax in the setting where the adversary is constrained to bounded Lipschitz functions on the uncountable expert set~$X = [0,1]$.

Our work connects the game-theoretic perspective of \textcite{gravin2016towards} with the analysis techniques of \textcite{abernethy14,abernethy2016perturbation,orabona2019modern}---and, in the process, removes learning rate schedule restrictions from these approaches, which would prohibit the analysis of Thompson sampling and related algorithms which correspond to FTPL with increasing learning~rates.

We believe our work constitutes a first step towards a technically-sound, yet implementable in practice, online learning algorithms for general state spaces---with numerics that resemble Thompson sampling implementations widely-used in Bayesian optimization.
We hope that future work can apply our approach in more general settings, beyond the one-dimensional kernel matrix calculation central to our analysis of the Lipschitz setting, and ultimately lead to practical and effective online learning algorithms for non-discrete problem classes.
To achieve this, what remains is to relate various adversary smoothness classes to corresponding Gaussian processes, or generalizations~thereof.



\acks{
This work is dedicated to the memory of David Draper, who supervised A.T.'s masters research, and believed in the value of the Bayesian approach to reasoning under uncertainty in order to make optimal decisions---including, and perhaps especially, in situations where the role of Bayesian thinking is subtle rather than obvious.
    
We thank James-Michael Leahy, Gergely Neu, Gabriele Farina, and Ziv Scully for sharing their thoughts about our approach at various points during the development of this work. 
A.T. would additionally like to thank Karthik Sridharan for making him aware of the work of \textcite{gravin2016towards}, which initiated the research which ultimately resulted in this work, and  Dan Roy, for introducing him to J.N. at ISBA 2024---a fitting venue, given the work's dedication---without which the collaboration would not have happened. 

A.T. was supported by Cornell University, jointly via the Center for Data Science for Enterprise and Society, the College of Engineering, and the Ann S. Bowers College of Computing and Information Science.
J.N. was supported by an NSERC Discovery Grant. 
}

\printbibliography

\newpage

\appendix

\begin{center}
\bfseries\Large
Supplement: An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces
\end{center}

\section{General Regret Bounds for Thompson Sampling}
\label{apdx:technical}

In order to properly analyze prediction with expert advice with potentially-uncountably-many experts, we make sufficient assumptions for the respective zero-sum game to have well defined values, and ensure good-enough behavior with respect to differentiation in the sense of variational calculus. 
The following defines a sufficiently-general framework in which to define the action spaces of the learner and adversary, and to establish that the values of the game are well-defined.
Throughout this work, all topological vector spaces we consider are assumed Hausdorff.

\begin{definition}[General-state online learning game]
Let $X$ be second-countable compact Hausdorff.
Let $Y\subset\c{Y}\subseteq C(X;\R)$.
Let $\c{M}_s(X)$ be the space of signed finite measures on $X$, and let $\c{M}_1(X) \subseteq \c{M}_s(X)$ be the corresponding subset of probability measures on $X$.
Let $\pair{\.}{\.} : C(X;\R) \x \c{M}_s(X) \to \R$ be the pairing $\pair{\mu}{f} = \int_X f(x) \d\mu(x)$.
Endow $\c{M}_s(X)$ and $\c{M}_1(X)$ with the topology induced by pairing with continuous functions, which by compactness are automatically bounded: this is the topology of weak convergence of probability measures.
Define the \emph{general-state online learning game} to be following minimax game:
\1 At round $t$ the learner selects $p_t \in \c{M}_1(X)$, based on the history of the game.
\2 Then, the adversary selects $q_t \in \c{M}_1(Y)$, based on the history of the game and the learner's choice of $p_t$.
\3 Random samples $x_t,y_t$ are drawn from $p_t$, $q_t$, respectively in a manner so that $x_t$ and $y_t$ are conditionally independent of each other given the history.
\4 The zero-sum final values are given by regret, which is 
\[
R(p,q) &= \E_{\substack{x_t\~p_t\\y_t\~q_t}} \sup_{x\in X} \sum_{t=1}^T y_t(x) - \sum_{t=1}^T y_t(x_t)
\\
&= \E_{y_t\~q_t} \sup_{p\in\c{M}_1(X)} \sum_{t=1}^T \pair{y_t}{p} - \sum_{t=1}^T \pair{y_t}{p_t}
.
\]
\0 
\end{definition}

As a side note, these assumptions suffice to ensure that Sion's Minimax Theorem holds, though we will not need this for our results.
In this game, for a learner that plays Thompson sampling with prior distribution $q^{(\gamma)}$, a core component of our analysis is demonstrating that the learner's regret can be decomposed into the \emph{prior regret}  and the \emph{excess regret}.
For this, we define $\Gamma^*_t : C(X;\R) \to \R$ to be $\Gamma^*_t(f) = \E \sup_{x\in X} (f(x) + \gamma_{t:T}(x))$. 
Following \textcite[Chapter 30.5]{lattimore20}, the corresponding FTRL regularizer $\Gamma_t$ can be defined as the convex conjugate of $\Gamma^*_t$, namely $\Gamma_t = (\Gamma^*_t)^*$ in which case $\Gamma_t^* = (\Gamma_t)^*$, as we show in the sequel, which ensures there is no ambiguity.
To facilitate comparisons with other approaches, we mirror the notation of \textcite{lattimore20}, even though $\Gamma^*_t$ is the primitive definition in our~formalism.

\DefTS*

\PropRegretDecomposition*

\begin{proof}
Start with 
\[
R(p,y) = \E \sup_{x\in X} \sum_{t=1}^T y_t(x) - \sum_{t=1}^T y_t(x_t) = \Gamma^*_{T+1}(y_{1:T}) - \E\sum_{t=1}^T \pair{y_t}{p_t}
.
\]
Next, consider the telescopic sum
\[
\sum_{t=1}^T \Gamma^*_{t+1}(y_{1:t}) - \Gamma^*_t(y_{1:t-1}) &= \sum_{t=2}^{T+1} \Gamma^*_t(y_{1:t-1}) - \sum_{t=1}^T \Gamma^*_t(y_{1:t-1}) 
\\
&= \Gamma^*_{T+1}(y_{1:T}) - \Gamma^*_1(0)
.
\]
Adding the difference of the two sides gives
\[
R(p,y) &= \Gamma^*_1(0) + \sum_{t=1}^T \Gamma^*_{t+1}(y_{1:t}) - \Gamma^*_t(y_{1:t-1}) - \E \pair{y_t}{p_t}
.
\]
Now, we add and subtract $\E\pair[0]{\gamma_t}{p_t^{(\gamma)}}$ from both sides to get 
\[
R(p,y) &= \ubr{\E\sbr{\Gamma^*_1(0) - \sum_{t=1}^T \pair[0]{\gamma_t}{p_t^{(\gamma)}}}}{R(p,q^{(\gamma)})} + \ubr{\sum_{t=1}^T \Gamma^*_{t+1}(y_{1:t}) - \Gamma^*_t(y_{1:t-1}) - \pair{y_t}{p_t} + \E\pair[0]{\gamma_t}{p^{(\gamma)}_t}}{E_{q^{(\gamma)}}(p,y)}
.
\]
The claim follows.
\end{proof}

The above claim generalizes to the case where, instead of the best-in-hindsight point, one instead works with some general comparator measure---we omit this as we will not need it.
We begin by verifying the basic structural properties of our setup.

\DefEqualizing*

\LemCentering*

\begin{proof}
This follows immediately by subtracting the respective means at each time from a given non-centered equalizing adversary.
\end{proof}

Next, we state conditions under which the prior regret is a lower bound on total regret.
The key assumption here is that of being supported on at most $Y$. 
While this fails in both of our examples for the Gaussian process priors we consider, by Gaussian universality we expect the prior-regret-based lower bound for a true equalizing and centered adversary given by a prior supported on $Y$ to be of the same order as the corresponding surrogate for the Gaussian process priors we used.

\PropLowerBound*

\begin{proof}
We have $R(p,q^{(\gamma)}) \leq \sup_q R(p,q)$: the claim follows by taking an infimum of both sides, and applying the property that $p\mapsto R(p,q^{(\gamma)})$ is constant in $p$.
\end{proof}


We will want to be able to bound the summands appearing in the excess regret, as we have defined it, using an appropriate Bregman divergence. 
However, since we are working in potentially-infinite-dimensional spaces, some care is required in order to ensure that Bregman divergences are well-defined and the corresponding convex analytic-tools we seek to apply are valid.
The first thing needed is to make sense of the derivatives appearing in the divergence.
Denote the extended real-line, as commonly occurs in convex analysis, by $\cl\R$.

\begin{definition}
\label{def:gateaux}
Let $G : \c{G} \to \cl\R$ be a proper convex function.
Define its \emph{Gâteaux derivative}, denoted $\p_{(\.)} G(\.) : \f{dom}(G) \x \c{G} \to \cl\R$, where the second argument refers to the subscript, by
\[
\p_v G(f) = \lim_{\eps\to0^+} \frac{G(f+\eps v) - G(f)}{\eps}
.
\]
\end{definition}

It is not hard to show that the Gâteaux derivative of a proper convex function always exists \cite[Theorem 2.1.13]{zalinescu02}.
However, the resulting Gâteaux derivative may fail to be finite-valued, continuous, or linear in its second argument.
All of these issues are already present in the case where the convex set is a closed interval: for example, linearity fails if one considers Gâteaux derivatives of the absolute value function at the origin, due to lack of smoothness.

We will show in the sequel that, so long as there are almost surely no ties---as is the case for our Gaussian process priors---the function $\Gamma^*_t$ of interest defines a bounded linear Gâteaux derivative.
Compare this with the situation that would occur for exponential-weights-type algorithms in our setting: in contrast with $\Gamma^*_t$, the usual Kullback--Leibler divergence $p \mapsto D_{\f{KL}}(p\from p_0)$, for a given $p_0\in\c{M}_1(X)$, is not smooth---for any given location, the set of directions in which its Gâteaux derivative is finite-valued is a strict subset of the space of signed measures and therefore would need to be characterized.

To proceed, we need an appropriate \emph{Envelope Theorem}: note that the usual results of \textcite{milgrom2002envelope} do not suffice, because they only provide inequalities, so we work with a sharp version similar to \textcite[Proposition 4.12]{bonnans13} and \textcite[Theorem 10.1]{basar08}.
Specifically, we work with the affine variant given by \textcite[Appendix B.6, Lemma 13]{xie24}: for completeness---and due to the presence of minor typographical errors in that work---we re-prove the result we need in \Cref{apdx:envelope}.
Before proceeding, we verify a certain maximizer choice continuity condition holds: in the following, let $\alpha_v(f) \in \argmax_{x\in X} f(x)$ denote a maximizer whose choice depends a function $v$. 

\begin{definition}
We call a function $\alpha:\c{Y}\to X$ with the property that $\alpha(f)\in \argmax_{x\in X} f(x)$ a \emph{tie-breaking rule}.
For a given tie breaking rule $\alpha:\c{Y}\to X$, and pair of functions $f\in\c{Y}$ and $v\in C(X; \R)$, define the \emph{variational value} $V^{(\alpha)}_{f,v}:\R \to \R$ by
\[
V^{(\alpha)}_{f,v}(s) = v(\alpha(f+sv)). 
\]
\end{definition}

\begin{lemma}
\label{lem:envelope_cont}
For every $v\in C(X; \R)$ there exists a tie-breaking rule  $\alpha_v:\c{Y}\to X$ such that, for any $y\in\c{Y}$, the variational value $V^{(\alpha_v)}_{y,v}$ is right-continuous with left limits.
In particular, for that tie-breaking rule, we have $v(\alpha_v(y+\eps v)) \to v(\alpha_v(y))$ as $\eps\to 0$.
\end{lemma}

\begin{proof}
Recall that, since $X$ is second-countable compact Hausdorff, it is metrizable, and in particular it is sequentially compact. Let $\rho$ be a metric that metrizes $X$.
In addition, note that the pair $(y,v)$ then defines an affine subset of $C(X;\R)$ given by
\[
\c{Y}_{y,v} = \cbr{y_s = y+sv \ :\  s\in \R}
.
\]
Let $\alpha_{\bdot}$ be any arbitrary tie-breaking rule. 
We will first show that $V_{y,v}^{(\alpha_{\bdot})}(s)$ is non-decreasing in $s$.
To do so, suppose that $s_1< s_2$.
Then, by the respective optimality, we have
\[
(y+s_1 v)(\alpha_{\bdot}(y_{s_2})) & \leq (y+s_1 v)(\alpha_{\bdot}(y_{s_1}))
&
(y+s_2 v)(\alpha_{\bdot}(y_{s_1})) & \leq (y+s_2 v)(\alpha_{\bdot}(y_{s_2})).
\]
Combining these we find that
\[
(y+s_1 v)(\alpha_{\bdot}(y_{s_1})) + (s_2-s_1) v(\alpha_{\bdot}(y_{s_1})) & \leq (y+s_1 v)(\alpha_{\bdot}(y_{s_2}))+ (s_2-s_1) v(\alpha_{\bdot}(y_{s_2})) 
\\
& \leq (y+s_1 v)(\alpha_{\bdot}(y_{s_1}))+ (s_2-s_1) v(\alpha_{\bdot}(y_{s_2})).
\]
We conclude
\[
v(\alpha_{\bdot}(y_{s_1})) \leq v(\alpha_{\bdot}(y_{s_2})).
\]
therefore $V_{y,v}^{(\alpha_{\bdot})}(s) = v(\alpha_{\bdot}(y_s))$ is non-decreasing in $s$ no matter which tie-breaking rule is used.

Using this, note that $V_{y,v}^{(\alpha_{\bdot})}:\R \to\R$, being non-decreasing, has at most countably many discontinuity points. 
Suppose that $s_c$ is a continuity point of $V_{y,v}^{(\alpha_{\bdot})}$.
If we change the tie-breaking rule at this point alone, while keeping all other points the same, by continuity the variational value must remain unchanged.
One can argue similarly along a partition of continuity points into dense subsets: we conclude all tie-breaking rules give the same variational value at continuity points, and that the set of points where $V_{y,v}^{(\alpha_{\bdot})}$ is discontinuous does not depend on the choice of tie-breaking rule $\alpha$---though different choices may yield different values between left and right limits.

We now show that a tie-breaking rule, $\alpha_v:\c{Y}\to X$, can be chosen so that $V_{y,v}^{(\alpha_v)}(s) = v(\alpha_v(y+sv))$ is right-continuous in $s$---and, hence right continuous with left limits, since it is non-decreasing---for all $y\in \c{Y}$. 
We will take our rule to be equivalent to $\alpha_{\bdot}$ if $s$ is a continuity point of $V_{y,v}^{(\alpha_{\bdot})}(s)$. 
We now need only define the rule for discontinuity points of $V_{y,v}^{(\alpha_{\bdot})}$.

Let $s_d$ be a discontinuity point of $V_{y,v}^{(\alpha_{\bdot})}$.
Since $X$ is sequentially compact, every sequence in $X$ has a convergent subsequence: thus, there is a decreasing sequence of continuity points $s_k$, $k\in\N$, of $V_{y,v}^{(\alpha_{\bdot})}$ with $s_k\geq s_d$ and $s_k\to s_d$ for which $x_k=\alpha_{\bdot}(y_{s_k})$ converges in $X$. 
Let $x_\star$---depending on $y$, $v$, and the sequence $s_k$---be the limit of an arbitrarily chosen such converging sequence, and let $\eps>0$. 
Since $y_{s_d}$ must be continuous at $x_\star$, and since $v$ is bounded, there is a $\delta>0$ and an $M>0$ so that for $\rho(x,x_\star)<\delta$, we have
\[
\abs{y_{s_d}(x) - y_{s_d}(x_\star)}& <\eps 
&
\norm{v}_\infty &\leq M.
\]
Since $x_k\to x_\star$, there is a $k_0\in\N$ with $\rho(x_k,x_\star) \leq \delta$ for all $k\geq k_0$.
Thus, for $k\geq k_0$, we get
\[
y_{s_d}(x_\star) \geq y_{s_d}(x_k)-\eps \geq y_{s_d}(x_k)+(s_k-s_d)v(x_k) -\eps- (s_k-s_d) \norm{v}_\infty
\] 
and 
\[
y_{s_k}(x_k) -\eps- (s_k-s_d) \norm{v}_\infty & \geq y_{s_k}(\alpha_{\bdot}(y_{s_d})) -\eps- (s_k-s_d) \norm{v}_\infty
\\
&\geq y(\alpha_{\bdot}(y_{s_d})) -\eps- 2  (s_k-s_d) \norm{v}_\infty.
\]
Since $\eps$ is arbitrary, and for a given $\eps$ we can take $k$ to be arbitrarily large, and hence $s_k-s_d$ arbitrarily small, it follows that $x_\star \in A(y)$.
Define the tie breaking rule $\alpha_v$ at the point $y_{s_d}$ by $\alpha_v(y_{s_d}) = x_\star$. 
Since $v$ is continuous and $x_k\to x_\star$ it follows that the variational value $V_{y,v}^{(\alpha_v)}(s) = v(\alpha_v(y+sv))$ is right-continuous in $s$ at $s_d$. 

We can modify $\alpha_{\bdot}$ at all discontinuity points of $V_{y,v}^{(\alpha_{\bdot})}$ at once in a similar way, yielding a tie-breaking rule $\alpha_v$ such that $V_{y,v}^{(\alpha_v)}$ is right-continuous everywhere on the affine space $\c{Y}_{y,v}$, and these rules can be chosen to be consistent between different function pairs $(y,v)$ that give rise to the same affine subspace of $\c{Y}$. 
We can then therefore join these rules together for different affine subspaces of $\c{Y}_{y,v}\subseteq\c{Y}$ to get a single tie-breaking rule $\alpha_v$ with $V_{y,v}^{(\alpha_v)}$ right-continuous with left limits for all $y\in\c{Y}$, as was claimed.
\end{proof}


\begin{lemma}
\label{lem:derivative}
Assume that $q^{(\gamma)}$ is independent across time, and is chosen such that maximizers defining $\Gamma^*_t$ are almost surely unique in the sense that, for all $f\in C(X;\R)$ and all $t\in[T]$, we have $q^{(\gamma)} (\{\gamma : |\argmax_{x\in X}(f(x)+\gamma_{t:T}(x))\}=1|)=1$.
Then $\Gamma^*_t$ is Lipschitz with respect to the supremum norm on $C(X;\R)$, and its Gâteaux derivative is
\[
\p_v \Gamma^*_t(f) = \pair[1]{v}{p^{(f)}_t}
\]
where $p_t^{(f)}$ is defined as the distribution of $\displaystyle\argmax_{x\in X} (f(x) + \gamma_{t:T}(x))$.
\end{lemma}

\begin{proof}
We first prove the second assertion.
Note first that, since $X$ is compact, $\c{Y}$ consists entirely of continuous functions, and $\gamma_{t:T}$ is almost surely continuous by assumption, the supremum $\sup_{x\in X} f(x) + \gamma_{t:T}(x)$ is almost surely achieved.
Mirroring the notation of \Cref{lem:envelope_sharp}, define $\c{V}^{(g)}(f) = \sup_{x\in X} f(x) + g(x)$, and $\c{L}^{(g)}(x,f) = f(x) + g(x)$, for which we have $\p_v \c{L}^{(g)}(x,f) = v(x)$, with differentiation taken in the second argument.
Write
\[
\p_v \Gamma^*_t(f) &= \p_v \E_{\gamma_{t:T}\~q^{(\gamma)}} \c{V}^{(\gamma_{t:T})}(f) 
\\
\overset{\t{(i)}}&= \E_{\gamma_{t:T}\~q^{(\gamma)}} \p_v \c{V}^{(\gamma_{t:T})}(f)
\\
\overset{\t{(ii)}}&= \E_{\gamma_{t:T}\~q^{(\gamma)}} \max_{x^* \in A(f + \gamma_{t:T})} v(x^*) 
\\
\overset{\t{(iii)}}&= \E_{x\~p_t^{(f)}} v(x)
\]
where (i) holds by a Dominated Convergence Theorem variant given by \textcite[Theorem 2.13]{legall2022measure} where all required conditions follow by convexity of $\Gamma^*_t$, and (ii) holds by \Cref{lem:envelope_sharp,lem:envelope_cont}, and (iii) holds because $p^{(f)}_t$ is the pushforward of $q^{(\gamma)}$ by $\gamma\mapsto\argmax_{x\in X}(f(x)+\gamma(x))$, using the assumption that maximizers are almost surely unique.
Since 
\[
\sup_{\norm{v}_\infty\leq 1} \pair[1]{v}{p^{(f)}_t} \leq 1
\]
the Lipschitz property, and the claim, follows.
\end{proof}

The next step is to establish a form of Fenchel duality induced by $\Gamma^*_t$.
To focus attention on regret-theoretic aspects, and given the generality of our setting, we adopt a minimalist approach which consists of deriving just-enough consequences of this duality for our calculation to go through.
To do so, let $C(X;\R)^*$ be the topological dual of $C(X;\R)$, and denote the canonical pairing by $\pair{\.}{\.} : C(X;\R) \x C(X;\R)^* \to \R$.
The resulting notational overlap is unambiguous: by the Reisz--Markov--Kakutani Representation Theorem \cite[Theorem 6.19]{rudin87},\footnote{Note that, while this reference proves the complex measure variant of this result, its real analog follows by the same argument. Also, recall that since we have assumed $X$ second-countable compact Hausdorff, the Borel regularity property in the aforementioned result holds for all of $\c{M}_s(X)$ by standard theory.} the dual $C(X;\R)^*$ in bijective isometry with $\c{M}_s(X)$, viewed as a Banach space under the total variation norm.


\begin{lemma}
\label{lem:biconjugate}
Define the Fenchel conjugate $\Gamma_t^{**} : C(X;\R)^* \to \cl\R$,  by
\[
\Gamma_t^{**}(\ell) = \sup_{f\in C(X;\R)} \pair[1]{f}{\ell} - \Gamma^*_t(f) 
\]
with the convention that signed measures are identified with the bounded linear functionals they give rise to by way of integration.
Then for $f\in C(X;\R)$ we have
\[
\Gamma_t^{**}(p^{(f)}_t) &= \pair[1]{f}{p^{(f)}_t} - \Gamma^*_t(f)
.
\]
\end{lemma}

\begin{proof}
\textcite[Theorem 2.4.2(iii)]{zalinescu02}: to apply this, we verify our our setting satisfies the assumptions assumed therein---which it does, since the space of continuous functions $C(X;\R)$ is Hausdorff and locally convex.
\end{proof}



\begin{lemma}
\label{lem:bregman_duality}
For $f,f'\in C(X;\R)$ and $\ell\in C(X;\R)^*$, define the \emph{Bregman divergences}
\[
D_{\Gamma^*_t}(f \from f') &= \Gamma^*_t(f) - \Gamma^*_t(f') - \pair[1]{f-f'}{p^{(f')}_t}
\\
D_{\Gamma^{**}_t}(\ell \from p^{(f)}_t) &= \Gamma^{**}_t(\ell) - \Gamma^{**}_t(p^{(f)}_t) - \pair[1]{f}{\ell - p^{(f)}_t}
.
\]
Then we have the Bregman duality formula
\[
D_{\Gamma^*_t}(f \from f') = D_{\Gamma_t^{**}}(p_t^{(f')} \from p_t^{(f)})
.
\]
\end{lemma}

\begin{proof}
This follows immediately from \Cref{lem:biconjugate} and the respective definitions.
\end{proof}

\begin{lemma}
\label{lem:bregman_conjugate}
We have
\[
D_{\Gamma^{**}_t}(\. \from p^{(f)}_t)^*(f') = \Gamma^*_t(f + f') + \Gamma^{**}_t(p^{(f)}_t) - \pair[1]{f}{p^{(f)}_t}
.
\]
\end{lemma}

\begin{proof}
Using the definition of a convex conjugate and linearity, write
\[
D_{\Gamma_t^{**}}(\. \from p^{(f)}_t)^*(f') &= \sup_{\ell\in C(X;\R)^*} \pair{f'}{\ell} - D_{\Gamma^{**}_t}(\ell \from p^{(f)}_t)
\\
&= \del{\sup_{\ell\in C(X;\R)^*} \pair{f + f'}{\ell} - \Gamma^{**}_t(\ell)}  + \Gamma^{**}_t(p^{(f)}_t) - \pair[1]{f}{p^{(f)}_t}
\\
&= \Gamma^*_t(f + f') + \Gamma^{**}_t(p^{(f)}_t) - \pair[1]{f}{p^{(f)}_t}
\]
where the final step follows by \textcite[Theorem 2.3.3(iii)]{zalinescu02}---where, to apply this result, lower semi-continuity of $\Gamma^*_t$ follows from the Lipschitz property established in \Cref{lem:derivative}.
\end{proof}

Note that, given our choice to mirror the notation of \textcite[Chapter 30.5]{lattimore20}, the definition of the FTRL regularizer is $\Gamma_t = \Gamma^{**}_t$: in what follows, we write the latter expression rather than the former for notational precision---recall that, in our formalism, $\Gamma^*_t$ is the primitive definition.
A point of technical subtlety is our definition of the Bregman divergence $D_{\Gamma^{**}_t}(\.\from\.)$: this definition essentially \emph{posits} what the respective derivative term appearing in it should be.
One can show, for virtual adversaries with large-enough support over the space of continuous functions, that this definition coincides with the standard one.
In cases where this fails, the respective Gâteaux derivative may not be linear: our definition then corresponds to picking an appropriate element from the respective subdifferential.
Using these calculations, we are finally ready to prove our Bregman-divergence-based bound on the excess regret.

\PropBregmanBound*

\begin{proof}
By definition
\[
E_{q^{(\gamma)}}(p,y) = \sum_{t=1}^T \ubr{\Gamma^*_{t+1}(y_{1:t}) - \Gamma^*_t(y_{1:t-1}) - \pair{y_t}{p_t} + \E\pair[0]{\gamma_t}{p_t^{(\gamma)}}}{E_t}
.
\]
We bound this term-by-term.
Recall that, 
Using this, write
\begingroup
\allowdisplaybreaks
\[
E_t &= \Gamma^*_{t+1}(y_{1:t}) - \Gamma^*_t(y_{1:t-1}) - \pair{y_t}{p_t} + \E\pair[0]{\gamma_t}{p_t^{(\gamma)}}
\\
\overset{\t{(i)}}&= \Gamma^*_{t+1}(y_{1:t}) - \Gamma^*_t(y_{1:t-1}) - \E\pair{y_t - \gamma_t}{p_t}
\\
&= \E_{\gamma_t\~q_t^{(\gamma)}} \pair{y_t - \gamma_t}{p_{t+1} - p_t} - \Gamma^*_{t+1}(y_{1:t-1} + \gamma_t) + \Gamma^*_{t+1}(y_{1:t}) + \pair{\gamma_t - y_t}{p_{t+1}}
\\
&= \E_{\gamma_t\~q_t^{(\gamma)}} \pair{y_t - \gamma_t}{p_{t+1} - p_t} - D_{\Gamma^*_{t+1}}(y_{1:t-1} + \gamma_t \from y_{1:t})
\\
\overset{\t{(ii)}}&= \pair{y_t}{p_{t+1} - p_t} - \E_{\gamma_t\~q_t^{(\gamma)}}  D_{\Gamma_{t+1}^{**}}(p_{t+1} \from p_{y_{1:t-1} + \gamma_t})
\\
\overset{\t{(iii)}}&= \E_{\gamma_t\~q_t^{(\gamma)}} \pair{y_t}{p_{t+1}} - \pair{y_t}{p_{y_{1:t-1} + \gamma_t}} - D_{\Gamma_{t+1}^{**}}(p_{t+1} \from p_{y_{1:t-1} + \gamma_t})
\\
\overset{\t{(iv)}}&\leq \E_{\gamma_t\~q_t^{(\gamma)}}D_{\Gamma_{t+1}^{**}}(\. \from p_{y_{1:t-1} + \gamma_t})^*(y_t) + D_{\Gamma_{t+1}^{**}}(p_{t+1} \from p_{y_{1:t-1} + \gamma_t})
\\
&\qquad- \pair{y_t}{p_{y_{1:t-1} + \gamma_t}} - D_{\Gamma_{t+1}^{**}}(p_{t+1} \from p_{y_{1:t-1} + \gamma_t})
\\
&= \E_{\gamma_t\~q_t^{(\gamma)}}D_{\Gamma_{t+1}^{**}}(\. \from p_{y_{1:t-1} + \gamma_t})^*(y_t) - \pair{y_t}{p_{y_{1:t-1} + \gamma_t}}
\\
\overset{\t{(v)}}&= \E_{\gamma_t\~q_t^{(\gamma)}} \Gamma^*_{t+1}(y_{1:t-1} + \gamma_t + y_t) - \pair{y_t}{p_{y_{1:t-1} + \gamma_t}}
\\
&\qquad+ \Gamma^{**}_{t+1}(p_{y_{1:t-1} + \gamma_t}) - \pair{y_{1:t-1} + \gamma_t}{p_{y_{1:t-1} + \gamma_t}} 
\\
\overset{\t{(vi)}}&= \E_{\gamma_t\~q_t^{(\gamma)}} \Gamma^*_{t+1}(y_{1:t-1} + \gamma_t + y_t) - \Gamma^*_{t+1}(y_{1:t-1} + \gamma_t) - \pair{y_t}{p_{y_{1:t-1} + \gamma_t}}
\\
&= \Gamma^*_t(y_{1:t}) - \Gamma^*_t(y_{1:t-1}) - \pair{y_t}{p_t}
\\
&= D_{\Gamma^*_t}(y_{1:t} \from y_{1:t-1})
.
\]
\endgroup
where (i) follow by equalization, since $\E\pair{\gamma_t}{p} = \E\pair{\gamma_t}{p'}$ for all $p$ and $p'$, (ii) follows by a combination of equalization and \Cref{lem:bregman_duality}, (iii) follows by the Tower Rule, (iv) follows by applying Young's inequality with respect to the convex function $\ell \mapsto D_{\Gamma_{t+1}^{**}}(\ell \from p_{y_{1:t-1} + \gamma_t})$, (v) follows by \Cref{lem:bregman_conjugate}, (vi) follows by \Cref{lem:biconjugate}, and all remaining lines follow by definition.
\end{proof}

\subsection{A Sharp Envelope Theorem}
\label{apdx:envelope}

We now prove the necessary Envelope Theorem used in \Cref{lem:derivative}.

\begin{lemma}
\label{lem:envelope}
Let $\Theta$ be a subset of a topological vector space, and let $X$ be an arbitrary set.
Let $\c{L} : X \x \Theta \to \R$ be bounded above, and define $\c{V}$ to be
\[
\c{V}(\theta) = \sup_{x\in X} \c{L}(x,\theta)
\]
Suppose that, for every $\theta\in\Theta$, the supremum is achieved, and let $\c{A}(\theta)$ be the maximizer set.
For any $v$, suppose that the Gâteaux derivatives $\p_v \c{V}(\theta)$ and $\p_v \c{L}(x,\theta)$ exist and are finite-valued, with the convention that the Gâteaux derivative of $\c{L}$ is taken in its second argument.
Then
\[
\sup_{x^*\in \c{A}(\theta)} \p_v \c{L}(x^*, \theta) \leq \p_v \c{V}(\theta)
.
\]
\end{lemma}

\begin{proof}
For a given $\theta\in\Theta$, let $x^*(\theta) \in \c{A}(\theta)$ be an arbitrary maximizer. 
Observe that, for all $\theta'\in\Theta$ and all $x^*(\theta')\in A(\theta')$, we have
\[
\c{L}(x^*(\theta),\theta') \leq \c{L}(x^*(\theta'),\theta') = \c{V}(\theta')
\]
by optimality, with equality if $\theta' = \theta$.
Now, let $\eps>0$ be sufficiently small, and choose $\theta' = \theta + \eps v$.
Then, if we subtract $\c{L}(x^*(\theta), \theta) = \c{V}(\theta)$ from both sides and form difference quotients, we get
\[
\frac{\c{L}(x^*(\theta),\theta + \eps v) - \c{L}(x^*(\theta), \theta)}{\eps} \leq \frac{\c{V}(\theta + \eps v) - \c{V}(\theta)}{\eps}
.
\]
Since we have assumed both derivatives to exist, taking limits gives
\[
\p_v \c{L}(x^*(\theta),\theta) \leq \p_v \c{V}(\theta)
\]
for all choices $x^*(\theta)\in \c{A}(\theta)$.
\end{proof}


\begin{lemma}
\label{lem:envelope_sharp}
With the assumptions and notation of \Cref{lem:envelope}, suppose further that:
\1 For all $x$, $\c{L}(x,\theta)$ is affine in $\theta$, namely $\c{L}(x,\theta) = \c{L}_0(x,\theta) + m(x)$ where $\c{L}_0$ is linear in $\theta$.
\2 For any $\theta$ and any $v$, there exists $x^*_v(\theta) \in \c{A}(\theta)$ such that, for all sufficiently-small $\eps>0$, there exist $x^*_v(\theta + \eps v) \in A(\theta + \eps v)$, such that we have $\c{L}_0(x^*_v(\theta+\eps v),v) \to \c{L}_0(x^*_v(\theta),v)$.
\0 
Then the supremum over $\c{A}(\theta)$ in \Cref{lem:envelope} is achieved---not necessarily uniquely---and we have
\[
\max_{x^*\in \c{A}(\theta)} \p_v \c{L}(x^*, \theta) = \p_v \c{L}(x^*_v(\theta),\theta) = \p_v \c{V}(\theta)
.
\]
\end{lemma}

\begin{proof}
Note first, by optimality, that $\c{L}(x^*(\theta + \eps v),\theta) \leq \c{L}(x^*(\theta),\theta)$.
Using this, for any $\eps>0$,~write
\[
\frac{\c{V}(\theta + \eps v) - \c{V}(\theta)}{\eps} &= \frac{\c{L}(x^*(\theta + \eps v),\theta + \eps v) - \c{L}(x^*(\theta),\theta)}{\eps}
\\
&= \frac{\c{L}(x^*(\theta + \eps v),\theta + \eps v) - \c{L}(x^*(\theta + \eps v),\theta)}{\eps}
\\
&\quad+ \frac{\c{L}(x^*(\theta + \eps v),\theta) - \c{L}(x^*(\theta),\theta)}{\eps}
\\
\overset{\t{(i)}}&\leq \frac{\c{L}(x^*(\theta + \eps v),\theta + \eps v) - \c{L}(x^*(\theta + \eps v),\theta)}{\eps}
\\
\overset{\t{(ii)}}&= \c{L}_0(x^*(\theta + \eps v),v)
\]
where (i) follows by the aforementioned optimality inequality, and (ii) follows because $\c{L}$ was assumed affine.
Next, we choose $x^*(\theta) = x^*_v(\theta)$ and $x^*(\theta+\eps v) = x^*_v(\theta+\eps v)$ according to the claim's assumption, and use this assumption to take limits of both sides: this gives
\[
\p_v \c{V}(\theta) \leq \c{L}_0(x^*_v(\theta), v) = \p_v \c{L}(x^*_v(\theta), \theta)
\]
where the final equality is the expression for the Gâteaux derivative of an affine function.
Combining this expression with the \Cref{lem:envelope}, we conclude that the respective inequality is tight, and that each $x^*_v(\theta)$ achieves the supremum over $\c{A}(\theta)$.
The claim follows.
\end{proof}





\section{Example: Bounded Lipschitz Adversary}
\label{apdx:bl}

In this section, we assume $X = [0,1]$, and $Y$ is a bounded Lipschitz unit ball $Y = \{y\in\f{BL}(X;\R) : \norm{y}_\infty \leq \beta, \abs{y}_{\f{Lip}} \leq \lambda\}$.
A core part of the argument, appearing in both the prior and excess regret terms is the expected supremum bound for the Gaussian process prior.
We first recall~the~setup.

\LemPrior*

\begin{proof}
Immediate.
\end{proof}

With this, we proceed to bound the prior regret for our specific setting: this follows by applying a standard chaining argument to bound the expected supremum in terms of the Dudley entropy integral, which we analyze as follows.

\begin{lemma}
\label{lem:ou-max-ineq}
Let $\gamma\~[GP](0,\sigma^2 k_\kappa)$ where the kernel is Matérn-1/2, which is given by the expression $k_\kappa(x,x') = \exp\del{-\frac{|x-x'|}{\kappa}}$. 
Then there is a universal constant $C>0$ such that
\[
\E\sup_{x\in[0,1]}  \gamma(x) \leq C \sigma\sqrt{\log\del{1+\frac{1}{\kappa}}}
.
\]
\end{lemma}

\begin{proof}
Note first that the result for general $\sigma>0$ follows immediately from the case $\sigma=1$, so assume this without loss of generality.
By the Dudley entropy integral bound \cite[Theorem 10.1]{lifshits12}, we have 
\[
\E \sup_{x\in[0,1]} \gamma(x) \leq 4\sqrt{2} \int_0^{1/2} \sqrt{\log C^{(d_\kappa)}_\eps(X)} \d\eps
\]
where $C^{(d_\kappa)}_\eps(X)$ is the minimum number of closed balls of radius $\eps$ needed to cover $X$ with respect to the \emph{kernel distance} on $X$, defined as
\[
d_{k_\kappa}(x,x')^2 = \E(\gamma(x) - \gamma(x'))^2 = 2 - 2k_\kappa(x,x')
.
\]
We bound $C^{(d_\kappa)}_\eps(X)$ from above by exhibiting a cover: choose one consisting of $N_\eps$ points, placed at $\cbr{\frac{1-\delta_\eps N_\eps}{2}+ k \delta_{\eps} \ : \ k\in[N_\eps]}$, where $\delta_\eps$ is to be determined and $N_\eps = \ceil{\delta_\eps^{-1}}-1 \leq \delta_\eps^{-1}$.
For neighboring points $x_1,x_2$ in the cover we have $|x_1-x_2| = \delta_\eps$, thus
\[
k(x_1,x_2) &= \exp\del{-\frac{\delta_\eps}{\kappa}}
&
d_{k_\kappa}(x_1,x_2) &= \sqrt{2 - 2 \exp\del{-\frac{\delta_\eps}{\kappa}}}
\]
To get $d_{k_\kappa}(x_1,x_2) \leq \eps$, we must have $2 - 2 \exp\del{-\frac{\delta_\eps}{\kappa}} \leq \eps^2$, and solving for $\delta_\eps$ gives $\delta_\eps \leq -\kappa \log\del{1 - \frac{\eps^2}{2}}$. Combining this with $\delta_\eps\leq 1$ gives:
Dropping the constant additive term, we conclude that
\[
C^{(d_\kappa)}_\eps(X) \leq N_\eps \leq \max\del{1,\frac{1}{-\kappa \log\del{1 - \frac{\eps^2}{2}}}} \leq \max\del{1,\frac{2}{\kappa \eps^2}}
\]
where the latter inequality follows from $-\log(1-x) \geq x$, which itself follows by convexity.
We use this to bound the Dudley entropy integral.
Let $\psi = \frac{1}{\kappa}$ and $m=\min\del{\frac{1}{2}, \sqrt{2\psi }}$.
Write
\[
\int_0^{m} \sqrt{\log\del{\frac{2\psi}{\eps^2}}} \d \eps &= \sbr{\eps \sqrt{\log\del{\frac{2\psi}{\eps^2}}} - \sqrt{\pi\psi} \cdot \f{erf} \del{\sqrt{\frac{1}{2}\log\del{\frac{2\psi}{\eps^2}}}}}_0^m 
\\
&= 
\begin{cases}
\frac{1}{2}\sqrt{\log(8\psi)} + \sqrt{\pi\psi}\del{1-\f{erf}\del{\sqrt{\frac{1}{2} \log\del{8\psi}}}} & : \psi\geq \frac{1}{8} 
\\
\sqrt{\pi\psi}\del{1-\f{erf}(0)}& : \psi< \frac{1}{8} 
\end{cases} 
\\
& = 
\begin{cases}
\frac{1}{2}\sqrt{\log(8\psi)} + \sqrt{\pi\psi}\del{1-\f{erf}\del{\sqrt{\frac{1}{2} \log\del{8\psi}}}} & : \psi\geq \frac{1}{8} 
\\
\sqrt{\pi\psi}& : \psi< \frac{1}{8}.
\end{cases}        
\]
where $\f{erf}$ is the error function. 
Using $1-\f{erf}(x)\leq \min\del{1, \frac{\exp(-x^2)}{x\sqrt{\pi}}}$, we get 
\[
\int_0^{m} \sqrt{\log\del{\frac{2\psi}{\eps^2}}} \d \eps \leq 
\begin{cases}
\frac{1}{2}\sqrt{\log(8\psi)} + \min\del{\sqrt{\pi\psi},\frac{1}{2\sqrt{\log(8\psi)}}}& : \psi\geq \frac{1}{8} \\
\sqrt{\pi\psi}& : \psi< \frac{1}{8}
\end{cases}        
\]
For $C$ large enough, it therefore follows that 
\[
\int_0^{m} \sqrt{\log\del{\frac{2\psi}{\eps^2}}} \d \eps &\leq 
\begin{cases}
\frac{1}{2}\sqrt{\log(8\psi)} + \min\del{\sqrt{\pi\psi},\frac{1}{2\sqrt{\log(8\psi)}}}& : \psi\geq \frac{1}{8} 
\\
\sqrt{\pi\psi}& : \psi< \frac{1}{8}
\end{cases}        
\\
&\leq  C\sqrt{\log(1+\psi)}
\]
where one can verify numerically that $C=2.7$ is sufficient.
Substituting this above, we get
\[
\E \sup_{x\in[0,1]} \gamma(x) \leq 16 \sqrt{\log\del{1+\frac{1}{\kappa}}}
\]
which gives the claim.
\end{proof}

Next, we need a result which lets us move from the Bregman divergence on the infinite-dimensional space to a Bregman divergence on a finite-dimensional space obtained from a cover.

\LemCover*

\begin{proof}
Recall that our Bregman divergence can be written in probabilistic form as
\[
D_{\Gamma^*_t}(y_{1:t} \from y_{1:t-1}) = \E ((y_{1:t} + \gamma_{t:T})(x^*_{y_{1:t} + \gamma_{t:T}}) - (y_{1:t} + \gamma_{t:T})(x^*_{y_{1:t-1} + \gamma_{t:T}}))
\]
where we pass to the cover in stages, separately for the deterministic and random part of the sum.
To ease notation, let $\Upsilon = y_{1:t} + \gamma_{t:T}$.
For the deterministic part, using that $ y_{1:t}$ is $t$-Lipschitz, we have
\[
y_{1:t}(x^*_\Upsilon) -  y_{1:t}(x^*_{\Upsilon - y_t}) &= y_{1:t}(x^*_\Upsilon) -  y_{1:t}(x^*_\Upsilon|_{X_h}) + y_{1:t}(x^*_\Upsilon|_{X_h})
\\
&\quad- y_{1:t}(x^*_{\Upsilon - y_t}) + y_{1:t}(x^*_{\Upsilon - y_t}|_{X_h}) -  y_{1:t}(x^*_{\Upsilon - y_t}|_{X_h})
\\
&\leq  y_{1:t}(x^*_\Upsilon|_{X_h}) -  y_{1:t}(x^*_{\Upsilon - y_t}|_{X_h}) + 2th
\]
where the notation $x^*_\Upsilon|_{X_h}$ refers to the maximizer of $\Upsilon$ over $X_h$, and similarly for $x^*_{\Upsilon-y_t}$.
For the stochastic part, we have
\[
\E(\gamma_{t:T}(x^*_{y_{1:t} + \gamma_{t:T}}) - \gamma_{t:T}(x^*_{y_{1:t-1} + \gamma_{t:T}})) &\leq \gamma_{t:T}(x^*_\Upsilon|_{X_h}) -  \gamma_{t:T}(x^*_{\Upsilon - y_t}|_{X_h}) 
\\
&\quad+ 2\sqrt{T-t+1} \psi(h)
\]
where $\psi(h)$ is the expected modulus of continuity for $\gamma$ at $h$, which by \textcite{fischer2009moments} is known to be $\c{O}\del{\sqrt{h\log\del{\frac{1}{h}}}}$ for this process.
\end{proof}

Finally, we need to understand how the inverse of the kernel matrices for the chosen prior interact with the bounded Lipschitz class that adversary chooses reward functions from.
To this end, we have the following.

\LemKernelMatrixInvNorm*

\begin{proof}
Without loss of generality, using shift-invariance of the kernel, take $X_h = \{h, 2h, .., 1-h\}$.
For purposes of this argument, it will be cleaner to reinterpret the vector $y_h\in\R^{|X_h|}$ as a function $y_h : X_h \to \R$, which we henceforth do.
The kernel matrix arising from this kernel is a Kac--Murdock--Szegő matrix, whose inverse is tridiagonal with an explicit analytic form \cite{trench2001properties}.
Letting $\theta = \exp(-h/\kappa) \in (0,1]$ and multiplying this form by $y_h$ gives the finite-difference-type expression
\[
K^{-1} y_h(x)  
= \frac{1}{\sigma^2}\begin{cases}
\dfrac{y_h(x) - \theta y_h(x+h)}{1-\theta^2}
&:  x=h 
\\[2ex]
\dfrac{(1+\theta^2) y_h(x) - \theta (y_h(x-h)+y_h(x+h))}{1-\theta^2}
&: 2h\leq x\leq 1-2h
\\[2ex]
\dfrac{y_h(x) - \theta y_h(x-h)}{1-\theta^2} 
&: x=1-h
.
\end{cases}
\]
The aim now is to bound these terms for all values of $h$ and $\theta$.
For this, note that for all $h>0$ we have $\frac{2\theta}{1-\theta^2} h = \frac{2 h e^{-h/\kappa} }{1-e^{-2h/\kappa}} = \frac{h}{\sinh(h/\kappa)} \leq \kappa$ which is equivalent to the expression $h/\kappa\leq\sinh(h/\kappa)$.
For the case $x=h$, using this inequality and $\frac{1-\theta}{1-\theta^2}\leq 1$, write 
\[
\frac{y_h(x) - \theta y_h(x+h)}{1-\theta^2} = 
\frac{(1-\theta)}{1-\theta^2} y_h(x) + \frac{\theta}{1-\theta^2}(y_h(x) - y_h(x+h)) \leq \norm{y_h}_\infty + \frac{\kappa}{2}\abs{y_h}_{\f{Lip}}
\]
The case $x=1-h$ follows identically.
The remaining case follows by noting that $(1 + \theta^2) = (1 - \theta)^2 + 2\theta$ with $\frac{(1-\theta)^2}{(1-\theta^2)}\leq 1$ and writing
\[
&\frac{(1+\theta^2) y_h(x) - \theta (y_h(x-h)+y_h(x+h))}{1-\theta^2} 
\\
&\quad=\frac{(1 - \theta)^2 y_h(x) - \theta (y_h(x-h) - 2y_h(x) +y_h(x+h))}{1-\theta^2}
\\
&\quad\leq \norm{y_h} + \kappa\abs{y_h}_{\f{Lip}}
\]
The claim follows.
\end{proof}

We are now ready to prove the main regret bound.

\ThmLipschitzTS*

\begin{proof}
We apply the developed theory: decomposing total regret into prior regret and excess regret, we can bound the prior regret using \Cref{lem:ou-max-ineq}, which gives the first term in the bound.
For the second term, we first apply the Bregman bound of \Cref{prop:excess_regret}, pass to the cover using \Cref{lem:cover}, then rewrite the resulting terms using the Lagrange form of their Taylor expansions.
These can be bounded term-by-term using
\[
y_h^T\grad^2 \Gamma^*_{h,t}(y_{h,1:t-1} + \tl{y}_h) y_h
\overset{\t{(i)}}&\leq \frac{2}{T-t+1}\E\del{\max_{x\in X_h}\gamma_{t:T}(x)} \del{\frac{1}{\sigma^2}\norm{y_h}_\infty + \frac{\kappa}{\sigma^2}  \abs{y_h}_{\f{Lip}} }\norm{y_h}_\infty
\\
\overset{\t{(ii)}}&\leq \frac{2}{T-t+1}\E\del{\max_{x\in X}\gamma_{t:T}(x)} \del{\frac{1}{\sigma^2}\norm{y}_\infty + \frac{\kappa}{\sigma^2} \abs{y}_{\f{Lip}}}\norm{y}_\infty
\\
\overset{\t{(iii)}}&= \frac{2\cdot C \sqrt{\log(1+\frac{1}{\kappa})}}{\sqrt{T-t+1}}\del{\frac{1}{\sigma}\norm{y}_\infty + \frac{\kappa}{\sigma} \abs{y}_{\f{Lip}}}\norm{y}_\infty
\]
where (i) follows by \Cref{lem:kernel_matrix_inv_norm}, (ii) follows by taking a limit as $h\to0$ to return to the infinite-dimensional setting, and (iii) follows by \Cref{lem:ou-max-ineq}.
\end{proof}


\end{document}
