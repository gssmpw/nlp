% !TEX root = ./main.tex

\section{Learning from Network Outcomes}\label{sec:learning}

% In this section, we study how an observe

To mitigate the effects of strategic behavior, a platform must understand whether manipulation has occurred, and who the strategic actors are. In this section, we give computationally efficient methods to do so based on knowledge of the network edges and observing the corrupted equilibrium $\bz^\prime$. The latter can be found, for example, by performing sentiment analysis on the users' posts. 

% In the previous section, we studied the computation of the Nash equilibrium if the set of strategic agents $S$ and intrinsic beliefs $\bs$ are given. However, in a real-world context, the platform can only observe the corrupted equilibrium $\bz^\prime$, for example, by performing sentiment analysis on the users' posts. 

\subsection{Detecting Manipulation with a Hypothesis Test}

In many real-world networks, the distribution of truthful opinions follows a Gaussian distribution (Figure~\ref{fig:t_test}). Given estimates $(\widehat{L}, \widehat{A})$ for the graph Laplacian and susceptibility matrix, the platform can observe the corrupted equilibrium $\bz^\prime$ and solve for the strategic opinions $\bs^\prime$ via: 
\begin{align}
\widehat{\bs^\prime} := \widehat{A}^{-1}((I - \widehat{A})\widehat{L} + \widehat{A}) \bz^\prime. \label{eq:solve_sprime}
\end{align}
We propose that the platform perform a one-sample $t$-test with the entries $\bs^\prime$, with a population mean $\mu_0 \in \RR$ based on e.g. historical data. Under the null hypothesis in which no manipulation has occurred, $\bs^\prime = \bs$, so the test should fail to reject the null hypothesis. However, when agents $S \subset [n]$ deviate, then $\bs_i^\prime \neq \bs_i$ for $i \in S$, so the test should reject the null for large enough deviations. The test is simple, and described in \cref{alg:hypothesis_testing}. Figure~\ref{fig:t_test} shows the results of the test for varying choices of $S$. We see that at significance level $0.05$, the test has low Type I error, as it will return ``No Manipulation'' when $S = \emptyset$, and low Type II error as it will return ``Manipulation'' when $S \neq \emptyset$. 



\begin{algorithm}[t]
\SetAlgoLined
\KwIn{Estimated graph information $\widehat L$ and $\widehat A$, observed equilibrium $\bm {z}'$}
\KwOut{``Manipulation'' or ``No Manipulation''}

Observe corrupted equilibrium $\bz^\prime$. 

Solve for $\widehat{\bs^\prime}$ (Eq.~\eqref{eq:solve_sprime}). 


Perform one-sample $t$ test on the entries of $\widehat{\bs^\prime}$ with a population mean $\mu_0$ under the null hypothesis. 

\Return{If the $t$-test rejects, return ``Manipulation.'' Otherwise, return ``No Manipulation.''}
\caption{Learning from Misreporting Equilibrium with Hypothesis Testing}
\label{alg:hypothesis_testing}
\end{algorithm}



For the Political Blogs dataset, intrinsic opinions belong to $\{\pm 1\}$, so the null hypothesis should be a biased Rademacher distribution. In this case, one should use a $\chi^2$-test, as in ~\citet{agarwal2020chisel,chen2022antibenford}. 


% If this is the case, we propose 


% {\bf Test.} Solve for solve for $\bs^\prime := A^{-1}((I - A)L + A) \bz^\prime$ from $z^\prime$. 
% For a given prior mean $\hat \bmu \in \RR^n$ and covariance $\Sigma \in \RR^{n \times n}$, compute the $\chi^2$ statistic: 
% \begin{align*}
% \chi^2 := (\bs^\prime - \hat \bmu)^T L (\bs^\prime - \hat \bmu)    
% \end{align*}


% \begin{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{opinion_distributions_and_pvalues.pdf}
    \caption{The true opinions for Twitter (left) and Reddit (middle) both follow a normal distribution. When simulating strategic manipulation with random choices of $S$ (right), the detection test (Algorithm~\ref{alg:hypothesis_testing}) has no Type I or Type II error at significance level $0.05$. The tester uses $\widehat{L} = L, \widehat{A} = A = \frac{1}{2} I$, and $\mu_0$ equal to the mean of the true intrinsic opinions. Shaded regions are $95\%$ confidence intervals for $p$-values of the test across $5$ independent runs.}
    \label{fig:t_test}
\end{figure}
%                 \label{fig:enter-label}
%     \end{figure}
%     \label{fig:enter-label}
% \end{figure}

% {\bf Detection Problem.} If we have access to $G, (\alpha_i)_{i \in [n]}$ and some $\bz^\prime$, can we detect whether misreporting occurred? 


% The use of $L^\dagger$ as the covariance prior is motivated by graph signal processing~\cite{dong2016learning,leng2020learning}. The signal is assumed to lie in a mixture of the low and high frequency components, where the mixture is weighted by the inverse eigenvalues of the Laplacian. 

% Suppose we have an $\hat S \subset [n]$ and $s$ has a prior of $N(\mu, \Sigma)$. We can solve for $s^\prime$ from $z^\prime$ and then construct $\Tilde{T}, A, B$ from graph information. Then, recall that $\Tilde T x = \Tilde A \Tilde diag(B) s$. So, given that the true $x$ is just $s^\prime$ restricted to $S$, we can check if $\hat S$ is good or not by getting the posterior distribution of $x$ under this $\hat S$ (also a Gaussian) and then checking what the likelihood of $s^\prime \rvert_{\hat S}$ is under this distribution. Searching over all $\hat S$ and returning the argmax should give the corect $\hat S$. Once we have $\hat S = S$, we can immediately solve for $s \in \RR^n$ as well. 

% As a Corollary, if $S$ has $O(1)$ members then we have a polytime algorithm for recovering everything, with no features and no robust regression assumptions. If $S$ is large, then we believe that solving for the true $S$ should be NP hard in general, e.g. through reduction from Set Cover. Maybe we can show it fails to be submodular. 


% \begin{algorithm}[t]
% \SetAlgoLined
% \KwIn{Graph information $L$ and $A$, observed equilibrium $\bm {z}'$, set size $|S|$, and a prior $N(\bmu, \Sigma)$}
% \KwOut{Set of strategic agents $\hat S$, estimated intrisic beliefs $\hat \bs$.}

% Solve for $\bm{s}^\prime \in \RR^n$, i.e. $\bs^\prime = A^{-1} ((I - A) L + A) \bz^\prime$. 

% For every $\hat S \subseteq [n]$ with size $|S|$:
% \begin{itemize}
%     \item Let $\hat \bx \in \RR^{\abs{S}}$ be the restriction of $\bs^\prime$ to $\hat S$.
%     \item Construct $\Tilde{A}, \Tilde{B}, \Tilde{T} \in \RR^{\abs{S} \times \abs{S}}$. 
%     \item Let $\bm{u} = (\Tilde{A} \widetilde{\diag (B)})^{-1} \Tilde{T} \hat \bx$. 
%     \item Let $\hat \bs \in \RR^n$ be such that: 
%     \begin{align*}
%     \hat \bs_i &= \begin{cases}
%     \bu_i & i \in \hat S \\
%     \bs_i^\prime & i \not \in \hat S
%     \end{cases}
%     \end{align*}
%     \item Compute the Mahalonobis score 
%     $f(\hat S) = (\hat \bs - \bmu)^T \Sigma^{-1} (\hat \bs - \bmu)$. 
% \end{itemize}

% \Return{Return the $\hat S$ with the minimum Mahalonobis score.}
% \caption{Learning from Misreporting Equilibrium without Robust Regression}
% \label{alg:learning_no_robust_regression}
% \end{algorithm}


% Now, given any $\hat \bs$, we have: 
% \begin{align*}
% \log \mathcal{L}(\hat \bs \vert \bmu, \Sigma)
% &\propto -\frac{1}{2} (\hat \bs - \bmu)^T \Sigma^{-1} (\hat \bs - \bmu)
% \end{align*}


% % \begin{theorem}
% % For x \sim \mathcal{N}(\mu, \Sigma)$ and any point y, the probability that x achieves lower Mahalanobis distance than y approaches 1 as y moves away from \mu.
% % \end{theorem}

% % \begin{proof}
% \begin{lemma}
% Let $\hat S \neq S$ be such that $\hat S, S$ have the same size. Then, with probability blah, the Mahalonobis score of $\hat S$ is larger than that of $S$.
% \end{lemma}
% \ajcomment{This reduces to analysis of the $\Tilde{T}$ matrices. After that we can use subGamma concentration to prove that the hypothesis testing problem works. For a {\em polynomially large} hypothesis class it works.}

% \begin{proof}
% Consider any $\by \in \RR^n$. We analyze the distribution of $g(\by) := (\by - \bmu)^T \Sigma^{-1} (\by - \bmu)$ as follows. Let $\Sigma = U D U^T$ for orthogonal $U$ and $D \succ 0$. Then $g(\by) = \norm D^{-1/2}U^T (\by - \bmu)\norm_2^2$. If $\by \sim \mathcal{N}(\bmu^\prime, \Sigma)$ then $(\by - \bmu) \sim \mathcal{N}(\bmu^\prime - \bmu, \Sigma)$, so $\norm D^{-1/2}U^T (\by - \bmu)\norm_2^2 \sim N(D^{-1/2}U^T (\bmu^\prime - \bmu), D^{-1/2}U^T \Sigma (D^{-1/2}U^T)^T)$. This simplifies since $D^{-1/2}U^T \Sigma (D^{-1/2}U^T)^T = I$. 

% Now, suppose that $\hat S = S$. Then by definition, $\bu = \bs^\prime \vert_{S}$, so $\bs^\prime = \bs$. Therefore, $g(\bs^\prime) = N(\bm{0}, I)$. 

% On the other hand, suppose that $\hat S \neq S$. Since $\hat S$ is of the same size, there are some $i, j$ such that $i \in \hat S \setminus S$ and $j \in S \setminus \hat S$. 

% The true set of deviating opinions, restricted to $S$, is given as $(\bs^\prime) \vert_{S}$ is equal to 
% $\Tilde{T}^{-1} (\Tilde{A} \Tilde{ \textrm{diag}(B)}) \bs_{S}$. 

% Let $\bmu_S \in \RR^{\abs{S}}$ be the restriction of $\bmu$ to $S$ and similarly $\Sigma_S \in \RR^{S \times S}$ be the rows and columns belonging to $S$. Then $\bs^\prime \vert_{S} \sim (M \bmu_S, M \Sigma_S M^T)$ where $M = \Tilde{T}^{-1} (\Tilde{A} \Tilde{\textrm{diag}(B)})$. 

% Now, let $\bu(\hat S)$ be the $\bu$ corresponding to $\hat S$. Then $\bu = (\Tilde{A}(\hat S) \textrm{diag}(\Tilde{B}(\hat S)))^{-1} \Tilde{T}(\hat S)  (\bs^\prime)\vert_{\hat S}$. Notice that $\Tilde{A}(\hat S)$ is not the same as $\Tilde{A}$, which captures indices in $S$ rather than $\hat S$. 

% Therefore, we have that $\hat \bs(\hat S) \sim \mathcal{N}(\bmu^\prime, \Sigma^\prime)$. In particular, the $i^{th}$ entry is distributed as $\be_i^T \bu(\hat S) = \frac{1}{\alpha_i B_{ii}} \Tilde{T}(\hat S) (\bs^\prime)\vert_{\hat S}$. 
% \end{proof}


\subsection{Learning the Strategic Actors with Robust Regression} \label{sec:learning_regression}

The algorithm described in the previous section (\cref{alg:hypothesis_testing}) can be used to detect whether there exists manipulation in the network. However, the set $S$ is unknown, and therefore the platform cannot target the deviators to perform interventions to mitigate strategic behavior.  

It is, therefore, essential for the platform to be able to identify the set of deviators $S$, in case the platform needs to take regulatory actions. While at first, it may seem that finding the set of deviators $S$ is a hard task, it turns out that under mild assumptions on the intrinsic opinion formation process, we can learn the set of deviators $S$ from observing the strategically corrupted equilibrium $\bz^\prime$ via \cref{alg:learning} in polynomial time, described in \cref{alg:learning}. Our algorithm is based on robust regression leveraging the \textsc{Torrent} algorithm developed by \citet{torrent-2015} and requires access to a node embedding matrix $X \in \RR^{n \times d}$, and the size $|S|$ of the set of deviators. 

\begin{algorithm}[t]
\SetAlgoLined
\KwIn{Features $X \in \mathbb{R}^{n \times d}$, graph information $L$ and $A$, observed equilibrium $\bm {z}'$, set size $|S|$}
\KwOut{Set of strategic agents $\hat S$, estimated intrisic beliefs $\hat \bs$.}

% Step 1: Solve for s'
$\widehat{\bm{s}^\prime} \leftarrow A^{-1}((I-A)L + A)\bz^\prime$\;

% Step 2: Solve for v hat through robust regression
$\hat{v} \leftarrow $ Robust Regression (\textsc{Torrent}) with design matrix $X$, response vector $\widehat{\bm{s}^\prime}$\;

% Step 3: Compute s hat
$\hat \bs \leftarrow X\hat{\bv}$\;

% Step 4: Find differences and threshold
$\text{diffs} \leftarrow \abs{\hat \bs - \widehat{\bm{s}^\prime}}$\;

% Step 5: Return top |S| indices
$\hat S \leftarrow \text{indices of top } k \text{ largest values in diffs}$\;

\Return{$\hat \bs \in \RR^n, \hat S \subseteq [n]$}
\caption{Learning from Misreporting Equilibrium}
\label{alg:learning}
\end{algorithm}

The key idea of \cref{alg:learning} is that if the size of the strategic set $S$ is sufficiently small, in general, $|S| \le C n$ for some small constant $C$, then we can view the misreported intrinsic opinions $\bs^\prime$ as a {\em perturbation} of the truthful opinion vector $\bs$, and then use a robust regression algorithm to estimate $\bs$. We assume that the embedding matrix $X \in \RR^{n \times d}$ determines intrinsic beliefs: for example, demography, geographic location, etc. Node-level features can be learned by a variety of methods, such as spectral embeddings on the graph Laplacian or graph neural networks. Previous works have used the framework of combining a robust estimator with model-specific information to learn from ``strategic sources'' of data, such as in bandits~\cite{kapoor2019corruption}, controls~\cite{russo2023analysis}, and network formation games~\cite{jalan-chakrabarti-2024}. 

In the sequel, we give the precise technical condition of the features required for robust regression to work \citep{torrent-2015}, which is based on conditions on the minimum and maximum eigenvalues of the correlation matrix determined by the features corresponding to agents in $S$. Specifically, for a matrix $X \in \RR^{n \times d}$ with $n$ samples in $\RR^d$ and $S \subset [n]$ let $X_S \in \RR^{\abs{S} \times d}$ select rows in $S$. Note that $\lambda_{\min}(\cdot), \lambda_{\max}(\cdot)$ are the min/max eigenvalues respectively.

\begin{definition}[SSC and SSS Conditions]
Let $\gamma \in (0,1)$. The features matrix $X \in \RR^{n \times d}$ satisfies the Subset Strong Convexity Property at level $1 - \gamma$ and Subset Strong Smoothness Property at level $\gamma$ with constants $\xi_{1-\gamma}, \Xi_\gamma$ respectively if: 
\begin{align*}
\xi_{1-\gamma} &\leq 
\min\limits_{S \subset [n]: \abs{S} = (1-\gamma) n} \lambda_{\min}(X_S^T X_S), \\
\Xi_{\gamma} &\geq \max\limits_{S \subset [n]: \abs{S} = \gamma n} \lambda_{\max}(X_S^T X_S).
\end{align*}
\label{defn:ssc}
\end{definition}

\noindent We give our guarantee for \cref{alg:learning}.

\begin{proposition}
Let $X$ be as in Algorithm~\ref{alg:learning}, and suppose that $X \bv = \bs$ for some $\bv \in \RR^d$, and that $X$ satisfies the SSC condition at level $1 - \gamma$ with constant $\xi_{1-\gamma}$, and SSS condition at level $\gamma$ with constant $\Xi_\gamma$ (Definition~\ref{defn:ssc}). Then, there exist absolute constants $C, C^\prime > 0$ such that if $\abs{S} \leq Cn$ and $4\frac{\sqrt{\Xi_{\gamma}}}{\sqrt{\xi_{1-\gamma}}} < 1$, \cref{alg:learning} returns $\hat \bs$ such that: 
\begin{align*}
\norm \hat \bs - \bs \norm_2 \leq \norm X \norm_2 n^{-\omega(1)},
\end{align*}

using $T = C^\prime (\log n)^{2}$ iterations of \textsc{Torrent} for the Robust Regression step. Moreover, if for all $j \in S$ we have $\abs{\bs_j - \bs_j^\prime} \gg \norm X \norm_2 n^{-\omega(1)}$, then $\hat S = S$. 
\label{prop:b-recovery}
\end{proposition}
 
\noindent For the proof, we first state the technical result of \citet{torrent-2015} that we require. 

\begin{thrm}[\citet{torrent-2015}]
Let $X \in \RR^{n \times d}$ be a 
design matrix and $C > 0$ an absolute constant. Let $\bm b \in \{ 0, 1 \}^n$ be a corruption vector with $\norm \bm b \norm_0 \leq \alpha n$, for $\alpha \leq C$. Let $\bm{y} = X\bw^* + \bm{r}$ be the observed responses, and $\gamma \ge \alpha$ be the active set threshold given to the Algorithm 2 of \citet{torrent-2015}. Suppose $X$ satisfies the SSC property at level $1 - \gamma$ and SSS property at level $\gamma$, with constants $\xi_{1-\gamma}$ and and $\Xi_\gamma$ respectively. If the data $(X, \bm{y})$ are such that $\frac{4\sqrt{\Xi_\gamma}}{\sqrt{\xi_{1 - \gamma}}} < 1$, 
then after $t$ iterations, Algorithm 2 of \citet{torrent-2015} with active set threshold $\zeta \ge \gamma$ obtains a solution $\bw^T \in \RR^d$ such that, for large enough $n$, 
\begin{align*}
\norm \bw^T - \bw^* \norm_2 &\leq \frac{\norm \bm{r} \norm_2}{\sqrt{n}} \exp(-cT).
\end{align*}
\label{thrm:torrent}
\end{thrm}

\noindent We are ready to prove Proposition~\ref{prop:b-recovery}. 


\begin{proof}[Proof of Proposition~\ref{prop:b-recovery}]
Let $\widehat{\bm{s}^\prime}$ be as in Algorithm~\ref{alg:learning}, and $\bm{y} = \widehat{\bm{s}^\prime}$. Notice $\bm{y} = \bm{s} + (\bm{s}^\prime - \bs) + (\widehat{\bm{s}^\prime} - \bs^\prime)$. Let $\bw := (\bm{s}^\prime - \bs)$ be the corruption vector due to strategic negotiations and $\bm{r} =  (\widehat{\bm{s}^\prime} - \bs^\prime)$ be the residual vector due to least-squares regression. We claim that $\bm{r} = \bm{0}$, because $A^{-1}$ is full rank and $((I - A) L + A)$ is full rank, so $\widehat{\bm{s}^\prime} = A^{-1} ((I - A) L + A) \bz^\prime = \bs^\prime$. 

Next, we apply Theorem ~\ref{thrm:torrent}. Notice that $\norm \bw \norm_0 \leq Cn$ by assumption. Moreover, $X$ satisfies the SSC and SSS coniditons. Therefore, after $T$ iterations, Algorithm~\ref{alg:learning} obtains $\hat \bs$ such that: 
\[
\norm \hat \bv - \bv \norm_2 \leq \frac{\exp(-cT)}{\sqrt{n}} \norm \bs^\prime - \bs \norm_2.
\]
Therefore, letting $T = C^\prime (\log n)^2$ for large enough constant $C^\prime > 0$, we see that $\norm \hat \bv - \bv \norm_2 \leq O(n^{-\omega(1)})$. Hence $\norm \hat \bs - \bs \norm_2 = \norm X \hat \bv - X \bv \norm_2 \leq \norm X \norm_2 \cdot n^{-\omega(1)}$. Now, let $\bm{u} = \hat \bs - \bs^\prime$. If $i \in [n] \setminus S$, then $\abs{\bm{u}_i} \leq \norm X \norm_2 \cdot n^{-\omega(1)}$. On the other hand for $j \in S$, $\abs{\bm{u}_{j}} \geq \abs{\bm{s}_j - \bm{s}_j^\prime} - \norm X \norm_2 n^{-\omega(1)}$. Therefore the top-$|S|$ entries of $\bm{u}$ recover $S$. 
\end{proof}

It is interesting to investigate what an upper bound on the size of $S$ is when nodes have community memberships, such that recovery is possible, as the SSC and SSS conditions determine it. Specifically, we show the following for a blockmodel graph (proof deferred in the Appendix). 

\begin{prop} \label{prop:blockmodel}
    If $G$ has two communities with $n_1$ and $n_2$ nodes respectively such that $n_1 \ge n_2 \ge 1$, and $X \in \{ 0, 1 \}^{n \times 2}$ is an embedding vector where each row $\bx_i$ corresponds to a one-hot vector for the community of node $i$, then \cref{alg:learning} can recover $S$ perfectly as long as $|S| < \frac {n_1} {17}$.
\end{prop}

For instance, when \cref{prop:blockmodel} is applied to the Polblogs dataset, it shows that $S$ can be fully recovered as long as $|S| \le 9$. We can obtain a slightly worse bound and extend the result to a blockmodel graph with $K$ communities (proof in the Appendix). 

\begin{prop} \label{prop:blockmodel_k}
    If $G$ has $K \ge 2$ communities with sizes $n_1 \ge n_2 \ge \dots \ge n_K$  with $\left ( \frac {16K} {16K + 1} \right ) \frac n K < n_K \le \frac n K$ and $X \in \{ 0, 1 \}^{n \times K}$ is an embedding vector where $\bx_i$ corresponds to an one-hot encoding of the community membership, then \cref{alg:learning} can recover $S$ perfectly as long as $|S| < \frac {1} {16K + 1} n$.  
\end{prop}

In detail, \cref{prop:blockmodel_k} states that as long as the smallest community of the graph has size $\Theta (n/K)$ then the recovery of a set of size $|S| = O(n/K)$ is possible. If $\abs{S} \gg n/K$ and $S$ contains all members of the smallest community, then robust regression can fail. 

We show that in real-world datasets, \cref{alg:learning} can identify the set of deviators with high accuracy (cf. \cref{fig:learn_S}). Specifically, in the real-world datasets we take $S$ to be a randomly sampled set of size $\lceil pn \rceil$ for $p \in \{ 0.05, 0.1, 0.15, 0.2 \}$ and the embeddings to be 128-dimenaional Node2Vec embeddings for Twitter and Reddit and community membership embeddings for the Pollblogs dataset. \cref{alg:learning} achieves low recovery error as well as high balanced accuracy score.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/learn_S.pdf}
    \caption{Reconstruction error and balanced accuracy for the robust regression problem presented in \cref{alg:learning}. The $x$-axis shows the percentage of strategic agents. The left subfigure shows the recovery error, measured as $\frac 1 n \sum_{i \in [n]} \left  | \frac {\hat \bs_i - \bs_i} {\bs_i} \right |$, and the right subfigure measures the balanced accuracy between the recovered $\hat S$ and the true $S$. To construct the confidence intervals, for each size $|S|$ of the set $S$, we draw $S$ five times randomly from the vertex set $[n]$. For the Twitter and Reddit datasets, we have used 128-dimensional Node2vec embeddings. For the Polblogs dataset we have used the community membership (which corresponds to the political orientation) of each node, such that $\bx_i = (1, 0)$ corresponds to liberal and $\bx_i = (0, 1)$ corresponds to conservative, and (the true) $\bs$ is such that $\bs = X \bv$ for $\bv = (1, -1)^T$. We have also provided results using 128-dimensional spectral embeddings. We have set the recovery threshold for \textsc{Torrent} to be $|S|/n$, and the step size to be $\eta = 1 / \| \bar X \|_2^2$ where $\bar X$ is the min-max normalized embedding matrix.}
    \label{fig:learn_S}
\end{figure}





% \subsection{Learning without robust regression}


% Consider any $f(\hat S) = (\hat \bs - \bmu)^T \Sigma^{-1} (\hat \bs - \bmu)$. 
% Let $f(z) = (z-\mu)^T\Sigma^{-1}(z-\mu)$ be the Mahalanobis distance.

% 1. For $x \sim \mathcal{N}(\mu, \Sigma), d(x) \sim \chi^2_n$ where n is the dimension.

% 2. For fixed y, $d(y) = \lambda > 0$ is constant.

% 3. We want $P(d(x) < d(y)) = P(\chi^2_n < \lambda)$

% 4. By properties of chi-squared distribution:
%    $P(\chi^2_n > t) \leq 2e^{-t/4}$ for $t > 2n$

% 5. Therefore:
%    $P(d(x) > \lambda) \leq 2e^{-\lambda/4} \text{ for } \lambda > 2n$

% 6. Thus:
%    $P(d(x) < d(y)) \geq 1 - 2e^{-\lambda/4} \to 1 \text{ as } \lambda \to \infty$
% % \end{proof}