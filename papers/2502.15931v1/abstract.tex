Opinion dynamics model how the publicly expressed opinions of users in a social network coevolve according to their neighbors as well as their own intrinsic opinion. Motivated by the real-world manipulation of social networks during the 2016 US elections and the 2019 Hong Kong protests, a growing body of work models the effects of a {\em strategic actor} who interferes with the network to induce disagreement or polarization. We lift the assumption of a single strategic actor by introducing a model in which {\em any subset} of network users can manipulate network outcomes. They do so by acting according to a fictitious intrinsic opinion. Strategic actors can have {\em conflicting goals}, and push competing narratives. We characterize the Nash Equilibrium of the resulting meta-game played by the strategic actors. Experiments on real-world social network datasets from Twitter, Reddit, and Political Blogs show that strategic agents can significantly increase polarization and disagreement, as well as increase the ``cost'' of the equilibrium. To this end, we give worst-case upper bounds on the Price of Misreporting (analogous to the Price of Anarchy).  Finally, we give efficient learning algorithms for the platform to (i) detect whether strategic manipulation has occurred, and (ii) learn who the strategic actors are. Our algorithms are accurate on the same real-world datasets, suggesting how platforms can take steps to mitigate the effects of strategic behavior.

% Opinion dynamics model how the publicly expressed opinions of users in a social network coevolve according to their neighbors as well as their own intrinsic opinion. Motivated by the real-world manipulation of social networks during the 2016 US elections and the 2019 Hong Kong protests, a growing body of work models the effects of a *strategic actor* who interferes with the network to induce disagreement or polarization. We lift the assumption of a single strategic actor by introducing a model in which *any subset* of network users can manipulate network outcomes. They do so by acting according to a fictitious intrinsic opinion. Strategic actors can have *conflicting goals*, and push competing narratives. We characterize the Nash Equilibrium of the resulting meta-game played by the strategic actors. Experiments on real-world social network datasets from Twitter, Reddit, and Political Blogs show that strategic agents can significantly increase polarization and disagreement, as well as increase the "cost" of the equilibrium. To this end, we give worst-case upper bounds on the Price of Misreporting (analogous to the Price of Anarchy).  Finally, we give an efficient learning algorithm for the platform to (i) detect whether strategic manipulation has occurred, and (ii) learn who the strategic actors are. Our algorithm is accurate on the same real-world datasets, suggesting how platforms can take steps to mitigate the effects of strategic behavior.