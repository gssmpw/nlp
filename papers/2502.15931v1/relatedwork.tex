\section{Related Work}
\paragraph{Opinion Dynamics} Opinion dynamics are well-studied in computer science and economics, as well as sociology, political science, and related fields. There have been many models proposed for opinion dynamics, such as with network interactions as we study in this paper (FJ model) \citep{Friedkin1990,Bindel2015}, bounded confidence dynamics (Hegselman-Krausse Model) \citep{hegselmann2002opinion}, coevolutionary dynamics \citep{bhawalkar2013} as well as many variants of them; see, for example \citet{abebe2018opinion,hazla2019geometric,fotakis2016opinion,fotakis2023opinion,tsourakakis-2024}. The work of \citep{bindel2011} shows bounds on the Price of Anarchy (PoA) between the PSNE and the welfare-optimal solution for the FJ model, and the subsequent work of \cite{bhawalkar2013} shows PoA bounds for the coevolutionary dynamics. Additionally, the opinion dynamics have been modeled by the control community; see, for example, \citep{nedic2012,de2022,bhattacharyya2013convergence,chazelle2011total}. 

As in these works, we treat the FJ model as a basis. However, our work is significantly different as it studies a framework where any subset $S \subseteq [n]$ of strategic agents can {\em deviate} from their truthful intrinsic opinions, as opposed to studying the evolution of the expressed opinions and their PSNE in the FJ model. In our model, each strategic agent $i \in S$ can only choose a single entry $\bs_i^\prime$ of the overall deviation $\bs^\prime$, but pays a cost based on the resulting equilibrium ($\bz^\prime = B \bs^\prime$), which depends on the choices of other members of $S$. 

\paragraph{Disagreement and Polarization in Social Networks} Motivated by real-world manipulation of social networks in, e.g., the 2016 US election, a recent line of work studies polarization and strategic behavior in opinion dynamics \cite{gaitonde2020adversarial,gaitonde2021polarization,Chen2022,wang2024relationship,tsourakakis-2024,ristache2025countering}. 
\citet{Chen2022} consider a model in which an adversary can control $k \leq n$ nodes' internal opinions and seeks to maximize polarization at equilibrium. Similarly, \citet{gaitonde2020adversarial} considers a single adversary who can modify intrinsic opinions $\bm{s}$ belonging to an $\ell_2$-ball. More recent work also studies modification of agents' susceptibility parameters $\alpha_i$ to alter the median opinion at equilibrium~\cite{ristache2025countering}. By contrast, we study a setting in which any subset $S \subseteq [n]$ can be strategic. Unlike previous works, these ``adversaries'' can have conflicting goals in our model.

% \ajcomment{Write more here. Real world examples of multiple adversaries?}

\paragraph{Manipulation of Dynamic Games.} Opinion dynamics are a widely studied instance of a network game, which is a game played by nodes in a network with payoffs depending on the actions of their neighbors~\cite{kearns2001graphical, tardos-2004}. In addition to the manipulation of opinion dynamics, researchers have studied strategic manipulation of financial network formation~\cite{jalan-chakrabarti-2024}. In the non-network setting, researchers have studied the manipulation of recommendation systmes from a game-theoretic perspective~\cite{ben2018game}, as well as security games~\cite{nguyen2019deception}, repeated auctions~\cite{kolumbus2022auctions} and Fisher markers with linear utilities~\cite{kolumbus2023asynchronous}. 

% \blue{Cite more work here?}
% Moreover, works in multi-agent learning consider optimal play against a learning algorithm~\cite{deng-2019, camara2020mechanisms, assos2024maximizing}, but only a single agent against a single learner. By contrast we study strategic behavior by 
% in network games, in which an agent must consider not only their neighbors, but the neighbors of their neighbors, and so on. 

% \paragraph{Anomaly Detection in Graphs.} Our work is also related to the anomaly detection literature in networks. One work closely related to ours is the work of \citet{chen2022antibenford} where the authors want to detect Anti-Benford subgraphs in a large transaction or financial graph. The Anti-Benford subgraphs consist of a set of nodes that perform many transactions that significantly deviate from Benford's law, and the authors develop a hypothesis test to detect such graphs. Similarly, the work of \citet{agarwal2020chisel} proposes a framework based on a chi-squared statistic to perform a graph similarity search.  In our paper, similarly, we develop a robust regression and a hypothesis testing algorithm that is able to detect nodes that are strategic and misreport their opinions. Additionally, our work assumes the strategic behavior of the agents, whereas the works of \citep{agarwal2020chisel,chen2022antibenford} do not. 

% In a similar spirit, the work of \citet{jalan-chakrabarti-2024} studies financial networks, whereas a subset of strategic agents has incentives to misreport their own local network connections in order to obtain higher utility, and develop algorithms that can identify the set of such agents. Our paper works in a similar flavor, however, in a significantly different application domain and context, which corresponds to social networks.


\paragraph{Learning from Strategic Data.} We develop learning algorithms which observe the (possibly manipulated) equilibrium $\bz^\prime$ to detect if manipulation occurred, and if so who was responsible. The former problem relates to anomaly detection in networks. \citet{chen2022antibenford} develop a hypothesis test to detect such fraud in financial transaction neteworks, by testing if certain subgraphs deviate from Benford's Law. Similarly, \citet{agarwal2020chisel} propose a framework based on a $\chi^2$-statistic to perform graph similarity search. 

The problem of recovering the set of deviators relates to the broader literature of learning from observations of network games. Most works give learning algorithms for games {\em without} manipulation~\cite{irfan-2014,garg-2016,de2016learning,leng-2020-learning,rossi2022learning,jalan-2023}. But our data $\bz^\prime$ can be a manipulated equilibrium, which is a {\em strategic sources} of data~\cite{zampetakis2020statistics}. Learning algorithms for strategic sources are known for certain settings such as linear classifiers with small-deviation assumptions~\citep{chen2020learning}, or binary classifiers in a linear reward model~\citep{harris2023strategic}. When agents can modify their features to fool a known algorithm, even strategy-robust classifiers such as \cite{hardt2016strategic} can be inaccurate~\cite{ghalme2021strategic}.  
Since agents can deviate arbitrarily in our model, we use a robust regression method with guarantees against {\em adversarial} corruptions~\citep{torrent-2015}, similar to the learning algorithms in~\citep{kapoor2019corruption,russo2023analysis}. The work of \citet{jalan-chakrabarti-2024} studies learning from financial networks with strategic manipulations, which is in a similar spirit to our work but differs significantly in the application domain and context. 

% Our setting is closest to that of \citet{jalan-chakrabarti-2024}, who give 

% .  A growing body of work studies learning from ``strategic sources'' of data such as $\bz^\prime$. 

%  \citet{chen2020learning} study strategy-awareness for linear classification, but assume that agents can only misreport data up to an $\eps$-ball. Our setting is closer to that of \citet{ghalme2021strategic}, who show that agents who are evaluated by a third-party classifier (e.g. for approval for a bank loan) can strategically modify their features to game the classifier, even if the classifier used is strategy-robust in the sense of \cite{hardt2016strategic}. 
% \cite{harris2023strategic} give a $\Tilde{O}(n^{(d+1)/(d+2)})$-regret algorithm for online binary classification against $n$ strategic agents with $d$-dimensional features, but in a linear reward model. Finally, \cite{daskalakis-2015} give a mechanism to encourage strategic data providers to report truthful data, but in a model where the data providers have no incentive to hurt the classifier's accuracy (e.g. crowdsourcing).

% \paragraph{Network Games.} Network games, also known as graphical games, involve $n$ agents whose payoffs are influenced by the actions of their neighbors~\cite{littman2001efficient, roughgarden2004bounding,roughgarden2011local}. A substantial body of research examines learning from observations of such games~\cite{irfan2018causal,garg2016learning,rossi2022learning,leng2020learning}. These studies typically focus on games with finite or one-dimensional action spaces, whereas in our model, each of the $n$ agents operates in an $n$-dimensional action space.

% Furthermore, another area of research explores influencing the outcomes of network games. Most of these studies address games with one-dimensional action spaces and a single strategic actor~\cite{galeotti2020targeting,gaitonde2020adversarial,gaitonde2021polarization, wang2024relationship}. In contrast, we consider arbitrary sets of strategic actors. Recent works have also examined scenarios with multiple strategic actors, such as in repeated auctions~\cite{kolumbus2022auctions} and Fisher markets with linear utilities~\cite{kolumbus2023asynchronous}. While our work shares a similar motivation, it centers on opinion formation.

% Our work is broadly concerned with strategic considerations in machine learning systems, and is at the intersection of growing fields such as artificial intelligence, algorithmic game theory, and multi-agent learning. 

% {\bf Network games with strategic behavior and learning.} Network games, or graphical games, involve $n$ agents whose payoffs depend on the actions of their neighboring agents~\cite{kearns2001graphical, tardos-2004}. A large body of work studies learning from observations of network games~\cite{irfan-2014,garg-2016,de2016learning,leng-2020-learning,rossi2022learning}. All of these works study games with finite or one-dimensional action spaces, whereas each of the $n$ agents in our model has an $n$-dimensional action space. This results in a vector of contracts $\bm{w}_k \in \RR^n$ for each agent $k$. The correlations between entries of $\bm{w}_k$ can have strategic consequences, as we show in Section~\ref{sec:welfare}. 


% {\bf Learning from strategic sources.} 
% %Agents in our model wish to estimate other agents' beliefs from observing the stable networks generated by strategic negotiations (Section~\ref{sec:learning}).  
% \cite{chen2020learning} study strategy-awareness for linear classification, but assume that agents can only misreport data up to an $\eps$-ball. Our setting is closer to that of \cite{ghalme2021strategic}, who show that agents who are evaluated by a third-party classifier (e.g. for approval for a bank loan) can strategically modify their features to game the classifier, even if the classifier used is strategy-robust in the sense of \cite{hardt2016strategic}. 
% \cite{harris2023strategic} give a $\Tilde{O}(n^{(d+1)/(d+2)})$-regret algorithm for online binary classification against $n$ strategic agents with $d$-dimensional features, but in a linear reward model. Finally, \cite{daskalakis-2015} give a mechanism to encourage strategic data providers to report truthful data, but in a model where the data providers have no incentive to hurt the classifier's accuracy (e.g. crowdsourcing).
%\blue{a few more cites about mechanism design for strategy aware statistics?}

% {\bf Multi-agent learning.} In terms of the ``five agendas'' of multi-agent learning \cite{shoham2007if}, we consider our work closest to the ``Prescriptive, non-cooperative'' agenda. The goal of this program is to design agents that are optimal for environments consisting of other learning agents. Several works study the problem of optimal play against a learning algorithm \cite{deng-2019, camara2020mechanisms, assos2024maximizing}, but each of these considers a single player against a single learner. Our work is more related to strategic behavior in network games, in which an agent must consider not only their neighbors, but the neighbors of their neighbors, and so on. 



% \mpcomment{I added the refs, but it may need more writing to make sure it looks different}

% \ajcomment{Add refs from old paper. Emphasize Nisan-Kolumbus paper on how to manipulate your learning algorithm.}