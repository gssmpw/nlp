% !TEX root = ./main.tex

% \section{Introduction}

% Over the past decade, social media has experienced rapid growth in both usage and significance. Online social networks, which allow users to share updates about their lives and opinions with a broad audience instantaneously, are now utilized by billions of people globally. These platforms serve various purposes, such as being informed about politics, news, health-related updates, products, and many more \citep{backstrom2012four,young2006diffusion,banerjee2013diffusion,shearer2021news}. 

% Unfortunately, networks can induce polarization as the network connections serve as a pathway for polarization to increase \cite{musco2018minimizing,chen2021adversarial,wang2024relationship,gaitonde2020adversarial}. This is a well-studied sociological phenomenon called the \textit{filter-bubble theory} \citep{pariser2011filter}. The filter-bubble theory argues that personalized algorithms used by online platforms, such as search engines and social media, selectively display content that aligns with a user's past behaviors, preferences, and beliefs. This customization creates an \textit{``invisible algorithmic editing''} of the web, isolating individuals within their own ideological bubbles where they encounter only information that reinforces their existing views. As a result, users are less likely to be exposed to diverse perspectives, potentially narrowing their worldview and fostering polarization. \citet{pariser2011filter} warns that such bubbles undermine democratic discourse by limiting opportunities for individuals to engage with challenging or unfamiliar ideas.
% \ajcomment{We might want to de-emphasize polarization if we go with a different framing around economics and market competition. For now I like this though.}

% Additionally, social networks can be manipulated by external entities in order to create discord and cause disagreement. For instance,  the 2017 indictment of the Russian Internet Research Agency (IRA) by the U.S. Department of Justice Special Counsel’s Office alleged that the IRA leveraged social media and targeted advertising to achieve \textit{``a strategic goal to sow discord in the U.S. political system, including the 2016 U.S. presidential election''} \citep{mueller2018united}. In 2019, \citet{twitter2019hongkong} disclosed that coordinated external bots attempted to induce discord in Hong Kong to hinder protesters’ ability to organize effectively during the independence movement. As social media continues to proliferate, it is likely that these types of external interferences will become increasingly common. Additionally, networks of Facebook pages have targeted Americans with sports betting scams, amplifying their reach by disseminating provocative conspiracy theories about political figures and natural disasters \citep{wired2024profiteers}. These schemes leverage the economics of the internet, where engagement with inflammatory content is monetized, and social media algorithms inadvertently amplify such content, enabling bad actors to exploit audiences for profit.

% To model the opinions' evolution, computer scientists, sociologists, and statisticians have relied on the framework of \emph{opinion dynamics} where the users' opinions coevolve according to a weighted network $G = (V, E, w)$, and each user updates their opinion as a combination of their own intrinsic opinion as well as the opinions of their neighbors \citep{Friedkin1990}. This model of opinion exchange has the advantage of taking into account both network interactions and their own intrinsic opinion. So far, all of the existing works consider a single actor who has the ability to act on the network to induce disagreement or polarization \cite{musco2018minimizing,chen2021adversarial,wang2024relationship,tsourakakis-2024,gaitonde2020adversarial,racz2023towards,Chitra2020}.  

% In this work, we lift the assumption of requiring a single actor (such as the platform) to act as an adversary to induce polarization or disagreement and consider the case of several decentralized actors. It is known that empirically, a very small percentage ($25\%$) of the users in a network need to disagree to sway consensus \citep{centola2018experimental}.  In this paper, we attempt to provide a theoretical basis for this phenomenon: Specifically, in our setting, we assume that there is a set $S \subseteq V$ of strategic agents whose goal is to report false intrinsic opinions ($s'$) that are different from their true intrinsic opinions ($s \neq s'$)  such that they influence others while not deviating much from their neighbors, namely they want to reach an equilibrium where their neighbors agree with them. For instance, assume a social network where a set of $S$ of political actors want the network to believe that their stance on a topic (e.g., abortion, elections, drug legalization, etc.) is the best. They achieve this by adversarially reporting different intrinsic opinions. This ensures that their influence is both persuasive and credible within the local network context. Such adversarial behavior can result in significantly different (cf. \cref{fig:psne_data}) and highly polarized equilibria, where the strategic agents' opinions appear dominant despite not reflecting the actual intrinsic views of the majority (cf. \cref{fig:ratios_susceptibility_to_persuation}). 

% Our work investigates the conditions under which these strategic manipulations are successful, the extent of their impact on network-wide opinion dynamics, and potential interventions to mitigate their influence. This corresponds to learning the set of strategic actors. 

% \subsection{Our Contribution}

% In this paper, we ask the following research question (RQ): 

% \begin{quote}
%     \textit{\textbf{(RQ)} What if a set of strategic actors with \textbf{possibly conflicting goals} tries to manipulate the consensus by strategically reporting beliefs different than their true beliefs?}
% \end{quote}
 

% We rely on the Friedkin-Johnsen (FJ) model \citep{Friedkin1990}, where the opinions of agents coevolve via the help of a weighted undirected network $G = (V = [n], E, w)$ with non-negative weights. According to the FJ model, the agents possess intrinsic opinions $s$ and express opinions $z$ which they update via the following rule for each agent $i$:  

% \begin{equation}
%     \bz_i(t + 1) = \frac {\alpha_i \bs_i + (1 - \alpha_i) \sum_{i \sim j} w_{ij} \bz_j(t)} {1 + \sum_{i \sim j} w_{ij}}.
% \end{equation}

% where $\alpha_i \in (0, 1)$ is $i$'s susceptibility to persuation \citep{abebe2018opinion}. We additionally define $\Tilde \alpha_i = \alpha_i / (1 - \alpha_i)$ to be the regularization parameter corresponding to $i$. This update rule corresponds to the best-response dynamics arising from minimizing the quadratic cost function for each $i$ \citep{bindel2011,abebe2018opinion}:

% \begin{equation} \label{eq:cost_fcn}
%     c_i(\bz_i, \bz_{-i}) = (1 - \alpha_i) \sum_{i \sim j} w_{ij} (\bz_i - \bz_j)^2 + \alpha_i (\bz_i - \bs_i)^2
% \end{equation}

% The PSNE can be written as $z = ((I - A)L + A)^{-1} A \bs = B \bs$ where $L$ is the Laplacian of graph $G$, $A = \mathrm{diag}(\alpha_1, \dots, \alpha_n)$ is the diagonal matrix of susceptibilities. When an external \textit{single actor aims to induce disagreement or polarization} -- see, e.g., \citet{gaitonde2020adversarial,racz2022towards,musco2018minimizing} -- the adversary is coined with optimizing the objective function 

% \begin{equation}
%     \sum_{i \in [n]} c_i(\bz_i, \bz_{-i}) = \bs^T ((I - A)L + A)^{-1} A f(L) ((I - A)L + A)^{-1} A \bs,
% \end{equation}

% where $f(L)$ is a function of the Laplacian of $G$, either with optimizing towards $\bs$ \citep{gaitonde2020adversarial}, or the graph itself \citep{musco2018minimizing,racz2022towards}. 

% Usually, as we also discussed earlier, many adverse actions on social networks come from \textit{several independent strategic adversaries} who try to manipulate the network by infiltrating intrinsic opinions $\bs_i^\prime$, which are different from their true stances $\bs_i$ but are simultaneously close to $\bs_i$. Unlike previous works, these ``adversaries'' can have conflicting goals. 

% Concretely, the true opinions of the agents are $\bs_1, \dots, \bs_n$, and there is a set $S$ of deviating agents who report $\{ \bs_i^\prime \}_{i \in S}$, the goal of the strategic agents is to minimize the cost function of \cref{eq:cost_fcn} at consensus $z' = ((I - A)L + A)^{-1} A \bs^\prime$ where $\bs^\prime$ is the vector which has entries $\bs_i$ for all $i \notin S$ and $\bs_i^\prime$ for all $i \in S$. The local optimization of agent $i$ becomes:

% \begin{equation} \label{eq:strategic_cost}
%     \min_{\bs_i^\prime \in \RR} c_i \left (z' = ((I - A)L + A)^{-1} A \bs^\prime \right )
% \end{equation}

% Our contributions consider the following: 

% \begin{itemize}
%     \item We characterize the Nash Equilibrium of the game defined by \cref{eq:strategic_cost}. Specifically, we show that the game has a Pure Strategy Nash Equilibrium (PSNE) that is given by solving a constrained linear system. Given the PSNE of the game, we characterize the actors who can have the most influence in strategically manipulating the network. In the case of one deviator ($|S| = 1$) we show that the optimal strategic opinion $\bs_i^\prime$ is a linear function of the true opinion $\bs_i$ and the effect size depends on the diagonal entries of the equilibrium matrix $B$, and the bias depends on the equilibrium matrix, the vector of internal opinions $\bs$ the link weights of edges adjacent to $i$, and the susceptibility parameters of the agents.  
%     \item We apply our framework to real-world social network data from Twitter and Reddit \citep{Chitra2020}, and data from the Political Blogs (polblogs) dataset \citep{adamic2005political}. We find that the influence of strategic agents can be rather significant as strategic agents can significantly increase polarization and disagreement, as well as increase the overall ``cost'' of the consensus.  To address this, we give worst-case upper bounds on the \textit{Price of Misreporting} (PoM), which is analogous to the well-studied Price of Anarchy bounds (see, for example, \citet{bhawalkar2013,roughgarden2011local}) and suggest ways that the platform can be used to mitigate the effect of strategic behavior on their network. 
%     \item We give an efficient learning algorithm for the platform to infer the set of strategic agents $S$ from observing their publicly reported opinions $\bz^\prime$ as long as the size of $S$ is sufficiently small. Our algorithm is inspired by the robust regression algorithm of \citet{torrent-2015}, and is practical for real-world networks: \textit{(i)} requires the platform to have access to node embeddings $X$ which have been shown computable even in billion-scale networks such as Twitter \citep{el2022twhin}, and \textit{(ii)} can be computed in polynomial time in the number of nodes $n$ and edges $m$ of the network.  We apply our algorithm to the real-world datasets from Twitter, Reddit, and Polblogs and show that our algorithm can recover the strategic set $S$ each time with high accuracy. Also, we give sufficient conditions for the maximum size of $S$ when the embeddings of the nodes correspond to a stochastic block model with $K$ communities, and show that as long as the smallest community is of size $\Theta (n/K)$ recovery of $S$ is possible as long as $|S| = O(n/K)$. 
%     \item \mpcomment{In the sequel, we lift the need for node embeddings and provide a general algorithm that requires only graph information, and the publicly reported opinions $\bz^\prime$ can recover $|S|$ as long as $|S| \le blah$. Contrary to the previously mentioned robust regression algorithm, this algorithm is based on performing a goodness-of-fit test and assumes prior information on the vector of intrinsic opinions $\bs$.}=
% \end{itemize}


% \subsection{Preliminaries and Notations}

% The set $[n]$ denotes $\{ 1, \dots, n \}$. $\| \bx \|_p$ denotes the $\ell_p$-norm of vector $\bx$. $X \succeq 0$ denotes that the matrix $X$ is positive semi-definite and $\| X \|$ corresponds to the norm of $X \succeq 0$ which is its largest singular value. The Laplacian of the graph $G$ is denoted by $L = D - W$ where $W$ is the weight matrix of the graph, which has entries $w_{ij} \ge 0$, and $D$ is the diagonal degree matrix with diagonal entries $D_{ii} = \sum_{i \sim j} w_{ij}$. The Laplacian has eigenvalues $0 = \lambda_1 \le \lambda_2 \le \dots \le \lambda_n$. For any undirected and connected graph $G$, we can write the eigendecomposition of $L$ as 

% \begin{equation}
%     L = \sum_{i \in [n]} \lambda_i \bu_i \bu_i^T \succeq 0,
% \end{equation}

% where $\bu_1, \dots, \bu_n$ are orthonormal eigenvectors, with $\bu_1 = (1 / \sqrt n) \one$, where $\one$ is the column vector of all 1s. $U$ denotes the matrix which has the eigenvectors of $L$ as columns; i.e., such that $L = U^T \Lambda U$ where $\Lambda = \mathrm{diag} (\lambda_1, \dots, \lambda_n)$ is the diagonal matrix of $L$'s eigenvalues. $L_i$ denotes the $i$-restricted Laplacian which corresponds to the Laplacian of the graph with all edges that are non-adjacent to $i$ being removed, and, similarly, $L_{\{ u, v \}}$ corresponds to the Laplacian of an edge $\{ u, v\}$. Note that $L_i = \sum_{i \sim j} L_{\{ i, j \}}$. For a function $f(L)$ of the Laplacian we write $f(L) = U^T f(\Lambda) U$ where $f(\Lambda) = \mathrm{diag} (f(\lambda_1), \dots, f(\lambda_n))$. For brevity, regarding the equilibrium $z$ of the FJ model, we write $B =((I - A)L + A)^{-1} A$, such that $\bz = B \bs$ and $\bz^\prime = B \bs^\prime$. $\be_i$ denotes the $i$-th basis column vector. We define the total cost of an equilibrium $z$ to be 

% \begin{equation}
%     C(\bz) = \sum_{i \in [n]} c_i(\bz).
% \end{equation}

% We define the platform-wide metrics to be 

% \begin{align}
%     \mathcal P(\bz) & = \sum_{i \in [n]} (\bz_i - \bar z)^2, \qquad \bar z = \frac 1 n \sum_{i \in [n]} \bz_i, \\
%     \mathcal D(\bz) & = \sum_{i, j \in [n]} w_{ij} (\bz_i - \bz_j)^2 = \bz^T L \bz, 
% \end{align}

% where we assume that the platform does not have access to the susceptibility parameters $\alpha_i$ (i.e., the $\alpha_i$s are endogenous and private to each agent). Finally, we define the ``Price of Misreporting'' (PoM) to be the ratio between the cost $C(\bz^\prime)$ when the agents are deviating, and the cost $C(\bz)$ when the agents are reporting truthfully, i.e.,   

% \begin{equation}
%     \pom = \frac {C(\bz^\prime)} {C(\bz)}.
% \end{equation}

% Note that always $\pom \ge 1$. 

% \subsection{Related Work}

% \paragraph{Opinion Dynamics} Opinion dynamics are well-studied in computer science and economics, as well as sociology, political science, and related fields. There have been many models proposed for opinion dynamics, such as with network interactions as we study in this paper (FJ model) \citep{Friedkin1990,Bindel2015}, bounded confidence dynamics (Hegselman-Krausse Model) \citep{hegselmann2002opinion}, coevolutionary dynamics \citep{bhawalkar2013} as well as many variants of them; see, for example \citep{abebe2018opinion,hazla2019geometric,fotakis2016opinion,fotakis2023opinion,tsourakakis-2024}. The work of \citep{bindel2011} shows bounds on the Price of Anarchy (PoA) between the PSNE and the welfare-optimal solution for the FJ model, and the subsequent work of \cite{bhawalkar2013} shows PoA bounds for the coevolutionary dynamics. Additionally, the opinion dynamics have been modeled by the control community; see, for example, \citet{nedic2012,de2022,bhattacharyya2013convergence,chazelle2011total}. 

% Similarly to the existing models, our work assumes the FJ model as a basis. However, our work is significantly different as it studies a framework where a subset of strategic agents can deviate where the agents can basically choose their intrinsic opinions ($\bs^\prime$) and are given the equilibrium state of the system ($\bz^\prime = B \bs^\prime$), as opposed to studying the evolution of the expressed opinions and their PSNE. 

% \paragraph{Disagreement and Polarization in Social Networks} Motivated by real-world manipulation of social networks in, e.g., the 2016 US election, a recent line of work studies polarization and strategic behavior in opinion dynamics \cite{gaitonde2020adversarial,gaitonde2021polarization,Chen2022,tsourakakis-2024}. 
% Next, \cite{Chen2022} considers a model in which an adversary can control $k \leq n$ nodes' internal opinions. They want to arrive at an equilibrium that maximizes polarization. In contrast, we study a setting in which any subset $S \subseteq [n]$ can be strategic. Unlike previous works, these ``adversaries'' can have conflicting goals. 


% \paragraph{Network Games.} Network games, also known as graphical games, involve $n$ agents whose payoffs are influenced by the actions of their neighbors~\cite{littman2001efficient, roughgarden2004bounding,roughgarden2011local}. A substantial body of research examines learning from observations of such games~\cite{irfan2018causal,garg2016learning,rossi2022learning,leng2020learning}. These studies typically focus on games with finite or one-dimensional action spaces, whereas in our model, each of the $n$ agents operates in an $n$-dimensional action space.

% Furthermore, another area of research explores influencing the outcomes of network games. Most of these studies address games with one-dimensional action spaces and a single strategic actor~\cite{galeotti2020targeting,gaitonde2020adversarial,gaitonde2021polarization, wang2024relationship}. In contrast, we consider arbitrary sets of strategic actors. Recent works have also examined scenarios with multiple strategic actors, such as in repeated auctions~\cite{kolumbus2022auctions} and Fisher markets with linear utilities~\cite{kolumbus2023asynchronous}. While our work shares a similar motivation, it centers on opinion formation.

% \paragraph{Anomaly Detection in Graphs.} Our work is also related to the anomaly detection literature in networks. One work closely related to ours is the work of \citet{chen2022antibenford} where the authors want to detect Anti-Benford subgraphs in a large transaction or financial graph. The Anti-Benford subgraphs consist of a set of nodes that perform many transactions that significantly deviate from Benford's law, and the authors develop a hypothesis test to detect such graphs. Similarly, the work of \citet{agarwal2020chisel} proposes a framework based on a chi-squared statistic to perform a graph similarity search.  In our paper, similarly, we develop a robust regression and a hypothesis testing algorithm that is able to detect nodes that are strategic and misreport their opinions. Additionally, our work assumes the strategic behavior of the agents, whereas the works of \citep{agarwal2020chisel,chen2022antibenford} do not. 

% In a similar spirit, the work of \citet{jalan-chakrabarti-2024} studies financial networks, whereas a subset of strategic agents has incentives to misreport their own local network connections in order to obtain higher utility, and develop algorithms that can identify the set of such agents. Our paper works in a similar flavor, however, in a significantly different application domain and context, which corresponds to social networks.

% % \mpcomment{I added the refs, but it may need more writing to make sure it looks different}

% % \ajcomment{Add refs from old paper. Emphasize Nisan-Kolumbus paper on how to manipulate your learning algorithm.}

% \subsection{Real-world Datasets}

% To support our results, we use data grounded in practice, which has also been used in previous studies to study polarization and disagreement (cf. \citet{Chitra2020,wang2024relationship,adamic2005political}). Specifically, we use Twitter, Reddit, and Political blog networks. Twitter has $n = 548$ nodes and $m = 3638$ unweighted edges. Edges correspond to user interactions regarding the debate over the Delhi legislative assembly elections of 2013. The Reddit dataset has $n = 556$ nodes and $m = 8969$ edges. Nodes are users who posted in the \texttt{r/politics} subreddit, and there is an edge between two users if two subreddits (other than \texttt{r/politics}) exist that both users posted during the given time period. The vectors $\bs$ of initial opinions are obtained via sentiment analysis and also follow the post-processing of \citet{wang2024relationship}. The Political Blogs -- or Polblogs, for brevity --  dataset is due to \citet{adamic2005political} and contains political blogs (liberal and conservative) and links between blogs were automatically extracted from a crawl of the front page of the blog. Each blog is either liberal -- where we assign a value $\bs_i = -1$ -- or conservative -- where we assign $\bs_i = +1$. The Polblogs dataset has $n = 1490$ and $m = 16178$. 

\section{Strategic Opinion Formation}\label{sec:strategic_opinion_formation}

% \mpcomment{it needs more text here to look smoother}

% \ajcomment{Make it a list, but with nice text to flow.}

% \ajcomment{Here's an idea since we have space: we show the networks, colored by opinions, for three scenarios. First the actual equilibrum, second the equilibrim with one strategic actor, and third with multiple strategic actors. Show like a spring-mass embedding visualization. This helps visualize the differences in our model.}

The opinion formation game has two phases. First, strategic agents privately choose a strategic intrinsic opinion according to \cref{eq:strategic_cost}. Second, agents exchange opinions and reach consensus {\em as if} they were in the Friedkin-Johnson dynamics, except the strategic opinions are used in place of the true intrinsic opinions. 
\begin{enumerate}
    \item {\em Strategy Phase.} Each strategic agent $i \in S$ independently and privately chooses a fictitious strategic opinion $\bs_i^\prime \in \RR$. For honest agents ($i \notin S$) we have $\bs_i^\prime = \bs_i$.
    \item {\em Opinion Formation Phase.} Reach equilibrium $\bz^\prime = B \bs^\prime$ as if $\bs^\prime$ were the true intrinsic opinions~$\bs$. 
\end{enumerate}

% In the first phase -- called the ``strategy phase'' --  each strategic agent $i \in S$ independently and privately chooses a negotiating position $\bs_i^\prime \in \RR$. For honest agents ($i \notin S$) we have $\bs_i^\prime = \bs_i$. The strategic agents select $\bs_i^\prime$ according to \cref{eq:strategic_cost}. In the second phase -- called the ``opinion formation phase'' -- the agents exchange opinions and reach consensus $\bz^\prime = B \bs^\prime$ as if every agent's intrinsic opinion was their true opinion $\bs_i$. 
The network $G$ and the true beliefs $\bs$ determine each agent’s utility. We pose the following problem: 

\begin{defn}[Instrinsic belief lying problem.] \label{defn:lying}
Let $S \subseteq [n]$ be a set of strategic agents. If agent $i \in S$ wants network members to express opinions close to $\bs_i$, what choice of $\bs_i^\prime$ is optimal and minimizes the cost function of \cref{eq:strategic_cost}? 
\end{defn}

The following theorem characterizes the Nash Equilibria of the Intrinsic Belief Lying Problem. 
\begin{theorem}[Nash Equilibrium] \label{theorem:psne}
Let $\TT_i = (1 - \alpha_i) (B^T L_i B) + \alpha_i (B^T \be_i \be_i^T B) \in \RR^{n \times n}$ and $\by_i = \alpha_i B_{ii} \bs_i$. The Nash equilibria, if any exist, are given by solutions $s^\prime \in \RR^n$ to the following constrained linear system: 
\begin{align*}
\forall i \in S: \be_i^T \TT_i \bs^\prime & = \bm{y}_i, \\
\forall j \not \in S: \bs_j^\prime & = \bs_j. 
\end{align*}
\end{theorem}

\begin{proof}
Consider agent $i \in S$. To calculate the best-response $\bs_i^\prime$ of $i$ in response to $\bs_{-i}^{\prime}$, we analyze derivatives of its cost function with respect to $\bs^\prime$. Since the equilibrium $\bz^\prime$ is $\bz^\prime = B \bs^\prime$, we have: 
\begin{align*}
c_i(\bz^\prime) &= (1 - \alpha_i) \sum_{j \sim i}
w_{ij} (\bz_i^\prime - \bz_j^\prime)^2 + \alpha_i (\bz_i^\prime - \bs_i)^2 \\
c_i(\bs^\prime) &= (1 - \alpha_i) \sum_{j \sim i}
w_{ij} ((\be_i - \be_j)^T B \bs^\prime)^2 
+ \alpha_i (\be_i^T (B \bs^\prime - \bs))^2 \\
&=  (1 - \alpha_i) \sum_{j \sim i}
w_{ij} (\bs^\prime)^T (B^T (\be_i - \be_j) (\be_i - \be_j)^T B) (\bs^\prime) \\
&+ \alpha_i ((\bs^\prime)^T B^T e_i e_i^T B \bs^\prime 
- 2 (\bs^\prime)^T B^T e_i e_i^T \bs 
+ \bs^T e_i e_i^T \bs) \\
\nabla_{\bs^\prime} c_i(\bs^\prime) 
&= (1 - \alpha_i) \sum_{j \sim i}
w_{ij} 2 (B^T (\be_i - \be_j) (\be_i - \be_j)^T B) (\bs^\prime) 
+ \alpha_i (2 B^T \be_i \be_i^T B \bs^\prime 
- 2 B^T \be_i \be_i^T \bs), \\
\nabla_{\bs^\prime}^2 c_i(\bs^\prime) &= 2 (1 - \alpha_i) B^T \bigg[\sum_{j \sim i}
w_{ij} 2 (\be_i - \be_j) (\be_i - \be_j)^T \bigg] B
+2  \alpha_i B^T \be_i \be_i^T B.
 \end{align*}
 Let $L_i \in \RR^{n \times n}$ be: 
 \begin{align*}
L_i := \sum_{j \sim i} w_{ij} (\be_i - \be_j) (\be_i - \be_j)^T  .
\end{align*}
Notice that $L_i$ is precisely the Laplacian of the graph when all edges not incident to $i$ are equal to zero. Therefore $L_i \succeq 0$. Since $\be_i \be_i^T \succeq 0$, the Hessian of $c_i$ with respect to $\bs^\prime$ is PSD. In particular, its $(i,i)$ entry is non-negative, so $\frac{\del^2 c_i(\bs^\prime)}{\del (\bs_i^\prime)^2} \geq 0$, and hence the optimal $\bs_i^\prime$ is at the critical point. This is given as: 
\begin{align*}
0 &= \frac{1}{2} \frac{\del}{\del \bs_i^\prime} c_i(\bs^\prime) \\
&= \be_i^T (1 - \alpha_i) B^T \bigg[\sum_{j \sim i}
w_{ij} (\be_i - \be_j) (\be_i - \be_j)^T \bigg] B \bs^\prime 
+ \be_i^T \alpha_i (B^T \be_i \be_i^T B s^\prime - B^T \be_i \be_i^T \bs) \\
&= (1 - \alpha_i) \be_i^T B^T L_i B \bs^\prime 
+ \be_i^T \alpha_i (B^T \be_i \be_i^T B \bs^\prime - B^T \be_i \be_i^T \bs).
\end{align*}
The above display gives the solution for $\bs_i^\prime$ in terms of all entries of $s^\prime$. Assembling the critical points into a linear system, we obtain precisely that for all $i \in S$, $\be_i^T \TT_i \bs^\prime = \by_i$. Since $\bs_j^\prime = \bs_j$ for $j \not \in S$, the overall linear system describes the Nash equilibria. 
\end{proof}

To illustrate the Theorem, we consider a toy example. 
\begin{example}[Two-Node Graph]
    
% \ajcomment{I would make this an Example environment}
% \paragraph{Toy Example.} 
Consider a graph with 2 nodes and one edge with weight $w > 0$. We set $\alpha_1 = \alpha_2 = 0.5$ for simplicity. Suppose that both agents deviate, i.e., $S = [2]$. Then, we can calculate $B$ to be 

\begin{align*}
    B = \frac {1} {2w + 1} \begin{pmatrix} w + 1 & w \\ w & w + 1 \end{pmatrix}
\end{align*}

and 

\begin{align}
    \bz_0' = \frac {(w + 1) \bs_0' + w \bs_1'} {2w + 1}, \quad \bz_1' = \frac {w \bs_0' + (w + 1) \bs_1'} {2w + 1},
\end{align}

yielding the two cost functions

\begin{align*}
    c_0(\bs^\prime) & = \frac 1 2 w \left ( \frac {\bs_0' - \bs_1'} {2w + 1} \right )^2 + \frac 1 2 \left ( \frac {(w + 1) \bs_0' + w \bs_1'} {2w + 1} - \bs_0 \right )^2 \\
    c_1(\bs^\prime) & = \frac 1 2 w \left ( \frac {\bs_0' - \bs_1'} {2w + 1} \right )^2 + \frac 1 2 \left ( \frac {(w + 1) \bs_1' + w \bs_0'} {2w + 1} - \bs_1 \right )^2.
\end{align*}

Taking the first order conditions $\frac {\partial c_0}{\partial \bs_0'} = 0$ and $\frac {\partial c_1} {\partial \bs_1'} = 0$ we get a linear system whose solutions are:  

\begin{align*}
\bs_0^\prime & = \frac{w^2(s_0 - s_1) + (3w + 1) \bs_0}{3w + 1}, \quad \bs_1^\prime  = \frac{w^2(\bs_0 - \bs_1) + (3w + 1) s_1}{3w + 1}.
\end{align*}

Replacing these values back to the costs we get that 

\begin{align*}
\forall i: c_i(s_0^\prime, s_1^\prime) = \frac 1 2 \frac{w(w^2 + 3w - 1)(\bs_0 - \bs_1)^2}{9w^2 + 6w + 1},
\end{align*}

On the other hand, if all agents are honest, then the cost for each is: 

\begin{align*}
\forall i: c_i(\bs_0, \bs_1) = \frac 1 2 \frac{w(w+1)(\bs_0 - \bs_1)^2}{(2w + 1)^2}.    
\end{align*}

and the ratio of the two costs is at least $\max \{ 1, w / 3 \}$. 
\end{example}


Next, we discuss some consequences of Theorem~\ref{theorem:psne}. First, we characterize $\bs^\prime$ as the solution to a linear system. 
\begin{cor} \label{cor:psne}
Let $T \in \RR^{|S| \times n}$ have rows $\{ \be_i^T \TT_i \}_{i \in S}$ given by \cref{theorem:psne}. Let $\Tilde T \in \RR^{|S| \times |S|}$ be the submatrix of $T$ selecting columns belonging to $S$. Let $\by \in \RR^{|S|}$ have entries $\by_i = \alpha_i B_{ii} \bs_i$ as above. Let $\Tilde \by = \by - \sum_{j \not \in S} \bs_j T \be_j$. Then the set of Nash equilibria, if any exist, are given by the solutions to the unconstrained linear system 

\begin{equation} \label{eq:psne}
    \Tilde T \bx = \Tilde \by.
\end{equation}
The resulting opinions vector $\bs^\prime$ is given by $\bs_i^\prime = \bx_i$ if $i \in S$ and $\bs_i^\prime = \bs_i$ otherwise.
\end{cor}




Thus, in a Nash equilibrium, every strategic agent solves their corresponding equation given by \cref{eq:psne}. The explicit characterization of equilibria also implies that Nash equilibria cannot be mixed. 
\begin{cor}[Pure Strategy Nash Equilibria]
    The Nash equilibrium corresponds to solving the system of $|S|$ linear equations in the scalars $\{ \bs_i^\prime | i \in S \}$ given by \cref{eq:psne}. Also, all Nash equilibria are pure-strategy Nash equilibria.
\end{cor}


\begin{figure}[t]
    \centering
    % \includegraphics[width=0.3\linewidth]{figures/PoM_example.pdf} \\
    \includegraphics[width=0.49\linewidth]{figures/Reddit_alpha_0.25.pdf} 
    \includegraphics[width=0.49\linewidth]{figures/Reddit_alpha_0.5.pdf} 

    \includegraphics[width=0.49\linewidth]{figures/Twitter_alpha_0.25.pdf}
    \includegraphics[width=0.49\linewidth]{figures/Twitter_alpha_0.5.pdf}

    \includegraphics[width=0.49\linewidth]{figures/Polblogs_alpha_0.25.pdf} 
    \includegraphics[width=0.49\linewidth]{figures/Polblogs_alpha_0.5.pdf} 

    \caption{Plot of truthful intrinsic opinions ($s$) and strategic opinions ($s'$), and truthful public opinions ($z$) compared to the strategic public opinions ($z'$) for the nodes belonging to $S$. $S$ is taken to be the top-50\% in terms of their eigenvector centrality. In both cases we have taken $\alpha_i \in \{ 0.25, 0.5 \}$ for all nodes. We fit a linear regression between $s'$ and $s$ (resp. between $z$ and $z'$). We report the effect size $\theta$ which corresponds to the slope of the linear regression and the $P$-value with respect to the null hypothesis ($\theta = 0$). $^{***}$ stands for $P < 0.001$, $^{**}$ stands for $P < 0.01$ and $^{*}$ stands for $P < 0.05$.}
    \label{fig:psne_data}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/polblogs.pdf}
    \caption{Strategic misreports for the Polblogs dataset where $S$ is taken to be the top-50\% of the agents in terms of their eigenvector centralities. The nodes are labeled either as liberal ($\bs_i = -1$) or conservative ($\bs_i = +1$), and we consider the nodes that change their beliefs as the nodes for which $\bz_i'$ and $\bz_i$ do not have the same sign. In the scatterplots (a), (c), (d), (e), the shape of each point indicates whether that user changed belief or not, and the color indicates their true (intrinsic) opinion. 
    Overall, we discover a higher amount of change among liberal blogs compared to conservative ones (panel (b)). Additionally, we report the truthful/strategic public opinion as a function of the logarithm of the eigenvector centrality $\bpi_i$ (cf. panels (c, d)) for each node, as well as the absolute change $|\bz_i' - \bz_i|$ (cf. panel (e)). We fit a regression model, and we detect significant effects ($^{***}: P < 0.001, \; ^{**}: P < 0.01, \; ^{*}: P < 0.05$; effects denoted by $\theta$) of the logarithm of the centrality to the truthful equilibrium $\bz$, the strategic equilibrium $\bz'$, and the change $|\bz' - \bz|$, revealing the structure of a power law. Finally, we observe that relative changes are more dispersed along liberal sources compared to conservative sources (cf. panel (f)).} 
    \label{fig:polblogs}
\end{figure}
\vspace{-1em}

\paragraph{Optimal Deviation for One Agent and All Agents} Assuming that we have one strategic agent, what is the change in their opinion? We can show that the new opinion is a scalar multiple of the initial opinion plus a bias term, where neither the scalar multiple nor the bias term can be zero. 

\begin{corollary}[Deviation for One Agent] \label{theorem:one_deviation}
    Let $S = \{ i \}$. Then, $\bs_i^\prime = \theta_i \bs_i + \beta_i$ where

    \begin{align*}
        \theta_i & = \frac {\alpha_i B_{ii}} {(1 - \alpha_i) \sum_{i \sim j} w_{ij} (B_{ii} - B_{ij})^2 + \alpha_i B_{ii}^2} > 0, \\
        \beta_i & = - \frac {\alpha_i \sum_{j \neq i} B_{ij} s_j} {(1 - \alpha_i) \sum_{i \sim j} w_{ij} (B_{ii} - B_{ij})^2 + \alpha_i B_{ii}^2}.
    \end{align*}

\end{corollary}
 
Similarly, we can relate the maximum deviation of $\bs^\prime$ from $\bs$ in the other extreme case, i.e., when all agents are deviating ($S = [n]$). %The proof is deferred to the Appendix.
\begin{corollary}
    \label{theorem:all_deviation}
    When all agents are deviating ($S = [n]$), and $\alpha_i = \alpha$, then $\bs^\prime$ satisfies: $$\frac {\norm \bs^\prime \norm_2} {\norm \bs \norm_2}  \le \frac {\lambda_n + \tilde \alpha} {\tilde \alpha}.$$
\end{corollary}

\begin{proof}[Proof of Corollary~\ref{theorem:all_deviation}]
    When all agents are deviating, it is straightforward to show that $\Tilde T = \tilde \alpha B$ with minimum eigenvalue $\tilde \alpha^2 / (\lambda_n + \tilde \alpha) > 0$. Thus $\Tilde T$ is invertible, and therefore $\bs^\prime = \frac {1} {\tilde \alpha} B^{-1} \widetilde {\diag (B)} \bs$, where $\widetilde {\diag (B)}$ is a diagonal matrix with entries $B_{ii}$. Then 

    \begin{align*}
        \norm \bs^\prime \norm_2 & \le \frac {1} {\tilde \alpha} \norm B^{-1} \norm_2 \norm \widetilde{\diag (B)} \norm_2 \norm \bs \norm_2 \\ &  = \left ( \max_i B_{ii} \right ) \left ( \max_i \frac {\lambda_i + \tilde \alpha} {\tilde \alpha} \right ) \norm \bs \norm_2 \\ & \le \frac {\tilde \alpha} {\lambda_1 + \tilde \alpha} \frac {\lambda_n + \tilde \alpha} {\alpha} \norm \bs \norm_2 \\ & = \frac {\lambda_n + \tilde \alpha} {\tilde \alpha} \norm \bs \norm_2.
    \end{align*}
    
\end{proof}

The proof of Corollary~\ref{theorem:all_deviation} shows that the adjusted susceptibility ($\tilde \alpha$) and the maximum eigenvalue of the Laplacian ($\lambda_n$) are responsible for changes in the norm of $\bs^\prime$. From classic spectral graph theory, we know that $\lambda_n = \Theta(d_{\textup{max}})$ where $d_{\textup{max}}$ is the maximum degree of the graph; therefore, graphs with a lower maximum degree experience smaller distortions. Also, regarding the susceptibility to persuasion, the distortion becomes $1 + o(1)$ as long as $\tilde \alpha = \omega (d_{\textup{max}})$. 

\medskip

\noindent {\bf Equilibria for real-world datasets.} Next, we discuss the results of experiments simulating the strategically manipulated equilibria for our real-world datasets. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/polarization_disagreement_ratios.pdf}
    \caption{Polarization ratio ($\mathcal P(z')/\mathcal P(z)$), disagreement ratio ($\mathcal D(z') / \mathcal D(z)$), and price of misreporting ($C(z') / C(z)$) for the three datasets for varying susceptibility to persuasion values. We have set all susceptibilities $\alpha_i$ to the same value $\alpha$. The Twitter dataset has the largest variation in all three ratios compared to the others. $S$ is taken to be the top-50\% nodes in terms of their eigenvector centrality.}
    \label{fig:ratios_susceptibility_to_persuation}
\end{figure}

% \ajcomment{Maybe change ordering of the figs?}


\paragraph{Effect of Susceptibility to Persuasion in Real-world Data} Regarding real-world data, \cref{fig:psne_data} shows the relationship between the truthful opinions ($\bs$ and $\bz$) and the strategic ones ($\bs^\prime$ and $\bz^\prime$) for the datasets, along with the corresponding correlation coefficient $R^2$, assuming that $S$ consists of the top-50\% nodes in terms of their eigenvector centrality, for susceptibility parameters set to $\alpha_i = 0.5$ (equal self-persuasion and persuasion due to others) and $\alpha_i = 0.25$ (higher persuasion due to others). 

Regarding the public opinions, even though in the Reddit dataset, the strategic opinions seem to be correlated with the truthful ones ($R^2 = 0.78$ for $\alpha_i = 0.25$ and $R^2 = 0.94$ for $\alpha_i = 0.5$ respectively), in the Twitter dataset, we do not get the same result (i.e., $R^2 < 0.25$). Finally, in the Polblogs dataset, the situation is somewhere in the middle; when $\alpha_i = 0.25$ we get a low $R^2$ ($R^2 = 0.18$) where for $\alpha_i = 0.5$ we get a high $R^2$ ($R^2 = 0.74$). Additionally, in all cases except Twitter, we get that the effect is significant ($P < 0.01$). 

Regarding the relationship between the intrinsic opinions, we do not detect any significant effect in most cases except Reddit with $\alpha_i = 0.5$ ($P < 0.01$) and Twitter with $\alpha_i = 0.5$ ($P < 0.05$).

\paragraph{Asymmetric Effects of Strategic Behavior on Liberals and Conservatives.} \cref{fig:polblogs} analyzes the opinions of the strategic set $S$ on the Polblogs dataset. Specifically, we find that larger changes in sentiment happen across liberal outlets compared to conservative ones. Additionally, the changes in the truthful/strategic opinions are related to the eigenvector centrality $\bpi_i$ as a power law, i.e., $\bz_i' \propto \bpi_i^{\theta}$ ($P < 0.001$; linear regression between the log centralities $\log \bpi_i$ and $\bz_i'$). The same finding holds for $|\bz_i' - \bz_i|$ and $\bz_i$. 

At this point, one may wonder whether the eigenvector centrality really influences the strategic opinions $\bz_i'$ for $i \in S$. Our answer is negative. We repeat the same experiment with the Twitter and Reddit datasets, where we find no effects ($P > 0.1$; linear regression between the log centralities $\log \bpi_i$ and $\bz_i'$). Due to space limitations, the corresponding figures are deferred to \cref{app:additional_figures}. 

\paragraph{Polarization and Disagreement.} \cref{fig:ratios_susceptibility_to_persuation} shows how the polarization, disagreement, and cost change as a function of the susceptibility parameter $\alpha_i$. Except for $\alpha_i \approx 0.3$, the polarization ratio, disagreement ratio, and the price of misreporting experience a downward trend as $\alpha_i$ increases. This indicates that as as users prioritize their own opinions more than their neighbors, they are less susceptible to strategic manipulation.
% \ajcomment{This indicates that...}

\paragraph{Effect of the number of deviators ($|S|$)} Next, we study the effect of the number of deviators, which corresponds to $|S|$, on the changes in polarization, disagreement, and the total cost (through the price of misreporting). \cref{fig:ratios_number_of_deviators} shows how the polarization and disagreement when $S$ consists of the top-1-10\% most central agents with respect to eigenvector centrality. We show that even if only 1\% of agents are strategic, this can impact consensus by several orders of magnitude. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/polarization_disagreement_ratios_percent_strategic.pdf}
    \caption{Polarization ratio ($\mathcal P(\bz^\prime)/\mathcal P(\bz)$), disagreement ratio ($\mathcal D(\bz^\prime) / \mathcal D(\bz)$), and price of misreporting ($C(\bz^\prime) / C(\bz)$) for the three datasets for varying the size of $|S|$. The size of $|S|$ corresponds to the top $p$ percent of the actors ($|S| = \lceil p n \rceil$) based on their eigenvector centrality (in decreasing order), for $p \in [0.01, 0.1]$. The susceptibility parameter is set to $\alpha_i = 0.5$.} 
    \label{fig:ratios_number_of_deviators}
\end{figure}







% \section{Price of Misreporting}

% We use the local smoothness technique. The local smoothness technique has been used to bound the Price of Anarchy in coevolutionary opinion formation games (see, e.g. \citet{bhawalkar2013}). We give the following theorem due to \citet{bhawalkar2013}, which is an extended result from \citet{roughgarden2011local}. 

% \begin{theorem}[\cite{bhawalkar2013}]
% Let $\sigma$ denote a correlated equilibrium. Suppose for any outcome $z$, with respect to a fixed outcome $o$ and scalars $\mu < 1, \lambda > 0$, that: 
% \begin{align}
% C(z) + (o-z)^T \nabla_z C(z) \leq \lambda C(o) + \mu C(z).
% \end{align}
% Then, the correlated PoA is bounded as $\frac{\EE_{\sigma}[C(z)]}{C(o)} \leq \frac {\lambda} {(1-\mu)}$. 
% \end{theorem}

% Using this method gives PoF bounds against any correlated equilibrium, and hence any Nash equilibrium. We can use the same framework to bound the PoM \mpcomment{are you sure we can do this?} \mpcomment{otherwise we can just have the PoA though the PoM is more useful IMHO}

% \mpcomment{quick question: shall we use different $y$ because we also have $y$ in the PSNE? It is defined in the scope of a theorem so I am not sure if it is indeed a problem regarding notation; just it may make things easier to read}

% \ajcomment{Not clear how to deal with differing $\alpha_i$, because the eigenvectors of $B$ are no longer the same as $L$.}

% \begin{theorem} \label{theorem:pom}
% Suppose all network members are strategic ($S = [n]$). Let $L$ have eignevalues $\lambda_i$ and suppose that there exists $\alpha$ such that for all $i$, $\alpha_i = \alpha$. Let $\Tilde \alpha = \frac{\alpha}{1 - \alpha}$. For $\mu \in (0,1)$ and $i \in [n]$ let: 
% \[
% f(\mu, i) = \bigg(\lambda_i + \Tilde\alpha +\Tilde\alpha(1 + \lambda_i)^2 \bigg)^{-1}
% \bigg(\frac{(\lambda_i + 2 \Tilde\alpha + 2 \mu \Tilde\alpha (1 + \lambda_i))^2}{(\lambda_i + \alpha)(1 + \mu)} - \frac{2\Tilde\alpha\lambda_i}{(1 + \mu)} + \frac{1 - \mu}{1 + \mu} \Tilde\alpha (1 + \lambda_i)^2 
% \bigg)
% \]
% Let $\mu^* = \arg\inf_{\mu \in (0,1)} \max_{i \in [n]} f(\mu, i)$ and $i^* = \arg\max_{i \in [n]} f(\mu^*, i)$. Then, for all $\eps > 0$, 
% \[
% \pom \leq \frac{f(\mu^*, i^*) + \eps}{(1 - \mu^*)}
% \]
% %Let $s^\prime$ be any deviation. Let $L$ be the graph Laplacian and $U \in O(n,\RR)$ its eigenbasis. Let $y = Us$.
% % Suppose that $\norm s^\prime \norm_2 \leq R$. The correlated price of fooling is at most: 
% % \[
% % \pom \leq \frac{2R + 2}{\min\{1, \min\{\abs{y_j}: y_j \neq 0 \}\}}.
% % \]
% \end{theorem}

% A few remarks are in order: First, notice that we give a strict generalization of PoM because the bound is against correlated equilibria, which are a generalization of Nash equilibria. Therefore in restricted classes such as PSNEs, the PoM may be smaller. Second, our bound is for the case when all $n$ agents are strategic. If only a small subset of the nodes are strategic the PoM may be smaller. In fact, this can be read off from our proof: what changes is that the entries of some entries of $s^\prime$ are constrained to be zero, but the proof would otherwise be the same. 

% % Finally, notice that our bound depends on the norm of the deviation $R$, as well as the quantity $\min_{j: y_j \neq 0} \abs{y_j}$. The ladder incorporates graph structure through the eigenvectors $U$ of the Laplacian (these can be interpreted as spectral clustering assignments), as well as the intrinsic opinions $s$. In particular, if the (graph-filtered) opinions $y_j$ have small nonzero entries, this indicates that the PoM will be large, because these members will be more swayed. 

% \mpcomment{each cost has an $1 - \alpha_i$ in the pairwise disagreement term -- so I think the algebra is a bit different (even for $\alpha_i = \alpha$). Also the $\lambda$ of the smoothness can be renamed to $\kappa$ to avoid confusion with the eigenvalues.}
% \begin{proof}
% Let $C(z) = \sum_i c_i(z)$. We want to show that for some $\lambda > 0, 0 < \mu < 1$: 
% \begin{align}
% C(z^\prime) + (s-s^\prime)^T \nabla_{s^\prime} C(z^\prime) \leq \lambda C(z) + \mu C(z^\prime)
% \label{eq:poa-condition}
% \end{align}
% Now, up to scaling, 
% \begin{align*}
% \frac{1}{1 - \alpha_i} C(z^\prime) &= \sum_i C_i(z^\prime) \\
% &= \sum_i \big(\sum_j w_{ij}(\bz_i^\prime - \bz_j^\prime)^2\big) + \frac{\alpha_i}{1 - \alpha_i} (\bz_i^\prime - \bs_i)^2 \\
% &= \la z^\prime, L z^\prime \ra + \Tilde \alpha \norm z^\prime - s \norm_2^2 
% \end{align*}
% %Where $\Tilde \alpha > 0$ is the shared $\alpha = \alpha_i$. 
% From the above, we see that $\nabla_{z^\prime} C(z^\prime) = 2 Lz^\prime + 2 \Tilde \alpha z^\prime - 2 \Tilde \alpha s$. 
% \ajcomment{Need to replace all occurrences of $\alpha$ with $\Tilde \alpha$, and $\lambda$ with $\kappa$.}

% Next, recall $z^\prime = B s^\prime$ for $B = (L + \alpha I)^{-1}$ and $s^\prime \in \RR^n$ the strategic internal opinions. By the chain rule, $\nabla_{s^\prime} C(z^\prime) = 2 BLBs^\prime + 2 \alpha B^2 s^\prime - 2 \alpha B \bs$. Therefore the LHS of Eq~\eqref{eq:poa-condition} becomes: 
% \begin{align*}
% C(z^\prime) + (s-s^\prime)^T \nabla_{s^\prime} C(z^\prime)
% &= \la s^\prime, BLB s^\prime \ra + \alpha \norm B s^\prime - s \norm_2^2 
% + (s-s^\prime)^T (2 BLBs^\prime + 2 \alpha B^2 s^\prime - 2 \alpha B \bs) \\
% &= (s^\prime)^T (BLB + \alpha B^T B - 2 BLB - 2 \alpha B^T B)s^\prime \\
% &+ s^T (\alpha I - 2 \alpha B)s
% + s^T (-2\alpha B + 2BLB + 2\alpha B^2 + 2\alpha B)s^\prime
% \end{align*}
% On the other hand, the RHS of Eq~\eqref{eq:poa-condition} becomes: 
% \begin{align*}
% \lambda s^T (BLB + \alpha (B - I)^T (B-I))s 
% + \mu C(s^\prime)
% &= \lambda s^T (BLB + \alpha (B - I)^T (B-I))s  
% + \mu(\la s^\prime, BLB s^\prime \ra + \alpha \norm B s^\prime - s \norm_2^2 )\\
% &= (s^\prime)^T (\mu BLB + \mu \alpha B^T B) s^\prime \\
% &+ s^T (\lambda BLB + \lambda \alpha (B - I)^T (B-I) + \mu \alpha I)s  
% + s^T (-2 \mu \alpha B) s^\prime 
% \end{align*}
% Combining the above displays, we see that Eq~\eqref{eq:poa-condition} holds iff: 
% \begin{align*}
% (s^\prime)^T ((1 + \mu)BLB + (1 + \mu)\alpha B^T B)s^\prime 
% + s^T (\lambda BLB + \lambda \alpha (B-I)^T (B-I) + \mu \alpha I) s
% + s^T (-2\mu\alpha B) s^\prime \\
% > s^T (\alpha I - 2 \alpha B)s 
% + s^T (-2\alpha B + 2 BLB + 2 \alpha B^2 + 2 \alpha B) s^\prime 
% \end{align*}
% Since $L \succeq 0$ is symmetric, let $L = UDU^T$ for some $D \succeq 0$ and eigenbasis $U$. Then $B = (L + I)^{-1} = U (D + I)^{-1} U^T$. Let $\Tilde D = (D + I)^{-1}$, so $B = U \Tilde D U^T$. We can perform a change of variables, letting $y = U s$ and $y^\prime = U s^\prime$. The above display becomes: 
% \begin{align*}
% (y^\prime)^T ((1 + \mu)\Tilde D^2 D + (1 + \mu)\alpha \Tilde D^2)y^\prime 
% + y^T (\lambda \Tilde D^2 D + \lambda \alpha \Tilde D^2 - 2 \alpha \Tilde D^2 + (\lambda + \mu) \alpha I) y
% + y^T (-2\mu\alpha \Tilde D) y^\prime \\
% > y^T (\alpha I - 2 \alpha \Tilde D)y
% + y^T (-2\alpha \Tilde D + 2 \Tilde D^2 D + 2 \alpha \Tilde D^2 + 2 \alpha \Tilde D) y^\prime \\
% \iff 
% (y^\prime)^T ((1 + \mu)\Tilde D^2 D + (1 + \mu)\alpha \Tilde D^2)y^\prime 
% + y^T (\lambda \Tilde D^2 D + \lambda \alpha \Tilde D^2 - 2 \alpha \Tilde D^2 + 2 \alpha \Tilde D + (\lambda + \mu - 1) \alpha I) y \\
% > y^T (2 \Tilde D^2 D + 2 \alpha \Tilde D^2 + 2 \mu \alpha \Tilde D) y^\prime 
% \end{align*}
% Now, let
% \begin{align*}
% F &= (1 + \mu)\Tilde D^2 D + (1 + \mu)\alpha \Tilde D^2 \\
% G &= \lambda \Tilde D^2 D + \lambda \alpha \Tilde D^2 - 2 \alpha \Tilde D^2 + 2 \alpha \Tilde D + (\lambda + \mu - 1) \alpha I \\
% H &= 2 \Tilde D^2 D + 2 \alpha \Tilde D^2 + 2 \mu \alpha \Tilde D
% \end{align*}
% Then, the above condition becomes: 
% \begin{align*}
% \begin{bmatrix} 
% y^\prime \\ y
% \end{bmatrix}^T 
% \begin{bmatrix}
% F & -\frac{1}{2} H \\
% -\frac{1}{2} H & G \end{bmatrix}
% \begin{bmatrix} 
% y^\prime \\ y
% \end{bmatrix} > 0
% \end{align*}
% Letting $M = \begin{bmatrix}
% F & -\frac{1}{2} H \\
% -\frac{1}{2} H & G \end{bmatrix}$, it is sufficient to show $M \succ 0$. 

% Notice that $F, G, H$ are all diagonal and $M$ is Hermitian. Hence $M$ has $2n$ real eigenvalues. For $i \in [n]$, let $M^{(i)} = \begin{bmatrix}
% F_{ii} & -\frac{1}{2} H_{ii} \\
% -\frac{1}{2} H_{ii} & G_{ii} \end{bmatrix}$.

% The eigenvalues of $M$ are given by $\bigcup\limits_{i \in [n]} \{\lambda_1(M^{(i)}), \lambda_2(M^{(i)})\}$. Therefore, it suffices to show $M^{(i)} \succ 0$ for all $i$. The following conditions are necessary and sufficient: 
% \begin{align*}
% F_{ii} &> 0 \\
% G_{ii} &> 0 \\
% \frac{1}{4} H_{ii}^2 &\leq F_{ii} G_{ii}
% \end{align*}
% First, $F_{ii} > 0 \iff (1 + \mu)(\lambda_i + \alpha) > 0$. This is true as long as $\alpha > 0$. Next, letting $\kappa = \lambda$, 
% \begin{align*}
% G_{ii} &> 0 \\
% `\frac{\kappa \lambda_i + \kappa \alpha - 2 \alpha}{(1 + \lambda_i)^2} + \frac{2\alpha}{1 + \lambda_i} + (\kappa + \mu - 1) \alpha &> 0 \\
% \iff \kappa \lambda_i + \alpha \bigg(
% \kappa - 2 + 2 (1 + \lambda_i) + (\kappa + \mu - 1) (1 + \lambda_i)^2
% \bigg) &> 0
% \end{align*}
% Since the Laplacian is PSD, $(1 + \lambda_i) \geq 1$ for all $i$. Hence, a sufficient condition for $G_{ii} > 0$ is that $\alpha > 0$ and: 
% \begin{align*}
% \kappa - 2 + 2 + (\kappa + \mu - 1) &> 0 \\
% \iff 2 \kappa + \mu - 1 &> 0
% \end{align*}
% Since $\kappa \geq 1$ and $\mu \geq 0$, this is always true. Finally, 
% \begin{align*}
% \frac{1}{4} H_{ii}^2 &\leq F_{ii} G_{ii} \\
% \bigg(\frac{\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i)}{(1 + \lambda_i)^2}
% \bigg)^2 
% &\leq \bigg[\frac{(1 + \mu) (\lambda_i + \alpha)}{(1 + \lambda_i)^2} \\
% &\cdot 
% \frac{\kappa \lambda_i + (\kappa - 2) \alpha + 2 \alpha (1 + \lambda_i) + (\kappa + \mu - 1)\alpha (1 + \lambda_i)^2}{(1 + \lambda_i)^2} \bigg] \\
% \frac{(\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i))^2}{\lambda_i + \alpha} &\leq 
% (1 + \mu) \bigg(\kappa \lambda_i + (\kappa - 2) \alpha \\
% &+ 2 \alpha (1 + \lambda_i) + (\kappa + \mu - 1)\alpha (1 + \lambda_i)^2\bigg) \\
% \frac{(\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i))^2}{\lambda_i + \alpha} - 2 \alpha \lambda_i 
% &\leq \bigg[(1 - \mu)(1 + \mu) \frac{\kappa}{1 - \mu} \big(\lambda_i + \alpha + \alpha (1 + \lambda_i)^2\big) \\
% &- (1 - \mu) \alpha (1 + \lambda_i)^2 
% \bigg]\\
% % \bigg(\frac{1}{\lambda_i + \alpha + \alpha (1 + \lambda_i)^2}\bigg) 
% \bigg(\frac{(\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i))^2}{\lambda_i + \alpha} - 2 \alpha \lambda_i + (1 - \mu) \alpha (1 + \lambda_i)^2
% \bigg)
% &\leq (1 - \mu^2) \frac{\kappa}{1-\mu}\big(\lambda_i + \alpha + \alpha (1 + \lambda_i)^2 \big) 
% \end{align*}
% We conclude that it is sufficient for $\kappa, \mu$ to be such that: 
% \[
% \bigg(\lambda_i + \alpha + \alpha (1 + \lambda_i)^2 \bigg)^{-1}
% \bigg(\frac{(\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i))^2}{(\lambda_i + \alpha)(1 + \mu)} - \frac{2 \alpha \lambda_i}{(1 + \mu)} + \frac{1 - \mu}{1 + \mu} \alpha (1 + \lambda_i)^2 
% \bigg) \leq \kappa 
% \]
% Notice the LHS is $f(\mu,i)$. Choosing $\mu^* = \arg \inf_{\mu \in (0,1)} \max_{i \in [n]} f(\mu,i)$, and setting $\kappa = f(\mu^*, i^*)$
% ensures that $\frac{1}{4} H_{ii}^2 \leq F_{ii} G_{ii}$ for all $i$. Hence we conclude that $M \succ 0$.  
% \end{proof}

% \begin{cor}
% If $i^* = 1$, where the eigenvalues of $L$ are ordered $\lambda_1 \leq \dots \lambda_n$ then: 
% \[
% \pom \leq \frac{5}{2} + \eps
% \]
% for arbitrarily small $\eps > 0$. 
% \end{cor}
% \begin{proof}
% Since $L$ is a graph Laplacian, we have $L \bm{1} = \bm{0}$, so $\lambda_1 = 0$. We can simplify $f(\mu, 1)$ as: 
% \begin{align*}
% f(\mu, 1) &= \frac{- \mu + 4 \left(\mu + 1\right)^{2} + 1}{2 \left(\mu + 1\right)} \\
% \frac{d}{d\mu} f(\mu, 1) &= \frac{2 \mu^{2} + 4 \mu + 1}{\mu^{2} + 2 \mu + 1} \\
% \frac{d^2}{d\mu^2} f(\mu, 1) &= \frac{2}{\mu^{3} + 3 \mu^{2} + 3 \mu + 1}
% \end{align*}
% Notice there is no dependence on $\Tilde \alpha$. Since $\mu > 0$, $\frac{d^2}{d\mu^2} > 0$ for all $\mu$, so $\mu^* \in \{0, 1\}$. We can compute that $f(0, 1) = 5/2$ and $f(1,1) = 4$. Hence, by setting $\mu = \eps$ for $\eps > 0$, we conclude that $\pom \leq \frac{5}{2} + \eps$ for arbitrarily small $\eps > 0$. 
% \end{proof}
% \ajcomment{This is useless because $i^* \neq 1$.}

% \begin{prop}
% Let $g(\mu, \lambda) = \frac{1}{1-\mu}f(\mu, i)$ if $\lambda_i = \lambda$. We claim that for all $\lambda > 0$, $\Tilde \alpha \in (0,1)$ and $\mu \in (0,1)$ that: 

% 1. $\frac{\del g}{\del \mu} \neq 0$

% 2. $\frac{\del^2 g}{\del \mu^2} > 0$. 
% % Let $f(\mu, \lambda)$ be defined by letting $\lambda_i = \lambda$. For all $\Tilde \alpha, \mu \in (0,1)$, $\frac{df(\mu, \lambda)}{d\lambda} < 0$. 
% \end{prop}

% \begin{prop}
% For all $\Tilde \alpha, \mu \in (0,1)$ and $\lambda \geq 0$, we have $\frac{dg(\mu, \lambda)}{d\lambda} < 0$
% \end{prop}

% The above two propositions imply that $i^* = n$ and that the optimal $\mu^*$ is within $\{0,1\}$. We can read off: 
% \[
% g(\mu, \lambda) = \frac{\tilde \alpha \left(\tilde \alpha + \lambda\right) \left(2 \lambda + \left(\lambda + 1\right)^{2} \left(\mu - 1\right)\right) - \left(2 \tilde \alpha \mu \left(\lambda + 1\right) + 2 \tilde \alpha + \lambda\right)^{2}}{\left(\tilde \alpha + \lambda\right) \left(\mu - 1\right) \left(\mu + 1\right) \left(\tilde \alpha \left(\lambda + 1\right)^{2} + \tilde \alpha + \lambda\right)}
% \]
% From this, it is clear that $\lim\limits_{\mu \to 1} g(\mu, \lambda) = \infty$. Therefore $\mu^* = 0$. 
% \ajcomment{Weird things happening, somehow we get that the PoM is at most 1 numerically...this shouldn't happen. I will revisit the math with $\alpha_i$ differing and see what happens.}

% % \begin{proof}
% % This can be read off from: 
% % \[
% % \frac{- \alpha \left(\lambda + 1\right) \left(\mu - 1\right) \left(\mu + 1\right) \left(8 \alpha \mu \left(\lambda + 1\right) + 8 \alpha + 4 \lambda - \left(\alpha + \lambda\right) \left(\lambda + 1\right)\right) - \left(\mu - 1\right) \left(\alpha \left(\alpha + \lambda\right) \left(2 \lambda + \left(\lambda + 1\right)^{2} \left(\mu - 1\right)\right) - \left(2 \alpha \mu \left(\lambda + 1\right) + 2 \alpha + \lambda\right)^{2}\right) - \left(\mu + 1\right) \left(\alpha \left(\alpha + \lambda\right) \left(2 \lambda + \left(\lambda + 1\right)^{2} \left(\mu - 1\right)\right) - \left(2 \alpha \mu \left(\lambda + 1\right) + 2 \alpha + \lambda\right)^{2}\right)}{\left(\alpha + \lambda\right) \left(\mu - 1\right)^{2} \left(\mu + 1\right)^{2} \left(\alpha \left(\lambda + 1\right)^{2} + \alpha + \lambda\right)}
% % \]
% % \end{proof}

% \ajcomment{This can be shown for $f/(1-\mu)$ as well.}

% Therefore, we know that for any $\mu$, that either $i^* = n$. 
% % \clearpage



% % % \clearpage

% % \ajcomment{The stuff here is old.}
% % Letting $y^\prime = y + \delta$, we can simplify: 
% % \begin{align*}
% % (y^\prime)^T ((1 + \mu)\Tilde D^2 D + (1 + \mu)\alpha \Tilde D^2)y^\prime 
% % + y^T ((\lambda - 2) \Tilde D^2 D + (\lambda - 2) \alpha \Tilde D^2 + 2 \alpha (1-\mu) \Tilde D + (\lambda + \mu - 1) \alpha I) y \\
% % > y^T (2 \Tilde D^2 D + 2 \alpha \Tilde D^2 + 2 \mu \alpha \Tilde D) \delta 
% % \end{align*}
% % Next, suppose that $\norm \delta \norm_2 \leq R$. Let $0 = \lambda_n \leq \lambda_{n-1} \leq \dots \leq \lambda_1$ be the eigenvalues of $L$, so that $D_{ii} = \lambda_i$. Then, a variational argument implies that to maximize the RHS of the above display we would set $\delta = \frac{R y_j}{\abs{y_j}} \be_j$, where $j = \arg\max_{i \in [n]} \abs{y_i (\frac{\lambda_i}{(1 + \lambda_i)^2} + \frac{2\alpha}{(1 + \lambda_i)^2} + \frac{2\mu\alpha}{1 + \lambda_i})}$. Hence the RHS is upper bounded as: 
% % \[
% % R \max_i \bigg(\abs{y_i} \cdot \bigg(\frac{\lambda_i}{(1 + \lambda_i)^2} + \frac{2\alpha}{(1 + \lambda_i)^2} + \frac{2\mu\alpha}{1 + \lambda_i}
% % \bigg) \bigg)
% % \]
% % To lower bound the LHS, notice that the first summand (the quadratic form in $y^\prime$) is non-negative since $\Tilde D^2 D \succeq 0$ and $\Tilde D^2 \succeq 0$. The second summand is a quadratic form in $y$, and hence is of the form $\sum_i c_i y_i^2$ for coefficients $c_i$. If $\lambda > 2$ then all the coefficients $c_i > 0$. In fact, 
% % \begin{align*}
% % y^T ((\lambda - 2) \Tilde D^2 D + (\lambda - 2) \alpha \Tilde D^2 + 2 \alpha (1-\mu) \Tilde D + (\lambda + \mu - 1) \alpha I) y \\
% % = \sum_i \bigg(
% % \frac{(\lambda - 2)\lambda_i}{(1 + \lambda_i)^2}
% % + \frac{(\lambda - 2) \alpha}{(1 + \lambda_i)^2}
% % + \frac{2 \alpha (1 - \mu)}{(1 + \lambda_i)}
% % + (\lambda + \mu - 1)\alpha
% % \bigg) y_i^2
% % \end{align*}
% % We proceed by casework. First, if the $j$ chosen in $\delta$ is such that $y_j = 0$ then Eq.~\eqref{eq:poa-condition} is trivially satisfied for any $\lambda \geq 2R + 2$. 

% % On the other hand, if $y_j \neq 0$, then set $lambda = \frac{2R + 2}{\min_{j: y_j \neq 0} \abs{y_j}}$. We claim this suffices to satisfy Eq.~\eqref{eq:poa-condition}. First, it is clear that for all $i$, 
% % \[
% % \bigg(\frac{(\lambda - 2)\lambda_i}{(1 + \lambda_i)^2}
% % + \frac{(\lambda - 2) \alpha}{(1 + \lambda_i)^2}\bigg) y_i^2
% % \leq R \abs{y_i} \bigg(\frac{\lambda_i}{(1 + \lambda_i)^2} + \frac{2\alpha}{(1 + \lambda_i)^2} + \frac{2\mu\alpha}{1 + \lambda_i}
% % \bigg)
% % \]
% % Moreover, we claim that for any $i$ such that $\abs{y_i} \neq 0$, 
% % \begin{align*}
% % \frac{2R \abs{y_i} \mu \alpha}{1 + \lambda_i} 
% % &\leq 
% % \bigg(\frac{2 \alpha (1 - \mu)}{(1 + \lambda_i)}
% % + (\lambda + \mu - 1)\alpha
% % \bigg) y_i^2 \\
% % \iff \frac{2 R \mu}{\abs{y_i}} 
% % &\leq 2(1-\mu)  + (\lambda + \mu - 1)(1 + \lambda_i)
% % \end{align*}
% % Notice that $(1 + \lambda_i) \geq 1$ and $\lambda \geq \frac{2R}{\abs{y_i}}$. 
% % We conclude that for any permissible $\mu$ and for $\lambda$ as above, that Eq.~\eqref{eq:poa-condition} holds. 

% \subsection{Differing alpha}

% Let $B = ((I - A) L + A)^{-1} A$. Notice $z^\prime = B \bs^\prime$. 

% First, it can be shown that: 
% \begin{align*}
% C(z^\prime)
% &= (z^\prime - \bs)^T A (z^\prime - s)
% + (z^\prime)^T (I - A)D z^\prime - 2(z^\prime)^T (I - A) W z^\prime
% + \sum_{i \in [n], j \in [n]} w_{ij} \bz_j^2 (1 - \alpha_i)
% \end{align*}

% Here $D$ is the degrees matrix and $W$ the weighted adjacency matrix. 

% We want to analyze for $\kappa > 1$ and $\mu \in (0,1)$, 
% \begin{align*}
% (1 - \mu) C(s^\prime) + (s - s^\prime)^T \nabla_{s^\prime} C(s^\prime) 
% \leq \kappa C(s)
% \end{align*}

% Let $M_1 = 2B^T AB + 2B^T (I - A) D B - 4 B^T (I - A) B$. 

% First, we argue that: 
% \begin{align*}
% \nabla_{s^\prime} C(s^\prime) 
% &= M_1 s^\prime - 2 B^T A s
% + 2 \sum_{ij} w_{ij} (1 - \alpha_i) (B^T e_j e_j^T B) s^\prime 
% \end{align*}
% Letting $M_2 = 2 \sum_{ij} w_{ij} (1 - \alpha_i) (B^T e_j e_j^T B)$, we have that
% \begin{align*}
% \nabla_{s^\prime} C(s^\prime) 
% &= (M_1 + M_2) s^\prime - 2 B^T A s
% \end{align*}
% So, 
% \begin{align*}
% (s - s^\prime)^T \nabla_{s^\prime} C(s^\prime) 
% &= s^T (M_1 + M_2 + 2 AB^T) s^\prime 
% + s^T (-2B^T A) s 
% + (s^\prime)^T (- M_1 - M_2) s^\prime 
% \end{align*}
% Next, let $D_2$ be the diagonal matrix with $D_{2;j} = \sum_{i=1}^{n} W_{ij} (1 - \alpha_i)$. Then, 
% \begin{align*}
% C(z^\prime) = z^\prime 
% \big( A + (I - A) D - 2(I - A)W + D_2
% \big) z^\prime  
% + s^T (-AB - B^T A) z^\prime 
% + s^T A s
% \end{align*}

% Therefore, let $M_3 = A + (I - A) D - 2(I - A)W + D_2$. Then, 
% \begin{align*}
% C(s^\prime) &= (s^\prime)^T B^T M_3 B s^\prime 
% + s^T (- AB - B^T A)B s^\prime 
% + s^T A s \\ 
% C(s) &= s^T B^T M_3 B s
% + s^T (- AB - B^T A)B s
% + s^T A s \\ 
% \end{align*}

% Let $M_4 = B^T M_3 B - AB^2 - B^T AB + A$. 

% \begin{align*}
% \begin{bmatrix} 
% y^\prime \\ y
% \end{bmatrix}^T 
% \begin{bmatrix}
% F & -\frac{1}{2} H \\
% -\frac{1}{2} H & G \end{bmatrix}
% \begin{bmatrix} 
% y^\prime \\ y
% \end{bmatrix} > 0
% \end{align*}