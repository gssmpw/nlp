% !TEX root = ./main.tex
\section{Price of Misreporting}
In Section~\ref{sec:strategic_opinion_formation}, we saw that strategic manipulation can substantially affect network outcomes via the Polarization Ratio and Disruption Ratio. We now give an upper bound for the Price of Misreporting (Eq.~\eqref{eq:pom}), which is the analogue of the Price of Anarchy in our setting. The $\pom$ measures the total cost paid by agents under the corrupted equilibrium $\bz^\prime$, versus the total cost under the non-corrupted $\bz$. Since the cost captures an agent's deviation from her {\em truthful} intrinsic opinion as well as her deviation from the expressed opinions of her neighbors, it is a natural measure of the network's discord at equilibrium. 

Theorem~\ref{thrm:pom_shared_alpha} shows that the PoM is small when the spectral radius of the Laplacian is small, and when agents are somewhat susceptible to their neigbhors ($\alpha \not \to 0$). Note that the spectral radius can be replaced by a degree bound: if $d_{\textup{max}}$ is the maximum degree of the graph, then $\lambda_n \leq 2 d_{\textup{max}}$. So the PoM is small if the maximum degree is small.
% If either of these conditions fail, our upper bound for the PoM can grow arbitrarily. 

\begin{theorem}
Suppose all agents deviate ($S = [n]$) and there exists $\alpha$ such that $\alpha_i = \alpha$ for all $i$.
Let $\tilde \alpha = \alpha / (1 - \alpha)$, and $\lambda_n$ be the spectral radius of the Laplacian. Then the price of misreporting is bounded as:
    \begin{align*}
        \pom \le \frac {(\lambda_n + 4 \tilde \alpha) (\lambda_n + \tilde \alpha)^2} {\tilde \alpha^5} = O \left ( \max \left \{ \frac {\lambda_n} {\tilde \alpha^5}, \frac {1} {\tilde \alpha^2} \right \} \right ).
    \end{align*}
\label{thrm:pom_shared_alpha}
\end{theorem}

% \ajcomment{Define }
\begin{proof}[Proof of Theorem~\ref{thrm:pom_shared_alpha}.]
First, we set $\tilde \alpha = \alpha / (1 - \alpha)$. By substituting $\bz = B \bs$ we can show by straightforward algebra that $C(\bz) / (1 - \alpha) = \bs^T Q \bs$ where $Q \succeq 0$ with 

\begin{align}
    Q = B L B + \tilde \alpha (I - 2B + B^2)
\end{align}
Since $Q \succeq 0$, it has eigendecomposition $Q = U \Lambda_Q U^T$. Moreover, $U$ is precisely the matrix of eigenvectors for the Laplacian. The eigenvalues of $Q$ can be shown to be $\tilde \alpha^2 / (\lambda_i + \tilde \alpha)$. Therefore, $C(z) = \bs^T Q \bs \ge \frac {\tilde \alpha^2} {\lambda_n + \tilde \alpha} \norm \bs \norm_2^2$. 

Next, let $\diag (B)$ be the diagonal matrix with entries $B_{ii}$ and $\widetilde {\diag (B)}$ be as in \cref{theorem:all_deviation}. In the proof of \cref{theorem:all_deviation}, we show that $\bs^\prime = (1/\tilde \alpha) B^{-1} \widetilde {\diag (B)} \bs$ and $\bz^\prime = (1 /\tilde \alpha) \widetilde{\diag (B)} \bs$, which similarly implies (after algebraic operations) that $C(\bz^\prime) / (1 - \alpha) = \bs^T Q^\prime \bs$ where: 

\begin{align*}
    Q^\prime := \frac {1} {\tilde \alpha^2} \widetilde{\diag (B)} L \widetilde {\diag (B)} + \frac {1} {\tilde \alpha} B^{-1} \left ( \widetilde{\diag (B)} \right )^2 B^{-1} - 2 \frac {1} {\tilde \alpha} B^{-1} \left ( \widetilde{\diag (B)} \right )^2 + \frac {1} {\tilde \alpha} \left ( \widetilde{\diag (B)} \right )^2
\end{align*}

Note that $Q^\prime$ cannot be diagonalized since, in general, $\widetilde{\diag (B)}$ has a different eigenbasis than $L$. However, we note that: 
\begin{align} \label{eq:ineq_Bi}
    \norm \widetilde {\diag (B)} \norm_2 & = \max_i B_{ii} \le \norm B \norm_2 = 1, \\
    \norm B^{-1} \norm_2 & = \max_i \frac {\lambda_i + \tilde \alpha} {\tilde \alpha} = \frac {\lambda_n + \tilde \alpha} {\tilde \alpha}. \label{eq:ineq_Binv}
\end{align}
By the triangle inequality, the Cauchy-Schwarz inequality, and \cref{eq:ineq_Bi,eq:ineq_Binv}, we have that: 
\begin{align*}
    \norm Q^\prime \norm_2 & \le \frac {1} {\tilde \alpha^2} \norm L \norm_2 \left ( \norm \widetilde {\diag (B)} \norm_2 \right )^2 + \frac {1} {\tilde \alpha} \left ( \norm \widetilde {\diag (B)} \norm_2 \right )^2 \norm B^{-1} \norm_2^2 + \frac 2 {\tilde \alpha} \norm B^{-1} \norm_2 + \frac {1} {\tilde \alpha} \left ( \norm \widetilde {\diag (B)} \norm_2 \right )^2 \\
    & \le \frac {(\lambda_n + 4 \tilde \alpha) (\lambda_n + \tilde \alpha)} {\tilde \alpha^3}.
\end{align*}

Therefore $C(\bz^\prime) / (1 - \alpha) \le \frac {(\lambda_n + 4 \tilde \alpha) (\lambda_n + \tilde \alpha)} {\tilde \alpha^3} \norm \bs \norm_2^2$. Hence,
\begin{align}
    \frac {C(\bz^\prime)} {C(\bz)} \le \frac {(\lambda_n + 4 \tilde \alpha) (\lambda_n + \tilde \alpha)^2} {\tilde \alpha^5}.
\end{align}

Finally, we can simplify: % \ajcomment{The 64 can be imporved to $9$ I believe, if we care about constants.}
\begin{align}
    \frac {(\lambda_n + 4 \tilde \alpha) (\lambda_n + \tilde \alpha)^2} {\tilde \alpha^5} \le \frac {64 (\lambda_n + \tilde \alpha)^3} {\tilde \alpha^5} \le \frac {128 (\max \{ \lambda_n, \tilde \alpha \})^3} {\tilde \alpha^5}.
\end{align}    
\end{proof}

From Theorem~\ref{thrm:pom_shared_alpha}, we can show that the upper bound is minimized when $\lambda_n = \Theta (\tilde \alpha^3)$ and has a value of $O(1/\tilde \alpha^2)$. As we noted, Theorem~\ref{thrm:pom_shared_alpha} can be written with $d_{\textup{max}}$ in the place of $\lambda_n$ as well. 

Next, we give an easy generalization to the case of differing susceptibility. 
\begin{cor}[Price of Misreporting for Heterogeneous Susceptibility]
If the $\alpha_i$ are differing, let $\alpha_{\min} = \min_i \alpha_i$ and $\alpha_{\max} = \max_j \alpha_j$. Define $\tilde \alpha_{\min} = \frac{\alpha_{\min}}{1 - \alpha_{\max}}, \tilde \alpha_{\max} = \frac{\alpha_{\max}}{1 - \alpha_{\min}}$. The Price of Misreporting is bounded as:
\begin{align*}
        \pom \le \frac {1 -\alpha_{\min}} {1 - \alpha_{\max}}\frac {(\lambda_n + 4 \tilde \alpha_{\max}) (\lambda_n + \tilde \alpha_{\max})^2} {\tilde \alpha_{\min}}. 
        %= O \left ( \max \left \{ \frac {\lambda_n} {\tilde \alpha^5}, \frac {1} {\tilde \alpha^2} \right \} \right ).
    \end{align*}
\end{cor}
\begin{proof}
Note that $C(\bz^\prime) \le (1 - \alpha_{\min}) (\bz^\prime)^T L \bz^\prime + \alpha_{\max} \norm \bz^\prime - \bs \norm_2^2 = \overline C(\bz^\prime)$, and $C(\bz) \ge (1 - \alpha_{\max}) \bz^T L \bz + \alpha_{\min} \norm \bz - \bs \norm_2^2 = \underline C(\bz)$ where $\alpha_{\min} = \min_{i \in [n]} \alpha_i$, and $\alpha_{\max} = \max_{i \in [n]} \alpha_i$. Then, the same analysis of \cref{thrm:pom_shared_alpha} can be applied, since $\overline C(\bz^\prime) / \underline C(\bz) \ge C(\bz^\prime) / C(\bz)$. 
\end{proof}

Finally, we discuss how one may generalize Theorem~\ref{thrm:pom_shared_alpha} to the case where some agents are honest. 
% \ajcomment{Why not rewrite the theorem?}

\paragraph{Towards fine-grained $\pom$ guarantees.} Figure~\ref{fig:ratios_susceptibility_to_persuation} shows that the PoM is {\em not} monotonic in $\abs{S}$. As the number of strategic agents grows, the PoM can fall or grow, depending on the choice of $S$, network parameters, and so on. Therefore, we would like to give a version of Theorem~\ref{thrm:pom_shared_alpha} for {\em any} set of strategic agents $S \subset [n]$, not just the case of $S = [n]$. However, proving such a bound would require analyzing $S \times S$ principal submatrices of $B, L$ to obtain characterizations of the cost at the corrupted equilibrium $\bz^\prime$. In particular, we would require a {\em restricted invertibility} estimate to prove the analogue of Eq.~\eqref{eq:ineq_Binv}. To our knowledge, the best such estimates~\citep{marcus2022interlacing} are too lossy when $n - \abs{S}$ is large. We leave this question to future work. 

% \ajcomment{Todo comment on RIP and random principal submatrix.}

% \citep{rudelson2007sampling}


% bounds the PoM when all agents manipulate the network. 

% Hence for networks whose largest degree is sufficiently small, the PoM is well-behaved. 
%additionally shows that in networks where the largest degree is sufficiently small, the PoM is well-behaved.

% \newpage


% We use the local smoothness technique. The local smoothness technique has been used to bound the Price of Anarchy in coevolutionary opinion formation games (see, e.g. \citet{bhawalkar2013}). We give the following theorem due to \citet{bhawalkar2013}, which is an extended result from \citet{roughgarden2011local}. 

% \begin{theorem}[\cite{bhawalkar2013}]
% Let $\sigma$ denote a correlated equilibrium. Suppose for any outcome $z$, with respect to a fixed outcome $o$ and scalars $\mu < 1, \lambda > 0$, that: 
% \begin{align}
% C(z) + (o-z)^T \nabla_z C(z) \leq \lambda C(o) + \mu C(z).
% \end{align}
% Then, the correlated PoA is bounded as $\frac{\EE_{\sigma}[C(z)]}{C(o)} \leq \frac {\lambda} {(1-\mu)}$. 
% \end{theorem}

% Using this method gives PoF bounds against any correlated equilibrium, and hence any Nash equilibrium. We can use the same framework to bound the PoM \mpcomment{are you sure we can do this?} \mpcomment{otherwise we can just have the PoA though the PoM is more useful IMHO}

% \mpcomment{quick question: shall we use different $y$ because we also have $y$ in the PSNE? It is defined in the scope of a theorem so I am not sure if it is indeed a problem regarding notation; just it may make things easier to read}

% \ajcomment{Not clear how to deal with differing $\alpha_i$, because the eigenvectors of $B$ are no longer the same as $L$.}

% \begin{theorem} \label{theorem:pom}
% Suppose all network members are strategic ($S = [n]$). Let $L$ have eignevalues $\lambda_i$ and suppose that there exists $\alpha$ such that for all $i$, $\alpha_i = \alpha$. Let $\Tilde \alpha = \frac{\alpha}{1 - \alpha}$. For $\mu \in (0,1)$ and $i \in [n]$ let: 
% \[
% f(\mu, i) = \bigg(\lambda_i + \Tilde\alpha +\Tilde\alpha(1 + \lambda_i)^2 \bigg)^{-1}
% \bigg(\frac{(\lambda_i + 2 \Tilde\alpha + 2 \mu \Tilde\alpha (1 + \lambda_i))^2}{(\lambda_i + \alpha)(1 + \mu)} - \frac{2\Tilde\alpha\lambda_i}{(1 + \mu)} + \frac{1 - \mu}{1 + \mu} \Tilde\alpha (1 + \lambda_i)^2 
% \bigg)
% \]
% Let $\mu^* = \arg\inf_{\mu \in (0,1)} \max_{i \in [n]} f(\mu, i)$ and $i^* = \arg\max_{i \in [n]} f(\mu^*, i)$. Then, for all $\eps > 0$, 
% \[
% \pom \leq \frac{f(\mu^*, i^*) + \eps}{(1 - \mu^*)}
% \]
% %Let $s^\prime$ be any deviation. Let $L$ be the graph Laplacian and $U \in O(n,\RR)$ its eigenbasis. Let $y = Us$.
% % Suppose that $\norm s^\prime \norm_2 \leq R$. The correlated price of fooling is at most: 
% % \[
% % \pom \leq \frac{2R + 2}{\min\{1, \min\{\abs{y_j}: y_j \neq 0 \}\}}.
% % \]
% \end{theorem}

% A few remarks are in order: First, notice that we give a strict generalization of PoM because the bound is against correlated equilibria, which are a generalization of Nash equilibria. Therefore in restricted classes such as PSNEs, the PoM may be smaller. Second, our bound is for the case when all $n$ agents are strategic. If only a small subset of the nodes are strategic the PoM may be smaller. In fact, this can be read off from our proof: what changes is that the entries of some entries of $s^\prime$ are constrained to be zero, but the proof would otherwise be the same. 

% % Finally, notice that our bound depends on the norm of the deviation $R$, as well as the quantity $\min_{j: y_j \neq 0} \abs{y_j}$. The ladder incorporates graph structure through the eigenvectors $U$ of the Laplacian (these can be interpreted as spectral clustering assignments), as well as the intrinsic opinions $s$. In particular, if the (graph-filtered) opinions $y_j$ have small nonzero entries, this indicates that the PoM will be large, because these members will be more swayed. 

% \mpcomment{each cost has an $1 - \alpha_i$ in the pairwise disagreement term -- so I think the algebra is a bit different (even for $\alpha_i = \alpha$). Also the $\lambda$ of the smoothness can be renamed to $\kappa$ to avoid confusion with the eigenvalues.}
% \begin{proof}
% Let $C(z) = \sum_i c_i(z)$. We want to show that for some $\lambda > 0, 0 < \mu < 1$: 
% \begin{align}
% C(z^\prime) + (s-s^\prime)^T \nabla_{s^\prime} C(z^\prime) \leq \lambda C(z) + \mu C(z^\prime)
% \label{eq:poa-condition}
% \end{align}
% Now, up to scaling, 
% \begin{align*}
% \frac{1}{1 - \alpha_i} C(z^\prime) &= \sum_i C_i(z^\prime) \\
% &= \sum_i \big(\sum_j w_{ij}(z_i^\prime - z_j^\prime)^2\big) + \frac{\alpha_i}{1 - \alpha_i} (z_i^\prime - s_i)^2 \\
% &= \la z^\prime, L z^\prime \ra + \Tilde \alpha \norm z^\prime - s \norm_2^2 
% \end{align*}
% %Where $\Tilde \alpha > 0$ is the shared $\alpha = \alpha_i$. 
% From the above, we see that $\nabla_{z^\prime} C(z^\prime) = 2 Lz^\prime + 2 \Tilde \alpha z^\prime - 2 \Tilde \alpha s$. 
% \ajcomment{Need to replace all occurrences of $\alpha$ with $\Tilde \alpha$, and $\lambda$ with $\kappa$.}

% Next, recall $z^\prime = B s^\prime$ for $B = (L + \alpha I)^{-1}$ and $s^\prime \in \RR^n$ the strategic internal opinions. By the chain rule, $\nabla_{s^\prime} C(z^\prime) = 2 BLBs^\prime + 2 \alpha B^2 s^\prime - 2 \alpha Bs$. Therefore the LHS of Eq~\eqref{eq:poa-condition} becomes: 
% \begin{align*}
% C(z^\prime) + (s-s^\prime)^T \nabla_{s^\prime} C(z^\prime)
% &= \la s^\prime, BLB s^\prime \ra + \alpha \norm B s^\prime - s \norm_2^2 
% + (s-s^\prime)^T (2 BLBs^\prime + 2 \alpha B^2 s^\prime - 2 \alpha Bs) \\
% &= (s^\prime)^T (BLB + \alpha B^T B - 2 BLB - 2 \alpha B^T B)s^\prime \\
% &+ s^T (\alpha I - 2 \alpha B)s
% + s^T (-2\alpha B + 2BLB + 2\alpha B^2 + 2\alpha B)s^\prime
% \end{align*}
% On the other hand, the RHS of Eq~\eqref{eq:poa-condition} becomes: 
% \begin{align*}
% \lambda s^T (BLB + \alpha (B - I)^T (B-I))s 
% + \mu C(s^\prime)
% &= \lambda s^T (BLB + \alpha (B - I)^T (B-I))s  
% + \mu(\la s^\prime, BLB s^\prime \ra + \alpha \norm B s^\prime - s \norm_2^2 )\\
% &= (s^\prime)^T (\mu BLB + \mu \alpha B^T B) s^\prime \\
% &+ s^T (\lambda BLB + \lambda \alpha (B - I)^T (B-I) + \mu \alpha I)s  
% + s^T (-2 \mu \alpha B) s^\prime 
% \end{align*}
% Combining the above displays, we see that Eq~\eqref{eq:poa-condition} holds iff: 
% \begin{align*}
% (s^\prime)^T ((1 + \mu)BLB + (1 + \mu)\alpha B^T B)s^\prime 
% + s^T (\lambda BLB + \lambda \alpha (B-I)^T (B-I) + \mu \alpha I) s
% + s^T (-2\mu\alpha B) s^\prime \\
% > s^T (\alpha I - 2 \alpha B)s 
% + s^T (-2\alpha B + 2 BLB + 2 \alpha B^2 + 2 \alpha B) s^\prime 
% \end{align*}
% Since $L \succeq 0$ is symmetric, let $L = UDU^T$ for some $D \succeq 0$ and eigenbasis $U$. Then $B = (L + I)^{-1} = U (D + I)^{-1} U^T$. Let $\Tilde D = (D + I)^{-1}$, so $B = U \Tilde D U^T$. We can perform a change of variables, letting $y = U s$ and $y^\prime = U s^\prime$. The above display becomes: 
% \begin{align*}
% (y^\prime)^T ((1 + \mu)\Tilde D^2 D + (1 + \mu)\alpha \Tilde D^2)y^\prime 
% + y^T (\lambda \Tilde D^2 D + \lambda \alpha \Tilde D^2 - 2 \alpha \Tilde D^2 + (\lambda + \mu) \alpha I) y
% + y^T (-2\mu\alpha \Tilde D) y^\prime \\
% > y^T (\alpha I - 2 \alpha \Tilde D)y
% + y^T (-2\alpha \Tilde D + 2 \Tilde D^2 D + 2 \alpha \Tilde D^2 + 2 \alpha \Tilde D) y^\prime \\
% \iff 
% (y^\prime)^T ((1 + \mu)\Tilde D^2 D + (1 + \mu)\alpha \Tilde D^2)y^\prime 
% + y^T (\lambda \Tilde D^2 D + \lambda \alpha \Tilde D^2 - 2 \alpha \Tilde D^2 + 2 \alpha \Tilde D + (\lambda + \mu - 1) \alpha I) y \\
% > y^T (2 \Tilde D^2 D + 2 \alpha \Tilde D^2 + 2 \mu \alpha \Tilde D) y^\prime 
% \end{align*}
% Now, let
% \begin{align*}
% F &= (1 + \mu)\Tilde D^2 D + (1 + \mu)\alpha \Tilde D^2 \\
% G &= \lambda \Tilde D^2 D + \lambda \alpha \Tilde D^2 - 2 \alpha \Tilde D^2 + 2 \alpha \Tilde D + (\lambda + \mu - 1) \alpha I \\
% H &= 2 \Tilde D^2 D + 2 \alpha \Tilde D^2 + 2 \mu \alpha \Tilde D
% \end{align*}
% Then, the above condition becomes: 
% \begin{align*}
% \begin{bmatrix} 
% y^\prime \\ y
% \end{bmatrix}^T 
% \begin{bmatrix}
% F & -\frac{1}{2} H \\
% -\frac{1}{2} H & G \end{bmatrix}
% \begin{bmatrix} 
% y^\prime \\ y
% \end{bmatrix} > 0
% \end{align*}
% Letting $M = \begin{bmatrix}
% F & -\frac{1}{2} H \\
% -\frac{1}{2} H & G \end{bmatrix}$, it is sufficient to show $M \succ 0$. 

% Notice that $F, G, H$ are all diagonal and $M$ is Hermitian. Hence $M$ has $2n$ real eigenvalues. For $i \in [n]$, let $M^{(i)} = \begin{bmatrix}
% F_{ii} & -\frac{1}{2} H_{ii} \\
% -\frac{1}{2} H_{ii} & G_{ii} \end{bmatrix}$.

% The eigenvalues of $M$ are given by $\bigcup\limits_{i \in [n]} \{\lambda_1(M^{(i)}), \lambda_2(M^{(i)})\}$. Therefore, it suffices to show $M^{(i)} \succ 0$ for all $i$. The following conditions are necessary and sufficient: 
% \begin{align*}
% F_{ii} &> 0 \\
% G_{ii} &> 0 \\
% \frac{1}{4} H_{ii}^2 &\leq F_{ii} G_{ii}
% \end{align*}
% First, $F_{ii} > 0 \iff (1 + \mu)(\lambda_i + \alpha) > 0$. This is true as long as $\alpha > 0$. Next, letting $\kappa = \lambda$, 
% \begin{align*}
% G_{ii} &> 0 \\
% `\frac{\kappa \lambda_i + \kappa \alpha - 2 \alpha}{(1 + \lambda_i)^2} + \frac{2\alpha}{1 + \lambda_i} + (\kappa + \mu - 1) \alpha &> 0 \\
% \iff \kappa \lambda_i + \alpha \bigg(
% \kappa - 2 + 2 (1 + \lambda_i) + (\kappa + \mu - 1) (1 + \lambda_i)^2
% \bigg) &> 0
% \end{align*}
% Since the Laplacian is PSD, $(1 + \lambda_i) \geq 1$ for all $i$. Hence, a sufficient condition for $G_{ii} > 0$ is that $\alpha > 0$ and: 
% \begin{align*}
% \kappa - 2 + 2 + (\kappa + \mu - 1) &> 0 \\
% \iff 2 \kappa + \mu - 1 &> 0
% \end{align*}
% Since $\kappa \geq 1$ and $\mu \geq 0$, this is always true. Finally, 
% \begin{align*}
% \frac{1}{4} H_{ii}^2 &\leq F_{ii} G_{ii} \\
% \bigg(\frac{\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i)}{(1 + \lambda_i)^2}
% \bigg)^2 
% &\leq \bigg[\frac{(1 + \mu) (\lambda_i + \alpha)}{(1 + \lambda_i)^2} \\
% &\cdot 
% \frac{\kappa \lambda_i + (\kappa - 2) \alpha + 2 \alpha (1 + \lambda_i) + (\kappa + \mu - 1)\alpha (1 + \lambda_i)^2}{(1 + \lambda_i)^2} \bigg] \\
% \frac{(\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i))^2}{\lambda_i + \alpha} &\leq 
% (1 + \mu) \bigg(\kappa \lambda_i + (\kappa - 2) \alpha \\
% &+ 2 \alpha (1 + \lambda_i) + (\kappa + \mu - 1)\alpha (1 + \lambda_i)^2\bigg) \\
% \frac{(\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i))^2}{\lambda_i + \alpha} - 2 \alpha \lambda_i 
% &\leq \bigg[(1 - \mu)(1 + \mu) \frac{\kappa}{1 - \mu} \big(\lambda_i + \alpha + \alpha (1 + \lambda_i)^2\big) \\
% &- (1 - \mu) \alpha (1 + \lambda_i)^2 
% \bigg]\\
% % \bigg(\frac{1}{\lambda_i + \alpha + \alpha (1 + \lambda_i)^2}\bigg) 
% \bigg(\frac{(\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i))^2}{\lambda_i + \alpha} - 2 \alpha \lambda_i + (1 - \mu) \alpha (1 + \lambda_i)^2
% \bigg)
% &\leq (1 - \mu^2) \frac{\kappa}{1-\mu}\big(\lambda_i + \alpha + \alpha (1 + \lambda_i)^2 \big) 
% \end{align*}
% We conclude that it is sufficient for $\kappa, \mu$ to be such that: 
% \[
% \bigg(\lambda_i + \alpha + \alpha (1 + \lambda_i)^2 \bigg)^{-1}
% \bigg(\frac{(\lambda_i + 2 \alpha + 2 \mu \alpha (1 + \lambda_i))^2}{(\lambda_i + \alpha)(1 + \mu)} - \frac{2 \alpha \lambda_i}{(1 + \mu)} + \frac{1 - \mu}{1 + \mu} \alpha (1 + \lambda_i)^2 
% \bigg) \leq \kappa 
% \]
% Notice the LHS is $f(\mu,i)$. Choosing $\mu^* = \arg \inf_{\mu \in (0,1)} \max_{i \in [n]} f(\mu,i)$, and setting $\kappa = f(\mu^*, i^*)$
% ensures that $\frac{1}{4} H_{ii}^2 \leq F_{ii} G_{ii}$ for all $i$. Hence we conclude that $M \succ 0$.  
% \end{proof}

% \begin{cor}
% If $i^* = 1$, where the eigenvalues of $L$ are ordered $\lambda_1 \leq \dots \lambda_n$ then: 
% \[
% \pom \leq \frac{5}{2} + \eps
% \]
% for arbitrarily small $\eps > 0$. 
% \end{cor}
% \begin{proof}
% Since $L$ is a graph Laplacian, we have $L \bm{1} = \bm{0}$, so $\lambda_1 = 0$. We can simplify $f(\mu, 1)$ as: 
% \begin{align*}
% f(\mu, 1) &= \frac{- \mu + 4 \left(\mu + 1\right)^{2} + 1}{2 \left(\mu + 1\right)} \\
% \frac{d}{d\mu} f(\mu, 1) &= \frac{2 \mu^{2} + 4 \mu + 1}{\mu^{2} + 2 \mu + 1} \\
% \frac{d^2}{d\mu^2} f(\mu, 1) &= \frac{2}{\mu^{3} + 3 \mu^{2} + 3 \mu + 1}
% \end{align*}
% Notice there is no dependence on $\Tilde \alpha$. Since $\mu > 0$, $\frac{d^2}{d\mu^2} > 0$ for all $\mu$, so $\mu^* \in \{0, 1\}$. We can compute that $f(0, 1) = 5/2$ and $f(1,1) = 4$. Hence, by setting $\mu = \eps$ for $\eps > 0$, we conclude that $\pom \leq \frac{5}{2} + \eps$ for arbitrarily small $\eps > 0$. 
% \end{proof}
% \ajcomment{This is useless because $i^* \neq 1$.}

% \begin{prop}
% Let $g(\mu, \lambda) = \frac{1}{1-\mu}f(\mu, i)$ if $\lambda_i = \lambda$. We claim that for all $\lambda > 0$, $\Tilde \alpha \in (0,1)$ and $\mu \in (0,1)$ that: 

% 1. $\frac{\del g}{\del \mu} \neq 0$

% 2. $\frac{\del^2 g}{\del \mu^2} > 0$. 
% % Let $f(\mu, \lambda)$ be defined by letting $\lambda_i = \lambda$. For all $\Tilde \alpha, \mu \in (0,1)$, $\frac{df(\mu, \lambda)}{d\lambda} < 0$. 
% \end{prop}

% \begin{prop}
% For all $\Tilde \alpha, \mu \in (0,1)$ and $\lambda \geq 0$, we have $\frac{dg(\mu, \lambda)}{d\lambda} < 0$
% \end{prop}

% The above two propositions imply that $i^* = n$ and that the optimal $\mu^*$ is within $\{0,1\}$. We can read off: 
% \[
% g(\mu, \lambda) = \frac{\tilde \alpha \left(\tilde \alpha + \lambda\right) \left(2 \lambda + \left(\lambda + 1\right)^{2} \left(\mu - 1\right)\right) - \left(2 \tilde \alpha \mu \left(\lambda + 1\right) + 2 \tilde \alpha + \lambda\right)^{2}}{\left(\tilde \alpha + \lambda\right) \left(\mu - 1\right) \left(\mu + 1\right) \left(\tilde \alpha \left(\lambda + 1\right)^{2} + \tilde \alpha + \lambda\right)}
% \]
% From this, it is clear that $\lim\limits_{\mu \to 1} g(\mu, \lambda) = \infty$. Therefore $\mu^* = 0$. 
% \ajcomment{Weird things happening, somehow we get that the PoM is at most 1 numerically...this shouldn't happen. I will revisit the math with $\alpha_i$ differing and see what happens.}

% % \begin{proof}
% % This can be read off from: 
% % \[
% % \frac{- \alpha \left(\lambda + 1\right) \left(\mu - 1\right) \left(\mu + 1\right) \left(8 \alpha \mu \left(\lambda + 1\right) + 8 \alpha + 4 \lambda - \left(\alpha + \lambda\right) \left(\lambda + 1\right)\right) - \left(\mu - 1\right) \left(\alpha \left(\alpha + \lambda\right) \left(2 \lambda + \left(\lambda + 1\right)^{2} \left(\mu - 1\right)\right) - \left(2 \alpha \mu \left(\lambda + 1\right) + 2 \alpha + \lambda\right)^{2}\right) - \left(\mu + 1\right) \left(\alpha \left(\alpha + \lambda\right) \left(2 \lambda + \left(\lambda + 1\right)^{2} \left(\mu - 1\right)\right) - \left(2 \alpha \mu \left(\lambda + 1\right) + 2 \alpha + \lambda\right)^{2}\right)}{\left(\alpha + \lambda\right) \left(\mu - 1\right)^{2} \left(\mu + 1\right)^{2} \left(\alpha \left(\lambda + 1\right)^{2} + \alpha + \lambda\right)}
% % \]
% % \end{proof}

% \ajcomment{This can be shown for $f/(1-\mu)$ as well.}

% Therefore, we know that for any $\mu$, that either $i^* = n$. 
% % \clearpage





% \subsection{Differing alpha}

% \begin{theorem}
% Suppose that agent $i$ has $\alpha_i \in (0,1)$. Then, the POM is bounded as blah...
% \ajcomment{Todo finish the calculation.}
% \label{thrm:pomgeneral}
% \end{theorem}

% \begin{proof}
% Let $B = ((I - A) L + A)^{-1} A$. Notice $z^\prime = Bs^\prime$. 

% First, it can be shown that: 
% \begin{align*}
% C(z^\prime)
% &= (z^\prime - \bs)^T A (z^\prime - s)
% + (z^\prime)^T (I - A)D z^\prime - 2(z^\prime)^T (I - A) W z^\prime
% + \sum_{i \in [n], j \in [n]} w_{ij} z_j^2 (1 - \alpha_i)
% \end{align*}

% Here $D$ is the degrees matrix and $W$ the weighted adjacency matrix. 

% We want to analyze for $\kappa > 1$ and $\mu \in (0,1)$, 
% \begin{align*}
% (1 - \mu) C(s^\prime) + (s - s^\prime)^T \nabla_{s^\prime} C(s^\prime) 
% \leq \kappa C(s)
% \end{align*}

% Let $M_1 = 2B^T AB + 2B^T (I - A) D B - 4 B^T (I - A) B$. 

% First, we argue that: 
% \begin{align*}
% \nabla_{s^\prime} C(s^\prime) 
% &= M_1 s^\prime - 2 B^T A s
% + 2 \sum_{ij} w_{ij} (1 - \alpha_i) (B^T e_j e_j^T B) s^\prime 
% \end{align*}
% Letting $M_2 = 2 \sum_{ij} w_{ij} (1 - \alpha_i) (B^T e_j e_j^T B)$, we have that
% \begin{align*}
% \nabla_{s^\prime} C(s^\prime) 
% &= (M_1 + M_2) s^\prime - 2 B^T A s
% \end{align*}
% So, 
% \begin{align*}
% (s - s^\prime)^T \nabla_{s^\prime} C(s^\prime) 
% &= s^T (M_1 + M_2 + 2 AB^T) s^\prime 
% + s^T (-2B^T A) s 
% + (s^\prime)^T (- M_1 - M_2) s^\prime 
% \end{align*}
% Next, let $D_2$ be the diagonal matrix with $D_{2;j} = \sum_{i=1}^{n} W_{ij} (1 - \alpha_i)$. Then, 
% \begin{align*}
% C(z^\prime) = z^\prime 
% \big( A + (I - A) D - 2(I - A)W + D_2
% \big) z^\prime  
% + s^T (-AB - B^T A) z^\prime 
% + s^T A s
% \end{align*}

% Therefore, let $M_3 = A + (I - A) D - 2(I - A)W + D_2$. Then, 
% \begin{align*}
% C(s^\prime) &= (s^\prime)^T B^T M_3 B s^\prime 
% + s^T (- AB - B^T A)B s^\prime 
% + s^T A s \\ 
% C(s) &= s^T B^T M_3 B s
% + s^T (- AB - B^T A)B s
% + s^T A s \\ 
% \end{align*}

% Let $M_4 = B^T M_3 B - AB^2 - B^T AB + A$. Then, we have: 

% \begin{align}
% \begin{bmatrix} 
% s^\prime \\ s
% \end{bmatrix}^T 
% \begin{bmatrix}
% F &  H^T \\
% H & G \end{bmatrix}
% \begin{bmatrix} 
% s^\prime \\ s
% \end{bmatrix} > 0
% \label{eq:sufficient}
% \end{align}

% Where the matrices $F, G, H$ are defined as: 
% \begin{align*}
% F &= M_1 + M_2 - (1-\mu) B^T M_3 B \\
% G &= \kappa M_4 + 2 B^T A + (1-\mu) A \\
% H &= \frac{1}{2} ((1 - \mu) AB^2 + (1 - \mu) B^T A B - M_1 - M_2 - 2 AB^T )
% \end{align*}

% Now, notice that $s$ is fixed. We only need to show that Eq~\ref{eq:sufficient} holds for all $s^\prime$. Let $f(s^\prime)$ be equal to the Eq.~\ref{eq:sufficient} when $s$ is fixed. Then, we see that $\nabla_{s^\prime}f(s^\prime) = (F + F^T)s^\prime + 2 H^T s$, and that $\nabla_{s^\prime}^2 f(s^\prime) = F + F^T$. Therefore if $F + F^T \succeq 0$, it is sufficient to show that $f(s^\prime) \geq 0$ for $s^\prime = (F + F^T)^{-1} 2H^T s$. 

% To this end, notice: 
% \begin{align*}
% F + F^T = B^T \bigg(
% (14 - 2\mu) A + (4 - 2\mu) D_2 + 4D - 4 AD - 8I 
% + (2 - 2 \mu)(I - A) L - (2 - 2 \mu) W (I - A)
% \bigg) B
% \end{align*}
% % This can be shown to be PD through Gershgorin disc theorem and for large enough $A, D$. 

% Next, let 
% \[
% M_5 := (14 - 2\mu) A + (4 - 2\mu) D_2 + 4D - 4 AD - 8I 
% + (2 - 2 \mu)(I - A) L - (2 - 2 \mu) W (I - A)
% \]
% By Lemma~\ref{lemma:fftpd}, $M_5 \succeq 0$. This immediately implies $F + F^T \succeq 0$. 
% % Next, let the middle term of $F + F^T$ be $M_5$. 

% Next, let $H_0 = 2H$ and $M_6 = (F + F^T)^{-1}$. Let $s^\prime = (F + F^T)^{-1} 2H^T s$. 
% \begin{align*}
% f(s^\prime) &= \frac{1}{2} (s^\prime)^T (F + F^T) s^\prime + 2 (s^\prime)^T H^T s + s^T G s \\
% &= \frac{1}{2} s^T H_0 M_6 H_0^T s 
% + s^T H_0 M_6 H_0^T s + s^T G s \\
% &= s^T \bigg(
% \frac{3}{2} H_0 M_6 H_0^T + G 
% \bigg) s
% \end{align*}
% Next, let $B_0 = (I - A)L + A$. Notice $B = B_0^{-1} A$, so: 
% \begin{align*}
% M_6 &= (F + F^T)^{-1} \\
% &= A^{-1} B_0 M_5^{-1} B_0^T A^{-1} \\
% &= ((A^{-1} - I)L + I) M_5^{-1} (L (A^{-1} - I) + I)
% \end{align*}
% By Lemma~\ref{lemma:fftpd}, we know $M_5 \succ 0$ so $M_5^{-1} \succ 0$. We immediately obtain $M_6 \succ 0$, so $H_0 M_6 H_0^T \succeq 0$. 
% \ajcomment{Justify the step with $L$ here.}


% Hence, since $s^T G s = \frac{1}{2}s^T (G + G^T) s$ a sufficient condition is to now show $s^T (G + G^T) s \geq 0$. 
% Next, we analyze $G$. Notice that: 
% \begin{align*}
% G &\succeq 0 \\
% \iff \kappa B^T M_3 B + (\kappa + 1 - \mu) A + 2 B^T A &\succeq \kappa (AB^2 + B^T A B) 
% \end{align*}
% Moreover, let $g = \sum_i \frac{1-\alpha_i}{\alpha_i} D_{ii}$. By Corollary~\ref{cor:suminv}, we have: 
% \begin{align*}
% B^T M_3 B &= A^{-1} (B_0^T)^{-1} (B_0 + D_2 - (I - A) W) B_0^{1} A \\
% &= A (B_0^T)^{-1} A +  A (B_0^T)^{-1}(D_2 - (I - A) W) B_0^{-1} A \\
% &= A \bigg[
% \big(A^{-1} - \frac{1}{1+g} A^{-1} L(I - A)A^{-1}\big)
% \big(I + (D_2 - (I - A)W)\big)
% \big(
% A^{-1} - \frac{1}{1 + g} A^{-1} (I - A)L A^{-1}\big)
% \bigg] A
% \end{align*}

% Next, 
% \begin{align*}
% AB^2 + B^T A B &= A( (B_0^{-1} + (B_0^T)^{-1}) A B_0^{-1} )A \\
% &= A \bigg[
% A^{-1} - \frac{1}{1 + g} A^{-1} \big(
% (I - A)L + L(I - A)\big) A^{-1}
% \bigg] A 
% \Bigg[
% A^{-1} - \frac{1}{1 + g} A^{-1} (I - A)L A^{-1}
% \bigg] A \\
% &= \Bigg[
% I - \frac{1}{1 + g} \big(
% (I - A)L + L(I - A)\big) A^{-1}
% \bigg]
% \bigg[
% A - \frac{1}{1 + g}(I - A)L
% \bigg]
% \end{align*}
% Next, let $M_7 = B^T M_3 B - (AB^2 + B^T AB)$. Let $h = \frac{1}{1 + g}$ for shorthand and $\barr{A} = I - A$. We want to show $G \succeq 0$. A sufficient condition for this is: 
% \[
% \frac{\kappa}{1-\mu} M_7
% + A + \frac{2}{1-\mu} I 
% \succeq \frac{2}{1-\mu} (h \barr{A} L)
% \]

% Fix some unit vector $v \in \RR^n$. First, notice if $v = \frac{1}{\sqrt{n}} \bm{1}$ then: 
% \begin{align*}
% v^T (\frac{\kappa}{1-\mu} M_7
% + A + \frac{2}{1-\mu} I ) v &= 
% v^T A v + \frac{2}{1-\mu}
% + v^T (\frac{\kappa}{1-\mu} M_7)v \\
% &= v^T (\frac{\kappa}{1-\mu} M_7)v
% + \frac{2}{1-\mu} + \frac{1}{n}\sum_i \alpha_i \\ 
% v^T ( \frac{2}{1-\mu} h \barr{A} L) v &= 0
% \end{align*}
% Therefore, it suffices to analyze $v$ such that $v \perp \bm{1}$. \ajcomment{Come back and plug in the $\lambda = 0$ case.}
% These are the eigenvectors of $L$, so $Lv =\lambda v$ for some $\lambda \geq 0$ (assuming the graph is connected). 

% Now, 
% \begin{align*}
% v^T (\frac{\kappa}{1-\mu} M_7
% + A + \frac{2}{1-\mu} I ) v
% &\geq v^T ( \frac{2}{1-\mu} h \barr{A} L) v \\
% \kappa v^T M_7 v + 2 + (1 - \mu) v^T A v 
% &\geq 2\lambda h v^T (A^{-1} - I) v 
% \end{align*}
% Next, 
% \begin{align*}
% v^T M_7 v &= 
% (v^T - h\lambda v^T \barr{A}A^{-1})
% (v - h\lambda v \barr{A} A^{-1})
% + (v^T - h\lambda v^T \barr{A}A^{-1})
% (D_2 - \barr{A}W)
% (v - h\lambda v \barr{A} A^{-1}) \\
% &+ 3h\lambda v^T \barr{A} v - v^T A v 
% + \lambda h^2 v^T (\barr{A} L + L \barr{A})v
% - \lambda h^2 v^T (\barr{A} L + L \barr{A})A^{-1} v \\
% &= (v - h\lambda v \barr{A} A^{-1})^T (v - h\lambda v \barr{A} A^{-1})
% + (v - h\lambda v \barr{A} A^{-1})^T
% (D_2 - \barr{A}W)
% (v - h\lambda v \barr{A} A^{-1}) \\
% &+  3h\lambda v^T \barr{A} v - v^T A v 
% + 2 \lambda^2 h^2 v^T \barr{A} v
% - \lambda^2 h^2 v^T \barr{A} A^{-1} v 
% - \lambda h^2 v^T \barr{A} L A^{-1} v \\
% &= (v - h\lambda v \barr{A} A^{-1})^T
% \bigg(
% I + D_2 - \barr{A} W 
% \bigg) (v - h\lambda v \barr{A} A^{-1})\\
% &+ v^T \bigg(
% 3h\lambda \barr{A} - A + 2 \lambda^2 h^2 \barr{A} - \lambda^2 h^2 \barr{A}A^{-1} - \lambda h^2 \barr{A} L A^{-1}
% \bigg) v
% \end{align*}
% It suffices to show analyze the matrices in the middle of each quadratic form. 

% First, by the Gershgorin disc theorem, a sufficient condition for $I + D_2 - \barr{A} W  \succeq 0$ is that for all $j$, 
% \begin{align*}
% 1 + \sum_{k \neq j} (\alpha_j - \alpha_k) W_{kj} \geq 0
% \end{align*}
% In other words, if $\alpha_j < \alpha_k$, then $W_{kj}$ must be relatively small. In particular, if $j$ is the person with the least $\alpha_j$, then they must have small degree. 

% % Next, to analyze the other matrix, notice that 
% % \begin{align*}
% % v^T \barr{A} L A^{-1} v &= (1 - \alpha_{max}) (1 / \alpha_{max}) \lambda^2 
% % + v^T (\barr{A} - (1 - \alpha_{max}) I) L (A^{-1} - (1 / \alpha_{max}) I)v \\
% % &\leq (1 - \alpha_{max}) (1 / \alpha_{max}) \lambda^2
% % + (\alpha_{max} - \alpha_{min}) \lambda_n^2 (\frac{1}{\alpha_{min}} - \frac{1}{\alpha_{max}}) 
% % \end{align*}
% % Where $\lambda_n$ is the maximum eigenvalue. Notice that $\lambda_n \leq 2 \max_{i} D_{ii}$. 
% Now, we want to show that: 
% \begin{align*}
% \kappa v^T \bigg(
% 3h\lambda \barr{A} - A + 2 \lambda^2 h^2 \barr{A} - \lambda^2 h^2 \barr{A}A^{-1} - \lambda h^2 \barr{A} L A^{-1}
% \bigg) v
% + v^T \bigg(
% 2 I + (1-\mu)A + 2 \lambda h (I - A^{-1})
% \bigg) v
% &\geq 0
% \end{align*}
% A sufficient condition is that for all $i$, 
% \begin{align*}
% \kappa &\geq \frac{2 \lambda h (\frac{1}{\alpha_i} - 1) - 2 - (1-\mu)\alpha_i}{\alpha_i 
% + (\frac{1}{\alpha_i} - 2)\lambda^2 h^2 (1 - \alpha_i) 
% + \lambda h^2 \norm \barr{A}LA^{-1}\norm_2
% - 3 h \lambda (1-\alpha_i)
% }
% \end{align*}
% Therefore, it remains to upper bound the RHS. Notice since $\alpha_i > 0$ that we can set $\mu > 0$ arbitrarily small. Therefore, it remains to upper bound: 
% \begin{align*}
% \frac{2 \lambda h (\frac{1}{\alpha_i} - 1)}{\alpha_i 
% + (\frac{1}{\alpha_i} - 2)\lambda^2 h^2 (1 - \alpha_i) 
% + \lambda h^2 \norm \barr{A}LA^{-1}\norm_2
% - 3 h \lambda (1-\alpha_i)
% }
% \end{align*}

% % \[
% % v^T (3h\lambda \barr{A} - A + 2 \lambda^2 h^2 \barr{A} - \lambda^2 h^2 \barr{A}A^{-1} - \lambda h^2 \barr{A} L A^{-1}) v \geq 0
% % \]

% \ajcomment{Now combine back with the $\kappa$ stuff to get a polynomial in $h$, $\lambda$, and the $\alpha_i$.}

% A sufficient condition for the above display is that for all $i$, 
% \begin{align*}
% \kappa &\geq \frac{2(1-\alpha_i)}{(\lambda h)^{-1} \alpha_i^2 + (1 - 2\alpha_i) \lambda h (1 - \alpha_i) + h\lambda \norm \barr{A} L A^{-1}\norm_2 - 3\alpha_i (1-\alpha_i)}
% \end{align*}
% %Suppose $L = \sum_i \lambda_i v_i v_i^T$. Then we can write $s$ in the Laplacian eigenbasis as $s = \sum_i q_i v_i$. 

% % \ajcomment{Todo finish.}
% \end{proof}
% % Notice that by the Gershgoring disc theorem, that the eigenvalues of $D_2 - (I - A) W$ are bounded within $\bigcup\limits_{j \in [n]} \bigg(\sum_{i} W_{ij} (1 - \alpha_i) \pm \sum_{i} \abs{W_{ij} (1 - \alpha_j)}\bigg)$. Since all weights are non-negative $W_{ij} \geq 0$, 
% % we see that a sufficient condition for eigenvalues to be bounded within $\pm C$ is that: 
% % \[
% % \forall j: 
% % \bigg[\frac{- C + \sum_{i} W_{ij} \alpha_i }{\sum_{i} W_{ij}} \leq 
% % \alpha_j \leq \frac{C + \sum_{i} W_{ij} \alpha_i}{\sum_{i} W_{ij}}\bigg]
% % \]


% % $M_6 = (F + F^T)^{-1} = A^{-1} B_0 M_5^{-1} B_0^T A^{-1}$

% % Now, it remains to show that if $K = \begin{bmatrix}
% % F &  H^T \\
% % H & G \end{bmatrix}$ then $K \succeq 0$. We can analyze this through Schur complements. Notice that $M_2$ is diagonal, $M_1$ is symmetric. However $M_3$ fails to by symmetric...
% \begin{lemma}[Sherman-Morrison]
% Let $A, B$ be square matrices such that $A^{-1}$ exists. Then if $g = tr(BA^{-1})$, then: 
% \[
% (A + B)^{-1} = A^{-1} - \frac{1}{1 + g} A^{-1} B A^{-1}
% \]
% \end{lemma}

% \begin{cor}
% Let $B_0 = (I - A)L + A^{-1}$. Then, 
% \begin{align*}
% B_0^{-1} &= A^{-1} - \frac{1}{1 + g_1} A^{-1} (I - A)L A^{-1} \\
% (B_0^T)^{-1} &= A^{-1} - \frac{1}{1 + g_2} A^{-1} L(I - A) A^{-1}
% \end{align*}
% Where $g_1 = g_2 = \sum_i \frac{1-\alpha_i}{\alpha_i} D_{ii}$. 
% \label{cor:suminv}
% \end{cor}

% \begin{lemma}
% Let $M_5$ be as in the proof of Theorem~\ref{thrm:pomgeneral}. 
% \[
% M_5 := (14 - 2\mu) A + (4 - 2\mu) D_2 + 4D - 4 AD - 8I 
% + (2 - 2 \mu)(I - A) L - (2 - 2 \mu) W (I - A)
% \]
% Let $d_{min} = \min_i D_{ii}$ and $\alpha_{min} = \min_i \alpha_i$. 
% Then $M_5 \succeq 0$ if 
% \[
% \mu \leq \alpha_{min}(7 + d_{min}) + (d_{min} - 4)
% \]
% If the RHS above is negative, then any $\mu \in (0,1)$ ensures that $M_5 \succeq 0$. 
% \label{lemma:fftpd}
% \end{lemma}

% \begin{proof}
% First, notice that since $L = D - W$, that: 
% \[
% M_5 := (14 - 2\mu) A + (4 - 2\mu) D_2 + 4D - 4 AD - 8I 
% + (2 - 2 \mu)(I - A) L 
% + (2 - 2\mu) L (I - A)
% - (2 - 2 \mu) D (I - A)
% \]
% Now, since $4D - 4AD \succeq 0$, it suffices to show that: 
% \begin{align*}
% (14 - 2\mu) A + (4 - 2\mu) D_2 
% + (2 - 2 \mu)((I - A) L + ((I - A) L)^T)
% \succeq 8I + (2 - 2\mu) D(I - A)
% \end{align*}
% Notice all but one of the matrices here is diagonal. Moreover, since $L \succeq 0$ and $(I - A) \succ 0$, we know $(2 - 2 \mu)((I - A) L + ((I - A) L)^T) \succeq 0$. Therefore, considering the $(i,i)$ term of the remaining diagonal matrices, we see that a sufficient condition is that for all $i$, 
% \begin{align*}
% \mu &\leq \frac{7 \alpha_i + 2 D_{2;ii} - 4 - D_{ii}(1-\alpha_i)}{\alpha_i + D_{2;ii} - D_{ii}(1-\alpha_i)} \\
% \iff (7 - \mu)\alpha_i + (2 - \mu) D_{2;ii} - 4 - D_{ii}(1 -\alpha_i) 
% &\geq -\mu D_{ii}(1-\alpha_i) \\
% \iff 
% (7 - \mu)\alpha_i 
% + \sum_{j \in [n]}\bigg[
% (2 - \mu)(1-\alpha_j) - (1 - \mu)(1-\alpha_i)
% \bigg] W_{ij} &\geq 4 \\
% \iff (7 - \mu)\alpha_i 
% + \sum_{j \in [n]}\bigg[
% (1-\mu)(\alpha_i - \alpha_j)
% + (1 - \alpha_j)
% \bigg] W_{ij} &\geq 4 
% \end{align*}

% A sufficient condition for the above is that: 
% \begin{align*}
% (7 - \mu)\alpha_{min} + (1-\mu)\alpha_{min} D_{ii} + \sum_{j \in [n]}
% \bigg[
% (1 - \alpha_j) - (1 - \mu)\alpha_j
% \bigg] W_{ij} &\geq 4 \\
% \iff (7 - \mu)\alpha_{min} + (1-\mu)\alpha_{min} D_{ii} + \sum_{j \in [n]}
% \bigg[
% 1 + \mu \alpha_j 
% \bigg] W_{ij} &\geq 4 \\
% \iff (7 - \mu)\alpha_{min} + (1-\mu)\alpha_{min} D_{ii} + (1 + \mu \alpha_{min}) D_{ii}  &\geq 4 \\
% \iff 
% (7 - \mu)\alpha_{min} + \alpha_{min}D_{ii}
% + D_{ii} &\geq 4 \\
% \iff \mu \leq \alpha_{min}(7 + D_{ii}) + (D_{ii} - 4)
% \end{align*}
% The conclusion follows by taking the minimum across $i$. 
% \end{proof}


% \newpage

% \mpcomment{an alternative approach}
% \ajcomment{Nice, I like this. I have to check the algebra but I think that this generally makes sense since $\lambda_n \leq 2 d_{max}$, so it scales in terms of susceptbility and degree. I will see if I can try to generalize this idea. It seems like you're sidestepping the local smoothness technique by directly analyzing the cost in both cases. I wonder if this can work, since the eigenvectors in the general $\alpha_i$ case do not correspond to Laplacian eigenevectors.}
