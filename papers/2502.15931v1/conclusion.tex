% !TEX root = ./main.tex
\section{Discussion and Conclusion}

In this paper, we examine how opinions evolve in social networks, where individuals adjust their publicly stated views based on interactions with others and their inherent beliefs. In our model, strategically motivated users can distort these dynamics by misrepresenting their intrinsic opinions, often to advance conflicting objectives or promote rival narratives. We analyze the Nash Equilibrium of the resulting strategic interactions and empirically show --- using diverse datasets from Twitter, Reddit, and Political Blogs --- that such deceptive behavior intensifies polarization, fuels disagreement, and increases equilibrium costs. Additionally, we establish worst-case guarantees on the Price of Misreporting, akin to the Price of Anarchy, and introduce scalable learning algorithms to help platforms (i) detect opinion manipulation and (ii) identify the users responsible. Our algorithms perform effectively on real-world data, suggesting how platforms might mitigate the effects of strategic opinion shaping.

\smallskip

\noindent We conclude with some discussion of the implications of our work, and directions for future work. 
% \ajcomment{Maybe we can replace these subsections with paragraphs? Just a personal preference.}

% \ajcomment{Add the network degrees law fig and a table.}
% \ajcomment{add more related and more future work discussion.}

\paragraph{Structural Platform Interventions.}

% \ajcomment{We give methods for learning and figuring out the souce of poliarization. The platform has to decide what to do. One idea is to modify the graph, another is to come up with a mechanism. Discuss earlier works of Kleinberg, Bindel, ... on POA and mechanisms for FJ.}

We give algorithms for platforms to detect in strategic manipulation has occurred, and who is responsible (Section~\ref{sec:learning}). Having done so, a platform might seek to mitigate the effects of strategic behavior. There are multiple plausible avenues to do so. 

First, they may seek to reduce degree disparities in the network by algorithmically encouraging balanced connections, such as by suggesting users connect with those who have fewer connections, or reducing the visibility of central nodes (hubs). As we saw, both the upper bound on the $\pom$ and the ratio $\norm \bs^\prime \norm_2 / \norm \bs \norm_2$ depend on the largest eigenvalue $\lambda_n$ of the Laplacian, which scales with the maximum degree. This motivates interventions to balance the degree distribution. 

Second, platforms can design strategy-proof mechanisms to incentivize the agents to report their true opinions. For resource allocation games on networks, it is known that the classical Vickrey–Clarke–Groves (VCG) mechanism is susceptible to adversarial behaviors such as collusion, motivating the need for different  mechanisms~\citep{chorppath2015adversarial}. In the case of social networks, platforms have unique tools such as fines or banning of accounts to modify agents' utility functions. 

% \ajcomment{other papers...}

% When all agents can be strategic ($S = [n]$), the upper bound on the $\pom$ and the ratio $\norm \bs^\prime \norm_2 / \norm \bs \norm_2$ depends on the largest eigenvalue $\lambda_n$. To reduce this bound, the platform can perform interventions to reduce $\lambda_n$. To reduce the largest eigenvalue of the Laplacian matrix in a social network, the platform can algorithmically encourage balanced connections by suggesting users connect with those who have fewer connections, thereby reducing degree disparities across the network. Additionally, it can limit the influence of overly central nodes (hubs) by reducing their visibility or suggesting connections to less central users, which directly lowers the maximum degree and the largest eigenvalue. Finally, the platform can optimize feed algorithms to prioritize content from less central users, indirectly balancing the network and reducing the largest eigenvalue by diminishing the dominance of highly connected nodes.
% \ajcomment{Cite some papers here. Musco Musco Tsourakakis do it. Chen and Racz?}
% Finally, the platform can design a strategy-proof mechanism to incentivize the agents to report their true opinions. 


\paragraph{Future Work}

First, it is not clear which measure of centrality should be used to identify the users who are most capable of manipulating others. As can be seen from Theorem~\ref{theorem:psne}, the influence of agent $i \in S$ should depend on the other members of $S$, as well as the spectral properties of the localized Laplacian matrices $L_j$ for $j \in S$ and susceptibility parameters $\alpha_k$ for $k \in [n]$. 

Second, providing $\pom$ bounds where $S$ can be any set of agents constitutes another interesting research direction, especially if platforms can assume that $S$ is a small fraction of all users. As we noted after Theorem~\ref{thrm:pom_shared_alpha}, we believe that this would require restricted invertibility analysis of the matrices determining Nash equilibria. 

Third, future work might consider different models of strategic manipulation. For example, one could consider a ``feedback equilibrium'' model (in the sense of dynamic games~\cite{li2024computation}), in which agents $i \in S$ can report arbitrary $\bz_i^\prime(t)$ at each timestep $t$, rather than following Eq.~\eqref{eq:fj_dynamics}. This flexbility may give strategic agents more power to influence outcomes. 
%but may also cause agents to lose credibility by appearing inconsistent, and be easier to detect. 
%in influencing equilibria, but may also cause agents to lose credibility by appearing inconsistent, and be easier to detect. 

% Third, 

% \ajcomment{Third, we give open loop equilibria, what about closed loop...}


% \paragraph{Concluding Remarks}

% This paper examines how opinions evolve in social networks, where individuals adjust their publicly stated views based on interactions with others and their inherent beliefs. However, some strategically motivated users distort these dynamics by misrepresenting their intrinsic opinions, often to advance conflicting objectives or promote rival narratives. We analyze the Nash Equilibrium of the resulting strategic interactions and empirically show—using data from Twitter, Reddit, and Political Blogs—that such deceptive behavior intensifies polarization, fuels disagreement, and increases equilibrium costs. Additionally, we establish worst-case guarantees on the impact of misreporting, akin to the Price of Anarchy, and introduce efficient learning-based methods to help platforms (i) detect opinion manipulation and (ii) identify the users responsible. Our algorithms perform effectively on real-world data, offering practical solutions to curb the adverse effects of strategic opinion shaping.











