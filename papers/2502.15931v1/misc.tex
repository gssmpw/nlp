% !TEX root = ./main.tex


\section{Problem Formalization} 

In our setting, we consider an arbitrary subset $S \subseteq [n]$ of strategic actors within the network. Each actor $i \in S$ has a true internal opinion $s_i \in \RR$ but can express a different internal opinion $s_i'$. The key differences between our works and earlier ones are: 

\begin{itemize}
    \item Multiple strategic actors, who may belong to coalitions, etc. 
    \item We can model arbitrary objectives by simply setting the internal (target) opinion of each node 
\end{itemize}

Note that in our setting, there is no {\em global} cost to others' disagreements, only local costs. 


Formally, suppose we are in the simplified opinion dynamics with one-dimensional opinions and no network formation. 

% Notice that even if $i \in S$, they are penalized from deviating from their {\em true} internal opinion $s_i$, rather than the fictitious $s_i^\prime$. 

% Note also that if $i \not\sim j$ then $w_{ij} = 0$. 

\begin{prop}
The Nash equilibrium solution to the ordinary opinion dynamics game with $m = 1$ is given as $\bz^* = (L + I)^{-1} \bs$. 
\end{prop}

% Now, it turns out that the Nash equilibrium solution to this game is given as $z^* = (L + I)^{-1} s$. 
We pose the following problem: 

% \begin{align*}
% \frac{\partial}{\partial {s_{prime}}_{0}} {s_{prime}}_{0} = \sum_{j=1}^{n} \left({s_{prime}}_{0} - {s_{prime}}_{j}\right)^{2} {w}_{0,j} = 0   
% \end{align*}

\begin{defn}[Instrinsic belief lying problem.]
Let $S \subseteq [n]$ be a set of strategic agents. If agent $i \in S$ wants network members to express opinions close to $s_i \in \RR^m$, what choice of $s_i^\prime \in \RR^m$ is optimal? 
\end{defn}

Let $s^\prime \in \RR^n$ have the ``lie'' $s_i^\prime$ in the first coordinate and $\bs_{(-i)}$ in the others. The Nash equilibrium of the game is simply $(L + I)^{-1} s^\prime$. Therefore, player $i \in S$ solves the following optimization problem: 
\[
\min\limits_{s_i^\prime \in \RR} c_1((L + I)^{-1} s^\prime)
\]







\subsection{Multiple strategic actors}

Suppose that $S \subseteq [n]$ is a strategic set. Supppose that $z = (L + I)^{-1} s$ is the equilibrium at honesty and $z^\prime = (L + I)^{-1} s^\prime$ is the equilibrium for strategically reported $s^\prime$. Let $B = (L + I)^{-1}$. Then, $z^\prime - z = B(s^\prime - s)$. 

\begin{defn}[Restricted Laplacian]
Fix $i \in [n]$. The $i$-restricted Laplacian $L_i \in \RR^{n \times n}$ is the Laplacian of the graph with Laplacian $L$ where all edges not incident to $i$ are removed. 
\end{defn}

Explicitly, recall the one-edge Laplacian $L_{(u,v)}$ for edge $(u,v)$ with weight $w_{uv}$. Then $L_i = \sum_{j \sim i} L_{(i,j)}$. 

\ajcomment{Todo this cost function is off, the gradient is not right.}
\begin{prop}
Let $B = (L + I)^{-1}$ and $T_i = (B^T L_i B) + (B^T e_i e_i^T B)$ and $y_i = B_{ii} s_i$. The Nash equilibria, if any exist, are given by solutions $s^\prime \in \RR^n$ to the following constrained linear system: 
\begin{align*}
\forall i \in S: e_i^T T_i s^\prime &= y_i \\
\forall j \not \in S: s_j^\prime &= s_j 
\end{align*}
\end{prop}

\begin{proof}
Consider agent $i \in S$. To calculate the best-response $s_i^\prime$ of $i$ in response to $s_{(-i)}^{\prime}$, we can analyze derivatives of its cost function with respect to $s^\prime$. Notice that for a fixed $s^\prime \in \RR^n$, 
\begin{align*}
c_i(s^\prime) &= (Bs^\prime)^T L_i B s^\prime + (e_i^T (s - Bs^\prime))^2 \\
\nabla_{s^\prime} c_i(s^\prime) &= 2 (B^T L_i B + B^T e_i e_i^T B) s^\prime - 2 B^T e_i e_i^T  s
\\
\nabla_{s^\prime}^2 c_i(s^\prime) &= 2 B^T L_i B + B^T e_i e_i^T B \succeq 0 
\end{align*}

% To see that the Hessian is PSD, notice that for $x \in \RR^n$: 
% \[
% x^T (\nabla_{s^\prime}^2 c_i(s^\prime)) x 
% = 2 x^T B^T L_i B x + 2 x^T (B - I) e_i e_i^T (B - I) x \geq 0 
% \]

Since the Hessian of $c_i(s^\prime)$ is PSD, its $(i,i)$ entry is non-negative, so $\frac{\del^2 c_i(s^\prime)}{\del (s_i^\prime)^2} \geq 0$, and hence the optimal $s_i^\prime$ is at the critical point. Solving for the critical point for agent $i$ gives precisely $e_i^T T_i s^\prime = y_i$. Assembling the critical point equations for $i \in S$ gives the solution. 
\end{proof}

\begin{cor}
Let $T \in \RR^{S \times n}$ have rows $e_i^T T = e_i^T T_i$. Let $\Tilde T \in \RR^{S \times S}$ be the submatrix of $T$ selecting columns belonging to $S$. Suppose there is some $\alpha$ such that $\alpha_i = \alpha$ for all $i$. 
Let $y \in \RR^S$ have entries $y_i = B_{ii} s_i$ as above. Let $\Tilde y = y - \sum_{j \not \in S} s_j T e_j$. Then the set of Nash equilibria, if any exist, are given by the solutions to the unconstrained linear system $\Tilde Tx = \Tilde y$. 
\end{cor}

\ajcomment{Todo calculate best response for $i \in S$ when uncertain about the true $s$ of others.}

{\bf Remark.} Notice that to calculate the existence of equilibria, agents merely need to know the $T_i, y_i$ which are functions of the graph weights, {\em and} the opinions of the honest nodes $s_j$ for $j \not \in S$. Therefore if $S = [n]$, agents need only to know the graph weights to find the existence of equilibria. 

\subsection{Price of anarchy}

The overall welfare of the system is the deviation from each player's internal opinion. 

\begin{defn}[Welfare]
Suppose the expressed opinions are $z \in \RR^{n}$. Then: 
\[
\mathsf{cost}(z) = - \mathsf{welfare}(z) := \sum\limits_{j \in [n]} c_j(z)
\]    
\end{defn}

The price of anarchy is the cost at equilibrium for the strategic game versus the honest game. 
\begin{defn}[Price of anarchy]
The price of anarchy is the ratio of costs for the strategic versus the honest game. 
\[
PoA := \frac{\mathsf{cost}(z^\prime)}{\mathsf{cost}(z)}
\]    
Where $z = (L + I)^{-1} s$ and $z^\prime = (L + I)^{-1} s^\prime$. 
\end{defn}


It seems difficult to analyze this in general. 
\begin{example}[2 nodes, one edge]
Suppose there is a single edge of weight $a > 0$ and $\alpha_1 = \alpha_2 = 0.5$. Then $L_0 = L_1 = L$ and $B = \begin{bmatrix}
\frac{a + 1}{2a + 1} & \frac{a}{2a + 1} \\
\frac{a}{2a + 1} & \frac{a + 1}{2a + 1}
\end{bmatrix}$. Suppose $S = [2]$. Then, a straightforward calculation gives: 
\begin{align*}
s_0^\prime = \frac{a^2(s_0 - s_1) + (3a + 1) s_0}{3a + 1} \\
s_1^\prime = \frac{a^2(s_0 - s_1) + (3a + 1) s_1}{3a + 1} \\
\forall i: c_i(s_0^\prime, s_1^\prime) = \frac{a(a^2 + 3a - 1)(s_0 - s_1)^2}{9a^2 + 6a + 1}
\end{align*}
Whereas if all are honest, then the cost for each is: 
\begin{align*}
\forall i: c_i(s_0, s_1) = \frac{a(a+1)(s_0 - s_1)^2}{(2a + 1)^2}    
\end{align*}
Hence, the price of anarchy is: 
\[
\mathsf{PoA} = \frac{4 a^{4} + 16 a^{3} + 17 a^{2} + 7 a + 1}{9 a^{3} + 15 a^{2} + 7 a + 1} \geq \max \left \{1, \frac{a}{3} \right \}
\]
Notably, in this example the PoA does not depend on the level of intrinsic disagremeent $\abs{s_0 - s_1}$. 
\end{example}



% \ajcomment{Todo figure out PoA in the 2 player example.}

\ajcomment{Todo figure it out for star graph.}

% {\em Discussion.}  Vectors of the form $U^T \bm{e}_j$ are precisely the projection on node $j$ onto the $n$ communities given by spectral clustering. If we have bounds on the goodness or badness of spectral clustering, we can translate those into statements about the $B_{ij}$. 

% \begin{lemma}
% If $\frac{\del^2 c_1((L + I)^{-1} s^\prime)}{\del (s_1^\prime)^2} < 0$, then the optimal $s_1^\prime$ is given by: 
% \begin{align*}
% s_1^\prime &= \frac{B_{11}s_1 - \sum\limits_{r \neq 1} B_{11} B_{1r} - \sum_j w_{1j} \sum\limits_{r \neq 1} (B_{11} - B_{j1})(B_{1r} - B_{jr}) s_r}{B_{11}^2 + \sum_j w_{1j} (B_{11} - B_{j1})^2}    
% \end{align*}
% \end{lemma}

% \ajcomment{This is even simpler - it turns out that we {\em always} have a positive second derivative, which indicates a local minimum. The derivative is zero at a unique $s_1^\prime$ which gives the optimal solution.}


\subsection{Questions} 
A few questions are raised: 
\begin{itemize}
    \item Ideally, player $1$ would like to force $z^* = \bm{1} s_1$, meaning everyone just shares the intrinsic opinion of P1. Is this possible in interesting conditions? We know that $L \bm{1} = \bm{0}$ for degree-regular graphs, hence $(L + I) (s_1 \bm{1}) = s_1 \bm{1}$. So, if $s_1 = s_2 = s_3 = \dots = s_n$ then $s_1^\prime = s_1$ achieves this, but this is a trivial example since all players already have the same intrinsic opinion. In general, this seems to be the only example, and the above argument would prove it for degree-regular graphs. What about more general graphs? Directed graphs? 
    \item The role of unknown information: In general, we should not expect player $1$ to know the whole graph Laplacian $L$, nor should they know the other players' intrinsic beliefs $s_2, \dots, s_n$. Given this, how can they play optimally? A: They can try to estimate $s_i$ from previous timesteps. 
    \item Price of anarchy? 
    \item Role of network topology? 
    \item We have ignored the role of dynamics in reaching the Nash equilibrium. The ordinary averaging dynamics may reach this equilibrium; in this case, player $1$ can select a different lie about their intrinsic belief at each step. More generally, forgetting about the ``lying'' formulation, the natural myopic dynamics are for each player to simply update their $z_i$ as: 
    \[
    z_i^\prime = \frac{s_i + \sum_j w_{ij} z_j}{1 + \sum_j w_{ij}}
    \]
    Therefore, we can imagine any $z_1^\prime$, and ask what happens at this step. The natural setup is to consider {\em feedback equilibria.} 
    \item Is positive definite Hessian necessary for the existence of a unique global optimum, given a $C^\infty$ cost function? \ajcomment{Shouldn't matter since the Hessian is always positive}
    \item Can we work out a 2 player feedback equilibrium? Think of political parties...
\end{itemize}

\subsection{Spectral clustering}

In our analysis of entries of $B = (L + I)^{-1}$, it could be fruitful to consider a spectral clustering and graph signal processing interpretation of these matrix entries. Let $L \succeq 0$ have eigendecomposition $L = U\Lambda U^T$. Then $B = U (\Lambda + I)^{-1} U^T$. Therefore $B_{ij} = e_i^T U (\Lambda +I)^{-1} U^T e_j$. 

Now, we can interpret vectors of the form $U^T e_j$ as follows. If we spectrally cluster the graph into $k$ clusters then the first $i$ columns of $U$ form the basis of the latent space. In this case, looking at $e_r^T U^T e_j$ for $r = 1, 2, \dots, k$ tells us how well node $j$ is captured by cluster $r$. For example if  $\abs{e_1^T U^T e_j} \approx 0.99$, then almost all of node $j$'s edge weight belongs to cluster $1$. 

The gradient terms we considered are summing over the entries $B_{11}, B_{12}, \dots, B_{1n}$ in multiple places. These matrix entries are nothing but the projections of node $1$ onto the possible spectral clusters of the graph, albeit weighted by $(\Lambda + I)^{-1}$. 

\subsection{N player feedback equilibrium (in progress)}

If we can describe our game as an LQ game then we can compute the feedback equilibria explicitly. This will probably still assume that the players know more information than they do; let's leave that for now. 

% Supose that $x_{t+1} = Ax_{t} + \sum_j B_t^j u_t^j$ is the system dynamics for a state $x$, control $u$. Suppose each player has the cost function: 

% \[
% J^i = \frac 1 2 \sum_{t \in [T]} 
% \bigg(
% (x_t^T Q_t^i + 2q_t^{i T}) x_t 
% + \sum_{j \in [N]} (u_{t}^{jT} R_{t}^{ij} + 2 r_{t}^{ijT}) u_t^j
% \bigg)
% \]

% We want to proceed by writing a value function $V_t^i(x_t)$ which is a minimization over $u_t^i$ for each $i, t$. 


{\bf Ordinary opinion dynamics.} At time step $t$, a player sees the opinions $z^{(t)} \in \RR^n$ and has the cost function $c_i(z^{(t)})$. The second derivative wrt $z_i$ is just $2 + 2 \sum_j w_{ij} > 0$. Hence the optimal choice of $z_i^{(t+1)}$ is the stationary point

\[
z_i^{(t+1)} = \frac{s_i + \sum_j w_{ij} z_j^{(t)}}{1 + \sum_j w_{ij}}
\]

Letting $D$ be the diagonal degrees matrix, and $W$ the weighted adjacency matrix, we get the affine dynamics 

\[
z^{(t+1)} = (D + I)^{-1} (s + W z^{(t)})
\]

Since $\norm (D+I)^{-1} W \norm < 1$, these dynamics converge. In particular, from the fixed point equation $z = (D + I)^{-1} (s + Wz)$ we obtain $z = (D + I - W)^{-1} s = (L + I)^{-1} s$, which is the PSNE of \cite{bindel2011}. 

{\bf Instrinsic belief as control.} Notice that in the update equation $z^{(t+1)} = (D + I)^{-1} (s + W z^{(t)})$ we are considering $s \in \RR^n$ as fixed. Suppose instead that player $i$ chooses to play as though their intrinsic belief is $s_{i}^{(t)} \in \RR$. Observe that at time $t + 1$, only their expressed belief $z_{i}^{(t+1)}$ is affected by this choice. But then at time $t + 2$, since every neighbor of $i$ acts based on $z_{i}^{(t+1)}$, this decision ``flows through'' the graph, and in fact diffuses exactly according to the edge structure around $i$. 

We obtain the following dynamics:  

\[
z^{(t+1)} = (D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{(t)}
\]
Where $s_j^{(t)} = u_t^j$ and $B_t^j = \frac{1}{d_{j} + 1} e_j$. 

Let us suppose for now that there are no costs to lying, and therefore the cost function is purely a function of state. If we play the game for $T$ steps, player $i$ has the cost function 
\[
J^i = \sum_{t \in [T]} (z_i^{(t)} - s_i)^2 + \sum_{j \in [n]} w_{ij} (z_i^{(t)} - z_j^{(t)})^2 
\]

\begin{theorem}
Consider an $n$-person game, $T$-stage game with cost function 

\[
J^i = \sum_{t \in [T]} (z_i^{(t)} - s_i)^2 + \sum_{j \in [n]} w_{ij} (z_i^{(t)} - z_j^{(t)})^2 
\]

for $i \in [n]$, and dynamics    

\[
z^{(t+1)} = (D + I)^{-1} (s^{(t)} + W z^{(t)})
\]

Where $s^{(t)} \in \RR^n$ contains the control inputs at time $t$. 

Then the feedback equilibria are given by solving the following, with... 
%\ajcomment{todo.}
\end{theorem}

\begin{proof}
Denote the value function at time $t$ for player $i$ as $V_t^i(z_t)$. This informally captures the cost for player $i$ at timesteps $t, t + 1, \dots, T$ under optimal play.  

The following recurrence relation is immediate from the definition of $J^i$ and the dynamics. 

\begin{align*}
V_t^i(z_t) &= \min\limits_{s_i^{(t)}}
V_{t+1}^{i}(z_{t+1})
+ (z_i^{(t)} - s_i)^2 + \sum_{j \in [n]} w_{ij} (z_i^{(t)} - z_j^{(t)})^2 \\
&= \min\limits_{s_i^{(t)}}
V_{t+1}^{i}
\bigg(
(D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{(t)}
\bigg)
+ (z_i^{(t)} - s_i)^2 + \sum_{j \in [n]} w_{ij} (z_i^{(t)} - z_j^{(t)})^2 \\
\end{align*}

Since the dynamics are linear, it follows from induction that the value function is a quadratic function of state. Hence, there exist $A_t^i \in \RR^{n\times n}, b_t^i \in \RR^n, c_t^i \in \RR$ such that:  
\begin{align*}
V_t^i(z_t) &= \frac 1 2 z_t^T (A_t^i) z_t + z_t^T b_t^i + c_t^i
\end{align*}

Moreover, $V_{T+1}^i(z_{t+1}) = 0$. 

Next, observe that if each player is at Nash equilibrium then their value functions are mutually consistent. Hence the optimal control $s^{t *}$ at time $t$ is an affine function of state. 

For shorthand, let $\alpha_t^i = (D + I)^{-1} W z^{(t)} + \sum_{j \neq i} \frac{1}{d_{j} + 1} e_j s_j^{(t)}$ and $\beta_t^i = \frac{1}{d_i + 1} e_i s_i^{(t)}$. Then, $(D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{(t)} = \alpha_t^i + \beta_t^i = z_{t+1}$. Moreover, since there are no control costs, $V_t^i(z_t)$ depends on $s_i^{(t)}$ only through the recursive term $V_{t+1}^i(z_{t+1})$. Hence using $z_{t+1} 
 = \alpha_t^i + \beta_t^i$, we obtain:
\begin{align*}
0 &= \frac{\del V_i^t}{\del s_i^{(t)}} \\
&= \frac{\del}{\del s_i^{(t)}} b_t^{iT} (\alpha_t^i + \beta_t^i)
+ \frac 1 2 \frac{\del}{\del s_i^{(t)}} (\alpha_t^i + \beta_t^i)^T A_t^i (\alpha_t^i + \beta_t^i) \\
&= \frac{1}{d_i + 1} (b_t^i)_i 
+ \frac 1 2 \frac{1}{d_i + 1} ((A_t^i + A_t^{iT}) \alpha_t^i)_{i}
+ \frac{s_{i}^{(t)}}{(d_i + 1)^2} (A_t^i)_{ii} 
\end{align*}
% \frac{1}{d_i + 1} s_{i}^{(t)} &= \frac{-1}{(A_t^i)_{ii}} 
% ((b_t^i)_i  + \frac 1 2  ((A_t^i + A_t^{iT}) \alpha_t^i)_{i})

Since the optimal $s_i^{(t)}$ is an affine function of state, there exist $\zeta_i^t \in \RR^n, \gamma_i^t \in \RR$ such that $s_i^{t *} = - z_t^T \zeta_i^t - \gamma_i^t$. Further, let $M_i^t = \frac 1 2 (A_t^i + A_{t}^{i T})$ for shorthand. 

Plugging the expression for $s_j^{t *}$ into the earlier expression, we will obtain a system of equations involving only the state $z_t$. In particular, we obtain: 

\begin{align*}
0 &= 
(b_t^i)_i + ((A_t^i + A_t^{iT}) \alpha_t^i)_{i}
+ \frac{1}{d_i + 1} s_{i}^{(t)} (A_t^i)_{ii} \\
&= (b_t^i)_i + 
\frac 1 2 e_i^T (A_t^i + A_t^{iT})
\big((D + I)^{-1} W z^{(t)} + \sum_{j \neq i} \frac{1}{d_{j} + 1} e_j s_j^{(t *)}
\big)
+ \frac{1}{d_i + 1} s_{i}^{(t *)} (A_t^i)_{ii} \\
&= (b_t^i)_i + e_i^T M_t^i (D + I)^{-1} W z^{(t)}
+ \sum\limits_{j \in [n]} \frac{1}{d_j + 1} (M_t^i)_{ij} s_{j}^{(t *)}
\end{align*}

Plugging in $s_j^{t *} = - z_t^T \zeta_j^t - \gamma_j^t$, and noticing that the resulting system must hold for all states $z_t$, we obtain the following system of equations for each $i$. 

\begin{align*}
\sum\limits_{j \in [n]} \frac{1}{d_j + 1} (M_i^t)_{ij} \zeta_j^t 
&= M_i^T (D + I)^{-1} W e_i \\
\sum\limits_{j \in [n]} \frac{1}{d_j + 1} (M_i^t)_{ij} \gamma_j^t 
&= (b_t^i)_i
\end{align*}

We need to recover these unknowns $\zeta_j^t, \gamma_j^t$. To do so, we will plug in our expression for the optimal control $s_{j}^{t *}$ back into the value function, and then obtain closed form expressions for these unknowns. Notice that we no longer need the $\min_{s_i^{(t)}}$ because we have an expression for the optimal control already. It will be convenient to write the value function as a quadratic form in $z_t$. To this end, let $(z_i^{(t)} - s_i)^2 + \sum_{j \in [n]} w_{ij} (z_i^{(t)} - z_j^{(t)})^2 = \frac 1 2 z_t^T Q_i^t z_t + z_t^T q_i^t + p_i^t$ for some $Q_i^t, q_i^t, p_i^t$. Then, 

% &= \frac 1 2 z_t^T Q_i^t z_t + z_t^T q_i^t + p_i^t \\
% &+ \bigg(
% (D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{t * }
% \bigg)^T A_{t+1}^{i} \bigg(
% (D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{t * }
% \bigg) \\
% &+ \bigg(
% (D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{t * }
% \bigg)^T b_{t+1}^{i} 
% + c_{t+1}^{i} \\

\begin{align*}
V_t^i(z_t) &= 
V_{t+1}^{i}(z_{t+1})
+ (z_i^{(t)} - s_i)^2 + \sum_{j \in [n]} w_{ij} (z_i^{(t)} - z_j^{(t)})^2 \\
&= V_{t+1}^{i}
\bigg(
(D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{t * }
\bigg)
+ (z_i^{(t)} - s_i)^2 + \sum_{j \in [n]} w_{ij} (z_i^{(t)} - z_j^{(t)})^2 \\
&= \frac 1 2 z_t^T Q_i^t z_t + z_t^T q_i^t + p_i^t \\
&+ \bigg(
(D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{t * }
\bigg)^T A_{t+1}^{i} \bigg(
(D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{t * }
\bigg) \\
&+ \bigg(
(D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j s_j^{t * }
\bigg)^T b_{t+1}^{i} 
+ c_{t+1}^{i} \\
&= \frac 1 2 z_t^T Q_i^t z_t + z_t^T q_i^t + p_i^t \\
&+ \frac 1 2 \bigg(
(D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j 
\big(
-z_t^T \zeta_j^T - \gamma_j^t
\big)
\bigg)^T A_{t+1}^{i} \\
&\cdot \bigg(
(D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j 
\big(
-z_t^T \zeta_j^T - \gamma_j^t
\big)
\bigg) \\
&+ \bigg(
(D + I)^{-1} W z^{(t)} + \sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j 
\big(
-z_t^T \zeta_j^T - \gamma_j^t
\big)
\bigg)^T b_{t+1}^{i} 
+ c_{t+1}^{i} \\
&= \frac 1 2 z_t^T Q_i^t z_t + z_t^T q_i^t + p_i^t \\
&+ \frac 1 2 \bigg(
(D + I)^{-1} W z^{(t)} 
\bigg)^T A_{t+1}^{i} \bigg(
(D + I)^{-1} W z^{(t)}
\bigg) 
+ \bigg(
(D + I)^{-1} W z^{(t)} 
\bigg)^T b_{t+1}^i 
+ c_{t+1}^i \\
&+ \bigg(
(D + I)^{-1} W z^{(t)} 
\bigg)^T M_{t+1}^{i} 
\bigg(
\sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j 
\big(
-z_t^T \zeta_j^t - \gamma_j^t
\big)
\bigg) \\
&+ \frac 1 2 \bigg(
\sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j 
\big(
-z_t^T \zeta_j^t - \gamma_j^t
\big)
\bigg)^T A_{t+1}^i
\bigg(
\sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j 
\big(
-z_t^T \zeta_j^t - \gamma_j^t
\big)
\bigg) \\
&+
\bigg(
\sum_{j \in [n]} \frac{1}{d_{j} + 1} e_j 
\big(
-z_t^T \zeta_j^t - \gamma_j^t
\big)
\bigg)^T b_{t+1}^i + c_{t+1}^{i} 
\end{align*}

We want to recover the terms $A_t^i, b_t^i, c_t^i$ as functions of other parameters, so that we can recursively solve for these terms. Let's collect terms in terms of $z_t$ above. We get: 

\begin{align*}
V_t^i(z_t) &= 
\frac 1 2 z_t^T Q_i^t z_t + z_t^T q_i^t + p_i^t \\
&+ z_t^T
\bigg[
\frac 1 2 W (D + I)^{-1} A_{t+1}^i (D + I)^{-1} W
- W (D + I)^{-1} M_{t+1}^i 
\bigg(\sum_{j \in [n]} (d_j + 1)^{-1} e_j (\zeta_j^t)^T \bigg) \\
&+ \frac 1 2  
\bigg(\sum_{j \in [n]} (d_j + 1)^{-1} \zeta_j^t e_j^T \bigg)
A_{t+1}^i 
\bigg(\sum_{j \in [n]} (d_j + 1)^{-1} e_j (\zeta_j^t)^T \bigg)
\bigg] z_t \\
&+ z_t^T 
\bigg[
W (D + I)^{-1} b_{t+1}^i + W (D + I)^{-1} M_{t+1}^i 
\bigg(\sum_{j \in [n]} \frac{1}{d_j + 1} e_j (-\gamma_j^t) \bigg)\\
&+ \bigg(\sum_{j \in [n]} \frac{1}{d_j + 1} e_j (\zeta_j^t)^T \bigg)
M_{t+1}^i
\bigg(\sum_{j \in [n]} \frac{1}{d_j + 1} e_j \gamma_j^t \bigg)
+ \sum_{j \in [n]} \frac{1}{d_j + 1} e_j (-\zeta_j^t)^T b_{t+1}^i 
\bigg] \\
&+ 
\bigg[
c_{t+1}^i 
+ \frac 1 2 \bigg(\sum_{j \in [n]} \frac{1}{d_j + 1} \gamma_j^t e_j^T \bigg)
A_{t+1}^i 
\bigg(\sum_{j \in [n]} \frac{1}{d_j + 1} e_j \gamma_j^t \bigg)
+ \bigg(\sum_{j \in [n]} \frac{1}{d_j + 1} (- \gamma_j^t) e_j^T \bigg) b_{t+1}^i
\bigg]
\end{align*}

Let $F_t = \sum_{j \in [n]} \frac{1}{d_j + 1} \zeta_j^t e_j^T$ and $v_t = \sum_{j \in [n]} \frac{1}{d_j + 1} \gamma_j^t e_j$. 

Then we can fold in these terms as well into the expression for the value function. Therefore the equation above gives a recurrence for the parameters of the value function $V_t^i(z_t) = \frac 1 2 z_t^T (A_t^i) z_t + z_t^T b_t^i + c_t^i$. Hence we obtain: 

\begin{align*}
c_t^i &= p_t^i + c_{t+1}^i 
+ \frac 1 2 v_t^T
A_{t+1}^i 
v_t
- v_t^T b_{t+1}^i \\
b_t^i &= q_i^T + W(D+I)^{-1} b_{t+1}^i 
+ W(D+I)^{-1} M_{t+1}^i (-v_t)
+ F_t^T M_{t+1}^i v_t - F_t^T b_{t+1}^i \\
\frac 1 2 A_t^i &= \frac 1 2 Q_i^t + \frac 1 2 W (D + I)^{-1} A_{t+1}^i (D+I)^{-1} W 
- W (D+I)^{-1} M_{t+1}^i F_t^T 
+ \frac 1 2 F_t A_{t+1}^i F_t^T
\end{align*}

Dynamic programming with back-substitution, starting from $V_{T + 1} = 0$, gives the solutions for feedback equilibria. 
\end{proof}

{\bf The $T \to \infty$ limit.} Now, suppose the game is infinite, and take $T \to \infty$. Then the above equations naturally give fixed-point equations that are satisfied if and only if the limit as $T \to \infty$ exists. 

Indeed, if $A_i = A_i^T$ then we obtain a discrete-time algebraic Riccati equation for $A_i$, and related equations for $b_i, v$. 

\begin{align*}
A_i &= Q_i + (W (D+I)^{-1} - F) A_i(W (D+I)^{-1} - F)^T \\
b_i &= q_i + (W (D+I)^{-1} - F^T) (b_i - A_i v) \\
0 &= p_i + \frac 1 2 v^T A_i v - b_i^T v 
\end{align*}

Moreover, by definition we know $p_i = s_i^2$, $q_i = -2s_i e_i$, and $(Q_i)_{ij} = -w_{ij}$ for $j \neq i$, $(Q_i)_{ii} = 1 + d_i$, and $(Q_i)_{jj} = w_{ij}$. 

% \ajcomment{Todo finish the calculation, and take the $T \to \infty$ limit. }

% \ajcomment{Todo compare to the Q-learning and two-timescale stochastic approximation literature. In particular, this literature seems to assume finite or compact action spaces. Can we allow actions on all of $\RR$? See e.g. this paper: \footnote{\url{https://arxiv.org/pdf/1412.0543.pdf}}.}

% \ajcomment{Look at information structures needed. My guess is that each agent will need full information of their connected component. In particular, to solve for the optimal control $u_t^{i *}$ they need to know $A_t^i, b_t^i$. But notice from above that even $b_t^i$ depends on knowing $D$ and $W$, which are the entire adjacency graph structure. Moreover, it's not clear but I think they also need to know $\alpha_t^i$ which depends on the control inputs of all other players; this is of course not known to them in general...}



\section{Coupling in higher dimensions}

Consider a generalization of opinion dynamics where each player must express $m$ opinions. If the utility function is extended in the obvious way for vectors $\bm{s}^i, \bm{z}^j \in \RR^m$, 

\begin{align*}
c_i(\bm{z^i}) &= \norm \bm{z^i} - \bm{s^i} \norm_2^2
+ \sum_j w_{ij} \norm \bm{z^i} - \bm{z^j} \norm_2^2
\end{align*}

Then the game is separable, as each player's best-response (given by taking the gradient) has coordinate $\ell \in [m]$ depending only on the $\ell$-coordinate of the intrinsic and expressed beliefs. 

Therefore, we introduce a {\bf coupling matrix} $K \in \RR^{m \times m}$ and consider the coupled high-dimensional opinion dynamics: 

\begin{align*}
c_i(\bm{z^i}) &= \norm \bm{z^i} - \bm{s^i} \norm_K^2
+ \sum_j w_{ij} \norm \bm{z^i} - \bm{z^j} \norm_K^2
\end{align*}

Recall that the {\bf Mahalanobis norm} $\norm v \norm_K^2 = v^T K v$ is a norm for any symmetric $K \succ 0$. We actually don't need $K$ to be this nice; any symmetric $K \in \RR^{m \times m}$ should give a meaningful coupling, although it will fail to be a norm. 

\begin{example}[Two-party politics]
Suppose that the $n$ players are members of Parliament. Edge weights $w_{ij} \in \RR$ represent relationships between politicians who influence one another. Suppose there are two parties labeled $0, 1$. There are functions $[n] \to \bb, [m] \to \bb$ denoting a division of Parliamentary members and issues by party. Then the coupling matrix may have a block structure $K = \begin{bmatrix} \bm{1} \bm{1}^T & - \bm{1} \bm{1}^T \\ - \bm{1} \bm{1}^T & \bm{1} \bm{1}^T \end{bmatrix}$ corresponding to the division of issues $[m] \to \bb$. If we interpret a positive opinion $x \in \RR$ on opinion $j$ as a politician being ``for'' $j$, and $x < 0$ as them being ``against'' $j$, then the coupling matrix enforces the fact that a politician who is ``for'' party-$0$ policies should also be against party-$1$ policies. A ``moderate'' who is for issues from both parties would be punished by the coupling matrix $K$ more than an ``extremist'' who is for issues from one party and against issues from the other party.
\end{example}

{\bf The role of constraints:} As we saw in the one-dimensional version, sometimes the utility function $c_i$ is not concave, and hence the utility of lying is unbounded. Therefore it makes sense to constrain the opinions of all players, e.g. to within $[-1, 1]$. 

{\bf Motivating question:} How does the coupling matrix $K$ encourage or discourage cooperation? As we saw in the previous example, the stylized two-party $K$ encourages sorting into party extremes. Can we classify broad classes of coupling matrices according to competitive/cooperative effects?

\subsection{Gradients}

Let's collect the vectors $z^i \in \RR^m$ into a matrix $Z \in \RR^{m \times n}$ and similarly for $S \in \RR^{m \times n}$. 

For simplicity consider the $K = I$ case first (separability). Then 

% \ajcomment{Note that not only has nobody seemed to consider the coupled game before, nobody has even considered the version with $K = I$. Most of the multidimensional HK dynamics papers seem to just assert the dynamics occur rather than motivating them with a utility function...}

First, notice that for $Z \in \RR^{m \times n}$ that $\frac{\del}{\del Z} \norm Z(e_i - e_j) \norm_2^2 = Z(e_i - e_j)e_i^T + Z(e_j - e_i)e_j^T$. Moreover $\frac{\del}{\del Z} \norm Z - S \norm_F^2 = 2(Z - S)$. 

To derive the natural dynamics, we are following \cite{bindel2011} in considering the total costs of all players $\sum_i c_i$, which gives 
\[
\norm Z - S \norm_F^2 + \sum_{i, j} w_{ij} \norm Z(e_i - e_j) \norm_2^2
\]

% From this, we need to first derive the PSNE and then move on to the lying dynamics. Lots of calculations there...

% Perhaps the structure theorem of \cite{macgregor-sun-2022} can be helpful. 

% \ajcomment{We remark that the decomposition order $L = UDU^T$ is necessary. We cannot instead take $L = U^T D U$.

% Geometrically, $U^T$ goes from the standard basis into the graph basis, and then $U$ goes back to the standard basis. In particular, if some vector $v = \alpha u_1 + \beta u_2$, then it is represented as $(\alpha, \beta, 0, \dots, 0)$ in the $U$-basis, and then $U \cdot (\alpha, \beta, 0, \dots, 0)^T$ recovers it in the standard basis. Similarly $U^T (\alpha u_1 + \beta u_2) = (\alpha, \beta, 0, \dots, 0)^T$, going from standard to graph basis.}


\section{Old}


\subsection{Coevolutionary Opinion Formation Games}
In the coevolutionary setting, consider a so-called {\em Asymmetric K-NN game.}. We have some initial graph $G = ([n], E)$, intrinsic opinions $s \in \RR^n$, and parameters $K \geq 1, \rho > 0$. 

Let $N(i, K, z) \subset V$ be the set of $K$ nodes whose expressed opinions in $z$ are closest to $z_i$. Then the cost function of node $i$ is given by

\[
c(z) = \sum\limits_{j \in N(i, K, z)} w_{ij} (z_i - z_j)^2 + \rho K (z_i - s_i)^2
\]

The first term is a consensu term, and the second is an intrinsic opinion term. This can be expressed in terms of the weighted graph Laplacian as $z^T L z + \rho K (z_i - s_i)^2$. 

Now, we obtain a natural dynamics: 
\begin{enumerate}
    \item Start with a graph $G = ([n], E, w)$, intrinsic opinions $s \in \RR^n$, and parameters $K \geq 1, \rho > 0$. 
    \item Each agent expresses some initial opinions $z_i^{(0)}$. 
    \item Based on the expressed opinions, the graph is updated to a $K$-regular directed graph with $K$-nearest neighbors. 
    \item Each agent expresses opinions which optimize their cost function $c(z)$, based on the opinions expressed in the previous timestep. 
    \item Repeat step 3. 
\end{enumerate}

Now, we can imagine that we have a dynamic game where the control is the expressed opinion, and then the dynamics involve updating the network with a K-NN step. For simplicity suppose that $w_{ij} = 1$ and $K = O(1)$. Then each agent can just enumerate over all subset $\binom{[n]}{K}$ to figure out the optimal (one-step) choice. But then are there feedback controls? 

For example, can we work it out if just one agent is doing the feedback optimization and the others are acting myopically? 

Maybe we can ignore the network formation altogether? That is, maybe an agent just lies about their $z_i$ throughout the game whereas the others behave normally? 

\subsection{Optimal lie for one player}

% \ajcomment{The previous calculations were wrong. Sympy gives the following.}

\begin{lemma}
Let $B = (L + I)^{-1}$ and  $z_i = (B s^\prime)_i$. Then the derivatives of the cost function are given as: 
\begin{align*}
c_1((L + I)^{1} s^\prime) &=
(s_1 - z_1)^2 
+ \sum_j w_{1j} \big(z_1 - z_j)^2 \\
\frac{\del c_1((L + I)^{-1} s^\prime)}{\del s_1^\prime}
&= 2 B_{11}(z_1 - s_1) + \sum_{j \neq 1} 2 (B_{11} - B_{j1})(z_{1} - z_{j}) w_{1j} \\
\frac{\del^2 c_1((L + I)^{-1} s^\prime)}{\del (s_1^\prime)^2} &=
2B_{11}^2 + \sum_{j \neq 1} 2 (B_{11} - B_{j1})^2 w_{1j}
\end{align*}
\end{lemma}\label{lemma:derivatives}

Notice that for non-negative edge weigths $w_{1j} \geq 0$ that the second derivative is non-negative except for degenerate cases. Therefore, 


\begin{prop}
The optimal $s_1^\prime$ is given as: 

\[
s_1^\prime = \frac{B_{11} s_1 - 
\sum_{r > 1} s_r \big(
- B_{11} B_{1r}
+ \sum_{j > 1} w_{1j} (B_{1r} - B_{jr})
\big)
}{B_{11}^2 + \sum_j w_{1j} (B_{11} - B_{1j})^2}  
\]
\end{prop}


\begin{proof}
By Lemma \ref{lemma:derivatives}, the second derivative is always positive, and hence solving for $\frac{\del c_1((L + I)^{-1} s^\prime)}{\del s_1^\prime} = 0$ gives the following: 
\begin{align*}
B_{11} s_1 &= B_{11} z_1 + \sum_j w_{1j} (B_{11} - B_{1j})^2 s_1^\prime 
+ \sum_{j > 1} w_{1j} (\sum_{r > 1} B_{1r} - B_{jr}) s_r \\
s_1^\prime &= \frac{B_{11} s_1 - \sum_{j > 1}  w_{1j} (\sum_{r > 1} B_{1r} - B_{jr}) s_r 
- B_{11} \sum_{r > 1} B_{1r} s_r
}{B_{11}^2 + \sum_j w_{1j} (B_{11} - B_{1j})^2}   \\
&= \frac{B_{11} s_1 - 
\sum_{r > 1} s_r \big(
- B_{11} B_{1r}
+ \sum_{j > 1} w_{1j} (B_{1r} - B_{jr})
\big)
}{B_{11}^2 + \sum_j w_{1j} (B_{11} - B_{1j})^2}  
\end{align*}
\end{proof}

{\bf Note.} This optimal choice requires knowing the true $s_r$ of others as well as the graph weights in general. Note that these can be observed from a previous timestep easily, as $z^* = (L + I)^{-1} s$, so $s = (L + I) z^*$ (however, this assumes all were honest at that timestep).

{\bf Note.} This $s_1^\prime$ will not be equal to $s_{1}$ in general. Moreover, there should be a nonzero benefit to lying, which we can quantify as $c_{1}$ given $s_1^\prime$ versus $c_1$ given $s_1$. 

\section{[old] Learning LQ Games}

We have $n$ agents on a graph with adjacency matrix $G \in \RR^{n \times n}$. Each agent has an action given by some $a_i \in \RR$, and then the utility of agent $i$ is:

\[
u_i(a_i) = b_i a_i - \frac 1 2 a_i^2 + \beta_i a_i \sum_{j \sim i} G_{ij} a_j
\]

If we consider the vector $\bm{b} - \frac 1 2 \bm{a} + \Gamma G \bm{a}$ where $\Gamma$ is the diagonal matrix with $\beta_i$ on the diagonals, then 
\[
u_i(\bm{a}) = a_i \la \bm{e}_i, \bm{b} - \frac 1 2 \bm{a} + \Gamma G \bm{a} \ra 
\]

\begin{remark}[Heterogeneous $\beta_i$]
For \cite{leng2020learning}, there is a $\beta$ such that $\beta_i = \beta$ for all $i$. We want to consider heterogeneous $\beta_i$ to consider mixed competition / cooperation structures. For example, if we consider a block model then we would want intra-group cooperation and inter-group competition. This actually would require $n^2$ values of $\beta$, one for each edge in general, but let's ignore this for now. 
\end{remark}

\subsection{Heterogeneous beta case}

It's not hard to see that if we modify the formulation of \cite{leng2020learning} to have heterogeneous $\beta_i$, so that the utility becomes $u_i(\bm{a}) = b_i - \frac 1 2 a_i^2 + \beta_i \bm{e}_i^T G \bm{a}$, then if $\Gamma$ is the diagonal matrix with $\beta_i$ on the diagonals, the solution is 

$$\bm{a} = (I - \Gamma G)^{-1} \bm{b}$$

Strictly speaking this requires the second derivative $\frac{\del^2 u_i}{\del a_i^2}$ to be negative everywhere, but this is trivial if we assume no self-loops in the graph ($G_{ii} = 0$). 

Now, some questions are raised. 

\begin{itemize}
    \item What are the eigenvectors and values of $\Gamma G$? Can we recover clean interpretations of the solution vector in terms of signal processing, even if $\rho(\Gamma G) < 1$? 
    \item If we want to learn $\Gamma$ in addition to $G$, what happens? Note that \cite{leng2020learning} don't even learn $\beta$. 
    \item What happens in the case where some $\beta_i$ are positive and some are negative? This is an especially interesting setting as it captures cooperation / competition structure in the same game. 
\end{itemize}

\subsection{Heterogeneous pairwise influence}

In the most general setting, we can imagine that each pair $(i, j)$ has its own $\beta$. In this setting, we can imagine a matrix of $B_{ij} = \beta(i, j)$ and then replace $\Gamma G$ with the Hadamard product $B \odot G$. 

\subsubsection{Spectrum of scaled G}

The scaled $G$ matrix $\Gamma G$ has rows of the form $\beta_i \bm{e}_i^T G$. 
What can we say about its eigenvectors and eigenvalues? 

Assuming that $G \succ 0$: 

\begin{itemize}
    \item First, it has strictly positive eigenvalues, since $\sigma(\Gamma G) = \sigma(\Gamma^{1/2} G \Gamma^{1/2})$ by similarity transforms, and $\Gamma^{1/2} G \Gamma^{1/2}$ is symmetric positive definite {\em even if} $\Gamma$ has negaitve entries. 
    \item Second, if $\norm \Gamma \norm_2$ is small enough, then the spectral radius $\rho(\Gamma G) < 1$ by the above argument. 
    \item Let $\lambda_1, \dots, \lambda_n$ be the eigenvalues of $G$ in descending order, and similarity for $\beta_i, \zeta_i$ for $\Gamma$ and $\Gamma G$ respectively. Then $\lambda_1 \cdots \lambda_k \cdot \beta_1 \cdots \beta_k \geq \zeta_1 \dots \zeta_k$ for $k \in [n]$. We can see this through a Rayleigh quotient argument if $\Gamma \succ 0$, namely via: 
    \begin{align*}
    \zeta_1 = \max\sigma(\Gamma^{1/2} G \Gamma^{1/2}) &= \sup_{\norm \bm{v} \norm \leq 1} \bm{v}^T (\Gamma^{1/2} G \Gamma^{1/2}) \bm{v} \\
    &= \sup_{\norm \Gamma^{-1/2} \bm{w} \norm \leq 1} \bm{w}^T G \bm{w} \\
    &\leq \sup_{\norm \bm{w} \norm \leq \norm \Gamma^{1/2} \norm_2} \bm{w}^T G \bm{w} \\
    &\leq \norm \Gamma^{1/2}\norm_2^2 \lambda_1 = \beta_1 \lambda_1
    \end{align*}
\end{itemize}

\subsection{The role of dependent data} 


Arguably the independent model of data from \cite{leng2020learning}, which considers independent observations $(\bm{a}^{(i)}, \bm{b}^{(i)})_{i \in [K]}$ from independent games played on the same graph $G$ (with the same $\beta$) is not realistic. How can we describe dependencies in the observations? 

We can also consider a $\bm{b} \sim N(\bm{0}, L_\alpha^\dagger)$ for $\alpha \in (0, 1)$, and $L_\alpha = \alpha L_G + (1 - \alpha) I$, for normalized graph Laplacian $L_G = D^{-1/2} (D - G) D^{-1/2}$. 


\subsubsection{Time series}

Sequential data from the same game is a reasonable setup. Consider exogenous or endogenous shocks to the game. Maybe the graph is modified somehow (e.g. network formation games), or maybe the utility functions change (e.g. our setting with mean/covariance estimation). 

There is some work that looks at sequential observations from the same game setting. This is more interesting, perhaps, since one can write down a similar looking optimization problem, as in \cite{peters2021inferring}. 



\subsubsection{Coupled oscillators model} 

There are some recent papers that look at spontaneous synchornization on graphs and relate it to the expansion of the graph \cite{abdalla2022expander}. See if we can say something about this setting? 

This is an example of graph dynamics, but in this case there is no game theory; just oscillators that are coupled based on the graph topology. 

\subsection{Riccati equations and ONLE}

Consider the model of \cite{peters2021inferring}. We have $n$ players who jointly contribute to a state $\bm{x} \in \RR^d$. Each player has a control input $\bm{u}_t^i \in \RR^{m_i}$ at time $t = 0, 1, \dots$. The dynamics are 

$$\bm{x}_{t+1} = f(\bm{x}_t, \bm{u}_t^1, \dots, \bm{u}_t^n)$$

Now, the cost function for player $i$ is given as 

$$J^i = \sum_{t \in [T]} g_t^i(\bm{x}_t, \bm{u}_t^1, \dots, \bm{u}_t^n)$$

In other words, we look at the state and control variables for all players over all timesteps. 

Now, Nash equilibria for the game, given an initial condition $\bm{x}_0$, are just choices of control for all players and all times which simultaneously minimize all $J^i$. The definition of Nash here is that given an equilibrium, stylistically written as $(\bm{u}^{1 *}, \dots, \bm{u}^{n *})$, that for any control input trajectory $\bm{u}^{1 \prime}$, the cost is worse, so: 

$$
J^1(\bm{u}^{1 \prime}, \bm{u}^{2 *}, \dots, \bm{u}^{n *}) \geq J^1(\bm{u}^{1 *}, \dots, \bm{u}^{n *})
$$

{\bf Open loop}: The players in this formulation must choose their entire control sequence in advance (open loop control). Instead of this one can consider choosing {\em policies} $\pi^1, \dots, \pi^n$ and then ask about equilibria, as later works do. 

The optimization problem is as follows. Given a sequence of partial state observations $\bm{y}_1, \dots, \bm{y}_T$, we want to infer the state and control trajectories, written as $\bm{x}, \bm{u}$, {\em and} we want to infer the parameters of the parameters of the game $\theta$. In particular, a game is fully characterized by the tuple $\Gamma = (\bm{x}_0, f, J^1, \dots, J^n)$. If $f$ and the $J^i$ have parametric forms then we can lump all those into $\theta$, and then $\Gamma = \Gamma(\theta)$ is the game parameterized by $\theta$. 

The optimization problem becomes: 

\begin{align}
\max\limits_{\bm{x}, \bm{u}, \theta} 
&\PP[(\bm{y}_1, \dots, \bm{y}_T)| \bm{x}, \bm{u}] \\
\text{s.t. } &(\bm{x}, \bm{u}) \text{ is an Open Loop Nash Eq. of } \Gamma(\theta) \\
&(\bm{x}, \bm{u}) \text{ is dynamically feasible under } f
 \end{align} 

{\bf Application to Leng's setting.} Consider a sequential version of Leng's setup where the vectors $\bm{b}^{(i)}$ are the empirical mean of a Brownian motion. That is, we have measurements $\bm{z}_1, \dots, \bm{z}_T \sim N(\bm{0}, I)$ and then $\bm{b}^{(t)} = \frac 1 t \sum_{i \leq t} \bm{z}_i$. 

In this case, we can explicitly write out the joint distribution of the $\bm{b}^{(i)}$. For example $Cov(\bm{b}^{(i)}, \bm{b}^{(j)}) = \frac{1}{i \lor j} I$. 

Putting aside issues of regularization, can we prove normality of a least squares estimator for the regression problem? 

As before, we have to learn a $\hat G$ such that $\norm (I - \beta \hat G) A - B \norm_F^2$ is minimized. Let $M_G = (I - \beta G)$ for shorthand. 

Calculating $\frac{\del}{\del G}$ of the objective gives:

\begin{align*}
\frac{\del}{\del G} \norm (I - \beta \hat G) A - B \norm_F^2
&= \frac{\del}{\del G} tr[
A^T M_G^T M_G A - B^T M_G A - A^T M_G B
]
\end{align*}

Now, two identities that are useful. First, $\frac{\del}{\del X}tr(B^T X^T C X B) = (C^T + C) X BB^T$. Second, $\frac{\del}{\del X} tr(AXB) = A^T B^T$. Hence, 

\begin{align*}
\frac{\del}{\del M_G} tr[
A^T M_G^T M_G A - B^T M_G A - A^T M_G B
] 
&= 2M_G AA^T - BA^T - AB^T    
\end{align*}

Next, the chain rule says that if $U = f(X)$ is a matrix function, then $\frac{\del g(U)}{\del X_{ij}} = tr((\frac{\del g(U)}{\del U})^T \frac{\del U}{\del X_{ij}})$. 

For us, $U = M_G$, and $X = G$. So $U = f(G) = I - \beta G$. Hence $\frac{\del M_G}{\del G_{ij}} = -\beta \bm{e}_i \bm{e}_j^T$. 

Hence $\frac{\del \norm (I - \beta \hat G) A - B \norm_F^2}{\del G_{ij}} = tr((\frac{\del \norm (I - \beta \hat G) A - B \norm_F^2}{\del M_G})^T \frac{\del M_G}{\del G_{ij}}) = -\beta (2M_G AA^T - BA^T - AB^T )_{ij}$. 

Hence, for $\frac{\del}{\del G} = 0$ we obtain 

\begin{align*}
(I - \beta G) AA^T &= \frac{1}{2} (BA^T + AB^T) \\
(AA^T \otimes I) vec(I - \beta G) 
\end{align*}

What happens if we optimize for $A, G, \beta$ jointly? First, I think that solving for $\beta$ is truly underdetermined in general. If we fix $A$ and make distributional assumptions on $B$ (e.g. Brownian motion) as well as making structural assumptions on $G$ (e.g. the graph is $d$-regular or something), then maybe something can be said about the MLE. 

\subsubsection{Jointly opitmizing G, B}

I claim that jointly optimizing for $G, B$ corresponds to optimizing a bilinear function. In fact, we can explicitly vectors and then obtain: 

\begin{align*}
(AA^T \otimes I) vec(I - \beta G) - \frac 1 2 (A \otimes I) vec(B) - \frac 1 2 (I \otimes A) vec(B^T) 
&= \bm{0} \\
(AA^T \otimes I) vec(I - \beta G) - \frac 1 2 (A \otimes I) vec(B) - \frac 1 2 (I \otimes A) \Pi^{(n^2, n^2)} vec(B) &= \bm{0}
\end{align*}

Where $\Pi^{(n^2, n^2)}$ is the commutation operator. 

Fixing one of $B, G$, solving for the other term is just solving a linear system. 

(i) What is the distribution of the solution vector $G$ when solving for a fixed $B$? 

