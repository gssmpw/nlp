\section{Related Work}
\textbf{Multivariate Time Series Forecasting.} Extensive deep learning-based MTSF methods prove that both capturing intra-series temporal dependencies and inter-series spatial dependencies are beneficial for modeling future changes~\cite{zhang2017deep, shi2015convolutional, liu2023itransformer, NEURIPS2023_dc1e32dd}. Particularly, spatio-temporal graph neural networks such as Graph WaveNet~\cite{wu2019graph} and AGCRN~\cite{bai2020adaptive} further enhance spatio-temporal representations by combining GNNs with LSTM and GNNs with TCN, respectively.
However, these works cannot be directly performed in the EVTSF task. Even with padding strategies, their performance still suffers from data imbalance. Recent efforts~\cite{chen2021trafficstream, wang2023pattern} have explored the expansion of traffic networks where observed traffic areas are continuously expanding as new sensors are deployed. 
These studies primarily address this problem as a two-phase learning task within the continual learning or online learning paradigm~\cite{chen2018lifelong}. 
The main challenge that these studies intend to address, is the catastrophic forgetting phenomenon~\cite{french1999catastrophic}. They first discover the continual variables that need to learn new patterns, then employ historical data replay~\cite{rolnick2019experience}, elastic weight consolidation~\cite{kirkpatrick2017overcoming}, and memory bank mechanism~\cite{lopez2017gradient} to consolidate historical knowledge. In contrast, the performance achieved by the retraining paradigm is an upper bound of continual learning due to the global optimization for model parameters with better convergence and performance~\cite{kirkpatrick2017overcoming, rolnick2019experience}. Another similar task to EVTSF is spatio-temporal Kriging~\cite{wu2021inductive}, a.k.a. spatio-temporal extrapolation~\cite{hu2023graph}, which performs imputation for expanding variables based on the context of continual variables. Although both are aimed at expanding variables, it has different learning objectives from EVTSF.

\noindent{\textbf{Contrastive Learning in Time Series.}} Recently, unsupervised contrastive learning has gained significant traction in the time series domain~\cite{yue2022ts2vec, tonekaboni2021unsupervised, liu2024timesurl}. The primary objective of this approach is to make positive samples attractive and separate negative samples, thereby enabling the learning of inherent temporal characteristics. Prior efforts mainly explore the effect of positive and negative pairs. For instance, motivated by the local smoothness of a signal during the generative process, TNC~\cite{tonekaboni2021unsupervised} encourages the consistency of samples within a temporal neighborhood. TS2Vec~\cite{yue2022ts2vec} learns contextual information between timesteps at different temporal resolutions. In this approach, two augmented views of the same time step exhibit similarity and are considered dissimilar otherwise. To further explore the impact of augmentation, TimesURL~\cite{liu2024timesurl} employs a frequency-temporal-based augmentation technique to encourage temporal consistency. However, due to the imbalanced data and the potential conflict between negative samples in contrastive learning and neighbors in the graph structure, directly leveraging contrastive learning is hard to benefit EVTSF (refer to Table~\ref{tab:abl}).