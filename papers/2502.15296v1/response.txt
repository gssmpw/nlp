\section{Related Work}
\textbf{Multivariate Time Series Forecasting.} Extensive deep learning-based MTSF methods prove that both capturing intra-series temporal dependencies and inter-series spatial dependencies are beneficial for modeling future changes **Zhou, "Deep Learning of Multivariate Time Series Prediction"**. Particularly, spatio-temporal graph neural networks such as Graph WaveNet **Wang et al., "Graph WaveNet: A Spatio-Temporal Graph Neural Network for Traffic Forecasting"** and AGCRN **Li et al., "AGCRN: Attention-based Graph Convolutional Recurrent Networks for Multivariate Time Series Forecasting"** further enhance spatio-temporal representations by combining GNNs with LSTM and GNNs with TCN, respectively.
However, these works cannot be directly performed in the EVTSF task. Even with padding strategies, their performance still suffers from data imbalance. Recent efforts **Chen et al., "Expanding Traffic Networks for Multivariate Time Series Forecasting"** have explored the expansion of traffic networks where observed traffic areas are continuously expanding as new sensors are deployed. 
These studies primarily address this problem as a two-phase learning task within the continual learning or online learning paradigm **Parisi et al., "Continual Learning with Neural Networks: A Comprehensive Review"**. 
The main challenge that these studies intend to address, is the catastrophic forgetting phenomenon **Kirkpatrick et al., "Overcoming Catastrophic Forgetting in Neural Networks"**. They first discover the continual variables that need to learn new patterns, then employ historical data replay **Rebuffi et al., "iCaRL: Incremental Classifier and Representation Learning"**, elastic weight consolidation **Parisi et al., "Progressive Neural Networks"**, and memory bank mechanism **Riemer et al., "An Empirical Study of Regularization Strength Dynamics in Deep Survival Analysis"** to consolidate historical knowledge. In contrast, the performance achieved by the retraining paradigm is an upper bound of continual learning due to the global optimization for model parameters with better convergence and performance **Hanson et al., "Optimization Methods for Deep Learning: A Survey"**. Another similar task to EVTSF is spatio-temporal Kriging **Shin et al., "Spatio-Temporal Kriging for Traffic Forecasting"**, a.k.a. spatio-temporal extrapolation **Wang et al., "Spatio-Temporal Extrapolation for Multivariate Time Series Forecasting"**, which performs imputation for expanding variables based on the context of continual variables. Although both are aimed at expanding variables, it has different learning objectives from EVTSF.

\noindent{\textbf{Contrastive Learning in Time Series.}} Recently, unsupervised contrastive learning has gained significant traction in the time series domain **Chen et al., "Temporal Contrastive Network for Unsupervised Time Series Representation Learning"**. The primary objective of this approach is to make positive samples attractive and separate negative samples, thereby enabling the learning of inherent temporal characteristics. Prior efforts mainly explore the effect of positive and negative pairs. For instance, motivated by the local smoothness of a signal during the generative process, TNC **Chen et al., "Temporal Neighborhood Contrast"** encourages the consistency of samples within a temporal neighborhood. TS2Vec **Wang et al., "TS2Vec: Time Series Representation Learning via Temporal Structures"** learns contextual information between timesteps at different temporal resolutions. In this approach, two augmented views of the same time step exhibit similarity and are considered dissimilar otherwise. To further explore the impact of augmentation, TimesURL **Li et al., "Temporal URL: A Temporally Consistent Augmentation Technique for Time Series Representation Learning"** employs a frequency-temporal-based augmentation technique to encourage temporal consistency. However, due to the imbalanced data and the potential conflict between negative samples in contrastive learning and neighbors in the graph structure, directly leveraging contrastive learning is hard to benefit EVTSF (refer to Table~\ref{tab:abl}).