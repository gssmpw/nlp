\section{Related Work}
\label{related_work}
\textbf{Audio captioning.} The audio captioning task can be formulated as a conditional text generation task, therefore, the prior works utilize the maximum likelihood estimation method to train audio captioning models~\citep{Mei2021AudioCT, mei2024wavcaps, Sun2023DualTD, kim2022exploring, deshmukh2023pengi}. There are two popular architectures for audio captioning models: encoder-decoder architecture~\cite{mei2024wavcaps, kim2024enclap} and prefix-tuning architecture~\citep{deshmukh2023pengi, Kim2023PrefixTF}. Although both architectures are effective in generating plausible captions, they suffer from the inherent weakness of the MLE training method: exposure bias. Some recent works deal with exposure bias by leveraging a regularization~\citep{zhang2023actual, deshmukh2024training}, contrastive loss. The contrastive regularization can slightly remedy the exposure bias issue for audio captioning models. Another technique to combat with exposure bias is to utilize stochastic decoding methods~\citep{arora2022exposure}. ~\citep{su2022contrastive} proposed a contrastive search framework with stochastic decoding methods to alleviate text degeneration for conditional text generation. The contrastive search framework is yet successful to deal with exposure bias for text generation, it can not be directly applied for audio captioning task. The reason is that the contrastive score is not able to take temporal information of acoustic and linguistic features into account. To deal with the shortcomings of the contrastive framework, we develop a new framework, called~\acrshort{acus}, which can handle the temporal information between acoustics and linguistic modalities when measuring the similarity score and alleviate exposure bias at the inference stage for audio captioning. 


\textbf{Wasserstein distance.} Wasserstein distance is a metric to measure the discrepancy between two distributions. There are enormous applications of the Wasserstein distance for multimodal learning, such as audio-text retrieval~\citep{luong2024revisiting}, multimodal representation learning~\citep{tsai2018learning}, and multimodal alginment~\citep{lee2019hierarchical}. The prior work~\citep{su2017order} proposed an order-preserving Wasserstein distance between sequences by incorporating a soft-monotonic alignment prior for optimal matching, however, it still suffers from dimensionality curse and a strict monotonic alignment across modalities. Although the Wasserstein distance is capable of measuring the cross-modality distance, it suffers from the dimensionality curse. In this work, we develop the~\acrshort{usw} kernel equipped with positional encoding to deal with the dimensionality curse and the strict monotonic alignment issue of measuring cross-modal similarity for audio captioning.