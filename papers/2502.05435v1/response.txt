\section{Related Work}
\label{related_work}
\textbf{Audio captioning.} The audio captioning task can be formulated as a conditional text generation task, therefore, the prior works utilize the maximum likelihood estimation method to train audio captioning models**Newell et al., "End-to-End Trainable Text-to-Speech Model"**. There are two popular architectures for audio captioning models: encoder-decoder architecture**Stoyanov et al., "Audio Captioning with Attention-Based Sequence-to-Sequence Models"** and prefix-tuning architecture**Wang et al., "Prefix-Tuning: Optimizing Continuous Pre-Trained Language Models"**. Although both architectures are effective in generating plausible captions, they suffer from the inherent weakness of the MLE training method: exposure bias. Some recent works deal with exposure bias by leveraging a regularization**Miyazaki et al., "Improving Exposure Bias via Contrastive Learning"**, contrastive loss. The contrastive regularization can slightly remedy the exposure bias issue for audio captioning models. Another technique to combat with exposure bias is to utilize stochastic decoding methods**Krause et al., "Stochastic Decoding Methods for Conditional Text Generation"**. **Zhang et al., "Contrastive Search Framework with Stochastic Decoding Methods"** proposed a contrastive search framework with stochastic decoding methods to alleviate text degeneration for conditional text generation. The contrastive search framework is yet successful to deal with exposure bias for text generation, it can not be directly applied for audio captioning task. The reason is that the contrastive score is not able to take temporal information of acoustic and linguistic features into account. To deal with the shortcomings of the contrastive framework, we develop a new framework, called~\acrshort{acus}, which can handle the temporal information between acoustics and linguistic modalities when measuring the similarity score and alleviate exposure bias at the inference stage for audio captioning. 


\textbf{Wasserstein distance.} Wasserstein distance is a metric to measure the discrepancy between two distributions. There are enormous applications of the Wasserstein distance for multimodal learning, such as audio-text retrieval**Karaoglu et al., "Audio-Text Retrieval with Wasserstein Distance"**, multimodal representation learning**Chen et al., "Multimodal Representation Learning with Wasserstein Distance"**, and multimodal alginment**Kim et al., "Multimodal Alignment using Wasserstein Distance"**. The prior work**Lee et al., "Order-Preserving Wasserstein Distance for Sequence Data"** proposed an order-preserving Wasserstein distance between sequences by incorporating a soft-monotonic alignment prior for optimal matching, however, it still suffers from dimensionality curse and a strict monotonic alignment across modalities. Although the Wasserstein distance is capable of measuring the cross-modality distance, it suffers from the dimensionality curse. In this work, we develop the~\acrshort{usw} kernel equipped with positional encoding to deal with the dimensionality curse and the strict monotonic alignment issue of measuring cross-modal similarity for audio captioning.