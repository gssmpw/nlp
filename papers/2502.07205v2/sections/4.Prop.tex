\section{Proposed Method}
\label{sec:prop}
In this work, we propose a novel framework named VINP for joint speech dereverberation and blind RIR identification.
We propose using an arbitrary discriminative dereverberation DNN to predict the prior distribution of anechoic speech from reverberant microphone recording and then applying VBI to analytically estimate the anechoic spectrum and the CTF filter.
After that, we use a pseudo measurement process to transform the CTF filter into the RIR waveform.
The overview of VINP is shown in Fig. \ref{fig:overview}.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\linewidth]{figs/overview.pdf}
    \caption{Overview of VINP.}
    \label{fig:overview}
\end{figure}

\subsection{Prediction of Anechoic Speech Prior}

The direct-path speech signal, which is a scaled and delayed version of the anechoic source speech signal, is free from noise and reverberation as well.
To avoid estimating the arbitrary direct-path propagation delay and attenuation, instead of the actual source speech, we setup the direct-path speech as the source speech and our target signal, still denoted as $s(n)$ or $\mathbf{S}$.  Correspondingly, the RIR begins with the impulse response of the direct-path propagation.
% with normalized amplitude of 1.
% By doing this, the source speech signal is the same as the direct-path speech signal. 
% Consequently, the same identifier $\mathbf{S}$ is employed to represent both of them in the following statement.


In VINP, we consider the power spectrum of the direct-path speech signal as the variance of the anechoic speech prior~$p(\mathbf{S})$.
Then, in each T-F bin, the oracle estimation of precision $\alpha(f,t)$ is
\begin{equation}
\label{eq:alpha_hat}
    \alpha(f,t) = 1/{|S(f,t)|^{2}}.
\end{equation}
However, the oracle estimation is unavailable in practice.
% Given the existence of many advanced discriminative dereverberation DNNs, 
We propose integrating an arbitrary discriminative dereverberation DNN as an estimator of anechoic speech prior. Note that, in the proposed framework, we need to redesign the training loss function of the discriminative dereverberation DNNs.

Specifically, the discriminative DNN constructs a mapping from reverberant magnitude spectrum $|\mathbf{X}|$ to the anechoic magnitude spectrum $| \hat{\mathbf{S}}|$ as 
\begin{equation}
\label{eq:fdnn}
    |\hat{\mathbf{S}}|=f_{\mathrm{DNN}}\left(\left|\mathbf X\right|\right).
\end{equation}
Then Eq. (\ref{eq:alpha_hat}) becomes
\begin{equation}
\label{eq:alpha_hat2}
    \alpha(f,t) = 1/{|\hat S(f,t)|^{2}}.
\end{equation}

Regarding the loss function, we employ the average Kullback-Leibler (KL) divergence~\cite{kullback1951information} to measure the distance of estimated prior distribution $p(\hat{\mathbf{S}})$ and oracle prior distribution $p(\mathbf{S})$ as
\begin{equation}
\begin{aligned}
\mathcal{L} &= \mathrm{E}_{\mathrm{data}}\left[\frac{\mathrm{KL}\left(p(\hat{\mathbf{S}})||p(\mathbf{S})\right)}{FT}\right]\\
% &= \mathrm{E}_{\mathrm{data}}\left[\frac{\sum_{f=1}^F\sum_{t=1}^T\left[\ln\left({\frac{\alpha(f,t)}{\hat{\alpha}(f,t)}}\right)+{\frac{\hat{\alpha}(f,t)}{\alpha(f,t)}}-1\right]}{FT}\right]\\
&= \mathrm{E}_{\mathrm{data}}\left[\frac{\sum_{f=1}^F\sum_{t=1}^T\left[\ln\left({\frac{|S(f,t)|^2}{|\hat{S}(f,t)|^2}}\right)+{\frac{|\hat{S}(f,t)|^2}{|S(f,t)|^2}}-1\right]}{FT}\right].\\
\end{aligned}
\end{equation}
In practice, we use 
\begin{equation}
\begin{aligned}
        &\mathcal{L}\\ 
        &=\mathrm{E}_{\mathrm{data}}\left[\frac{\sum_{f=1}^F\sum_{t=1}^T\left[\ln\left({\frac{|S(f,t)|^2+\epsilon}{|\hat{S}(f,t)|^2+\epsilon}}\right)+{\frac{|\hat{S}(f,t)|^2+\epsilon}{|S(f,t)|^2+\epsilon}}-1\right]}{FT}\right]\\
\end{aligned}
\end{equation}
instead to avoid numerical instabilities, where $\epsilon$ is a small constant.
Such a loss function is different from that in regular discriminative dereverberation DNNs which directly predict the anechoic spectrum.



% Since VINP does not care about the complex and unpredictable phase of the estimated anechoic speech, 

% The output of DNN is real-valued, which means that it does not need to predict the complex and unpredictable phase of the anechoic speech.
% Therefore, the task of the DNN has been reduced, making the training easier.
% Moreover, 
Research indicates that the complex nonlinear operations of DNNs often lead to outputs with unpredictable artificial errors. 
While these errors do not significantly affect speech perceptual quality, their impact on back-end speech recognition applications remains uncertain. 
As a result, DL-based approaches may not improve ASR performance~\cite{iwamoto2024does,menne2019investigation,iwamoto22_interspeech}.
However, in our method, by regarding the DNN output as a prior distribution of anechoic speech and utilizing the subsequent CTF-based VBI stage to refine it, this problem can be alleviated, leading to better ASR performance.
Experiments in Section~\ref{sec:expset} will provide evidence to support this conclusion.

\subsection{Variational Bayesian Inference }
Given the estimated prior distribution of anechoic speech and the observed recording, the estimation of every hidden variable and parameter is carried out through variational Bayesian inference.


The posterior distribution $p(\mathbf{S}|\mathbf{X})$ is intractable due to the integral term in Eq.~(\ref{eq:posterior}).
Therefore, we turn to VBI, which is a powerful tool for resolving hierarchical probabilistic models~\cite{wang2022off,bianco2019machine}.
More specifically, we employ the variational expectation-maximization (VEM) algorithm, which provides a way for approximating the complex posterior distribution $p(\mathbf{S}|\mathbf{X})$ with a factored distribution $q(\mathbf{S})$ according to the mean-field theory (MFT) as
\begin{equation}
    p(\mathbf{S}|\mathbf{X}) \approx q(\mathbf{S})=\prod_{f=1}^{F}\prod_{t=1}^{T} q\left(S(f,t)\right).
\end{equation}
After factorization, VEM can estimate the posterior $q(\mathbf{S})$ and model parameters $\boldsymbol{\theta}=\left\{\delta(f)|_{f=1}^F,H(f,t)|_{f=1,t=1}^{F,T}\right\}$ by E-step and M-step respectively and iteratively as
\begin{equation}
    \label{eq:estposterior}
    \ln{q\left(S(f,t)\right)}=\left<\ln{p(\mathbf{S},\mathbf{X})}\right>_{\mathbf{S}\backslash S(f,t)}
\end{equation}
and
\begin{equation}
    \hat{\boldsymbol{\theta}}=\argmax\limits_{\boldsymbol{\theta}}\ln{p(\mathbf{S},\mathbf{X})},
\end{equation}
where $\backslash$ denotes the set subtraction and $\left<\cdot\right>$ denotes expectation.
Because the solution of such a probabilistic graphical model is an underdetermined problem,
we do not update the precision parameter $\alpha(f,t)$ during the VEM iterations to prevent a deterioration in prior quality.

The specific update formulae are as follows:

\subsubsection{E-step}
In this step, we update the posterior distribution of the anechoic spectrum given the observation and estimated model parameters.
Substitute the probabilistic model into Eq.~(\ref{eq:estposterior}), we have
\begin{equation}
    \begin{aligned}
        &\ln{q\left({S}(f,t)\right)} \\
        &=\left<\ln{p({\mathbf{S}},\mathbf{X})}\right>_{{\mathbf{S}}\backslash {S}(f,t)}\\
        &=
        \left<\ln{p\left({S}(f,t)\right)}\right>_{{\mathbf{S}}\backslash {S}(f,t)}\\
        &\quad+\left<\sum_{l=0}^{L-1}\ln{p\left(X(f,t+l)|{\mathbf{S}}(f,t+l)\right)}\right>_{{\mathbf{S}}\backslash {S}(f,t)}+c,
    \end{aligned}
\end{equation}
% \begin{equation}
%     \begin{aligned}
%         &\ln{q\left(\tilde{S}(t)\right)} \\
%         &=\left<\ln{p(\tilde{\mathbf{S}},\mathbf{X})}\right>_{\tilde{\mathbf{S}}\backslash \tilde{S}(t)}\\
%         & \overset{\tilde{S}(t)}{=}
%         \left<\ln{p\left(\tilde{S}(t)\right)}
%         +\sum_{l=0}^{L-1}\ln{p\left(\mathbf{X}(t+l)|\tilde{\mathbf{S}}(t+l)\right)}\right>_{\tilde{\mathbf{S}}\backslash \tilde{S}(t)}+C,
%     \end{aligned}
% \end{equation}
where $c$ is a constant term that is independent of ${S}(f,t)$, and
\begin{equation}
\left\{
\begin{aligned}
    &\ln{p\left({S}(f,t)\right)}=\ln{\alpha(f,t)}-\alpha(f,t)\left|{S}(f,t)\right|^2+c \\
    &\ln{p\left(X(f,t+l)|{\mathbf{S}}(f,t+l)\right)} \\
    &\quad = \ln{\delta(f)}-\delta(f)\left|X(f,t+l)-\mathbf{H}(f){\mathbf{S}}(f,t+l)\right|^2+c. \\ 
\end{aligned}
\right.
\end{equation}
% \begin{equation}
% \left\{
% \begin{aligned}
%     &\ln{p\left(\tilde{S}(t)\right)}=\ln{\alpha(t)}-\alpha(t)\left|\tilde{S}(t)\right|^2/c^2+C, \\
%     &\ln{p\left(X(t+l)|\tilde{\mathbf{S}}(t+l)\right)} \\
%     &\qquad = \ln{\delta}-\delta\left|X(t+l)-\mathbf{H}\tilde{\mathbf{S}}(t+l)/c\right|^2+C \\ 
% \end{aligned}
% \right.
% \end{equation}

According to the property of Gaussian distribution, the posterior distribution is also Gaussian, written as 
\begin{equation}
    q\left({S}(f,t)\right)=\mathcal{CN}\left({\mu}(f,t),{\gamma}^{-1}(f,t)\right),
\end{equation}
% \begin{equation}
%     q\left(\tilde{S}(t)\right)=\mathcal{CN}\left(\tilde{\mu}(t),\tilde{\gamma}^{-1}(t)\right),
% \end{equation}
whose precision and mean have close-form solutions as
\begin{equation}
\label{eq:mu_var}
\left\{
\begin{aligned}
    &{\gamma}(f,t)=\alpha(f,t)+\delta(f) ||\mathbf{H}(f)||_2^2 \\
    &{\mu}(f,t)={\gamma}^{-1}(f,t)\delta(f)\\
    &\quad\times\left[\sum_{l=0}^{L-1}H_l(f)^*\left[X(f,t+l)-\mathbf{H}_{\backslash l}(f)\hat{\boldsymbol{\mu}}(f,t+l)\right]\right],
\end{aligned}
\right.
\end{equation}
where $\mathbf{H}_{\backslash l}(f)$ is same as $\mathbf{H}(f)$ except that $H_l(f)$ is set to $0$, 
and $\hat{\boldsymbol{\mu}}(f,t+l)=\left[\hat\mu(f,t+l-L+1),\cdots,\hat\mu(f,t+l)\right]^T$ contains the estimates of means from the previous VEM iteration.

In order to make VEM converge more smoothly, we further apply an exponential moving average (EMA) to the estimates in Eq.~(\ref{eq:mu_var}) as
\begin{equation}
\label{eq:mu_var_sm}
\left\{
\begin{aligned}
    &\hat{\gamma}^{-1}(f,t)=\lambda\hat\gamma^{-1}_{\mathrm{pre}}(f,t)+(1-\lambda)\gamma^{-1}(f,t)\\
    &\hat{\mu}(f,t)=\lambda\hat{\mu}_{\mathrm{pre}}(f,t)+(1-\lambda){\mu}(f,t),
\end{aligned}
\right.
\end{equation}
where $\lambda$ is a smoothing factor, $\hat{\mu}_{\mathrm{pre}}(f,t)$ and $\hat\gamma^{-1}_{\mathrm{pre}}(f,t)$ are the estimates from the previous VEM iteration. 


The prior of anechoic speech narrows down the infinite number of possible solutions when decoupling the anechoic speech and the RIR, which is the key to achieving dereverberation~\cite{10638210}.
The mean of the posterior distribution is the MAP estimate of anechoic spectrum $\hat{\mathbf{S}}$.
It is also worth noticing that the E-step can be implemented in parallel across all T-F bins.

\subsubsection{M-step}
In this step, VEM updates the noise precision and CTF filter by maximizing the logarithmic joint probability of the anechoic speech and observation, which is
\begin{equation}
    \begin{aligned}
        &\ln{p\left({\mathbf{S}},\mathbf{X}\right)}\\
        &=
        \ln{p\left(\mathbf{X}|{\mathbf{S}}\right)} +c\\
        &= T\ln{\delta(f)}-\delta(f)\sum_{t=1}^{T}\left|X(f,t)-\mathbf{H}(f){\mathbf{S}}(f,t)\right|^2 +c, \\
    \end{aligned}
\end{equation}
where $c$ is a constant term that is independent of $\delta(f)$ and $\mathbf{H}(f)$.
Setting the first derivative with respect to the parameters to zero, the noise precision is updated as
\begin{equation}
\label{eq:est_delta}
\begin{aligned}
    &\hat{\delta}(f)\\
    &= T\bigg/\sum\limits_{t=1}^T\left[\left|X(f,t)\right|^2-2\mathrm{Re}\left\{X^*(f,t)\mathbf{H}(f)\left<{\mathbf{S}}(f,t)\right>\right\}\right.\\
    &\quad +\left.{\mathbf{H}(f)\left<{\mathbf{S}}(f,t){\mathbf{S}}^H(f,t)\right>\mathbf{H}^H(f)}\right],
    % \\&\frac{T}{\sum\limits_{t=1}^T\left[\left|X(t)\right|^2-2\mathrm{Re}\left\{X^*(t)\mathbf{H}\left<{\mathbf{S}}(t)\right>\right\}+{\mathbf{H}\left<{\mathbf{S}}(t){\mathbf{S}}^H(t)\right>\mathbf{H}^H}\right]}, \\
\end{aligned}
\end{equation}
and the CTF filter is updated as
\begin{equation}
\label{eq:est_H}
    \begin{aligned}
        &\hat{\mathbf{H}}(f) \\
        &= \left[\sum_{t=1}^{T}
        X(f,t)\left<{\mathbf{S}}^H(f,t)\right>\right]
        \left[\sum_{t=1}^{T}\left<{\mathbf{S}}(f,t){\mathbf{S}}^H(f,t)\right>\right]^{-1},
    \end{aligned}
\end{equation}
where
\begin{equation}
    \left<\mathbf{S}(f,t)\right>=\boldsymbol{\mu}(f,t)=\left[\mu(f,t-L+1),\cdots,\mu(f,t)\right]^T,
\end{equation}
and
\begin{equation}
\begin{aligned}
    &\left<\mathbf{S}(f,t)\mathbf{S}^H(f,t)\right>\\
    &=\boldsymbol{\mu}(f,t)\boldsymbol{\mu}^H(f,t)\\
    &\quad +\mathrm{diag}([\alpha^{-1}(f,t-L+1),\cdots,\alpha^{-1}(f,t)]),
\end{aligned}
\end{equation}
$\mathrm{diag}(\cdot)$ denotes the operation of constructing a diagonal matrix.
Just like the E-step, the M-step can also be implemented in parallel across all T-F bins.



\subsubsection{Initialization of VEM Parameters}



The initialization of parameters plays a crucial role in the convergence of VEM.
Before iteration, the mean and variance of the anechoic speech posterior $p(\mathbf{S})$ are set to zero and the power spectrum of reverberant recording, respectively. 
This means we have
\begin{equation}
\label{eq:init1}
\left\{
\begin{aligned}
    &\mu(f,t)=0\\
    &\alpha(f,t)=1/\left|X(f,t)\right|^{2}.\\
\end{aligned}
\right.
\end{equation}
The CTF coefficients in each frequency band are set to 0 except that the first coefficient is set to 1, which means
\begin{equation}
\label{eq:init2}
\left\{
\begin{aligned}
    &H_{0}(f)=1\\
    &H_{l\neq 0}(f)=0.\\
\end{aligned}
\right.
\end{equation}
Because even during speech activity, the short-term power spectral density of observation often decays to values that are representative of the noise power level~\cite{martin2001noise}, the initial variance of the additional noise is set to the minimum power in each frequency band, which means
\begin{equation}
\label{eq:init3}
    \delta(f) = \left[\min\limits_t\left(\left|X(f,t)\right|^2\right)\right]^{-1}.
\end{equation}


The VBI procedure is summarized in Algorithm~\ref{algo:VBI}.
The final outputs of VBI procedure are the complex-valued anechoic spectrum and CTF filter estimates.
Notice that in the proposed framework, the prior distribution of anechoic speech is not updated during iterations.
Therefore, the DNN only infers once.


\begin{algorithm}[H]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{VBI procedure.}
    \label{algo:VBI}
    \begin{algorithmic}[1] % 控制是否有序号
        \REQUIRE Reverberant microphone recording $\mathbf{X}$; % input 的内容
	    \ENSURE Anechoic speech spectrum estimate $\hat{\mathbf{S}}$, CTF filter estimate $\hat{\mathbf{H}}$; % output 的内容

        % \STATE Setting the hyperparameters 
        \STATE Estimate prior distribution of anechoic speech using Eq. (\ref{eq:fdnn}) and Eq. (\ref{eq:alpha_hat2});
        \STATE Initialize VEM parameters using Eq. (\ref{eq:init1}), Eq. (\ref{eq:init2}) and Eq. (\ref{eq:init3});
        \REPEAT
            \STATE E-step: update the posterior distribution of anechoic speech using Eq. (\ref{eq:mu_var}) and Eq. (\ref{eq:mu_var_sm});
            \STATE M-step: update the parameter estimates of the signal model using Eq. (\ref{eq:est_delta}) and Eq. (\ref{eq:est_H});
        \UNTIL{Converge or reach the maximum number of iterations.}
    \end{algorithmic}
\end{algorithm}


\subsection{Transformation to Waveforms}
After the VBI procedure, both the anechoic spectrum and the CTF filter are estimated.
We need to further transform these T-F representations into waveforms.
By applying inverse STFT to the anechoic spectrum estimate, we can easily get the anechoic speech waveform.
For the estimation of RIR, we design a pseudo intrusive measurement process as follows.

A common method for intrusive measuring the RIR of an acoustical system is to apply a known excitation signal and measure the microphone recording~\cite{stan2002comparison}.
Playing an excitation signal $e(n)$ with a loudspeaker, the noiseless microphone recording $y(n)$ can be written as
\begin{equation}
    y(n) = h(n)*e(n),
\end{equation}
where $h(n)$ has the same meaning as in Eq.~(\ref{eq:signal_model_time}).
A commonly used logarithmic sine sweep excitation signal can be expressed as~\cite{stan2002comparison,farina2000simultaneous}
\begin{equation}
\label{eq:excitation}
    e(n)=\sin \left[\frac{N\omega_1}{\ln \left({\omega_2}/{\omega_1}\right)}\left(e^{n\ln \left({\omega_2}/{\omega_1}\right)/N}-1\right)\right],
\end{equation}
where $\omega_1$ is the initial radian frequency and $\omega_2$ is the final radian frequency of the sweep with duration $N$.
Through an ideal inverse filter $v(n)$, the excitation signal can be transformed into a Dirac's delta function $\delta(n)$, as
\begin{equation}
    e(n)*v(n)=\delta(n).
\end{equation}
For the logarithmic sine sweep excitation, the inverse filter $v(n)$ is an amplitude-modulated and time-reversed version of itself~\cite{stan2002comparison,farina2000simultaneous}.
The RIR can be estimated by convolving the measurement $y(n)$ with the inverse filter $v(n)$ as
\begin{equation}
    h(n)=y(n)*v(n).
\end{equation}


% For blind RIR estimation, the exact excitation signal $e(n)$ and the corresponding measurement $y(n)$ are not available.
% Conversely, a reverberant speech recording $x(n)$ is given as an alternative.
% In our approach, the recording $x(n)$ is firstly enhanced by VBI to estimate the CTF filter $\hat{\mathbf{H}}$ in the T-F domain.
In our approach, the excitation signal is convoluted by the CTF estimates (along the time dimension) to get a pseudo measurement $\tilde{Y}(f,t)$ in the T-F domain as
\begin{equation}
\label{eq:pseudo_measure}
\tilde{Y}(f,t)=\hat{\mathbf{H}}(f)\mathbf{E}(f,t),
\end{equation}
where $\mathbf{E}(f,t)=\left[E(f,t-L+1),\cdots,E(f,t)\right]^T\in \mathbb{C}^{L \times 1}$, $\tilde{Y}(f,t)$ and $E(f,t)$ are the STFT coefficient of $\tilde{y}(n)$ and $e(n)$, respectively.
Finally, after applying inverse STFT to $\tilde{Y}(f,t)$, we use the pseudo measurement $\tilde{y}(n)$ to estimate the RIR waveform by
\begin{equation}
\label{eq:inverse_filtering}
    \hat{h}(n)=\tilde{y}(n)*v(n).
\end{equation}
The transformation from the CTF filter to RIR waveform is summarized in Algorithm~\ref{algo:RIR}.
\begin{algorithm}[H]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Transformation from CTF to RIR.}
    \label{algo:RIR}
    \begin{algorithmic}[1] % 控制是否有序号
        \REQUIRE CTF filter estimate $\hat{\mathbf{H}}$; % input 的内容
	    \ENSURE RIR waveform estimate $\hat{h}(n)$; % output 的内容
        % \STATE Estimate the CTF filter $\hat{\mathbf{H}}$ using Algorithm \ref{algo:VBI};
        \STATE Pick a pair of excitation signal $e(n)$ and its inverse filter $v(n)$;
        \STATE Build a pseudo measurement signal $\tilde{y}(n)$ using Eq. (\ref{eq:pseudo_measure});
        \STATE Estimate RIR $\hat{h}(n)$ by inverse filtering as Eq. (\ref{eq:inverse_filtering}).

    \end{algorithmic}
\end{algorithm}


% Owing to the length limitation of the CTF filter, the extremely late portion of the tail of the measured RIR is modeled as additive noise.
% However, the energy of the clipped part is negligible and there is essentially no influence on the estimation of acoustic parameters such as RT60 and DRR.
% In fact, the clipping of RIR is widely used for noise reduction in RT60 estimation~\cite{lundeby1995uncertainties,faiget1998optimization,guski2014comparison}.

% 由于CTF长度有限，非常晚期的混响是建模在噪声中的，所估计的RIR在时间上是被截断的。根据RIR的模型，在EDC曲线上加了一个常数。对T60的估计没有影响。此外在存在测量噪声的情况下估计RT60时本来就会在计算EDC时不考虑低于给定阈值样本的操作\cite{lundeby1995uncertainties,faiget1998optimization,guski2014comparison}，因此我们使用重建的RIR进行T60估计相当于设置了一个较高的噪声门限。