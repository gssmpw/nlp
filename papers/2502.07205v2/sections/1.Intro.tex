\section{Introduction}

\IEEEPARstart{R}{everberation}, which is formed by the superposition of multiple reflections, scattering, and attenuation of sound waves in a closed space, is one of the main factors degrading speech quality and intelligibility in daily life.
The reverberant speech contains the crucial knowledge of both anechoic source speech and room impulse response (RIR).
The anechoic speech, often regarded as an oracle, serves as the ultimate target of speech dereverberation.
Moreover, RIR characterizes the sound propagation from sound source to microphone constrained in a room environment.
Therefore, given a reverberant microphone recording, there is a strong need to estimate the anechoic speech and RIR, leading to two distinct tasks: speech dereverberation and blind RIR identification, respectively.


A series of classical speech dereverberation approaches build deterministic signal models of anechoic source speech, RIR, and reverberant microphone recording.
After that, dereverberation is solved by designing an inverse filter in the time domain~\cite{nakatani2010speech} or time-frequency (T-F) domain~\cite{nakatani2010speech,nakatani2008blind,kinoshita2009suppression,yoshioka2012generalization,kodrasi2014frequency}.
In contrast, another series of classical methods regard the anechoic source speech signal and the reverberant microphone recording as random variables, and use hierarchical probabilistic graphical models to describe the conditional independent relationship between these random variables.
Speech dereverberation is then performed by estimating every unknown latent variable and model parameter, including the anechoic speech spectral coefficient and the reverberation model parameters~\cite{schmid2014variational,mohammadiha2015speech}.
Actually, the construction of the probabilistic graphical model is highly flexible. 
By modeling different factors that affect the observed signal as latent variables and model parameters, similar methods can be applied to a variety of fields, such as speech denoising~\cite{schmid2014variational,8462460,8492427} and direct-of-arrival estimation~\cite{yang2012off,wang2022off,wang2022joint}.
Classical methods are model-based and unsupervised, which makes them free from generalization problems.
However, due to the insufficient utilization of prior knowledge regarding speech and distortion, the performance of these methods remains unsatisfactory.


In recent decades, data-driven deep learning (DL)-based approaches have developed rapidly and surpassed the performance of classical ones.
These approaches rely less on the assumptions of signal models, instead directly learn the characteristics of speech signals using deep neural networks (DNNs).
Regarding such methods, the focal point of algorithm development is the design of DNN structures, network inputs and outputs, and loss functions.
% In most cases, the same DNN architecture can be employed for different tasks (such as speech denoising and dereverberation) by simply changing the training data. 
The most straightforward DL-based idea is to construct a discriminative DNN to learn the mapping from degraded speech to target speech.
For instance, authors in \cite{han2015learning,luo2018real,zhao2020monaural,hao2021fullsubnet,zhou2023speech,xiong2022spectro} developed various discriminative DNNs and loss functions to build mappings in various representation domains.
% For instance, in~\cite{luo2018real}, the authors formulated the dereverberation problem as a separation problem in the time domain and investigated an encoder-decoder structure for dereverberation.
% In~\cite{han2015learning}, the authors built the mapping for the logarithmic magnitude spectra.
% Similarly, the authors in~\cite{zhao2020monaural} used the self-attention module and the temporal convolutional network to remove the reverberant portion of the compressed magnitude spectrum.
% In~\cite{xiong2022spectro}, the authors utilized an improved subband long short-term memory network to learn the complex-valued mask in the T-F domain.
% In~\cite{hao2021fullsubnet}, the authors proposed a full-band and sub-band fusion model in the T-F domain for speech denoising, which is proved to be effective in dereverberation as shown in~\cite{zhou2023speech}.
% However, one limitation of such discriminative DNNs is the generalization ability.
% Sometimes, these methods may not perform as well in unseen acoustic environments~\cite{richter2023speech}.
Another concept is to consider speech dereverberation as a generative task and utilize generative DNNs, such as variational autoencoder (VAE)~\cite{Kingma2014auto}, generative adversarial network (GAN)~\cite{goodfellow2020generative}, and diffusion model (DM)~\cite{ho2020denoising}, to generate audio samples~\cite{fu2019metricgan,abdulatif2024cmgan,richter2023speech,lemercier2023storm}.
% VAE is commonly utilized in unsupervised approaches.
% Authors in \cite{bando2018statistical} were the first to use a VAE to learn the prior distribution of clean speech in speech denoising.
% Subsequently, the prior distribution was used through Bayesian inference based on a Markov chain Monte Carlo (MCMC) to generate the clean audio samples.
% Similarly, authors in \cite{baby2021speech} applied a similar framework to unsupervised reverberation tasks.
% In our previous work RVAE-EM~\cite{wang2024rvae}, we built a convolutive transfer function (CTF)-based signal model and implemented both unsupervised and supervised speech dereverberation by means of recurrent VAE and Bayesian inference.
% In \cite{wang2024rvae}, we found that MCMC is unnecessary when the anechoic prior distribution is good enough.
% The most significant advantage of GAN is that it can automatically learn the inherent distribution of data through the competition between the generator and the discriminator, without any explicit assumptions of the data distribution.
% The most commonly used GAN-based speech enhancement method is MetricGAN~\cite{fu2019metricgan}, which successfully establishes a connection between training objectives and non-differentiable speech evaluation metrics.
% For instance, the authors in \cite{abdulatif2024cmgan} combined a conformer-based generator and a metric discriminator and achieved excellent performance for denoising, dereverberation, and super-resolution.
% Recently, DM-based approaches have received widespread attention due to their ability to generate realistic samples.
% DiffuSE~\cite{lu2021study} and CDiffuSE~\cite{lu2022conditional} were the first to introduce DM into the field of speech enhancement by 
% modeling noise into the diffusion process. 
% In contrast, the researchers in \cite{welker22_interspeech} argued that it is a more preferable option to utilize artificially added Gaussian noise within the diffusion process and to employ noisy speech as a conditioning signal.
% Furthermore, by employing a significantly more effective complex U-Net~\cite{choi2018phase}, SGMSE+~\cite{richter2023speech} achieved remarkable performance in both speech denoising and speech dereverberation.
% In \cite{lemercier2023storm}, the authors presented a DM-based stochastic regeneration approach in which an estimate given by a predictive model is provided as a guide.
Thanks to the powerful non-linear modeling ability of DNNs, DL-based methods are able to make the best of a large amount of data and typically lead to a better performance than classical ones.
However, it is still challenging to improve automatic speech recognition (ASR) performance because such a single-channel front-end system without joint training always introduces artifact errors into speech waveforms~\cite{iwamoto22_interspeech,iwamoto2024does}.

Particularly, some methods combine the VAE and classical methods based on probabilistic graphical models, forming a so-called semi-supervised~\cite{mysore2011non,bando2018statistical} or unsupervised~\cite{bie2022unsupervised,baby2021speech} branch.
Unlike the supervised DL-based approaches, the VAE in these methods is trained with only clean speech utterances.
At the inference stage, the VAE decoder participates in solving the probabilistic graphical model through the estimation of clean speech prior.
Usually, the Markov chain Monte Carlo (MCMC) algorithm is used to sample the latent variables in VAE.
Compared with classical methods, the prior generated by VAE has higher quality and can yield better speech enhancement results.
Such algorithms were first applied to speech denoising~\cite{mysore2011non,bando2018statistical,bie2022unsupervised} and then expanded to speech dereverberation~\cite{baby2021speech,wang2024rvae}.
For instance, in \cite{baby2021speech}, the authors developed a Monte Carlo expectation-maximization (EM) algorithm based on a convolutional VAE and non-negative matrix factorization (NMF) model.
In our previous work RVAE-EM~\cite{wang2024rvae}, we employed a more powerful recurrent VAE to learn the prior distribution of anechoic speech. 
Also, we found that when the anechoic speech prior is of sufficient quality, MCMC is unnecessary, thus avoiding the repeated inference by VAE decoders.
Moreover, by introducing supervised data into the training of VAE, we also obtained a supervised version of RVAE-EM that performs better than the unsupervised one.
This finding inspired our current work. 
Since there are already many advanced dereverberation DNN architectures, it is possible to directly apply them as supervised anechoic speech prior estimators to the solution of the probabilistic graphical model with some simple modifications.

Within the domain of audio signal processing, blind RIR identification from reverberant microphone recording constitutes a crucial and challenging area of research, since RIR characterizes the acoustic characteristics of the sound propagation and environment.
% The RIR estimates are usually employed in a variety of audio tasks, including the estimation of acoustic parameters~\cite{eaton2016estimation} and augmented reality~\cite{valimaki2015assisted}.
% Traditional intrusive approaches for estimating RIR typically play a carefully designed excitation signal, such as white noise or swept sine signal, through a distant loudspeaker and estimate the RIR based on the measured signal~\cite{rife1989transfer,farina2000simultaneous}. 
% However, in real-life applications, it is not always practical to play the excitation signal.
% To overcome this limitation, blind RIR estimation aims to determine the RIR without any excitation signal.
Currently, 
methods for blind RIR identification are rather scarce. 
% Therefore, most approaches choose to estimate the room acoustic parameters such as reverberation time 60dB (RT60) and direct-to-reverberant ratio (DRR) directly~\cite{ratnam2003blind,jeub2011blind,srivastava2022realistic}.
In recent years, with the development of DL techniques, some DL-based approaches have been proposed~\cite{steinmetz2021filtered,richard2022deep,lemercier2024unsupervised}.
For instance, the authors in~\cite{lemercier2024unsupervised} established a parameterized model for the reverberation effect and proposed an unsupervised method for joint speech dereverberation and blind RIR estimation. 
This approach is similar to our work in terms of tasks.


In this paper, we propose a \textbf{v}ariational Bayesian \textbf{i}nference framework with \textbf{n}eural speech \textbf{p}rior (VINP) for joint speech dereverberatiion and blind RIR identification.
Our motivation is as follows: Given that the convolution transfer function (CTF) approximation is a rather accurate model of the reverberation effect,
by making reasonable probabilistic assumptions and regarding the output of DNN as the prior distribution of anechoic speech, we are able to analytically estimate the anechoic spectrum and the CTF filter, and further obtain the time-domain waveforms of anechoic speech and RIR.
The basis of VINP is a probabilistic signal model based on CTF approximation~\cite{talmon2009relative}, the same as in our previous work RVAE-EM~\cite{wang2024rvae}.
The signal model describes the relationship among the source speech, CTF filter, and reverberant microphone recording.
Different from RVAE-EM~\cite{wang2024rvae}, considering the existence of advanced discriminative DNN structures for dereverberation, we proposed to employ such DNNs to learn the prior distribution of anechoic speech by modifying the loss function of network training. 
Sequentially, we use variational Bayesian inference (VBI)~\cite{beal2003variational,tzikas2008variational} to analytically solve the hidden variables and parameters in our probabilistic model.
By doing this, VINP avoids the direct utilization of DNN output but still utilizes the powerful nonlinear modeling capability of the network, thereby benefiting ASR.
Moreover, a major drawback of RVAE-EM~\cite{wang2024rvae} is that the computational cost grows with the cube of the speech length. 
Through the variational inference, the computational cost in the proposed method increases linearly with the speech length.
Additionally, parallel computation can be used across T-F bins for fast implementation.


This paper has the following contributions:
\begin{itemize}
    \item We propose VINP, a novel VBI framework with neural speech prior for joint speech dereverberation and blind RIR identification.
    For the first time, we propose introducing an arbitrary discriminative dereverberation DNN into VBI to successfully complete the tasks.
    \item 
    VINP avoids the direct utilization of DNN output but still utilizes the powerful nonlinear modeling capability of the network.
    Thus, it can enhance the ASR performance of the original backbone DNN without the need for any joint training with the ASR system.
    Experiments demonstrate that VINP attains an advanced level in most metrics related to human perception and displays unquestionable state-of-the-art (SOTA) performance in ASR-related metrics.
    \item 
    VINP can be used for blind RIR identification from reverberant microphone recording.
    Experiments indicate that VINP attains the SOTA level in the blind estimation of reverberation time at 60dB (RT60) and direct-to-reverberation ratio (DRR). 
    \item From the perspective of computational cost, VINP exhibits linear growth with respect to the speech length, which is different from the cubical growth in our previous work RVAE-EM~\cite{wang2024rvae}.
    Moreover, the VBI procedure in VINP can be implemented in parallel across T-F bins, thereby further reducing the processing time.
\end{itemize}

The remainder of this paper is organized as follows: 
Section \ref{sec:problem_formulation} formulates the signal model and the tasks.
Section \ref{sec:prop} elaborates on the proposed VINP framework. 
Experiments and discussions are presented in Section \ref{sec:expset}. 
Finally, Section \ref{sec:conclusion} concludes the entire paper.
