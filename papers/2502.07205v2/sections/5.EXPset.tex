\section{Experiments}
\label{sec:expset}


\subsection{Datasets}
VINP is designed for both speech dereverberation and blind RIR identification.
These two tasks share the same DNN for speech prior distribution prediction.
Therefore, we use a single training set and two different test sets.
% \subsubsection{{Training and validation sets}}
% Both these two sets are generated by convolving clean speech signals with simulated RIRs and then adding stationary noise.
\subsubsection{{Training Set}}
The training set is composed of 200 hours of high-quality English speech utterances from the corpora of EARS~\cite{richter2024ears}, DNS Challenge~\cite{dubey2023icassp}, and VCTK~\cite{valentini2016investigating}. 
All speech utterances are downsampled to 16 kHz if necessary. 
We simulate 100,000 pairs of reverberant and direct-path RIRs using the gpuRIR toolbox~\cite{diaz2021gpurir}.
The simulated speaker and microphone are randomly placed in rooms with dimensions randomly selected within a range of 3 m to 15 m (for length and width) and 2.5 m to 6 m (for height).
The minimum distance between the speaker/microphone and the wall is 1 m. 
Reverberant RIRs have RT60 values uniformly selected within the range of 0.2 s to 1.5 s. 
Direct-path RIRs are generated using the same geometric parameters as the reverberant ones but with an absorption coefficient of 0.99.
Noise recordings from the Noise92 corpus and the training set from the REVERB Challenge~\cite{kinoshita2013reverb} are employed.
The signal-to-noise ratio (SNR) is randomly selected within the range of 5 dB to 20 dB.


% In the validation set, we use 1.48 hours of clean speech from the 'si\_et\_05' subset in WSJ0 corpus~\cite{paul1992design}, 5,000 generated RIRs and the stationary noise from the test set of the REVERB challenge~\cite{kinoshita2013reverb}. 
% The RIR simulation settings for the validation set are the same as those for the training set.



\subsubsection{{Test Set for Speech Dereverberation}}
For speech dereverberation, we utilize the official single-channel test set from the REVERB Challenge~\cite{kinoshita2013reverb}, which includes both simulated data (marked as 'SimData') and real recordings (marked as 'RealData').

In SimData, there exist six distinct reverberation conditions: three room volumes (small, medium, and large), and two distances between the speaker and the microphone (50 cm and 200 cm). The RT60 values are approximately 0.25 s, 0.5 s, and 0.7 s. 
The noise is stationary background noise, mainly generated by air conditioning systems. 
SimData has a SNR of 20 dB.

RealData consists of utterances spoken by human speakers in a noisy and reverberant meeting room. It includes two reverberation conditions: one room and two distances between the speaker and the microphone array (approximately 100 cm and 250 cm). The RT60 is about 0.7 s.

\subsubsection{{Test Set for Blind RIR Identification}}
A test set named 'SimACE' is constructed to evaluate blind RIR estimation.
In SimACE, microphone signals are simulated by convolving the clean speech from the 'si\_et\_05' subset in WSJ0 corpus~\cite{paul1992design} with the downsampled recorded RIRs from the 'Single' subset in ACE Challenge~\cite{eaton2016estimation}, and adding noise from the test set in REVERB Challenge~\cite{kinoshita2013reverb}.
The minimum and maximum RT60s are 0.332~s and 1.22 s, respectively.
More details about the RIRs can be found in~\cite{eaton2016estimation}.
We create the SimACE test set because the RT60 labels of the RIRs in SimACE are more accurate than those in REVERB Challenge.
SimACE has a SNR of 20 dB.

\subsection{Implementation of VINP}
\subsubsection{Data Representation}
% VINP focuses on speech recordings sampled at 16kHz. 
Before feeding the speech into VINP, the reverberant waveform is normalized by its maximum absolute value. 
Subsequently, the utterance is transformed using STFT with a Hann window of 512 samples (32~ms) and a hop length of 128 samples (8~ms). 

\begin{figure}[H]
    \centering
    \subfloat[VINP-TCN+SA+S]{
        \centering
        {\includegraphics[width=0.45\linewidth]{figs/TCNSAS.pdf}}
    }
    \subfloat[VINP-oSpatialNet]{
        \centering
        {\includegraphics[width=0.45\linewidth]{figs/mOSPN.pdf}}
    }\hfill
    \caption{DNN architectures in VINP.}
    \label{fig:DNNarch}
\end{figure}

\subsubsection{DNN Architecture}
VINP is able to employ any discriminative dereverberation DNNs to predict the prior distribution of anechoic speech.
In this paper, we evaluate with the network proposed in TCN+SA+S~\cite{zhao2020monaural} and a modified network of the Mamba version of oSpatialNet~\cite{10570301}, resulting in two different versions marked as \textbf{'VINP-TCN+SA+S'} and \textbf{'VINP-oSpatialNet'}, respectively.
For both versions, the input and output of DNNs are the 10-based logarithmic magnitude spectra.
To avoid numerical issues, a small constant $10^{-8}$ is added to the magnitude spectra before taking the logarithm.

The DNN architecture in VINP-TCN+SA+S is composed of temporal convolutional networks (TCNs) and self-attention layers, as shown in Fig. \ref{fig:DNNarch}.
We use the same settings as the original TCN+SA+S~\cite {zhao2020monaural}, except that there is no activation function after the output layer and we do not use dropout.

The DNN architecture in VINP-oSpatialNet is derived from oSpatialNet~\cite{10570301}, as depicted in Fig. \ref{fig:DNNarch}.
A two-dimensional convolution layer with a kernel size of $3 \times 3$  is employed to expand the input to 96 dimensions.
% The stacked cross-band block and narrow-band block 
The remaining modules are the same as those in the original oSpatialNet. 
Except that, for offline processing, we replace the second forward Mamba layer in the narrow-band block with a backward Mamba layer by simply reversing the input and output along the time dimension.
% Subsequently, a linear layer is utilized to
% reduce the dimension to match the output.


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.6\linewidth]{figs/mOSPN.pdf}
%     \caption{DNN architecture of NeGI-mOSPN.}
%     \label{fig:mOSPN}
% \end{figure}

\subsubsection{Training Configuration}
For DNN training, we set the hyperparameter in the loss function to $\epsilon=0.0001$.
The speech utterances are segmented into 3 s.
The batch size of VINP+TCN+SA+S and VINP-oSpatialNet are set to 16 and 4, respectively.
The AdamW optimizer~\cite{loshchilov2017decoupled} with an initial learning rate of 0.001 is employed.
The learning rate exponentially decays with $\mathrm{lr} \leftarrow{} 0.001\times0.97^{\mathrm{epoch}}$ and $\mathrm{lr} \leftarrow{} 0.001\times0.9^{\mathrm{epoch}}$ in VINP-TCN+SA+S and VINP-oSpatialNet, respectively.
Gradient clipping is applied with a L2-norm threshold of 10.
The training is carried out for 800,000 steps in total.
We average the model weights of the best three epochs as the final model.

\subsubsection{VBI Settings}
The length of the CTF filter is set to $L=30$. The fixed smoothing factor $\lambda$ in Eq. (\ref{eq:mu_var_sm}) is set to 0.7 to obtain a stable result.
In rare cases, if the likelihood of the complete data decreases, the VBI iteration will be stopped prematurely.
Since the fundamental frequency of human speech is always higher than 85 Hz~\cite{howard2007voice}, we ignore the three lowest frequency bands and set their coefficients to zero to avoid the effect of extremely low SNR in these bands. 
Therefore, VBI processes a total of 254 frequency bands.


\subsubsection{Pseudo Excitation Signal}
We use a logarithmic sine sweep signal with a frequency range of 100 Hz to 8000 Hz and a duration of 5 s as the pseudo excitation signal.
The formulae for the excitation signal and the corresponding inverse filter can be found in Eq. (\ref{eq:excitation}) and \cite{stan2002comparison,farina2000simultaneous}.

\subsubsection{RT60 and DRR Estimation}
RT60 and DRR are key acoustic parameters that feature the characteristic of RIR, which are thus used in this work for evaluating the accuracy of RIR identification. RT60 and DRR can be directly calculated from RIR.
% Here, we introduce our implementation.

RT60 is the time required for the sound energy in an enclosure to decay by 60 dB after the sound source stops.
% In practice, waiting for the sound energy to drop by 60dB completely may not be a good choice.
% Since the fact that sound energy decays exponentially over time, a relatively smaller range of energy decay is often measured, and then RT60 is estimated through a certain proportional relationship.
Given the RIR waveform, Schroederâ€™s integrated energy decay curve (EDC) is calculated as~\cite{schroeder1965new}
\begin{equation}
\label{eq:edc}
    \mathrm{EDC}(n)=\sum_{m=n}^{\infty}h^2(m).
\end{equation}
Since the fact that sound energy decays exponentially over time, a linear fitting is applied to a portion of logarithmic EDC, and the slope is utilized to calculate RT60.
During this process, the key to linear fitting lies in the heuristic strategy of selecting the fitting range.
% This strategy is usually heuristic.
% In this work, we conduct a traverse linear fitting operation on EDC.
% Specifically, 
In this work, the starting point of linear fitting is selected within the range of a time delay from 20 ms to 50 ms at the direct-path sample, which represents the sample corresponding to the maximum value of the RIR. 
Meanwhile, the ending point is established at a 5 dB attenuation relative to the starting point.
We fit all intervals that meet the conditions and apply the result with the maximum absolute Pearson correlation coefficient to the estimation process of RT60 as
% Subsequently, the fitting result with the maximum absolute Pearson correlation coefficient is applied to the estimation process of RT60, as
\begin{equation}
    T_{60}=-60/k,
\end{equation}
where $k$ is the slope of the fitted line.

DRR refers to the ratio between the energy of the direct-path sound and the reverberant sound.
In this work, the DRR is defined as
\begin{equation}
    \mathrm{DRR}=10\log_{10}\frac{\sum_{n=n_d-\Delta n_d}^{n_d+\Delta n_d}h^2(n)}{\sum_{n=0}^{n_d-\Delta n_d}h^2(n)+\sum_{n=n_d+\Delta n_d}^{\infty}h^2(n)},
\end{equation}
where the direct-path signal arrives at the $n_d$th sample, and $\Delta n_d$ is the additional sample spread for the direct-path response which typically corresponds to 2.5 ms~\cite{eaton2016estimation}.








\subsection{Comparison Methods}
\subsubsection{{Speech Dereverberation}}
We compare VINP with various advanced dereverberation approaches including GWPE~\cite{yoshioka2012generalization},
SkipConvNet~\cite{kothapally2020skipconvnet}\footnote{\url{https://github.com/zehuachenImperial/SkipConvNet}}, 
CMGAN~\cite{abdulatif2024cmgan}\footnote{\url{https://github.com/ruizhecao96/CMGAN}},
and StoRM~\cite{lemercier2023storm}\footnote{\url{https://github.com/sp-uhh/storm}}.
In addition, to demonstrate the effect of VINP, the DNNs in VINP-TCN+SA+S and VINP-oSpatialNet are also compared, i.e. TCN+SA+S~\cite{zhao2020monaural} and oSpatialNet~\cite{10570301}.
In TCN+SA+S, we use the recommended Griffin-Lim's iterative algorithm~\cite{griffin1984signal} to restore the phase spectrum. 
In oSpatialNet, for a fair comparison, the backward Mamba is also applied to oSpatialNet.
Different from VINP-oSpatialNet, the oSpatialNet employed for comparison processes the complex-valued speech spectrum as the original paper~\cite{10570301}.
And this version is marked as 'oSpatialNet*'. 





% In addition, we also compare with the speech dereverberation networks that used in the proposed framework, i.e. TCN+SA+S~\cite{zhao2020monaural} and oSpatialNet~\cite{10570301}, following the configurations of their original papers. Except that, in TCN+SA+S, we use Griffin-Lim's iterative algorithm~\cite{griffin1984signal} to restore the phase spectrum. 
% For fair comparison, the backward Mamba is also applied for oSpatialNet, and this version is marked as 'oSpatialNet*'. 


For all comparison methods, we use their recommended configuration and official codes (if available).
All DL-based approaches are trained from scratch on the same training set.
GWPE is implemented using the NaraWPE python package~\cite{Drude2018NaraWPE}\footnote{\url{https://github.com/fgnt/nara_wpe}}.

% Additionally, the DNN in VINP-oSpatialNet without VBI is also compared, in which we expand the number of channels in the input and output layers to 2. 
% Both the input and output are set to be the concatenated real and imaginary parts of the spectrum.
% And the negative scale-invariant signal-to-distortion ratio (SISDR) of speech waveform is adopted as the loss function.
% The DNN architecture is marked as 'oSpatialNet-mamba*'.

The number of parameters and the multiply-accumulate operations per second (MACs, G/s) of the DL-based approaches are shown in Table \ref{tab:para_mac}.
Additionally, under the same settings of STFT and CTF length, a comparison of MACs per second and per iteration between the VEM algorithm in VINP and the EM algorithm in our previous work RVAE-EM~\cite{wang2024rvae} with regard to speech length is presented in Fig. \ref{fig:macs}.
The asymptotic complexity of the VBI procedure in VINP is $O(FTL^2)$, indicating a linear growth with respect to the speech length, and its MACs is approximately a constant value of 0.27 G/s. 
In contrast, the asymptotic complexity of the EM algorithm in RVAE-EM is $O(FT^3)$ and there is always $T \gg L$.
\begin{table}[H]
\centering
\renewcommand\arraystretch{1.2}
\caption{Number of parameters and MACs per second\\ for DL-based methods}
\label{tab:para_mac}
\begin{tabular}{c|c|c}
    \Xhline{1pt}
    Method&Params~(M)&MACs~(G/s)\\
    \Xhline{0.4pt}
    SkipConvNet~\cite{kothapally2020skipconvnet}&64.3&11\\
    CMGAN~\cite{abdulatif2024cmgan} & 1.8 & 31\\
    StoRM~\cite{lemercier2023storm} & 27.3+27.8=55.1 & 2300\\
    \cdashline{1-3}
    TCN+SA+S~\cite{zhao2020monaural}& 4.7 & 0.7\\
    oSpatialNet*~\cite{10570301}&1.7&36.6\\
    \Xhline{0.4pt}
    VINP-TCN+SA+S& 4.7 & 0.7+0.27$\times$iterations\\
    VINP-oSpatialNet&1.7&36.6+0.27$\times$iterations\\
    \Xhline{1pt}
\end{tabular}
\end{table}
\begin{figure}
    \centering
    \includegraphics[width=0.46\linewidth]{figs/MACs.pdf}
    \caption{MACs per second and per iteration versus speech length.}
    \label{fig:macs}
\end{figure}

\subsubsection{{Blind RIR Identification}}
We measure RT60 and DRR through the RIR estimates to show the effectiveness of RIR identification.
The comparison methods include several classical blind RT60 and DRR estimation methods, such as Ratnam's approach~\cite{ratnam2003blind}\footnote{\url{https://github.com/nuniz/blind_rt60}} and Jeub's approach~\cite{jeub2011blind}\footnote{\url{https://ww2.mathworks.cn/matlabcentral/fileexchange/32752-blind-direct-to-reverberant-energy-ratio-drr-estimation}}.
As a DL-based blind RIR identification method, BUDDy~\cite{lemercier2024unsupervised}\footnote{\url{https://github.com/sp-uhh/buddy}} is also compared.
All these methods are implemented through the open-sourced official codes. 
Specifically, because the output of BUDDy is the RIR waveform, we employ the same implementation as in VINP to estimate the RT60 and DRR.
Notice that the acoustic conditions of our training set and test set are mismatched. 
BUDDy, as an unsupervised approach, is designed to bridge the performance gap between scenarios with matched and mismatched acoustic conditions. 
Therefore, we directly employ the model parameters provided by the authors without retraining.


\begin{table*}[htbp]
\centering
\renewcommand\arraystretch{1.2}
\caption{Dereverberation results on REVERB~(1-ch)}
\label{tab:results_SD}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
    \Xhline{1pt}
    \multirow{3}{*}{Method}&\multicolumn{7}{c|}{SimData}&\multicolumn{5}{c}{RealData} \\
    \Xcline{2-13}{0.4pt}
    &\multirow{2}{*}{PESQ}&\multirow{2}{*}{ESTOI}&\multicolumn{2}{c|}{DNSMOS}&\multicolumn{3}{c|}{WER~(\%)}&\multicolumn{2}{c|}{DNSMOS}&\multicolumn{3}{c}{WER~(\%)}\\
    \Xcline{4-13}{0.4pt}
    &&&P.835&P.808&tiny&small&medium&P.835&P.808&tiny&small&medium\\
    \Xhline{0.4pt}
    Unprocessed&1.48&0.70&2.37&3.20&13.1&5.6&4.6&1.31&2.82&24.1&7.9&5.7\\
    \textcolor{gray}{Oracle}&\textcolor{gray}{-}&\textcolor{gray}{-}&\textcolor{gray}{3.76}&\textcolor{gray}{3.90}&\textcolor{gray}{7.6}&\textcolor{gray}{4.5}&\textcolor{gray}{4.0}&\textcolor{gray}{-}&\textcolor{gray}{-}&\textcolor{gray}{-}&\textcolor{gray}{-}&\textcolor{gray}{-}\\
    \Xhline{0.4pt}
    GWPE~\cite{yoshioka2012generalization}&1.55&0.72&2.41&3.22&12.1&5.5&4.6&1.42&2.83&21.4&6.8&5.7\\
    SkipConvNet~\cite{kothapally2020skipconvnet}&2.12&0.81&3.20&3.60&\cellcolor{graybackground}13.3&\cellcolor{graybackground}6.3&\cellcolor{graybackground}5.2&2.84&3.32&\cellcolor{graybackground}24.5&\cellcolor{graybackground}9.3&\cellcolor{graybackground}7.3\\
    
    CMGAN~\cite{abdulatif2024cmgan} &2.85&0.90&\textbf{3.82}&3.81&9.5&5.1&4.4&\textbf{3.87}&4.00&12.9&5.9&5.0\\
    StoRM~\cite{lemercier2023storm} &2.34&0.86&3.73&\textbf{3.96}&11.4&\cellcolor{graybackground}6.2&\cellcolor{graybackground}5.1&3.72&\textbf{4.01}&17.5&\cellcolor{graybackground}10.2&\cellcolor{graybackground}8.0\\
    % mFSN&&&&&&&&&&\\
    \cdashline{1-13}
    TCN+SA+S~\cite{zhao2020monaural}&2.60&0.86&3.50&3.73&12.1&\cellcolor{graybackground}6.5&\cellcolor{graybackground}5.5&3.37&3.73&\cellcolor{graybackground}27.3&\cellcolor{graybackground}12.6&\cellcolor{graybackground}10.0 \\
    oSpatialNet*~\cite{10570301}&\textbf{2.87}&\textbf{0.92}&3.57&3.88&8.9&4.9&4.3&3.48&3.87&10.5&5.4&4.5\\
    \Xhline{0.4pt}
    % VINP-TCN+SA+S (50 iterations)&2.46&0.87&3.47&\textbf{3.89}&8.9&5.1&4.5&3.19&3.80&11.9&6.2&5.1\\
    VINP-TCN+SA+S (prop.) &2.52&0.87&3.47&3.88&8.9&5.1&4.3&3.18&3.77&11.6&6.1&5.3\\
    % VINP-oSpatialNet (50 iterations)&2.74&\textbf{0.90}&3.49&3.86&\textbf{8.2}&\textbf{4.8}&\textbf{4.3}&3.33&3.81&\textbf{8.9}&\textbf{5.1}&\textbf{4.3}\\
    VINP-oSpatialNet (prop.)&2.82&0.90&3.49&3.86&\textbf{8.3}&\textbf{4.8}&\textbf{4.1}&3.33&3.80&\textbf{8.9}&\textbf{5.0}&\textbf{4.3}\\
    \Xhline{1pt}

\end{tabular}
\end{table*}



% \begin{table*}[htbp]
% \centering
% \renewcommand\arraystretch{1.2}
% \caption{Dereverberation results on REVERB~(1-ch)}
% \label{tab:results_SD}
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
%     \Xhline{1pt}
%     \multirow{3}{*}{Method}&\multicolumn{7}{c|}{SimData}&\multicolumn{5}{c}{RealData} \\
%     \Xcline{2-13}{0.4pt}
%     &\multirow{2}{*}{PESQ}&\multirow{2}{*}{ESTOI}&\multicolumn{2}{c|}{DNSMOS}&\multicolumn{3}{c|}{WER~(\%)}&\multicolumn{2}{c|}{DNSMOS}&\multicolumn{3}{c}{WER~(\%)}\\
%     \Xcline{4-13}{0.4pt}
%     &&&P.835&P.808&base&small&medium&P.835&P.808&base&small&medium\\
%     \Xhline{0.4pt}
%     Unprocessed&1.48&0.70&2.37&3.20&10.1&5.6&4.7&1.31&2.82&14.6&7.7&5.6\\
%     \textcolor{gray}{Oracle}&-&-&3.76&3.90&7.9&4.5&4.1&-&-&-&-&-\\
%     \Xhline{0.4pt}
%     GWPE~\cite{yoshioka2012generalization}&1.57&0.72&2.41&3.22&9.8&5.4&4.7&1.43&2.83&12.3&7.0&5.5\\
%     SkipConvNet~\cite{kothapally2020skipconvnet}&2.12&0.81&3.20&3.60&10.4&6.4&5.3&2.74&3.32&15.5&9.3&7.4\\
    
%     CMGAN~\cite{abdulatif2024cmgan} &\textbf{2.85}&\textbf{0.90}&\textbf{3.82}&3.81&8.4&5.1&4.5&\textbf{3.86}&\textbf{4.00}&10.6&5.8&5.2\\
%     StoRM~\cite{lemercier2023storm} &2.34&0.86&\textbf{3.73}&\textbf{3.96}&10.4&6.0&5.1&\textbf{3.72}&\textbf{4.01}&18.1&9.9&9.7\\
%     % mFSN&&&&&&&&&&\\
%     \cdashline{1-13}
%     TCN+SA+S~\cite{zhao2020monaural}&2.59&0.86&3.50&3.73&10.3&6.5&5.0&3.37&3.73&18.8&12.9&6.7 \\
%     mOSPN&\textbf{2.87}&\textbf{0.92}&3.54&3.88&\textbf{8.0}&4.8&\textbf{4.3}&3.39&3.86&10.4&5.3&\textbf{4.5}\\
%     % \Xhline{0.4pt}
%     % VINP-TCN+SA+S (40 iterations)&2.43&0.87&3.47&\textbf{3.89}&9.1&&4.5&3.20&3.81&9.2&6.1&\\
%     % VINP-TCN+SA+S (80 iterations)&2.50&0.87&3.47&3.88&9.0&&4.4&&&&6.1&\\
%     % VINP-mOSPN (40 iterations)&2.70&\textbf{0.90}&3.49&3.86&\textbf{7.8}&&\textbf{4.3}&3.34&3.81&\textbf{7.2}&\textbf{5.1}&\\
%     % VINP-mOSPN (80 iterations)&2.80&\textbf{0.90}&3.48&3.86&8.3&&\textbf{4.3}&3.32&3.80&&\textbf{5.1}&\textbf{4.3}\\
%     \Xhline{0.4pt}
%     NeGI-TCN+SA+S-10&1.92&0.84&3.34&3.74&10.0&&4.7&3.16&3.76&&6.7&\\
%     NeGI-TCN+SA+S-20&2.20&0.86&3.45&3.85&8.7&&4.6&3.21&3.82&&6.3&\\
%     NeGI-TCN+SA+S-30&2.36&0.87&3.47&3.88&9.2&&4.5&3.21&3.82&&6.3&\\
%     NeGI-TCN+SA+S-40&2.43&0.87&3.47&3.89&9.1&&4.5&3.20&3.81&9.2&6.1&\\
%     NeGI-TCN+SA+S-50&2.46&0.87&3.47&3.89&9.1&5.1&4.5&3.19&3.80&9.2&6.2&5.1\\
%     NeGI-TCN+SA+S-60&2.48&0.87&3.47&3.88&9.1&&4.5&3.19&3.79&&6.1&\\
%     NeGI-TCN+SA+S-70&2.49&0.87&3.47&3.88&9.1&&4.4&3.18&3.79&&6.1&\\
%     NeGI-TCN+SA+S-80&2.50&0.87&3.47&3.88&9.0&&4.4\\
%     NeGI-TCN+SA+S-90&2.51&0.87&3.47&3.88&&&\\
%     NeGI-TCN+SA+S-100&2.51&0.87&3.47&3.88&9.1&5.1&4.4&3.17&3.77&&6.2&5.1\\
%     \Xhline{0.4pt}
%     NeGI-mOSPN-10&2.17&0.87&3.34&3.67&8.6&&4.4&3.24&3.71&&5.0&\\
%     NeGI-mOSPN-20&2.45&0.89&3.46&3.80&8.2&&4.3&3.33&3.79&&5.0&\\
%     NeGI-mOSPN-30&2.62&0.90&3.48&3.84&7.8&&4.3&3.34&3.81&&5.1&\\
%     NeGI-mOSPN-40&2.70&0.90&3.49&3.86&7.8&&4.3&3.34&3.81&7.2&5.1&\\
%     NeGI-mOSPN-50&2.74&0.90&3.49&3.86&8.0&4.8&4.3&3.33&3.81&7.1&5.1&4.3\\
%     NeGI-mOSPN-60&2.77&0.90&3.49&3.86&8.3&&4.3&3.33&3.81&&5.1&\\
%     NeGI-mOSPN-70&2.79&0.90&3.48&3.86&8.3&&4.3&3.32&3.80&&5.1&\\
%     NeGI-mOSPN-80&2.80&0.90&3.48&3.86&8.3&&4.3&3.32&3.80&7.1&5.1&\\
%     NeGI-mOSPN-90&2.81&0.90&3.48&3.86&&&&3.32&3.80&&5.0&\\
%     NeGI-mOSPN-100&2.82&0.90&3.48&3.86&8.5&4.8&&3.32&3.80&7.0&5.0&4.3\\
%     % EMA 0.8\\
%     % NeGI-TCN+SA+S (50 iterations)&2.40&0.87&3.47&3.88&9.1&4.5&3.21&3.82&8.9&5.3\\
%     % NeGI-TCN+SA+S (100 iterations)&2.51&0.87&3.48&3.88&9.1&4.5&3.19&3.79&8.8&5.1\\
%     % NeGI-mOSPN (50 iterations)&2.67&0.90&3.49&3.85&8.4&&3.33&3.81&\textbf{7.1}&\textbf{4.3}\\
%     % NeGI-mOSPN (100 iterations)&2.81&0.90&3.49&3.86&8.2&4.4&&&&\\
%     % \Xhline{0.4pt}
%     % EMA 0.8 0.7\\
%     % NeGI-TCN+SA+S (50 iterations)&&&&&&&3.19&3.82&9.2&\\
%     % \Xhline{0.4pt}
%     % EMA 0.7 0.8\\
%     % NeGI-TCN+SA+S (50 iterations)&&&&&&&3.21&3.81&8.9&\\

%     % % NeGI-TCN+SA+S-e111&2.45&0.87&3.46&3.88&9.1&&3.19&3.80&9.2&\\
%     % % \textbf{NeGI-TCN+SA+S-e119-121}&&&&&&&&&&\\
%     % % NeGI-mOSPN-ori-e21&\textbf{2.83}&\textbf{0.90}&3.47&\textbf{3.86}&&\textbf{4.3}&3.34&3.82&&\textbf{4.5}\\
%     % % \textbf{NeGI-mOSPN-e13}&2.77&0.90&3.49&3.87&8.3&4.3&3.33&3.82&9.1&4.4\\
%     % % \textbf{NeGI-mOSPN-e19}&2.75&0.90&3.48&3.86&8.3&&3.29&3.80&7.2&\\
%     % % \Xhline{0.4pt}
%     % % % \textcolor{gray}{mOSPN~(cMSE)}&2.84&0.91&3.58&3.86&9.5&4.3&3.38&3.80&10.8&4.7  \\
%     % % \textcolor{gray}{mOSPN-e10}&2.87&0.92&3.54&3.88&8.0&4.3&3.39&3.86&10.4&4.5\\
%     % % \textcolor{gray}{mOSPN~(KL)}&&&&&&&&&&\\
%     \Xhline{1pt}

% \end{tabular}
% \end{table*}


% \begin{table}[H]
% \centering
% \renewcommand\arraystretch{1.2}
% \caption{Dereverberation results on REVERB SimData}
% \label{tab:results_sim}
% \begin{tabular}{c|c|c|c|c|c|c|c}
%     \Xhline{1pt}
%     \multirow{2}{*}{Method}
%     &\multirow{2}{*}{PESQ}&\multirow{2}{*}{ESTOI}&\multicolumn{2}{c|}{DNSMOS}&\multicolumn{3}{c|}{WER~(\%)}\\
%     \Xcline{4-5}{0.4pt}
%     &&&P.835&P.808&m&s&b\\
%     \Xhline{0.4pt}
%     Unprocessed  &  1.48  & 0.70  & 2.37 & 3.20 & 4.7   \\
%     Oracle &-&-&3.76&3.90&4.1\\
%     \Xhline{0.4pt}
%     GWPE~\cite{yoshioka2012generalization}&1.57&0.72&2.41&3.22&4.7\\
%     SkipConvNet~\cite{kothapally2020skipconvnet} &2.12&0.81&3.20&3.60&5.3\\
%     TCN+SA+S~\cite{zhao2020monaural}&2.59&0.86&3.50&3.73&5.0 \\
%     CMGAN~\cite{abdulatif2024cmgan} &\textbf{2.85}&\textbf{0.90}&\textbf{3.82}&3.81&\textbf{4.5}\\
%     StoRM~\cite{lemercier2023storm} &2.34&0.86&\textbf{3.73}&\textbf{3.96}&5.1\\
%     \Xhline{0.4pt}
%     NeGI-mOSPN&\textbf{2.83}&\textbf{0.90}&3.47&\textbf{3.86}&\textbf{4.3}\\
%     NeGI (TCN+SA+S) \\
%     \Xhline{0.4pt}
%     NeGI-mOSPN-e1&2.53&0.88&3.40&3.88&\\
%     NeGI-mOSPN-e2&2.68&0.89&3.48&3.88&\\
%     NeGI-mOSPN-e3&2.64&0.88&3.41&3.86&\\
%     NeGI-mOSPN-e4&2.63&0.89&3.46&3.88&\\
%     NeGI-mOSPN-e5&2.66&0.89&3.48&3.88&\\
%     NeGI-mOSPN-e7&2.69&0.89&3.47&3.87&\\
%     NeGI-mOSPN-e13&2.77&0.90&3.49&3.87&4.3&&8.3\\
%     NeGI-mOSPN-e9&&&&&\\
%     \Xhline{0.4pt}
%     ablation\\
%     mOSPN-cMSE-e10&2.84&0.91&3.58&3.86&4.3\\
%     \Xhline{1pt}
% \end{tabular}
% \end{table}
% \begin{table}[H]
% \centering
% \renewcommand\arraystretch{1.2}
% \caption{Dereverberation results on REVERB RealData}
% \label{tab:results_real}
% \begin{tabular}{c|c|c|c}
%     \Xhline{1pt}
%     \multirow{2}{*}{Method}
%     &\multicolumn{2}{c|}{DNSMOS}&\multirow{2}{*}{WER~(\%)}\\
%     \Xcline{2-3}{0.4pt}
%     &P.835&P.808&\\
%     \Xhline{0.4pt}
%     Unprocessed  &1.31 & 2.82 &5.6 \\
%     \Xhline{0.4pt}
%     GWPE~\cite{yoshioka2012generalization}&1.43&2.83&5.5\\
%     SkipConvNet~\cite{kothapally2020skipconvnet} &2.74&3.32&7.4\\
%     TCN+SA+S~\cite{zhao2020monaural}&3.37&3.73&6.7 \\
%     CMGAN~\cite{abdulatif2024cmgan} &\textbf{3.86}&\textbf{4.00}&\textbf{5.2}\\
%     StoRM~\cite{lemercier2023storm} &\textbf{3.72}&\textbf{4.01}&9.7\\
%     \Xhline{0.4pt}
%     NeGI-mOSPN&3.34&3.82&\textbf{4.5}\\
%     \Xhline{0.4pt}
%     NeGI-mOSPN-e13&3.33&3.82&4.4\textbar5.1\textbar9.1\\
%     \Xhline{0.4pt}
%     ablation\\
%     mOSPN-cMSE-e10&3.38&3.80&4.7\textbar5.5\textbar10.8\\
%     \Xhline{1pt}
% \end{tabular}
% \end{table}



\subsection{Evaluation Metrics}
\subsubsection{{Speech Dereverberation}}

Speech dereverberation performance is evaluated in terms of both perception quality and ASR accuracy. 
% Apart from large-scale manual evaluation, there is no gold standard for perception quality assessment.
% Therefore,
We use the commonly used speech quality metrics including perceptual evaluation of speech quality (PESQ)~\cite{rix2001perceptual}, extended short-time objective intelligibility (ESTOI)~\cite{jensen2016algorithm}, and
% speech to reverberation modulation energy ratio (SRMR)~\cite{falk2010non,6953337}, 
deep noise suppression mean opinion score (DNSMOS)~\cite{reddy2021dnsmos,reddy2022dnsmos}.
% Notice that for SRMR, we use the normalized version proposed in \cite{6953337}.
Speech utterances are normalized by their maximum absolute value before evaluation. 
Higher scores indicate better speech quality and intelligibility.

To demonstrate the effectiveness of our method on ASR systems, we utilize the pre-trained Whisper~\cite{radford2023whisper} â€˜tiny' model (with 39 M parameters), 'small' model (with 244 M parameters), and â€˜mediumâ€™ model (with 769 M parameters) for ASR evaluation. 
No additional dataset-specific finetuning or retraining is applied before ASR inference. 
The word error rate (WER) is used as the evaluation metric.
Lower WER indicates better ASR performance.

\subsubsection{{Blind RIR Identification}}
We use the accuracy of RT60 and DRR estimation to evaluate the performance of blind RIR identification.
We present the mean absolute error (MAE) and the root mean square error (RMSE) of RT60 and DRR between their estimates and ground-truth values over the SimACE test set.
Lower MAE and RMSE indicate better results.