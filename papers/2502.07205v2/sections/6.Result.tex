



\begin{figure}
    \centering
    \subfloat[Reverberant speech]{
        \includegraphics[width=0.9\linewidth]{figs/demo/reverb.pdf}
        }\hfill
    \subfloat[Output of TCN+SA+S]{
        \includegraphics[width=0.9\linewidth]{figs/demo/TCNSAS.pdf}
        }\hfill
    \subfloat[Output of VINP-TCN+SA+S]{
        \includegraphics[width=0.9\linewidth]{figs/demo/NeGI-TCNSAS.pdf}
        }\hfill
    \subfloat[Oracle speech]{
        \includegraphics[width=0.9\linewidth]{figs/demo/oracle.pdf}
        }\hfill
    \caption{Examples of magnitude spectra (RT60=0.7 s).}
    \label{fig:demo}
\end{figure}
\begin{figure*}[!ht]
    \centering
    \subfloat[Logarithmic likelihood]{
        \includegraphics[width=0.185\linewidth]{figs/LIKELIvsSTEP.pdf}
        }
    \subfloat[PESQ]{
        \includegraphics[width=0.185\linewidth]{figs/PESQvsSTEP.pdf}
        }
    \subfloat[ESTOI]{
        \includegraphics[width=0.185\linewidth]{figs/ESTOIvsSTEP.pdf}
        }
    \subfloat[DNSMOS]{
        \includegraphics[width=0.185\linewidth]{figs/DNS808vsSTEP.pdf}
        }
    \subfloat[WER with 'tiny' model]{
        \includegraphics[width=0.185\linewidth]{figs/WERvsSTEP.pdf}
        }
    % \subfloat[Logarithmic likelihood on RealData]{
    %     \includegraphics[width=0.23\linewidth]{figs/LIKELIvsSTEP-Real.pdf}
    %     }
    % \subfloat[DNSMOS on RealData]{
    %     \includegraphics[width=0.23\linewidth]{figs/DNS808vsSTEP-Real.pdf}
    %     }
    % \subfloat[WER with tiny model on RealData]{
    %     \includegraphics[width=0.23\linewidth]{figs/WERvsSTEP-Real.pdf}
    %     }
    
    \caption{Logarithmic likelihood and dereverberation metrics as a function of VEM iterations on REVERB SimData.}
    \label{fig:CurveDereverb}
\end{figure*}
% \section{Experimental Results and Discussion}
% \label{sec:exp}
\subsection{Results and Analysis}
\subsubsection{Speech Dereverberation}

Setting the number of VEM iterations to 100, the dereverberation results are presented in Table \ref{tab:results_SD}, where the bold font denotes the best results.
The results with negative dereverberation effects are marked with a darkened background.

As the metrics of unprocessed speech and oracle speech indicate, a larger ASR system implies a stronger recognition ability for oracle speech and stronger robustness to reverberation.
Compared with using a large ASR system alone, introducing an ASR-effective front-end speech dereverberation system can significantly improve the performance of ASR with fewer parameters and lower computational costs.
Among the comparison methods, only a few approaches are proven to be effective for ASR, such as GWPE, CMGAN, oSpatialNet*, and the proposed VINP-TCN+SA+S and VINP-oSpatialNet.
The classical GWPE yields a marginal improvement in ASR performance.
% , which can be attributed to the fact that the whisper model employed herein exhibits a certain level of robustness against speech distortion.
Among the DL-based methods, SkipConvNet, StoRM and TCN+SA+S encounter failures in ASR.
TCN+SA+S and StoRM demonstrate marginal effectiveness solely when the 'tiny' ASR model is employed.
The reason for such ASR performance lies in the artifact induced by DNN, which is a common phenomenon in similar DL-based single-channel systems~\cite{iwamoto22_interspeech,iwamoto2024does}.
Different from the comparison methods, VINP employs a linear CTF filtering process to model the reverberation effect and leverages the DNN output as a prior distribution of anechoic source speech.
During the VBI procedure, VINP takes into account both this prior information and the observed data.
By doing this, VINP circumvents the direct utilization of DNN output and thereby reduces the artifacts.
Meanwhile, VINP still utilizes the powerful non-linear modeling ability of DNN.
Consequently, VINP exhibits a remarkable superiority over the other approaches in ASR performance.
When comparing VINP-TCN+SA+S with its backbone network TCN+SA+S, a gap in WER can be discerned, indicating that VINP can make an ASR-ineffective DNN effective. 
Both oSpatialNet* and VINP-oSpatialNet achieve ASR performance close to that of oracle speech on REVERB SimData when using the small and medium ASR systems.
In other situations, a gap still exists, indicating that VINP can make an ASR-effective DNN even more effective.
Because VINP-oSpatialNet provides an anechoic speech prior with higher quality, its ASR performance is superior to VINP-TCN+SA+S.
Notably, VINP-oSpatialNet achieves SOTA performance in ASR.



Regarding speech quality, the ESTOI of VINP is roughly the same as that of the backbone network, while the PESQ shows a slight decrease compared to the backbone networks.
This indicates that VINP is restricted by the capabilities of the backbone DNN. 
It is also worth noting that the PESQ increases with the growth of the number of VEM iterations, which we will demonstrate in detail in the experiments presented later.
In subjective evaluation, the DNSMOS metrics of VINP-TCN+SA+S and VINP-oSpatialNet reach an advanced level.
However, it does not rank the highest among all methods, especially when considering DNSMOS P.835.
We hold the view that DNSMOS exhibits a certain preference for enhanced speech.
As evidence, it can be seen that the DNSMOS of CMGAN and StoRM is even higher than that of oracle speech, indicating a preference for generative DNNs.
In addition, VINP-TCN+SA+S and VINP-oSpatialNet exhibit worse and comparable performance to TCN+SA+S in DNSMOS P.835.
However, in our auditory perception, the output speech of VINP exhibits a higher degree of naturalness.
To illustrate the characteristics of different methods, examples of the reverberant speech, the output of TCN+SA+S, the output of VINP-TCN+SA+S, and the oracle speech are shown in Fig.~\ref{fig:demo}.
% In addition, some examples of magnitude spectra are also illustrated in Fig. \ref{fig:demo}.
% Because the random components in the spectrum are difficult for DNNs to learn,
Due to the insufficient modeling capabilities of DNN, the output of TCN+SA+S has some overly smoothed artifacts.
On the contrary, VINP-TCN+SA+S not only refers to the priors provided by DNNs, but also directly utilizes the observation during the VBI procedure, resulting in a better estimation of the anechoic speech spectrum.
Moreover, VINP-TCN+SA+S demonstrates a better performance of background noise control.
A series of enhanced audio examples are available online\footnote{\url{https://github.com/Audio-WestlakeU/VINP}}.
% CMGAN tends to enhance the features of spectrograms by increasing the power of resonance peaks, whereas VINP tends to reduce more background noise.
% \be


To illustrate the influence of VBI iterations on the performance of VINP, we show the average logarithmic likelihood of the complete data $\ln p(\mathbf{S},\mathbf{X})$ (where the constant is omitted) and some metrics on REVERB SimData as the iteration count rises in Fig. \ref{fig:CurveDereverb}.
As the iteration count increases, the logarithmic likelihood and all metrics shift in a positive direction and tend to converge.
Specifically, ESTOI, DNSMOS, and WER exhibit rapid convergence. 
In contrast, PESQ shows a continuous improvement that closely resembles the behavior of the logarithmic likelihood.
This phenomenon indicates that better dereverberation performance can be achieved at the cost of computational complexity.
Additionally, the capability of the backbone network determines the upper bound of performance of PESQ, ESTOI and DNSMOS.
The logarithmic likelihood of VINP-TCN+SA+S and VINP-oSpatialNet converge to nearly the same values, yet their metrics are different, indicating that when the prior quality varies, the logarithmic likelihood cannot be directly used as an indicator to measure the enhancement performance.




\subsubsection{Blind RIR Identification}

Also setting the number of VEM iterations to 100, the RT60 and DRR estimation results are illustrated in Table \ref{tab:RIR_estimation}, where the bold font represents the best results.

VINP surpasses all comparison methods, reaching the SOTA level for all metrics.
Theoretically, one limitation of our method is that a CTF filter with a finite length can only model a RIR with a finite length.
Nevertheless, it remains sufficient for RT60 and DRR estimation even under large-reverberation conditions, because the signal energy at the tail of the RIR is relatively negligible.
From the perspective of spatial acoustic parameter estimation, VINP can estimate RIR well.
\begin{table}
\centering
\caption{Blind RT60 and DRR estimation results on SimACE}
\label{tab:RIR_estimation}
\renewcommand\arraystretch{1.2}
\begin{tabular}{c|c|c|c|c}
    \Xhline{1pt}
    \multirow{2}{*}{Method}& \multicolumn{2}{c|}{RT60 (s)}& \multicolumn{2}{c}{DRR (dB)}\\
    \Xcline{2-5}{0.4pt}
    &MAE&RMSE&MAE&RMSE\\
    \Xhline{0.4pt}
    Ratnam's~\cite{ratnam2003blind}&0.151&0.182&-&-\\
    Jeub's~\cite{jeub2011blind}&-&-&7.14&8.69\\
    BUDDy~\cite{lemercier2024unsupervised}&0.089&0.132&3.93&4.57\\
    \Xhline{0.4pt}
    VINP-TCN+SA+S (prop.)&\textbf{0.079}&\textbf{0.094}&\textbf{3.83}&\textbf{4.27}\\
    VINP-oSpatialNet (prop.)&\textbf{0.079}&0.098&3.87&4.32\\
    % \Xhline{0.4pt}
    % VINP-TCN+SA+S\\
    % 10step&0.113&0.127&4.67&5.03\\
    % 20step&0.097&0.113&4.38&4.76\\
    % 30step&0.090&0.107&4.23&4.63\\
    % 40step&0.087&0.103&4.12&4.53\\
    % 50step&0.084&0.101&4.04&4.46\\
    % 60step&0.082&0.098&3.98&4.40\\
    % 70step&0.081&0.097&3.93&4.36\\
    % 80step&0.080&0.096&3.89&4.32\\
    % 90step&0.079&0.095&3.85&4.30\\
    % 100step&0.079&0.094&3.83&4.27\\
    % \Xhline{0.4pt}
    % VINP-mOSPN\\
    % 10step&0.098&0.114&3.84&4.14\\
    % 20step&0.089&0.109&3.63&3.95\\
    % 30step&0.086&0.106&3.92&4.26\\
    % 40step&0.084&0.104&3.99&4.38\\
    % 50step&0.083&0.103&3.99&4.40\\
    % 60step&0.082&0.102&3.96&4.39\\
    % 70step&0.081&0.101&3.94&4.37\\
    % 80step&0.080&0.100&3.91&4.36\\
    % 90step&0.080&0.098&3.89&4.34\\
    % 100step&0.079&0.098&3.87&4.32\\
    \Xhline{1pt}
\end{tabular}
\end{table}
\begin{figure}
    \centering
    \subfloat[RT60 estimation]{
        \includegraphics[width=0.46\linewidth]{figs/T60MAEvsSTEP-RIR.pdf}
        }
    \subfloat[DRR estimation]{
        \includegraphics[width=0.46\linewidth]{figs/DRRMAEvsSTEP-RIR.pdf}
        }
    \caption{MAE of RT60 and DRR estimation as a function of VEM iterations on SimACE.}
    \label{fig:CurveRIR}
\end{figure}

We also show the MAE of RT60 and DRR estimation as the iteration count rises in Fig. \ref{fig:CurveRIR}.
As can be observed, as the iteration count increases, the errors of RT60 and DRR estimation decrease, which indicates an improved quality of RIR estimation.
After 30 iterations, VINP-TCN+SA+S and VINP-oSpatialNet start to outperform BUDDy.
When the iteration count is less than 70, VINP-oSpatialNet outperforms VINP-TCN+SA+S. 
Conversely, when the iteration count is larger than 70, VINP-TCN+SA+S is slightly superior to VINP-oSpatialNet.
This phenomenon may be related to the DNN structure. 
Owing to the complexity of the entire system, we are currently unable to draw a conclusion about the preference for a specific DNN architecture for RIR identification.
Unlike the experimental results of dereverberation, the identification of RIR does not seem to be sensitive to the capability of DNN when there are enough iterations.

