\section{Related Work}
\label{sec:related}

\textbf{Social Welfare Function and RL.}
In RL with multiple stakeholders, various social welfare functions have been explored, including generalized Gini-Welfare \cite{Yu24, cousins2024welfare}, $p$-means \cite{verma2024, fan23, cousins2024welfare}, proportional-fairness \cite{ju2024achieving}, Nash welfare \cite{mandal2023sociallyfairreinforcementlearning}, among others. \cite{alamdari2024policy} investigated ordinal social welfare functions rather than cardinal ones. 

A related line of work has emerged in RLHF for LLMs, where social choice theory has been used to aggregate multiple reward models. For example, approaches leveraging Nash welfare \cite{zhong2024rlhf}, $\alpha$-fairness \cite{park2024rlhf}, and Egalitarian welfare \cite{Chakraborty24} have been proposed. However, in both RLHF and broader multi-stakeholder RL, existing works typically assume a fixed social welfare function and focus on optimizing a single policy \cite{hayes_practical_2022}.

\textbf{Portfolios in MORL.}
In the broader MORL literature, the concept of portfolios of policies is well-established. A common approach is to compute solutions that approximate the Pareto front \cite{parisi14, moff14, radulescu_multi-objective_2019, hayes_practical_2022}. While the Pareto front provides a characterization of trade-offs between conflicting objectives, it does not assume a specific set of social welfare functions. This generality, while powerful, presents challenges in practical multi-stakeholder settings. That is, the Pareto front does not directly correspond to any specific notion of welfare or utility, making it difficult to interpret the societal implications of these policies. Furthermore, without an explicit scalarization function to aggregate preferences, decision-makers must choose among Pareto-efficient policies without clear guidance on how to weigh the trade-offs, which is often critical in real-world applications where diverse preferences must be aggregated into a single deployable policy. Additionally, the Pareto front is typically large, making it prohibitively expensive to compute in practice \cite{hayes_practical_2022}. 

A more refined notion of portfolio similar to the one we study is the concept of convex coverage sets. However, this concept only assumes weighted linear combinations of reward functions \cite{roijers13}. Our approach unifies diverse perspectives on $p$-means-based scalarization, bridging gaps left by existing methods that rely on a single fixed social welfare function or linear reward combinations. 

\textbf{Portfolios in Optimization.}
In the optimization literature, \cite{drygala2024data} studied portfolios with stochastic guarantees for fixed objectives. Recent work in approximation algorithms also explores the notions of small-sized portfolios that approximately optimize a class of social welfare functions \cite{gupta2024a, gupta2024b, goel2006simultaneous, golovin2008all,chakrabarty2019approximation}, by exploiting the combinatorial properties of facility location, scheduling and set cover problems. While these works operate in well-structured combinatorial settings, our work addresses the significantly more complex landscape of RL where the policy space is vast and computing optimal policies is inherently challenging. This necessitates novel algorithmic and theoretical advancements to efficiently construct portfolios with strong guarantees.