@InProceedings{Chakraborty24,
  title = 	 {{M}ax{M}in-{RLHF}: Alignment with Diverse Human Preferences},
  author =       {Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Manocha, Dinesh and Huang, Furong and Bedi, Amrit and Wang, Mengdi},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {6116--6135},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/chakraborty24b/chakraborty24b.pdf},
  url = 	 {https://proceedings.mlr.press/v235/chakraborty24b.html},
  abstract = 	 {Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, the single reward model overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. Next, we propose to learn a mixture of reward models via an expectation-maximization algorithm and solve a MaxMin alignment objective inspired by the Egalitarian principle in social choice theory to better honor diverse human preferences. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language (with Tulu2-7B)) and show the efficacy of the proposed approach in the presence of diversity among human preferences. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.}
}

@inproceedings{Yu24,
	address = {Cham},
	author = {Yu, Guanbao and Siddique, Umer and Weng, Paul},
	booktitle = {Autonomous Agents and Multiagent Systems. Best and Visionary Papers},
	editor = {Amigoni, Francesco and Sinha, Arunesh},
	pages = {3--29},
	publisher = {Springer Nature Switzerland},
	title = {Fair Deep Reinforcement Learning with Generalized Gini Welfare Functions},
	year = {2024}}

@inproceedings{chakrabarty2019approximation,
  title={Approximation algorithms for minimum norm and ordered optimization problems},
  author={Chakrabarty, Deeparnab and Swamy, Chaitanya},
  booktitle={Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing},
  pages={126--137},
  year={2019}
}

@article{cousins2024welfare,
    title={On Welfare-Centric Fair Reinforcement Learning},
    author={Cousins, Cyrus and Asadi, Kavosh and Lobo, Elita and Littman, Michael},
    journal={Reinforcement Learning Journal},
    volume={3},
    pages={1124--1137},
    year={2024}
}

@article{drygala2024data,
  title={Data-Driven Solution Portfolios},
  author={Drygala, Marina and Lattanzi, Silvio and Maggiori, Andreas and Stouras, Miltiadis and Svensson, Ola and Vassilvitskii, Sergei},
  journal={arXiv preprint arXiv:2412.00717},
  year={2024}
}

@inproceedings{fan23,
author = {Fan, Ziming and Peng, Nianli and Tian, Muhang and Fain, Brandon},
title = {Welfare and Fairness in Multi-objective Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimentally that our approach outperforms techniques based on linear scalarization, mixtures of optimal linear scalarizations, or stationary action selection for the Nash Social Welfare Objective.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1991–1999},
numpages = {9},
keywords = {algorithmic fairness, multi-objective reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{goel2006simultaneous,
  title={Simultaneous optimization via approximate majorization for concave profits or convex costs},
  author={Goel, Ashish and Meyerson, Adam},
  journal={Algorithmica},
  volume={44},
  pages={301--323},
  year={2006},
  publisher={Springer}
}

@inproceedings{golovin2008all,
  title={All-norms and all-l\_p-norms approximation algorithms},
  author={Golovin, Daniel and Gupta, Anupam and Kumar, Amit and Tangwongsan, Kanat},
  booktitle={IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (2008)},
  year={2008},
  organization={Schloss-Dagstuhl-Leibniz Zentrum f{\"u}r Informatik}
}

@inproceedings{gupta2024a,
    author = {Gupta, Swati and Moondra, Jai and Singh, Mohit},
    title = {Which Lp norm is the fairest? {A}pproximations for fair facility location across all "p"},
    year = {2023},
    isbn = {9798400701047},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3580507.3597664},
    doi = {10.1145/3580507.3597664},
    abstract = {Given a set of facilities and clients, and costs to open facilities, the classic facility location problem seeks to open a set of facilities and assign each client to one open facility to minimize the cost of opening the chosen facilities and the total distance of the clients to their assigned open facilities. Such an objective may induce an unequal cost over certain socioeconomic groups of clients (i.e., total distance traveled by clients in such a group). This is important when planning the location of socially relevant facilities such as emergency rooms.},
    booktitle = {Proceedings of the 24th ACM Conference on Economics and Computation},
    pages = {817},
    numpages = {1},
    location = {London, United Kingdom},
    series = {EC '23}
}

@inproceedings{gupta2024b,
    author = {Swati Gupta and Jai Moondra and Mohit Singh},
    title = {Balancing Notions of Equity: Trade-offs Between Fair Portfolio Sizes and Achievable Guarantees},
    booktitle = {Proceedings of the 2025 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)},
    year = 2025,
    URL = {https://arxiv.org/abs/2311.03230},
    eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611978322.33}
}

@article{hayes_practical_2022,
	author = {Hayes, Conor F. and R{\u a}dulescu, Roxana and Bargiacchi, Eugenio and K{\"a}llstr{\"o}m, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa M. and Dazeley, Richard and Heintz, Fredrik and Howley, Enda and Irissappane, Athirai A. and Mannion, Patrick and Now{\'e}, Ann and Ramos, Gabriel and Restelli, Marcello and Vamplew, Peter and Roijers, Diederik M.},
	journal = {Autonomous Agents and Multi-Agent Systems},
	month = apr,
	number = {1},
	pages = {26},
	title = {A practical guide to multi-objective reinforcement learning and planning},
	volume = {36},
	year = {2022}}

@misc{mandal2023sociallyfairreinforcementlearning,
      title={Socially Fair Reinforcement Learning}, 
      author={Debmalya Mandal and Jiarui Gan},
      year={2023},
      eprint={2208.12584},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.12584}, 
}

@article{moff14,
author = {Van Moffaert, Kristof and Now\'{e}, Ann},
title = {Multi-objective reinforcement learning using sets of pareto dominating policies},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Many real-world problems involve the optimization of multiple, possibly conflicting objectives. Multi-objective reinforcement learning (MORL) is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, in essence, one for each objective. MORL is the process of learning policies that optimize multiple criteria simultaneously. In this paper, we present a novel temporal difference learning algorithm that integrates the Pareto dominance relation into a reinforcement learning approach. This algorithm is a multi-policy algorithm that learns a set of Pareto dominating policies in a single run. We name this algorithm Pareto Q-learning and it is applicable in episodic environments with deterministic as well as stochastic transition functions. A crucial aspect of Pareto Q-learning is the updating mechanism that bootstraps sets of Q-vectors. One of our main contributions in this paper is a mechanism that separates the expected immediate reward vector from the set of expected future discounted reward vectors. This decomposition allows us to update the sets and to exploit the learned policies consistently throughout the state space. To balance exploration and exploitation during learning, we also propose three set evaluation mechanisms. These three mechanisms evaluate the sets of vectors to accommodate for standard action selection strategies, such as ε-greedy. More precisely, these mechanisms use multi-objective evaluation principles such as the hypervolume measure, the cardinality indicator and the Pareto dominance relation to select the most promising actions. We experimentally validate the algorithm on multiple environments with two and three objectives and we demonstrate that Pareto Q-learning outperforms current state-of-the-art MORL algorithms with respect to the hypervolume of the obtained policies. We note that (1) Pareto Q-learning is able to learn the entire Pareto front under the usual assumption that each state-action pair is sufficiently sampled, while (2) not being biased by the shape of the Pareto front. Furthermore, (3) the set evaluation mechanisms provide indicative measures for local action selection and (4) the learned policies can be retrieved throughout the state and action space.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3483–3512},
numpages = {30},
keywords = {reinforcement learning, multiple criteria analysis, multi-objective, hypervolume, Pareto sets}
}

@INPROCEEDINGS{parisi14,
  author={Parisi, Simone and Pirotta, Matteo and Smacchia, Nicola and Bascetta, Luca and Restelli, Marcello},
  booktitle={2014 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Policy gradient approaches for multi-objective sequential decision making}, 
  year={2014},
  volume={},
  number={},
  pages={2323-2330},
  keywords={Optimization;Approximation methods;Vectors;Approximation algorithms;Water resources;Search problems;Covariance matrices},
  doi={10.1109/IJCNN.2014.6889738}}

@misc{park2024rlhf,
      title={{RLHF} from Heterogeneous Feedback via Personalization and Preference Aggregation}, 
      author={Chanwoo Park and Mingyang Liu and Dingwen Kong and Kaiqing Zhang and Asuman Ozdaglar},
      year={2024},
      eprint={2405.00254},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.00254}, 
}

@article{radulescu_multi-objective_2019,
	author = {R{\u a}dulescu, Roxana and Mannion, Patrick and Roijers, Diederik M. and Now{\'e}, Ann},
	journal = {Autonomous Agents and Multi-Agent Systems},
	month = dec,
	number = {1},
	pages = {10},
	title = {Multi-objective multi-agent decision making: a utility-based analysis and survey},
	volume = {34},
	year = {2019}}

@article{roijers13,
author = {Roijers, Diederik M. and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard},
title = {A survey of multi-objective sequential decision-making},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. This article surveys algorithms designed for sequential decision-making problems with multiple objectives. Though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. Therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. Furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. We show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we survey the literature on multi-objective methods for planning and learning. Finally, we discuss key applications of such methods and outline opportunities for future work.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {67–113},
numpages = {47}
}

@misc{verma2024,
      title={Balancing Act: {P}rioritization Strategies for {LLM}-Designed Restless Bandit Rewards}, 
      author={Shresth Verma and Niclas Boehmer and Lingkai Kong and Milind Tambe},
      year={2024},
      eprint={2408.12112},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.12112}, 
}

@misc{zhong2024rlhf,
      title={Provable Multi-Party Reinforcement Learning with Diverse Human Feedback}, 
      author={Huiying Zhong and Zhun Deng and Weijie J. Su and Zhiwei Steven Wu and Linjun Zhang},
      year={2024},
      eprint={2403.05006},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.05006}, 
}

