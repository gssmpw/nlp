@misc{verma2024,
      title={Balancing Act: {P}rioritization Strategies for {LLM}-Designed Restless Bandit Rewards}, 
      author={Shresth Verma and Niclas Boehmer and Lingkai Kong and Milind Tambe},
      year={2024},
      eprint={2408.12112},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.12112}, 
}


@article{papadimitriou1999complexity,
  title={The complexity of optimal queuing network control},
  author={Papadimitriou, Christos H and Tsitsiklis, John N},
  journal={Mathematics of Operations Research},
  volume={24},
  number={2},
  pages={293--305},
  year={1999},
  publisher={INFORMS}
}

@article{behari2024decision,
  title={A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health},
  author={Behari, Nikhil and Zhang, Edwin and Zhao, Yunfan and Taneja, Aparna and Nagaraj, Dheeraj and Tambe, Milind},
  journal={arXiv preprint arXiv:2402.14807},
  year={2024}
}

@inproceedings{golovin2008all,
  title={All-norms and all-l\_p-norms approximation algorithms},
  author={Golovin, Daniel and Gupta, Anupam and Kumar, Amit and Tangwongsan, Kanat},
  booktitle={IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (2008)},
  year={2008},
  organization={Schloss-Dagstuhl-Leibniz Zentrum f{\"u}r Informatik}
}

@article{kumar2009unified,
  title={A unified approach to scheduling on unrelated parallel machines},
  author={Kumar, VS Anil and Marathe, Madhav V and Parthasarathy, Srinivasan and Srinivasan, Aravind},
  journal={Journal of the ACM (JACM)},
  volume={56},
  number={5},
  pages={1--31},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@article{azar2004all,
  title={All-norm approximation algorithms},
  author={Azar, Yossi and Epstein, Leah and Richter, Yossi and Woeginger, Gerhard J},
  journal={Journal of Algorithms},
  volume={52},
  number={2},
  pages={120--133},
  year={2004},
  publisher={Elsevier}
}

@inproceedings{chakrabarty2019simpler,
  title={Simpler and better algorithms for minimum-norm load balancing},
  author={Chakrabarty, Deeparnab and Swamy, Chaitanya},
  booktitle={27th Annual European Symposium on Algorithms (ESA 2019)},
  year={2019},
  organization={Schloss-Dagstuhl-Leibniz Zentrum f{\"u}r Informatik}
}

@inproceedings{chakrabarty2019approximation,
  title={Approximation algorithms for minimum norm and ordered optimization problems},
  author={Chakrabarty, Deeparnab and Swamy, Chaitanya},
  booktitle={Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing},
  pages={126--137},
  year={2019}
}


@inproceedings{goel2017metric,
  title={Metric distortion of social choice rules: Lower bounds and fairness properties},
  author={Goel, Ashish and Krishnaswamy, Anilesh K and Munagala, Kamesh},
  booktitle={Proceedings of the 2017 ACM Conference on Economics and Computation},
  pages={287--304},
  year={2017}
}

@article{verma2023expanding,
  title={Expanding impact of mobile health programs: SAHELI for maternal and child care},
  author={Verma, Shresth and Singh, Gargi and Mate, Aditya and Verma, Paritosh and Gorantla, Sruthi and Madhiwalla, Neha and Hegde, Aparna and Thakkar, Divy and Jain, Manish and Tambe, Milind and others},
  journal={AI Magazine},
  volume={44},
  number={4},
  pages={363--376},
  year={2023},
  publisher={Wiley Online Library}
}

@article{goel2006simultaneous,
  title={Simultaneous optimization via approximate majorization for concave profits or convex costs},
  author={Goel, Ashish and Meyerson, Adam},
  journal={Algorithmica},
  volume={44},
  pages={301--323},
  year={2006},
  publisher={Springer}
}

@article{fan2022welfare,
  title={Welfare and fairness in multi-objective reinforcement learning},
  author={Fan, Zimeng and Peng, Nianli and Tian, Muhang and Fain, Brandon},
  journal={arXiv preprint arXiv:2212.01382},
  year={2022}
}
@article{dietterich2000hierarchical,
  title={Hierarchical reinforcement learning with the MAXQ value function decomposition},
  author={Dietterich, Thomas G},
  journal={Journal of artificial intelligence research},
  volume={13},
  pages={227--303},
  year={2000}
}
@misc{park2024rlhf,
      title={{RLHF} from Heterogeneous Feedback via Personalization and Preference Aggregation}, 
      author={Chanwoo Park and Mingyang Liu and Dingwen Kong and Kaiqing Zhang and Asuman Ozdaglar},
      year={2024},
      eprint={2405.00254},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.00254}, 
}

@misc{zhong2024rlhf,
      title={Provable Multi-Party Reinforcement Learning with Diverse Human Feedback}, 
      author={Huiying Zhong and Zhun Deng and Weijie J. Su and Zhiwei Steven Wu and Linjun Zhang},
      year={2024},
      eprint={2403.05006},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.05006}, 
}







@inproceedings{gupta2024b,
    author = {Swati Gupta and Jai Moondra and Mohit Singh},
    title = {Balancing Notions of Equity: Trade-offs Between Fair Portfolio Sizes and Achievable Guarantees},
    booktitle = {Proceedings of the 2025 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)},
    year = 2025,
    URL = {https://arxiv.org/abs/2311.03230},
    eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611978322.33}
}

@inproceedings{gupta2024a,
    author = {Gupta, Swati and Moondra, Jai and Singh, Mohit},
    title = {Which Lp norm is the fairest? {A}pproximations for fair facility location across all "p"},
    year = {2023},
    isbn = {9798400701047},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3580507.3597664},
    doi = {10.1145/3580507.3597664},
    abstract = {Given a set of facilities and clients, and costs to open facilities, the classic facility location problem seeks to open a set of facilities and assign each client to one open facility to minimize the cost of opening the chosen facilities and the total distance of the clients to their assigned open facilities. Such an objective may induce an unequal cost over certain socioeconomic groups of clients (i.e., total distance traveled by clients in such a group). This is important when planning the location of socially relevant facilities such as emergency rooms.},
    booktitle = {Proceedings of the 24th ACM Conference on Economics and Computation},
    pages = {817},
    numpages = {1},
    location = {London, United Kingdom},
    series = {EC '23}
}



@article{roijers13,
author = {Roijers, Diederik M. and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard},
title = {A survey of multi-objective sequential decision-making},
year = {2013},
issue_date = {October 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {48},
number = {1},
issn = {1076-9757},
abstract = {Sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. This article surveys algorithms designed for sequential decision-making problems with multiple objectives. Though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. Therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. Furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. We show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we survey the literature on multi-objective methods for planning and learning. Finally, we discuss key applications of such methods and outline opportunities for future work.},
journal = {J. Artif. Int. Res.},
month = oct,
pages = {67â€“113},
numpages = {47}
}

@misc{mandal2023sociallyfairreinforcementlearning,
      title={Socially Fair Reinforcement Learning}, 
      author={Debmalya Mandal and Jiarui Gan},
      year={2023},
      eprint={2208.12584},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.12584}, 
}


@article{
satija2023group,
title={Group Fairness in Reinforcement Learning},
author={Harsh Satija and Alessandro Lazaric and Matteo Pirotta and Joelle Pineau},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=JkIH4MeOc3},
note={}
}


@InProceedings{jabbari17a,
  title = 	 {Fairness in Reinforcement Learning},
  author =       {Shahin Jabbari and Matthew Joseph and Michael Kearns and Jamie Morgenstern and Aaron Roth},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1617--1626},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/jabbari17a/jabbari17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/jabbari17a.html},
  abstract = 	 {We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.}
}


@article{cousins2024welfare,
    title={On Welfare-Centric Fair Reinforcement Learning},
    author={Cousins, Cyrus and Asadi, Kavosh and Lobo, Elita and Littman, Michael},
    journal={Reinforcement Learning Journal},
    volume={3},
    pages={1124--1137},
    year={2024}
}

@inproceedings{
ju2024achieving,
title={Achieving Fairness in Multi-Agent {MDP} Using Reinforcement Learning},
author={Peizhong Ju and Arnob Ghosh and Ness Shroff},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=yoVq2BGQdP}
}

@inproceedings{agarwal22,
author = {Agarwal, Mridul and Aggarwal, Vaneet and Lan, Tian},
title = {Multi-Objective Reinforcement Learning with Non-Linear Scalarization},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-Objective Reinforcement Learning (MORL) setup naturally arises in many places where an agent optimizes multiple objectives. We consider the problem of MORL where multiple objectives are combined using a non-linear scalarization. We combine the vector objectives with a concave scalarization function and maximize this scalar objective. To work with the non-linear scalarization, in this paper, we propose a solution using steady-state occupancy measures and long-term average rewards. We show that when the scalarization function is element-wise increasing, the optimal policy for the scalarization is also Pareto optimal. To maximize the scalarized objective, we propose a model-based posterior sampling algorithm. Using a novel Bellman error analysis for infinite horizon MDPs based proof, we show that the proposed algorithm obtains a regret bound of ~O(LKDSâˆšA/T) for K objectives, and L-Lipschitz continous scalarization function for MDP with S states, A actions, and diameter D. Additionally, we propose policy-gradient and actor-critic algorithms for MORL. For the policy gradient actor, we obtain the gradient using chain rule, and we learn different critics for each of the K objectives. Finally, we implement our algorithms on multiple environments including deep-sea treasure, and network scheduling setups to demonstrate that the proposed algorithms can optimize non-linear scalarization of multiple objectives.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {9â€“17},
numpages = {9},
keywords = {actor-critic algorithms, multi-objective reinforcement learning, regret analysis, reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{
alamdari2024policy,
title={Policy Aggregation},
author={Parand A. Alamdari and Soroush Ebadian and Ariel D. Procaccia},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=ybiUVIxJth}
}


@inproceedings{Yu24,
	address = {Cham},
	author = {Yu, Guanbao and Siddique, Umer and Weng, Paul},
	booktitle = {Autonomous Agents and Multiagent Systems. Best and Visionary Papers},
	editor = {Amigoni, Francesco and Sinha, Arunesh},
	pages = {3--29},
	publisher = {Springer Nature Switzerland},
	title = {Fair Deep Reinforcement Learning with Generalized Gini Welfare Functions},
	year = {2024}}




@article{hayes_practical_2022,
	author = {Hayes, Conor F. and R{\u a}dulescu, Roxana and Bargiacchi, Eugenio and K{\"a}llstr{\"o}m, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa M. and Dazeley, Richard and Heintz, Fredrik and Howley, Enda and Irissappane, Athirai A. and Mannion, Patrick and Now{\'e}, Ann and Ramos, Gabriel and Restelli, Marcello and Vamplew, Peter and Roijers, Diederik M.},
	journal = {Autonomous Agents and Multi-Agent Systems},
	month = apr,
	number = {1},
	pages = {26},
	title = {A practical guide to multi-objective reinforcement learning and planning},
	volume = {36},
	year = {2022}}

@inproceedings{fan23,
author = {Fan, Ziming and Peng, Nianli and Tian, Muhang and Fain, Brandon},
title = {Welfare and Fairness in Multi-objective Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimentally that our approach outperforms techniques based on linear scalarization, mixtures of optimal linear scalarizations, or stationary action selection for the Nash Social Welfare Objective.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1991â€“1999},
numpages = {9},
keywords = {algorithmic fairness, multi-objective reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}


@article{radulescu_multi-objective_2019,
	author = {R{\u a}dulescu, Roxana and Mannion, Patrick and Roijers, Diederik M. and Now{\'e}, Ann},
	journal = {Autonomous Agents and Multi-Agent Systems},
	month = dec,
	number = {1},
	pages = {10},
	title = {Multi-objective multi-agent decision making: a utility-based analysis and survey},
	volume = {34},
	year = {2019}}


@article{moff14,
author = {Van Moffaert, Kristof and Now\'{e}, Ann},
title = {Multi-objective reinforcement learning using sets of pareto dominating policies},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Many real-world problems involve the optimization of multiple, possibly conflicting objectives. Multi-objective reinforcement learning (MORL) is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, in essence, one for each objective. MORL is the process of learning policies that optimize multiple criteria simultaneously. In this paper, we present a novel temporal difference learning algorithm that integrates the Pareto dominance relation into a reinforcement learning approach. This algorithm is a multi-policy algorithm that learns a set of Pareto dominating policies in a single run. We name this algorithm Pareto Q-learning and it is applicable in episodic environments with deterministic as well as stochastic transition functions. A crucial aspect of Pareto Q-learning is the updating mechanism that bootstraps sets of Q-vectors. One of our main contributions in this paper is a mechanism that separates the expected immediate reward vector from the set of expected future discounted reward vectors. This decomposition allows us to update the sets and to exploit the learned policies consistently throughout the state space. To balance exploration and exploitation during learning, we also propose three set evaluation mechanisms. These three mechanisms evaluate the sets of vectors to accommodate for standard action selection strategies, such as Îµ-greedy. More precisely, these mechanisms use multi-objective evaluation principles such as the hypervolume measure, the cardinality indicator and the Pareto dominance relation to select the most promising actions. We experimentally validate the algorithm on multiple environments with two and three objectives and we demonstrate that Pareto Q-learning outperforms current state-of-the-art MORL algorithms with respect to the hypervolume of the obtained policies. We note that (1) Pareto Q-learning is able to learn the entire Pareto front under the usual assumption that each state-action pair is sufficiently sampled, while (2) not being biased by the shape of the Pareto front. Furthermore, (3) the set evaluation mechanisms provide indicative measures for local action selection and (4) the learned policies can be retrieved throughout the state and action space.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3483â€“3512},
numpages = {30},
keywords = {reinforcement learning, multiple criteria analysis, multi-objective, hypervolume, Pareto sets}
}

@INPROCEEDINGS{parisi14,
  author={Parisi, Simone and Pirotta, Matteo and Smacchia, Nicola and Bascetta, Luca and Restelli, Marcello},
  booktitle={2014 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Policy gradient approaches for multi-objective sequential decision making}, 
  year={2014},
  volume={},
  number={},
  pages={2323-2330},
  keywords={Optimization;Approximation methods;Vectors;Approximation algorithms;Water resources;Search problems;Covariance matrices},
  doi={10.1109/IJCNN.2014.6889738}}



@inproceedings{perez09,
author = {Perez, Julien and Germain-Renaud, C\'{e}cile and K\'{e}gl, Bal\'{a}zs and Loomis, Charles},
title = {Responsive elastic computing},
year = {2009},
isbn = {9781605585789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1555301.1555311},
doi = {10.1145/1555301.1555311},
abstract = {Two production models are candidates for e-science computing: grids enable hardware and software sharing; clouds propose dynamic resource provisioning (elastic computing). Organized sharing is a fundamental requirement for large scientific collaborations; responsiveness, the ability to provide good response time, is a fundamental requirement for seamless integration of the large scale computing resources into everyday use. This paper focuses on a model-free resource provisioning strategy supporting both scenarios. The provisioning problem is modeled as a continuous action-state space, multi-objective reinforcement learning problem, under realistic hypotheses; the high level goals of users, administrators, and shareholders are captured through simple utility functions. We propose an implementation of this reinforcement learning framework, including an approximation of the value function through an Echo State Network, and we validate it on a real dataset.},
booktitle = {Proceedings of the 6th International Conference Industry Session on Grids Meets Autonomic Computing},
pages = {55â€“64},
numpages = {10},
keywords = {scheduling, reinforcement learning, grid},
location = {Barcelona, Spain},
series = {GMAC '09}
}


@ARTICLE{hao23,
  author={Hao, Hao and Xu, Changqiao and Zhang, Wei and Yang, Shujie and Muntean, Gabriel-Miro},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Computing Offloading With Fairness Guarantee: A Deep Reinforcement Learning Method}, 
  year={2023},
  volume={33},
  number={10},
  pages={6117-6130},
  keywords={Optimization;Cloud computing;Delays;Computational modeling;Reinforcement learning;Deep learning;Markov processes;Mobile edge computing;computing offloading;deep reinforcement learning;fairness guarantee},
  doi={10.1109/TCSVT.2023.3255229}}

@INPROCEEDINGS{chen21,
  author={Chen, Jingdi and Wang, Yimeng and Lan, Tian},
  booktitle={IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}, 
  title={Bringing Fairness to Actor-Critic Reinforcement Learning for Network Utility Optimization}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  keywords={Wireless communication;Training;Shape;Heuristic algorithms;Decision making;Reinforcement learning;Scheduling},
  doi={10.1109/INFOCOM42981.2021.9488823}}


@ARTICLE{wu18,
  author={Wu, Qingqing and Zeng, Yong and Zhang, Rui},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Joint Trajectory and Communication Design for Multi-UAV Enabled Wireless Networks}, 
  year={2018},
  volume={17},
  number={3},
  pages={2109-2121},
  keywords={Trajectory;Throughput;Wireless networks;Unmanned aerial vehicles;Optimization;Delays;UAV communications;throughput maximization;optimization;trajectory design;mobility control},
  doi={10.1109/TWC.2017.2789293}}



@InProceedings{Chakraborty24,
  title = 	 {{M}ax{M}in-{RLHF}: Alignment with Diverse Human Preferences},
  author =       {Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Manocha, Dinesh and Huang, Furong and Bedi, Amrit and Wang, Mengdi},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {6116--6135},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/chakraborty24b/chakraborty24b.pdf},
  url = 	 {https://proceedings.mlr.press/v235/chakraborty24b.html},
  abstract = 	 {Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, the single reward model overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. Next, we propose to learn a mixture of reward models via an expectation-maximization algorithm and solve a MaxMin alignment objective inspired by the Egalitarian principle in social choice theory to better honor diverse human preferences. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language (with Tulu2-7B)) and show the efficacy of the proposed approach in the presence of diversity among human preferences. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.}
}

@article{whittle98,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3214163},
 abstract = {We consider a population of n projects which in general continue to evolve whether in operation or not (although by different rules). It is desired to choose the projects in operation at each instant of time so as to maximise the expected rate of reward, under a constraint upon the expected number of projects in operation. The Lagrange multiplier associated with this constraint defines an index which reduces to the Gittins index when projects not being operated are static. If one is constrained to operate m projects exactly then arguments are advanced to support the conjecture that, for m and n large in constant ratio, the policy of operating the m projects of largest current index is nearly optimal. The index is evaluated for some particular projects.},
 author = {P. Whittle},
 journal = {Journal of Applied Probability},
 pages = {287--298},
 publisher = {Applied Probability Trust},
 title = {Restless Bandits: Activity Allocation in a Changing World},
 urldate = {2025-01-20},
 volume = {25},
 year = {1988}
}



@book{Bullen03,
	address = {Dordrecht},
	author = {Bullen, P.S},
	edition = {2nd ed. 2003.},
	publisher = {Springer Netherlands : Imprint: Springer},
	series = {Mathematics and Its Applications ; 560},
	title = {Handbook of Means and Their Inequalities},
	year = {2003}}

@inproceedings{
pardeshi2024learning,
title={Learning Social Welfare Functions},
author={Kanad Shrikar Pardeshi and Itai Shapira and Ariel D. Procaccia and Aarti Singh},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=7O6KtaAr8n}
}


@InProceedings{cousins23,
  title = 	 {Revisiting Fair-PAC Learning and the Axioms of Cardinal Welfare},
  author =       {Cousins, Cyrus},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {6422--6442},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/cousins23a/cousins23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/cousins23a.html},
  abstract = 	 {Cardinal objectives serve as intuitive targets in fair machine learning by summarizing utility (welfare) or disutility (malfare) $u$ over $g$ groups. Under standard axioms, all welfare and malfare functions are $w$-weighted $p$-power-means, i.e. $M_p(u;w) = \sqrt[p]{\sum_{i=1}^g w_i u_i^p}$, with $p \leq 1$ for welfare, or $p \geq 1$ for malfare. We show the same under weaker axioms, and also identify stronger axioms that naturally restrict $p$. It is known that power-mean malfare functions are Lipschitz continuous, and thus statistically easy to estimate or learn. We show that all power means are locally Holder continuous, i.e., $|M(u; w)-M(uâ€™ ; w)| \leq \lambda \parallel u - uâ€™\parallel^\alpha$ for some $\lambda$, $\alpha$,$\parallel \cdot \parallel$. In particular, $\lambda$ and $1/\alpha$ are bounded except as $p \rightarrow 0$ or $\min_i w_i \rightarrow 0$, and via this analysis we bound the sample complexity of optimizing welfare. This yields a novel concept of fair-PAC learning, wherein welfare functions are only polynomially harder to optimize than malfare functions, except when $p \approx 0$ or $\min_i w_i$ $\approx$ 0, which is exponentially harder.}
}


@book{moulin_fair_2003,
	author = {Moulin, Herv{\'e}},
	month = jan,
	publisher = {The MIT Press},
	title = {Fair {Division} and {Collective} {Welfare}},
	year = {2003}}


@article{roberts_interpersonal_1980,
	author = {Roberts, Kevin W. S.},
	journal = {The Review of Economic Studies},
	month = jan,
	number = {2},
	pages = {421--439},
	title = {Interpersonal {Comparability} and {Social} {Choice} {Theory}},
	volume = {47},
	year = {1980}}

@article{drygala2024data,
  title={Data-Driven Solution Portfolios},
  author={Drygala, Marina and Lattanzi, Silvio and Maggiori, Andreas and Stouras, Miltiadis and Svensson, Ola and Vassilvitskii, Sergei},
  journal={arXiv preprint arXiv:2412.00717},
  year={2024}
}

@misc{ARMMAN, author = {ARMMAN}, title = {ARMMAN: Advancing Reduction in Mortality and Morbidity of Mothers, Children, and Neonates}, year = 2024, url = "https://armman.org/" }


@article{Verma2024b,
	author = {Verma, Shresth and Singh, Gargi and Mate, Aditya and Verma, Paritosh and Gorantla, Sruthi and Madhiwalla, Neha and Hegde, Aparna and Thakkar, Divy and Jain, Manish and Tambe, Milind and Taneja, Aparna},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = {Jul.},
	number = {13},
	pages = {15594-15602},
	title = {Increasing Impact of Mobile Health Programs: SAHELI for Maternal and Child Care},
	volume = {37},
	year = {2024}}
