\section{Introduction}

With the development of generative artificial intelligence (GenAI) technologies, 2D or 3D content creation~\citep{rombach2022latentdiff} already witnessed a huge improvement in recent years. Automatic creation of 4D content~\cite{singer2023text} is an emerging research topic in recent years, which has wide applications in various fields such as AR/VR, video generation, and robotics. However, due to the scarcity of 4D datasets, automatically creating 4D content is still a challenging task.

Due to the huge success of 2D diffusion models for image or video generation, most works~\citep{singer2023text,zhao2023animate124,bahmani20244dfy,zheng2024dreamin4d} focus on how to utilize these 2D diffusion models for 4D content creation. These works aim to generate synchronized multiview videos for a 3D object and then reconstruct a dynamic 3D representation such as 3DGS or NeRF from the multiview videos. However, such a pipeline mainly faces the challenge of maintaining spatial consistency, that multiview videos are spatially consistent on the same timestep, and temporal consistency, that each video is consistent across different frames on different timesteps. Early stage works~\citep{singer2023text,zhao2023animate124,bahmani20244dfy,zheng2024dreamin4d,ling2024alignyourgauss} attain both consistencies by utilizing both spatial and temporal Score Distillation Sampling (SDS) losses. They utilize SDS to distill an image diffusion model~\citep{liu2023zero123,rombach2022latentdiff} to create a 3D representation and then animate the 3D representations by distilling a video diffusion model~\citep{blattmann2023stable}. Based on these SDS pipelines, some works~\citep{jiang2023consistent4d,yin20234dgen,pan2024fastdy4d} explicitly generate a video as guidance and design new 4D representations to learn better motion fields. However, 4D contents created by these SDS-based methods suffer from low-quality motions and over-saturated appearances. Some very recent works~\citep{zhang20244diffusion,liang2024diffusion4d,li2024vividzoo,ren2024l4gm,jiang2024animate3d} directly train a temporally consistent multiview diffusion model from 4D data and reconstruct the 4D representation from the generated multiview images. Though these methods achieve high-quality 4D content, they often require a large amount of 4D training data and extensive computation resources for training.


In this paper, we propose a novel 4D generation framework called \methodname based on multiview diffusion models~\citep{li2024era3d} and the token flow technique~\citep{geyer2023tokenflow}. 
The challenge in high-quality 4D generation is to create temporally consistent multiview images from monocular 2D videos. Our core motivation is to first construct a coarse 4D field, temporally consistent in the front view, and refine it to achieve high temporal consistency across all views. Specifically, in the coarse stage, we utilize multiview diffusion and the temporal consistency of the front view to enhance the spatiotemporally consistent 4D field for better rendered 2D flow. In the refinement stage, this rendered 2D flow guides the regeneration of consistent multiview images, culminating in a high-quality 4D field.
As shown in Fig.~\ref{fig:teaser}, given an input video that can be generated from a text- or image-to-video generative model, \methodname generates high-quality 4D contents represented by a dynamic Gaussian field that can be rendered with the splatting technique on arbitrary viewpoints and timesteps. \methodname achieves spatial consistency by applying the multiview diffusion model to generate multiview images on every frame. The multiview diffusion model is trained on a large 3D dataset to preserve the spatial consistency among all the multiview images. Then, these multiview images will be used in the training of an initial dynamic Gaussian field as the coarse 4D field.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/teaser2.pdf}
    \caption{Given an input monocular video containing a foreground dynamic object (left), \methodname generates a 4D video represented by a dynamic 3D Gaussian field (right) by utilizing a multiview diffusion model and a token propagation method to improve both the spatial and temporal consistency. On the right, we also show the colors of these Gaussian spheres and the rendered normal maps besides the rendered RGB images.}
    \label{fig:teaser}
\end{figure}

Since the multiview diffusion model is applied to each frame separately, the generated multiview images on different time steps are not temporal consistent with each other, which causes blurry renderings of the coarse 4D field. To improve the temporal consistency between different timesteps, we further apply token flow to associate the generated images of the same viewpoint but on different timesteps. This is based on the fact that a temporally consistent video should share similar tokens among different frames, which has already been utilized by the Token Merging~\citep{li2024vidtome} or Token Reduction~\citep{geyer2023tokenflow} for video editing. To determine the similar tokens on different frames, we utilize the rendered 2D flows from the coarse 4D field to associate pixels from different frames. Then, we regenerate all the multiview images on all timesteps and force the associated pixels to have similar tokens in the reverse diffusion sampling process. This token flow technique effectively improves the temporal consistency of the regenerated multiview images. Finally, the coarse 4D field is refined by the regenerated images to get a high-quality 4D field.


Our contributions can be summarized as follows: (1) We extend the 2D token flow to multiview diffusion to improve temporal consistency. (2) We design a novel pipeline that interleaves dynamic Gaussian field reconstruction with Multiview Token flow to generate multiview consistent videos. (3) We have shown improved quality in 4D content creation from a monocular video.
We conduct experiments on both the Consistent4D~\citep{jiang2023consistent4d} dataset and a self-collected dataset to validate the effectiveness of our methods. The results demonstrate that our method generates videos with high-fidelity and high-quality motion on unseen views.