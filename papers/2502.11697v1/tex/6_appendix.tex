\appendix

\newpage
\section{Appendix}

\subsection{Additional Implementation Details}

\textbf{Loss functions.} As introduced in Sec.~\ref{sec:recon}, we employed several losses during the training of the dynamic Gaussian field, including rendering loss, mask loss, DSSIM loss, ARAP loss, normal map loss, and 2D flow loss. We denote these loss as $L_r$, $L_m$, $L_{DSSIM}$, $L_{arap}$, $L_n$, and $L_f$, respectively. Thus, our loss function can be expressed as $L = \lambda_r L_r +\lambda_m L_m + \lambda_{DSSIM} L_{DSSIM} +\lambda_{arap} L_{arap} + \lambda_n L_n + \lambda_f L_f$, where $\lambda$ represent hyperparameters. For general cases, we set $\lambda_r$ to 0.8, $\lambda_{DSSIM}$ to 0.2, $\lambda_m$ to 2, and the remaining hyperparameters to 1.

\textbf{Training iterations.} Following~\citep{kerbl20233dgs}, our training consists of a total of 30K iterations. We first use 5K iterations to learn a static 3D Gaussian from multiview images of a keyframe, which serves as the initialization for the dynamic 3D Gaussian representation. Next, we utilize 10K iterations to learn a coarse dynamic 3D Gaussian field from multiview videos. After regenerating the multiview videos with improved quality, we perform 15K iterations to refine and obtain the final dynamic 3D Gaussian field.
The diffusion process requires approximately 30 GB of GPU memory, while the Gaussian field reconstruction utilizes around 10 GB. The comparison of training times with other methods is shown in Table~\ref{tab:time}.
\input{table/time_tab}

\textbf{Sampling strategy for training the dynamic 3D Gaussian field.} During each training step of the dynamic 3D Gaussian field, we sample various timesteps and utilize all corresponding multiview images for training. Specifically, we employ paired flow inputs to compute the flow loss. However, when the intervals between sampled timesteps are large, rapid movements or changes increase the risk of unreliable flow, which is detrimental to training the dynamic 3D Gaussian field. To mitigate this issue, we implemented an imbalanced sampling strategy that increases the probability of sampling adjacent frames simultaneously.


\begin{figure}[b]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/exp_qualitative_view_2.pdf}
    \caption{Additional results on spatial consistency with baseline methods, Consistent4D~\citep{jiang2023consistent4d}, SC4D~\citep{wu2024sc4d}, and STAG4D~\citep{zeng2024stag4d}.}
    \label{fig:compare_view2}
\end{figure}

\subsection{Custom Data Preparation}

We manually select high-quality clips from web-collected video data, ensuring each clip contains fewer than 64 frames. Dynamic objects of interest are segmented using SAM2, after which we apply padding to the segmented objects before inputting them into MVD. The resulting videos typically span approximately 4 to 5 seconds.

\subsection{Additional Results}
\label{sec:addtional-results}
We show additional results on the Consistent4D dataset and a self-collected dataset including the novel-view images and the rendered 2D flow maps in Fig.~\ref{fig:add_in} and Fig.~\ref{fig:add_out_1}. In addition, results with more viewpoints are presented in Fig.~\ref{fig:compare_view2} and Fig.~\ref{fig:add_out_2}. For the self-collected dataset, we use videos collected from the Internet. 





\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/add_in.pdf}
    \caption{Additional results with flow map of samples in Consistent4D.}
    \label{fig:add_in}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/add_out.pdf}
    \caption{Additional results with flow maps of samples collected on the Internet.}
    \label{fig:add_out_1}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/add_out_2.pdf}
    \caption{Additional results with more viewpoints of samples collected on the Internet.}
    \label{fig:add_out_2}
\end{figure}

