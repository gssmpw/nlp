\section{Method}

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{fig/pipeline4.pdf}
    \caption{\textbf{Overview}. Given an input video that can be generated by video diffusion models, we first apply the Era3D~\cite{li2024era3d} to generate the multiview-consistent images and normal maps for each timestep. Then, we reconstruct a coarse dynamic 3D Gaussian field field from the generated multiview images. After that, we use the coarse dynamic 3D Gaussian field to render 2D flows to guide the re-generation of the multiview images of Era3D, which greatly improves the temporal consistency and image quality. Finally, the regenerated images are used in the refinement of our dynamic 3D Gaussian field to improve the quality.}
    \label{fig:overview}
\end{figure}

Given a monocular video with a dynamic foreground object, our target is to reconstruct the dynamic 3D field represented by a static 3D Gaussian field (3DGS)~\citep{kerbl20233dgs} and a time-dependent deformation field to deform the 3D Gaussian field to a specific timestep. Note that the video can be either provided or generated from a text description or an image by video diffusion models.
Fig.~\ref{fig:overview} illustrates the outline of our pipeline with 3 steps.
% The following section outlines our pipeline. 
First, we run a pretrained multiview diffusion model Era3D~\citep{li2024era3d} to generate multiview videos on predefined viewpoints (Sec.~\ref{sec:mv-init}). 
Then, we reconstruct a coarse dynamic 3D field from the generated multiview videos in Sec.~\ref{sec:recon}, where we introduce the representation and training process.
Finally, we regenerate the multiview video by combining the pretrained multiview diffusion model with the token flow with the rendered 2D flow from our coarse dynamic 3D field (Sec.~\ref{sec:mv-re}). The regenerated images are used in the refinement of our coarse 4D field to an improved 4D field with better quality and consistency.


\subsection{Multiview Video Generation}
\label{sec:mv-init}

In this stage, we introduce the pretrained multiview diffusion model Era3D~\citep{li2024era3d} to generate multi-view videos from the input monocular video, which will be used in the supervision of dynamic 3D Gaussian reconstruction. 

\textbf{Multiview diffusion model}. Given a reference video, we split it into $N$ frames $I_{\text{ref}}^{(n)}\in \mathcal{R}^{H\times W\times 3}$ with $n=1,...,T$. For every frame, we apply the multiview diffusion model Era3D~\citep{li2024era3d} to generate multiple novel view images $I^{(n,k)}$ where $k=1,...,K$ is an index of the viewpoint. Note that combining all the generated frames on the same viewpoint but different time steps leads to $K$ videos on different viewpoints. Era3D not only estimates the RGB images but also predicts normal maps on these viewpoints. We use the same symbols to denote both the normal maps and RGB images.


\textbf{Discussion about temporal inconsistency}. Simply applying the multiview diffusion model to every frame maintains the consistency between all images of different viewpoints $I^{(n,1:K)}:=\{I^{(n,k)}|k=1,...,K\}$ but loses coherence between images $I^{(n_0,k)}$ and $I^{(n_1,k)}$ from two arbitrary different timesteps $n_0$ and $n_1$. The reason is that the multiview diffusion model can only maintain the consistency between different viewpoints but the independent generation on different timesteps leads to temporal inconsistency. Thus, the key problem is to improve the temporal consistency of the generated videos. An important observation is that for two frames of a plausible video, their diffusion features or so-called tokens share a strong similarity and are correlated by the 2D flow between them~\citep{geyer2023tokenflow}. This attribute has been observed by many video editing papers~\citep{geyer2023tokenflow,li2024vidtome} to design token merging~\citep{li2024vidtome} and token flow~\citep{geyer2023tokenflow}. Based on this observation, we improve the temporal consistency by the following two strategies, i.e. enlarged self-attention layers and token propagation with 2D flows.

\textbf{Enlarged self-attention}. As observed by many previous works, enlarging the self-attention of stable diffusion~\citep{rombach2022latentdiff} (SD) to all the images of different timesteps is helpful in improving the temporal consistency. The adopted Era3D model is also based on the SD model so we enlarge the self-attention layers to include all timesteps to improve temporal consistency. Specifically, in each self-attention layer of the image $I^{(n,k)}$, we keep the query features unchanged but adopt all the features from images $I^{(m,k)}$ of the same $k$-th viewpoint but different timesteps $m$ as the keys and values. This enlarged self-attention provides free temporal consistency without retraining the diffusion model.

\textbf{Token propagation with 2D flows}.
To further improve the consistency, for a specific video on a specific viewpoint, we only conduct denoising on several keyframes and then propagate the features (tokens) of keyframes to the rest frames.
Although minor inconsistencies may exist in the keyframes, we can still reconstruct a high-quality 4D field because keyframes are derived from a temporally consistent input video, and the dynamic 3D Gaussian field is supervised by the video, smoothing residual inconsistencies.
Specifically, to conduct denoising on the video $I^{(1:N,k)}_t$ corresponding to the $k$-th viewpoint to get $I^{(1:N,k)}_{t-1}$, we first sample $M$ equidistant keyframes $\{I^{(n_m,k)}_t|m=1,2,...,M\}$ and we conduct the normal denoising process on all these keyframes to get their denosied $\{I^{(n_m,k)}_{t-1}\}$. We obtain the self-attention features $F^{(n_m,k)}_t$ of all these keyframes. Then, for the rest frames, we propagate the features of keyframes to denoise them. Specifically, for a specific frame $I_t^{(n,k)}$ with $n_{m-1}\le n \le n_{m}$, we utilize the 2D flows $\pi({n_{m-1}\to n})$ and $\pi(n \to n_{m})$ to warp the features, resulting warped features $F^{(n_{m-1}\to n,k)}_t$ and $F^{(n_{m}\to n , k)}_t$. We compute the features on the  $I_t^{(n,k)}$ by
\begin{equation}
    F^{(n,k)}_t =(1-\lambda_{n}) \cdot F^{(n_{m-1}\to n,k)}_t + \lambda_{n} F^{(n_{m}\to n,k)}_t,
    \label{eq:propagate}
\end{equation}

where $\lambda_n = (n_{m}-n)/(n_{m}-n_{m-1})$ is a position-dependent weighting parameter. Then, we use these propagated features to denoise these intermediate frames between keyframes. Due to the presence of regions in the video that cannot be covered by optical flow, we only propagate features in the early stages $t\le \tau$, allowing the diffusion process to add more details and occluded regions. This token propagation scheme effectively utilizes the redundancy in a video and has the potential to improve the temporal consistency of the generated video. Implementing the token propagation requires the estimation of the 2D flow, which is introduced in the following. 


\textbf{Estimation of 2D flows}. In the beginning, we only have access to the input video but do not have any information on other unseen viewpoints. Thus, we estimate the 2D flow of the input reference video by RAFT~\citep{teed2020raft}. Then, we use this estimated 2D flow to propagate the features of the video on the first viewpoint, i.e. the front view of Era3D, while for other viewpoints, we apply the full denoising process for all diffusion timesteps. Though only one 2D flow is utilized on the front view, the Era3D will utilize cross-viewpoint attention layers to propagate the consistency of the front view to other views and thus improve the temporal consistency. 


\subsection{Reconstruction with Gaussian Splatting}
\label{sec:recon}

Given the generated multiview videos, in this stage, we aim to reconstruct a dynamic 3D Gaussian field. Our method employs a keypoint-controlled dynamic 3D Gaussian representation~\citep{huang2024sc}, comprising a static 3D Gaussian and a time-dependent deformable field. For the static 3D field, we represent the field as a set of 3D Gaussians as proposed in 3DGS~\citep{kerbl20233dgs}. For the deformation field, we adopt the representation from SC-GS~\citep{huang2024sc}, which first generates a set of 3D control points by clustering 3D Gaussians, then applies an MLP network to translate and rotate these control points, and finally deforms the 3D Gaussians with these control points. On each control point, we associate a set of learnable radius parameters of a radial-basis-function (RBF) kernel that controls how the impact of the control point on a
Gaussian will decrease as their distances increase. We train this dynamic Gaussian field using the generated multiview videos. Besides the rendering loss, mask loss, structural dissimilarity (D-SSIM)~\citep{kerbl20233dgs} loss, and as-rigid-as-possible (ARAP) loss, we also adopt a normal map loss and a 2D flow loss.

\textbf{Flow loss}. The flow loss here is to minimize the difference between the rendered 2D flows and the estimated 2D flows on the front view by RAFT~\citep{teed2020raft}. Specifically, for two timesteps, we project the 3D offset of each 3D Gaussian onto to image plane to get the 2D offset of the 3D Gaussian. Then, we combine these 2D offsets with the same alpha blending method as used in splatting to render a 2D flow map. We minimize the difference between the rendered 2D flow map and the estimated 2D flow map with an L1 loss and skip the invisible regions caused by occlusions. 

\textbf{Normal loss}. Since the Era3D model also generates normal maps for every viewpoint, we also supervise the dynamic 3D Gaussian field with these normal maps. However, computing normal maps from 3DGS is ambiguous without a clear definition of the normal directions. Instead, we first render the depth maps by alpha blending the depth values of all 3D Gaussians and then compute a normal map from the rendered depth map. Finally, we minimize the difference between the generated normal maps and the rendered normal maps. These normal maps pose a geometric constraint on the dynamic 3D Gaussian field and improve the rendering quality.


\subsection{Regeneration with 2D flows}
\label{sec:mv-re}
In this section, we regenerate the multiview images by Era3D with the help of the coarse dynamic 3D Gaussian field trained in the previous section. Then, these regenerated images are used in the refinement of the coarse dynamic 3D Gaussian field. The coarse dynamic 3D Gaussian field often produces blurry results because the images generated in Sec.~\ref{sec:mv-init} are not temporally consistent enough. However, we observe that these coarse 3D Gaussian fields already produce a reasonable 3D flow field that could improve the temporal consistency for the multiview generation. Thus, we regenerate all the multiview videos with the help of the 3D flow field and refine the dynamic 3D Gaussian field with the regenerations.


\textbf{Regeneration and refinement}. In Sec.~\ref{sec:mv-init}, we only have access to the 2D flow of the front view to guide the generation of the multiview videos so all these unseen views are not well constrained with temporal consistency. With the coarse dynamic 3D Gaussian field, we render 2D flow maps for all viewpoints and then incorporate the token propagation in the generation process as introduced in Eq.~(\ref{eq:propagate}). By propagating tokens on all viewpoints, we greatly improve the temporal consistency of all generated videos. Then, we utilize the regenerated videos to refine our dynamic 3D Gaussian field, which achieves better rendering quality with less blurry results.




