\section{Related work}

In recent years, diffusion models have gained prominence as a powerful generative framework, excelling in tasks such as image~\citep{rombach2022latentdiff,nichol2021improvedddpm,nichol2021glide,ramesh2022hierarchicaldiff} and video synthesis~\citep{blattmann2023alignlatentvd,an2023latentshiftvd,ge2023preservecovd,guo2023animatediffvd,singer2022makeavideovd}. These models generate data by progressively denoising randomly initialized samples until a coherent structure or scene emerges. Leveraging the flexibility and effectiveness of generative models, they have been adapted to a wide range of tasks~\cite{zheng2023ddcot, tang2023cotdet, shi2024part2object, tang2023temporal, tang2023contrastive}, including 3D and 4D content generation. 
% In this section, we review the related works on diffusion models and their applications in this field.
In this section, we will review three parts: diffusion models, 4D scene representations, and 4D generation with diffusion models.


\paragraph{Diffusion for Generation}
Recently, diffusion models, pre-trained on large-scale datasets~\citep{schuhmann2022laion}, have made significant strides in generating high-quality and diverse visual content for both 2D image and video tasks ~\citep{rombach2022latentdiff,nichol2021improvedddpm,blattmann2023alignlatentvd,an2023latentshiftvd,huang2024free}. Leveraging aligned vision-language representations~\cite{shi2024plain, dai2024curriculum, shi2024devil, shi2023logoprompt, shi2023edadet, shi2022spatial}, these models can produce various forms of visual content with impressive diversity and realism conditioned on text or images. To adapt 2D diffusion models for 3D generation, some methods utilize Score Distillation Sampling Loss~\citep{poole2022dreamfusion,lin2023magic3d,chen2023fantasia3d,wang2024prolificdreamer} to distill 3D priors and train a neural radiance field~\citep{mildenhall2020nerf} for 3D asset creation. However, this approach often faces challenges such as slow training speeds and multi-face artifacts~\citep{shi2023mvdream}. To address these limitations, another strategy involves fine-tuning pre-trained 2D diffusion models to directly generate multi-view consistent images~\citep{shi2023mvdream,liu2023zero123,long2024wonder3d,liu2023syncdreamer,li2024era3d} from large-scale multi-view datasets~\citep{deitke2023objaverse}. These images are then processed through 3D reconstruction algorithms~\citep{wang2021neus,kerbl20233dgs,liu2023nero} to produce high-quality 3D assets. Despite these advancements, efficiently leveraging these techniques for 4D generation, ensuring both spatial and temporal coherence, remains a challenging problem.


\paragraph{4D Scene Representation}
Current 4D scene representations can be broadly categorized into two types based on their underlying 3D scene representation: 1) NeRF-based \citep{mildenhall2020nerf} and 2) 3D Gaussian Splatting (3DGS)-based \citep{kerbl20233dgs}. Both approaches extend static 3D scene representations into the temporal domain by introducing deformable fields or animation-driven training frameworks.
NeRF (Neural Radiance Fields) was initially proposed to encode the geometry and appearance of static scenes using implicit models with MLPs. Building upon this, many works have extended static NeRF to handle dynamic scenes, either by modeling a dynamic deformation field on the top of a canonical static scene representation~\citep{pons2021dnerf,tretschk2021nonrigidnerf,yuan2021stardnerf,park2021nerfies,fang2022fastdnerf} or by directly learning a time-conditioned radiance field~\citep{li2022neural3dvideo,gao2021dynamicviewsynthesis,park2021hypernerf,xian2021spacenerfvideo}. Despite its success, NeRF-based methods often face limitations in training and inference speed, making them less suitable for real-time applications.
Recently, 3D Gaussian Splatting (3DGS) has shown impressive performance due to its efficient training and real-time novel view synthesis capabilities. This method represents static scenes as a set of Gaussian primitives and employs a fast Gaussian differentiable rasterizer with adaptive density control. As an explicit representation, 3DGS also simplifies tasks such as scene editing. 
3DGS then has been applied to model dynamic scenes with the similar idea of building a deformation field~\citep{luiten2023dynamic3dgauss,wu20244dgauss,yang2024deformable4dgauss,zeng2024stag4d,wu2024sc4d}.
For example, Dynamic 3D Gaussians~\citep{luiten2023dynamic3dgauss} enable the Gaussians to move and rotate over time under local rigid constraints. This approach efficiently models fine details and temporal changes, making it highly effective for 4D content creation.
Together, these representations offer a robust framework for generating realistic and temporally coherent dynamic scenes in 4D space, supporting applications such as animation, scene reconstruction, and motion capture.

\paragraph{4D Generation} By efficiently integrating advanced diffusion techniques with 4D scene representations, significant progress has been made toward 4D generation. One approach in this direction leverages Score Distillation Sampling~\citep{poole2022dreamfusion} to distill spatial and temporal prior knowledge from multiple diffusion models into a 4D scene representation, producing spatially and temporally consistent 4D objects, including text-to-video and text-to-image generation. 
A pioneering work, MAV4D~\citep{singer2023mav3d} introduced a multi-stage training pipeline for dynamic scene generation, utilizing a Text-to-Image (T2I) model to initialize static scenes and a Text-to-Video (T2V)~\citep{singer2022makeavideovd} model to handle motion dynamics.
Building on this paradigm, several methods have sought to improve 4D generation quality by incorporating image conditions ~\citep{zhao2023animate124}, hybrid Score Distillation Sampling~\citep{bahmani20244dfy}, strategies that decouple static elements from dynamic ones~\citep{zheng2024dreamin4d}, and related techniques. However, these methods are largely based on NeRF variants, which suffer from issues like over-saturated appearance and long optimization times. To overcome these limitations,  Align-Your-Gaussians~\citep{ling2024alignyourgauss} proposed using dynamic 3D Gaussian Splatting (3DGS)~\citep{kerbl20233dgs} as the underlying 4D scene representation to learn a deformation field~\citep{park2021nerfies,pons2021dnerf}, offering faster training and better real-time capabilities. Despite this, the reliance on SDS loss in these methods leads to slow optimization speeds, limiting their applicability in downstream tasks.
Another approach uses video as guidance. Several video-to-4D frameworks~\citep{jiang2023consistent4d,yin20234dgen,pan2024fastdy4d} have been introduced that use video inputs as references to guide 4D generation. These methods attempt to generate dynamic scenes by leveraging video-driven information for more precise motion dynamics.
Additionally, to ensure multi-view consistency, recent works have focused on retraining multi-view video diffusion models~\citep{zhang20244diffusion,liang2024diffusion4d,li2024vividzoo,ren2024l4gm,jiang2024animate3d} with 4D datasets, integrating both spatial and temporal modules. However, these models often require large amounts of data and are computationally intensive.
