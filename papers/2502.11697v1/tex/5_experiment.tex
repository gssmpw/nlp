\section{Experiment}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/exp_time.pdf}
    \caption{Qualitative comparison on temporal consistency of our method with baseline methods, Consistent4D~\citep{jiang2023consistent4d}, SC4D~\citep{wu2024sc4d}, and STAG4D~\citep{zeng2024stag4d}.}
    \label{fig:exp-time}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/exp_view.pdf}
    \caption{Qualitative comparison on spatial consistency of our method with baseline methods, Consistent4D~\citep{jiang2023consistent4d}, SC4D~\citep{wu2024sc4d}, and STAG4D~\citep{zeng2024stag4d}.}
    \label{fig:exp-view}
\end{figure}
\subsection{Experimental Settings}

\textbf{Implementation details.} For multi-view video generation, we utilize Era3D~\citep{li2024era3d} to generate $K=6$ viewpoints at a resolution of 512x512 for one frame, using 40 denoising steps. We set $\tau =20$, executing token propagation during the denoising process when $t<\tau$. For keyframe selection, we employ a keyframe interval of 8 frames. In the initialization phase of dynamic 3D Gaussian representation, we initialize 512 control points with Farthest Point Sampling (FPS) sampling. Each Gaussian point is influenced by its 3 nearest control points. During the training of the dynamic Gaussian field, we use an initial learning rate of $3\times 10^{-4}$ for the MLP, followed by exponential decay. In the refinement phase of the dynamic 3D Gassuian field with regenerated multiview videos, we reset the learning rate to its initial value and applied the same decay strategy. All experiments are conducted on an NVIDIA A40 GPU. We provide more implementation details in the appendix about the losses and data preparation.

\textbf{Dataset.} For quantitative comparisons and part of qualitative comparisons, we evaluate our method with the dataset from Consistent4D~\citep{jiang2023consistent4d}. The dataset comprises 12 synthetic videos and 12 in-the-wild videos. All videos are monocular, captured by a stationary camera, consisting of 32 frames and lasting approximately 2 seconds. We also collect some videos on the Internet for evaluation.

\textbf{Evaluation metrics.} We evaluate our method from three perspectives: consistency with reference videos, spatial consistency, and temporal consistency. Following Consistent4D~\citep{jiang2023consistent4d}, we utilize PSNR, SSIM, and LPIPS to assess the consistency with reference videos. For multi-view consistency, we employ the CLIP score to measure the semantic similarity of images from different viewpoints. 



\input{table/quantitative_tab}
\subsection{Comparisons} We compare our \methodname with recent available open-source methods, including Consistent4D~\cite{jiang2023consistent4d}, SC4D~\cite{wu2024sc4d} and STAG4D~\cite{zeng2024stag4d}. Experiments are conducted on the Consistent4D dataset and a selection of collected videos. This section analyzes representative qualitative results and quantitative results, with additional visualization available in the supplementary videos. We also provide more results of our method in Sec.~\ref{sec:addtional-results} of appendix.


\begin{figure}[b]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/abl_flow_loss.pdf}
    \caption{Ablation study of the overall architecture. The four parts illustrate (a) Input viewpoint. (b) Our final results. (c) The intermediate outcome from our coarse dynamic 3D field. (d) Result without flow loss.}
    \label{fig:ablation}
\end{figure}
\textbf{Qualitative Comparison on Temporal Consistency.} Fig.~\ref{fig:exp-time} shows the comparison from the perspective of temporal consistency on three samples (\textit{skull, triceratops, and man turning his head}). Consistent4D~\cite{jiang2023consistent4d} is limited by the expressiveness of cascaded DyNeRF~\cite{li2022neural3dvideo}, making it challenging to represent the textures of dynamic objects. As a result, the generated outputs are often blurry and distorted, accompanied by artifacts and color discrepancies. SC4D struggles to model motion; as demonstrated in the example of the \textit{triceratops}, its results for the second and third frames are inconsistent with the reference viewpoint due to the weak temporal consistency by simply distilling video diffusion models. For complex real-world scenarios, such as the example of \textit{man turning his head}, early methods are unable to generate coherent and realistic motions. Though recent work (STAG4D) achieves satisfactory temporal consistency in simple samples, it suffers from significant blurriness and distortion when modeling the man turning his head. This failure may be attributed to their reliance on the temporal consistency provided by the single-view reference video, which hinders their ability to learn the correct 3D temporal variations.

Compared to previous methods, we adopt flow loss in the generation process, decoupling the learning of motion and appearance. Such a strategy allows our \methodname to capture high-quality motions while simultaneously modeling clear and detailed appearances.



\textbf{Qualitative Comparison on Spatial Consistency.} We also perform comparisons with other methods across different viewpoints to demonstrate spatial consistency of our method, as shown in Fig.~\ref{fig:exp-view} with three samples (\textit{skull, dog and dragon}). 
For novel viewpoints generation, both SC4D and STAG4D tend to produce artifacts, resulting in a significant performance drop compared to the input view. In the example of the \textit{dog}, other methods struggle to maintain the pattern of the bag the dog is wearing, often generating blurred or noisy results. This phenomenon becomes even more pronounced with complex inputs (\textit{dragon}), where previous methods fail to generate eyes that correspond with the input view and similar whiskers. Meanwhile, the generations show over-saturated colors and some artifacts with inconsistent colors.

\begin{figure}[t]
\begin{floatrow}

\capbtabbox{

\begin{tabular}{lccc}
\toprule
 & LPIPS $\downarrow$ & CLIP $\uparrow$ & FVD $\downarrow$    \\
\midrule
DG4D & 0.1748 & 0.915 & 856.86  \\
Consistent4D & 0.1729 & 0.865 & 1072.94  \\
SC4D & 0.1659 & 0.915 & 879.66 \\
STAG4D & 0.1506 & 0.885 & 972.73  \\
Ours & \textbf{0.1216} &  \textbf{0.948} & \textbf{846.32}  \\
\bottomrule
\end{tabular}%
}{%
  \caption{Quantitative results of novel view synthesis on Consistent4D synthetic objects with multi-view videos.}%
  \label{tab:exp_novel_view}
}

\ffigbox{
\includegraphics[width=0.4\textwidth]{fig/abl_tokenflow.pdf}
}{
    % \centering
    \caption{Ablation study on token propagation for multiview video generation.}
    \label{fig:abl_mvd}
}

\end{floatrow}
\end{figure}

In comparison, our \methodname outperforms these methods in both fine-grained spatial consistency and generation quality. These advantages result from our utilization of multiview diffusion combined with normal loss and rendered 2D flow maps for all viewpoints, which leads to strong geometry constraints, whereas previous methods rely on SDS loss as the source of multi-view consistency.


\textbf{Quantitative Comparison.} For quantitative comparisons, we evaluate metrics introduced above on Consistent4D dataset and the results are shown in Table~\ref{tab:quantity}. Across all metrics, our method consistently outperforms previous approaches including Consistent4D, SC4D and STAG4D. As mentioned in the metric introduction, these results demonstrate that our method exhibits superior consistency with the reference video and enhanced spatiotemporal coherence. Notably, the improvement in LPIPS, which reveals the perceptual consistency of generated images, is particularly significant. This demonstrates that our method can produce more realistic results, aligning with the detailed and accurate textures showcased in our qualitative analysis.
Furthermore, as shown in Table~\ref{tab:exp_novel_view}, we evaluated our method in terms of novel view video synthesis result on Consistent4D synthetic dataset. Our approach exhibits superior temporal consistency under novel views when compared to recent methods.


\subsection{Ablation Study}


To demonstrate the effectiveness of token propagation in the multiview video generation stage and the overall architecture we proposed, we conducted ablation studies for each aspect.


First, we ablate token propagation for multiview video generation as shown in Fig.~\ref{fig:abl_mvd}. Without token propagation with 2D flows, both the pose and body shape of the \textit{Trump} exhibit significant variations across different time steps, resulting in a video with flickering changes. In contrast, our proposed method effectively constrains the phenomenon of flickering, significantly improving the temporal consistency of multiview video generation.


\begin{figure}[h]
\begin{floatrow}

\capbtabbox{%
  % \begin{tabular}{cc} \hline
  % Author & Title \\ \hline
  % Knuth & The \TeX book \\
  % Lamport & \LaTeX \\ \hline
  % \end{tabular}
  \begin{tabular}{lccc}
    \toprule
     & LPIPS $\downarrow$ &  FVD $\downarrow$  & CLIP $\uparrow$    \\
    \midrule
    w/o Enlarged SA & 0.0425 & 348.36  & 0.881 \\
    w/o Flow & 0.0408 & 341.50 & 0.887 \\
    w/o Noraml  & \textbf{0.0405} & 320.93 & 0.885  \\
    Full & 0.0423 &  \textbf{313.47} & \textbf{0.891}  \\
    \bottomrule
    \end{tabular}%
    % }
}{%
  \caption{Ablation study on different components in the Consistent4D Dataset.}%
    \label{tab:ablation}
}

\ffigbox{
    \includegraphics[width=0.4\textwidth]{fig/limitation2.pdf}
}{
    % \centering
    \caption{Limitation on generating novel views for uncommon viewpoints.}
    \label{fig:limitation}
}

\end{floatrow}
\end{figure}


Then, we remove the flow loss or skip the regeneration and refinement phase, and compare the results with our full model. To analyze the impact on both motion and appearance, we present both rendered RGB images and extracted optical flow at different timesteps in Fig.~\ref{fig:ablation}. Comparing (b) with (d), the flow loss significantly enhances the quality of the extracted optical flow, indicating more reliable 3D temporal variations. While (c) displays a relatively blurry image compared to the other experiments, highlighting the significant improvement in generation quality during the regeneration and refinement phase.

Table~\ref{tab:ablation} presents the quantitative results of our ablation study on the Consistent4D~\cite{jiang2023consistent4d} dataset across three metrics: reference view alignment (LPIPS), temporal consistency (FVD), and multi-view consistency (CLIP). The results demonstrate that flow propagation improves temporal consistency, while the normal loss contributes to enhancing multi-view consistency.




\vspace{-2mm}
\section{Limitations}
Though \methodname succeeds in reconstructing a 4D video from a monocular video in most cases, \methodname is limited by the ability of the multiview diffusion model, i.e. Era3D~\citep{li2024era3d}, which may have difficulty in handling complex objects and uncommon viewpoints, as shown in Fig.~\ref{fig:limitation}. Improvements in multiview diffusion models could alleviate this problem.


\section{Conclusion}

In this paper, we introduce a new pipeline, called \methodname, to generate 4D videos from just a monocular video. The main challenge in 4D content creation is to simultaneously keep the spatial consistency and the temporal consistency in the generations from diffusion models. Our key idea is to adopt the multiview diffusion models to generate multiview consistent images and then apply the 2D flows to guide the generation of images of different frames to improve temporal consistency. We utilize the information redundancy in a coherent video by adopting the 2D flows to reuse tokens from different frames to generate content for a specific frame. This token re-usage greatly improves the coherence and temporal consistency of the generated images from multiview diffusion models. Experiments demonstrate the effectiveness of our design and show improved quality than baseline methods.


\textbf{Acknowledgment: }This work was supported by the National Natural Science Foundation of China(No.62206174) and MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University).