% mnras_template.tex 
%
% LaTeX template for creating an MNRAS paper
%
% v3.3 released April 2024
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.3 April 2024
%   Updated \pubyear to print the current year automatically
% v3.2 July 2023
%	Updated guidance on use of amssymb package
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[fleqn,usenatbib]{mnras}

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}

% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}

% Allow "Thomas van Noord" and "Simon de Laguarde" and alike to be sorted by "N" and "L" etc. in the bibliography.
% Write the name in the bibliography as "\VAN{Noord}{Van}{van} Noord, Thomas"
\DeclareRobustCommand{\VAN}[3]{#2}
\let\VANthebibliography\thebibliography
\def\thebibliography{\DeclareRobustCommand{\VAN}[3]{##3}\VANthebibliography}


%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Avoid using amssymb if newtxmath is enabled, as these packages can cause conflicts. newtxmatch covers the same math symbols while producing a consistent Times New Roman font. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
\usepackage{natbib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[MadVoro: Distributed Voronoi Constructor]{\textsc{MadVoro}: Parallel Construction of Voronoi Diagrams in Distributed Memory Systems}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor

\author[M. Mizrachi et al.]{
Maor Mizrachi,$^{1}$\thanks{E-mail: maormiz@cs.huji.ac.il}
Barak Raveh,$^{1}$
and Elad Steinberg$^{2}$
\\
% List of institutions
$^{1}$School of Computer Science and Engineering, The Hebrew University, 9190401 Jerusalem, Israel\\
$^{2}$Racah Institute of Physics, The Hebrew University, 9190401 Jerusalem, Israel \\
}
% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Prints the current year, for the copyright statements etc. To achieve a fixed year, replace the expression with a number. 
\pubyear{\the\year{}}

\usepackage{algorithm}
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
% For tables
%\usepackage{multirow}
\usepackage{standalone}
\usepackage{graphicx}					%Calls figure environment
\usepackage{float}						%Helps to place figures, tables, etc. 
\usepackage{tikz}						%geometric/algebraic description.
\usetikzlibrary{arrows, shapes, snakes, automata, backgrounds, petri, topaths}
\usepackage{subcaption}
\usepackage{algpseudocode}

%\counterwithout{footnote}{section}

\newcommand{\centers}[1]{\ensuremath{\textrm{centers}\left(#1\right)}}
\newcommand{\conv}[1]{\ensuremath{\textrm{conv}\left(#1\right)}}
\newcommand{\intersect}[1]{\ensuremath{\textrm{intersect}\left(#1\right)}}
\newcommand{\radius}[1]{\ensuremath{\textrm{radius}\left(#1\right)}}
\newcommand{\argmin}[2]{\ensuremath{\textrm{argmin}_{#1}\left(#2\right)}}
\newcommand{\functionimage}[1]{\ensuremath{\textrm{Im}\left(#1\right)}}

% Algorithms
\ProvidesPackage{include/algorithms}

\makeatletter


% updated with editorial comments 8/9/2021
\newcommand*{\algrule}[1][\algorithmicindent]{%
  \hspace*{.2em}% <------------- This is where the rule starts from
  \vrule %height .75\baselineskip depth .25\baselineskip
  \hspace*{\dimexpr#1-.2em-.4pt}%
}

\newcommand{\StatePar}[1]{%
  \State\parbox[t]{\dimexpr\linewidth-\ALG@thistlm}{\strut #1\strut}%
}

\newcount\ALG@printindent@tempcnta
\def\ALG@printindent{%
  \ifnum \theALG@nested > 0% is there anything to print
    \ifx\ALG@text\ALG@x@notext% is this an end group without any text?
      % do nothing
    \else
      \unskip
      % draw a rule for each indent level
      \ALG@printindent@tempcnta=1
      \loop
        \algrule[\csname ALG@ind@\the\ALG@printindent@tempcnta\endcsname]%
        \advance \ALG@printindent@tempcnta 1
        \ifnum \ALG@printindent@tempcnta<\numexpr\theALG@nested+1\relax
      \repeat
        \fi
    \fi
}

\patchcmd{\ALG@doentity}{\noindent\hskip\ALG@tlm}{\ALG@printindent}{}{\errmessage{failed to patch}}

\makeatother

\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother

        \algrenewcommand\algorithmicend{\strut\textbf{end}}
        \algrenewcommand\algorithmicdo{\strut\textbf{do}}
        \algrenewcommand\algorithmicwhile{\strut\textbf{while}}
        \algrenewcommand\algorithmicfor{\strut\textbf{for}}
        \algrenewcommand\algorithmicforall{\strut\textbf{for all}}
        \algrenewcommand\algorithmicloop{\strut\textbf{loop}}
        \algrenewcommand\algorithmicrepeat{\strut\textbf{repeat}}
        \algrenewcommand\algorithmicuntil{\strut\textbf{until}}
        \algrenewcommand\algorithmicprocedure{\strut\textbf{procedure}}
        \algrenewcommand\algorithmicfunction{\strut\textbf{function}}
        \algrenewcommand\algorithmicif{\strut\textbf{if}}
        \algrenewcommand\algorithmicthen{\strut\textbf{then}}
        \algrenewcommand\algorithmicelse{\strut\textbf{else}}
        
        \algrenewcommand\algorithmicrequire{\strut\textbf{Input:}}
        \algrenewcommand\algorithmicensure{\strut\textbf{Output:}}
        
        \let\oldState\State
        \renewcommand{\State}{\oldState\strut}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
Voronoi diagrams are essential geometrical structures with numerous applications, particularly astrophysics-driven finite volume methods. While serial algorithms for constructing these entities are well-established, parallel construction remains challenging. This is especially true in distributed memory systems, where each host manages only a subset of the input points. This process requires redistributing points across hosts and accurately computing the corresponding Voronoi cells. In this paper, we introduce a new distributed construction algorithm, which is implemented in our open-source C++ 3-dimensional Voronoi construction framework. Our approach leverages Delaunay triangulation as an intermediate step, which is then transformed into a Voronoi diagram. We introduce the algorithms we implemented for the precise construction and our load-balancing approach and compare the running time with other state-of-the-art frameworks. \textsc{MadVoro} is a versatile tool that can be applied in various scientific domains, such as mesh decomposition, computational physics, chemistry, and machine learning.  
\end{abstract}

% Select between one and six entries from the list of approved keywords.
% Don't make up new ones.
\begin{keywords}
Voronoi Diagrams -- Delaunay Triangulations -- Parallel Computing -- Distributed Computing -- Computational Geometry
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

\section{Introduction}
The Voronoi diagram is an elementary geometry structure. It is widely used for many purposes across various disciplines, including mathematics, computer science, physical sciences, and even health and social sciences. For example, in computer science, Voronoi diagrams play a crucial role in Lloyd’s algorithm, which underpins methods such as $k$-means clustering. In physics, Voronoi diagrams can be used to design a mesh decomposition in FVM (Finite-Volume Method) simulations, for example, as described in \cite{RICH} and \cite{AREPO}. In chemistry, they can be used to analyze protein structures.

In this paper, we introduce a novel distributed-memory framework for Voronoi diagram construction, originally developed for the \textsc{RICH} hydrodynamic simulation (\cite{RICH}). Our framework is specifically designed for astrophysical simulations, where consecutive mesh reconstructions are required, and it excels in handling complex meshes commonly encountered in this domain, as we will demonstrate. Due to memory and computational constraints, the construction must be performed in parallel. Each processor is responsible for a subset of the domain, constructing the Voronoi diagram locally while ensuring proper connectivity with remote cells managed by other processors.
In Voronoi-based simulations, the mesh must be reconstructed at each time step, making mesh generation a critical bottleneck. By optimizing this process, our method significantly reduces construction time, thereby improving overall computational efficiency.

The rest of the paper is organized as follows. Section \ref{sec:previous_studies} deals with Voronoi diagrams construction and the load balancing problem, an essential problem related to distributed construction. Section \ref{sec:voronoi_delaunay} provides an overview of Voronoi diagrams, Delaunay triangulations, and their construction methods. In section \ref{sec:springel_alg}, we introduce the algorithms implemented by Springel's in \cite{AREPO}. In section \ref{sec:load_balancing}, we describe our load-balancing approach. In section \ref{sec:our_algorithm}, we present our algorithm, which ensures a valid Delaunay triangulation construction in distributed memory, then to be translated into a Voronoi diagram. Section \ref{sec:evaluation} evaluates the methods compared to other construction methods. Finally, section \ref{sec:conclusion} concludes the paper and outlines directions for future research. 
\section{Previous Studies}
\label{sec:previous_studies}
\subsection{Previous Work on Load Balancing}
\label{litrature_load}
To compute the Voronoi diagram in distributed memory or even in parallel memory, one has to consider re-distributing the generating points to alleviate the construction process. This process resembles mesh decomposition in physical simulations, where the space is broken down to a mesh, later to be partitioned into zones, each under the responsibility of a single processor. A key objective is to minimize communication overhead while ensuring a balanced computational load across processors.

Our load balancing problem is equivalent to the graph partitioning problem, where the graph's vertices are the points or Voronoi cells, and the edges represent a shared face. Unfortunately, the graph partitioning problem is known as $\mathsf{NP}$-hard to solve, and many approximations and heuristics attempt to address its solution. Many of these methods rely on multi-level graph partitioning, where the graph is recursively coarsened, partitioned at a simplified level, and then refined to achieve a full partition. These methods are called multi-level graphs and differ by refining rules and partition steps. \cite{ChenJie2022Gcfs} describes many of those techniques. \cite{METIS} introduced a new multi-level partition algorithm, led to the programming of \textsc{METIS}\footnote{https://github.com/KarypisLab/METIS}, a widely used serial graph partitioning package. They incorporated several heuristics, such as the Kernighan-Lin algorithm (\cite{KL}) or the graph growing partitioning (GGP) algorithm. \\
\textsc{METIS} also possesses a distributed version named \textsc{ParMETIS}\footnote{https://github.com/KarypisLab/ParMETIS}, in case the graph's vertices are distributed among multiple participants, as in our case.
Other common frameworks are \textsc{JOSTLE}\footnote{https://chriswalshaw.co.uk/jostle/} \cite{JOSTLE}, \textsc{Scotch}\footnote{https://www.labri.fr/perso/pelegrin/scotch/} and \textsc{Zoltan}\footnote{https://sandialabs.github.io/Zoltan/}. They are all compared in \cite{benchmark_graph_frameworks}, along with other frameworks. \\
\cite{Steinberg_2015} introduced a load-balancing approach specifically designed for unstructured meshes, with a focus on Voronoi diagrams. In their method, given a set of $P$ processors, they construct a secondary Voronoi mesh which is then used to assign spatial subregions to different processors. This technique has gained prominence in successive Voronoi constructions within astrophysical simulations, where the processor-based tessellation dynamically adjusts itself using heuristic methods. \\
Another partitioning method is simulated annealing (\cite{SIMULATED_ANNEALING,YANIV}), which approaches the optimization problem by taking random steps toward a locally optimal solution. This is achieved by making perturbations in the partitioning process, such as moving vertices between different partitions, while gradually reducing the likelihood of accepting worse solutions over time.
Spectral graph partitioning (\cite{RSB}) is a well-established theoretical technique that utilizes the spectral properties of a graph to generate partitions, typically aiming for a high ratio of internal to external edges. However, these methods are computationally expensive and often challenging—or even infeasible—to implement efficiently in distributed-memory systems.
Other partitioning methods based on local improvement heuristics, such as the Tabu search and the genetic algorithm, were also studied. Some of them are discussed in \cite{survey}. \\
Curve-based load balancing, where the space is partitioned according to the behavior of a one-dimensional curve (see subsection \ref{subsec:curve_based_load_balancing}), has been studied theoretically and experimentally. A commonly used curve is the Hilbert curve, whose locality-preserving properties were first rigorously defined in \cite{499920}. Further analyses of this method can be found in \cite{908985} and \cite{BaumanK.E.2006Tdfo}.
Harlacher et al. (\cite{curve_balancing_comparison}) analyzed the performance of curve-based load balancing for distributed meshes. \\
\cite{BORRELL2018264} compared the curve-based load balancing to other heuristics. Other studies were done by \cite{7336274} (2D case) and \cite{hilbert_testing}. 
Further details can be found in \cite{MIZRACHI}.
\subsection{Voronoi and Delaunay Construction Frameworks and Methods}
Due to the widespread interest in Voronoi diagrams, their construction has been extensively studied, and several open-source frameworks are available. \\
\textsc{Voro++}\footnote{https://github.com/chr1shr/voro} (\cite{Vorocpp}) provides a C++ implementation for constructing three-dimensional Voronoi tessellations. Another well-known tool is \textsc{QVoronoi}, part of the \textsc{Qhull}\footnote{http://www.qhull.org/html/qvoronoi.htm} software, which computes Voronoi diagrams by first constructing the Delaunay triangulation using a projection-based algorithm. Additionally, the Computational Geometry Algorithms Library (\textsc{CGAL}) offers an implementation for both Voronoi and Delaunay constructions (\cite{CGAL_VORONOI}). \\
Lo \cite{LO201288} also introduced a parallel framework for constructing Delaunay triangulations in two and three dimensions. Their approach follows a common strategy used by other frameworks: partitioning the space into zones—often using a Cartesian grid or a KD-tree—constructing the triangulation locally within each zone, and then incorporating points from neighboring zones. This point exchange process can be performed using various techniques. 
Duffell et al. (\cite{Duffell_2011}) introduced a serial two-dimensional code based on Voronoi diagram decomposition. Their construction method relies on edge flipping; however, this approach does not generalize trivially to three dimensions.
A distributed framework for constructing Delaunay or Voronoi diagrams must overcome a critical challenge: correctly identifying and retrieving the boundaries of all cells, including those adjacent to cells managed by other processors. 
These borders are defined by \textit{ghost cells} or \textit{ghost points}, which are points from other processors that will later be used to construct the local diagram. The main challenge is identifying the necessary ghost points and efficiently partitioning the points among processors to optimize the overall construction process. Once the ghost points are known, constructing the Voronoi diagram or Delaunay triangulation locally becomes straightforward, often utilizing the libraries previously discussed. \\
An early distributed Voronoi construction was introduced as open-source software by \textsc{TESS2}\footnote{https://github.com/diatomic/tess2}, following the method described in \cite{DELAUNAY_KD}. In this approach, the space is partitioned into blocks using a KD tree. Points are assigned to these blocks, and each processor may be responsible for multiple blocks. As necessary, local points are communicated to neighboring blocks (or even neighbors of neighbors, and so on) until the local Delaunay triangulation is completed. A similar algorithm will be presented later. \\
Peterka et al. (\cite{7013068}) and Gonz\'alez (\textsc{PARAVT}, (\cite{PARAVT})) proposed a similar technique and also proved the correctness of the algorithm. \textsc{PARAVT} offers two distinct domain decomposition approaches; however, like the KD-Tree method, it partitions the space into rectangular blocks that are subsequently assigned to processors.
Wu et al. (\cite{WU2023102995}) presented \textsc{ParVoro++} - a distributed Voronoi tessellation code. They provide two frameworks: one for automatic ghost-point search and another for manual search. In the manual approach, the user specifies a radius within which all ghost points for a given point are guaranteed to be located, thus reducing the search effort. They evaluated the tessellation time and strong scaling performance of their framework. \\
The authors of \cite{singh2024votessmultitargetgpucapableparallel} introduced \textsc{Votess}, a three-dimensional distributed Voronoi diagram construction framework. Unlike other frameworks, including our own, this paper presents a code with a construction algorithm specifically designed for execution on GPUs. The algorithm works by independently determining the list of nearest neighbors for each cell, and then constructing the cell by clipping a box based on the bisections (Voronoi faces). The framework has demonstrated successful acceleration on GPUs when handling sufficiently large point datasets. However, it should be noted that the presented framework is a multithreaded code designed for parallel systems with shared memory. \\
Springel (\cite{AREPO}), in his astrophysical code \textsc{AREPO}, which uses Voronoi diagrams as a spatial discretization method, proposed two algorithms for identifying ghost points and constructing the Delaunay triangulation (which is later converted into a Voronoi diagram). Unlike other KD-Tree implementations, Springel employs the Hilbert curve for space decomposition. In this paper, we present Springel’s algorithms and enhance them to address challenges that arise in more complex scenarios.
\section{Voronoi Diagrams and Delaunay Triangulations}
\label{sec:voronoi_delaunay}
\subsection{Voronoi Diagrams}
\subsubsection{Definition}
A \textit{Voronoi diagram} of a set of points $S \subseteq \mathbb{R}^n$ is defined as the partition of $\mathbb{R}^n$ into cells, each is the subspace of all the points in $\mathbb{R}^n$ closer to a certain point $p \in S$ than to other points of $S$. The points of $S$ are called the \textit{generating points}. A point induces a cell and vice versa, and therefore, we interchangeably refer to a generating point and the cell induced by this point as the same. 
In cases where a point is equidistant from two points in $S$, it is arbitrarily assigned to a cell to keep the Voronoi diagram a valid partition. \\
In this way, each point in $S$ corresponds to a Voronoi cell, and vice versa. \\
The cells of a Voronoi diagram are unique for a given set of mesh-generating points.
An example of a Voronoi diagram is portrayed in figure \ref{fig:voronoi_diagram}.
\begin{figure}
    \centering
    \includestandalone[width=0.9\columnwidth]{figures/voronoi}
    \caption{A Voronoi diagram of the set of blue two-dimensional points (the border cells are infinite, and the figure is clipped)}
    \label{fig:voronoi_diagram}
\end{figure}
\subsubsection{Construction of the Voronoi Diagram}
The importance of the Voronoi diagram raises questions about how to find the Voronoi diagram of a given set of points $S$. By "finding the Voronoi diagram," we refer to identifying the neighboring cells of each cell and determining the vertices of these cells. Since each cell corresponds to a point, this is equivalent to determining which points in $S$ are neighbors. \\
Multiple methods exist to construct the Voronoi tessellation of a given set of points. These methods are detailed and discussed in \cite{VORONOI_TESS}.
In this paper, we focus on one such method, which involves an equivalent construction of a geometric structure known as the Delaunay triangulation.
\subsection{Delaunay Triangulations}
\subsubsection{Definition}
A \textit{triangulation} of a set of points (often called \textit{sites} in this context) $S$ partitions the space into triangles\footnote{The definition of a triangle in dimensions higher than $2$ is defined more often as \textit{simplex}.}, whose vertices are all in $S$. A Delaunay triangulation of a set of points $S \subseteq \mathbb{R}^n$ is a special triangulation of $S$. It is a triangulation that follows the \textit{Delaunay property}, or the \textit{Empty Circumcircle Property}. The empty circumcircle property states that when examining the circumcircle of a triangle $T$, assuming $T$ is defined by the vertices $v_0, v_1, v_2 \in S$, there are no other points from $S$ inside the circumcircle. \\
Boundary points may not have corresponding points to form some triangles. Therefore, it is common to enclose the points within a large triangle that contains all the points and construct the Delaunay triangulation using these additional points as well. 

An example is shown in figure \ref{fig:delaunay_triangulation}.
\begin{figure}
    \centering
    \includestandalone[width=0.9\columnwidth]{figures/delaunay}
    \caption{Delaunay triangulation of points (in blue). Red points are the bounding triangle's points.}
    \label{fig:delaunay_triangulation}
\end{figure}
\subsubsection{Delaunay-Voronoi Duality}
A well-established result is that the Delaunay Triangulation of a set of points $A$ and the Voronoi Diagram of the same set of points are conjugate and equivalent in computation. The centers of the circumcircles of the triangles constitute the vertices of the Voronoi cells. In addition, assuming a set of generating points $S$, then if $a$ and $b$ are both neighboring cells in $S$'s Voronoi diagram, the edge $\left(a,b\right)$ exists in $S$'s Delaunay triangulation. \\
The duality is more profound, as a one-to-one dual mapping can be done from $k$-dimensional objects in the Voronoi diagram to $\left(n-k\right)$-dimensional objects in the Delaunay triangulation.
Figure \ref{fig:voronoi_delaunay_duality} shows an example of this duality.
\begin{figure}
    \centering
    \includestandalone[width=0.9\columnwidth]{figures/duality}
    \caption{The duality between the Delaunay triangulation of a set of points (in red) and its Voronoi diagram (in blue). For example, the center of the circumcircle of a triangle in the Delaunay triangle is a vertex of the Voronoi diagram.}
    \label{fig:voronoi_delaunay_duality}
\end{figure}
As previously noted, our approach to building the Voronoi diagram is based on the latter duality between Voronoi diagrams and Delaunay triangulations. \cite{HUGO_DELAUNAY_VORONOI} explains how to transform a valid $3$-dimensional Delaunay triangulation into a valid Voronoi diagram.
\subsubsection{Construction of the Delaunay Triangulation}
Various algorithms exist to generate the Delaunay triangulation of a given set of points. One straightforward but computationally inefficient approach is the brute-force method, which involves examining all possible triangulations. This approach costs $\mathcal{O}\left(n^4\right)$, since there are $\mathcal{O}\left(n^3\right)$ triangulations possible. \\
However, in the two-dimensional case, an optimal construction algorithm operates in $\mathcal{O}\left(n\log{n}\right)$. \\
The \textit{Flip Algorithm} in the two-dimensional case initiates with an arbitrary triangulation and iteratively flips edges (i.e., replaces an edge with a new one) according to predefined rules until a valid Delaunay triangulation is achieved. This method can be extended to the three-dimensional case, although the flipping process becomes more complex. \\ 
The \textit{Incremental Algorithm} follows the fundamental principles of the Flip Algorithm. However, rather than starting with the entire set of points, points are added incrementally, with edge flips performed as necessary to maintain the validity of the Delaunay triangulation (hence the term "incremental"). This approach can also be extended to three dimensions. \\
In the three-dimensional case, incremental and flip algorithms are used due to their simplicity. The complexity of the incremental algorithm is quadratic ($\mathcal{O}\left(n^2\right)$). However, insertion in a certain order reduces the complexity to $\mathcal{O}\left(n\log{n}\right)$ in expectation (\cite{Edelsbrunner_Shah_1992}). The incremental algorithm is also the implementation that is adopted in our code. \\
Other notable methods for Delaunay triangulation include the DeWall algorithm (\cite{DeWall}), divide-and-conquer techniques, and a projection-based approach. The projection algorithm works by embedding the points into a higher-dimensional space (by adding an extra coordinate), computing the convex hull in that space, and then projecting the points back into the original dimension by removing the added coordinate. \\
A comprehensive comparison of these algorithms, along with detailed implementation considerations, is provided in \cite{DELAUNAY_ALL}. This source also includes additional insights into Delaunay triangulation, its applications, and GPU-based implementations.
\section{Springel's Algorithms}
\label{sec:springel_alg}
\subsection{The Unified Circle Algorithm}
Let us introduce Springel's first algorithm presented in his code \textsc{AREPO} (\cite{AREPO}), a three-dimensional hydrodynamic simulation. In this framework, the Voronoi diagram serves as the spatial discretization method, partitioning space into cells, each of which holds physical data. As the generating points move with each timestep of the simulation, the Voronoi diagram must be reconstructed (or at least partially updated). Due to the continuous nature of the system, minor movements of the points result in only minimal changes to the Voronoi diagram. The following terms are defined for convenience: 
\begin{itemize}
    \item $C_{R}\left(p\right)$: the sphere (circle) of radius $R$ around a point $p$.
    \item $R_{T}$: the radius of the circumcircle of a triangle $T$.
    \item $\intersect{C_{R}\left(p\right)}$: the processors that intersects with the sphere $C_{R}\left(p\right)$.
    \item $\alpha$: the multiplicative factor of circle inflation (a hyper-parameter which in our implementation is determined by default as $1.1$).
\end{itemize}
\begin{breakablealgorithm}
\caption{Springel's Algorithm of Building a Distributed Voronoi Diagram}\label{alg:springel_ghost_points}
\begin{algorithmic}[1]
\Require $radiuses$ array. \Comment{Will be discussed later.}
\State Compute a triangle containing all $points$ and locally build the Delaunay triangulation of $points$.
\State $cur\_points \gets points$ \Comment{Remaining points to build}
\State $cur\_radiuses \gets radiuses$ \Comment{Remaining points' radiuses}
\State $all\_points \gets points$
\For{$P=0,...,N_{proc}-1$}
    \State $sent\left[P\right] \gets \emptyset$
\EndFor
\While{There's a processor that insists to continue}
    \State $new\_points \gets \emptyset$
    \State $reuqests \gets \emptyset$
    \For{$p$ in $cur\_points$}
        \State $r \gets cur\_radiuses\left[p\right]$
        \For{$P$  in $\intersect{C_{r}\left(p\right)}$ which is not self}
            \State add $\left(P, p, r\right)$ to $requests$ (outcoming)
            \EndFor
    \EndFor
    \State \textbf{Exchange} $requests$
    \For{$\left(P, p, r\right)$ in $requests$ (incoming)}
        \State $A \gets \left(points \cap C_{r}\left(p\right)\right) \setminus sent\left[P\right]$
        \State $sent\left[P\right] \leftarrow sent\left[P\right] \cup A$
        \State \textbf{Send} $A$ to $P$
    \EndFor
    \While{There's an incoming message $A$ from $P$}
        \State \textbf{Receive} $A$ from $P$
        \State $new\_points \gets new\_points \cup A$
    \EndWhile
    \State Add $new\_points$ to the current Delaunay triangulation.
    \For{$p$ in $cur\_points$}
        \State $r \gets cur\_radiuses\left[p\right]$
        \State $T_{p} \gets$ triangles that $p$ is a part of
        \State $R \gets \max_{T \in T_{p}}\left(R_{T}\right)$
        \If{$r \geq 2 \cdot R$} \Comment{Done}
            \State remove $p$ from $cur\_points$ \label{alg1:point_done}
            \State $radiuses\left[p\right] \gets r$
        \Else
            \State $cur\_radiuses\left[p\right] \gets cur\_radiuses\left[p\right] \cdot \alpha$
        \EndIf
    \EndFor
    \State Call to halt if $cur\_points = \emptyset$, otherwise insist to continue.
\EndWhile
\State Find the dual Voronoi diagram of the resulting Delaunay triangulation.
\end{algorithmic}
\end{breakablealgorithm}
It is important to notice that the algorithm itself is not optimized. For example, one may consider aggregating requests in a buffer to decrease communication congestion or maintaining special data structures to compute intersections quickly. \\
Assuming one is persuaded that a point is removed from the $cur\_points$ list only when all of its correct Delaunay neighbors arrive at the processor, the correctness is evident. This property is correct since we remove the point only when its test circle encompasses all of its circumcircles (line \ref{alg1:point_done}). At this point, we have already brought all the points within the test circle and, consequently, all points inside the circumcircles as well. Therefore, the circumcircles are free of any foreign points that have not yet been brought in.
To facilitate the successive constructions, Springel maintains a list of the final search radii from Algorithm \ref{alg:springel_ghost_points}. This list is then used as an initial estimate for each point’s search radius in subsequent constructions, reducing the number of required iterations. The difference in running time is noticeable. Consequentially, the first construction typically takes considerably longer than subsequent builds. In this context, the key metric is the construction time for the advanced builds, as they account for the majority of the simulation’s total running time.

\subsubsection{Demonstration}
We illustrate the steps of Springel's algorithm through a simple example. For clarity, we'll have the example in the two-dimensional space (although our algorithm operates in three dimensions). In addition, we will deal with two processors only (the red at the top and the blue at the bottom) and focus on a single point. In an original single iteration, the algorithm runs for all local points (unless, of course, they are done) and, in particular, for our point. \\
Consider figure \ref{fig:springel_demonstartion_a}, which depicts the first iteration of the build process. An initial circle is placed around the local point (the red circle).  In this phase, the circle neither intersects the blue processor nor contains any local points other than the center point. Consequently, no new points are brought in. Additionally, we have not completed the build process for our point because its Delaunay circles (shown in gray) are not fully contained within the red circle. Therefore, we proceed to the next build iteration, in which the testing circle is increased by $\alpha$. \\
Next, we move to the situation illustrated in figure \ref{fig:springel_demonstartion_b}. The previous phase circle is drawn in dashed purple. We now ask for points located inside the red circle. This time, there is a local point matching the query (located right below our point), but since it's a local point and already a part of our local Delaunay triangulation, there is no need to do something. Again, no new points are found, and we cannot finish the build process for the point, as the red circle does not contain all the Delaunay circles (in gray). Therefore, we enlarge the circle once more.
Refer to figure \ref{fig:springel_demonstartion_c}. We scrutinize the points inside the red circle. This time, the circle intersects with the blue processor, prompting us to send a remote range query. In response, the remote process sends us a new ghost point. We build a new Delaunay triangulation (in fact, we modify the existing one) to incorporate this newly received ghost point. Since the Delaunay circles before the latest build (in gray) are still not included in the red circle, we inflate it even more. 
In figure \ref{fig:springel_demonstartion_d}, we reiterate this procedure, retrieving two additional remote points. We construct a new Delaunay triangulation and increase the red circle radius, as the old Delaunay circles are not contained in the red one.
Turning to Figure \ref{fig:springel_demonstartion_e}, all requisite ghost points have already been assimilated. Nevertheless, the termination criterion is not yet fulfilled. Two additional points are incorporated, albeit superfluously.
As we can see in figure \ref{fig:springel_demonstartion_f}, the iterations for our point are done, as the current circle contains all the gray circumcircles (the point triangle's circumcircles). We can remove our point from the list of unfinished points (\textit{cur\_points} in algorithm \ref{alg:springel_ghost_points}).

\begin{figure*}
\centering
\subfloat[An initial circle is drawn around the point. Current circumcircles (in gray) are not contained, so we increase it.]{\hspace{-2mm}
  \centering
 \includestandalone[width=.9\columnwidth]{figures/springel/1}
  \label{fig:springel_demonstartion_a}
}
\hspace{8mm}
\subfloat[We bring all the points inside the blue area and the red circle, and then we build the Delaunay triangulation again. However, no points were brought in this step.]{
  \centering
 \includestandalone[width=.9\columnwidth]{figures/springel/2}
  \label{fig:springel_demonstartion_b}
}
\end{figure*}
\begin{figure*}\ContinuedFloat
\centering
\subfloat[We increase the red circle's radius since it didn't contain all the gray circles. One more remote point is brought and added to the Delaunay triangulation.]{
  \centering
 \includestandalone[width=.9\columnwidth]{figures/springel/3}
  \label{fig:springel_demonstartion_c}
}
\hspace{8mm}
\subfloat[We increase the red circle again. Two more points are added to the triangulation.]{
  \centering
 \includestandalone[width=.9\columnwidth]{figures/springel/4}
  \label{fig:springel_demonstartion_d}
}
\end{figure*}
\begin{figure*}\ContinuedFloat
\centering
\subfloat[Even though the current point triangles are legal, the gray circles are not all contained in the red circle. So we'll increase it one last time. Three more points are added. Notice that they are all redundant points.]{
  \centering
 \includestandalone[width=.9\columnwidth]{figures/springel/5}
  \label{fig:springel_demonstartion_e}
}
\hspace{8mm}
\subfloat[Since the red circle contains all the gray circumcircles, we are done for this point.]{
  \centering
 \includestandalone[width=.9\columnwidth]{figures/springel/6}
  \label{fig:springel_demonstartion_f}
}
\caption{Example for running algorithm \ref{alg:springel_ghost_points} for one point only and two processors (red and blue).}
\label{fig:springel_demonstartion}
\end{figure*}

\subsection{Individual Circles Approach}
Recall that the algorithm asks for all the ghost points inside the big testing circle, imports them, and then uses all to build a Delaunay triangulation. However, creating excessive layers of unnecessary ghost points, which might occur in multiple cases (as we will demonstrate soon), may severely degrade efficiency.
Springel (\cite{AREPO}) proposed an alternative approach, where instead of testing a big circle, attempting to capture at once all the points inside this point triangles' circumcircles, we instead break the range query into multiple alternative queries, each corresponds to one circumcircle only. In other words, we ask the relevant processors to bring all the points inside each circumcircle separately. The algorithm is described in \ref{alg:springel_alternative_ghost_points}. Notations are the same as in algorithm \ref{alg:springel_ghost_points}. \\
In algorithm \ref{alg:springel_alternative_ghost_points}, we iterate over any point, say $p$, then over all of $p$'s circumcircles and ask the intersecting processors for their closest point to $p$ inside the circumcircle. We add imported points to the Delaunay triangulation. Instead of retrieving an unbound amount of points in a big circle, we examine the circumcircles separately.
\begin{breakablealgorithm}
\caption{Alternative Algorithm for Distributed Construction of Voronoi Diagrams}\label{alg:springel_alternative_ghost_points}
\begin{algorithmic}[1]
\State Compute a triangle containing all $points$ and locally build the Delaunay triangulation of $points$.
\State $cur\_points \gets points$
\State $cur\_triangles \gets \bigcup_{p \in points} T_p$ \Comment{All triangles}
\State $new\_triangles \gets cur\_triangles$
\State $all\_points \gets points$
\For{$P=0,...,N-1$}
    \State $sent\left[P\right] \gets \emptyset$
\EndFor
\While{There's a processor that insists to continue}
    \State $reuqests \gets \emptyset$
    \State $triangles\_checked \gets \emptyset$
    \For{$p$ in $cur\_points$}
        \For{$T$ in $T_p \cap cur\_triangles$}
            \State $r \gets R_{T}$
            \State $c \gets $ the center of the circumcircle of $T$
            \State add $\left(P, p, c, r\right)$ to $reuqests$ (outcoming)
        \EndFor
    \EndFor
    \State \textbf{Exchange} $requests$
    \For{$\left(P, p, c, r\right)$ in $requests$ (incoming)}
        \State $A \gets C_{r}\left(c\right) \setminus sent\left[P\right]$
        \State $p' \gets \argmin{x \in A}{d\left(x, p\right)}$
        \State $sent\left[P\right] \leftarrow sent\left[P\right] \cup \left\{p'\right\}$
         \State \textbf{Send} $p'$ to $P$
    \EndFor
    \While{There's an incoming message $p'$ from $P$}
         \State \textbf{Receive} $p'$ from $P$
         \State $all\_points \gets all\_points \cup \left\{p'\right\}$
    \EndWhile
    \State Build a Delaunay triangulation of $all\_points$ and mark new triangles
    \State $new\_triangles \gets \emptyset$
    \For{$p$ in $points$}
        \For{$T$ in $T_p$}
            \If{$T$ is new}
                \State $new\_triangles \gets new\_triangles \cup \left\{T\right\}$
            \EndIf
        \EndFor
    \EndFor
    % \State Build a Delaunay triangulation of $all\_points$
    % \For{$p$ in $cur\_points$}
    %     \State $r \gets cur\_radiuses\left[p\right]$
    %     \State $T_{p} \gets$ triangles that $p$ is a part of
    %     \State $R \gets \max_{T \in T_{p}}\left(R_{T}\right)$
    %     \If{$r \geq 2 \cdot R$}
    %         \State remove $p$ from $cur\_points$
    %         \State $radiuses\left[p\right] \gets r$
    %     \Else
    %         \State $cur\_radiuses\left[p\right] \gets cur\_radiuses\left[p\right] \cdot \alpha$ \Comment{$\alpha$ is a constant}
    %     \EndIf
    % \EndFor
    \State Call to halt if $new\_triangles = \emptyset$, otherwise insist to continue.
\EndWhile
\State Find the dual Voronoi diagram of the resulting Delaunay triangulation.
\end{algorithmic}
\end{breakablealgorithm}
The advantage of algorithm \ref{alg:springel_alternative_ghost_points} is that it prevents a situation in which the circles around the points are inflated unnecessarily. This situation may happen if the triangles of a point in a legal Delaunay triangulation are not uniform with their size, as shown in figure \ref{fig:problem_with_springel}.
\begin{figure}
    \centering
    \includestandalone[width=0.9\columnwidth]{figures/original_algorithm_is_bad}
    \caption{The circles of the purple point in the current local Delaunay triangulation have a large variance. Applying Springel's algorithm might bring all the remote points (blue points) into one of the big circles. A new Delaunay triangulation will be built using all the new points. However, only a small fraction of the blue points are essential.}
    \label{fig:problem_with_springel}
\end{figure}
\section{Load Balancing}
\label{sec:load_balancing}
To build the Voronoi diagram in parallel, it is imperative to consider changing the distribution of generating points across hosts. This way, we allocate a subspace (or multiple subspaces) for each host. The host is responsible for exclusively constructing the Voronoi cells of the points inside its designated regions. \\
Better distribution diminishes communication overhead for several reasons. First, for many points, all neighbors might be assigned to the same host, and no further communication is needed in the algorithm as the cell can be built locally. Second, a well-balanced distribution minimizes inter-host dependencies, as fewer points will have neighbors spanning multiple hosts. Third, the nodes' physical topology and rank assignment should be considered; if two nodes can communicate efficiently,\footnote{In MPI, ranks close to each other are often assigned to physically proximate hosts, improving communication speed.} their assigned subspaces should also be spatially close to reduce communication costs.  \\
The mesh partitioning problem, assuming one seeks to minimize communication, is equivalent to the graph partitioning problem, where the goal is to partition a given graph's vertices into disjoint sets, minimizing the number of edges connecting nodes of different sets (these are called the cut edges). Unfortunately, this problem and its similar variations are $\mathsf{NP}$-hard, rendering the pursuit of efficient exact solutions impractical. Consequently, a variety of algorithms and heuristics have been developed, which are briefly outlined in section \ref{litrature_load}. \\
We implement a heuristic called curve-based load balancing.
\subsection{Curve-Based Load Balancing}
\label{subsec:curve_based_load_balancing}
Before describing the curve-based load balancing, let us recall that in Voronoi diagrams, we can refer to a generating point as an equivalent to the cell it induces and vice versa. So, it is sufficient to distribute the points to the processors efficiently. \\
We begin by assuming an initial poor distribution of points across processors (for example, each participant maintains a random partial list of the points in space) as shown in figure \ref{fig:load_balancing_1}.  \\
In curve-based load balancing, we define a curve that traverses multiple predefined points (which are typically distinct from the Voronoi generating points), which will be called \textit{curve points}. Each curve point is assigned an order based on its position along the curve. Each generating point is then mapped to its closest curve point and inherits its corresponding order. \\
Each processor maintains a local list of numbers representing the enumeration of the closest curve point for each of its initial generating points. Ideally, these local lists would be combined into a global list containing enumeration numbers for all points across processors. If feasible, this global list is then sorted and divided into $N$ equally sized parts, which can be viewed as numerical ranges due to the sorting. \\
As illustrated in figure \ref{fig:load_balancing_3} where, after this process, domains are allocated to processors according to the curve. Then, as shown in figure \ref{fig:load_balancing_4}, each rank sends each one of its initial generating points to the processor responsible for its assigned numerical range.

The quality of space-filling curves and the metric used for evaluation is contingent upon the specific problem at hand. However, space-filling curves are widely used due to their \textit{locality preserving properties}: Close points tend to have close enumerations.
Unfortunately, assembling the entire list of numbers into a single global array on one processor and performing local sorting and division is infeasible due to memory limitations. The challenge remains to determine the partition boundaries of this global array. \\
Reevaluating the problem, it becomes clear that a full sort is unnecessary. Assuming there are $P$ processors and $n$ numbers in total, we merely wish to find the smallest values at positions $\nicefrac{n}{P}, \nicefrac{2\cdot n}{P}, ..., \nicefrac{P \cdot n}{P}$ in the global array to define the partition boundaries. \\
The $k$-smallest value in an array is called the \textit{$k$th-order statistic} of the array, and the task of finding it is called a \textit{selection} problem. We developed a technique for efficiently finding those statistics in a distributed memory setting. \\
Our suggested order statistics finding algorithm generalizes the known \textit{QuickSelect} algorithm to distributed memory systems. The QuickSelect algorithm is known to find the $k$-th order statistic of an array by recursively partitioning the array until the desired element is found. The algorithm can be generalized into a distributed memory system by using synchronization 
and collective communication operations such as \textit{Allreduce} to determine the current pivot element order statistic. We also generalized the algorithm to find all the desired statistics (these are, $\nicefrac{n}{P}, ..., \nicefrac{P \cdot n}{P}$) in a single run. \\
This method allows us to find the exact partition points of the curve. \\
We further extended the algorithm to accommodate weighted partitions. In case one doesn't want to distribute the points equally, a weight function can be assigned to the points, and the algorithm then computes a partition in which the total weight assigned to each processor - calculated as the sum of the weights of the points allocated to it - remains approximately balanced.
\subsection{Hilbert Curve}
Our code uses curve-based load balancing where the enumeration is based on the Hilbert curve. The \textit{Hilbert curve} is a sequence of curves $f_0, f_1, ...$, recursively defined as follows:
\begin{itemize}
    \item $f_{0} \equiv \left(\frac{1}{2}, \frac{1}{2}\right)$.
    \item Let $n \geq 0$, and assume that $f_n$ was defined. \\
    $f_{n+1}$ (also called \textit{the $\left(n+1\right)$-level}) is defined as the following: split the space $\left[0,1\right]^2$ into 4 squares, as instructed in figure \ref{fig:hilbert_division}.
    Hold, for each subsquare, a copy of $f_n$. Rotate the copy of subsquare 1 ${90}^{\circ}$ clockwise, and rotate subsquare 4 ${90}^{\circ}$ anti-clockwise. \\
    Then, connect each subsquare's endpoint to the next one's beginning.
\end{itemize}
\begin{figure}
    \centering
    \includestandalone[width=0.2\textwidth]{figures/hilbert_division}
    \caption{A division of the unit square into four subsquares.}
    \label{fig:hilbert_division}
\end{figure}
The function $f_n$ is called a \textit{Hilbert curve of order $n$}. Examples for $f_1, f_2, f_3, f_4$ are shown in figure \ref{fig:hilbert_curves}. \\
It has been demonstrated that the Hilbert curve effectively preserves locality. Informally, that is, given an order $n$, the curve $f_{n}$ ensures that if $x \approx y$, then $f_{n}^{-1}\left(x\right) \approx f_{n}^{-1}\left(y\right)$ (for all $x, y \in \functionimage{f_{n}}$). This attribute is not a trivial or easily achieved property, as the curve may follow a lengthy path when transitioning between points that are spatially close.
\begin{figure*}
\centering
\subfloat[$f_1$]{\hspace{-2mm}
  \centering
 \includestandalone[width=.4\columnwidth]{figures/hilbert/1}
  \label{fig:hilbert_curve_1}
}
\hspace{8mm}
\subfloat[$f_2$]{
  \centering
 \includestandalone[width=.4\columnwidth]{figures/hilbert/2}
  \label{fig:hilbert_curve_2}
}
\hspace{8mm}
\subfloat[$f_3$]{
  \centering
 \includestandalone[width=.4\columnwidth]{figures/hilbert/3}
  \label{fig:hilbert_curve_3}
}
\hspace{8mm}
\subfloat[$f_4$]{
  \centering
 \includestandalone[width=.4\columnwidth]{figures/hilbert/4}
  \label{fig:hilbert_curve_4}
}
\caption{Example for the first Hilbert curves in the sequence. Recursive division lines (to subsquares) are shown in gray.}
\label{fig:hilbert_curves}
\end{figure*}

\begin{figure*}
\centering
\subfloat[Assuming each processor holds a subset of the points.]{\hspace{-2mm}
  \centering
 \includestandalone[width=.5\columnwidth]{figures/load-balancing/1}
  \label{fig:load_balancing_1}
}
\hspace{8mm}
\subfloat[We divide into square subspaces and create a Hilbert curve passing through them.]{
  \centering
 \includestandalone[width=.5\columnwidth]{figures/load-balancing/2}
  \label{fig:load_balancing_2}
}
\vspace{2mm}

\subfloat[Step 1 - Determine the curve cut points (balance).]{
  \centering
 \includestandalone[width=.5\columnwidth]{figures/load-balancing/3}
  \label{fig:load_balancing_3}
} 
\hspace{8mm}
\subfloat[Step 2 - Exchange points according to the partitioning.]{
  \centering
 \includestandalone[width=.5\columnwidth]{figures/load-balancing/4}
  \label{fig:load_balancing_4}
} 
\centering
\caption{Example for the load balancing process in case of $48$ points and $3$ processors (red, blue, green). The color of the point expresses its ownership. The ownership of the curve cells—where all points within a cell are allocated to a specific processor—is indicated by the background color.}
\label{fig:load_balancing_process}
\end{figure*}
We used curve-based load balancing, using the Hilbert curve in our implementation. Our implementation uses a generalized Hilbert curve, presenting a curve similar to the Hilbert curve, but only for rectangular (instead of squared) areas\footnote{https://github.com/jakubcerveny/gilbert}. To calculate enumeration on the curve, we use a data structure similar to an R-tree, which is a tree maintaining the Hilbert subdomains recursively, along with each one's range of enumerations. \\
The load-balancing approach in a generalized rectangular space is also crucial for supporting Voronoi diagram construction within arbitrary boundaries, assuming they create a convex polygon, instead of a conventional bounding box, as load balancing becomes more challenging in irregular domains. The boundaries are defined by their vertices, and the Voronoi diagram is computed accordingly. For example, Figure \ref{fig:frustum} illustrates a Voronoi diagram constructed within a pyramidal domain. It is important to emphasize that the computational domain itself is shaped like a pyramid, rather than just the point distribution, making the construction significantly more complex. \\
To facilitate load balancing, we enclose the entire computational space within a bounding box, which is necessary for computing the Hilbert curve. However, this bounding box is only used for partitioning and does not influence the actual Voronoi construction. Additionally, the technique of \textit{kernelization}, introduced in \cite{MIZRACHI}, may provide a more efficient load-balancing strategy for handling complex geometrical domains.
\begin{figure*}
\centering
\includegraphics[width=.45\textwidth]{figures/frustum}
\caption{An example for a construction of a pyramid-shaped space. Voronoi cells are clipped by their matching face, if needed.}
\label{fig:frustum}
\end{figure*}
\section{A new Construction Algorithm}
\label{sec:our_algorithm}
The proposed method integrates both of Springel's approaches. We categorize the points to \textit{small points} and \textit{large points}. A point is classified as \textit{small} as long as the testing circle around the point does not bring a lot of unnecessary ghost points. The moment too many points are brought, it will be considered as \textit{large}. The terminology does not refer to the actual sizes of the circumcircles, but rather to the spatial density of the region surrounding the point. A large point likely corresponds to circumcircles with unequal radii, as the testing radius has incorporated too many points, often due to issues such as those illustrated in figure \ref{fig:problem_with_springel}.
Just like both algorithm \ref{alg:springel_ghost_points} and algorithm \ref{alg:springel_alternative_ghost_points}, our construction process follows an iterative approach, where additional points are incrementally introduced and incorporated into the Delaunay triangulation. However, in this method, the specific algorithm applied to each point depends on its classification. For small points, we employ the unified circle approach as described in algorithm \ref{alg:springel_ghost_points}. Once a point is classified as large, we alternate to the technique used in algorithm \ref{alg:springel_alternative_ghost_points}, where each one of the point's circumcircle is checked individually for ghost points. We call the points for small points \textit{the small algorithm} and for large points \textit{the large algorithm}. \\
To enhance performance, we introduced several optimizations to the algorithm. First, in the large algorithm, a circle range query is designed to bring at most one point. The point is the closest point to the circle's generating point among all the points inside the circle (of course, the result is restricted to exclude any points that were already sent). The reason for the latter constraint is that a circle might grow to a substantial size and return an overwhelming number of points. For the same reason, we bound the small algorithm queries to return, at most, a constant maximum number (say, $15$) for each query. If a query exceeds this threshold, the point is reclassified as large, thereby switching the algorithm used in subsequent iterations. \\
Second, when running large points queries, difficulties may arise if the triangle’s circumcircle is excessively large. In such cases, the circumcircle may intersect multiple processor domains, requiring queries to each of them. In small queries, that is less likely to happen because as soon as too many points are brought, the point changes its classification to large. 
Instead of immediately querying all processors intersected by a large query, we adopt a two-phase approach. Initially, we query only the nearest intersecting processors. In a subsequent iteration, we extend the request to all intersecting processors. A large query is considered complete only after passing both phases. In many cases, querying only the nearest processors yields sufficient points to construct the Delaunay triangulation in the same iteration, and that makes the circles for the next iteration small. In other words, the large circumcircle may be eliminated in the process, potentially making the second check less expensive.
Like Springel's first algorithm (algorithm \ref{alg:springel_ghost_points}), we store the final search radii at the end of the construction to facilitate later constructions. The radius saved to points is determined by the last iteration in which the point was classified as small. If the point was moved to be classified as large, we multiply this radius by a factor smaller than $1$ to prevent the circle from inflating significantly in future builds. Without this adjustment, the radius—used as the initial estimate for subsequent constructions would continuously and exponentially expand, potentially degrading performance.
\subsection{Demonstration}
We demonstrate our algorithm in figure \ref{fig:our_alg_demonstartion}. As in figure \ref{fig:springel_demonstartion}, we present a two-dimensional example and apply the algorithm to a single point to facilitate comprehension. \\
In figure \ref{fig:our_alg_demonstartion}, we focus on the red processor, having the two blue and green neighbors. \\
Let us assume the threshold of a point status change is by bringing three or more points in a single iteration. \\
In the beginning, the red processor is unaware of any point outside of the red area, excluding the big triangle that bounds the whole space. It starts by building a Delaunay triangulation of the points it is aware of only. Figure \ref{fig:our_alg_demonstartion_a} starts by assigning the point to a \textit{small point} status, so an initial testing circle is drawn around the point. The small status remains for several iterations, and hence, the testing circle grows exponentially, intersecting the blue and then the green processors. However, no new points are brought until a further iteration, shown in figure \ref{fig:our_alg_demonstartion_b}. In this iteration, two points are brought. Those points are replicated in the red processor, and added to the current Delaunay triangulation as shown in figure \ref{fig:our_alg_demonstartion_c}, depicting the start of the next iteration where the red testing circle grows again since the building is not complete (the red circle does not contain all the gray circumcircles).
After multiple iterations, figure \ref{fig:our_alg_demonstartion_d} shows the retrieval of three additional points: one from the blue processor and two from the green one. Since the number of points brought in this iteration is $3$, which is the threshold of status change, we change the point status to be \textit{large}. This status change is reflected in figure \ref{fig:our_alg_demonstartion_e}, where after adding the newly acquired points into the local Delaunay triangulation, the final testing circle (depicted as a dashed purple line) is drawn. From this point onward each of the point’s circumcircles is tested individually.
Recall that in the large phase of a point, each processor only gives the closest point to the query point, located inside each circumcircle it intersects. A tested circumcircle will be emphasized in orange. In \ref{fig:our_alg_demonstartion_e}, all the circumcircles are tested for an accurate start, and hence they are all orange. While most of the tested circles return empty, one of them retrieves a ghost point located in the blue processor, which is imported.
Again, this point is added to the local triangulation. New triangles created by this point will have their circumcircle tested in the following iteration, as shown in figure \ref{fig:our_alg_demonstartion_f}. At this stage, all tested circumcircles are found to be empty, confirming that no further points need to be added. Consequently, the construction is complete, as shown in figure \ref{fig:our_alg_demonstartion_g}. \\
This example highlights a key limitation of algorithm \ref{alg:springel_ghost_points}: its exponential circumcircle growth, which can eventually encompass the entire domain. This issue arises because the given point forms an obtuse-angled triangle with an extremely large circumcircle that must ultimately be contained within the testing circle.
The given example underscores the limitations of algorithm \ref{alg:springel_ghost_points} against algorithm \ref{alg:springel_alternative_ghost_points} in this particular example. Yet, using algorithm \ref{alg:springel_alternative_ghost_points} requires more memory and communication (since each circumcircle is tested individually). As discussed in Section \ref{sec:evaluation}, this approach may introduce other complications
We believe our approach successfully resolves this challenging issue by leveraging the advantages of both algorithms, thereby optimizing performance while minimizing their respective limitations.
\begin{figure*}
\centering
\subfloat[The point starts by being classified as a small point. We draw a testing circle around it.]{
  \centering
  \includestandalone[width=.9\columnwidth]{figures/our_algorithm/output_0}
  \label{fig:our_alg_demonstartion_a}
}
\hspace{8mm}
\subfloat[In each iteration, we ask intersecting processors for all their points inside the testing circle. Only in iteration $18$ two more points are brought.]{
  \centering
  \includestandalone[width=.9\columnwidth]{figures/our_algorithm/output_18}
  \label{fig:our_alg_demonstartion_b}
}
\end{figure*}
\begin{figure*}\ContinuedFloat
\subfloat[The incoming points are brought and added to the local Delaunay triangulation. These are only $2$ points (below of the threshold of $3$). The testing circle does not contain the gray circumcircles, so we increase it.]{
  \centering
  \includestandalone[width=.9\columnwidth]{figures/our_algorithm/output_19}
  \label{fig:our_alg_demonstartion_c}
}
\hspace{8mm}
\subfloat[In iteration $22$ the testing circle brings $3$ points to be added to the Delaunay triangulation. The point will now be classified as large.]{
  \centering
  \includestandalone[width=.9\columnwidth]{figures/our_algorithm/output_22}
  \label{fig:our_alg_demonstartion_d}
}
\end{figure*}
\begin{figure*}\ContinuedFloat
\subfloat[Each one of the circumcircles is tested individually. One point is brought from the blue processor.]{
  \centering
  \includestandalone[width=.9\columnwidth]{figures/our_algorithm/output_23}
  \label{fig:our_alg_demonstartion_e}
}
\hspace{8mm}
\subfloat[The point is added to the Delaunay triangulation. Circumcircles of the new triangles created are tested now.]{
  \centering
 \includestandalone[width=.9\columnwidth]{figures/our_algorithm/output_24}
  \label{fig:our_alg_demonstartion_f}
}
\end{figure*}
\begin{figure*}\ContinuedFloat
\centering
\subfloat[The point is added to the local Delaunay triangulation. No further circles to be tested, so we are done.]{
  \centering
  \includestandalone[width=.9\columnwidth]{figures/our_algorithm/output_25}
  \label{fig:our_alg_demonstartion_g}
}
\caption{A demonstration of our proposed algorithm for one point of the red processor. The background colors denote the processor affinity of points. The red circle is the testing circle (in the small phase), and the dashed purple is the previous testing circle. The dotted gray circles are the current circumcircles of the point's triangles. In the big phase, tested circumcircles will be colored orange.}
\label{fig:our_alg_demonstartion}
\end{figure*}

\section{Evaluation}
\label{sec:evaluation}
We present three types of evaluations for our framework. First, we compare the construction time using two different datasets of points, running each framework on a single shared-memory machine. Second, we evaluate our framework in a distributed-memory setup, comparing it to other frameworks while varying the number of points per processor, keeping the number of processors constant. The third evaluation type focuses on scaling, where we examine both weak and strong scaling of our code. \\
All tests were executed on Linux machines running Rocky 9.3 with kernel version 5.14.0, powered by Intel(R) Xeon(R) Gold 6434 processors (2 sockets). Each node is equipped with $64$ GB (DDR 5) in main memory and features Infiniband with a throughput of $100 \textrm{Gb/s}$ ($2$ HDR links). Hyperthreading was disabled.  We compiled all codes using the Intel Compiler (2024.2.1) and employed IntelMPI as our MPI implementation.
\subsection{Comparing to Parallel Frameworks}
\label{subsec:parallel_comparison}
Here, we compare our execution to parallel frameworks, including our code and \textsc{AREPO}, in a single machine of $16$ cores (capable of running $16$ threads or MPI processes). We compare here the first build only. In ParVoro++ (\cite{WU2023102995}) we determined the optimal number of blocks for partitioning by running multiple executions with varying options (powers of 2), selecting the best configuration for each dataset (which, in both cases, turned out to be $512$). For \textsc{Votess} (\cite{singh2024votessmultitargetgpucapableparallel}), the points were scaled into $\left[0,1\right]^3$, preserving the ratio of distances.
We present two datasets:
\begin{itemize}
    \item Random points sampled, each one, uniformly in the cube $\left[0,1\right]^3$ (all frameworks use the same dataset of points).
    \item Astrophysical dataset, taken from a simulation of a half a solar mass star being tidally disrupted by a $10^5\;M_\odot$ SMBH. The geometry of the Voronoi cells in the midplane color-coded by their density is shown in figure \ref{fig:elad_stro}.
\end{itemize}
\begin{figure*}
\centering
\includegraphics[width=1.9\columnwidth]{figures/astro_build}
\caption{The geometry of the Voronoi cells for the astrophysical dataset color coded by their density. the middle and the right figures are inset of the figure to their left, highlighting the large dynamical range in cell sizes.}
\label{fig:elad_stro}
\end{figure*}

Figure \ref{fig:first_builds} shows the average construction rate (points per second for a single processor) of the first build. \textsc{ParVoro++} and \textsc{Votess} do not support successive builds, so we compare ourselves to Springel in figure \ref{fig:advance_builds}, showing the average construction time of the next $5$ builds. Graphs are shown in log scale. \\
As seen, our construction method performs exceptionally well in more complex scenarios while maintaining efficient behavior even in a uniform setting. It's important to highlight that a uniform mesh is relatively uncommon in real-world applications, where more complex point distributions are typical.
\begin{figure*}
\centering
\subfloat[The construction rate compared to different frameworks for the first build (log scale).]{
  \centering
  \includegraphics[width=.9\columnwidth]{figures/results/first_builds}
  \label{fig:first_builds}
}
\hspace{8mm}
\subfloat[The average construction rate compared of the next $5$ constructions.]{
  \centering
  \includegraphics[width=.9\columnwidth]{figures/results/advanced_builds}
  \label{fig:advance_builds}
}
\caption{Results of parallel execution ($16$ cores).}
\end{figure*}
\subsection{Comparison with AREPO}
\subsubsection{Uniform mesh}
In this dataset, the points are uniformly distributed in $\left[0,1\right]^3$. Results are shown in figure \ref{fig:uniform_results}.
As one can see, our new suggested algorithm improves all the other listed methods.
\subsubsection{Star-like mesh}
In this dataset, the points distribution is as follows: the whole space is a cube with a side length of $1$ ($\left[0,1\right]^3$). $95\%$ of the points are uniformly distributed inside the cube $\left[0.45, 0.5\right]^3$ (a cube with a side length of $0.05$). The other $5\%$ are uniformly distributed in all the space. The remaining $5\%$ points may also fall in the small cube.
The results are shown in figure \ref{fig:unbalanced_results}.

\begin{figure*}
\centering
\subfloat[Results for points samples from a distributed uniform mesh (log-log scale).]{
  \centering
  \includegraphics[width=.9\columnwidth]{figures/results/uniform}
  \label{fig:uniform_results}
}
\hspace{8mm}
\subfloat[Results for points located in an unbalanced star-like shape mesh, as described earlier (log-log scale).]{
  \centering
  \includegraphics[width=.9\columnwidth]{figures/results/unbalanced}
  \label{fig:unbalanced_results}
}
\caption{Results of distributed execution ($512$ cores).}
\end{figure*}

\subsection{Scalability}
\subsubsection{Weak Scale}
To demonstrate the weak scale of MadVoro, we test the running time of advanced builds (average of the first $10$ builds) of a mesh of points uniformly sampled in $\left[0,1\right]^2$. 
We compare our code to Springel's implementation (\textsc{AREPO}), when conserving the number of points per processor to be $10000$, and changing the number of participating processors from $8$ to $512$. Results are shown in figure \ref{fig:weak_scale}, where the $x$ axis, representing the number of processors, is in log scale.
It is clear that although there is a room for improvement, MadVoro preserves weak scale good enough.
\begin{figure*}
\centering
\centering
\includegraphics[width=.9\columnwidth]{figures/results/weak_scale}
\caption{A weak scale execution of a uniform mesh. The number of points per processor is determined to be $10000$ (log-log scale).}
\label{fig:weak_scale}
\end{figure*}
\subsubsection{Strong Scaling}
To test the strong scaling, we used the same astrophysical dataset from section \ref{subsec:parallel_comparison}. The total number of points is determined to $5127679$, and the number of processors varies, from $8$ to $1024$.
Again, the measured times represent the average of the first 10 constructions following the initial one. The ideal scalability, given $P$ points in total and $N$ processors, is $\nicefrac{N}{P}$. Since the graph is displayed in log-log scale, a linear curve is expected. MadVoro consistently outperforms \textsc{AREPO} in construction time, with both methods demonstrating excellent strong scaling.
Results are shown in figure \ref{fig:strong_scale}.
\begin{figure*}
\centering
\centering
\includegraphics[width=.9\columnwidth]{figures/results/strong_scale}
\caption{A strong scale execution with the astrophysical dataset (log-log scale).}
\label{fig:strong_scale}
\end{figure*}
\section{Conclusion and Future Work}
\label{sec:conclusion}
We discussed Voronoi diagrams and their duality with Delaunay triangulations, focusing on the construction problem of a distributed Delaunay triangulation. We introduced Springel's algorithms (\cite{AREPO}) and highlighted their limitations in building an unbalanced mesh. By merging both algorithms and improving their bottlenecks, we created a new, more efficient algorithm. Our assessments, based on several benchmarks, demonstrated that the new algorithm either improved upon or performed at least as well as all other assessed algorithms. \\
Delaunay triangulation in a distributed memory parallel system requires careful load balancing to ensure efficiency. Load balancing plays a crucial role by facilitating the construction process, as processors are more familiar with the communication patterns and the mesh decomposition that define it. We explored curve-based balancing, using the Hilbert curve as an example to demonstrate its advantages.

Future work will focus on enhancing the curve-based technique and exploring its generalization to other shapes and curves. A method we introduced previously, called \textit{kernelization} (\cite{MIZRACHI}), could be one avenue for this exploration. Additionally, there may be further improvements to be made to the construction algorithm itself. Specifically, the current approach—computing the Voronoi tessellation using Delaunay triangulation duality, followed by the flipping algorithm for Delaunay triangulation—may not be the most optimal.
Moreover, we are considering the potential limitations of the communication paradigm currently in use. While we have implemented a query-based communication approach, it might be worthwhile to investigate alternative communication paradigms that could offer better performance or scalability in the presented scheme.
\section*{Acknowledgements}
The principal author thanks Rainer Weinberger for his assistance in running \textsc{AREPO}. We also express our gratitude to the computational physics department at the Racah Institute of Physics, particularly Omri Reved, for their helpful discussions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data Availability}
Our construction framework is available to all as an open-source project in \href{https://github.com/maormizrachi/madvoro}{Github}. Installation and execution instructions can be found in the README file. 
%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%

% The best way to enter references is to use BibTeX:

\bibliographystyle{mnras}
\bibliography{bibliography} % if your bibtex file is called example.bib


% Alternatively you could enter them by hand, like this:
% This method is tedious and prone to error if you have lots of references
%\begin{thebibliography}{99}
%\bibitem[\protect\citeauthoryear{Author}{2012}]{Author2012}
%Author A.~N., 2013, Journal of Improbable Astronomy, 1, 1
%\bibitem[\protect\citeauthoryear{Others}{2013}]{Others2013}
%Others S., 2012, Journal of Interesting Stuff, 17, 198
%\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% APPENDICES %%%%%%%%%%%%%%%%%%%%%

% \appendix

% \section{Some extra material}

% If you want to present additional material which would interrupt the flow of the main paper,
% it can be placed in an Appendix which appears after the list of references.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}

% End of mnras_template.tex
