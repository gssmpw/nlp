\section{Related Work}
\label{ssec:Related Work}

We now summarize the current state of the art on the problem of stochastic linear
bandits with i.i.d.\ subgaussian rewards.
For fair comparison, we first highlight the difference in assumptions made by
authors, and whenever those assumptions differ from ours, we specialize their results
to our setting.

\textbf{Different settings} Degenne et al., "Regret Minimization for Stochastic Linear Bandits" consider an arbitrary action set $\mathcal{X}$ and assume that $\max_{x \in \mathcal{X}} |\theta^\top x| \le 1$, which in our setting is equivalent to $|c^\top \theta + \|\theta\|_{A}| \le 1$. 
Agrawal and Jia, "Pure Exploration in Discounted Stochastic Bandits" consider an arbitrary compact action set $\mathcal{X}$, but they assume intricate conditions on its curvature, see below. 
Dani et al., "The Price of Greed: A Study on the Robustness of Optimal Policy to Non-Greedy Algorithms" consider an arbitrary, possibly time-dependent, action set $\mathcal{X}_t$ and assume $\max_{x \in \mathcal{X}_t} |\theta^\top x | \le 1$, which in our setting is equivalent to $|c^\top \theta + \|\theta\|_{A}| \le 1$. 
They further assume that an upper bound for $\|\theta\|_{2}$ is known and provided as an input to the algorithm. 
Abbasi-Yadkori et al., "Improved Algorithms for Linear Stochastic Bandits" consider an action set $\mathcal{X}$ which is an arbitrary closed subset of the unit ball, and also assume that an upper bound for $\|\theta\|_{2}$ is known and provided as an input to the algorithm.

\textbf{Regret upper bounds } The two versions of the \gls{CB} (\algoname{ConfidenceBall}) algorithm of Abbasi-Yadkori et al., "Improved Algorithms for Linear Stochastic Bandits" achieve regret upper bounds of $O( d \sqrt{ T \log(T)^{3} })$ and $O( d^{3/2} \sqrt{ T \log(T)^{3} })$ in our setting.
The \gls{PEGE} (\algoname{Phased Exploration and Greedy Exploitation}) algorithm of Dani et al., "The Price of Greed: A Study on the Robustness of Optimal Policy to Non-Greedy Algorithms" achieves a regret of $O( ( \|\theta\|_{2} + 1/\|\theta\|_{2} ) d \sqrt{T})$ when $\mathcal{X}$ is a unit ball. 
The \gls{UE} (\algoname{Uncertainty
Ellipsoid})  algorithm of Koolen and van der Meulen, "Bandit problems with stochastic actions" achieves a regret of $O( f(\mathcal{X}) d \sqrt{T \log(T)^3})$ when $\mathcal{X}$ is an ellipsoid where $f$ has an intricate dependency on the curvature of $\mathcal{X}$, and can only be bounded by a universal constant if the maximal ratio of eigenvalues of $A$, $|\gls{lambdamax}(A)/\sqrt{\gls{lambdamin}(A)}|$ is bounded by another universal constant, so that $\mathcal{X}$ is "close" to the unit ball.
The \gls{OFUL} (\algoname{Optimism in the Face of Uncertainty Linear bandit}) algorithm of Abbasi-Yadkori et al., "Improved Algorithms for Linear Stochastic Bandits" achieves a regret $O( (\|\theta\|_{2} + \sigma \sqrt{d} \log(T) ) \sqrt{d T \log(T)})$ assuming that the algorithm is given an upper bound on $\|\theta\|_{2}$ as prior information in order to set the algorithm parameters correctly. 
The \gls{TS} (\algoname{Thompson Sampling}) algorithm of Kaufmann et al., "Thompson sampling for complex exponential family bandits" achieves a regret of $O( \sigma d^{3/2}\sqrt{T \log(dT)^3 })$ assuming that the algorithm is given an upper bound on $\|\theta\|_{2}$ as prior information in order to set the algorithm parameters correctly. 
The \gls{OLSOFUL} (\algoname{Ordinary Least Squares OFUL}) algorithm of Foster et al., "Bandit problems with stochastic actions" achieves a regret of $O( \sigma d \sqrt{ T \log(T/d)^2 } + \|\theta\|_{2}d \log(d) )$ without prior information on $\theta$. 
Therefore, the two versions of \algoname{CB}, \algoname{PEGE}, \algoname{UE}, \algoname{OFUL}, \algoname{OLSOFUL},  \algoname{TS} are not provably minimax optimal, in contrast with~\nous.

\textbf{Regret lower bounds } Bubeck et al., "Bandits and Experts" show that the minimax regret of any algorithm is at least $\Omega(d \sqrt{T})$ when the set of actions is the unit ball, using a Bayesian approach. 
Kaufmann et al., "Thompson sampling for complex exponential family bandits" show the same lower bound in a more general setting, using an Assouad-style construction. 
Our locally asymptotically minimax lower bound is a stronger result. with the correct
dependency on $d, \sigma, T$ and $\|\theta\|_{A}$. 

\textbf{Computational complexity} To the best of our knowledge, even when $\mathcal{X}$ is a unit ball, none of the optimistic algorithms such as \algoname{CB}, \algoname{UE}, \algoname{OFUL} and \algoname{OLSOFUL} cannot be implemented in polynomial time, because they must repeatedly maximize the bilinear function $x^\top \theta$ over the couple  $(x,\theta) \in \mathcal{X} \times \mathcal{C}_t$ where \gls{Ct} is the confidence ellipsoid computed at time $t$. 
To the best of our knowledge, no efficient algorithm is known to solve this bilinear optimitzation problem when $\mathcal{X}$ is an ellipsoid. 
The only case in which this is feasible in polynomial time is when $\mathcal{X}$ is finite with polynomial size in $d$. 
This is corroborated by our numerical experiments. 
Both \algoname{PEGE}, \algoname{TS} can be implemented in polynomial time, as they require to minimize a linear function over $\mathcal{X}$, and/or to compute the least-squares estimate of $\theta$. 
Our results show that there is no regret/complexity trade-off in linear bandits, in the sense that~\nous~achieves both minimax regret, and low computational complexity.  

\textbf{Related problems} 
Other authors have considered different but related problems in linear bandits, for
instance: Bubeck et al., "Bandits and Experts" consider the case where $\theta$ is sparse,
Dani et al., "The Price of Greed: A Study on the Robustness of Optimal Policy to Non-Greedy Algorithms" lower bound asymptotically the eigenvalues of the
expected design matrix, 
Kaufmann et al., "Thompson sampling for complex exponential family bandits" consider the case where the variance of the
reward from the optimal decision is small, and 
Agrawal and Jia, "Pure Exploration in Discounted Stochastic Bandits" consider
the adversarial case (we refer the reader to their discussion of the major differences
between stochastic and adversarial linear bandits).