@article{abbasi2011improved,
	author = {Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
	date-added = {2023-11-24 14:45:41 +0100},
	date-modified = {2023-11-24 14:45:41 +0100},
	journal = {Advances in neural information processing systems},
	title = {Improved algorithms for linear stochastic bandits},
	volume = {24},
	year = {2011}}

@inproceedings{abeille_2017,
	abstract = {We derive an alternative proof for the regret of Thompson sampling (TS) in the stochastic linear bandit setting. While we obtain a regret bound of order $O(d^3/2\sqrtT)$ as in previous results, the proof sheds new light on the functioning of the TS. We leverage on the structure of the problem to show how the regret is related to the sensitivity (i.e., the gradient) of the objective function and how selecting optimal arms associated to \textitoptimistic parameters does control it. Thus we show that TS can be seen as a generic randomized algorithm where the sampling distribution is designed to have a fixed probability of being optimistic, at the cost of an additional $\sqrtd$ regret factor compared to a UCB-like approach. Furthermore, we show that our proof can be readily applied to regularized linear optimization and generalized linear model problems.},
	author = {Abeille, Marc and Lazaric, Alessandro},
	booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	date-modified = {2025-02-06 11:31:09 +0100},
	month = {20--22 Apr},
	pages = {176--184},
	pdf = {http://proceedings.mlr.press/v54/abeille17a/abeille17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {{Linear Thompson Sampling Revisited}},
	volume = {54},
	year = {2017},
	bdsk-url-1 = {https://proceedings.mlr.press/v54/abeille17a.html}}

@article{banerjee2022exploration,
	author = {Banerjee, Debangshu and Ghosh, Avishek and Chowdhury, Sayak Ray and Gopalan, Aditya},
	date-added = {2025-02-06 11:33:32 +0100},
	date-modified = {2025-02-06 11:33:32 +0100},
	journal = {arXiv preprint arXiv:2207.11597},
	title = {Exploration in Linear Bandits with Rich Action Sets and its Implications for Inference},
	year = {2022}}

@inproceedings{dani_stochastic_2008,
	abstract = {In the classical stochastic k-armed bandit problem, in each of a sequence of T rounds, a decision maker chooses one of k arms and incurs a cost chosen from an unknown distribution associated with that arm. The goal is to minimize regret, defined as the difference between the cost incurred by the algorithm and the optimal cost.},
	author = {Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
	booktitle = {Conference on Learning Theory},
	language = {en},
	title = {Stochastic {Linear} {Optimization} under {Bandit} {Feedback}},
	year = {2008}}

@inproceedings{gales2022norm-agn,
	author = {Gales, Spencer B and Sethuraman, Sunder and Jun, Kwang-Sung},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2024-11-29 11:21:15 +0100},
	date-modified = {2024-11-29 11:21:18 +0100},
	organization = {PMLR},
	pages = {73--91},
	title = {Norm-agnostic linear bandits},
	year = {2022}}

@article{jun2024noise,
	author = {Jun, Kwang-Sung and Kim, Jungtaek},
	date-added = {2024-11-29 12:06:22 +0100},
	date-modified = {2024-11-29 12:06:22 +0100},
	journal = {arXiv preprint arXiv:2402.07341},
	title = {Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization},
	year = {2024}}

@book{lattimore_bandit_2020,
	author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
	doi = {10.1017/9781108571401},
	edition = {1},
	isbn = {978-1-108-57140-1 978-1-108-48682-8},
	language = {en},
	month = jul,
	publisher = {Cambridge University Press},
	title = {Bandit {Algorithms}},
	urldate = {2021-01-14},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1017/9781108571401}}

@article{rusmi_linearly_2010,
	abstract = {We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an r-dimensional random vector Z Є R r , where r ≥ 2. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order Θ(r√T), by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form O(r√T log 3/2 T).},
	author = {Paat Rusmevichientong and John N. Tsitsiklis},
	issn = {0364765X, 15265471},
	journal = {Mathematics of Operations Research},
	number = {2},
	pages = {395--411},
	publisher = {INFORMS},
	title = {Linearly Parameterized Bandits},
	volume = {35},
	year = {2010}}

@inproceedings{zhu2022pareto,
	author = {Zhu, Yinglun and Nowak, Robert},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2024-11-29 11:58:42 +0100},
	date-modified = {2024-11-29 11:58:42 +0100},
	organization = {PMLR},
	pages = {6793--6813},
	title = {Pareto optimal model selection in linear bandits},
	year = {2022}}

