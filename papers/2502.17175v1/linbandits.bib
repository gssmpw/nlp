%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Hadiji at 2025-02-06 11:33:38 +0100 


%% Saved with string encoding Unicode (UTF-8) 

@inproceedings{garivier2016explore,
 author = {Garivier, Aurelien and Lattimore, Tor and Kaufmann, Emilie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Explore-Then-Commit strategies},
 volume = {29},
 year = {2016}
}

@article{banerjee2022exploration,
	author = {Banerjee, Debangshu and Ghosh, Avishek and Chowdhury, Sayak Ray and Gopalan, Aditya},
	date-added = {2025-02-06 11:33:32 +0100},
	date-modified = {2025-02-06 11:33:32 +0100},
	journal = {arXiv preprint arXiv:2207.11597},
	title = {Exploration in Linear Bandits with Rich Action Sets and its Implications for Inference},
	year = {2022}}

@book{hoff2009first,
	author = {Hoff, Peter D},
	date-added = {2025-02-06 11:09:11 +0100},
	date-modified = {2025-02-06 11:09:11 +0100},
	publisher = {Springer},
	title = {A first course in Bayesian statistical methods},
	volume = {580},
	year = {2009}}

@article{hadiji2023adaptation,
	author = {Hadiji, H{\'e}di and Stoltz, Gilles},
	date-added = {2024-11-29 15:10:52 +0100},
	date-modified = {2024-11-29 15:10:52 +0100},
	journal = {Journal of Machine Learning Research},
	number = {13},
	pages = {1--33},
	title = {Adaptation to the range in k-armed bandits},
	volume = {24},
	year = {2023}}

@article{jun2024noise,
	author = {Jun, Kwang-Sung and Kim, Jungtaek},
	date-added = {2024-11-29 12:06:22 +0100},
	date-modified = {2024-11-29 12:06:22 +0100},
	journal = {arXiv preprint arXiv:2402.07341},
	title = {Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization},
	year = {2024}}

@inproceedings{zhu2022pareto,
	author = {Zhu, Yinglun and Nowak, Robert},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2024-11-29 11:58:42 +0100},
	date-modified = {2024-11-29 11:58:42 +0100},
	organization = {PMLR},
	pages = {6793--6813},
	title = {Pareto optimal model selection in linear bandits},
	year = {2022}}

@inproceedings{ghosh2021problem,
	author = {Ghosh, Avishek and Sankararaman, Abishek and Kannan, Ramchandran},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2024-11-29 11:22:49 +0100},
	date-modified = {2024-11-29 11:22:49 +0100},
	organization = {PMLR},
	pages = {1396--1404},
	title = {Problem-complexity adaptive model selection for stochastic linear bandits},
	year = {2021}}

@inproceedings{gales2022norm-agn,
	author = {Gales, Spencer B and Sethuraman, Sunder and Jun, Kwang-Sung},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2024-11-29 11:21:15 +0100},
	date-modified = {2024-11-29 11:21:18 +0100},
	organization = {PMLR},
	pages = {73--91},
	title = {Norm-agnostic linear bandits},
	year = {2022}}

@inproceedings{hajek1972local,
	author = {H{\'a}jek, Jaroslav},
	booktitle = {Proceedings of the sixth Berkeley symposium on mathematical statistics and probability},
	date-added = {2024-11-28 13:42:01 +0100},
	date-modified = {2024-11-28 13:42:01 +0100},
	pages = {175--194},
	title = {Local asymptotic minimax and admissibility in estimation},
	volume = {1},
	year = {1972}}

@article{abbasi2011improved,
	author = {Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
	date-added = {2023-11-24 14:45:41 +0100},
	date-modified = {2023-11-24 14:45:41 +0100},
	journal = {Advances in neural information processing systems},
	title = {Improved algorithms for linear stochastic bandits},
	volume = {24},
	year = {2011}}

@inproceedings{dani_stochastic_2008,
	abstract = {In the classical stochastic k-armed bandit problem, in each of a sequence of T rounds, a decision maker chooses one of k arms and incurs a cost chosen from an unknown distribution associated with that arm. The goal is to minimize regret, defined as the difference between the cost incurred by the algorithm and the optimal cost.},
	author = {Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
	booktitle = {Conference on Learning Theory},
	language = {en},
	title = {Stochastic {Linear} {Optimization} under {Bandit} {Feedback}},
	year = {2008}}

@article{rusmi_linearly_2010,
	abstract = {We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an r-dimensional random vector Z Є R r , where r ≥ 2. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order Θ(r√T), by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form O(r√T log 3/2 T).},
	author = {Paat Rusmevichientong and John N. Tsitsiklis},
	issn = {0364765X, 15265471},
	journal = {Mathematics of Operations Research},
	number = {2},
	pages = {395--411},
	publisher = {INFORMS},
	title = {Linearly Parameterized Bandits},
	volume = {35},
	year = {2010}}

@book{lattimore_bandit_2020,
	author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
	doi = {10.1017/9781108571401},
	edition = {1},
	isbn = {978-1-108-57140-1 978-1-108-48682-8},
	language = {en},
	month = jul,
	publisher = {Cambridge University Press},
	title = {Bandit {Algorithms}},
	urldate = {2021-01-14},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1017/9781108571401}}

@article{laurent2000,
	author = {B. Laurent and P. Massart},
	date-modified = {2025-02-06 11:30:49 +0100},
	doi = {10.1214/aos/1015957395},
	journal = {The Annals of Statistics},
	keywords = {$l_p$-bodies, adaptive estimation, Besov bodies, efficient estimation, Gaussian sequence model, Model selection, quadratic functionals},
	number = {5},
	pages = {1302 -- 1338},
	publisher = {Institute of Mathematical Statistics},
	title = {{Adaptive estimation of a quadratic functional by model selection}},
	volume = {28},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1015957395}}

@misc{vershynin2011,
	archiveprefix = {arXiv},
	author = {Roman Vershynin},
	eprint = {1011.3027},
	primaryclass = {math.PR},
	title = {Introduction to the non-asymptotic analysis of random matrices},
	url = {https://arxiv.org/abs/1011.3027},
	year = {2011},
	bdsk-url-1 = {https://arxiv.org/abs/1011.3027}}

@misc{hsu_tail_2011,
	abstract = {We prove an exponential probability tail inequality for positive semidefinite quadratic forms in a subgaussian random vector. The bound is analogous to one that holds when the vector has independent Gaussian entries.},
	author = {Hsu, Daniel and Kakade, Sham M. and Zhang, Tong},
	date-modified = {2025-02-06 11:09:30 +0100},
	doi = {10.48550/arXiv.1110.2842},
	file = {Preprint PDF:/home/zhang/Zotero/storage/JERCQGPZ/Hsu et al. - 2011 - A tail inequality for quadratic forms of subgaussian random vectors.pdf:application/pdf;Snapshot:/home/zhang/Zotero/storage/SLMN44LE/1110.html:text/html},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability},
	month = oct,
	note = {arXiv:1110.2842},
	publisher = {arXiv},
	title = {A tail inequality for quadratic forms of subgaussian random vectors},
	urldate = {2024-10-30},
	year = {2011},
	bdsk-url-1 = {http://arxiv.org/abs/1110.2842},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1110.2842}}

@inproceedings{abeille_2017,
	abstract = {We derive an alternative proof for the regret of Thompson sampling (TS) in the stochastic linear bandit setting. While we obtain a regret bound of order $O(d^3/2\sqrtT)$ as in previous results, the proof sheds new light on the functioning of the TS. We leverage on the structure of the problem to show how the regret is related to the sensitivity (i.e., the gradient) of the objective function and how selecting optimal arms associated to \textitoptimistic parameters does control it. Thus we show that TS can be seen as a generic randomized algorithm where the sampling distribution is designed to have a fixed probability of being optimistic, at the cost of an additional $\sqrtd$ regret factor compared to a UCB-like approach. Furthermore, we show that our proof can be readily applied to regularized linear optimization and generalized linear model problems.},
	author = {Abeille, Marc and Lazaric, Alessandro},
	booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	date-modified = {2025-02-06 11:31:09 +0100},
	month = {20--22 Apr},
	pages = {176--184},
	pdf = {http://proceedings.mlr.press/v54/abeille17a/abeille17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {{Linear Thompson Sampling Revisited}},
	volume = {54},
	year = {2017},
	bdsk-url-1 = {https://proceedings.mlr.press/v54/abeille17a.html}}

@misc{gurobi,
	author = {{Gurobi Optimization, LLC}},
	title = {{Gurobi Optimizer Reference Manual}},
	url = {https://www.gurobi.com},
	year = 2024,
	bdsk-url-1 = {https://www.gurobi.com}}
