\section{Related Work}
\label{ssec:Related Work}

We now summarize the current state of the art on the problem of stochastic linear
bandits with i.i.d.\ subgaussian rewards.
For fair comparison, we first highlight the difference in assumptions made by
authors, and whenever those assumptions differ from ours, we specialize their results
to our setting.

\textbf{Different settings} \cite{dani_stochastic_2008} consider an arbitrary action set $\mathcal{X}$ and assume that $\max_{x \in \mathcal{X}} |\theta^\top x| \le 1$, which in our setting is equivalent to $|c^\top \theta + \|\theta\|_{A}| \le 1$. 
\cite{rusmi_linearly_2010} consider an arbitrary compact action set $\mathcal{X}$, but they assume intricate conditions on its curvature, see below. 
\cite{abbasi2011improved} consider an arbitrary, possibly time-dependent, action set $\mathcal{X}_t$ and assume $\max_{x \in \mathcal{X}_t} |\theta^\top x | \le 1$, which in our setting is equivalent to $|c^\top \theta + \|\theta\|_{A}| \le 1$. 
They further assume that an upper bound for $\|\theta\|_{2}$ is known and provided as an input to the algorithm. 
\cite{abeille_2017} consider an action set $\mathcal{X}$ which is an arbitrary closed subset of the unit ball, and also assume that an upper bound for $\|\theta\|_{2}$ is known and provided as an input to the algorithm. 

\textbf{Regret upper bounds } The two versions of the \gls{CB} (\algoname{ConfidenceBall}) algorithm of~\cite{dani_stochastic_2008} achieve regret upper bounds of $O( d \sqrt{ T \log(T)^{3} })$ and $O( d^{3/2} \sqrt{ T \log(T)^{3} })$ in our setting.
The \gls{PEGE} (\algoname{Phased Exploration and Greedy Exploitation}) algorithm of~\cite{rusmi_linearly_2010} achieves a regret of $O( ( \|\theta\|_{2} + 1/\|\theta\|_{2} ) d \sqrt{T})$ when $\mathcal{X}$ is a unit ball. 
The \gls{UE} (\algoname{Uncertainty
Ellipsoid})  algorithm of \cite{rusmi_linearly_2010} achieves a regret of $O( f(\mathcal{X}) d \sqrt{T \log(T)^3})$ when $\mathcal{X}$ is an ellipsoid where $f$ has an intricate dependency on the curvature of $\mathcal{X}$, and can only be bounded by a universal constant if the maximal ratio of eigenvalues of $A$, $|\gls{lambdamax}(A)/\sqrt{\gls{lambdamin}(A)}|$ is bounded by another universal constant, so that $\mathcal{X}$ is "close" to the unit ball.
The \gls{OFUL} (\algoname{Optimism in the Face of Uncertainty Linear bandit}) algorithm of~\cite{abbasi2011improved} achieves a regret $O( (\|\theta\|_{2} + \sigma \sqrt{d} \log(T) ) \sqrt{d T \log(T)})$ assuming that the algorithm is given an upper bound on $\|\theta\|_{2}$ as prior information in order to set the algorithm parameters correctly. 
The \gls{TS} (\algoname{Thompson Sampling}) algorithm of~\cite{abeille_2017} achieves a regret of $O( \sigma d^{3/2}\sqrt{T \log(dT)^3 })$ assuming that the algorithm is given an upper bound on $\|\theta\|_{2}$ as prior information in order to set the algorithm parameters correctly. 
The \gls{OLSOFUL} (\algoname{Ordinary Least Squares OFUL}) algorithm of~\cite{gales2022norm-agn} achieves a regret of $O( \sigma d \sqrt{ T \log(T/d)^2 } + \|\theta\|_{2}d \log(d) )$ without prior information on $\theta$. 
Therefore, the two versions of \algoname{CB}, \algoname{PEGE}, \algoname{UE}, \algoname{OFUL}, \algoname{OLSOFUL},  \algoname{TS} are not provably minimax optimal, in contrast with~\nous.

\textbf{Regret lower bounds } \cite{rusmi_linearly_2010} show that the minimax regret of any algorithm is at least $\Omega(d \sqrt{T})$ when the set of actions is the unit ball, using a Bayesian approach. 
\cite{lattimore_bandit_2020} show the same lower bound in a more general setting, using an Assouad-style construction. 
Our locally asymptotically minimax lower bound is a stronger result. with the correct
dependency on $d, \sigma, T$ and $\|\theta\|_{A}$. 

\textbf{Computational complexity} To the best of our knowledge, even when $\mathcal{X}$ is a unit ball, none of the optimistic algorithms such as \algoname{CB}, \algoname{UE}, \algoname{OFUL} and \algoname{OLSOFUL} cannot be implemented in polynomial time, because they must repeatedly maximize the bilinear function $x^\top \theta$ over the couple  $(x,\theta) \in \mathcal{X} \times \mathcal{C}_t$ where \gls{Ct} is the confidence ellipsoid computed at time $t$. 
To the best of our knowledge, no efficient algorithm is known to solve this bilinear optimitzation problem when $\mathcal{X}$ is an ellipsoid. 
The only case in which this is feasible in polynomial time is when $\mathcal{X}$ is finite with polynomial size in $d$. 
This is corroborated by our numerical experiments. 
Both \algoname{PEGE}, \algoname{TS} can be implemented in polynomial time, as they require to minimize a linear function over $\mathcal{X}$, and/or to compute the least-squares estimate of $\theta$. 
Our results show that there is no regret/complexity trade-off in linear bandits, in the sense that~\nous~achieves both minimax regret, and low computational complexity.  

\textbf{Related problems} 
Other authors have considered different but related problems in linear bandits, for
instance: \cite{zhu2022pareto} consider the case where $\theta$ is sparse,
\citet{banerjee2022exploration} lower bound asymptotically the eigenvalues of the
expected design matrix, \cite{jun2024noise} consider the case where the variance of the
reward from the optimal decision is small, and \cite{lattimore_bandit_2020} consider
the adversarial case (we refer the reader to their discussion of the major differences
between stochastic and adversarial linear bandits).