\section{Related Work}
\paragraph{Adversarial Attacks on DRL Agents}
The vulnerability of DRL agents to adversarial attacks was first highlighted by ____, who demonstrated the susceptibility of DRL policies to Fast Gradient Sign Method (FGSM) attacks____ in Atari games. This foundational work sparked further research into various attack methods and robust policies.
Following this, ____ introduced limited-step attacks to deceive DRL policies,
while ____ further explored these vulnerabilities by employing a critic action-value function and gradient descent to undermine DRL performance.
Additionally, ____ proposed black-box attacks on DQN and verified the transferability of adversarial examples across different models.
____ showed that even adversaries with restricted access to only action and reward signals could execute highly effective and damaging attacks. 
For continuous control agents, ____ developed a two-step attack algorithm based on learned model dynamics.
____ developed learned adversaries by training attackers as RL agents, resulting in SA-RL and PA-AD attacks. 

Research by____ further explored backdoor attacks in reinforcement learning, uncovering significant vulnerabilities.
In a novel approach, ____ introduced an adversarial cheap talk setting and trained an adversary through meta-learning. 
____ analyzed adversarial directions in the Arcade Learning Environment and found that even state-of-the-art robust agents____ remain vulnerable to policy-independent sensitivity directions.
____ used dual ascent to learn an illusory attack end-to-end.
____ further studied the impact of adversarial policies in multi-agent scenarios.
Lastly, ____ proposed a temporally-coupled attack, further degrading the performance of robust agents.
This body of work underscores the ongoing challenge of enhancing the adversarial robustness of DRL agents and highlights the need for continued research in this critical area.

\paragraph{Adversarial Robust Policy for DRL Agents}
Earlier studies by____ incorporated adversarial states into the replay buffer during training in Atari environments, resulting in limited robustness. 
____ proposed separating the DQN architecture into a Q-network and a policy network, robustly training the policy network with generated adversarial states and provably robust bounds.
____ characterized state-adversarial RL as SA-MDP and revealed the potential non-existence of the ORP. They addressed this challenge by balancing robustness and natural returns through a KL-based regularization.
____ leveraged robustness certification bounds to design the adversarial loss and combined it with the vanilla training loss.
____ improved training efficiency by estimating the worst-case value estimation and combining it with classic Temporal Difference~(TD)-target____ or Generalized Advantage Estimation~(GAE)____.
____ built the DRL architecture for discrete action spaces upon SortNet____, enabling global Lipschitz continuity and reducing the need for training extra attackers or finding adversaries. 
Recently, ____ proposed learning a pessimistic discrete policy combined with belief state inference and diffusion-based purification.
Prior methods often constrained local smoothness or invariance heuristically to achieve commendable robustness, sometimes compromising natural performance. In contrast, our approach seeks optimal robust policies with strict theoretical guarantees, simultaneously improving both natural and robust performance.

____ found that smooth regularization can enhance both natural performance and robustness for TRPO____ and DDPG____.
____ used Randomized Smoothing~(RS) to enable certifiable robustness.
The latest work by ____ introduced a novel smoothing strategy to address the overestimation of robustness.
Moreover, ____ proposed an adaptive defense based on a family of non-dominated policies during the testing phase. 
In multi-agent settings, ____ analyzed state adversaries in a Markov Game and proposed robust multi-agent Q-learning and actor-critic methods to solve the robust equilibrium.
____ extended robustness regularization____ to multi-agent environments by considering a sub-optimal Lipschitz policy in smooth environments. 
____ proposed adversarial training with two timescales for effective convergence to a robust policy.
Another line of research focuses on alternated training for agents with learned adversaries____, further developed in a game-theoretic framework by____.
This body of work underscores the importance of developing robust DRL policies and highlights the progress and challenges in enhancing the adversarial robustness of DRL agents.