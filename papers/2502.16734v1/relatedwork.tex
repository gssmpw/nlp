\section{Related Work}
\paragraph{Adversarial Attacks on DRL Agents}
The vulnerability of DRL agents to adversarial attacks was first highlighted by \cite{huang2017adversarial}, who demonstrated the susceptibility of DRL policies to Fast Gradient Sign Method (FGSM) attacks~\citep{goodfellow2014explaining} in Atari games. This foundational work sparked further research into various attack methods and robust policies.
Following this, \cite{lin2017tactics, kos2017delving} introduced limited-step attacks to deceive DRL policies,
while \cite{pattanaik2017robust} further explored these vulnerabilities by employing a critic action-value function and gradient descent to undermine DRL performance.
Additionally, \cite{behzadan2017vulnerability} proposed black-box attacks on DQN and verified the transferability of adversarial examples across different models.
\cite{inkawhich2019snooping} showed that even adversaries with restricted access to only action and reward signals could execute highly effective and damaging attacks. 
For continuous control agents, \cite{weng2019toward} developed a two-step attack algorithm based on learned model dynamics.
\cite{zhang2021robust, sun2021strongest} developed learned adversaries by training attackers as RL agents, resulting in SA-RL and PA-AD attacks. 

Research by~\cite{kiourti2020trojdrl, wang2021backdoorl, bharti2022provable, guo2023policycleanse} further explored backdoor attacks in reinforcement learning, uncovering significant vulnerabilities.
In a novel approach, \cite{lu2023adversarial} introduced an adversarial cheap talk setting and trained an adversary through meta-learning. 
\cite{korkmaz2023adversarial} analyzed adversarial directions in the Arcade Learning Environment and found that even state-of-the-art robust agents~\citep{zhang2020robust, oikarinen2021robust} remain vulnerable to policy-independent sensitivity directions.
\cite{franzmeyerillusory} used dual ascent to learn an illusory attack end-to-end.
\cite{gleave2019adversarial} further studied the impact of adversarial policies in multi-agent scenarios.
Lastly, \cite{liang2023game} proposed a temporally-coupled attack, further degrading the performance of robust agents.
This body of work underscores the ongoing challenge of enhancing the adversarial robustness of DRL agents and highlights the need for continued research in this critical area.

\paragraph{Adversarial Robust Policy for DRL Agents}
Earlier studies by~\cite{kos2017delving, behzadan2017whatever} incorporated adversarial states into the replay buffer during training in Atari environments, resulting in limited robustness. 
\cite{fischer2019online} proposed separating the DQN architecture into a Q-network and a policy network, robustly training the policy network with generated adversarial states and provably robust bounds.
\cite{zhang2020robust} characterized state-adversarial RL as SA-MDP and revealed the potential non-existence of the ORP. They addressed this challenge by balancing robustness and natural returns through a KL-based regularization.
\cite{oikarinen2021robust} leveraged robustness certification bounds to design the adversarial loss and combined it with the vanilla training loss.
\cite{liang2022efficient} improved training efficiency by estimating the worst-case value estimation and combining it with classic Temporal Difference~(TD)-target~\citep{sutton1988learning} or Generalized Advantage Estimation~(GAE)~\citep{schulman2015high}.
\cite{nie2023improve} built the DRL architecture for discrete action spaces upon SortNet~\citep{zhang2022rethinking}, enabling global Lipschitz continuity and reducing the need for training extra attackers or finding adversaries. 
Recently, \cite{sun2024belief} proposed learning a pessimistic discrete policy combined with belief state inference and diffusion-based purification.
Prior methods often constrained local smoothness or invariance heuristically to achieve commendable robustness, sometimes compromising natural performance. In contrast, our approach seeks optimal robust policies with strict theoretical guarantees, simultaneously improving both natural and robust performance.

\cite{shen2020deep} found that smooth regularization can enhance both natural performance and robustness for TRPO~\citep{schulman2015trust} and DDPG~\citep{silver2014deterministic}.
\cite{wu2021crop, kumar2021policy} used Randomized Smoothing~(RS) to enable certifiable robustness.
The latest work by \cite{sun2024breaking} introduced a novel smoothing strategy to address the overestimation of robustness.
Moreover, \cite{liu2024beyond} proposed an adaptive defense based on a family of non-dominated policies during the testing phase. 
In multi-agent settings, \cite{he2023robust} analyzed state adversaries in a Markov Game and proposed robust multi-agent Q-learning and actor-critic methods to solve the robust equilibrium.
\cite{bukharin2024robust} extended robustness regularization~\citep{shen2020deep, zhang2020robust} to multi-agent environments by considering a sub-optimal Lipschitz policy in smooth environments. 
\cite{liu2023rethinking} proposed adversarial training with two timescales for effective convergence to a robust policy.
Another line of research focuses on alternated training for agents with learned adversaries~\citep{zhang2021robust, sun2021strongest}, further developed in a game-theoretic framework by~\citep{liang2023game}.
This body of work underscores the importance of developing robust DRL policies and highlights the progress and challenges in enhancing the adversarial robustness of DRL agents.