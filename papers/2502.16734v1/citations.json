[
  {
    "index": 0,
    "papers": [
      {
        "key": "huang2017adversarial",
        "author": "Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter",
        "title": "Adversarial attacks on neural network policies"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "goodfellow2014explaining",
        "author": "Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian",
        "title": "Explaining and harnessing adversarial examples"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lin2017tactics",
        "author": "Yen-Chen Lin and Zhang-Wei Hong and Yuan-Hong Liao and Meng-Li Shih and Ming-Yu Liu and Min Sun",
        "title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents"
      },
      {
        "key": "kos2017delving",
        "author": "Kos, Jernej and Song, Dawn",
        "title": "Delving into adversarial attacks on deep policies"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "pattanaik2017robust",
        "author": "Anay Pattanaik and Zhenyi Tang and Shuijing Liu and Gautham Bommannan and Girish Chowdhary",
        "title": "Robust Deep Reinforcement Learning with Adversarial Attacks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "behzadan2017vulnerability",
        "author": "Behzadan, Vahid and Munir, Arslan",
        "title": "Vulnerability of deep reinforcement learning to policy induction attacks"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "inkawhich2019snooping",
        "author": "Inkawhich, Matthew and Chen, Yiran and Li, Hai",
        "title": "Snooping Attacks on Deep Reinforcement Learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "weng2019toward",
        "author": "Weng, Tsui-Wei and Dvijotham, Krishnamurthy Dj and Uesato, Jonathan and Xiao, Kai and Gowal, Sven and Stanforth, Robert and Kohli, Pushmeet",
        "title": "Toward evaluating robustness of deep reinforcement learning with continuous control"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2021robust",
        "author": "Huan Zhang and Hongge Chen and Duane S Boning and Cho-Jui Hsieh",
        "title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary"
      },
      {
        "key": "sun2021strongest",
        "author": "Yanchao Sun and Ruijie Zheng and Yongyuan Liang and Furong Huang",
        "title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep {RL}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kiourti2020trojdrl",
        "author": "Kiourti, Panagiota and Wardega, Kacper and Jha, Susmit and Li, Wenchao",
        "title": "TrojDRL: evaluation of backdoor attacks on deep reinforcement learning"
      },
      {
        "key": "wang2021backdoorl",
        "author": "Lun Wang and Zaynah Javed and Xian Wu and Wenbo Guo and Xinyu Xing and Dawn Song",
        "title": "BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning"
      },
      {
        "key": "bharti2022provable",
        "author": "Bharti, Shubham and Zhang, Xuezhou and Singla, Adish and Zhu, Jerry",
        "title": "Provable Defense against Backdoor Policies in Reinforcement Learning"
      },
      {
        "key": "guo2023policycleanse",
        "author": "Guo, Junfeng and Li, Ang and Wang, Lixu and Liu, Cong",
        "title": "Policycleanse: Backdoor detection and mitigation for competitive reinforcement learning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lu2023adversarial",
        "author": "Lu, Chris and Willi, Timon and Letcher, Alistair and Foerster, Jakob Nicolaus",
        "title": "Adversarial cheap talk"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "korkmaz2023adversarial",
        "author": "Korkmaz, Ezgi",
        "title": "Adversarial robust deep reinforcement learning requires redefining robustness"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang2020robust",
        "author": "Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Li, Bo and Liu, Mingyan and Boning, Duane and Hsieh, Cho-Jui",
        "title": "Robust deep reinforcement learning against adversarial perturbations on state observations"
      },
      {
        "key": "oikarinen2021robust",
        "author": "Oikarinen, Tuomas and Zhang, Wang and Megretski, Alexandre and Daniel, Luca and Weng, Tsui-Wei",
        "title": "Robust deep reinforcement learning through adversarial loss"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "franzmeyerillusory",
        "author": "Franzmeyer, Tim and McAleer, Stephen Marcus and Henriques, Joao F and Foerster, Jakob Nicolaus and Torr, Philip and Bibi, Adel and de Witt, Christian Schroeder",
        "title": "Illusory Attacks: Information-theoretic detectability matters in adversarial attacks"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gleave2019adversarial",
        "author": "Adam Gleave and Michael Dennis and Cody Wild and Neel Kant and Sergey Levine and Stuart Russell",
        "title": "Adversarial Policies: Attacking Deep Reinforcement Learning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "liang2023game",
        "author": "Yongyuan Liang and Yanchao Sun and Ruijie Zheng and Xiangyu Liu and Benjamin Eysenbach and Tuomas Sandholm and Furong Huang and Stephen Marcus McAleer",
        "title": "Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "kos2017delving",
        "author": "Kos, Jernej and Song, Dawn",
        "title": "Delving into adversarial attacks on deep policies"
      },
      {
        "key": "behzadan2017whatever",
        "author": "Behzadan, Vahid and Munir, Arslan",
        "title": "Whatever does not kill deep reinforcement learning, makes it stronger"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "fischer2019online",
        "author": "Fischer, Marc and Mirman, Matthew and Stalder, Steven and Vechev, Martin",
        "title": "Online robustness training for deep reinforcement learning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhang2020robust",
        "author": "Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Li, Bo and Liu, Mingyan and Boning, Duane and Hsieh, Cho-Jui",
        "title": "Robust deep reinforcement learning against adversarial perturbations on state observations"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "oikarinen2021robust",
        "author": "Oikarinen, Tuomas and Zhang, Wang and Megretski, Alexandre and Daniel, Luca and Weng, Tsui-Wei",
        "title": "Robust deep reinforcement learning through adversarial loss"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "liang2022efficient",
        "author": "Liang, Yongyuan and Sun, Yanchao and Zheng, Ruijie and Huang, Furong",
        "title": "Efficient adversarial training without attacking: Worst-case-aware robust reinforcement learning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "sutton1988learning",
        "author": "Sutton, Richard S",
        "title": "Learning to predict by the methods of temporal differences"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "schulman2015high",
        "author": "Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter",
        "title": "High-dimensional continuous control using generalized advantage estimation"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "nie2023improve",
        "author": "Nie, Buqing and Ji, Jingtian and Fu, Yangqing and Gao, Yue",
        "title": "Improve robustness of reinforcement learning against observation perturbations via $ l\\_\\infty $ lipschitz policy networks"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "zhang2022rethinking",
        "author": "Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei",
        "title": "Rethinking lipschitz neural networks and certified robustness: A boolean function perspective"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "sun2024belief",
        "author": "Xiaolin Sun and Zizhan Zheng",
        "title": "Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "shen2020deep",
        "author": "Shen, Qianli and Li, Yan and Jiang, Haoming and Wang, Zhaoran and Zhao, Tuo",
        "title": "Deep reinforcement learning with robust and smooth policy"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "schulman2015trust",
        "author": "Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp",
        "title": "Trust region policy optimization"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "silver2014deterministic",
        "author": "Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin",
        "title": "Deterministic policy gradient algorithms"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "wu2021crop",
        "author": "Fan Wu and Linyi Li and Zijian Huang and Yevgeniy Vorobeychik and Ding Zhao and Bo Li",
        "title": "{CROP}: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing"
      },
      {
        "key": "kumar2021policy",
        "author": "Aounon Kumar and Alexander Levine and Soheil Feizi",
        "title": "Policy Smoothing for Provably Robust Reinforcement Learning"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "sun2024breaking",
        "author": "Sun, Chung-En and Gao, Sicun and Weng, Tsui-Wei",
        "title": "Breaking the Barrier: Enhanced Utility and Robustness in Smoothed {DRL} Agents"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "liu2024beyond",
        "author": "Xiangyu Liu and Chenghao Deng and Yanchao Sun and Yongyuan Liang and Furong Huang",
        "title": "Beyond Worst-case Attacks: Robust {RL} with Adaptive Defense via Non-dominated Policies"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "he2023robust",
        "author": "Sihong He and Songyang Han and Sanbao Su and Shuo Han and Shaofeng Zou and Fei Miao",
        "title": "Robust Multi-Agent Reinforcement Learning with State Uncertainty"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "bukharin2024robust",
        "author": "Bukharin, Alexander and Li, Yan and Yu, Yue and Zhang, Qingru and Chen, Zhehui and Zuo, Simiao and Zhang, Chao and Zhang, Songan and Zhao, Tuo",
        "title": "Robust multi-agent reinforcement learning via adversarial regularization: Theoretical foundation and stable algorithms"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "shen2020deep",
        "author": "Shen, Qianli and Li, Yan and Jiang, Haoming and Wang, Zhaoran and Zhao, Tuo",
        "title": "Deep reinforcement learning with robust and smooth policy"
      },
      {
        "key": "zhang2020robust",
        "author": "Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Li, Bo and Liu, Mingyan and Boning, Duane and Hsieh, Cho-Jui",
        "title": "Robust deep reinforcement learning against adversarial perturbations on state observations"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "liu2023rethinking",
        "author": "Xiangyu Liu and Souradip Chakraborty and Yanchao Sun and Furong Huang",
        "title": "Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in {RL}"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "zhang2021robust",
        "author": "Huan Zhang and Hongge Chen and Duane S Boning and Cho-Jui Hsieh",
        "title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary"
      },
      {
        "key": "sun2021strongest",
        "author": "Yanchao Sun and Ruijie Zheng and Yongyuan Liang and Furong Huang",
        "title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep {RL}"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "liang2023game",
        "author": "Yongyuan Liang and Yanchao Sun and Ruijie Zheng and Xiangyu Liu and Benjamin Eysenbach and Tuomas Sandholm and Furong Huang and Stephen Marcus McAleer",
        "title": "Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"
      }
    ]
  }
]