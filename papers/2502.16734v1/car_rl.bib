@InProceedings{li2024towards,
  title = 	 {Towards Optimal Adversarial Robust Q-learning with {B}ellman Infinity-error},
  author =       {Li, Haoran and Zhang, Zicheng and Luo, Wang and Han, Congying and Hu, Yudong and Guo, Tiande and Liao, Shichen},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {29324--29372},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24cl/li24cl.pdf},
  url = 	 {https://proceedings.mlr.press/v235/li24cl.html},
  abstract = 	 {Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CAR-DQN) by minimizing a surrogate of Bellman Infinity-error. The top-tier performance of CAR-DQN across various benchmarks validates its practical effectiveness and reinforces the soundness of our theoretical analysis.}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@inproceedings{lillicrap2015continuous,
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  title={Continuous control with deep reinforcement learning},
  year={2016},
  cdate={1451606400000},
  url={https://openreview.net/forum?id=kJP8gA8BxRY},
  booktitle={International Conference on Learning Representations}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}
@article{ibarz2021train,
  title={How to train your robot with deep reinforcement learning: lessons we have learned},
  author={Ibarz, Julian and Tan, Jie and Finn, Chelsea and Kalakrishnan, Mrinal and Pastor, Peter and Levine, Sergey},
  journal={The International Journal of Robotics Research},
  volume={40},
  number={4-5},
  pages={698--721},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{kiran2021deep,
  title={Deep reinforcement learning for autonomous driving: A survey},
  author={Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A and Yogamani, Senthil and P{\'e}rez, Patrick},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={23},
  number={6},
  pages={4909--4926},
  year={2021},
  publisher={IEEE}
}
@article{yu2021reinforcement,
  title={Reinforcement learning in healthcare: A survey},
  author={Yu, Chao and Liu, Jiming and Nemati, Shamim and Yin, Guosheng},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={1},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY}
}
@inproceedings{zheng2018drn,
author = {Zheng, Guanjie and Zhang, Fuzheng and Zheng, Zihan and Xiang, Yang and Yuan, Nicholas Jing and Xie, Xing and Li, Zhenhui},
title = {DRN: A Deep Reinforcement Learning Framework for News Recommendation},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3185994},
doi = {10.1145/3178876.3185994},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {167â€“176},
numpages = {10},
keywords = {deep Q-Learning, news recommendation, reinforcement learning},
location = {Lyon, France},
series = {WWW '18}
}
@article{huang2017adversarial,
  title={Adversarial attacks on neural network policies},
  author={Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1702.02284},
  year={2017}
}
@inproceedings{behzadan2017vulnerability,
  title={Vulnerability of deep reinforcement learning to policy induction attacks},
  author={Behzadan, Vahid and Munir, Arslan},
  booktitle={Machine Learning and Data Mining in Pattern Recognition: 13th International Conference, MLDM 2017, New York, NY, USA, July 15-20, 2017, Proceedings 13},
  pages={262--275},
  year={2017},
  organization={Springer}
}
@inproceedings{lin2017tactics,
  author={Yen-Chen Lin and Zhang-Wei Hong and Yuan-Hong Liao and Meng-Li Shih and Ming-Yu Liu and Min Sun},
  title={Tactics of Adversarial Attack on Deep Reinforcement Learning Agents},
  year={2017},
  cdate={1483228800000},
  pages={3756-3762},
  url={https://doi.org/10.24963/ijcai.2017/525},
  booktitle={International Joint Conference on Artificial Intelligence}
}
@article{ilahi2021challenges,
  title={Challenges and countermeasures for adversarial attacks on deep reinforcement learning},
  author={Ilahi, Inaam and Usama, Muhammad and Qadir, Junaid and Janjua, Muhammad Umar and Al-Fuqaha, Ala and Hoang, Dinh Thai and Niyato, Dusit},
  journal={IEEE Transactions on Artificial Intelligence},
  volume={3},
  number={2},
  pages={90--109},
  year={2021},
  publisher={IEEE}
}

@article{zhang2020robust,
  title={Robust deep reinforcement learning against adversarial perturbations on state observations},
  author={Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Li, Bo and Liu, Mingyan and Boning, Duane and Hsieh, Cho-Jui},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21024--21037},
  year={2020}
}
@article{oikarinen2021robust,
  title={Robust deep reinforcement learning through adversarial loss},
  author={Oikarinen, Tuomas and Zhang, Wang and Megretski, Alexandre and Daniel, Luca and Weng, Tsui-Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={26156--26167},
  year={2021}
}
@article{liang2022efficient,
  title={Efficient adversarial training without attacking: Worst-case-aware robust reinforcement learning},
  author={Liang, Yongyuan and Sun, Yanchao and Zheng, Ruijie and Huang, Furong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22547--22561},
  year={2022}
}
@inproceedings{zhang2021robust,
    title={Robust Reinforcement Learning on State Observations with Learned Optimal Adversary},
    author={Huan Zhang and Hongge Chen and Duane S Boning and Cho-Jui Hsieh},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=sCZbhBvqQaU}
}
@inproceedings{sun2021strongest,
    title={Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep {RL}},
    author={Yanchao Sun and Ruijie Zheng and Yongyuan Liang and Furong Huang},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=JM2kFbJvvI}
}
@article{bukharin2024robust,
  title={Robust multi-agent reinforcement learning via adversarial regularization: Theoretical foundation and stable algorithms},
  author={Bukharin, Alexander and Li, Yan and Yu, Yue and Zhang, Qingru and Chen, Zhehui and Zuo, Simiao and Zhang, Chao and Zhang, Songan and Zhao, Tuo},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{wang20222,
  title={Is $L^{2}$ Physics Informed Loss Always Suitable for Training Physics Informed Neural Network?},
  author={Wang, Chuwei and Li, Shanda and He, Di and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8278--8290},
  year={2022}
}
@inproceedings{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  year={2015},
  booktitle={International Conference on Learning Representations}
}
@inproceedings{madry2017towards,
    title={Towards Deep Learning Models Resistant to Adversarial Attacks},
    author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=rJzIBfZAb},
}
@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International Conference on Machine Learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}
@inproceedings{wang2016dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International Conference on Machine Learning},
  pages={1995--2003},
  year={2016},
  organization={PMLR}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}
@article{kos2017delving,
  title={Delving into adversarial attacks on deep policies},
  author={Kos, Jernej and Song, Dawn},
  journal={arXiv preprint arXiv:1705.06452},
  year={2017}
}
@inproceedings{pattanaik2017robust,
  author={Anay Pattanaik and Zhenyi Tang and Shuijing Liu and Gautham Bommannan and Girish Chowdhary},
  title={Robust Deep Reinforcement Learning with Adversarial Attacks},
  year={2018},
  cdate={1514764800000},
  pages={2040-2042},
  url={http://dl.acm.org/citation.cfm?id=3238064},
  booktitle={International Conference on Autonomous Agents and Multiagent Systems}
}
@inproceedings{gleave2019adversarial,
    title={Adversarial Policies: Attacking Deep Reinforcement Learning},
    author={Adam Gleave and Michael Dennis and Cody Wild and Neel Kant and Sergey Levine and Stuart Russell},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=HJgEMpVFwB}
}
@inproceedings{inkawhich2019snooping,
    author = {Inkawhich, Matthew and Chen, Yiran and Li, Hai},
    title = {Snooping Attacks on Deep Reinforcement Learning},
    year = {2020},
    isbn = {9781450375184},
    publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
    address = {Richland, SC},
    booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
    pages = {557â€“565},
    numpages = {9},
    keywords = {security, machine learning, deep reinforcement learning},
    location = {Auckland, New Zealand},
    series = {AAMAS '20}
}
@inproceedings{shen2020deep,
  title={Deep reinforcement learning with robust and smooth policy},
  author={Shen, Qianli and Li, Yan and Jiang, Haoming and Wang, Zhaoran and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={8707--8718},
  year={2020},
  organization={PMLR}
}
@article{fischer2019online,
  title={Online robustness training for deep reinforcement learning},
  author={Fischer, Marc and Mirman, Matthew and Stalder, Steven and Vechev, Martin},
  journal={arXiv preprint arXiv:1911.00887},
  year={2019}
}
@article{behzadan2017whatever,
  title={Whatever does not kill deep reinforcement learning, makes it stronger},
  author={Behzadan, Vahid and Munir, Arslan},
  journal={arXiv preprint arXiv:1712.09344},
  year={2017}
}
@inproceedings{lu2023adversarial,
  title={Adversarial cheap talk},
  author={Lu, Chris and Willi, Timon and Letcher, Alistair and Foerster, Jakob Nicolaus},
  booktitle={International Conference on Machine Learning},
  pages={22917--22941},
  year={2023},
  organization={PMLR}
}
@article{bharti2022provable,
  title={Provable Defense against Backdoor Policies in Reinforcement Learning},
  author={Bharti, Shubham and Zhang, Xuezhou and Singla, Adish and Zhu, Jerry},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14704--14714},
  year={2022}
}
@inproceedings{kiourti2020trojdrl,
  title={TrojDRL: evaluation of backdoor attacks on deep reinforcement learning},
  author={Kiourti, Panagiota and Wardega, Kacper and Jha, Susmit and Li, Wenchao},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}
@inproceedings{wang2021backdoorl,
  author={Lun Wang and Zaynah Javed and Xian Wu and Wenbo Guo and Xinyu Xing and Dawn Song},
  title={BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning},
  year={2021},
  cdate={1609459200000},
  pages={3699-3705},
  url={https://doi.org/10.24963/ijcai.2021/509},
  booktitle={International Joint Conference on Artificial Intelligence}
}
@inproceedings{guo2023policycleanse,
  title={Policycleanse: Backdoor detection and mitigation for competitive reinforcement learning},
  author={Guo, Junfeng and Li, Ang and Wang, Lixu and Liu, Cong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4699--4708},
  year={2023}
}
@inproceedings{korkmaz2023adversarial,
  title={Adversarial robust deep reinforcement learning requires redefining robustness},
  author={Korkmaz, Ezgi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={8369--8377},
  year={2023}
}
@inproceedings{nie2023improve,
  title={Improve robustness of reinforcement learning against observation perturbations via $ l\_\infty $ lipschitz policy networks},
  author={Nie, Buqing and Ji, Jingtian and Fu, Yangqing and Gao, Yue},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={14457--14465},
  year={2024}
}
@article{zhang2022rethinking,
  title={Rethinking lipschitz neural networks and certified robustness: A boolean function perspective},
  author={Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19398--19413},
  year={2022}
}
@article{he2023robust,
    title={Robust Multi-Agent Reinforcement Learning with State Uncertainty},
    author={Sihong He and Songyang Han and Sanbao Su and Shuo Han and Shaofeng Zou and Fei Miao},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2023},
    url={https://openreview.net/forum?id=CqTkapZ6H9}
}
@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}
@inproceedings{zhang2019towards,
    title={Towards Stable and Efficient Training of Verifiably Robust Neural Networks},
    author={Huan Zhang and Hongge Chen and Chaowei Xiao and Sven Gowal and Robert Stanforth and Bo Li and Duane Boning and Cho-Jui Hsieh},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=Skxuk1rFwB}
}
@inproceedings{gowal2018effectiveness,
  title={Scalable verified training for provably robust image classification},
  author={Gowal, Sven and Dvijotham, Krishnamurthy Dj and Stanforth, Robert and Bunel, Rudy and Qin, Chongli and Uesato, Jonathan and Arandjelovic, Relja and Mann, Timothy and Kohli, Pushmeet},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4842--4851},
  year={2019}
}
@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  year={2016}
}
@inproceedings{kingma2014adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  cdate={1420070400000},
  url={https://openreview.net/forum?id=8gmWwjFyLj},
  booktitle={International Conference on Learning Representations}
}
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}
@InProceedings{sun2024breaking,
  title = 	 {Breaking the Barrier: Enhanced Utility and Robustness in Smoothed {DRL} Agents},
  author =       {Sun, Chung-En and Gao, Sicun and Weng, Tsui-Wei},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {46957--46987},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/sun24b/sun24b.pdf},
  url = 	 {https://proceedings.mlr.press/v235/sun24b.html}
}
@inproceedings{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations},
  year={2016}
}
@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine Learning},
  volume={3},
  pages={9--44},
  year={1988},
  publisher={Springer}
}
@inproceedings{weng2019toward,
  title={Toward evaluating robustness of deep reinforcement learning with continuous control},
  author={Weng, Tsui-Wei and Dvijotham, Krishnamurthy Dj and Uesato, Jonathan and Xiao, Kai and Gowal, Sven and Stanforth, Robert and Kohli, Pushmeet},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{wu2021crop,
    title={{CROP}: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing},
    author={Fan Wu and Linyi Li and Zijian Huang and Yevgeniy Vorobeychik and Ding Zhao and Bo Li},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=HOjLHrlZhmx}
}
@inproceedings{kumar2021policy,
    title={Policy Smoothing for Provably Robust Reinforcement Learning},
    author={Aounon Kumar and Alexander Levine and Soheil Feizi},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=mwdfai8NBrJ}
}
@inproceedings{sun2024belief,
    title={Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations},
    author={Xiaolin Sun and Zizhan Zheng},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=7gDENzTzw1}
}
@inproceedings{liu2024beyond,
    title={Beyond Worst-case Attacks: Robust {RL} with Adaptive Defense via Non-dominated Policies},
    author={Xiangyu Liu and Chenghao Deng and Yanchao Sun and Yongyuan Liang and Furong Huang},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=DFTHW0MyiW}
}
@inproceedings{liang2023game,
    title={Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations},
    author={Yongyuan Liang and Yanchao Sun and Ruijie Zheng and Xiangyu Liu and Benjamin Eysenbach and Tuomas Sandholm and Furong Huang and Stephen Marcus McAleer},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=wZWTHU7AsQ}
}
@inproceedings{liu2023rethinking,
    title={Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in {RL}},
    author={Xiangyu Liu and Souradip Chakraborty and Yanchao Sun and Furong Huang},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=pDCublKPmG}
}
@inproceedings{franzmeyerillusory,
  title={Illusory Attacks: Information-theoretic detectability matters in adversarial attacks},
  author={Franzmeyer, Tim and McAleer, Stephen Marcus and Henriques, Joao F and Foerster, Jakob Nicolaus and Torr, Philip and Bibi, Adel and de Witt, Christian Schroeder},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{ghosh2020operator,
  title={An operator view of policy gradient methods},
  author={Ghosh, Dibya and C Machado, Marlos and Le Roux, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3397--3406},
  year={2020}
}
@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine Learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}
@book{TongZhang2023Algorithms,
  title={Mathematical Analysis of Machine Learning Algorithms},
  author={Tong Zhang},
  year={2023},
  publisher={Cambridge University press}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}
@article{gelfand1991recursive,
  title={Recursive stochastic algorithms for global optimization in R\^{}d},
  author={Gelfand, Saul B and Mitter, Sanjoy K},
  journal={SIAM Journal on Control and Optimization},
  volume={29},
  number={5},
  pages={999--1018},
  year={1991},
  publisher={SIAM}
}