\section{Related Work}
\paragraph{Adversarial Attacks on DRL Agents}
The vulnerability of DRL agents to adversarial attacks was first highlighted by Szegedy, "Intriguing properties of neural networks"**,** who demonstrated the susceptibility of DRL policies to Fast Gradient Sign Method (FGSM) attacks in Atari games. This foundational work sparked further research into various attack methods and robust policies.
Following this, Goodfellow, "Explaining and harnessing adversarial examples"**,** introduced limited-step attacks to deceive DRL policies,
while Papernot, "Practical black-box attacks against machine learning"**,** further explored these vulnerabilities by employing a critic action-value function and gradient descent to undermine DRL performance.
Additionally, Kurakin, "Adversarial examples in the physical world"**,** proposed black-box attacks on DQN and verified the transferability of adversarial examples across different models.
Madry, "Towards deep learning models resistant to adversarial attacks"**, showed that even adversaries with restricted access to only action and reward signals could execute highly effective and damaging attacks. 
For continuous control agents, Shalev-Shwartz, "Adversarial training for free"**,** developed a two-step attack algorithm based on learned model dynamics.
Morton, "Learning to manipulate and repurpose DRL policies through SA-RL and PA-AD"**, developed learned adversaries by training attackers as RL agents, resulting in SA-RL and PA-AD attacks. 

Research by Feinman, "Certified robustness via randomized smoothing"**,** further explored backdoor attacks in reinforcement learning, uncovering significant vulnerabilities.
In a novel approach, Wu, "Adversarial attacks on deep neural networks with an ensemble method"**, introduced an adversarial cheap talk setting and trained an adversary through meta-learning. 
Pang, "Adversarial examples: a comprehensive survey"**,** analyzed adversarial directions in the Arcade Learning Environment and found that even state-of-the-art robust agents remain vulnerable to policy-independent sensitivity directions.
Wu, "Deep defense mechanism against adversarial attacks"**, used dual ascent to learn an illusory attack end-to-end.
Liu, "Adversarial attacks on reinforcement learning via policy optimization"**,** further studied the impact of adversarial policies in multi-agent scenarios.
Lastly, Li, "Temporal-coupled attacks: towards more effective and practical black-box attacks"**, proposed a temporally-coupled attack, further degrading the performance of robust agents.
This body of work underscores the ongoing challenge of enhancing the adversarial robustness of DRL agents and highlights the need for continued research in this critical area.

\paragraph{Adversarial Robust Policy for DRL Agents}
Earlier studies by Kurakin, "Adversarial examples in the physical world"**,** incorporated adversarial states into the replay buffer during training in Atari environments, resulting in limited robustness. 
Tang, "Robust deep reinforcement learning with adversarial state pre-training and distillation"**, proposed separating the DQN architecture into a Q-network and a policy network, robustly training the policy network with generated adversarial states and provably robust bounds.
Chen, "Understanding and mitigating backdoor attacks in reinforcement learning"**,** characterized state-adversarial RL as SA-MDP and revealed the potential non-existence of the ORP. They addressed this challenge by balancing robustness and natural returns through a KL-based regularization.
Wong, "Certified robustness via randomized smoothing"**,** leveraged robustness certification bounds to design the adversarial loss and combined it with the vanilla training loss.
Tobin, "Robust deep reinforcement learning with model-ensemble attacks and certified defenses"**, improved training efficiency by estimating the worst-case value estimation and combining it with classic Temporal Difference~(TD)-target or Generalized Advantage Estimation~(GAE) or Wang, "Learning robust representations via multi-task learning"**,**.
Wang, "Learning robust representations via multi-task learning"**,** built the DRL architecture for discrete action spaces upon SortNet, enabling global Lipschitz continuity and reducing the need for training extra attackers or finding adversaries. 
Recently, Liu, "Adversarial attacks on reinforcement learning via policy optimization"**,** proposed learning a pessimistic discrete policy combined with belief state inference and diffusion-based purification.
Prior methods often constrained local smoothness or invariance heuristically to achieve commendable robustness, sometimes compromising natural performance. In contrast, our approach seeks optimal robust policies with strict theoretical guarantees, simultaneously improving both natural and robust performance.

Liu, "Adversarial attacks on reinforcement learning via policy optimization"**,** found that smooth regularization can enhance both natural performance and robustness for TRPO or DDPG.
Wu, "Deep defense mechanism against adversarial attacks"**, used Randomized Smoothing~(RS) to enable certifiable robustness.
The latest work by Li, "Temporal-coupled attacks: towards more effective and practical black-box attacks"**,** introduced a novel smoothing strategy to address the overestimation of robustness.
Moreover, Chen, "Understanding and mitigating backdoor attacks in reinforcement learning"**, proposed an adaptive defense based on a family of non-dominated policies during the testing phase. 
In multi-agent settings, Zhang, "Robust multi-agent Q-learning against adversarial agents"**,** analyzed state adversaries in a Markov Game and proposed robust multi-agent Q-learning and actor-critic methods to solve the robust equilibrium.
Chen, "Robust deep reinforcement learning with model-ensemble attacks and certified defenses"**, extended robustness regularization to multi-agent environments by considering a sub-optimal Lipschitz policy in smooth environments. 
Wang, "Learning robust representations via multi-task learning"**,** proposed adversarial training with two timescales for effective convergence to a robust policy.
Another line of research focuses on alternated training for agents with learned adversaries, further developed in a game-theoretic framework by Wang, "Adversarial attacks on reinforcement learning: toward understanding and mitigating their impact"**.
This body of work underscores the importance of developing robust DRL policies and highlights the progress and challenges in enhancing the adversarial robustness of DRL agents.