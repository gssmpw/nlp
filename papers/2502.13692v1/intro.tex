\section{Introduction}
Halfspaces are arguably among the simplest and most fundamental classic learning models. Given a normal vector $w \in \R^d$ and a bias $b \in \R$ defining a hyperplane, the corresponding halfspace classifier predicts the label of a data point $x \in \R^d$ by returning $\sign(\langle w, x \rangle+b)$, corresponding to a $+1$ label on points inside the halfspace above the hyperplane, and $-1$ on points below.

Classic examples of learning algorithms for obtaining a halfspace classifier from a training set of points $S = \{(x_i,y_i)\}_{i=1}^n$ with $(x_i, y_i) \in \R^d \times \{-1,1\}$, includes the Perceptron Learning Algorithm (PLA)~(\cite{perceptron}) and Support Vector Machines (SVM)~(\cite{Cortes1995}). A key intuition underlying SVM, is the empirical observation that halfspaces with a large \emph{margin} to the training data tend to generalize well. Ignoring the bias variable $b$ (which may be handled by adding a special feature) and assuming $w \in \cH$ (i.e.\ $w$ has unit length), the margin of the halfspace with normal vector $w$ on a labeled point $(x,y)$ is $y \langle w, x \rangle$. Observe that $\langle w, x\rangle$ gives the signed distance of $x$ from the hyperplane, and the margin is positive when $\sign(\langle w, x \rangle)$ correctly predicts the label $y$. With this definition, \emph{hard-margin} SVM 
computes the normal vector $w$ of the hyperplane with the largest minimum margin.
There are also margin variants of the Perceptron~(\cite{marginperceptron}) that computes a halfspace with minimum margin approaching the optimal, as in hard-margin SVM.

To handle data that is not linearly separable, and to add robustness to outliers, the \emph{soft-margin} SVM relaxes the optimization problem to the following
\begin{align*}
    \min_{w, \xi} \|w\|_2^2 + \lambda \sum_i \xi_i,\ \qquad
    \textrm{s.t. } y_i \langle w, x_i \rangle \geq 1- \xi_i,\qquad
    \xi_i \geq 0.
\end{align*}
Here $\lambda>0$ is a regularization parameter. The soft-margin SVM thus allows for smaller margins on some training points at the cost of a penalty $\lambda \xi_i$.

To theoretically justify and explain the empirical success of focusing on large margins,~\cite{Bartlett98generalizationperformance} proved the first generalization bounds upper bounding the probability $\Loss_\cD(w) := \Pr_{(\rx,\ry) \sim \cD}[\sign(\langle w, \rx \rangle) \neq \ry]$ of misclassifying the label of a new data point. Concretely they first studied the hard-margin case and proved that for any distribution $\cD$ over $\Balld \times \{-1,1\}$ and any $0 < \delta < 1$, it holds with probability at least $1-\delta$ over a training set $\rS \sim \cD^n$ that for every $w \in \cH$ and every margin $0 < \gamma < 1$, if $y\langle w,x\rangle \geq \gamma$ for all $(x,y) \in \rS$ then
\begin{align}
    \Loss_\cD(w)  \leq c \cdot \left(\frac{\ln^2(n)}{\gamma^2 n} + \frac{\ln(e/\delta)}{n}\right),\label{eq:bartlett}
\end{align}
for a constant $c>0$. The restriction to $x \in \Balld$ can be relaxed by multiplying the first term by $R^2$ for $x \in R\cdot \Balld$. A dependency on the scaling of input points is inevitable as margins scale with $\|x\|_2$. Throughout the paper, we state bounds for $R=1$ and remark that all bounds generalize to arbitrary $R$ by replacing $\gamma$ by $\gamma/R$.

Defining $\Loss^\gamma_S(w)$ as the fraction of data points in a training set $S$ where $w$ has margin at most $\gamma$,~\cite{Bartlett98generalizationperformance} also prove a more general result, saying that with probability $1-\delta$ over $\rS \sim \cD^n$, it holds for every $w \in \cH$ that
\begin{align}
    \Loss_\cD(w) \leq \Loss_{\rS}^\gamma(w) + c \cdot \sqrt{\frac{\ln^2(n)}{\gamma^2 n} + \frac{\ln(e/\delta)}{n}}.\label{eq:bartlettSoft}
\end{align}
This was later improved by~\cite{DBLP:journals/jmlr/BartlettM02} using Rademacher complexity arguments, replacing the $\ln^2(n)$ term in~\eqref{eq:bartlettSoft} by $1$. Here, and throughout the paper, we refer to $\Loss^\gamma_\rS(w)$ as the (empirical) \emph{margin loss}.

\paragraph{First-Order Bounds.} 
The first work to interpolate between the hard-margin and soft-margin bounds was due to ~\cite{DBLP:conf/colt/McAllester03}, who gave a general tradeoff of
\begin{align}
    \Loss_\cD(w) \leq \Loss_{\rS}^\gamma(w) + c \cdot \left(\sqrt{ \Loss_\rS^\gamma(w) \cdot \frac{\ln n}{\gamma^2 n} }  + \frac{\ln n}{\gamma^2 n} + \sqrt{\frac{\ln n + \ln(e/\delta)}{n}}\right).\label{eq:mcallester}
\end{align}
Notice how the $\Loss_\rS^\gamma(w)$ term is multiplied onto $\ln n/(\gamma^2 n)$ inside the first square-root. Since the hard-margin case corresponds to this term being $0$, this gives a way of interpolating between the cases. Such bounds are often refered to as \emph{first-order bounds}. Unfortunately,~\eqref{eq:mcallester} still has the seemingly superfluous $\sqrt{(\ln n + \ln(e/\delta))/n}$ term even when $\Loss_\rS^\gamma(w)=0$ and thus falls short of even matching~\eqref{eq:bartlett} in the hard-margin case.

The current state-of-the-art generalization bound is due to~\cite{SVMbest} and states that with probability $1-\delta$ over $\rS \sim \cD^n$, it holds for every $w \in \cH$ that
\begin{align}
    \Loss_\cD(w) \leq \Loss_{\rS}^\gamma(w) + c \cdot \left(\sqrt{\Loss_{\rS}^\gamma(w) \cdot \left(\frac{\ln n}{\gamma^2 n} + \frac{\ln(e/\delta)}{n}\right)} + \frac{\ln n}{\gamma^2 n} + \frac{\ln(e/\delta)}{n}\right).\label{eq:sota}
\end{align}
This improves previous hard-margin bounds by a logarithmic factor and gives a cleaner interpolation between the hard- and soft-margin cases. Furthermore, the bound is close to optimal. Concretely, the dependency on $\delta$ is optimal by tweaking standard results for agnostic PAC learning, see e.g.~\cite{DevroyeGyorfiLugosi1996} [Chapter 11]. Moreover,~\cite{SVMbest} complemented their upper bound by the following lower bound 
\begin{theorem}[\cite{SVMbest}]
\label{thm:lower}
There is a constant $c>0$ such that for any $c n^{-1/2} < \gamma < c^{-1}$, any parameter $0 \leq \tau \leq 1$, and any $n \geq c$, there is a distribution $\cD$ such that it holds with constant probability over $\rS \sim \cD^n$ that there is a $w \in \cH$ such that $\Loss_\rS^\gamma(w) \leq \tau$ and
\begin{align*}
\Loss_\cD(w) &\geq \Loss_\rS^\gamma(w) + c \cdot \left(\sqrt{\tau \cdot \frac{\ln(e/\tau)}{\gamma^2 n}} + \frac{\ln(\gamma^2 n)}{\gamma^2 n} \right) 
\geq \Loss_\rS^\gamma(w) + c \cdot \left(\sqrt{\Loss_{\rS}^\gamma(w) \cdot \frac{\ln(e/\Loss_{\rS}^\gamma(w))}{\gamma^2 n}} + \frac{\ln(\gamma^2 n)}{\gamma^2 n} \right).
\end{align*}
\end{theorem}
Notice how the parameter $\tau$ allows for showing that the upper bound~\eqref{eq:sota} is nearly tight across the range of $\Loss_\rS^\gamma(w)$. Let us also remark that~\cite{SVMbest} states their lower bound with a $\ln n$ rather than $\ln(e \gamma^2 n)$, but require that $\gamma > n^{-0.499}$. A careful examination of their proof however reveals the more general lower bound stated here.

Unfortunately there still remains a discrepancy between the lower bound and~\eqref{eq:sota}. Concretely there is a gap of $\sqrt{\ln n/\ln(e/\Loss_{\rS}^\gamma(w))}$. Moreover, for constant $\Loss_{\rS}^\gamma(w)$, the Rademacher complexity based bound in~\eqref{eq:bartlettSoft} improves over both of the first-order bounds~\eqref{eq:mcallester} and~\eqref{eq:sota}, and matches the lower bound in Theorem~\ref{thm:lower}. This seem to suggest that a better upper bound might be possible.

\paragraph{Our Contribution.}
In this work, we settle the generalization performance of large-margin halfspaces by proving a new upper bound matching the lower bound in Theorem~\ref{thm:lower} across the entire tradeoff between $\gamma$, $\Loss_\rS^\gamma(w)$ and $n$ (and is also tight in terms of $\delta$). Our result is stated in the following theorem
\begin{theorem}
\label{thm:main}
There is a constant $c>0$ such that for any distribution $\cD$ over $\Ball_2^d \times \{-1,1\}$, it holds with probability at least $1-\delta$ over $\rS \sim \cD^n$ that for every $w \in \cH$ and every margin $n^{-1/2} \leq \gamma \leq 1$, we have
\[
\Loss_\cD(w) \leq \Loss_{\rS}^\gamma(w) + c \left(\sqrt{\Loss^{\gamma}_\rS(w) \cdot \left( \frac{ \ln(e/\Loss^{\gamma}_\rS(w))}{\gamma^2 n} + \frac{\ln(e/\delta)}{n} \right)} + \frac{\ln(e\gamma^2 n)}{\gamma^2 n} + \frac{\ln(e/\delta)}{n}  \right).
\]
\end{theorem}
While one might argue that our improvement is small in magnitude, this finally pins down the exact generalization performance of a classic learning model. Furthermore, our proof of Theorem~\ref{thm:main} brings several novel ideas that we hope may find further applications in generalization bounds.

We next proceed to give an overview of our proof and new ideas in Section~\ref{sec:overview}, before giving the full details of the proof in Section~\ref{sec:mainproof}.

