\section{Proof Overview}
\label{sec:overview}
In this section, we present the main ideas in our proof of Theorem~\ref{thm:main}. As our proof builds on, and greatly extends, the work of~\cite{SVMbest} establishing the previous state-of-the-art in~\eqref{eq:sota}, we first present their overall proof strategy and the barriers we need to overcome to obtain our tight generalization bound. Throughout this proof overview, we use the notation $x \lesssim y$ to denote that there is an absolute constant $c>0$ so that $x \leq cy$.

\subsection{Previous Proof}
The proof of~\cite{SVMbest} follows a framework proposed by~\cite{SFBL98} for proving generalization of large-margin \emph{voting classifiers} (i.e.\ boosting). The main idea is to randomly discretize the infinite hypothesis set $\cH$ to obtain a finite set $\Disc \subseteq \R^d \to \{-1,1\}$. If $\Disc$ is small enough, then a standard union bound over all $h \in \Disc$ suffices to bound the difference between the empirical error and the true error $\Loss_\cD(h)$ for every $h \in \Disc$. The key trick is to exploit large margins to allow for a discretization to a smaller $\Disc$.

To elaborate on the above, let us first generalize our notation $\Loss_\cD(w)$ and $\Loss^\gamma_S(w)$ a bit. For a distribution $\cD$ over $\Balld \times \{-1,1\}$, let
$
\Loss^\gamma_\cD(w) := \Pr_{(\rx,\ry) \sim \cD}[\ry\langle w , \rx \rangle \leq \gamma]
$,
that is, $\Loss^\gamma_\cD(w)$ is the probability over a fresh sample $(\rx, \ry)$ from $\cD$, of $w$ having margin no more than $\gamma$ on $(\rx,\ry)$. For a training set $S$, we slightly abuse notation and write $(\rx,\ry) \sim S$ to denote a uniform random sample from $S$. We thus have
\[
\Loss^\gamma_S(w) := \Pr_{(\rx,\ry) \sim S}[\ry\langle w , \rx \rangle \leq \gamma] = \frac{|\{(x,y) \in S : y\langle w , x \rangle \leq \gamma\}|}{|S|}.
\]
When writing $\Loss_{\cD}(w)$ we implicitly mean $\Loss_\cD^0(w)$ and note that this coincides with our previous definition of $\Loss_\cD(w) = \Pr_{(\rx ,\ry)\sim \cD}[\sign(\langle w, \rx \rangle) \neq \ry]$ (defining $\sign(0)=0$).

\paragraph{Random Discretization.}
With this notation, the main idea in the proof of~\cite{SVMbest}, is to apply a Johnson-Lindenstrauss transform~(\cite{JL84}), followed by a random snapping to a grid, in order to map each $w \in \cH$ to a point on a grid $\Disc$ of size $\exp(c k)$ in $\R^k$, with $c > 0$ a sufficiently large constant. In more detail, let $\rA$ be a $k \times d$ matrix with i.i.d.\ $\Norm(0,1/k)$ normal distributed entries. Such a matrix is a classic implementation of the Johnson-Lindenstrauss transform and has the property that $|\langle \rA w, \rA x \rangle - \langle w, x \rangle|$ is greater than $\eps$ with probability at most $\exp(-\eps^2 k/c)$ when $\|w\|_2,\|x\|_2 \leq 1$~(\cite{DG03}). Note that this also preserves the norm of a vector $w$ by considering $x=w$ and noting $\langle w, w \rangle = \|w\|_2^2$. Secondly, following an idea of~\cite{AK17} in a lower bound proof for the Johnson-Lindenstrauss transform,~\cite{SVMbest} randomly round $\rA w$ to a point $h_{\rA,\rt}(w)$ with coordinates integer multiples of $k^{-1/2}$ while guaranteeing that $|\langle h_{\rA,\rt}(w), \rA x \rangle-\langle \rA w, \rA x \rangle|$ is less than $\eps$, except with probability $\exp(- \eps^2 k/c)$. Here we use $\rt$ to denote the randomness involved in the rounding.

Now choosing $\eps= \gamma/4$ gives, by the triangle inequality, that $|\langle h_{\rA,\rt}(w), \rA x \rangle-\langle w, x \rangle| \leq \gamma/2$, except with probability $2\exp(- \gamma^2 k/(16c))$. Furthermore, by plugging in $x=w$ and setting $\eps=1$, we can also deduce that $\|h_{\rA,\rt}(w)\|_2 \leq 2$ except with probability $\exp(-k/c)$. Simple counting arguments show that there are only $\exp(c k)$ many vectors of norm at most $2$ with all coordinates integer multiples of $k^{-1/2}$. That is, except with probability $\exp(-k/c)$, $h_{\rA,\rt}(w)$ belongs to a finite set $\Disc$ of $\exp(c k)$ many points. 

\paragraph{Framework.}
With the above random discretization, the proof of~\cite{SVMbest} now follows the framework of~\cite{SFBL98} by relating $\Loss_\cD(w)$ to $\Loss^{\gamma/2}_{\rA\cD}(h_{\rA,\rt}(w))$ and $\Loss^\gamma_{\rS}(w)$ to $\Loss^{\gamma/2}_{\rA \rS}(h_{\rA,\rt}(w))$. Here $\rA \cD$ is the distribution obtained by sampling $(\rx,\ry) \sim \cD$ and returning $(\rA \rx, \ry)$. Similarly, $\rA S$ is the training set obtained by replacing each $(x,y) \in S$ by $(\rA x, y)$. The intuition is that the random discretization changes margins by no more than $\gamma/2$ for most data points and hence points with margin at most $0$ under $\cD$ often have margin at most $\gamma/2$ under $\rA \cD$ and similarly for $S$ and $\rA S$. Let us make this more formal. We have for any $A,t$ in the support of $\rA,\rt$ that
\begin{align}
\Loss_\cD(w) &\leq \Loss^{\gamma/2}_{A \cD}(h_{A,t}(w)) + \Pr_{(\rx,\ry) \sim \cD}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{A,t}(w) , A\rx\rangle > \gamma/2] \label{eq:Dupper}
\end{align}
Similarly, we have
\begin{align}
\Loss^\gamma_S(w) &\geq \Loss^{\gamma/2}_{A S}(h_{A,t}(w)) - \Pr_{(\rx,\ry) \sim S}[\ry \langle w, \rx\rangle > \gamma \wedge \ry \langle h_{A,t}(w) , A\rx\rangle \leq \gamma/2].\label{eq:Supper}
\end{align}
Taking expectation we see that
\begin{align}
    \Loss_\cD(w) - \Loss_\rS^\gamma(w) &= \E_{\rA,\rt}[\Loss_\cD(w) - \Loss_\rS^\gamma(w)] \nonumber\\
    &\leq \E_{\rA,\rt}[\Loss^{\gamma/2}_{\rA \cD}(h_{\rA,\rt}(w)) -\Loss^{\gamma/2}_{\rA S}(h_{\rA,\rt}(w)) ] \label{eq:needsup}\\
    &+ \E_{\rA,\rt}[\Pr_{(\rx,\ry) \sim \cD}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{\rA,\rt}(w) , \rA \rx\rangle > \gamma/2]]\label{eq:dRel}\\
    &+ \E_{\rA,\rt}[\Pr_{(\rx,\ry) \sim S}[\ry \langle w, \rx\rangle > \gamma \wedge \ry \langle h_{\rA,\rt}(w) , \rA \rx\rangle \leq \gamma/2]]\label{eq:sRel}.
\end{align}
To bound~\eqref{eq:needsup}, we exploit that $h_{\rA,\rt}(w)$ belongs to the grid $\Disc$, except with probability $\exp(- k/c)$. Using Bernstein's inequality (and a careful partitioning of hypotheses $w$ depending on $\Loss_{\cD}^\gamma(w)$), it is possible to union bound over the entire grid and conclude
\begin{align}
\E_{\rA,\rt}[\Loss^{\gamma/2}_{\rA \cD}(h_{\rA,\rt}(w)) -\Loss^{\gamma/2}_{\rA S}(h_{\rA,\rt}(w)) ] &\leq \nonumber\\
\E_{\rA,\rt}[\sup_{h \in \Disc} \Loss^{\gamma/2}_{\rA \cD}(h) -\Loss^{\gamma/2}_{\rA S}(h) ]  + \Pr_{\rA,\rt}[h_{\rA,\rt}(w) \notin \Disc]&\lesssim \nonumber\\
\sqrt{\Loss_{\rS}^\gamma(w) \cdot \frac{\ln(|\finalH_\rA|/\delta)}{n}}  + \frac{\ln(|\finalH_\rA|/\delta)}{n} + \exp(-k/c) &\lesssim \nonumber\\
\sqrt{\Loss_{\rS}^\gamma(w) \cdot \frac{k + \ln(e/\delta)}{n}} + \frac{k + \ln(e/\delta)}{n} + \exp(-k/c).\label{eq:boundgamma/2}
\end{align}

To bound~\eqref{eq:sRel}, we use the guarantees of the random discretization to conclude that
\begin{align*}
    \E_{\rA,\rt}[\Pr_{(\rx,\ry) \sim S}[\ry \langle w, \rx\rangle > \gamma \wedge \ry \langle h_{\rA,\rt}(w) , \rA \rx\rangle \leq \gamma/2]] &= \\
   \E_{(\rx,\ry) \sim S}[ \Pr_{\rA,\rt}[\ry \langle w, \rx\rangle > \gamma \wedge \ry \langle h_{\rA,\rt}(w) , \rA \rx\rangle \leq \gamma/2]] &\leq \\
   \E_{(\rx,\ry) \sim S}[ \Pr_{\rA,\rt}[ \ry \langle h_{\rA,\rt}(w) , \rA \rx\rangle \leq \gamma/2 \mid \ry \langle w, \rx\rangle > \gamma]] &\leq \\
   2\exp(-\gamma^2 k/(16c)).
\end{align*}
We can bound~\eqref{eq:dRel} in a similar fashion (even with slightly better guarantees scaled by $\Loss_\cD(w)$, but this does not help for~\eqref{eq:sRel}). The final generalization error thus becomes
\begin{align}
    \Loss_\cD(w) \leq \Loss_{\rS}^\gamma(w) + c' \cdot \left(\sqrt{\Loss_{\rS}^\gamma(w) \cdot \frac{k + \ln(e/\delta)}{n}} + \frac{k + \ln(e/\delta)}{n} + \exp(-\gamma^2 k/c') \right),\label{eq:finalOld}
\end{align}
where $c'>0$ is a sufficiently large constant. Comparing this expression with the desired bound from Theorem~\ref{thm:main}, we see that we have to choose $k$ large enough that $c'\exp(- \gamma^2 k/c')$ is no larger than
\[
\sqrt{\Loss^{\gamma}_\rS(w) \cdot \left( \frac{ \ln(e/\Loss^{\gamma}_\rS(w))}{\gamma^2 n} + \frac{\ln(e/\delta)}{n} \right)} + \frac{\ln(e\gamma^2 n)}{\gamma^2 n} + \frac{\ln(e/\delta)}{n}.
\]
This basically solves to 
\[
k \gtrsim  \gamma^{-2} \ln\left( \frac{\gamma^2 n}{\Loss_{\rS}^\gamma(w) \ln(e/\Loss_{\rS}^\gamma(w))} \right) \geq \gamma^{-2} \ln\left(\gamma^2 n\right).
\]
Inserting this $k$ in~\eqref{eq:finalOld} recovers the bound by~\cite{SVMbest} stated in~\eqref{eq:sota}.

\paragraph{Barriers.}
In light of the above discussion, we identify some key barriers for the previous proof technique. Concretely, if we examine~\eqref{eq:finalOld}, the term $\sqrt{\Loss_\rS^\gamma(w) k/n}$ requires us to choose $k$ no larger than $c \gamma^{-2} \ln(e/\Loss_\rS^\gamma(w))$ to match the optimal bound we get in Theorem~\ref{thm:main}. Unfortunately, the additive $\exp(-\gamma^2 k/c')$ term originating from handling~\eqref{eq:dRel} and~\eqref{eq:sRel} then becomes $\poly(\Loss_\rS^\gamma(w))$, which is too expensive. In fact, even the additive $\exp(-k/c)$ term from handling~\eqref{eq:needsup} is too expensive for e.g.\ constant $\gamma$. Nonetheless, we will in fact choose such $k$ and identify a tighter strategy for analysing $\Loss_\cD(w) - \Loss^\gamma_{\rS}(w)$.


\subsection{Our Key Improvements}
Our first main observation is that the two upper bounds in~\eqref{eq:Dupper} and~\eqref{eq:Supper} are not completely tight, i.e.\ they are inequalities, not equalities. In~\eqref{eq:Dupper} we for instance ignore points $(x,y)$ that had a margin greater than $0$ for $w$, but where the margin of $(Ax,y)$ is less than $\gamma/2$ for $h_{A,t}(w)$. Taking these into accounts, we get the tighter bounds
\begin{align*}
    \Loss_{\cD}(w) &= \Loss^{\gamma/2}_{A \cD}(h_{A,t}(w))+ \Pr_{(\rx,\ry) \sim \cD}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{A,t}(w) , A\rx\rangle > \gamma/2] \\
    &- \Pr_{(\rx,\ry) \sim \cD}[\ry \langle w, \rx\rangle > 0 \wedge \ry \langle h_{A,t}(w) , A\rx\rangle \leq \gamma/2],
\end{align*}
and
\begin{align*}
\Loss^\gamma_S(w) &= \Loss^{\gamma/2}_{A S}(h_{A,t}(w)) - \Pr_{(\rx,\ry) \sim S}[\ry \langle w, \rx\rangle > \gamma \wedge \ry \langle h_{A,t}(w) , A\rx\rangle \leq \gamma/2] \\
&+ \Pr_{(\rx,\ry) \sim S}[\ry \langle w, \rx\rangle \leq \gamma \wedge \ry \langle h_{A,t}(w) , A\rx\rangle > \gamma/2].
\end{align*}
With these refined bounds, we can now split $\Loss_\cD(w)-\Loss^\gamma_S(w)$ into a sum of three terms:
\begin{align}
    &\Loss^{\gamma/2}_{A \cD}(h_{A,t}(w)) - \Loss^{\gamma/2}_{A S}(h_{A,t}(w)) \nonumber\\ &+
    \Pr_\cD[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{A,t}(w) , A\rx\rangle > \gamma/2] - \Pr_S[\ry \langle w, \rx\rangle \leq \gamma \wedge \ry \langle h_{A,t}(w) , A\rx\rangle > \gamma/2]\label{eq:diffDS} \\ &+
    \Pr_S[\ry \langle w, \rx\rangle > \gamma \wedge \ry \langle h_{A,t}(w) , A\rx\rangle \leq \gamma/2] - \Pr_\cD[\ry \langle w, \rx\rangle > 0 \wedge \ry \langle h_{A,t}(w) , A\rx\rangle \leq \gamma/2].\label{eq:diffSD} 
\end{align}
The first line is the same as~\eqref{eq:needsup} from before, but~\eqref{eq:diffDS} and~\eqref{eq:diffSD} improves over~\eqref{eq:dRel} and~\eqref{eq:sRel} by subtracting off a term. Intuitively, our more refined bounds allow us to argue that if the randomized rounding creates a big difference between $\Loss_\cD(w)$ and $\Loss^{\gamma/2}_\cD(h_{A,t}(w))$, then it creates a comparably large difference between $\Loss_{S}^\gamma(w)$ and $\Loss^{\gamma/2}_S(h_{A,t}(w))$, thereby canceling out. We will carefully exploit this in the following. Let us focus on~\eqref{eq:diffDS} and remark that~\eqref{eq:diffSD} is handled symmetrically. For~\eqref{eq:diffDS}, we see that
\begin{align*}
    \Pr_{(\rx,\ry) \sim S}[\ry \langle w, \rx\rangle \leq \gamma \wedge \ry \langle h_{A,t}(w) , A\rx\rangle > \gamma/2] &\geq 
    \Pr_{(\rx,\ry) \sim S}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{A,t}(w) , A\rx\rangle > \gamma/2],
\end{align*}
and thus~\eqref{eq:diffDS} is at most
\begin{align*}
\Pr_{(\rx,\ry) \sim \cD}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{A,t}(w) , A\rx\rangle > \gamma/2] - \Pr_{(\rx,\ry) \sim S}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{A,t}(w) , A\rx\rangle > \gamma/2].
\end{align*}
Now introducing the expectation over the randomized rounding $\rA$ and $\rt$ as in the previous proof, and using linearity of expectation, we want to bound the following expression with probability $1-\delta$ over $\rS \sim \cD^n$
\begin{align}
    \sup_{w \in \cH} \bigg(\E_{\rA,\rt}[\Pr_{(\rx,\ry) \sim \cD}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{\rA,\rt}(w) , \rA\rx\rangle > \gamma/2]] 
    &-\nonumber\\ \E_{\rA,\rt}[\Pr_{(\rx,\ry) \sim S}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{\rA,\rt}(w) , \rA\rx\rangle > \gamma/2]]\bigg) &= \nonumber\\
    \sup_{w \in \cH} \bigg(\E_{(\rx,\ry) \sim \cD}[\Pr_{\rA,\rt}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{\rA,\rt}(w) , \rA\rx\rangle > \gamma/2]] 
    &-\nonumber\\ \E_{(\rx,\ry) \sim \rS}[\Pr_{\rA,\rt}[\ry \langle w, \rx\rangle \leq 0 \wedge \ry \langle h_{\rA,\rt}(w) , \rA\rx\rangle > \gamma/2]]\bigg). \label{eq:needRad}
\end{align}
This now has a form that looks familiar. Concretely, we have a function 
\begin{align}
\psi_w(x,y) = \indi{ y\langle w ,x \rangle \leq 0} \cdot \Pr_{\rA,\rt}[y \langle h_{\rA,\rt}(w),\rA x\rangle > \gamma/2] \label{eq:psi}
\end{align}
for each $w \in \cH$, and wish to bound $\sup_w \E_{(\rx,\ry) \sim \cD}[\psi_w(\rx,\ry)] - \E_{(\rx,\ry) \sim \rS}[\psi_w(\rx,\ry)]$ with high probability over $\rS \sim \cD^n$. Rademacher complexity (see e.g.~\cite{rademacherbound}) is one key tool for bounding such differences. In particular, the contraction principle from~\cite{ledoux1991probability} allows us to bound such a supremum when the functions $\psi_w$ are composite functions $\psi_w = f \circ g_w$ with $f : \R \to \R$ having bounded Lipschitz constant. Indeed margin generalization bounds for halfspaces have previously been proved via Rademacher complexity~(\cite{DBLP:journals/jmlr/BartlettM02}) by composing the two functions $g_w(x,y) = y \langle w, x\rangle$ and $f(u) = \min\{1, \max\{0,\frac{\gamma-u}{\gamma}\}\}$ being the ramp loss. Since the ramp loss $f$ is $\gamma^{-1}$-Lipschitz, one can thus reduce bounding the Rademacher complexity of $\psi_w$ to bounding the Rademacher complexity of the simpler $g_w(x,y) = y\langle w ,x\rangle$. When using this $g_w(x,y)$ and an $L$-Lipschitz $f$, the resulting bound on~\eqref{eq:needRad} becomes $c L/\sqrt{n}$, i.e.\ $\sqrt{1/(\gamma^2 n)}$ when using the ramp loss.

We wish to take a similar approach for our $\psi_w$ in~\eqref{eq:psi}. We thus want to argue that $\psi_w(x,y)$ can be written as a composite function $f(y\langle w, x\rangle)$ of the margin of $(x,y)$ on $w$. Examining~\eqref{eq:psi}, we thus need to argue that the probability over $\rA,\rt$ is a function solely of the original margin $y \langle w, x\rangle$. The matrix $\rA$ was $k \times d$ with i.i.d.\ $\Norm(0,1/k)$ entries. Now if we make the assumption that $x$ is unit length (and not just $\|x\|_2 \leq 1$), then by the rotational invariance of Gaussians, the joint distribution of $\rA w, y\rA x$ can be shown to be completely determined from $y \langle w, x\rangle$. Since the random rounding considers only the vector $\rA w$ (and not the original $w$), it follows that the joint distribution of $h_{\rA,\rt}(w), y \rA x$ is also completely determined from $y \langle w, x\rangle$. This implies that $\psi_w(x,y)$ indeed can be written as a function $f$ of $y \langle w, x\rangle$ alone. 

We thus proceed to bound the Lipschitz constant of the function $f$ in~\eqref{eq:psi}. To avoid discontinuities, we have to alter $\psi_w(x,y)$ somewhat to not include the discontinuous indicator function (similarly to using the ramp loss in previous works), and we eventually bound the Lipschitz constant $L$ by roughly
\[
L \lesssim \gamma^{-1} \Pr_{\rA,\rt}[y \langle h_{\rA,\rt}(w),\rA x\rangle > \gamma/2 \mid y \langle w, x\rangle = 0].
\]
With a slight abuse of notation, we write $\Pr_{\rA,\rt}[y \langle h_{\rA,\rt}(w),\rA x\rangle > \gamma/2 \mid y \langle w, x\rangle = 0]$ to denote the probability $\Pr_{\rA,\rt}[y \langle h_{\rA,\rt}(w),\rA x\rangle > \gamma/2]$ for an arbitrary $x,w \in \cH$ and $y \in \{-1,1\}$ with $y\langle w,x\rangle = 0$ as $y\ipr{w,x}$ completely determines this probability as argued above.

Since our randomized rounding preserves inner products to within $\gamma/2$ except with probability $\exp(-\gamma^2 k/c)$, we get $L \lesssim \gamma^{-1} \exp(-\gamma^2 k/c)$. This finally bounds~\eqref{eq:needRad} by
\[
c \cdot \sqrt{\frac{\exp(-\gamma^2 k/c)}{\gamma^2 n}}.
\]
This should be compared to proof by~\cite{SVMbest} that got a bound of $c\exp(-\gamma^2 k/c)$. This improvement is precisely enough to derive our tight Theorem~\ref{thm:main}. Indeed, as mentioned in~\eqref{eq:boundgamma/2}, we can bound $\Loss^{\gamma/2}_{\rA \cD}(h_{\rA,\rt}(w)) - \Loss^{\gamma/2}_{\rA \rS}(h_{\rA,\rt}(w))$ by 
\begin{align*}
\sqrt{\Loss_{\rS}^\gamma(w) \cdot \frac{k + \ln(e/\delta)}{n}} + \frac{k + \ln(e/\delta)}{n} + \exp(-k/c).
\end{align*}
If we ignore the $\exp(-k/c)$ term and set $k = c' \gamma^{-2} \ln(e/\Loss_{\rS}^\gamma(w))$, this gives the tight bound in Theorem~\ref{thm:main}.

Unfortunately, we cannot afford to ignore the $\exp(-k/c)$ term and we need additional ideas for dealing with it. Recall that in the previous proof by~\cite{SVMbest}, it originates from bounding
\begin{align*}
\E_{\rA,\rt}[\Loss^{\gamma/2}_{\rA \cD}(h_{\rA,\rt}(w)) -\Loss^{\gamma/2}_{\rA S}(h_{\rA,\rt}(w)) ] &\leq 
\E_{\rA,\rt}[\sup_{h \in \Disc} \Loss^{\gamma/2}_{\rA \cD}(h) -\Loss^{\gamma/2}_{\rA S}(h) ]  + \Pr_{\rA,\rt}[h_{\rA,\rt}(w) \notin \Disc],
\end{align*}
and upper bounding $\Pr_{\rA,\rt}[h_{\rA,\rt}(w) \notin \Disc]$ by $\exp(-k/c)$. Here we instead consider an infinite sequence of discretizations/grids $\Disc_0,\Disc_1,\dots,$ and argue that the random rounding $\rA, \rt$ and training set $\rS$ is simultaneously \emph{good} (for some appropriate definition) for all grids with high probability. Here the grids $\Disc_i$ correspond to increasingly large norms of $h_{\rA,\rt}(w)$, i.e.\ $\Disc_i$ contains all vectors of norm at most $2^{i+1} \Balld$ and all coordinates integer multiples of $k^{-1/2}$. Multiple careful applications of Cauchy-Schwartz, Jensen's inequality and upper bounds on the probability that $h_{\rA,\rt}(w) \notin \Disc_i$ allows us to finally get rid of the $\exp(-k/c)$ factor.