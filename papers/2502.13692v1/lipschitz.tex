\subsection{Bounding the Lipschitz Constants}
\label{sec:lip}
In this section, we proceed to bound the Lipschitz constants of $\phi$ and $\rho$ and thereby prove Lemma~\ref{lem:finalLip}.
%\begin{customlem}{\ref{lem:finalLip}}
%There are constants $c_L,c>0$ such that the Lipschitz constants $L_\phi$ and $L_\rho$ of $\phi$ and $\rho$ are bounded by
%\[
%c_L \exp(-\gamma_{i+1}^2 k/c_L)\cdot \left(\sqrt{k} + %\gamma^{-1}_{i+1}\right),
%\]
%when $k \geq c \gamma_{i+1}^{-2}$.
%\end{customlem}
We  split it into two tasks depending on the value of $\alpha$. The simplest case is the following
\begin{lemma}
\label{lem:easyLip}
There is a constant $c>0$ such that the Lipschitz constants of $\phi$ and $\rho$, when $0 < \alpha \leq \gamma_i$, are less than:
\[
\frac{c \exp(-k\gamma_{i+1}^2/c)}{\gamma_{i+1}}.
\]
\end{lemma}

\begin{proof}
Since $\phi$ is linear when $0 < \alpha \leq \gamma_i$, its Lipschitz constant equals the slope of the line, i.e.
\begin{align*}
\frac{1}{\gamma_i} \cdot \Pr_{\rA,\rt}[y \langle h_{\rA,\rt}(w),\rA x\rangle > \gamma_i/2 \mid  y\langle w,x \rangle = 0].
\end{align*}
By Lemma~\ref{lem:concdiscretize} and using $\gamma_i = \gamma_{i+1}/2$, this is bounded by
\begin{align*}
    \frac{c\exp(-\gamma_{i+1}^2 k/c)}{\gamma_{i+1}},
\end{align*}
for a constant $c>0$. The same arguments applies immediately to $\rho$.
\end{proof}
The trickier case is when $\alpha \in [-c_\gamma, 0]$ for $\phi$ and when $\alpha \in (\gamma_i, c_\gamma]$ for $\rho$. If we set $c_\gamma \leq 1/\sqrt{2}$, then we have
\begin{lemma}\label{lem:PhiLips}
    There is a constant $c>0$ such that the Lipschitz constant of $\phi$ when $\alpha\in [-1/\sqrt{2}, 0]$ and $\rho$ when $\alpha\in \left(\gamma_i,1/\sqrt{2}\right]$ is less than
    $$ c\exp\left(-\gamma_{i+1}^2 k/c\right)\sqrt{k},$$
    for $k\ge c \gamma_{i+1}^{-2}$.
\end{lemma}
Combining this result with Lemma~\ref{lem:easyLip} completes the proof of Lemma~\ref{lem:finalLip}.

To prove Lemma~\ref{lem:PhiLips}, we need to bound the Lipschitz constants of $\phi$ when $\alpha\in [-1/\sqrt{2}, 0]$ and $\rho$ when $\alpha\in \left(\gamma_i,1\sqrt{2}\right]$.
We will go through the details for $\phi$, and comment how the argument for $\rho$ differs along the way.

First recall the following claim
\begin{customclm}{\ref{clm:distDeterm}}
For any $(x,y) \in \finalX \times \{-1,1\}$ and any $w \in \finalH$, the distribution of $y \langle h_{\rA,\rt}(w),\rA x \rangle$ is completely determined from $y \langle w, x \rangle$. 
\end{customclm}

As we need to understand the distribution of the random variable $y \langle h_{\rA,\rt}(w),\rA x \rangle$ to bound the Lipschitz constants of $\phi$ and $\rho$, we proceed to give the proof of Claim~\ref{clm:distDeterm} while introducing convenient notation for establishing Lemma~\ref{lem:PhiLips}.

\begin{proof}
Firstly, write $h_{\rA,\rt}(w) =\rA w + \rv$ with $\rv =(h_{\rA,\rt}(w)-\rA w)$. Then observe that $(\rA w)_i = \langle \ra_i, w\rangle \sim \Norm(0, \|w\|^2/k) \eqd \Norm(0,1/k)$ where $\ra_i$ denotes the $i$'th row of $\rA$. Here $\eqd$ denotes equality in distribution. Now write $x = \langle w, x \rangle w + u$ where $\langle u, w \rangle = 0$ and $\|u\|^2 = \|x\|^2-\langle w,x \rangle^2 = 1-\langle w,x \rangle^2$ (i.e.\ a Gram-Schmidt step). We have $(\rA x)_i = \langle \ra_i , w\rangle \langle w,x\rangle + \langle \ra_i , u\rangle$. By rotational invariance of the Gaussian distribution and orthogonality of $w$ and $u$, we have that $\langle \ra_i , u \rangle \sim \Norm(0,(1-\langle w, x\rangle^2)/k)$ and that this is independent of $\langle \ra_i, w \rangle$. Using the independence, we also conclude that if we condition on any fixed outcome of $c_i = (\rA w)_i$, we have that $c_i \langle \ra_i, u \rangle$ is $\Norm(0, c^2_i (1-\langle w , x \rangle^2)/k)$ distributed.

We now argue that we can sample from the distribution of $y \langle h_{\rA,\rt}(w), \rA x \rangle = \langle h_{\rA,\rt}(w), y \rA x \rangle$ knowing only $y \langle w, x \rangle$ as follows: Sample independent $\Norm(0,1/k)$ distributed random variables $\rX_1,\dots,\rX_k$. Next sample independent $\Norm(0,(1-y^2 \langle w, x\rangle^2)/k)$ distributed random variables $\rY_1,\dots,\rY_k$ and let $\rZ_i = y \langle w, x \rangle \rX_i + \rY_i \eqd y \langle w, x \rangle \rX_i + y\rY_i$, where the last step follows from independence of $\rX_i$ and $\rY_i$ and symmetry in the distribution of $\rY_i$. Let $\rX$ be the vector with the $\rX_i$'s as entries and $\rZ$ similarly. Then the joint distribution of $(\rX, \rZ)$ is equal to the joint distribution of $(\rA w, y\rA x)$. Finally draw offsets $\rt'_1,\dots,\rt'_k$ uniformly and independently in $[0,1]$ and round $\rX_i$ to a number of the form $(1/2)(10\sqrt{k})^{-1} + z(10\sqrt{k})^{-1}$ for $z \in \Ints$ as in the definition of $h_{\rA,\rt}$. The resulting variables $\rX_i'$ satisfy that $y \langle h_{\rA, \rt}(w), \rA x \rangle \eqd \langle \rX', \rZ \rangle$.
\end{proof}

With Claim~\ref{clm:distDeterm} established, we will use the notation in the proof as we proceed with bounding the Lipschitz constants of $\phi$ and $\rho$.

Let $\alpha = y\ipr{w,x}$ for some $w \in \finalH$ and $(x,y) \in \finalX \times \{-1,1\}$ and $\alpha \in [-1/\sqrt{2}, 0]$ (for $\rho$, let $\alpha \in (\gamma_i, 1/\sqrt{2}]$). Let $\rX_i\sim \Norm(0,1/k)$, $\rY_i\sim\Norm(0,(1-\alpha^2)/k)$ and let $\rX_i'$ be the random rounding of $\rX_i$. We argued, in the proof of Claim~\ref{clm:distDeterm}, that
$y\ipr{h_{\rA,\rt}(w),\rA x} \eqd \ipr{\rX',\alpha \rX + \rY}$.
Let additionally $E_i$ be the event that $\rX_i'$ is rounded up. For notational convenience, let $\rM_i = \sqrt{k} \rX_i$ and observe that $\rM_i\sim \Norm(0,1)$. With this notation, we have that $\rX_i'$ has the form
\begin{align*}
    \rX_i'
    = \frac{1}{10\sqrt{k}}\left(\left\lfloor{10 \rM_i-\frac{1}{2\sqrt{k}10}}\right\rfloor+\indi{E_i}+\frac{1}{2}\right).
\end{align*}
Hence, 
$$y\ipr{h_{\rA,\rt}(w),\rA x} 
\eqd \ipr{\rX',\alpha \rX + \rY}
= \frac{\alpha}{\sqrt{k}}\ipr{\rX',\rM}+\ipr{\rX',\rY}.$$
Recall that the variables $\rt_i$ and $\rM_i$ determine $\rX_i, E_i$ and thus also $\rX'_i$. If we condition on an outcome $\rt_i=t_i$ and $\rM_i=M_i$, only $\rY_i$ remains random. We may thus write
\begin{align*}
  \Pr[y\langle h_{\rA,\rt}(w),\rA x\rangle > \gamma_i/2 ] &=\\ \Pr\left[\frac{\alpha}{\sqrt{k}}\ipr{\rX',\rM}+\ipr{\rX',\rY} > \gamma_i/2\right]
  &=\\
  \int_{\R^k\times [0,1]^k} f_{\rM,\rt}(M,t)\Pr\left[\frac{\alpha}{\sqrt{k}}\ipr{\rX',\rM}+\ipr{\rX',\rY} >\gamma_i/2 \ \bigg| \  \rM_i = M_i, \rt_i = t_i\right] d(M,t)
  &=\\ \int_{\R^k\times[0,1]^k} \Pr\left[\frac{\alpha}{\sqrt{k}}\ipr{X',M}+\ipr{X',\rY} > \gamma_i/2\right] d(M,t),
  \end{align*}
where $f_{M,t}(M,t)$ is the joint probability density function of $\rM$ and $\rt$. 

Let us now define $\rN_i$ such that $\rY_i = \sqrt{\frac{1-\alpha^2}{k}}\rN_i$ and let $\rN = (\rN_1,\dots,\rN_k)$. Then $\rN_i\sim \Norm(0,1)$ and the event 
\[
\frac{\alpha}{\sqrt{k}}\ipr{X',M}+\ipr{X',\rY} > \gamma_i/2,
\]
may be rewritten as
\begin{align*}
    \frac{\alpha}{\sqrt{k}}\ipr{X',M}+\ipr{X',\rY} > \gamma_i/2 \Longleftrightarrow
    \ipr{X',\rY} &> \gamma_i/2 - \frac{\alpha}{\sqrt{k}}\ipr{X',M} \Longleftrightarrow\\
    \sqrt{\frac{1-\alpha^2}{k}}\ipr{X', \rN} &> \gamma_i/2 - \frac{\alpha}{\sqrt{k}}\ipr{X',M} \Longleftrightarrow\\
    \sqrt{\frac{1-\alpha^2}{k}}\norm{X'}_2 \ipr{X'/\|X'\|_2,\rN} &> \gamma_i/2 - \frac{\alpha}{\sqrt{k}}\ipr{X',M} \Longleftrightarrow\\
    \ipr{X'/\|X'\|_2,\rN} &> \frac{\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M}}{\sqrt{1-\alpha^2}\norm{X'}_2}.
\end{align*}
Observe that $\ipr{X'/\|X'\|_2, \rN} \sim \Norm(0,1)$. If we let $\Phi$ denote the cumulative density function of a standard normal distribution, then we have established
\begin{align}
  \Pr[y\langle h_{\rA,\rt}(w),\rA x\rangle > \gamma_i/2] &= \int_{\R^k\times[0,1]^k} f_{M,t}(M,t)\left(1-\Phi\left(\frac{\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M}}{\sqrt{1-\alpha^2}\norm{X'}_2}\right)\right) d(M,t) \nonumber \\
  &=\int_{\R^k\times[0,1]^k} f_{\rM}(M)\left(1-\Phi\left(\frac{\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M}}{\sqrt{1-\alpha^2}\norm{X'}_2}\right)\right) d(M,t).
  \label{eq:integral}
  \end{align}
In the last equality, we use that $\rM$ and $\rt$ are independent and that the probability density function of $\rt$ is $1$ since each $\rt_i$ is uniform in $[0,1]$. This reduces $f_{M,t}(M,t)$ to the probability density function $f_{\rM}(M)$ of $\rM$ alone.
  
The same arguments for $\rho$ also gives the integral~\eqref{eq:integral}, with the small difference that $(1-\Phi(\cdot))$ is replaced by $\Phi(\cdot)$. This difference is irrelevant, since to bound the Lipschitz constant, we will differentiate and bound the differential's absolute value.

Let $g(M,t,\alpha)$ be the integrant above, we want to differentiate $\int_{\R^k\times[0,1]^k} g(M,t,\alpha) \enspace d(M,t)$ by differentiating under the integral. Standard measure theory results (Theorem 6.28, \cite{Klenke:2013}) allows us to do this if we satisfy three conditions. These conditions are, in this case, equivalent to the following
\begin{enumerate}
    \item[i)] for all constant $\alpha$
    , the integral $\int_{\R^k\times[0,1]^k} g(M,t,\alpha) \enspace d(M,t)$ is finite.
    \item[ii)] for all constant $M,t$, the partial differential of $g(M,t,\alpha)$ with respect to $\alpha$ exists.
    \item[iii)] There exists a function $h(M,t)$, where $\int_{\R^k\times[0,1]^k} h(M,t) \enspace d(M,t)$ is finite and such that $|\frac{\partial}{\partial\alpha}g(M,t,\alpha)|\le h(M,t)$ for all $\alpha$.
\end{enumerate}
The first two conditions are straightforward: The integral is equal to a probability, which is finite. And $g$ is a combination of differentiable functions making it differentiable itself.
The last condition is more cumbersome, but the goal of this proof is to upperbound the integral by a constant, which clearly dosn't depend on $\alpha$. Hence the last condition will be satisfied during the proof. 

Hence we can continue with our differentiation by differentiating under the integral.
\begin{align*}
    \bigg| \frac{\partial}{\partial\alpha}\Pr[y \ipr{h_{\rA,\rt}(w),\rA x}> \gamma_i/2 ] \bigg| 
    &= \\\left| \int_{\R^k\times[0,1]^k} \dfrac{\partial}{\partial\alpha}f_{\rM}(M)\left(1-\Phi\left(\frac{\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M}}{\sqrt{1-\alpha^2}\norm{X'}_2}\right)\right) d(M,t) \right| 
    &= \\ \frac{1}{2\pi}\left| \int_{\R^k\times[0,1]^k} f_{\rM}(M)\exp\left(-\frac{1}{2}\left(\frac{\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M}}{\sqrt{1-\alpha^2}\norm{X'}_2}\right)^2 \right)\left(\frac{\frac{\alpha\sqrt{k}\gamma_i}{2} - \ipr{X',M}}{(1-\alpha^2)^{3/2}\norm{X'}_2}\right) \enspace d(M,t) \right|
    &\le \\ \frac{1}{2\pi}\int_{\R^k\times[0,1]^k} f_{\rM}(M)\exp\left(-\frac{(\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M})^2}{2(1-\alpha^2)\norm{X'}_2^2}\right)\left|\frac{\frac{\alpha\sqrt{k}\gamma_i}{2} - \ipr{X',M}}{(1-\alpha^2)^{3/2}\norm{X'}_2} \right| \enspace d(M,t).
\end{align*}
For both $\alpha \in [-1/\sqrt{2}, 0]$ and $\alpha \in \left(\gamma_i, 1/\sqrt{2}\right]$, we have that $1\ge (1-\alpha^2) \geq 1/2$ and thus, for both $\phi$ and $\rho$, the above is upper bounded by
\begin{align*}
\frac{2^{3/2}}{2 \pi} \cdot &\int_{\R^k\times[0,1]^k} f_{\rM}(M)\exp\left(-\frac{(\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M})^2}{2\norm{X'}_2^2}\right)\frac{\sqrt{k}\gamma_i + \left|\ipr{X',M}\right| }{\norm{X'}_2} \enspace d(M,t)  \\
&\le
\int_{\R^k\times[0,1]^k} f_{\rM}(M)\exp\left(-\frac{(\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M})^2}{2\norm{X'}_2^2}\right)\left(\frac{\sqrt{k}\gamma_i}{\norm{X'}_2} + \|M\|_2 \right)\enspace d(M,t).
\end{align*}
We now use that $|X'_i| \geq (1/2)(10 \sqrt{k})^{-1}$ for all $i$. This implies $\|X'\|_2 \geq \sqrt{k (1/4)(10 \sqrt{k})^{-2}} = 1/20$. We may thus further upper bound the above by
\begin{align}
20 \cdot \int_{\R^k\times[0,1]^k} f_{\rM}(M)\exp\left(-\frac{(\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M})^2}{2\norm{X'}_2^2}\right)\left(\sqrt{k}\gamma_i + \|M\|_2 \right)\enspace d(M,t).\label{eq:finalintegral}
\end{align}
We will bound~\eqref{eq:finalintegral}, by splitting it into 3 cases: 
\begin{align*}
\textit{i)}\enspace\norm{M}_2^2\le \frac{9}{10}k\qquad\qquad\qquad
\textit{ii)}\enspace\frac{9}{10}k\le\norm{M}_2^2\le \frac{4}{3}k\qquad\qquad\qquad
\textit{iii)}\enspace\norm{M}_2^2\ge \frac{4}{3}k.
\end{align*}
The arguments for cases \textit{i)} and \textit{iii)} do not depend on $\alpha$, and hence are identical for $\rho$ and $\phi$. In those cases, we simply exploit that $\|\rM\|_2^2 \sim \chi^2_k$ and thus these cases are very unlikely. This implies that the integral over $f_M(M)$ is so small that we can afford to upper bound the exponential term in~\eqref{eq:finalintegral} by 1. For case \textit{ii)}, we can use the assumptions on $\|M\|_2^2$ to show that the exponential term is no more than $c\exp(-\gamma_i^2 k/c)$ for a constant $c>0$. We proceed to the three cases.

\paragraph{case \textit{i)}.}
We simply upper bound the exponential term in~\eqref{eq:finalintegral} by $1$ and use the assumption that $\norm{M}_2^2\le \frac{9}{10}k$ to conclude
\begin{align*}
\exp\left(-\frac{(\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M})^2}{2\norm{X'}_2^2}\right)\left(\sqrt{k}\gamma_i + \|M\|_2 \right)
\le 2\sqrt{k}.
\end{align*}
Now since $\rM$ is multivariate standard normal, $\norm{\rM}_2^2$ is $\chi_k^2$ distributed. Let $\rZ \sim \chi_k^2$ with probability density function $f_Z(z)$. Then the integral in~\eqref{eq:finalintegral} in is bounded by:
\begin{align*}
40\sqrt{k} \int_{(\sqrt{9 k/10})\Ball_2^k} f_{\rM}(M)\enspace dM
= 40\sqrt{k} \int_0^{9 k/10} f_{Z}(z)\enspace dz 
= 40\sqrt{k} \cdot \Pr[\rZ<9k/10],
\end{align*}
which by Theorem~\ref{thm:chibound} is less than
$$80\sqrt{k} \exp\left(-k\gamma_i^2/800\right).$$
\paragraph{case \textit{ii)}.}
We use the assumption that $\frac{9}{10}k\le\norm{M}_2^2\le \frac{4}{3}k$ together with the following observations
\begin{remark}\label{rem:case_ii_norm_bound}
   If $\|X\|_2^2 \leq 4/3$, then $\norm{X'}_2^2 < 2$.
\end{remark}
\begin{remark}\label{rmk:ipXX'}
    If $\norm{X}_2^2\ge 9/10$, then $(8/9)\norm{X}_2^2\le \ipr{X,X'}\le (10/9)\norm{X}_2^2$.
\end{remark}
We prove Remark~\ref{rem:case_ii_norm_bound} and Remark~\ref{rmk:ipXX'} in Appendix~\ref{sec:aux}.
Since $\sqrt{k}X = M$, Remark~\ref{rmk:ipXX'} gives $\ipr{X',M}\ge (8/10) \sqrt{k}$ 
and hence
\begin{alignat*}{3}
    \alpha > \gamma_i 
    &\Longrightarrow\quad  \frac{\sqrt{k}\gamma_i}{2} - \alpha\ipr{X',M}
    \le-\frac{3\sqrt{k}\gamma_i}{10}
    \le 0
    &&\Longrightarrow\quad 
    -\left(\frac{\sqrt{k}\gamma_i}{2} - \alpha\ipr{X',M}\right)^2
    \le -\frac{9k\gamma_i^2}{100}\\
    \alpha < 0 
    &\Longrightarrow\quad
    \frac{\sqrt{k}\gamma_i}{2} - \alpha\ipr{X',M} 
    \ge\frac{\sqrt{k}\gamma_i}{2}
    \ge 0 
    &&\Longrightarrow\quad
    -\left(\frac{\sqrt{k}\gamma_i}{2} - \alpha\ipr{X',M}\right)^2
    \le -\frac{k\gamma_i^2}{4}.
\end{alignat*}
Hence for both $\phi$ and $\rho$, the last two factors of the integral in~\eqref{eq:finalintegral} are bounded by:
\begin{align*}
\exp\left(-\frac{(\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M})^2}{2\norm{X'}_2^2}\right)\left(\sqrt{k}\gamma_i + \norm{M}_2 \right)
\le\frac{5}{2}\sqrt{k}\exp\left(-\frac{9 k\gamma_i^2}{100 \cdot 4}\right).
\end{align*}
Which gives the following:
\begin{align*}
50\sqrt{k}\exp\left(-\frac{k\gamma_i^2}{50}\right) \int_{(\sqrt{9 k/10} \cdot \Ball_2^k)^C\cap (\sqrt{4 k/3} \cdot \Ball_2^k)} f_{\rM}(M)\enspace dM
\le 50\sqrt{k}\exp\left(-\frac{k\gamma_i^2}{50}\right).
\end{align*}

\paragraph{case \textit{iii)}.}
We bound the last two factors of the integral~\eqref{eq:finalintegral} under the assumption that $\norm{M}_2^2\ge \frac{4}{3}k$. Here we simply upper bound the exponential by $1$ and get
\begin{align*}
\exp\left(-\frac{(\sqrt{k}\gamma_i/2 - \alpha\ipr{X',M})^2}{2\norm{X'}_2^2}\right)\left(\sqrt{k}\gamma_i + \|M\|_2 \right)
\le 2\norm{M}_2,
\end{align*}
hence the integral~\eqref{eq:finalintegral} is bounded by:
\begin{align}
40 \int_{(\sqrt{4 k/3} \cdot \Ball_2^k)^C} f_{\rM}(M)\norm{M}_2 \enspace dM \label{eq:intz}.
\end{align}
Recall that $\norm{\rM}_2^2\sim\chi_k^2$ and let $\rZ\sim \chi_k^2$ with probability density function $f_Z(z)$. Then the integral~\eqref{eq:intz} is equal to
\begin{align*}
    40\int_{4k/3}^\infty f_{Z}(z)\sqrt{z} \enspace dz.
\end{align*}
Let also $L_i = \left[\frac{4}{3}k\cdot2^{i}, \frac{4}{3}k\cdot2^{i+1} \right)$  for $i\in\Z_{\ge0}$. By definition, the $L_i$'s partition $\left[\frac{4}{3}k,\infty\right)$, and we upper bound with: 
\begin{align*}
    40\sum_{i=0}^\infty \int_{L_i} f_{Z}(z)\sqrt{\frac{4}{3}k\cdot2^{i+1}} \enspace dz
    &\le 70 \sqrt{k}\sum_{i=0}^\infty \Pr[\rZ\in L_i] 2^{i/2}
    \le 70 \sqrt{k} \sum_{i=0}^\infty \Pr\left[\rZ\ge \frac{4}{3}k\cdot2^{i}\right] 2^{i/2}.
\end{align*}
Using the following remark:
\begin{remark}[\cite{LaurentMassart}, equation 4.3, page 1326]\ \newline
    Let $\rZ\sim \chi^2_k$, $y > 0$ then:
    $$\Pr[\rZ\ge2\sqrt{ky}+2y+k]\le \exp(-y).$$
\end{remark}
With $y= c\frac{4}{3} k$, where $c = \frac{1}{8^2}2^i$. We have: 
\begin{align*}
    2\sqrt{ky}+2y+k 
    &= \left(\sqrt{c}\frac{4}{\sqrt{3}}+c\frac{8}{3}+1\right)k
    \le \left(\frac{4\sqrt{3}}{24}+\frac{1}{24}+1\right)k\cdot 2^i
    \le \frac{4}{3}k\cdot 2^i.
\end{align*}
Hence we can finish the bound for this case, using the assumption that $k\ge \gamma_i^{-2} 72 \ln(2)$
\begin{align*}
    70\sqrt{k} \sum_{i=0}^\infty \Pr\left[\rZ\ge \frac{4}{3}k\cdot2^{i}\right] 2^{i/2}
    &\le70\sqrt{k} \sum_{i=0}^\infty \exp\left(-\frac{1}{48} k \cdot 2^i\right) 2^{i/2}\\
    &\le70\sqrt{k}\exp\left(-\frac{1}{48} k\right) \sum_{i=0}^\infty \exp\left(-\frac{1}{48} k \right)^{i} 2^{i/2}\\
    &\le140\sqrt{k}\exp\left(-\frac{1}{48} k\gamma_i^2\right).
\end{align*}

\paragraph{Collecting the three cases.}
Hence in total $|\frac{\partial}{\partial\alpha}\phi|$ for $\alpha\in [-1/\sqrt{2}, 0]$ and $|\frac{\partial}{\partial\alpha}\rho|$ for $\alpha\in \left(\gamma_i,1\sqrt{2}\right]$ are bounded by:
\begin{align*}
    80\sqrt{k} \exp\left(- k\gamma_i^2/800\right)
    + 50\sqrt{k}\exp\left(-\frac{k\gamma_i^2}{50}\right)
    + 140\sqrt{k}\exp\left(-\frac{1}{48} k\gamma_i^2\right)\\
    \le 140\exp\left(-k\gamma_i^2/800\right).
\end{align*}
Using that $\gamma_{i+1}=2\gamma_i$, this completes the proof of Lemma~\ref{lem:PhiLips}.