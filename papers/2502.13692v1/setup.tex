\subsection{Setup}
\label{sec:reduct}
When eventually bounding the Lipschitz constant, as discussed in Section~\ref{sec:overview}, the task turns out to be simpler if $\|x\|_2=1$ (and not just $\|x\|_2 \leq 1$) for all $x$ in the support of $\cD$ and if $|\langle w,x\rangle|$ is sufficiently smaller than $1$ for all hypotheses $w$ and data points $(x,y)$ in the support of $\cD$. We thus start by reducing to this case. 

Consider the following distribution $\cD'$ obtained by sampling an $(\rx,\ry) \sim \cD$ and replacing $\rx$ by $\rx'=(c_\gamma \rx) \times \{ \sqrt{1-c_\gamma^2\|\rx\|_2^2}\} \in \ncH$ for a sufficiently small constant $0 < c_\gamma < 1$. That is, scale down all coordinates of $\rx$ by $c_\gamma$ and append a $(d+1)$'st coordinate taking the value $\sqrt{1-c_\gamma^2\|\rx\|_2^2}$. Then the norm of the resulting point $\rx'$ is $\sqrt{c_\gamma^2\|\rx\|_2^2 + 1-c_\gamma^2\|\rx\|_2^2} = 1$. Similarly, for any $w \in \cH$, consider instead the hypothesis $w'=w \times \{0\}$. We observe that for any $x,w$, we have that $\langle w',x'\rangle = \langle w, c_\gamma x\rangle = c_\gamma\langle w, x \rangle$ and thus lies in the range $[-c_\gamma,c_\gamma]$ by Cauchy-Schwartz. This also implies that $\sign(\langle w', x'\rangle) = \sign(\langle w, x\rangle)$ and thus the generalization error of $w'$ under $\cD'$ and $w$ under $\cD$ are the same. 

With this in mind, we define $\finalH := \cH \times \{0\}$ and $\finalX$ as the set of all vectors $x$ in $\ncH$ where the norm of $x$ without its $(d+1)$'st coordinate is at most $c_\gamma$.

From hereon, we let $\cD$ be an arbitrary distribution over $\finalX \times \{-1,1\}$, and set out to prove that there is a constant $c>1$, such that with probability at least $1-\delta$ over $\rS \sim \cD^n$, it holds for all margins $\gamma \in (n^{-1/2}, c_\gamma]$ and all $w \in \finalH$ that
\begin{align}
\Loss_\cD(w) \leq \Loss_{\rS}^\gamma(w) + c \left(\sqrt{\Loss^{\gamma}_\rS(w) \cdot \left( \frac{ \ln(e/\Loss^{\gamma}_\rS(w))}{\gamma^2 n} + \frac{\ln(e/\delta)}{n} \right)} + \frac{\ln(e\gamma^2 n)}{\gamma^2 n} + \frac{\ln(e/\delta)}{n}  \right). \label{eq:maingoal}
\end{align}
Note that Theorem~\ref{thm:main} follows as an immediate corollary since margins change by a $c_\gamma$ factor in our transformation of the input distribution. Since $c_\gamma$ is a constant, this disappears in the constant factor $c$ in Theorem~\ref{thm:main} (note that for margins $\gamma \in [n^{-1/2}, c_\gamma^{-1} n^{-1/2})$ in Theorem~\ref{thm:main}, we cannot use the reduction, but here Theorem~\ref{thm:main} follows trivially as $c \ln(e \gamma^2 n)/(\gamma^2 n) > 1$ for sufficiently large $c$). 

\paragraph{Smaller Tasks.}
We now break the task of establishing~\eqref{eq:maingoal} into smaller tasks, where we consider margins $\gamma$ in a small range $(\gamma_i, \gamma_{i+1}]$ and only vectors $w \in \finalH$ with $\Loss^{(3/4)\gamma_i}_\cD(w)$ in a small range $(\ell_j,\ell_{j+1}]$. The purpose here is, that for one sub-task, we can treat margins and margin losses as the same to within constant factors. A union bound over all the sub-tasks then suffices to establish~\eqref{eq:maingoal}.

For a given distribution $\cD$, partition the range of values of the margin $\gamma \in (n^{-1/2},c_\gamma]$ into intervals $\Gamma_i = (2^{i-1} n^{-1/2}, 2^{i} n^{-1/2}]$ for $i=1,\dots,\lg_2(c_\gamma n^{1/2})$. Similarly, partition the possible values of $\Loss^\gamma_\cD(w) \in [0,1]$ into intervals $L_0 = [0, n^{-1}]$ and $L_i = (2^{i-1} n^{-1}, 2^{i} n^{-1}]$ with $i=1,\dots \lg_2 n$.

For a pair $(\Gamma_i, L_j)$ with $\Gamma_i = (\gamma_i, \gamma_{i+1}]$, define
\[
\subH(\Gamma_i,L_j) = \{ w \in \finalH : \Loss^{(3/4)\gamma_{i}}_\cD(w) \in L_j \}.
\]
For each pair $(\Gamma_i,L_j)$ we now prove an equivalent of~\eqref{eq:maingoal}, but tailored to the sub-task. The result is stated in the following lemma
\begin{lemma}
\label{lem:subgoal}
There is a constant $c>1$, such that for any $0 < \delta < 1$ and any pair $(\Gamma_i, L_j) = ((\gamma_i, \gamma_{i+1}], (\ell_j, \ell_{j+1}])$, it holds with probability at least $1-\delta$ over a random sample $\rS \sim \cD^n$ that
\begin{align}
\sup_{\substack{w \in \subH(\Gamma_i, L_j)\\\gamma \in \Gamma_i}} \left|\Loss_\cD(w) - \Loss^{\gamma}_\rS(w) \right| &\leq 
c \left(\sqrt{\ell_{j+1}\left( \frac{ \ln(e/\ell_{j+1})}{\gamma_{i+1}^2 n} + \frac{\ln(e/\delta)}{n} \right)} + \frac{\ln(e/\ell_{j+1})}{\gamma_{i+1}^2 n} + \frac{\ln(e/\delta)}{n} \right)\label{eq:subgoal}
\end{align}
\end{lemma}
Observe that while~\eqref{eq:maingoal} depends on $\gamma$ and ~\eqref{eq:subgoal} depends on $\gamma_{i+1}$, this is fine since $\gamma \leq \gamma_{i+1}$ for all $\gamma \in \Gamma_i$. However, recall that $\subH(\Gamma_i,L_j)$ refers to $w \in \finalH$ with $\Loss_{\cD}^{(3/4)\gamma_i}(w) \in L_j = (\ell_j,\ell_{j+1}]$. But the $\ell_{j+1}$ terms in~\eqref{eq:subgoal} need to be replaced by $\Loss^\gamma_{\rS}(w)$ to obtain~\eqref{eq:maingoal}. Thus we relate the two via the following lemma
\begin{lemma}
\label{lem:subgoal2}
There is a constant $c>1$, such that for any $0 < \delta < 1$ and any $\Gamma_i = (\gamma_i, \gamma_{i+1}]$, it holds with probability at least $1-\delta$ over a random sample $\rS \sim \cD^n$ that
\begin{align}
\forall w \in \finalH : \Loss_{\rS}^{\gamma_i}(w) \geq \frac{\Loss_{ \cD}^{(3/4)\gamma_i}(w)}{4} - c \left(\frac{\ln(e\gamma_{i+1}^2 n)}{\gamma_{i+1}^2 n} - \frac{\ln(e/\delta)}{n}\right).\label{eq:subgoal2}
\end{align}
\end{lemma}
We combine the sub-tasks and conclude
\begin{claim}
\label{clm:union}
    For any $0 < \delta < 1$, it holds with probability $1-\delta$ over $\rS \sim \cD^n$ that~\eqref{eq:subgoal} and~\eqref{eq:subgoal2} simultaneously hold for all $(\Gamma_i,L_j)$ and $\Gamma_i$, with slightly different constants $c$.
\end{claim}
Since Claim~\ref{clm:union} follows by a simple union bound, exploiting that for different values of $\ell_{j+1}$ and $\gamma_{i+1}$, we can afford to use different $\delta_{i,j} \approx \delta \exp(-\gamma_{i+1}^2 \ln(e/\ell_{j+1}))$ and $\delta_i \approx \delta \exp(-\gamma_{i+1}^{-2} \ln(e \gamma^2_{i+1} n))$, we have deferred the proof to Appendix~\ref{sec:aux}.

A simple combination of~\eqref{eq:subgoal} and~\eqref{eq:subgoal2} now gives
\begin{claim}
\label{clm:combine}
For any $0 < \delta < 1$ and training set $S$, if~\eqref{eq:subgoal} and~\eqref{eq:subgoal2} hold simultaneously for all $(\Gamma_i, L_j)$ and $\Gamma_i$, then~\eqref{eq:maingoal} holds for all $\gamma \in (n^{-1/2}, c_\gamma]$ and all $w \in \finalH$ for large enough constant $c>1$ in~\eqref{eq:maingoal}.
\end{claim}
Claim~\ref{clm:combine} follows by using that $\gamma \leq \gamma_{i+1}$ for $\gamma \in \Gamma_i$, and by using Lemma~\ref{lem:subgoal2} to relate all occurrences of $\ell_{j+1}$ in~\eqref{eq:subgoal} to $\Loss^\gamma_S(w)$. As this is rather straight forward calculations, we have deferred the proof to Appendix~\ref{sec:aux}.

What remains is thus to establish Lemma~\ref{lem:subgoal} and Lemma~\ref{lem:subgoal2}, where we may now focus on a small range of $\gamma$ and $\Loss^{(3/4)\gamma_i}_{\cD}(w)$. While both require substantial work and non-trivial arguments, the proof of Lemma~\ref{lem:subgoal2} follows mostly the previous work by~\cite{SVMbest} and has thus been deferred to Section~\ref{sec:withinconstant}.


