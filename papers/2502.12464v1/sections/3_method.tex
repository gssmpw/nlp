\section{Method}
\label{sec:method}
\subsection{Preliminaries}
\label{sec:preliminaries}
Given a user prompt $\rvx \in \mathcal{X}$ and its response $\rvy\in\mathcal{Y}$, generated by an LLM, we utilize a safety guard model $p: \mathcal{X} \times \mathcal{Y} \to [0,1]$ to predict its harmfulness, where $\mathcal{X}$ is the set of all possible prompts and $\mathcal{Y}$ is the set of all possible responses, including an empty response. The safety guard model estimates the probability of the pair being harmful as $p(c=1\mid \rvx, \rvy)$ and classifies it as harmful if the probability exceeds a threshold $\delta\in (0,1)$. Here $c\in\{0,1\}$ is a binary variable indicating the harmfulness of the prompt-response pair. Note that when the response  $\rvy$ is empty, the safety guard model only evaluates the harmfulness of the prompt $\rvx$.





\subsection{SafeRoute: Adaptive Model Selection}
\label{sec:adaptive_safety_guard_model_selection}
In this section, we introduce \textbf{SafeRoute}, our proposed adaptive mechanism for selecting safety guard models to optimize the trade-off between efficiency and accuracy.
% motivation of adaptive small/lare model selectin
\paragraph{Observation.}  We observe that a smaller safety guard model $q: \mathcal{X} \times\mathcal{Y}\to[0,1]$ correctly predicts harmfulness of many prompt-response pairs. 
However, there are cases where the larger safety guard model $p$ correctly classifies harmfulness, while the smaller safety guard model $q$ makes mistakes. 
Based on this, if we can identify which model makes the correct prediction for each prompt-response pair $(\rvx_i, \rvy_i)$, with label $c_i$, we can potentially improve prediction accuracy by selecting the appropriate safety guard model's prediction, while simultaneously minimizing the overhead of using the larger model, as follows: 
\begin{align*}
    \begin{cases}
        \one_{\{p(c=1 \mid \rvx_i, \rvy_i) > \delta \}}\text{,} 
        & \text{if } 
        \begin{array}[t]{rl}
            \one_{\{p(c=1 \mid \rvx_i, \rvy_i) > \delta\}} & = c_i, \\
            \one_{\{q(c=1 \mid \rvx_i, \rvy_i) > \delta\}} & \neq c_i
        \end{array} \\
        \one_{\{q(c=1 \mid \rvx_i, \rvy_i) > \delta \}}\text{,} 
        & \text{otherwise},
    \end{cases}
\end{align*}
where $\one$ denotes an indicator function. We use the prediction of the larger safety guard model, $p$, if it correctly classifies the prompt-response pair $(\rvx_i, \rvy_i)$, while the smaller model does not. Otherwise, we rely on the prediction of the smaller safety guard model, as there is no benefit to using the larger model in such cases.  

As shown in \Cref{tab:oracle-prompt}, this hypothetical combination of two safety guard models, denoted as ``Oracle'', achieves a significantly higher F1 score on the WildguardMix~\citep{wildguard} test split compared to using either the smaller model $q$, \texttt{Llama-Guard-3-1B}~\citep{metallamaguard3} or the larger model $p$, \texttt{Llama-Guard-3-8B}~\citep{metallamaguard3} alone, while utilizing only a small portion of the larger model.
\begin{table}[t]
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{llcc}
    \toprule    
    \textbf{Model} & \textbf{Type} & \textbf{F1} &  \textbf{Usage of Large} \\
    \midrule
    \rowcolor{rowgray}
    \texttt{Llama-Guard-3-1B}     & Small & 0.6702   & \phantom{00}0.00\% \\
    \texttt{Llama-Guard-3-8B} & Large & 0.7054 & 100.00\% \\
    \rowcolor{rowgray}
    Oracle  & Hybrid    &  \textbf{0.8101} & \phantom{00}5.09\% \\
    \bottomrule
    \end{tabular}
}
\vspace{-0.5em}
\caption{\looseness=-1
\textbf{Safety F1 score and larger model usage ratio} on the WildGuardMix test split~\cite{wildguard}.}
\label{tab:oracle-prompt}
\vspace{-0.15in}
\end{table}

% hypothesis: easy/hard prompt
\paragraph{Dataset creation and training.}
Building on the observation that some examples are ``easy'' while others are ``hard'', we propose training a binary safety guard router, \textbf{SafeRoute}, to distinguish between these instances. 
This allows for adaptive selection between smaller and larger safety guard models, thereby optimizing the trade-off between efficiency and accuracy compared to using either model in isolation.
To train \textbf{SafeRoute},  we use a dataset of prompt-response pairs with harmfulness labels, $\mathcal{D}=\{(\rvx_i, \rvy_i, c_i)  \}_{i=1}^n$, and assign a binary label $t_i\in\{0,1\}$ to each prompt-response pair, $(\rvx_i, \rvy_i)$, as follows:
\begin{align}
    t_i=\begin{cases}
        1\text{,} 
        & \text{if } 
        \begin{aligned}[t]
            \one_{\{p(c=1 \mid \rvx_i, \rvy_i) > \delta\}} &= c_i \\
            \text{and } \one_{\{q(c =1 \mid \rvx_i, \rvy_i) > \delta\}} &\neq c_i
        \end{aligned} \\
        0\text{,} 
        & \text{otherwise}.
    \end{cases}
\label{eq:label}
\end{align}
Then, we train a neural network-based router $f_\theta: \mathcal{X}\times \mathcal{Y}\to [0,1]$ to minimize the following binary cross-entropy loss:
\begin{align*}
\begin{split}
    \mathcal{L}(\theta;\hat{\mathcal{D}}) = -\frac{1}{\lvert \hat{\mathcal{D}}\rvert}&\sum_{(\rvx,\rvy, t)\in \hat{\mathcal{D}}} \big( t \cdot \log f_\theta(\rvx, \rvy) + \\ 
    &(1-t)  \cdot \log \left(1-f_\theta(\rvx, \rvy)\right) \big),
\end{split}
\end{align*}
where $\hat{\mathcal{D}}=\{(\rvx_i, \rvy_i, t_i)\}_{i=1}^n$.

\looseness=-1
\paragraph{Data augmentation.} Since the dataset $\hat{\mathcal{D}}$ contains only a small number of examples with label $t_i=1$,  we augment the training dataset $\mathcal{D}$ with paraphrased inputs. Specifically, we prompt the LLM, \texttt{Llama-3.1-8B-Instruct}~\citep{llama-3}, to generate multiple paraphrases for each prompt-response pair $(\rvx,\rvy)\in\mathcal{D}$. We then label both the synthesized dataset and the original dataset following~\Cref{eq:label}, resulting in an augmented dataset $\hat{\mathcal{D}}_\texttt{aug}=\{(\rvx_i, \rvy_i, t_i) \}_{i=1}^m$. Finally, we train the router $f_\theta$ to minimize the loss $\mathcal{L}(\theta;\hat{\mathcal{D}}_\texttt{aug})$.

\paragraph{Parameterization.} There are many ways to parameterize the binary router $f_\theta$. However, additional overhead of utilizing $f_\theta$ should be minimized to ensure efficiency. Moreover, for better decision-making, the router should capture what the smaller safety guard model, $q$, knows and does not know about its input. To achieve this, we extract the last tokenâ€™s hidden representation from the final layer of the smaller safety guard model, as the safety guard model directly uses this last token representation for harmfulness prediction. The binary router can utilize this extracted feature to learn patterns of correct and incorrect predictions. For efficient training and inference, we always freeze the feature extractor, which enables us to reuse the last layer feature for predictions of harmfulness with $q$.


\paragraph{Inference.} At inference time, for given a test prompt-response pair $(\rvx_*, \rvy_*)$, we compute the score of selecting the larger model as $f_\theta(\rvx_*, \rvy_*)$. If the score exceeds a certain threshold $\epsilon \in (0,1)$, we utilize the larger safety guard model $p$ to predict the harmfulness of the prompt-response pair $(\rvx_*, \rvy_*)$. Otherwise, we use the smaller safety guard model $q$ for the prediction of $(\rvx_*, \rvy_*)$.



\subsection{Theoretical analysis} To further understand the effectiveness of our proposed adaptive approach, we provide a theoretical analysis of its risk bound. Specifically, we analyze how the selection mechanism, governed by the router $f_\theta$, influences the overall performance by comparing the risk of the adaptive model to that of an oracle model with perfect selection.

Let $\ell(p(\rvx,\rvy), c)= -(c \log p(c=1\mid \rvx,\rvy) +(1-c)\log p(c=0\mid \rvx,\rvy))$ be the binary cross-entropy loss with the larger safety guard model $p$ and labeled data $(\rvx,\rvy,c)$. The loss $\ell(q(\rvx,\rvy),c)$ is defined in the same manner for $q$. We define, $I(\rvx,\rvy) = \one_{\{ f_\theta(\rvx,\rvy) >\epsilon \}}$, where  the router $f_\theta$ determines which safety guard model is selected. The risk of our adaptive model given $p$ and $q$ is: 
\begin{align*}
\begin{split}
    R_\text{adaptive} = \mathbb{E}&[I(\rvx,\rvy)\ell(p(\rvx,\rvy), c)  \\+
    &(1-I(\rvx,\rvy))\ell(q(\rvx,\rvy), c) ],
\end{split}
\end{align*}
where the expectation is taken over an unknown data distribution.  The oracle risk is then given by:
\begin{align*}
    \begin{split}
        R_\text{oracle} = \mathbb{E}&[t(\rvx,\rvy)\ell(p(\rvx,\rvy),c) 
        \\+ &(1-t(\rvx,\rvy)) \ell(q(\rvx,\rvy),c)],
    \end{split}
\end{align*}
where $t(\rvx,\rvy)$ represents the optimal model selection strategy, as defined in~\Cref{eq:label}. 
\looseness=-1
\begin{thm}
Assuming that $\mathbb{E}[\lvert \ell(p(\rvx,\rvy),c)-\ell(q(\rvx,\rvy),c)\rvert^2]$ is bounded, we can bound the risk of our adaptive model as follows:
\begin{equation*}
\begin{gathered}
    R_\text{adaptive} \leq R_\text{oracle} + M\sqrt{\mathbb{P}\left(I(\rvx,\rvy)\neq t(\rvx,\rvy)\right)},
\end{gathered}
\end{equation*}
where $M=\sqrt{\mathbb{E}[\lvert \ell(p(\rvx,\rvy),c)-\ell(q(\rvx,\rvy),c)\rvert^2]}$.
\label{thm}
\end{thm}
The proof is deferred to~\Cref{app:proof}.
This theorem indicates that the gap between $R_{\text{adaptive}}$ and $R_{\text{oracle}}$ depends on the probability of incorrect selection $\mathbb{P}(I(\rvx,\rvy)\neq t(\rvx,\rvy))$, which decreases as $f_\theta$ improves. Consequently, as the number of training samples for $f_\theta$ increases, reducing its generalization error, the risk bound tightens. In the asymptotic case where $f_\theta$ perfectly approximates $t$, we achieve $R_{\text{adaptive}} = R_{\text{oracle}}$. In contrast, other entropy-based model selection baselines, described in~\Cref{sec:exp}, do not guarantee such optimality. 
A smaller model, even with perfect calibration, cannot predict what the larger model knows and therefore cannot reduce the error. 
