\clearpage
\appendix
\label{sec:appendix}


\section{\texorpdfstring{Proof of~\Cref{thm}}{Proof Theorem 3.1}}
\label{app:proof}
\begin{proof}
\begin{align*}
    R_\text{adaptive} - R_\text{oracle} = &\mathbb{E}[I(\rvx,\rvy)\ell(p(\rvx,\rvy),c) \\
    &+ (1-I(\rvx,\rvy))\ell(q(\rvx,\rvy),c) \\
    &- t(\rvx,\rvy)\ell(p(\rvx,\rvy),c) \\
    & - (1-t(\rvx,\rvy))\ell(q(\rvx,\rvy),c)].
\end{align*}
Taking the absolute value and using the fact that
\begin{equation*}
    \lvert I(\rvx,\rvy) -t(\rvx,\rvy)\rvert = \one_{\{ I(\rvx,\rvy) \neq t(\rvx,\rvy)\}},
\end{equation*}
we obtain the following inequality,
\begin{align*}
    &\lvert R_\text{adaptive} - R_\text{oracle} \rvert \\
    &\leq \mathbb{E}[
    \one_{\{I(\rvx,\rvy) \neq t(\rvx,\rvy) \}} \lvert \ell(p(\rvx,\rvy),c) - \ell(q(\rvx,\rvy),c)\rvert
    ].
\end{align*}
For notational brevity, we use $I\neq t$ to denote $I(\rvx,\rvy)\neq t(\rvx,\rvy)$. By applying the Cauchy-Schwarz inequality, we obtain the final result,
\begin{align*}
    &\lvert R_\text{adaptive} - R_\text{oracle} \rvert \\&\leq \sqrt{\mathbb{E}[
    \one^2_{\{I \neq t \}}]} \sqrt{\mathbb{E}[ \lvert \ell(p(\rvx,\rvy),c) - \ell(q(\rvx,\rvy),c)\rvert^2 
    ]} \\
    &= \sqrt{\mathbb{E}[\one_{\{I(\rvx,\rvy) \neq t(\rvx,\rvy)\}}]} M\\
    &= \sqrt{\mathbb{P}(I(\rvx,\rvy)\neq t(\rvx,\rvy))} M
\end{align*}
where $M=\sqrt{\mathbb{E}[ \lvert \ell(p(\rvx,\rvy),c) - \ell(q(\rvx,\rvy),c)\rvert^2 
    ]}$. Thus, we have
\begin{equation*}
    R_\text{adaptive} \leq R_\text{oracle} + M \sqrt{\mathbb{P}(I(\rvx,\rvy) \neq t(\rvx,\rvy))}.
\end{equation*}

\end{proof}


\section{Data Statistics}
\label{sec:data_statistics}

\begin{table}[ht]
    \centering
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}{lccc}
    \toprule
    \textbf{Dataset}     & \# of safe & \# of harmful & Total \\
    \midrule
    OAI     & 1,158 & \phantom{0}522  & 1,680\\
    WildGuardMix & 1,407 &  \phantom{0}282& 1,689\\
    WildGuardMix-p & \phantom{0,}945 & \phantom{0}754 & 1,699 \\
    ToxicChat & 4,721 & \phantom{0}362 & 5,083\\
    XSTest & \phantom{0,}368 & \phantom{0}\phantom{0}78 & \phantom{0,}446 \\
    Harmbench & \phantom{0,}329 & \phantom{0}273 & \phantom{0,}602 \\
    \bottomrule
    \end{tabular}}
    \caption{Statistics of each dataset.}
    \label{tab:data-stat}
\end{table}

\section{Safety Guard Models}
\label{sec:model}
We use PyTorch~\citep{pytorch} and Transformers~\citep{transformers} to implement all methods. All the pre-trained models, including safety guard models, used for our experiments are available in Hugging Face Hub. We list the identifier and link for each model on the Hugging Face Hub in~\Cref{tab:model}.

\begin{table}[t]
    \centering
    \resizebox{0.98\columnwidth}{!}{\begin{tabular}{lc}
    \toprule
    \textbf{Model} & \textbf{Hugging Face Hub Identifier} \\
    \midrule
    \rowcolor{rowgray}
     \texttt{Llama-Guard-3-1B}    & \href{https://huggingface.co/meta-llama/Llama-Guard-3-1B}{meta-llama/Llama-Guard-3-1B}  \\
    \texttt{Llama-Guard-3-8B}     & \href{https://huggingface.co/meta-llama/Llama-Guard-3-8B}{meta-llama/Llama-Guard-3-8B}  \\
    \rowcolor{rowgray}
    \texttt{Granite-Guardian-3-8B} & \href{https://huggingface.co/ibm-granite/granite-guardian-3.0-8b}{ibm-granite/granite-guardian-3.0-8b}\\
    \texttt{ModernBert} & \href{https://huggingface.co/answerdotai/ModernBERT-large}{answerdotai/ModernBERT-large} \\
    \rowcolor{rowgray}
    \texttt{Llama-3.1-8B-Instruct} & \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{meta-llama/Llama-3.1-8B-Instruct}\\
    \bottomrule
    \end{tabular}
    }
    \caption{Hugging Face Hub model identifiers for the pre-trained models used in our work.}
    \label{tab:model}
\end{table}

\input{captions/fig_flops}
\input{captions/fig_flops_guardian}
\input{captions/fig_large_ratio}
\input{captions/fig_large_ratio_guardian}
\section{Additional Experimental Results}
\label{sec:additional_experimental_results}

In \Cref{fig:flops} and \Cref{fig:flops_guardian}, we present trade-off between FLOPs and F1 score when adaptively selecting between  the smaller (\texttt{Llama-Guard-
3-1B}) and larger (\texttt{Llama-Guard-3-8B} and \texttt{Granite-Guardian-3-8B}, respectively) models. 
In \Cref{fig:large_ratio} and \Cref{fig:large_ratio_guardian}, we present trade-off between usage ratio of large model and F1 score when adaptively selecting between  the smaller (\texttt{Llama-Guard-
3-1B}) and larger (\texttt{Llama-Guard-3-8B} and \texttt{Granite-Guardian-3-8B}, respectively) models. 

\begin{figure*}[t]
    \centering
    \resizebox{0.95\linewidth}{!}{\begin{prompt}{Prompt}
        \footnotesize
        \textbf{USER}: Paraphrase the following prompt and response respectively while preserving their original semantics. Adhere strictly to the following format. First do not include original prompt and response.  Second, start the paraphrased prompt with ``prompt:''. Third, start the paraphrased response with ``response:''. Fourth, if response is given as None, just provide it as None. \\
        prompt: \{\texttt{prompt}\} \\
        response: \{\texttt{response}\} \\
        \textbf{ASSISTANT}: \\
\end{prompt}}    
    \caption{The prompt format for paraphrasing prompt-response pairs.}
    \label{fig:prompt}
\end{figure*}

\section{Prompt for Paraphrasing}
\label{app:para}
We present the prompt format for paraphrasing prompt-response pairs in \Cref{fig:prompt}.

