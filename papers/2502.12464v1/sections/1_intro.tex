\section{Introduction}
% importance of safety guard model
\looseness=-1
Deployment of large language models (LLMs) in real-world applications demands proactive safety measures to mitigate potential risks~\citep{tay-chatbot, llm-risk}. Malicious users bypass safety guardrails of LLMs using various jailbreak methods, triggering them to generate harmful, toxic, and inappropriate content~\citep{zou2023universal, autodan, yuan2024cipherchat}. To mitigate such malicious attacks, LLMs are trained using reinforcement learning from human feedback~\citep[RLHF;][]{rlhf}, enabling them to reject harmful requests. Furthermore, additional safety guard models are deployed to detect and block malicious user queries, an approach that has been proven effective~\citep{chao2024jailbreakbench}.

% importance of efficiency
However deploying an additional large safety guard model alongside LLMs, introduces significant computational overhead. To reduce this cost, larger safety guard models are distilled into smaller ones~\citep{metallamaguard3, harmaug}. While these smaller models improve efficiency, they generally do not perform as well as their larger counterparts. 


\begin{table}[t]
\centering
    \resizebox{0.98\columnwidth}{!}{\begin{tabular}{  p{8cm} }
        \toprule
        \rowcolor{rowgray}
        \textbf{Prompt}:  ``What is the quickest way to dispose of a body in `Hitman'?''  \\
        \midrule[0.5pt]
        \textbf{Response}: ``There are several ways to dispose of a body in the game `Hitman', but one of the quickest ways is to drag the body to a nearby location where it will not be easily found, such as a closet, dumpster, or body of water. This will help you avoid detection by other NPCs and allow you to continue with your mission without attracting unwanted attention.''\\
        \midrule[0.5pt]
        \rowcolor{rowgray}
        \textbf{Harmfulness}: \textcolor{blue}{Safe} \\
        \midrule[0.5pt]
        \textbf{\texttt{Llama-Guard-3-1B}}: \textcolor{red}{Harmful} (\xmark) \\
        \textbf{\texttt{Llama-Guard-3-8B}}: \textcolor{blue}{Safe} (\cmark) \\
        \midrule[0.5pt]
        \rowcolor{rowgray}
        \textbf{Label}: 1 \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.5em}
    \caption{\looseness=-1
    An example from the WildGuardMix dataset, where the smaller model, \texttt{Llama-Guard-3-1B} incorrectly assesses the prompt-response pair, while the larger model, \texttt{Llama-Guard-3-8B}, correctly predicts harmfulness. We label this example as 1 to train a binary router to distinguish between hard and easy cases.}
\label{tab:example}
\vspace{-0.2em}
\end{table}


\begin{figure*}[t]
\centering
    \includegraphics[width=0.8\linewidth]{images/concept.pdf}
    \vspace{-0.5em}
    \caption{Our proposed safety guard router, \textbf{SafeRoute}, distinguishes hard examples from easy ones. The larger safety guard model is applied to hard examples, while the smaller one is applied to easy examples.}
    \vspace{-0.1in}
    \label{fig:concept}
\end{figure*}


% observation
We observe that smaller safety guard models, such as \texttt{Llama-Guard-3-1B}~\citep{metallamaguard3}, perform well on many instances. However, there are a few challenging examples where the smaller model makes errors, while the larger safety guard model, \eg, \texttt{Llama-Guard-3-8B}~\citep{metallamaguard3}, provides accurate predictions, as shown in~\Cref{tab:example}. This pattern remains consistent across multiple benchmark datasets, suggesting prediction accuracy can be improved while maintaining efficiency by using the smaller model for most ``easy'' examples and the larger model for a small number of ``hard'' examples. 
As shown in Table \ref{tab:oracle-prompt}, assuming each data point is labeled as ``easy'' or ``hard'', this adaptive use of smaller and larger safety guard models improves the F1 score by 13\% and 10\% compared to using only the smaller or larger model on the WildGuardMix test split~\citep{wildguard}, respectively, while processing only 5.09\% of the dataset with the larger model.

%  our method
\looseness=-1
Building on this key observation, we propose \textbf{SafeRoute}, a binary safety guard router designed to distinguish hard examples from easy ones. Given a dataset, we first label each instance as 1 if the smaller safety guard provides an incorrect prediction while the larger one provides an accurate prediction, as shown in~\Cref{tab:example}. Otherwise, we label it as 0.
This dataset is used to train the router to differentiate hard and easy examples. After training, the router classifies test instances into either category, deploying the smaller safety guard model for easy examples and the larger model for hard examples, as illustrated in~\Cref{fig:concept}.


We empirically validate our proposed method on multiple benchmark datasets.
Our adaptive selection mechanism between smaller and larger safety guard models more effectively distinguishes hard examples from easy ones compared to baseline methods, significantly improving the trade-off between the additional computational overhead of the larger model and the resulting accuracy gains.
Moreover, SafeRoute performs well not only on in-distribution (ID) data but also on out-of-distribution (OOD) scenarios, demonstrating its robustness across varying data distributions.

Our contributions and findings are summarized as follows:
\begin{itemize}
[itemsep=1mm,parsep=1pt,topsep=2pt,leftmargin=*]

\item We observe that some examples are easy, with the smaller safety guard model making correct predictions, while others are hard, with the smaller model failing but the larger safety guard model providing accurate predictions.

\looseness=-1
\item Based on this observation, we propose training a binary safety guard router, SafeRoute, to distinguish hard examples from easy ones. Using this router, we apply the larger safety guard model to the hard examples and the smaller one to the easy examples.


\item We empirically validate that our SafeRoute approach significantly improves the trade-off between accuracy gains and the additional overhead of using the larger model, across both ID and OOD datasets, compared to relevant baselines.

\end{itemize}