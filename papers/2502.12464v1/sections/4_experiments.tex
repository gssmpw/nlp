\section{Experiments}


\subsection{Experimental Setups}
\label{sec:exp}
\paragraph{Datasets.} For the training dataset $\mathcal{D}$, we use the train split of WildGuardMix~\citep{wildguard}. We evaluate our method on six public benchmark datasets: the test split of \textbf{WildGuardMix}, \textbf{WildGuardMix-p}, 
OpenAI Moderation~\citep[\textbf{OAI};][]{oai-ds}, \textbf{ToxicChat}~\citep{toxic-chat}, \textbf{XSTest}~\cite{xstest}, and \textbf{HarmBench}~\citep{harmbench}. The WildGuardMix-p dataset is a subset of the WildGuardMix test split, containing only instances with prompt harmfulness labels, excluding those without them.
WildGuardMix-p, OAI, and ToxicChat datasets are used for prompt classification (\ie, a response is always an empty sequence), while the others are for prompt-response pair classification. Please see \Cref{tab:data-stat} in \Cref{sec:data_statistics} for data statistics.


\looseness=-1
\paragraph{Implementation details.} 
We use \texttt{Llama-Guard} \\ \texttt{-3-1B}~\citep{metallamaguard3} as the smaller model \( q \) and \texttt{Llama-Guard-3-8B}~\citep{metallamaguard3} or \texttt{Granite-Guardian-3-8B}~\citep{granite-guardian} as the larger model \( p \). Following \citet{liu2024calibration}, we define the safety binary distribution as follows:
\begin{align*}
p(c=1|\rvx,\rvy) = \frac{\exp(z_{p,1})}{\exp(z_{p,0}) + \exp(z_{p,1})},
\end{align*}
where \( z_{p,0} \) and \( z_{p,1} \) are the logits of the safe and unsafe tokens from the safety guard model $p$. We use 10\% of the WildGuardMix training split as a validation set for tuning \( f_\theta \) and set the number of paraphrases per example to 7. The input features of \( f_\theta \) are the last-layer outputs of the small model, selecting only the final token.
We implement \( f_\theta \) as a three-layer Bayesian neural network~\citep{blundell2015weight}, where each layer consists of an affine transformation, layer normalization~\cite{ba2016layer}, and a ReLU~\citep{Nair2010RectifiedLU} activation, except in the last layer. The posterior is approximated by a Gaussian with a diagonal covariance matrix, while the prior follows \( \mathcal{N}(0, 0.1) \). The Kullback-Leibler divergence weight is set to 0.01.
To maintain efficiency, we use 1 Monte Carlo sample for both training and inference. We train \( f_\theta \) for 1000 epochs with a mini-batch size of 512, approximately balancing \( t = 0 \) and \( t = 1 \) per batch. The parameters \( \theta \) are optimized using Adam~\cite{kingma2014adam} with a 0.001 learning rate, linear decay, and 100 warmup steps.
We run experiments five times with different random seeds for the \textbf{Random} baseline and \textbf{SafeRoute}, both of which involve stochastic components. All experiments are conducted on a single \href{https://www.nvidia.com/en-us/data-center/h200/}{NVIDIA H200 Tensor Core GPU}.
We present the \href{https://huggingface.co/docs/hub/index}{Hugging Face Hub} identifiers for all pretrained models used in this paper in \Cref{tab:model} of \Cref{sec:model}.

\input{captions/tab_binary_classification}
\input{captions/fig_latency}

\paragraph{Baselines.} We compare our method against the following baselines: 
\begin{enumerate}[leftmargin=0in, topsep=5pt, leftmargin=*]
\item[1-2.] \textbf{Small} and \textbf{Large}: These methods use either only the smaller or larger safety guard models.

\vspace{-0.05in}
\item[3.] \textbf{Random}: This method randomly selects a safety guard model, choosing the larger one with 50\% probability.
%This method selects either the smaller or larger safety guard model at random, with a given probability 0.5 of selecting the larger model. %Specifically, it uses the larger model if $b=1$, where $b\sim\text{Bernoulli}(\rho)$.


\vspace{-0.05in}
\item[4.] \textbf{Entropy}: In this method, the entropy of smaller safety guard model is computed as follows:
\begin{align*}
    \begin{split}
    H(\rvx,\rvy)=&-q(c=0|\rvx,\rvy) \log_2 q(c=0|\rvx,\rvy) \\ 
    &-q(c=1|\rvx,\rvy) \log_2 q(c=1|\rvx,\rvy).
    \end{split}
\end{align*}
When the entropy exceeds 0.5, indicating high uncertainty, we use the larger safety guard model. In the following three calibration methods (\textbf{TS}, \textbf{CC}, and \textbf{BC}), we calibrate the distribution $q$ of the smaller guard model to improve uncertainty estimation for better decision-making.


\vspace{-0.05in}
\item[5.] \textbf{Temperature Scaling (TS)}~\cite{guo2017calibration}: This method is a widely used confidence calibration technique for neural networks. We divide the logits, $z_{q,0}$ and $z_{q,1}$, of the smaller safety guard model $q$  by $\tau\in\mathbb{R}_{>0}$ and renormalize it:
\begin{equation*}
    \hat{q}(c=1\mid \rvx,\rvy)= \frac{\exp(z_{q,1}/\tau)}{\exp(z_{q,0}/\tau) + \exp(z_{q,1}/\tau)}.
\end{equation*}
We optimize $\tau$ to maximize the log-likelihood of the  WildGuardMix training split~\citep{wildguard}. Then we 
compute the entropy $H(\rvx,\rvy)$ using the calibrated distribution $\hat{q}$ and select the larger model if the entropy exceeds 0.5; otherwise, the smaller model is chosen.

\vspace{-0.05in}
\item[6.] \textbf{Contextual Calibration (CC)}~\cite{zhao2021calibrate}: This method is a matrix scaling technique designed to mitigate contextual bias in LLMs, with the key advantage of not requiring a validation set. It calibrates the output distribution of $q$ using content-free tokens, such as a string of whitespace, $\emptyset=\text{`` ''}$, as follows:
\begin{align*}
\hat{q}(c=1|\rvx,\rvy) &= \frac{\frac{q(c=1|\rvx,\rvy)}{q(c=1|\emptyset)}}{\frac{q(c=0|\rvx,\rvy)}{q(c=0|\emptyset)}+\frac{q(c=1|\rvx,\rvy)}{p(c=1|\emptyset)}}
\end{align*}
with $\hat{q}(c=0\mid \rvx, \rvy)=1-\hat{q}(c=1\mid \rvx, \rvy)$. Similar to TS, we select the larger model $p$ based on the entropy with the distribution $\hat{q}$.

\vspace{-0.05in}
\item[7.] \textbf{Batch Calibration (BC)}~\cite{zhou2023batch}: 
BC is another matrix scaling technique that calibrates the output distribution $q$ using batch probabilities ($\bar{q}_0, \bar{q}_1$), computed as follows:
\begin{align*}
\hat{q}(c=1|\rvx,\rvy) &= \frac{\frac{q(c=1|\rvx, \rvy)}{\bar{q}_1}}{\frac{q(c=0|\rvx, \rvy)}{\bar{q}_0}+\frac{q(c=1|\rvx, \rvy)}{\bar{q}_1}}
\end{align*}
with $\hat{q}(c=0\mid \rvx, \rvy)=1-\hat{q}(c=1\mid \rvx, \rvy)$, where $\bar{q}_1 = \frac{1}{|\mathcal{D}^\prime|} \sum_{(\rvx^\prime,\rvy^\prime)\in\mathcal{D}^\prime} q(c=1|\rvx', \rvy')$ and $\bar{q}_0 = 1-\bar{q}_1$. 
For a fair comparison, we use the training split of WildGuardMix for $\mathcal{D}^\prime$ (\ie, $\mathcal{D}^\prime = \mathcal{D}$). Similar to TS, we select the larger safety guard model based on the entropy with the distribution $\hat{q}$.

\vspace{-0.05in}
\item[8.] \textbf{Oracle}: As described in \Cref{sec:adaptive_safety_guard_model_selection}, this method combines the smaller and larger safety guard models, using the larger one only when the smaller one is incorrect and the larger one is correct. Assuming access to the true label $c$, it provides an upper bound on accuracy for adaptive model selection. However, it always requires two forward passes, one for the smaller model and one for the larger model, making it the most computationally expensive method.

\end{enumerate}




% \paragraph{Evaluation metrics.}
% Following the convention of prior works~\citep{llama-guard, wildguard}, we evaluate the safety guard models using F1 score. 


\input{captions/tab_binary_classification_guardian}
\input{captions/fig_latency_guardian}
\subsection{Experimental results.}

\paragraph{Routing results using \texttt{Llama-Guard-3-8B}.} To evaluate how accurately our SafeRoute model $f_\theta$ is able to distinguish hard examples from easy ones, we compare its routing predictions with the corresponding labels $t_i$, as defined in~\Cref{eq:label}, and compute F1 score. As shown in~\Cref{tab:binary_classification}, SafeRoute outperforms naive entropy-based methods, such as TS, CC, and BC, by a large margin on most benchmark datasets, except for OAI. The performance of SafeRoute shows the importance of learning to identify examples where the larger model classifies correctly while the smaller model makes errors. While the entropy of the smaller model correlates with its likelihood of making incorrect predictions, it provides no insight into the behavior of the larger model. This limitation leads to an increased number of false positives, resulting in lower F1 scores compared to our approach. 

% \looseness=-1

% \input{captions/fig:three}
\input{captions/fig_ablation}
\input{captions/fig_jailbreaktype}

\paragraph{Trade-off using \texttt{Llama-Guard-3-8B}.}
We observe a similar pattern in trade-off between latency and F1 score when adaptively selecting between smaller and larger models. As shown in~\Cref{fig:latency}, SafeRoute significantly improves the F1 score over using the smaller model alone while achieving performance comparable to the larger model. Moreover, the increase in latency due to using the larger model on some examples is smaller than that of any baseline. 
This can be attributed SafeRoute's more accurate routing decisions compared to entropy-based methods, which frequently misclassify examples and introduce significantly higher computational overhead. 

\vspace{-0.05in}
\paragraph{Routing results using \texttt{Granite-Guardian-3-8B}.}
In addition to \texttt{Llama-Guard-3-8B}, we train the router $f_\theta$ using \texttt{Llama-Guard-3-1B} and \texttt{Granite-Guardian-3-8B}, and evaluate the router on the same six benchmark datasets used in previous experiments. As shown in~\Cref{tab:binary_classification_guardian}, our proposed SafeRoute  more accurately distinguishes hard examples from easy ones across all datasets except for OAI, which is consistent with the results from previous experiments.

\vspace{-0.05in}
\paragraph{Trade-off using \texttt{Granite-Guardian-3-8B}.}
When \texttt{Granite-Guardian-3-8B} is used, the improved routing ability also leads to a better trade-off between latency and F1 score improvements compared to other baselines across four datasets, as illustrated in~\Cref{fig:latency_guardian}. For OAI and Harmbench, SafeRoute achieves lower latency but slightly lower F1 score gains than the CC and Entropy baselines. Although some entropy-based selection methods improve F1 score relative to using the smaller model alone, they introduce significantly higher latency overhead by more frequently selecting the larger model even when it provides no performance benefit.


\paragraph{Ablation studies.} 
We conduct ablation studies to evaluate how our design choices in \textbf{SafeRoute} affect performance, reporting the average routing F1 score across the six benchmark datasets used in previous experiments. Specifically, we examine the impact of:  
\textbf{(a)} Replacing the original sequence pooling method (last token) with an average, maximum, or minimum operator.  
\textbf{(b)} Replacing features from the smaller model $q$ with those from \texttt{ModernBERT}~\citep{modernbert}, a bidirectional encoder based on BERT~\citep{bert} with rotary positional embeddings~\citep{rope} and local-global alternating attention. We also explore using features from layers of the smaller model other than the last (16th) layer.  
\textbf{(c)} Removing paraphrased prompt-response pairs from the training dataset $\mathcal{D}$.  

As shown in \Cref{fig:pooling_ablation}, using the last token as the feature for our router $f_\theta$ improves the average routing F1 score across all six datasets, highlighting both the simplicity and effectiveness of using the last token.  
\Cref{fig:layer_ablation} shows the importance of how inputs to the router is encoded. Notably, replacing features from the smaller model $q$ with \texttt{ModernBERT} features leads to severe overfitting, suggesting that \texttt{ModernBERT} fails to capture the uncertainties of $q$ and does not generalize well to unseen examples. This highlights the importance of leveraging features from the smaller model rather than relying on an external encoder. Additionally, using features from layers other than the last layer results in underperformance, indicating that these layers do not accurately capture what the smaller model does not know.  
Finally, as seen in \Cref{fig:aug_ablation}, removing paraphrased data degrades generalization performance, while increasing the number of paraphrases per example improves performance. However, performance plateaus beyond a certain number of paraphrases, likely due to limited diversity. Developing methods to synthesize diverse, high-quality data for augmentation remains an interesting direction for future work.



\paragraph{Analysis of jailbreak attacks.} We analyze how SafeRoute selects the larger safety guard model for different jailbreak attacks in the HarmBench dataset. Specifically, we examine its behavior against AutoDan~\citep{autodan}, TAP~\citep{tap}, PAP~\citep{pap}, AutoPrompt~\citep{autoprompt}, GCG~\citep{zou2023universal}, UAT~\citep{uat}, PAIR~\citep{pair}, and GBDA~\citep{gbda}. As shown in~\Cref{fig:jailbreaktype}, both the oracle and SafeRoute select the larger model most frequently for the PAP attack. Since this attack exploits persuasive taxonomy to elicit harmful responses from LLMs, the smaller model is more prone to errors than other types of attacks. On the other hand, both models select the larger model less frequently for the GCG attack. This may be attributed to the fact that this jailbreak attack is well-known, and many of its instances are included in the dataset used to train the smaller model.


% \input{captions/tab:ablation}


% \input{captions/fig:aug_ablation}