\section{Related Work}
\vspace{-0.05in}
\paragraph{Safety guard models.} Detecting harmful sentences has been a longstanding interest in the safety research community. Deep neural networks have been widely adopted to detect harmful user queries~\citep{hatebert, offensive-reddit, roberta-hate-speech}. Recently, LLMs with safety alignment have been prompted to judge the harmfulness of conversations between users and AI assistants~\citep{chao2024jailbreakbench}. 
Instead of relying on general-purpose LLMs, specialized safety guardrails are implemented by fine-tuning LLMs on labeled datasets~\citep{granite-guardian, wildguard, harmaug, metallamaguard3}. 
They moderate input prompts and output responses, thereby enabling the safe use of LLMs. 

% \looseness=-1
\vspace{-0.05in}
\paragraph{Efficiency.} Deploying safety guard models alongside LLMs introduces additional computational overhead. To mitigate this cost, larger safety guard models are distilled into smaller ones~\citep{metallamaguard3, harmaug}. While this improves efficiency, smaller models typically underperform compared to their larger counterparts.
In this work, we aim to optimize the trade-off between computational overhead and accuracy by adaptively selecting between a larger and a smaller safety guard model based on input difficulty. Our approach is conceptually similar to speculative decoding~\citep{speculative-decoding, speculative-decoding-2, speculative-decoding-3}, where a smaller model generates a draft and a larger model verifies it, as both methods leverage models of different sizes to enhance computational efficiency.
Our method adaptively selects the model for each data point, allowing the larger model to be bypassed when appropriate. 
In contrast, speculative decoding always relies on the larger model to verify the smaller modelâ€™s output.
