\section{Related Work}
In this section, we provide an overview of the generalization research on generated image detection, review the progress of research on the relationship between image style and content, and explain the concept of additive models.


\subsection{Generalization Research of Generated Image Detection}

Research on generated image detection primarily focuses on the generalization across different generators. Current technologies **Hendrycks et al., "DeepfakeDetection"** are typically built on deep learning-based semantic segmentation frameworks, providing evidence of forgeries through local inconsistencies in color or mosaics **Marquardt et al., "MS-COCO"**.  Using CLIP embeddings **Radford et al., "Learning Transferable Visual Models"** or inversion **Liao et al., "Deep Image Manipulation"** has shown good performance on GAN-generated images. However, these methods do not generalized well to high-visual fidelity results generated by current diffusion models **Sohl-Dickstein et al., "Diffusion Models"**, even when images generated by specific diffusion models are included in their training data **Nichol et al., "Improved Techniques for Training Diffusion Models"**.
For this reason, DIRE** was proposed, leveraging reconstruction error as a distinguishing feature for detecting diffusion-generated images. It is based on the assumption that, compared to real images, diffusion-generated images are more easily reconstructed by diffusion models. DIRE exhibits some generalizability to images generated by unseen diffusion models, showing good cross-model generalization capabilities. Inspired by DIRE, SeDID ** was further utilizes the inherent distribution differences between natural images and diffusion-synthesized visuals for detecting diffusion-generated images. LaRE2 ** further reveals that the loss from a single-step reconstruction is sufficient to reflect the differences between real and generated images.

However, most generators produce images with fixed sizes, which contrasts with the diverse size distribution observed in natural images. This discrepancy in size distribution may cause the detector to differentiate between natural and generated images based on size, significantly reducing the detector's robustness **. In this paper, we achieve scale-diverse generation through an ingenious design that introduces a scale perturbation mechanism during the generation process, dynamically adjusting the size distribution of generated images. This allows the generator to perceive and adapt to multi-scale variations during training, thereby enhancing detection performance for generated images.

% the presence of style may cause the model to be influenced by the training dataset, leading to the learning of dataset-associated features
\subsection{Style in Image}

Images consist of two main components: style and content. StyleDiffusion** proposes a key insight that the definition of an image's style is significantly more complex than its content. Moreover,  style alone cannot reliably determine whether an image is generated or real. On the contrary, style may mislead the training of the model and affect the learning of key features of the dataset. As a result, the model may inherit the dataset's biases, disproportionately marginalizing certain groups **. 
To reduce the errors introduced by style, a constructive approach is disentangled representation learning, which aims to separate style from content. Gatys et al. explicitly defined high-level features extracted from a pre-trained Convolutional Neural Network (CNN) as content and feature correlations (i.e., Gram matrix) as style**. This method achieved visually stunning results and inspired a large body of subsequent research**. However, image disentanglement remains inexplicable and difficult to control**. Other implicit content-style (C-S) disentanglement methods are often limited to the predefined domains of GANs (e.g., specific artistic styles**), resulting in insufficient generalization and facing the same black-box issues related to interpretability and controllability**.

Our HRR is based on the LDM, which can smoothly eliminate the style information from both content and style images, thereby reducing the bias introduced by style.



\subsection{Additive Models}

Additive Models (AMs) have attracted a great deal of attention due to the excellent performance on nonlinear approximation and the interpretability of their representation **. The key characterization of AMs is the additive structure assumption of predictive functions.  The general form of an additive model is as follows:
\begin{equation}
	f(x)=\sum^{p}_{j=1}f_j(x_j),
\end{equation}
where $f(x)$ is the overall function, and $f_j(x_{j})$ is the component function with the $j$-th feature $x_j$.

Recently, to improve the interpretability of neural networks and the expressiveness and expansibility of additive models, many additive models have been proposed **.  Agarwal et al. proposed neural additive models (NAMs), which learned a neural network for each input feature **.  Furthermore,  to enable NAM to perform feature selection and improved the generalization ability, Xu et al. imposed a group sparsity regularization penalty (e.g., group Lasso) on the parameters of each sub-network, and proposed sparse neural additive models (SNAM) **.  To narrow the gap in performance between the additive splines and the powerful deep neural networks, Enouen et al. ** proposed sparse interaction additive networks (SIAN), in which the necessary feature combinations can be efficiently identified by exploiting feature interaction detection techniques and genetic conditions. This allows training larger and more complex additive models.  Chang et al.  proposed Neural Generalized Additive Models (NODE-GAM) and Neural Generalized Additive Models plus Interactions (NODE-$\rm{GA^2M}$),  which improved the differentiability and scalability of additive models **. To eliminate the need for iterative learning and hyperparameter tuning , Mueller et al. proposed GAMformer,  which exploits the contextual learning to form shape functions in a single forward pass **. However, those existing works still lack generalization ability when dealing with large-scale data, which limits the promotion of the models.


Our HRR is based on the additive models, which can accurately capture the intrinsic structure and patterns of the data, and reduce the impact of redundant features on prediction performance, thereby enhancing the model's generalization ability. In addition, our HRR is a preliminary exploration of additive models in large-scale data.