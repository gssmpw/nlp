
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Threat Model and Scope}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our threat model considers scenarios where attacker and victim processes are co-located in shared hardware environments, which expose vulnerabilities to \Mi attacks.
Co-location can manifest in several forms, including but not limited to threads on the same process, processes on the same host and virtual machines on a shared server. These attacks exploit shared \Mi resources to infer sensitive data from victim processes, bypassing traditional memory isolation mechanisms.

We assume the CPU microcode is up-to-date with the latest mitigations, and the software has no bugs and SMT is enabled.
We assume no access to the confidential design details of the processor, limiting our analysis to black-box testing. This restriction reflects the real-world scenario where attackers must rely on external observations and performance counters to reverse-engineer the processor's internal behavior.

Although we have not seen example of such an exploit in real-life yet, if unmitigated, these attacks can lead to significant data breaches, including the extraction of cryptographic keys and other sensitive information.
In this work, we focus on discovering \Mi vulnerabilities using reinforcement learning and we focus on the following questions: 
\medskip

\noindent
\textbf{Q1.} How can we design an RL framework that efficiently explores the \Mi space? \\

\noindent \textbf{Q2.} Can RL discover unknown \Mi vulnerabilities? \\

\noindent \textbf{Q3.} What are the challenges and limitations of using RL for \Mi vulnerability discovery?

