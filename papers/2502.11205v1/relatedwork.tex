\section{Literature Review}
Recognizing the importance of a joint housing and household inventory for disaster management and urban planning, several researchers have contributed to the housing-household joining effort. Harada and Murata \cite{harada2017projecting} used geospatial data to allocate households to housing units based on building specifications, such as building types and locations. Rosenheim et al. \cite{rosenheim2021integration} developed a more granular joint synthetic household and housing inventory by expanding and linking different datasets through random or rule-based processes for assigning households to housing units. Ye et al. \cite{ye2024enhancing} integrated LiDAR (Light Detection and Ranging) data, Points of Interest (POI), and quadratic programming techniques to create a population-building inventory. 

These studies illustrate the progression of joint household and housing unit modeling, evolving from random allocations to data-driven approaches. However, they also reveal a persistent challenge and research gap: the lack of housing-household relationships that capture the interaction between household characteristics and housing unit features, which is critical for advancing accurate disaster impact research. As noted by Aerts et al. \cite{aerts2018integrating}, individual and family characteristics play a significant role in decision-making processes, influencing housing choices. Ignoring these correlations undermines the accuracy and validity of subsequent research. Thus, developing methods that can accurately model housing-household relationships is essential. However, matching tabular information such as household characteristics and housing unit features faces many data and methodological challenges, as detailed earlier. 

Capturing relationships among heterogeneous datasets, commonly termed schema matching \cite{nachouki2008multi, johnston2008web}, is an active research area. Current table-matching methods largely focus on identifying content similarity between tables. For instance, the Valentine system \cite{koutras2021valentine} is an open-source experimental suite with a user-friendly graphical user interface (GUI) that is designed for large-scale schema matching. Structure-aware Bidirectional Encoder Representations from Transformers (StruBERT) \cite{trabelsi2022strubert} introduces structure- and context-aware features by integrating structural and textual information, leveraging deep contextualized language models like BERT to capture semantic similarities between tables. Similarly, Schema Matching Using Generative Tags and Hybrid Features (SMUTF) \cite{zhang2024smutf} approach combines rule-based feature engineering, pre-trained language models, and generative large language models to perform large-scale schema matching, excelling in cross-domain tasks. While these methods have advanced table-matching techniques, they mainly focus on matching similar content (e.g., Merging duplicate records in databases when two entries share overlapping attributes), whereas feature alignment involves interpreting and transforming different data types (e.g., household attributes and housing unit attributes) to reveal meaningful relationships, which is the core challenge of our research.

Recent advances in information fusion have demonstrated remarkable progress in integrating heterogeneous data sources for enhanced predictive capabilities. In biomedical applications, deep learning-based fusion methods have shown effectiveness in combining medical imaging, biomarkers, and clinical data to improve diagnostic accuracy and treatment planning \cite{zitnik2019machine, duan2024deep, zhao2024review}. In cancer research, fusion methods have successfully integrated imaging and omics data to enhance prognosis prediction and treatment response assessment \cite{lu2024privacy}.
Urban computing has similarly benefited from cross-domain data fusion techniques, particularly in integrating geographical, traffic, social media, and environmental data \cite{zou2025deep}.

While existing information fusion approaches primarily focus on aggregating heterogeneous data sources for enhanced prediction \cite{mai2023learning}, our method takes a distinct approach by learning the inherent alignment patterns from co-occurring data pairs. We leverage the co-occurrence relationships in housing-household microdata to learn a generalizable feature alignment model. This approach enables us to extend beyond the training dataset and match previously unseen housing-household pairs, providing a valuable tool for researchers in social sciences, urban computing, and disaster management. The learned feature alignment model effectively captures the complex interplay between housing and household characteristics, facilitating more accurate housing allocation analysis and disaster impact assessment.

Recent advancements in feature alignment have leveraged developments in learning-based methods from computer vision and multimodal data fusion, often referred to as Cross-modal Retrieval Methods \cite{cao2022image}. For example, the CLIP (Contrastive Language-Image Pre-training) method \cite{radford2021learning} pioneered a transformative approach by training on large-scale datasets of image-text pairs, addressing limitations in labeled data for visual recognition tasks. Building on this, ALIGN (A Large-scale ImaGe and Noisy-text embedding) \cite{jia2021scaling} scaled training data to billions of image-text pairs, achieving significant improvements in zero-shot classification. BLIP (Bootstrapping Language-Image Pre-training) \cite{li2022blip} introduced a unified framework for vision-language pre-training, incorporating captioning and filtering techniques to manage noisy web-crawled data. Other notable work includes ALBEF (Align Before Fuse) \cite{li2021align}, which used contrastive loss to align image and text representations, and UNITER (Universal Image-Text Representation) \cite{chen2020uniter}, which proposed a unified architecture for diverse vision-language tasks. These methodologies have significantly advanced cross-modal retrieval and representation learning. 

While cross-modal retrieval methods hold great promise, they are not directly suited to our objectives. These methods focus on matching data from different modalities that describe the same subject, such as pairing an image of a cat with its textual description. In contrast, our goal is to match inherently distinct features within tabular data—specifically, housing unit attributes and household characteristics. While these features pertain to different domains, one describing physical structures and the other human subjects, they share latent connections. This distinction highlights unique challenges and opportunities. While we can draw inspiration from the contrastive learning techniques employed in cross-modal retrieval, these methods must be adapted to align two different features within a single modality (tabular data). Our approach aims to capture the complex, multi-dimensional relationships between housing and household features, bridging a gap that traditional cross-modal techniques fail to address.

Given the data challenges we face, self-supervised learning with pretext tasks, where models learn from auxiliary tasks without requiring labeled data, presents a promising alternative. Pretext tasks allow models to learn from unlabeled data by solving contrived objectives that reveal inherent data patterns, with the learned representations transferable to downstream tasks. For example, BERT’s Next Sentence Prediction (NSP) task trains models to predict whether one sentence logically follows another, enabling an understanding of how two segments of discourse are connected to each other, either logically or structurally \cite{devlin2018bert}. However, RoBERTa \cite{liu2019roberta} showed that removing NSP and instead training on longer, contiguous text sequences improves downstream performance. In tabular data feature alignment, SubTab introduces pretext tasks like Reconstruction, Contrastive Learning, and Feature Vector Distance, which effectively capture tabular representations \cite{ucar2021subtab}. 

While these methods focus on general-purpose representations, our approach is tailored specifically to housing-household matching relationships. First, we use a task-specific architecture: our CLIP-inspired model processes household and housing features separately, simplifying the computational complexity from $O(N^2)$ to $O(N)$ for matching tasks. Second, we formulate a task-specific objective: we employ a single contrastive learning task optimized for housing-household alignment rather than multiple generic pretext tasks. This specialized design results in superior performance, robustness, and computational efficiency, enabling our model to address the unique challenges of housing-household matching more effectively than existing general-purpose methods, as shown in Section \ref{sec:results}.