%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{lmodern}

% Define custom colors
\definecolor{BrightGreen}{RGB}{0,153,51}
\definecolor{Crimson}{RGB}{220,20,60}

% Define custom colors (HTML or RGB values):
\definecolor{TemporalGreen}{HTML}{238B45}     % A darker green for temporal carryover
\definecolor{SpatialBlue}{HTML}{0E67E3}         % Blue for spatial confounding
\definecolor{InterferenceRed}{HTML}{E31A1C}   % Red for interference

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}

\usepackage{std_definitions}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{GST-UNet: Spatiotemporal Causal Inference with Time-Varying Confounders}

\begin{document}

\twocolumn[
\icmltitle{GST-UNet: Spatiotemporal Causal Inference with Time-Varying Confounders}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Miruna Oprescu}{sch}
\icmlauthor{David K. Park}{yyy}
\icmlauthor{Xihaier Luo}{yyy}
\icmlauthor{Shinjae Yoo}{yyy}
\icmlauthor{Nathan Kallus}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Computing and Data Sciences, Brookhaven National Laboratory}
\icmlaffiliation{sch}{Cornell University, Cornell Tech}

\icmlcorrespondingauthor{Miruna Oprescu}{amo78@cornell.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Spatiotemporal causal inference, time-varying confounders, G-computation, neural networks, treatment effects, observational data}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Estimating causal effects from spatiotemporal data is a key challenge in fields such as public health, social policy, and environmental science, where controlled experiments are often infeasible. However, existing causal inference methods relying on observational data face significant limitations: they depend on strong structural assumptions to address spatiotemporal challenges -- such as interference, spatial confounding, and temporal carryover effects -- or fail to account for \textit{time-varying confounders}. These confounders, influenced by past treatments and outcomes, can themselves shape future treatments and outcomes, creating feedback loops that complicate traditional adjustment strategies.
To address these challenges, we introduce the \textbf{GST-UNet} (\textbf{G}-computation \textbf{S}patio-\textbf{T}emporal \textbf{UNet}), a novel end-to-end neural network framework designed to estimate treatment effects in complex spatial and temporal settings. The GST-UNet leverages regression-based iterative G-computation to explicitly adjust for time-varying confounders, providing valid estimates of potential outcomes and treatment effects. To the best of our knowledge, the GST-UNet is the first neural model to account for complex, non-linear dynamics and time-varying confounders in spatiotemporal interventions.
We demonstrate the effectiveness of the GST-UNet through extensive simulation studies and showcase its practical utility with a real-world analysis of the impact of wildfire smoke on respiratory hospitalizations during the 2018 California Camp Fire. Our results highlight the potential of GST-UNet to advance spatiotemporal causal inference across a wide range of policy-driven and scientific applications.
\end{abstract}

\section{Introduction}

Environmental hazards, public health interventions, and large-scale socio-economic policies often require understanding complex cause-and-effect relationships across space and time \citep{reid2016differential, papadogeorgou2019causal, song2020spatial}. For instance, when evaluating air quality interventions, policymakers need to assess how industrial regulations affect both immediate local health outcomes and broader regional impacts that evolve over time. These applications demand robust methods for estimating causal effects from observational spatiotemporal data.

However, spatiotemporal causal inference poses unique challenges. First, outcomes at each location are typically influenced by both local and neighboring covariates and interventions, leading to spatial confounding and interference. Second, observations exhibit strong temporal dependencies, with the effects of interventions “carrying over” in time. Finally, \textit{time-varying confounders}--covariates that both influence and are influenced by past treatments and outcomes--create feedback loops that violate standard assumptions of independence over time. Consider, for example, how air quality policies are often implemented: governments may impose stricter regulations in response to poor air quality and high hospitalization rates, which in turn affect future air quality and health outcomes. Moreover, when estimating effects, we aim to understand the impact of different intervention sequences at specific locations given the observed history---a more challenging task than estimating average effects across space or time.

Current approaches to spatiotemporal causal inference either rely on restrictive modeling assumptions that may not capture real-world complexity, or employ flexible neural networks that are limited to single time points, independent time series, or settings without time-varying confounding (see \cref{sec:lit-review} for a comprehensive overview). Further complicating matters, spatiotemporal settings often provide only a single realization of the process over time, rather than multiple independent time series.

To bridge this methodological gap, we propose GST-UNet (G-computation Spatiotemporal UNet), a novel end-to-end neural network framework designed to estimate treatment effects in complex spatiotemporal settings. GST-UNet addresses the key challenges of interference, spatial confounding, temporal carryover, and time-varying confounding by combining two powerful approaches: a U-Net-based encoder-decoder network
%convolutional architecture 
for capturing spatial dependencies, and a regression-based, iterative G-computation procedure \citep{bang2005doubly, robins2008estimation} for adjusting for time-varying covariates. This integration enables valid estimation of potential outcomes while flexibly modeling complex 
% and potentially nonlinear 
dynamics across the spatial grid.

 Our contributions are threefold: (1) We introduce the GST-UNet model architecture and provide theoretical foundations and implementation details for its iterative G-computation scheme—to the best of our knowledge, this is the first neural model to account for complex spatial dynamics and time-varying confounders in spatiotemporal interventions; (2) We demonstrate GST-UNet's effectiveness through extensive simulation studies designed to showcase key spatiotemporal complexities and time-varying confounders; and (3) We illustrate its practical utility through a real-world analysis of wildfire smoke impacts on respiratory hospitalizations during the 2018 Camp Fire in California. %We expect GST-UNet to enable more precise estimation of spatiotemporal intervention effects, helping policymakers design targeted interventions across environmental, public health, and social policy domains.


\section{Related Work}\label{sec:lit-review}

We summarize the most relevant prior work here, with a more detailed discussion in \cref{app:extended-lit}.

\textbf{Classical Spatiotemporal Causal Inference.} Earlier spatiotemporal causal inference methods (e.g., spatial econometrics \citep{anselin2013spatial}, difference-in-differences \citep{keele2015geographic}, and synthetic controls \citep{ben2022synthetic}) rely on strong assumptions (e.g., parallel trends) and often fail to address interference or time-varying confounders. More recent classical approaches, on the other hand, typically estimate average effects at the regional level or rely on structural and modeling assumptions that may not hold in real-world spatiotemporal contexts \citep{wang2021causal, christiansen2022toward, papadogeorgou2022causal, zhang2023spatiotemporal, zhou2024estimating}.

\textbf{Machine Learning for Spatiotemporal Modeling.} Machine learning models (e.g., convolutional and recurrent networks-based methods \citep{shi2015convolutional, zhang2017deep}, or graph-based approaches \citep{li2017diffusion, wu2019graph}) capture spatiotemporal patterns for prediction but lack formal causal adjustments.

\textbf{Time Series Causal Inference.} Time-series causal inference often uses recurrent or transformer-based methods \citep{bica2020estimating, seedat2022continuous, melnychuk2022causal} but assumes independent time series, ignoring potential interference effects. Although iterative G-computation \citep{bang2005doubly, robins2008estimation} or marginal structural models \citep{robins2000marginal} can handle time-varying confounders, most ML extensions \citep{lim2018forecasting, li2021g, hess2024g} exclude interference or cross-series confounding.

\textbf{Neural-Based Spatiotemporal Causal Inference.} In the context of neural spatiotemporal models, \citet{tec2023weather2vec} integrate spatial representations for causal inference, accounting for spatial confounding and leveraging temporal data to train a UNet model. However, they do not address feedback effects from lagged or time-varying confounders. Most similar to our work, \citet{ali2024estimating} present a climate-focused model that shares certain architectural similarities but emphasizes prediction rather than adjusting for time-varying confounders, leaving causal identification concerns largely unaddressed. 

%\textbf{Our contribution.} The GST-UNet bridges these gaps by combining a U-Net-based architecture for complex spatiotemporal grids with iterative G-computation to handle time-varying confounders. Unlike prior methods, GST-UNet accommodates interference, nonlinear spatiotemporal dynamics, and valid causal identification, enabling fine-grained, location-specific causal effects estimation.

\section{Problem Formulation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/ST_Data_Diagram_2.jpg}
    \vspace{-1.5em}
    \caption{Observational data (left) versus interventional data (right) for a horizon $\tau=2$ across multiple locations $(s,s')$. Green arrows indicate temporal carryover, blue arrows show spatial confounding, and red arrows depict interference; dashed arrows denote time-varying confounding, and dashed circles represent unobserved variables at inference time. Under the intervention (right), treatments are set independently of confounders, and the full history is not observed for the entire horizon.}
    \vspace{-1em}
    \label{fig:st-data}
\end{figure*}

\textbf{Spatiotemporal Data.} We model the observed data as random variables on a discrete spatial domain represented by an $N_X \times N_Y$ lattice: $\mathcal{S} = \{(i, j) \mid i \in [N_X], j \in [N_Y] \},$ where $[N] = \{1, \dots, N\}$ denotes the index set. Time is indexed by $t \in [T]$. At each spatial location $s = (i, j)$ at time $t$, we observe a tuple $(\Xb_{s,t}, A_{s,t}, Y_{s,t})$, where $A_{s,t} \in \{0, 1\}$ represents a binary treatment (or intervention), $Y_{s,t} \in \RR$ is a continuous outcome of interest, and $\Xb_{s,t} \in \RR^{d_X}$ is a vector of time-varying covariates (\eg local weather conditions, pollution levels, or socioeconomic indicators). Additionally, each location $s$ is associated with static features $V_s\in \RR^{d_v}$ (\eg geographical characteristics and socioeconomic indicators). While we focus on binary interventions for clarity, the methods generalize to more complex treatments. Conceptually, the data forms a 3D spatiotemporal tensor of size $T \times N_X \times N_Y$, though in practice, observations may be incomplete. Missing data can be accommodated using masking techniques during downstream modeling.

To streamline notation, we use boldface symbols for random variables defined over the entire spatial domain. For $U \in \{X, A, Y\}$, let $\mathbf{U}_t$ denote its value at time $t$, and let $\mathbf{U}_{t:t+\tau} = (\mathbf{U}_t, \dots, \mathbf{U}_{t+\tau})$ denote its value over a time interval. For a specific location $s$, we write $U_{s, t:t+\tau} = (U_{s, t}, \dots, U_{s, t+\tau})$. The history up to time $t$ is denoted by $\Hb_{1:t} = (\Xb_{1:t}, \Ab_{1:t-1}, \Yb_{1:t}, \Vb)$ for the entire spatial domain and $H_{s, 1:t} = (X_{s, 1:t}, A_{s, 1:t-1}, Y_{s, 1:t})$ for a specific location $s$. Specific instantiations of these random variables are denoted using lowercase letters (e.g., $u \in \{x, a, y, h\}$).

\textbf{Quantities of Interest.} Our primary goal is to estimate location-specific Conditional Average Potential Outcomes (CAPOs) for a sequence of future spatiotemporal interventions, conditioned on observed history. Our approach builds on Rubin's potential outcomes framework \citep{rubin1978bayesian, robins2008estimation, robins2000marginal}, which we extend to accommodate spatiotemporal settings. More concretely, we consider a future time horizon of length $\tau \geq 1$ and a predetermined interventional sequence $\ab_{t:t+\tau-1}$ applied across the spatial domain starting at time $t$. Our goal is to estimate the potential outcomes at time $t+\tau$, denoted as $\mathbf{Y}_{t+\tau}[\mathbf{a}_{t:t+\tau-1}]$. In particular, we aim to compute:
\begin{equation}\label{eq:capo}
    \EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t}=\hb_{1:t}]
\end{equation}
which represents the CAPOs at time $t+\tau+1$ under the given treatment sequence. Given two different interventional sequences $\ab_{t:t+\tau}$ and $\ab'_{t:t+\tau}$, a related secondary goal is to estimate the location specific Conditional Average Treatment Effect (CATE), given by:
\begin{equation*}
    \EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]-\Yb_{t+\tau}[\ab'_{t:t+\tau-1}]\mid \Hb_{1:t}=\hb_{1:t}]
\end{equation*}
Although we focus primarily on CAPOs, CATEs and other effect measures can be derived similarly.

\textbf{Assumptions.} Identification of CAPOs from observational data relies on standard causal inference assumptions.  Additionally, our setup relies on observing only \textit{a single spatiotemporal outcome path}. This prevents direct estimation of the CAPOs in Eq. \eqref{eq:capo} -- which relies on an expectation over multiple data samples -- without additional assumptions. We therefore introduce the following assumptions:

\begin{assumption}[Standard Causal Inference Assumptions]\label{assump:standard} 
We assume the following properties hold: \textit{(Consistency)} $\Yb_{t+\tau}=\Yb_{t+\tau}[\ab_{t:t+\tau-1}]$ whenever the observed sequence of treatments $\Ab_{t:t+\tau-1}$ satisfies $\Ab_{t:t+\tau-1}=\ab_{t:t+\tau-1}$; \;(\textit{Positivity}) $P(A_{s, t}=a_{s,t} \mid \Hb_{1:t}=\hb_{1:t})>0$ for any $a_{s,t}\in\{0, 1\}$ and feasible realization of history $\hb_{1:t}$; \;
(\textit{Sequential Unconfoundedness}) $\Yb_{t+1:T}[\ab_{t+1:T}]\perp \Ab_t\mid \Hb_{1:t}$, $ \forall \ab_{t+1:T}\in\{0,1\}^{T-t}$, \ie at each time step $t$, the treatment assignment is independent of future potential outcomes.
\end{assumption}

\begin{assumption}[Representation-Based Time Invariance]\label{assump:embedding} There exists a function (or embedding) $\phi:\Hcal \times \Acal \rightarrow \Zcal\subseteq \RR^h$ that maps $(\Hb_{1:t}, \Ab_t)$ to a finite-dimensional representation such that once we condition on $z=\phi(\Hb_{1:t}, \Ab_t)$, the distribution $(\Xb_{t+1}, \Yb_{t+1})$ does not explicitly depend on $t$. Formally, for any $t,t'\in \{1 ,\dots,T\}$ and $z\in \Zcal$, we have: 
 \begin{align*}
        & p(\Xb_{t+1}, \Yb_{t+1}\mid \phi(\Hb_{1:t}, \Ab_t)=z) \\
        & \quad = p(\Xb_{t'+1}, \Yb_{t'+1}\mid \phi(\Hb_{1:t'}, \Ab_{t'})=z).
\end{align*}
\end{assumption}
\cref{assump:standard} is standard in longitudinal causal inference settings (e.g., \citep{robins2000marginal, robins2008estimation, bica2020estimating, li2021g, melnychuk2022causal, hess2024g}). \cref{assump:embedding} is specific to the single-time series setting, where pooling information across time is essential to enable estimation. We note that the single time-series setting frequently arises in causal inference, where assumptions such as stationarity or strict time homogeneity enable consistent estimation \citep{bojinov2019time, papadogeorgou2022causal, zhou2024estimating}. In contrast, our representation-based time invariance is \emph{weaker}: rather than requiring $\Xb_{t}, \Yb_{t}$ themselves to have a time-invariant distribution, we only assume that, once the history is summarized by $\phi(\Hb_{1:t}, \Ab_{t})$, the transition to $(\Xb_{t+1}, \Yb_{t+1})$ follows a single shared mechanism. This approach aligns with modern time-series causal inference that learn time-invariant latent embeddings to pool information across time steps \citep{lim2018forecasting, li2021g, hess2024g}, thus leveraging more data for a single, stable representation rather than time-dependent parameters.

\textbf{Identification and G-Computation.}  We splice our single time series into multiple ``prefixes'' of varying lengths such that for each $t\in\{1,\dots,T-\tau\}$, we define the following 
\[
\Pb_t^\tau
\;=\;
\bigl(\Xb_{1:t+\tau},\Ab_{1:t+\tau},\Yb_{1:t+\tau},\Vb\bigr).
\]
Under time-invariance (\cref{assump:embedding}), conditioning on a suitable embedding of $\Pb_t^\tau$ renders the distribution of $\Yb_{t+\tau}$ independent of $t$. We will thus write expectations over the prefixes given history embeddings as 
\[
\EE_\Pb\!\bigl[\Yb_{t+\tau}\;\big|\;\phi(\Hb_{1:t}, \Ab_t)\bigr],
\]
where $t$ here only identifies the location in the series (\ie the segment's ending point) rather than implying a distinct distribution. Pooling across $t$ then supplies $T-\tau$ conditionally independent segments from a \emph{single} time series, enabling regression-based methods to estimate future outcomes from (embedded) histories.

Given $\Pb_t^\tau$, we show how to identify CAPOs from observational data. For $\tau \ge 2$, \emph{future} covariates and outcomes (\ie $\Xb_{t+1:t+\tau-1}, \Yb_{t+1:t+\tau-1}$) may influence subsequent treatments, creating \emph{time-varying confounding} \citep{coston2020counterfactual}. These confounders, shaped by past treatments and outcomes, can alter \emph{future} assignments and outcomes, forming feedback loops that simple “condition-on-history” adjustments miss. Hence, adjustment---\eg via iterative G-computation---is needed to avoid bias. \cref{fig:st-data} illustrates these complexities by comparing observational data (left) and hypothetical interventional data (right) for $\tau=2$. By contrast, when $\tau=1$, conditioning on $\Hb_{1:t}$ is sufficient under standard assumptions, as no future confounders lie between $\Ab_t$ and $\Yb_{t+1}$. In other words, the following naive identification does \emph{not} generally hold for $\tau>1$: 
\begin{align}\label{eq:naive}
    &\EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t}=\hb_{1:t}] \\
    & \neq \EE[\Yb_{t+\tau}\mid \Hb_{1:t}=\hb_{1:t}, \Ab_{t:t+\tau-1}=\ab_{t:t+\tau-1}]
\end{align}
To address time-varying confounding, we adapt \emph{regression-based iterative G-computation} \citep{bang2005doubly, robins2008estimation} to the spatiotemporal setting, providing a systematic way to adjust for evolving confounders and achieve unbiased CAPO estimates. We integrate our single time-series spatiotemporal setting with the G-computation framework in the following result:

\begin{theorem}[Identification with G-Computation] \label{thm:identification} 
    Assume that \cref{assump:standard} and \cref{assump:embedding} hold. 
    Further, let $\Hb^\ab_{1:t+k}:=(\Xb_{1:t+k}, [\Ab_{1:t-1}, \ab_{t:t+k-1}], \Yb_{1:t+k})$ denote the history where observed treatments from time $t$ onward are replaced by $\ab_{t:t+k-1}$. Define recursively:
    \begin{align*}
        & Q_\tau (\Hb_{1:t+\tau-1}, \Ab_{t+\tau-1}) \\
        & \;\; = \EE_\Pb[\Yb_{t+\tau}\mid \phi(\Hb_{1:t+\tau-1}, \Ab_{t+\tau-1})]\\
         & Q_{\tau-1} (\Hb_{1:t+\tau-2}, \Ab_{t+\tau-2}) \\
        & \;\; = \EE_\Pb[Q_\tau(\Hb^\ab_{1:t+\tau-1}, \ab_{t+\tau-1})\mid \phi(\Hb_{1:t+\tau-2}, \Ab_{t+\tau-2})]\\
        & \dots\\
        & Q_1(\Hb_{1:t}, \Ab_{t}) \\
         & \;\; = \EE_\Pb[Q_2(\Hb^\ab_{1:t+1}, \ab_{t+1})\mid \phi(\Hb_{1:t}, \Ab_{t})]
    \end{align*}
    Then:
    \begin{align*}
        & \EE_\Pb[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \phi(\Hb_{1:t}, \ab_t)=\phi(\hb_{1:t}, \ab_t)] \\
        & = Q_1(\hb_{1:t}, \ab_t)
    \end{align*}
\end{theorem}
We provide a proof of \cref{thm:identification} in \cref{app:identification-proof}. This result naturally motivates a recursive regression approach for spatiotemporal CAPO estimation, fitting each $Q_k(\cdot)$ in reverse order and substituting interventional treatments where required. In the next section, we detail how our end-to-end GST-UNet framework implements these ideas, providing a principled neural solution for time-varying confounding in spatiotemporal settings.

\section{Methodology} \label{sec:methodology}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/GSTUNet_Architecture.jpg}
    \vspace{-1.5em}
    \caption{Overview of the GST-UNet architecture. The embedding block (left) is a U-Net augmented with a ConvLSTM layer and attention gates. Its final feature map is passed to a set of \emph{G-heads} (right), where each G-head $Q_k$  implements iterative G-computation (see \cref{alg:gstunet}) to predict the potential outcomes or pseudo-outcomes at step $k$.}
    \vspace{-1em}
    \label{fig:gstunet-overview}
\end{figure*}

To address the challenges of time-varying confounders, spatial dependencies, and temporal carryover in observational data, we introduce \textbf{GST-UNet}, an end-to-end spatiotemporal neural model. Building on the identification guarantees in \cref{thm:identification} and the single-series assumptions in \cref{assump:embedding}, GST-UNet integrates iterative G-computation with a modular neural architecture to consistently estimate potential outcomes and treatment effects. We first convert the iterative G-computation result into a practical, recursive regression approach for CAPO estimation,  before introducing the GST-UNet design in subsequent subsections.

\subsection{Estimating CAPOs with Iterative G-Computation} \label{sec:iterative-g}

While \cref{thm:identification} motivates a recursive regression algorithm for each $Q_k$ ($k=1,\dots,\tau$), only $Q_\tau$ can be directly estimated from the prefix data.  At the next step, $Q_{\tau-1}$ depends on $Q_\tau\bigl(\Hb^\ab_{1:t+\tau-1}, \ab_{t+\tau-1}\bigr)$---where the observed treatments $\Ab_{t:t+\tau-1}$ are replaced by $\ab_{t:t+\tau-1}$---but such substituted outcomes are not observed in the prefix data.  Therefore, for $k<\tau$, we propose a procedure where we generate \emph{pseudo-outcomes} by predicting with the previously learned $\widehat{Q}_{k+1}$.  Going forward, we use $\widehat{F}$ to denote any quantity $F$ estimated from data. Formally, let $\phi\in\Phi$ be an embedding satisfying \cref{assump:embedding}, and let $\Qcal$ be our function class for $Q_k$.  We learn the sequence $\widehat{Q}_\tau,\dots,\widehat{Q}_1$ from prefix data 
$\{\Pb_t^\tau : t=1,\dots,T-\tau\}$, via: %the following steps:

\setlist{nolistsep}
\begin{enumerate}
    \item \textbf{Initialization.}
    With the prefix data, fit $\widehat{Q}_\tau$ to predict $\Yb_{t+\tau}$ from the embedding $\phi(\Hb_{1:t+\tau-1},\Ab_{t+\tau-1})$.
    \item \textbf{Backward recursion.} 
    For $k=\tau-1,\dots,1$:
    \begin{enumerate}
        \item \textit{Substitute interventions.} 
        For each prefix $\Pb_t^\tau$, replace $\Ab_{t+k}$ by the interventional $\ab_{t+k}$ to form the modified history $\Hb^\ab_{1:t+k}$.
        \item \textit{Generate pseudo-outcomes.} 
        Let 
        $\widetilde{Y}_{t+k+1} \;=\; \widehat{Q}_{k+1}\bigl(\Hb^\ab_{1:t+k}, \ab_{t+k}\bigr)$, 
        where $\widehat{Q}_{k+1}$ is the previously learned function.  
        These $\widetilde{Y}_{t+k+1}$ act as surrogates for $\Yb_{t+k+1}$ in the prefix data.
        \item \textit{Fit $\widehat{Q}_k$.} 
        Regress $\widetilde{Y}_{t+k+1}$ on the current embedding $\phi\bigl(\Hb_{1:t+k-1},\,\Ab_{t+k-1}\bigr)$ to learn $\widehat{Q}_k \in \Qcal$.
    \end{enumerate}

    \item \textbf{Final step.} 
    Given a new history $\hb_{1:t}$ and an interventional path $\ab_{t:t+\tau-1}$, we predict
    \begin{align*}
         &\widehat{\EE}_\Pb[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \phi(\Hb_{1:t}, \ab_t)=\phi(\hb_{1:t}, \ab_t)]\\
         & = \widehat{Q}_1\bigl(\hb_{1:t},\,\ab_t\bigr).
    \end{align*}
\end{enumerate}  
The iterative procedure yields a consistent estimator for the CAPOs if the $Q_k$'s are estimated consistently from data \citep{laan2003unified}. In the following subsections, we instantiate this procedure in our \textbf{GST-UNet} architecture, illustrating how to incorporate spatial dependencies and interference into $\phi$ and each $Q_k$, and implement a streamlined, end-to-end training strategy that unifies history embeddings and outcome predictions.

% TODO: show consistency against \widehat{\}
%\begin{proposition}[Consistency of the Iterative G-Computation Procedure]
%\label{prop:consistency}
%Suppose that for each $k=\tau,\tau-1,\dots,1$, there exist true functions $Q_k$ and a true embedding $\phi$ satisfying the assumptions in \cref{thm:identification} and \cref{assump:embedding}.  Let $\widehat{\phi}^{(n)}$ be a consistent estimator of $\phi$, and let $\widehat{Q}_k^{(n)}$ be the estimator of $Q_k$ learned via the backward-recursive procedure described in \cref{sec:iterative-g}, each fit on $n$ prefix samples.  If $\widehat{\phi}^{(n)}\xrightarrow{p}\phi$ and $\widehat{Q}_k^{(n)}\xrightarrow{p}Q_k$ for all $k$, then $\widehat{Q}_1^{(n)}$ is consistent for $Q_1$, and thus $\widehat{Q}_1^{(n)}(\hb_{1:t},\ab_t)$ consistently estimates the CAPOs.  We provide the full proof of this result in the Appendix.
%\end{proposition} % TODO: see if there is time for this proof and what other assumptions are required

\subsection{Model Architecture}\label{sec:gstunet}

The \textbf{GST-UNet} consists of two main components: 
\setlist{nolistsep}
\begin{enumerate}[leftmargin=1.5em,itemsep=0.2em,topsep=0.2em]
    \item \textbf{Spatiotemporal Learning Module.} A U-Net-based network augmented with ConvLSTM and attention gates for spatiotemporal processing.
    \item \textbf{Neural Causal Module.} $\tau$ G-computation heads, each mapping the spatiotemporal features to the final outcome predictions in the iterative procedure.
\end{enumerate}
We illustrate the GST-UNet architecture in \cref{fig:gstunet-overview} and describe its main components below.

\textbf{Spatiotemporal Learning Module.}
\textit{(1) Spatial Module.} To efficiently process high-dimensional spatial data, we employ U-Net~\citep{ronneberger2015u}, a fully convolutional architecture originally developed for biomedical image segmentation. It employs an encoder-decoder design with skip connections: the encoder progressively downsamples the spatial grid through convolution and pooling, while the decoder upsamples it back to the original resolution, merging encoder features at each scale. 
%More specifically, in the \emph{contractive path} (encoder), convolutions with non-linear activations progressively duplicate the number of latent channels, while pooling reduces spatial resolution and expands the receptive field. The \emph{expansive path} (decoder) then up-samples these high-level features, uses additional convolutions to localize them at a finer lattice, and merges encoder features via skip connections to retain spatial details.
% As a result, the U-Net output can have an arbitrary number of feature channels, enabling us to represent complex confounding and interference patterns in a rich, high-dimensional embedding.
\textit{(2) Temporal Module.} U-Net has limitations in capturing temporal information. To address this, we integrate a Convolutional Long Short-Term Memory (ConvLSTM) layer \citep{shi2015convolutional} to the U-Net encoder. This module captures temporal dependencies by maintaining a hidden state across time steps while aggregating spatial information through convolutions. After computing the final ConvLSTM state, we append static (time-invariant) covariates $\Vb$ as additional feature channels, ensuring the subsequent U-Net encoder-decoder has direct access to both temporal dynamics and static location-specific information. In the decoder, we incorporate \emph{attention gates} \citep{oktay2018attention} to selectively highlight relevant spatial regions, refining skip connections and emphasizing critical global or local patterns. The embedding module ultimately produces a $d_h$-dimensional feature map of size $N_X \times N_Y$, capturing essential spatiotemporal context---including interference, spatial confounding, and static covariates---for downstream G-computation. 

\textbf{Neural Causal Module.}
We attach $\tau$ \emph{G-computation heads} to the U-Net’s final feature maps, corresponding to the $Q_k$ estimators in the iterative procedure (see \cref{sec:iterative-g}). Each head can be a small convolutional module or a simple feed-forward network, depending on how much spatial structure remains to be captured. The information flow at the G-computation heads proceeds as follows: each head $Q_k$ ($k=1,\dots,\tau$) receives the $d_h \times N_X \times N_Y$ U-Net embedding $\widehat{\phi}\bigl(\Hb_{1:t+k-1}, \Ab_{t+k-1}\bigr)$ (encompassing spatiotemporal and static context) and outputs an $N_X \times N_Y$ prediction for that time step. We refer to this as the \emph{supervision step}, since $Q_\tau$ compares its predictions to the \emph{real} observed outcomes $\Yb_{t+\tau}$, anchoring the model in genuine data, while each $Q_{k<\tau}$ compares its predictions to pseudo-outcomes $\widetilde{\Yb}_{t+k+1}$ provided by $\widehat{Q}_{k+1}$. These pseudo-outcomes arise in a subsequent \emph{generation step}, wherein $Q_{k+1}$ processes the intervened history $\widehat{\phi}\bigl(\Hb^\ab_{1:t+k}, \ab_{t+k}\bigr)$ in a \emph{detached} forward pass (so $\widehat{Q}_{k+1}$ is not updated by $Q_k$’s loss), thereby creating surrogate targets for $Q_k$. This procedure realizes the iterative G-computation logic from \cref{sec:iterative-g}, enabling GST-UNet to estimate future outcomes under various counterfactual treatments. By separating the spatiotemporal embedding from the G-heads, we maintain a common representation for all prefix data (see \cref{assump:embedding}) and flexibly capture interference and spatial confounding. Each G-head enforces the proper temporal adjustments to yield bias-free counterfactual inference. 

\subsection{Training and Inference}\label{sec:gstunet-train}

\begin{algorithm}[tb]
   \caption{GST-UNet Training and Inference}
   \label{alg:gstunet}
\begin{algorithmic}[1]
   \STATE \textbf{Input:} Horizon $\tau$, prefix data $\{\Pb_t^\tau\}_{t=1}^{T-\tau}$, interventions $\ab_{t:t+\tau-1}$, curriculum schedule $\alpha_k^{(e)}$, total epochs $E$.
   \STATE \textbf{Initialize:} parameters $\theta$ (U-Net embedding + G-heads).
   \FOR{$e = 1 \ldots E$}
       \FOR{$k=\tau \ldots 1$}
           \STATE \textbf{(Supervision)} For each prefix $i$, predict outcomes:
            \vspace{-5pt}
            \begin{equation*}
               \widehat{Y}_{t+k}^{(i)} = Q_k\bigl(\phi(\Hb_{1:t+k-1}^{(i)}, \Ab_{t+k-1}^{(i)});\theta\bigr).
           \end{equation*}
           \STATE \textbf{(Generation (detached))} For each prefix $i$, generate pseudo-outcomes:
           \vspace{-5pt}
           \begin{align*}
               & \widetilde{Y}_{t+k}^{(i)}=\\
               & \begin{cases}
                Q_k\Bigl(\phi\bigl((\Hb^\ab_{1:t+k-1})^{(i)},\,\ab_{t+k-1}^{(i)}\bigr);\theta\Bigr), & k < \tau,\\
                Y_{t+\tau}^{(i)}, & k=\tau.
                \end{cases}
           \end{align*}
           where the observed $\Ab_{t:t+k-2}$'s were replaced with $\ab_{t:t+k-2}$ in the history.
        \ENDFOR
           \STATE \textbf{(Loss aggregation)} Compute the overall MSE loss
           \vspace{-5pt}
           \begin{equation*}
                \Lcal(\theta;e) = \frac{1}{\tau} \sum_{k=1}^\tau \alpha_k^{(e)} \sum_{i}
             \bigl(
                \widehat{Y}_{t+k}^{(i)}
               - 
                \widetilde{Y}_{t+k+1}^{(i)}
             \bigr)^2.
           \end{equation*}
           \STATE \textbf{(Backward pass)} Update $\theta$ by backpropagation.
   \ENDFOR
   \STATE \textbf{(Inference)} Given a $\hb_{1:t}$, return
   $Q_1(\phi(\hb_{1:t},\ab_t);\widehat{\theta})$.
\end{algorithmic}
\end{algorithm}


A key obstacle in learning from a single spatiotemporal series is that we must splice the data into many prefixes (\cref{assump:embedding}), then estimate all $Q_k$ in an iterative G-computation procedure (\cref{sec:iterative-g}).  In principle, we could train each G-head $Q_k$ sequentially (from $\tau$ down to $1$) by feeding its pseudo-outcomes to the next head.  However, this creates complications when sharing the U-Net embedding $\phi$ across all heads: each $Q_k$ might attempt to tailor $\phi$ to its own objective, leading to misaligned training signals. 

\textbf{Joint Loss and Multi-Task Training.}
To address this issue, we employ a \emph{joint} (or \emph{multi-task}) training approach \citep{caruana1997multitask, evgeniou2004regularized} by aggregating the loss terms from all G-heads into a single objective, then backpropagating once per batch.  Concretely, for each head $Q_k$, let $\widetilde{Y}_{t+k+1}$ be the \emph{real} outcomes if $k=\tau$ or \emph{pseudo-outcomes} (generated by $\widehat{Q}_{k+1}$) if $k<\tau$.  Our head-specific loss is a mean squared error (MSE) over all prefix samples:  
\begin{equation*}
\Lcal_k(\theta)
\,=\,
\sum_{i=1}^{T-\tau}
\left[
  Q_k\!\bigl(\phi(\Hb_{1:t+k-1}^{(i)}, \Ab_{t+k-1}^{(i)});\theta\bigr)
  -
  \widetilde{Y}_{t+k+1}^{(i)}
\right]^2,
\end{equation*}
where $\theta$ encompasses \emph{all} model parameters (the shared U-Net embedding $\phi$ and the G-heads $Q_k$).  Let $\alpha_k^{(e)}$ denote a \emph{head-weight} for epoch $e$.  We then form the overall training objective at epoch $e$ by
\begin{equation}\label{eq:multitask-obj}
    \Lcal(\theta; e) 
    \;=\;
    \frac{1}{\tau}\sum_{k=1}^\tau 
    \alpha_k^{(e)} \;\Lcal_k(\theta).
\end{equation}
By summing the losses and performing a single backward pass, we learn a common embedding $\widehat{\phi}$ that balances the needs of all G-heads, rather than fitting each head separately.

\textbf{Curriculum Training.} A naive implementation of \eqref{eq:multitask-obj} -- where we give each G-head an equal weights -- can be suboptimal: early in training, $Q_\tau$ (which sees real data) is inaccurate, and thus the pseudo-outcomes generated for $Q_{k<\tau}$ are effectively noise.  Consequently, heads $Q_{1},\dots,Q_{\tau-1}$ may overfit to poor targets before $Q_\tau$ has converged, leading to suboptimal solutions.  To mitigate this, we employ a \emph{curriculum} training approach \citep{bengio2009curriculum}, gradually increasing the loss weight of earlier heads as $Q_\tau$ improves.

While many curricula are possible, we adopt a simple scheme controlled by a single hyperparameter $e_c$ (the ``curriculum period'') so we can readily tune it.  Let $p(e) = \min\{\tau,\;\lceil e / e_c\rceil\}$, which indexes a ``phase'' based on the current epoch $e$.  We then define
\[
\alpha_k^{(e)}
\,=\,
\begin{cases}
1 / p(e),
 & \text{if } k \in \{\tau,\,\tau-1,\,\dots,\tau - p(e) +1\},\\
0,
 & \text{otherwise}.
\end{cases}
\]
Hence, during epochs $1 \le e \le e_c$ (phase $p(e)=1$), only $Q_\tau$ is active with $\alpha_\tau^{(e)}=1$; in the next interval $e_c < e \le 2e_c$ (phase $p(e)=2$), $Q_\tau$ and $Q_{\tau-1}$ each have weight $1/2$, and so on, until phase $\tau$ sets all heads to uniform weight $1/\tau$.  For $e > \tau\,e_c$, we continue training with $\alpha_k^{(e)}=1/\tau$ across all G-heads for additional joint refinement. This progressive schedule ensures $Q_\tau$ becomes reasonably accurate on the observed data before earlier heads rely on its pseudo-outcomes.  The single hyperparameter $e_c$ succinctly controls how quickly we introduce each head, preventing excessive noise in early training. 

We also adopt standard neural network practices, including mini-batch optimization and early stopping, to stabilize training and mitigate overfitting.  At \textit{inference} time, given a new history $\hb_{1:t}$ and an interventional sequence $\ab_{t:t+\tau-1}$, we compute $\widehat{Q}_1(\phi(\hb_{1:t},\ab_t);\theta)$ as our target CAPO estimate. We sketch the overall training and inference procedure in \cref{alg:gstunet}.

\section{Experiments}\label{sec:experiments}

We evaluate the proposed GST-UNet framework through two applications. First, we simulate synthetic data that incorporates key spatiotemporal causal inference challenges: interference, spatial confounding, temporal carryover, and time-varying confounding. Using this synthetic data generation process (DGP), we compare the GST-UNet algorithm against several baselines. Next, we demonstrate the utility of GST-UNet on a real-world dataset analyzing the impact of wildfire smoke on respiratory hospitalizations during the 2018 California Camp Fire.

%For both applications, GST-UNet employs a U-Net backbone with a single ConvLSTM layer (hidden dimension 32) and a contracting-expanding path of channel sizes $16\rightarrow 32\rightarrow 64\rightarrow 128\rightarrow 256$. The G-computation heads are implemented as shallow feed-forward neural networks that operate on the U-Net feature maps at each grid cell for G-computation. In practice, to ensure stable ConvLSTM training and reduce computational overhead, we truncate the input history to a fixed length.
Additional details--including exact simulation parameters, model architecture and execution setups, hyperparameter selection strategies, and validation procedures--can be found in \cref{sec:app-experiments}.
Replication code is available at \url{https://github.com/moprescu/GSTUNet}.

\subsection{Synthetic Data}\label{sec:exp-sim}

\begin{table*}[t]
\centering
\caption{Comparison of models across horizons ($\tau$). Values represent RMSE ± standard deviation across mean test trajectories. GST-UNet (ours) is highlighted in the last row. Bold values indicate the smallest number in each column for each horizon. Relative changes for GST-UNet compared to the best baseline are shown in \textcolor{BrightGreen}{\textbf{green}} (improvement) or \textcolor{Crimson}{\textbf{red}} (decrease).}
\small % Slightly smaller font
\begin{tabular}{|l|l|ccccccc|}
\toprule
$\tau$ & Model & $\beta_1=0.0$ & $\beta_1=0.5$ & $\beta_1=1.0$ & $\beta_1=1.5$ & $\beta_1=2.0$ & $\beta_1=2.5$ & $\beta_1=3.0$ \\
\midrule
\multirow{3}{*}{5} 
 & UNet+ & \textbf{0.51 ± 0.00} & 0.62 ± 0.01 & 0.84 ± 0.01 & 1.03 ± 0.01 & 1.10 ± 0.01 & 1.16 ± 0.01 & 1.25 ± 0.01 \\
 & STCINet & 0.52 ± 0.00 & 0.68 ± 0.01 & 0.93 ± 0.01 & 1.11 ± 0.01 & 1.20 ± 0.01 & 1.33 ± 0.01 & 1.32 ± 0.01 \\
 & \textbf{GST-UNet} & 0.55 ± 0.01 & \textbf{0.61 ± 0.01} & \textbf{0.60 ± 0.01} & \textbf{0.61 ± 0.01} & \textbf{0.64 ± 0.01} & \textbf{0.58 ± 0.01} & \textbf{0.64 ± 0.01} \\
 &  & (\textcolor{Crimson}{\textbf{+7.8\%}}) & (\textcolor{BrightGreen}{\textbf{-1.6\%}}) & (\textcolor{BrightGreen}{\textbf{-28.6\%}}) & (\textcolor{BrightGreen}{\textbf{-40.8\%}}) & (\textcolor{BrightGreen}{\textbf{-41.8\%}}) & (\textcolor{BrightGreen}{\textbf{-50.0\%}}) & (\textcolor{BrightGreen}{\textbf{-48.8\%}}) \\
\midrule
\multirow{3}{*}{10} 
 & UNet+ & 0.56 ± 0.00 & 0.53 ± 0.00 & 0.68 ± 0.00 & 0.85 ± 0.00 & 1.00 ± 0.01 & 1.02 ± 0.01 & 1.01 ± 0.01 \\
 & STCINet & 0.57 ± 0.00 & 0.59 ± 0.00 & 0.74 ± 0.00 & 0.86 ± 0.01 & 1.11 ± 0.01 & 1.15 ± 0.01 & 1.26 ± 0.01 \\
 & \textbf{GST-UNet} & \textbf{0.50 ± 0.00} & \textbf{0.44 ± 0.01} & \textbf{0.45 ± 0.01} & \textbf{0.57 ± 0.01} & \textbf{0.48 ± 0.01} & \textbf{0.53 ± 0.01} & \textbf{0.49 ± 0.01} \\
 &  & (\textcolor{BrightGreen}{\textbf{-10.7\%}}) & (\textcolor{BrightGreen}{\textbf{-16.7\%}}) & (\textcolor{BrightGreen}{\textbf{-33.8\%}}) & (\textcolor{BrightGreen}{\textbf{-32.9\%}}) & (\textcolor{BrightGreen}{\textbf{-52.1\%}}) & (\textcolor{BrightGreen}{\textbf{-48.0\%}}) & (\textcolor{BrightGreen}{\textbf{-51.5\%}}) \\
\bottomrule
\end{tabular}\label{tab:sim-results}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.3819\textwidth]{figures/PM2.5_vs_month.pdf}%
    \hfill
    \includegraphics[width=0.2790\textwidth]{figures/PM2.5_Camp_Fire_exposed.pdf}%
    \hfill
    \includegraphics[width=0.3390\textwidth]{figures/Factual_vs_counterfactual_respiratory_illness_daily_hashed.pdf}
    \vspace{-2em}
    \caption{%
    \textbf{(Left)} Daily PM\textsubscript{2.5} levels across California from May to December~2018, 
    with red lines marking major wildfires. 
    \textbf{(Center)} Counties exposed to average PM\textsubscript{2.5}~$>$~10\,\textmu g/m\textsuperscript{3} 
    during the Camp Fire (red), origin county in dark red. 
    \textbf{(Right)} Factual minus CAPO‐predicted daily respiratory admissions during peak Camp Fire. 
    Hashed areas indicate small-population counties ($<30{,}000$).
    }
    \label{fig:camp_fire_maps}
    \vspace{-1em}
\end{figure*}

We generate $T=200$ time steps of a $64 \times 64$ ($N_X \times N_Y$) grid of observational data using the following system:
\begin{align*}
\Xb_{t} &= \alpha_0 + \alpha_1 \Xb_{t-1} 
+ \alpha_2 \Ab_{t-1} 
+ \alpha_3 (K_X * \Xb_{t-1}) 
+ \epsilon_X,\\
\Ab_{t} &\sim \mathrm{Bern}\!\Bigl(\sigma\!\bigl(\beta_1\!\bigl(\beta_0 
+ \textcolor{TemporalGreen}{\tfrac{1}{L}\!\sum_{l=0}^{L-1}}\,\textcolor{SpatialBlue}{K_A * \Xb_{t-l}}\bigr)\bigr)\Bigr),\\
\Yb_{t} &= \gamma_0 + \gamma_1 \bigl(\textcolor{InterferenceRed}{K_{YA} * \Ab_{t-1}}\bigr) 
+ \gamma_2 \textcolor{TemporalGreen}{\tfrac{1}{L}\!\sum_{l=1}^L}\,\bigl(\textcolor{SpatialBlue}{K_{YX} * \Xb_{t-l}}\bigr)\\ 
&\qquad + \gamma_3 \textcolor{TemporalGreen}{\Yb_{t-1}} 
+ \epsilon_Y,
\end{align*}
where $d_X=1$ (one feature), ``$*$'' denotes a 2D convolution over the $N_X \times N_Y$ grid, $K_X, K_A, K_{YA}, K_{YX}$ are $d_k \times d_k$ convolution kernels, $L$ is the number of temporal lags, and $\epsilon_X, \epsilon_Y \sim \mathcal{N}(0,1)$ are i.i.d.\ noise terms. Each equation is evaluated at every location $s$ in the grid, so $\Xb_{t}, \Ab_{t}, \Yb_{t}$ represent $N_X \times N_Y$ matrices at time $t$. We choose parameter values such that the simulation remains stable (\emph{i.e.}, the process does not diverge). This data-generating process (DGP) includes \textcolor{InterferenceRed}{\textbf{interference}} and \textcolor{SpatialBlue}{\textbf{spatial confounding}} from neighboring cells, as well as \textcolor{TemporalGreen}{\textbf{temporal carryover}}. Furthermore, $\Xb_{t}$ is a \emph{time-varying} confounder, since its past values affect $\Ab$ and $\Yb$, while current $\Ab$ influences future $\Xb$.

A concise example scenario is pollution control measures influencing health outcomes: $\Ab_{t}$ might represent binary interventions (e.g.\ traffic restrictions  or industrial regulations), $\Xb_{t}$ could be spatially diffused air quality, and $\Yb_{t}$ could track hospital visits or economic indicators. Government policies reacting to past pollution levels naturally create the feedback loops modeled here.
   
We vary $\beta_1$ to control the strength of time-varying confounding. When $\beta_1=0$, $\Xb_t$ does not directly affect $\Ab_t$, eliminating time-varying confounding; larger values of $\beta_1$ strengthen the time-varying confounding. For each $\beta_1$, we generate $n_{\text{test}}=50$ test trajectories from random initial states, fix their histories $\hb_{1:t}$, and simulate 100 potential $\tau$-length futures to average the terminal outcomes and obtain each true CAPO. We evaluate horizon lengths $\tau \in \{5,\,10\}$. We compare GST-UNet with:
\emph{(i)} \textbf{UNet+}, a naive variant using U-Net + ConvLSTM + Attention but no G-computation (the treatments are simply appended as static channels); and
\emph{(ii)} \textbf{STCINet}~\citep{ali2024estimating}, a spatiotemporal causal forecasting model that similarly lacks an adjustment for time-varying confounders. 
Table~\ref{tab:sim-results} reports the RMSE~$\pm$~standard deviation across the mean test trajectories for $\beta_1\in\{0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0\}$. When $\beta_1=0$ (no time-varying confounding), all methods perform similarly. However, as $\beta_1$ increases, GST-UNet’s error remains nearly constant, while the baselines degrade considerably, demonstrating increasing bias from time-varying confounders. Thus, these results highlight the value of GST-UNet’s iterative G-computation in effectively adjusting for time-varying confounding in spatiotemporal settings.

\subsection{Impact of Wildfires on Respiratory Health}\label{sec:exp-wildfire}


Wildfire smoke has been associated with short-term adverse respiratory outcomes~\citep{reid2016differential, reid2016critical, cascio2018wildland, cleland2021estimating, letellier2025applying}, with older adults especially vulnerable~\citep{deflorio2019cardiopulmonary}. Recent events (e.g., ongoing Los Angeles wildfires) have amplified concerns about acute health impacts. In 2018, California experienced multiple severe wildfires~\citep{wiki2018}, including two major incidents: the \emph{Carr Fire} (July--August) and the \emph{Camp Fire} (November), which significantly worsened air quality across the state.

We analyze daily, county-level data from \citet{letellier2025applying} that include PM$_{2.5}$ (particulate matter~$<\!2.5\,\mu m$), hospitalization counts for respiratory and cardiovascular conditions, and weather variables (temperature, precipitation, humidity, radiation, wind), plus population estimates from the California Department of Finance. Each of the weather variables can serve as a \emph{time-varying confounder} because weather conditions affect future smoke levels and health outcomes, while also potentially being influenced by prior smoke levels.

We focus on the period from week~20 to week~48 (May~18--December~2, 2018), covering both the Carr and Camp fires. Following standard practice, we label a county as ``treated'' on any day it has mean PM$_{2.5}$ above $10\,\mu g{/}m^3$, and we use the raw hospitalization counts as the outcome (rather than per-10{,}000 incidence to avoid instability in small counties). To represent counties in a spatial grid, we interpolate each day’s county-level data (treatment, outcome, five covariates) onto a $40\times44$ latitude--longitude lattice (discarding cells outside California), yielding a spatiotemporal tensor of size $203\times7\times40\times44$. Interpolation ensures that each grid cell approximates the region it overlaps (weighted by intersection area), so the downstream model captures spatial gradients in PM$_{2.5}$, weather, and hospitalizations. We train GST-UNet with a horizon $\tau=10$. The earlier wildfire period (Jun--July) is used for validation and hyperparameter tuning. After model selection, we generate counterfactual predictions for the peak phase of the Camp Fire, November~8--17, 2018. See \cref{sec:app-experiments} for details on data preprocessing, interpolation, and masking.

Figure~\ref{fig:camp_fire_maps} (left) shows the rise in PM$_{2.5}$ levels during the mid-late 2018 wildfire season, and (center) highlights counties with daily PM$_{2.5}>10\,\mu g{/}m^3$. Using GST-UNet, we estimate the daily CAPOs had the Camp Fire never occurred (i.e., treating all counties as if PM$_{2.5}\le10\,\mu g{/}m^3$). Figure~\ref{fig:camp_fire_maps} (right) compares these CAPOs to the factual daily mean incidence (hospitalization cases per $10{,}000$ residents). Hatching marks low-population counties ($<30{,}000$ compared to $>70{,}000$ for other exposed counties) with higher uncertainty; we exclude these from the analysis. Over November~8--17, GST-UNet predicts \textbf{approximately 4{,}650 excess respiratory hospitalizations} (465 per day) attributable to the Camp Fire, with the highest incidence near the fire source. This result is qualitatively consistent with \citet{letellier2025applying}, who report around 259 excess daily cases \emph{averaged} over November~8--December~5---a longer window including lower-intensity days, thus yielding a smaller daily estimate. Overall, GST-UNet captures the spatiotemporal variation in smoke exposure and health outcomes, illustrating its promise for real-world causal inference in environmental health and policy.

\section{Conclusion}

We introduced GST-UNet, a neural framework for estimating causal effects in spatiotemporal settings by combining a U-Net-based architecture for spatial modeling with iterative G-computation to adjust for time-varying confounders. GST-UNet addresses key challenges such as interference, spatial confounding, temporal carryover, and time-varying confounders. Through simulations, the GST-UNet outperformed existing baselines, and in a real-world case study of wildfire smoke exposure during the 2018 Camp Fire, it provided fine-grained, location-specific, and credible effect estimates. These results highlight GST-UNet’s potential to improve causal inference in fields such as public health, environmental science, and social policy.

\clearpage

\section*{Acknowledgements}

This material is based upon work supported by the National Science Foundation under Grant No. 1846210 and by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, under Award Number DE-SC0023112. Part of this work was conducted while Miruna Oprescu was a research intern at Brookhaven National Laboratory. 


\section*{Impact Statement}

This work advances the field of machine learning by developing a spatiotemporal causal inference framework that enables more accurate estimation of treatment effects in complex real-world settings. GST-UNet has broad applications in public health, environmental science, and social policy, where understanding intervention effects can inform evidence-based decision-making. While our method is designed to improve causal inference from observational data, care must be taken when applying it to high-stakes policy decisions, ensuring robustness against biases in data collection and model assumptions. We encourage responsible use, particularly in applications affecting vulnerable populations.

\bibliography{ref}
\bibliographystyle{abbrvnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\allowdisplaybreaks

\section{Extended Literature Review}\label{app:extended-lit}

\textbf{Classical Spatiotemporal Causal Inference.} Early spatiotemporal causal inference methods—including spatial econometrics \citep{anselin2013spatial}, difference-in-differences \citep{keele2015geographic}, and synthetic controls \citep{ben2022synthetic}—provide useful frameworks for estimating treatment effects across regions but rely on strong assumptions such as parallel trends or stable treatment assignment. These approaches struggle with interference, nonlinear dependencies, and time-varying confounders, limiting their applicability in complex settings. More recent classical approaches estimate average treatment effects across regions or impose structural and modeling assumptions that may not generalize well to real-world spatiotemporal contexts \citep{wang2021causal, christiansen2022toward, papadogeorgou2022causal, zhang2023spatiotemporal, zhou2024estimating}.  

\textbf{Machine Learning for Spatiotemporal Modeling.}  
The rise of large-scale spatiotemporal datasets has led to the adoption of machine learning models for predictive tasks. Convolutional and recurrent neural networks \citep{shi2015convolutional, zhang2017deep} effectively capture spatial and temporal dependencies but lack explicit causal adjustment mechanisms. Graph-based deep learning methods \citep{li2017diffusion, wu2019graph} model spatial interactions but typically ignore feedback effects from time-varying confounders. Some recent work integrates spatial representations for causal inference—e.g., \citet{tec2023weather2vec} incorporate geographic confounders using a UNet-based model—but these methods do not explicitly model iterative dependencies over time or adjust for time-varying confounders.  

\textbf{Time-Series Causal Inference.}  
In the longitudinal domain, time-series causal inference typically employs recurrent networks, Transformers, or propensity-based models \citep{bica2020estimating, seedat2022continuous, melnychuk2022causal}, but these approaches often assume independent time series, overlooking spatial interference and cross-series confounding. Handling time-varying confounders has relied on marginal structural models \citep{robins2000marginal} or iterative G-computation \citep{bang2005doubly, robins2008estimation}, but machine learning adaptations \citep{lim2018forecasting, li2021g, hess2024g} continue to assume independent observations, making them ill-suited for fully spatiotemporal settings where interference is prevalent.  

\textbf{Neural-Based Spatiotemporal Causal Inference.}  
Recent efforts have explored neural models for spatiotemporal causal inference. \citet{tec2023weather2vec} integrate spatial confounding adjustments using a UNet-based framework but do not model feedback effects from time-varying confounders. The most similar work to ours, \citet{ali2024estimating}, introduces a climate-focused neural model that shares certain architectural components but is designed primarily for predictive tasks rather than causal identification, leaving time-varying confounders unaddressed.  

\textbf{Our Contribution.}  
GST-UNet bridges these gaps by combining a U-Net-based architecture for spatiotemporal grids with iterative G-computation to adjust for time-varying confounders. Unlike prior methods, GST-UNet explicitly accounts for interference, nonlinear spatial-temporal dynamics, and feedback loops, ensuring valid causal identification under standard assumptions. This enables fine-grained, location-specific estimates of potential outcomes, improving spatiotemporal causal inference in domains where observational data is abundant but controlled experiments are infeasible.

\section{\pfref{thm:identification}}\label{app:identification-proof}

We aim to show that under \cref{assump:standard} and \cref{assump:embedding}, the CAPOs in \cref{eq:capo} can be identified recursively from a single time series via a sequence of conditional expectations.

\paragraph{Step 1: Recursive decomposition for the intractable expectation} We first demonstrate the recursive decomposition of the intractable expectation in the CAPO definition (\cref{eq:capo}). While this expectation is theoretically well-defined, it cannot be directly estimated in practice due to the limited availability of data. Specifically, we only observe a single time series, meaning we have just one sample of the history at time $t+\tau$ for each $t$. Nevertheless, as we will show, we can convert these expectations into expectations over prefix-based segments that allow us to estimate these quantities from the data.

Starting from $\EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t}=\hb_{1:t}]$, we have:
\begin{align*}
    &\EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t}=\hb_{1:t}]\\
    & = \EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t}=\hb_{1:t}, \Ab_t = \ab_{t}] \tag{Sequential ignorability and positivity (\cref{assump:standard})}\\
    & =\EE\big[\EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t+1}^\ab]\mid \Hb_{1:t}=\hb_{1:t}, \Ab_t = \ab_{t}\big] \tag{Law of total probability}\\
    & = \EE\big[\EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t+1}^\ab, \Ab_{t+1}=\ab_{t+1}]\mid \Hb_{1:t}=\hb_{1:t}, \Ab_t = \ab_{t}\big] \tag{Sequential ignorability and positivity}\\
    & = \EE\Big[\EE\big[\EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t+2}^\ab] \;\big|\; \Hb_{1:t+1}^\ab, \Ab_{t+1}=\ab_{t+1}\big] \;\Big|\; \Hb_{1:t}=\hb_{1:t}, \Ab_t = \ab_{t}\Big] \tag{Law of total probability}\\
    & = \EE\Big[\EE\big[\EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t+2}^\ab, \Ab_{t+2}=\ab_{t+2}] \;\big|\; \Hb_{1:t+1}^\ab, \Ab_{t+1}=\ab_{t+1}\big]  \;\Big|\; \Hb_{1:t}=\hb_{1:t}, \Ab_t = \ab_{t}\Big] \tag{Sequential ignorability and positivity}\\
    & \dots\\
    & = \EE\Big[\dots\EE\big[\EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t+\tau-1}^\ab, \Ab_{t+\tau-1}=\ab_{t+\tau-1}] \\
    & \hspace{4.35cm}\;\big|\; \Hb_{1:t+\tau-2}^\ab, \Ab_{t+\tau-2}=\ab_{t+\tau-2}\big] \\
    & \hspace{4.35cm} \;\big|\; \dots \\
    & \hspace{4.35cm} \;\Big|\; \Hb_{1:t}=\hb_{1:t}, \Ab_t = \ab_{t}\Big] \tag{Sequential ignorability and positivity}\\
    & = \EE\Big[\dots\EE\big[\EE[\Yb_{t+\tau}\mid \Hb_{1:t+\tau-1}^\ab, \Ab_{t+\tau-1}=\ab_{t+\tau-1}] \\
    & \hspace{3cm}\;\big|\; \Hb_{1:t+\tau-2}^\ab, \Ab_{t+\tau-2}=\ab_{t+\tau-2}\big] \\
    & \hspace{3cm} \;\big|\; \dots \\
    & \hspace{3cm} \;\Big|\; \Hb_{1:t}=\hb_{1:t}, \Ab_t = \ab_{t}\Big] \tag{Consistency}
\end{align*}
Thus, if we had multiple spatiotemporal time-series samples, we could directly estimate this nested expression from data, since the right-hand side depends solely on observed quantities, ensuring identifiability.

\paragraph{Step 2: From intractable to prefix-based expectations} We now show how to estimate the nested expectations using the prefix data. First, by \cref{assump:embedding}, we can rewrite the inner-most expectation as
\begin{align*}
    \EE[\Yb_{t+\tau}\mid \Hb_{1:t+\tau-1}^\ab, \Ab_{t+\tau-1}=\ab_{t+\tau-1}] & = \EE_\Pb[\Yb_{t+\tau}\mid \phi(\Hb_{1:t+\tau-1}^\ab, \ab_{t+\tau-1})]\\
    & = Q_\tau(\Hb_{1:t+\tau-1}^\ab, \ab_{t+\tau-1}). \tag{Definition of $Q_\tau$}
\end{align*}
Thus, by using \cref{assump:standard}, we can write this expectation over the prefix data which we have many samples of. Now consider the next nested expectation:
\begin{align*}
    & \EE[Q_\tau(\Hb_{1:t+\tau-1}^\ab, \ab_{t+\tau-1})\mid \Hb_{1:t+\tau-2}^\ab=\hb_{1:t+\tau-2}^\ab, \Ab_{t+\tau-2}=\ab_{t+\tau-2}]\\
    & = \int Q_\tau(\hb_{1:t+\tau-1}^\ab, \ab_{t+\tau-1}) p(x_{t+\tau-1}, y_{t+\tau-1}\mid \hb^\ab_{t+\tau-2}, \ab_{t+\tau-2}) d(x_{t+\tau-1}, y_{t+\tau-1})\\
    & = \int_P Q_\tau(\hb_{1:t+\tau-1}^\ab, \ab_{t+\tau-1}) p(x_{t+\tau-1}, y_{t+\tau-1}\mid \phi(\hb^\ab_{t+\tau-2}, \ab_{t+\tau-2})) d(x_{t+\tau-1}, y_{t+\tau-1}) \tag{\cref{assump:embedding}}\\
    & = \EE_P[Q_\tau(\Hb_{1:t+\tau-1}^\ab, \ab_{t+\tau-1})\mid \phi(\Hb_{1:t+\tau-2}^\ab, \Ab_{t+\tau-2})=\phi(\hb_{1:t+\tau-2}^\ab, \ab_{t+\tau-2})]\\
    & = Q_{\tau-1}(\hb_{1:t+\tau-2}^\ab, \ab_{t+\tau-2})
\end{align*}
Tracing this argument recursively through the nested expectation in Step 1, we obtain:
\begin{align*}
    \EE[\Yb_{t+\tau}[\ab_{t:t+\tau-1}]\mid \Hb_{1:t}=\hb_{1:t}] = Q_1(\hb_{1:t}, \ab_t),
\end{align*}
as desired. Thus, $Q_1$ -- which can be estimated from the prefix data -- recovers the CAPOs, under our assumptions, even from a single chain.  

\section{Experimental Details}\label{sec:app-experiments}

In this appendix, we provide further information on the simulation experiments (\cref{sec:exp-sim}) and the real-world wildfire application (\cref{sec:exp-wildfire}), including exact parameter settings, model architecture and execution details, hyperparameter selection strategies, and validation procedures. All code for generating, preprocessing, and analyzing both the synthetic and real-world datasets—and for training and evaluating GST-UNet—is available at \url{https://github.com/moprescu/GSTUNet}, with step-by-step replication instructions in the repository’s \texttt{README.md}.

For both applications, GST-UNet employs a U-Net backbone with a single ConvLSTM layer (hidden dimension 32) and a contracting-expanding path of channel sizes $16\rightarrow 32\rightarrow 64\rightarrow 128\rightarrow 256$. The G-computation heads are implemented as shallow feed-forward neural networks that operate on the U-Net feature maps at each grid cell for G-computation. In practice, to ensure stable ConvLSTM training and reduce computational overhead, we truncate the input history to a fixed length.
All neural networks are implemented via the \texttt{nn} module in \texttt{PyTorch}~\citep{pytorch}. Experiments were conducted on an NVIDIA A100 (Ampere) GPU using the Perlmutter system at the National Energy Research Scientific Computing Center (NERSC). The synthetic experiments required roughly 55 minutes per hyperparameter set, while the wildfire experiment completed in about 5 minutes.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/dgp_samples.pdf}
    \caption{
    Samples from the DGP at $t=100$, comparing feature $X_{100}$ (left), 
    intervention $A_{100}$ (center), and outcome $Y_{101}$ (right) 
    for varying $\beta_1\in\{0.0,1.0,2.0\}$.}
    \label{fig:dgp_samples} \vspace{-1em}
\end{figure*}

\subsection{Synthetic Experiments}\label{sec:app-sim}

\textbf{Data Simulation Process.} For our primary simulation experiments, we generate $T=200$ time steps on a $64 \times 64$ grid. The simulation parameters in the generating equations
\begin{align*}
\Xb_{t} &= \alpha_0 + \alpha_1 \Xb_{t-1} 
+ \alpha_2 \Ab_{t-1} 
+ \alpha_3 (K_X * \Xb_{t-1}) 
+ \epsilon_X,\\
\Ab_{t} &\sim \mathrm{Bern}\!\Bigl(\sigma\!\bigl(\beta_1\!\bigl(\beta_0 
+ \textcolor{TemporalGreen}{\tfrac{1}{L}\!\sum_{l=0}^{L-1}}\,\textcolor{SpatialBlue}{K_A * \Xb_{t-l}}\bigr)\bigr)\Bigr),\\
\Yb_{t} &= \gamma_0 + \gamma_1 \bigl(\textcolor{InterferenceRed}{K_{YA} * \Ab_{t-1}}\bigr) 
+ \gamma_2 \textcolor{TemporalGreen}{\tfrac{1}{L}\!\sum_{l=1}^L}\,\bigl(\textcolor{SpatialBlue}{K_{YX} * \Xb_{t-l}}\bigr)\\ 
&\qquad + \gamma_3 \textcolor{TemporalGreen}{\Yb_{t-1}} 
+ \epsilon_Y,
\end{align*}
are given by:
\begin{itemize}
    \item $\Xb_t$:\begin{equation*}
        \alpha_0=0.5,\; \alpha_1=0.5,\; \alpha_2=-2.0,\; \alpha_3=0.2,\;  K_X = 
            \begin{pmatrix}
            0   & 0.45 & 0   \\
            0.15& 0   & 0.35\\
            0   & 0.05 & 0   
            \end{pmatrix}.
    \end{equation*}
    where $K_X$ influences how $\Xb$ diffuses across neighboring cells, with an asymmetry due to advection.
    \item $\Ab_t$: 
    \begin{equation*}
        \beta_0=-1.0, \;\beta_1\in\{0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0\},\; K_A = \frac{1}{16}
            \begin{pmatrix}
            1.0 & 1.0 & 1.0 \\
            1.0 & 8.0 & 1.0 \\
            1.0 & 1.0 & 1.0
            \end{pmatrix}.  
    \end{equation*}
    \item $\Yb_t$:
    \begin{equation*}
        \gamma_0=2.0,\; \gamma_1=1.5,\; \gamma_2=0.5,\; \gamma_3=0.5,\; K_{YX} = 
            \frac{1}{16}
            \begin{pmatrix}
            1.0 & 1.0 & 1.0 \\
            1.0 & 8.0 & 1.0 \\
            1.0 & 1.0 & 1.0
            \end{pmatrix}  ,\;
            K_{YA} = 
            \frac{1}{16}
            \begin{pmatrix}
            1.0 & 1.0 & 1.0 \\
            1.0 & 8.0 & 1.0 \\
            1.0 & 1.0 & 1.0
            \end{pmatrix}.
    \end{equation*}
\end{itemize}

\begin{table}[t]
    \centering
    \caption{Hyperparameters and their ranges. We boldface the values that provided the best validation performance.}
    \label{tab:sim-hyperparam}
    \begin{tabular}{l l l}
    \toprule
    \textbf{Hyperparameter} & \textbf{Model(s)} & \textbf{Value Range} \\
    \midrule
    Batch size & All models & \{2, \textbf{4}, 8\} \\
    Learning rate & All models & \{$10^{-4}$, $\mathbf{5\times 10^{-4}}$, $10^{-3}$\} \\
    Scheduler patience & All models & \{3, \textbf{5}, 10\} \\
    Early stopping patience & All models & \{5, \textbf{10}\} \\
    Curriculum period & GST-UNet & \{1, 3, \textbf{5}, 7\} \\
    Curriculum learning rate & GST-UNet & \{$10^{-4}$, $\mathbf{5\times 10^{-4}}$, $10^{-3}$\} \\
    UNet output dim $d_h$ & GST-UNet & \{8, \textbf{16}, 32\} \\
    G-head hidden size & GST-UNet & \{\textbf{8}, 16\} \\
    G-head layers & GST-UNet & \{\textbf{1}, 2, 3\} \\
    \bottomrule
    \end{tabular}
\end{table}

We use $L=5$ temporal lags for \(\Xb\) and \(\Yb\), a seed of $42$ for reproducibility. See Figure~\ref{fig:dgp_samples} for representative $t=100$ snapshots of $X_{100}$, $A_{100}$, and $Y_{101}$ under varying $\beta_1$.

For each $\beta_1$, we first generate a factual dataset of length $T=200$ (\ie, $\{(\Xb_t,\Ab_t,\Yb_t)\}_{t=1}^{200}$). We then create $n_{\text{test}}=50$ test histories of length $l_H=10$. For each test history, we simulate $100$ trajectories under a randomly chosen (yet fixed over the test data) counterfactual intervention of length $\tau=10$, and average the outcomes at each step to approximate the true CAPOs. This procedure yields a final test set of shape $n_{\text{test}} \times (l_H + \tau + 1) \times 64 \times 64$, \ie, $50 \times 21 \times 64 \times 64$.

\textbf{Neural Architectures.}
The \textbf{GST-UNet} comprises a single ConvLSTM layer (hidden dimension $32$), followed by a U-Net with channel sizes $16 \!\rightarrow\! 32 \!\rightarrow\! 64 \!\rightarrow\! 128 \!\rightarrow\! 256$. Its G-computation heads are shallow feed-forward networks operating on the final U-Net feature maps at each grid cell; both the U-Net’s output dimension ($d_h$) and the G-head architecture (number of layers, hidden size) are treated as hyperparameters. The \textbf{UNet+} baseline uses the same ConvLSTM+U-Net backbone as GST-UNet but outputs a single channel ($d_h=1$), omitting any G-computation. For direct comparison, we also implement \textbf{STCINet}~\citep{ali2024estimating} with an identical ConvLSTM+U-Net backbone, and retaining their original Latent Factor Model (LFM) details.

\textbf{Training Details.}
We randomly initialize all model parameters (GST-UNet and baselines) with Xavier uniform weights~\citep{glorot2010understanding}. We use the Adam optimizer \citep{kingma2014adam} with an initial learning rate, halving it whenever the validation loss plateaus for a specified scheduler patience. To mitigate overfitting, we adopt early stopping when the validation loss fails to improve for a specified early stopping patience epochs. Validation uses 40 of the 190 training prefixes, and the total training is capped at 100 epochs. We tune the following hyperparameters: 
\emph{(i)}~batch size, learning rate, scheduler patience, and early stopping patience (common to all models);
\emph{(ii)}~for GST-UNet, the curriculum period and learning rate for curriculum phases, the U-Net output dimension $d_h$, and the number and width of hidden layers in the feed-forward G-heads.
Table~\ref{tab:sim-hyperparam} lists the hyperparameter ranges considered, with the values yielding the best validation performance in \textbf{bold}.

\textbf{Evaluation Procedure.} We evaluate each model by averaging the root mean square error (RMSE) of the estimated CAPOs against ground truth across 50 test trajectories. Table~\ref{tab:sim-results} in the main text reports RMSE~$\pm$~standard deviation for horizon lengths $\tau\in\{5,\,10\}$ and $\beta_1\in\{0,0.5,1.0,1.5,2.0,2.5,3.0\}$.


\subsection{Wildfire Application}
\label{sec:app-wildfire}

\begin{figure*}[t]
    \centering
    % Left: Daily respiratory illness incidence
    \includegraphics[width=0.34\textwidth]{figures/Resp_rate_vs_day.pdf}\hfill
    % Center: Weekly respiratory illness incidence
    \includegraphics[width=0.34\textwidth]{figures/Resp_rate_vs_week.pdf}\hfill
    % Right: Average daily PM2.5
    \includegraphics[width=0.30\textwidth]{figures/PM2.5_Camp_Fire.png}
    \vspace{-0.5em}
    \caption{
    \textbf{(Left)}~Daily respiratory illness incidence (cases per 10{,}000).
    \textbf{(Center)}~Weekly aggregated incidence.
    \textbf{(Right)}~Average daily PM\textsubscript{2.5} during the Camp Fire.
    }
    \label{fig:app_wildfire}
    \vspace{-1em}
\end{figure*}

\begin{figure*}[t]
    \centering
    % Left: County-level PM2.5, Nov 18
    % Right: Interpolated grid PM2.5, Nov 18
    \includegraphics[width=0.70\textwidth]{figures/PM2.5_Grid.pdf}
    \vspace{-0.75em}
    \caption{%
    An example of county-level (\emph{left}) vs.\ grid-interpolated (\emph{right}) PM\textsubscript{2.5} levels 
    on November~18 (during the Camp~Fire). The grid interpolation 
    produces a $40\times44$ lattice of area-weighted estimates
    aligned with our spatiotemporal framework.
    }
    \label{fig:pm25_interpolation}
    \vspace{-1em}
\end{figure*}

\textbf{Data Preprocessing and Interpolation} We utilize 2018 daily county-level PM$_{2.5}$, respiratory/cardiovascular hospitalizations, weather variables (temperature, precipitation, humidity, radiation, wind) from \citet{letellier2025applying}, along with population data from the California Department of Finance. Our study period spans weeks~20--48 (May~18--December~2, 2018), covering both the Carr and Camp fires. As illustrated in Figure~\ref{fig:app_wildfire}, daily and weekly aggregated respiratory illness rates rise around these events, while PM\textsubscript{2.5} levels also surge during the Camp Fire.

To align with our spatiotemporal framework, we use \texttt{geopandas}~\citep{kelsey_jordahl_2020_3946761} to interpolate county-level covariates, PM$_{2.5}$, and hospitalizations onto a latitude--longitude grid from 32$^\circ$N to 42$^\circ$N latitude and -125$^\circ$ to -114$^\circ$ longitude, at a resolution of $0.25^\circ$. Each grid cell’s values are an area-weighted average of the counties it intersects, yielding a $40\times44$ spatial lattice. We mask out non-California cells by setting them to zero, thus obtaining a consistent dataset for further analysis. As an example, \cref{fig:pm25_interpolation} illustrates how the raw county-level data compare to the interpolated grid for PM\textsubscript{2.5} on November~18.

\textbf{Model Training and Validation} We train GST-UNet with prediction horizon $\tau=10$ days. The loss function is MSE with two key modifications: (1) we mask grid cells outside California's boundaries to exclude them from loss computation, and (2) we apply cell-specific weights proportional to the number of cells per county to prevent bias towards geographically larger counties. For validation and hyperparameter tuning, we use data from the first 50 days of the wildfire season.
The GST-UNet hyperparameters are: batch size $=40$, learning rate $=5\times 10^{-4}$, scheduler patience $=5$, early stopping patience $=10$, curriculum period $=5$, curriculum learning rate $=5\times 10^{-4}$, UNet output dimension $d_h=16$, G-head hidden layer size $=8$, and G-head layers $=1$.
Using this configuration, we generate counterfactual predictions for the Camp Fire peak period (November~8--17) by iteratively applying the trained model with increasing history lengths. We note that counties with populations below $20{,}000$--$30{,}000$ can yield unreliable incidence rate estimates, given baseline daily rates of approximately 4 cases per 10{,}000 individuals. In \cref{fig:camp_fire_maps}, we denote these high-uncertainty counties with hatched markings.

\end{document}

