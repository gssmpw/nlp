\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{float}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{makecell}    % 支持 \makecell
\usepackage{pifont}  % For additional symbols
\usepackage{booktabs} 
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{xcolor}
\usepackage{stfloats}
\usepackage[utf8]{inputenc}
% Include other packages here, before hyperref.


\newcommand{\cmark}{\checkmark} % Define cmark for check mark
\newcommand{\xmark}{\ding{55}} % Define xmark for cross mark

\usepackage{color}

\definecolor{darkgreen}{RGB}{0,127,0}
\definecolor{darkred}{RGB}{200,0,0}
\def\greencheckmark{\textcolor{darkgreen}{\checkmark}}
\def\redxmark{\textcolor{darkred}{\xmark}}
\newcommand{\green}[1] {\footnotesize \color[rgb]{0.13, 0.55, 0.13}(\ensuremath #1\%)}
\usepackage{colortbl}

\usepackage{multirow}
\usepackage{tabularx}
\usepackage{hhline}
\usepackage[para,online,flushleft]{threeparttable}
\usepackage{stackengine,scalerel}



\newcommand\overstarbf[1]{\ThisStyle{\ensurestackMath{%
  \stackengine{0mm}{\SavedStyle\mathbf{#1}}{\smash{\SavedStyle*}}{O}{c}{F}{T}{S}}}}
\newcommand\overstar[1]{\ThisStyle{\ensurestackMath{%
  \setbox0=\hbox{$\SavedStyle#1$}%
  \stackengine{0mm}{\copy0}{\kern.2\ht0\smash{\SavedStyle*}}{O}{c}{F}{T}{S}}}}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{6092} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Distill Any Depth:  \\Distillation Creates a Stronger Monocular Depth Estimator}

\author{Xiankang He$^{*1,2}$ \:\: Dongyan Guo$^{*1}$ \:\: Hongji Li$^{2,3}$ \:\: Ruibo Li$^{4}$ \:\: Ying Cui$^{1}$ \:\: Chi Zhang$^{\dagger2 }$\\
\begin{tabular}[h]{cc}
    $^{1}$Zhejiang University of Technology \quad\quad $^{2}$ AGI Lab, Westlake University\\ 
    $^{3}$Lanzhou University  \quad\quad $^{4}$Nanyang Technological University \\
    {\tt\small \{hexiankang577, 3420670269neon\}@gmail.com \quad \{guodongyan,cuiying\}@zjut.edu.cn} \\
    {\tt\small ruibo.li@ntu.edu.cn \quad chizhang@westlake.edu.cn} \\
    \url{https://distill-any-depth-official.github.io/}
\end{tabular}
}



\twocolumn[{\maketitle
  \ificcvfinal\thispagestyle{empty}\fi
  \centering
  \vspace{-0.7cm}
  \includegraphics[width=0.97\textwidth]{image/teaser8.pdf}
  \vspace{-.05 in}
  \captionof{figure}{
    \textbf{Zero-shot prediction on in-the-wild images.} Our model, distilled from Genpercept~\cite{xu2024diffusion} and DepthAnythingv2~\cite{depth_anything_v2}, outperforms other methods by delivering more accurate depth details and exhibiting superior generalization for monocular depth estimation on in-the-wild images. 
    }
    \vspace{0.1cm}
    \label{fig:distill_with_mt}
    }
]

{
  \renewcommand{\thefootnote}%
    {\fnsymbol{footnote}}
  \footnotetext[1]{denotes co-first authorship. This work was done while Xiankang He was a visiting student at the AGI Lab, Westlake University.\\
  \hspace*{1.35em}$^{\dagger}$ denotes corresponding author.
  % Code available at \href{https://github.com/Westlake-AGI-Lab/Distill-Any-Depth}{https://github.com/Westlake-AGI-Lab/Distill-Any-Depth}
  }
}

\begin{abstract}
Monocular depth estimation (MDE) aims to predict scene depth from a single RGB image and plays a crucial role in 3D scene understanding.
Recent advances in zero-shot MDE leverage normalized depth representations and distillation-based learning to improve generalization across diverse scenes.
However, current depth normalization methods for distillation, relying on global normalization, can amplify noisy pseudo-labels, reducing distillation effectiveness. In this paper, we systematically analyze the impact of different depth normalization strategies on pseudo-label distillation. Based on our findings, we propose Cross-Context Distillation, which integrates global and local depth cues to enhance pseudo-label quality. Additionally, we introduce a multi-teacher distillation framework that leverages complementary strengths of different depth estimation models, leading to more robust and accurate depth predictions. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, both quantitatively and qualitatively.


\end{abstract}




\section{Introduction}
\label{sec:intro}
% \vspace{-0.1in}
Monocular depth estimation (MDE) predicts scene depth from a single RGB image, offering flexibility compared to stereo or multi-view methods. This makes MDE ideal for applications like autonomous driving and robotic navigation~\cite{eigen2014depth,garg2016unsupervised,guizilini20203d,yang2020d3vo,li2020ar}. Recent research on zero-shot MDE models~\cite{ranftl2020midas,yin2020diversedepth,wei2021leres, marigold} aims to handle diverse scenarios, but training such models requires large-scale, diverse depth data, which is often limited by the need for specialized equipment~\cite{mayer2016large,yin2021learning}. A promising solution is using large-scale unlabeled data, which has shown success in tasks like classification and segmentation~\cite{kirillov2023segment,zoph2020rethinking,xie2020self}. Studies like DepthAnything~\cite{yang2024depthanything} highlight the effectiveness of using pseudo labels from teacher models for training student models.


To enable training on such a diverse, mixed dataset, 
%many 
most
state-of-the-art methods~\cite{depth_anything_v2, ranftl2020towards, yin2020diversedepth} employ scale-and-shift invariant (SSI) depth representations for loss computation. This approach normalizes raw depth values within an image, making them invariant to scaling and shifting, and ensures that the model learns to focus on relative depth relationships rather than absolute values. The SSI representation facilitates the joint use of diverse depth data, thereby improving the model's ability to generalize across different scenes~\cite{ranftl2021vision, caron2021emerging}. Similarly, during evaluation, the metric depth of the prediction is recovered by solving for the unknown scale and shift coefficients of the predicted depth using least squares, ensuring the application of standard evaluation metrics.


Despite its advantages, using SSI depth representation for pseudo-label distillation in MDE models presents several issues. Specifically, the inherent normalization process in SSI loss makes the depth prediction at a given pixel not only dependent on the teacher model’s raw prediction at that location but also influenced by the depth values in other regions of the image. 
This becomes problematic because pseudo-labels inherently introduce noise. Even if certain local regions are predicted accurately, inaccuracies in other regions can negatively affect depth estimates after global normalization, leading to suboptimal distillation results.
As shown in Fig.~\ref{fig:normalization}, we empirically demonstrate that normalizing depth maps globally tends to degrade the accuracy of local regions, as compared to only applying normalization within localized regions during evaluation.


Building on this insight, in this paper, 
we first investigate the issue of depth normalization in pseudo-label distillation.
We begin by analyzing various depth normalization strategies, including global normalization, local normalization, hybrid global-local approaches, and the absence of normalization. Through empirical experiments, we explore how each technique affects the performance of various distillation designs, especially when using pseudo-labels for training. 
Our analysis provides valuable insights into how different normalization methods influence the MDE loss function and distillation outcomes, offering a set of best practices for optimizing performance in diverse scenarios.




Building on this empirical foundation, we introduce a Cross-Context Distillation method, designed to distill knowledge from the teacher model more effectively. We are motivated by our finding that local regions, when used for distillation, produce pseudo-labels that capture higher-quality depth details, improving the student model’s depth estimation accuracy. However, focusing solely on local regions might overlook the broader contextual relationships in the image. To address 
%this
the issue
, we combine local and global inputs within a unified distillation framework. By combining the context-specific advantages of local distillation with the broader understanding provided by global methods, our method achieves more detailed and reliable depth predictions.


Furthermore, we propose a multi-teacher distillation framework that leverages the complementary strengths of multiple depth estimation models. Our design is motivated by the observation of recent advancements that diffusion-based models, benefiting from large-scale image priors, excel at capturing fine-grained details but are computationally expensive, while encoder-decoder models provide higher accuracy and efficiency but relatively lack fine-detail reconstruction. To harness these strengths, we randomly select different models to generate pseudo-labels, and then supervise the student model based on these labels. This operation enables the student model to learn from the detailed depth information of diffusion-based models while benefiting from the precision of encoder-decoder models. 

To validate the effectiveness of our design, we conduct extensive experiments on various benchmark datasets. 
%Our 
The empirical results show that our method significantly outperforms existing baselines qualitatively and quantitatively. The contributions can be summarized below:
\begin{itemize}[itemsep=0.01in, topsep=0.02in]
\item 
% 1) 
We systematically analyze the role of different depth normalization strategies in pseudo-label distillation, providing insights into their effects on MDE performance.
\item 
% 2) 
We propose Cross-Context Distillation, a hybrid local-global distillation framework that enhances distillation by leveraging both fine-grained details and global depth relationships.
\item 
% 3) 
We develop a multi-teacher distillation framework that integrates pseudo labels from multiple depth estimation models, combining the strengths of various depth models.
\item 
% 4) 
We conduct extensive experiments on benchmark datasets, demonstrating that our method outperforms state-of-the-art approaches both quantitatively and qualitatively. Code and models are made publicly available.
\end{itemize}



% \vspace{-0.1in}
\section{Related Work}
\label{sec:Related}
% \vspace{-0.05in}
\subsection{Monocular Depth Estimation}
% \vspace{-0.05in}
Monocular depth estimation (MDE) has evolved from hand-crafted methods to deep learning, significantly improving accuracy \cite{eigen2014depth, laina2016deeper, fu2018deep, godard2017unsupervised, zhou2017unsupervised, ranftl2021vision}. Architectural refinements, such as multi-scale designs and attention mechanisms, have further enhanced feature extraction \cite{hu2018squeeze, chen2017deeplab, zhao2017pyramid}. However, most models remain reliant on labeled data and struggle to generalize across diverse environments.
Zero-shot MDE improves generalization by leveraging large-scale datasets, geometric constraints, and multi-task learning \cite{ranftl2020midas,yin2020diversedepth,yin2020learning,zhang2023robust}. Metric depth estimation incorporates intrinsic data for absolute depth learning \cite{bhat2023zoedepth, yin2023metric3d, hu2024metric3d, wang2024moge}, while generative models such as Marigold refine depth details using diffusion priors \cite{marigold, xu2024diffusion}. 
%Despite these advances, effectively utilizing unlabeled data remains a challenge, as pseudo-labels often contain noise and suffer from inconsistencies across different contexts.
Despite these advances, effectively utilizing unlabeled data remains a challenge due to pseudo-label noise and inconsistencies across different contexts. DepthAnything \cite{depth_anything_v2} explores large-scale unlabeled data but struggles with pseudo-label reliability. PatchFusion \cite{patchfusion2023, miangoleh2021boosting} improves depth estimation by refining high-resolution image representations but lacks adaptability in generative settings.
To address these issues, we propose Cross-Context and Multi-Teacher Distillation, which enhances pseudo-label supervision by leveraging diverse contextual information and multiple expert models, improving both accuracy and generalization ability.

% \vspace{-0.05in}
\subsection{Semi-supervised Monocular Depth Estimation}
% \vspace{-0.05in}

Semi-supervised depth estimation has gained attention by utilizing temporal consistency to better use unlabeled data~\cite{kuznietsov2017semi, guizilini2020robust}. Some methods~\cite{left_right, smolyanskiy2018importance, cho2019large, yang2018deep, monodepth2} apply stereo geometric constraints, enforcing left-right consistency to enhance depth accuracy, while others use additional supervision like semantic priors~\cite{ramirez2018geometry,hoyer2023improving} or GANs, such as DepthGAN~\cite{gandepth}. However, these approaches are limited by their reliance on temporal cues or stereo constraints, restricting their applicability. Recent work~\cite{petrovai2022exploiting} explored pseudo-labeling for semi-supervised MDE but lacks generative modeling capabilities. DepthAnything~\cite{yang2024depthanything} demonstrated the potential of large-scale unlabeled data, though pseudo-label reliability remains challenging. In contrast, our approach improves pseudo-label reliability and enhances MDE accuracy, relying solely on unlabeled data without additional constraints.

\begin{figure}[t]
    \centering  \includegraphics[width=0.47\textwidth]{image/ssi_problem6.pdf}
    % \vspace{-1em}
    \caption{
    \textbf{Issue with Global Normalization (SSI).} 
    In (a), we compare two alignment strategies for the central \( w/2, h/2 \) region: (1) \textit{Global Least-Square}, where alignment is applied to the full image before cropping, and (2) \textit{Local Least-Square}, where alignment is performed on the cropped region. Metrics are computed on the cropped region. As shown in (b), 
    the outperformed local strategy demonstrates
    that \textbf{global normalization degrades local accuracy compared to local normalization}.
    }
    % \vspace{-1.5em}
    \label{fig:normalization}
\end{figure}

\begin{figure*}[!t]
    \centering    \includegraphics[width=0.9\textwidth]{image/method6.pdf}
    \vspace{-1em}
    \caption{
    \textbf{Overview of Cross-Context Distillation.} 
    Our method combines local and global depth information to enhance the student model’s predictions. It includes two scenarios: (1) \textit{Shared-Context Distillation}, where both models use the same image for distillation;
    and (2) \textit{Local-Global Distillation}, where the teacher predicts depth for overlapping patches while the student predicts the full image. The Local-Global loss $\mathcal{L}_{\text{lg}}$ (Top Right) ensures consistency between local and global predictions, enabling the student to learn both fine details and broad structures, improving accuracy and robustness.
    }
    \vspace{-1em}
    \label{fig:method}
\end{figure*}


\section{Method}
% \vspace{-0.05in}
\label{sec:Method}
In this section, we introduce a novel distillation framework designed to leverage unlabeled images for training zero-shot Monocular Depth Estimation (MDE) models. We begin by exploring various depth normalization techniques in Section~\ref{sec:depth_norm}, followed by detailing our proposed distillation method in Section~\ref{sec:ref_dis}, which combines predictions across multiple contexts. The overall framework is illustrated in Fig.~\ref{fig:method}. Finally, we describe a multi-teacher distillation mechanism in Section~\ref{sec:multi_teacher} that integrates diverse depth estimators as teacher models to train the student model.

% \vspace{-0.05in}
\subsection{Depth Normalization}
\label{sec:depth_norm}
% \vspace{-0.05in}
Depth normalization is a crucial component of our framework as it adjusts the pseudo-depth labels \( \mathbf{d}^t \) from the teacher model and the depth predictions \( \mathbf{d}^s \) from the student model for effective loss computation. To understand the influence of normalization techniques on distillation performance, we systematically analyze several approaches commonly employed in prior works. These strategies are visually illustrated in Fig.~\ref{fig:norm}.




% \vspace{-0.05in}
\noindent \textbf{Global Normalization:} The first strategy we examine is the global normalization ~\cite{yang2024depthanything,depth_anything_v2} used in recent distillation methods. 
Global normalization~\cite{ranftl2020midas} adjusts depth predictions using global statistics of the entire depth map. This strategy aims to ensure scale-and-shift invariance by normalizing depth values based on the median and mean absolute deviation of the depth map. For each pixel \( i \), the normalized depth for the student model and pseudo-labels are computed as:
\begin{equation}
\begin{aligned}
\tilde{d}^s_i &= \mathcal{N}_{glo}(\mathbf{d}^s) = \frac{d^s_{i} - \operatorname{med}(\mathbf{d}^s)}{\frac{1}{M} \sum_{j=1}^M \left| d^s_{j} - \operatorname{med}(\mathbf{d}^s) \right|} \\
\tilde{d}^t_i &= \mathcal{N}_{glo}(\mathbf{d}^t) = \frac{d^t_{i} - \operatorname{med}(\mathbf{d}^t)}{\frac{1}{M} \sum_{j=1}^M \left| d^t_{j} - \operatorname{med}(\mathbf{d}^t) \right|},
\end{aligned}
\end{equation}
where \( \operatorname{med}(\mathbf{d}^s) \) and \( \operatorname{med}(\mathbf{d}^t) \) are the medians of the predicted depth and pseudo depth, respectively. The final regression loss for distillation is computed as the average absolute difference between the normalized predicted depth and the normalized pseudo depth across all valid pixels \( M \):
\begin{equation}
\mathcal{L}_{\text{Dis}} = \frac{1}{M} \sum_{i=1}^M \left| \tilde{d}^s_i - \tilde{d}^t_i \right|.
\label{eq:ssi}
\end{equation}


\begin{figure}[t]  
    \centering  
      \includegraphics[width=0.45\textwidth]{image/norm5.pdf}  
    % \vspace{-1.5em}
    \caption{\textbf{Normalization Strategies.} We compare four normalization strategies: Global Norm~\cite{ranftl2020midas}, Hybrid Norm~\cite{zhang2022hdn}, Local Norm, and No Norm. The figure visualizes how each strategy processes pixels within the normalization region (Norm. Area). The red dot represents any pixel within the region.
    }

    \vspace{-1.5em}
    \label{fig:norm}
\end{figure}


\noindent \textbf{Hybrid Normalization:}
In contrast to global normalization, Hierarchical Depth Normalization~\cite{zhang2022hdn} employs a hybrid normalization approach by integrating both global and local depth information. This strategy is designed to preserve both the global structure and local geometry in the depth map.
The process begins by dividing the depth range into \( S \) segments, where \( S \) is selected from \( \{ 1, 2, 4 \} \). When \( S = 1 \), the entire depth range is normalized globally, treating all pixels as part of a single context, akin to global normalization.
In the case of \( S = 2 \), the depth range is divided into two segments, with each pixel being normalized within one of these two local contexts. Similarly, for \( S = 4 \), the depth range is split into four segments, allowing normalization to be performed within smaller, localized contexts.
By adapting the normalization process to multiple levels of granularity, hybrid normalization achieves a balance between global coherence and local adaptability.
For each context \( u \), the normalized depth values for the student model \( \mathcal{N}_u(d_i^s) \) and pseudo-labels \( \mathcal{N}_u(d_i^t) \) are calculated within the corresponding depth range. The loss for each pixel \( i \) is then computed by averaging the losses across all contexts \( U_i \) to which the pixel belongs:
\begin{equation}
\mathcal{L}_{Dis}^i = \frac{1}{|U_i|} \sum_{u \in U_i} \left| \mathcal{N}_u(d_i^s) - \mathcal{N}_u(d_i^t) \right|,
    \label{eq:hdn}
\end{equation}
where \( |U_i| \) denotes the total number of groups (or contexts) that pixel \( i \) is associated with.
To obtain the final loss \( \mathcal{L}_{\text{Dis}} \), we average the pixel-wise losses across all valid pixels \( M \):
\begin{equation}
    \mathcal{L}_{\text{Dis}} = \frac{1}{M} \sum_{i=1}^M \mathcal{L}_{Dis}^i.
\end{equation}



% \vspace{-0.05in}
\noindent \textbf{Local Normalization:} In addition to global and hybrid normalization, we investigate Local Normalization, a strategy that focuses exclusively on the finest-scale groups used in hybrid normalization. This approach isolates the smallest local contexts for normalization, emphasizing the preservation of fine-grained depth details without considering hierarchical or global scales.
Local normalization operates by dividing the depth range into the smallest groups, corresponding to \( S = 4 \) in the hybrid normalization framework, and each pixel is normalized within its local context.
The loss for each pixel \( i \) is computed using a similar formulation as in hybrid normalization, but with \( u^i \) now representing the local context for pixel \( i \), defined by the smallest four-part group:
\begin{equation}
    \mathcal{L}_{\text{Dis}} = \frac{1}{M} \sum_{i=1}^M \left| \mathcal{N}_{u^i}(d_i^s) - \mathcal{N}_{u^i}(d_i^t) \right|.
\end{equation}





% \vspace{-0.1in}
\noindent\textbf{No Normalization:}
As a baseline, we also consider a direct depth regression approach with no explicit normalization. The absolute difference between raw student predictions and teacher pseudo-labels is used for loss computation:
\begin{equation}
\mathcal{L}_{\text{Dis}} = \frac{1}{M} \sum_{i=1}^M \left| d^s_i - d^t_i \right|,
\label{eq:l1_loss}
\end{equation}

This approach eliminates the need for normalization, assuming pseudo-depth labels naturally reside in the same domain as predictions. It provides insight into whether normalization enhances distillation effectiveness or if raw depth supervision suffices.



\vspace{-0.05in}
\subsection{Distillation Pipeline}
\vspace{-0.05in}
\label{sec:ref_dis}
In this section, we introduce an enhanced distillation pipeline that integrates two complementary strategies: Cross-Context Distillation and Multi-Teacher Distillation. Both strategies aim to improve the quality of pseudo-label distillation, enhance the model’s fine-grained perception, and boost generalization across diverse scenarios.

\begin{figure}[t]  
    \centering  
    \includegraphics[width=0.45\textwidth]{image/context-distll1.pdf}  
    \vspace{-0.5em}
    \caption{\textbf{Different Inputs Lead to Different Pseudo Labels.} Global Depth: The teacher model predicts depth using the entire image, and the local region's prediction is cropped from the output. Local Depth: The teacher model directly takes the cropped local region as input, resulting in more refined and detailed depth estimates for that area, capturing finer details compared to using the entire image.}

    % \vspace{-1.5em}
    \label{fig:context_dis}
\end{figure}

\vspace{-0.1in}
\paragraph{Cross-context Distillation.}  
A key challenge in monocular depth distillation is the trade-off between local detail preservation and global depth consistency. As shown in Fig.~\ref{fig:context_dis}, providing a local crop of an image as input to the teacher model enhances fine-grained details in the pseudo-depth labels, but it may fail to capture the overall scene structure. Conversely, using the entire image as input preserves the global depth structure but often lacks fine details. To address this limitation, we propose Cross-Context Distillation, a method that enables the student model to learn both local details and global structures simultaneously.
Cross-context distillation consists of two key strategies:\\
\textbf{1) Shared-Context Distillation:} In this setup, both the teacher and student models receive the same cropped region of the image as input. Instead of using the full image, we randomly sample a local patch of varying sizes from the original image and provide it as input to both models. This encourages the student model to learn from the teacher model across different spatial contexts, improving its ability to generalize to varying scene structures.
For the loss of shared-context distillation, the teacher and student models receive identical inputs and produce each depth prediction, denoted as \( \mathbf{d}^t_{\text{local}} \) and \( \mathbf{d}^s_{\text{local}} \):
\begin{equation}
\mathcal{L}_{\text{sc}} = \mathcal{L}_{\text{Dis}}\left (\mathbf{d}^s_{\text{local}}, \mathbf{d}^t_{\text{local}} \right),
\end{equation}
This loss encourages the student model to refine its fine-grained predictions by directly aligning with the teacher’s outputs at local scales.\\
\textbf{2) Local-Global Distillation:} In this approach, the teacher and student models operate on different input contexts. The teacher model processes local cropped regions, generating fine-grained depth predictions, while the student model predicts a global depth map from the entire image. To ensure knowledge transfer, the teacher’s local depth predictions supervise the corresponding overlapping regions in the student’s global depth map. This strategy allows the student to integrate fine-grained local details into its holistic depth estimation.
Formally, the teacher model produces multiple depth predictions for cropped regions, denoted as \( \mathbf{d}^t_{\text{local}_n} \), while the student generates a global depth map, \( \mathbf{d}^s_{\text{global}} \). The loss for  Local-Global distillation is computed only over overlapping areas between the teacher's local predictions and the corresponding regions in the student’s global depth map:
\begin{equation}
\mathcal{L}_{\text{lg}} = \frac{1}{N} \sum_{n=1}^N \mathcal{L}_{\text{Dis}}\left( \text{Crop}(\mathbf{d}^s_{\text{global}}),  \mathbf{d}^t_{\text{local}_n} \right),
\end{equation}
where \( \text{Crop}(\cdot) \) extracts the overlapping region from the student’s depth prediction, and \( N \) is the total number of sampled patches. This loss ensures that the student benefits from the detailed local supervision of the teacher model while maintaining global depth consistency.
The total loss function integrates both local and cross-context losses along with additional constraints, including feature alignment and gradient preservation, as proposed in prior works~\cite{depth_anything_v2}:
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{sc}} + \lambda_1 \cdot \mathcal{L}_{\text{lg}} + \lambda_2 \cdot \mathcal{L}_{\text{feat}} + \lambda_3 \cdot \mathcal{L}_{\text{grad}}.
\end{equation}
Here, \( \lambda_1 \), \( \lambda_2 \), and \( \lambda_3 \) are weighting factors that balance the different loss components. 
By incorporating cross-context supervision, this framework effectively allows the student model to integrate both fine-grained details from local crops and structural coherence from global depth maps. 


\begin{figure}[t]  
    \centering  
    \includegraphics[width=0.45\textwidth]{image/multi-teacher1.pdf}  
    \vspace{-1em}
    \caption{\textbf{Multi-teacher Mechanism.} We introduce a multi-teacher distillation approach, where pseudo-labels are generated from multiple teacher models. At each training iteration, one teacher is randomly selected to produce pseudo-labels for unlabeled images.
    }

    \vspace{-1em}
    \label{fig:multi-teacher}
\end{figure}

\vspace{-0.1in}
\paragraph{Multi-teacher Distillation.}  
\label{sec:multi_teacher} 
In addition to cross-context distillation, we adopt a multi-teacher distillation strategy, illustrated in Fig.~\ref{fig:multi-teacher}, to further enhance the quality and robustness of the distilled depth knowledge. This approach leverages multiple teacher models, each trained with distinct architectures, optimization strategies, or data distributions, to generate diverse pseudo-labels. By aggregating knowledge from multiple sources, the student model benefits from a richer and more generalized depth representation.
Formally, given a set of pre-trained teacher models \( \mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_N \), we employ a probabilistic teacher selection mechanism, where one teacher model is randomly selected at each training iteration to generate pseudo-labels for the input image. 
The inclusion of multiple teacher models allows the student to learn from a diverse set of predictions, effectively mitigating biases and limitations inherent to any single model.









\section{Experiment}
\vspace{-0.05in}
\subsection{Experimental Settings}
\vspace{-0.05in}


\noindent \textbf{Datasets.} To evaluate the effectiveness of our proposed distillation framework, we follow the methodology outlined in DepthAnythingv2~\cite{depth_anything_v2}. Specifically, we conduct our distillation experiments using a subset of 200,000 samples from the SA-1B dataset~\cite{sa1b}.

For evaluation, we assess the performance of the distilled student model on five widely used depth estimation benchmarks, ensuring that these datasets remain unseen during training to enable a robust zero-shot evaluation. The chosen benchmarks include:
% We evaluate the student model on five popular benchmarks that were not seen during training: 
NYUv2~\cite{silberman2012indoor}, KITTI~\cite{geiger2012we}, ETH3D~\cite{schoeps2017eth3d}, ScanNet~\cite{dai2017scannet}, and DIODE~\cite{vasiljevic2019diode}. Additional dataset details are provided in the Appendix.


% \vspace{-0.05in}
\noindent \textbf{Metrics.} We assess depth estimation performance using two key metrics: the mean absolute relative error (AbsRel) and $\delta_1$ accuracy. 
Following previous studies~\cite{ranftl2020midas,yin2023metric3d, marigold} on zero-shot MDE, we align predictions with ground truth in both scale and shift before evaluation.





% \vspace{-0.05in}
\noindent\textbf{Implementation.}
Our experiments use state-of-the-art monocular depth estimation models as teachers to generate pseudo-labels, supervising various student models in a distillation framework with only RGB images as input. 
In shared-context distillation, both teacher and student receive the same global region, extracted via random cropping from the original image. The crop maintains a 1:1 aspect ratio and is sampled within a range from 644 pixels to the shortest side of the image, then resized to \( 560 \times 560 \) for local predictions.
In global-local distillation, the global region is cropped into overlapping local patches, each sized \( 560 \times 560 \), for the teacher model to predict pseudo-labels.
We use GenPercept~\cite{xu2024diffusion}) and DepthAnythingv2 (DAv2)~\cite{depth_anything_v2} as teacher models for the multi-teacher mechanism.
The learning rate is in tune with that of the corresponding student model. For DAv2~\cite{depth_anything_v2}, the decoder learning rate is set to \( 5 \times 10^{-5} \). For the total loss function, we set the parameters as follows: \( \lambda_1 = 0.5 \), \( \lambda_2 = 1.0 \) and \( \lambda_3 = 2.0 \).



% \vspace{-0.05in}
\subsection{Analysis}
% \vspace{-0.05in}
For the ablation study and analysis, we sample a subset of 50K images from \textbf{SA-1B}~\cite{sa1b} as our training data, with an input image size of 560 × 560 for the network. We conduct experiments on two of the most challenging benchmarks, DIODE~\cite{vasiljevic2019diode} and ETH3D~\cite{schoeps2017eth3d}, which include both indoor and outdoor scenes. The model was trained with a batch size of 4 and converged after approximately 20,000 iterations on a single NVIDIA V100 GPU.

% \vspace{-0.05in}
\noindent\textbf{Impact of Normalization across Cross-Context Distillation.}
We evaluate the effect of different normalization strategies 
% (Section~\ref{sec:depth_norm})
on Cross-Context Distillation, as shown in Table~\ref{impact_of_norm}. The results indicate that the optimal normalization method varies across different distillation strategies. For shared-context distillation, no normalization achieves the best performance, assuming that pseudo-depth labels naturally reside in the same domain as predictions.
% preserving pixel-level depth values and avoiding interference from global context adjustments.
For Local-Global distillation, Hybrid Normalization proves most effective, maintaining consistent depth predictions across regions through hierarchical normalization within specific depth ranges.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{image/vis_comp.pdf}
    \vspace{-2em}
    \caption{
    \textbf{Qualitative Comparison of Relative Depth Estimations.} We present visual comparisons of depth predictions from our method ("Ours") alongside other classic depth estimators (''MiDaS v3.1''~\cite{birkl2023midasv31model}, and models using DINOv2~\cite{oquab2023dinov2} or SD as priors (''DepthAnythingv2~\cite{depth_anything_v2}'', ''Marigold''~\cite{marigold}, ''Genpercept''~\cite{xu2024diffusion}). Compared to state-of-the-art methods, the depth map produced by our model, particularly at the position indicated by the \textbf{black arrow}, exhibits finer granularity and more detailed depth estimation. 
    }

    \vspace{-1em}
    \label{fig:vis}
\end{figure*}





% \vspace{-0.1in}
\noindent\textbf{Ablation Study of Cross-Context Distillation.}
To further validate the effectiveness of our distillation framework, we conduct ablation studies by removing Shared-Context Distillation and Local-Global Distillation in Table~\ref{tab:performance_comparison}. The results show that both components contribute significantly to 


% \vspace{-1pt}
\begin{figure}[H]
    \vspace{-1em}
    \centering
    \begin{minipage}{0.45\textwidth}  % 缩小宽度，避免重叠
        \centering
        \captionsetup{skip=0.5em} % 调整caption与表格之间的间距
        \begin{table}[H]
            \centering
            \caption{\textbf{Analysis of Normalization Strategies.} Performance comparison of different normalization strategies across Shared-Context Distillation and Local-Global Distillation.}
            \vspace{0.1in}
            \label{impact_of_norm}
            \resizebox{\textwidth}{!}{
                \begin{tabular}{c c S[table-format=1.3] S[table-format=1.3]}
                \toprule
                \multirow{2}{*}{Method} &
                \multirow{2}{*}{Normalization} & 
                \multicolumn{1}{c}{ETH3D} & \multicolumn{1}{c}{DIODE} \\
                && \multicolumn{1}{c}{AbsRel↓} & \multicolumn{1}{c}{AbsRel↓} \\
                \midrule
                \multirow{4}{*}{\shortstack{Shared-Context \\ Distillation}} 
                & Global Norm. & 
                0.067 & 0.243 \\
                \cmidrule{2-4}
                & \cellcolor[rgb]{ .886,  .937,  .855}No Norm. & 
                \cellcolor[rgb]{ .886,  .937,  .855} \textbf{0.058} & \cellcolor[rgb]{ .886,  .937,  .855} \textbf{0.236} \\
                & Local Norm. & 
                0.060 & 0.238 \\
                & Hybrid Norm. & 
                0.059 & 0.237 \\
                \midrule
                \multirow{4}{*}{\shortstack{Local-Global \\ Distillation}} 
                & Global Norm. & 
                0.065 & 0.253 \\
                \cmidrule{2-4}
                & No Norm. & 
                0.060 & 0.235 \\
                & Local Norm. & 
                0.059 & 0.235 \\
                & \cellcolor[rgb]{ .886,  .937,  .855}Hybrid Norm. & 
                \cellcolor[rgb]{ .886,  .937,  .855} \textbf{0.056} & \cellcolor[rgb]{ .886,  .937,  .855} \textbf{0.232} \\
                \bottomrule
                \end{tabular}
            }
            \vspace{-1em}
            
        \end{table}

    \end{minipage} %%%%%%%%%%%%%
    \hfill
    \begin{minipage}{0.45\textwidth}  % 缩小宽度，避免重叠
        \captionsetup{skip=0.5em} % 调整caption与表格之间的间距
       \centering
        \begin{table}[H]
        \caption{\textbf{Effect of Cross-context Distillation.} Performance comparison of various combinations of Shared-Context Distillation and Local-Global distillation on the ETH3D~\cite{schoeps2017eth3d} and DIODE~\cite{vasiljevic2019diode} datasets. The baseline corresponds to a simple shared-context approach with no random cropping. When neither method is applied, the model defaults to this baseline.}
        \vspace{0.1in}
        \label{tab:performance_comparison}
        \resizebox{\textwidth}{!}{ % 缩放表格内容
        \begin{tabular}{ccll}
        \toprule
        \multirow{2}{*}{\makecell{Shared-Context \\ Distillation}} & \multirow{2}{*}{\makecell{Local-Global \\ Distillation}} & \multicolumn{1}{c}{ETH3D} & \multicolumn{1}{c}{DIODE} \\
        & & \multicolumn{1}{c}{AbsRel↓} & \multicolumn{1}{c}{AbsRel↓} \\
        \midrule
        \xmark & \redxmark & 0.075 & 0.270 \\
        \redxmark & \greencheckmark & 0.057 \green{-24.0} & 0.234 \green{-13.3} \\
        \greencheckmark & \redxmark & 0.058 \green{-22.6} & 0.237  \green{-12.2}\\
        \rowcolor[rgb]{ .886,  .937,  .855} \greencheckmark & \greencheckmark & \textbf{0.056} \green{-25.3} & \textbf{0.232}  \green{-14.1}\\
        \bottomrule
        \end{tabular}
        }
        % \vspace{-1em}
        
        \end{table}
       
    \end{minipage}
    \vspace{-1em}
\end{figure}



\noindent improving the student model’s ability to utilize pseudo-labels, demonstrating the robustness of our approach.



\begin{figure}[H]
    % \vspace{-1em}
    \centering
    \begin{minipage}{0.5\textwidth}  % 缩小宽度，避免重叠
        % \vspace{-1em}
        % \captionsetup{skip=0.5em} % 调整caption与表格之间的间距
        \centering
        \begin{table}[H]    \caption{\textbf{Comparison in Cross-Architecture Distillation.} Evaluation of our distillation pipeline in the context of Cross-Architecture Distillation. We adopt different architectures as teacher and student models, where the \textbf{Base} represents the previous distillation method~\cite{depth_anything_v2}. Our method consistently improves the performance of the distilled student models.}
        \label{tab:cross_architecture_distillation}
        \resizebox{\textwidth}{!}{ % 缩放表格内容
        \begin{tabular}{c c c >{\raggedright\arraybackslash}p{2.2cm} 
        >{\raggedright\arraybackslash}p{2.2cm} }
        \toprule
        \multirow{2}{*}{Teacher} & 
        \multirow{2}{*}{Student} & 
        \multirow{2}{*}{\makecell{Training \\ Loss}} & 
        \multicolumn{1}{c}{DIODE} & 
        \multicolumn{1}{c}{ETH3D} \\
        \cmidrule(lr){4-4} \cmidrule(lr){5-5}
        & & &  \multicolumn{1}{c}{AbsRel↓} &  \multicolumn{1}{c}{AbsRel↓} \\
        \midrule      
        \multirow{2}{*}{DA-L} & \multirow{2}{*}{DA-S} &  Base & 0.290 & 0.110\\
         &  & \cellcolor[rgb]{ .886,  .937,  .855} Ours & \cellcolor[rgb]{ .886,  .937,  .855} \textbf{0.262} \green{-9.6} & \cellcolor[rgb]{ .886,  .937,  .855} \textbf{0.098} \green{-10.9} \\
        \midrule
        \multirow{2}{*}{DA-L} & \multirow{2}{*}{Midas-L} &  Base  & 0.313 & 0.147\\
         &  & \cellcolor[rgb]{ .886,  .937,  .855}Ours & \cellcolor[rgb]{ .886,  .937,  .855}\textbf{0.295}\green{-5.7} & \cellcolor[rgb]{ .886,  .937,  .855}\textbf{0.126}\green{-14.3} \\
        \midrule
        \multirow{2}{*}{Midas-L} & \multirow{2}{*}{Midas-S} & Base  & 0.303 & 0.150 \\
         &  & \cellcolor[rgb]{ .886,  .937,  .855}Ours & \cellcolor[rgb]{ .886,  .937,  .855}\textbf{0.272} \green{-10.2}& \cellcolor[rgb]{ .886,  .937,  .855}\textbf{0.120}\green{-20.0}\\
        \bottomrule
        \end{tabular}
        }
        \vspace{-1em}
    
        \end{table}

    \end{minipage}
    % \hfill
    % \begin{minipage}{0.45\textwidth}  % 调整图像的宽度为0.65\textwidth，避免占太大空间
    %     \centering
    %     \includegraphics[width=\textwidth]{image/scaleup.pdf}
    %     \vspace{-2em}
    %     \caption{\textbf{Comparison of Data Scaling 
    %     % and SSI
    %     .} Performance comparison of our model with SSI Loss as the dataset size increases, measured by the average AbsRel. The results indicate that global normalization degrades accuracy as the dataset size grows.
    %     % Performance comparison of our model with SSI Loss and Hybrid Normalization Loss across multiple benchmarks as the dataset size increases, measured by the average AbsRel metric. The results indicate that global normalization degrades accuracy as the dataset size grows.
    %     % With our cross-context distillation, SSI can even surpass the teacher model.
    %     }
    %     \label{fig:data_scaling_comparison}
    % \end{minipage}
    % \vspace{-1em}
\end{figure}


% ###############
% \subsection{Cross Architecture Distillation}
% \vspace{-0.05in}
\noindent\textbf{Cross-Architecture Distillation.} To evaluate our normalization strategy, we conducted experiments using MiDaS~\cite{ranftl2020midas} and DepthAnything~\cite{depth_anything_v2}, testing four configurations (DA-L, 
% \noindent 
MiDaS-L, DA-S, MiDaS-S) as shown in Table~\ref{tab:cross_architecture_distillation}. Our method consistently outperforms previous distillation approaches that use global normalization on the DIODE~\cite{vasiljevic2019diode} and ETH3D~\cite{schoeps2017eth3d} datasets, demonstrating superior performance both within and across architectures, and highlighting the limitations of global normalization in pseudo-label distillation.



% \vspace{-0.05in}
\noindent\textbf{Multi-teacher Mechanism.}
We evaluate the effectiveness of our multi-teacher distillation strategy across five benchmarks in Table~\ref{tab:multi-teacher}.
To handle the diverse output depth distributions of different teacher models, we use Hybrid Normalization for Shared-Context Distillation in this experiment.
Using diffusion-based Genpercept~\cite{xu2024diffusion} and Dinov2-based DepthAnythingv2~\cite{depth_anything_v2} as teacher models, we train a lightweight DPT-based depth estimation model. Our approach outperforms both teacher models overall, with only a minor gap on KITTI~\cite{geiger2012we}, demonstrating the effectiveness of multi-teacher distillation.


\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \caption{
    \textbf{Quantitative comparison of our multi-teacher distillation model on zero-shot benchmarks.} The \textbf{bold} values indicate the best performance. Our model, which integrates diverse depth estimation models, achieves higher accuracy than any individual teacher model.}
    \label{tab:multi-teacher}
\resizebox{\linewidth}{!}{
    \begin{tabular}{ 
        @{}l 
        c@{\hspace{0.5em}}c 
        c@{\hspace{0.5em}}c 
        c@{\hspace{0.5em}}c 
        c@{\hspace{0.5em}}c 
        c@{\hspace{0.5em}}c 
        c@{} 
    }
        \toprule
        \multirow{2}{*}{Method} &
        \multicolumn{2}{c}{NYUv2} & 
        \multicolumn{2}{c}{KITTI} & 
        \multicolumn{2}{c}{DIODE} & 
        \multicolumn{2}{c}{ScanNet} & 
        \multicolumn{2}{c}{ETH3D} & 
        \multirow{2}{*}{Avg. Rank} \\
        
        & AbsRel↓ & $\delta$1↑ & 
        AbsRel↓ & $\delta$1↑ & 
        AbsRel↓ & $\delta$1↑ & 
        AbsRel↓ & $\delta$1↑ & 
        AbsRel↓ & $\delta$1↑ & 
        \\
        \midrule

        DepthAnything v2 \tiny{NeurIPS'24}& 
        0.045 & 0.979 & 
        \textbf{0.074} & \textbf{0.946} & 
        0.262 & 0.754 & 
        0.042 & 0.978 & 
        0.131 & 0.865 & 
        1.9 \\

        Genpercept(Disparity)\tiny{ICLR'25} &
        0.058 & 0.969 & 
        0.080 & 0.934 & 
        \textbf{0.226} & 0.741 & 
        0.063 & 0.960 & 
        0.096 & 0.959 & 
        2.6 \\

        \rowcolor[rgb]{ .886,  .937,  .855} Ours(Multi-teacher) &
        \textbf{0.043} & \textbf{0.981} & 
        0.077 & 0.945 & 
        0.298 & \textbf{0.756} & 
        \textbf{0.042} & \textbf{0.979} & 
        \textbf{0.065} & \textbf{0.983} & 
        \textbf{1.4} \\
        \bottomrule
    \end{tabular}
}

      \vspace{-1em}
    
\end{table*}

% 上面是对比multi-teacber的表


\begin{table*}[t]
    
    \centering
\renewcommand{\arraystretch}{1.1}
\caption{
    \textbf{Quantitative comparison with other affine-invariant depth estimators on several zero-shot benchmarks.} The \textbf{bold} values indicate the best performance, and \underline{underscored} represent the second-best results.}
    \label{benchmark}
    \resizebox{\linewidth}{!}{
\begin{tabular}{
    @{}l 
    c@{\hspace{0.5em}}c @{}p{1em}@{} 
    c@{\hspace{0.5em}}c @{}p{1em}@{} 
    c@{\hspace{0.5em}}c @{}p{1em}@{} 
    c@{\hspace{0.5em}}c @{}p{1em}@{} 
    c@{\hspace{0.5em}}c @{}p{1em}@{}
}
    \toprule
    \multirow{2}{*}{Method} &
    \multicolumn{2}{c}{NYUv2} & &
    \multicolumn{2}{c}{KITTI} & &
    \multicolumn{2}{c}{DIODE} & &
    \multicolumn{2}{c}{ScanNet} & &
    \multicolumn{2}{c}{ETH3D} \\
    
    & AbsRel↓ & $\delta$1↑ & &
    AbsRel↓ & $\delta$1↑ & &
    AbsRel↓ & $\delta$1↑ & &
    AbsRel↓ & $\delta$1↑ & &
    AbsRel↓ & $\delta$1↑ \\

    \midrule

    DiverseDepth~\cite{yin2020diversedepth} & 
    0.117 & 0.875 & & 
    0.190 & 0.704 & &
    0.376 & 0.631 & &
    0.108 & 0.882 & &
    0.228 & 0.694 \\


    MiDaS~\cite{ranftl2020midas} & 
    0.111 & 0.885 & & 
    0.236 & 0.630 & &
    0.332 & 0.715 & &
    0.111 & 0.886 & &
    0.184 & 0.752 \\


    LeReS~\cite{wei2021leres} & 
    0.090 & 0.916 & & 
    0.149 & 0.784 & &
    0.271 & 0.766 & &
    0.095 & 0.912 & &
    0.171 & 0.777 \\


    Omnidata~\cite{eftekhar2021omnidata} & 
    0.074 & 0.945 & & 
    0.149 & 0.835 & &
    0.339 & 0.742 & &
    0.077 & 0.935 & &
    0.166 & 0.778 \\


    HDN~\cite{zhang2022hdn} & 
    0.069 & 0.948 & & 
    0.115 & 0.867 & &
    0.246 & \underline{0.780} & &
    0.080 & 0.939 & &
    0.121 & 0.833 \\


    DPT~\cite{ranftl2021dpt} & 
    0.098 & 0.903 & & 
    0.100 & 0.901 & &
    \underline{0.182} & 0.758 & &
    0.078 & 0.938 & &
    0.078 & 0.946 \\

    % DepthAnythingv1~\cite{yang2024depthanythingv2} & 
    % \textbf{0.043} & \textbf{0.981} & & 
    % 0.076 & 0.947 & &
    % 0.277 & 0.759 & &
    % \textbf{0.042} & \textbf{0.980} & &
    % 0.127 & 0.882 \\

    DepthAnything v2~\cite{yang2024depthanything} & 
    \underline{0.045} & 0.979 & & 
    0.074 & 0.946 & &
    0.262 & 0.754 & &
    \textbf{0.042} & \underline{0.978} & &
    0.131 & 0.865 \\


    Marigold~\cite{ke2024marigold} & 
    0.055 & 0.961 & & 
    0.099 & 0.916 & &
    0.308 & 0.773 & &
    0.064 & 0.951 & &
    0.065 & 0.960 \\
    
    Midas v3.1
    % $^\diamond$
    ~\cite{birkl2023midasv31model} & 
    - & 0.980 & & 
    - & \underline{0.949} & &
    - & - & &
    - & - & &
    0.061 & 0.968 \\




    \midrule
    % \rowcolor[rgb]{ .886,  .937,  .855} 
    Ours$^\dagger$ & 
    0.046 & \textbf{0.985} & & 
    \textbf{0.063} & \textbf{0.972} & & 
    \textbf{0.142} & \textbf{0.788} & & 
    \underline{0.049} & \textbf{0.980} & & 
    \underline{0.057} & \underline{0.976} \\


    % \rowcolor[rgb]{ .886,  .937,  .855}
    Ours$^*$ &
    \textbf{0.043} & \underline{0.981} & &
    \underline{0.070} & \underline{0.949} & &
    0.233 & 0.753 & &
    \textbf{0.042} & \textbf{0.980} & &
      \textbf{0.054} & \textbf{0.981}\\
    % \bottomrule
    % Ours$^{multi-teacher}$ &
    % \textbf{0.043} & \underline{0.981} & &
    % \underline{0.077} & \underline{0.945} & &
    % 0.298 & 0.756 & &
    % \textbf{0.042} & \textbf{0.979} & &
    %   \textbf{0.065} & \textbf{0.983}\\
    \bottomrule
\end{tabular}
    }
    \\
    \begin{minipage}{0.96\linewidth}
        \scriptsize
        % \vspace{0.4em}
        \begin{itemize}
        \item[$^\dagger$] Refers to our method applied on the MiDaS v3.1. $^*$ Refers to our method applied on the DepthAnythingv2-Large. 
        % \item[$^\diamond$] Metrics for MiDaS v3.1 are incomplete because only partial metrics for certain datasets were reported in the original paper.
        \end{itemize}
    \end{minipage}
    \vspace{-0.1in}
    
\end{table*}

% 上面是对应不同模型大表


% \vspace{-0.1in}
\subsection{Comparison with State-of-the-Art}
% \vspace{-0.05in}
\label{sec:analysis_zero_shot}
% To validate our method, we compare it with state-of-the-art (SOTA) approaches on multiple zero-shot benchmarks.


\noindent\textbf{Quantitative Analysis.}
%Our model demonstrates robust 
Our model achieves SOTA performance across both indoor and outdoor datasets, demonstrating strong generalization from structured indoor scenes (NYUv2~\cite{silberman2012indoor}, ScanNet~\cite{dai2017scannet}) to complex outdoor environments (KITTI~\cite{geiger2012we}, DIODE~\cite{vasiljevic2019diode}, ETH3D~\cite{schoeps2017eth3d}), as shown in Table~\ref{benchmark}. By optimizing pseudo-label distillation and depth normalization, 
our student model not only surpasses its teacher but also achieves a new SOTA on multiple benchmarks, demonstrating the effectiveness of our approach.

% \vspace{-0.05in}
\noindent\textbf{Qualitative analysis.}
%Fig.~\ref{fig:vis} compares depth maps from SOTA models and our method. 
We show a qualitative comparison of different depth estimations between SOTA models and the proposed method in Fig.~\ref{fig:vis}.
Compared with DAv2~\cite{depth_anything_v2}, our method preserves finer details, particularly in areas marked by arrows. 
%While 
Although
Marigold~\cite{marigold} and Genpercept~\cite{xu2024diffusion} generate detailed maps using generative priors, they struggle with correct relative depth relationships. In contrast, our model preserves fine details while maintaining accurate relative depth relationships, resulting in a visually consistent and reliable depth estimation.



% \vspace{-0.1in}
\section{Conclusion}
\vspace{-0.05in}
In this work, we study pseudo-label distillation strategies for MDE. We find that the widely used SSI normalization amplifies noise in teacher-generated pseudo-labels, impairing local depth accuracy. To address 
the problem, we propose Cross-Context Distillation, which combines local refinement with global consistency, enabling the model to learn fine details and structural context. Our multi-teacher framework, integrating diffusion-based models and encoder-decoder networks, achieves state-of-the-art performance on multiple benchmarks. 
Future work could improve the efficiency of unlabeled data distillation.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egpaper_final}
}

\clearpage


\section{Appendix}

\subsection{Dataset Details}
\noindent\textbf{Datasets.}  
We train our model on \textbf{SA-1B}~\cite{sa1b}, a large-scale dataset covering diverse indoor and outdoor environments, enabling robust depth learning for real-world scenes.
For evaluation, we use established monocular depth benchmarks:  
\begin{itemize}
    \item \textbf{NYUv2}~\cite{silberman2012indoor}: Indoor depth estimation and semantic segmentation.
    \item \textbf{KITTI}~\cite{geiger2012we}: Autonomous driving dataset with outdoor scenes and high-quality LiDAR depth.
    \item \textbf{ETH3D}~\cite{schoeps2017eth3d}: High-resolution stereo images for indoor/outdoor depth estimation and 3D reconstruction.
    \item \textbf{ScanNet}~\cite{dai2017scannet}: Large-scale RGB-D dataset for 3D scene reconstruction and semantic segmentation.
    \item \textbf{DIODE}~\cite{vasiljevic2019diode}: Dense, high-quality depth maps for both indoor and outdoor environments.
\end{itemize}

\noindent\textbf{Metrics.}  
We evaluate depth estimation using mean absolute relative error (AbsRel) and $\delta_1$ accuracy. AbsRel is defined as:  
\begin{equation}
AbsRel = \frac{1}{M} \sum_{i=1}^M \frac{|d_i - d_i^*|}{d_i^*}
\end{equation}
where \( d_i \) is the predicted depth, \( d_i^* \) is the ground truth, and \( M \) is the total number of depth values. $\delta_1$ accuracy measures the percentage of pixels where:
\begin{equation}
\delta_1 = \max\left(\frac{d_i}{d_i^*}, \frac{d_i^*}{d_i}\right) < 1.25
\end{equation}
indicating prediction accuracy within a specific tolerance. Following Metric3D~\cite{ranftl2020midas,yin2023metric3d, marigold}, we align predictions with ground truth in scale and shift before evaluation.  

\subsection{More Experiments}
% \subsection{Effect of Data Scaling}
\noindent\textbf{Effect of Data Scaling.}
\label{sec:data_scaling_effect}
To investigate how dataset size affects model performance, we conducted experiments using progressively larger training datasets and compared our method against the SSI Loss baseline. Fig.~\ref{fig:data_scaling_comparison} shows the Absolute Relative Error (AbsRel) as the dataset size increases from 10K to 200K images.

\noindent\textbf{Distilling Generative Models vs. DepthAnythingv2.}  
Beyond distilling encoder-decoder depth models, we extend our approach to generative models, specifically Genpercept~\cite{xu2024diffusion}, aiming to transfer their superior detail preservation to a more efficient student model. While diffusion-based depth estimators achieve fine-grained depth reconstruction, their high computational cost limits practical applications. We investigate whether their depth estimation capability can be effectively distilled into a lightweight DPT-based model. Experimental results in Fig.~\ref{fig:distill_with_generation} show that compared to using DepthAnythingv2 as the teacher, distilling from a diffusion-based model yields a student model with significantly enhanced fine-detail prediction.

\noindent\textbf{Qualitative Comparison with Baseline Distillation.}  
We present a qualitative comparison between our method and the previous distillation method~\cite{depth_anything_v2}, where the \textbf{Base} model relies solely on global normalization. We analyze the depth map details and the distribution differences between predicted and ground truth depths. The red diagonal lines represent the ground truth, with results closer to these lines indicating better performance. As shown in Fig.~\ref{fig:qual_ssi}, our method produces smoother surfaces, sharper edges, and more detailed depth maps.

\noindent\textbf{Qualitative Comparison: Additional Results on Depth Estimation in the Wild.} We present additional depth maps generated by our model on in-the-wild scenes, emphasizing its robustness and precision. As shown in Fig.~\ref{fig:more_results}, our method produces sharper edges and more detailed depth maps, even in challenging regions such as hair, cartoon scenes, and other diverse environments.

\begin{figure}[t]
    \vspace{-1em}
    \centering
    \begin{minipage}{0.45\textwidth}  % 调整图像的宽度为0.65\textwidth，避免占太大空间
        \centering
        \includegraphics[width=\textwidth]{image/scaleup.pdf}
        % \vspace{-2em}
        \caption{\textbf{Comparison of Data Scaling 
        % and SSI
        .} Performance comparison of our model with SSI Loss as the dataset size increases, measured by the average AbsRel. The results indicate that our method consistently outperforms the baseline method.
        }
        \label{fig:data_scaling_comparison}
    \end{minipage}
\end{figure}

\begin{figure*}[h]  
    \centering  
    \includegraphics[width=1\textwidth]{image/distill_gen.pdf}  
    % \vspace{-1.5em}
    \caption{\textbf{Distilled Generative Models}: Instead of just distilling classical depth models, we also apply distillation to generative models, aiming for the student model to capture their rich details.}
    % \vspace{-1.5em}
    \label{fig:distill_with_generation}
\end{figure*}


\begin{figure*}[h]  
    \centering  
    \includegraphics[width=1\textwidth]{image/depth_point2.pdf}  
    \vspace{-1.5em}
    \caption{\textbf{Qualitative Comparison with Baseline Distillation.} We compare our method with the baseline as the previous distillation method, which uses only global normalization. The red diagonal lines represent the ground truth, with results closer to the lines indicating better performance. Our method produces smoother surfaces, sharper edges, and more detailed depth maps.}

    % \vspace{-1.5em}
    \label{fig:qual_ssi}
\end{figure*}

\begin{figure*}[h]  
    \centering  
    \includegraphics[width=1\textwidth]{image/more_results1.pdf}  
    \vspace{-1.5em}
    \caption{\textbf{Additional Results on Depth Estimation in the Wild.} We showcase more depth maps generated by our model on in-the-wild scenes, highlighting its robustness and precision.}

    % \vspace{-1.5em}
    \label{fig:more_results}
\end{figure*}

\end{document}