\section{Related Work}
\label{sec:Related}
% \vspace{-0.05in}
\subsection{Monocular Depth Estimation}
% \vspace{-0.05in}
Monocular depth estimation (MDE) has evolved from hand-crafted methods to deep learning, significantly improving accuracy ____. Architectural refinements, such as multi-scale designs and attention mechanisms, have further enhanced feature extraction ____. However, most models remain reliant on labeled data and struggle to generalize across diverse environments.
Zero-shot MDE improves generalization by leveraging large-scale datasets, geometric constraints, and multi-task learning ____. Metric depth estimation incorporates intrinsic data for absolute depth learning ____, while generative models such as Marigold refine depth details using diffusion priors ____. 
%Despite these advances, effectively utilizing unlabeled data remains a challenge, as pseudo-labels often contain noise and suffer from inconsistencies across different contexts.
Despite these advances, effectively utilizing unlabeled data remains a challenge due to pseudo-label noise and inconsistencies across different contexts. DepthAnything ____ explores large-scale unlabeled data but struggles with pseudo-label reliability. PatchFusion ____ improves depth estimation by refining high-resolution image representations but lacks adaptability in generative settings.
To address these issues, we propose Cross-Context and Multi-Teacher Distillation, which enhances pseudo-label supervision by leveraging diverse contextual information and multiple expert models, improving both accuracy and generalization ability.

% \vspace{-0.05in}
\subsection{Semi-supervised Monocular Depth Estimation}
% \vspace{-0.05in}

Semi-supervised depth estimation has gained attention by utilizing temporal consistency to better use unlabeled data____. Some methods____ apply stereo geometric constraints, enforcing left-right consistency to enhance depth accuracy, while others use additional supervision like semantic priors____ or GANs, such as DepthGAN____. However, these approaches are limited by their reliance on temporal cues or stereo constraints, restricting their applicability. Recent work____ explored pseudo-labeling for semi-supervised MDE but lacks generative modeling capabilities. DepthAnything____ demonstrated the potential of large-scale unlabeled data, though pseudo-label reliability remains challenging. In contrast, our approach improves pseudo-label reliability and enhances MDE accuracy, relying solely on unlabeled data without additional constraints.

\begin{figure}[t]
    \centering  \includegraphics[width=0.47\textwidth]{image/ssi_problem6.pdf}
    % \vspace{-1em}
    \caption{
    \textbf{Issue with Global Normalization (SSI).} 
    In (a), we compare two alignment strategies for the central \( w/2, h/2 \) region: (1) \textit{Global Least-Square}, where alignment is applied to the full image before cropping, and (2) \textit{Local Least-Square}, where alignment is performed on the cropped region. Metrics are computed on the cropped region. As shown in (b), 
    the outperformed local strategy demonstrates
    that \textbf{global normalization degrades local accuracy compared to local normalization}.
    }
    % \vspace{-1.5em}
    \label{fig:normalization}
\end{figure}

\begin{figure*}[!t]
    \centering    \includegraphics[width=0.9\textwidth]{image/method6.pdf}
    \vspace{-1em}
    \caption{
    \textbf{Overview of Cross-Context Distillation.} 
    Our method combines local and global depth information to enhance the student modelâ€™s predictions. It includes two scenarios: (1) \textit{Shared-Context Distillation}, where both models use the same image for distillation;
    and (2) \textit{Local-Global Distillation}, where the teacher predicts depth for overlapping patches while the student predicts the full image. The Local-Global loss $\mathcal{L}_{\text{lg}}$ (Top Right) ensures consistency between local and global predictions, enabling the student to learn both fine details and broad structures, improving accuracy and robustness.
    }
    \vspace{-1em}
    \label{fig:method}
\end{figure*}