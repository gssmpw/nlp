\subsection{Bayesian Inference}
\label{sec:inference}
Unfortunately, the inference of the given posterior distribution is hard due to the non-conjugacy between normal prior for $\pmb\eta$ and the logistic transformation function \citep{blei2007correlated}. Variational inference is the most frequently employed tool to address this problem, with the additional advantage of computational speed. However, obtained parameters are for the variational distribution which is an approximation to the target posterior. The quality of the approximation is often not sufficiently explored. Furthermore, the variational inference is an optimization method that outputs point estimates. This requires additional steps to obtain a measure of uncertainty in estimation. Quantifying uncertainty in variational inference is often done through bootstrapping \citep{chen2018use,imai2016fast}. However, obtaining bootstrap samples representative of the pseudo population can be highly challenging for network data since observations are connected \citep{chen2019snowboot,levin2019bootstrapping}. It often requires block sampling which entails computing other network quantities (i.e. geodesic distance in \cite{raftery2012fast}) but these additional processes could defeat the advantage of the computational efficiency of using variational inference. 

To remedy this problem, we follow the recent advances in the inference of Correlated Topic Model (CTM) that adopts partial collapsing \citep{held2006bayesian,chen2013scalable,linderman2015dependent}. We first partially collapse the posterior distribution by integrating out the topic-word probability parameter $\pmb\Psi$. Then we introduce an auxiliary Polya-Gamma variable $\pmb\lambda$ and augment the collapsed posterior. Partial collapsing and data augmentation enables us to use Gibbs sampling which is known to produce samples that converge to the exact posterior. With $\pmb\Psi$ integrated out, our new posterior is proportional to
{\small
\begin{align}
	\int_{\pmb\Psi}p(\pmb\eta,\pmb\Psi,\textbf{Z},\pmb\tau|\textbf{W},\textbf{D}) \propto p(\pmb\mu|\pmb\mu_0,\pmb\Sigma_0)p(\pmb\tau|\pmb\mu_{\tau},\pmb\Sigma_{\tau})p(\pmb\eta|\pmb\mu,\pmb\Sigma)p(\textbf{Z}|\pmb\eta)p(\textbf{W}|\textbf{Z})p(\textbf{D}|\textbf{D}^*)p(\textbf{D}^*|\pmb\tau,\pmb\eta,\textbf{Z},\textbf{D})
\end{align}
}
where $p(\textbf{W}|\textbf{Z})$ results from collapsing $\pmb\Psi$ as follows.
\begin{align}
	p(\textbf{W}|\textbf{Z}) &= \int_{\pmb\Psi} p(\textbf{W},\pmb\Psi|\textbf{Z})d\pmb\Psi \nonumber \\
	&= \int_{\pmb\Psi} p(\textbf{W}|\pmb\Psi,\textbf{Z})p(\pmb\Psi|\textbf{Z})d\pmb\Psi \nonumber \\
	&= \int_{\pmb\Psi} p(\textbf{W}|\pmb\Psi,\textbf{Z})p(\pmb\Psi)d\pmb\Psi
\end{align}
The above takes the form of Dirichlet-multinomial distribution which enters in the conditional posterior distribution of $\textbf{Z}$ below. The conditional posterior distribution of \textbf{Z} for $ip$th paragraph is 
{ \small
\begin{align}   
    p(z_{ip}^k=1|\mathbf{Z}_{-ip},\pmb\eta,\mathbf{W},\mathbf{D}^*) &\propto p(z_{ip}^k=1|\pmb\eta_i)p(\mathbf{W}_{ip}|z_{ip}^k=1,\mathbf{Z}_{-ip},\mathbf{W}_{-ip}) \prod_{j=1}^{i-1}p(D_{ipj}^*|z_{ip}^k=1,\textbf{Z}_{-ip},\pmb\tau,\pmb\eta,\kappa) \nonumber \\
    &\propto \pi_{ipj,k}
\end{align}
}
where 
{ \small
\begin{align}
	\pi_{ipj,k} = \text{exp}\Bigg\{  \eta_{ik} + \text{log}\prod_v \Gamma(\beta_v + c_{k,ip}^v + c_{k,-ip}^v) - \text{log}\Gamma(\sum_v \beta_v + c_{k,ip}^v + c_{k,-ip}^v) \nonumber \\
	- \frac{1}{2}\Big(\tau_2^2\eta_{jk}^2 + 2\big(\tau_0\tau_2 + \tau_1\tau_2\kappa_j^{(i)} - \tau_2 D_{ipj}^* \big)\eta_{jk} \Big) \Bigg\}
\end{align}
}

We use $\textbf{Z}_{-ip}$ and $\textbf{W}_{-ip}$ to denote the set of all topic assignments and words except for the $ip$th paragraph, respectively.
Here, $c_{k,ip}^v$ denotes the total number of times the $v$th word appears in paragraph $ip$ of topic $k$ such that $c_{k,ip}^v = \sum_{l=1}^{n_{ip}}\mathbb{I}(W_{ipl}=v)\mathbb{I}(z_{ip}^k=1)$. Likewise, 
$c_{k,-ip}^v$ is the total number of times the $v$th term appears in paragraphs with $k$th topic except for $ip$. 
The form of the conditional posterior for the $ip$th paragraph-level topic $z_{ip}^k$ offers a convenient interpretation on the \textit{source of information}. The first part $p(z_{ip}^k=1|\pmb\eta_i)$ displays the topic information from document-level topic prevalence. The second part represents topic information from the words in $ip$th paragraph. The third part $\prod_{j=1}^{i-1}p(D_{ipj}^*|z_{ip}^k=1,\textbf{Z}_{-ip},\pmb\tau,\pmb\eta,\kappa)$ is equivalent to the total amount of topic information from citations. 

The conditional posterior distribution of $\pmb\eta$ for $i$th document is jointly defined with the augmenting Polya-Gamma distribution for $\pmb\lambda$. The conditional posterior distribution for $\lambda_{ik}$ is
\begin{align}
    p(\lambda_{ik}|\mathbf{Z},\mathbf{W},\pmb\eta) &\propto PG(N_i,\rho_{ik})
\end{align}
\noindent where $\rho_{ik} = \eta_{ik} - \text{log}(\sum_{l \neq k} e^{\eta_{il}})$.

With $\lambda_{ik}$, we can obtain the conditional posterior of $\pmb\eta$ for $i$th document as follows.
\begin{align}
    p(\eta_{ik}|\eta_{i,-k},\mathbf{Z},\mathbf{W},\mathbf{D},\pmb\tau,\lambda_{ik}) &\propto \mathcal{N}(\eta_{ik}|\tilde{\mu}_{ik},\tilde{\sigma}_k^2)
\end{align}
where
\begin{align}
    \tilde{\sigma}_k^2 &= (\sigma_k^{-2} + \lambda_{ik} + v_{i,kk}^{-1})^{-1} \nonumber \\
    \tilde{\mu}_{ik} &= \tilde{\sigma}_k^2 \big( v_{i,kk}^{-1}m_{ik} + \sigma_k^{-2}\nu_{ik} + t_{ik} - \frac{N_i}{2} + \lambda_{ik}\text{log}(\sum_{l\neq k}e^{\eta_{il}}) \big)
\end{align}
For the definition of $v_{i,kk}$, $m_{ik}$, $\nu_{ik}$, and $t_{ik}$ as well as the detailed derivation, see Supplementary Information, Section~\ref{sec:si_inference}.

The conditional posterior for latent citation propensity parameter $\textbf{D}^*$ is 
{ \footnotesize
\begin{align}
    p(D^*_{ipj}|\pmb\eta,\mathbf{Z},\pmb\tau, \mathbf{D}) &\propto \begin{cases} TN_{[0,\infty)}(\tau_0+\tau_1\kappa_j^{(i)}+\tau_2\eta_{j,z_{ip}},1) & \text{ if } D_{ipj} = 1 \\
    TN_{(-\infty,0]}(\tau_0+\tau_1\kappa_j^{(i)}+\tau_2\eta_{j,z_{ip}},1) & \text{ if } D_{ipj} = 0
    \end{cases}
\end{align}
}
\noindent where $TN_{[a,b)}(\mu,\sigma^2)$ denotes the truncated normal distribution with mean $\mu$ and variance $\sigma^2$ truncated to the interval $[a,b)$.
The conditional posterior for $\pmb\tau$ follows the following distribution. Let $\mathbf{x}_{ipj} = [1,\kappa_j^{(i)},\eta_{j,z_{ip}}]^T$ and $\pmb\tau = [\tau_0,\tau_1,\tau_2]^T$
\begin{align}
    p(\pmb\tau|\pmb\eta,\mathbf{Z},\mathbf{D}^*) &\propto exp\Bigg\{-\frac{1}{2} \sum_{ipj}\Big(D_{ipj}^* - \mathbf{x}_{ipj}^T\pmb\tau \Big)^2 \Bigg\} N(\pmb\mu_{\pmb\tau},\pmb\Sigma_{\pmb\tau})\nonumber \\
    &\propto N(\tilde{\pmb\tau},\tilde{\pmb\Sigma_{\pmb\tau}})
\end{align}
\noindent where $\tilde{\pmb\Sigma_{\pmb\tau}} = \Bigg(\Big(\sum_{ipj}\mathbf{x}_{ipj}\mathbf{x}_{ipj}^T \Big) + \pmb\Sigma_{\tau}^{-1} \Bigg)^{-1}$ 
and $\tilde{\pmb\tau} = \tilde{\pmb\Sigma_{\pmb\tau}}\Bigg(\Big(\sum_{ipj}\mathbf{x}_{ipj}^TD_{ipj}^*\Big) + \pmb\Sigma_{\tau}^{-1}\pmb\mu_{\pmb\tau}\Bigg)$

Using simulation data, we confirm that the proposed Gibbs sampler recovers the true latent topics from random initialization.
Our discussion about the initialization of the Gibbs sampler is presented in Supplementary Information, Section~\ref{sec:init}, and the results of the simulation studies are presented in Supplementary Information, Section~\ref{sec:simulation}.

