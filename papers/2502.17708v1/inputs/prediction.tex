\section{Posterior Predictive Distributions of PCTM}

$\mathbf{W}_{iq}$ is a vector of length $n_{iq}$, the number of words in paragraph $q$ of document $i$, and its $l$ th element, $W_{iql}$, is the $l$ the word in paragraph $p$ of document $i$.
$\mathbf{D}_{iq}$ is a vector of length $i-1$, the number of all possible documents to be cited, and its $j$ th element, $D_{iqj}$ is 1 if paragraph $p$ of document $i$ cited document $j$; 0 otherwise.
We use $\mathbf{W}_{iq}$ and $\mathbf{D}_{iq}$ as the test data to be predicted, and all words and citations in other paragraph than $q$ in document $i$ as well as all data in previous documents as the training data.
Thus, $\mathbf{W}^{train}$ be the set of $\mathbf{W}_{ir, r \neq q}$ and $\mathbf{W}_{jp}$ for all $j < i$ and $p \in \{1 , \ldots , n_j\}$ where $n_j$ is the number of paragraphs in document $j$.
Likewise, $\mathbf{D}^{train}$ be the set of $\mathbf{D}_{ir, r \neq q}$ and $\mathbf{D}_{jp}$ for all $j < i$ and $p \in \{1 , \ldots , n_j\}$.

\begin{equation}
\begin{split}
  &p(\mathbf{W}_{iq}, \mathbf{D}_{iq} \vert \mathbf{W}^{train}, \mathbf{D}^{train}) \\
  &\propto \int_{\eta, \Psi, \tau} \sum_{\mathbf{Z}} p(\mathbf{W}_{iq}, \mathbf{D}_{iq} \vert \mathbf{Z}, \eta, \Psi, \tau) 
  \times p(\mathbf{Z}, \eta, \Psi, \tau, \vert \mathbf{W}^{train}, \mathbf{D}^{train}) d\eta d\Psi d\tau \\
  &\propto \int_{\eta, \Psi, \tau} \sum_{\mathbf{Z}} p(\mathbf{W}_{iq}, \mathbf{D}_{iq} \vert \mathbf{Z}, \eta, \Psi, \tau) 
  \times p(\mathbf{Z} \vert \eta, \Psi, \tau, \mathbf{W}^{train}, \mathbf{D}^{train}) p( \eta, \Psi, \tau \vert \mathbf{W}^{train}, \mathbf{D}^{train}) d\eta d\Psi d\tau \\
  &\approx \sum_{Z_{iq}} p(\mathbf{W}_{iq}, \mathbf{D}_{iq} \vert Z_{iq}, \hat{\mathbf{Z}}^{train}, \hat{\eta}, \hat{\Psi}, \hat{\tau}) \times p(Z_{iq} \vert \hat{\eta})  \\
  &= \sum_{k=1}^K \Big\{ p(\mathbf{W}_{iq}, \mathbf{D}_{iq} \vert Z_{iq} = k, \hat{\mathbf{Z}}^{train}, \hat{\eta}, \hat{\Psi}, \hat{\tau}) \times p(Z_{iq} = k \vert \hat{\eta}) \Big\} \\
  &= \sum_{k=1}^K \Big\{ p(\mathbf{W}_{iq} \vert Z_{iq} = k, \hat{\Psi})
  \times \prod_{j=1}^{i-1} p(D_{iqj} \vert \hat{\tau}, \hat{\eta}, Z_{ip})
  \times p(Z_{iq} = k \vert \hat{\eta}) \Big\} \\
  &= \sum_{k=1}^K \Big\{ p(\mathbf{W}_{iq} \vert Z_{iq} = k, \hat{\Psi})
  \times \prod_{j=1}^{i-1} \mathbb{P}(D_{iqj}^* > 0 \vert \hat{\tau}, \hat{\eta}, Z_{ip} = k)^{\mathbb{I}\{D_{iqj}=1\}}\mathbb{P}(D_{iqj}^* < 0 \vert \hat{\tau}, \hat{\eta}, Z_{ip} = k)^{\mathbb{I}\{D_{iqj}=0\}} \\
  &\quad \times p(Z_{iq} = k \vert \hat{\eta}) \Big\} \\
  &\propto \sum_{k=1}^K \Bigg\{ \prod_{v=1}^V \Psi_{vk}^{W_{iqv}} 
  \times \prod_{j=1}^{i-1} \Big[\int_{t=0}^{\infty} p(D_{iqj}^* = t | \tau_0 + \tau_1 \kappa_j^{(i)} + \tau_2\eta_{jk}) dt\Big]^{\mathbb{I}\{D_{iqj}=1\}} \\
  &\quad \times \Big[\int_{t=-\infty}^{0} p(D_{iqj}^* = t | \tau_0 + \tau_1 \kappa_j^{(i)} + \tau_2\eta_{jk}) dt \Big]^{\mathbb{I}\{D_{iqj}=0\}} 
  \times \frac{\exp(\eta_{ik})}{\sum_{k'=1}^K \exp(\eta_{ik'})} \Bigg\} \\
\end{split}
\end{equation}

The approximation part can use Monte Carlo simulation - We draw parameters $\eta, \tau$ from the posterior distributions and take the average.
Alternatively, we could use the point estimate of $\eta, \tau$ to match the computation in RTM and LDA+logistic.
