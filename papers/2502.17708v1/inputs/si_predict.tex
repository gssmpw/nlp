\section{Comparison of the Predictive Performance against Existing Methods}
\label{sec:predict_prob}

In this section, we compare the predictive performance of the PCTM against two alternative models for document networks: the RTM and the LDA combined with Logistic Regression (LDA + Logistics).
In both alternative models, citations arise as a function of topic similarity at the word level. 
We use documents in the Privacy subset for this exercise. 
We choose paragraphs in \texttt{Gonzales v. Carhart} as our test set because \texttt{Gonzales v. Carhart} contains a sufficiently large number of citations and words to demonstrate how they contribute to the predictive performance.\footnote{\texttt{Gonzales v. Carhart} contains 12 citations, which is about 94 percentile of the distribution of the number of citations per document. It is the 9th latest document in our corpus.} 
We discard documents temporally later than \texttt{Gonzales v. Carhart}. 

Our exercise is essentially a leave-one-out cross-validation for each paragraph in \texttt{Gonzales v. Carhart}. 
Specifically, we take a paragraph in \texttt{Gonzales v. Carhart} as test data, and all other paragraphs in \texttt{Gonzales v. Carhart} and documents prior to it are assigned to the training data. 
Then we compute the predictive probability on the paragraph in the test set given our parameters fit on the training data. 
Note that due to the structure of this exercise, \texttt{Gonzales v. Carhart} will appear in both the training set and the test set. % maybe I don't need to say this
The above exercise is repeated for all 88 paragraphs in \texttt{Gonzales v. Carhart}. 

One challenge in this exercise is that the PCTM assigns topics to each paragraph while the RTM and the LDA assign topics to each word. 
That is, the RTM and the LDA do not recognize paragraphs in the data. 
Therefore, we treat the paragraph in the test data as if it is a new version of \texttt{Gonzales v. Carhart} when we compute the predictive probability in the RTM and the LDA.
In other words, we estimate topics of words in the test data from the topic probability for \texttt{Gonzales v. Carhart}. 

A formal description of the prediction exercise is as follows.
$\mathbf{W}_{iq}$ and $\mathbf{D}_{iq}$ are the data in a paragraph $q$ of a document $i$.
This corresponds to the test paragraph.
$\mathbf{W}^{train}, \mathbf{D}^{train}$ are the data in the training set.
This includes all paragraphs other than the paragraph $q$ of the document $i$ as well as all documents prior to the document $i$.
The parameters with $\hat{\cdot}$ symbol indicate that they are estimates based on the training data.
The following gives the posterior predictive probability for the PCTM.

\begin{equation}
\begin{split}
  &p(\mathbf{W}_{iq}, \mathbf{D}_{iq} \vert \mathbf{W}^{train}, \mathbf{D}^{train}) \\
  &= \sum_{k=1}^K \Big\{ p(\mathbf{W}_{iq} \vert z_{iq} = k, \hat{\boldsymbol \Psi})\\
  &\quad \times \prod_{j=1}^{i-1} \mathbb{P}(D_{iqj}^* > 0 \vert \hat{\boldsymbol \tau}, \hat{\boldsymbol \eta}, z_{iq} = k)^{\mathbb{I}\{D_{iqj}=1\}}\mathbb{P}(D_{iqj}^* < 0 \vert \hat{\boldsymbol \tau}, \hat{\boldsymbol \eta}, z_{iq} = k)^{\mathbb{I}\{D_{iqj}=0\}} \\
  &\quad \times p(z_{iq} = k \vert \hat{\boldsymbol \eta_i}) \Big\}
\end{split}
\end{equation}

By contrast, the following gives the posterior predictive probability for the RTM and LDA. 
We follow \cite{chang2009relational} for the notation of parameters.
$\boldsymbol\theta$ is a $N \times K$ document-topic matrix.
$\boldsymbol\eta$ is a $K$-length vector of coefficient and $\nu$ is intercept in the regression of citation on the topic.

\begin{equation}
\begin{split}
  &p(\mathbf{W}_{iq}, \mathbf{D}_{iq} \vert \mathbf{W}^{train}, \mathbf{D}^{train}) \\
  &= \sum_{\mathbf{z}} \Big\{ p(\mathbf{W}_{i} \vert \mathbf{Z}_{iq} = \mathbf{z}, \hat{\boldsymbol \Psi})\\
  &\quad \times \prod_{j=1}^{i-1}
  \left[\psi\left(\hat{\boldsymbol\eta}(\bar{\mathbf{Z}_{iq}} \circ \bar{\hat{\mathbf{Z}_j}}) + \hat{\nu}\right)\right]^{\mathbb{I}\{D_{iqj} = 1\}}
  \left[1-\psi\left(\hat{\boldsymbol\eta}(\bar{\mathbf{Z}_{iq}} \circ \bar{\hat{\mathbf{Z}_j}}) + \hat{\nu}\right)\right]^{\mathbb{I}\{D_{iqj} = 0\}} \\
  &\quad \times p(\mathbf{Z_{iq}} = \mathbf{z} \vert \hat{\boldsymbol \theta_i}) \Big\}
\end{split}
\end{equation}

Note that $\mathbf{Z}_{iq}$ is a vector with its length equal to the number of words in the test paragraph.
Since it is infeasible to compute all possible values of $\mathbf{Z}_{iq}$, we use Monte Carlo simulation to approximate its distribution. 
For LDA+Logistic model, the parameters are estimated by fitting LDA on the training data and then regressing the citation on the topics.

The results are displayed in Figure~\ref{pred_privacy_realdata}.
Each symbol represents the difference in the log posterior probability between models for each paragraph. 
The left panel compares the PCTM with the RTM and the right panel compares it with the LDA+Logistic regression.
Solid symbols denote the differences in the predictive probabilities for paragraphs without citations and hollow symbols are for ones with citations.
The main takeaway is that the PCTM almost always outperforms the other two models.
In particular, the improvement in predictive probability becomes greater when the prediction is made on paragraphs with more words. 
One explanation for this is that the PCTM suffers less from overfitting than the RTM or the LDA does with respect to predictions.
Since the RTM and the LDA assign topic parameters to each word, the model complexity for both models increases exponentially as more words are included in the document.
For the PCTM, on the other hand, increasing the number of words in paragraphs does not significantly impact the model complexity because the topic parameter is for paragraphs, not words. 

\begin{figure}
  \includegraphics[width=\textwidth]{figs/predict_privacy98_diff_word_cite.pdf}
  \caption{Difference in Predicted Probability with PCTM, RTM, and LDA + Logistic Regression. 
  The x-axis is the number of words per paragraph.
  The y-axis is the difference in the log posterior probability between PCTM and other models.
  The compared models are RTM for the left panel and LDA + Logistic regression for the right panel.
  Each symbol represents the difference in the log posterior probability between models for each paragraph. 
  Solid symbols are paragraphs without citations and hollow symbols are with citations.
  The prediction was performed by first fitting the models on a subset of the corpus temporally prior to the test paragraph, and then computing the predictive probability of the test paragraph as if the test paragraph is a new paragraph in the last document of the training corpus.  
  R package \texttt{lda} was used to fit the RTM and the LDA.
  Overall, the PCTM achieves higher posterior predictive probability compared to the RTM and the LDA + Logistic Regression models, particularly when a paragraph contains many words. 
  }
  \label{pred_privacy_realdata} 
\end{figure}

