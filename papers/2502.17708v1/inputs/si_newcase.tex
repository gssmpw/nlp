\section{Posterior Predictive Probability}
\label{sec:predictive}

The posterior probability of words and citations in a paragraph $p$ in a document $i$ can be computed by the following formula.
\begin{equation}
\begin{split}
  &p(\mathbf{W}_{ip}, \mathbf{D}_{ip} \vert \mathbf{W}^{train}, \mathbf{D}^{train}) \\
  &\propto \int_{\boldeta, \bPsi, \btau} \sum_{\mathbf{Z}} p(\mathbf{W}_{ip}, \mathbf{D}_{ip} \vert \mathbf{Z}, \boldeta, \bPsi, \btau) 
  \times p(\mathbf{Z}, \boldeta, \bPsi, \btau, \vert \mathbf{W}^{train}, \mathbf{D}^{train}) d\boldeta d\bPsi d\btau \\
  &\propto \int_{\boldeta, \bPsi, \btau} \sum_{\mathbf{Z}} p(\mathbf{W}_{ip}, \mathbf{D}_{ip} \vert \mathbf{Z}, \boldeta, \bPsi, \btau) 
  \times p(\mathbf{Z} \vert \boldeta, \bPsi, \btau, \mathbf{W}^{train}, \mathbf{D}^{train}) p( \boldeta, \bPsi, \btau \vert \mathbf{W}^{train}, \mathbf{D}^{train}) d\boldeta d\bPsi d\btau \\
  %&\approx \sum_{z_{ip}} p(\mathbf{W}_{ip}, \mathbf{D}_{ip} \vert z_{ip}, \hat{\mathbf{Z}}^{train}, \hat{\boldeta}, \hat{\bPsi}, \hat{\btau}) \times p(z_{ip} \vert \hat{\boldeta})  \\
  &\approx \sum_{k=1}^K \Big\{ p(\mathbf{W}_{ip}, \mathbf{D}_{ip} \vert z_{ip}^k = 1, \hat{\mathbf{Z}}^{train}, \hat{\boldeta}, \hat{\bPsi}, \hat{\btau}) \times \mathbb{P}(z_{ip}^k = 1 \vert \hat{\boldeta}) \Big\} \\
  &= \sum_{k=1}^K \Big\{ p(\mathbf{W}_{ip} \vert z_{ip}^k = 1, \hat{\bPsi})
  \times \prod_{j=1}^{i-1} p(D_{ipj} \vert \hat{\btau}, \hat{\boldeta}, z_{ip}^k = 1)
  \times \mathbb{P}(z_{ip}^k = 1 \vert \hat{\boldeta}) \Big\} \\
  &= \sum_{k=1}^K \Big\{ p(\mathbf{W}_{ip} \vert z_{ip}^k = 1, \hat{\bPsi})
  \times \prod_{j=1}^{i-1} p(D_{ipj}^* > 0 \vert \hat{\btau}, \hat{\boldeta}, z_{ip}^k = 1)^{\mathbb{I}\{D_{ipj}=1\}} p(D_{ipj}^* < 0 \vert \hat{\btau}, \hat{\boldeta}, z_{ip}^k = 1)^{\mathbb{I}\{D_{ipj}=0\}} \\
  &\quad \times \mathbb{P}(z_{ip}^k = 1 \vert \hat{\boldeta}) \Big\} \\
  &\propto \sum_{k=1}^K \Bigg\{ \prod_{v=1}^V \bPsi_{vk}^{W_{ipv}} 
  \times \prod_{j=1}^{i-1} \Big[\int_{t=0}^{\infty} p(D_{ipj}^* = t | \btau_0 + \btau_1 \kappa_j^{(i)} + \btau_2\boldeta_{jk}) dt\Big]^{\mathbb{I}\{D_{ipj}=1\}} \\
  &\quad \times \Big[\int_{t=-\infty}^{0} p(D_{ipj}^* = t | \btau_0 + \btau_1 \kappa_j^{(i)} + \btau_2\boldeta_{jk}) dt \Big]^{\mathbb{I}\{D_{ipj}=0\}} 
  \times \frac{\exp(\boldeta_{ik})}{\sum_{k'=1}^K \exp(\boldeta_{ik'})} \Bigg\} \\
\end{split}
\end{equation}
In the third line, we approximate the integral over $\boldeta$, $\bPsi$, and $\btau$ as well as the summation over $\mathbf{Z}$ in the training data.
We draw samples of these parameters from the posterior of the model fit on the training data for $\boldeta$, $\btau$, and $\mathbf{Z}$ in the training data, and we use an MLE estimate for $\bPsi$ (see Appendix~\ref{subsec:Psi}). 
The integrals in the last line can be easily computed because $D_{ipj}^*$ follows normal distributions with unit variance. 
We can also see that the posterior probability of a paragraph $p$ in a document $i$ being topic $k$ is proportional to the components inside the summation over $k$.


