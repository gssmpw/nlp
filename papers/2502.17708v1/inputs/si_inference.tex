\section{Model inference: collapsed Gibbs sampler}
\label{sec:si_inference}
This section describes the details of the collapsed Gibbs sampler for the proposed model. 
Our model is as follows.

\begin{align}
  \begin{split}
	D_{ipj} &= \begin{cases}
		1 \text{ if } D_{ipj}^* \geq 0 \\
		0 \text { if } D_{ipj}^* < 0
	\end{cases}\\
    D_{ipj}^* &\sim \mathcal{N}(\pmb\tau^T\textbf{x}_{ipj},1)\quad \text{where}\ \textbf{x}_{ipj} = [1, \kappa_{j}^{(i)}, \eta_{j,z_{ip}}]\\
	\textbf{w}_{ip} &\sim \text{Multinomial}(N_{ip},\pmb\Psi_{z_{ip}})  \\
	z_{ip} &\sim \text{Multinomial}(1,\text{softmax}(\pmb\eta_i))  \\
	\pmb\Psi_k &\sim \text{Dirichlet}(\pmb\beta)  \\
	\pmb\eta_i &\sim \mathcal{N}(\pmb\mu,\pmb\Sigma)  \\
	\pmb\mu &\sim \mathcal{N}(\pmb\mu_0, \pmb\Sigma_0)  \\
	\pmb\tau &\sim \mathcal{N}(\pmb\mu_{\tau},\pmb\Sigma_{\tau})
  \end{split}
\end{align}


The full posterior is denoted as follows.
{\small
\begin{align}
	p(\pmb\eta,\pmb\Psi,\textbf{Z},\pmb\tau|\textbf{W},\textbf{D}) \propto p(\pmb\mu|\pmb\mu_0,\pmb\Sigma_0)p(\pmb\tau|\pmb\mu_{\tau},\pmb\Sigma_{\tau})p(\pmb\eta|\pmb\mu,\pmb\Sigma)p(\pmb\Psi|\pmb\beta)p(\textbf{Z}|\pmb\eta)p(\textbf{W}|\pmb\Psi,\textbf{Z})p(\textbf{D}|\textbf{D}^*)p(\textbf{D}^*|\pmb\tau,\pmb\eta,\textbf{Z},\textbf{D})
\end{align}
}

Unfortunately, the inference of the given posterior distribution is hard due to the non-conjugacy between normal prior for $\pmb\eta$ and the logistic transformation function \citep{blei2007correlated}. Variational inference is the most frequently employed tool to address this problem, with an additional advantage of computational speed. However, obtained parameters are for the variational distribution which is an approximation to the target posterior. Moreover, the quality of the approximation is often not sufficiently explored (Add citations here). 

To remedy this problem, we follow the recent advances in the inference of CTM models \citep{held2006bayesian,chen2013scalable,linderman2015dependent}. We first partially collapse the posterior distribution by integrating out $\pmb\Psi$. Then we introduce an auxiliary Polya-Gamma variable $\pmb\lambda$ and augment the collapsed posterior. Partial collapsing and data augmentation enables us to use Gibbs sampling which is known to produce samples that converge to the exact posterior. 

With $\pmb\Psi$ integrated out, our new posterior is proportional to
{\small
\begin{align}
	\int_{\pmb\Psi}p(\pmb\eta,\pmb\Psi,\textbf{Z},\pmb\tau|\textbf{W},\textbf{D}) \propto p(\pmb\mu|\pmb\mu_0,\pmb\Sigma_0)p(\pmb\tau|\pmb\mu_{\tau},\pmb\Sigma_{\tau})p(\pmb\eta|\pmb\mu,\pmb\Sigma)p(\textbf{Z}|\pmb\eta)p(\textbf{W}|\textbf{Z})p(\textbf{D}|\textbf{D}^*)p(\textbf{D}^*|\pmb\tau,\pmb\eta,\textbf{Z},\textbf{D})
\end{align}
}
% derivation of Z conditional
\subsection{Derivation of the conditional distribution for \textbf{Z}}
For $ip$th paragraph, the conditional distribution of $z_{ip}$ is
{\small
\begin{align}   
    p(z_{ip}^k=1|\mathbf{Z}_{-ip},\pmb\eta,\mathbf{W},\mathbf{D}^*) &\propto p(z_{ip}^k=1|\pmb\eta_i)p(\mathbf{W}_{ip}|z_{ip}^k=1,\mathbf{Z}_{-ip},\mathbf{W}_{-ip}) \prod_{j=1}^{i-1}p(D_{ipj}^*|z_{ip}^k=1,\textbf{Z}_{-ip},\pmb\tau,\pmb\eta,\kappa)
\end{align}
}
The first term is $\frac{\text{e}^{\eta_{ik}}}{\sum_{l}\text{e}^{\eta_{il}}}$ which is proportional to $\text{e}^{\eta_{ik}}$.

The form of second term warrants further elaboration. Integrating out $\pmb\Psi$ as
\begin{align}
	p(\textbf{W}|\textbf{Z}) &= \int_{\pmb\Psi} p(\textbf{W},\pmb\Psi|\textbf{Z})d\pmb\Psi \nonumber \\
	&= \int_{\pmb\Psi} p(\textbf{W}|\pmb\Psi,\textbf{Z})p(\pmb\Psi|\textbf{Z})d\pmb\Psi \nonumber \\
	&= \int_{\pmb\Psi} p(\textbf{W}|\pmb\Psi,\textbf{Z})p(\pmb\Psi)d\pmb\Psi
\end{align}
for $ip$th paragraph with $k$th topic yields the following.
\begin{align}
    p(\mathbf{W}_{ip}|z_{ip}^k=1,\mathbf{Z}_{-ip},\mathbf{W}_{-ip}) &\propto \int_{\pmb{\Psi}_k}\Psi_{k1}^{\beta_1-1}\Psi_{k2}^{\beta_2-1} ... \Psi_{kV}^{\beta_V-1} \prod_v \Psi_{kv}^{\sum_{l=1}^{n_{ip}} \mathbb{I}(W_{ipl}=v)} \nonumber \\
    & \times \prod_v \prod_{(i^{'},p^{'}) \neq (i,p)} \Psi_{kv}^{\sum_{l=1}^{n_{i^{'}p^{'}}} \mathbb{I}(W_{i^{'}p^{'}l}=v)\mathbb{I}(z_{i^{'}p^{'}}^k=1)} d\pmb{\Psi}_k
\end{align}

Here, $N_{ip}$ denotes the total number of words in $ip$th paragraph, and $n_{ip}$ denotes the total number of unique words in $ip$th paragraph. Let $C_k^v = \sum_{i=1}^N\sum_{p=1}^{N_{ip}}\sum_{l=1}^{n_{ip}}\mathbb{I}(W_{ipl}=v)\mathbb{I}(z_{ip}^k=1)$, and $c_{k,ip}^v = \sum_{l=1}^{n_{ip}}\mathbb{I}(W_{ipl}=v)\mathbb{I}(z_{ip}^k=1)$ then the above can be simplified as
\begin{align}
    p(\mathbf{W}_{ip}|z_{ip}^k=1,\mathbf{Z}_{-ip},\mathbf{W}_{-ip}) &\propto \int_{\pmb{\Psi}_k}\Psi_{k1}^{\beta_1+c_{k,ip}^1+ c_{k,-ip}^1-1}\Psi_{k2}^{\beta_2+c_{k,ip}^2+ c_{k,-ip}^2-1} ... \Psi_{kV}^{\beta_V+c_{k,ip}^V+ c_{k,-ip}^V-1} d\pmb{\Psi}_k \nonumber \\
    &= \frac{\prod_v \Gamma(\beta_v + c_{k,ip}^v + c_{k,-ip}^v)}{\Gamma(\sum_v \beta_v + c_{k,ip}^v + c_{k,-ip}^v)}
\end{align}

Imagine a paragraph of 3 words $\mathbf{W}_{ip} = \{1,1,3\}$, two of the first word and one of the third word. Then
\begin{align}
    p(\mathbf{W}_{ip}|z_{ip}^k=1,\mathbf{Z}_{-ip},\mathbf{W}_{-ip}) &\propto \frac{\prod_v \Gamma(\beta_v + c_{k,ip}^v + c_{k,-ip}^v)}{\Gamma(\sum_v \beta_v + c_{k,ip}^v + c_{k,-ip}^v)}
\end{align}

The numerator is 
\begin{align}
    \Gamma(\beta_1 + 2 + c_{k,-ip}^1)\Gamma(\beta_3 + 1 + c_{k,-ip}^3) \times \prod_{v \neq (1,3)}\Gamma(\beta_v + c_{k,-ip}^v) \nonumber \\
    = (\beta_1 + 1 + c_{k,-ip}^1)(\beta_1 + c_{k,-ip}^1)(\beta_3 + c_{k,-ip}^3) \times \prod_v \Gamma(\beta_v + c_{k,-ip}^v)
\end{align}

In the same sense, the denominator is
{\small
\begin{align}
    \Gamma(3 + \sum_v \beta_v + c_{k,-ip}^v) = (2+\sum_v \beta_v + c_{k,-ip}^v)(1+\sum_v \beta_v + c_{k,-ip}^v)(\sum_v \beta_v + c_{k,-ip}^v)\Gamma(\sum_v \beta_v + c_{k,-ip}^v)
\end{align}
}

Rearrange the above and we have
\begin{align}
    \frac{(\beta_1 + 1 + c_{k,-ip}^1)(\beta_1 + c_{k,-ip}^1)(\beta_3 + c_{k,-ip}^3)}{(2+\sum_v \beta_v + c_{k,-ip}^v)(1+\sum_v \beta_v + c_{k,-ip}^v)(\sum_v \beta_v + c_{k,-ip}^v)} \times \frac{\prod_v \Gamma(\beta_v + c_{k,-ip}^v)}{\Gamma(\sum_v \beta_v + c_{k,-ip}^v)}
\end{align}

The second term does not depend on $z_{ip}^k$. Then for $\textbf{W}_{ip}=\{1,1,3\}$, we have 
\begin{align}
    p(\mathbf{W}_{ip}|z_{ip}^k=1,\mathbf{Z}_{-ip},\mathbf{W}_{-ip}) \propto \frac{(\beta_1 + 1 + c_{k,-ip}^1)(\beta_1 + c_{k,-ip}^1)(\beta_3 + c_{k,-ip}^3)}{(2+\sum_v \beta_v + c_{k,-ip}^v)(1+\sum_v \beta_v + c_{k,-ip}^v)(\sum_v \beta_v + c_{k,-ip}^v)}
\end{align}

If a paragraph consists of only one word such that $W_{ip}=l$, the above changes to 
\begin{align}
    p(\mathbf{W}_{ip}|z_{ip}^k=1,\mathbf{Z}_{-ip},\mathbf{W}_{-ip}) \propto \frac{\beta_l + c_{k,-ip}^l}{\sum_v \beta_v + c_{k,-ip}^v}
\end{align}
which matches with the form for the equivalent part in collapsed Gibbs for LDA \citep{porteous2008fast,xiao2010efficient,asuncion2012smoothing}.

The third term $p(D_{ipj}^*|z_{ip}^k=1,\textbf{Z}_{-ip},\pmb\tau,\pmb\eta,\pmb\kappa) = \text{exp}\{ -\frac{1}{2} \big(D_{ipj}^* - (\tau_0 + \tau_1\kappa_j^{(i)} + \tau_2\eta_{j,z_{ip}}) \big)^2 \}$ is proportional to 
\begin{align}
\text{exp}\Bigg\{-\frac{1}{2}\Big(\tau_2^2\eta_{jk}^2 + 2\big(\tau_0\tau_2 + \tau_1\tau_2\kappa_j^{(i)} - \tau_2 D_{ipj}^* \big)\eta_{jk} \Big) \Bigg\}
\end{align}


% derivation of eta conditional
\subsection{Derivation of the conditional distribution for $\pmb\eta$}
\begin{align}
    p(\pmb{\eta}|\textbf{Z},\textbf{W},\textbf{D}) &= \prod_{i=1}^N \Big( \prod_{p=1}^{N_i} p(z_{ip}|\pmb\eta_i) \Big) \mathcal{N}(\pmb{\eta}_i|\pmb\mu,\pmb\Sigma) \prod_{p=1}^{N_i}\prod_{j=1}^{i-1}p(D^*_{ipj}|\kappa,\pmb\eta_i,\mathbf{Z})\nonumber \\
    &= \prod_{i=1}^N \Big(\prod_{p=1}^{N_i}\frac{e^{\eta_{i,z_{ip}}}}{\sum_{j=1}^K e^{\eta_{ij}}} \Big) \mathcal{N}(\pmb{\eta}_i|\pmb\mu,\pmb\Sigma) \prod_{p=1}^{N_i}\prod_{j=1}^{i-1}p(D^*_{ipj}|\kappa,\pmb\eta_i,\mathbf{Z})
\end{align}

Following \cite{held2006bayesian}, the likelihood for $\eta_{ik}$ conditioned on $\eta_{i,-k}$ is
\begin{align}
    \ell(\eta_{ik}|\eta_{i,-k}) &= \prod_{p=1}^{N_i} \Big(\frac{e^{\rho_{ik}}}{1+e^{\rho_{ik}}} \Big)^{z_{ip,k}} \Big(\frac{1}{1+e^{\rho_{ik}}} \Big)^{1-z_{ip,k}} \nonumber \\
    &= \frac{(e^{\rho_{ik}})^{t_{ik}}}{(1+e^{\rho_{ik}})^{N_i}}
\end{align}
where $\rho_{ik} = \eta_{ik} - \text{log}(\sum_{l \neq k} e^{\eta_{il}})$ and $t_{ik} = \sum_{p=1}^{N_i}\mathbb{I}(z_{ip}=k)$.

Then
\begin{align}
    p(\eta_{ik}|\eta_{i,-k},\mathbf{Z},\mathbf{W},\mathbf{D},\pmb\tau) \propto \ell(\eta_{ik}|\eta_{i,-k})\mathcal{N}(\eta_{ik}|\nu_{ik},\sigma_k^2)p(D^*|\pmb\eta,\pmb\tau,\mathbf{Z})
\end{align}

where
\begin{align}
    \nu_{ik} &= \mu_k - \Lambda_{kk}^{-1}\pmb\Lambda_{k,-k}(\pmb\eta_{i,-k}-\pmb\mu_{i,-k}) \nonumber \\
    \sigma_k^2 &= \pmb\Lambda_{kk}^{-1} \nonumber \\
    \pmb\Lambda &= \pmb\Sigma^{-1}
\end{align}

The third term can be rewritten with respect to $\pmb\eta$ as 
\begin{align}
    p(\mathbf{D}^*|\pmb\eta,\pmb\tau,\mathbf{Z}) &=  \prod_i\prod_p\prod_{j=1}^{i-1}\text{exp}\Bigg\{-\frac{1}{2}\big(D_{ipj}^* - (\tau_0 + \tau_1\kappa_j^{(i)} + \tau_2\eta_{j,z_{ip}})\big)^2 \Bigg\} \nonumber \\
    &\propto \prod_i\prod_p\prod_{j=1}^{i-1}\text{exp}\Bigg\{-\frac{1}{2(1/\tau_2^2)}\Big(\eta_{j,z_{ip}}^2 - 2\frac{D_{ipj}^* - \tau_0 - \tau_1\kappa_j^{(i)}}{\tau_2}\eta_{j,z_{ip}} \Big) \Bigg\}  \nonumber \\
    &\propto \prod_i\prod_p\prod_{j=1}^{i-1} \mathcal{N}(\eta_{j,z_{ip}}|\mu_{ipj}^*,\frac{1}{\tau_2^2}) \nonumber \\
    &= \prod_i\prod_p\prod_{j=1}^{i-1}\prod_k \mathcal{N}(\eta_{jk}|\mu_{ipj}^*,\frac{1}{\tau_2^2})^{\mathbb{I}(z_{ip}=k)}
\end{align}
where $\mu_{ipj}^* = \frac{D_{ipj}^* - \tau_0 - \tau_1\kappa_j^{(i)}}{\tau_2}$.
We notice that the above can be rewritten as a product of univariate normal distributions such that
\begin{align}
    & \prod_k\prod_{s=i+1}^N\prod_{p=1}^{N_s}\mathcal{N}(\eta_{ik}|\mu_{spi}^*,{\sigma^2}^*)^{\mathbb{I}(z_{sp}=k)} \nonumber \\
    &\equiv  \prod_{k=1}^K \mathcal{N}(\eta_{ik} | m_{ik},V_{i,kk})
\end{align}

$\mathbf{V}_i$ is a diagnoal matrix with the $k$th diagonal entry of the inverse of $\mathbf{V}_i$ (or $\mathbf{V}_i^{-1})$ as
\begin{align}
    V_{i,kk}^{-1} &= \frac{1}{{\sigma^2}^*} \sum_{s=i+1}^N\sum_{p=1}^{N_s}\mathbb{I}(z_{sp} = k) \nonumber \\
    &= \tau_2^2 \sum_{s=i+1}^N\sum_{p=1}^{N_s}\mathbb{I}(z_{sp} = k)
\end{align}

The $k$th entry of $\mathbf{m}_i$ then is
\begin{align}
    m_{ik} &= \frac{\tau_2^2\sum_{s=i+1}^N\sum_{p=1}^{N_s}\mu_{spi}^*\mathbb{I}(z_{sp}=k)}{V_{i,kk}^{-1}} \nonumber \\
    &= \frac{\sum_{s}\sum_{p}\mu_{spi}^*\mathbb{I}(z_{sp}=k)}{\sum_s\sum_{p}\mathbb{I}(z_{sp}=k)}
\end{align}

Then the $\eta$ conditional is
\begin{align}
    p(\eta_{ik}|\eta_{i,-k},\mathbf{Z},\mathbf{W},\mathbf{D},\pmb\tau) \propto \ell(\eta_{ik}|\eta_{i,-k})\mathcal{N}(\eta_{ik}|\nu_{ik},\sigma_k^2)\mathcal{N}(\eta_{ik}|m_{ik},V_{i,kk})
\end{align}

We now introduce Polya-Gamma augmentation such that 
{\footnotesize
\begin{align}
    p(\eta_{ik}|\eta_{i,-k},\mathbf{Z},\mathbf{W},\mathbf{D},\pmb\tau,\lambda_{ik}) &\propto \text{exp}\{(t_{ik}-\frac{N_i}{2})\rho_{ik} - \frac{\lambda_{ik}}{2}\rho_{ik}^2 \}\mathcal{N}(\eta_{ik}|\nu_{ik},\sigma_k^2)\mathcal{N}(\eta_{ik}|m_{ik},V_{i,kk}) \nonumber \\
    &\propto \mathcal{N}(\eta_{ik}|\frac{t_{ik}-N_i/2}{\lambda_{ik}}+\text{log}(\sum_{l\neq k}e^{\eta_{il}}),1/\lambda_{ik})\mathcal{N}(\eta_{ik}|\nu_{ik},\sigma_k^2)\mathcal{N}(\eta_{ik}|m_{ik},V_{i,kk})
\end{align}
}
\normalsize
Summing all of the above, the conditional distribution of $\eta_{ik}$ is 
\begin{align}
    p(\eta_{ik}|\eta_{i,-k},\mathbf{Z},\mathbf{W},\mathbf{D},\pmb\tau,\lambda_{ik}) &\propto \mathcal{N}(\eta_{ik}|\tilde{\mu}_{ik},\tilde{\sigma}_k^2)
\end{align}
where
\begin{align}
    \tilde{\sigma}_k^2 &= (\sigma_k^{-2} + \lambda_{ik} + v_{i,kk}^{-1})^{-1} \nonumber \\
    \tilde{\mu}_{ik} &= \tilde{\sigma}_k^2 \big( v_{i,kk}^{-1}m_{ik} + \sigma_k^{-2}\nu_{ik} + t_{ik} - \frac{N_i}{2} + \lambda_{ik}\text{log}(\sum_{l\neq k}e^{\eta_{il}}) \big)
\end{align}

% derivation of lambda
\subsection*{Derivation of conditional distribution for $\pmb\lambda$}
The Gibbs sampling for the augmentation variable $\pmb\lambda$ is obtained by collecting terms that include $\pmb\lambda_i$ in the joint of $\pmb{z}_i$ and $\pmb\eta_i$.
\begin{align}
    %p(\pmb\lambda_{i}|\mathbf{Z},\mathbf{W},\pmb\eta) &\propto PG(\pmb\lambda_i|M(\pmb{c}_i),\pmb\eta_i)
    p(\lambda_{ik}|\mathbf{Z},\mathbf{W},\pmb\eta) &\propto PG(N_i,\rho_{ik})
\end{align}

% derivation of Dstar
\subsection{Derivation of conditional distribution for $\textbf{D}^*$}
{\footnotesize
\begin{align}
    p(D^*_{ipj}|\pmb\eta,\mathbf{Z},\pmb\tau, \mathbf{D}) &\propto \begin{cases} TN_{(0,\infty)}(\tau_0+\tau_1\kappa_j^{(i)}+\tau_2\eta_{j,z_{ip}},1) & \text{ if } D_{ipj} = 1 \\
    TN_{(-\infty,0]}(\tau_0+\tau_1\kappa_j^{(i)}+\tau_2\eta_{j,z_{ip}},1) & \text{ if } D_{ipj} = 0
    \end{cases}
\end{align}
}

% dervation of tau
\subsection{Derivation of conditional distribution for $\pmb\tau$}

Let $\mathbf{x}_{ipj} = [1,\kappa_j^{(i)},\eta_{j,z_{ip}}]^T$ and $\pmb\tau = [\tau_0,\tau_1,\tau_2]^T$
\begin{align}
    p(\pmb\tau|\pmb\eta,\mathbf{Z},\mathbf{D}^*) &\propto exp\Bigg\{-\frac{1}{2} \sum_{ipj}\Big(D_{ipj}^* - \mathbf{x}_{ipj}^T\pmb\tau \Big)^2 \Bigg\} N(\pmb\mu_{\pmb\tau},\Sigma_{\tau})\nonumber \\
    &\propto N(\tilde{\pmb\tau},\tilde{\Sigma_{\tau}})
\end{align}

where $\tilde{\Sigma_{\tau}} = \Bigg(\Big(\sum_{ipj}\mathbf{x}_{ipj}\mathbf{x}_{ipj}^T \Big) + \Sigma_{\tau}^{-1} \Bigg)^{-1}$ 
and $\tilde{\tau} = \tilde{\Sigma_{\tau}}\Bigg(\Big(\sum_{ipj}\mathbf{x}_{ipj}^TD_{ipj}^*\Big) + \Sigma_{\tau}^{-1}\pmb\mu_{\pmb\tau}\Bigg)$

\subsection{Recovering $\pmb\Psi$}
\label{subsec:Psi}
We estimate the integrated out parameter $\pmb\Psi$ from our posterior samples as follows.
\begin{align}
	\hat{\Psi}_{kv} =  \frac{\sum_{i}\sum_{p} \big(\beta_v + \mathbb{I}(z_{ip}^k=1)\text{W}_{ip,v} \big)}{\sum_{i}\sum_{p}\sum_l \big(\beta_l + \mathbb{I}(z_{ip}^k=1)\text{W}_{ip,l}\big)}
\end{align}
