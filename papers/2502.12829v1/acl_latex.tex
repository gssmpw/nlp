 % This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{array}
\usepackage{booktabs}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{todonotes}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage[T2A]{fontenc}  % Supports Cyrillic encoding
\usepackage[utf8]{inputenc}  % UTF-8 encoding
\usepackage[russian,english]{babel}  % Enable Kazakh and Russian support


% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}



\usepackage[T2A]{fontenc}  % Supports Cyrillic encoding
\usepackage[utf8]{inputenc}  % UTF-8 encoding
%\usepackage[russian,english]{babel}  % Enable Kazakh language support



\usepackage{tcolorbox}
\usepackage{arabtex}
\usepackage{utf8}
%\usepackage{lmodern}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\usepackage{times}
\usepackage{tabu}
\usepackage{latexsym}
\usepackage{stmaryrd}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{tablefootnote}
\usepackage{enumitem}  % More control
\usepackage{lipsum}
\usepackage{xspace}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{arydshln}
\usepackage{array}
\usepackage{cleveref}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

% \newcommand{\equalsign}{\footnotemark[1]\hspace{0.1cm}}

% if url is too long
\usepackage{hyperref}
% TT every character and hyphenate after it
\def\hyphenateAndTtWholeString #1{\xHyphenate#1\wholeString\unskip}
\def\xHyphenate#1#2\wholeString {\if#1%
    \else\transform{#1}%
    \takeTheRest#2\ofTheString\fi}
\def\takeTheRest#1\ofTheString\fi
{\fi \xHyphenate#1\wholeString}
\def\transform#1{\url{#1}\hskip 0pt plus 1pt}

% Define the \urlx command which works like \url, but with line brakes
\def\urlx #1{\href{#1}{\hyphenateAndTtWholeString{#1}}}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
%\usepackage[utf8]{inputenc}


% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\newcommand{\datasetname}{\texttt{KazMMLU}\xspace}
\newcommand{\vquad}{\hspace{0.3em}} 
\newcommand{\ex}[1]{\textit{#1}\xspace} 
\newcommand{\gl}[1]{``#1''\xspace} 
\newcommand{\equalsign}{\footnotemark[1]\hspace{0.1cm}}

\newcommand{\dummy}[1]{\textcolor{red}{#1}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\cmark}{\ding{51}}
\definecolor{mygreen}{RGB}{217, 234, 211}
\definecolor{myred}{RGB}{244, 204, 204}

\newcommand{\ok}{\cellcolor{mygreen}}
\newcommand{\no}{\cellcolor{myred}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% Original title
% \title{Do Large Language Models Capture Kazakhstan's Local Knowledge?}

% Preslav's proposed title
%\title{\datasetname: Evaluating Large Language Models on Kazakh Language and %Knowledge about Kazakhstan}

\title{\datasetname: Evaluating Language Models on Kazakh, Russian, \\and Regional Knowledge of Kazakhstan}



% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Mukhammed Togmanov\thanks{These authors contributed equally.} \quad Nurdaulet Mukhituly$^*$ \quad  Diana Turmakhan$^*$ \\  \textbf{Jonibek Mansurov} \quad   \textbf{Maiya Goloburda} \quad    \textbf{Akhmed Sakip} \quad   \textbf{Zhuohan Xie} \\   \textbf{Yuxia Wang}  \quad   \textbf{Bekassyl Syzdykov}  \quad  \textbf{Alham Fikri Aji} \quad \textbf{Ekaterina Kochmar}  \\   \textbf{Preslav Nakov}  \quad  \textbf{Fajri Koto} \\
Department of Natural Language Processing, MBZUAI \\
	\texttt{\small \{mukhammed.togmanov,nurdaulet.mukhituly,diana.turmakhan\}@mbzuai.ac.ae 
	} 
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\begin{abstract}
%\todo{Preslav: I think the title should have the name of the dataset and be more broad. See my proposed new title.}
Despite having a population of twenty million, Kazakhstan's culture and language remain underrepresented in the field of natural language processing. Although large language models (LLMs) continue to advance worldwide, progress in Kazakh language has been limited, as seen in the scarcity of dedicated models and benchmark evaluations. To address this gap, we introduce \datasetname, the first MMLU-style dataset specifically designed for Kazakh language. \datasetname comprises 23,000 questions that cover various educational levels, including STEM, humanities, and social sciences, sourced from authentic educational materials and manually validated by native speakers and educators. The dataset includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting Kazakhstan's bilingual education system and rich local context. Our evaluation of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4, and DeepSeek V3) demonstrates substantial room for improvement, as even the best-performing models struggle to achieve competitive performance in Kazakh and Russian. These findings underscore significant performance gaps compared to high-resource languages. We hope that our dataset will enable further research and development of Kazakh-centric LLMs.\footnote{\datasetname{} can be accessed at \url{https://huggingface.co/datasets/MBZUAI/KazMMLU}.}
\end{list}
\end{abstract}


%The evaluation of language models has increasingly focused on reasoning and knowledge-intensive tasks, propelled by advancements in pretraining large-scale models. Despite progress in multilingual language models, their performance in underrepresented languages, such as Kazakh, remains underexplored due to the scarcity of comprehensive benchmarks. To address this gap, we present \textbf{}, the first multitask language understanding benchmark specifically designed for  Kazakhstan. Our dataset comprises \textbf{23,000 multiple-choice questions} spanning STEM, humanities, and social sciences, sourced from authentic educational materials and enriched with input from native speakers and educators. These questions are distributed across High School and University levels and cover both Kazakh and Russian, reflecting the bilingual nature of Kazakhstan's education system. Our evaluations of state-of-the-art multilingual models, including BLOOMZ, mT0, Llama, and Falcon, reveal substantial room for improvement. Notably, even the best-performing models struggle to achieve competitive accuracy in Kazakh, highlighting significant gaps compared to high-resource languages. \footnote{Data and code can be accessed at: \url{https://huggingface.co/datasets/MukhammedTogmanov/jana}}

\section{Introduction} %[Diana, Maiya, Nurdaulet, Fajri]

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{latex/img/image__6.png}
\caption{Overview of the  dataset. This diagram illustrates the distribution of questions by educational level (High School and University) and language (Kazakh and Russian), along with the variety of subjects covered.}
\label{fig:overview}
\end{figure}



%Large language models (LLMs) have transformed natural language processing (NLP) and artificial intelligence, achieving significant breakthroughs in reasoning and knowledge-intensive tasks. Although much progress has been made in evaluating these models for widely spoken languages, their capabilities in underrepresented languages like Kazakh remain under-explored. This imbalance is particularly critical as language plays a fundamental role in preserving cultural identity and enabling equitable access to technology. Regionally and culturally specific datasets are essential for ensuring that AI technologies serve diverse populations accurately and fairly. 

%Large language models (LLMs) have transformed natural language processing (NLP) and artificial intelligence, achieving significant breakthroughs in reasoning and knowledge-intensive tasks. Although much progress has been made in evaluating these models for widely spoken languages, their capabilities in underrepresented languages like Kazakh remain under-explored. This imbalance is particularly critical as language plays a fundamental role in preserving cultural identity and enabling equitable access to technology. Regionally and culturally specific datasets are essential for ensuring that AI technologies serve diverse populations accurately and fairly. 

With a population exceeding twenty million, the Republic of Kazakhstan in Central Asia remains underrepresented in the field of natural language processing (NLP) \cite{joshi-etal-2020-state}. This gap is highlighted by the limited progress in developing large language models (LLMs) and evaluation benchmarks specifically tailored to the languages and the cultural context of Kazakhstan. 
Kazakh, a Turkic language spoken by more than fourteen million people (around 70\% of the population), holds substantial cultural and geopolitical significance in Central Asia. Russian, used by approximately 15\% of the population, serves as the country’s second primary language.\footnote{\url{https://glottolog.org/}} 

Although Kazakh appears in certain multilingual datasets \citep{yeshpanov2024kazqadkazakhopendomainquestion, yeshpanov-etal-2022-kaznerd, yeshpanov2024kazsandra}, most of these resources rely heavily on translations from English, lacking the cultural richness essential for inclusive LLM development. Previous work has primarily addressed classic NLP tasks such as named entity recognition \cite{yeshpanov-etal-2022-kaznerd} and sentiment analysis \cite{yeshpanov2024kazsandra}. Meanwhile, recent developments in LLM research have shifted toward more reasoning-focused evaluation \cite{meta2024Llama3,openai2024gpt4o}, highlighting a clear research gap for inclusive NLP in the Kazakh context.

Here, we aim to bridge this gap. In particular, we introduce \datasetname, a curated dataset of school- and university-level questions from Kazakhstan, available in both Kazakh and Russian. \datasetname follows the framework of the Massive Multitask Language Understanding (MMLU) dataset \cite{hendrycks2021measuringmassivemultitasklanguage,koto-etal-2024-arabicmmlu, li2024cmmlumeasuringmassivemultitask, koto-etal-2023-large}, which features multiple-choice questions across various subjects and education levels. MMLU has become a standard benchmark for evaluating LLMs' reasoning and knowledge capabilities \cite{meta2024Llama3,gemmateam2024gemma2improvingopen,qwen2025qwen25technicalreport}. Unlike general MMLU, \datasetname incorporates Kazakhstan-specific content, including topics on Kazakh history, traditions, and linguistics, while also reflecting the country's multilingual landscape by providing questions in both Kazakh and Russian.


As shown in Figure \ref{fig:overview}, the dataset is divided into two categories: High School and University. \datasetname consists of approximately 48\% of the questions in Kazakh and 52\% in Russian. The High School section includes questions in both Kazakh and Russian, covering subjects such as Mathematics, Physics, and Kazakh Literature. The University section only features questions in Russian, focusing on professional disciplines such as Law, Economics, and Medicine. This structure aligns with Kazakhstan's bilingual education system and provides a more representative benchmark for evaluating LLMs in the region. \datasetname is sourced from authentic educational materials, including national exams, textbooks, and professional certification repositories. Each question is accompanied by metadata, including the subject, level, source, and correct answer key, ensuring transparency and usability for downstream evaluations.

\textbf{Our contributions can be summarized as follows:}
\begin{itemize}
\item We present the first \textbf{MMLU-style dataset} specifically tailored to the Kazakhstan context, covering diverse subject areas across educational and professional levels. The dataset is made available in both Kazakh and Russian.
\item We evaluate various \textbf{multilingual LLMs}, including Llama-3.1 \cite{meta2024Llama3}, Qwen \cite{qwen2025qwen25technicalreport}, GPT-4o \cite{openai2024gpt4o}, BLOOMZ \cite{muennighoff2023crosslingualgeneralizationmultitaskfinetuning}, mT0 \cite{muennighoff2023crosslingualgeneralizationmultitaskfinetuning}, and DeepSeek V3 \cite{deepseek2024}, across different model sizes. 
\item We conduct a thorough analysis of the top-performing open-source models across various dimensions, encompassing (1) individual \textbf{subject areas, educational levels, and Kazakhstan-specific topics}, (2) \textbf{few-shot inference performance}, (3) \textbf{model confidence}, and (4) the \textbf{influence of negation} on model performance. This comprehensive evaluation framework allows us to identify the performance gaps and opportunities for improvement in multilingual LLMs when applied to Kazakh and Russian contexts.
\end{itemize}



%Despite recent advances in LLM that emphasize enhanced reasoning capabilities, most research still prioritizes European languages \cite{meta2024Llama3,openai2024gpt4o}. Only a few regional initiatives have emerged, notably in Indonesia \cite{koto-etal-2023-large} and Latin America \cite{kellert-zaman-2023-use}. 

%Moreover, since Kazakh is classified as a low-resource language \cite{joshi-etal-2020-state}, it remains comparatively underexplored. 

%We introduce \datasetname, the first large-scale benchmark designed to evaluate multitask language understanding specifically for the Kazakh language. Inspired by the design of MMLU \citep{hendrycks2021measuringmassivemultitasklanguage}, our dataset comprises 23,000 questions derived from authentic sources, including national educational materials, documents, and culturally specific subjects such as Kazakh history, traditions, and linguistics. These tasks span multiple domains, including STEM, social sciences, and humanities, allowing for a holistic assessment of both general and Kazakhstan-specific knowledge.

%The emergence of benchmarks like MMLU by \citet{hendrycks2021measuringmassivemultitasklanguage} reflects the growing need for standardized evaluation frameworks to measure LLMs' performance across diverse domains. These benchmarks are designed to assess not only linguistic fluency but also reasoning and knowledge in subjects ranging from STEM to social sciences and humanities. While MMLU has become a cornerstone for evaluating English-language models, adaptations for other languages, such as Chinese, Arabic and Indonesian \citep{koto-etal-2024-arabicmmlu, li2024cmmlumeasuringmassivemultitask, koto-etal-2023-large}, highlight the importance of localized evaluation tools. Such efforts underscore the necessity of benchmarks that reflect the linguistic, educational, and cultural nuances of specific languages, enabling more targeted advancements in LLM development.

%Spoken by over 13 million people, Kazakh is a Turkic language with deep cultural and geopolitical significance in Central Asia. Although Kazakh is featured in some multilingual datasets and a few task-specific datasets exist \citep{yeshpanov2024kazqadkazakhopendomainquestion, yeshpanov-etal-2022-kaznerd, yeshpanov2024kazsandra}, there is still a significant gap in comprehensive benchmarks to evaluate knowledge across subjects and domains. To address this gap, we introduce \textbf{}, the first large-scale benchmark designed to evaluate multitask language understanding specifically for the Kazakh language. Inspired by the design of MMLU \citep{hendrycks2021measuringmassivemultitasklanguage}, our dataset comprises 23,000 questions derived from authentic sources, including national educational materials, documents, and culturally specific subjects such as Kazakh history, traditions, and linguistics. These tasks span multiple domains, including STEM, social sciences, and humanities, allowing for a holistic assessment of both general and Kazakhstan-specific knowledge.






% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl\_latex.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

\section{Related Work} %[Akhmed, Jonibek, Maiya, Diana, Fajri, revised by Mukhammed]

\paragraph{Language Models in Kazakh and Russian}
% First paragraph: any multilingual language models that has Kazakh and Russian.
Prominent models such as OpenAI's ChatGPT, Anthropic's Claude, and Yandex's Yandex-GPT are designed to handle multiple languages, including Russian and Kazakh, enabling a wide range of applications from translation to content generation~\citep{openai2024gpt4o,anthropic_claude, yandex_yandexgpt}. Additionally, open-source models like Meta's Llama series provide multilingual support~\citep{meta2024Llama3}. While these models can produce text in Kazakh, they were not specifically trained or fine-tuned for it. In contrast, the Aya model, an open-access multilingual LLM, supports 101 languages, including Kazakh~\citep{ustun-etal-2024-aya}.

Kazakh-specific language models have been scarce, with most multilingual models offering only limited support. To bridge this gap, \citet{issai_kazllm} introduced KazakhLLM, a model fine-tuned on Kazakh data from Llama, though its evaluation has primarily relied on machine-translated datasets.

%Second paragraph: is there any Kazakh-specific language model in the past?


\paragraph{NLP Benchmark for Kazakhstan Context}
% Divided into multilingual benchmark: XCOPA, XFGLUE, XTREME. Do they have Kazakh and Russian? Also talk about GlobalMMLU by Aya Cohere, and INCLUDE by AyaCohere? Do they have Kazakh?
% Move to Kazakh and Russian dataset; Talk about NLP datases in Kazakh, and then Russian. Highlight that the Russian dataset does not have Kazakhstan specific context.

%[MukhammedJ] Revised

Evaluating LLMs across diverse linguistic and cultural contexts is increasingly critical; however, existing benchmarks overlook Kazakhstan. While benchmarks such as XCOPA~\citep{ponti2021xcopa}, XFGLUE~\citep{liang2020xfglue}, and XTREME~\citep{hu2020xtreme} assess cross-lingual performance, they exclude Kazakh, and GlobalMMLU~\citep{cohere2024globalmmlu} lacks Kazakhstan-specific content.
In contrast, several Kazakh-specific datasets exist, including KazNERD~\citep{yeshpanov-etal-2022-kaznerd} for named entity recognition, KazQAD~\citep{yeshpanov2024kazqadkazakhopendomainquestion} for question answering, and KazSANDRA~\citep{yeshpanov2024kazsandra} for sentiment analysis. However, these datasets focus on narrow tasks and do not assess reasoning, factual recall, or domain-specific knowledge.
To address these limitations, \datasetname{} presents a large-scale, Kazakhstan-specific benchmark covering STEM, humanities, and social sciences. Unlike previous datasets, \datasetname{} supports a holistic evaluation of reasoning and domain-specific knowledge, offering a more accurate assessment of multilingual LLM capabilities and advancing AI for low-resource languages.

To further illustrate the differences between \datasetname{} and previous benchmarks, we compare it with two existing datasets, SIGTURK~\citep{maxutov2024sigturk} and INCLUDE~\citep{romanou2024include}, in Table~\ref{tab:comparison-mmlu}.
As shown, \datasetname{} is the only dataset that incorporates \textbf{real-world educational materials, professional subjects}, and \textbf{domain-specific reasoning} in both Kazakh and Russian, offering a more \textbf{localized} and \textbf{comprehensive} evaluation of LLMs. This structured assessment underscores the \textbf{importance of country-specific benchmarks} in multilingual NLP research and contributes to bridging the gap in Kazakh language understanding.

% We compare \datasetname{} against existing benchmarks, namely SIGTURK 2024 \cite{maxutov2024sigturk} and INCLUDE \cite{romanou2024include}, in Table~\ref{tab:comparison-mmlu}. Unlike SIGTURK, which focuses on specific NLP tasks in Kazakh, and INCLUDE, which evaluates general multilingual reasoning but lacks Kazakh content, \datasetname{} introduces a \textbf{large-scale, bilingual} evaluation benchmark tailored to Kazakhstan’s unique linguistic and educational landscape. 




% Therefore, there is a need for evaluation of low-resource languages since LLMs struggle with low-resource languages and often miss nuanced linguistic features~\citep{wang2023all} due to imbalanced training data~\citep{conneau2020unsupervised}.


% Although Russian NLP datasets are more extensive, they do not capture Kazakhstan’s unique linguistic and cultural contexts.

% Prior work has shown that LLMs struggle with low-resource languages due to imbalanced training data \cite{conneau2020unsupervised} and often miss nuanced linguistic features \cite{wang2023all}. In response, language-specific models like Vikhr \citep{nikolich2024vikhrconstructingstateoftheartbilingual} and Aya \citep{ustun-etal-2024-aya} have been developed; however, they are not tailored for the knowledge-rich reasoning required by Kazakhstan’s bilingual educational system.



%[Mukhammed] Comparison Between SIGTURK

% \begin{table*}[ht]
% \centering
% \renewcommand{\arraystretch}{1.2} % Adjust row height
% \setlength{\tabcolsep}{3pt} % Adjust column spacing
% \footnotesize % Compact text
% \begin{tabularx}{\textwidth}{|p{3.6cm}|p{3.8cm}|p{3.8cm}|p{3.8cm}|} % Ensuring equal column widths
% \hline
% \textbf{Feature} & \textbf{KazMMLU} & SIGTURK & INCLUDE\\
% \hline
% \textbf{Dataset Size} & 23,000 questions & 6 tasks, existing datasets & 197,243 questions \\
% \hline
% \textbf{Languages Covered} & \textbf{Kazakh and Russian} & Kazakh & 44 languages (but no Kazakh) \\
% \hline
% \textbf{Kazakh-Specific Content} & \textbf{Yes}, sourced from \newline local curriculum, national exams & Limited (Kazakh NLP tasks) & No \\
% \hline
% \textbf{Education Levels} & \textbf{High School, University} & Not explicitly structured & General education \\
% \hline
% \textbf{Subjects Covered} & \textbf{STEM, Humanities,} \newline \textbf{Social Sciences, Law, Medicine} & QA, MT, \newline causal reasoning & Broad general knowledge \\
% \hline
% \textbf{Task Type} & \textbf{Bilingual MCQs} \newline reflecting real-world knowledge & QA, classification, \newline generative tasks & General MCQs across languages \\
% \hline
% \textbf{Model Benchmarking} & \textbf{41 LLMs} \newline (GPT-4o, Llama-3.1, DeepSeek, etc.) & 7 models on \newline Kazakh NLP tasks & Multiple LLMs \newline across 44 languages \\
% \hline
% \end{tabularx}
% \caption{Comparison of \textbf{KazMMLU} with SIGTURK and INCLUDE.}
% \label{tab:comparison-mmlu}
% \end{table*}


\begin{table*}[t!]
\centering
\renewcommand{\arraystretch}{1.2} % Adjust row height
\setlength{\tabcolsep}{5pt} % Slightly increase column spacing
\footnotesize % Compact text
\begin{tabularx}{\textwidth}{X X X X} 
\toprule
\textbf{Feature} & \textbf{KazMMLU} & \textbf{SIGTURK} & \textbf{INCLUDE} \\
\midrule
% \bf Dataset Size & 23,000 questions & 3,000 questions & 5736 questions \\
\bf Public Dataset Size & 23,000 questions in Kazakh and Russian & 3,000 questions exclusively in Kazakh & 23,741 total questions, including 500 in Kazakh \\
\bf Languages Covered & Kazakh, Russian & Kazakh & 44 (including Kazakh) \\
\bf Kazakh-Specific Content & \textbf{Yes}, sourced from local curriculum, national exams & Limited (Kazakh NLP tasks) & Limited \\
\bf Education Levels & High School, University & Not explicitly structured & General education \\
\bf Subjects Covered & STEM, Humanities, Social Sciences, Law, Medicine & QA, MT, causal reasoning & Broad general knowledge \\
\bf Task Type & Bilingual MCQs reflecting real-world knowledge & QA, classification, generative tasks & General MCQs across languages \\
\bf Model Benchmarking & 41 LLMs (GPT-4o, Llama-3.1, DeepSeek V3, etc.) & 7 models on Kazakh NLP tasks & Multiple LLMs across 44 languages \\
\bottomrule
\end{tabularx}
\caption{Comparison of \textbf{KazMMLU} with SIGTURK and INCLUDE.}
\label{tab:comparison-mmlu}
\end{table*}





% \paragraph{Comparison with Existing Benchmarks}
% We compare \datasetname{} against existing benchmarks, namely SIGTURK 2024 \cite{maxutov2024sigturk} and INCLUDE \cite{romanou2024include}, in Table~\ref{tab:comparison-mmlu}. Unlike SIGTURK, which focuses on specific NLP tasks in Kazakh, and INCLUDE, which evaluates general multilingual reasoning but lacks Kazakh content, \datasetname{} introduces a \textbf{large-scale, bilingual} evaluation benchmark tailored to Kazakhstan’s unique linguistic and educational landscape. 

% \datasetname{} is the only dataset that integrates \textbf{real-world educational materials, professional subjects}, and \textbf{domain-specific reasoning} in both Kazakh and Russian, providing a more \textbf{localized} and \textbf{comprehensive} assessment of LLMs. This structured evaluation highlights the \textbf{importance of country-specific benchmarks} in multilingual NLP research and helps bridge the existing gap in Kazakh language understanding.



%Evaluating the performance of large language models (LLMs) has become an important task, especially as these models are being deployed across a wide variety of languages and tasks. 


%The MMLU benchmark has been widely adopted for this purpose, with a focus on English and other major languages \citep{hendrycks2021measuringmassivemultitasklanguage}. However, as the demand for multilingual models grows, there is an increasing need for benchmarks that address the unique challenges of non-English languages, including Kazakh. To address this, several MMLU-style datasets in languages like Arabic, Chinese and Indonesian have been introduced \citep{koto-etal-2024-arabicmmlu, li2024cmmlumeasuringmassivemultitask, koto-etal-2023-large}. 


% Multilingual LLMs often demonstrate performance disparities across languages, particularly struggling with low-resource languages due to training data imbalances \citep{conneau2020unsupervised}. These challenges underscore the importance of developing comprehensive evaluation frameworks that can assess model performance across diverse linguistic and cultural contexts. Recent work has shown that even state-of-the-art multilingual models may fail to capture nuanced cultural and linguistic features in low-resource settings \citep{wang2023all}. This has led to the development of language-specific models, such as Vikhr \citep{nikolich2024vikhrconstructingstateoftheartbilingual} for Russian and Aya \citep{ustun-etal-2024-aya} for Kazakh, alongside commercial solutions like YandexGPT. These models are evaluated through frameworks like the MERA project for Russian ethical diagnostics \citep{fenogenova2024meracomprehensivellmevaluation}, but such efforts lack adaptation to Kazakhstan's unique linguistic and cultural context.

% Existing Kazakh datasets, such as KazNERD \citep{yeshpanov-etal-2022-kaznerd}, KazQAD \citep{yeshpanov2024kazqadkazakhopendomainquestion}, and KazSANDRA \citep{yeshpanov2024kazsandra}, provide valuable resources for specific tasks like named entity recognition, question answering, and sentiment analysis. However, they do not offer a holistic evaluation of reasoning and domain-specific knowledge across multiple disciplines. Our work addresses this gap by providing a comprehensive benchmark that evaluates Kazakh language understanding across a wide range of subjects, enabling more accurate assessments of multilingual LLM performance and fostering advancements in AI for low-resource languages.

% \subsection{Multilingual and Cross-Lingual Benchmarks}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

\section{KazMMLU}

%[FJ] check and edit the factuality
In Kazakhstan, the K-12 education system operates in a multilingual setting, with Kazakh as the primary language of instruction in most schools. However, Russian and other languages, such as Uzbek, Uyghur, and Tajik, are also used in specific regions. The curriculum includes core subjects such as mathematics, science, and history, with students required to study both Kazakh and Russian as part of their language education. At the university and professional levels, Russian remains the dominant language of instruction, especially in fields such as law, medicine, economics, and engineering.

%[duplicate from related ]
% \datasetname{} is the first multiple-choice question dataset specifically designed for Kazakhstan, consisting of 23K questions across various subjects in Kazakh and Russian. Unlike prior benchmarks such as \texttt{GlobalMMLU} \cite{cohere2024globalmmlu}, which provide broad multilingual evaluation but lack dedicated coverage of Kazakhstan’s linguistic and cultural context, \datasetname{} fills this gap by incorporating questions that reflect the country’s educational and professional landscape. 




%Inspired by the success of benchmarks such as ArabicMMLU \citep{koto-etal-2024-arabicmmlu} and IndoMMLU \citep{koto-etal-2023-large},  is the first multitask language understanding benchmark specifically designed for the Kazakh and Russian languages. It aims to address the lack of resources for evaluating large language models (LLMs) in underrepresented languages by providing a comprehensive dataset tailored to the unique linguistic and cultural characteristics of Kazakhstan.  consists of 23,000 multiple-choice questions (MCQs) that span two educational levels (High School and University) and cover a wide range of subjects, including STEM, humanities, and social sciences.

%Figures \ref{fig:dataset-overview} and \ref{fig:subject-distribution} illustrate key aspects of the dataset's composition and subject distribution. These visualizations provide a comprehensive understanding of the dataset's scope and the challenges it presents for multilingual language models.


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{latex/img/image2.png}
\caption{Subject-wise distribution of questions in \datasetname{}.}
\label{fig:subject-distributions}
\end{figure}


\datasetname{} covers a wide range of subjects spanning multiple disciplines, including STEM, humanities, social sciences, and professional studies. Figure~\ref{fig:subject-distributions} illustrates the distribution of questions across different subjects. The dataset exhibits a strong representation in STEM fields, with subjects like Biology, Mathematics and Physics accounting for a significant portion of the total questions. Humanities and social sciences are also well represented, particularly Kazakh History, World History, and Law, reflecting the importance of these disciplines in the Kazakh educational system. 

Additionally, \datasetname{} includes questions from professional domains such as Economics, Finance, Jurisprudence, and Medicine, enabling the evaluation of language models in specialized areas. The inclusion of both Kazakh and Russian language subjects further ensures a balanced linguistic representation. This diverse subject distribution provides a robust benchmark for evaluating multilingual language models across various domains.

To illustrate the question format in \datasetname, Figure \ref{fig:kazakh-mcq} presents a sample multiple-choice question in Kazakh. Answering this question requires an understanding of Kazakhstan's local context, as it covers topics such as history and geography. This example highlights the dataset's diversity across subjects and difficulty levels.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{latex/img/image__9.png}
\caption{Examples of a Kazakh history question and a geography test question from \datasetname{}. The \textbf{left} side presents the original text, while the \textbf{right} side provides the English translation for reference. The bolded options indicate the correct answers.}
\label{fig:kazakh-mcq}
\end{figure}






\subsection{Data Construction}

To construct \datasetname, we adopted a systematic approach inspired by MMLU datasets \citep{koto-etal-2023-large,koto-etal-2024-arabicmmlu,li2024cmmlumeasuringmassivemultitask}. The dataset comprises questions sourced from national exams, textbooks, and professional certification materials such as iTest.kz, ymnik.kz, oltest.kz and Book - Shyn Kitap. To ensure diversity, we employed three data collection strategies: (1) automated online crawling, (2) manual transcription from scanned books, and (3) manual extraction from online sources. A detailed breakdown of dataset sources is provided in Appendix~\ref{tab:grouped_dataset_sources}.


%85% automatic scrapping
%15% manual effort

For automatic online crawling, we collect question texts, multiple-choice options, correct answer keys, and metadata. For books, authors manually scan materials and apply document processing for machine-readable conversion. Two expert workers fluent in Kazakh and Russian manually extract questions from scanned books and online sources, recording metadata such as source, country, subject, level, and answer key. We compiled 23,000 questions, with (85\%) from automated crawling and the rest from manual extraction.

Only questions with valid answer keys were included, while multimodal ones requiring images or videos were excluded. For context-dependent questions, annotators ensured necessary context was included. A training workshop clarified guidelines, and weekly check-ins ensured consistency. Annotators were competitively compensated to maintain quality.

\subsection{Quality Control}

Our quality control process primarily targets the automatically-crawled data, as the other two data collection strategies involve direct human involvement. To ensure accuracy, we recruited two professional annotators, each holding at least a bachelor's degree and fluent in both Kazakh and Russian. They manually reviewed all questions to verify correctness and completeness. Any question containing errors or missing components (e.g., incomplete contexts or broken answer options) was discarded. Through this extensive human verification, every question included in \datasetname{} undergoes manual validation, ensuring a high-quality dataset.

% Additionally, we developed automated scripts to detect duplicates and verify the completeness of metadata fields. These scripts help identify and eliminate errors such as duplicate questions, incorrect answer keys, and formatting inconsistencies, further enhancing the dataset’s reliability.
Additionally, we developed scripts to detect duplicates, verify metadata, and eliminate errors like duplicate questions, incorrect answer keys, and formatting issues, enhancing dataset reliability

%To ensure the quality of the dataset, we implemented a multi-step validation process. First, annotators were trained extensively to follow the guidelines and maintain uniformity across collected questions. Next, the collected questions underwent a peer-review process, wherein a second annotator independently reviewed each question for accuracy, clarity, and completeness. Questions flagged during this process were returned to the original annotator for clarification or correction.

%A random sample of questions from each subject and level was manually verified by domain experts to validate the dataset's accuracy and relevance. These measures ensured that the dataset meets the high-quality standards required for reliable evaluation of large language models.


\subsection{Data Statistics}

\datasetname{} comprises 23,000 multiple-choice questions spanning two educational levels: high school and university. As shown in Figure~\ref{fig:overview}, 48\% of the dataset consists of high school-level questions in Kazakh, 36\% in Russian, and 16\% university-level questions in Russian. Table~\ref{tab:subject-areas} outlines the subject distribution, covering STEM, Humanities, Social Sciences, and Languages. Notably, the Humanities, Social Sciences, and Language sections contain extensive Kazakhstan-specific knowledge. The dataset maintains a balanced distribution between Kazakh and Russian, reflecting the bilingual nature of Kazakhstan’s education system.

Table~\ref{tab:average-lengths} presents the average question and answer lengths across educational levels and subject areas. While the overall question length remains relatively consistent between high school and university levels, answer lengths (in characters) tend to be longer at the university level. Additionally, questions in Humanities and STEM subjects are generally longer compared to those in Social Sciences and Languages.

% \small
% \begin{table}[t!]
% \centering
% \renewcommand{\arraystretch}{1.3} % Increase row height for readability
% \setlength{\tabcolsep}{3.2pt} % Reduce column spacing for compactness
% \begin{tabular}{p{2.2cm}|p{5.0cm}}
% \hline
% \textbf{Group} & \textbf{Subjects} \\
% \hline
% Humanities & \raggedright Culture and Art (U), Kazakh History (H), Kazakh Literature (H), Philosophy and Psychology (U), Russian Literature (H), World History (H)\tabularnewline
% \hline
% Language & \raggedright Kazakh Language (H), Reading Literacy (H), Russian Language (H)\tabularnewline
% \hline
% Other & \raggedright General Education Disciplines (U) \tabularnewline
% \hline
% STEM & \raggedright Biology (H), Chemistry (H), Informatics (H), Math (H), Math Literacy (H), Medicine (U), Physics (H) \tabularnewline
% \hline
% Social Science & \raggedright Accounting and Auditing (U), Economics and Entrepreneurship (U), Education and Training (U), Finance, Credit, Insurance (U), General Education Disciplines (U), Geography (H), Jurisprudence (U), State and Law (U), Management and Marketing (U), Social Science (U) \tabularnewline
% \hline
% \end{tabular}
% \caption{``H'' indicates middle/high school subjects, and ``U'' indicates university subjects.}
% \label{tab:subject-areas}
% \end{table}

\begin{table}[t]
\centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{p{1.55cm}p{5.3cm}}
    \toprule
    \textbf{Group} & \textbf{Subjects} \\
    \midrule
    \multirow{5}{*}{Humanities} & Culture and Art (U), Kazakh History (H), Kazakh Literature (H), Philosophy and Psychology (U), Russian Literature (H), World History (H) \\
    \midrule
    \multirow{3}{*}{Language} & Kazakh Language (H), Reading Literacy (H), Russian Language (H) \\
    \midrule
    Other & General Education Disciplines (U) \\
    \midrule
    \multirow{3}{*}{STEM} & Biology (H), Chemistry (H), Informatics (H), Math (H), Math Literacy (H), Medicine (U), Physics (H) \\
    \midrule
    \multirow{9}{*}{\shortstack[l]{Social\\Science}} & Accounting and Auditing (U), Economics and Entrepreneurship (U), Education and Training (U), Finance, Credit, Insurance (U), General Education Disciplines (U), Geography (H), Jurisprudence (U), State and Law (U), Management and Marketing (U), Social Science (U) \\
    % Others & 82.0 & 37.1 \\
    % STEM & 83.8 & 15.1 \\
    % Social Science & 52.9 & 15.4 \\
    \bottomrule
    \end{tabular}
}
\caption{Subject groups covered by KazMMLU. ``H'' indicates high school subjects, and ``U'' indicates university subjects.}
\label{tab:subject-areas}
\end{table}

\begin{table}[t]
\centering
\resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lrr}
    \toprule
    \textbf{Group} & \textbf{Question} & \textbf{Answer} \\
    \midrule
    High School & 78.3 & 16.6 \\
    University & 84.4 & 29.6 \\
    \midrule
    Humanities & 81.3 & 19.1 \\
    Language & 49.3 & 20.3 \\
    Others & 82.0 & 37.1 \\
    STEM & 83.8 & 15.1 \\
    Social Science & 52.9 & 15.4 \\
    \bottomrule
    \end{tabular}
}
\caption{Average question and answer length (in characters) for each educational group and subject area.}
\label{tab:average-lengths}
\end{table}




\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{latex/img/image__7.png}
\caption{Prompt templates in Kazakh and English.}
\label{fig:prompt-example}
\end{figure}

\section{Experiment } % [Diana, Nurdaulet, Mukhammed, Zhouhan]
\subsection{Setup}
We evaluate 26 multilingual LLMs of various sizes in both zero-shot and few-shot settings. Our selection includes models from diverse architectures, such as BLOOM~\cite{Scao2022BloomA1}, BLOOMZ, and mT0~\cite{muennighoff2023crosslingualgeneralizationmultitaskfinetuning}, Falcon~\cite{falcon40b}, Llama-3.1~\cite{touvron2023Llama,touvron2023Llama2}, GPT-4o~\cite{openai2024gpt4o}, mT5~\cite{xue2020mt5}, Vikhr~\cite{nikolich2024vikhrconstructingstateoftheartbilingual}, and DeepSeek V3~\cite{deepseek2024}. 

%\subsection{Prompt Structure and Format}
For the evaluation purpose, we use two distinct prompt configurations to examine the effect of prompt language: (1) a Kazakh prompt with English (Latin script) alphabetic output and (2) an English prompt with English alphabetic output, as illustrated in Figure~\ref{fig:prompt-example}. For placeholders such as \texttt{[Subject]} and \texttt{[Level]}, we use Kazakh in the Kazakh prompt and translate them into English for the English prompt. However, the question and answer choices remain in their original language (Kazakh or Russian). A complete example prompt is provided in Appendix B (Figure~\ref{fig:prompt-examples}).

Following prior studies~\citep{koto-etal-2023-large, li2024cmmlumeasuringmassivemultitask}, we adopt different answer selection methods based on model accessibility. For open-weight models, we apply the \textit{next-token prediction} approach, computing probabilities for each multiple-choice option (A, B, C, D, or E) and selecting the one with the highest probability. This method is well-suited for autoregressive models that perform token-wise scoring. For closed-weight models (e.g., GPT--4o, Yandex-GPT, and DeepSeek V3), we use a \textit{free-text generation} strategy, prompting the model to generate a textual response, from which the predicted answer is extracted via string matching. This approach is necessary due to the lack of direct token probability outputs in closed-source APIs. For evaluation, we use accuracy as the primary metric, following prior studies~\citep{koto-etal-2023-large, li2024cmmlumeasuringmassivemultitask}.



% \begin{enumerate}
%     \item .
%     \item 
% \end{enumerate}
% duplicate
% \paragraph{Impact of Prompt Language on Performance}
% To examine whether the prompt language influences model performance, we evaluate LLMs under both \textbf{Kazakh-prompted} and \textbf{English-prompted} settings. 

% Zhuohan: Isn't this from the next section?
% Table~\ref{tab:kazakh-mmlu} presents results for English prompts, while Table~\ref{tab:kazakh-mmlu-kaz} (Appendix~\ref{sec:appendix}) provides performance under Kazakh prompts.
% Our findings show that prompt language significantly affects accuracy. Models generally perform better on \textbf{English text} when prompted in English rather than Kazakh, indicating that linguistic alignment between the prompt and test language enhances comprehension. However, instruction-tuned models (e.g., GPT-4o, DeepSeek) demonstrate greater robustness across prompt styles, while smaller models exhibit a larger performance gap. 
% Further analysis is needed to quantify this effect across different subjects and reasoning types.

% \paragraph{Prompt Structure and Format}
% Figure~\ref{fig:prompt-example} illustrates the standardized format for Kazakh and English prompts. 




%By employing these two complementary evaluation strategies, we ensure a fair and consistent comparison of models with varying levels of accessibility and architectural constraints.



\subsection{Results and Analysis} %[Nurdaulet, Mukhammed, Zhouhan]

First, we observe that LLMs achieve higher accuracy when prompted in English, as shown in Table~\ref{tab:kazakh-mmlu} and Table~\ref{tab:kazakh-mmlu-kaz}. To provide clearer insights into model performance, we focus on English-prompted results in the main body of the paper.


\begin{table*}[t!]
\setlength{\tabcolsep}{5pt}
\footnotesize
\centering
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccccc}
\hline
\textbf{Model} & \textbf{STEM} & \textbf{Social Science} & \textbf{Humanities} & \textbf{Language} & \textbf{Other} & \textbf{Average} \\
\hline
Mistral-7B-Instruct-v0.3            & 36.5 & 44.7 & 41.8 & 30.0 & 40.6 & 38.2 \\
Mistral-7B-v0.3                     & 31.5 & 37.1 & 37.9 & 27.3 & 34.6 & 33.3 \\
\hdashline
Vikhr-Nemo-12B-Instruct-R-21-09-24  & 39.5 & 49.5 & 48.1 & 32.7 & 44.0 & 42.2 \\
\hdashline
aya-23-35B                          & 34.5 & 38.5 & 38.5 & 29.4 & 33.2 & 35.2 \\
aya-23-8B                           & 29.4 & 31.9 & 32.7 & 25.8 & 26.5 & 29.9 \\
\hdashline
bloom-1b1                           & 24.1 & 20.4 & 21.0 & 21.6 & 20.8 & 22.2 \\
bloomz-1b7                          & 24.2 & 22.7 & 22.7 & 23.0 & 25.8 & 23.4 \\
bloomz-3b                           & 24.2 & 23.5 & 23.5 & 22.1 & 25.2 & 23.6 \\
bloomz-7b                           & 24.0 & 24.0 & 24.2 & 23.9 & 22.5 & 24.0 \\
\hdashline
Gemma-2-27b                         & 54.8 & 59.4 & 55.2 & 37.3 & 47.3 & \textbf{52.7} \\
Gemma-2-27b-IT                     & 57.0 & 60.3 & 54.6 & 39.1 & 48.3 & \textbf{54.0} \\
Gemma-2-9b                          & 50.8 & 57.4 & 53.8 & 36.2 & 42.6 & 50.2 \\
Gemma-2-9b-IT                    & 50.6 & 52.8 & 49.1 & 35.8 & 44.3 & 48.1 \\
\hdashline
issai-8b                            & 39.1 & 45.9 & 43.7 & 31.2 & 38.6 & 40.1 \\
\hdashline
Llama3.1-70b                        & 57.0 & 60.0 & 58.7 & 41.7 & 49.3 & \textbf{55.2} \\
Llama3.1-70b-instruct               & 50.6 & 47.5 & 47.9 & 33.9 & 45.6 & \textbf{46.5} \\
Llama3.1-8b                         & 36.6 & 43.8 & 42.4 & 28.9 & 38.3 & 38.0 \\
Llama3.1-8b-instruct                & 41.6 & 47.4 & 45.4 & 30.4 & 39.3 & 41.6 \\
\hdashline
mt0-large                           & 24.6 & 23.9 & 23.3 & 22.9 & 23.5 & 23.9 \\
mt0-xl                              & 27.9 & 35.5 & 28.9 & 26.5 & 31.9 & 29.5 \\
mt0-xxl                             & 30.0 & 38.9 & 32.0 & 29.5 & 37.9 & 32.3 \\
\hdashline
qwen-2.5-7b                         & 45.5 & 42.0 & 41.2 & 29.3 & 39.9 & 41.0 \\
qwen-2.5-7b-instruct                & 47.7 & 49.6 & 46.4 & 33.9 & 42.3 & 45.4 \\
\hdashline
GPT-4o                        & 69.7 & \textbf{83.1} & \textbf{81.7} & 73.4 & 62.1 & \textbf{75.7} \\
DeepSeek V3              & 78.7 & 82.3 & 77.0 & 61.2 & 65.1 & 75.6 \\
YandexGPT              & 53.7 & 69.8 & 52.2 & 42.6 & 57.0 & 54.9 \\
\hline
\end{tabular*}
\caption{Performance of different models on the Kazakh MMLU benchmark across different subject categories using \textbf{English} prompt.}
\label{tab:kazakh-mmlu}
\end{table*}


\paragraph{Results across all models}
Table ~\ref{tab:kazakh-mmlu} presents the average accuracy for each subject area across 26 models on the \datasetname{} using English prompts. The performance analysis reveals several notable patterns. GPT-4o and DeepSeek V3 emerge as the top performers, achieving remarkably similar average scores of 75.7\% and 75.6\% respectively, outperforming other models. 
Among the open-source models, the Llama3.1 and Gemma families demonstrated strong performance. Llama3.1-70b achieved the highest average score (55.2\%), followed by the Gemma-2-27b instruction-tuned model (54.0\%) and its base variant (52.7\%). Interestingly, the impact of instruction tuning varies across model families - while Gemma-2-27b-it showed a slight improvement over its base model (+1.3\%), Llama3.1-70b-instruct performed worse than its base variant (-8.7\%). 
% However, for smaller models, instruction tuning appears more beneficial, as seen in Llama3.1-8b-instruct outperforming its base model by 3.6\% (41.6\% vs 38.0\%).
Consistently across all models, the Language category seems to be the most challenging, with scores lower than other categories. 
% Models without Kazakh language exposure in pretraining, BLOOM family and MT0 models showed relatively modest performance, with averages ranging from 22.2\% to 32.3\%, further reinforcing the importance of linguistic diversity in training datasets. The performance gap between commercial and open-source models remains substantial, with a difference of over 20\% points in average accuracy.
% Notably, GPT4-o showed particular strength in Social Science (83.1\%) and Humanities (81.7\%), while DeepSeek excelled in STEM subjects (78.7\%). 


\paragraph{Few-Shot Performance} 
As shown in Figure~\ref{fig:few-shot}, our few-shot results show a consistent improvement across all models as the number of shots increases, with Qwen-2.5-7B and Mistral-7B-v0.3 benefiting the most.
% , with Qwen-2.5-7B improving from 38.5\% (0-shot) to 44.6\% (3-shot) for Kazakh prompts and from 42.5\% to 63.3\% for English prompts. 
English prompts consistently outperform Kazakh prompts in 1, 2, and 3-shot settings, though this trend does not hold in 0-shot, where Kazakh sometimes performs better. Instruction-tuned models also improve, though the gains are smaller, with Qwen-2.5-7B-Instruct (English prompt) increasing from 47.8\% (0-shot) to 58.9\% (3-shot). The largest accuracy jumps occur between 0-shot and 1-shot, indicating that even a single in-context example significantly enhances model understanding. Overall, these results highlight the robustness of few-shot learning across diverse model architectures and prompt settings.


\paragraph{Kazakh vs Russian Performance}
In Figure~\ref{fig:analysis_language}, we compared model performance across two languages: Kazakh and Russian. 
To ensure a fair comparison, we only considered the High School level subjects and excluded the Professional \& University level tasks because they are not available in Kazakh. The results indicate that GPT-4o achieves the highest accuracy in Kazakh, scoring 76.90\%, while DeekSeek performs best in Russian with an accuracy of 81.8\%. 
% This suggests that GPT-4o has stronger capabilities in Kazakh text understanding, whereas DeekSeek generalizes better to Russian-language tasks.
Llama 3.1-70B and Gemma 2-27B show lower but comparable results, with a slight advantage in Russian over Kazakh.
Overall, models tend to perform slightly better in Russian than in Kazakh, which could be due to differences in training data availability, language complexity, or tokenization differences.



\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{latex/img/few_shot.png}
\caption{The few-shot accuracy (\%) of LLMs on \datasetname{}, averaged across all tasks, comparing base models and instruction-tuned models using Kazakh (dotted lines) and English (solid lines) prompts.}
\label{fig:few-shot}
\end{figure}
 



\paragraph{Results Across Education Level}
Results in Figure~\ref{fig:analysis_level} indicate that GPT-4o performs approximately the same across both education levels. 
% with 77.04\% accuracy in High School and 75.68\% in Professional \& University. 
Similarly, DeepSeek V3 maintains a balanced performance, but with a slight preference towards High School.
% , scoring 77.0\% compared to 76.7\% in Professional \& University. 
In contrast, the open-source models Llama 3.1-70B and Gemma 2-27B show considerably lower accuracy and a pronounced gap between education levels. Llama 3.1-70B achieves 57.7\% in High School but drops to 53.2\% in Professional \& University. These results suggest that while proprietary models generalize well across different subject complexities, open-source models struggle more with specialized university-level knowledge.




% \paragraph{Impace of Negations in Russian vs English}
%Focus on High School subject



\paragraph{Negation Sensitivity Analysis} Table~\ref{tab:negation_analysis} presents the accuracy of LLaMA3-70B, Gemma-2-27B-IT, and DeepSeek V3 on \textit{negation-sensitive subjects}, comparing performance with and without negation. The results indicate that DeepSeek V3 consistently outperforms both LLaMA3-70B and Gemma-2-27B-IT, demonstrating greater resilience to negation-based reasoning challenges.

To systematically analyze the impact of negation, we employed a negation phrase filtering method inspired by ArabicMMLU \cite{koto-etal-2024-arabicmmlu}. Specifically, we identified questions containing common negation phrases in Kazakh: \begin{otherlanguage*}{kazakh}
жоқ (no), емес (is not), болмайды (not allowed), жарамайды (prohibited), невозможно (impossible), не (not), нельзя (forbidden). \end{otherlanguage*}. After applying this filtering, we obtained a total of 2,554 negation-related questions. To validate our filtering accuracy, we randomly sampled 100 questions and manually inspected them. The detection accuracy exceeded 92\%, confirming the reliability of our filtering process.

For LLaMA3-70B and Gemma-2-27B-IT, accuracy generally decreases on questions containing negation, suggesting a negative impact on reasoning capabilities. Notably, LLaMA3-70B exhibits a larger drop in accuracy, particularly in \textit{Reading Literacy}, where performance declines from 57.1\% to 50.0\%. Meanwhile, Gemma-2-27B-IT demonstrates greater robustness in certain cases, showing less fluctuation in performance across negation and non-negation settings.

% These findings suggest that DeepSeek V3 handles logical negation more effectively, while LLaMA3-70B and Gemma-2-27B-IT exhibit moderate sensitivity to negation-related reasoning. 
% Future work could explore whether syntactic structure, logical contradictions, or contextual cues influence negation robustness in multilingual models. 
% Further analysis is needed to understand how each model handles \textit{syntactic negation, logical contradictions, and factual reversals}.


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{latex/img/accuracy_by_language.png}
\caption{LLM Performance across different languages(only at the high-school level).}
\label{fig:analysis_language}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{latex/img/accuracy_by_education.png}
\caption{LLM Performance across different education levels}
\label{fig:analysis_level}
\end{figure}


\begin{table*}[t!]
\scriptsize
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{p{4cm}ccc}
        \toprule
        \textbf{Subject} & \textbf{Model} & \textbf{W/o Negation} & \textbf{W/ Negation} \\
        \midrule
        \multirow{3}{*}{\textit{Jurisprudence (University)}} & Llama3-70B & \textbf{56.2} & 55.2 \\
        & Gemma-2-27B-IT & 55.2 & \textbf{56.5} \\
        & DeepSeek V3 & \textbf{78.1} & 76.4 \\
        \midrule
        \multirow{3}{*}{\textit{Law (High School)}} & Llama3-70B & \textbf{60.8} & 59.0 \\
        & Gemma-2-27B-IT & 59.0 & \textbf{58.1} \\
        & DeepSeek V3 & \textbf{79.5} & 78.1 \\
        \midrule
        \multirow{3}{*}{\textit{Reading Literacy (High School)}} & Llama3-70B & 57.1 & \textbf{50.0} \\
        & Gemma-2-27B-IT & \textbf{100.0} & 87.5 \\
        & DeepSeek V3 & \textbf{85.7} & 83.5 \\
        \midrule
        \multirow{3}{*}{\textit{Philosophy and Psychology (University)}} & Llama3-70B & 55.9 & \textbf{56.6} \\
        & Gemma-2-27B-IT & \textbf{56.6} & 55.9 \\
        & DeepSeek V3 & \textbf{83.2} & 81.9 \\
        \bottomrule
    \end{tabular}}
    \caption{Model accuracy on negation-sensitive questions across various subjects. Bold values indicate higher accuracy in each category.}
    \label{tab:negation_analysis}
\end{table*}







\paragraph{Model Confidence}
We analyze whether the evaluated models, including Gemma-2.9B-IT, Qwen-2.5-7B-Instruct, and Llama3.1-70B, are well-calibrated when answering \datasetname{} questions by comparing the probability of the correct answers with the actual accuracy for each subject and level combination. The answer probability is obtained through softmax normalization across the available candidate answers. 
In Figure~\ref{fig:confidence-vs-accuracy}, we observe that the evaluated models exhibit a strong correlation between confidence and accuracy, with correlation scores exceeding 0.9. This indicates that models with higher confidence generally produce more accurate predictions. 

% However, some overconfidence is evident in certain cases, where high-confidence predictions do not always translate into correct answers.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{latex/img/image__12.png}
    \caption{Confidence vs. Accuracy for different models in a zero-shot setting. \textbf{Confidence (\%)} denotes the average probability scores in percentage.}
    \label{fig:confidence-vs-accuracy}
\end{figure}

 



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{latex/img/image__14.png}
    \caption{Correlation between model confidence and question length across different models.}
    \label{fig:confidence-length}
\end{figure}

Additionally, we investigate the correlation between model confidence and question length in Figure~\ref{fig:confidence-length}. The results show that question length has minimal impact on model confidence, as evidenced by the weak correlation scores across all evaluated models.
For Qwen-2.5-7B-Instruct ($r = 0.29$) and Llama3.1-70B ($r = 0.28$) exhibit a mild positive correlation, suggesting that \textbf{longer questions slightly increase model confidence}. However, the effect remains weak overall, implying that confidence calibration remains relatively stable across different question lengths.
% These findings indicate that question complexity (as measured by length) does not strongly influence model confidence, making the evaluated models robust to variations in input length. 
% Future work may explore whether linguistic complexity, rather than just character length, has a greater effect on confidence calibration and model uncertainty, particularly for low-confidence predictions.



\section{Conclusion and Future Work} %[Nurdaulet, Mukhammed, Zhouhan]
We introduced \datasetname{}, the first large-scale multi-task language understanding dataset designed to evaluate real-world knowledge in Kazakhstan's bilingual setting. Through experiments with over 23K multiple-choice questions spanning various subjects and education levels, we observed that models perform much better in Russian than in Kazakh, with proprietary models such as GPT-4o and DeepSeek V3 achieving the highest accuracy. 

\datasetname{} provides a \textbf{bilingual} (Kazakh and Russian) evaluation framework tailored to Kazakhstan's educational and professional landscape, distinguishing itself from previous multilingual benchmarks. Unlike SIGTURK, which focused on Kazakh NLP tasks, and INCLUDE, which lacks Kazakhstan-specific content, \datasetname{} enables a localized and comprehensive assessment of LLMs in Kazakhstan. 

Future research directions include extending \datasetname{} to multimodal evaluations, improving reasoning-based question assessments, and mitigating biases in data sources. We hope that this benchmark will encourage further development of high-performance LLMs for Kazakh and other low-resource languages.

\newpage

\section*{Limitations} %[Maiya, Diana, Jonibek, Revised by Mukhammed]
While we are confident that our benchmark will significantly advance the development of Kazakh LLMs, it is important to acknowledge certain limitations that need to be addressed in future research.

While we are confident that our benchmark will significantly advance the development of Kazakh LLMs, it is important to acknowledge certain limitations that need to be addressed in future research. We outline these limitations as follows:

\paragraph{Limited Modality} \datasetname{} is focused solely on text-based assessment, and the exploration of multimodal questions (including those involving images, audio, or other media types) has been excluded. Future work could explore the integration of multimodal content to better reflect real-world applications, such as vision-language tasks, speech recognition, and interactive assessments.

\paragraph{Lack of Explicit Reasoning Evaluation} While \datasetname{} provides a broad and representative set of multiple-choice questions, it does not explicitly evaluate reasoning processes beyond answer selection. Investigating how models approach complex reasoning, justification, and open-ended question answering would be a valuable direction for further improvement.

\paragraph{Static Evaluation Limitation} \datasetname{} primarily evaluates static model performance on pre-defined questions, which may not fully capture how models generalize to dynamic, real-world language use. Exploring benchmarks that assess interactive and adaptive reasoning, as well as domain adaptation, could enhance our understanding of model capabilities in evolving contexts.

By addressing these limitations, future research can further refine the evaluation of Kazakh LLMs, ensuring more robust, fair, and practically useful language models for Kazakhstan and beyond.

\section*{Ethics and Broader Impact}

We adhered to the internal policies of web resources while scraping data and included only publicly available information verified by authorities.

All human subjects in our study provided informed consent, were fully aware of the study's objectives, and had the right to withdraw at any time. They were also appropriately compensated as part of their job.

%\section*{Acknowledgments}
%
%This document has been adapted
%by Steven Bethard, Ryan Cotterell and Rui Yan
%from the instructions for earlier ACL and NAACL proceedings, including those for
%ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
%NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
%ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
%NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
%Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
%ACL 2017 by Dan Gildea and Min-Yen Kan,
%NAACL 2017 by Margaret Mitchell,
%ACL 2012 by Maggie Li and Michael White,
%ACL 2010 by Jing-Shin Chang and Philipp Koehn,
%ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
%ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
%ACL 2002 by Eugene Charniak and Dekang Lin,
%and earlier ACL and EACL formats written by several people, including
%John Chen, Henry S. Thompson and Donald Walker.
%Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.










% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

\bibliography{acl_latex}


\clearpage
\appendix
\section{Appendix}
\label{sec:appendix}

\subsection{Additional Prompt Examples}
Figure~\ref{fig:prompt-examples} illustrates the Kazakh and English prompts used in our evaluation. The formatting emphasizes key aspects such as the subject (e.g., \textbf{Law}) and the educational level (e.g., \textbf{High School}). The structure maintains consistency in question presentation, ensuring uniformity across both languages.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\columnwidth]{latex/img/image7.png}
    \caption{Examples of Kazakh and English prompts. The placeholders are dynamically replaced based on the question context.}
    \label{fig:prompt-examples}
\end{figure}

\section{Appendix B:}
\label{sec:appendix-b}

\subsection{Additional Multiple-Choice Question Example}
In addition to the Kazakh-language question provided in the main document, we present an example of a Russian-language multiple-choice question (Figure~\ref{fig:russian-mcq}). This figure highlights a range of subjects from social sciences to STEM disciplines, demonstrating the dataset’s diversity.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{latex/img/image__10.png}
\caption{Example of a Russian-language multiple-choice question from \datasetname{}. The \textbf{left} side shows the original text, while the \textbf{right} side provides the English translation for illustrative purposes. The bold options represent the correct answer keys.}
\label{fig:russian-mcq}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{latex/img/image__15.png}
    \caption{Correlation between model confidence and question length}
    \label{fig:confidence-length}
\end{figure}

\subsection{Breakdown of Dataset Sources}
Table~\ref{tab:grouped_dataset_sources} provides a categorized breakdown of dataset sources used in \datasetname{}, covering national exams, professional certification tests, and textbooks.

\subsection{Additional Evaluation Details}
For further insights into our evaluation framework, we provide additional examples and implementation details. Table~\ref{tab:kazakh-mmlu} summarizes our experimental settings.

\begin{table*}[h]
\centering
\renewcommand{\arraystretch}{1.9} % Increase row height for better readability
\setlength{\tabcolsep}{5pt} % Adjust column spacing
\footnotesize
\rowcolors{2}{gray!15}{white} % Alternating row colors

\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{2.5cm}p{2cm}p{2cm}X}
\toprule
\rowcolor{gray!30} % Header row color
\textbf{Main Source} & \textbf{Language} & \textbf{Level} & \textbf{Subjects} \\
\midrule
\textbf{itest.kz} & Kazakh & High School &Biology, Chemistry, Geography, Informatics, Kazakh History, Kazakh Literature, Law, Math, Math Literacy, Physics, Reading Literacy, Russian Language, Russian Literature, World History \\
\textbf{oltest.kz} & Russian & University and Professional & Accounting and Auditing, Biology, Culture and Art, Economics and Entrepreneurship, Education and Training, Finance, General Education Disciplines, Jurisprudence, Management and Marketing, Medicine, Philosophy and Psychology, Social Science \\
\textbf{ymnik.kz} & Russian & High School & Biology, Geography, Kazakh History, Kazakh Language, World History \\
\textbf{Book - Shyn Kitap} & Kazakh & High School & Biology, Geography, Kazakh History, Kazakh Language, World History \\
\bottomrule
\end{tabularx}

\caption{Breakdown of dataset sources in \datasetname{}, categorized by domain and subject area. The alternating row colors improve readability.}
\label{tab:grouped_dataset_sources}
\end{table*}


\begin{table*}[!ht]
\setlength{\tabcolsep}{5pt}
\footnotesize
\centering
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccccc}
\hline
\textbf{Model} & \textbf{STEM} & \textbf{Social Science} & \textbf{Humanities} & \textbf{Language} & \textbf{Other} & \textbf{Average} \\
\hline
Mistral-7B-Instruct-v0.3  & 33.3 & 41.2 & 37.6 & 28.0 & 31.9 & 34.9 \\
Mistral-7B-v0.3           & 28.7 & 29.2 & 31.2 & 24.6 & 25.2 & 28.6 \\
\hdashline
Vikhr-Nemo-12B-Instruct-R-21-09-24 & 38.3 & 52.0 & 48.5 & 32.8 & 41.9 & 42.3 \\
\hdashline
aya-23-35B                 & 29.8 & 32.6 & 34.4 & 25.8 & 26.5 & 30.6 \\
aya-23-8B                  & 26.8 & 26.9 & 27.3 & 23.2 & 22.8 & 26.2 \\
\hdashline
bloom-1b1                  & 24.3 & 20.0 & 20.7 & 22.9 & 19.1 & 22.4 \\
bloomz-1b7                 & 23.3 & 21.1 & 21.1 & 23.3 & 20.1 & 22.3 \\
bloomz-3b                  & 23.7 & 21.4 & 20.9 & 21.9 & 17.8 & 22.3 \\
bloomz-7b                  & 22.7 & 21.0 & 22.5 & 21.1 & 21.5 & 22.0 \\
\hdashline
gemma-2-27b                & 57.2 & 64.4 & 59.8 & 41.1 & 46.6 & 56.3 \\
gemma-2-27b-it             & 58.7 & 63.1 & 57.1 & 40.9 & 46.3 & 56.1 \\
gemma-2-9b                 & 47.3 & 54.6 & 52.4 & 33.6 & 39.3 & 47.4 \\
gemma-2-9b-it              & 50.5 & 51.9 & 48.3 & 35.2 & 41.9 & 47.6 \\
\hdashline
issai-8b                   & 36.0 & 39.6 & 39.5 & 28.4 & 33.9 & 36.1 \\
\hdashline
Llama3.1-70b               & 57.9 & 65.8 & 64.0 & 44.4 & 50.3 & 58.4 \\
Llama3.1-70b-instruct      & 56.3 & 57.4 & 55.5 & 39.5 & 50.3 & 53.4 \\
Llama3.1-8b                & 33.7 & 38.4 & 39.0 & 28.0 & 31.5 & 34.7 \\
Llama3.1-8b-instruct       & 36.9 & 39.5 & 39.9 & 28.0 & 31.5 & 36.4 \\
\hdashline
mt0-large                  & 24.5 & 24.8 & 23.1 & 22.9 & 24.8 & 24.0 \\
mt0-xl                     & 28.4 & 37.1 & 29.8 & 26.8 & 34.6 & 30.3 \\
mt0-xxl                    & 28.8 & 37.2 & 31.1 & 29.3 & 36.2 & 31.2 \\
\hdashline
qwen-2.5-7b                & 46.5 & 48.2 & 44.4 & 30.5 & 42.3 & 43.6 \\
qwen-2.5-7b-instruct       & 47.4 & 50.6 & 47.1 & 33.2 & 43.3 & 45.5 \\
\hdashline
GPT4-o                        & 69.7 & 83.1 & 81.7 & 73.4 & 62.1 & 75.7 \\
DeepSeek V3              & 78.5 & 81.8 & 77.1 & 61.4 & 65.3 & 72.8 \\
YandexGPT              & 39.3 & 50.7 & 48.1 & 44.0 & 34.6 & 44.4 \\
\hline
\end{tabular*}
\caption{Performance of different models on the Kazakh MMLU benchmark across different subject categories using \textbf{Kazakh} prompt.}
\label{tab:kazakh-mmlu-kaz}
\end{table*}



\end{document}
