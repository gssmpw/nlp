@software{anthropic_claude,
  author = {Anthropic},
  title = {Claude},
  year = {2023},
  url = {https://www.anthropic.com/claude},
  note = {Accessed: 2025-02-10}
}

@article{cohere2024globalmmlu,
  title={Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation},
  author={Singh, Shivalika and Romanou, Angelika and Fourrier, Cl{\'e}mentine and Adelani, David I and Ngui, Jian Gang and Vila-Suero, Daniel and Limkonchotiwat, Peerat and Marchisio, Kelly and Leong, Wei Qi and Susanto, Yosephine and others},
  journal={arXiv preprint arXiv:2412.03304},
  year={2024}
}

@article{conneau2020unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:2007.13135},
  year={2020}
}

@misc{fenogenova2024meracomprehensivellmevaluation,
      title={MERA: A Comprehensive LLM Evaluation in Russian}, 
      author={Alena Fenogenova and Artem Chervyakov and Nikita Martynov and Anastasia Kozlova and Maria Tikhonova and Albina Akhmetgareeva and Anton Emelyanov and Denis Shevelev and Pavel Lebedev and Leonid Sinev and Ulyana Isaeva and Katerina Kolomeytseva and Daniil Moskovskiy and Elizaveta Goncharova and Nikita Savushkin and Polina Mikhailova and Denis Dimitrov and Alexander Panchenko and Sergei Markov},
      year={2024},
      eprint={2401.04531},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.04531}, 
}

@misc{hendrycks2021measuringmassivemultitasklanguage,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@inproceedings{hu2020xtreme,
  author    = {Junjie Hu and others},
  title     = {XTREME: A Massively Multilingual Benchmark for Evaluating Cross-Lingual Generalisation},
  booktitle = {Proceedings of ICML},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.11080}
}

@misc{issai_kazllm,
  author = {ISSAI},
  title = {LLama-3.1-KazLLM-1.0-8B},
  year = {2024},
  url = {https://huggingface.co/issai/LLama-3.1-KazLLM-1.0-8B},
  note = {Accessed: 2025-02-10}
}

@inproceedings{koto-etal-2023-large,
    title = "Large Language Models Only Pass Primary School Exams in {I}ndonesia: A Comprehensive Test on {I}ndo{MMLU}",
    author = "Koto, Fajri  and
      Aisyah, Nurul  and
      Li, Haonan  and
      Baldwin, Timothy",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.760/",
    doi = "10.18653/v1/2023.emnlp-main.760",
    pages = "12359--12374",
    abstract = "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46{\%} of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels."
}

@inproceedings{koto-etal-2024-arabicmmlu,
    title = "{A}rabic{MMLU}: Assessing Massive Multitask Language Understanding in {A}rabic",
    author = "Koto, Fajri  and
      Li, Haonan  and
      Shatnawi, Sara  and
      Doughman, Jad  and
      Sadallah, Abdelrahman  and
      Alraeesi, Aisha  and
      Almubarak, Khalid  and
      Alyafeai, Zaid  and
      Sengupta, Neha  and
      Shehata, Shady  and
      Habash, Nizar  and
      Nakov, Preslav  and
      Baldwin, Timothy",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.334/",
    doi = "10.18653/v1/2024.findings-acl.334",
    pages = "5622--5640",
    abstract = "The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding benchmark for the Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA) and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and Falcon struggle to achieve a score of 50{\%}, while even the top-performing Arabic-centric model only achieves a score of 62.3{\%}."
}

@misc{li2024cmmlumeasuringmassivemultitask,
      title={CMMLU: Measuring massive multitask language understanding in Chinese}, 
      author={Haonan Li and Yixuan Zhang and Fajri Koto and Yifei Yang and Hai Zhao and Yeyun Gong and Nan Duan and Timothy Baldwin},
      year={2024},
      eprint={2306.09212},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09212}, 
}

@inproceedings{liang2020xfglue,
  author    = {Hongyu Liang and others},
  title     = {XFGLUE: A Benchmark Dataset for Cross-Lingual Pretraining, Understanding and Generation},
  booktitle = {Proceedings of NeurIPS},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.11856}
}

@inproceedings{maxutov2024sigturk,
  author    = {Maxutov and others},
  title     = {SIGTURK 2024: Do LLMs Speak Kazakh? A Pilot Evaluation of Seven Models},
  booktitle = {Proceedings of SIGTURK 2024},
  year      = {2024},
  url       = {https://aclanthology.org/2024.sigturk-1.8/}
}

@misc{nikolich2024vikhrconstructingstateoftheartbilingual,
      title={Vikhr: Constructing a State-of-the-art Bilingual Open-Source Instruction-Following Large Language Model for Russian}, 
      author={Aleksandr Nikolich and Konstantin Korolev and Sergei Bratchikov and Igor Kiselev and Artem Shelmanov},
      year={2024},
      eprint={2405.13929},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.13929}, 
}

@misc{openai2024gpt4o,
  author       = "{OpenAI}",
  title        = "{GPT-4o}",
  year         = 2024,
  url          = "https://openai.com/index/hello-gpt-4o/",
  note         = "Accessed: 2024-11-29"
}

@inproceedings{ponti2021xcopa,
  author    = {Junjie Hu and others},
  title     = {XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning},
  booktitle = {Proceedings of EMNLP 2021},
  year      = {2021},
  url       = {https://aclanthology.org/2021.emnlp-main.98}
}

@misc{romanou2024include,
  author    = {Angelika Romanou and Negar Foroutan and Anna Sotnikova and Zeming Chen and Sree Harsha Nelaturu and Shivalika Singh and Rishabh Maheshwary and Micol Altomare and Mohamed A. Haggag and Imanol Schlag and others},
  title     = {INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge},
  year      = {2024},
  eprint    = {2411.19799},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2411.19799}
}

@inproceedings{ustun-etal-2024-aya,
    title = "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
    author = {{\"U}st{\"u}n, Ahmet  and
      Aryabumi, Viraat  and
      Yong, Zheng  and
      Ko, Wei-Yin  and
      D{'}souza, Daniel  and
      Onilude, Gbemileke  and
      Bhandari, Neel  and
      Singh, Shivalika  and
      Ooi, Hui-Lee  and
      Kayid, Amr  and
      Vargus, Freddie  and
      Blunsom, Phil  and
      Longpre, Shayne  and
      Muennighoff, Niklas  and
      Fadaee, Marzieh  and
      Kreutzer, Julia  and
      Hooker, Sara},
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.845",
    doi = "10.18653/v1/2024.acl-long.845",
    pages = "15894--15939",
    abstract = "Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50{\%} are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages {---}{---} including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.",
}

@inproceedings{wang2023all,
  title={All Languages Matter: On the Multilingual Safety of Large Language Models},
  author={Wang, Chang and Zhang, Wanjun and Li, Lei and Song, Chenguang and Zhang, Zhihong and Zhou, Ming},
  booktitle={Findings of EMNLP},
  year={2023}
}

@software{yandex_yandexgpt,
  author = {Yandex},
  title = {YandexGPT},
  year = {2023},
  url = {https://yandex.cloud/en/services/yandexgpt},
  note = {Accessed: 2025-02-10}
}

@inproceedings{yeshpanov-etal-2022-kaznerd,
    title = "{K}az{NERD}: {K}azakh Named Entity Recognition Dataset",
    author = "Yeshpanov, Rustem  and
      Khassanov, Yerbolat  and
      Varol, Huseyin Atakan",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.44",
    pages = "417--426",
    abstract = "We present the development of a dataset for Kazakh named entity recognition. The dataset was built as there is a clear need for publicly available annotated corpora in Kazakh, as well as annotation guidelines containing straightforward{---}but rigorous{---}rules and examples. The dataset annotation, based on the IOB2 scheme, was carried out on television news text by two native Kazakh speakers under the supervision of the first author. The resulting dataset contains 112,702 sentences and 136,333 annotations for 25 entity classes. State-of-the-art machine learning models to automatise Kazakh named entity recognition were also built, with the best-performing model achieving an exact match F1-score of 97.22{\%} on the test set. The annotated dataset, guidelines, and codes used to train the models are freely available for download under the CC BY 4.0 licence from https://github.com/IS2AI/KazNERD.",
}

@misc{yeshpanov2024kazqadkazakhopendomainquestion,
      title={KazQAD: Kazakh Open-Domain Question Answering Dataset}, 
      author={Rustem Yeshpanov and Pavel Efimov and Leonid Boytsov and Ardak Shalkarbayuli and Pavel Braslavski},
      year={2024},
      eprint={2404.04487},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.04487}, 
}

@misc{yeshpanov2024kazsandra,
      title={KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes}, 
      author={Rustem Yeshpanov and Huseyin Atakan Varol},
      year={2024},
      eprint={2403.19335},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

