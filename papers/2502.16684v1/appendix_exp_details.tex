\label{sec:appendix_exp_details}

\subsection{Document Classifier}
\label{sec:appendix_doc_classifier}
We trained a random forest classifier on semantic features extracted from a small language model. The classifier was trained on annotations of 20,000 long documents from SlimPajama, achieving 90\% accuracy on a held-out test set. 
Specifically, we annotated $20,000$ long documents sampled from SlimPajama using {\tt GPT-4}. The annotation prompt explicitly required the output to match one of the predefined document types, ensuring consistency with the categories defined during meta information clustering. Using these annotations, we trained a random forest classifier on semantic features extracted with StableLM-2-1.6B \citep{bellagente2024stablelm}, where the mean of the last layer's hidden states was used as the feature representation. The classifier achieved 90\% accuracy on a held-out test set, enabling efficient and accurate predictions of document types for unseen SlimPajama data.

\subsection{Instruction Generation with Paths}
\label{sec:appendix_demo}
To synthesize instructions aligned with sampled meta information paths, we prompt {\tt GPT-4} with a one-shot demonstration derived from seed paths extracted from the WildChat long conversations. 
Each seed path includes all meta information fields and a corresponding simplified instruction.
Given a sampled path $\hat{P}$, we identify the most similar seed path $P^*$ based on the of their nodes. The similarity between paths is computed as $\text{intersection\_sim}(\hat{P}, P^*) = | \hat{P} \cap P^* |.$ The selected example path and its instruction are included in the prompt to guide {\tt GPT-4} in generating a new instruction given a new path. This ensures the generated instruction aligns with the sampled meta information criteria, while benefiting from the contextual relevance provided by the seed example.
{\tt GPT-4} synthesizes a natural language instructions adhering to the sampled path's constraints with the prompt shwon in Table \ref{tab:prompt_path_to_instruct}.

\subsection{Evaluation Settings}
\label{sec:appendix_eval_settings}
% On long-context benchmarks, we evaluate our models and all baselines following the settings in the original benchmarks. 
For short-context evaluation, we utilize the \texttt{lm-evaluaton-harness} framework \cite{eval-harness} and following the evaluation settings in \citep{open-llm-leaderboard}: 25-shots for ARC-C, and 5-shots for MMLU, Winogrande and GSM8K. We use 0-shot for IFEval. We report the \texttt{acc\_norm} metric for ARC-C, the \texttt{acc} metric for MMLU, Winogrande and GSM8K. We average the metrics \texttt{prompt\_level\_strict\_acc}, \texttt{inst\_level\_strict\_acc}, \texttt{prompt\_level\_loose\_acc}, and \texttt{inst\_level\_loose\_acc} for IFEval. 

For long-context evaluations, we evaluate our models and all baselines following the settings in the original benchmarks. Table \ref{tab:appendix_model_info}  presents the sources of evaluation results for the models across three benchmarks.

\begin{table}[h]
\caption{Evaluation source for each model on three benchmarks. 
    \cmark\ indicates that the evaluation was conducted by ourselves, while \ostar\ indicates that results were sourced from the original benchmark. 
}
\centering
\scalebox{0.8}{
\begin{tabular}{l|ccc}
\toprule
\textbf{Models} & \textbf{RULER} & \textbf{HELMET} & \textbf{Longbench-Chat} \\
\midrule
\rowcolor{gray!20}
\multicolumn{4}{c}{\it{Proprietary Long-Context Models}}\\
\midrule
Gemini-1.5-Pro   & \ostar  & \ostar  & \cmark  \\
GPT-4            & \ostar  & \ostar  & \ostar  \\
\midrule
\rowcolor{gray!20}
\multicolumn{4}{c}{\it{Open-Sourced Pretrained Long-Context Models}}\\
\midrule
GLM-4-1M         & \ostar  & \cmark  & \cmark  \\
Yi-200k          & \ostar  & \ostar  & \cmark  \\
Llama-3.1-70B    & \ostar  & \ostar  & \cmark  \\
Phi-3-medium     & \ostar  & \ostar  & \cmark  \\
Qwen2.5          & \cmark  & \cmark  & \cmark  \\
Mistral-7B       & \cmark  & \cmark  & \cmark  \\
Llama-3.1-8B     & \cmark  & \cmark  & \cmark  \\
\midrule            
\rowcolor{gray!20}
\multicolumn{4}{c}{\it{Specialized Long-Context Optimized Models}}\\
\midrule
FILM             & \cmark  & \cmark  & \cmark  \\
ProLong-512k     & \cmark  & \cmark  & \cmark  \\
ChatQA-2         & \cmark  & \cmark  & \cmark  \\
SEALONG          & \cmark  & \cmark  & \cmark  \\
\bottomrule
\end{tabular}
}
\label{tab:appendix_model_info}
\end{table}

\subsection{Technical Details}
We employ several open-source libraries and tools for model training. Specifically, we use PyTorch \citep{paszke2019pytorch} and the Hugging Face Transformers library \citep{wolf2019huggingface} for implementing and fine-tuning the model. To enhance computational efficiency, we integrate FlashAttention 2 \citep{dao2023flashattention2} for optimized attention computation. The fine-tuning process is conducted on eight AMD Radeon Instinct MI300 GPUs, each equipped with 192GB of memory. Training on 150K synthetic data samples requires approximately 480 GPU hours.
% \subsection{Evaluation Settings}
