% resource intensive - annotation cost (requires GPT annotation resources)
While WildLong advances synthetic data generation for long-context tasks, several limitations warrant consideration. First, although the framework mimics ``realistic'' instruction-response pairs, synthetic data may lack the nuanced complexity, ambiguity, or cultural specificity inherent in human-generated interactions. This could limit the model’s ability to handle edge cases or interpret context-dependent subtleties in real-world scenarios. Second, biases embedded in the source meta-information extracted from real user queries—such as language preferences, cultural assumptions, or domain-specific imbalances—risk propagating into the generated dataset, potentially reinforcing societal or structural inequities. Finally, while graph-based modeling captures co-occurrence relationships between entities, it may oversimplify semantic or causal dependencies, leading to superficial multi-document reasoning. %For instance, cross-document synthesis tasks requiring resolution of contradictions or integration of conflicting information might be inadequately represented, reducing the framework’s effectiveness in highly complex, real-world applications. 
These limitations highlight the need for hybrid data pipelines combining synthetic and human-curated examples, alongside rigorous bias audits, to enhance robustness.

% limited to single-turn, length
% We limit context length to 32k for single-doc and 40k for multi-doc settings due to the annotation cost and time constraints with GPT-4, leaving longer-context annotation for future work. 
% Our approach relies on extracting meta-information from the realistic WildChat dataset, which we assume approximates real user needs. However, exploring other real-world data sources or starting without seed data could further strengthen our approach. 
% Finally, we focus on single-turn conversations and instruction-response pairs, as multi-turn dialogues require significantly more annotation resources. Thus, we leave multi-turn exploration for future work. 

% only explore SFT 
% future direction
% Recent approaches like LongPO \citep{chen2025longpo} and SEALONG \citep{li2024sealong} have shown that LLMs can self-improve on long-context tasks, particularly in contextual QA. LongPO extends short-context capabilities to long contexts through self-generated preference data, while SEALONG uses multiple output sampling and preference optimization to refine model responses. However, these methods focus primarily on QA tasks and do not address the broader range of challenges requiring full-context reasoning. Our approach, WildLong, is orthogonal to these methods, offering a scalable way to generate generalized data for diverse long-context tasks. 
% Preference optimization is a promising direction for future work, and WildLong can be viewed as an initial step, offering insights into how generalized data might be produced for more complex long-context reasoning.


% harmful content, deduplication