% \textbf{Long-context Extending of LLMs.}
% % Positional encoding
% Many works explore extending the LM context windows with minimal training, either by position extrapolation \citep{chen2023extendingcontextwindowlarge,pengyarn, su2021roformer, dinglongrope, chen2024clex, liu-etal-2024-e2, zhu2024pose, wu2024skipalign, longrecipe} or manipulating attention mechanisms to extend context length \citep{jin2024selfextend, xiao2024sink, xiao2024infllm, ding2023longnet, antraining, an2025why}.
% Some works innovate architectures for efficient long-context processing \citep{lieber2024jamba, bertsch2024unlimiformer, wang2024augmenting, yen2024long}.
% Several approaches have been proposed to equip large language models (LLMs) with long-context capabilities through dataset construction, fine-tuning, and alignment techniques.
% Llama3.1 (\cite{dubey2024llama}) employs extensive continued pretraining on a corpus of 800B tokens, complemented by fine-tuning on a small fraction of long-context data. Similarly, GLM (\cite{glm2024chatglm}) relies on human-annotated long-context data for supervised fine-tuning (SFT) and DPO. While effective, these methods demand significant resources or human annotation, limiting scalability.
% To address this, synthetic long-context datasets have been explored. \cite{an2024make} synthesizes QA for short context and concatenates short contexts to form long contexts, while \cite{zhao2024longskywork} synthesizes long tables to improve long-context reasoning. \cite{xu2024chatqa} uses NarrativeQA to construct long contexts by combining semantically related paragraphs, offering task-specific solutions.
% Structured approaches have targeted specific long-context tasks. \cite{chen2024essential} model document correlations to curate multi-hop datasets and generate QA pairs from intra-document data. \cite{bai2024longalign} leverage Self-Instruct to create long-context instruction datasets but restrict prompts to four task types, limiting task generalization.
% \cite{artificialneedles} trains the model on synthetic key-value retrieval data to improve multi-document reasoning.  
% While these methods achieve notable results, they often focus on narrow objectives or rely on resource-intensive processes, highlighting the need for scalable, diverse, and realistic long-context data synthesis.

% % Llama3.1 (\cite{dubey2024llama}) achieves long-context capabilities by extensive continued pretraining on a massive long-context corpus of around 800B tokens and incorporating a small fraction of long context data for supervised fine-tuning. 
% % \cite{xu2024chatqa} uses constructs SFT data with NarrativeQA where they construct long context by combining multiple semantically related paragraphs and QA pairs about one paragraph.
% % GLM (\cite{glm2024chatglm}) utilizes human-annotated long-context data for SFT and DPO to align long-context LLMs. Despite their effectiveness, these models require either extensive training or human annotation of long-context data, which might lack scalability. 
% % \cite{bai2024longalign} constructs long instruction-following dataset using Self-Instruct while only apply prompts that target four types of tasks. 
% % \cite{an2024make} constructs long SFT dataset by synthesizing QA pairs for short contexts and randomly concatenating them to form a long context. 
% % \cite{zhao2024longskywork} synthesizes long tables for enhancing long context reasoning capabilities
% % \cite{chen2024essential} curates long-context multi-hop data by modeling documents correlation, and create multi-hop QA pairs using intra-document data. 

% \textbf{Scaling synthetic data creation.}
% Previous work on synthetic data creation for alignment has focused on leveraging human interactions with LLMs \citep{DatabricksBlog2023DollyV2, zhao2024wildchat, zheng2024lmsyschatm, pf2023openassistant}. However, manually crafting instructions is time-consuming and labor-intensive. Recent approaches have scaled instruction datasets by prompting LLMs to generate synthetic instructions, starting with a small set of human-annotated seed instructions \citep{yu2023metamath, wang2023selfinstrct, taori2023stanfordalapca, xu2024wizardlm, sun2024principle}.
% Keypoints-driven methods diversify the prompts by incorporating topics, subjects, or knowledge areas \citep{li2024syntheticdataalmostscratch, tangmathscale, huang2024key}. PersonaHub \citep{ge2024personahub} scales synthetic data by extracting billions of personas as knowledge carriers.
% Our approach is closely related to keypoints-driven methods, as we start with real-world conversation data and extract meta-information to guide instruction creation. Our work distinguishes itself by targeting long-context instruction synthesis, where the instructions are closely tied to the long context itself. This involves generating diverse and realistic instructions while ensuring alignment and relevance to the document type. 
\textbf{Long-context Extending of LLMs.}
Several methods attempt to extend context windows with minimal training overhead. Position extrapolation approaches (\cite{chen2023extendingcontextwindowlarge, pengyarn, su2021roformer, dinglongrope, chen2024clex, liu-etal-2024-e2, zhu2024pose, wu2024skipalign, longrecipe}) adjust positional embeddings or apply rope-based techniques to accommodate longer inputs. Others manipulate attention mechanisms to scale context length (\cite{jin2024selfextend, xiao2024sink, xiao2024infllm, ding2023longnet, antraining, an2025why}), ensuring model capacity for extended sequences without complete retraining.
A separate line of work focuses on novel architectures designed for efficient long-context modeling. These include methods like Jamba (\cite{lieber2024jamba}), Unlimiformer (\cite{bertsch2024unlimiformer}), and other enhancements (\cite{wang2024augmenting, yen2024long}) to handle large inputs without quadratic complexity scaling.
Some methods rely on significant resources to equip LLMs with long-context capabilities. Llama3.1 (\cite{dubey2024llama}) conducts extensive continued pretraining on 800B tokens plus targeted fine-tuning on long-context data, and GLM (\cite{glm2024chatglm}) uses human-annotated datasets for supervised fine-tuning and DPO. While effective, these strategies can be labor-intensive or costly.
To mitigate data constraints, synthetic long-context datasets have been explored. For instance, \cite{an2024make} synthesizes QA for short context and concatenates short contexts to form long contexts, while \cite{zhao2024longskywork} synthesizes long tables to improve long-context reasoning. \cite{xu2024chatqa} uses NarrativeQA to construct long contexts by combining semantically related paragraphs, offering task-specific solutions.
Structured approaches have targeted specific long-context tasks. \cite{chen2024essential} model document correlations to curate multi-hop datasets and generate QA pairs from intra-document data. \cite{bai2024longalign} leverage Self-Instruct to create long-context instruction datasets but restrict prompts to four task types, limiting task generalization.
\cite{artificialneedles} trains the model on synthetic key-value retrieval data to improve multi-document reasoning. These methods show promise but often remain narrow in focus or require substantial manual or computational effort.
Recent approaches like LongPO \citep{chen2025longpo} and SEALONG \citep{li2024sealong} have shown that LLMs can self-improve on long-context tasks, particularly in contextual QA. LongPO extends short-context capabilities to long contexts through self-generated preference data, while SEALONG uses multiple output sampling and preference optimization to refine model responses. However, these methods focus primarily on QA tasks and do not address the broader range of challenges requiring full-context reasoning. Our approach, WildLong, is orthogonal to these methods, offering a scalable way to generate generalized data for diverse long-context tasks. 
% Preference optimization is a promising direction for future work, and WildLong can be viewed as an initial step, offering insights into how generalized data might be produced for more complex long-context reasoning.

\textbf{Scaling synthetic data creation}
Previous work on synthetic data creation for alignment has focused on leveraging human interactions with LLMs \citep{DatabricksBlog2023DollyV2, zhao2024wildchat, zheng2024lmsyschatm, pf2023openassistant}. However, manually crafting instructions is time-consuming and labor-intensive. Recent approaches have scaled instruction datasets by prompting LLMs to generate synthetic instructions, starting with a small set of human-annotated seed instructions \citep{yu2023metamath, wang2023selfinstrct, taori2023stanfordalapca, xu2024wizardlm, sun2024principle}.
Keypoints-driven strategies (\cite{li2024syntheticdataalmostscratch, tangmathscale, huang2024key}) enrich prompts with diverse topics or knowledge bases. PersonaHub \cite{ge2024personahub} introduces billions of personas to maximize coverage.
We follow this keypoints-driven philosophy but focus on long-context data synthesis, extracting meta-information from real-world conversations to generate diverse, realistic instructions closely tied to document context. By integrating document typeâ€“specific details, our framework provides a scalable route for creating high-quality long-context training data without excessive manual overhead.
