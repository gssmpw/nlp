% % Context and Background (long context LLM)
% LLMs with long context windows excel in tasks like information extraction, QA, and planning.
% Research emphasizes extending context windows.
% Expanding context size alone is insufficient for practical use.
% Need for optimizing long-context utilization, especially in instruction tuning.

% % Context and Background (long context instruction tuning)
% Long-context instruction tuning requires high-quality data, which is costly and difficult to obtain.
% Self-Instruct framework (Xiong et al., 2023; Bai et al., 2024a) addresses data scarcity by using LLMs for generation.
% Current methods focus on task-specific objectives (e.g., QA, summarization) with limited task coverage.
% QA and summarization data are often synthesized by prompting GPT but lack transferability to other tasks like RAG or in-context learning.

% % Problem Statement
% Existing approaches struggle to generalize due to limited focus on synthesis processes and data effectiveness.
% This paper aims to synthesize diverse, scalable, and generalized long-context instruction-following data.

% % Motivation and Significance (challenge -> motivation -> significance)
% Challenges: 
% 1. Long context instruction tuning data is scarce and hard to obtain. 
% 2. Existing approaches mainly focus on long document QA and summarization to synthesize data, which does not generalize well. Lack of realistic and diverse instruction-response pairs tailored for long-document setting. 
% We hypothesis that when model has done enough long-context pretraining, it needs higher-quality and more diverse synthetic data.

The growing demand for AI systems capable of processing and reasoning over extensive information has driven the development of large language models (LLMs) with significantly expanded context windows \citep{dubey2024llama, achiam2023gpt, team2024gemini}. 
Among long-context tasks, needle-in-a-haystack (NIAH) \citep{niah} retrieval—where models locate specific information within large contexts—has emerged as a relatively simple benchmark, with previous work showing that continued pretraining on long-context data significantly boosts NIAH performance \citep{fudata, hsieh2024ruler, li2024needlebenchllmsretrievalreasoning}.
However, while many LLMs excel at NIAH, they often struggle with more complex challenges, such as passage ranking and dialogue analysis, which require reasoning and synthesis across extended contexts \citep{hsieh2024ruler, yen2025helmet, zhang2024infinitebench, levy-etal-2024-task, vodrahalli2024michelangelolongcontextevaluations, li2024alr2retrievethenreasonframeworklongcontext}. 
% The ability to perform complex reasoning over long contexts is essential for real-world applications, such as synthesizing information across multiple sources and understanding nuanced, multi-turn interactions \citep{liu2024lost, karpinska2024one, xuretrieval, xu2024detectiveqa, jimenez2024swebench, wang-etal-2024-leave}.
The ability to reason over long contexts is essential for real-world applications, such as legal document analysis and book review \citep{liu2024lost, karpinska2024one, xuretrieval, xu2024detectiveqa, jimenez2024swebench, wang-etal-2024-leave}.
These more difficult tasks require models not only to retrieve information but also to integrate and reason over it in realistic, multi-faceted scenarios. Addressing this gap calls for high-quality, diverse, and generalized instruction-tuning datasets designed specifically for long-context reasoning. Such datasets are essential for equipping LLMs to effectively leverage their extended context capabilities in complex, real-world applications.



% These advancements enable LLMs to handle tasks such as synthesizing information from multiple sources and reasoning across lengthy documents \citep{liu2024lost, karpinska2024one, xuretrieval, xu2024detectiveqa}. 
% % However, simply increasing the size of a model’s context window is not sufficient \citep{hsieh2024ruler, fudata}. 
% While many recent long-context LLMs achieve near-perfect performance on needle-in-a-haystack (NIAH) tasks—designed to test basic in-context retrieval abilities—they often struggle with more complex tasks such as passage ranking and dialogue analysis \citep{hsieh2024ruler, yen2024helmetevaluatelongcontextlanguage, zhang2024infinitebench}. 
% Although significant progress has been made to enable model with strong NIAH performance, 
% the availability of high-quality, diverse, and general datasets tailored for instruction tuning in long-context scenarios remains limited. 
% Such datasets are critical for equipping LLMs to effectively leverage their extended context capabilities in realistic, complex settings.


% Instruction tuning for long-context tasks introduces unique challenges that distinguish it from short-context instruction tuning. 
% The primary challenge is the difficulty of human annotation, as reasoning over lengthy contexts is often impractical due to its time-consuming nature.
% To address this, existing approaches rely heavily on synthetic data generated by LLMs\citep{dubey2024llama, an2024make, bai2024longalign, xiong2024effective, xiong2024artificialneedlesrealhaystacks}. 
% However, these methods often fail to support the broader range of tasks required for long-context reasoning.
% point out the key where you achieve but they dont. (in high-level)
% then use "for example" for one/two existing works to elaborate what they have done but fail in what u can do (in low level)
% For instance, existing methods \citep{xiong2024effective, bai2024longalign} and Llama-3.1 \citep{dubey2024llama} rely on the Self-Instruct framework \citep{wang2023selfinstrct} to generate QA pairs from short chunks of long documents, with full-document context incorporated during training; Llama-3.1 further synthesizes long-context summarization data using a hierarchical summarization method.
% While these approaches effectively leverage models' short-context capabilities for data generation, they are narrowly focused on specific objectives, such as fact extraction and summarization. 
% This narrow scope limits the diversity and generalizability of the resulting datasets, leaving critical gaps in supporting more complex tasks, such as rewriting and synthesizing arguments from multiple documents.
A major bottleneck in enhancing long-context reasoning is the lack of high-quality instruction tuning data. Unlike short-context tuning, which benefits from abundant human-annotated data, manually constructing long-context instruction data is impractical due to the complexity of reasoning over extended contexts.
Existing methods rely on data synthesis using LLMs \citep{dubey2024llama, an2024make, bai2024longalign, xiong2024effective, xiong2024artificialneedlesrealhaystacks}. 
For instance, prior approaches \citep{xiong2024effective, bai2024longalign} generate long-context instruction-tuning data by extracting short text spans from long documents, synthesizing question-answer pairs based on these snippets, and incorporating the full document during training. Other approaches, such as Llama-3.1 \citep{dubey2024llama}, further utilize hierarchical summarization to construct long-context datasets.
While effective at leveraging models' short-context capabilities for data generation, these methods primarily focus on fact extraction and summarization.
This narrow scope limits the diversity and generalizability of the resulting data, leaving critical gaps in supporting more complex and realistic tasks.

% These limitations underscore the need for a framework that can generate diverse, realistic, and scalable datasets for long-context instruction tuning—a gap our method is specifically designed to fill.
% While these methods leverage model to synthesize data, they often concentrate on narrow objectives, such as QA and summarization, limiting their diversity and generalizability. 
% Consequently, current datasets fail to support a broader range of tasks, such as multi-document reasoning and in-context learning, leaving a significant gap in advancing instruction tuning for long-context models. 


% % Proposed Solution / Approach
% This paper addresses these challenges by introducing a novel framework for synthesizing diverse, realistic, and scalable instruction-response datasets tailored for long-context tasks. Our method combines meta information extraction from real-world user-chatbot conversations, document classification, and graph-based modeling of meta information to systematically generate instruction-response pairs. By capturing and leveraging relationships among user intents, tasks, and constraints, our framework ensures that the synthesized data reflects the complexity and diversity of real-world scenarios, addressing the shortcomings of existing methods that focus narrowly on QA and summarization.

% Our approach is built on several core components:
% \textbf{Meta Information Extraction.} We extract 13 fields of meta information, such as task types, user intentions, knowledge requirements, and sentiment, from real-world user-chatbot conversations. This structured representation provides a rich foundation for generating realistic instructions.
% \textbf{Graph-Based Modeling.} We construct document-specific concept graphs where nodes represent discrete meta information values, and edges encode their co-occurrence relationships. Paths sampled from these graphs represent diverse and realistic combinations of meta information criteria.
% \textbf{Instruction Generation.} Sampled paths are used to generate instruction templates, which are paired with long documents. We prompt LLMs to refine these instructions, ensuring they align with the context of the paired document and reflect realistic user scenarios.
% \textbf{Multi-Document Extension.} Building on the graph-based framework, we adapt single-document tasks to multi-document settings, enabling the synthesis of instructions for tasks like cross-document comparison, synthesis, and aggregation, which are essential for enhancing long-context model performance.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/method_overview_v3.pdf}
    % \vspace{0.1in}
    \caption{Overview of the two-stage WildLong Framework. Stage 1 extracts meta-information from real-world user-chatbot conversations, classifies documents by type, constructs graphs to represent meta-information relationships, and samples paths to generate tailored instructions. Stage 2 pairs long documents from the pre-training corpus with these instructions, generating instruction-response pairs by rewriting the instructions and answering based on the document context.}
    \label{fig:method_overview}
    % \vspace{-0.1in}
\end{figure*}

% % Proposed Solution / Approach
% This paper introduces a novel framework to address the limitations of existing methods by synthesizing instruction-response datasets that are realistic, diverse, and scalable, tailored for long-context tasks.
To address this limitation, we propose WildLong, a scalable framework for generating diverse and realistic instruction-response pairs for long-context reasoning. Our approach integrates \textit{meta-information extraction}, \textit{graph-based modeling}, and \textit{adaptive instruction-response generation}. The pipeline of our framework is illustrated in Figure~\ref{fig:method_overview}.
% To address these needs, we propose a novel framework that integrates three key components—\textit{meta-information extraction}, \textit{graph-based modeling}, and \textit{adaptive instruction-response generation}.
We extract meta-information, such as user intents, tasks, and constraints, from real-world user-chatbot conversations. This process ensures that the generated instruction-response pairs are grounded in realistic interactions and reflect the complexity of real-world scenarios.
To enhance diversity and scalability, we model the extracted meta-information as a graph, where nodes represent individual meta-information value and edges capture their co-occurrence frequencies. By performing random walks on this graph, we generate novel combinations of meta-information, introducing diverse and varied instruction templates. 
%Scalability is achieved through the combinatorial nature of the graph, enabling efficient expansion of the dataset by exploring numerous permutations of meta-information.
Adaptive instruction-response generation further supports scalability and diversity. Each combination of meta-information is paired with long-context examples sampled from the pretraining corpus, introducing variability in the contexts associated with the instructions. The availability of abundant pretraining data ensures that large-scale datasets can be generated efficiently.
As shown in Figure \ref{fig:task_doc_type_distribution}, our dataset spans a wide range of document types and task types, reflecting the diversity and complexity required for real-world long-context reasoning.


% We evaluate the effectiveness of our fine-tuned models on a diverse suite of benchmarks, including long-context tasks with input lengths up to 128K tokens and short-context tasks. These benchmarks assess a wide range of capabilities, from fundamental long-context information retrieval to more complex tasks such as citation and passage reranking. Our evaluation shows that models fine-tuned with our dataset perform robustly across various long-context scenarios while maintaining strong performance on standard short-context tasks, highlighting the generalizability of our approach.
% We fine-tuned Mistral-7B-Instruct-v0.2\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}} and Llama-3.1-8B-Instruct\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}} models on our dataset of 150K synthesized instruction-response pairs. Notably, our fine-tuned Mistral-7B model achieves a substantial performance improvement on the RULER \citep{hsieh2024ruler} benchmark, with an average score increase of $+14.7$. Additionally, our fine-tuned Llama-3.1-8B model demonstrates competitive performance with superior long-context LLMs even at larger scales, achieving $84.1$ on RULER (vs. $85.1$ for Llama-3.1 70B) and $6.8$ on LongBench-Chat \citep{bai2024longalign} (vs. $6.7$ for Llama-3.1 70B). Importantly, our fine-tuned models retain strong performance on short-context tasks without finetuning on additional short-context instruction-tuning data, which existing methods typically use to prevent degradation. This highlights the robustness and generalizability of our realistic and diverse instruction-tuning dataset, enabling effective long-context fine-tuning without compromising short-context benchmarks.
We fine-tuned Mistral-7B-Instruct-v0.2\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}} and Llama-3.1-8B-Instruct\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}} on 150K synthesized instruction-response pairs and evaluated them on various long-context benchmarks with input lengths up to 128K tokens.
Notably, our fine-tuned Mistral-7B model achieves a substantial $+14.7$ improvement on the RULER benchmark \citep{hsieh2024ruler}, while our Llama-3.1-8B model performed competitively with much larger models, scoring $84.1$ on RULER (vs. $85.1$ for Llama-3.1 70B) and $6.8$ on LongBench-Chat \citep{bai2024longalign} (vs. $6.7$ for Llama-3.1 70B). 
Importantly, our fine-tuned models retain short-context performance without fine-tuning on additional short-context data, which existing methods typically use to prevent degradation. This demonstrates the robustness and generalizability of our synthetic data.




% Contribution
% By bridging the gap between synthetic data for long-context fine-tuning, this work provides a foundational step toward advancing the scalability and utility of LLMs in real-world applications. Our framework addresses the limitations of existing methods by generating diverse and realistic datasets tailored for long-context tasks. Through meta-information extraction, graph-based modeling, and multi-document extensions, we create data that captures real-world complexity. Comprehensive evaluations show that our approach significantly improves long-context performance while maintaining short-context generalization, enabling models to excel in long-context reasoning and setting the stage for future research in diverse and complex task settings.

% In summary, this work addresses the challenge of creating diverse, realistic, and scalable datasets for long-context instruction tuning, enabling models to generalize effectively to complex tasks across extended contexts. Through extensive evaluations, we demonstrate that our approach enhances performance on diverse long-context benchmarks while maintaining robustness on traditional short-context tasks. These findings underscore the importance of data quality and scalability in equipping models with the generalizability needed for real-world applications that involve long contexts.

