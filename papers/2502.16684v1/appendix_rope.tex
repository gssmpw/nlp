\label{sec:appendix_rope}
Recent studies demonstrate that adjusting the base value in Rotary Position Embedding (RoPE) significantly enhances language models' ability to handle long-context sequences \citep{ntkawarerope}. 
The scalability of RoPE for long-context has been systematically demonstrated through base parameter adjustments \citep{liuscaling}.
By increasing the base parameter (e.g., from $10^4$ to $10^6$), the wavelength of positional encoding grows exponentially as \( \lambda_i \propto \text{base}^{2i/d} \), where \(d\) is the embedding dimension and \(i\) indexes frequency bands. This prolongs the non-repeating positional patterns across distant tokens, effectively mitigating encoding collisions that impair long-range dependency modeling. Practical implementations like Code Llama \citep{grattafiori2023code} and ChatGLM \citep{glm2024chatglm} have adopted base scaling to extend context windows to 16k+ tokens while preserving local positional sensitivity. Our experiments align with these findings, showing that larger base values improve coherence in tasks requiring cross-sentence reasoning.