Large language models (LLMs) with extended context windows enable tasks requiring extensive information integration but are limited by the scarcity of high-quality, diverse datasets for long-context instruction tuning. Existing data synthesis methods focus narrowly on objectives like fact retrieval and summarization, restricting their generalizability to complex, real-world tasks.
% We introduce WildLong, a framework for generating diverse, scalable, and realistic instruction-response datasets tailored to long-context tasks. WildLong combines meta-information extraction from real user queries, graph-based modeling of co-occurrence relationships, and adaptive generation to produce scalable data. 
% Moreover, the framework extends beyond single-document tasks to support multi-document reasoning, including cross-document comparison, synthesis, and aggregation.
WildLong extracts meta-information from real user queries, models co-occurrence relationships via graph-based methods, and employs adaptive generation to produce scalable data. It extends beyond single-document tasks to support multi-document reasoning, such as cross-document comparison and aggregation.
% We validate the effectiveness of WildLong by fine-tuning models on a dataset of 150K synthesized instruction-response pairs and evaluating their performance on benchmarks with input lengths up to 128K tokens. 
% Fine-tuning models on 150K synthesized instruction-response pairs, our approach achieves a +14.7 improvement on the RULER benchmark with Mistral-7B and our finetuned Llama-8B model surpasses other open-source long-context-optimized models across benchmarks, while maintaining competitive results on short-context tasks without mixing supplementary short-context data. By producing a more diverse and realistic long-context instruction tuning dataset, WildLong establishes a new paradigm for equipping LLMs with the capability to handle complex, real-world tasks involving long contexts.
Our models, finetuned on 150K instruction-response pairs synthesized using WildLong, surpasses existing open-source long-context-optimized models across benchmarks while maintaining strong performance on short-context tasks without incorporating supplementary short-context data.
By generating a more diverse and realistic long-context instruction dataset, WildLong enhances LLMs' ability to generalize to complex, real-world reasoning over long contexts, establishing a new paradigm for long-context data synthesis.