

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/path_example.pdf}
    \caption{This figure demonstrates examples of instructions generated from sampled paths in a narrative text graph. Solid lines represent connections within paths, while dotted lines show node interconnections during graph construction. Using a random walk algorithm, diverse instructions are generated by combining nodes. For instance, the knowledge node ``understanding of narrative structure'' and the context node ``participation in a creative storytelling exercise'' appear in multiple paths but result in distinct instructions due to varying other meta information.}
    \label{fig:path_example}
\end{figure*}

In this section, we describe our methodology for generating diverse and realistic instruction-response pairs for long-context tasks. 
% As shown in Figure \ref{fig:method_overview}, we begin by extracting meta information from real-world user-chatbot conversations and organizing this information into a concept graph.
% The graph serves as a foundation for sampling realistic meta information combinations, which are then used to synthesize instructions. These instructions are paired with documents, classified by type, and refined to ensure alignment. Below, we detail each step.
As shown in Figure \ref{fig:method_overview}, our approach comprises two main stages. In Stage 1, we extract meta-information from real-world user-chatbot conversations and construct document-type-specific graphs to model co-occurrences among meta-information. Instructions are generated by sampling paths from these graphs. In Stage 2, these instructions are paired with long documents from the pretraining corpus to create instruction-response pairs. Below, we provide an overview of each component in the framework.

\subsection{Meta Information Extraction}
We leverage the WildChat dataset \citep{zhao2024wildchat}, a large corpus of user-chatbot conversations, and focus specifically on single-turn conversations that involve long contexts. WildChat is particularly suitable for our task because it contains realistic user queries and high-quality responses, which facilitate the accurate annotation of meta information fields.
% \footnote{Conversations with user input and chatbot output exceeding 2,000 tokens are considered long context, resulting in a subset of 32,000 conversations.}
From each conversation, we extract 13 fields of meta information that represent key attributes relevant to understanding and modeling long-context instructions:

\small{\texttt{document type, tasks or requests, user intention, user profile, language style, context, knowledge/commonsense involved for user, knowledge/commonsense involved for chatbot, long context capability involved, output format, sentiment, constraint of the request, simplified instruction}}.

These fields encompass essential aspects of the interaction, ensuring a comprehensive representation of user intent, contextual nuances, and task-specific requirements.
We prompt {\tt GPT-4} to extract meta information from each conversation\footnote{The prompt used for meta information extraction is detailed in Table \ref{tab:prompt_extract_meta}.}. For example, tasks such as ``extract details'' for informational articles or ``continue the story'' for fictional narratives are explicitly labeled. Contexts like ``preparing for a presentation'' or ``research related to ancient Greece'' are extracted for professional or historical texts, respectively.
The extracted meta information reflects realistic user scenarios involving long-context conversations and serves as a structured foundation for subsequent stages of our methodology.


% \subsection{Document Classification}
% \label{sec:doc_classification}
% To enable the generation of document-type-specific instruction-response pairs, we categorize long documents into a predefined set of 10 types. During the meta information extraction process, we identify the document type for each long context in the filtered WildChat conversation data. These document types are grouped using K-Means clustering, resulting in 10 coherent clusters that serve as the predefined document types. 
% To ensure compatibility between the WildChat and SlimPajama \citep{cerebras2023slimpajama} datasets, we annotate 20,000 long documents from SlimPajama using {\tt GPT-4}. The annotation prompt explicitly requires the output to match one of the predefined document types. 
% These annotations allow us to train a document classifier directly on SlimPajama data, ensuring that the classifier is well-suited for the domain of the sampled long documents used later in the instruction-response pairing process.
% For each document, we extract the features using a small language model. 
% % \footnote{We use StableLM-2-1.6B \citep{bellagente2024stablelm} and compute the mean of the hidden states of the last layer as a representation of semantic features}
% A random forest classifier is then trained on these semantic features and the GPT-annotated labels, achieving 90\% accuracy on a held-out test set. This classifier enables efficient and accurate document type predictions for unseen SlimPajama data, forming a critical step for pairing documents with generated instructions in subsequent stages.

\subsection{Graph Construction}
Instructions are generally document-type-specific, necessitating the construction of separate graphs for each document type. 
To build document-type-specific graphs, we first identify document types for each conversation as free-form values during the meta information extraction process. 
To group these values into coherent and meaningful categories, we apply K-Means clustering. The total number of clusters, set to 10, is predefined to balance between generalization and specificity based on the observed diversity of the dataset. Each cluster represents a distinct document type, and the cluster centers are rewritten to serve as the final document type labels. The distribution of the document types is illustrated in Figure \ref{fig:task_doc_type_distribution}. 

For each document type $d$, we construct an undirected graph $G_d = (\mathbb{V}_d, E_d)$ to model the co-occurrence relationships among meta information values extracted from user-chatbot conversations\footnote{Eleven meta information fields are used to construct the graph. The ``document type'' field is used to classify documents such that we can construct a separate graph for each document type. The ``simplified instruction'' field is used as a demonstration when generating instructions based on paths, see Section \ref{sec:path_to_instructs}.}. This graph represents the interactions between meta information fields and facilitates the systematic exploration of realistic and diverse combinations for instruction generation. The construction process is detailed as follows.

\paragraph{Nodes}
Each node corresponds to a unique value of a meta information field. Let $\mathbb{M} = \{m_1, m_2, \dots, m_{11}\}$ denote the set of 11 meta information fields used to construct the graph (e.g., task type, sentiment, output format). The set of nodes $\mathbb{V}_d$ is defined as:
\[
\mathbb{V}_d = \{v \mid v \text{ is a value of some field } m_i \in \mathbb{M}\text{ in any conversation for document type } d\}.
\]
Nodes are independent of individual conversations and collectively capture all unique meta information values observed for the document type.

\paragraph{Edges}
Edges represent the co-occurrence of meta information values in the same conversation, provided they belong to different fields.
Formally, an edge $(v, u) \in E_d$ exists if:
\begin{enumerate}
    \item $v$ is a value of field $m_i \in \mathbb{M}$,
    \item $u$ is a value of field $m_j \in \mathbb{M}$, where $i \neq j$, and  
    \item $v$ and $u$ co-occur in at least one conversation for document type $d$.
\end{enumerate}
For each conversation, the extracted meta information values from the 11 fields are interconnected, forming a fully connected bipartite subgraph, where edges connect values from different fields.

\paragraph{Edge Weights}
The weight of an edge $(v, u) \in E_d$ reflects the frequency of co-occurrence of $v$ and $u$ across all conversations for document type $d$. The edge weight is computed as:
\[
w(v, u) = \log(f_{\text{co}}(v, u) + \varepsilon),
\]
where $f_{\text{co}}(v, u)$ is the raw count of co-occurrences, and $\varepsilon$ is a small constant for numerical stability. The logarithmic scaling mitigates the influence of highly frequent pairs while preserving distinctions among lower-frequency edges.

% This graph design ensures that the co-occurrences among meta information values are faithfully represented, while maintaining the diversity of values across different fields. 
% This graph that ...
We build document-type-specific graphs by fully connecting meta information values co-occurring in the same conversation, with edges weighted by log-scaled co-occurrence. 
By preserving the variety of meta information and accurately capturing their co-occurrence patterns, the graph facilitates the generation of realistic, meaningful and diverse instruction paths.

% For each document type $d$, we construct an undirected graph $G_d$, where the nodes represent values of meta information fields\footnote{Eleven meta information fields are used to construct the graph. The ``document type'' field is used to classify documents such that we can construct a separate graph for each document type. The ``simplified instruction'' field is used as a demonstration when generating instructions based on paths, see Section \ref{sec:path_to_instructs}.}, and the edges encode their co-occurrence relationships within each conversations. The graph is designed to model the co-occurence between meta information components, enabling systematically exploration of realistic and diverse combinations. 
% This design ensures that the sampled paths represent a diverse combination of nodes, with each node contributing a unique aspect to the instruction. The construction of the graph is formalized as follows. 

% \textbf{Nodes}: Let $\mathbb{M} = \{ m_1, m_2, \dots, m_{|\mathbb{M}|}\}$ represent the set of meta information values, where each ${m_i}$ belongs to a specific field (e.g., task type, sentiment, output format).

% % Edges: An edge $(m_i, m_j) \in E$ exists if $m_i$ and $m_j$ co-occur in at least one conversation, with edge weight $w_{ij}$ determined by their co-occurrence frequency.
% \textbf{Edges}: Undirected edges are established only between nodes of different meta information categories. An edge $(m_i, m_j)$ exists if $m_i \in \mathbb{M}_a$ and $m_j \in \mathbb{M}_b$ (with $a \neq b$) and if $m_i$ and $m_j$ co-occur in the same meta information set extracted from a user-chat conversation. This design ensures that paths in the graph cover diverse aspects of an instruction without revisiting possibly similar or contradictory nodes from the same category.

% \textbf{Edge Weights}: The edge weights $w_{ij}$ reflects the co-occurrence frequency of nodes $m_i$ and $m_j$, scaled logarithmically to mitigate overemphasis on dominant frequency pairs:
% \begin{equation}
%     f_{\text{co}}(m_i, m_j) = \log(w_{ij} + \varepsilon),
% \end{equation}
%  where $\varepsilon$ is a small constant to prevent computational issues.


\subsection{Meta Information Path Sampling}
To ensure that instruction generation is guided by realistic and diverse criteria, we first sample structured combinations of meta information values. Since meta information fields interact in complex ways, manually enumerating all meaningful combinations is infeasible. Instead, we apply a weighted random walk on the document-type-specific graphs to systematically explore plausible meta information combinations.
To generate sampled paths $\hat{P} = \{ v_1, v_2, \dots, v_k \}$ that represent meta information combinations, we employ a weighted random walk algorithm on $G_d$. 

The walk begins by randomly selecting an initial node $v_1 \in \mathbb{V}_d$ from a uniformly sampled meta information category $m_c \in \mathbb{M}$. 
At each step $t$, the walk transitions from the current node $v_t$ to a neighboring node $v_{t+1}$, which belongs to a different meta information category that has not yet been visited. The transition probability from $v_t$ to $v_{t+1}$ is determined by edge weights:
\begin{equation}
    p(v_{t+1} \mid v_t) = \frac{\exp(w(v_t, v_{t+1}))}{\sum_{v_k \in \mathcal{N}(v_t)} \exp(w(v_t, v_k))},
\end{equation}
where $w(v_t, v_{t+1})$ is the weight of the edge between $v_t$ and $v_{t+1}$, and $\mathcal{N}(v_t)$ is the set of neighbors of $v_t$. 
The walk continues for up to $N$ steps, producing a path that spans $N$ distinct meta information fields. 
Based on our preliminary experiments on instruction synthesis, we determined that $N = 6$ strikes the right balance: larger values of $N$ introduce overly restrictive criteria, making instruction generation challenging and prone to producing convoluted instructions joined by ``and'' to satisfy all requirements. Conversely, smaller values of $N$ result in overly simple instructions with limited complexity. 
%This ensures that each category is represented only once in a path, preventing multiple visits to the same category within a single path. 
The limit of six meta information fields provides sufficient criteria to guide instruction generation while allowing the model to flexibly incorporate other relevant meta information creatively.
% This approach ensures that the sampled path $\hat{P}$ covers multiple aspects of an instruction while avoiding redundancy. By integrating both frequent and less common connections, the random walk balances diversity and representativeness in the generated paths. 
By leveraging edge weights to guide transitions, the algorithm captures realistic co-occurrence patterns, enabling the scalable synthesis of diverse instruction templates, while maintaining flexibility to explore less frequent connections. These paths serve as structured templates to generate diverse and representative instructions for long-context tasks.

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-0.2in}
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/task_doc_type_distribution.pdf}
    % \vspace{0.1in}
    \caption{Distribution of document types (inner circle) and task types (outer circle) in our dataset.}
    \label{fig:task_doc_type_distribution}
    \vspace{-0.2in}
\end{wrapfigure}


\subsection{Instruction Generation with Sampled Paths}
\label{sec:path_to_instructs}

To synthesize instructions aligned with the sampled meta-information paths, we prompt {\tt GPT-4} with a one-shot demonstration.
{\tt GPT-4} generates natural language instructions that follow the criteria defined by the meta information fields in the sampled path\footnote{Details about how to select the demonstration can be found in Appendix \ref{sec:appendix_demo} and the prompt can be found in Table \ref{tab:prompt_path_to_instruct} in Appendix \ref{sec:appendix_prompt}.}.  
Figure \ref{fig:path_example} illustrates instructions generated from sampled paths in a narrative text graph. Using the random walk algorithm, diverse instructions emerge by combining different meta information values. For example, two paths may share ``understanding of narrative structure'' as the ``knowledge involved for chatbot'' field but differ in others. One path, with values like ``detailed language style'' and ``literature research purpose,'' guides an instruction for analyzing plot development. Another, with ``entertainment purpose'' and ``emotional sentiment,'' leads to an instruction for crafting a heartfelt monologue.


\subsection{Instruction-Response Pair Generation}
% To ensure compatibility between the WildChat and SlimPajama \citep{cerebras2023slimpajama} datasets, we annotate 20,000 long documents from SlimPajama using {\tt GPT-4}. The annotation prompt explicitly requires the output to match one of the predefined document types. 
% These annotations allow us to train a document classifier directly on SlimPajama data, ensuring that the classifier is well-suited for the domain of the sampled long documents used later in the instruction-response pairing process.
% For each document, we extract the features using a small language model. 
% % \footnote{We use StableLM-2-1.6B \citep{bellagente2024stablelm} and compute the mean of the hidden states of the last layer as a representation of semantic features}
% A random forest classifier is then trained on these semantic features and the GPT-annotated labels, achieving 90\% accuracy on a held-out test set. This classifier enables efficient and accurate document type predictions for unseen SlimPajama data, forming a critical step for pairing documents with generated instructions in subsequent stages.

% Once the instructions are generated, we pair them with long documents sampled from the SlimPajama dataset \citep{cerebras2023slimpajama}, an open-source reproduction of the LLaMA pretraining data mixture \citep{touvron2023llama}. SlimPajama is a deduplicated, multi-corpora dataset specifically designed for training large language models and contains a wealth of long documents, making it ideal for tasks requiring extensive context. 
Once the instructions are generated, we pair them with long documents sampled from the SlimPajama\footnote{SlimPajama is an open-source reproduction of the LLaMA pretraining data mixture \citep{touvron2023llama}.} dataset \citep{cerebras2023slimpajama}.
 SlimPajama’s wealth of long documents makes it well-suited for tasks requiring extensive context.
% The sampled documents have token counts ranging from $2,000$ to $32,000$. 
As instructions are document-type-specific, we first classify sampled documents into one of ten predefined document types using a custom classifier\footnote{Details about the classifier are provided in Appendix \ref{sec:appendix_doc_classifier}.}.

% To train this classifier, we annotated $20,000$ long documents sampled from SlimPajama using {\tt GPT-4}. The annotation prompt explicitly required the output to match one of the predefined document types, ensuring consistency with the categories defined during meta information clustering. Using these annotations, we trained a random forest classifier on semantic features extracted with StableLM-2-1.6B \citep{bellagente2024stablelm}, where the mean of the last layer's hidden states was used as the feature representation. The classifier achieved 90\% accuracy on a held-out test set, enabling efficient and accurate predictions of document types for unseen SlimPajama data.
% We trained a random forest classifier on semantic features extracted from a small language model. The classifier was trained on annotations of 20,000 long documents from SlimPajama, achieving 90\% accuracy on a held-out test set. Details of the annotation process, feature extraction, and training settings are provided in the Appendix \ref{sec:appendix_doc_classifier}.

To ensure the paired documents reflect realistic user needs for long-context capabilities, we resample SlimPajama’s long documents to align their document type distribution with that of WildChat long conversations. This adjustment ensures the data distribution is representative of how users typically query about long contexts.
Once the document types are predicted, we pair each document with an instruction generated from the graph corresponding to its type. 
To make the instructions more contextually grounded, the sampled instruction and paired document are provided as input to {\tt GPT-4}, which generates an adapted instruction aligned with the document, and a corresponding response\footnote{Details about the prompt can be found in Table \ref{tab:prompt_instruct_response} in Appendix \ref{sec:appendix_prompt}.}. This ensures the final instruction-response pairs are coherent, relevant, and reflective of the document’s context.

% Once the instructions are generated, we pair them with long documents sampled from the SlimPajama dataset \citep{cerebras2023slimpajama}. The sampled documents have token counts between 2,000 and 32,000.
% Since instructions are document-type-specific, we classify the long documents using the classifier described in Section \ref{sec:doc_classification}. 
% We resampled documents such that their distribution matches that of the WildChat long conversations to ensure they reflect realistic user needs when long-context capabilities are required. 
% Since the sampled instruction may not align with the paried document, we prompt {\tt GPT-4} to rewrite the instruction for contextual relevance. The final output includes the aligned instruction and the corresponding response generated by {\tt GPT-4} for the paired document.


\subsection{Extending Instructions to Multi-Document Settings}
We observe that the filtered WildChat dataset predominantly contains instructions designed only for single-document contexts, with limited coverage of multi-document tasks. To address this gap, we extend our method to generate instructions suitable for multi-document settings by adapting the extracted meta information and graph-based framework.

The extension begins with adapting the ``tasks or requests'' field in the meta information to reflect multi-document requirements while keeping other fields unchanged. 
% Using {\tt GPT-4}, we rewrite the tasks to explicitly involve handling information across multiple sources. 
Each single-document task node is rewritten to explicitly involve handling information across multiple documents using {\tt GPT-4}.
For instance, a task like ``Summarize the key points of the document'' is transformed into ``Summarize and compare the key points across multiple documents.''\footnote{The rewriting prompt is shown in Table \ref{tab:prompt_single_to_multi}. The modifications emphasize the need for synthesis, comparison, or aggregation across documents while preserving coherence and relevance. }

Following this adaptation, we construct document-type-specific graphs for multi-document tasks, sample paths using the same random walk algorithm, and generate instructions based on the sampled paths. The steps for graph construction, path sampling, and instruction synthesis remain largely consistent with the single-document setting.
During the document-instruction pairing stage, we sample pairs of documents of the same type from the SlimPajama dataset, concatenate them, and pair the concatenated documents with a multi-document instruction of the same type.
The concatenated documents and their paired instruction are then input to {\tt GPT-4} to generate a refined, contextually aligned instruction and a corresponding response.
% The graph construction, path sampling, instruction generation, document classification processes remain largely consistent with the single-document setting. 
% Following the adaptation of the ``tasks or requests'' field, the graph construction and path sampling processes remain largely consistent with the single-document setting. Updated graphs include nodes for multi-document-specific ``tasks or requests'' while retaining other meta information fields. Path sampling on these graphs ensures that sampled paths reflect diverse combinations of multi-document tasks and other meta information.
% For each document type, a graph is built with the updated multi-document "tasks or requests" nodes, while other meta information fields remain unchanged. Path sampling is then conducted on these graphs, ensuring that sampled paths reflect diverse combinations of multi-document tasks and other meta information.
% This approach enables the generation of instructions involving multi-document reasoning, such as ``Compare the conclusions from multiple reports and summarize common themes.''
By integrating these modifications, our method systematically generates instructions and responses that support multi-document reasoning tasks.



