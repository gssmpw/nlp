% In this work, we propose WildLong, a novel framework for synthesizing diverse, scalable, and realistic instruction-response datasets designed for long-context tasks. Our approach addresses key challenges in dataset creation by leveraging meta-information extraction from real-world user queries, graph-based modeling of co-occurrence relationships, and adaptive instruction-response generation.
% WildLong is built on the principles of diversity, scalability, and realism, enabling it to support complex reasoning tasks such as cross-document comparison, and aggregation, which are essential for real-world applications. By integrating meta-information into the data generation process and systematically exploring new combinations through graph-based modeling, WildLong generates diverse datasets that reflect the complexity of extended contexts.
% Experimental results demonstrate that WildLong significantly improves long-context task performance, surpassing other open-source long-context-optimized models across multiple benchmarks. Importantly, this improvement is achieved without requiring supplementary short-context instruction tuning, highlighting the robustness and generalizability of our approach.
% The success of WildLong highlights the potential of structured, meta-information-driven data synthesis to enhance the capabilities of LLMs for complex, real-world tasks. By addressing the critical gaps in long-context dataset diversity and quality, WildLong sets a new standard for long-context instruction tuning and paves the way for further advancements in equipping LLMs to tackle the challenges of extended-context reasoning.
% We propose WildLong, a framework for synthesizing diverse, scalable, and realistic instruction-response datasets for long-context tasks. By leveraging meta-information extraction, graph-based modeling, and adaptive instruction generation, WildLong generates long-context instruction-tuning data with real-world complexity.
% Experiments show improved long-context task performance while retaining short-context performance without additional short-context fine-tuning, demonstrating its robustness and generalizability. We hope WildLong provides insights into generalizing instruction tuning and inspires further advancements in long-context reasoning for LLMs.
We propose WildLong, a framework for synthesizing diverse, scalable, and realistic instruction-response datasets for long-context tasks. 
It integrates meta-information extraction to ensure realistic complexity, graph-based modeling for systematic instruction expansion, and adaptive instruction generation for enhanced contextual relevance.
Our fine-tuned models consistently outperform baselines and maintain short-context performance without mixing short-context data. Notably, our finetuned Llama-3.1-8B model surpasses most open-source long-context models on Longbench-Chat and demonstrates competitive performances with even larger models across benchmarks.
WildLong enables the synthesis of instruction-tuning data that produces robust models capable of handling diverse long-context tasks. Extending beyond synthetic QA and summarization, it bridges the gap to more complex, realistic challenges, advancing the effectiveness of long-context LLMs.
We hope WildLong provides insights into generalizing synthetic data and inspires further progress in long-context reasoning for LLMs.