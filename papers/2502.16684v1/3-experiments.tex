% We conduct comprehensive evaluations of our framework across both long-context and short-context benchmarks. This section begins with the implementation details of our method, followed by a comparison with baseline pretrained language models and existing long-context optimized models. Additionally, we benchmark against existing long-context SFT datasets and present detailed ablation studies to analyze the contributions of our framework.
We evaluate our framework comprehensively on both long-context and short-context benchmarks. This section outlines implementation details, compares our method with baseline pretrained and specialized long-context optimized models, benchmarks against existing long-context supervised fine-tuning (SFT) datasets, and presents ablation studies to analyze the contributions of essential components in our framework.

\subsection{Implementation Details}
% \textbf{Data Curation Details.} To construct the SFT dataset, we leverage two complementary sources. The WildChat dataset serves as the basis for generating instructions. 
% From this dataset, we filter single-turn conversations exceeding 2K tokens, yielding a seed dataset of 32K instances that represent realistic long-context scenarios. 
% For pairing with the generated instructions, we filtered long-context documents from the SlimPajama pretraining corpus. 
% The corpus is filtered into two subsets to support both single- and multi-document settings. For the single-document setting, we select documents ranging in length from 2K to 64K tokens. For the multi-document setting, we filter another set of documents between 2K and 20K tokens and pair two documents of the same type, concatenating them into a single composite document to simulate multi-document tasks. 
% We extract semantic features using StableLM-2-1.6B \citep{bellagente2024stablelm} and compute the mean of the hidden states of the last layer as a representation of semantic features.
% We employ a random forest classifier trained on a set of seed SlimPajama data annotated by {\tt GPT-4} to assign each document to one of the ten predefined categories. We randomly sample a generated instruction from the corresponding category and pair it with the document. 
% To maintain a realistic distribution of document types, we sample SlimPajama data to mirror the distribution observed in the filtered WildChat dataset. This alignment ensures that the paired instruction-document samples reflect natural usage patterns. We sampled 100K documents for the single-document setting, and 50K documents for the multi-document setting, resulting in a total of 150K samples in the instruction tuning dataset. 

% \textbf{Instruction Generation.} To construct the SFT dataset, we leverage two complementary sources. The WildChat dataset serves as the basis for generating instructions. 
% From this dataset, we filter single-turn conversations exceeding 2K tokens, yielding a seed dataset of 32K instances that represent realistic long-context scenarios. 

% \textbf{SFT Data Curation.} For pairing with the generated instructions, we filtered long-context documents from the SlimPajama pretraining corpus. The corpus is filtered into two subsets to support both single- and multi-document settings. For the single-document setting, we select documents ranging in length from 2K to 32K tokens. For the multi-document setting, we filter another set of documents between 2K and 20K tokens and pair two documents of the same type, concatenating them into a single composite document to simulate multi-document tasks. We sampled 100K documents for the single-document setting, and 50K documents for the multi-document setting, resulting in a total of 150K samples in the instruction tuning dataset. 

% \textbf{Training Details.} We fine-tune Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct using our curated dataset. For Mistral-7B-Instruct-v0.2, we modify the RoPE theta from 1e6 to 1e7 to support longer positional embeddings. The model is optimized with the Adam optimizer, employing a learning rate of 1e-6 and 5e-7 respectively. We use a batch size of 512 and train for 2 epochs.
\textbf{Data Curation}
We filter single-turn WildChat conversations exceeding 2K tokens, yielding 32K instances. We then filter long-context documents from the SlimPajama corpus into two subsets: single-document (2K–30K tokens) and multi-document (2K–20K tokens). For multi-document, we pair two same-type documents and concatenate them. We sample 100K single-document and 50K multi-document examples, totaling 150K samples.

\textbf{Training Details.} We fine-tune Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct using our curated dataset. For Mistral-7B-Instruct-v0.2, we adjust the RoPE base from 1e6 to 1e7 to support longer positional embeddings\footnote{Increasing RoPE base supports longer context. More details can be seen in Appendix~\ref{sec:appendix_rope}.}. Both models are optimized using the Adam optimizer, with learning rates of 1e-6 and 5e-7 respectively. Training is conducted for 2 epochs with a batch size of 512\footnote{More details about computational budget and and infrastructure can be found in Appendix~\ref{sec:appendix_exp_details}.}.



% \subsection{Baselines}
% \textbf{Baseline Pretrained Language Models.}
% We include two proprietary long-context models \texttt{Gemini-1.5-Pro} and \texttt{GPT-4} as upper bounds due to their strong performance on long-context tasks.
% Additionally, we evaluate open-source pretrained language models with long-context capabilities, including Yi-34B  (200K context length) \citep{ai2024yi}, Phi-3-medium \citep{abdin2024phi3report}, LLaMA3.1-70B \citep{dubey2024llama}, Qwen2.5 \citep{qwen2.5report} (128K context length), and GLM4-9B \citep{glm2024chatglm} (1M context length).

% \textbf{Existing Long-Context Optimized Models.}
% We compare our model against models explicitly optimized for long-context capabilities. FILM \citep{an2024make} fine-tunes Mistral-7B-Instruct-v0.2 using synthetic QA pairs generated by prompting GPT-4-Turbo on short contexts, then concatenating multiple short contexts to form long inputs. Similarly, ChatQA-2 \citep{xu2024chatqa} fine-tunes LLaMA3-8B using NarrativeQA, combining related and unrelated paragraphs to generate synthetic long-context QA pairs. SEALONG \citep{li2024sealong} conducts preference optimization on LLaMA3.1-8B-Instruct using QA pairs derived from Wikipedia documents combined with unrelated documents to simulate long contexts, the preference pairs are scored with Minimum Bayes Risk. ProLong \citep{gao2024prolong} extends LLaMA3-8B’s context length to 512K by continued pretraining on long-context data and fine-tuning with high-quality short-context instruction data. 

% \textbf{Prior Long-Context SFT Data.}
% We also fine-tune the same LLaMA3.1 model on open-source long-context instruction-tuning datasets and compare it with models fine-tuned on our proposed dataset. LongAlpaca \citep{chenlonglora} contains 9K curated long QA pairs and 3K short QA pairs, covering tasks such as book questions, summarization, and document analysis.
% LongAlign \citep{bai2024longalign} collects long documents and uses Claude 2.1 to generate QA pairs with prompts designed for general, summary, reasoning, and information extraction tasks.
% LongReward \citep{zhang2024longreward} is similar to LongAlign where they employ GLM4 \citep{glm2024chatglm} to produce QA pairs using a self-instruct framework.
% baseline models
% baseline methods
% baseline data

\subsection{Baselines}
\textbf{Proprietary Long-Context Models.}
We include two proprietary long-context models \texttt{Gemini-1.5-Pro} and \texttt{GPT-4} as upper bounds due to their strong performance on long-context tasks.

\textbf{Open-Sourced Pretrained Long-Context Models.}
Additionally, we evaluate open-source pretrained language models with long-context capabilities, including GLM4-9B~\citep{glm2024chatglm}, Yi-34B~\citep{ai2024yi}, Llama3.1-70B~\citep{dubey2024llama}, Phi-3-medium~\citep{abdin2024phi3report}, and Qwen2.5~\citep{qwen2.5report}.

\textbf{Specialized Long-Context Optimized Models.}
We compare our approach to specialized long-context LLMs that extend or optimize model capabilities for long inputs. FILM~\citep{an2024make} and ChatQA-2~\citep{xu2024chatqa} fine-tunes Mistral-7B-Instruct-v0.2 and Llama-3-8B with synthetic long-context QA pairs. SEALONG~\citep{li2024sealong} applies preference optimization on Llama-3.1-8B-Instruct with extended-context QA pairs, while ProLong~\citep{gao2024prolong} continue-pretrain Llama-3-8B-Instruct to 512K context window and finetune with short-context data.

\textbf{Prior Long-Context SFT Data.}
We fine-tune Llama-3.1 on open-source long-context instruction-tuning datasets. LongAlpaca~\citep{chenlonglora} comprises 9K curated long QA pairs and 3K short QA pairs, covering tasks such as book questions and summarization. LongAlign~\citep{bai2024longalign} includes QA pairs generated by Claude 2.1 from extended documents, while LongReward~\citep{zhang2024longreward} similarly uses GLM4 to produce long-context QA pairs via a self-instruct framework.

\begin{table}[]
\caption{Main evaluation results of our models on RULER, HELMET and Longbench-Chat compared with baselines. Results on RULER and HELMET are averaged over sequence lengths ranging from 4K to 128K and 8K to 128K respectively.
% GILDS: “Generalized Instructions for Long-context tasks, using Diverse and Scalable synthetic data.”
}
\scalebox{0.75}{
\begingroup
\renewcommand{\arraystretch}{1.1}
% \begin{tabularx}{\textwidth}{l|*{5}{X}|*{6}{X}|>{\centering\arraybackslash}p{1cm}}
\centering
\begin{tabular}{lc|ccccc|ccccccc|c}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Size}}               & \multicolumn{5}{c|}{\textbf{RULER}}  & \multicolumn{7}{c|}{\textbf{HELMET}}  & \multicolumn{1}{c}{\textbf{Long}} \\
            &     & \textbf{NIAH} & \textbf{VT}   & \textbf{Agg}  & \textbf{QA}   & \textbf{Avg}      & \textbf{RAG}  & \textbf{ICL}  & \textbf{Cite} & \textbf{Rank} & \textbf{QA} & \textbf{Summ} & \textbf{Avg} & \textbf{Chat}  \\
\midrule
\rowcolor{gray!20}
\multicolumn{15}{c}{\it{Proprietary Long-Context Models}}\\
\midrule
Gemini-1.5-Pro   & -    & 99.7 & 99.9 & 96.6 & 77.2 & 93.4     & 72.1 & 78.8 & 44.5 & 69.0 & 47.6 & 38.5 & 58.4 & 7.6 \\
GPT-4            & -    & 95.4 & 99.9 & 93.4 & 70.3 & 89.8     & 70.6 & 65.1 & 24.9 & 53.4 & 47.7 & 32.6 & 49.1 & 8.4 \\
\midrule
\rowcolor{gray!20}
\multicolumn{15}{c}{\it{Open-Sourced Pretrained Long-Context Models}}\\
\midrule
GLM-4-1M         & 9B   & 98.2 & 99.4 & 72.2 & 69.4 & 84.8     & 67.9 & 77.3 & 31.4 & 41.7 & 44.2 & 28.8 & 48.6 & 5.9 \\
Yi-200k          & 34B  & 95.1 & 93.6 & 74.3 & 67.1 & 82.5     & 64.1 & 78.6 & \textcolor{white}{0}4.8  & 33.4 & 25.1 & 12.2 & 36.4 & 4.0 \\
Llama-3.1        & 70B  & 96.1 & 93.2 & 83.3 & 67.8 & 85.1     & 68.6 & 77.2 & 32.9 & 52.2 & 46.0 & 33.3 & 51.7 & 6.7 \\
Phi-3-medium     & 14B  & 88.7 & 76.5 & 77.4 & 59.3 & 75.5     & 58.9 & 67.0 & 17.1 & 23.9 & 22.4 & 26.6 & 36.0 & 5.2 \\
Qwen2.5          & 7B   & 83.3 & 81.7 & 73.2 & 57.0 & 73.8     & 53.1 & 75.8 & 17.7 & 31.2 & 28.4 & 28.1 & 39.1 & 5.8 \\
\midrule            
\rowcolor{gray!20}
\multicolumn{15}{c}{\it{Specialized Long-Context Optimized Models}}\\
\midrule
% LongChat         & 55.7 & 56.2 & 37.1 & 31.6 & 45.2     &      &      &      &      &      &      \\
% LongAlpaca       & 45.2 & 7.6  & 18.2 & 33.2 & 26.1     &      &      &      &      &      &      \\
% LongPO-512k      & 97.3 & 97.5 &      & 64.9 &          &      &      &      &      &      &   & 5.8   \\
FILM             & 7B   & 81.7 & 92.8 & 64.9 & 63.0 & 75.6     & 52.6 & 78.0 & 6.4  & 28.0 & 26.9 & 22.1 & 35.7 & 4.9 \\
% LongPO-512k      & 97.3 & 97.5 &      & 64.9 &          &      &      &      &      &      &   & 5.8   \\
ProLong-512k     & 8B   & 98.5 & 97.8 & 69.4 & 65.5 & 82.8     & 67.2 & 76.4 & 14.4 & 39.1 & 36.7 & 25.9 & 43.3 & 5.9 \\
ChatQA-2         & 8B   & 97.1 & \textbf{98.1} & 66.8 & 53.6 & 78.9     & 63.2 & \textbf{81.3} & \textcolor{white}{0}2.9 & 23.7 & 36.2 & 13.9 & 36.9 & 3.7 \\
SEALONG          & 8B   & 98.4 & 91.0 & 66.6 & 66.1 & 80.5     & 64.9 & 78.5 & 19.6 & \textbf{45.0} & 36.2 & 30.1 & 45.7 & 6.6 \\


\midrule
Mistral          & 7B   & 72.6 & 74.4 & 64.4 & 52.2 & 65.9     & 47.1 & 63.6 & 8.2  & 25.0 & 19.2 & 20.3 & 30.6 & 4.5 \\
% + Ours (20k)     & 95.4 & 92.0 & 46.6 & 65.0 & 74.8     & 57.6 & 72.5 & 9.7  & 32.7 & 24.8 & 39.5 \\
% + Ours (100k)    & 94.8	& 94.4 & 67.4 & 64.7 & 80.3     & 60.9 & 73.3 & 8.5  & 34.7 & 28.1 & 41.1 \\
+ WildLong       & 7B   & 95.2 & 95.9 & 67.0 & 64.2 & 80.6     & 62.1 & 74.6 & 12.4 & 34.3 & 34.4 & 29.2 & 41.2 & 6.3 \\
\midrule
LLaMA 3.1        & 8B   & 98.1 & 91.6 & 66.2 & 66.1 & 80.5     & 66.1 & 77.4 & 18.5 & 39.0 & 37.1 & 28.0 & 44.5 & 6.2 \\
+ WildLong       & 8B   & \textbf{98.7} & 95.7 & \textbf{74.3} & \textbf{67.9} & \textbf{84.1}     & \textbf{67.6} & 78.8 & \textbf{22.6} & 40.8 & \textbf{38.5} & \textbf{30.8} & \textbf{46.5} & \textbf{6.8} \\
% + single-doc 50k & 98.8 & 95.1 & 71.5 & 68.1 & 83.4     & 66.6 & 78.2 & 19.6 & 41.4 & 39.5 & 49.1 \\
% + multi-doc 50k  & 98.8 & 94.1 & 72.1 & 67.6 & 83.1     & 66.6 & 78.7 & 21.1 & 41.2 & 39.9 & 49.5 \\
% + single+multi   & 98.6 & 94.8 & 74.9 & 68.4 & 84.2     & 66.9 & 79.2 & 21.8 & 41.8 & 40.4 & 50.0 \\
\bottomrule

\end{tabular}
\endgroup}
% \end{tabularx}
% }
\label{tab:overall_performance}
\end{table}

\subsection{Evaluation Benchmarks}
To comprehensively evaluate the performance of our model, we assess both long-context and short-context capabilities. For long-context tasks, we benchmark our model against established baselines, whereas for short-context tasks, we compare its performance with the base model used for fine-tuning.

For long-context tasks, we use three benchmarks designed to test a wide range of capabilities across varying input lengths:

\textbf{RULER} \citep{hsieh2024ruler}. This benchmark evaluates four synthetic task types across input lengths ranging from 4K to 128K tokens, including Needle-in-a-haystack (NIAH) retrieval, Multi-hop Tracing with Variable Tracking (VT), Aggregation (Agg), and Question Answering (QA).

\textbf{HELMET} \citep{yen2025helmet}. We evaluate our model on six tasks from HELMET: Retrieval-augmented generation (RAG), Generation with citations (Cite), Passage re-ranking (Re-rank), Long-document question answering (LongQA), Summarization (Summ), and Many-short in-context learning (ICL). The Recall task is excluded due to its overlap with the synthetic NIAH task in RULER.

\textbf{Longbench-Chat} \citep{bai2024longalign}. This benchmark tests instruction-following abilities over long contexts (10K to 100K tokens) using real-world queries. It includes 40 queries in English and 10 in Chinese. \texttt{GPT-4-128K} serves as an impartial judge to evaluate model-generated responses.

For short-context tasks, we assess general language understanding and reasoning using MMLU \citep{hendrycksmeasuring}, Winogrande \citep{sakaguchi2020winogrande}, ARC-C \citep{clark2018think}, and GSM8K \citep{cobbe2021gsm8k}, and evaluate instruction-following capabilities with IFEval\footnote{Details on evaluation settings are in Appendix \ref{sec:appendix_eval_settings}.} \citep{zhou2023instruction}.

\subsection{Results}
% In this section, we demonstrate the effectiveness of our data synthesis method through two types of comparisons: (1) comparison with long-context LLMs; (2) comparison with existing long-context instruction-finetuning datasets on long-context benchmarks.

\textbf{Our finetuned models demonstrates strong performance over established models.} 
% Our model demonstrates significant improvement over the baseline models. 
% (compare with FILM: our model generalize better to diverse tasks)
% (compare with GLM4: GLM4 is slightly better than our model on long context tasks, but these gains come at the costs of degenerated short-context performance. Notably our models still outperform GLM4 on Aggregation and ICL tasks even trained with substantially shorter sequences)
% Our finetuned Mistral-7B improves upon the baseline by +14.7 and +10.6 points on RULER and HELMET, respectively, while our LLaMA-3.1-8B achieves gains of +3.6 and +2.0 points. These results highlight the effectiveness of our generated dataset across diverse tasks. Additionally, on LongBench-Chat, which consists of realistic user queries, our finetuned models consistently surpass baseline models, demonstrating their utility in real-world long-context applications.
% Compared to existing methods for enhancing long context capabilities, our approach achieves superior results. FILM underperforms relative to our Mistral-7B model despite using ten times more data, scoring 75.6 vs. 80.6 on RULER, 35.7 vs. 41.2 on HELMET, and 4.9 vs. 6.3 on LongBench-Chat, highlighting the efficiency of our dataset.
% % LongPO performs well on synthetic tasks but generalizes poorly to aggregation tasks, where our model excels. 
% SEALONG, using the same LLaMA-3.1-8B-Instruct model, achieves lower scores, particularly on RULER, where we observe an +8 point advantage. ProLong and ChatQA-2 perform well on synthetic tasks but fall short on real-world queries and more complex tasks, scoring 14.4 and 2.9 on citation and 5.9 and 3.7 on LongBench-Chat, respectively. In contrast, our approach balances strong performance across both synthetic and real-world tasks, achieving 22.6 on citation and 6.8 on LongBench-Chat, demonstrating superior performance and robustness.
% On LongBench-Chat, our method further underscores its strength by outperforming larger models such as Llama-3.1-70B-Instruct. This highlights its capability to handle realistic and complex scenarios, making it highly practical for real-world applications.
We significantly improve upon our baseline models, with Mistral-7B gaining +14.7 and +10.6 points on RULER and HELMET, and Llama-3.1-8B gaining +3.6 and +2.0.
Against open-source long-context models, our Llama-3.1-8B matches or exceeds larger alternatives. 
Notably on LongBench-Chat, our Llama-3.1-8B model outperforms most established models except for proprietary ones.
We also outperform specialized long-context methods. Despite using ten times more data, FILM scores lower than our Mistral-7B (e.g., 75.6 vs. 80.6 on RULER). SEALONG, based on Llama-3.1-8B-Instruct achieves lower scores, with an 8-point deficit on RULER compared with our Llama-based model. ProLong and ChatQA-2 perform well on synthetic tasks but struggle with real-world queries and complex tasks. 
These results highlight the effectiveness of our framework.


\textbf{Our method enhances performance compared to other long-context instruction-tuning data.}
We compare our method with previous long-context instruction-tuning datasets, including LongAlpaca, LongAlign and LongReward. We finetune Llama-3.1-8b-instruct with all these datasets with the same hyperparameters. As demonstrated in Table 
\ref{tab:other_long_data}, these datasets yield only slight improvements, with scores of 81.4, 81.9, and 81.2 on RULER, respectively. In contrast, our method significantly improves performance across all tasks, achieving an average score of 84.1 on RULER.
% A notable improvement is observed in the Aggregation task, which requires integrating multiple pieces of relevant information from long contexts. Referring to Figure \ref{fig:task_doc_type_distribution}, we suspect this improvement stems from the inclusion of tasks like "detail-oriented summarization" and "information synthesis" in our dataset. These tasks challenge the model to extract, integrate, and synthesize nuanced details across long contexts, a capability that appears underrepresented in other datasets. This suggests that the diversity and complexity of our data better equip the model to excel in tasks requiring advanced aggregation and reasoning.
The substantial improvements in aggregation tasks, which involve integrating multiple relevant details, can likely be attributed to our dataset's focus on detail-oriented summarization and information synthesis, as illustrated in Figure~\ref{fig:task_doc_type_distribution}. This broad coverage appears to better equip models for complex long-context reasoning.
This suggests our dataset’s diversity better equips models for complex reasoning and aggregation tasks.
% \begin{table}[]
\begin{wraptable}{r}{0.5\textwidth}
\caption{Performance comparison of Llama-3.1-8B-instruct fine-tuned with various long-context instruction-tuning datasets.}
\vspace{0.1in}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
\multirow{2}{*}{\textbf{Models}}    & \multicolumn{5}{c}{\textbf{RULER}}   \\
                 & \textbf{NIAH} & \textbf{VT}   & \textbf{Agg}  & \textbf{QA}   & \textbf{Avg}         \\
\midrule
LongAlpaca     & 97.9	& 95.2 & 67.0 & 65.4 & 81.4 \\
LongAlign      & 98.5	& 94.8 & 65.7 & \textbf{68.5} & 81.9 \\
LongReward     & 98.4	& 94.2 & 65.6 & 66.7 & 81.2 \\
WildLong       & \textbf{98.7} & \textbf{95.7} & \textbf{74.3} & 67.9 & \textbf{84.1} \\
\bottomrule
\end{tabular}}
\label{tab:other_long_data}
% \end{table}
\end{wraptable}
% Our synthesized dataset achieves superior performance over existing long-context instruction-finetuning data and improves the generalizability of the model to diverse tasks, underscoring the critical role of instruction diversity.
% We analyse the instruction diversity of the previous datasets and observe that most of the task types are contextual QA or summarization, which might constrain the generalizability of the finetuned model. 



% \begin{table}[]
% \centering
% \begin{tabular}{lccccccc}
% \toprule
% \textbf{Model}    & \textbf{MMLU} & \textbf{Winogrande} & \textbf{IFEval} & \textbf{GSM8K}  & \textbf{ARC-C} & \textbf{Avg}  \\
% \midrule
% Mistral-7B        & 59.2 & 79.1 & 47.7 & 41.4  & 63.4 & 58.2 \\
% + WildLong        & 59.4 & 76.7 & 49.0 & 40.6  & 61.1 & 57.4 \\
% \midrule
% LLaMA 3.1-8B      & 68.6  & 78.1 & 79.4 & 85.5 & 64.7 & 75.8 \\
% + WildLong        & 68.5  & 77.7 & 80.2 & 84.1 & 66.0 & 75.9 \\
% \bottomrule
% \end{tabular}
% \caption{Short-context task performance comparison. Models fine-tuned with our method preserve short-context capabilities.}
% \label{tab:short_performance}
% \end{table}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/short_context_performance.pdf}
    \caption{Comparison of short-context performances between finetuned and the baseline models. Models fine-tuned with our method preserve short-context capabilities.}
    \label{fig:short_performance}
\end{figure}


% \textbf{Short context performance is preserved without mixing short-context data.} Previous works \citep{an2024make, bai2024longalign, zhang2024longreward} often mix short-context instruction-tuning data into the finetuning data to prevent the model from degradation in short-context capabilities after long-context alignment. In contrast, our approach exclusively employs long-context data yet achieves robust performance on short-context tasks, maintaining an average score of 75.2 across six tasks compared to 75.3 from the original short-context model (Table \ref{fig:short_performance}).
% This highlights the effectiveness of our method, where finetuning with a general and realistic long-context instruction-tuning dataset not only significantly enhances long-context capabilities but also preserves short-context performance without the need for additional data mixing. 
\textbf{Short context performance is preserved without mixing short-context data.} Previous works \citep{an2024make, bai2024longalign, zhang2024longreward} mix short-context instruction-tuning data into the finetuning data to mitigate degradation in short-context capabilities after long-context alignment. 
% In contrast, our approach exclusively employs long-context data while effectively preserving short-context performance. Referring to Figure \ref{fig:short_performance}, we maintain an average score of 75.9 for Llama-3.1-8B,, comparable to the baseline 75.8. For Mistral-7B, we observe a slight drop from 58.2 to 57.4, which we hypothesize results from changes in RoPE $\theta$; we analyze this further in Section \ref{mistral_analysis_rope}.
% These results underscore the effectiveness of our dataset: finetuning on general, realistic long-context data significantly enhances long-context capabilities while largely preserving short-context performance without additional data mixing.
In contrast, our approach exclusively employs long-context data while effectively preserving short-context performance. Referring to Figure \ref{fig:short_performance}, we maintain an average score of 75.9 for Llama-3.1-8B, comparable to the baseline 75.8. For Mistral-7B, we observe a slight drop of less than one point, potentially due to changes in RoPE base. We analyze this further in Section \ref{mistral_analysis_rope}.
These results underscore the effectiveness of our dataset: finetuning on general, realistic long-context data significantly enhances long-context capabilities while largely preserving short-context performance without additional data mixing.

% Moreover, while recent work \citep{gao2024prolong} observed deteriorating results when incorporating long-context data into finetuning the long-context model, our method demonstrates that high-quality and general-purpose long-context data can instead strengthen the model's long-context capabilities. 

\begin{wrapfigure}{r}{0.5\textwidth}
    \includegraphics[width=0.9\linewidth]{Figures/performance_length.pdf}
    % \vspace{0.1in}
    \caption{Short-context and long-context performance of variations of Mistral models. }
    \label{fig:performance_length}
    % \vspace{-0.4in}
\end{wrapfigure}


\subsection{Ablation Studies}
\label{mistral_analysis_rope}
We conduct comprehensive ablation studies to investigate the efficacy of our data synthesis framework.

\textbf{Effectiveness of graph-based modeling.}
% A straightforward alternative method to synthesize long-context instruction tuning dataset is directly extracting the instructions from the user-chat conversations from WildChat and pair these instructions with the long documents from the pretraining corpus. We synthesize 20k data with the extracted instructions, denoted as simple-instruct. We also finetune directly with 20k of the filtered WildChat subset. For fair comparsion, we sample 20k data that are synthesized with our graph-based approach. All data used in this experiment only involve single-document context. The results are shown in Table \ref{tab:graph_ablation}. 
% Compared with simple-instruct and the filtered WildChat data, our graph-based method shows consistent improvement on long-context benchmarks. 
To evaluate the effectiveness of our graph-based instruction generation approach, we compare it against two baseline methods for synthesizing long-context instruction-tuning datasets, using 20k samples for each setting. The first baseline, denoted as simple-instruct, directly extracts instructions from user-chat conversations in WildChat and pairs them with long documents sampled from SlimPajama. The second baseline, denoted as WildChat-long, finetunes directly on samples from the filtered long WildChat subset.
We finetune both Mistral-7B-Instruct-v0.2 and Llama-3.1-8b-instruct using these three datasets and evaluated them on the RULER benchmark. As shown in Table \ref{tab:ablation_graph}, our graph-based method consistently outperforms the baselines. In particular, Mistral-7B achieves a score of 78 with WildLong, outperforming WildChat-Long and Simple-Instruct by +5 and +3.8 points. We suspect the improved performance, particularly on complex tasks like aggregation and variable tracking, arises from the graph-based method’s ability to generate more diverse and challenging instructions while preserving generalizability. 
% Longer instruction lengths observed in the graph-based method further suggest increased complexity, which may contribute to its effectiveness in enhancing long-context capabilities.



\textbf{Effectiveness of multi-document data.}
We assess the impact of multi-document data by fine-tuning both Mistral and Llama models on 20k datasets across three settings: single-document, multi-document, and a mix of both. As shown in Table \ref{tab:ablation_multi}, the results reveal varying effects depending on the model and task.
For the Mistral models, the multi-document setting significantly enhances performance on tasks requiring complex reasoning, such as variable tracking (VT) and aggregation (Agg). In contrast, single-document data proves more effective for QA tasks, which focus on extracting specific information from a single source.
For the Llama models, the effect of multi-document data is less pronounced. The multi-document setting performs slightly worse on aggregation (69.3 vs. 70.5) and QA (67.0 vs. 68.2) compared to the single-document setting. However, the mixed setting achieves the highest performance on variable tracking (93.7 vs. 93.0 for both single and multi) and matches the single-document setting in overall average performance (82.6).
These findings suggest that while single-document and multi-document data have distinct strengths, combining them provides greater diversity and balance, enabling models to perform robustly across a wide range of tasks.
% Single-document data mainly contribute to the fundamental long-context capabilities as demonstrated superior performance on synthetic benchmark such as RULER, while multi-document data are more effective for improving the multihop and potentially higher-level reasoning tasks as shown by the slightly better results on HELMET. 

% \subsection{Scaling Effect of Data Size}
% \subsection{Length Extrapolation Capabilities}


\textbf{Effectiveness of WildLong under RoPE Scaling.} 
We investigate the impact of modifying the RoPE base parameter to extend the context length of the Mistral-7B model. Specifically, we compare three variants: (1) Mistral-7B (Baseline): The original model with context length 32k and RoPE base $1e6$, (2) Mistral-7B (RoPE $1e7$): Extended RoPE base of $1e7$, and (3) Mistral-7B (Ours):  RoPE base $1e7$, finetuned with our WildLong data. Performance is evaluated on short-context tasks (\textless 1k) and long-context tasks (4k-128k).

The length-wise performance is shown in Figure \ref{fig:performance_length}. Our results reveal that increasing the RoPE base parameter enables support for longer contexts, improving performance on tasks requiring extreme context lengths (e.g., 64k-128k, +18.6 and +32.7 points over the baseline Mistral-7B respectively). 
However, this adjustment comes with a significant trade-off, as short-context performance drops markedly from 58.2 to 55.0, and performance on mid-range context lengths (4k-8k) also slightly declines. 
Finetuning with WildLong mitigates these trade-offs, recovering short-context performance to 57.4 while further boosting mid- and long-context performance.
This analysis highlights that while extending RoPE theta directly allows models to process longer contexts, it introduces a clear trade-off in short-context capability. Finetuning with generalized long-context datasets, such as Wildlong, not only recovers some short-context degradation but also enhances performance across mid-range and extended contexts. These findings address a gap in prior research and emphasize the importance of finetuning strategies to balance short- and long-context performance effectively.


% \begin{wraptable}{r}{7.2cm}
% \centering
% \vspace{-0.12in}
% \begin{tabular}{lcc}
% \toprule
% Model             & RULER & HELMET \\
% \midrule
% LLaMA 3.1-8B      & 87.7  & 47.4   \\
% + wildchat        & 88.7  & 48.2   \\
% + simple-instruct & 88.8  & 48.7   \\
% + graph-instruct  & 89.2  & 48.9   \\
% \bottomrule
% \end{tabular}
% % \vspace{-0.12in}
% \caption{Our preliminary experiments with 20k data shows graph-based synthetic instruction consistently improves the performance of Llama-3.1-8b-instruct on RULER and HELMET benchmarks.}
% % \vspace{-0.18in}
% \label{tab:ablation_20k_data}
% \end{wraptable}


% short context results
% MMLU 5-shots
% arc-c 25-shots
% Hellaswag 10-shots
% Winogrande 5-shots
% MT-bench 

% \begin{table}[]
% \centering
% \begin{tabular}{lcc|ccccccc}
% \toprule
% Model             & RULER & HELMET & MMLU & ARC-C & HellaSwag & Winogrande & IFEval & GSM8K & Avg  \\
% \midrule
% LLaMA 3.1-8B      & 80.5 & 47.9 & 68.6  & 61.9 & 78.4 & 78.1 & 79.4 & 85.5 & 75.3 \\
% + wildchat        & 82.2 & 48.2 & 68.8  & 62.1 & 79.0 & 78.8 & 79.0 & 84.5 & 75.4 \\
% + simple-instruct & 82.2 & 48.7 & 68.6  & 62.0 & 79.9 & 78.1 & 81.3 & 84.5 & 75.7 \\
% + graph-instruct  & 82.6 & 48.9 & 68.7  & 61.9 & 79.7 & 78.5 & 80.0 & 84.8 & 75.6 \\
% + all             & 84.1 & 50.3 & 68.5  & 61.5 & 79.2 & 77.7 & 80.2 & 84.1 & 75.2 \\
% \bottomrule
% \end{tabular}
% \caption{Performance on short-context tasks.}
% \end{table}

% \begin{table}[]
% \centering
% \begin{tabular}{lcc|ccccc}
% \toprule
% Model             & RULER & HELMET & MMLU & Winogrande & IFEval & GSM8K & Avg  \\
% \midrule
% LLaMA 3.1-8B      & 80.5 & 47.9 & 68.6  & 78.1 & 79.4 & 85.5 & 77.9 \\
% + wildchat        & 82.2 & 48.2 & 68.8  & 78.8 & 79.0 & 84.5 & 77.8 \\
% + simple-instruct & 82.2 & 48.7 & 68.6  & 78.1 & 81.3 & 84.5 & 78.1 \\
% + graph-instruct  & 82.6 & 48.9 & 68.7  & 78.5 & 80.0 & 84.8 & 78.0 \\
% + all             & 84.1 & 50.3 & 68.5  & 77.7 & 80.2 & 84.1 & 77.6 \\
% \bottomrule
% \end{tabular}
% \caption{Performance on short-context tasks.}
% \label{tab:short_performance}
% \end{table}


% \begin{table}[]
% \centering
% \begin{tabular}{lcc}
% \toprule
% Model             & RULER & HELMET  \\
% \midrule
% LLaMA 3.1-8B      & 80.5 & 47.9  \\
% + wildchat        & 82.2 & 48.2  \\
% + simple-instruct & 82.2 & 48.7  \\
% + graph-instruct  & 82.6 & 48.9  \\
% + all             & 84.1 & 50.3  \\
% \bottomrule
% \end{tabular}
% \caption{Performance comparison among our synthetic data framework. }
% \label{tab:graph_ablation}
% \end{table}



% \begin{table}[]
% \scalebox{0.9}{
% \begin{tabular}{l|ccccc|cccccc}
% \toprule
% \multirow{2}{*}{Models}                & \multicolumn{5}{c|}{RULER}  & \multicolumn{6}{c}{HELMET}  \\
%                  & NIAH & VT   & Agg  & QA   & Avg      & RAG  & ICL  & Cite & Re-rank & LongQA & Avg   \\
% \midrule
% + single-doc 50k & 98.8 & 95.1 & 71.5 & 68.1 & 83.4     & 66.6 & 78.2 & 19.6 & 41.4 & 39.5 & 49.1 \\
% + multi-doc 50k  & 98.8 & 93.7 & 71.2 & 67.5 & 82.8     & 66.6 & 78.4 & 20.5 & 42.0 & 39.6 & 49.4 \\
% \bottomrule
% \end{tabular}}
% \caption{Performance comparison between single-doc data and multi-doc data.}
% \end{table}

\begin{table}[ht]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\caption{Effect of graph-based modeling adopted by WildLong compared with two baseline methods.}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll|ccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{5}{c}{\textbf{RULER}} \\
                       &                          & \textbf{NIAH} & \textbf{VT} & \textbf{Agg} & \textbf{QA} & \textbf{Avg} \\
\midrule
\multirow{3}{*}{Mistral}   & WildChat-long     & 87.7	& 84.2	 & 59.9	 & 60.3	 & 73.0 \\
                           & Simple Instruct   & 89.5	 & 90.2	 & 52.6	 & \textbf{64.3}	 & 74.2 \\ 
                           & WildLong          & \textbf{91.4}	 & \textbf{92.0}	 & \textbf{64.7}	 & 63.9	 & \textbf{78.0} \\
\midrule
\multirow{3}{*}{LLaMA}     & WildChat-long     & 98.2	& 93.5	 & 69.3	 & 67.7	 & 82.2 \\
                           & Simple Instruct   & 98.5	 & \textbf{94.1}	 & 67.6	 & \textbf{68.7}	 & 82.2 \\ 
                           & WildLong          & \textbf{98.9}	 & 93.7	 & \textbf{70.0}	 & 67.7	 & \textbf{82.6} \\
\bottomrule
\end{tabular}
\label{tab:ablation_graph}
}
\end{minipage}%
\hspace{0.5cm} 
\begin{minipage}[b]{0.45\textwidth}
\centering
\caption{Performance comparison among single-document, multi-document, and a mixture of single- and multi-document data.}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll|ccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{5}{c}{\textbf{RULER}} \\
                       &                          & \textbf{NIAH} & \textbf{VT} & \textbf{Agg} & \textbf{QA} & \textbf{Avg} \\
\midrule
\multirow{3}{*}{Mistral}   & Single            & 91.6	& 90.9	& 63.9	& 64.2	& 77.7 \\
                           & Multi             & 92.1	 & 94.4	 & 66.9	 & 64.1	 & 79.4 \\
                           & WildLong          & 91.4	 & 92.0	 & 64.7	 & 63.9	 & 78.0 \\
\midrule               
\multirow{3}{*}{LLaMA}     & Single            & 98.6	& 93.0	& 70.5	& 68.2	& 82.6 \\
                           & Multi             & 98.8	 & 93.0	 & 69.3	 & 67.0	 & 82.0 \\
                           & WildLong          & 98.9	 & 93.7	 & 70.0	 & 67.7	 & 82.6 \\
\bottomrule
\end{tabular}
\label{tab:ablation_multi}}
\end{minipage}
\end{table}


% \begin{table}[]
% \caption{Performance comparison on RULER using 20k datasets from three settings: WildChat-Long (filtered long WildChat data), Simple Instruct (simplified instructions extracted from WildChat and paired with long documents from SlimPajama), and Ours (graph-based instructions paired with long documents from SlimPajama).}
% \centering
% \begin{tabular}{ll|ccccc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{5}{c}{\textbf{RULER}} \\
%                        &                          & \textbf{NIAH} & \textbf{VT} & \textbf{Agg} & \textbf{QA} & \textbf{Avg} \\
% \midrule
% \multirow{3}{*}{Mistral}   & WildChat-long     & 87.7	& 84.2	 & 59.9	 & 60.3	 & 73.0 \\
%                            & Simple Instruct   & 89.5	 & 90.2	 & 52.6	 & \textbf{64.3}	 & 74.2 \\ 
%                            & WildLong          & \textbf{91.4}	 & \textbf{92.0}	 & \textbf{64.7}	 & 63.9	 & \textbf{78.0} \\
% \midrule
% \multirow{3}{*}{LLaMA}     & WildChat-long     & 98.2	& 93.5	 & 69.3	 & 67.7	 & 82.2 \\
%                            & Simple Instruct   & 98.5	 & \textbf{94.1}	 & 67.6	 & \textbf{68.7}	 & 82.2 \\ 
%                            & WildLong          & \textbf{98.9}	 & 93.7	 & \textbf{70.0}	 & 67.7	 & \textbf{82.6} \\
% \bottomrule
% \end{tabular}
% \label{tab:ablation_graph}
% \end{table}


% \begin{table}[]
% \caption{Performance comparison on RULER using 20k datasets from three settings: single-document data, multi-document data, and a mixture of single- and multi-document data.}
% \centering
% \begin{tabular}{ll|ccccc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{5}{c}{\textbf{RULER}} \\
%                        &                          & \textbf{NIAH} & \textbf{VT} & \textbf{Agg} & \textbf{QA} & \textbf{Avg} \\
% \midrule
% \multirow{3}{*}{Mistral}   & Single            & 91.6	& 90.9	& 63.9	& 64.2	& 77.7 \\
%                            & Multi             & 92.1	 & 94.4	 & 66.9	 & 64.1	 & 79.4 \\
%                            & WildLong          & 91.4	 & 92.0	 & 64.7	 & 63.9	 & 78.0 \\
% \midrule               
% \multirow{3}{*}{LLaMA}     & Single            & 98.6	& 93.0	& 70.5	& 68.2	& 82.6 \\
%                            & Multi             & 98.8	 & 93.0	 & 69.3	 & 67.0	 & 82.0 \\
%                            & WildLong          & 98.9	 & 93.7	 & 70.0	 & 67.7	 & 82.6 \\
% \bottomrule
% \end{tabular}
% \label{tab:ablation_multi}
% \end{table}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{Figures/performance_length.pdf}
%     \caption{Short-context and long-context performance of variations of Mistral-7B models. }
%     \label{fig:performance_length}
% \end{figure}

