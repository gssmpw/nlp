\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{comment}
\usepackage{tikz}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    }
\usepackage{url}
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage[caption=false,font=footnotesize]{subfig}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\input{symbols.tex}
\definecolor{pleasant_red}{HTML}{FFB6C1}  % Light pinkish red
\definecolor{golden_color}{HTML}{FFD700}   % Golden color
\definecolor{spring_green}{HTML}{00FF7F}   % Bright and vibrant green
%% Title
\title{Unlocking Efficient Large Inference Models: One-Bit Unrolling Tips the Scales}

\author{
  Arian Eamaz$~^{*}$\\
  Department of Electrical and Computer Engineering \\
  University of Illinois Chicago \\
  Chicago, IL, USA \\
  \texttt{aeamaz2@uic.edu} \\
  \And Farhang Yeganegi \thanks{The first two authors contributed equally to this work.} \\
  Department of Electrical and Computer Engineering \\
  University of Illinois Chicago \\
  Chicago, IL, USA \\
  \texttt{fyegan2@uic.edu} \\
  \And Mojtaba Soltanalian \\
  Department of Electrical and Computer Engineering \\
  University of Illinois Chicago \\
  Chicago, IL, USA \\
  \texttt{msol@uic.edu} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
Recent advancements in Large Language Model (LLM) compression, such as BitNet and BitNet b1.58, have marked significant strides in reducing the computational demands of LLMs through innovative one-bit quantization techniques. We extend this frontier by looking at Large Inference Models (LIMs) that have become indispensable across various applications. However, their scale and complexity often come at a significant computational cost. We introduce a novel approach that leverages one-bit algorithm unrolling, effectively integrating information from the physical world in the model architecture. Our method achieves a bit-per-link rate significantly lower than the 1.58 bits reported in prior work, thanks to the natural sparsity that emerges in our network architectures.
We numerically demonstrate that the proposed one-bit algorithm unrolling scheme can improve both training and test outcomes by effortlessly increasing the number of layers while substantially compressing the network. Additionally, we provide theoretical results on the generalization gap, convergence rate, stability, and sensitivity of our proposed one-bit algorithm unrolling.
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
\label{sec1}
Large neural networks, including Large Language Models (LLMs), have become essential in nearly every inference task due to their exceptional performance. However, this performance comes at a significant cost: scale. For example, Llama 2 and GPT-4 have $65$ billion and $1.76$ trillion parameters, respectively \cite{ghane2024one}. These models face challenges related to their size, such as requiring vast amounts of memory for storage and retrieval, consuming substantial power, and being inefficient in communicating over networks or storage for inference. This highlights the need for efficient methods like model quantization and pruning. Quantization involves using fewer bits to store each model parameter, while pruning involves setting some parameter values to zero \cite{guo2018survey,frantar2023qmoe,liang2021pruning}. One effective way to significantly compress a network through quantization is by applying coarse quantization, or one-bit quantization, which converts parameters into binary values \cite{nagel2021white,zhang2022quantization}. 

%Recent studies show that quantization often outperforms pruning, except in scenarios with extremely high compression ratios, where pruning might offer accuracy benefits \cite{liang2021pruning}. 
A recent paper demonstrates that an LLM with just $1.58$-bit or ternary weights in its linear layers can perform as well as a high-resolution model \cite{ma2024era}. DeepSeek \cite{guo2024deepseek} has demonstrated a remarkable ability to achieve state-of-the-art performance in LLMs while significantly reducing computational resource needs through innovative use of $8$-bit quantization. In our paper, we take a step beyond quantization alone and show that leveraging the knowledge of the original problem, including models and domain structure, can significantly enhance computational efficiency and reduce the amount of data required during the training process. The associated process, called deep unrolling, leads to a 
%In specific neural networks, 
network sparsity that naturally comes from the 
%network's architecture or 
underlying domain knowledge, eliminating the need for techniques like pruning or manually identifying and zeroing out specific neurons \cite{zhang2023physics}. Compared to traditional neural networks,  Deep Unrolling Networks (DUNs) typically have far fewer parameters and are sparse by nature
%, as they incorporate domain knowledge through the unrolling process 
\cite{yang2018admm} (afforded by the integration of the problem-level reasoning or the model, see Fig.~\ref{figure_0}). These networks are a form of Learning to Optimize (L2O) models, specifically structured to solve optimization and inference problems efficiently \cite{chen2022learning}. 

%Traditional optimization methods rely on well-established techniques like gradient descent, all of which are grounded in strong theoretical foundations. These methods are typically concise in their formulation, and their theoretical guarantees assure a certain level of performance. When solving an optimization problem, one can choose a method suited to the problem type and expect it to deliver a solution consistent with its theoretical promises \cite{chen2022learning,gilton2021deep}.

%L2O develops optimization methods by training on sample problems. While it may lack the theoretical rigor of classical methods, L2O improves performance during training. 
Although training is often resource-intensive and offline, a worthy goal is to achieve faster online application times. For complex problems like nonconvex optimization, where solutions are hard to find, a well-trained L2O method can outperform traditional approaches, often delivering higher-quality results within similar computational budgets \cite{chen2020understanding,yang2020learning}. For instance, L2O can recover sparse signals more accurately and efficiently than LASSO \cite{chen2018theoretical,chen2021hyperparameter,aberdam2021ada}.

%In \emph{algorithm unrolling}, each iteration of an algorithm is represented as a network layer. Stacking these layers forms a deep neural network, where forward passes simulate running the algorithm iteratively.

%The L2O methods discussed in \cite{chen2018theoretical,liu2019alista}, Learned Iterative Shrinkage Thresholding Algorithm (LISTA), significantly outperform traditional ISTA/FISTA optimizers, converging much faster. 
%L2O approaches can be applied across various applications, including image restoration and reconstruction, medical and biological imaging \cite{zhang2023physics}, and wireless communication \cite{balatsoukas2019deep}. 

While L2O methods offer significant advantages over classical approaches, they have drawbacks. For challenging problems, increasing the number of iterations is straightforward and computationally manageable in classical methods. However, in DUNs, adding iterations equates to adding layers and parameters, such as weight matrices, which can be inefficient for inference. Pruning the weights may be not a viable solution because each weight in a DUN plays a critical role in solving the primary objective, and removing them would be counterproductive. Therefore, a more practical approach to compress the network is to employ weight quantization, especially for linear transformation weights. 

\subsection{Motivations and Contributions}

Quantized DUNs tend to be both sparse and quantized simultaneously. To the best of our knowledge, this paper introduces, for the first time, a one-bit DUN, where the linear components of each layer are reduced to one-bit. This architecture inherently results in a network that is both sparse and binary (or ternary) in nature. However, a key difference between this approach and the quantization method proposed in \cite{ma2024era} is that our network is not truly ternary. Unlike traditional ternary schemes, there is no need to identify and zero out specific neurons. Instead, the sparsity arises from the structure of the DUN and \textit{is a priori known}, making it purely binary.

To quantize the weights, we employ the Quantization-Aware Training (QAT) approach. By simulating the quantization process during training or fine-tuning, the network can adapt to quantization noise \cite{nagel2021white}. A key drawback of the QAT scheme is the necessity for pre-training to achieve meaningful results, which can be challenging for large-scale networks \cite{nagel2020up}. However, this issue can be significantly mitigated if the network is sparse, like DUN, leveraging the problem's domain knowledge. This requires a much smaller dataset for training compute footprint, as demonstrated in our numerical results.

The scale of quantization is crucial, as it significantly impacts both the resolution and overall performance of the quantization process. Therefore, choosing the appropriate scale parameter is essential for effective quantization. Unlike the method used in \cite{wang2023bitnet,ma2024era}, where the quantizer scale is calculated through a closed-form solution to minimize the squared error between high-resolution weights and scaled binary weights, we propose a data-driven approach. Our method learns the optimal scale for one-bit quantization based on the available data. Additionally, while the approach in \cite{wang2023bitnet,ma2024era} designs a separate scale for each layer, our scheme uses a single scale for all layers, reducing the number of parameters. 

One of the key advantages of using one-bit weights is the ability to efficiently increase the number of layers or iterations in the algorithm. We demonstrate numerically that our one-bit DUN, with more than four times the layers, outperforms the high-resolution network in both the training and inference stages. While loss is generally expected to decrease monotonically with each layer in an unrolled algorithm, inconsistencies can appear in the convergence. Despite this issue, our one-bit DUN approximates monotonic behavior more closely than the high-resolution network.

%The proposed hybrid method combines the low computational cost of deep neural networks during execution with the integration of problem-level reasoning and one-bit weights. This emerging network is particularly well-suited for scalable learning applications, thanks to its reduced degrees of freedom required for training. Leveraging prior knowledge of the system or inference model, the network inherently exhibits a sparse structure. We further enhance its efficiency and reduce costs by binarizing its linear weights. Despite utilizing binary weights, the network outperforms its high-resolution counterpart because it allows for the easy expansion of layers. We demonstrate that, with the one-bit configuration, adding more layers not only maintains high compression rates but also improves performance.% Consequently, one-bit DUN holds significant potential for use in a new generation of deep signal processing and computing architectures that are both interpretable and ideally suited for large-scale scenarios.

In \cite{chen2020understanding}, the generalization ability of algorithm unrolling for quadratic programs was comprehensively discussed. The study demonstrated that the generalization gap depends on key algorithmic properties, such as the convergence rate. Beyond the simple gradient descent algorithm, the authors analyzed the critical features of their proposed algorithm. However, they were unable to derive a convergence rate when a neural network was employed as the update process.

In this paper, we extend this analysis by investigating sparse quadratic programs, using sparse signal recovery as a representative example, and providing the convergence rate for our one-bit algorithm unrolling. A key distinction of our approach is that our proposed algorithm unrolling transforms each iteration of the algorithm into a layer of the unrolled network. This differs from \cite{chen2020understanding}, where the same network (with at least $10$ layers) is used for each update process at every iteration.

From a theoretical perspective, to ensure the convergence rate of LISTA, the authors of \cite{chen2018theoretical} demonstrated that the weights of the DUN must belong to a specific set, referred to as ``good" weights, and proved that this set is non-empty. However, ensuring that this set is non-empty or that binary weights can satisfy these conditions presents significant challenges in our work. To address this issue for one-bit LISTA, we modify the proof framework for the convergence rate, adapting it to establish more relaxed conditions that are achievable even with binary matrices. Recent works assume a fixed sensing matrix \cite{chen2018theoretical,chen2021hyperparameter,aberdam2021ada,yang2020learning}. However, in our paper, we analyze the robustness of our algorithm even when the sensing matrix varies.

\emph{Notation:} Throughout this paper, we use bold lowercase and bold uppercase letters for vectors and matrices, respectively. $\|\cdot\|$ denotes the second norm for both vector and matrix. The set $[n]$ is defined as $[n]=\left\{1,\cdots,n\right\}$. The covering number
$\mathcal{N}\left(\epsilon,\mathcal{F},\|\cdot\|_k\right)$ is defined as the cardinality of the smallest subset $\mathcal{F}^{\prime}$ of $\mathcal{F}$ for which every element of
$\mathcal{F}$ is within the $\epsilon$-neighborhood of some element of $\mathcal{F}^{\prime}$ with respect to the $k$-th norm $\|\cdot\|_k$. We represent a vector $\mathbf{x}$ and a matrix $\mbB$ in terms of their elements as $\mathbf{x}=[x_{i}]$ and $\mathbf{B}=[B_{i,j}]$, respectively. $(\cdot)^{\top}$ is the vector/matrix transpose. 
%The Frobenius norm of a matrix $\mathbf{B}\in \mathbb{C}^{M\times N}$ is defined as $\|\mathbf{B}\|_{\mathrm{F}}=\sqrt{\sum^{M}_{r=1}\sum^{N}_{s=1}\left|b_{rs}\right|^{2}}$, where $b_{rs}$ is the $(r,s)$-th entry of $\mathbf{B}$. 
The hard-thresholding (HT) operator is defined as 
\begin{equation}
\operatorname{HT}(x) = x \mathbb{I}_{x\geq 0},      
\end{equation}
where $\mathbb{I}_{x\geq 0}$ is the indicator function. The soft-thresholding operator is represented by
\begin{equation}
\operatorname{ST}_{\theta}(x) = \operatorname{sgn}(x) \left(|x|-\theta\right).
\end{equation}
We define the operators $\operatorname{Ro}(\mbx_{1:u})$ and $\operatorname{Co}(\mbx_{1:u})$ as the row-wise and column-wise concatenations of the vectors $\{\mbx_i\}_{i=1}^u$, respectively. For a matrix $\mbB$, the operator $\operatorname{BlockDiag}_{u}(\mbB)$ constructs a block diagonal matrix by repeating the matrix $\mbB$, $u$ times along its diagonal entries.

%-----------------------------------------------------
\begin{figure}[t]
	\centering
		{\includegraphics[width=0.7\columnwidth]{deep_unfolding3.eps}}
	\caption{General DNNs vs DUNs. DUNs appear to be an excellent tool in scalable or real-time machine
learning applications due to the smaller degrees of freedom required for training and execution.}
\label{figure_0}
\end{figure}
%-----------------------------------------------------

\section{Preliminaries}
\label{sec2}
Assume we have the following optimization problem:
\begin{equation}
\label{eq1}
\mathcal{P}:~\underset{\mbx\in\mathcal{K}}{\textrm{minimize}}\quad f(\mbx)+\gamma \mathcal{R}(\mbx),
\end{equation}
where the target parameter $\mbx$ belongs to an arbitrary set $\mathcal{K}$ and the regularizer $\mathcal{R}(\cdot)$ is applied to $\mbx$
%the parameter 
with a tuning parameter $\gamma$. 
%If the objective is differentiable, one approach to solving the problem is to use the gradient descent method, where for iteration $k = 1, 2, \cdots$, we set
%\begin{equation}
%\label{eq2}
%\mbx_{k+1}=\mbx_{k}-\alpha \nabla f(\mbx_{k})-\alpha\gamma \nabla \mathcal{R}(\mbx_{k}).
%\end{equation}
%The core idea of L2O is to substitute the gradient term, specifically the gradient of the regularizer, with a neural network, denoted as $\mathcal{N}_{\btheta}$, where the weights $\btheta$ are learned from the training data,
%\begin{equation}
%\label{eq3}
%\gamma \nabla \mathcal{R}(\mbx_{k})\rightarrow \mathcal{N}_{\btheta}(\mbx_{k}).
%\end{equation}
This paper focuses on a hybrid architecture where the neural module is an energy function of the form 
$E_{\phi}((\bxi,\mbb),\mbx)=\frac{1}{2}\mbx^{\top}\mbQ_{\phi}(\bxi)\mbx-\mbb^{\top}\mbx$, with $\mbQ_{\phi}$ a neural network that maps $\bxi$ to a matrix, and $\mbx\in\mathcal{K}_{\mathcal{S}}$ where $\mathcal{K}_{\mathcal{S}}$ is defined as
\begin{equation}
\mathcal{K}_{\mathcal{S}}\triangleq\{\mbx=[x_i]\in\mathbb{R}^n:x_{i}=0, \forall i\in[n]\setminus\mathcal{S},|\mathcal{S}|=s\}.
\end{equation}
This problem is equivalent to the sparse quadratic program. 
Note that we consider the set $\mathcal{S}$ fixed but unknown in our settings.
%denotes the sparse set defined in Section~\ref{sec2-1}. 
Each energy $E_{\phi}$ can be uniquely represented by $(\mbQ_{\phi}(\bxi),\mbb)$, so we can write the overall optimization problem as 
\begin{equation}
\label{a1}
\begin{aligned}
\mathcal{C}:\quad\underset{\mbx\in\mathcal{K}_{\mathcal{S}}}{\textrm{minimize}} \quad E_{\phi}((\bxi,\mbb),\mbx).
\end{aligned}
\end{equation}
%Interestingly, the CS problem in Section~\ref{sec2-1} can be represented as the above minimization problem. 
To solve this problem, one can employ the following unrolling algorithm:
\begin{equation}
\mbx_{k}=f_{\btheta,\phi}(\bxi,\mbb)\triangleq\operatorname{Alg}^{k}_{\btheta}\left(\mbQ_{\phi}(\bxi),\mbb\right),~k\in[K],
\end{equation}
with the algorithm parameter set $\btheta_k$ at $k$-th iteration. Assume we are given a set of $N$ i.i.d. samples $\mathcal{S}_N=\left\{\left((\bxi_i,\mbb_i),\mbx^{\mathrm{opt}}(\mbQ^{\mathrm{opt}}_i,\mbb_i)\right)\right\}_{i=1}^N$, where the labels $\mbx^{\mathrm{opt}}(\mbQ^{\mathrm{opt}}_i,\mbb_i)$ are given by the exact minimizer of \eqref{a1}.
After processing $K$ iterations, the learning problem is then to find the best model $f_{\btheta,\phi}(\cdot)$ from the space $\mathcal{F}\triangleq\left\{f_{\btheta,\phi}:(\btheta,\phi) \in\Theta\times\Phi\right\}$ by minimizing the empirical loss function
%To optimize the learnable parameters $\btheta\triangleq\{\btheta_i\}_{i=1}^k$, the training process will solve the following problem:
\begin{equation}
\underset{\btheta,\phi}{\textrm{minimize}} \quad \mathbb{P}_N\ell_{\btheta,\phi},
\end{equation}
where %\par\noindent\small
\begin{equation}
\mathbb{P}_{N}\ell_{\btheta,\phi}=\frac{1}{N}\sum^{N}_{i=1}\left\|\operatorname{Alg}^{k}_{\btheta}\left(\mbQ_{\phi}(\bxi_i),\mbb_i\right)-\mbx^{\mathrm{opt}}(\mbQ_i,\mbb_i)\right\|.
\end{equation} %\normalsize

\subsection{Algorithm Unrolling}
\label{sec2-1}
We focus here on a specific deep unrolling method based on the gradient descent algorithm.
%although many other variants exist based on the alternating direction method of multipliers (ADMM), approximate message passing (AMP), and conditional gradient method (CGM). 
For the sparse quadratic program $\mathcal{C}$, one can write the following iterative process at each iteration (we replace $\mbQ_{\phi}(\bxi)$ by $\mbQ$ for simplicity of notation):
\begin{equation}
\label{update_sqp}
\mbx_{k} = \mathcal{H}_{\btheta}\left(\delta\mbx_{k-1}-\alpha\left(\mbQ\mbx_{k-1}-\mbb\right)\right),    
\end{equation}
where $0<\delta\leq 1$, the learnable parameters are $\left\{\btheta,\alpha\right\}$ and $\mathcal{H}_{\btheta}(\cdot)$ is a trainable network. 

Using a neural network (typically with more than 10 layers) for each iteration can lead to training instability due to the large parameter space and challenges in providing theoretical guarantees \cite{gilton2021deep}. Moreover, a network used as a proximal operator may struggle to accurately replicate the behavior of the actual operator for structured parameters, such as sparse signals.

Another approach is to design a network based on the algorithm's iterative process, where the number of layers in the network corresponds to the number of algorithmic iterations. In this framework, the activation function within each layer mimics the proximal operator rather than utilizing a multi-layer network at each iteration:
\begin{equation}
\label{update_sqp_1}
\mbx_{k} = \operatorname{ST}_{\theta_k}\left(\delta\mbx_{k-1}-\mbW^{\top}_{k}\left(\mbQ\mbx_{k-1}-\mbb\right)\right),    
\end{equation}
where $\left\{\theta_k,\mbW_{k}\in\mathbb{R}^{n\times n}\right\}^{K}_{k=1}$ are trainable parameters. 
One popular example of sparse quadratic program can be found in the compressed sensing (CS) literature, where the measurements $\mby\in\mathbb{R}^{m}$ are linearly related to the sparse signal $\mbx\in\mathbb{R}^{n}\cap\mathcal{K}_s$, with $\mathcal{K}_s=\{\mbx\in\mathbb{R}^{n}:\|\mbx\|_{0}\leq s\}$, through a linear operator $\mbA\in\mathbb{R}^{m\times n}$ with $m<n$. To reconstruct the sparse signal $\mbx$, we have the following $\ell_{1}$-relaxed problem:
\begin{equation}
\label{eq9} 
\mathcal{P}^{(\text{CS})}:~\underset{\mathbf{x}}{\operatorname{minimize}}~\frac{1}{2}\|\mathbf{y}-\mathbf{A} \mathbf{x}\|^2+\gamma\|\mathbf{x}\|_1.
\end{equation}
A widely recognized method for addressing the problem in \eqref{eq9} is ISTA.
%,
%of the solutions of $\mathcal{P}^{\text{CS}}$ is ISTA, 
%which has the following update process:
%\begin{equation}
%\label{eq10}
%\mbx_k=\text{ST}_{\gamma/L}\bigl(\mbx_{k-1}+\frac{1}{L} \mbA^{\top}\bigl(\mby-\mbA \mbx_{k-1}\bigr)\bigr).
%\end{equation}
%where $\text{ST}_{\theta}\left(\cdot\right)=\operatorname{sign}(\cdot) \operatorname{max}(0, |\cdot|-\theta)$ is the soft-thresholding operator with threshold $\frac{\gamma}{L}$ and $L$ is usually taken as the largest eigenvalue of the Gram matrix $\mbQ=\mbA^{\top}\mbA$. 
To unroll ISTA, LISTA has been proposed with the following update process
%\begin{equation}
%\label{eq11}
%\mbx_{k}=\text{ST}_{\theta_{k}}\bigl(\mbW^{(1)}_{k}\mbx_{k-1}+ \mbW^{(2)}_{k}\mby\bigr),~k\in[K],
%\end{equation}
%with learnable parameters $\bigl\{\theta_{k}, \mbW^{(1)}_k, \mbW^{(2)}_{k}\bigr\}^{K}_{k=1}$. 
%It is straightforward that with $\mbW^{(1)} = \mbI-\frac{1}{L}\mbQ$, $\mbW^{(2)}= \frac{1}{L}\mbA^{\top}$ and $\theta = \frac{\gamma}{L}$, LISTA boils down to ISTA. 
%In \cite{chen2018theoretical}, it was demonstrated that the update process in \eqref{eq11} must meet certain necessary conditions for convergence. This insight led to the development of LISTA-CP, whose convergence was thoroughly examined under the following update formulation 
\cite{chen2018theoretical}:
\begin{equation}
\label{eq12}
\mathbf{x}_{k}=\operatorname{ST}_{\theta_{k}}\bigl(\mathbf{x}_{k-1}-\mathbf{W}_k^{\top}\bigl(\mathbf{A} \mathbf{x}_{k-1}-\mathbf{y}\bigr)\bigr),~k\in[K],
\end{equation}
where $\bigl\{\theta_{k},\mbW_{k}\bigr\}^{K}_{k=1}$ are the parameters to be trained. 
%In the remainder of the paper, we focus on the forward model in \eqref{eq12}. Specifically, our goal is to binarize the weight matrices in \eqref{eq12} while maintaining train/test loss that is comparable to the high-resolution counterpart.
\begin{comment}    
\subsection{Plug-and-Play}
ADMM is often used to solve $\mathcal{P}$ with the following update process:
\begin{equation}
\begin{aligned}
\label{eq13}
\mbx^{(k+1)}&=\underset{\mbx \in \mathbb{R}^m}{\operatorname{argmin}}\left\{\alpha^2 \gamma^2 \mathcal{R}(\mbx)+(1 / 2)\left\|\mbx-\left(\mbv^{(k)}-\mbu^{(k)}\right)\right\|^2_2\right\} \\
\mbv^{(k+1)}&=\underset{\mbv \in \mathbb{R}^m}{\operatorname{argmin}}\left\{\alpha f(\mbv)+(1 / 2)\left\|\mbv-\left(\mbx^{(k+1)}+\mbu^(k)\right)\right\|^2_2\right\}\\\mbu^{(k+1)}&=\mbu^{(k)}+\mbx^{(k+1)}-\mbv^{(k+1)},
\end{aligned}
\end{equation}
or equivalently,
\begin{equation}
\begin{aligned}
\label{eq14}
\mbx^{(k+1)} & =\operatorname{prox}_{\alpha^2\gamma^2\mathcal{R}}\left(\mbv^{(k)}-\mbu^{(k)}\right) \\
\mbv^{(k+1)} & =\operatorname{prox}_{\alpha f}\left(\mbx^{(k+1)}+\mbu^k\right) \\
\mbu^{(k+1)} & =\mbu^{(k)}+\mbx^{(k+1)}-\mbv^{(k+1)}.
\end{aligned}
\end{equation}
The first step of ADMM can be seen as a denoising process. Based on this simple observation, employing a well-known denoiser instead of the proximal operator, which is called Plug-and-Play (PnP), has been proposed. In \cite{ryu2019plug}, the DnCNN was employed as a denoiser and showed a great empirical performance. Additionally, Plug-and-play forward-backward splitting (PNP-FBS) was proposed, which has a simpler update process than ADMM and is similar to the proximal gradient descent \eqref{eq7}. By using the DnCNN, we have $\mathcal{H}_{\btheta}(\mbx) = \mbx-\boldsymbol{\rho}_{\btheta}(\mbx)$ where $\boldsymbol{\rho}_{\btheta}(\mbx)$ is DnCNN with learnable parameter $\btheta$. 
\end{comment}
\section{One-bit Unrolling: Training Process and Solvers}
\label{sec3}
%In this study, we employ Quantization-Aware Training (QAT), also referred to as during-training quantization. This approach integrates the quantization operator into the training process, applying the quantization to the trainable parameters as the model learns. After training, a final quantization step is performed on the trained parameters, enabling the storage of low-precision values for the inference phase.

Denote the weights or linear transformations within each layer by $\mbW^{(k)}=[W_{i,j}^{(k)}]\in\mathbb{R}^{n_1\times n_2}$ for $k\in[K]$. Our approach in QAT consists of two stages, discussed below.
%namely \textbf{stage \rom{1}} and \textbf{stage \rom{2}}.

\textbf{Stage \rom{1}}: In this stage, we consider an scaled one-bit quantization of weights. 
\begin{comment}
Specifically, we start by setting the scaling factor to $\lambda=\lambda_0$. Next, we apply the sign function to $\mbW^{(k)}$ matrices, preserving only the sign information. 
%To refine the quantization output, we introduce a scaling factor $\lambda$ post-one-bit quantization to minimize the $\ell_2$ error between the dynamic ranges of real-valued and one-bit weights. %Additionally, we utilize dithered one-bit quantization, where dithers, which remain constant, are tailored based on the weights for enhanced quantization performance. The dithered one-bit quantization for the linear transformation $\mbW\in\mathbb{R}^{n_1\times n_2}$ is obtained as
This process is expressed as follows:
\begin{equation}
\label{eq16}
R_{i,j}^{(k)}=\lambda_0\operatorname{sign}\bigl(W_{i,j}^{(k)}\bigr)= \begin{cases}+\lambda_0, & \text { if } W_{i,j}^{(k)}>0 \\ -\lambda_0, & \text { if } W_{i,j}^{(k)} \leq 0\end{cases}, \quad (i,j)\in[n_1]\times [n_2],\quad k\in[K].
\end{equation}
\end{comment}
%where the dither can be chosen based on the $\ell_1$-norm of pre-trained weights:
\begin{comment}
\begin{equation}
\label{eq17}
\tau = \frac{\sum_{i,j} \left|W_{i,j}\right|}{n_1 n_2}.
\end{equation}
\end{comment}
%By using the scale factor $\lambda$ defined as $\lambda = \sup_{i,j} \left|W_{i,j}\right|$, we have the following quantized parameters:
\begin{comment}
\begin{equation}
\label{eq18}
\widehat{\mbW} = \lambda \mbR \in\mathbb{R}^{n_1\times n_2}. 
\end{equation}
\end{comment}
%In this stage, based on our quantization scheme in \eqref{eq16}, 
Specifically, we investigate two approaches: (i) training with the straight-through gradient method or lazy projection, and (ii) conducting QAT as a regularized optimization process. The straight-through gradient method is equivalent to a projected gradient method, as follows:
\begin{equation}
\label{eq19}
\left\{\begin{array}{l}
\mbz^{(t)}=\lambda_{0}\operatorname{sign}\bigl(\btheta^{(t)}\bigr),\\
\btheta^{(t+1)}=\btheta^{(t)}-\eta_t \nabla \mathbb{P}_N\ell_{\btheta}|_{\btheta=\mbz^{(t)}},
%\mathcal{L}\left(\btheta\right)
\end{array}\right.
\end{equation}
where 
%$\mathcal{L}(\btheta)$ is defined below \eqref{eq8}, and 
$\btheta^{(t)}$ represents the network's parameters at the $t$-th epoch.
%It is important to emphasize that in this work, 
Notably, in this work, we apply the quantization operator exclusively to the network weights $\mbW^{(k)}$, leaving the activation function parameters unchanged:
\begin{equation}
\label{eq16}
\begin{aligned}
&\operatorname{sign}\bigl(\btheta\bigr)\triangleq\operatorname{sign}\bigl(W_{i,j}^{(k)}\bigr),~ (i,j)\in[n_1]\times [n_2],~ k\in[K],\\&\lambda_0\operatorname{sign}\bigl(W_{i,j}^{(k)}\bigr)\triangleq R_{i,j}^{(k)}=\begin{cases}+\lambda_0, & \text { if } W_{i,j}^{(k)}>0 \\ -\lambda_0, & \text { if } W_{i,j}^{(k)} \leq 0\end{cases}.
\end{aligned}
\end{equation}
%where, for simplicity, the epoch instance $t$ is omitted in \eqref{eq16}.

Utilizing the sign function at each iteration, akin to a \emph{hard projection}, could potentially result in the solution being ensnared in a poor local minimum. To help the solver in escaping these local optima and advancing towards a better solution, the hard projection can be alleviated through regularization \cite{bai2018proxquant}:
\begin{equation}
\label{eq20}
\mathcal{P}^{(\text{QAT})}:~\underset{\btheta}{\operatorname{minimize}}~\mathbb{P}_N\ell_{\btheta}
%\frac{1}{N}\sum^{N}_{i=1}\left\|\mathbf{x}^{(K)}\bigl(\btheta, \mathbf{y}_i, \mathbf{x}^{(0)}\bigr)-\mathbf{x}^{\mathrm{opt}}_i\right\|_2^2
%\mathbb{E}_{\mathbf{x}^{\star},\mathbf{y}\sim \mathcal{D}}\left\|\mathbf{x}^{(K)}\bigl(\btheta, \mathbf{y}, \mathbf{x}^{(0)}\bigr)-\mathbf{x}^{\star}\right\|_2^2
+\beta \mathcal{R}\left(\btheta\right).
\end{equation}
For example, one may use $\ell_1$ regularization
\begin{equation}
\label{eq21}
\mathcal{R}\left(\btheta\right) =\sum_{j} \min\left\{\left|\theta_{j}-\lambda_0\right|, \left|\theta_{j}+\lambda_0\right|\right\}.
\end{equation}
In this version, the update process in each epoch becomes
\begin{equation}
\label{eq22} 
\btheta^{(t+1)}=\operatorname{prox}_{\eta_t \beta \mathcal{R}}\bigl(\btheta^{(t)}-\eta_t\nabla \mathbb{P}_N\ell_{\btheta}|_{\btheta=\btheta^{(t)}}\bigr).
%\mathcal{L}\bigl(\btheta^{(t)}\bigr)\bigr).
\end{equation}
%Unlike hard projection, using a finite value for $\beta$ provides a more moderate approach and can help prevent excessive fluctuations early in the training process. Additionally, 
%Since the proximal operator $\operatorname{prox}_{\eta_t \beta \mathcal{R}}(\cdot)$ in \eqref{eq22} does not rigidly enforce quantization, it can evaluate gradients at various points in the space, offering access to a richer set of information compared to the straight-through gradient method.
\begin{comment}
An alternative method to incorporate one-bit quantization at each iteration involves the concept of a converging set to soften the sign function. In this approach, instead of employing a regularizer and its related proximal operator, an smooth operator is utilized in lieu of the sign function. This smooth operator gradually transitions towards the sign function as the number of iterations increases. Essentially, in this strategy, the weights are not strictly quantized but rather pass through a smooth function that mimics the characteristics of the sign function more closely as the iterations progress.
\begin{definition}\cite{eamaz2023marli}
\label{Neg-def3}
Consider a function $h(x,t): \mathbb{C}\times\mathbb{Z}\rightarrow \mathbb{C}$; as an extension, for every vector $\mbx$ let $h(\mbx,t)$ be an element-wisely monotonic operator in $t$. A set $\mathcal{X}$ is converging to a set $\mathcal{X}^{\prime}$ under a function $h$ iff for every $x\in \mathcal{X}$
\begin{equation}
\label{Neg21}
\left\{\begin{array}{l}
h(x, 0)=x, \\
\lim _{t \rightarrow \infty} h(x, t) \in \mathcal{X}^{\prime},
\end{array}\right.
\end{equation}
and for every $x^{\prime}\in \mathcal{X}^{\prime}$, there exists an element $x\in \mathcal{X}$ such that
\begin{equation}
\label{Neg22}
\lim _{t \rightarrow \infty} h(x, t)=x^{\prime}.
\end{equation}
The function $h$ is called identity iff for any $x \in \mathcal{X}$ and $x^{\prime} \in \mathcal{X}^{\prime}$ satisfying (\ref{Neg22}), $x^{\prime}$ is the closest element of $\mathcal{X}^{\prime}$ to $x$. The sequence of sets $\left\{\mathcal{X}^{(t)}\right\}_{t=0}^{\infty}$ where $\mathcal{X}^{(t)}=\{h(x, t) \mid x \in \mathcal{X}\}$ is called a sequence of converging sets.
\end{definition}
In our case, $\mathcal{X}=\mathbb{R}^{n}$ and $\mathcal{X}^{\prime}=\left\{-1,1\right\}^{n}$. With the converging set idea, the sign function is softened as
\begin{equation}
\label{eq23}
f(x, t)=\operatorname{sgn}(x) |x|^{e^{-\nu t}},
\end{equation}
where $\nu$ is the tuning parameter of the converging set. If we apply this function at each iteration, we will have
\begin{equation}
\label{eq24}
\left\{\begin{array}{l}
\mbz^{(t)}=\bigl[\lambda_0\operatorname{sign}\bigl(\btheta^{(t)}\bigr) \bigl|\btheta^{(t)}\bigr|^{e^{-\nu t}}\bigr],\\
\btheta^{(t+1)}=\btheta^{(t)}-\eta_t \nabla \mathcal{L}\left(\btheta\right)|_{\btheta=\mbz^{(t)}}.
\end{array}\right.
\end{equation}
\end{comment}

\textbf{Stage \rom{2}:} Previous research used $\frac{\sum_{i,j}|W_{i,j}|}{n_1n_2}$ as the per-layer scale for one-bit quantization \cite{wang2023bitnet}. However, we propose a data-driven method to determine this value more effectively. Additionally, we use a single scale for all layers of the network which can introduce more efficiency to the trained model during the inference phase.

Let $\widehat{\btheta}$ represent the solution obtained after \textbf{Stage \rom{1}}, where the weights of the network are given by
\begin{equation}
\label{eq25}
\begin{aligned}
&\widehat{W}_{i,j}^{(k)}=\widehat{R}_{i,j}^{(k)},~\widehat{R}_{i,j}^{(k)}\in\{-\lambda_0,\lambda_0\},\\&(i,j)\in[n_1]\times[n_2],~ k\in[K],
\end{aligned}
\end{equation}
or more compactly as $\widehat{\mbW}^{(k)}=\widehat{\mbR}^{(k)}$ for $k\in[K]$. In this stage, we change $\lambda_0$ to $\lambda_0\lambda$, where the goal is to optimize the following objective with respect to $\lambda$ given the training dataset:
\begin{equation}
\label{eq26}
\mathcal{P}^{(\lambda)}:~\underset{\lambda}{\operatorname{minimize}}~\mathbb{P}_N\ell_{\lambda},
%\mathbb{E}_{\mathbf{x}^{\star},\mathbf{y}\sim \mathcal{D}}\left\|\mathbf{x}^{(K)}\bigl(\lambda\widehat{\btheta}, \mathbf{y}, \mathbf{x}^{(0)}\bigr)-\mathbf{x}^{\star}\right\|_2^2,
\end{equation}
where $\mathbb{P}_N\ell_{\lambda}=\frac{1}{N}\sum_{i=1}^{N}\left\|\operatorname{Alg}^{K}_{\lambda \widehat{\btheta}}\bigl(\mbQ_{\phi}(\bxi_i),\mbb_i\bigr)-\mathbf{x}^{\mathrm{opt}}_i\right\|$. Note that $\lambda$ only affects the weights of the network and not the parameters of the activation function within $\lambda\widehat{\btheta}$. 
%Algorithm~\ref{algorithm_1} provides a summary of our proposed two-stage one-bit algorithm unrolling method.

\section{Towards Sparse Network with Unrolling Algorithm}
\label{sec4}
Although LISTA reduces the number of parameters compared to a fully connected network (FCN), it still preserves full connectivity between the input and output at each layer. To incorporate sparsity, we can exploit the model-based structure of DUNs. Specifically, if the original inverse problem involves a sparse, block-structured matrix, we can partition the inputs, where each partition of the output is generated by only a corresponding subset of the input rather than all input elements. This structure results in a sparse, block-structured matrix that the iterative algorithms must respect, reflecting the original problem's sparsity pattern.

In this approach, unlike traditional sparsification techniques, sparsity is naturally embedded in the model through predefined zero elements in the weight matrix. This sparsity stems from the iterative update process of the classical algorithm that the network unrolls, where the weight matrix is inherently sparse. Thus, the network structure is not artificially pruned but instead mirrors the sparsity present in the underlying problem. This leads to a reduced complexity that benefits adoption in large-scale regimes. It is important to note that this type of sparsification is more computationally efficient compared to the approaches utilized in sparsifying the LLMs since the locations of zero links in the weight matrices of each layer are predefined in our model and there is no need to update the locations during the training.
%In other words, due to the structure of the model, each output partition is generated solely from its corresponding input partition. Therefore, when designing a linear weight matrix for this model, it is essential to account that certain links must be zero, resulting in a sparse network.

For example, consider the following linear matrix equation as the original inverse problem, where the goal is to recover $\mbX=\operatorname{Ro}(\mbx_{1:u})\in\mathbb{R}^{p\times u}$
\begin{comment}
$\mbX = \left[\begin{array}{c|c|c|c}
\mbx_1 &\mbx_2 &\cdots &\mbx_{n_2} 
\end{array}\right] \in \mathbb{R}^{n_1 \times n_2}$ 
\end{comment}
from the measurement matrix $\mbY=\operatorname{Ro}(\mby_{1:u})\in\mathbb{R}^{v\times u}$,
\begin{comment}
$\mbY=\left[\begin{array}{c|c|c|c}
\mby_1 &\mby_2 &\cdots &\mby_{n_2} 
\end{array}\right] \in \mathbb{R}^{m_1 \times n_2}$
\end{comment}
using the sensing matrix $\mbA\in\mathbb{R}^{v \times p}$, $\mbY = \mbA\mbX$.
%\begin{equation}
%\label{100}
%\end{equation}
This equation can be reformulated as $\operatorname{vec}\left(\mbY\right) = \mbA^{\prime}\operatorname{vec}\left(\mbX\right)$, where $\mbA^{\prime}=\left(\mbI_{u}\otimes \mbA\right)$.
%\begin{equation}
%\label{200}
%\end{equation}
Therefore, we have a block-structured matrix whose diagonal parts are $\mbA$. In this equation, each column of $\mbX$ is assigned to the corresponding column in the measurement matrix $\mbY$, and the column vectors of the input matrix do not influence the output of other columns. The linear matrix equation can be interpreted as a set of independent linear equations, where each column of the input matrix corresponds to the input of one equation, and each column of the measurement matrix represents the output of that specific equation.

Assume these linear equations represent a sparse recovery problem where each input signal is also sparse. To solve this set of linear equations, we can either apply LISTA to each equation individually or treat the concatenation of all equations as a single update process, as described in the following model:%\par\noindent\small
\begin{equation}
\begin{aligned}
\operatorname{Co}(\mbx_{1:u,k})=\operatorname{ST}_{\theta_k}\bigl(\operatorname{Co}(\mbx_{1:u,k-1})-\widehat{\mbW}_k^{\top}\left(\mbA^{\prime}\operatorname{Co}(\mbx_{1:u,k-1})-\operatorname{Co}(\mby_{1:u})\right)\bigr),
\end{aligned}
\end{equation} %\normalsize
for $k\in[K]$.
\begin{comment}
\begin{equation}
\begin{bmatrix}
\mbx^{(k+1)}_1 \\
\mbx^{(k+1)}_2 \\
\vdots \\
\mbx^{(k+1)}_{n_2}
\end{bmatrix}= \text{ST}_{\theta^{(k)}}\bigl(\begin{bmatrix}
\mbx^{(k)}_1 \\
\mbx^{(k)}_2 \\
\vdots \\
\mbx^{(k)}_{n_2}
\end{bmatrix} -\bigl(\mathbf{W}_{s}^{(k)}\bigr)^{\top}\bigl(\left(\mbI_{n_2}\otimes \mbA\right) \begin{bmatrix}
\mbx^{(k)}_1 \\
\mbx^{(k)}_2 \\
\vdots \\
\mbx^{(k)}_{n_2}
\end{bmatrix}-\begin{bmatrix}
\mby_1 \\
\mby_2 \\
\vdots \\
\mby_{n_2}
\end{bmatrix}\bigr)\bigr),\quad k\in[K].
\end{equation}\normalsize
\end{comment}
Since each input partition is independent of the others, and each corresponding output partition is influenced solely by its respective input, we can construct the following weight matrix that is sparse and blocky, mirroring the structure of the sensing matrix:
\begin{equation}
\widehat{\mbW}_k=\operatorname{BlockDiag}_{u}(\mbW_k)\in\{-\lambda,0,\lambda\}^{vu\times pu},~k\in[K],
\end{equation}
\begin{comment}
\begin{equation}
\mbW^{(k)}_{s} = \begin{bmatrix}
\mbW^{(k)} & 0   & 0   & \dots & 0 \\
0   & \mbW^{(k)} & 0   & \dots & 0 \\
0   & 0   & \mbW^{(k)} & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0   & 0   & 0   & \dots & \mbW^{(k)}
\end{bmatrix}\in\{-\lambda,0,\lambda\}^{m_1n_2\times n_1 n_2},    
\end{equation}
\end{comment}
where $\mbW_k\in\{-\lambda,\lambda\}^{v\times p}$ for $k\in[K]$ are learnable parameters. This block structure ensures that each part of the model interacts only with the relevant input, maintaining the sparsity of the system.
The interesting aspect of this model is that as the number of block partitions increases, the sparsity of the network also increases significantly. In some problems, the sensing matrix $\mbA$ itself is sparse. In such cases, even certain elements within the partitions are independent of one another, generating their outputs based solely on these corresponding elements. These relations, or lack thereof, help make the inference model increasingly sparse.
%These models can further sparsify the weight matrix, making it increasingly sparse.

\section{Generalization Ability}
\label{sec5}
To present the generalization result of a given algorithm, we first define three important algorithmic properties as

%All algorithms are characterized by three important properties defined as follows:
%\begin{itemize}
    %\item 
    
    \textbf{Convergence:} The convergence rate of an algorithm indicates how quickly the optimization error diminishes as $k$ increases,%\par\noindent\small
    \begin{equation}
    \begin{aligned}
    \left\|\operatorname{Alg}^{k}_{\btheta}\left(\mbQ_{\phi},\mbb\right)-\mbx^{\mathrm{opt}}\right\|\leq\operatorname{Cvg}(k)\|\mbx_0-\mbx^{\mathrm{opt}}\|.
    \end{aligned}
    \end{equation}%\normalsize
    %\item 
    \textbf{Stability:} The robustness of an algorithm to small perturbations in the optimization
    objective, which corresponds to the perturbation of $\mbQ$ and $\mbb$ in the quadratic case is demonstrated as,%\par\noindent\small
    \begin{equation}
    \begin{aligned}
    \left\|\operatorname{Alg}^{k}_{\btheta}\left(\mbQ_{\phi},\mbb\right)- \operatorname{Alg}^{k}_{\btheta}\left(\mbQ^{\prime}_{\phi},\mbb^{\prime}\right)\right\|\leq\operatorname{Stab}^{\mbQ}(k)\|\mbQ-\mbQ^{\prime}\|+\operatorname{Stab}^{\mbb}(k)\|\mbb-\mbb^{\prime}\|.
    \end{aligned}
    \end{equation}%\normalsize
    %\item 
    \textbf{Sensitivity:} The robustness to small perturbations in the algorithm parameters $\btheta$ is given by,%\par\noindent\small
    \begin{equation}
    \left\|\operatorname{Alg}^{k}_{\btheta}\left(\mbQ_{\phi},\mbb\right)- \operatorname{Alg}^{k}_{\btheta^{\prime}}\left(\mbQ_{\phi},\mbb\right)\right\|\leq \operatorname{Sens}^{\btheta}(k)\|\btheta-\btheta^{\prime}\|.
    \end{equation}%\normalsize
%\end{itemize}
As we will show in Theorem~\ref{theorem_1}, these algorithmic properties play a critical role in determining the generalization ability of a given algorithm.
In this section, we analyze two algorithms: the ST algorithm in \eqref{update_sqp_1} and the HT algorithm. The HT algorithm follows the same update process as the ST algorithm, with the key difference being the replacement of the operator $\operatorname{ST}_{\theta_k}$ with $\operatorname{HT}_{\theta_k}$. As demonstrated in \cite{chen2018theoretical}, to guarantee the convergence of the ST algorithm (e.g., LISTA) for the sparse quadratic program, the network weights must belong to a specific set defined as follows:
\begin{definition}
\label{def_1}
Given $\mbQ\in\mathbb{R}^{n\times n}$ and $\mathcal{I}=\{-\lambda,\lambda\}^{n\times n}$, a weight matrix is ``good" if it belongs to %\par\noindent\small
\begin{equation}
\label{cvg1}
\begin{aligned}
\mathcal{X}_{\mbW}(\mbQ)=\arg\min_{\mbW\in\mathcal{I}}\left\{\max_{1\leq i,j\leq n}|W_{i,j}|:\mbW_i^{\top}\mbQ_i=1,\max_{i\neq j}|\mbW_{i}^{\top}\mbQ_j|=\mu\right\}.    
\end{aligned}
\end{equation}%\normalsize
\end{definition}
Selecting the weight matrices from the set of ``good" weights imposes a significant constraint within the class of binary weights. This restriction suggests that satisfying the first term of \eqref{cvg1} with a binary weight may be particularly difficult. Even under the assumption that the set in Definition~\ref{def_1} is non-empty, identifying the minimizer of \eqref{cvg1} remains a highly challenging task. It is also important to note that the set of ``good" weights in Definition~\ref{def_1} only guarantees convergence. However, as we will see in Theorem~\ref{theorem_1}, stability and sensitivity are also key factors, alongside convergence, in determining the generalization ability of an algorithm. Therefore, our objective is to develop an alternative strategy that not only ensures uniform convergence across all training samples but also guarantees generalization ability for both binary and high-resolution models.

In this paper, our strategy centers on analyzing the spectral norm of the form $\left\|\mbI - \mbW^{\top}_k \mbQ\right\|$ for all $k\in[K]$. As shown in Appendices~\ref{App_B}, \ref{App_D1}, and \ref{App_E1}, this term plays a pivotal role in determining the algorithm properties of the ST update process. Our analysis reveals that this spectral norm must be less than $1$ to ensure guaranteed generalization ability. Interestingly, Definition~\ref{def_1} can be interpreted from this perspective: imposing specific assumptions on the weights ensures that the spectral norm satisfies this condition. 

To theoretically enforce this condition, we incorporate a new parameter, $0<\delta \leq 1$, into the ST algorithm (as seen in the update process \eqref{update_sqp_1}). This modification introduces the parameter $\alpha_{\phi}=\sup_{k\in[K]}\|\delta\mbI-\mbW^{\top}_k\mbQ\|$, which plays a central role in all algorithm properties, as shown in the following lemma:
%------------------------------------------------------------------------------
\begin{table*}[t]
%\color{blue}
\caption{Algorithm properties of the ST update process in \eqref{update_sqp_1}.
%Main parameters of one-bit algorithm unrolling \eqref{update_sqp_1} %(Detailsare given in Appendix~).
}
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{ c || c|| c|| c|| c|| c|| c }
\hline
$\operatorname{Cvg}(k)$ &  $\operatorname{Stab}^{\mbQ}(k)$ &  $\operatorname{Stab}^{\mbb}(k)$&  $\operatorname{Sens}^{\mbW}(k)$&  $\operatorname{Sens}^{\theta}(k)$&  $\operatorname{Sens}^{\mbB}(k)$&  $\operatorname{Sens}^{\lambda}(k)$\\[0.5 ex]
\hline
$\mathcal{O}\left(\alpha_{\phi}^{k}\right)$ &  $\mathcal{O}\left(k\alpha^{k-1}_{\phi}\right) $ &  $\mathcal{O}\left(1-\alpha^{k}_{\phi}\right)$&  $\mathcal{O}\left(\alpha^{k-1}_{\phi}\right)$&  $\mathcal{O}(1)$ &  $\mathcal{O}\left(\alpha^{k-1}_{\phi}\right)$ &  $\mathcal{O}\left(k\alpha^{k-1}_{\phi}\right)$\\[0.5 ex]
\hline
\end{tabular}
\label{table_3}
\end{table*}
%------------------------------------------------------------------------------
\begin{lemma}
\label{lem_1}
Denote $\alpha_\phi=\sup_{k\in[K]}\|\delta\mbI-\mbW^{\top}_k\mbQ\|$. The convergence rate, stability, and sensitivity of the ST algorithm in \eqref{update_sqp_1} are summarized in Table~\ref{table_3}.
\end{lemma}
In Appendices~\ref{App_B}, \ref{App_D1}, and \ref{App_E1}, we present a thorough analysis and detailed proof of Lemma~\ref{lem_1}. As shown in Table~\ref{table_3}, in order to ensure bounded algorithm properties and, consequently, guarantee generalization ability, it is required that $\alpha_\phi<1$.
%The proof of Lemma~\ref{lem_1}, along with a comprehensive analysis and detailed discussion of the parameters, is provided in the Appendix.

Unlike the set of ``good" weights in Definition~\ref{def_1}, it is relatively straightforward to demonstrate that a binary network weight exists such that $\alpha_{\phi}<1$. To illustrate this, based on our quantization scheme in Section~\ref{sec3}, we can express all weights as $\mbW_k=\lambda\mbB_k$, where $\mbB_k\in\{-1,1\}^{n\times n}$ for all $k\in[K]$. Then, we can write
\begin{equation}
\left\|\delta\mbI-\mbW_k^{\top}\mbQ\right\|\leq\delta+\lambda\|\mbB_k\|
\|\mbQ\|.
\end{equation}
Define $h=\sup_{k\in[K]}\|\mbB_k\|\|\mbQ\|$. By appropriately selecting values for $\delta$ and $\lambda=\mathcal{O}(h^{-1})$, we can ensure that $\delta+\lambda h<1$. It is important to note that this choice of $\delta$ and $\lambda$ is specifically designed for the worst-case scenario. In Appendix~\ref{App_B}, we provide further details on how to systematically enforce the constraint $\alpha_{\phi}<1$ during the learning process.
%Additionally, note that one can efficiently minimize $\|\delta\mbI-\lambda\mbB_k^{\top}\mbQ\|$ by optimizing over $\delta$ and $\lambda$. 
%Furthermore, the assumptions made in Definition~1 seem to be ineffective in ensuring stability and sensitivity. As our analysis shows, the only parameter that appears to play a significant role in their formulation is $\alpha_{\phi}$.

Similar to the ST update process, in the case of HT, our approach involves incorporating $0<\delta\leq1$ and focuses on analyzing the spectral norm of the form $\left\|\delta\mbI-\mbW^{\top}_{\mathcal{S},k}\mbQ_{\mathcal{S}}\right\|$ for all $k\in[K]$. In Appendices~\ref{App_C}, \ref{App_D2}, and \ref{App_E2}, we show that the parameter $\alpha_\phi=\sup_{k\in[K]}\|\delta\mbI-\mbW^{\top}_{\mathcal{S},k}\mbQ_{\mathcal{S}}\|$ plays a crucial role in all algorithm properties of the HT update process. Regarding order complexity, the impact of $\alpha_\phi$ on all algorithm properties in this case is equivalent to the ST update process discussed in Table~\ref{table_3}, and as such, we do not repeat it here. Therefore, to guarantee generalization ability for the HT update process, it is required that $\alpha_\phi<1$. Further details on how to systematically enforce the constraint $\alpha_{\phi}<1$ during the learning process can be found in Appendix~\ref{App_C}.
%In addition to analyzing the soft-thresholding operator, we also examine the hard-thresholding operator. Specifically, beyond LISTA, we investigate the algorithmic properties of the hard-thresholding algorithm. 
%In Appendix~2, we provide a comprehensive analysis showing that, in this case, the spectral parameter $\alpha_{\phi}$ is restricted to only $s$ columns of $\mbW^{\top}_{k}\mbQ$, namely: $\sup_{k,\forall S^{\prime} \subseteq \mathbb{R}}\left\|\delta\mbI - \mbW^{\top}_{k,s}\mbQ_{s}\right\|$.

We will now demonstrate how these algorithm properties affect the generalization gap of algorithm unrolling. Before presenting the generalization result, we will first outline the assumptions underlying it:
%\begin{itemize}
    %\item 
    
    \textbf{Assumption~\rom{1}:} The input $(\bxi,\mbb)$ is a pair of random variables where $\bxi\in\mathcal{Y}\subseteq\mathbb{R}^d$ and $\mbb\in\mathcal{B}\subseteq\mathbb{R}^n$. Assume $\mbb$ satisfies $\mathbb{E}\mbb\mbb^{\top}=\sigma_b^2\mbI$. Assume $\bxi$ and $\mbb$ are independent, and their joint distribution follows a probability measure $\mathbb{P}$.
    %\item 
    
    \textbf{Assumption~\rom{2}:} In our settings, we assume that both matrices $\mbQ_{\phi}$ and $\mbQ^{\mathrm{opt}}$ are positive-semi-definite, with a positive definite structure on the support $\mathcal{S}$. Based on this assumption, the exact minimizer of \eqref{a1} for each sample follows a $s$-sparse structure, where the $s$ non-zero values are obtained as
    \begin{equation}
    \label{opt_sol_1}
    \mbx^{\mathrm{opt}}_{\mathcal{S}}(\mbQ_i^{\mathrm{opt}},\mbb_i)=
    \left(\mbQ_i^{\mathrm{opt}}[\mathcal{S},\mathcal{S}]\right)^{-1}\mbb_{i,\mathcal{S}},~i\in[N],
    \end{equation}
    where $\mbx^{\mathrm{opt}}_{\mathcal{S}}$ denotes the non-zero values of $\mbx^{\mathrm{opt}}$ on the support $\mathcal{S}$, $\mbQ^{\mathrm{opt}}[\mathcal{S},\mathcal{S}]$ is the $s\times s$ sub-matrix of $\mbQ^{\mathrm{opt}}$ corresponding to the indices from $\mathcal{S}$, and $\mbb_{\mathcal{S}}$ represents the sub-vector of $\mbb$ on the support $\mathcal{S}$. A similar argument holds for $\mbQ_{\phi}$, yielding:
    \begin{equation}
    \label{opt_sol_2}
    \mbx^{\mathrm{opt}}_{\mathcal{S}}(\mbQ_{i,\phi},\mbb_i)=
    \left(\mbQ_{i,\phi}[\mathcal{S},\mathcal{S}]\right)^{-1}\mbb_{i,\mathcal{S}},~i\in[N].
    \end{equation}
    %\textcolor{blue}{any assumption on the matrix $\mbQ_{\phi}$?}
%\end{itemize}
It is generally known \cite{chen2020understanding,bartlett2005local} that the standard Rademacher complexity provides global estimates of the complexity of the model space, which ignores the fact that the training process will likely pick models with small errors. Therefore, similar to \cite{chen2020understanding}, we resort to utilizing the local Rademacher complexity, which is defined as follows:
\begin{definition}
\label{local_rad_com}
The local Rademacher complexity of $\ell_{\mathcal{F}}$ at level $r$ is defined as $\mathbb{E} R_N \ell_{\mathcal{F}}^{\text {loc }}(r)$ where  
\begin{equation}
\ell_{\mathcal{F}}^{\text {loc }}(r)\triangleq\left\{\ell_{\btheta,\phi}: \phi \in \Phi, \btheta \in \Theta, \mathbb{P}_N\ell_{\btheta,\phi}^2 \leq r\right\}.
\end{equation}
\end{definition}
Based on Definition~\ref{local_rad_com} and our assumptions, the following theorem provides the generalization ability of an algorithm unrolling:
\begin{theorem}
\label{theorem_1}
The generalization ability of an algorithm unrolling utilized to solve the program $\mathcal{C}$ in \eqref{a1} is obtained as %\par\noindent\small
\begin{equation}
\begin{aligned}
\mathbb{E} R_n \ell_{\mathcal{F}}^{\text {loc }}(r)\leq\operatorname{Sens}(k)B_{\theta}+\sqrt{2} s N^{-\frac{1}{2}}\operatorname{Stab}^{\mbQ}(k)\left(\sqrt{(\operatorname{Cvg}(k)+\sqrt{r})^2 C_1(N)+C_2(N, t)}+C_3(N, t)+4\right),
\end{aligned}
\end{equation}%\normalsize
where $\operatorname{Stab}^{\mbQ}(k)$ and $\operatorname{Cvg}(k)$ are worst-case stability and convergence, $B_{\btheta}=\frac{1}{2}\sup_{\btheta, \btheta^{\prime} \in \Theta}\left\|\btheta-\btheta^{\prime}\right\|$, 
$C_1(N)=\mathcal{O}(\log \Gamma(N))$, $C_3(N, t)=\mathcal{O}\left(\frac{\log \Gamma(N)}{\sqrt{N}}+\frac{\sqrt{\log \Gamma(N)}}{e^t}\right)$, $C_2(N, t)=\mathcal{O}\left(\frac{t \log \Gamma(N)}{n}+\left(C_3(N, t)+1\right) \frac{\log \Gamma(N)}{\sqrt{N}}\right)$, and $\Gamma(N)=\mathcal{N}\left(\frac{1}{\sqrt{N}}, \ell_{\mathcal{Q}}, L_{\infty}\right)$.
\end{theorem}
The proof of the aforementioned theorem is straightforward and follows closely to the approach used in the proof of Theorem~5.1 in \cite{chen2020understanding}.


%For the sparse blocky equations discussed in Section~2, since the spectral norm of the matrix is equal to the maximum spectral norm of its diagonal matrices, or $\left\|\mbI-\widehat{\mbW}^{\top}_k\mbA^{\prime}\right\|=\left\|\mbI-\mbW^{\top}_k\mbA\right\|$, this sparse structure ensures that the spectral norm, particularly the spectral norm of the weights that play a critical role in the algorithm's properties, will be bounded by the dimension of $\mbA$ rather than $\mbA^{\prime}$.

%\begin{equation}
%\label{params}
%\begin{aligned}
%\operatorname{Cvg}(k) &= \mathcal{O}\left(\alpha_{\phi}^{k}\right),\quad k\in [K],\\
%\operatorname{Stab}^{\mbQ}(k) &= \mathcal{O}\left((k-1)\alpha^{k-1}_{\phi}\right) ,\quad k\in[K],\\
%\operatorname{Stab}^{\mbb}(k) &= \mathcal{O}\left(1-\alpha^{k}_{\phi}\right),\quad k\in [K]\\
%\operatorname{Sens}^{\mbW}(k) &=  \mathcal{O}\left(\alpha^{k-1}_{\phi}\right),\quad k\in [K]\\
%\operatorname{Sens}^{\theta}(k) &= \mathcal{O}(1)  ,\\
%\operatorname{Sens}^{\mbB}(k) &= \operatorname{Sens}^{\mbW}(k) ,\\
%\operatorname{Sens}^{\lambda}(k) &= \mathcal{O}\left(k\alpha^{k-1}_{\phi}\right),\quad k\in [K].
%\end{aligned}   
%\end{equation}
%-----------------------------------------------------

\section{Numerical Illustrations}
\label{sec6}
In this section, we evaluate the effectiveness of our proposed two-stage one-bit algorithm unrolling method, focusing on the forward model in \eqref{eq12}. We set $m=50$ and $n=100$. The entries of the matrix $\mbA=[A_{i,j}]\in\mathbb{R}^{m\times n}$ are sampled i.i.d. from a standard Gaussian distribution, $A_{i,j}\sim\mathcal{N}(0,1/m)$. 
%We fix a matrix $\mbA$ in each setting where different methods are compared. 
To generate sparse vectors $\mbx^{\mathrm{opt}}$, we consider each of its entry to be non-zero following the Bernoulli distribution with $p=0.05$. The values of the non-zero entries are sampled from the standard Gaussian distribution, $\mathcal{N}(0,1)$. We generate a training set of $4000$ samples and a test set of $1000$ samples, both of which are fixed for all simulations. 
In all experiments, during the first stage of training with the forward model in \eqref{eq12} using high-resolution weights, the learning rate is kept constant at $\eta_t=1e-03$ across all epochs $t$. In the \textbf{stage \rom{1}} of our proposed one-bit algorithm unrolling scheme, we initialize the learning rate at $\eta_0=1e-03$ and adjust it after every 10 epochs with a decay factor of $0.9$. In \textbf{stage \rom{2}}, the learning rate remains fixed at $\eta_t=1e-03$ for all epochs.
%--------------------------------------------------------------------
\begin{figure*}[t]
	\centering
    \subfloat[]
		{\includegraphics[width=0.3\columnwidth]{one_bit_layer_compare.eps}}\quad
	\subfloat[]
		{\includegraphics[width=0.3\columnwidth]{spectral_plot.eps}}\quad
    \subfloat[]
		{\includegraphics[width=0.3\columnwidth]{gp_result.eps}}
	\caption{(a) The impact of the number of layers on the NMSE for both  ``Train'' and ``Test'' in one-bit DUN with $\ell_1$ regularization. (b) Comparison of the spectral norm between a $5$-layer DUN and a $5$-layer DUN incorporating the effect of $\delta$. (c) The effect of $\delta$ on the generalization gap of a trained DUN.}
%\vspace{-10pt}
\label{figure_1}
\end{figure*}
%---------------------------------------------------------------------

Define the relative reconstruction error as $\mathrm{NMSE}\triangleq\frac{1}{N} \sum^{N}_{i=1}\frac{\left\|\mathbf{x}_{K}\left(\widehat{\btheta}, \mathbf{y}_i\right)-\mbx^{\mathrm{opt}}_i\right\|^2}{\left\|\mbx^{\mathrm{opt}}_i\right\|^2}$,
\begin{comment}
\begin{equation}
\text{NMSE}\triangleq\frac{1}{N} \sum^{N}_{i=1}\frac{\left\|\mathbf{x}^{(K)}\bigl(\widehat{\btheta}, \mathbf{y}_i, \mathbf{x}^{(0)}_i\bigr)-\mbx^{\mathrm{opt}}_i\right\|^2_{2}}{\left\|\mbx^{\mathrm{opt}}_i\right\|^2_2},
\end{equation}
\end{comment}
where $\widehat{\btheta}$ represents the estimated parameters of the network. Fig.~\ref{figure_1}(a) displays both the training and test results for one-bit DUN across different layer counts, specifically $K\in\{5, 10, 15, 20, 22, 25\}$. The results show that as the number of layers increases, the training and test NMSE (in dB) improves. Table~\ref{table_1} provides the exact training and test errors corresponding to Fig.~\ref{figure_1}(a), offering a more detailed view of our findings. Additionally, Table~\ref{table_1} compares the training and test NMSE across three configurations: (i) a $5$-layer FCN with ReLU and ST activation functions, (ii) a $5$-layer DUN, and (iii) one-bit DUN with varying layer counts. Since the FCN does not leverage domain-specific knowledge from the model, we design $K$ layers with the configurations $\mbW_{1}\in\mathbb{R}^{m \times n}$ and $\{\mbW_{k}\in \mathbb{R}^{n\times n}\}_{k=2}^{K}$, leading to a higher bit count compared to the DUN with the same number of layers. The bit count for each model listed in Table~\ref{table_1} is provided in Appendix~\ref{App_F}.
\begin{comment}
%--------------------------------------------------------------------
\begin{figure}[t]
	\centering
		{\includegraphics[width=0.6\columnwidth]{one_bit_layer_compare.eps}}
	\caption{The impact of the number of layers on ``Train'' and ``Test'' NMSE in one-bit DUN, with \textit{Regularization} as the \textit{Quantization type} in Algorithm
    %~\ref{algorithm_1}.
    }
\label{figure_1}
\end{figure}
%-------------------------------------------------------------------
\end{comment}

%-----------------------------------------------------
\begin{table*}[ht]
%\color{blue}
\caption{Comparison of training and test NMSE for three configurations: (i) a 5-layer FCN with ReLU and ST activation functions, (ii) a 5-layer DUN, and (iii) one-bit DUN with varying layer numbers.}
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{  c || c | c | c }
\hline
& Training NMSE (dB) & Test NMSE (dB) & $\#$ bits \\[0.5 ex]
\hline
FCN with ReLU & $0.98$ & $2.34$ & $1440000$ \\
\hline
FCN with ST & $-4.44$ & $-1.34$ & $1440160$ \\
\hline
DUN with $5$ layers & $-19.20$ & $-16.40$ & $800160$ \\
\hline
One-Bit DUN with $5$ layers & $-5.00$ & $-4.94$ & $25160$ \\
\hline
One-Bit DUN with $10$ layers & $-11.98$ & $-11.28$ & $50320$ \\
\hline
One-Bit DUN with $15$ layers & $-13.33$ & $-12.69$ & $75480$ \\
\hline
One-Bit DUN with $20$ layers & $-16.36$ & $-15.51$ & $100640$ \\
\hline
One-Bit DUN with $22$ layers & $-19.16$ & $-18.24$ & $110704$ \\
\hline
One-Bit DUN with $25$ layers & $-20.96$ & $-19.30$ & $125800$ \\
\hline
\end{tabular}
\label{table_1}
\end{table*}
%-----------------------------------------------------
%-----------------------------------------------------------------
\begin{table*}[t]
\centering
\caption{Illustration the impact of incorporating the physics of the problem into learning on the number of parameters, as well as the training and test results for the one-bit DUN.}
\centering
\begin{tabular}{ c | c | c| c | c }
\hline
$\#$ Layers & $\#$Params of FCN & $\#$ Params of DUN & Training NMSE (dB) & Test NMSE (dB)\\ [0.5 ex]
\hline \hline
$10$ & $1.5~\mathrm{B}$ & $50~\mathrm{K}$ & $-15.66$ & $-14.90$ \\
\hline
$20$ & $3~\mathrm{B}$ & $100~\mathrm{K}$ & $-21.53$ & $-20.32$ \\
\hline
\end{tabular}
%\vspace{-10pt}
\label{table_4}
\end{table*}
%-----------------------------------------------------------------
%-----------------------------------------------------
%\begin{table}[ht]
%\color{blue}
%\caption{The scale value achieved after \textbf{Stage \rom{2}} of our proposed one-bit algorithm unrolling method.}
%\centering
%\setlength{\tabcolsep}{4pt}
%\begin{tabular}{  c || c | c | c | c | c | c }
%\hline
%\#$ Layers &  $5$ & $10$ & $15$& $20$  & $22$ & $25$ \\[0.5 ex]
%\hline
%$\lambda_0\lambda$ & $0.0501$ & $0.0594$& $0.0592$  & $0.0618$ & $0.0734$$25$ & $0.0604$\\
%\hline
%\end{tabular}
%\label{table_2}
%\end{table}
%-----------------------------------------------------

As observed, the $5$-layer DUN outperforms the $5$-layer FCN with both ReLU and ST activation functions in training and test NMSE while requiring fewer stored bits. This result is anticipated as DUN incorporates domain-specific knowledge from the model, resulting in superior performance. An intriguing observation is that, for the same level of test accuracy and with the same number of layers, DUN requires $10$ times fewer training samples compared to the FCN. This is a particularly significant finding, especially when dealing with very large, high-dimensional datasets, where the cost of acquiring and processing training data can be substantial.
Furthermore, the $5$-layer FCN with an ST activation function achieves better training and test NMSE compared to its ReLU-based counterpart. This is likely due to the sparse nature of $\mbx^{\mathrm{opt}}$, where the ST operator can better mimic the sparsity behavior compared to the ReLU function by optimizing the threshold parameters. However, incorporating threshold parameters in ST slightly increases the bit count required to store the FCN model's parameters compared to the FCN with the ReLU activation function. For more details, please refer to Table~\ref{table_1} and the bit count analysis provided in Appendix~\ref{App_F}. By comparing the one-bit DUN with the DUN in Table~\ref{table_1}, it is evident that when the one-bit DUN exceeds $20$ layers, its test performance surpasses that of the DUN with $5$ layers. For example, the one-bit DUN with $22$ layers outperforms the $5$-layer DUN in test NMSE while achieving an $86\%$ compression ratio. This highlights the scalability and efficiency of the one-bit DUN architecture for deeper networks.

Under the same data settings as before, Fig.~\ref{figure_1}(b) compares the spectral norms of the form $f_k\triangleq\left\|\delta\mbI-\mbW_{k}^{\top}\mbA\right\|$ for $k\in[5]$, between the DUN with ST update process in \eqref{eq12}, with and without consideration of $\delta$. As can be observed, when $\delta$ is not considered (i.e., $\delta=1$), the spectral terms $f_k$ at each layer exceed $1$. In contrast, by appropriately selecting $\delta$ during the training dynamics, we can ensure that the spectral terms $f_k$ remain bounded by $1$.It is important to note that the spectral terms $f_k$ in Fig.~\ref{figure_1}(b) are evaluated at the converging point of the DUN, both with and without consideration $\delta$. 

In Fig.~\ref{figure_1}(c), we investigate the impact of $\delta$ on the generalization ability of the DUN with the ST update process described in \eqref{eq12}.
Introducing the parameter $\delta$ into the algorithm, which ensures that $\alpha_\phi<1$, presents both advantages and disadvantages. On the positive side, incorporating $\delta$ significantly enhances the generalization ability of the DUN. This is evident in Fig.~\ref{figure_1}(c), where the generalization gaps not only decrease in magnitude but also exhibit much smaller variations compared to the case where $\delta$ is not considered. This improvement is consistent across different layer counts when both DUNs are analyzed at converging points. This result was expected since by incorporating $\delta$ and ensuring $f_k<1$ for all $k\in[K]$, Theorem~\ref{theorem_1} guarantees bounded generalization result. Additionally, introducing $\delta$ ensures that the convergence rate of the algorithm unrolling process exhibits a monotonic decreasing behavior, which is consistent with the expected behavior of classical iterative algorithms (see Appendix~\ref{App_B} for further details). In Appendix~\ref{App_F}, we numerically demonstrate and thoroughly analyze the inconsistencies in the convergence of LISTA (the case with $\delta=1$) across layers, as well as the impact of one-bit quantization on its performance.
Despite these advantages, we also highlight in Appendix~\ref{App_B} that introducing $\delta$ introduces a second term to the upper bound of the convergence result. This additional term can, in some cases, lead to degraded training performance. This is the case with the results plotted in Fig.~\ref{figure_1}(c).
%As we demonstrate numerically in Figs. 2 and 3 and discuss theoretically in the Appendix, introducing the parameter $\delta$ into the algorithm—or ensuring that $\alpha_{\phi}$ becomes less than one—comes with both advantages and disadvantages. On the positive side, it significantly enhances the generalization ability, as shown in Theorem~1, where a decrease in $\alpha_{\phi}$ leads to a lower upper bound on the generalization error. Additionally, it ensures that the convergence rate of algorithm unrolling exhibits a monotonic decreasing behavior, aligning with the expected behavior of classic algorithms. Our numerical results will support this, which shows that LISTA without $\delta$ may lack monotonicity across layers (or iterations in classic algorithms). However, despite these benefits, we also show that introducing $\delta$ adds a second term to the upper bound of the recovery error, which can result in worse performance in some instances.

%In the Appendix, we numerically demonstrate and thoroughly discuss the inconsistencies in the convergence of LISTA per layer and the impact of one-bit quantization on it.

To numerically scrutinize the performance of the proposed one-bit DUN with a sparse structure (as discussed in Section~\ref{sec4}), we generate sub-matrices $\mbA\in\mathbb{R}^{50\times 100}$ using the same settings as before and construct the block matrix $\mbA^{\prime}$ using $100$ such sub-matrices (see the sparse structure settings in Section~\ref{sec4}). We report the training and test results for networks with $10$ and $20$ layers, including the number of parameters for both the FCN and one-bit DUN, as summarized in Table~\ref{table_4}. By leveraging the domain knowledge of the system, the number of parameters in the one-bit DUN is drastically reduced from $3$ billion in the FCN to just $100$ thousand, representing a significant reduction in model complexity. Furthermore, the use of one-bit weights achieves a remarkable compression rate in terms of bit count. The training and test results demonstrate the successful performance of one-bit DUN, even for large networks.


%Table~\ref{table_2} is that binarizing the weights significantly narrows the generalization gap, a challenge that remains an active area of research. 
%Table~\ref{table_3} presents the designed scales for one-bit quantization used in \textbf{stage \rom{2}} of the proposed one-bit unrolling scheme. 
%Notably, when the one-bit DUN exceeds $20$ layers, its test performance surpasses that of the DUN with $5$ high-resolution layers.  


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliographystyle{unsrt}
\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Convergence Analysis Based on the Set of Good Weights in Definition~\ref{def_1}}
\label{App_A}
Before providing the convergence result, we define the residual error as follows
\begin{definition}
\label{def_2}
Assume we construct the exact minimizer of \eqref{a1}, $\mbx^{\mathrm{opt}}(\mbQ,\mbb)$, for a given $\mbQ\in\mathbb{R}^{n\times n}$ and $\mbb\in\mathbb{R}^{n}$ following the process in \eqref{opt_sol_2}. Then, we define the residual error as
\begin{equation}
\label{res_error}
\boldsymbol{\epsilon}\triangleq\mbQ\mbx^{\mathrm{opt}}(\mbQ,\mbb)-\mbb.
\end{equation}
\end{definition}
Note that the existence of such a residual error in Definition~\ref{def_2} comes from the fact that the program $\mathcal{C}$ in \eqref{a1} is constrained to $\mbx\in\mathcal{K}_{\mathcal{S}}$. In Definition~\ref{def_2}, we assume that $\boldsymbol{\epsilon}$ is uniformly bounded for all training samples, i.e., $\|\boldsymbol{\epsilon}\|_1\leq\sigma$. We also consider $\delta=1$ in the update process of \eqref{update_sqp}.
%Given the binary nature of the set $\mathcal{I}$, a set of good weights $\mathcal{X}_{\mbW}(\mbQ)$ is non-empty since \textcolor{blue}{think about it}. 

To present the convergence result, at first, we should find a proper value for $\theta_k$ to guarantee that $\operatorname{supp}(\mbx_{k})=\mathcal{S}$ when $\mbW_k$ is chosen from $\mathcal{X}_{\mbW_k}(\mbQ)$. Let $\mbx_0=0$ and by induction assume that $\operatorname{supp}(\mbx_{k-1})=\mathcal{S}$. Then $\forall i\notin\mathcal{S}$, we can write
\begin{equation}
\label{cvg2}
\begin{aligned}
x_{i,k}&=\operatorname{ST}_{\theta_k}\left(x_{i,k-1}-\mbW_{i,k}^{\top}(\mbQ\mbx_{k-1}-\mbb)\right),\\&=\operatorname{ST}_{\theta_k}\left(-\mbW_{i,k}^{\top}(\mbQ\mbx_{k-1}-\mbb)\right).
\end{aligned}
\end{equation}
Following Definition~\ref{def_2},
%our assumptions presented in the problem settings, 
we can rewrite \eqref{cvg2} as
\begin{equation}
\label{cvg3}
\begin{aligned}
x_{i,k}&=\operatorname{ST}_{\theta_k}\left(-\mbW_{i,k}^{\top}(\mbQ\mbx_{k-1}-\mbQ\mbx^{\mathrm{opt}}(\mbQ,\mbb)+\boldsymbol{\epsilon})\right),\\&=\operatorname{ST}_{\theta_k}\left(-\sum_{j\in\mathcal{S}}\mbW_{i,k}^{\top}\mbQ_j(x_{j,k-1}-x_j^{\mathrm{opt}}(\mbQ,\mbb))-\mbW_{i,k}^{\top}\boldsymbol{\epsilon}\right).
\end{aligned}
\end{equation}
For $\mbW_k\in\mathcal{X}_{\mbW_k}(\mbQ)$, we set $\theta_k=\mu\sup_{\mbx^{\mathrm{opt}}(\mbQ,\mbb)}\|\mbx_{k-1}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_1+\lambda\sigma$. Then, we can write
\begin{equation}
\label{cvg4}
\theta_k\geq\mu\|\mbx_{k-1}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_1+\lambda\|\boldsymbol{\epsilon}\|_1\geq\left|-\sum_{j\in\mathcal{S}}\mbW_{i,k}^{\top}\mbQ_j(x_{j,k-1}-x_j^{\mathrm{opt}}(\mbQ,\mbb))\right|,
\end{equation}
which implies $x_{i,k}=0$, $\forall i\notin\mathcal{S}$ by the definition of $\operatorname{ST}_{\theta_k}$.

Next, for $i\in\mathcal{S}$ we can write
\begin{equation}
\label{cvg5}
\begin{aligned}
x_{i,k}&=\operatorname{ST}_{\theta_k}\left(x_{i,k-1}-\mbW_{i,k}^{\top}\mbQ_{\mathcal{S}}(\mbx_{\mathcal{S},k-1}-\mbx_{\mathcal{S}}^{\mathrm{opt}}(\mbQ,\mbb))-\mbW_{i,k}^{\top}\boldsymbol{\epsilon}\right),\\&\in x_{i,k-1}-\mbW_{i,k}^{\top}\mbQ_{\mathcal{S}}(\mbx_{\mathcal{S},k-1}-\mbx_{\mathcal{S}}^{\mathrm{opt}}(\mbQ,\mbb))-\mbW_{i,k}^{\top}\boldsymbol{\epsilon}-\theta_k\partial\ell_1(z_{i,k-1}),
\end{aligned}
\end{equation}
where $z_{i,k-1}=x_{i,k-1}-\mbW_{i,k}^{\top}\mbQ_{\mathcal{S}}(\mbx_{\mathcal{S},k-1}-\mbx_{\mathcal{S}}^{\mathrm{opt}}(\mbQ,\mbb))-\mbW_{i,k}^{\top}\boldsymbol{\epsilon}$ and $\partial\ell_1(x)$ is the sub-gradient of $\ell_1$ norm. Since $\mbW_{i,k}^{\top}\mbQ_i=1$, we have
\begin{equation}
\label{cvg6}
\begin{aligned}
x_{i,k-1}-\mbW_{i,k}^{\top}\mbQ_{\mathcal{S}}(\mbx_{\mathcal{S},k-1}-\mbx_{\mathcal{S}}^{\mathrm{opt}}(\mbQ,\mbb))&=x_{i,k-1}-\sum_{j\in\mathcal{S},j\neq i}\mbW_{i,k}^{\top}\mbQ_j(x_{j,k-1}-x_{j}^{\mathrm{opt}}(\mbQ,\mbb))\\&-(x_{i,k-1}-x_i^{\mathrm{opt}}(\mbQ,\mbb)),\\&=x_i^{\mathrm{opt}}(\mbQ,\mbb)-\sum_{j\in\mathcal{S},j\neq i}\mbW_{i,k}^{\top}\mbQ_j(x_{j,k-1}-x_{j}^{\mathrm{opt}}(\mbQ,\mbb)).
\end{aligned}
\end{equation}
Then,
\begin{equation}
\label{cvg7}
x_{i,k}-x_i^{\mathrm{opt}}(\mbQ,\mbb)\in -\sum_{j\in\mathcal{S},j\neq i}\mbW_{i,k}^{\top}\mbQ_j(x_{j,k-1}-x_{j}^{\mathrm{opt}}(\mbQ,\mbb))-\mbW_{i,k}^{\top}\boldsymbol{\epsilon}-\theta_k\partial\ell_1(z_{i,k-1}),~\forall i\in\mathcal{S}.
\end{equation}
Note that every element in $\partial\ell_1(x)$ has a magnitude less than or equal to $1$. Therefore, for all $i\in\mathcal{S}$,
\begin{equation}
\label{cvg8}
\begin{aligned}
\left|x_{i,k}-x^{\mathrm{opt}}_i\right|&\leq\sum_{j\in\mathcal{S},j\neq i}\left|\mbW_{i,k}^{\top}\mbQ_j\right|\left|x_{j,k-1}-x_j^{\mathrm{opt}}(\mbQ,\mbb)\right|+\left|\mbW_{i,k}^{\top}\boldsymbol{\epsilon}\right|+\theta_k,\\&\leq\mu\sum_{j\in\mathcal{S},j\neq i}\left|x_{j,k-1}-x_j^{\mathrm{opt}}(\mbQ,\mbb)\right|+\lambda\|\boldsymbol{\epsilon}\|_1+\theta_k.
\end{aligned}
\end{equation}
Based on the chosen value of $\theta_k$, we can write $\|\mbx_k-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_1=\|\mbx_{\mathcal{S},k}-\mbx_{\mathcal{S}}^{\mathrm{opt}}(\mbQ,\mbb)\|_1$ for all $k$. Then
\begin{equation}
\label{cvg9}
\begin{aligned}
\|\mbx_{k}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_1&=\sum_{i\in\mathcal{S}}\left|x_{i,k}-x_i^{\mathrm{opt}}(\mbQ,\mbb)\right|,\\&\leq\sum_{i\in\mathcal{S}}\left(\mu\sum_{j\in\mathcal{S},j\neq i}\left|x_{j,k-1}-x_j^{\mathrm{opt}}(\mbQ,\mbb)\right|+\lambda\sigma+\theta_k\right),\\&=\mu(|\mathcal{S}|-1)\sum_{i\in\mathcal{S}}\left|x_{i,k-1}-x_i^{\mathrm{opt}}(\mbQ,\mbb)\right|+\lambda\sigma|\mathcal{S}|+\theta_k|\mathcal{S}|,\\&\leq\mu(|\mathcal{S}|-1)\|\mbx_{k-1}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_{1}+\lambda\sigma|\mathcal{S}|+\theta_k|\mathcal{S}|.
\end{aligned}
\end{equation}
To generalize \eqref{cvg9} for the whole dataset, we take the supremum over $\mbx^{\mathrm{opt}}(\mbQ,\mbb)$, and by $|\mathcal{S}|=s$ we can write
\begin{equation}
\label{cvg10}
\sup_{\mbx^{\mathrm{opt}}(\mbQ,\mbb)}\|\mbx_{k}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_1\leq\mu(s-1)\sup_{\mbx^{\mathrm{opt}}(\mbQ,\mbb)}\|\mbx_{k-1}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_{1}+s\lambda\sigma+s\theta_k,
\end{equation}
where by our choice of $\theta_k$, we have
\begin{equation}
\label{cvg11}
\sup_{\mbx^{\mathrm{opt}}(\mbQ,\mbb)}\|\mbx_{k}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_1\leq(2\mu s-\mu)\sup_{\mbx^{\mathrm{opt}}(\mbQ,\mbb)}\|\mbx_{k-1}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_{1}+2s\lambda\sigma.
\end{equation}
By recursively applying the bound \eqref{cvg11}, we can obtain
\begin{equation}
\label{cvg12}
\begin{aligned}
\sup_{\mbx^{\mathrm{opt}}(\mbQ,\mbb)}\|\mbx_{k}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_1&\leq(2\mu s-\mu)^{k}\sup_{\mbx^{\mathrm{opt}}(\mbQ,\mbb)}\|\mbx_0-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_{1}\\&+2s\lambda\sigma\sum_{i=0}^{k-1}(2\mu s-\mu)^i,\\&\leq(2\mu s-\mu)^{k}\sup_{\mbx^{\mathrm{opt}}(\mbQ,\mbb)}\|\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|_{1}+c,\\&=(2\mu s-\mu)^{k}B+c,
\end{aligned}
\end{equation}
where $c=\frac{2s\lambda\sigma}{1+\mu-2\mu s}$. Since $\|\mbx\|_2\leq\|\mbx\|_1$ for any $\mbx\in\mathbb{R}^n$, we can rewrite \eqref{cvg12} as
\begin{equation}
\label{cvg13}
\sup_{\mbx^{\mathrm{opt}}(\mbQ,\mbb)}\|\mbx_{k}-\mbx^{\mathrm{opt}}(\mbQ,\mbb)\|\leq(2\mu s-\mu)^{k}B+c.
\end{equation}
As long as $s<\frac{1}{2}+\frac{1}{2\mu}$, the convergence bound \eqref{cvg13} holds uniformly for all $\mbx^{\mathrm{opt}}(\mbQ,\mbb)$. Therefore, the convergence parameter is
\begin{equation}
\label{cvg14}
\mathrm{Cvg}(k)=(2\mu s-\mu)^{k}B+c.
\end{equation}
%\textcolor{blue}{Write down the extended version of cvg.}
%--------------------------------------------------------------------------
\section{Convergence Analysis For Soft-Thresholding Operator}
\label{App_B}
In this section, we provide the convergence analysis of the ST operator for any $0<\delta\leq 1$. For each layer $k\in[K]$, we select the weight matrix $\mbW_k$ according to the construction detailed in Section~\ref{sec5}. Specifically, the weight matrices $\{\mbW_k\}_{k=1}^K$ are chosen to satisfy:
\begin{equation}
\label{to10}
\mbW_k\in\arg\min_{\mbW\in\mathcal{I}}\left\{\sup_{\delta}\left\|\delta\mbI-\mbW^{\top}\mbQ\right\|:\left\|\delta\mbI-\mbW^{\top}\mbQ\right\|\leq 1,~0<\delta\leq1\right\},~k\in[K].
\end{equation}
To integrate this selection of $\mbW_k$ into the training dynamics, one can first extract the results from the training process and then optimize $\delta$ and $\lambda$ as per the expression in \eqref{to10}.
%As discussed in Section~\ref{sec5}, this selection of $\mbW_k$ can be simply integrated into the learning dynamics by properly choosing the parameter $\delta$. 
We select $\theta_k$ as\footnote{For simplicity, we replace $\mbx^{\mathrm{opt}}(\mbQ,\mbb)$ by $\mbx^{\mathrm{opt}}$. Additionally, we omit $\sup_{\mbx^{\mathrm{opt}}}$ in the convergence analysis, although our result remains valid uniformly across all samples.}
\begin{equation}
\label{to7}
\theta_k=\arg\min_{\theta_k}~\sup_{\mbx^{\mathrm{opt}}}\left\|\mbx_k-\mbx^{\mathrm{opt}}\right\|,~k\in[K].
\end{equation}
We can then write the convergence result as follows:
\begin{equation}
\label{to8}
\begin{aligned}
\left\|\mbx_k-\mbx^{\mathrm{opt}}\right\|&=\left\|\operatorname{ST}_{\theta_k}\left(\delta\mbx_{k-1}-\mbW^{\top}_k\left(\mbQ\mbx_{k-1}-\mbb\right)\right)-\mbx^{\mathrm{opt}}\right\|,\\&\leq\left\|\operatorname{ST}_{\theta_k=0}\left(\delta\mbx_{k-1}-\mbW^{\top}_k\left(\mbQ\mbx_{k-1}-\mbb\right)\right)-\mbx^{\mathrm{opt}}\right\|,\\&=\left\|\delta\mbx_{k-1}-\mbW^{\top}_k\left(\mbQ\mbx_{k-1}-\mbb\right)-\mbx^{\mathrm{opt}}\right\|,~k\in[K],
\end{aligned}
\end{equation}
where the second step follows from the selection of $\theta_k$ in \eqref{to7}. We can expand \eqref{to8} as 
%The convergence rate can be rewritten in the following way:
\begin{equation}
\label{to1}
\begin{aligned}
\left\|\mbx_k-\mbx^{\mathrm{opt}}\right\|&\leq\left\|\delta\mbx_{k-1}-\mbW^{\top}_k\left(\mbQ\mbx_{k-1}-\mbb\right)-\mbx^{\mathrm{opt}}\right\|,\\&=\left\|\delta\mbx_{k-1}-\mbW^{\top}_k\mbQ\left(\mbx_{k-1}-\mbx^{\mathrm{opt}}\right)-\mbW^{\top}_k\bepsilon-\delta\mbx^{\mathrm{opt}}-(1-\delta)\mbx^{\mathrm{opt}}\right\|,\\&\leq\left\|\left(\delta\mbI-\mbW^{\top}_k\mbQ\right)
\left(\mbx_{k-1}-\mbx^{\mathrm{opt}}\right)\right\|+\left\|\mbW^{\top}_k\boldsymbol{\epsilon}\right\|+(1-\delta)\left\|\mbx^{\mathrm{opt}}\right\|,~k\in[K].
\end{aligned}
\end{equation}
Note that $\left|\mbW^{\top}_{i,k}\boldsymbol{\epsilon}\right|$ for all $i\in[n]$ can be bounded as 
\begin{equation}
\label{to2}
\left|\mbW^{\top}_{i,k}\boldsymbol{\epsilon}\right|=\lambda\left|\mbB^{\top}_{i,k}\boldsymbol{\epsilon}\right|\leq\lambda\|\boldsymbol{\epsilon}\|_1\leq\lambda\sigma.
\end{equation}
Combining \eqref{to2} with \eqref{to1} results in
\begin{equation}
\label{to3}
\left\|\mbx_k-\mbx^{\mathrm{opt}}\right\|\leq\left\|\delta\mbI-\mbW^{\top}_k\mbQ\right\|\left\|\mbx_{k-1}-\mbx^{\mathrm{opt}}\right\|+\lambda\sigma\sqrt{n}+(1-\delta)B,~k\in[K].
\end{equation}
By recursively applying the bound \eqref{to3}, we can obtain
\begin{equation}
\label{to4}
\begin{aligned}
\left\|\mbx_k-\mbx^{\mathrm{opt}}\right\|&\leq\Pi_{i=1}^{k}\left\|\delta\mbI-\mbW^{\top}_i\mbQ\right\|\left\|\mbx_{0}-\mbx^{\mathrm{opt}}\right\|+\lambda\sigma\sqrt{n}+(1-\delta)B\\&+\sum_{i=1}^{k-1}\left(\lambda\sigma\sqrt{n}+(1-\delta)B\right)\Pi_{j=1}^{i}\left\|\delta\mbI-\mbW^{\top}_{k-i}\mbQ\right\|,~k\in[K].
\end{aligned}
\end{equation}
%Define $\alpha_{\phi}\triangleq\sup_{i\in[k]}\left\|\delta\mbI-\mbW^{\top}_i\mbQ\right\|$. 
Define $\alpha_{\phi}\triangleq\sup_{i\in[k]}\left\|\delta\mbI-\mbW^{\top}_i\mbQ\right\|$. With this definition, we can rewrite \eqref{to4} as
\begin{equation}
\label{to5}
\begin{aligned}
\left\|\mbx_k-\mbx^{\mathrm{opt}}\right\|&\leq\alpha_{\phi}^kB+\sum_{i=0}^{k-1}\left(\lambda\sigma\sqrt{n}+(1-\delta)B\right)\alpha_{\phi}^i\\&\leq\mathrm{Cvg}(k),~k\in[K],
\end{aligned}
\end{equation}
where
\begin{equation}
\label{to6}
\mathrm{Cvg}(k)=\alpha_{\phi}^kB+\frac{\lambda\sigma\sqrt{n}+(1-\delta)B}{1-\alpha_{\phi}},~k\in[K].
\end{equation}
%It is worth noting that the convergence result in \eqref{to6} can be further improved. This can be achieved by selecting $\theta_k$ as
%Then, we can derive the convergence result as
In practice, $\delta$ can be set to $1$, and at each layer (algorithm iteration) $k\in[K]$, one can efficiently minimize $\left\|\mbI-\lambda\mbB^{\top}_k\mbQ\right\|$ with respect to both $\lambda$ and $\mbB_k$ to ensure that $\mbW_k$ belongs to the set defined in \eqref{to10} for all $k\in[K]$. To theoretically demonstrate that the set in \eqref{to10} is non-empty for $\delta=1$, we can impose a sparse structure on the weight matrices $\mbW_k$. For instance, consider $\mbW_k$ as diagonal binary matrices with diagonal entries $\lambda$ or $-\lambda$. For a positive definite matrix $\mbQ$, if $\mbW_k=\operatorname{diag}([\lambda\cdots\lambda])$, choosing $\lambda$ smaller than the reciprocal of the largest eigenvalue of $\mbQ$ guarantees the non-emptiness of the set in \eqref{to10}. Under this construction, the learning dynamics can provably converge with the rate $\mathrm{Cvg}(k)=\alpha_{\phi}^kB+\frac{\lambda\sigma\sqrt{n}}{1-\alpha_{\phi}}$. 
%For the CS example in Section~\ref{sec2-1}, 
%If one can show that $\left\|\Pi^{k}_{i=1}\left(\mbI-\mbW^{\top}_{i}\mbQ\right)\right\|$ or $\Pi^{k}_{i=1}\left\|\mbI-\mbW^{\top}_{i}\mbQ\right\|$ decreases by increasing $k$, the monotonic convergence will be verified.
%--------------------------------------------------------------------------
\section{Convergence Analysis For Hard-Thresholding Operator}
\label{App_C}
In this section, we present the convergence analysis of the HT operator for any $0<\delta\leq 1$. For each layer $k\in[K]$, the weight matrices $\{\mbW_k\}_{k=1}^K$ are chosen to satisfy:
\begin{equation}
\label{ht1}
\mbW_{k}\in\arg\min_{\mbW\in\mathcal{I}}\left\{\sup_{\delta,\mathcal{S}^{\prime}}\left\|\delta\mbI-\mbW_{\mbS^{\prime}}^{\top}\mbQ_{\mathcal{S}^{\prime}}\right\|:\left\|\delta\mbI-\mbW_{\mbS^{\prime}}^{\top}\mbQ_{\mathcal{S}^{\prime}}\right\|\leq1,\forall\mathcal{S}^{\prime}\subseteq[n],|\mathcal{S}^{\prime}|=s,0<\delta\leq1\right\}.
\end{equation}
To incorporate this selection of $\mbW_k$ into the training dynamics, one can first identify the best $s$ elements of the output of each layer after the initial epoch of training. Subsequently, $\mbW_k$ for each layer $k\in[K]$ is progressively projected into the set defined in \eqref{ht1}. Interestingly, there is a connection between the set in \eqref{ht1} and the set of ``good" weights in \eqref{cvg1} for $\delta=1$. Specifically, the constraint $\left\|\mbI-\mbW_{\mbS^{\prime}}^{\top}\mbQ_{\mathcal{S}^{\prime}}\right\|\leq1$ for all $\mathcal{S}^{\prime}\subseteq[n]$ such that $|\mathcal{S}^{\prime}|=s$ implies that minimizing the spectral norm terms not only reduces the mutual coherence $\mu$ but also encourages the diagonal entries of $\mbW_{\mbS^{\prime}}^{\top}\mbQ_{\mathcal{S}^{\prime}}$ to be as close as possible to one. Building on this connection, we select $\theta_k$ based on the derivations in \eqref{cvg2} and \eqref{cvg3} as $\theta_k=\mu\sup_{\mbx^{\mathrm{opt}}}\|\mbx_{k-1}-\mbx^{\mathrm{opt}}\|_1+\lambda\sigma$, where $\mu$ is defined in Definition~\ref{def_1}. As argued in Appendix~\ref{App_A}, this choice of $\theta_k$ ensures that $\operatorname{supp}(\mbx_k)=\mathcal{S}$ for all $k\in[K]$. We can then write the convergence result as follows:
\begin{equation}
\label{ht2}
\begin{aligned}
\left\|\mbx_k-\mbx^{\mathrm{opt}}\right\|&=\left\|\operatorname{HT}_{\theta_k}\left(\delta\mbx_{k-1}-\mbW^{\top}_k\left(\mbQ\mbx_{k-1}-\mbb\right)\right)-\mbx^{\mathrm{opt}}\right\|,\\&=\left\|\delta\mbx_{\mathcal{S},k-1}-\mbW_{\mathcal{S},k}^{\top}\left(\mbQ_{\mathcal{S}}\mbx_{\mathcal{S},k-1}-\mbb\right)-\mbx^{\mathrm{opt}}_{\mathcal{S}}\right\|,\\&=\left\|\delta\mbx_{\mathcal{S},k-1}-\mbW_{\mathcal{S},k}^{\top}\mbQ_{\mathcal{S}}\left(\mbx_{\mathcal{S},k-1}-\mbx^{\mathrm{opt}}_{\mathcal{S}}\right)-\mbW_{\mathcal{S},k}^{\top}\bepsilon-\delta\mbx^{\mathrm{opt}}_{\mathcal{S}}-(1-\delta)\mbx^{\mathrm{opt}}_{\mathcal{S}}\right\|,\\&\leq\left\|\delta\mbI-\mbW_{\mathcal{S},k}^{\top}\mbQ_{\mathcal{S}}\right\|\left\|\mbx_{\mathcal{S},k-1}-\mbx^{\mathrm{opt}}_{\mathcal{S}}\right\|+\left\|\mbW_{\mathcal{S},k}^{\top}\bepsilon\right\|+(1-\delta)\left\|\mbx^{\mathrm{opt}}_{\mathcal{S}}\right\|,\\&=\left\|\delta\mbI-\mbW_{\mathcal{S},k}^{\top}\mbQ_{\mathcal{S}}\right\|\left\|\mbx_{k-1}-\mbx^{\mathrm{opt}}\right\|+\left\|\mbW_{\mathcal{S},k}^{\top}\bepsilon\right\|+(1-\delta)\left\|\mbx^{\mathrm{opt}}_{\mathcal{S}}\right\|,\\&\leq\left\|\delta\mbI-\mbW_{\mathcal{S},k}^{\top}\mbQ_{\mathcal{S}}\right\|\left\|\mbx_{k-1}-\mbx^{\mathrm{opt}}\right\|+\lambda\sigma\sqrt{s}+(1-\delta)B,~k\in[K].
\end{aligned}
\end{equation}
By recursively applying the bound \eqref{ht2}, we can obtain
\begin{equation}
\label{ht3}
\begin{aligned}
\left\|\mbx_k-\mbx^{\mathrm{opt}}\right\|&\leq\Pi_{i=1}^{k}\left\|\delta\mbI-\mbW_{\mathcal{S},i}^{\top}\mbQ_{\mathcal{S}}\right\|\left\|\mbx_{0}-\mbx^{\mathrm{opt}}\right\|+\lambda\sigma\sqrt{s}+(1-\delta)B\\&+\sum_{i=1}^{k-1}\left(\lambda\sigma\sqrt{s}+(1-\delta)B\right)\Pi_{j=1}^{i}\left\|\delta\mbI-\mbW_{\mathcal{S},k-i}^{\top}\mbQ_{\mathcal{S}}\right\|,~k\in[K].
\end{aligned}
\end{equation}
Define $\alpha_{\phi}\triangleq\sup_{i\in[k]}\left\|\delta\mbI-\mbW_{\mathcal{S},i}^{\top}\mbQ_{\mathcal{S}}\right\|$. With this definition, we can rewrite \eqref{ht3} as
\begin{equation}
\label{ht4}
\begin{aligned}
\left\|\mbx_k-\mbx^{\mathrm{opt}}\right\|&\leq\alpha_{\phi}^kB+\sum_{i=0}^{k-1}\left(\lambda\sigma\sqrt{s}+(1-\delta)B\right)\alpha_{\phi}^i\\&\leq\mathrm{Cvg}(k),~k\in[K],
\end{aligned}
\end{equation}
where
\begin{equation}
\label{ht5}
\mathrm{Cvg}(k)=\alpha_{\phi}^kB+\frac{\lambda\sigma\sqrt{s}+(1-\delta)B}{1-\alpha_{\phi}},~k\in[K].
\end{equation}
To theoretically demonstrate that the set in \eqref{ht1} is non-empty for $\delta=1$, we can impose a sparse structure on the weight matrices $\mbW_k$. Specifically, consider $\mbW_k$ as diagonal binary matrices with diagonal entries $\lambda$ or $-\lambda$. For a positive definite (or even positive semi-definite) matrix $\mbQ$, if $\mbW_k=\operatorname{diag}([\lambda\cdots\lambda])$, choosing $\lambda$ smaller than the reciprocal of the largest eigenvalue of any $s$-support sub-matrix of $\mbQ$ ensures that the set in \eqref{ht1} is non-empty. Under this construction, the learning dynamics can provably converge with the rate $\mathrm{Cvg}(k)=\alpha_{\phi}^kB+\frac{\lambda\sigma\sqrt{s}}{1-\alpha_{\phi}}$.
%--------------------------------------------------------------------------
%\newpage
\section{Stability Analysis For (Soft/Hard)-Thresholding Operators}
\label{App_D}
In this section, we analyze and derive the stability parameters for both ST and HT update processes.
\subsection{Soft-Thresholding}
\label{App_D1}
In the stability analysis, we replace $\mbx^{\mathrm{opt}}(\mbQ,\mbb)$ with $\mbx^{\mathrm{opt}}$ for simplicity. By the definition of $\mbx_k$ in \eqref{update_sqp}, we can upper bound the second norm of $\mbx_k$ for $0<\delta<1$ as
%It is straightforward to show that the second norm of the solution can be upper bounded by
\begin{equation}
\label{stab0}
\begin{aligned}
%\|\mbx_k\|&\leq \left\|\mbI-\mbW^{\top}_k\mbQ\right\| \|\mbx_{k-1}\|+\|\mbW_k\|\|\mbb\|,\\ & \leq \left(\|\mbW_k\|+\sum^{k-1}_{i=1}\|\mbW_{k-i}\|\Pi^{i}_{j=1}\left\|\mbI-\mbW^{\top}_{k-i+j}\mbQ\right\|\right) \|\mbb\|. 
\|\mbx_k\|&=\left\|\operatorname{ST}_{\theta_k}\left(\delta\mbx_{k-1}-\mbW_k^{\top}\left(\mbQ\mbx_{k-1}-\mbb\right)\right)\right\|,\\&\leq\left\|\delta\mbx_{k-1}-\mbW_k^{\top}\left(\mbQ\mbx_{k-1}-\mbb\right)\right\|,\\&=\left\|\delta\mbx_{k-1}-\mbW_k^{\top}\left(\mbQ\mbx_{k-1}-\mbQ\mbx^{\mathrm{opt}}+\mbQ\mbx^{\mathrm{opt}}-\mbb\right)\right\|,\\&=\left\|\delta\mbx_{k-1}-\mbW_k^{\top}\left(\mbQ\mbx_{k-1}-\mbQ\mbx^{\mathrm{opt}}\right)+\delta\mbx^{\mathrm{opt}}-\delta\mbx^{\mathrm{opt}}-\mbW_k^{\top}\bepsilon\right\|,\\&=\left\|\left(\delta\mbI-\mbW_{k}^{\mathrm{\top}}\mbQ\right)(\mbx_{k-1}-\mbx^{\mathrm{opt}})+\delta\mbx^{\mathrm{opt}}-\mbW_k^{\top}\bepsilon\right\|,\\&\leq\left\|\delta\mbI-\mbW_{k}^{\mathrm{\top}}\mbQ\right\|\left\|\mbx_{k-1}-\mbx^{\mathrm{opt}}\right\|+\delta\left\|\mbx^{\mathrm{opt}}\right\|+\left\|\mbW^{\top}_k\bepsilon\right\|,\\&\leq\zeta(\mbQ,k),
%\leq\mathrm{Cvg}(k-1)B\left\|\mbI-\mbW_{k}^{\mathrm{\top}}\mbQ\right\|+B.
\end{aligned}
\end{equation}
%Define 
where the parameter $\zeta(\mbQ,k)$ is
\begin{equation}
%\zeta(\mbQ,k)\triangleq\left(\|\mbW_k\|+\sum^{k-1}_{i=1}\|\mbW_{k-i}\|\Pi^{i}_{j=1}\left\|\mbI-\mbW^{\top}_{k-i+j}\mbQ\right\|\right).
\zeta(\mbQ,k)=\mathrm{Cvg}(k-1)\left\|\delta\mbI-\mbW_{k}^{\mathrm{\top}}\mbQ\right\|+\delta B+\lambda\sigma\sqrt{n},~0<\delta<1.
\end{equation}
\begin{comment}
Then, we can rewrite \eqref{stab0} as
\begin{equation}
\|\mbx_k\|\leq\zeta(\mbQ,k)\|\mbb\|.
\end{equation}
\end{comment}
%where $\zeta(\mbQ,k)$ can be obtained as
%Consequently, $\zeta(\mbQ,k)$ is given by
Following \eqref{stab0}, it can be simply shown that $\zeta(\mbQ,k)$ for $\delta=1$ becomes
\begin{equation}
\zeta(\mbQ,k)=\mathrm{Cvg}(k-1)\left\|\mbI-\mbW_{k}^{\mathrm{\top}}\mbQ\right\|+\lambda\sigma\sqrt{n}.
\end{equation}
In Appendix~\ref{App_B}, we have shown that $\operatorname{Cvg}(k) = \alpha^{k}_{\phi}+\mathrm{const}_{(1)}$. Thus,
\begin{equation}
\zeta(\mbQ,k)= (\alpha^{k-1}_{\phi}+\mathrm{const}_{(1)})\alpha_{\phi}+\mathrm{const}_{(2)}= \alpha^{k}_{\phi}+\mathrm{const}_{(1)}\alpha_{\phi}+\mathrm{const}_{(2)}. 
\end{equation}
By considering the parameters $(\mbQ^{\prime},\mbb^{\prime})$, we define $\mbx^{\prime}_k$ as
\begin{equation}
\label{stab_n}
\mbx^{\prime}_k = \operatorname{ST}_{\theta_k}\left(\delta\mbx^{\prime}_{k-1}-\mbW_k^{\top}\left(\mbQ^{\prime}\mbx^{\prime}_{k-1}-\mbb^{\prime}\right)\right).
\end{equation}
Due to the $1$-Lipschitz continuity of the soft thresholding operator, one can write for $0<\delta\leq 1$,
\begin{equation}
\label{stab1}
\begin{aligned}
\|\mbx_k-\mbx^{\prime}_k\| &= \|\operatorname{ST}_{\theta_k}(\mbz_k)-\operatorname{ST}_{\theta_k}(\mbz^{\prime}_k)\|,\\
&\leq\|\mbz_{k}-\mbz^{\prime}_k\|,\\
&=\left\|\delta\mbx_{k-1}-\delta\mbx^{\prime}_{k-1}-\mbW^{\top}_{k}\left(\mbQ\mbx_{k-1}-\mbb\right)+\mbW^{\top}_{k}\left(\mbQ^{\prime}\mbx^{\prime}_{k-1}-\mbb^{\prime}\right)\right\|,\\
&=\left\|\delta\mbx_{k-1}-\delta\mbx^{\prime}_{k-1}-\mbW^{\top}_{k}\left(\mbQ\mbx_{k-1}-\mbb\right)+\mbW^{\top}_{k}\left(\mbQ^{\prime}\mbx^{\prime}_{k-1}-\mbb^{\prime}\right)+\mbW^{\top}_{k}\mbQ^{\prime}\mbx_{k-1}-\mbW^{\top}_k\mbQ^{\prime}\mbx_{k-1}\right\|,\\
&=\left\|\left(\delta\mbI-\mbW^{\top}_k\mbQ^{\prime}\right)(\mbx_{k-1}-\mbx^{\prime}_{k-1})+\mbW^{\top}_k (\mbb-\mbb^{\prime})-\mbW^{\top}_k\left(\mbQ-\mbQ^{\prime}\right)\mbx_{k-1}\right\|,\\
&\leq \left\|\delta\mbI-\mbW^{\top}_k\mbQ^{\prime}\right\|\left\|\mbx_{k-1}-\mbx^{\prime}_{k-1}\right\|+\|\mbW_k \|\left\|\mbb-\mbb^{\prime}\right\|+\|\mbW_k\|\left\|\mbQ-\mbQ^{\prime}\right\|\|\mbx_{k-1}\|,\\
&\leq\left\|\delta\mbI-\mbW^{\top}_k\mbQ^{\prime}\right\|\left\|\mbx_{k-1}-\mbx^{\prime}_{k-1}\right\|+\|\mbW_k \|\left\|\mbb-\mbb^{\prime}\right\|+\|\mbW_k\|\left\|\mbQ-\mbQ^{\prime}\right\| \zeta(\mbQ,k-1).
\end{aligned}
\end{equation}
Under the assumption $\mbx_0=\mbx^{\prime}_0$, we have
\begin{equation}
\label{stab2}
\begin{aligned}
&\|\mbx_{k}-\mbx^{\prime}_k\|\leq \left(\|\mbW_k\|+\sum^{k-1}_{i=1}\|\mbW_{k-i}\|\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{k-j+1}\mbQ^{\prime}\right\|\right)\|\mbb-\mbb^{\prime}\|\\&+\left(\|\mbW_k\|\zeta(\mbQ,k-1)+\sum^{k-1}_{i=1}\|\mbW_{k-i}\|\zeta(\mbQ,k-i-1)\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{k-j+1}\mbQ^{\prime}\right\|\right)\left\|\mbQ-\mbQ^{\prime}\right\|.
\end{aligned}
\end{equation}
Thus, the stability parameter for $\mbQ$ is
\begin{equation}
\mathrm{Stab}^{\mbQ}\left(k,\left\{\mbW_i\right\}^{k}_{i=1}\right)=\|\mbW_k\|\zeta(\mbQ,k-1)+\sum^{k-1}_{i=1}\|\mbW_{k-i}\|\zeta(\mbQ,k-i-1)\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{k-j+1}\mbQ^{\prime}\right\|.
\end{equation}
Define $\mathrm{Stab}^{\mbQ}(k)=\sup_{\left\{\mbW_i\right\}^{k}_{i=1}}\mathrm{Stab}^{\mbQ}\left(k,\left\{\mbW_k\right\}^{k}_{i=1}\right)$. By writing the stability parameter in the order complexity format, we then have
%If we write the stability in the order shape, the order is obtained as
\begin{equation}
\begin{aligned}
\mathrm{Stab}^{\mbQ}(k) &= \mathcal{O}\left(\alpha^{k-1}_{\phi}+\mathrm{const}_{(1)}\alpha_{\phi}+\mathrm{const}_{(2)}+\sum^{k-1}_{i=1}\left(\alpha^{k-i-1}_{\phi}+\mathrm{const}_{(1)}\alpha_{\phi}+\mathrm{const}_{(2)}\right)\alpha^{i}_{\phi}\right),\\
&= \mathcal{O}\left(\alpha^{k-1}_{\phi}+(k-1)\alpha^{k-1}_{\phi}+\left(\alpha_{\phi}\mathrm{const}_{(1)}\sum^{k-1}_{i=1}\alpha^{i}_{\phi}\right)+\left(\mathrm{const}_{(2)}\sum^{k-1}_{i=1}\alpha^{i}_{\phi}\right)\right),\\
&= \mathcal{O}\left(k\alpha^{k-1}_{\phi}+\frac{\alpha_{\phi}-\alpha^{k+1}_{\phi}}{1-\alpha_{\phi}}+\frac{\alpha_{\phi}-\alpha^{k}_{\phi}}{1-\alpha_{\phi}}\right),\\
&= \mathcal{O}\left(k\alpha^{k-1}_{\phi}\right).
\end{aligned}
\end{equation}
The stability parameter for $\mbb$ is
\begin{equation}
\mathrm{Stab}^{\mbb}\left(k,\left\{\mbW_k\right\}^{k}_{i=1}\right)=\|\mbW_k\|+\sum^{k-1}_{i=1}\|\mbW_{k-i}\|\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{k-j+1}\mbQ^{\prime}\right\|,
\end{equation}
where in the order complexity format we have
\begin{equation}
 \begin{aligned}
\mathrm{Stab}^{\mbb}(k)=\sup_{\left\{\mbW_i\right\}^{k}_{i=1}}\mathrm{Stab}^{\mbb}\left(k,\left\{\mbW_k\right\}^{k}_{i=1}\right)&= \mathcal{O}\left(\sum^{k-1}_{i=1}\alpha^i_{\phi}\right),\\
 &= \mathcal{O}\left(1-\alpha^{k}_{\phi}\right). 
 \end{aligned}   
\end{equation}
\subsection{Hard-Thresholding}
\label{App_D2}
Similar to the stability analysis for the ST operator, by choosing $\theta_k$ as outlined in Appendix~\ref{App_C}, we can obtain the stability parameter for $\mbQ$ as
\begin{equation}
\mathrm{Stab}^{\mbQ}\left(k,\left\{\mbW_k\right\}^{k}_{i=1}\right)=\|\mbW_{\mathcal{S},k}\|\zeta(\mbQ,k-1)+\sum^{k-1}_{i=1}\|\mbW_{\mathcal{S},k-i}\|\zeta(\mbQ,k-i-1)\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{\mathcal{S},k-j+1}\mbQ^{\prime}_{\mathcal{S}}\right\|,
\end{equation}
where for $0<\delta< 1$ the parameter $\zeta(\mbQ,k)$ is
\begin{equation}
\zeta(\mbQ,k)=\mathrm{Cvg}(k-1)\left\|\delta\mbI-\mbW_{\mathcal{S},k}^{\mathrm{\top}}\mbQ_{\mathcal{S}}\right\|+\delta B+\lambda\sigma\sqrt{s},~0<\delta<1,
\end{equation}
and for $\delta=1$
\begin{equation}
\zeta(\mbQ,k)=\mathrm{Cvg}(k-1)\left\|\mbI-\mbW_{\mathcal{S},k}^{\mathrm{\top}}\mbQ_{\mathcal{S}}\right\|+\lambda\sigma\sqrt{s}.
\end{equation}
Similarly, the stability parameter for $\mbb$ is given by
\begin{equation}
\mathrm{Stab}^{\mbb}\left(k,\left\{\mbW_k\right\}^{k}_{i=1}\right)=\|\mbW_{\mathcal{S},k}\|+\sum^{k-1}_{i=1}\|\mbW_{\mathcal{S},k-i}\|\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{\mathcal{S},k-j+1}\mbQ^{\prime}_{\mathcal{S}}\right\|.
\end{equation}
%--------------------------------------------------------------------
%\newpage
\section{Sensitivity Analysis For (Soft/Hard)-Thresholding Operators}
\label{App_E}
In this section, we analyze and derive the sensitivity parameters for both ST and HT update processes.
\subsection{Soft-Thresholding}
\label{App_E1}
In the sensitivity analysis, we replace $\mbx^{\mathrm{opt}}(\mbQ,\mbb)$ with $\mbx^{\mathrm{opt}}$ for simplicity.
By considering the algorithmic parameters $\{\mbW^{\prime}_i,\theta^{\prime}_i\}_{i=1}^{k}$, we define $\mbx^{\prime}_k$ as
\begin{equation}
\label{sens0}
\mbx^{\prime}_k=\operatorname{ST}_{\theta^{\prime}_k}\left(\delta\mbx^{\prime}_{k-1}-\mbW^{\prime\top}_{k}\left(\mbQ\mbx^{\prime}_{k-1}-\mbb\right)\right).
\end{equation}
Then, we can write
\begin{equation}
\label{sens1}
\begin{aligned}
\|\mbx_{k}-\mbx^{\prime}_k\|&= \left\|\operatorname{ST}_{\theta_k}(\mbz_k)-\operatorname{ST}_{\theta^{\prime}_k}(\mbz^{\prime}_k)\right\|,\\
&=\left\|\operatorname{ST}_{\theta_k}(\mbz_k)-\operatorname{ST}_{\theta^{\prime}_k}(\mbz^{\prime}_k)+\operatorname{ST}_{\theta_k}(\mbz^{\prime}_k)-\operatorname{ST}_{\theta_k}(\mbz^{\prime}_k)\right\|,\\
&\leq \left\|\operatorname{ST}_{\theta_k}(\mbz_k)-\operatorname{ST}_{\theta_k}(\mbz^{\prime}_k)\right\|+\left\|\operatorname{ST}_{\theta_k}(\mbz^{\prime}_k)-\operatorname{ST}_{\theta^{\prime}_k}(\mbz^{\prime}_k)\right\|,\\
&\leq \|\mbz_k-\mbz^{\prime}_k\|+\left\|\operatorname{sgn}(\mbz^{\prime}_k)\odot \max\left(0,|\mbz^{\prime}_k|-\theta_k\mathbf{1}_n\right)- \operatorname{sgn}(\mbz^{\prime}_k)\odot \max\left(0,|\mbz^{\prime}_k|-\theta^{\prime}_k\mathbf{1}_n\right)\right\|,\\
&\leq\|\mbz_k-\mbz^{\prime}_k\|+\|\theta_k\mathbf{1}_n-\theta^{\prime}_k\mathbf{1}_n\|,\\
&= \left\|\delta \mbx_{k-1}-\delta \mbx^{\prime}_{k-1}-\mbW_{k}^{\top}\left(\mbQ\mbx_{k-1}-\mbb\right)+\mbW^{\prime\top}_{k}\left(\mbQ\mbx^{\prime}_{k-1}-\mbb\right)\right\|+n|\theta_k-\theta^{\prime}_k|,\\
&\leq \left\|\delta\mbI-\mbW^{\top}_k\mbQ\right\|\left\|\mbx_{k-1}-\mbx^{\prime}_{k-1}\right\|+\left\|\mbQ\mbx^{\prime}_{k-1}-\mbb\right\|\|\mbW_{k}-\mbW^{\prime}_{k}\|+n|\theta_k-\theta^{\prime}_k|,
\end{aligned}
\end{equation}
where by choosing $\mbW^{\prime}_{i}$ in the set \eqref{to10} for all $i\in[k]$, the term $\left\|\mbQ\mbx^{\prime}_{k-1}-\mbb\right\|$ can be bounded as
\begin{equation}
\label{sens2}
\left\|\mbQ\mbx^{\prime}_{k-1}-\mbb\right\|=\left\|\mbQ\left(\mbx^{\prime}_{k-1}-\mbx^{\mathrm{opt}}\right)+\bepsilon\right\|\leq \|\mbQ\|\|\mbx^{\prime}_{k-1}-\mbx^{\mathrm{opt}}\|+\left\|\bepsilon\right\|\leq c(k-1)= \|\mbQ\| \mathrm{Cvg}(k-1)+\sigma.
\end{equation}
%As a result, one can readily obtain
The parameter $c(k)$ can be written as a function of $\alpha_\phi$ as
\begin{equation}
c(k) = \mathrm{const}_{(1)}\alpha^{k}_{\phi}+\mathrm{const}_{(2)}.
\end{equation}
Under the assumption $\mbx_0=\mbx^{\prime}_0$, we can expand \eqref{sens1} for all $k$ layers and write
\begin{equation}
\label{sens3}
\begin{aligned}
\|\mbx_{k}-\mbx^{\prime}_k\|&\leq c(k-1)\|\mbW_{k}-\mbW^{\prime}_{k}\|+n|\theta_k-\theta^{\prime}_k|,\\&+\sum_{i=1}^{k-1}\|\mbW_{k-i}-\mbW^{\prime}_{k-i}\|c(k-i-1)\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{k-j+1}\mbQ\right\|,\\&+\sum_{i=1}^{k-1}|\theta_{k-i}-\theta^{\prime}_{k-i}|n\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{k-j+1}\mbQ\right\|.
\end{aligned}
\end{equation}
Therefore, the sensitivity parameter for each $\{\mbW_i,\theta_i\}_{i=1}^{k}$ is
\begin{equation}
\label{sens4}
\begin{aligned}
\mathrm{Sens}^{\mbW_k}(k,\mbQ)&= c(k-1),\\\mathrm{Sens}^{\mbW_i}(k,\mbQ)&=c(k-i-1)\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{k-j+1}\mbQ\right\|,~i\in[k-1],\\\mathrm{Sens}^{\theta_k}(k)&=n,\\\mathrm{Sens}^{\theta_i}(k,\mbQ)&=n\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{k-j+1}\mbQ\right\|,~i\in[k-1].
\end{aligned}
\end{equation}
For the scaled binary weights, i.e., $\mbW_i=\lambda\mbB_i$ for $i\in[k]$, we have
\begin{equation}
\begin{aligned}
\|\mbx_{k}-\mbx^{\prime}_k\|&\leq\sum_{i=1}^{k}\mathrm{Sens}^{\mbW_i}(k,\mbQ)\|\lambda\mbB_{i}-\lambda^{\prime}\mbB^{\prime}_i\|+\sum_{i=1}^{k}\mathrm{Sens}^{\theta_i}(k,\mbQ)|\theta_i-\theta^{\prime}_i|,\\
&=\sum_{i=1}^{k}\mathrm{Sens}^{\mbW_i}(k,\mbQ)\|\lambda\mbB_{i}-\lambda^{\prime}\mbB_i+\lambda^{\prime}\mbB_i-\lambda^{\prime}\mbB^{\prime}_i\|+\sum_{i=1}^{k}\mathrm{Sens}^{\theta_i}(k,\mbQ)|\theta_i-\theta^{\prime}_i|,\\
&\leq\sum_{i=1}^{k}\mathrm{Sens}^{\mbB_i}(k,\mbQ)\|\mbB_{i}-\mbB^{\prime}_i\|+\sum_{i=1}^{k}\mathrm{Sens}^{\theta_i}(k)|\theta_i-\theta^{\prime}_i|+
\mathrm{Sens}^{\lambda}(k,\mbQ)|\lambda-\lambda^{\prime}|,
\end{aligned}
\end{equation}
with
\begin{equation}
\begin{aligned}
\mathrm{Sens}^{\mbB_i}(k,\mbQ)&=\mathrm{Sens}^{\mbW_i}(k)|\lambda^{\prime}|,~i\in[k],\\
\mathrm{Sens}^{\lambda}(k,\mbQ)&=\sum_{i=1}^{k}\mathrm{Sens}^{\mbW_i}(k) \|\mbB_i\|.
\end{aligned}    
\end{equation}
According to the order of $c(k-1)$, and by taking supremum w.r.t. $\mbQ$, the results provided in Table~\ref{table_3} can be verified.
\subsection{Hard-Thresholding}
\label{App_E2}
In the sensitivity analysis of the HT operator, we select $\theta_k$ and $\theta_k^{\prime}$ as outlined in Appendix~\ref{App_C}. Therefore, we will only obtain the sensitivity parameters for binary matrices $\{\mbB_i\}_{i=1}^k$ and the scale $\lambda$. Following the analysis of the ST operator, the sensitivity parameter for each $\{\mbW_i\}_{i=1}^{k}$ is
\begin{equation}
\begin{aligned}
\mathrm{Sens}^{\mbW_k}(k,\mbQ)&= c(k-1),\\\mathrm{Sens}^{\mbW_i}(k,\mbQ)&=c(k-i-1)\Pi^{i}_{j=1}\left\|\delta\mbI-\mbW^{\top}_{\mathcal{S},k-j+1}\mbQ_{\mathcal{S}}\right\|,~i\in[k-1],
\end{aligned}
\end{equation}
where $c(k)$ is defined as
\begin{equation}
c(k)=\|\mbQ_{\mathrm{S}}\|\mathrm{Cvg}(k-1)+\sigma.
\end{equation}
We can then write
\begin{equation}
\begin{aligned}
\|\mbx_{k}-\mbx^{\prime}_k\|&\leq\sum_{i=1}^{k}\mathrm{Sens}^{\mbW_i}(k,\mbQ)\|\lambda\mbB_{\mathcal{S},i}-\lambda^{\prime}\mbB^{\prime}_{\mathcal{S},i}\|,\\
&=\sum_{i=1}^{k}\mathrm{Sens}^{\mbW_i}(k,\mbQ)\|\lambda\mbB_{\mathcal{S},i}-\lambda^{\prime}\mbB_{\mathcal{S},i}+\lambda^{\prime}\mbB_{\mathcal{S},i}-\lambda^{\prime}\mbB^{\prime}_{\mathcal{S},i}\|,\\
&\leq\sum_{i=1}^{k}\mathrm{Sens}^{\mbB_i}(k,\mbQ)\|\mbB_{\mathcal{S},i}-\mbB^{\prime}_{\mathcal{S},i}\|+
\mathrm{Sens}^{\lambda}(k,\mbQ)|\lambda-\lambda^{\prime}|,
\end{aligned}
\end{equation}
with
\begin{equation}
\begin{aligned}
\mathrm{Sens}^{\mbB_i}(k,\mbQ)&=\mathrm{Sens}^{\mbW_i}(k,\mbQ)|\lambda^{\prime}|,~i\in[k],\\
\mathrm{Sens}^{\lambda}(k,\mbQ)&=\sum_{i=1}^{k}\mathrm{Sens}^{\mbW_i}(k,\mbQ) \|\mbB_{\mathcal{S},i}\|.
\end{aligned}   
\end{equation}
%-------------------------------------------------------------
\section{Further Numerical Analysis}
\label{App_F}
In this section, we present additional numerical analysis.
\subsection{Bit Count Analysis}
\label{App_F1}
The number of bits required for the models in Table~\ref{table_1} is evaluated as:
\begin{equation}
\label{bit_count}
\begin{aligned}
\# \text{bits of FCN with ReLU}&=32(mn+Kn^2),\\
\# \text{bits of FCN with ST}&=32(mn+Kn^2)+32K,\\
\# \text{bits of DUN}&=32K(mn+1),\\
\# \text{bits of One-Bit DUN}&=K(mn+32).
\end{aligned}
\end{equation}
\subsection{Convergence Inconsistencies For DUN}
\label{App_F2}
In Fig.~\ref{figure_2}, we compare the convergence of DUN with high-resolution weights against one-bit weights, presenting both training and test results. One expectation from algorithm unrolling is that it should mimic the monotonic decreasing behavior of classical optimization solvers across iterations, corresponding to layers in the unrolled network. However, as shown in Fig.~\ref{figure_2}, the DUN deviates from this trend in some layer instances, exhibiting significant inconsistencies, a phenomenon thoroughly analyzed in \cite{heaton2023safeguarded}. In our empirical findings, we observe that binarizing the linear weights can improve the convergence of the unrolled algorithm in the sense of reducing these inconsistencies, as illustrated in Fig.~\ref{figure_2}(b)-(c). In particular, the convergence inconsistency in DUN results in a significantly high NMSE of $4$ dB in the $21$st layer. In contrast, the one-bit DUN maintains a much lower error, with NMSEs of $-2.5$ dB using Lazy projection and $-5$ dB with the $\ell_1$ regularization method. These findings highlight the effectiveness of binarization in reducing convergence inconsistencies. In high-resolution DUNs, the variability in weight magnitudes and directions across iterations can
lead to inconsistent convergence behaviors, including potential increases in the objective function.
Binarization standardizes the weight magnitudes, leading to more uniform influence of each weight on the updates.
Moreover, the discrete nature of binarized weights reduces the variance in the updates. Since the
weights can only take on two values, the updates become more predictable, and the optimization
process is less susceptible to the erratic behaviors caused by fluctuating weight values. Additionally, Fig.~\ref{figure_2}(b) and (c) compare one-bit DUN using lazy projection versus $\ell_1$-regularization. As shown, the latter achieves superior performance.
%-----------------------------------------------------
\begin{figure}[t]
	\centering
	\subfloat[]
		{\includegraphics[width=0.3\columnwidth]{highresol_perlayer.eps}}\quad
    \subfloat[]
		{\includegraphics[width=0.3\columnwidth]{one_bit_perlayer_lazy.eps}}\quad
    \subfloat[]
		{\includegraphics[width=0.3\columnwidth]{one_bit_perlayer.eps}}
	\caption{Comparison of per-layer error across both training and test stages for three models: (a) DUN with high-resolution weights, (b) one-bit DUN utilizing lazy projection, and (c) one-bit DUN employing $\ell_1$-regularization.}
%\vspace{-10pt}
\label{figure_2}
\end{figure}
%-----------------------------------------------------

\end{document}
