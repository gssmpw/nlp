% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{amssymb}
\usepackage{pifont} 
\usepackage{booktabs}
\usepackage{subcaption}  
\usepackage{caption}    

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\texttt{1bit}-Merging: Dynamic Quantized Merging for Large Language Models}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Shuqi Liu$^{1,2}$, Han Wu$^{2,}$$^\dagger$, Bowei He$^{1}$, Zehua Liu$^2$, Xiongwei Han$^2$, Mingxuan Yuan$^2$, Linqi Song$^{1,}$$^\dagger$\\
$^{1}$ Department of Computer Science, City University of Hong Kong\\
$^{2}$ Huawei Noah's Ark Lab\\
\texttt{\{shuqiliu4-c, boweihe2-c\}@my.cityu.edu.hk}\\
\texttt{wu.han1@huawei.com}\\
\texttt{linqi.song@cityu.edu.hk}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers—chat models primarily in attention layers and math/code models in MLP layers—enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that \texttt{1bit}-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.
\end{abstract}

{
\let\thefootnote\relax\footnotetext{
$^\dagger$Corresponding author.}
}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{latex/1bit_radar_1.png}
%         \caption{LLaMA2-7B models.}
%         \label{fig:radar1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{latex/1bit_radar_2.png}
%         \caption{LLaMA2-13B models.}
%         \label{fig:radar2}
%     \end{subfigure}
%     \caption{While individually fine-tuned models excel only in their specialized domains, our \texttt{1bit-Merging} achieves superior performance across all domains.}
%     \label{fig:radar_combined}
%     \vspace{-0.3cm}
% \end{figure}

\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{latex/radar_scale.pdf}
    \caption{While individually fine-tuned models excel only in their specialized domains, our \texttt{1bit-Merging} achieves superior performance across all domains.}
    \label{fig:radar_combined}
    \vspace{-0.5cm}
\end{figure}

Large language models have achieved remarkable progress, demonstrating strong performance on a wide range of tasks \cite{touvron2023llama, zhao2023survey}. As researchers continue to fine-tune these models for specific domains, there is a growing need to combine their specialized capabilities into a single model \citep{merge_survey, mergeKit}. While multi-task learning offers one solution \cite{sanh2022multitask, fifty2021efficiently}, it requires extensive computational resources and simultaneous access to all task-specific datasets. Recent advances in parameter-space model merging \cite{model_soup, task_arithmetic, ties_merging, dare} provide an efficient alternative - by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining.


Traditional model merging approaches \cite{task_arithmetic, ties_merging, dare} typically combine the parameters of multiple fine-tuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality.
However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models.
In contrast, merging with task-specific routing \citep{smear, twin-merging} dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating task-specific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments.


To effectively balance performance and storage efficiency, we introduce \texttt{1bit}-Merging, a novel dynamic merging framework that integrates task-specific routing with 1-bit quantized task vectors. 
Recognizing the substantial redundancy inherent within task vectors, we implement 1-bit compression, which significantly reduces storage requirements without notably compromising the model's effectiveness. Notably, we observe that different task-specific models store knowledge in distinct layers. For chat-oriented models, knowledge is predominantly stored in attention layers, enabling us to compress MLP layers. Conversely, for math and code-related models, knowledge is primarily stored in MLP layers, allowing us to compress attention layers.
Building upon the compressed task vectors, our framework employs task-specific routing to establish a task-specific base model. This base model serves as the foundation for integrating the remaining compressed task vectors, ensuring that each task leverages the most relevant and efficient parameters.
\texttt{1bit}-Merging thus offers a balanced solution that maintains the performance advantages of task-specific routing while addressing the storage inefficiencies of existing approaches.

To empirically demonstrate the effectiveness of \texttt{1bit-Merging}, we conduct extensive experiments by combining it with existing model merging approaches. We merged three widely adopted fine-tuned models—specializing in general knowledge (Chat), mathematical reasoning (Math), and code generation (Code)—derived from the LLaMA2-7B/13B and Mistral 7B families. 
Through extensive experiments across multiple tasks, we demonstrate that our approach not only outperforms traditional model merging methods but also achieves better storage efficiency than task-specific routing approaches.
Our task vector compression method effectively preserves and often enhances the capabilities of fine-tuned models. By integrating these compressed task vectors with our dynamic routing strategy, our \texttt{1bit-Merging} method achieves superior performance. 
As illustrated in Figure \ref{fig:radar_combined}, compared to individually fine-tuned models, our \texttt{1bit-Merging} method delivers better performance across all domains.
% While existing approaches like Ties-Merging excels in mathematical reasoning and DARE shows strengths in general knowledge and code generation, our \texttt{1bit-Merging} method delivers comparable or better performance across all domains.

To sum up, our contributions include:
(1) We propose a novel dynamic merging framework that integrates task-specific routing with 1-bit quantized task vectors.
(2) We empirically demonstrate that different task-specific models store knowledge in distinct layers, enabling targeted compression strategies based on specific compression position.
(3) Through comprehensive evaluations, we validate that our proposed method enhances model merging performance across various domains and achieves better storage efficiency than task-specific routing approaches.




\input{latex/related}

\begin{table*}[t]
\renewcommand{\arraystretch}{0.95}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccclcclccc}
\toprule
 &
   &
  \multicolumn{3}{c}{General Knowledge} &
   &
  \multicolumn{2}{c}{Mathmetical Reasoning} &
   &
  \multicolumn{2}{c}{Code Generation} &
  \multicolumn{1}{c}{} \\ \cline{3-5} \cline{7-8} \cline{10-11}
\multirow{-2}{*}{Models} &
  \multirow{-2}{*}{Position} &
  \multicolumn{1}{c}{MMLU} &
  \multicolumn{1}{c}{HellaSwag} &
  \multicolumn{1}{c}{TruthfulQA} &
   &
  \multicolumn{1}{c}{GSM8K} &
  \multicolumn{1}{c}{MATH} &
   &
  \multicolumn{1}{c}{MBPP} &
  \multicolumn{1}{c}{HumanEval} &
  \multicolumn{1}{c}{\multirow{-2}{*}{Average}} \\
  \hline
 &
  \cellcolor[HTML]{EFEFEF}/ &
  \cellcolor[HTML]{EFEFEF}\underline{46.38} &
  \cellcolor[HTML]{EFEFEF}57.79 &
  \cellcolor[HTML]{EFEFEF}\textbf{45.17} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}23.43 &
  \cellcolor[HTML]{EFEFEF}4.86 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}0.3 &
  \cellcolor[HTML]{EFEFEF}0.6 &
  \cellcolor[HTML]{EFEFEF}\textbf{25.50} \\
 &
  Attention &
   42.12&
   55.30&
   41.13&
   &
   22.74&
   5.08&
   &
   0.0&
   0.0&
   23.70\\
 &
  MLP &
   \cellcolor[HTML]{CBCEFB}\textbf{46.40}&
   \cellcolor[HTML]{CBCEFB}\underline{58.23}&
   \cellcolor[HTML]{CBCEFB}\underline{43.70}&
   &
   21.15&
   4.94&
   &
   0.0&
   0.0&
   \underline{25.30}\\
\multirow{-4}{*}{Chat} &
  Linear &
   45.61&
   \textbf{58.38}&
   42.96&
   &
   20.70&
   4.76&
   &
   0.0&
   0.0&
   24.63\\
   \hline
 &
  \cellcolor[HTML]{EFEFEF}/ &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}40.05} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}56.30} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}32.56} &
  \cellcolor[HTML]{EFEFEF} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{48.60}} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{8.50}} &
  \cellcolor[HTML]{EFEFEF} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}21.8} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}12.8} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}31.52} \\
 &
  Attention &
  \multicolumn{1}{c}{41.03} &
  \multicolumn{1}{c}{56.68} &
  \multicolumn{1}{c}{34.52} &
   &
  \cellcolor[HTML]{CBCEFB}{\underline{47.46}} &
  \cellcolor[HTML]{CBCEFB}{\underline{8.42}} &
   &
  \multicolumn{1}{c}{23.1} &
  \multicolumn{1}{c}{\textbf{14.0}} &
  \multicolumn{1}{c}{\textbf{32.17}} \\
 &
  MLP &
  \multicolumn{1}{c}{41.85} &
  \multicolumn{1}{c}{57.68} &
  \multicolumn{1}{c}{33.17} &
   &
  \multicolumn{1}{c}{44.88} &
  \multicolumn{1}{c}{7.06} &
   &
  \multicolumn{1}{c}{25.3} &
  \multicolumn{1}{c}{14.0} &
  \multicolumn{1}{c}{{31.99}} \\
\multirow{-4}{*}{Math} &
  Linear &
  \multicolumn{1}{c}{42.76} &
  \multicolumn{1}{c}{57.96} &
  \multicolumn{1}{c}{34.52} &
   &
  \multicolumn{1}{c}{42.29} &
  \multicolumn{1}{c}{6.84} &
   &
  \multicolumn{1}{c}{24.3} &
  \multicolumn{1}{c}{15.2} &
  \multicolumn{1}{c}{31.98} \\
  \hline
 &
  \cellcolor[HTML]{EFEFEF}/ &
  \cellcolor[HTML]{EFEFEF}40.76 &
  \cellcolor[HTML]{EFEFEF}57.87&
  \cellcolor[HTML]{EFEFEF}33.17&
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}7.13&
  \cellcolor[HTML]{EFEFEF}3.62&
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}\underline{26.8}&
  \cellcolor[HTML]{EFEFEF}5.5&
  \cellcolor[HTML]{EFEFEF}{24.98}\\
 &
  Attention &
  \multicolumn{1}{c}{41.36} &
  \multicolumn{1}{c}{57.81} &
  \multicolumn{1}{c}{32.80} &
   &
  \multicolumn{1}{c}{8.19} &
  \multicolumn{1}{c}{3.36} &
   &
  \cellcolor[HTML]{CBCEFB}{\textbf{27.3}} &
  \cellcolor[HTML]{CBCEFB}{\underline{11.0}} &
  \multicolumn{1}{c}{\textbf{25.97}} \\
 &
  MLP &
  \multicolumn{1}{c}{41.08} &
  \multicolumn{1}{c}{57.80} &
  \multicolumn{1}{c}{33.29} &
   &
  \multicolumn{1}{c}{5.23} &
  \multicolumn{1}{c}{3.16} &
   &
  \multicolumn{1}{c}{14.0} &
  \multicolumn{1}{c}{2.4} &
  \multicolumn{1}{c}{22.42} \\
\multirow{-4}{*}{Code} &
  Linear &
   41.50&
   57.60&
   33.54&
   &
   7.51&
   3.06&
   &
   0.0&
   0.0& 20.46\\
   \bottomrule
\end{tabular}%
}
\caption{Impact of different layer types on compression performance across Math, Code, and Chat expert models derived from LLaMA-2 7B model. Attention layer compression performs best for Math and Code models, while MLP layer compression yields optimal results for Chat model.}
\label{tab:ablation}
\vspace{-0.3cm}
\end{table*}


\section{Method}

\subsection{Traditional Model Merging and Merging with Task-Specific Routing}

We first compare traditional model merging with the merging strategies combined with task-specific routing. 
Starting with \( K \) fine-tuned models \( \{\theta^{t_1}_{\text{SFT}}, \theta^{t_2}_{\text{SFT}}, \ldots, \theta^{t_K}_{\text{SFT}}\} \) derived from a common pre-trained backbone \( \theta_{\text{PRE}} \), each task vector is defined as the difference between and after finetuning:
\[
\delta_{t_k} = \theta^{t_k}_{\text{SFT}} - \theta_{\text{PRE}}, \quad \text{for } k \in \{1, \ldots, K\}.
\]


Traditional merging aggregates all task vectors to construct a single static merged model:
\[
\theta_{\text{merged}}^{\text{traditional}} = \theta_{\text{PRE}} + \sum_{k=1}^{K} \delta_{t_k}.
\]
In contrast, merging with routing leverages task specialized knowledge tailored to each input \( \mathbf{x} \) through a router:
\[
k^* = \text{Router}(\mathbf{x}), \quad
\theta_{\text{merged}}^{\text{routing}} = \theta_{\text{PRE}} + \delta_{t_{k^*}}.
\]
As shown in Figure \ref{fig:ta_routing}, traditional merging methods like Task Arithmetic \cite{task_arithmetic} are highly impacted by task interference: merging dissimilar tasks (math and chat) degrades performance more than similar tasks (math and code), and adding more tasks exacerbates the decline. 
In contrast, task-specific routing prevents interference and preserves the accuracy of fine-tuned models.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\linewidth} 
        \centering
        \includegraphics[width=\linewidth]{latex/ta_routing.png}
        \caption{Gaps from merging with task-specific routing.}
        \label{fig:ta_routing}
    \end{subfigure}
    \hfill  % 在子图之间插入水平间距
    \begin{subfigure}[b]{0.49\linewidth}  
        \centering
        \includegraphics[width=\linewidth]{latex/gsm8k_acc.png}
        \caption{Impact of merging with the fine-tuned model as base.}
        \label{fig:gsm8k_acc}
    \end{subfigure}
    \caption{Performance of merged model in traditional model merging and merging with task-specific settings.}
    \label{fig:ta_vs_routing}
    \vspace{-0.3cm}
\end{figure}

\subsection{Impact of Base Model in Model Merging}
Building on the strength of task-specific routing, we further investigate the impact of selecting different base models in model merging. 
Specifically, we compare the performance outcomes when employing a pre-trained backbone \( \theta_{\text{PRE}} \) versus a math-finetuned model \( \theta_{\text{Math}} = \theta_{\text{PRE}} + \delta_{t_{\text{Math}}} \) as the base model for mathematical input data.
As shown in Figure \ref{fig:gsm8k_acc}, substituting the pre-trained backbone with a math-finetuned model significantly enhances performance on the GSM8K \cite{cobbe2021training} dataset. 
Consequently, initializing the base model with task-specific fine-tuning allows the merged model to more effectively solve the corresponding task. 
% resulting in superior accuracy compared to using the pre-trained backbone alone.
Despite surpassing individually fine-tuned models, utilizing a task-specific base model concurrently increases storage requirements due to the necessity of loading multiple task vectors.

\subsection{\texttt{1bit}-Merging}
To balance performance with storage efficiency, we introduce \textbf{\texttt{1bit}-Merging}, a dynamic merging method that combines task-specific routing with 1-bit quantized task vectors. 


\paragraph{Compression of Task Vectors.}
Recognizing the substantial redundancy within task vectors, we apply 1-bit weight quantization to the task vectors, converting the weight matrices in Linear layers from FP32/16 precision to a 1-bit format. In this quantization process, each element of the task vector is set to either +1 or -1. To maintain performance despite the aggressive compression, we scale the binary weight matrices with scalar values in FP16 format. This scaling ensures that the quantized weights preserve the original weight \( L_2 \) norm, thereby maintaining model performance after the extremely low-bit compression \cite{liu2024bitdeltafinetuneworthbit}. The scaling factor \(\alpha\) is computed as:
\[
\alpha = \frac{\|\mathbf{W}\|_1}{{m \cdot n}}
\]
where $m$ and $n$ are the dimensions of the weight matrix. Using this scaling factor, the transformed task vector $\tilde{\delta}_{t_k}$ is defined as:
\begin{equation}
    \tilde{\delta}_{t_k} = \alpha * \text{Sign}(\delta_{t_k})
\end{equation}

\paragraph{Compression Position.}

We examine 1-bit compression effectiveness across different model components by selectively compressing Attention layers, MLP layers, and all Linear layers. As shown in Table \ref{tab:ablation}, each model type exhibits distinct compression characteristics.
Math and Code models achieve optimal performance when only attention layers are compressed while Chat models perform best with MLP layer compression.
Remarkably, the compressed versions of both Math and Code models demonstrate slight performance improvements over their original fine-tuned counterparts
These patterns reveal a fundamental difference in how knowledge is distributed within the model architecture: task-specific capabilities (like mathematical reasoning and code generation) primarily reside in MLP layers, while attention layers appear to store more general, transferable knowledge.
Based on these insights, we strategically apply attention layer quantization to Math and Code models, and MLP layer quantization to Chat models when compressing task vectors.

\paragraph{Dynamic Routing and Merging.}

Our merging framework then incorporates the compressed task vectors with a task-specific routing mechanism, where a trained router analyzes the input data \(\mathbf{x}\) to produce a probability distribution \(\mathbf{p}\) across different tasks. The most relevant task vector \(\delta_{t_k^*}\) is selected based on the highest probability \(k^*\) and added to the pre-trained model parameters \(\theta_{\text{PRE}}\) to form a task-specific base model \(\theta_{\text{base}} = \theta_{\text{PRE}} + \delta_{t_k^*}\). Finally, we apply Ties Merging \cite{ties_merging} to integrate the remaining compressed task vectors into \(\theta_{\text{base}}\), resulting in a comprehensive model that dynamically adapts to the most relevant tasks while maintaining overall performance through efficient parameter integration.

 
% Our merging framework then incorporates these quantized task vectors with a task-specific routing mechanism, where a trained router analyzes the input data \(\mathbf{x}\) to produce a probability distribution \(\mathbf{p}\).
% \begin{equation}
%     \mathbf{p} = \text{Router}(\mathbf{x}), \quad k^* = \arg\max_{k} p_k
% \end{equation}

% The selected task vector \( \delta_{t_k^*} \) is then added to the pre-trained model parameters \( \theta_{\text{PRE}} \) to construct a task-specific base model \( \theta_{\text{base}} \):
%     \[
%     \theta_{\text{base}} = \theta_{\text{PRE}} + \delta_{t_k^*}
%     \]

% Here, $\mathbf{p}$ represents the probability distribution over the available task vectors, and $k^*$ denotes the index of the selected task vector. 

% Finally, we apply Ties Merging to integrate other compressed task vectors to the task-specific base model. 

\section{Experiments}

\subsection{Experimental Setup}
% - Training and evaluation datasets
% - Model architectures and configurations
% - Implementation details and hyperparameters
% - Baseline methods for comparison

\paragraph{Baselines.} 
We evaluate the effectiveness of our \texttt{1bit-Merging} method through a two-step process. First, we assess the performance of compressed task vectors by comparing compressed expert models— which integrate these vectors with the pretrained backbone—against the individually fine-tuned models. Second, we benchmark \texttt{1bit}-Merging against existing model merging methods: Task Arithmetic, Ties-Merging, and DARE. 
Task Arithmetic \citep{task_arithmetic} enhances the merging process by introducing task vectors, suggesting that simple arithmetic operations on these vectors can effectively merge models. Building on the concept of task vectors, both DARE \citep{dare} and Ties-Merging \citep{ties_merging} employ pruning-then-scaling methods to merge task vectors, based on the assumption that not all parameters contribute equally to the final performance.  

\begin{table*}[!ht]
% \caption{Performance evaluation of merged LLaMA2-7B Models (Chat, Math, Code) across 7 task-specific datasets}
% \label{tab:llama2-7b}
\renewcommand{\arraystretch}{0.95}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccclcclccc}
\toprule
\multirow{2}{*}{Method} &
  \multicolumn{3}{c}{General Knowledge} &
   &
  \multicolumn{2}{c}{Mathmetical Reasoning} &
   &
  \multicolumn{2}{c}{Code Generation} &
  \multirow{2}{*}{Average} \\ 
  \cline{2-4} \cline{6-7} \cline{9-10}
                & MMLU  & HellaSwag & TruthfulQA &  & GSM8K & MATH &  & MBPP & HumanEval &  \\
                \hline
                \rowcolor{gray!10} 
\multicolumn{11}{c}{\textit{Finetuned Models}} \\
Chat & \textbf{46.38} & 57.79 & \textbf{45.17} &  & 23.43 & 4.86 &  & 0.3  & 0.6 &  25.50\\
Math & 40.05 & 56.30 & 32.56 &  & \textbf{48.60} & \textbf{8.50} &  & 21.8 & \textbf{12.8} & \textbf{31.52} \\
Code & 40.76 & \textbf{57.87} & 33.17 &  & 7.13  & 3.62 &  & \textbf{26.8} & 5.5 & 24.98 \\
\hline
\rowcolor{gray!10} 
\multicolumn{11}{c}{\textit{Compressed Expert Models}} \\
\texttt{1bit}-Chat & \textbf{46.40} & \textbf{58.23} & \textbf{43.70} &  & 22.74 & 5.08 &  & 0.0  & 0.0 &  25.55\\
\texttt{1bit}-Math & 41.03 & 56.68 & 34.52 &  & \textbf{47.46} & \textbf{8.42} &  & 23.1 & \textbf{14.0} & \textbf{32.17} \\
\texttt{1bit}-Code & 41.36 & 57.81 & 32.80 &  & 8.19  & 3.36 &  &\textbf{27.3} & 11.0 & 25.97 \\
\hline
\rowcolor{gray!10} 
\multicolumn{11}{c}{\textit{Model Merging Methods}} \\
Task Arithmetic & 41.50 & 49.63 & 37.45 &  & 47.34 & 6.46 &  & 13.5 & 7.3 &  29.03\\
Ties-Merging    & 45.75 & 56.63 & 40.02 &  & 46.93 & 7.74 &  & 29.1 & 17.1 &  34.75\\
DARE & 46.81 & 57.57 & 38.19 &  & 44.05 & 6.98 &  & \textbf{31.6} & \textbf{18.9} & 34.87 \\
\rowcolor{gray!20}
\texttt{1bit}-Merging    & \textbf{47.23} & \textbf{58.04} & \textbf{45.37} &  & \textbf{48.52} & \textbf{9.04} &  & 30.1 & \textbf{18.9} & \textbf{36.74} \\
\bottomrule
\end{tabular}%
}
\vspace{-0.1cm}
\caption{Performance evaluation of merged LLaMA2-7B Models (Chat, Math, Code) across 7 task-specific datasets}
\label{tab:llama2-7b}
% \end{table*}

\vspace{1em}  % 在两个表格之间添加适当的间距

% \begin{table*}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccclcclccc}
\toprule
\multirow{2}{*}{Method} &
  \multicolumn{3}{c}{General Knowledge} &
   &
  \multicolumn{2}{c}{Mathmetical Reasoning} &
   &
  \multicolumn{2}{c}{Code Generation} &
  \multirow{2}{*}{Average} \\ 
  \cline{2-4} \cline{6-7} \cline{9-10}
                & MMLU  & HellaSwag & TruthfulQA &  & GSM8K & MATH &  & MBPP & HumanEval &  \\
                \hline
                \rowcolor{gray!10} 
\multicolumn{11}{c}{\textit{Finetuned Models}} \\
Chat & 59.05 & \textbf{65.97} & \textbf{55.69} &  & 42.53 & 9.16 &  & 49.6  & 42.7 & \textbf{46.37} \\
Math & \textbf{60.77} & 58.68 & 44.68 &  & \textbf{63.38} & \textbf{22.74} &  & 38.1 & 23.8 & 44.59 \\
Code & 50.58 & 53.19 & 45.29 &  & 31.69 & 4.84 &  & \textbf{50.9} & \textbf{40.9} & 39.63 \\
\hline
\rowcolor{gray!10} 
\multicolumn{11}{c}{\textit{Compressed Expert Models}} \\
\texttt{1bit}-Chat & 54.71 & 58.00 & \textbf{53.24} &  & 38.51 & 8.14 &  & \textbf{47.6} & \textbf{35.4} & 42.23 \\
\texttt{1bit}-Math & \textbf{60.11} & \textbf{58.48} & 41.13 &  & \textbf{58.76} & \textbf{19.92} &  & 41.1 & 25.6 & \textbf{43.59} \\
\texttt{1bit}-Code & 47.88 & 53.51 & 42.96 &  & 27.29  & 5.16 &  & 41.6 & 33.5 & 35.99 \\
\hline
\rowcolor{gray!10} 
\multicolumn{11}{c}{\textit{Model Merging Methods}} \\
Task Arithmetic & 47.34 & 46.80 & 41.00 &  & 52.16 & 13.26 &  & 32.1 & 29.9 &  39.17\\
Ties-Merging    & \textbf{57.20} & 57.59 & 48.71 &  & 55.50 & 15.00 &  & 48.4 & 41.5 &  46.30 \\
DARE & 55.36 & 55.77 & 42.84 &  & 57.39 & 15.00 &  & \textbf{49.4} & 39.0 & 44.97 \\
\rowcolor{gray!20}
\texttt{1bit}-Merging    & 56.37 & \textbf{57.87} & \textbf{53.06} &  & \textbf{60.42} & \textbf{20.60} &  & 48.6 & \textbf{42.1} & \textbf{48.43} \\
\bottomrule
\end{tabular}%
}
\vspace{-0.1cm}
\caption{Performance evaluation of merged Mistral 7B Models (Chat, Math, Code) across 7 task-specific datasets}
\label{tab:mistral-7b}
\vspace{-0.3cm}
\end{table*}

\paragraph{Benchmark.} 
Our experimental evaluation focuses on three model families: LLaMA-2 7B \cite{touvron2023llama}, Mistral 7B \cite{jiang2023mistral}, and LLaMA-2 13B \cite{touvron2023llama}, each covering distinct specializations in: general knowledge (Chat), mathematical reasoning (Math), and code generation (Code). 
We assess performance using seven benchmark datasets across three domains: MMLU \cite{hendrycks2020measuring}, HellaSwag \cite{zellers2019hellaswag} and TruthfulQA \cite{lin2022truthfulqa} for assessing general knowledge and reasoning capabilities; GSM8K \cite{cobbe2021training} and MATH \cite{hendrycks2021measuring} for testing mathematical reasoning proficiency; and HumanEval \cite{chen2021evaluating} and MBPP \cite{austin2021program} for evaluating code generation abilities. To ensure consistent and unbiased assessment, model performance is evaluated using zero-shot accuracy, with pass@1 rate specifically measuring code generation correctness.


\subsection{Main Results}

Using the Chat\footnote{huggingface.co/meta-llama/Llama-2-7b-chat-hf}, Math\footnote{huggingface.co/TIGER-Lab/MAmmoTH-7B}, and Code\footnote{huggingface.co/mrm8488/llama-2-coder-7b} fine-tuned models derived from the same LLaMA2-7B pretrained model\footnote{huggingface.co/meta-llama/Llama-2-7b-hf}, we first compare the performance of the compressed expert models with the individually fine-tuned models to demonstrate the effectiveness of our task vector compression. Subsequently, we evaluate the performance of our \texttt{1bit}-Merging model against existing task-vector-based model merging methods. Table~\ref{tab:llama2-7b} presents a comprehensive comparison across seven datasets.
% targeting general knowledge, mathematical reasoning, and code generation.


\begin{table*}[]
\renewcommand{\arraystretch}{0.95}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccclcclccc}
\toprule
\multirow{2}{*}{Method} &
  \multicolumn{3}{c}{General Knowledge} &
   &
  \multicolumn{2}{c}{Mathmetical Reasoning} &
   &
  \multicolumn{2}{c}{Code Generation} &
  \multirow{2}{*}{Average} \\ 
  \cline{2-4} \cline{6-7} \cline{9-10}
                & MMLU  & HellaSwag & TruthfulQA &  & GSM8K & MATH &  & MBPP & HumanEval &  \\
                \hline
                \rowcolor{gray!10} 
\multicolumn{11}{c}{\textit{Finetuned Models}} \\
% \hline
Chat & \textbf{53.17} & 60.73 & \textbf{40.88} &  & 32.37 & 6.70 &  & 16.5  & 7.9 & 31.18 \\
Math & 52.73 & \textbf{61.10} & 37.09 &  & \textbf{55.50} & \textbf{10.84} &  & \textbf{28.8} & \textbf{15.9} & \textbf{37.42} \\
Code & 52.65 & 60.42 & 40.64 &  & 27.29 & 5.74 &  & 21.3 & 10.4 & 31.21 \\
\hline
\rowcolor{gray!10} 
\multicolumn{11}{c}{\textit{Compressed Expert Models}} \\
% \hline
\texttt{1bit}-Chat & 53.47 & 60.40 & \textbf{41.25} &  & 31.39 & 6.58 &  & 28.1 & 5.5 & 32.38 \\
\texttt{1bit}-Math & \textbf{53.52} & \textbf{61.39} & 35.86 &  & \textbf{56.71} & \textbf{10.38} &  & \textbf{31.6} & \textbf{16.5} & \textbf{37.80} \\
\texttt{1bit}-Code & 53.06 & 60.08 & 39.17 &  & 26.61 & 6.12 &  & 21.6 & 12.2 & 31.26 \\
\hline
\rowcolor{gray!10} 
\multicolumn{11}{c}{\textit{Model Merging Methods}} \\
% \hline
% Weight Average  & 55.66 & 61.56 & 41.49 &  & 46.55 & 8.36 &  & 34.8 & 19.5 & 38.27 \\
Task Arithmetic & 52.22 & 57.52 & 41.49 &  & 49.89 & 7.32 &  & 24.1 & 9.1 & 34.95 \\
Ties-Merging    & 55.48 & 60.65 & 39.05 &  & 52.46 & 9.90 &  & 40.4 & 21.3 & 39.89 \\
DARE & \textbf{55.65} & \textbf{61.66} & 40.51 &  & 55.19 & 9.08 &  & 39.1 & 20.1 & 40.18 \\
\rowcolor{gray!20}
\texttt{1bit}-Merging    & 55.48 & 60.22 & \textbf{42.21} &  & \textbf{56.65} & \textbf{10.40} &  & \textbf{40.6} & \textbf{22.0} & \textbf{41.07} \\
\bottomrule
\end{tabular}%
}
\caption{Performance evaluation of merged LLaMA2-13B Models (Chat, Math, Code) across 7 task-specific datasets}
\label{tab:llama2-13b}
\vspace{-0.3cm}
\end{table*}

\paragraph{Compressed Expert Models.} 
Our task vector compression method demonstrates remarkable effectiveness, with the compressed expert models not only maintaining but often surpassing the performance of their fine-tuned counterparts.
The \texttt{1bit}-Chat model achieves an average score of 25.55, surpassing its fine-tuned counterpart's 25.50. More notably, the \texttt{1bit}-Math model improves from 31.52 to 32.17, while the \texttt{1bit}-Code model shows the most substantial gain, increasing from 24.98 to 25.97.
Moreover, the compressed expert models exhibit strong adaptability, maintaining high performance not only in their specialized domains but also across other non-specialized domains. 
\textbf{(1) Superior performance in specialized domains:} Within their specialized domains, the \texttt{1bit}-Chat model enhances general knowledge capabilities with a 0.63 point improvement, the \texttt{1bit}-Math model maintains near-optimal performance, and most impressively, the \texttt{1bit}-Code model achieves a substantial 3.0 point boost in code generation proficiency. These results suggest that our compression method not only preserves but often enhances domain-specific expertise.
\textbf{(2) Superior performance in non-specialized domains:} The benefits of our compression technique extend beyond primary specializations, showcasing enhanced cross-domain generalization. This is particularly evident in the compressed Math model, which achieves notable gains of 1.11 points in general knowledge and 1.25 points in code generation—domains outside its primary expertise. These consistent improvements across both specialized and general domains indicate that our compression method facilitates positive knowledge transfer.

\vspace{-0.1cm}
\paragraph{Merging Models with \texttt{1bit}-Merging.} 
Our \texttt{1bit}-Merging method then integrates the compressed expert models through dynamic routing. As shown in Table \ref{tab:llama2-7b}, our \texttt{1bit}-Merging method demonstrates superior performance compared to existing merging techniques in two significant aspects. 
\textbf{(1) Consistent improvements across all tasks:}
While baseline methods exhibit distinctive domain-specific strengths - with Ties-Merging excelling in mathematical reasoning and DARE showing superior results in general knowledge and code generation - our \texttt{1bit}-Merging method achieves comparable or better performance across all domains. 
% Rather than specializing in a single area, \texttt{1bit}-Merging maintains competitive performance throughout different task categories. 
Specifically, our \texttt{1bit}-Merging method surpasses Ties-Merging by 1.45 points in mathematical reasoning and DARE by 2.71 points in general knowledge. Although we observe a modest 0.75-point decrease in code generation compared to DARE, this minor regression is primarily attributable to the lower baseline performance of the Code fine-tuned model. Notably, our method still achieves a substantial 8.35-point improvement over the original code fine-tuned model.
\textbf{(2) Exceeds the highest average performance of expert fine-tuned models:}
Perhaps most significantly, our method achieves a remarkable average performance score of 36.74 across all tasks, markedly surpassing the average best performance (35.16) of task-specific fine-tuned models—a distinctive achievement that none of the existing merging methods have accomplished. This remarkable improvement of 1.58 points over the highest individual expert scores demonstrates that our \texttt{1bit}-Merging method not only preserves but actually enhances the specialized capabilities of individual models, suggesting the merged model leverages complementary strengths across different domains and maintains their peak performance.

\begin{table*}[t]
\renewcommand{\arraystretch}{0.95}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccclcclccc}
\toprule
 &
   &
  \multicolumn{3}{c}{General Knowledge} &
   &
  \multicolumn{2}{c}{Mathmetical Reasoning} &
   &
  \multicolumn{2}{c}{Code Generation} &
  \multicolumn{1}{c}{} \\ \cline{3-5} \cline{7-8} \cline{10-11}
\multirow{-2}{*}{Models} &
  \multirow{-2}{*}{Position} &
  \multicolumn{1}{c}{MMLU} &
  \multicolumn{1}{c}{HellaSwag} &
  \multicolumn{1}{c}{TruthfulQA} &
   &
  \multicolumn{1}{c}{GSM8K} &
  \multicolumn{1}{c}{MATH} &
   &
  \multicolumn{1}{c}{MBPP} &
  \multicolumn{1}{c}{HumanEval} &
  \multicolumn{1}{c}{\multirow{-2}{*}{Average}} \\
  \hline
 &
  \cellcolor[HTML]{EFEFEF}/ &
  \cellcolor[HTML]{EFEFEF}\textbf{59.05} &
  \cellcolor[HTML]{EFEFEF}\textbf{65.97} &
  \cellcolor[HTML]{EFEFEF}\textbf{55.69} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}42.53 &
  \cellcolor[HTML]{EFEFEF}9.16 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}49.6 &
  \cellcolor[HTML]{EFEFEF}42.7 &
  \cellcolor[HTML]{EFEFEF}{\textbf{46.37}} \\
 &
  Attention & 53.25& 57.03 & \underline{54.59} && 39.20 & 8.14 && 46.6 & 36.0 & 42.12\\
 &
  MLP &
   54.71&
   58.00&
   {53.24}&
   &
   38.51&
   8.14 &
   &
   47.6&
   35.4&
   42.23 \\
\multirow{-4}{*}{Chat} &
  Linear &
   \cellcolor[HTML]{CBCEFB}\underline{54.95}&
   \cellcolor[HTML]{CBCEFB}\underline{58.83}&
   \cellcolor[HTML]{CBCEFB}53.73&
   &
   38.82&
   8.26&
   &
   47.6&
   37.2&
   \underline{42.77}\\
   \hline
 &
  \cellcolor[HTML]{EFEFEF}/ &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}60.77} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}58.68} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}44.68} &
  \cellcolor[HTML]{EFEFEF} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{63.38}} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{22.74}} &
  \cellcolor[HTML]{EFEFEF} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}38.1} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}23.8} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{44.59}} \\
 &
  Attention &
  \multicolumn{1}{c}{60.11} &
  \multicolumn{1}{c}{58.48} &
  \multicolumn{1}{c}{41.13} &
   &
  \cellcolor[HTML]{CBCEFB}{\underline{58.76}} &
  \cellcolor[HTML]{CBCEFB}{\underline{19.92}} &
   &
  \multicolumn{1}{c}{41.1} &
  \multicolumn{1}{c}{25.6} &
  \multicolumn{1}{c}{\underline{43.59}} \\
 &
  MLP &
  \multicolumn{1}{c}{58.74} &
  \multicolumn{1}{c}{58.12} &
  \multicolumn{1}{c}{43.57} &
   &
  \multicolumn{1}{c}{53.37} &
  \multicolumn{1}{c}{16.68} &
   &
  \multicolumn{1}{c}{44.9} &
  \multicolumn{1}{c}{29.3} &
  \multicolumn{1}{c}{{43.53}} \\
\multirow{-4}{*}{Math} &
  Linear &
  \multicolumn{1}{c}{57.93} &
  \multicolumn{1}{c}{58.24} &
  \multicolumn{1}{c}{39.41} &
   &
  \multicolumn{1}{c}{47.16} &
  \multicolumn{1}{c}{13.9} &
   &
  \multicolumn{1}{c}{41.1} &
  \multicolumn{1}{c}{25.6} &
  \multicolumn{1}{c}{40.48} \\
  \hline
 &
  \cellcolor[HTML]{EFEFEF}/ &
  \cellcolor[HTML]{EFEFEF}50.58 &
  \cellcolor[HTML]{EFEFEF}53.19&
  \cellcolor[HTML]{EFEFEF}45.29&
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}31.69&
  \cellcolor[HTML]{EFEFEF}4.84&
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}\textbf{50.9}&
  \cellcolor[HTML]{EFEFEF}\textbf{40.9}&
  \cellcolor[HTML]{EFEFEF}{\textbf{39.63}}\\
 &
  Attention &
  \multicolumn{1}{c}{47.88} &
  \multicolumn{1}{c}{53.51} &
  \multicolumn{1}{c}{42.96} &
   &
  \multicolumn{1}{c}{27.29} &
  \multicolumn{1}{c}{5.16} &
   &
  \cellcolor[HTML]{CBCEFB}{{41.6}} &
  \cellcolor[HTML]{CBCEFB}{\underline{33.5}} &
  \multicolumn{1}{c}{{35.99}} \\
 &
  MLP &
  \multicolumn{1}{c}{45.65} &
  \multicolumn{1}{c}{54.56} &
  \multicolumn{1}{c}{42.59} &
   &
  \multicolumn{1}{c}{34.34} &
  \multicolumn{1}{c}{6.08} &
   &
  \multicolumn{1}{c}{\underline{47.9}} &
  \multicolumn{1}{c}{28.0} &
  \multicolumn{1}{c}{\underline{37.02}} \\
\multirow{-4}{*}{Code} &
  Linear &
   39.99&
   53.80&
   36.47&
   &
   22.52&
   4.82 &
   &
   40.6 &
   31.1 & 32.76\\
   \bottomrule
\end{tabular}%
}
\vspace{-0.2cm}
\caption{Impact of different layer types on compression performance on Mistral 7B models. 
}
\label{tab:position_mistral}
\vspace{-0.2cm}
\end{table*}

\subsection{Using Different Model Architecture} 
To validate the generalizability of our method, we extend our experiments to the Mistral 7B architecture. As illustrated in Table \ref{tab:mistral-7b}, Our approach demonstrates robust performance improvements despite architectural differences. When merging Chat\footnote{huggingface.co/mistralai/Mistral-7B-Instruct-v0.1}, Math\footnote{huggingface.co/TIGER-Lab/MAmmoTH2-7B}, and Code\footnote{huggingface.co/Nondzu/Mistral-7B-codealpaca-lora} fine-tuned models derived from pretrained backbone \footnote{huggingface.co/mistralai/Mistral-7B-v0.1}. 
\texttt{1bit}-Merging outperforms Task Arithmetic by an average of 9.26 points, Ties-Merging by 2.13 points, and DARE by 3.46 points across all evaluated datasets. These enhancements are particularly significant in mathematical reasoning and general knowledge tasks. On the benchmark datasets MATH and GSM8K, our method respectively achieves accuracies of 20.60\% and 60.42\%, significantly exceeding DARE—the strongest baseline for mathematical reasoning—by 5.60 points (a 33.73\% relative improvement) on MATH and by 3.03 points (a 5.28\% relative improvement) on GSM8K. 
Additionally, on the TruthfulQA dataset, \texttt{1bit}-Merging demonstrates substantial gains over all baselines (12.06 points over Task Arithmetic, 4.35 points over Ties-Merging, and 10.22 points over DARE) while performing on par with the Chat fine-tuned model. This underscores the effectiveness of \texttt{1bit}-Merging in maintaining the peak performance in fine-tuned expert models. 
Furthermore, the average performance of our \texttt{1bit}-Merging method across all datasets is 48.43, surpassing the strongest Chat fine-tuned model's average of 46.37—a distinction not achieved by any other baseline methods.

\begin{table*}[t]
\renewcommand{\arraystretch}{0.95}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccclcclccc}
\toprule
 &
   &
  \multicolumn{3}{c}{General Knowledge} &
   &
  \multicolumn{2}{c}{Mathmetical Reasoning} &
   &
  \multicolumn{2}{c}{Code Generation} &
  \multicolumn{1}{c}{} \\ \cline{3-5} \cline{7-8} \cline{10-11}
\multirow{-2}{*}{Models} &
  \multirow{-2}{*}{Position} &
  \multicolumn{1}{c}{MMLU} &
  \multicolumn{1}{c}{HellaSwag} &
  \multicolumn{1}{c}{TruthfulQA} &
   &
  \multicolumn{1}{c}{GSM8K} &
  \multicolumn{1}{c}{MATH} &
   &
  \multicolumn{1}{c}{MBPP} &
  \multicolumn{1}{c}{HumanEval} &
  \multicolumn{1}{c}{\multirow{-2}{*}{Average}} \\
  \hline
 &
  \cellcolor[HTML]{EFEFEF}/ &
  \cellcolor[HTML]{EFEFEF}{53.17} &
  \cellcolor[HTML]{EFEFEF}{60.73} &
  \cellcolor[HTML]{EFEFEF}\underline{40.88} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}32.37 &
  \cellcolor[HTML]{EFEFEF}6.70 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}16.5 &
  \cellcolor[HTML]{EFEFEF}7.9 &
  \cellcolor[HTML]{EFEFEF}{31.18} \\
 &
  Attention & \cellcolor[HTML]{CBCEFB}\textbf{53.47}& \cellcolor[HTML]{CBCEFB}60.40 & \cellcolor[HTML]{CBCEFB}\textbf{41.25} && 31.39 & 6.58 && 28.1 & 5.5 & \underline{32.38}\\
 &
  MLP &
   53.13&
   60.94&
   38.68&
   &
   31.39&
   6.58 &
   &
   20.1&
   9.1&
   31.42 \\
\multirow{-4}{*}{Chat} &
  Linear &
   \underline{53.38}&
   \textbf{61.30}&
   39.05&
   &
   31.08&
   6.20&
   &
   32.3&
   4.3&
   \textbf{32.52}\\
   \hline
 &
  \cellcolor[HTML]{EFEFEF}/ &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}52.73} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}61.10} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}37.09} &
  \cellcolor[HTML]{EFEFEF} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\underline{55.50}} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}\textbf{10.84}} &
  \cellcolor[HTML]{EFEFEF} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}28.8} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}15.9} &
  \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}{37.42}} \\
 &
  Attention &
  \multicolumn{1}{c}{53.52} &
  \multicolumn{1}{c}{61.39} &
  \multicolumn{1}{c}{35.86} &
   &
  \cellcolor[HTML]{CBCEFB}{\textbf{56.71}} &
  \cellcolor[HTML]{CBCEFB}{\underline{10.38}} &
   &
  \multicolumn{1}{c}{31.6} &
  \multicolumn{1}{c}{16.5} &
  \multicolumn{1}{c}{\textbf{37.80}} \\
 &
  MLP &
  \multicolumn{1}{c}{54.19} &
  \multicolumn{1}{c}{61.70} &
  \multicolumn{1}{c}{35.25} &
   &
  \multicolumn{1}{c}{53.98} &
  \multicolumn{1}{c}{9.10} &
   &
  \multicolumn{1}{c}{33.6} &
  \multicolumn{1}{c}{13.4} &
  \multicolumn{1}{c}{{37.32}} \\
\multirow{-4}{*}{Math} &
  Linear &
  \multicolumn{1}{c}{54.91} &
  \multicolumn{1}{c}{61.89} &
  \multicolumn{1}{c}{35.62} &
   &
  \multicolumn{1}{c}{53.90} &
  \multicolumn{1}{c}{8.80} &
   &
  \multicolumn{1}{c}{32.8} &
  \multicolumn{1}{c}{14.6} &
  \multicolumn{1}{c}{\underline{37.50}} \\
  \hline
 &
  \cellcolor[HTML]{EFEFEF}/ &
  \cellcolor[HTML]{EFEFEF}52.65 &
  \cellcolor[HTML]{EFEFEF}60.42&
  \cellcolor[HTML]{EFEFEF}40.64&
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}27.29&
  \cellcolor[HTML]{EFEFEF}5.74&
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}\underline{21.3}&
  \cellcolor[HTML]{EFEFEF}10.4&
  \cellcolor[HTML]{EFEFEF}31.21\\
 &
  Attention &
  \multicolumn{1}{c}{53.06} &
  \multicolumn{1}{c}{60.08} &
  \multicolumn{1}{c}{39.17} &
   &
  \multicolumn{1}{c}{26.61} &
  \multicolumn{1}{c}{6.12} &
   &
  \cellcolor[HTML]{CBCEFB}\textbf{21.6} &
  \cellcolor[HTML]{CBCEFB}{\textbf{12.2}} &
  \multicolumn{1}{c}{\textbf{31.26}} \\
 &
  MLP &
  \multicolumn{1}{c}{53.06} &
  \multicolumn{1}{c}{60.34} &
  \multicolumn{1}{c}{38.19} &
   &
  \multicolumn{1}{c}{22.29} &
  \multicolumn{1}{c}{4.76} &
   &
  \multicolumn{1}{c}{5.5} &
  \multicolumn{1}{c}{\underline{12.2}} &
  \multicolumn{1}{c}{{28.05}} \\
\multirow{-4}{*}{Code} &
  Linear &
   53.42&
   60.60&
   37.09&
   &
   15.84&
   4.58 &
   &
   7.8 &
   8.5 & 26.83 \\
   \bottomrule
\end{tabular}%
}
\vspace{-0.3cm}
\caption{Impact of different layer types on compression performance on LLaMA-2 13B models. 
}
\label{tab:position_llama_13}
\vspace{-0.3cm}
\end{table*}
 
% Similarly, our \texttt{1bit}-Merging achieves a higher average performance (47.57) compared to the best original fine-tuned models (46.37) across all tasks, a distinction not achieved by any other merging method.

\subsection{Scaling to Larger Model Size} 
We further evaluate the scalability of our method using LLaMA-2 13B\footnote{huggingface.co/meta-llama/Llama-2-13b-hf} architecture by merging Chat\footnote{huggingface.co/meta-llama/Llama-2-13b-chat-hf}, Math\footnote{huggingface.co/TIGER-Lab/MAmmoTH-13B}, and Code\footnote{huggingface.co/emre/llama-2-13b-code-chat} fine-tuned models. As shown in Table \ref{tab:llama2-13b}, our approach remains effective at this larger scale. The compressed expert models demonstrate consistent enhancement, achieving an average improvement of 0.54 points over their original fine-tuned counterparts. Specifically, the compressed experts show consistent improvements across all specialized domains: \texttt{1bit}-Chat achieves a 0.11-point gain in general knowledge tasks, \texttt{1bit}-Math demonstrates a 0.38-point improvement in mathematical reasoning, and \texttt{1bit}-Code shows a notable 1.05-point enhancement in code generation.
% Chat, Math, and Code models improve 1.20 points, achieve scores of 32.38 versus 31.18, 37.80 versus 37.42, and 31.26 versus 31.21 compared to the original fine-tuned models.
Additionally, our \texttt{1bit}-Merging method demonstrates superior performance against all baseline approaches, surpassing Task Arithmetic by 6.12 points, Ties-Merging by 1.18 points, and DARE by 0.89 points on average across all evaluated datasets. When compared to DARE, the strongest baseline for mathematical reasoning and general knowledge, our method achieves notable improvements of 1.32 points (14.54\% relative increase) on MATH and 1.70 points (4.20\% relative increase) on TruthfulQA. In code generation tasks, our method outperforms the leading baseline, Ties-Merging, by an average of 0.45 points.

\begin{table*}[!t]
\caption{Performance comparison between merging original and compressed fine-tuned LLaMA2-7B models.}
\renewcommand{\arraystretch}{0.95}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccclcclccc}
\toprule
\multirow{2}{*}{Method} &
  \multicolumn{3}{c}{General Knowledge} &
   &
  \multicolumn{2}{c}{Mathmetical Reasoning} &
   &
  \multicolumn{2}{c}{Code Generation} &
  \multirow{2}{*}{Average} \\ 
  \cline{2-4} \cline{6-7} \cline{9-10}
                & MMLU  & HellaSwag & TruthfulQA &  & GSM8K & MATH &  & MBPP & HumanEval &  \\
                \hline
                % \rowcolor{gray!10} 
% \multicolumn{11}{c}{\textit{Finetuned Models}} \\
% Chat & \textbf{46.38} & 57.79 & \textbf{45.17} &  & 23.43 & 4.86 &  & 0.3  & 0.6 &  25.50\\
% Math & 40.05 & 56.30 & 32.56 &  & \textbf{48.60} & \textbf{8.50} &  & 21.8 & \textbf{12.8} & \textbf{31.52} \\
% Code & 40.76 & \textbf{57.87} & 33.17 &  & 7.13  & 3.62 &  & \textbf{26.8} & 5.5 & 24.98 \\
% \hline
% \rowcolor{gray!10} 
% \multicolumn{11}{c}{\textit{Compressed Expert Models}} \\
% \texttt{1bit}-Chat & \textbf{46.40} & \textbf{58.23} & \textbf{43.70} &  & 22.74 & 5.08 &  & 0.0  & 0.0 &  25.55\\
% \texttt{1bit}-Math & 41.03 & 56.68 & 34.52 &  & \textbf{47.46} & \textbf{8.42} &  & 23.1 & \textbf{14.0} & \textbf{32.17} \\
% \texttt{1bit}-Code & 41.36 & 57.81 & 32.80 &  & 8.19  & 3.36 &  &\textbf{27.3} & 11.0 & 25.97 \\
% \hline
% \rowcolor{gray!10} 
% \multicolumn{11}{c}{\textit{Model Merging Methods}} \\
LLaMA2-7B & 41.50 & 49.63 & 37.45 &  & 47.34 & 6.46 &  & 13.5 & 7.3 &  29.03\\
\rowcolor{gray!10}
\quad w/ \textit{Compressed Experts} & 42.49 & 51.45 & 39.17 &  & 45.72 & 7.70 &  & 15.8 & 8.5 &  30.09 \textcolor{blue}{(+1.06)}\\
Mistral-7B & 47.34 & 46.80 & 41.00 &  & 52.16 & 13.26 &  & 32.1 & 29.9 &  39.17\\
\rowcolor{gray!10}
\quad w/ \textit{Compressed Experts} & 45.98 & 48.03 & 40.76 &  & 46.32 & 12.14 &  & 33.8 & 29.3 & 36.62 \textcolor{green}{(-2.55)}\\
LLaMA2-13B  & 52.22 & 57.52 & 41.49 &  & 49.89 & 7.32 &  & 24.1 & 9.1 & 34.95 \\
\rowcolor{gray!10}
\quad w/ \textit{Compressed Experts} & 52.57 & 58.01 & 41.49 &  & 53.60 & 7.48 &  & 27.3 & 12.2 & 36.09 \textcolor{blue}{(+1.14)} \\
% \texttt{1bit}-Merging    & \textbf{47.23} & \textbf{58.04} & \textbf{45.37} &  & \textbf{48.52} & \textbf{9.04} &  & 30.1 & \textbf{18.9} & \textbf{36.74} \\
\bottomrule
\end{tabular}%
}
\label{tab:bit-merge}
\vspace{-0.3cm}
\end{table*}
 
% These comprehensive improvements across diverse task domains validate the robustness and scalability of our \texttt{1bit}-Merging approach.


\subsection{Ablation Studies}

\paragraph{Layer Types on Compression Performance.} 
We further evaluate how compression position affects the performance of compressed expert models across Mistral 7B and LLaMA-2 13B architectures. As shown in Tables \ref{tab:position_mistral} and \ref{tab:position_llama_13}, for Chat models, while the optimal quantization locations showed slight variations—with Mistral 7B preferring all Linear layers and LLaMA2 13B favoring attention layers—the performance differences between these choices were minimal. However, for specialized Math and Code models, we find quantizing attention layers consistently to be most effective in preserving domain-specific capabilities.

\paragraph{Merging Compressed Expert Models.} Merging compressed expert models proves effective for Task Arithmetic but shows limitations with parameter-dropping methods like Ties-Merging and DARE. This is because parameter-dropping techniques destroy the compressed knowledge distributions essential for specialized task performance. In our experiments with LLaMA2-7B and LLaMA2-13B, where compressed experts show marginal improvements over individually fine-tuned models, merging compressed experts yields performance gains of 1.06 and 1.14 points respectively, compared to merging their fine-tuned counterparts.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{latex/storage.png}
    \caption{Performance and storage trade-off of different methods in merging fine-tuned Mistral 7B models.}
    \label{fig:storage}
\end{figure}

\subsection{Performance vs. Storage Trade-offs}
We demonstrate the trade-off between performance and storage requirements in Figure \ref{fig:storage}. We use Mistral 7B models, as its Code model demonstrates superior performance in code generation. Task-specific routing (Routing) achieves performance equivalent to individually fine-tuned models, serving as a performance upper bound. However, Routing requires maintaining full parameters for each task model, leading to substantial storage overhead. While model merging methods reduce storage requirements, they underperform Routing by over 4.93 points. Our \texttt{1bit}-Merging strikes a favorable balance between these extremes, maintaining 94.53\% of Routing performance while requiring only 66.25\% of its storage. Furthermore, leveraging the observation that Chat models exhibit minimal sensitivity to compression positions, we applied quantization on all Linear layers for the Chat model. This optimization achieves an even more efficient storage reduction to 55.02\% while preserving comparable performance levels.

% \subsection{Cross-task Generalization}
% - Evaluation of generalization ability across different tasks
% - Analysis of expert model selection impact
% - Comparison of different distance metrics for generalization measurement
% - Validation of generalization scaling factors

% \subsection{Router Analysis}


\section{Conclusion}

We propose \texttt{1bit}-Merging, a novel framework that effectively combines specialized language models while addressing the fundamental trade-off between performance and storage efficiency. By incorporating dynamic routing with binary quantization, our approach maintains task-specific expertise while significantly reducing storage overhead. Extensive experiments across general knowledge, mathematical reasoning, and code generation tasks demonstrate that \texttt{1bit}-Merging not only preserves the specialized capabilities of individual models but often enhances their performance. 

\section*{Limitations}
% Despite the promising results of 1bit-Merging, several limitations warrant consideration. First, while our method reduces storage requirements, the dynamic routing mechanism introduces few additional computational overhead during inference as the router must analyze each input to determine the most relevant task vector. Second, although compressed expert models show enhanced performance within their specialized domains, the framework may not fully capture complex cross-task interactions that could enable more sophisticated knowledge transfer.  
% Third, our current evaluation focuses on merging three specialized models, and the effectiveness of 1bit-Merging when combining a larger number of specialized models remains unexplored. 
% Fourth, while our 1-bit quantization approach preserves model performance overall, it may not be optimal for all types of task-specific knowledge. 
% Finally, our layer-specific compression strategy is based on empirical observations from specific architectures (LLaMA2 and Mistral), and may need to be re-evaluated for different model architectures or emerging architectural paradigms.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

Despite the promising results of 1bit-Merge, several limitations deserve attention. First, while our method significantly reduces storage requirements, the dynamic routing mechanism introduces computational overhead during inference, as the router must analyze each input to determine the optimal task vector and perform binary transformations. Although this overhead is relatively small, it could impact real-time applications with strict latency requirements. 

Second, while compressed expert models show enhanced performance within their specialized domains, our framework may not fully capture complex cross-task interactions that could enable more sophisticated knowledge transfer, particularly for tasks requiring simultaneous expertise from multiple domains.

Third, while our 1-bit quantization approach preserves model performance for most tasks, it may not be optimal for all types of task-specific knowledge, particularly for tasks requiring high numerical precision or fine-grained reasoning. 

\section*{Ethics Statement}
This study utilizes publicly available datasets for our models. Prior research endeavors have generally taken ethical considerations into account. We have manually inspected a subset of samples and found no explicit ethical concerns, including violent or offensive content. Nonetheless, it is crucial to highlight that the output generated by large language models lacks the degree of control we might assume. Consequently, we are prepared to implement measures to mitigate any unforeseen outputs.

\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
  