\section{Related Work}
Modeling merging \citep{merge_survey, mergeKit} has attracted much attention for its several advantages: 1) it significantly reduces the storage and deployment expenses by consolidating multiple models into a single one \citep{task_arithmetic,widen}; 2) it operates in a plug-and-play manner, eliminating the need for additional training\citep{model_soup,dare}; and 3) it can improve performance on individual tasks\citep{model_soup,task_arithmetic,ties_merging} while also improving out-of-domain generalization\citep{cha_2021_swad,reg_mean}.
Based on whether the merged model remains consistent across all samples or tasks, model merging approaches can be categorized into two types: static model merging and dynamic model merging.

\subsection{Static Model Merging}
Static model merging primarily explores general strategies for combining models, such as Average Merging \citep{model_soup}, Task Arithmetic \citep{task_arithmetic}, and Ties-Merging \citep{ties_merging}. Notably, \citet{model_soup} first demonstrated that even a straightforward weight averaging of base models can improve both the performance and robustness of downstream tasks.
Building on this, Task Arithmetic \citep{task_arithmetic} refines the merging process by introducing task vectors, proposing that simple arithmetic operations on these vectors can effectively modify models and yield a better merged model. Expanding upon the concept of task vectors, methods like DARE \citep{dare} and Ties \citep{ties_merging} adopt pruning-then-scaling techniques to merge task vectors, based on the premise that not all parameters equally contribute to the final performance. 
% While DARE is primarily tailored for merging models with minor parameter variations, WIDEN \citep{widen} addresses the challenge of merging models with significant parameter shifts by disentangling weight components.
% Another line of static model merging lies in geometric-based merging methods. For example, SLERP \citep{shoemake85} is specifically designed for the integration of two models, which finds the spherical path in models' parameter space.
% Model Stock \citep{model_stock} approximates the merged weights using only a few fine-tuned models, exploiting the geometric properties of the weight space and the anchoring effect of a pre-trained model.
However, static merging of models from different domains often sacrifices task-specific performance to strike a balance between generalization capacity and task-specific effectiveness. To this end, we explore the dynamic model merging in this work.

\subsection{Dynamic Model Merging}
% Activation plays a critic role in dynamic model merging. One popular approach to using activations is to formulate model merging as an optimization problem. For example, Fisher merging \citep{fisher} treats the merging process as selecting parameters that approximately maximize the joint likelihood of the posteriors of the models’ parameters, and leverages the Laplace approximation by using the diagonal of each model’s Fisher information as the precision matrix for that model’s posterior. \citep{reg_mean} seeks to minimize prediction discrepancies between the merged model and the individual models, proposing RegMean, which computes the optimal weights by minimizing the Euclidean distance to the model predictions.

Additionally, another line of studies focuses on routing-based model merging.
For instance, SMEAR \citep{smear} propose a routing-based merging paradigm where parameter fusion is implemented through weighted averaging guided by router input distributions across expert modules, maintaining computational efficiency comparable to singular expert operations.
Extending this paradigm, Twin-Merging \citep{twin-merging} develop an adaptive knowledge integration framework that dynamically reconciles task-shared and task-specific representations via routing mechanisms during inference.
\citep{weight-ensemble} advance this domain through a Transformer-based dynamic composition architecture, with empirical analysis revealing disproportionate parameter modification magnitudes between linear and nonlinear layers during fine-tuning - a critical factor affecting integration efficacy. Notwithstanding their methodological advancements, extant routing-based merging frameworks incur significant storage demands for task vector retention, which motivates the development of our computationally efficient model merging framework in this work.
