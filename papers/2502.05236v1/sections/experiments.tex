
\subsection{Datasets, Training and Evaluation Criteria}
\label{sec:datasets}

For our primary experiments, we train the models on a data-blend containing $18k$ hours of English TTS data from the following datasets: \emph{train-clean-360} and \emph{train-clean-100} subsets of LibriTTS~\cite{zen2019libritts}, HiFiTTS~\cite{bakhturina21_interspeech}, a $17k$-hour subset of the LibriVox MLS dataset~\cite{pratap20_interspeech} and a proprietary, 2-speaker, 63-hour dataset. 
With this dataset we create \textit{(context audio, transcript, target audio)} triplets where context and target audio are distinct utterances from the same speaker. During training, we use a random $5$ second context slice for the decoder context model and $3$ to $8$ seconds context slice for \textit{multi encoder} and \textit{SV conditioned} models. Model architectures and training details are provided in Appendix~\ref{sec:trainingdetails}.


% In each model, the decoder is built with $12$ Transformer layers using a hidden dimension of $768$ and a feed-forward network (FFN) dimension of $3072$. Rather than a standard FFN sublayer, this decoder employs a causal convolution layer with a kernel size of $3$. The transcript encoder, on the other hand, is composed of $6$ Transformer layers that do not use causal masking but otherwise match the decoder's specifications. For the multi-encoder configuration, a $3$ layer context encoder is added, which uses the same non-causal design as the transcript encoder. Our models are trained on $16$ NVIDIA A100 GPUs using a global batch size of $256$, optimized using Adam optimizer with an initial learning rate of $1e-4$. The learning rate is annealed every 1000 training steps using an exponential decay factor of 0.998.

\textbf{Evaluation:} We evaluate synthesized speech on intelligibility, speaker similarity, and naturalness. Intelligibility is measured using ASR-based character error rate (CER) and word error rate (WER), with Parakeet-TDT ~\cite{xu2023efficient} for English and \textit{whisper-large-v3}~\cite{radford2022whisper} for other languages. Speaker similarity is assessed via cosine similarity between speaker embeddings of synthesized and context audio, using \textit{Titanet-Small}~\cite{koluguri2022titanet} for embedding extraction—different from \textit{Titanet-Large}~\cite{koluguri2022titanetlarge} used in \textit{SV conditioned} model training. Naturalness is evaluated with Squim-MOS~\cite{kumar2023torchaudio}, and we also conduct a human evaluation for two of our zero-shot Koel-TTS models, benchmarking them against others in Section~\ref{sec:pastworkcomparison}.
For inference, we use multinomial top-k sampling ($k$\texttt{=80}, temperature\texttt{=0.6}). Due to probabilistic generation, each experiment is repeated five times, reporting mean metrics with 95\% confidence intervals.

For seen speaker TTS evaluation, we consider a test set of $200$ held-out utterances from \textit{train-clean-360} LibriTTS dataset. For unseen speakers, we create a subset of \textit{test-clean} LibriTTS containing $180$ utterances from a total of $36$ out of the $40$ speakers, using $5$ distinct context and target audios from each each speaker. We use a random $5$ second slice from the context audio during inference for all experiments.

\subsection{Architecture Comparison}
Table~\ref{tab:architecture_comp} presents the baseline results of different Koel-TTS architectures on seen and unseen English speakers, \textbf{without} incorporating preference alignment training or CFG inference.  
All three architectures achieve similar intelligibility, but the \textit{decoder context} model outperforms the \textit{multi encoder} model on unseen speaker similarity, while the latter performs slightly better on seen speakers.  
These results suggest that \emph{decoder context} model generalizes better to unseen speakers making it a more suitable choice for zero-shot TTS. 
The \textit{multi encoder} architecture tends to overfit to the training speakers, as indicated by consistently worse speaker similarity on unseen speakers, and better speaker similarity on seen speakers across all our experiments. 
While \textit{SV conditioned} model also achieves similar SSIM as decoder context, perceptually, we find the decoder context model captures the intended style of the context audio better. We encourage readers to listen to audio examples on our website.

\setlength{\tabcolsep}{2pt}
\begin{table}[ht]
\vspace{-4mm}
\caption{\footnotesize{Baseline TTS results on seen and unseen speakers for different Koel-TTS models, \textbf{without using CFG or preference alignment}. Lower CER(\%) \& WER(\%) indicate higher intelligibility. Higher SSIM indicates higher speaker similarity to ground-truth.}}
\centering
\resizebox{\columnwidth}{!}{%
% \footnotesize
\begin{tabular}{c|l|cccc}
\toprule
Eval Set & Model & CER(\%) $\downarrow$ & WER(\%) $\downarrow$ & SSIM $\uparrow$ & Squim-MOS $\uparrow$ \\
\midrule
& Ground Truth & $0.51 \pm 0.00$& $1.42 \pm 0.00 $  & $0.763 \pm 0.000$ & $4.616 \pm 0.03$\\
 Seen &  Decoder context & $1.73 \pm 0.60$ & $2.98 \pm 0.59$ & $0.700 \pm 0.001$ & $4.350 \pm 0.038$ \\ 
Speakers & SV Conditioned & $\mathbf{1.71 \pm 0.41}$ & $\mathbf{2.82 \pm 0.41}$ & $0.697 \pm 0.003$ & $\mathbf{4.360 \pm 0.021}$ \\ 
  & Multi Encoder  & $1.92 \pm 0.68$ & $3.02 \pm 0.76$ & $\mathbf{0.712 \pm 0.002}$ & $4.346 \pm 0.028$ \\ 
\midrule
 & Ground Truth & $0.80 \pm 0.00 $ & $1.83 \pm 0.00 $ & $0.771 \pm 0.000$ & $4.588 \pm 0.020$ \\
Unseen & Decoder  Context & $2.68 \pm 1.13$ & $4.02 \pm 1.12$ & $\mathbf{0.637 \pm 0.008}$ & $\mathbf{4.347 \pm 0.024}$ \\ 
Speakers& SV Conditioned & $3.12 \pm 0.98$ & $4.22 \pm 1.02$ & $0.619 \pm 0.003$ & $4.318 \pm 0.034$ \\ 
& Multi Encoder  & $\mathbf{2.56 \pm 1.44}$ & $\mathbf{3.74 \pm 1.36}$ & $0.601 \pm 0.004$ & $4.318 \pm 0.059$ \\
\bottomrule
\end{tabular} 
}
\label{tab:architecture_comp}
\vspace{-2mm}
\end{table}

\subsection{Preference Alignment}
To perform preference alignment, we create a preference dataset using the procedure described in Section~\ref{sec:pref_align}. 
Specifically, we first curate $800$ challenging texts generated using Llama-8b~\cite{touvron2023llama}. It is prompted to generate texts with repeated words and alliterations. The complete list of these texts and the prompts used for generating them can be found on our webpage. We pair each challenging text with $10$ random context audios sampled from our training dataset. Next, we sample $50{,}000$ regular transcripts from our training data, and pair each text with one random context audio from our training data. This results in a total $58{,}000$ text and context audio pairs. For each pair, we generate $6$ audio samples from each of our models and create chosen-rejected pairs using the reward and filtering criteria outlined in Section~\ref{sec:pref_align}.
% using the procedure described above. Additionally, we filter out any pair in which the chosen example has a CER $> 2\%$ to retain only high-quality preference pairs. This results in around $40k$ to $50k$ chosen-rejected pairs across the three Koel-TTS architectures.



% Multi-encoder  & $1.92 \pm 0.68$ & $3.02 \pm 0.76$ & $0.71 \pm 0.00$ & $4.346 \pm 0.028$ 
% Multi-encoder  & $2.56 \pm 1.44$ & $3.74 \pm 1.36$ & $0.60 \pm 0.00$ & $4.318 \pm 0.059$
% 0.7193 +/- 0.0030

\setlength{\tabcolsep}{3pt}
\begin{table*}[t]
\vspace{-2mm}
\caption{\footnotesize{Preference Alignment (DPO, RPO) and CFG evaluations for a multi encoder baseline model on seen and unseen speakers. Both methods improve intelligibility, speaker similarity and naturalness metrics, with best results achieved when they are used together. GT as Chosen indicates the ablation experiment considering ground truth audio as the chosen audio in the preference pairs.}}
\centering
\resizebox{\textwidth}{!}{%
% \footnotesize
\begin{tabular}{l|cccc|cccc}
\multicolumn{1}{c}{} & \multicolumn{4}{c}{\emph{Seen Speakers}} & \multicolumn{4}{c}{\emph{Unseen Speakers}} \\
\toprule
Model/Technique & CER(\%) $\downarrow$ & WER(\%) $\downarrow$ & SSIM $\uparrow$ & Squim-MOS $\uparrow$ & CER(\%) $\downarrow$ & WER(\%) $\downarrow$ & SSIM $\uparrow$ & Squim-MOS $\uparrow$ \\
\midrule
Ground Truth & $0.51 \pm 0.00$& $1.42 \pm 0.00 $  &  $0.763 \pm 0.000$ & $4.616 \pm 0.03$ & $0.80 \pm 0.00 $ & $1.83 \pm 0.00 $ & $0.771 \pm 0.000$ & $4.588 \pm 0.020$  \\
\midrule
Multi Encoder (BL) & $1.92 \pm 0.68$ & $3.02 \pm 0.76$ & $0.712 \pm 0.002$ & $4.346 \pm 0.028$ & $2.56 \pm 1.44$ & $3.74 \pm 1.36$ & $0.601 \pm 0.004$ & $4.318 \pm 0.059$ \\ 
BL + RPO & $1.01 \pm 0.58$ & $1.76 \pm 0.59$ & $0.737 \pm 0.002$ & $4.408 \pm 0.010$ & $0.79 \pm 0.12$ & $1.72 \pm 0.18$ & $0.641 \pm 0.002$ & $4.389 \pm 0.021$ \\ 
BL + DPO & $0.67 \pm 0.17$ & $1.48 \pm 0.34$ & $0.737 \pm 0.004$ & $4.406 \pm 0.011$ & $0.62 \pm 0.12$ & $1.49 \pm 0.20$ & $0.645 \pm 0.001$ & $4.402 \pm 0.029$ \\  
BL + DPO (GT as Chosen) & $1.58 \pm 0.42$ & $2.88 \pm 0.41$ & $0.710 \pm 0.005$ & $4.344 \pm 0.038$ & $2.67 \pm 0.006$ & $4.08 \pm 0.48$ & $0.522 \pm 0.004$ &  $4.281 \pm 0.055$
\\
\midrule
BL + CFG ($\gamma=2.5$) & $0.73 \pm 0.16$ & $1.63 \pm 0.24$ & $0.752 \pm 0.003$ & $4.420 \pm 0.010$ & $0.69 \pm 0.07$ & $1.59 \pm 0.10$ & $0.653 \pm 0.005$ & $4.415 \pm 0.007$ \\ 
BL + RPO + CFG ($\gamma=2.5$)& $0.75 \pm 0.23$ & $1.56 \pm 0.32$ & $0.766 \pm 0.003$ & $\mathbf{4.422 \pm 0.011}$ & $\mathbf{0.51 \pm 0.12}$ & $\mathbf{1.25 \pm 0.18}$ & $0.674 \pm 0.004$ & $4.392 \pm 0.029$ \\ 
BL + DPO + CFG ($\gamma=2.5$)& $\mathbf{0.51 \pm 0.12}$ & $\mathbf{1.32 \pm 0.23}$ & $\mathbf{0.767 \pm 0.004}$ & $4.418 \pm 0.012$ & $0.58 \pm 0.17$ & $1.38 \pm 0.08$ & $\mathbf{0.678 \pm 0.005}$ & $\mathbf{4.417 \pm 0.015}$ \\ 
\bottomrule
\end{tabular} 
}
\label{tab:dpo_rpo_cfg_base_multi}
\vspace{-4mm}
\end{table*}


% \setlength{\tabcolsep}{3pt}
% \begin{table*}[t]
% \caption{\footnotesize{}}
% \centering
% \resizebox{\textwidth}{!}{%
% % \footnotesize
% \begin{tabular}{l|cccc|cccc}
% \multicolumn{1}{c}{} & \multicolumn{4}{c}{\emph{Seen Speakers}} & \multicolumn{4}{c}{\emph{Unseen Speakers}} \\
% \toprule
% Model/Technique & CER(\%) $\downarrow$ & WER(\%) $\downarrow$ & SSIM $\uparrow$ & Squim-MOS $\uparrow$ & CER(\%) $\downarrow$ & WER(\%) $\downarrow$ & SSIM $\uparrow$ & Squim-MOS $\uparrow$ \\
% \midrule
% Multi-encoder (BL) & $1.73 \pm 0.60$ & $2.98 \pm 0.59$ & $0.70 \pm 0.00$ & $4.350 \pm 0.038$ & $2.68 \pm 1.13$ & $4.02 \pm 1.12$ & $0.64 \pm 0.01$ & $4.347 \pm 0.024$ \\
%  BL + DPO & $0.82 \pm 0.21$ & $1.88 \pm 0.21$ & $0.72 \pm 0.00$ & $4.411 \pm 0.011$ & $1.25 \pm 0.68$ & $2.26 \pm 0.56$ & $0.68 \pm 0.00$ & $4.401 \pm 0.013$ \\
%  BL + RPO & $0.98 \pm 0.33$ & $2.01 \pm 0.35$ & $0.72 \pm 0.00$ & $4.434 \pm 0.027$ & $0.87 \pm 0.24$ & $1.88 \pm 0.30$ & $0.68 \pm 0.00$ & $4.437 \pm 0.020$ \\
%  % BL + DPO (GT Chosen) & \\
%  \midrule 
%  BL + CFG ($\gamma=2.5$) & $0.59 \pm 0.13$ & $1.48 \pm 0.38$ & $0.74 \pm 0.00$ & $4.414 \pm 0.006$ & $0.63 \pm 0.06$ & $1.56 \pm 0.02$ & $0.71 \pm 0.00$ & $4.418 \pm 0.009$ \\
%  BL + DPO + CFG ($\gamma=2.5$) & $0.57 \pm 0.14$ & $1.45 \pm 0.19$ & $0.74 \pm 0.00$ & $4.427 \pm 0.017$ & $0.53 \pm 0.05$ & $1.38 \pm 0.10$ & $0.72 \pm 0.00$ & $4.436 \pm 0.015$ \\
%  BL + RPO + CFG ($\gamma=2.5$) & $0.57 \pm 0.14$ & $1.45 \pm 0.19$ & $0.74 \pm 0.00$ & $4.427 \pm 0.017$ & $0.53 \pm 0.05$ & $1.38 \pm 0.10$ & $0.72 \pm 0.00$ & $4.436 \pm 0.015$ \\
% \bottomrule
% \end{tabular} 
% }
% \label{tab:dpo_rpo_cfg_base_decoder}
% \end{table*}

Starting from our base checkpoints, we perform DPO or RPO finetuning for a maximum of $4{,}000$ mini-batch iterations using a batch-size of $64$ pairs, optimized using Adam optimizer with a fixed learning rate (LR) \texttt{2e-7}. We set $\beta$\texttt{=0.01} and $\eta$\texttt{=1.0} as the default loss coeffecients in DPO and RPO. We choose the checkpoint with the lowest validation loss and evaluate the preference-aligned models for intelligibility, speaker similarity  and naturalness. Table~\ref{tab:dpo_rpo_cfg_base_multi} shows the results of these experiments on the \textit{multi encoder} model. Results of DPO/RPO across all architectures can be found in Appendix~\ref{sec:dporpoall}.
As evident, both DPO and RPO significantly improve intelligibility and speaker-similarity metrics over the base model across all architectures. Interestingly, preference alignment also significantly improves zero-shot speaker similarity on unseen speakers, even though we do not include any new speakers in the preference data creation. The naturalness metric Squim-MOS also shows an improvement over the baseline models, even though we don't explicitly include it in our reward system. 
This suggests that CER and SSIM metrics serve as good proxy for human preferences and can be automatically computed, thereby allowing easy scaling up of the preference alignment process.
% Our experiments with DPO and RPO (Appendix~\ref{sec:dporpoall}) show that both methods achieve comparable improvements over baseline, across all models. 
We find RPO to be less sensitive to hyper-parameter tuning and number of training iterations than DPO. 
RPO also works reliably when preference data does not have high-contrast chosen-rejected pairs, since it considers reward differences instead of a binary pair, in its optimization objective.

To compare with a prior technique proposed in SpeechAlign ~\cite{zhang2024speechalign}, we perform an ablation in which the ground-truth audios are selected as our chosen examples (as opposed to generated samples).  
We use a subset of \textit{train-clean-360} LibriTTS data as chosen examples, and the worst ranked of the $6$ generations (for an input) as the rejected example, creating a similar sized preference dataset as our other experiments.
We find that preference alignment algorithms find it trivial to differentiate ground-truth examples from generated ones with the preference loss reducing to nearly zero within the first few hundred iterations.
With our default DPO hyperparameters ($\beta$\texttt{=0.01}, LR\texttt{=2e-7}) such a setup leads to model degeneration and a very high CER (\texttt{>90\%}). 
Fine-tuning DPO hyperparameters ($\beta$\texttt{=1.0}, LR\texttt{=1e-7}) and early stopping, 
prevents model degeneration, but does not yield improvement over the baseline model (Table~\ref{tab:dpo_rpo_cfg_base_multi}). This suggests it is important to obtain chosen-rejected pairs from model's output distribution, for preference optimization such as DPO to work effectively.
% Early stopping of this experiment \vspace{-2mm} 

\subsection{Classifier Free Guidance}
By controlling the CFG scale $\gamma$ during inference, we can steer the generations to be better aligned with conditional inputs.
We vary $\gamma$ between $1$ to $3$ at $0.2$ intervals and show the results of this experiment on unseen speakers in Figure~\ref{figs:cfgplots}. 
Increasing $\gamma$ significantly reduces the CER and simultaneously increases SSIM across all models. 
From these observations, we set $\gamma$\texttt{=2.5} as the optimal value. 
% and note the improvements for the multi encoder model in Table~\ref{tab:dpo_rpo_cfg_base_multi}.

\vspace{-2mm} 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{diagrams/CFGPlots.pdf}
    \vspace{-8mm} 
    \caption{Effect of CFG scale on CER/SSIM for unseen speakers}
    \label{figs:cfgplots}
    % \vspace{-2mm} 
\end{figure}

While CFG adds an inference-time cost by doubling the effective batch-size, it does not require any additional finetuning of the baseline models to achieve the substantial improvements across all metrics. 
Additionally, CFG inference on a preference aligned model results in further improvements and yields the best results across all metrics.
Figure~\ref{figs:bar_graphs} presents these improvements on unseen speaker TTS task across all models. Table~\ref{tab:dpo_rpo_cfg_base_multi} shows the improvements obtained with CFG on both seen and unseen speakers for the multi encoder architecture, on the baseline and preference aligned model. 
The reduction in CER/WER confidence intervals indicates that we can generate accurate speech reliably.

\begin{figure*}[tp]
% \vspace{-2mm}
    \centering
    \includegraphics[width=0.97\textwidth]{diagrams/PrefAlignCFGBarCharts.pdf}
\vspace{-4mm}    
    \caption{Performance of Koel-TTS Architectures. \textbf{Left:} 
    Intelligibility (CER) and speaker similarity (SSIM) evaluations of base and preference-aligned (RPO) Koel-TTS architectures, with and without CFG, on zero-shot TTS. \textbf{Right:} Equivalent evaluations for a \textit{Decoder Context} multilingual TTS model across various languages and text tokenizers. Both CFG and preference alignment, independently and in combination (green), improve CER and SSIM over the base model (gray).
    }
\vspace{-4mm}
    \label{figs:bar_graphs}
\end{figure*}


\subsection{Multilingual TTS}
\label{sec:multilingualexperiment}
For multilingual TTS, we investigate six languages English, Spanish, German, French, Italian, and Dutch.
For non-English languages, we use the CML dataset~\cite{oliveira2023cml} that contains $1{,}562$ hours of German, $642$ hours of Dutch, $476$ hours of Spanish, $283$ hours of French, $131$ hours of Italian speech data. Additionally, we incorporate $42$ hours of internal Spanish data from two speakers. Combining this with our $18k$ hours of English TTS data, we create a final blend of $21k$ hours of multilingual TTS data. We use the decoder context architecture for this task, and scale-up our model to a $1.1$ billion parameter model (See Appendix~\ref{sec:trainingdetails} for details).
% , by increasing the hidden dimension from $768$ to $1536$ and increasing the number of decoder layers from $12$ to $16$. The model is trained for around $120k$ mini-batch iterations using the same training hyperparameters as our primary models. 

For preference alignment of the multilingual model, we create $10k$ text and context audio pairs per language (by pairing texts with a random context audio), from the CML training data of each language. We combine these pairs with $20k$ English text and context audio pairs randomly sampled from the $58k$ pairs used in our primary experiments. We utilize the \textit{whisper-large-v3}~\cite{radford2022whisper} ASR model in our reward system to create preference pairs and perform DPO finetuning with $\beta$\texttt{=0.05}.

Figure~\ref{figs:bar_graphs}(right) presents the results of multilingual-TTS evaluations on unseen speakers from each language. For evaluations on non-English languages, we use $100$
utterances per language from the CML test set of each language. For English evaluation, we use the same $180$ utterances from LibriTTS \textit{test-clean} subset as used in our primary experiments. 
As shown by the results, both preference alignment and CFG (with $\gamma$\texttt{=2.5}) yield substantial improvement in both intelligibility and speaker similarity metrics across various languages and tokenizers.
More interestingly, CFG inference on a DPO finetuned checkpoint, yields substantial speaker similarity improvements over using either DPO or CFG in isolation, especially for non-English languages.

We find that Koel-TTS can work effectively on raw character tokens, and achieve similar results as using phonetic inputs, for languages in which we consider both phoneme and character tokenizers (English, Spanish and German). 
Incorporating both CFG and DPO, our multilingual model achieves similar CER as the English-only decoder context model and slightly improves speaker similarity (0.740 vs. 0.726).
% With both CFG and DPO, our multilingual model achieves similar CER as our English-only model and slightly better speaker similarity (0.74 vs 0.72) than  our smaller, English-only decoder-context model. 
We present ablations with alternate multilingual tokenization schemes in Appendix~\ref{sec:multilingualablations}.

% Furthermore, we demonstrate multi-turn dialogue generation capabilities of 

\setlength{\tabcolsep}{2pt}
\begin{table}[ht]
\centering
\vspace{-4mm}
\caption{\footnotesize{Intelligibility, SSIM and naturalness evaluation of various zero-shot TTS models on a subset of \textit{test-clean} LibriTTS data.}}
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{l|ccc|cc}
\toprule
Model & CER (\%) & $\downarrow$ WER (\%) $\downarrow$ & SSIM $\uparrow$ & MOS $\uparrow$ & SMOS $\uparrow$\\
\midrule
Ground Truth & $0.80$ & $1.83$ & $0.771$ & $3.937 \pm 0.028$ & - \\
VALLE-X~\cite{zhang2023speak} & $6.65$ & $11.28$ & $0.679$ & $3.532 \pm 0.046$ & $3.709 \pm 0.045$ \\
YourTTS~\cite{casanova2022yourtts} & $2.44$ & $5.19$ & $0.581$ & $3.235 \pm 0.047$ & $3.229 \pm 0.053$ \\
T5-TTS~\cite{t5tts} & $1.66$ & $3.28$ & $0.459$ & $3.533 \pm 0.046$ & $3.366 \pm 0.050$\\
E2-TTS~\cite{eskimez2024e2} & $1.29$ & $2.66$ & $\mathbf{0.848}$ & $3.889 \pm 0.040$ & $3.793 \pm 0.045$\\
F5-TTS~\cite{chen2024f5} & $1.23$ & $2.55$ & $0.834$ & $3.930 \pm 0.042$ & $3.785 \pm 0.045$\\
XTTS-v2~\cite{casanova2024xtts} & $0.99$ & $2.09$ & $0.680$ & $3.715 \pm 0.043$ & $3.434 \pm 0.050$\\
StyleTTS-2~\cite{li2024styletts} & $0.75$ & $1.52$ & $0.579$ & $4.047 \pm 0.039$ & $3.786 \pm 0.044$ \\
\midrule
Koel-TTS 380m English & $\mathbf{0.55}$ & $\mathbf{1.41}$ & $0.726$ & $4.054 \pm 0.039$ & $3.826 \pm 0.044$\\
Koel-TTS 1.1b Multilingual & $0.63$ & $1.42$ & $0.740$ &  $\mathbf{4.058 \pm 0.040}$ & $\mathbf{3.848 \pm 0.043}$ \\
\bottomrule
\end{tabular}
}
\vspace{-4mm}
\label{tab:results_libri_unseen_test}
\end{table}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{diagrams/cmos_plot.pdf}
    \vspace{-8mm} 
    \caption{\footnotesize{Koel-TTS vs. Previous Models: Dark green bars indicate the percentage of instances where human listeners preferred Koel-TTS for audio naturalness during side-by-side evaluations.}}
    \label{figs:cmosplot}
     \vspace{-4mm} 
\end{figure}


\subsection{Comparison against Past Work}
\label{sec:pastworkcomparison}
We benchmark two candidate Koel-TTS models: English-only decoder context model and the larger multilingual decoder context model against past work and open source models. We evaluate all models for zero-shot TTS on the unseen speaker evaluation set (test-clean LibriTTS subset), using the same evaluation procedure as described in Section~\ref{sec:datasets}.
We also compute three human evaluation metrics on Amazon Mechanical Turk namely Naturalness Mean Opinion Score (MOS), Speaker similarity MOS (SMOS) and Comparative MOS (CMOS). For complete details on MOS studies, see Appendix~\ref{sec:humaneval}. 
As shown in Table~\ref{tab:results_libri_unseen_test}, Koel-TTS achieves state-of-the-art intelligibility scores (CER/WER) despite being trained on significantly less data than competing models. While Koel-TTS outperforms LLM-based baselines (VALLE-X and XTTS-v2) in SSIM scores, it slightly underperforms CFM-based systems (F5-TTS and E2-TTS), which leverage 100k+ hours of speech data, compared to $21k$ hours for our largest model. Human evaluations of naturalness (MOS) and speaker similarity (SMOS) 
% consistently favor Koel-TTS over other models, and 
show Koel-TTS to be equally or more preferred compared to all other models. We attribute the difference between SSIM scores and SMOS to SSIM’s emphasis on timbre similarity, whereas human ratings consider additional factors such as style and accent. CMOS results in Figure~\ref{figs:cmosplot}, further confirm that Koel-TTS is preferred over all competing approaches.
% \vspace{-2mm}

% \edresson{ToDo: Disscuss MOS and SMOS soon it is on the table. I think we should report CMOS in a appendix or somehow remove content from the paper to it fit here.}

% Table~\ref{tab:results_libri_unseen_test} presents the results of this evaluation.  While being trained on significantly less data than many other models, Koel-TTS outperforms all previous works in intelligibility (CER and WER),  while achieving speaker similarity better than all previous LLM-based TTS models (e.g. VALL-X, T5-TTS, XTTS-v2),  while producing results slightly worse than CFM-based TTS model (eg. T5-TTS, E2-TTS). In this way, our models helped to close the speaker similarity gap between CFM-based TTS models and LLM-based ones.

% \subsection{Multi-turn Dialogue for Podcast style TTS}
% Recently released technologies such as NotebookLM~\cite{notebooklm} and GenFM~\cite{genfm} showcase the ability to generate speech and multi-turn dialogues for multiple speakers, in a conversational, podcast-like format.
% Towards this goal, we fine-tune our Koel-TTS model to handle multi-turn dialogue generation, by conditioning the audio generation on the past conversation context. 
% The encoder processes input text with speaker tags to differentiate between speakers, while the decoder generates audio tokens corresponding to the dialogue. Our approach addresses the scarcity of real-world multi-turn dialogue datasets by leveraging synthetic datasets, simulating turn-switching behavior between speakers. 
% During inference, text-based dialogue generation models provide contextual input, where the audio tokens of the last few seconds of generated dialogue are used as conditioning context for subsequent turns, enabling coherent and dynamic multi-speaker conversations.

% \subsection{Multilingual Dataset}
% % Using our best recipe with Attention Prior, $L_{\textit{align}}$, and the FSQ codec, 
% We further experiment on multi-lingual data by training the model on two data blends. The first blend is a proprietary dataset that contains 170 hours of audio with 8 speakers. The second data blend contains $3k$ hours of French, Italian, Spanish, Dutch, Polish and German subsets of CML-TTS~\cite{oliveira2023cml}, XX hours of Riva spanish data.
% the Thorsten Voice datasets \cite{muller_2021_5525342}, 
% the Mandarin subset of CSS10 \cite{park2019css10}, and Magicdata \cite{magicdata2019}. 

