Traditionally, TTS synthesis has been handled as a cascaded process involving an intermediate mel-spectrogram representation~\cite{tacotron,lancucki2021fastpitch,onealign,neekhara2021expressive,hussain2023ace}.
Recently, discrete neural audio codecs have emerged as an effective alternative, compressing raw audio into token-based representations that preserve fidelity~\cite{encodec,dac_kumar2024high,zeghidour2021soundstream,langman2024spectral,casanova2024low}. 
This shift has enabled autoregressive (AR)~\cite{wang2023neural,zhang2023speak,chen2024vall,borsos2023audiolm,yanguniaudio,wang2024speechx} and non-autoregressive (NAR)~\cite{langman2024spectral} Transformer-based models, trained on discretized audio tokens, to synthesize speech with improved naturalness. 
% and coherence while also facilitating large language model (LLM) integration for zero-shot learning, multilingual synthesis, and cross-modal audio generation.
% Inspired by the success of LLMs, framing TTS as a next-token prediction problem has gained significant traction in recent years, further enhancing the potential of token-based approaches.
% Several approaches have proposed training decoder-only LLMs on discretized audio tokens produced by neural codec models to achieve high-quality speech synthesis. Notably, models such as AudioLM~\cite{borsos2023audiolm}, VALL-E~\cite{wang2023neural,zhang2023speak,chen2024vall}, UniAudio~\cite{yanguniaudio}, Bark~\cite{bark2023}, SpeechX~\cite{wang2024speechx}, SpeechAlign~\cite{zhang2024speechalign} and XTTS~\cite{casanova2024xtts}  frame the audio generation task as an autoregressive language modeling problem, leveraging single or multiple discrete codebooks. 

% Particularly, framing TTS as a next-token prediction task has gained significant traction with decoder-only LLMs trained on discretized audio tokens to achieve high-quality speech synthesis~\cite{borsos2023audiolm,wang2023neural,zhang2023speak,chen2024vall,yanguniaudio,bark2023,wang2024speechx,zhang2024speechalign,casanova2024xtts}.

However, autoregressive token-based TTS models often struggle with hallucinations, where the generated speech deviates from intended input. 
While one way to partially address this issue is biasing text and speech alignment to follow a monotonic pattern~\cite{t5tts,kim2020glow}, 
it is non-trivial to construct such attention priors for inputs like reference speaker audio. 
This underscores the need for further refinement to better align model outputs with human expectations and preferences.

% \shehzeen{T5TTS to be added}
% Despite their success, these models often face challenges related to hallucinations and robustness. Hallucinations, where models generate audio that deviates from the intended input or fails to preserve semantic and prosodic accuracy, can undermine the reliability of synthesized speech. 

% \edresson{I think it is interesting to add T5-TTS previous works here. And Maybe also works that uses speech tokenizers that are not codecs (e.g Vq-VAE), For example XTTS and CosyVoice2, what do you guys think? These models uses a encoder that do not retain the full speech information. We could add it as disadvantage.}
% \edresson {I added Cosyvoice2 as a decoder only example as well. Cosyvoice2 uses an LLM based on Qwen2, so it is a decoder only. I have also improved the last sentence. Please check it and feel free to change if needed}

Recent research employs preference alignment algorithms, including RLHF~\cite{ouyang2022rlhf} and offline preference ranking methods~\cite{rafailov2024direct,azar2024ipo}, to refine audio LLM outputs.
% Recent research leverages preference alignment to improve speech quality in LLM-based TTS, aligning output more closely with user preferences. Widely used techniques, such as reinforcement learning with human feedback (RLHF)~\cite{} and preference ranking~\cite{}, have been applied to optimize model outputs, enhancing naturalness, coherence, and user satisfaction.
For example, SpeechAlign~\cite{zhang2024speechalign}, proposes an iterative strategy to align speech language models with human preferences by addressing the distribution gap between golden AR tokens (from real speech) and synthetic AR tokens (generated during inference). 
Although real speech from ground truth can be used to guide training, it introduces inconsistencies due to its fundamentally different distribution from model-generated tokens, making preference-based optimization such as DPO less effective~\cite{rafailov2024direct}. 
% MusicRL~\cite{} is another work that uses automatic metrics and human feedback to reward generated output for preference alignment and shows an improvement over the baseline music generation model with preference alignment. 
In our work, we propose a reward mechanism based on speaker verification and speech recognition models to guide the model to generate accurate speaker-informed speech. 

CFG has also emerged as a promising method to amplify the influence of conditioning inputs on the generated output, particularly in diffusion-based audio generation models~\cite{chen2024f5,liu2023audioldm}.
In our work, we show that autoregressive LLMs also benefit from CFG when trained with both conditional and unconditional inputs. Additionally, our experiments demonstrate that combining CFG with preference alignment can further improve performance over either technique in isolation.


% While CFG has been popularly used in diffusion based models, it's applicability in autoregressive conditional LLMs is under explored. 
% In our work, we find that both CFG and preference alignment can independently yield substantial improvements, 

% In our work, we propose 
% Therefore, SpeechAlign still requires human verification as a post-processing step, to verify that the constructed preference dataset aligns with perceptible speech quality differences.

% Our methodology extends the use of preference-based learning to a broader range of challenging inputs, such as complex transcripts and diverse speaker prompts, thereby enhancing robustness and generalization in TTS models.

% By leveraging XXX as preferred data and synthetic AR tokens as dis-preferred data, this work avoids the need for direct human annotation of codec tokens, which are numerically represented and challenging to evaluate. 

\begin{figure*}[tp]
% \vspace{-8mm}
    \centering
    \includegraphics[width=0.9\textwidth]{diagrams/ICML_Model_Diagram.pdf}
\vspace{-2mm}    
\caption{
Koel-TTS Model Architectures: We investigate three methods for conditioning TTS synthesis on context audio and transcripts. The decoder context approach utilizes the decoder's self-attention mechanism for speaker conditioning, while the multi encoder and SV conditioned models employ cross-attention layers for speaker conditioning.
}
\vspace{-4mm}
    \label{figs:model_overview}
\end{figure*}
