




% The advancement of large language models (LLMs) has brought transformative improvements to text-to-speech (TTS) synthesis, enabling more natural and contextually adaptive speech generation. For LLM-based audio generation, audio is quantized into discrete tokens, which allows the formulation of audio synthesis as a language modeling task.

The advancement of large language models (LLMs) has brought transformative improvements to speech synthesis, enabling more natural and contextually adaptive speech generation. In particular, there has been a recent surge in the use of LLMs for various applications such as text-to-speech (TTS) and speech-to-speech translation~\cite{wang2023neural,zhang2023speak,borsos2023audiolm,t5tts,yanguniaudio,wang2024speechx}.  LLM-based TTS systems enable prompt-based customization, generating speech with human-like intonation while adapting to stylistic cues, contexts, and expressive nuances. This allows for diverse applications, from conversational interfaces to expressive narration, without extensive retraining. These advancements have been largely driven by the emergence of discrete neural audio codecs, which compress raw audio into token-based representations while preserving high fidelity~\cite{encodec,dac_kumar2024high,zeghidour2021soundstream,langman2024spectral,casanova2024low}.


Despite these advances, LLM-based TTS systems face challenges, with hallucinations being a prominent issue~\cite{sahoo2024comprehensive,song2024ella,t5tts,borsos2023audiolm}.
For example, when encountering text with repeated or redundant phrases, LLM-based TTS models may overemphasize these repetitions or fail to capture the intended flow and naturalness of the sentence. Additionally, among the multiple outputs sampled for the same input, there can be significant variation in quality, with some outputs sounding more natural, accurate, and appealing than others. This issue is akin to challenges faced in text-generation LLMs, where outputs may range from highly coherent to erroneous, depending on the model's response to complex prompts.
%While one way to partially address this issue is biasing text and speech alignment to follow a monotonic pattern during training~\cite{t5tts} or inference~\cite{}, it is non-trivial to construct such attention priors for inputs like reference speaker audio.  This underscores the need for further refinement to better align model outputs with human expectations and preferences.

For text-generation, preference alignment techniques~\cite{christiano2017deep,ouyang2022rlhf,shao2024deepseekmath,rafailov2024direct, adler2024nemotron} have been proposed to guide models to produce outputs that better match human preferences in coherence, relevance, and clarity. This is achieved through training with human feedback or automated scoring, based on criteria such as factual correctness and fluency. Driven by these advances, recent research employs preference alignment algorithms, including RLHF~\cite{ouyang2022rlhf} and offline preference ranking methods~\cite{rafailov2024direct,azar2024ipo}, to refine audio LLM outputs. For instance, SpeechAlign~\cite{zhang2024speechalign}, proposes an iterative strategy to align speech language models with human preferences by addressing the distribution gap between golden AR tokens (from real speech) and synthetic AR tokens (generated during inference).  Although real speech from ground truth can be used to guide training, we will show that it introduces inconsistencies due to its fundamentally different distribution from model-generated tokens. This issue makes preference-based optimization such as DPO~\cite{rafailov2024direct} less effective. 
Nonetheless, this approach has been applied in scenarios where obtaining high-quality positive examples is particularly challenging \cite{chen2024self,zhang2024speechalign}. 

Another research direction to amplify the influence of conditioning inputs in generative models is Classifier-Free Guidance (CFG). 
% has also emerged as a powerful technique to enhance the quality and controllability of generative models, particularly in the domain of diffusion models~\cite{chen2024f5,liu2023audioldm}. 
CFG was originally introduced to trade-off sample fidelity and diversity without relying on a separate classifier in diffusion models~\cite{ho2021classifier}.  
Recently, CFG has been successfully explored in LLM-based text-generation models \cite{sanchez2023stay, fonseca2024can, smirnov2024classifier}. In the context of text-to-speech synthesis, CFG has been extended to improve non-autoregressive flow-matching models (CFM)~\cite{le2024voicebox, du2024cosyvoice, chen2024f5, eskimez2024e2}. However, the applicability of CFG for enhancing LLM-based speech token prediction models is underexplored, with only a few attempts at improving textual coherence~\cite{darefsky2024parakeet}.
% it has also been adapted to improve autoregressive models based on next token prediction~\cite{darefsky2024parakeet}.

Building upon the above insights, 
we propose preference alignment and CFG techniques to enhance contextual coherence of LLM-based TTS models.
% to improve contextual coherence for LLM-based TTS models.
% for LLM based speech synthesis. 
We introduce Koel-TTS, a transformer-based TTS model that leverages a low-frame-rate ($21.5$ FPS) audio codec~\cite{casanova2024low} to enable low-latency autoregressive speech generation. 
% To further enhance synthesis quality, reduce hallucinations and and improve speaker similarity, we adapt preference alignment and CFG specifically for speech synthesis.
% Despite these advancements, the combined potential of preference alignment and classifier-free guidance remains unexplored in LLM-based speech synthesis, and existing implementations of these techniques lack proper adaptation for the task. 
To perform preference alignment, we first identify key metrics that strongly correlate with human judgments of generated speech: transcription accuracy and target speaker similarity. Each metric captures distinct aspects of the generated output and can be evaluated using automatic speech recognition (ASR) and speaker verification (SV) models. We integrate these metrics into a reward system that ranks the generated outputs. 
With this foundation, we then explore preference alignment algorithms, focusing on pairwise ranking methods and scalar reward optimization. 
Our findings show that fine-tuning the base model with preference alignment significantly improves speaker similarity, intelligibility, and generalization to unseen speakers. Notably, our method also enhances naturalness, despite not explicitly optimizing for this metric.

To further enhance synthesis quality with CFG, we train the Koel-TTS model with both conditional and unconditional inputs, by randomly dropping out conditional information (text and context audio) during training.
During inference, the unconditional logits are combined with conditional logits using a CFG scale, to achieve significant improvement in intelligibility, speaker similarity, and naturalness of generated speech. Furthermore CFG can be applied independently to the base model or the preference aligned model, yielding substantial improvements across all
evaluation metrics for both. Combining preference alignment with CFG, we train a 1.1 billion parameter multilingual Koel-TTS model that
achieves state-of-the-art zero-shot TTS results across several human and automatic evaluation metrics.


The key contributions of this work are as follows:
\begin{itemize}
\vspace{-0.3cm}
\item We introduce Koel-TTS, a multilingual encoder-decoder transformer model that maps text and context audio directly to acoustic tokens using a low-frame-rate audio codec, enabling expressive, robust, and low-latency autoregressive speech synthesis.
\vspace{-0.2cm}
\item We propose a novel preference alignment framework for LLM-based TTS by leveraging ASR and SV models as reward signals, significantly improving speaker similarity, intelligibility, and generalization to seen and unseen speakers.
\vspace{-0.2cm}
\item We adapt classifier-free guidance, dropping both text and context conditioning, to enhance LLM-based speech synthesis, demonstrating its effectiveness in improving naturalness, speaker similarity, and intelligibility.
% \vspace{-0.2cm}
% \item We show that combining preference alignment and CFG enhances synthesis quality, reduces hallucinations, and improves intelligibility while improving zero-shot performance.
\vspace{-0.2cm}
\item Our Koel-TTS model, trained with preference alignment and CFG, achieves state-of-the-art zero-shot TTS performance while reducing hallucinations and improving intelligibility. Our model implementation is publicly available in the Koel-TTS repository\footnote{Omitted from blind review}.
\vspace{-0.2cm}
% \item Our model implementation is publicly available in the NeMo repository\footnote{https://github.com/NVIDIA/NeMo}.
% \item Our model implementation is publicly available in the Koel-TTS repository\footnote{Omitted from blind review}.

\end{itemize}


% Despite these advancements, the combined potential of preference alignment and classifier-free guidance was not explored in LLM-based speech synthesis. In addition, there is potential for improvement with the proper adaptation of these techniques for the speech synthesis domain. In this paper, we introduce the Koel-TTS model, an LLM-based speech synthesis model that leverages a low-frame-rate ($21.5$ frames per second) audio codec model~\cite{casanova2024low}, enabling low-latency autoregressive speech generation.  To further improve the model, reducing hallucinations and inconsistencies, we adapted preference alignment and classifier-free guidance for the speech synthesis domain. 

% For adapt preference alignment, we begin by identifying automatic metrics that strongly correlate with human judgments of generated speech. 
% Two key metrics---transcription accuracy and target speaker similarity---each capture distinct aspects of the output and can be evaluated using automatic speech recognition (ASR) and speaker verification (SV) models. We integrate these metrics into a reward system that ranks the generated outputs. With this foundation, we then explore preference alignment algorithms, focusing on pairwise ranking methods and scalar reward approaches.  We find that finetuning our base models with preference alignment algorithms~\cite{rafailov2024direct,adler2024nemotron}, significantly improves speaker similarity and intelligibility metrics across all models.  Interestingly, we also observed an improvement in TTS metrics for speakers that are not used in our base model training or preference dataset. Moreover, naturalness also improves under our method, even without direct optimization for this metric in the preference alignment procedure. For adapt classifier-free guidance (CFG), we demonstrate that dropping out conditional input during inference to obtain unconditional logits and then combining these with conditional logits using a CFG scale significantly enhances the intelligibility, speaker similarity, and naturalness of generated speech. Furthermore, CFG can be independently applied to either the base model or the preference-aligned model, yielding substantial improvements in both scenarios.  Combining CFG and preference alignment,  our Koel-TTS model achieves SOTA performance on several zero-shot TTS metrics, as compared to past works, excelling in both human and automatic evaluations. 


% The key contributions of this work are as follows:
% \begin{itemize}
%  \vspace{-0.05cm}
%  \item We introduce Koel-TTS, a multilingual encoder-decoder Transformer model that directly maps text and context audio to acoustic tokens using a low-frame-rate audio codec, enabling expressive, robust, and fast autoregressive speech synthesis.
% \item We introduce a novel preference alignment framework for LLM-based TTS by leveraging automatic speech recognition and speaker verification models as reward signals. This approach significantly improves speaker similarity, intelligibility, and generalization to unseen speakers.
% \item We adapted CFG, previously used in diffusion models, to enhance LLM-based speech synthesis. 
% \item We combined preference alignment and CFG yield improvements in naturalness, intelligibility, and speaker similarity.
% \item  Our model, trained with CFG and preference alignment, achieves state-of-the-art results, excelling in human evaluation.


% \item 
% \end{itemize}



\begin{figure*}[tp]
% \vspace{-8mm}
    \centering
    \includegraphics[width=0.9\textwidth]{diagrams/ICML_Model_Diagram.pdf}
\vspace{-2mm}    
\caption{
Koel-TTS Model Architectures: Three methods for conditioning TTS synthesis on context audio and transcripts. The \textit{Decoder Context} approach utilizes the decoder's self-attention mechanism for speaker conditioning. The \textit{Multi Encoder} and \textit{SV Conditioned} models employ cross-attention layers for speaker conditioning.
}
\vspace{-4mm}
    \label{figs:model_overview}
\end{figure*}
