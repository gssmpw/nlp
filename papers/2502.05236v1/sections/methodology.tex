
Our proposed framework is an autoregressive speech token generation model that is conditioned on a text input and a context audio.
In this section, 
we first
describe the tokenization scheme employed for representing speech and text. Next, we detail the model architecture and describe three model designs we explore for context-conditioning.
Finally, we propose two key techniques to improve the robustness and speaker similarity of our model using preference optimization algorithms and classifier free guidance.

\subsection{Tokenization}
\textbf{Speech:} We employ a neural audio codec model to transform raw speech signals into tokenized representations. For a given audio signal $\mathbf{a}$, the codec model outputs a two-dimensional acoustic matrix $\mathbf{C}_{T \times N}=\textit{CodecModel}(\mathbf{a})$. In this representation, $\mathbf{C}_{T \times N}$ consists of $m$-bit discrete codes, where $T$ corresponds to the downsampled sequence length, and $N$ represents the number of codebooks per timestep. We utilize the Low Frame-rate Speech Codec \cite{casanova2024low}, which achieves high-quality audio compression at a bitrate of $1.89$ kbps and a frame rate of $21.5$ frames per second, utilizing $N$\texttt{=8} independent codebooks.  
The codec uses  Finite Scalar Quantization (FSQ) \cite{mentzer2024finite}, which ensures independence among the codebooks. 
This independence eliminates the need for additional models or delay mechanisms, enabling the parallel prediction of all $N$ codebooks at each timestep.

\textbf{Text:} 
We explore two text tokenization methods: phonemes and characters. Phonemes, commonly used in neural TTS, capture fundamental sound units but require language-specific grapheme-to-phoneme (G2P) conversion. In contrast, character-based tokenization eliminates this need, enabling direct conversion of graphemes to acoustic information. In our experiments, we use IPA phonemes and character tokenizers for English, German, and Spanish, while applying only character tokenizers for other languages.
% We explore two text tokenization methods: phonemes and characters (bytes). Phoneme sequences are widely used as inputs for neural TTS models because they capture the fundamental sound units of a language. However, generating accurate phoneme sequences requires language-specific grapheme-to-phoneme (G2P) processors. In contrast, using raw bytes eliminates the need for such specialized text processing, allowing the TTS model to directly convert graphemes into acoustic information.
% For our experiments, during training, we apply both IPA phonemes and character tokenizers to English, German, and Spanish, while employing only character tokenizers for other languages. 
We utilize an aggregated tokenizer that maintains separate token embeddings for each language. Additionally, we perform an ablation study with a shared character tokenizer and a multilingual sentencepiece tokenizer across all languages, with results detailed in Appendix~\ref{sec:multilingualablations}.

% \textbf{Text:} For text, we use two tokenization schemes: sentencepiece \cite{kudo2018sentencepiece} and phonemes. Sentence-piece tokens allows us to leverage pretrained text LLMs. To allow phoneme tokens as input, we expand the vocabulary and embedding space of the pretrained text-LLM to include additional tokens for phonemes. We train a single model to perform both phoneme to speech and sentence-piece to speech by prepending the text with the task prompt “Phoneme TTS” or “Text to Speech” respectively.


\subsection{Model Overview}

Our speech generation model comprises an autoregressive (AR) transformer decoder conditioned on text encodings from a non-autoregressive (NAR) transformer encoder, using cross-attention layers. 
The AR transformer predicts audio tokens frame by frame, generating all $N$ codebooks in parallel at each time step, conditioned on previous generations and inputs to the cross-attention layers.
% For context audio conditioning, which influences the speaker and style of the generated audio, we explore three model designs as follows:
To influence the speaker and style of the generated audio through context audio conditioning, we explore three model designs as illustrated in Figure~\ref{figs:model_overview},

\textbf{SV Conditioned Koel-TTS:} In this configuration, a speaker embedding vector is extracted from the context audio using a pre-trained SV model~\cite{koluguri2022titanetlarge}. This embedding vector is projected to hidden dimension of the transformer network, temporally expanded (repeated across the time axis) and added to the text encoder's output. 
The resulting combined representation serves as the input to the cross-attention layers of the AR decoder, enabling the prediction of audio codes while conditioning on the speaker identity. 
The advantage of this design is the ability to leverage transfer learning from the SV model, thereby enhancing generalization in scenarios with limited data. 
However, since the speaker vector is a compressed representation that primarily preserves voice identity, it does not capture other nuanced aspects of the context audio, such as speaking style and accent, which limits control over the generated speech.

\textbf{Decoder Context Koel-TTS:} Here, the context audio tokens are directly provided as input to the AR decoder by prepending them to the target audio tokens. This approach eliminates the need for a separate audio encoder to process context audio. Instead, a single unified transformer decoder processes both the context and target audio tokens, leveraging a shared representation for conditioning and prediction.

\textbf{Multi encoder Koel-TTS:} In this architecture, context audio tokens are processed by a dedicated context encoder, which is a separate NAR transformer encoder. The outputs of the context encoder and text encoder are fed into alternate cross-attention layers of the AR decoder, as illustrated in Figure~\ref{figs:model_overview}c. This design allows for a clear separation of modalities, where each encoder operates independently, and the decoder employs dedicated cross-attention mechanisms to integrate the outputs.
This model also allows cross-attention biasing over the text tokens independently for learning monotonic alignment, while allowing variable length context audios.
% The separation also facilitates modality-specific feature learning.
% and potentially improves the conditioning capacity for both speaker and style information.


\subsection{Training Objective}
The output of each decoder-timestep is mapped to a vector of size $N \times 2^m$ using a linear layer to obtain the logits of all $N$ codebooks (each of size $m$-bits) at that timestep. Thereby, for all decoder time-steps, we obtain logits $\ell$ of size $T \times N \times 2^m$ and calculate cross-entropy as follows:
\vspace{-2mm}
\begin{align*}
\mathcal{L}_\textit{token} = \textit{CE}\left(\textit{softmax}\left(\ell\right), \textit{target}_{N\times T}\right)
\end{align*}
In addition to the above, to improve text and speech alignment, past work~\cite{t5tts} recommends biasing the cross-attention scores between the transcript encoder and AR decoder to be monotonic, using an attention prior and Connectionist Temporal Classification (CTC) loss. Specifically, given the cross-attention score matrix $\mathbf{A}_{T\times M}^{\textit{l,h}}$,  of the $h^{\textit{th}}$ cross-attention head in decoder layer \textit{l}, between the audio timesteps ($T$) and text timesteps ($M$), we generate a static prior using the 2D beta-binomial distribution $\mathbf{P}_{T \times M}$.
Given this prior, we obtain the re-scaled attention scores as: 
$$\mathbf{A}_{T\times M}^{\textit{l,h}} \gets \mathbf{A}_{T\times M}^{\textit{l,h}} \odot  \mathbf{P}_{T \times M}$$ 
The attention prior is applied for the first $10{,}000$ training iterations and then linearly annealed to a uniform distribution (all ones) for the next $5{,}000$ iterations and turned off thereafter. Turning off the prior is necessary since we cannot use this prior during inference\footnote{The final audio sequence length is unknown during inference.} and annealing ensures stability during training.

Additionally, to encourage valid monotonic sampling from the alignment matrix, we calculate likelihood of all possible monotonic reductions using the CTC algorithm. That is, given the alignment matrix $\mathbf{A}^{\textit{soft}_{l,h}}_{T\times M}=\textit{softmax}(\mathbf{A}_{T\times M}^{\textit{l,h}})$, we obtain the alignment loss for a decoder layer and head as: $$\mathcal{L}_{\textit{align}}^{l,h} = \textit{CTCLoss}\left(\mathbf{A}^{\textit{soft}_{l,h}}_{T\times M}, q_{M}\right)$$
where $q_{M}=\{1, 2, \dots M\}$ is the target monotonic sequence from $1$ to $M$. The alignment loss is summed across all cross-attention heads and layers to obtain $\mathcal{L}_{\textit{align}} = \sum_{\substack{l,h}} \mathcal{L}_{\textit{align}}^{l,h}$.
The final training loss is obtained as $\mathcal{L} = \mathcal{L}_\textit{token} + \alpha \mathcal{L}_{\textit{align}}$, where $\alpha$ is the alignment loss coefficient.


\begin{figure*}[tp]
% \vspace{-8mm}
    \centering
    \includegraphics[width=1.0\textwidth]{diagrams/icml_dpo_diagram.pdf}
\vspace{-8mm}    
\caption{Preference Alignment for Koel-TTS: Koel-TTS generates multiple outputs for challenging text and context audio prompts, which are then rewarded using ASR and SV Models to create a preference dataset for DPO and RPO.}
\vspace{-4mm}
\label{figs:dpo_diag}
\end{figure*}

\subsection{Preference Alignment}\label{sec:pref_align}


We employ preference optimization methods to steer the outputs of Koel-TTS towards more desirable results. 
% The underlying idea is that, 
For a given text and context audio input $x=(x_\textit{text}, x_\textit{audio})$, the model's response distribution $\pi(y|x)$ encompasses a range of potential outputs $y$ with varying levels of alignment to the desired criteria. By constructing a dataset that explicitly labels certain responses $y_c$ as chosen and others $y_l$ as rejected, we can leverage preference-based optimization algorithms to shift the model's distribution toward producing more preferred responses.

One such approach is Direct Preference Optimization (DPO)~\cite{rafailov2024direct}. DPO uses preference comparisons to modify the policy $\pi$ by contrasting it against a reference policy $\pi_{\text{ref}}$. Specifically, given an input $x$ and a chosen response $y_c$ that is preferred over a rejected response $y_l$, DPO seeks to increase the likelihood ratio $\frac{\pi(y_c|x)}{\pi_{\text{ref}}(y_c|x)}$ relative to $\frac{\pi(y_l|x)}{\pi_{\text{ref}}(y_l|x)}$. The core objective can be expressed as:
\vspace{-2mm}
\begin{align*}
\mathcal{L}_{\text{DPO}} = \mathbb{E}_{x, y_c, y_l}\left[ \beta \log\frac{\pi(y_c|x)}{\pi_{\text{ref}}(y_c|x)} - \beta \log\frac{\pi(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right].
\end{align*}

where $\beta$ is a parameter for controlling the deviation from the base reference policy $\pi_{\text{ref}}$. The above formulation, encourages $\pi$ to produce responses more similar to $y_c$ than $y_l$, effectively aligning the model with the desired preferences.

Building upon DPO, we also leverage Reward-aware Preference Optimization (RPO)~\cite{adler2024nemotron}, which considers the magnitude of reward differences in the optimization process.
Rather than treating all chosen versus rejected distinctions as equal, RPO utilizes scalar rewards to measure how much better the chosen response is compared to the rejected one. The RPO objective introduces a factor that scales the preference updates based on the reward gap $r^*(x,y_c) - r^*(x,y_l)$ as follows:

\vspace{-4mm}
\begin{align*}
\mathcal{L}_{\text{RPO}}(x, y_c, y_l) 
= \mathbb{D} \Bigg[ 
    & \beta \log \frac{\pi(y_c \mid x)}{\pi_{\text{ref}}(y_c \mid x)} 
    - \beta \log \frac{\pi(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \\
    & \;\Big\|\; 
    \eta \big(r^\ast(x, y_c) - r^\ast(x, y_l)\big)
\Bigg],
\end{align*}

where \(\eta\) is a scaling parameter and $\mathbb{D}$ is a distance metric given by 
$\mathbb{D}\left[a\|b\right] := \sigma(b) \log \frac{\sigma(b)}{\sigma(a)} + (1-\sigma(b)) \log \frac{1-\sigma(b)}{1-\sigma(a)}$
Thereby, RPO mitigates overfitting to narrowly better responses since the loss value is scaled as per the reward difference.
% , such that the policy does not completely discard nearly good options but instead proportionally adjusts to the quality gap indicated by the reward model.

\textbf{Preference Data Creation and Reward System:}
To construct the preference dataset $(x, y_c, y_l)$, we begin by selecting a diverse set of text and speaker prompt combinations that challenge the model's ability to produce accurate and natural speech. The text data includes a mix of regular sentences from standard TTS datasets, and carefully curated challenging transcripts generated by prompting text LLMs. These challenging texts are designed to test the model's robustness and include elements such as repeated words, numbers, and phonetically complex sequences. The inclusion of standard texts ensures generalizability, while the challenging examples target specific weaknesses of the TTS model as illustrated in Figure~\ref{figs:dpo_diag}. 

For each text and speaker prompt, we generate $P$ audio samples using multinomial top-k sampling ($k$\texttt{=80}, temperature=\texttt{0.7}). 
Each generation is evaluated using the Parakeet TDT 1.1B ASR~\cite{xu2023efficient} and Titanet-large SV~\cite{koluguri2022titanet} models. Specifically, we obtain the character error rate (CER) between the transcript of the generated audio and input text using the ASR model, and cosine similarity (SSIM) between the embeddings of the context audio and the generated audio obtained from the SV model. 
Based on the CER and SSIM, we perform Pareto optimal ranking~\cite{deb2011multi} on the set of $P$ generated audio samples for a given input pair---First, we identify the Pareto front, which consists of all audio samples that are not dominated by any other sample. That is, no other audio is strictly better on at least one metric and equally good or better on the other. Once we identify the first Pareto front, we remove those samples and repeat the process on the remaining audios to find the next front, and so on.
Within each Pareto front, we prioritize samples by assigning higher ranks to those with lower CER scores. If there are ties based on CER, we further differentiate by favoring samples with higher SSIM values. We detail this procedure in Appendix~\ref{paretorankingalgo}.

After ranking the examples, we select the highest-ranked as chosen and the lowest-ranked as rejected for DPO, since we empirically find high-contrast pairs to be beneficial for DPO. For RPO, which handles scalar reward differences, we pair the top two with the bottom two in all combinations. In both cases, we discard pairs where the chosen example scores worse on any metric (CER or SSIM) than the rejected one.
% Once we obtain the ranked examples, we choose the top-ranked example as chosen and last-ranked example as rejected for DPO, since we empirically found such high-contrast pairs to be better suited for DPO. For RPO, that can handle scalar reward differences, we pair the top $2$ examples with the last $2$ examples, considering all combinations.For both DPO and RPO, we filter out all pairs in which the chosen example is worse on any metric (CER or SSIM) than the rejected example.

To assign scalar rewards for RPO, we normalize the CER and SSIM differences between the chosen and rejected examples, and set the reward gap as: 
$$r^*(x,y_c) - r^*(x,y_l) = \Phi (\tilde{\Delta \text{CER}}) + \Phi (\tilde{\Delta \text{SSIM}})$$
where $\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution, and $\tilde{\Delta \text{CER}}$ and $\tilde{\Delta \text{SSIM}}$ are the normalized differences of CER and SSIM respectively, between the chosen and rejected examples.


% Specifically, we consider $800$ challenging texts and pair each text with $10$ speaker prompts sampled from our TTS training dataset. Next, we create an equal number of speaker prompt and text pairs where the text is sampled from $train-clean-360$ subset of LibriTTS.



\subsection{Classifier Free Guidance}\label{sec:cfg}
\input{sections/cfg}