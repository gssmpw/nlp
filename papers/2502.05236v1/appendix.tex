\section{Pareto optimal ranking for creating preference pairs}
\label{paretorankingalgo}
% Ranking of Pareto-optimal solutions and selecting the best solution in multi- and many-objective optimization problems using R-method
Pareto optimal ranking is a technique for multi-attribute decision making~\cite{deb2011multi}. The key idea is to find non-dominated solutions and removing them from the current set recursively till we have ranked all items. When we find multiple items in the same pareto front, we break the ties by prioritizing our preference for more robust examples (lower CER), and we break any remaining ties by preferring higher SSIM. Below is the python code for ranking for this procedure.
% In our case, we optimize preference alignment for two objectives - reducing CER (from an ASR model) and increasing SSIM (from a speaker-verification model). 

\begin{lstlisting}[style=mypython, caption=Pareto Optimal Ranking of generated outputs for a given text-context pair using CER and SSIM metrics]
def pareto_ranking(items):
    """
    Given a list of (cer, ssim, item_idx), return the list of items
    sorted by their Pareto rank (rank 1 is best). Items in the same
    rank are sorted by ascending cer and incase of a tie, by descending ssim.
    
    :param items: List of tuples (cer, ssim, item_idx).
    :return: A list of tuples (rank, cer, ssim, item_idx), sorted first by rank,
             then by ascending cer within the same rank.
    """
    
    # A helper function to check if item A is dominated by item B
    # A: (cerA, ssimA), B: (cerB, ssimB)
    def is_dominated(A, B):
        return (B[0] <= A[0]) and (B[1] >= A[1]) and (B != A)
    
    remaining = items[:]
    
    ranked_items = []  # Will hold tuples of (rank, cer, ssim, item_idx)
    current_rank = 1
    
    while remaining:
        # Find all non-dominated items in the current set 'remaining'
        non_dominated = []
        for i in range(len(remaining)):
            dominated = False
            for j in range(len(remaining)):
                if i != j:
                    if is_dominated(remaining[i], remaining[j]):
                        dominated = True
                        break
            if not dominated:
                non_dominated.append(remaining[i])
        
        # Assign current_rank to all non-dominated items
        # and remove them from remaining
        for nd in non_dominated:
            ranked_items.append((current_rank, nd[0], nd[1], nd[2]))
            remaining.remove(nd)
        
        current_rank += 1
    
    # Now sort the ranked items by (rank asc, cer asc, ssim desc)
    ranked_items.sort(key=lambda x: (x[0], x[1], -x[2]))
    
    return ranked_items
\end{lstlisting}


\setlength{\tabcolsep}{4pt}
\begin{table*}[t]
\vspace{-2mm}
\caption{\footnotesize{Evaluation of DPO, RPO and CFG on baseline models for all Koel-TTS architectures. We consider two DPO experiments with $\beta=(0.01,0.05)$. 
%Across all architectures, we see improvements in CER, WER and  
}}
% Across all architectures, we see improvements in CER, WER and
\centering
\resizebox{\textwidth}{!}{%
% \footnotesize
\begin{tabular}{l|cccc|cccc}
\multicolumn{1}{c}{} & \multicolumn{4}{c}{\emph{Seen Speakers}} & \multicolumn{4}{c}{\emph{Unseen Speakers}} \\
\toprule
Model/Technique & CER(\%) $\downarrow$ & WER(\%) $\downarrow$ & SSIM $\uparrow$ & Squim-MOS $\uparrow$ & CER(\%) $\downarrow$ & WER(\%) $\downarrow$ & SSIM $\uparrow$ & Squim-MOS $\uparrow$ \\
\midrule
Multi Encoder (BL-1) & $1.92 \pm 0.68$ & $3.02 \pm 0.76$ & $0.712 \pm 0.002$ & $4.346 \pm 0.028$ & $2.56 \pm 1.44$ & $3.74 \pm 1.36$ & $0.601 \pm 0.004$ & $4.318 \pm 0.059$ \\ 
BL-1 + RPO ($\beta=0.01$) & $1.01 \pm 0.58$ & $1.76 \pm 0.59$ & $0.737 \pm 0.002$ & $4.408 \pm 0.010$ & $0.79 \pm 0.12$ & $1.72 \pm 0.18$ & $0.641 \pm 0.002$ & $4.389 \pm 0.021$ \\ 
BL-1 + DPO ($\beta=0.01$) & $0.67 \pm 0.17$ & $1.48 \pm 0.34$ & $0.737 \pm 0.004$ & $4.406 \pm 0.011$ & $0.62 \pm 0.12$ & $1.49 \pm 0.20$ & $0.645 \pm 0.001$ & $4.402 \pm 0.029$ \\ 
BL-1 + DPO ($\beta=0.05$) & $1.30 \pm 0.51$ & $2.25 \pm 0.53$ & $0.737 \pm 0.003$ & $4.401 \pm 0.014$ & $1.01 \pm 0.40$ & $2.01 \pm 0.49$ & $0.643 \pm 0.005$ & $4.402 \pm 0.013$ \\ 
BL-1 + CFG ($\gamma=2.5$) & $0.73 \pm 0.16$ & $1.63 \pm 0.24$ & $0.752 \pm 0.003$ & $4.420 \pm 0.010$ & $0.69 \pm 0.07$ & $1.59 \pm 0.10$ & $0.653 \pm 0.005$ & $4.415 \pm 0.007$ \\ 
BL-1 + RPO + CFG ($\gamma=2.5$)& $0.75 \pm 0.23$ & $1.56 \pm 0.32$ & $0.766 \pm 0.003$ & $\mathbf{4.422 \pm 0.011}$ & $0.51 \pm 0.12$ & $1.25 \pm 0.18$ & $0.674 \pm 0.004$ & $4.392 \pm 0.029$ \\ 
BL-1 + DPO ($\beta=0.01$) + CFG ($\gamma=2.5$)& $\mathbf{0.51 \pm 0.12}$ & $\mathbf{1.32 \pm 0.23}$ & $\mathbf{0.767 \pm 0.004}$ & $4.418 \pm 0.012$ & $0.58 \pm 0.17$ & $1.38 \pm 0.08$ & $\mathbf{0.678 \pm 0.005}$ & $\mathbf{4.417 \pm 0.015}$ \\ 
BL-1 + DPO ($\beta=0.05$) + CFG ($\gamma=2.5$)& $1.12 \pm 0.83$ & $1.87 \pm 0.85$ & $0.766 \pm 0.002$ & $4.420 \pm 0.011$ & $\mathbf{0.49 \pm 0.07}$ & $\mathbf{1.24 \pm 0.11}$ & $0.676 \pm 0.004$ & $4.390 \pm 0.025$ \\ 
\midrule
Decoder Context (BL-2) & $1.73 \pm 0.60$ & $2.98 \pm 0.59$ & $0.700 \pm 0.001$ & $4.350 \pm 0.038$ & $2.68 \pm 1.13$ & $4.02 \pm 1.12$ & $0.637 \pm 0.008$ & $4.347 \pm 0.024$ \\ 
BL-2 + RPO ($\beta=0.01$) & $1.01 \pm 0.60$ & $2.03 \pm 0.62$ & $0.719 \pm 0.002$ & $4.403 \pm 0.013$ & $1.17 \pm 0.94$ & $2.09 \pm 1.00$ & $0.681 \pm 0.005$ & $4.401 \pm 0.012$ \\ 
BL-2 + DPO ($\beta=0.01$) & $1.32 \pm 0.40$ & $2.39 \pm 0.46$ & $0.708 \pm 0.004$ & $4.392 \pm 0.017$ & $0.89 \pm 0.15$ & $1.90 \pm 0.28$ & $0.667 \pm 0.003$ & $4.400 \pm 0.012$ \\ 
BL-2 + DPO ($\beta=0.05$) & $1.25 \pm 0.83$ & $2.27 \pm 0.97$ & $0.716 \pm 0.004$ & $4.393 \pm 0.016$ & $0.98 \pm 0.46$ & $2.03 \pm 0.49$ & $0.676 \pm 0.004$ & $4.408 \pm 0.010$ \\ 
BL-2 + CFG ($\gamma=2.5$) & $0.62 \pm 0.20$ & $1.58 \pm 0.44$ & $0.741 \pm 0.003$ & $\mathbf{4.418 \pm 0.009}$ & $0.57 \pm 0.11$ & $\mathbf{1.37 \pm 0.11}$ & $0.720 \pm 0.004$ & $\mathbf{4.417 \pm 0.007}$ \\ 
BL-2 + RPO + CFG ($\gamma=2.5$)& $\mathbf{0.51 \pm 0.12}$ & $\mathbf{1.38 \pm 0.25}$ & $\mathbf{0.751 \pm 0.002}$ & $4.415 \pm 0.013$ & $\mathbf{0.55 \pm 0.11}$ & $1.41 \pm 0.19$ & $\mathbf{0.729 \pm 0.003}$ & $4.415 \pm 0.012$ \\ 
BL-2 + DPO ($\beta=0.01$) + CFG ($\gamma=2.5$)& $0.62 \pm 0.09$ & $1.53 \pm 0.17$ & $0.744 \pm 0.002$ & $4.409 \pm 0.019$ & $0.60 \pm 0.10$ & $1.40 \pm 0.31$ & $0.720 \pm 0.001$ & $4.387 \pm 0.038$ \\ 
BL-2 + DPO ($\beta=0.05$) + CFG ($\gamma=2.5$)& $0.54 \pm 0.08$ & $1.43 \pm 0.19$ & $0.749 \pm 0.005$ & $4.413 \pm 0.018$ & $0.55 \pm 0.10$ & $1.42 \pm 0.28$ & $\mathbf{0.729 \pm 0.003}$ & $4.413 \pm 0.013$ \\ 
\midrule
SV Conditioned (BL-3) & $1.71 \pm 0.41$ & $2.82 \pm 0.41$ & $0.697 \pm 0.003$ & $4.360 \pm 0.021$ & $3.12 \pm 0.98$ & $4.22 \pm 1.02$ & $0.619 \pm 0.003$ & $4.318 \pm 0.034$ \\ 
BL-3 + RPO ($\beta=0.01$) & $0.72 \pm 0.14$ & $1.61 \pm 0.22$ & $0.717 \pm 0.001$ & $4.408 \pm 0.010$ & $1.25 \pm 0.38$ & $2.06 \pm 0.49$ & $0.668 \pm 0.003$ & $4.389 \pm 0.026$ \\ 
BL-3 + DPO ($\beta=0.01$) & $0.62 \pm 0.19$ & $1.46 \pm 0.31$ & $0.705 \pm 0.003$ & $4.402 \pm 0.011$ & $0.76 \pm 0.12$ & $1.67 \pm 0.11$ & $0.650 \pm 0.003$ & $4.384 \pm 0.023$ \\ 
BL-3 + DPO ($\beta=0.05$) & $1.24 \pm 0.33$ & $2.19 \pm 0.33$ & $0.713 \pm 0.002$ & $4.416 \pm 0.042$ & $1.64 \pm 0.70$ & $2.64 \pm 0.69$ & $0.663 \pm 0.003$ & $4.385 \pm 0.019$ \\ 
BL-3 + CFG ($\gamma=2.5$) & $0.48 \pm 0.12$ & $1.38 \pm 0.33$ & $0.738 \pm 0.003$ & $4.407 \pm 0.025$ & $0.52 \pm 0.11$ & $1.38 \pm 0.20$ & $0.703 \pm 0.002$ & $\mathbf{4.418 \pm 0.011}$ \\ 
BL-3 + RPO + CFG ($\gamma=2.5$)& $0.46 \pm 0.10$ & $1.25 \pm 0.14$ & $\mathbf{0.750 \pm 0.003}$ & $\mathbf{4.423 \pm 0.010}$ & $0.57 \pm 0.15$ & $1.33 \pm 0.21$ & $\mathbf{0.715 \pm 0.003}$ & $4.416 \pm 0.014$ \\ 
BL-3 + DPO ($\beta=0.01$) + CFG ($\gamma=2.5$)& $0.46 \pm 0.05$ & $\mathbf{1.24 \pm 0.11}$ & $0.743 \pm 0.002$ & $4.417 \pm 0.015$ & $\mathbf{0.47 \pm 0.07}$ & $\mathbf{1.26 \pm 0.09}$ & $0.706 \pm 0.004$ & $4.412 \pm 0.014$ \\ 
BL-3 + DPO ($\beta=0.05$) + CFG ($\gamma=2.5$)& $\mathbf{0.45 \pm 0.13}$ & $1.27 \pm 0.06$ & $0.747 \pm 0.001$ & $4.403 \pm 0.021$ & $0.70 \pm 0.45$ & $1.51 \pm 0.44$ & $\mathbf{0.715 \pm 0.004}$ & $4.373 \pm 0.055$ \\ 
\bottomrule
\end{tabular} 
}
\label{tab:dporpoallmodels}
\end{table*}
\vspace{-5mm}




\section{DPO and RPO on all model architectures}
\label{sec:dporpoall}
We perform DPO and RPO preference optimization on all models and evaluate the preference aligned checkpoint with and without CFG. Results are reported in Table~\ref{tab:dporpoallmodels}. We observe a significant improvement in CER, WER and SSIM across all baseline models, when preference alignment or CFG is done in isolation. Across most architectures, the best metrics are achieved by CFG inference on a preference aligned checkpoint (DPO + CFG or RPO + CFG). 
Both DPO and RPO perform similarly, but in practice, we find DPO to be more sensitive to $\beta$ hyperparameter as compared to RPO.



\section{MOS, SMOS and CMOS Evaluation}
\label{sec:humaneval}
\textbf{Naturalness MOS Evaluation:} We ask human listeners to rate the audio on a scale of $1$ to $5$ point naturalness scale with $1$ point increments. We present $180$ audio examples of each technique and each audio is independently rated by at least $11$ listeners. This results in a total of $1980$ evaluations per technique. 
The template used for the Naturalness human study is shown in Figure~\ref{figs:mostemplate}. We report the MOS with $95\%$ confidence intervals in Table~\ref{tab:results_libri_unseen_test} of the paper.

\textbf{Speaker Similarity MOS (SMOS):} For SMOS evaluation, we ask human listeners to rate the speaker similarity of a given pair of utterances. For this evaluation, each synthetic utterance is paired with a real context utterance of the target speaker. We create pairs for all of the $180$ synthesized utterances of each technique. Each pair is rated by at least $11$ independent listeners resulting in at least $1800$ speaker similarity evaluations of each technique. We ask the listeners to judge only the voice/speaker of the utterances and ignore the accent, content, grammar and expressiveness of speech. following past work~\citep{casanova2022yourtts,hussain2023ace,neekharaselfvc}. The templates used for this user study are shown in Figures~\ref{figs:simmostemplate}, \ref{figs:cmostemplate} and \ref{figs:mostemplate}. 
% The Sim-MOS with $95\%$ confidence intervals in Table~\ref{tab:} of the paper. For reference, the reported Sim-MOS for same-speaker ground truth pairs is $4.36 \pm 0.08$ and different-speaker ground truth pairs is $1.77 \pm 0.10$.

\textbf{Comparative MOS (CMOS):} For CMOS, listeners are asked to compare two audio utterances on naturalness and indicate their preference as one of the five options shown in Figure~\ref{figs:cmostemplate}. We pair the two Koel-TTS models with all other models. We evaluate the percentage of times across $1800$ evaluations that Koel-TTS is preferred over an alternate model.


\begin{figure}[h]
    \centering
    \begin{minipage}{0.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{diagrams/smos.png}
        \caption{\footnotesize{User Study template used for Speaker Similarity (SMOS) evaluation}}
        \label{figs:simmostemplate}
    \end{minipage}\hfill
    \begin{minipage}{0.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{diagrams/cmos.png}
        \vspace{2mm}
        \caption{\footnotesize{User Study template used for Comparative-CMOS evaluation}}
        \label{figs:cmostemplate}
    \end{minipage}\hfill
    \begin{minipage}{0.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{diagrams/mos.png}
        \vspace{6mm}
        \caption{\footnotesize{User Study template used for MOS evaluation}}
        \label{figs:mostemplate}
    \end{minipage}
\end{figure}

% \begin{figure}[h]
%     \centering
    
% \end{figure}





% \vspace{-5mm}
\section{Multilingual Tokenization Ablations}
\label{sec:multilingualablations}
We train three decoder-context Koel-TTS models considering three tokenization schemes besides phonemes --- Model A: Aggregated characters from different languages (Vocab size $=256 \times \text{Number of Languages}$). Model A is the default model in our primary multilingual experiments. Model B: Shared character tokenizer (256 character tokens shared across all languages). Model C: Multilingual sentence piece tokens~\footnote{ \url{https://huggingface.co/google-bert/bert-base-multilingual-uncased}} (Vocab size 110k). We find that character-based tokenizers perform significantly better than sentence piece tokenizer on intelligibility metrics, especially when unseen words are encountered during inference. Table~\ref{tab:multilingualablations} compares the different models for each language studied in our work. 
All results are reported using CFG scale $\gamma=2.5$, without any preference alignment. 
Additionally, we find the aggregated char tokenizer to perform better for cross-lingual TTS synthesis (when the context audio has a different language than the input text). This is because token embeddings for each language are independent from the others and not shared (as in the case of the shared character tokenizer).

% \renewcommand{\arraystretch}{0.9} % Reduce spacing
\setlength{\tabcolsep}{6pt}
\begin{table}
\vspace{-2mm}
\caption{\footnotesize{Comparison of decoder context Koel-TTS models trained using different text tokenizers, considering all allowed tokenizations at test time. (CFG Scale $\gamma=2.5$, No Preference Alignment). Evaluation conduced on unseen speakers for each language on the test set described in Section~\ref{sec:multilingualexperiment}}}
\centering
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{lll|cccc}
\toprule
Model & Language & Tokenizer & CER(\%) $\downarrow$ & WER(\%) $\downarrow$ & SSIM $\uparrow$  \\
\midrule
Model A (Phoneme + Aggregated characters) & English & Phonemes & $0.78 \pm 0.77$ & $1.65 \pm 0.85$ & $0.735 \pm 0.002$ \\
Model B (Phoneme + Multilingual sentencepiece) & English & Phonemes & $0.59 \pm 0.10$ & $1.57 \pm 0.21$ & $0.746 \pm 0.003$ \\
Model C (Phoneme + Shared characters) & English & Phonemes & $0.60 \pm 0.20$ & $1.41 \pm 0.23$ & $0.739 \pm 0.002$ \\
Model B (Phoneme + Multilingual sentencepiece) & English & Multilingual sentencepiece & $0.58 \pm 0.09$ & $1.44 \pm 0.17$ & $\mathbf{0.747 \pm 0.000}$ \\
Model C (Phoneme + Shared characters) & English & Shared characters & $\mathbf{0.52 \pm 0.04}$ & $\mathbf{1.37 \pm 0.08}$ & $0.739 \pm 0.001$ \\
Model A (Phoneme + Aggregated characters) & English & Aggregated characters & $0.77 \pm 0.49$ & $1.67 \pm 0.47$ & $0.731 \pm 0.005$ \\
\midrule
Model A (Phoneme + Aggregated characters) & Spanish & Phonemes & $1.37 \pm 1.53$ & $3.63 \pm 1.99$ & $0.693 \pm 0.013$ \\
Model B (Phoneme + Multilingual sentencepiece) & Spanish & Phonemes & $1.52 \pm 0.71$ & $4.01 \pm 0.35$ & $0.698 \pm 0.011$ \\
Model C (Phoneme + Shared characters) & Spanish & Phonemes & $1.41 \pm 0.56$ & $3.73 \pm 0.59$ & $0.703 \pm 0.007$ \\
Model B (Phoneme + Multilingual sentencepiece) & Spanish & Multilingual sentencepiece & $1.96 \pm 0.68$ & $4.93 \pm 0.67$ & $0.701 \pm 0.007$ \\
Model C (Phoneme + Shared characters) & Spanish & Shared characters & $1.64 \pm 0.81$ & $4.03 \pm 1.08$ & $\mathbf{0.704 \pm 0.011}$ \\
Model A (Phoneme + Aggregated characters) & Spanish & Aggregated characters & $\mathbf{1.03 \pm 0.11}$ & $\mathbf{3.11 \pm 0.18}$ & $0.693 \pm 0.008$ \\
\midrule
Model A (Phoneme + Aggregated characters) & German & Phonemes & $2.69 \pm 2.36$ & $5.75 \pm 3.25$ & $0.607 \pm 0.008$ \\
Model B (Phoneme + Multilingual sentencepiece) & German & Phonemes & $4.01 \pm 3.89$ & $6.88 \pm 5.05$ & $0.613 \pm 0.021$ \\
Model C (Phoneme + Shared characters) & German & Phonemes & $\mathbf{1.67 \pm 0.71}$ & $\mathbf{4.54 \pm 1.05}$ & $\mathbf{0.645 \pm 0.007}$ \\
Model B (Phoneme + Multilingual sentencepiece) & German & Multilingual sentencepiece & $4.28 \pm 3.13$ & $7.77 \pm 2.90$ & $0.611 \pm 0.014$ \\
Model C (Phoneme + Shared characters) & German & Shared characters & $1.93 \pm 1.06$ & $4.76 \pm 1.08$ & $0.644 \pm 0.005$ \\
Model A (Phoneme + Aggregated characters) & German & Aggregated characters & $2.02 \pm 1.13$ & $4.81 \pm 0.95$ & $0.614 \pm 0.003$ \\
\midrule
Model B (Phoneme + Multilingual sentencepiece) & French & Multilingual sentencepiece & $6.77 \pm 2.12$ & $10.33 \pm 2.17$ & $0.638 \pm 0.009$ \\
Model C (Phoneme + Shared characters) & French & Shared characters & $2.30 \pm 0.70$ & $\mathbf{5.35 \pm 0.81}$ & $\mathbf{0.643 \pm 0.006}$ \\
Model A (Phoneme + Aggregated characters) & French & Aggregated characters & $\mathbf{2.28 \pm 1.26}$ & $5.50 \pm 1.57$ & $0.641 \pm 0.011$ \\
\midrule
Model B (Phoneme + Multilingual sentencepiece) & Italian & Multilingual sentencepiece & $3.68 \pm 0.65$ & $12.83 \pm 1.07$ & $\mathbf{0.650 \pm 0.003}$ \\
Model C (Phoneme + Shared characters) & Italian & Shared characters & $4.99 \pm 1.83$ & $12.81 \pm 2.15$ & $0.649 \pm 0.009$ \\
Model A (Phoneme + Aggregated characters) & Italian & Aggregated characters & $\mathbf{2.37 \pm 0.50}$ & $\mathbf{9.64 \pm 1.09}$ & $0.615 \pm 0.003$ \\
\midrule
Model B (Phoneme + Multilingual sentencepiece) & Dutch & Multilingual sentencepiece & $4.19 \pm 1.24$ & $11.49 \pm 1.96$ & $0.607 \pm 0.004$ \\
Model C (Phoneme + Shared characters) & Dutch & Shared characters & $\mathbf{3.05 \pm 0.93}$ & $\mathbf{9.79 \pm 1.57}$ & $\mathbf{0.613 \pm 0.008}$ \\
Model A (Phoneme + Aggregated characters) & Dutch & Aggregated characters & $3.32 \pm 1.51$ & $10.14 \pm 1.18$ & $0.594 \pm 0.006$ \\
\bottomrule
\end{tabular} 
}
\label{tab:multilingualablations}
\end{table}

\section{Model Architecture and Training Hyperparameters}
\label{sec:trainingdetails}
For our primary experiments (Table~\ref{tab:architecture_comp}, Table~\ref{tab:dpo_rpo_cfg_base_multi} and Table~\ref{tab:dporpoallmodels}), in each architecture, the decoder is built with $12$ transformer layers using a hidden dimension of $768$ and a feed-forward network (FFN) dimension of $3072$. Rather than a standard FFN sublayer, this decoder employs a causal convolution layer with a kernel size of $3$. The transcript encoder, on the other hand, is composed of $6$ transformer layers that do not use causal masking but otherwise match the decoder's specifications. For multi encoder, a $3$ layer context encoder is added, which uses the same non-causal design as the transcript encoder. In the multi encoder architecture, the transcript encoding is fed to cross attention layers of decoder layer ($[0,2,4,6,8,10]$) and context audio tokens are fed to decoder layers ($[1,3,5,7,9,11]$). For all other models, transcript encoding goes to all decoder layers. Our models are trained on $16$ NVIDIA A100 GPUs using a global batch size of $256$, optimized using Adam optimizer with an initial learning rate of $1e-4$. The learning rate is annealed every $1000$ training steps using an exponential decay factor of $0.998$. 
Training for different architectures on English converges in around $200k$ steps with this configuration and takes around $40$ hours. 

For our larger multilingual model, we increase the hidden dimension to $1536$ and FFN dimension to $6144$ for both the encoder and decoder. The number of layers of the decoder are increased from $12$ to $16$, while keeping the same number of layers in the encoder. 
With this larger transformer, we set kernel size=$1$ for the decoder, and kernel size=$3$ for encoder. Training for this larger model converges in around $150k$ steps using a global batch size of $256$ across distributed across $32$ NVIDIA A100 GPUs.




% TODO add table




% \section{Adapting Koel-TTS for Multi-turn Dialogue Generation}
% \label{sec:multi-turn}

% Recently released technologies such as NotebookLM~\cite{notebooklm} and GenFM~\cite{genfm} showcase the ability to generate speech and multi-turn dialogues for multiple speakers, in a conversational, podcast-like format.
% Towards this goal, we fine-tune our Koel-TTS model to handle multi-turn dialogue generation, by conditioning the audio generation on the past conversation context. 
% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.6\linewidth]{diagrams/Koel_ICML_Podcast.pdf}   
% \caption{}
% \label{figs:podcast}
% \end{figure}
% The encoder processes input text with speaker tags to differentiate between speakers, while the decoder generates audio tokens corresponding to the dialogue. Our approach addresses the scarcity of real-world multi-turn dialogue datasets by leveraging synthetic datasets, simulating turn-switching behavior between speakers. 
% During inference, text-based dialogue generation models provide contextual input, where the audio tokens of the last few seconds of generated dialogue are used as conditioning context for subsequent turns, enabling coherent and dynamic multi-speaker conversations.

\section{Evaluation on hard sentences with repeated words}
Autoregressive TTS models often struggle with challenging sentences containing repeated words. Issues such as infinite silences, looping of words become more prominent when presented with such challenging inputs. While cross-attention biasing~\cite{onealign,t5tts} partially addresses this issue, we find CFG and preference alignment can further improve robustness of the base model by mitigating hallucinations. 
Table~\ref{tab:challengingtexts} presents evaluation of base model and preference aligned + CFG models on a set of $91$ hard sentences linked in our webpage. We conduct these evaluations by pairing the challenging texts with $2$ seen speakers in our training dataset. As indicated by the results, while CFG and preference alignment improve CER and WER, there is scope for further improvement using inference-time monotonic alignment strategies.

\renewcommand{\arraystretch}{1.0} % Reduce spacing
\setlength{\tabcolsep}{8pt}
\begin{table}[ht]
% \vspace{-4mm}

\caption{\footnotesize{Intelligibility and Speaker similarity evaluation on challenging sentences with repeated words on base models and the preference aligned models with CFG.}}

\centering
\resizebox{0.8\columnwidth}{!}{%
% \footnotesize
\begin{tabular}{l|cccccc}
\toprule
Model & CER(\%) $\downarrow$ & WER(\%) $\downarrow$ & SSIM $\uparrow$ \\
\midrule
Multi Encoder (Baseline) & $5.62 \pm 0.52$ & $10.60 \pm 0.90$ & $0.788 \pm 0.002$ \\
Multi Encoder (w Pref Align and CFG) & $\mathbf{5.03 \pm 0.16}$ & $\mathbf{9.19 \pm 0.46}$ & $\mathbf{0.807 \pm 0.001}$\\
\midrule
Decoder Context (Baseline) & $5.70 \pm 0.56$ & $10.38 \pm 0.35$ & $0.790 \pm 0.003$\\
Decoder Context (w Pref Align and CFG) & $\mathbf{4.69 \pm 0.14}$ &  $\mathbf{9.04 \pm 0.24}$ &  $\mathbf{0.798 \pm 0.002}$ \\
\midrule
SV Conditioned (Baseline) & $5.59 \pm 0.84$ & $10.33 \pm 0.94$ & $0.785 \pm 0.001$\\
SV Conditioned (w Pref Align and CFG) & $\mathbf{4.67 \pm 0.37}$ & $\mathbf{8.50 \pm 0.66}$ & $\mathbf{0.798 \pm 0.002}$ \\
% \midrule

\bottomrule
\end{tabular} 
}
\label{tab:challengingtexts}
\vspace{-2mm}
\end{table}

% We observe a considerable reduction in CER and WER with CFG and preference alignment across all model architectures.