\section{Learning Contracts with Single-Dimensional Costs and Continuous Types}\label{sec:learning}

%\subsection{What is achievable?}

We proved in \Cref{sec:hardness} that there are no polynomial-time algorithms that compute $\epsilon$-optimal contracts under additive approximations. Well-know reductions from offline to online problems (see, \emph{e.g.}, \citet{roughgarden2019minimizing}) essentially preclude the existence of efficient algorithms that achieve regret polynomial in the instance size and sub-linear in $T$ in this setting, unless $\NP\subseteq\RP$. 
%
Therefore, we shift our focus to the problem of achieving sub-linear regret with \emph{inefficient} algorithms, prioritizing the learning aspects over computational efficiency.


The main result of this section is a learning algorithm that provides regret of $\widetilde O(\poly(n,m)T^{3/4})$ assuming that the types are stochastic and single-dimensional.
Then, we extend our result to provide a sample complexity bound of $\widetilde O\left(\eta^{-4}\poly(m,n)\right)$ to learn $\eta$-optimal contracts.

\subsection{Setting and Assumptions}

In our learning setup, we introduce two standard assumptions.
First, as is common when handling continuous types \citep{alon2023bayesian, alon2021contracts}, we make some minimal assumptions on the underlying distributions. In particular, we consider distributions which admits density bounded by some constant $\beta$.

\begin{assumption}\label{ass:boundedDensity}
    The type distribution $\Gamma$ admits density $f_\Gamma$ such that  $\sup_{\theta\in[0,1]}f_\Gamma(\theta)\le \beta$.
\end{assumption}


Our second assumption is on the set of possible contracts available to the principal. Following previous work on learning in contract design problems \citep{bacchiocchi2023learning,zhu2022online,chen2024bounded,ho2014adaptive}, we restrict our focus to bounded contracts.
%
\begin{assumption}\label{ass:boundedContract}
	The principal can post only contracts in $p\in [0,1]^m$.%\footnote{With slight abuse of notation, we now refer to $\OPT$ as the value of the optimal bounded contract.}
\end{assumption}
%
Our results will easily extend to bounded contracts $p\in [0,C]^m$, by adding a linear dependence on $C$ in the regret bounds. Intuitively, the maximum possible payment determines the range of possible principal's utility. This affects the variance of the observed random variables and the regret bounds.
%
With slight abuse of notation, we now use $\OPT$ to denote the value of the optimal bounded contract in $[0,1]^m$.


\subsection{A No-Regret Algorithm}

In this section, we design a regret-minimization algorithm for single-dimensional contract design problem defined, as defined in \Cref{sec:prelimLearning}. Then, in the next section, we extend our results to derive sample complexity bounds.

There are two main challenges in obtaining no-regret algorithms for our problem.

\paragraph{Continuous decision space} The primary and most evident  challenge is that the decision space (the $m$-dimensional hypercube) is continuous, and that the function $p\mapsto \Up(p)$ lacks useful structural properties (indeed, it is neither concave nor Lipschitz). As a result, most existing techniques for multi-dimensional contract design, which operate directly on this decision space and do not take advantage of single-dimensional types, become inapplicable~\cite{zhu2022online,bacchiocchi2023learning,ho2014adaptive}.

\paragraph{Infinite types} The second challenge arises from having infinitely many possible types, as the support is $\Theta=[0,1]$. This renders standard approaches for Stackelberg-like games inapplicable. Indeed, these approaches for learning in games with commitments crucially rely on having a finite number of types.
%
For example, \citet{balcan2015commitment} employ an explore-then-commit algorithm that leverages barycentric spanners to estimate the values of exponentially many possible decision while playing only a polynomial number of them (equal to the number of types). As is typical in explore-then-commit algorithms, this approach achieves $    \widetilde O(T^{2/3})$ regret and a polynomial dependence on the number of types. Hence, similar approaches are doomed to fail in settings with continuous types.
%
As a further example, \citet{bernasconi2023optimal} avoid the exploration phase by reducing the problem to linear bandit optimization on a suitable small-dimensional space with dimension $d$ equal to the number of types $|\Theta|$. This approach yields regret bounds that depend only on the dimensionality of the space rather than the number of actions, resulting in $O(\poly(d)\sqrt{T})$ regret.
%
Both approaches perform poorly when the number of types is infinite. This is not just a technical limitation: there is strong evidence that as the number of types grows large, one cannot hope to obtain no-regret algorithms in general. For instance, \citet[Theorem~7.1]{balcan2015commitment} show that when the number of agent types is not bounded, then the regret is necessarily linear. For contract design, \citet{zhu2022online} provide similar results.

We address these challenges by introducing an approximate instance with finitely many types, using the same discretized grid as in \Cref{sec:PTAS}.
Here, we have the additional challenge of showing that such grid provides a good approximation despite not knowing the type distribution $\Gamma$. 
As we will discuss in details in the following, this is \emph{not} true for general distributions.
However, we show that if the underlying types distribution has bounded density, then for \emph{any} contract $p$ the expected principal's utilities with continuous and with discretized types are close.
%Intuitively, this holds in the case of single-dimensional contracts (and not in the negative examples provided above) because of the structural properties held by agents' costs in this model. Here \Cref{ass:boundedContract} and \Cref{ass:boundedDensity} will play a major role. 
Formally, we prove the following:



\begin{restatable}{lemma}{lemmaClose}\label{lem:valClose}
    For any type distribution $\Gamma$ and any $\epsilon>0$, let $\Theta_\epsilon\coloneqq\{\epsilon\cdot(i-\frac{1}{2})\}_{i\in\lceil1/\epsilon\rceil}$ and define $\gamma$ supported on $\Theta_\epsilon$ as 
\[\gamma_{\theta}\coloneqq \int_{\theta-\epsilon/2}^{\theta+\epsilon/2} f_\Gamma(\tilde\theta) d\tilde\theta\quad\textnormal{ for any }\theta\in\Theta_\epsilon.\]
Then, under \Cref{ass:boundedDensity} and \Cref{ass:boundedContract}, for any $p \in [0,1]^m$, it holds that
    \[  
        \left|\mathbb{E}_{\theta\sim\Gamma}[\Up(p,\theta)]-\sum_{\theta\in \Theta_\epsilon}  \gamma_\theta \ \Up(p, \theta) \right|\le 2\beta n \epsilon.
    \] 
\end{restatable}

\begin{remark}
    \Cref{lem:valClose} is fundamentally different from \Cref{lem:linearization} and other results that connect $\epsilon$-IC and IC contracts  to ``robustify'' contracts~\citep{dutting2021complexity,zhu2022online,bacchiocchi2023learning, bernasconi2024regret}. %Such results hold for arbitrary distributions and only provide a lowerbound on the achieved utility. Such estimations can highly underestimate the principal's utility, making them useless in our approach.
    These existing results apply to arbitrary distributions but only provide a lower bound on the achieved utility, often significantly underestimating the principal's utility, and rendering them inapplicable in our approach. 
    %
    In contrast, our result crucially relies on \Cref{ass:boundedDensity} and establishes both upper and lower bounds on the utility of any given contract.
\end{remark}

Hence, the above lemma addresses the 
challenge of infinitely many types. In particular, by incurring a discretization error of $O(\beta n\epsilon T)$, we can restrict to a finite set of discretized types of size $O(1/\epsilon)$.
%Hence, we ``solved'' the infinity type problem, since making a discretization error of $O(\beta n\epsilon T)$, we can restrict to a finite set of discretized types of size $O(1/\epsilon)$.
%
Next, we deal with the problem of having an infinite decision space. Our first result partially resolves this by reducing the set of possible decisions from infinite to exponentially many. 
To achieve this, we leverage a powerful characterization of optimal contracts in Bayesian contract design that is independent of the underlying type distribution. This is a standard approach in Stackelberg-like games and is, for example, one of the main tools employed by \citet{balcan2015commitment} for solving learning problems in Bayesian Stackelberg games. 
%
A similar result also exists in contract design, albeit for \emph{discrete types}. Indeed, \citet{castiglioni2022bayesian,guruganesh2021contracts} showed that, regardless of the underlying distribution over discrete types, the optimal contract always belongs to a finite set of possible contracts $\Pcal$.\footnote{While the original result is for unbounded contracts, it trivially extend to $[0,1]^m$ by adding the hyperplanes $p_\omega\le1$ for each $\omega\in \Omega$.}


\begin{theorem}[Essentially \cite{guruganesh2021contracts, castiglioni2022bayesian}]\label{thm:toFinite}
    For every finite set of types $\Theta$, there exists a finite set of contracts $\Pcal\subset [0,1]^m$ with $|\Pcal|= \poly\left((n,m,|\Theta|)^m\right)$ such that
    \[
    \max_{p\in \Pcal} \sum_{\theta\in \Theta}  \gamma_\theta \Up(p, \theta) = \max_{p\in [0,1]^m} \sum_{\theta\in \Theta}  \gamma_\theta \Up(p, \theta)\quad  \forall \gamma \in \Delta_{\Theta}.
    \]
\end{theorem} 


Now, we show how to combine the results in \Cref{lem:valClose} and \cref{thm:toFinite}.
%
In \Cref{lem:valClose}, we showed that reducing to discretized types provides a good approximation of the reward with the true continuous types, and \Cref{thm:toFinite} shows how to restrict to a finite number of possible contracts $\Pcal$. A simple approach would be to reduce our problem to a finite MAB in which $\Pcal$ is the finite set of arms. This would provide a regret upper bound of $\sqrt{|\Pcal| T}+O(\epsilon n\beta T)$. Since $|\Pcal|=\poly((n,m,1/\epsilon)^m)$, this yields a regret upper bound that, although sub-linear in 
$T$ (for some choice of $\epsilon$), is  exponential in the instance size. 

\citet{bernasconi2023optimal} provided a general framework to solve similar problems and remove the dependency on the size of the decision space.
Such a framework reduces the problem of online learning in Stackelberg-like games with finite types to linear bandits. Their main idea is to map the decisions into a ``utility space'' instead of the original decision space, and then map the decisions from the utility space back to the original one. The advantage is that the problem in the utility space is linear, and thus it scales only with the dimension rather than with the number of actions.
%
Employing such reduction, \citet{bernasconi2023optimal} provided regret bounds of order $O(\sqrt{T})$ that scale polynomially with the number of types rather than with the number of possible decisions.
%
Notice that their reduction cannot directly be employed in in our setting since the discrete type instance is only an approximation of the true one, breaking the linear structure of the utility space.

Since we cannot recover linearity in the utility space, our key idea is to continue working in such space while allowing the model to be only approximately linear. Crucially, because the linearity assumption is violated, we must use algorithms designed for a class of almost-linear bandits known as Misspecified Linear Bandits (MLB)~\cite{ghosh2017misspecified}.
%We circumvent this problem by still working in the utility space and showing that the model is ``almost linear''.
%Clearly, since the linearity assumption is broken, we need to employ algorithms designed for a class of almost-linear bandits called Misspecified Linear Bandits (MLB)~\cite{ghosh2017misspecified}.

\begin{definition}[Misspecified linear bandits~\citep{lattimore2020learning}]\label{def:MLB}
    At each round $t \in [T]$, the learner chooses an action $x_t$ from a finite set of action $X\subseteq [0,1]^d$ with $|X|=k$. The expected reward of an action $x$ is $\mu(x)=\langle x,\phi\rangle+ y(x) \in [-1,1]$, where $ |y(x)|\le\alpha$ for a misspecification parameter $\alpha>0$. The observed rewards $\mu_t$ are i.i.d.~and $O(1)$-subgaussian with mean $\mathbb{E}[\mu_t]=\mu(x_t)$.
    Finally, the regret is defined as
    \[
    R_T= T \cdot\max_{x\in X} \mu(x)- \mathbb{E}\left[\sum_{t \in [T] }  \mu(x_t)\right],
    \]
    where the expectation is on the possible randomization of the algorithm.
\end{definition}


We will use an algorithm proposed by \citet{lattimore2020learning} as a black-box oracle for this setting.

\begin{theorem}[\cite{lattimore2020learning}]\label{thm:misspecified}
    There exists an algorithm called $\PE$ for misspecified linear bandits which guarantees
    \[
        R_T=O\left(\sqrt{dT\log(Tk)}+ \alpha T\sqrt{d} \log(T)\right). 
    \]
\end{theorem}
Crucially, $\PE$ does not need to know the misspecification parameter $\alpha$ in advance.

Now, we are ready to combine all these components to provide our reduction, whose pseudo-code can be found in Algorithm~\ref{alg:reduction}.
%
\setlength{\algomargin}{1.5em}
\begin{algorithm}[t]
%\SetAlgoLined
    \nl Inputs: $\epsilon>0$, $\Pcal$\;
    \nl Set $\Rcal$ as $\PE$ and $X=\nu_\epsilon(\Pcal)$\;
    %\nl Instantiate $\Rcal$\;
    \For{$t = 1, \ldots, T$}{	
		%
    \nl Set $x_t$ as prescribed by $\Rcal$\;\nllabel{line:decision}
    \nl Commit to any $p_t$ such that $x_t = \nu_\epsilon(p_t)$\;\nllabel{line:invert}
    \nl Agent type $\theta_t$ is drawn according to $\Gamma$\;
    \nl Action $a_t=b^{\theta_t}(p_t)$ is played\;\nllabel{line:br}
    \nl Outcome $\omega_t\sim F_{a_t}$ is observed\;\nllabel{line:outcome}
    \nl Reward $r_{\omega_t}-p_{\omega_t}$ is received and it is used to update $\Rcal$\;
    }
    \caption{Regret minimization for single-dimensional contract design}\label{alg:reduction}
\end{algorithm}

% It uses as oracle a regret minimizer for MLB (see  \Cref{thm:misspecified}).
% 
% 
% We assume that such oracle can be invoked with three functions:
% \begin{itemize}
%     \item  \SetofActions(), that is used to specify the set of possible decisions,
%     \item \play(), that outputs the next decision, \item \observeutility(), that takes as input the utility of played decision.
% \end{itemize}

First, we define the set of actions to be given in input to the regret minimizer. Set $\epsilon=1/\sqrt{T}$.
For any $p \in \Pcal$ (see \Cref{thm:toFinite} for a definition of $\Pcal$), let 
\[
\nu_\epsilon(p)=[\Up(p,\theta)]_{\theta \in \Theta_\epsilon}\quad\text{and}\quad\nu_\epsilon(\Pcal)= \bigcup_{p \in \Pcal} \nu_\epsilon(p).
\]

Note that $\nu_\epsilon(\Pcal)\subset \Reals^{|\Theta_\epsilon|}$ is a finite set of cardinality $|\nu_\epsilon(\Pcal)|=|\Pcal|=\poly((n,m,1/\epsilon)^m)$.
%
The next decision $x_t$ is chosen in the utility space by the MLB regret minimizer $\Rcal$ (\Cref{line:decision}), while actions $p_t$ are made in the actual decision space of contracts inverting the function $\nu_\epsilon(\cdot)$ (\Cref{line:invert}). After the principal commits to contract $p_t$ the agent selects the best response and the outcome $\omega_t$ is selected with probability $F_{a_t,\omega_t}$ (\Cref{line:br} and \Cref{line:outcome}). Finally, the reward received $r_{\omega_t}-p_{\omega_t}$ is used to update the regret minimizer $\PE$.

%The following theorem shows that the two models are almost equivalent and Algorithm~\ref{alg:reduction} provides $\tilde O(\beta \cdot\poly(n,m)T^{3/4})$ regret.
The following theorem proves that \Cref{alg:reduction} provides a $\widetilde O(\beta \cdot\poly(n,m)T^{3/4})$ regret.
%
In the proof, we show that the discretized problem satisfies the conditions of \Cref{def:MLB}. Then, we prove that the regret guarantees of the discretized problem translate to the regret guarantees of the original problem. This holds since the optima of the two problems are close. 

\begin{theorem}\label{thm:regret}
	Under \Cref{ass:boundedDensity} and \Cref{ass:boundedContract}, \Cref{alg:reduction} guarantees
	 \[R_T=\widetilde O(\beta \cdot\poly(n,m)T^{3/4}).\]
\end{theorem}

\begin{proof}
	Let $X=\nu_\epsilon(\Pcal)$. First, we show that the structure of the reward observed by the regret minimizer for MLS satisfies \Cref{def:MLB}.
	Clearly, the reward observed is always in $[-1,1]$ since both $r_\omega$ and $p_\omega$ are bounded in $[0,1]$. Next, we show that there exists a good linear approximation of $\mu(x)$ for any $x\in X$. In particular, we will show that $\mu(x)$ is close to $\langle x, \gamma\rangle$ where $\gamma_{\theta}=\int_{\theta-\epsilon/2}^{\theta+\epsilon/2} f_\Gamma(\theta)d\theta$, which follows from \Cref{lem:valClose}:
    
    \begin{align*}
        |\mu(x)-\langle x,\gamma\rangle|&=\left|\int_{0}^1f_\Gamma(\theta)\Up(p,\theta)d\theta-\sum_{\theta\in\Theta_\epsilon}\gamma_\theta\Up(p,\theta)\right|\le 2\beta n\epsilon.
    \end{align*}
    Therefore, the misspecification parameter is $\alpha=2\beta n\epsilon$.
	%where $\gamma=[\gamma_\theta]_{\theta\in\Theta_\epsilon}$.
	

	Then, by \Cref{thm:misspecified}, we conclude that
	\begin{align*}
		 T\cdot \max_{x\in X} \mu(x)- \mathbb{E}\left[ \sum_{t\in [T]}  \mu(x_t)\right]& \le O(\sqrt{|\Theta_\epsilon|T}\,\log(T (\poly(n,m,|\Theta_\epsilon|)^m))+ 2n\beta\epsilon T \sqrt{|\Theta_\epsilon|}\, \log(T))\\
		 & \le  \widetilde O\left(\beta\cdot \poly(m,n) T^{3/4}\right),
	\end{align*}
	where the last inequality follows by setting $\epsilon=1/\sqrt{T}$,  and the expectation is over the randomness of the algorithm.
	
	%Now, observe that the expected principal utility is exactly 
	%\[
        %\sum_{t\in[T]} E_{\theta \sim \Gamma} [\Up(p,\theta)] = \sum_{t\in [T]} \mu(x_t)
        %\]
	
	Hence, we are left with bounding the difference in utility between $\OPT$ and the optimum in the discretized space, which is $\max_{x \in X} \mu(x)$.
	%
        Since the contract is bounded, we know that there exists an optimal bounded contract $p^*\in \arg \max_{p \in [0,1]} \mathbb{E}_{\theta\sim \Gamma}[\Up(p,\theta)]$.
	Then
	\begin{align*}
		\mathbb{E}_{\theta\sim \Gamma}[\Up(p^*,\theta)]&
        %\int_{0}^{1} f_\Gamma(\theta) \Up(p^*,\theta) d\theta \\
        \le \sum_{\theta \in \Theta_\epsilon}  \gamma_\theta  \Up(p^*,\theta)  + 2\beta n \epsilon\tag{\Cref{lem:valClose}}\\
		& = \max_{p \in \Pcal} \sum_{\theta \in \Theta_\epsilon}  \gamma_\theta  \Up(p,\theta)  + 2\beta n \epsilon,\tag{\Cref{thm:toFinite}}.
	\end{align*}
	
	Hence, using this inequality we can finally prove that
	% \begin{align*}
	% 	  R_T&= T\cdot \mathbb{E}_{\theta\sim \Gamma}[\Up(p^*,\theta)]- \sum_{t \in T}  \mathbb{E}_{\theta\sim \Gamma}[\Up(p_t,\theta)]  \\
	% 	  & =T \int_{0}^{1} f_\Gamma(\theta) \Up(p^*,\theta) d\theta  - \sum_{t \in T}  \mathbb{E}_{\theta\sim \Gamma}[\Up(p_t,\theta)] \\
	% 	  & \le T \max_{p \in \Pcal} \sum_{\theta \in \Theta_\epsilon}  \gamma_\theta  \Up(p,b^\theta(p)) -  \sum_{t \in T}  \mathbb{E}_{\theta\sim \Gamma}[\Up(p_t,\theta(p)]    + 2\beta n \epsilon T\\
	% 	  & = \tilde O\left(\beta \poly(m,n) T^{3/4}\right) + 2 \beta n \epsilon T\\
	% 	  &=\tilde O\left(\beta \poly(m,n) T^{3/4}\right) 
	% \end{align*}
    \begin{align*}
        R_T&=T\cdot\mathbb{E}_{\theta\sim \Gamma}[\Up(p^*,\theta)]-\mathbb{E}\left[ \sum_{t \in T}  \mathbb{E}_{\theta\sim \Gamma}[\Up(p_t,\theta)]\right]\\
        &\le T\cdot\max_{p\in \Pcal} \sum_{\theta\in\Theta_\epsilon}\gamma_\theta \Up(p^*,\theta)-\mathbb{E}\left[ \sum_{t \in T}  \mathbb{E}_{\theta\sim \Gamma}[\Up(p_t,\theta)]\right]+2\beta n\epsilon T\\
        & = \widetilde O\left(\beta \cdot\poly(m,n) T^{3/4}\right) + 2 \beta n \epsilon T,
    \end{align*}
    which is $\widetilde O\left(\beta \cdot\poly(m,n) T^{3/4}\right)$, thus concluding the proof.
\end{proof}

\subsection{Sample Complexity of Single-Dimensional Contracts}
In this section, we show how our results can be extended to derive sample complexity bounds (see \Cref{sec:prelimLearning} for a definition of the problem).
%
%\mat{decidere se vogliamo seguire questa strada}
%
Such extension is based on the observation that the regret minimizer by \citet{lattimore2020learning} that we used in \Cref{alg:reduction} actually guarantees a stronger result. In particular, $\PE$ guarantees that at each time $t$, there exists a subset of feasible arms $X_t\subseteq X$ such that with probability at least $1-\delta$ it holds:
\[
\mu(x)\ge \max_{x\in X}\mu(x)-O\left(\alpha\sqrt{d}+\sqrt{\frac{d}{t}\log(k/\delta)}\right) \quad \forall x\in X_t.
\]
%
Thus, after $\widetilde O\left(\frac{d\log(k/\delta)}{(\eta-c\cdot\alpha\sqrt{d})^2}\right)$ samples, where $c=O(1)$, any arm left in the set $X_t$ is approximately optimal (specifically, $\eta$-optimal). However, this only holds if $\eta\ge\Omega(\alpha\sqrt{d})$. Indeed, it is known that outside this regime, the problem requires samples that are either exponential in dimension or linear in the number of arms \citep{du2019good, lattimore2020learning}, essentially losing the advantage of having an underlying linear structure.

% In our problem, we can control the misspecification error $\alpha$ by tuning the parameter $\epsilon$, and thus, we can guarantee that for any approximation level $\eta$ given as input, we can have good sample complexity bounds.
% Unfortunately, this is possible only assuming to known a upper bound $\beta$ on the density. 
% Hence, in the following $\beta$ is a \emph{known} upper bound and our results will be based on this slightly stronger assumption. 


In our problem, we can control the misspecification error $\alpha$ by adjusting the parameter $\epsilon$, allowing us to ensure good sample complexity bounds for any given approximation level $\eta$. However, this is only possible if we have prior knowledge of an upper bound $\beta$ on the density. Therefore, the following result relies on a slight strengthening of \cref{ass:boundedDensity}, where we assume knowledge of such an upper bound $\beta$.
%
Setting $\epsilon=O\Big(\big(\frac{\eta}{2\beta n}\big)^2\Big)$, we get a number of samples which is $\widetilde O\left(\frac{\poly(\beta, n, m)}{\eta^4}{\log\left(\frac{1}\delta\right)}\right)$.
%
We have to make a minor adjustment to the algorithm of \citet{lattimore2020learning} to have simple stopping rules (our algorithm just stops after a fixed amount of samples, after which it returns a solution). Formally, we need to have any-time concentration bounds in order to handle the $\delta$ correctness of the algorithm.

\begin{restatable}{theorem}{samplecomplexity}\label{thm:sample}
    Under \Cref{ass:boundedDensity} and \Cref{ass:boundedContract}, for any given $\eta>0$ and $\delta\in(0,1]$ there exists an algorithm which finds a contract $p$ in $\widetilde O\left(\frac{\poly(\beta, n, m)}{\eta^4}{\log\left(\frac{1}\delta\right)}\right)$ samples such that
    \[
    \mathbb{P}\left[\mathbb{E}_{\theta\sim \Gamma}\left[\Up(p,\theta)\right]\ge \OPT-\eta\right]\ge 1-\delta.
    \]
\end{restatable}

%\ma{OLD====================}

% \cite{lattimore2020learning}, it guarantees that with high probability at each round $t$ the chosen arm is at most $\tilde O(\sqrt{d/T}+\sqrt{d\alpha})$ suboptimal.
% Following the proof of \Cref{thm:misspecified} and without summing over rounds, we get

% \mat{capire come inserire $\delta$... non Ã¨ semplice senza fare i conti}
% \begin{corollary}\label{thm:sample}
%     Given a desired error probability $\delta>0$ and an approximation error $\eta>0$, Algorithm~\ref{alg:reduction} with $T=\poly(\log(1/\delta))/\eta^{4}$ and $\epsilon=\poly(\log(1/\delta))/\eta^2$ guarantees 
%     \[\mathbb{P}\left[\mathbb{E}_{\theta\sim \Gamma}[\Up(p_T,\theta)\right]\ge \OPT-\eta]\ge 1-\delta.\]
% \end{corollary}


% %Looking at the proof, we get that at round $t$, error at most $\epsilon \sqrt{d}+ \sqrt{d/t}$.

% %In our setting, $\sqrt{\epsilon}+ \sqrt{1/(\epsilon t)}$, and setting $\epsilon=t^{-1/2}$, $t^{-1/4}$

% %To get an error $\eta$, need $\eta^{-4}$ samples.
 




