\section{Preliminaries}
%\ac{notation $\Delta_\Theta$}

\subsection{The Bayesian Principal-Agent Problem}\label{sec:preliminaries_problem}


An instance of a principal-agent problem is defined by a set $A$ of $n \coloneqq |A|$ agent's (hidden) actions. 
Each action leads stochastically to one of the $m$ possible different outcomes $\omega\in \Omega$, which provides  a reward $r_\omega$ to the principal. Each action defines a different probability distribution on the outcomes. We denote with $F_{a,\omega}$ the probability that action $a\in A$ leads to outcome $\omega\in \Omega$. Thus, we have that $F_{a,\omega}\in[0,1]$ for each $a\in A$ and $\omega\in\Omega$, and $\sum_{\omega\in\Omega}F_{a,\omega}=1$ for each $a\in A$. We consider the \emph{single-dimensional hidden-type model} introduced by \citet{alon2021contracts}, in which each action $a$ requires $c_a$ units of effort from the agent. Each agent has a hidden type $\theta\in\Theta\coloneqq [0,1]$, which represents their cost per unit of effort. Consequently, an agent of type $\theta$ incurs a cost of $c_{a,\theta}=\theta\cdot c_a$ when performing action $a\in A$.
%
We say that this setting is single-dimensional in the sense that the agent's type $\theta$ is a single real value that only influences the cost of the agent's actions. In contrast, in general \emph{multi-dimensional} contract design problems, both the action probability $F_{a,\theta}$ and the action cost $c_{a,\theta}$ can depend arbitrarily on the agent's type $\theta$.


The goal of the principal is to design a contract which specifies a payment for each possible outcome. We denote with $p\in\Reals^m_{\ge 0}$ such a payment vector where $p_\omega$ defines the payment when outcome $\omega$ realized.\footnote{The assumption that payments are non-negative is a standard assumption in contract design, known as \emph{limited-liability}~\citep{carroll2015robustness, innes1990limited}.}


When an agent of type $\theta$ takes action $a$, then their expected utility is
\(
\Ua_\theta(p,a):=\sum_{\omega \in \Omega} F_{ a, \omega } p_\omega - \theta c_{a}.
\)
On the other hand, the principal expected utility is 
\(
\Up(p,a):= \sum_{\omega \in \Omega} F_{a, \omega} (r_\omega - \, p_\omega).
\)
Given a contract $p\in\Reals^m_{\ge 0}$ an agent of type $\theta$ plays an action that is
\begin{enumerate}
	\item \emph{incentive compatible} (IC), \emph{i.e.}, it maximizes their expected utility over actions in $A$; and
	%
	\item \emph{individually rational} (IR), \emph{i.e.}, it has non-negative expected utility.
\end{enumerate}


To simplify exposition we can assume w.l.o.g.~that there is an action $a\in A$ such that $c_a=0$, thus conveniently including IR into the IC constraints. In the case in which there are multiple actions which satisfy the IC constraints, we assume (as it it standard in contract design \citep{carroll2015robustness}) that ties are broken in favor of the principal. We can define the set $\Bcal^\theta(p)$ of all actions which are best-responses for an agent of type $\theta$ to a contract $p$. For ease of presentation, we will also need to define the set of actions $\Bcal_\epsilon^\theta(p)$ which are $\epsilon$-IC, formally
\[
{\Bcal}^{\theta}_\epsilon(p)\coloneqq\left\{a\in A: \Ua_\theta(p,a) \ge \Ua_\theta(p,a')-\epsilon\quad\forall a'\in A\right\},
\]
and clearly ${\Bcal}^{\theta}(p)={\Bcal}^{\theta}_0(p)$. The assumption that ties are broken in favor of the principal can be written by assuming that the action $b^\theta(p)\in A$ taken by an agent of type $\theta$ for a contract $p$, is any action such that
$b^\theta(p)\in \arg \max_{a \in \Bcal^{\theta}(p)} \Up(a,p).$ \footnote{It is convenient, especially in \Cref{sec:learning}, to fix the way in which ties are further broken, and we assume that $b^\theta(p)$ is chosen in the set $\arg \max_{a \in \Bcal^{\theta}(p)} \Up(a,p)$ according to a fixed ordering of the agent's actions.}

\subsection{Single-Dimensional Bayesian Principal-Agent Problem}

In the first part of the paper, we focus on \emph{single-dimensional Bayesian} principal-agent problems, in which the type of each agent is drawn from a publicly-known distribution $\Gamma$ over types. This distribution can be absolutely continuous, discrete, or a combination of the two.
% We describe such probability distribution by means of its (generalized) density function $f_\Gamma$ (so we can model both discrete and continuous, or combinations of the two, distributions).

With slight abuse of notation, we write $\Up(p,\theta)$ instead of $\Up(p,b^\theta(p))$ to denote the utility of the principal when the agent's realized type is $\theta$.
Then, given an instance of our problem, we define $\OPT\coloneqq\sup_{p\in\Reals_{\ge 0}^m}\Up(p)$, where $\Up(p)$ is the principal expected utility of contract $p$ which can be written as
\[
\Up(p)\coloneqq\mathbb{E}_{\theta \sim \Gamma} \left[\Up(p,\theta)\right].
\]

\subsection{Learning an Optimal Contract}\label{sec:prelimLearning}

In the second part of the paper, we focus on a regret minimization setting in which the principal interacts sequentially with a stream of $T$ agents.
At each round $t\in [T]$, the type $\theta_t$ of the $t$-th agent is sampled accordingly to the distribution $\Gamma$, which is unknown to the principal.
%
%Notice that we focus on a stochastic setting in which the distribution is the same at each round.
Then, the principal commits to a contract $p_t\in\Reals^m_{\ge 0}$, and the agent plays the best response $a_t=b^{\theta_t}(p_t)$. Finally, the principal observes the outcome $\omega \sim F_{a_t}$, and receives utility $r_{\omega_t}-p_{\omega_t}$. 

The principal's regret is defined as:
\[
R_T= T \cdot\OPT- \mathbb{E}\left[\sum_{t \in T}  \mathbb{E}_{\theta\sim \Gamma}[\Up(p_t,\theta)]\right],   
\]
where the expectation is on the randomness of the algorithm.
Our goal is to design algorithms with regret sub-linear in $T$ and polynomial in the instance size.

%\mat{piace questa descrizione che riusa quanto definito sopra o no?}
Another problem studied in the learning literature consists in finding an approximately optimal contract with high probability. The interaction between the learner and the environment is the same as in the regret-minimizing scenario. However, the goal is to minimize the number of rounds needed to find a good approximation of the optimal contract.
Formally, given an approximation error $\eta$ and a probability $\delta$, we aim at finding a contract $p$ such that
\[
\mathbb{P}\left[\mathbb{E}_{\theta\sim \Gamma}\left[\Up(p,\theta)\right]\ge \OPT-\eta\right]\ge 1-\delta.
\]
The goal is to find such a contract with a small number of samples $T$.

\begin{comment}
\ma{==========OLD==========}

Moreover there is a cost vector specifying the costs $c_a\in[0,1]$ that the agent must suffer in order to undertake action $a\in A$.




An instance of the \emph{Bayesian principal-agent problem} is defined by a tuple $(\Theta,A,\Omega)$, where: $\Theta=[0,1]$ is a set of single dimensional agent's types; $A$ is a finite set of $n \coloneqq |A|$ agents' actions; and $\Omega$ is a finite set of $m \coloneqq |\Omega|$ possible outcomes.\footnote{For the ease of presentation, we assume that all the agent's types share the same action set. All the results in this paper can be easily extended to the case in which each agent's type $\theta \in \Theta$ has their own action set $A_\theta$.}
%
The agent's type is drawn according to a fixed probability distribution known to the principal. 
%
%We let $\mu \in \Delta_{\Theta}$ be such a distribution, with $\mu_\theta$ denoting the probability of type $\theta \in \Theta$ being selected.
We describe such probability distribution by means of its density function $\Gamma$ (so we can model both discrete and continuous, or combinations of the two, distributions).
%
We denote by $F_{ a} \in \Delta_\Omega$ the probability distribution over outcomes $ \Omega$ when an agent selects action $a \in A$, while $c_{a} \in [0,1]$ is the agent's cost for that action.\footnote{In the rest of this work, we assume that rewards and costs are in $[0,1]$. All the results in this paper can be easily generalized to the case of an arbitrary range of positive numbers, by applying a suitable normalization.}
%
For the ease of notation, we let $F_{a, \omega} $ be the probability that $F_{a}$ assigns to outcome $\omega \in \Omega$, so that $\sum_{\omega \in \Omega} F_{a, \omega}  =1$.
%
Each outcome $\omega \in \Omega$ has a reward $r_\omega \in [0,1]$ for the principal.
%
As a result, when an agent of selects an action $a \in A$, then the principal achieves an expected reward of $ \sum_{\omega \in \Omega} F_{a, \omega} \, r_\omega$.






In the standard model, the principal commits to a contract maximizing their expected utility.
%
A \emph{contract} specifies payments from the principal to the agent, which are contingent on the actual outcome achieved with the agent's action.
%
We formally define a contract by a vector $p\in \mathbb{R}_+^m$, whose components $p_\omega$ represent payments associated to outcomes $\omega \in\Omega$.
%
The assumption that payments are non-negative (\emph{i.e.}, they can only be from the principal to the agent, and \emph{not} the other way around) is known as \emph{limited liability} and it is common in contract theory~\cite{carroll2015robustness}.
%
When an agent of type $\theta \in \Theta$ selects an action $a \in A$, then the expected payment to the agent is $\sum_{\omega \in \Omega} F_{a, \omega} \, p_\omega$, while their utility is $\sum_{\omega \in \Omega} F_{a, \omega} \, p_\omega- \theta c_{ a}$.
%
On the other hand, the principal's expected utility in that case is $\sum_{\omega \in \Omega} F_{ a, \omega} \, r_\omega -\sum_{\omega \in \Omega} F_{ a, \omega} \, p_\omega$.
%, resulting in an expected social welfare of $R_{\theta,a}- c_{\theta,a}$.






Given a contract $p\in \mathbb{R}_{\ge 0}^m$, an agent of type $\theta \in \Theta$ plays a \emph{best response}, \emph{i.e.}, an action that is:
%
\begin{enumerate}
	\item \emph{incentive compatible} (IC), \emph{i.e.}, it maximizes their expected utility over actions in $A$; and
	%
	\item \emph{individually rational} (IR), \emph{i.e.}, it has non-negative expected utility (if there is no IR action, then the agent abstains from playing so as to preserve the \emph{status quo}).
\end{enumerate}




In the rest of this work, we make the following w.l.o.g. common assumption guaranteeing that IR is always enforced~\citep{dutting2019simple}.
%
This allows us to focus on IC only.
%
Intuitively, the following assumption ensures that each agent's type has always an action providing them with a non-negative utility, thus ensuring IR of any IC action.
%
\begin{assumption}\label{ass:ir}
	There exists an action $a \in A$ such that $c_{ a} = 0$.
\end{assumption} 


For a given contract $p$ we can compute the utility of an agent of type $\theta$ to play an action $a\in A$, as
\[
\Ua_\theta(p,a):=\sum_{\omega \in \Omega} F_{ a, \omega } p_\omega - \theta c_{a}.
\]


Formally, given a contract $p\in \mathbb{R}_+^m$, we denote by ${\Bcal}^\theta(p) \coloneqq \arg \max_{a \in A} \Ua_\theta(p,a)$ the set of \emph{best responses} of an agent of type $\theta \in \Theta$, \emph{i.e.}, given Assumption~\ref{ass:ir}, the set of all the actions that are IC for an agent of type $\theta$ under contract $p$. Moreover we will use the symbol ${\Bcal}^{\theta}_\epsilon(p)$ to denote the set of all $\epsilon$-best-responses to contract $p$ of an agent of type $\theta$,  \emph{i.e.}
\[
{\Bcal}^{\theta}_\epsilon(p)\coloneqq\left\{a\in A: \Ua_\theta(p,a) \ge \Ua_\theta(p,a')-\epsilon\quad\forall a'\in A\right\}.
\]
%


For convenience we define the value of the principal for a contract $p$, when action $a\in A$ is played as 
\[
\Up(p,a):= \sum_{\omega \in \Omega} F_{a, \omega} (r_\omega - \, p_\omega).
\]
As it is common in the literature (see, \emph{e.g.},~\citep{dutting2019simple}), we assume that the agent breaks ties in favor of the principal, selecting a best response that maximizes the principal's expected utility, \emph{i.e.}, for any contract $p\in \mathbb{R}_{\ge0}^m$, we have $b^\theta(p) \in \arg \max_{a \in \Bcal^{\theta}(p)} \Up(a,p)$.
Given an instance of our problem we define $\OPT:=\sup_{p\in\Reals_{\ge 0}^m}\Up(p)$ where $\Up(p)$ is the principal expected utility of contract $p$ and it is given by the following expression
\[
\Up(p):=\int_{\Theta}\Gamma(\theta) \Up(p,\theta) d\theta,
\]


where with slight abuse of notation we write $\Up(p,\theta)$ instead of $\Up(p,b^\theta(p))$ to be the utility of the principal against when the agent's realized type is $\theta$.
\end{comment}