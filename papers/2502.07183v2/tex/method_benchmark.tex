\subsection{Dataset Collection and Benchmark}
\label{sec:dataset_collection_and_benchmark}

\begin{table*}[t]
%\vspace{-8mm}
\caption{Summary of datasets}
%: the basis of the SAIT dataset and SA-Bench, focusing on landmarks and obstacles in the walking environment.
\setlength{\tabcolsep}{5pt}
\vspace{-3mm}
\centering
\begin{tabular}{|c|c|l|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Purpose} & \textbf{Annotations} &\textbf{\#classes}& \textbf{\#images} & \textbf{\#instances} \\
\hline
\multirow{2}{*}{VIN}& Pedestrian guidance& Door, Elevator, Escalator, Stairs, Pedestrian traffic light,&\multirow{2}{*}{7}&\multirow{2}{*}{7,493}&\multirow{2}{*}{25,480} \\
&landmark detection& Entrance of subway station, Subway ticket gate && & \\
\cline{1-6}
\multirow{5}{*}{SideGuide\cite{Kibaek2020}}&{Moving object}&Person, Stroller, Car, Wheelchair, Bus, Dog, Truck, &\multirow{5}{*}{29}&\multirow{5}{*}{349,741}&\multirow{5}{*}{3,367,063}\\
&detection& Cat, Bicycle, Carrier, Motorcycle, Movable signage, Scooter &&&\\
\cline{2-3}
& \multirow{2}{*}{Fixed object}& Tree trunk, Bus/Taxi stop, Potted plant, Kiosk, Traffic light,  &&&\\
&\multirow{2}{*}{detection}&Fire hydrant, Traffic sign, Parking meter, Pole, Bollard, Bench,  &&&\\
&&Barricade, Chair, Power controller, Table, Traffic light controller&&&\\
\hline
SAIT&Instruction-tuning&Goal position, Queries, Descriptions, Object classes \& locations, Path array&-&20,000&-\\
\hline
SA-Bench&Evaluation&Goal position, Descriptions, Path array, Passibility (go/stop)&-&1,000&-\\
\hline
\end{tabular}
\label{tab:dataset}
\vspace{-18pt}
\end{table*}
%\color{red}{with reason}

\vspace{-3pt}
\subsubsection{Visually Impaired Navigation (VIN) dataset}
We collected image data in a scenario reflecting everyday life where visually impaired individuals move from their homes to a destination using the subway as a representative means of public transportation.
Specifically, we captured 7,493 images along 75 distinct routes—starting at random apartment buildings, passing through subway stations, and ending at various destinations like the city hall civil affairs office—using an RGB camera positioned approximately 80 cm above the ground to match the height of the guide dog robot.
%Considering various times of day, weather, and seasonal changes, we collected data over six months.
In particular, crosswalks with pedestrian traffic lights, where visually impaired individuals may experience the most difficulty, were included in the routes to capture crossing street situations.
We defined essential pedestrian guidance landmarks for the visually impaired, captured images of them from various angles along planned routes, and annotated their bounding boxes as described in Table \ref{tab:dataset}.

\subsubsection{SideGuide dataset} Furthermore, to incorporate various moving and fixed obstacles in a wider variety of walking environments, we used a total 349,741 images in the pedestrian walking dataset with 3,367,063 object instances \cite{Kibaek2020} as described in Table \ref{tab:dataset}.

\subsubsection{Space-Aware Instruction Tuning (SAIT) dataset}
% 데이터를 생성해서 학습하려는 동기
% VLMs are known to have limited spatial understanding and reasoning abilities \cite{Mert2023}\cite{Shengbang2024}.
% If VLMs are used to explain the current situation to visually impaired individuals based on images, there is a risk of incorrect descriptions, such as confusing left and right or misunderstanding the intended path or direction. Therefore, if a dataset with accurate spatial descriptions were available, it could be used to train VLMs to develop better spatial understanding.
% However, 
To the best of our knowledge, there are currently limited image datasets captured from the pedestrian's viewpoint \cite{Kibaek2020}\cite{Karnan2022}\cite{Zhao2023}, and no dataset with precise spatial descriptions.
To address this, we composed a novel dataset with images captured from the pedestrian's viewpoint and their spatial descriptions to provide walking guidance to the visually impaired.
We sampled 20,000 images from the SideGuide to construct our SAIT dataset.
The dataset, designed to enable the VLM to understand images in a space-aware manner, contains object-centered descriptions of the destination, path, and left/right sides of the path, all based on a virtual path to a GP. 
It also provides walkability information based on these descriptions.
Landmark and obstacle data were provided by separate object detectors to improve dataset accuracy, and details are described in Section \ref{sec:pipeline}.
% Leveraging the obstacle information and images, we created an instruction-tuning dataset for training VLMs based on the auto-generation pipeline described in Section \ref{sec:pipeline}.

\subsubsection{Space-aware benchmark (SA-Bench)}
To evaluate the accuracy and conciseness of the guiding sentences generated by the instruction-tuned VLM, we created a benchmark set by extracting images from VIN and SideGuide that are mutually exclusive from the SAIT dataset.
The benchmark set consists of 1,000 images, including 500 `go' scenarios and 500 `stop' scenarios, determined based on the decision of whether the path to the GP is passable.
For example, even if there are no obstacles to a GP in an image, it is classified as a `stop' if the pedestrian traffic light is red.
For each image, five types of text descriptions describing the walking situation with a GP and path to it were manually annotated as described in Table \ref{tab:benchmark}.



\begin{table}[b]
\vspace{-5mm}
\caption{Annotations of the SA-Bench}
\vspace{-3mm}
\centering
\begin{tabular}{|c|c|l|}
\hline
\textbf{Category}&\textbf{Format}&\textbf{Content}\\
\hline
GP&(x, y)&Human-assigned position in an image\\
% 이미지 내의 갈 법한 곳을 랜덤하게 사람이 지정
\hline
\multirow{2}{*}{Path Array}&Array of&\multirow{2}{*}{Path to the GP}\\
&(x, y)&\\
% Path Array&Array of (x, y)&Path to the GP\\
%&(x, y)&\\
\hline
Dest.&Text&Description of the destination\\
\hline
Left&Text&Description of the left side of the path\\
\hline
Right&Text&Description of the right side of the path\\
\hline
Path&Text&Description of the path itself\\
\hline
\multirow{2}{*}{Reco.}&\multirow{2}{*}{Text}&Decision on whether the path is passable,\\
&& with a simple reason\\
\hline
\end{tabular}
\label{tab:benchmark}
\end{table}
