\vspace{-8pt}
\subsection{Training Dataset Generation Pipeline}
\label{sec:pipeline}
\vspace{-2pt}

\begin{figure*}[ht]
    \centering
%    \vspace{-8mm}    
    \includegraphics[width=\linewidth]{fig/pipeline_flowchart.png} % adjust width as necessary
    \vspace{-8mm}
    \caption{Our automatic dataset generation pipeline that focuses on the virtual path to the destination in 3D space and the surroundings. Given an input image and a goal position, we extract object locations and generate a depth map to identify walkable areas and surrounding environments, which are then utilized as inputs for a VLM. 
    Subsequently, we generate five distinct sentences through separate queries and integrate them to produce a recommendation statement on whether visually impaired individuals can traverse the specified path with a brief reason. }
    \label{fig:pipeline_flow}
    \vspace{-6mm}
\end{figure*}

% 자동 생성 과정에 대한 전체적인 구조
Overall process of our automatic dataset generation pipeline is presented in Fig.~\ref{fig:pipeline_flow}.

\subsubsection{Acquiring object and depth information}
\label{sec:object_detection}

To provide landmark information, as well as the locations of moving and stationary obstacles within the images, we trained YOLOv8 models \cite{Jocher_Ultralytics_YOLO_2023} on each of the two datasets, VIN and SideGuide. 
Due to the imbalance in the quantity of these two datasets and the exclusive labels of objects, we trained separate detectors. For each input image, we obtain 7 types of landmarks and 29 types of obstacles from the two object detectors, which are then added in input prompts for the VLM.
The distance information was estimated using Depth-Anything-V2, a monocular depth estimation algorithm \cite{depth_anything_v2}. 

\subsubsection{Goal setting and pathfinding from viewer position to destination}
\label{sec:GP_and_PP}
To describe an image based on the path, both the GP and the path to the GP must be provided. The reason for specifying the GP is that even with the same image, different GPs and paths result in different descriptions. 
In the Guide Dog Robot system, the GP is determined through global path planning. However, for generating training data, this approach is not feasible, so we randomly generated GPs in plausible locations within the image. The generation method assumes the bottom center of the RGB image as the starting point of an user, then randomly sets a direction within a 45-degree angle to the left or right, and finally places the GP 10 meters away from the starting point.
The path from the start point to the GP was assumed to be a straight line. 

\subsubsection{Masking and prompting VLM for descriptive information}
Since VLMs are known to understand coordinates \cite{liu2023llava}, the simplest way to obtain a description of the path to the GP is to provide an image along with a related prompt (e.g., ``Describe the path to the coordinates (x, y)''. However, we found that VLMs are unable to estimate a path based solely on GP coordinates, and they struggle with understanding the path and its left and right sides \cite{Mert2023}\cite{Shengbang2024}.
To address this challenge, instead of providing a single prompt describing the destination, the left and right sides of the path, and the path itself all at once, we input separate prompts for each area along with images showing only those specific sections, as illustrated in Fig.~\ref{fig:pipeline_flow}. 
To provide only the relevant regions of the image, we estimated the path area and masked either the left or right side of the path. This approach helped reduce confusion for the VLM when describing each section and improved accuracy. By splitting the prompts, we were able to overcome the issue of degraded performance in VLMs when handling long prompts \cite{Han2024lminfinite}\cite{levy2024tasktokensimpactinput}. 

Specifically, once the GP is determined, the path is calculated to have a width of 2 meters, centered on the ground, using the camera's intrinsic parameters and the depth values obtained in Section~\ref{sec:object_detection}. However, due to the inaccuracies in both the depth values and the camera's intrinsic parameters, corrections were made using the actual sizes of objects such as pavement blocks, cars, and people. The formula to calculate the left and right points of the path, $\mathbf{P_{i, left}, P_{i, right}}$, using the $i$-th point on the path at image coordinates $\mathbf{P_{i, path}} = (x_{i, path}, y_{i, path})$, depth value $z_i$, and the camera's focal length $\ell_x$ and $\ell_y$ is as follows.
\begin{equation}
% \vspace{-20pt}
\mathbf{p}_\text{i, left} = \begin{bmatrix} x_i -  \frac{2}{z_i}\ell_x \\ y_i \end{bmatrix}, \quad \mathbf{p}_\text{i, right} = \begin{bmatrix} x_i +  \frac{2}{z_i}\ell_x \\ y_i \end{bmatrix}
\end{equation}
The left area of the path is shown by displaying only the region formed by connecting the top-left corner, bottom-left corner, and the left points of the path, $\mathbf{p}_\text{i, left}$ of the image. Similarly, the right area is displayed by connecting the top-right corner, bottom-right corner, and the right points of the path, $\mathbf{p}_\text{i, right}$ to form a polygon. The path area is displayed by sequentially connecting the left points $\mathbf{p}_\text{i, left}$ and the right points $\mathbf{p}_\text{i, right}$. 
In these displayed areas, areas located a certain distance behind the destination are removed to exclude irrelevant background elements, such as the sky. 
% that are not necessary for the description.
% An example of these results is shown in Fig.~\ref{fig:pipeline_flow}. 
For the destination, since the VLM can understand coordinates, the original image is provided without masking. 
Different prompts are used for each area’s description as described in Fig. \ref{fig:pipeline_flow}.
The prompts include the class and location 
of the detected objects in Section~\ref{sec:object_detection}. Only the regions of objects that overlap with the non-masked area are provided.

\subsubsection{Path navigability decision using VLM descriptions}
Based on the path-centered descriptions obtained earlier, we queried the VLM to determine whether the path was walkable. Prior to this query, a full image description focused on crosswalks and traffic lights had already been included in the previous descriptions. Additionally, we incorporated rules related to safety (e.g., if the user is in front of a crosswalk with a red pedestrian signal or in a hazardous area, they should stop). 
%The same VLM, LLaVA, was used for this inference.
To summarize this process, the input for the VLM, $f$, for each image description $t_i$, can be formulated using the image $I$, mask $M$, object information $k$, query prompt $q$, and the concatenation operator `;'. In particular, the query prompt $q_{\text{dest}}$ contains the goal position $\mathbf{p}_{\text{goal}}$.
\begin{align}
\begin{split}
    I_i &= I \cap M_i, \quad k_i = k \cap M_i \\
    t_i &= f(I_i, (q_i ; k_i)), \quad i \in \{\text{left}, \text{right}, \text{path}\} \\
    t_i &= f(I, (q_i ; k)), \quad i \in \{\text{dest}, \text{desc}\} \\
    t_i &= f(I, (t_{\text{dest}} ; t_{\text{left}} ; t_{\text{right}} ; t_{\text{path}} ; t_{\text{desc}} ; q_i)),  i = \text{reco}
\end{split}
\end{align}
