\subsection{Comparison Experiments on SA-Bench}
\label{sec:comp_sota}
We conducted comparative experiments with our SA-VLM and a variety of VLMs of different sizes, including 7B, 13B, 34B, 72B, as well as the widely-used GPT-4o.
For GPT-4o, we utilized specific versions: gpt-4o-mini-2024-07-18 and gpt-4o-2024-08-06.
To ensure fair comparisons, we adopted the multi-turn query approach for methods other than SA-VLM because it demonstrated higher performance than the single-turn query approach, as in Section \ref{sec:single_vs_multi}.
Specifically, we first generated four types of sentences using in-context learning.
As required by SA-Bench in Table \ref{tab:benchmark}, these sentences describe the GP area (Dest.), the path to the GP (Path), and the left and right sides of the path (Left, Right), with four separate inferences.
Then, a full image description is generated once more. 
The five collected sentences are then used as input prompts for them.
Based on these prompts, a recommendation (Reco.) sentence is produced through a chain of thought (CoT) process to determine whether it is possible to reach the GP.
In short, our SA-VLM performs a single inference, while the other approaches require a total of six inferences to generate the five sentences required by SA-Bench. 
For our evaluation metric, we used the LLM Judge based on GPT-4o as explained in Section \ref{sec:eval}.

\setlength{\tabcolsep}{1pt}
\begin{table}[t!]
\caption{Performance comparisons on SA-Bench \\with SOTA algorithms using LLM-judge. Dest.: Destination, Reco.: Recommendation, Inf: Inference}
\vspace{-3mm}
\centering
\begin{tabular}{|l|c|c|c|c|c||c||c|c|}
\hline
\multirow{3}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Dest.}} & \multirow{2}{*}{\textbf{Left}}& \multirow{2}{*}{\textbf{Right}} & \multirow{2}{*}{\textbf{Path}} & \multirow{2}{*}{\textbf{Reco.}} & \multirow{2}{*}{\textbf{Avg.}} & \textbf{\# of} & \textbf{Inf.} \\
&&&&&&&\textbf{Words} & \textbf{Time}\\
\cline{2-9}
& \multicolumn{6}{c||}{\(\uparrow\) Higher is better} & \multicolumn{2}{c|}{\(\downarrow\)} \\
\hline
\hline
llava-v1.6-vicuna-7b\cite{liu2023improved} & 5.14 & 2.64 & 2.81 & 1.85 & 3.57 & 3.20 &12.75 & 11.66\\ \hline
llava-v1.6-vicuna-13b\cite{liu2023improved} & \textbf{5.25} & 1.44 & 1.22 & 2.11 & 4.07 & 2.82 &14.12 & 15.80 \\ \hline
llava-v1.6-34b\cite{liu2023improved} & 4.49 & 3.03 & 2.99 & 2.43 & 4.70 &3.53& 12.56 & 51.89 \\ \hline
llava-next-72b\cite{li2024llavanext-strong} & 4.57 & 2.44 & 2.46 & 3.41 & 4.90 &3.55& 11.96 & 46.63 \\ \hline
gpt-4o-mini\cite{openai2024gpt4technicalreport} & 3.32 & 2.56 & 2.47 & 2.83 & 4.31&3.10 & 14.29 & 34.66 \\ \hline
gpt-4o\cite{openai2024gpt4technicalreport} & 3.51 & 3.14 & 3.19 & \textbf{3.66} & 4.39 & 3.58&12.11 & 40.86 \\ \hline
\hline
\textbf{SA-VLM-7b (ours)} & 4.43 & \textbf{3.36} & \textbf{3.25} & 2.64 & \textbf{4.93}&\textbf{3.72} & \textbf{11.95} & \textbf{6.81} \\ 
\hline
\end{tabular}
\label{tab:comparison}
\vspace{-8pt}
\end{table}

\setlength{\tabcolsep}{2pt}
\begin{table}[t!]
% \vspace{-10pt}
\caption{Performance comparison on SA-Bench \\with the base model using LLM-judge}
\vspace{-3mm}
\label{tab:with_train_performance}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Dest.}} & \multirow{2}{*}{\textbf{Left}} & \multirow{2}{*}{\textbf{Right}} & \multirow{2}{*}{\textbf{Path}} & \multirow{2}{*}{\textbf{Reco.}}& \textbf{\# of} & \textbf{Inf.} \\
&&&&&&\textbf{Words}&\textbf{Time}\\
\hline
\hline
llava-ov-qwen2-7b-si \cite{li2024llavaonevisioneasyvisualtask} & 3.55 & 2.43 & 2.29 & 2.25 & 4.05 & 14.21 & 20.39\\ 
\hline
\textbf{SA-VLM-7b (ours)} & \textbf{4.43} & \textbf{3.36} & \textbf{3.25} & \textbf{2.64} & \textbf{4.93} & \textbf{11.95} & \textbf{6.81} \\ 
\hline
\end{tabular}
\vspace{-20pt}
\end{table}

As shown in Table \ref{tab:comparison}, our SA-VLM demonstrated superior average performance across five types of responses, even surpassing larger models such as GPT-4o.
This is because general VLMs have not yet effectively learned space-related concepts, making it difficult to distinguish between left and right. 
Thus, they exhibited lower performance in responses of the Left and Right type.
This weakness also impacts CoT reasoning when generating the Reco. sentence.
Interestingly, smaller models such as LLaVA-v1.6-Vicuna 7B and 13B performed better in the Dest. type compared to larger models but exhibited lower performance in other types. 
This discrepancy can be attributed to the nature of SA-Bench, which naturally includes walking environments with a relatively large number of sidewalks.
Smaller models tend to generate biased responses or select the first example from the query prompt, such as `The destination is ahead on the sidewalk,' without adequately analyzing the image. This behavior artificially inflated the score for the Dest. type, while reducing accuracy in other types.

The model shows the best performance in terms of average word count and inference time across five sentence types.
This means that visually impaired individuals can receive walking guidance that is more concise and faster, allowing them to navigate more efficiently without unnecessary detail or complexity.
Furthermore, we also measured performance of our base model, LLaVA-OneVision-7b.
As shown in Table \ref{tab:with_train_performance}, the results clearly demonstrate that training was effective, improving performance across all aspects.

Lastly, we evaluated the key approaches using METEOR, BERTScore, and ROUGE-L metrics.
As shown in Fig. \ref{fig:metric_comp}, both llava-next-72b and our model achieved the best performance across all metrics. However, ROUGE-L and METEOR were not fully suitable as evaluation metrics for SA-Bench, since they prioritize word overlap rather than the semantic meaning of sentences. 
BERTScore also exhibited an issue, where it rated semantically opposite words like ``go'' and ``stop'' similarly. 
This error arises because these words are more related to navigation, leading to high embedding similarity and inflated scores.
In contrast, LLM Judge provided more reasonable evaluations by assessing sentences based on their contextual meaning, offering a more accurate reflection of performance in SA-Bench.

\begin{figure}[b!]
    \centering
    \vspace{-15pt}
    \includegraphics[width=\linewidth]{fig/metric_comp2.png} % adjust width as necessary
    \vspace{-25pt}
    \caption{Comparisons of algorithms across different metrics for the generated `Reco.' sentences. We chose LLM Judge as our main metric because it provides scoring based on the contextual meaning of the generated sentences.}
    \label{fig:metric_comp}
    % \vspace{-5mm}
\end{figure}

