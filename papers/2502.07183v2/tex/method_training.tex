\begin{figure*}[t]
  % \vspace{-8mm}
  \centering
  \includegraphics[width=\linewidth]{fig/conv_template3.png}
    \vspace{-8mm}
    \caption{An image and annotations of the SAIT dataset: queries were generated based on the goal positions, and answers were structured in XML format. Note that words like \texttt{<dest\_desc>} are plain text in the answer, with only the image token in the query treated as a special token.}
  \label{fig:conv_template}
  \vspace{-5mm}
\end{figure*}
\subsection{Training} 
\vspace{-2pt}
Utilizing constructed the SAIT dataset, the automatically generated walking guidance annotations were transformed into a conversation format for VLM training.
As illustrated in Fig. \ref{fig:conv_template}, the query includes the image token and the GP, while the answer contains five types of descriptions and the path array to the GP in XML format.
In other words, given a query containing a GP, our instruction-tuned Space-Aware-VLM (SA-VLM) can predict five descriptions and a path to the GP in a single inference.
Note that the words such as \texttt{<dest\_desc>} are not special tokens but plain text.
