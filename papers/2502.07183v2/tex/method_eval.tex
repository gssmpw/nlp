\vspace{-5pt}
\subsection{Evaluation Criteria}
\label{sec:eval}
\subsubsection{LLM Judge}
% LLM-as-a-judge
LLM Judge (referred to as LLM-as-a-judge in \cite{zheng2023llmjudging}) is a proposed method for evaluating LLM responses to open-ended questions, involving the use of structured prompts with strong LLMs \cite{zheng2023llmjudging}. 
% 뭔지 무엇을 찾아내고, 어떻게 측정하는지
With a strong LLM as a judge, the prompt is structured to compare the reference answer with the assistant's answer, identifies any errors, corrects them, and then assigns a score between 1 and 10 based on this comparison.
This approach achieved a rating agreement level equivalent to human evaluations.
In our experiment, we adapted the prompt for reference-guided multi-turn single-answer grading from \cite{zheng2023llmjudging} by modifying it to a single-turn format and replacing conversation-related terms with description-related ones.
% The average number of words used in the response was also included as a metric to assess the conciseness of the answers.


\subsubsection{Text summarization metrics: }
%METEOR, BERTScore, ROUGE
To assess the quality of the generated text, we also utilize three widely-used metrics: METEOR\cite{banerjee2005meteor}, ROUGE\cite{lin2004rouge}, and BERTScore\cite{zhang2019bertscore}. These metrics capture different dimensions of similarity between the generated outputs and reference texts, ensuring a thorough evaluation.
