\section{Introduction}
Guide dog robots have the potential to significantly improve mobility and safety for the visually impaired, with continued research driving their advancement \cite{wei2014guide}\cite{xiao2021robotic}\cite{hong2024collaborative}\cite{saegusa2010development}.
Unlike living guide dogs, robots are expected to overcome limitations in perceptual intelligence, such as a restricted ability to understand their surroundings and the one-way nature of communication \cite{Hochul2024}. 
Recent advancements in Vision-Language Models (VLMs), which enable natural language scene descriptions, allow robots to more effectively convey critical situational information \cite{chen2020uniter}\cite{alayrac2022flamingo}\cite{you2023ferret}\cite{li2023blip}.
To enhance safe decision-making for blind and low-vision individuals at complex street crossings, the use of VLMs is explored to interpret scenes and generate safety scores and descriptions in natural language \cite{hochul2024crossstreet}.
As a dataset focused on walking environments for the visually impaired, the SideGuide Dataset \cite{Kibaek2020} offers images from these scenarios, along with instance-level annotations for object detection and segmentation, and dense disparity maps.
Visually Impaired Assistance with Large Models \cite{zhao2024vialmsurveybenchmarkvisually} defines a task for providing step-by-step guidance to the visually impaired using VLMs and proposes a benchmark for evaluating various state-of-the-art models. 
However, the benchmark is restricted to environments such as homes and supermarkets, which are not as critical as walking environments for the visually impaired.

\begin{figure}[t]
    \centering
    % \vspace{-3pt}
    \includegraphics[width=\linewidth]{fig/intro.png} % adjust width as necessary
    \vspace{-15pt}
    \caption{Our space-aware instruction tuning method provides helpful and concise walking guidance to individuals with visual impairments.
   Given an image and a specified goal position as input, our method identifies a virtual path in 3D space and provides compact responses to five types of queries in a single inference: descriptions of the destination, the left and right sides of the path, the path itself, and a decision on whether the path is traversable, with a brief reason.    
   }
    \label{fig:intro}
    \vspace{-20pt}
\end{figure}

Although VLMs exhibit rich descriptive capabilities for images due to extensive pre-training on numerous images and a text corpus, they often struggle with interpreting spatial information such as spatial relationships and object positions \cite{chen2024spatialvlm}.
% This limitation restricts their ability to effectively understand and utilize these spatial aspects within the visual data \cite{Mert2023}\cite{Shengbang2024}.
The study \cite{Mert2023} has shown that VLMs frequently fail to accurately represent simple spatial relationships such as ``to the right of'' or ``behind'' in images, due to limitations in the CLIP vision encoder. 
For instance, when VLMs asked to distinguish between ``on the right of'' and ``on the left of'' in an image where a dog is positioned to the right of a tree, VLMs often perform at chance level. 
In VQA tests, similar failures of VLMs were observed in handling orientation and direction, e.g., identifying whether a rabbit is facing right or left, and positional and relational context, e.g., identifying whether glasses are on the right or left of a slipper \cite{Shengbang2024}. 
Even high-performance models, such as Gemini \cite{googleGemini} and GPT-4 \cite{openai2024gpt4technicalreport}, faced these challenges.
The highest accuracy in orientation tasks was only approximately 25\%, while positional and relational context tasks achieved around 45\% accuracy.

In this paper, we present the Space-Aware Instruction Tuning (SAIT) dataset, designed to help guide dog robots provide precise and effective walking guidance to the visually impaired. 
We also propose a novel automatic data generation pipeline that is applicable to any images in a walking environment. 
To integrate spatial awareness into the dataset, the pipeline explicitly extracts a depth map and identifies a virtual path to the goal position in 3D space. 
By showing only the relevant regions to the VLMs, it addresses the common challenge of distinguishing directions in standard VLMs.
To evaluate the model's ability to convey concise and meaningful information to the visually impaired, we manually annotated and created the Space-Aware Benchmark (SA-Bench) with an evaluation protocol.

Our key contributions are as follows:

\begin{itemize}
   \item[1)] We release the SAIT dataset and SA-Bench, along with an evaluation protocol to assess how effectively a VLM provides walking guidance for the visually impaired.
   \item[2)] We propose a novel data generation pipeline for automatically creating data to facilitate the understanding of three-dimensional space.
   \item[3)] We demonstrated the efficacy through comparisons experiments between the space-aware instruction-tuned model and other state-of-the-art algorithms.
\end{itemize}


