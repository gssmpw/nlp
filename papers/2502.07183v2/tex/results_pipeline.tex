\subsection{Pipeline Evaluation}
We evaluated effectiveness of the pipeline by applying it on SA-Bench. The GP is provided by the SA-Bench, and the resulting descriptions were compared with the ground truth.

\subsubsection{Single-turn query vs Multi-turn query}
\label{sec:single_vs_multi}
When querying a VLM for scene descriptions, one can easily consider structuring the desired information into a single-turn query, where all relevant details are asked in one prompt.
We compared the single-turn query approach with a multi-turn query method, used in our pipeline, in terms of performance and speed.
In our experiment, we structured the single-turn query prompt so that numbered questions for each region and final recommendation were input into VLMs, and the answers were returned in a corresponding numbered format.

As shown in Table~\ref{table:one_turn}, the multi-turn query approach resulted in more accurate descriptions across four types compared to the single-turn method except for the Dest. type. 
This pattern of the single-turn query results mirrors that of the smaller model in Table~\ref{tab:comparison}, where the model's reasoning abilities declined. 
Upon analyzing the results, we found that the single-turn query often selected answers from examples in the query prompt rather than analyzing the image.
This aligns with previous findings that longer prompts in single-turn query can weaken reasoning abilities, as highlighted in prior studies \cite{Han2024lminfinite}\cite{levy2024tasktokensimpactinput}. 
In terms of processing speed and description length, the single-turn method performed better, but the difference was not significant. 
This is because, in VLMs, response time scales with the length of the input and output text, and there was no significant difference in the total number of output words between single-turn and multi-turn queries.

\begin{table}[!ht]
    \vspace{-10pt}
    \caption{Performance comparisons on SA-Bench with single-turn query and multi-turn query using LLM Judge}
    \vspace{-7pt}
    \centering
    \begin{tabular}{|l|c|c|c|c|c||c|c|}
    \hline
        \textbf{Methods}         & \textbf{Dest.}  & \textbf{Left} & \textbf{Right} & \textbf{Path} & \textbf{Reco.} & \textbf{\# Words} & \textbf{Inf. Time} \\ \hline\hline
        Multi-turn     & 4.49 & \textbf{3.03} & \textbf{2.99} & \textbf{2.43} & \textbf{3.53} & 12.56 & 51.89 \\ \hline
        Single-turn    & \textbf{4.91} & 1.46 & 1.45 & 1.71 & 2.72 & \textbf{10.54} & \textbf{34.67} \\ \hline
    \end{tabular}
    \label{table:one_turn}
    \vspace{-8pt}
\end{table}

\subsubsection{Masking region vs Prompting region} 
This experiment examines whether masking the image to a specific spatial region helps the VLM better understand the target of the description. 
The targets for description include the destination represented by coordinates, the path represented by the starting and goal points, and nearby objects located to the left and right of the path.
In the region-masked method, only the area corresponding to the target is visible, with the rest of the image masked, while a standard prompt is provided to the VLMs.
In the region-prompted method, the target area information, such as coordinates, is included directly in the prompt, and the entire image is presented to the VLMs.
Table~\ref{table:masked_prompted} presents the performance of each target area for the two methods on the LLM Judge and the difference between the two methods was pronounced.

\begin{table}[!ht]
    \vspace{-8pt}
    \caption{Performance comparisons on SA-Bench with masked-image and region-prompt methods using LLM Judge}
    \centering
    \vspace{-3mm}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
        \textbf{Methods} &\textbf{ Dest.}  & \textbf{Left} & \textbf{Right} & \textbf{Path} \\ \hline\hline
        Masked-Image    & 3.53 & \textbf{3.26} & \textbf{3.20} & \textbf{2.93} \\ \hline
        Region-Prompt  & \textbf{4.52} & 3.09 & 2.88 & 2.29 \\ \hline
        %Masked-Image (BERTScore)  & 0.6803 & \textbf{0.7680} & \textbf{0.7675} & 0.6821 \\ \hline
        %Region-Prompt (BERTScore)& \textbf{0.7399} & 0.7674 & 0.7636 & \textbf{0.6857} \\ \hline
    \end{tabular}
    \label{table:masked_prompted}
    \vspace{-8pt}
\end{table}

For destination descriptions, the region-prompt method using the full image and coordinates information in the prompt improved performance. On the other hand, the masked-region method using masked images with ordinary prompt enhanced the VLM's ability to describe the left, right, and path areas. This is because VLMs and GPT-based models are known to understand and match coordinates with images, making it possible to generate accurate descriptions based on coordinates alone \cite{liu2023llava}. However, since the models lack the ability to set paths and describe the left, right, and path areas based on those paths, it was found to be more effective to use masking to show only the relevant areas and obtain descriptions for them.
