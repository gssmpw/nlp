\section{Experiments}
In this section, we conduct comparative experiments to demonstrate the impact of our SAIT dataset on SA-Bench described in \ref{sec:dataset_collection_and_benchmark}. 
We also perform experiments to validate the effectiveness of the pipeline described in Section \ref{sec:pipeline}.

\vspace{-5pt}
\subsection{Experiment Details}
For our SA-VLM, we selected LLaVA-OneVision \cite{li2024llavaonevisioneasyvisualtask}  as our base model, a VL multimodal model to improve comprehension in complex visual scenarios.
We fine-tuned the 7B model across all components: the vision encoder, MLP adapter, and the large language model on our SAIT dataset with a batch size of 64 for one epoch. 
The learning rate was set to 1e-5 with a cosine scheduler using 8 NVIDIA RTX 6000 Ada GPUs for around 4 hours.
For our automatic generation pipeline, we used LLaVA-v1.6-34b \cite{liu2023improved} without additional fine-tuning. 
Including object detection, depth estimation, masking, and generating descriptions, the process took about 62 seconds per image, using 4 NVIDIA RTX 6000 Ada and 4 Quadro RTX 6000 GPUs.

\input{tex/results_comparison}
\input{tex/results_pipeline}