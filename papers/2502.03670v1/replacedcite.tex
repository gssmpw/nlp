\section{Literature Review}
\label{sec:Methodology}
\subsubsection{Deep Learning in Partial Differentiable Equations}
Recent advances have been made in the numerical estimation of PDEs, particularly in the field of deep learning ____. Of these advances, a notable contribution is the Deep Galerkin Method (DGM) ____. DGM creates a neural network that approximates the solution to the PDE. However, it does not require the true output of the PDE because DGM works by creating a special loss function based on each PDE, where the loss function works by minimizing the residual of the PDE. For example, consider the Poisson PDE:
\begin{equation*}
    -\Delta u = f(x)
\end{equation*}
where \(u\) is the function we want to solve for and \(f(x, y)\) is some known source term that depends on the spatial variable  \(x\). The loss function would be presented as:
\begin{equation*}
    \mathcal{L} = \left\| f(x) + \Delta u \right\|^2_{\Omega \times [0,T]}
\end{equation*}
where the goal of the loss function is to get this residual as close to \(0\) as possible in the least square sense. Thus, to train the model, it is only required to simulate different values of \(x\). As mentioned previously, with boundary or initial conditions, infinitely many solutions exist. To address this, DGM incorporates these conditions into the loss function. For example, consider the equation in two dimensions with the Dirichlet boundary conditions, the boundary conditions become:
\begin{equation*}
    u(x, y) = 0 \quad \text{for} \quad (x, y) \in \partial\Omega
\end{equation*}
then the updated loss function to handle these becomes:
\begin{equation*}
    \mathcal{L} = \left\| f(x) + \Delta u \right\|^2_{\Omega \times [0,T]} + \lambda \left\| u(x, y) \right\|^2_{\partial\Omega \times [0,T]}
\end{equation*}
where \( \lambda \) is a penalty parameter that controls the trade-off between minimizing the residual of the PDE and satisfying the boundary conditions. 

To further DGM, Liyao Lyu, \textit{et al.}. developed  the deep mixed residual method (MIM) ____. MIM works similarly to DGM where the goal is to construct a model that minimizes the residual of the PDE in a least-square sense. The main difference is that MIM introduces auxiliary variables for higher-order derivatives, expressing them as single variables. In addition, in MIM these auxiliary variables are also incorporated into the model, which allows the boundary and initial conditions to be integrated directly into the model itself and omits them from the loss function ____. Continuing with the Poisson equation, the following auxiliary variable is introduced:
\begin{align*}   
    p &= \nabla u \\
\end{align*}
These auxiliary variables are then minimized as well in the loss function, which is defined as
\begin{equation*}
    \mathcal{L} = \left\| f(x) + \nabla \cdot p \right\|^2_{\Omega \times [0,T]} +\left\| p - \nabla u \right\|^2_{\Omega \times [0,T]}.
\end{equation*}
allowing for stricter enforcement of boundary and initial conditions which are reliant on the gradient for \(u\). 

\subsubsection{Deep Learning in Stochastic Partial Differentiable Equations}
Recent developments in the intersection of deep learning and SPDEs have introduced innovative methodologies to estimate SPDE solutions. Salvi \textit{et al.} propose the neural SPDE model, which generalizes neural controlled differential equations (CDE) and neural operators ____. This model offers resolution-invariant learning of solution operators for SPDEs, achieving efficiency by leveraging Fourier space representations. The neural SPDE approach enables evaluations through spectral Galerkin schemes or fixed-point methods, showcasing superior adaptability to SPDEs such as the stochastic Navier-Stokes and Ginzburg-Landau equations. In particular, the model requires significantly fewer training data and is up to three orders of magnitude faster than traditional solvers, making it a promising framework for learning spatio-temporal dynamics with random perturbations.

Karumuri \textit{et al.} present a simulator-free approach to solving elliptic SPDEs in high dimensions by parameterizing the solutions using Deep Residual Networks (ResNets) ____. Their methodology employs a physics-informed loss function derived from energy principles, bypassing the need for deterministic solvers. This approach effectively propagates uncertainty in high-dimensional settings and demonstrates robust performance in both forward and inverse problems. Their work highlights the capability of neural networks to handle the curse of dimensionality in SPDEs, particularly under stochastic parameter spaces.

In another advancement, Beck \textit{et al.} introduce a deep learning algorithm for SPDEs, such as the stochastic heat equation and the Zakai equations ____. Their model uses separate neural networks for each realization of the noise process, achieving high accuracy even in 50-dimensional problems. This work emphasizes the efficient handling of additive and multiplicative noise, demonstrating the versatility of deep learning in high-dimensional filtering and stochastic modeling tasks.

Zhang \textit{et al.} extend Physics-Informed Neural Networks (PINNs) to SPDEs using modal space decomposition through dynamic orthogonal (DO) and biorthogonal (BO) methodologies ____. By incorporating these constraints into the loss function, their framework overcomes the limitations of traditional DO/BO methods, such as eigenvalue crossing or covariance matrix invertibility. The NN-DO/BO approach effectively handles forward and inverse problems with noisy initial data and high-dimensional stochastic inputs, offering a flexible and robust method for solving SPDEs.