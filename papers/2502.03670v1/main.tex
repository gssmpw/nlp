\documentclass[lettersize,journal]{IEEEtran}
%\documentclass[9pt,technote]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{booktabs}
 \usepackage{multirow}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Chaos into Order: Neural Framework for Expected Value Estimation of Stochastic Partial Differential Equations}

\author{Ísak~Pétursson
        and~María~Óskarsdóttir
         % <-this % stops a space
\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem Í. Pétursson and M. Óskarsdóttir are with Department of Computer Science, Reykjavík University, Menntavegi 1, 102 Reykjavík, Iceland \protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: isak23@ru.is, mariaoskars@ru.is

\IEEEcompsocthanksitem M. Óskarsdóttir is with the School of Mathematical Sciences, University of Southampton, University Road, SO17 1BJ, Southampton, United Kingdom \protect\\
Email: m.oskarsdottir@soton.ac.uk} % <-this % stops a space
\thanks{Manuscript received \today}}
        % <-this % stops a space


% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.

 % <-this % stops a space
%\thanks{Manuscript received \today}}
        % <-this % stops a space
%\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Stochastic Partial Differential Equations (SPDEs) are fundamental to modeling complex systems in physics, finance, and engineering, yet their numerical estimation remains a formidable challenge. Traditional methods rely on discretization, introducing computational inefficiencies, and limiting applicability in high-dimensional settings. In this work, we introduce a novel neural framework for SPDE estimation that eliminates the need for discretization, enabling direct estimation of expected values across arbitrary spatio-temporal points. We develop and compare two distinct neural architectures: Loss Enforced Conditions (LEC), which integrates physical constraints into the loss function, and Model Enforced Conditions (MEC), which embeds these constraints directly into the network structure. Through extensive experiments on the stochastic heat equation, Burgers' equation, and Kardar-Parisi-Zhang (KPZ) equation, we reveal a trade-off: While LEC achieves superior residual minimization and generalization, MEC enforces initial conditions with absolute precision and exceptionally high accuracy in boundary condition enforcement. Our findings highlight the immense potential of neural-based SPDE solvers, particularly for high-dimensional problems where conventional techniques falter. By circumventing discretization and explicitly modeling uncertainty, our approach opens new avenues for solving SPDEs in fields ranging from quantitative finance to turbulence modeling. To the best of our knowledge, this is the first neural framework capable of directly estimating the expected values of SPDEs in an entirely non-discretized manner, offering a step forward in scientific computing.
\end{abstract}

\begin{IEEEkeywords}
Stochastic partial differential equations, Physics informed neural networks, Neural-based solvers, Unsupervised learning.
\end{IEEEkeywords}


\section{Introduction}
% The introduction section includes a review of the existing literature to position your research within the broader scientific field and to show the novelty of your work.  The introduction should also describe the question you’re trying to answer with your research and why that question is important to the field.

\IEEEPARstart{P}{artial} differentiable equations (PDEs) are ubiquitous in modern science. They are used in various contexts, such as physics \cite{pde-physics}, engineering \cite{pde-eng}, chemistry \cite{pde-chem}, biology, and finance \cite{pde-fin}. To obtain a unique solution for a PDE, initial conditions (for time-dependent problems) and boundary conditions (for spatial variables) are typically required. Without these, the PDE may have infinitely many possible solutions. Some PDEs have closed from solution under certain boundaries and/or initial conditions such as the Black-Scholes equation \cite{bs, bs2}, the heat equation \cite{heat}, and the wave equation \cite{wave}. However, some PDEs have no analytical solution, such as the Heston stochastic volatility model \cite{heston}, the Fokker-Planck equation \cite{fokker}, or the Boltzmann equation \cite{boltzmann}. When dealing with PDEs that do not have an analytical solution, they must be estimated using methods such as Monte Carlo simulations \cite{mc}, the finite difference method \cite{finite-diff}, and the finite element method \cite{finite-ele}. 

A special class of PDEs is stochastic partial differential equations (SPDEs), which extend PDEs by incorporating randomness, typically in the form of white noise \cite{white-noise}. While PDEs describe deterministic relationships—where a given initial and boundary condition uniquely determines the solution. SPDEs introduce stochastic forcing terms, leading to solutions that are not single functions but random processes evolving over space and time. This added randomness makes SPDEs substantially more complex, as their solutions often require probabilistic interpretation, such as expectations, variances, or distributions, rather than explicit deterministic functions \cite{spde-hard}. SPDEs are more difficult to solve, but certain equations under specific conditions have analytical solutions, such as the stochastic heat equation \cite{heat-sol}. It is important to note that when analytical solutions exist, they are usually under highly constrained conditions, and most SPDEs under most conditions do not have analytical solutions \cite{rare-sol}. The same numerical methods used to estimate PDEs, such as the finite difference method or Monte Carlo simulations, can be applied to SPDEs, but they are more complex due to the need to discretize the SPDE and handle the stochastic elements \cite{spde-fe, spde-mc, spde-fi}.

In recent years, neural networks have become powerful tools for solving PDEs and aspects of their stochastic counterparts (SPDEs). Among the most prominent approaches are Physics-Informed Neural Networks (PINNs), which approximate solutions by incorporating the governing equations and boundary conditions directly into the loss function \cite{pinn}. This allows the network to optimize for both data fidelity and physical consistency simultaneously. The Deep Galerkin Method (DGM) further refines this idea by formulating the loss as the residual of the PDE, using a point-wise sampling approach to avoid discretization of the solution domain \cite{DGM}. Building on DGM, the Mixed Residual Method (MIM) introduces auxiliary variables for higher-order derivatives, integrating boundary conditions directly into the network architecture instead of the loss function \cite{MIM-PDE-NN}. These methodologies highlight the potential of neural networks to solve complex high-dimensional PDEs, which forms a foundation for extending such approaches to SPDEs. In regards to SPDEs, several approaches have been explored, including Fourier-based neural models for resolution-invariant learning \cite{nn-spde-3}, simulator-free deep residual networks for high-dimensional uncertainty propagation \cite{nn-spde-4}, and neural solvers designed to handle stochastic noise realizations efficiently \cite{nn-spde-2}. While these methods have improved scalability and efficiency, they often rely on discretization or predefined noise samples, limiting their flexibility in generalizing to arbitrary spatio-temporal domains \cite{nn-spde-1}. 


In this paper, we build upon these advancements by proposing a novel framework for directly estimating the expected values of SPDEs, rather than solving for the full distribution or operator. While methods such as neural SPDEs and ResNets rely on discretization or specific domain constraints, our approach operates without discretization, enabling application to arbitrary spatio-temporal points. We demonstrate the framework on three well known, and inreasingly more complex, SPDEs, namely the stochastic heat equation, Burgers' equation, and the Kardar-Parisi-Zhang (KPZ) equation, comparing the performance of our methods across various dimensions. Another key contribution of this work is the ability to estimate the expected value of SPDEs eliminating the need to directly incorporate noise realizations as inputs to the model. Unlike prior approaches that require specific noise samples to predict pathwise solutions, our methodology estimates the expected value rather than individual realizations, thereby avoiding the computational overhead and complexity associated with modeling explicit stochastic paths. To the best of our knowledge, this is the first work to directly estimate the expected values of SPDEs using neural networks while also employing non-discretized methods, providing a significant contribution to the literature. 

The rest of this paper is structured as follows. In the next section, we provide a comprehensive literature review covering both classical and recent deep learning methods for solving PDEs and SPDEs. In Section \ref{sec:Methodology}, we present our proposed methodology, detailing the mathematical formulation of the SPDEs under consideration and introducing the loss enforced conditions (LEC) and model enforced conditions (MEC) frameworks. Section \ref{sec:Results} then reports our experimental results, including a comparative analysis of the LEC and MEC approaches in terms of residual accuracy, boundary/initial condition enforcement, and computational performance. In Section \ref{sec:Discussion}, we discuss the implications of our findings, the trade-offs between LEC and MEC, and potential avenues for improving training stability and scalability. Finally, Section \ref{sec:Conclusion} concludes the paper by summarizing our contributions and suggesting future research directions in applying deep learning to high-dimensional stochastic problems. 

\subsection{Literature Review}\label{sec:Methodology}
\subsubsection{Deep Learning in Partial Differentiable Equations}
Recent advances have been made in the numerical estimation of PDEs, particularly in the field of deep learning \cite{PDE-Meta}. Of these advances, a notable contribution is the Deep Galerkin Method (DGM) \cite{DGM}. DGM creates a neural network that approximates the solution to the PDE. However, it does not require the true output of the PDE because DGM works by creating a special loss function based on each PDE, where the loss function works by minimizing the residual of the PDE. For example, consider the Poisson PDE:
\begin{equation*}
    -\Delta u = f(x)
\end{equation*}
where \(u\) is the function we want to solve for and \(f(x, y)\) is some known source term that depends on the spatial variable  \(x\). The loss function would be presented as:
\begin{equation*}
    \mathcal{L} = \left\| f(x) + \Delta u \right\|^2_{\Omega \times [0,T]}
\end{equation*}
where the goal of the loss function is to get this residual as close to \(0\) as possible in the least square sense. Thus, to train the model, it is only required to simulate different values of \(x\). As mentioned previously, with boundary or initial conditions, infinitely many solutions exist. To address this, DGM incorporates these conditions into the loss function. For example, consider the equation in two dimensions with the Dirichlet boundary conditions, the boundary conditions become:
\begin{equation*}
    u(x, y) = 0 \quad \text{for} \quad (x, y) \in \partial\Omega
\end{equation*}
then the updated loss function to handle these becomes:
\begin{equation*}
    \mathcal{L} = \left\| f(x) + \Delta u \right\|^2_{\Omega \times [0,T]} + \lambda \left\| u(x, y) \right\|^2_{\partial\Omega \times [0,T]}
\end{equation*}
where \( \lambda \) is a penalty parameter that controls the trade-off between minimizing the residual of the PDE and satisfying the boundary conditions. 

To further DGM, Liyao Lyu, \textit{et al.}. developed  the deep mixed residual method (MIM) \cite{MIM-PDE-NN}. MIM works similarly to DGM where the goal is to construct a model that minimizes the residual of the PDE in a least-square sense. The main difference is that MIM introduces auxiliary variables for higher-order derivatives, expressing them as single variables. In addition, in MIM these auxiliary variables are also incorporated into the model, which allows the boundary and initial conditions to be integrated directly into the model itself and omits them from the loss function \cite{mim-cont}. Continuing with the Poisson equation, the following auxiliary variable is introduced:
\begin{align*}   
    p &= \nabla u \\
\end{align*}
These auxiliary variables are then minimized as well in the loss function, which is defined as
\begin{equation*}
    \mathcal{L} = \left\| f(x) + \nabla \cdot p \right\|^2_{\Omega \times [0,T]} +\left\| p - \nabla u \right\|^2_{\Omega \times [0,T]}.
\end{equation*}
allowing for stricter enforcement of boundary and initial conditions which are reliant on the gradient for \(u\). 

\subsubsection{Deep Learning in Stochastic Partial Differentiable Equations}
Recent developments in the intersection of deep learning and SPDEs have introduced innovative methodologies to estimate SPDE solutions. Salvi \textit{et al.} propose the neural SPDE model, which generalizes neural controlled differential equations (CDE) and neural operators \cite{nn-spde-3}. This model offers resolution-invariant learning of solution operators for SPDEs, achieving efficiency by leveraging Fourier space representations. The neural SPDE approach enables evaluations through spectral Galerkin schemes or fixed-point methods, showcasing superior adaptability to SPDEs such as the stochastic Navier-Stokes and Ginzburg-Landau equations. In particular, the model requires significantly fewer training data and is up to three orders of magnitude faster than traditional solvers, making it a promising framework for learning spatio-temporal dynamics with random perturbations.

Karumuri \textit{et al.} present a simulator-free approach to solving elliptic SPDEs in high dimensions by parameterizing the solutions using Deep Residual Networks (ResNets) \cite{nn-spde-4}. Their methodology employs a physics-informed loss function derived from energy principles, bypassing the need for deterministic solvers. This approach effectively propagates uncertainty in high-dimensional settings and demonstrates robust performance in both forward and inverse problems. Their work highlights the capability of neural networks to handle the curse of dimensionality in SPDEs, particularly under stochastic parameter spaces.

In another advancement, Beck \textit{et al.} introduce a deep learning algorithm for SPDEs, such as the stochastic heat equation and the Zakai equations \cite{nn-spde-2}. Their model uses separate neural networks for each realization of the noise process, achieving high accuracy even in 50-dimensional problems. This work emphasizes the efficient handling of additive and multiplicative noise, demonstrating the versatility of deep learning in high-dimensional filtering and stochastic modeling tasks.

Zhang \textit{et al.} extend Physics-Informed Neural Networks (PINNs) to SPDEs using modal space decomposition through dynamic orthogonal (DO) and biorthogonal (BO) methodologies \cite{nn-spde-1}. By incorporating these constraints into the loss function, their framework overcomes the limitations of traditional DO/BO methods, such as eigenvalue crossing or covariance matrix invertibility. The NN-DO/BO approach effectively handles forward and inverse problems with noisy initial data and high-dimensional stochastic inputs, offering a flexible and robust method for solving SPDEs.

\section{Methodology}
This section is organized as follows. First, we define the problem and introduce the specific SPDEs under consideration. Next, we detail the formulation of the model architectures and corresponding loss functions for both Loss Enforced Conditions (LEC) and Model Enforced Conditions (MEC). Following that, we describe the performance metrics used to evaluate the models and provide a step-by-step explanation of the training processes. Finally, we outline the experimental setup, including the selection of hyperparameters via Bayesian optimization. The complete code for this work is available on the public GitHub repository: \href{https://github.com/izzak98/NN-SPDE}{https://github.com/izzak98/NN-SPDE}.

\subsection{Problem Definition}
In this paper, we adapt the DGM and MIM frameworks for three SPDEs: the stochastic heat equation, the stochastic Burgers' equation, and the Kardar–Parisi–Zhang equation. The equations and assumptions become progressively more complex in order to test the ability of the approaches. We deviate slightly from the standard methodology in the sense that we always enforce boundary and initial conditions in the loss functions for DGM and always enforce the same conditions directly in the model for MIM. As such, we shall henceforth refer to them as LEC and MEC. We will explore the effectiveness of these approaches on the following dimensions \(2, 4, 8, 16\). For all SPDEs, we set the spatial domain to \(\Omega = [0,1] \) and the time domain to \(t \in [0, 1]\). 
These numbers are chosen because it is trivial to convert any domain to this specification. Furthermore, it has been shown that neural networks perform best within this domain \cite{norm}. 

In all following equations, the function \( W (t ,\vec{x})\) represents a Brownian Sheet. For implementation details see the publicly available \href{https://github.com/izzak98/NN-SPDE}{GitHub repository}. A Brownian Sheet is an extension of Brownian motion into higher dimensions. Where Brownian motion only accounts for the time dimensions, a Brownian Sheet encompasses both time and spatial dimensions. The Brownian Sheet has the following characteristics
\begin{align*}
    &\mu = 0 \\
    &\sigma^2 = t \prod^d_{i=1} x_i \\
    &\text{cov}(W(t, \vec{x}), W(p, \vec{s})) = \min(t, s) \prod^d_{i=1} \min(x_i, p_i),
\end{align*}
where $t, p$ are timepoints and $\vec{x},\vec{s}$ are vectors in the $d$ dimensional space $\Omega$.

A problem with the standard definition of the Brownian Sheet is that as dimensions increase, the variance gets closer and closer to 0 when the spatial bounds are between $0$ and $1$. We adapt it to our usecase by rewriting the variance and covariance functions such that they scale with the number of spatial dimensions as follows
\begin{align*}
    &\mu = 0 \\
    &\sigma^2 = \left(t \prod^d_{i=1} x_i\right)^{\frac{1}{d}} \\
    &\text{cov}(W(t, \vec{x}), W(p, \vec{s})) = \left(\min(t, s) \prod^d_{i=1} \min(x_i, p_i)\right)^{\frac{1}{d}}.
\end{align*}

This scaling problem is also present in the initial condition in the SPDEs described below, as they take some form of the inner product of spatial coordinates. Therefore, we introduce the following scaling variable which is utilized across initial conditions
\begin{equation}
    \vartheta = \sqrt{d} \cdot \log_{10}[\exp(d)]
\end{equation}
\subsection{Definitions of SPDEs}
\subsubsection{Stochastic Heat Equation}
The dimension-free form of the stochastic heat equation is given by 
\begin{equation*}
       \frac{\partial u(t,\vec{x})}{\partial t} = \nu\Delta u(t,\vec{x}) + \sigma(u(t,\vec{x})) W(t,\vec{x}),
\end{equation*}
where \( u(t,\vec{x}) \) represents the quantity of interest at time \( t \),  position \( \vec{x} \), \( \Delta \) is the Laplacian operator in \(d\) dimensions and \(\nu\) is the diffusion coefficient determining the speed of the diffusion. Here, \(\sigma(u(t,\vec{x})) = u(t,\vec{x})\) introduces standard multiplicative noise, which means that the intensity of the noise depends on the value of \( u(t,\vec{x}) \).

We impose Neumann boundary conditions, representing no flux across the boundaries, that is
\begin{equation}
\frac{\partial u(t, \vec{x})}{\partial n} = 0 \quad \text{on the boundary}, \quad \forall t > 0, \label{eq:heat_boundry}
\end{equation}
where \(\frac{\partial}{\partial n}\) is the normal derivative. The initial condition is set to
\begin{equation}
u(0, \vec{x}) = \vartheta \prod_{i=1}^{d} \cos(\pi x_i), \quad \vec{x} \in \Omega, \label{eq:heat_in}
\end{equation}
that is, no heat enters or leaves the system.
\subsubsection{Stochastic Burgers' Equation}
The dimension-free form of the stochastic Burgers' equation is
\begin{equation*}
       \frac{\partial u(t,\vec{x})}{\partial t} + u(t,\vec{x}) \cdot \nabla u(t,\vec{x}) = \nu \Delta u(t,\vec{x}) + \sigma(u(t,\vec{x})) W(t,\vec{x}),
\end{equation*}
where \( u(t,\vec{x}) \cdot \nabla u(t,\vec{x}) \) is the nonlinear convection term. We set \(\sigma(u(t, \vec{x})) = \alpha u(t, \vec{x})^d\), where \(\alpha\in [0,1]\) is dynamically sampled during training to create a more complex noise pattern. 

We impose periodic boundary conditions on the spatial domain, ensuring that the solution wraps around itself, that is
\begin{align}
    &u(t, 0, x_2, \dots, x_d) = u(t, 1, x_2, \dots, x_d), \label{eq:burger_bound}\\ \nonumber
    &u(t, x_1, 0, \dots, x_d) = u(t, x_1, 1, \dots, x_d), \\ \nonumber
    &\vdots \\ \nonumber
    &u(t, x_1, x_2, \dots, 0) = u(t, x_1, x_2, \dots, 1) \\ \nonumber
    &\forall t > 0. \nonumber
\end{align}

The initial condition is set to
\begin{equation}
u(0, \vec{x}) = \vartheta \prod_{i=1}^{d} \sin(\pi x_i), \quad \vec{x} \in \Omega. \label{eq:burger_in}
\end{equation}
thus enforcing the boundary condition while painting a similar complexity as the heat equation. 

\subsubsection{Stochastic Kardar–Parisi–Zhang Equation}
The dimension-free form of the Kardar–Parisi–Zhang (KPZ) equation is:
\begin{equation*}
    \frac{\partial u(t,\vec{x})}{\partial t} = \nu \Delta u(t,\vec{x}) + \frac{\lambda}{2}\|\nabla u(t,\vec{x})\|^2 + \sigma(u(t,\vec{x})) W(t,\vec{x}),
\end{equation*}
where \( u(t,\vec{x}) \) represents the height of an interface evolving in time and space. The term \(\tfrac{\lambda}{2}\|\nabla u(t,\vec{x})\|^2\) introduces a non-linear growth mechanism, making this problem significantly more challenging than the stochastic Burgers' equation. We set \(\sigma(u(t,\vec{x})) = \alpha \exp[u(t,\vec{x})^d]\) iterating the complexity of the noise pattern from the Burgers' equation.

Like in the Burgers' equation, we introduce the periodic boundary conditions
\begin{align}
    &u(t, 0, x_2, \dots, x_d) = u(t, 1, x_2, \dots, x_d), \label{eq:kpz_bound}\\ \nonumber
    &\vdots \\ \nonumber
    &u(t, x_1, x_2, \dots, 0) = u(t, x_1, x_2, \dots, 1) \quad \forall t > 0. \nonumber
\end{align}

Finally, the initial condition is set to
\begin{equation} 
u(0, \vec{x}) = \frac{1}{\vartheta}  \sum_{i=1}^{d} \bigl[\sin(2\pi x_i) + \cos(2\pi x_i)\bigr] , \quad \vec{x} \in \Omega. \label{eq:kpz_in_complex}
\end{equation}
Note that due to the sum of the trig functions, we have the opposite problem as before, where the values are growing too large. Thus, instead of multiplying by $\vartheta$, we divide, to hamper growth when we reach higher dimensions. 

\subsection{Model Architecture \& Loss Architecture}
In this section, we detail the model architecture of both DGE and MEC for all three SPDEs. First, we define the variables
\begin{align*}
    &u_\theta \approx \mathbb{E}[u(t,\vec{x})] \\
    &p_{\theta} \approx \nabla u_\theta,
\end{align*}
which represent the expected value of the SPDE and its gradient, respectively. 
\subsubsection{Stochastic Heat Equation}
\paragraph{DGE}
Using the DGE method, the network that estimates the heat equation is rather simple; the model can be defined as
\begin{equation*}
    u_{\theta} = \text{LEC}_\text{heat}(t, \vec{x}, \nu)
\end{equation*}
where the only constraints on the network are the activation functions, which are set as hyperparameters, as with all the other networks. It is trained using the loss functions
\begin{align}
    \mathcal{L}_{\mathcal{R}}^{\text{LEC}} &= \left\| \frac{\partial u_{\theta}}{\partial t} - \nu \Delta u_{\theta} - u_{\theta} W \right\|^2_{\Omega \times [0,T]} \label{eq:heat_LEC_res} \\ 
    \mathcal{L}_\mathcal{I}^{\text{LEC}} &=\left\|  u_{\theta} - \vartheta \prod_{i=1}^{d} \cos(\pi x_i) \right\|^2_{\Omega} \label{eq:heat_LEC_int}\\
    \mathcal{L}_\mathcal{B}^{\text{LEC}} &= \left\| \frac{\partial u_{\theta}}{\partial n} \right\|^2_{\partial \Omega \times [0,T]} \label{eq:heat_LEC_bound}
\end{align}
The primary term to minimize, shown in Eq.~\eqref{eq:heat_LEC_res}, is the residual that represents the differential equation. In addition, Eq.~\eqref{eq:heat_LEC_int} enforces the initial conditions as specified in Eq.~\eqref{eq:heat_in}, while Eq.~\eqref{eq:heat_LEC_bound} enforces the boundary conditions defined in Eq.~\eqref{eq:heat_boundry}.
\paragraph{MEC}
When using MEC, the model definition becomes more complex, as the boundary and initial conditions must be enforced within the network architecture; it is defined as
\begin{align}
    &u_\text{raw}, p_\text{raw} = \text{MEC}_\text{heat}(t, \vec{x}), \nonumber\\
    &u_\theta = u_\text{raw} \cdot t +  \vartheta \prod_{i=1}^{d} \cos(\pi x_i), \label{eq:heat_MEC_in}\\
    &p_\theta = p_\text{raw} \prod_{i=1}^{d} x_i \cdot (1 - x_i). \label{eq:heat_MEC_bound}
\end{align}
Here Eq.~\eqref{eq:heat_MEC_in} enforces the initial conditions specified in Eq.~\eqref{eq:heat_in} and Eq.~\eqref{eq:heat_MEC_bound} enforces the boundary conditions in Eq.~\eqref{eq:heat_boundry}.

In MEC, the loss function is simpler to define than in LEC. It is given by
\begin{align}
    \mathcal{L}_{\mathcal{R}}^\text{MEC} &= \left\| \frac{\partial u_{\theta}}{\partial t} - \nu \nabla p_{\theta} - u_{\theta} W \right\|^2_{\Omega \times [0,T]} \label{eq:MEC_heat_res} \\
    \mathcal{L}_{\mathcal{G}}^\text{MEC} &=  \left\| p_{\theta} - \nabla u_{\theta} \right\|^2_{\Omega \times [0,T]} \label{eq:MEC_heat_con}
\end{align}
Similar to LEC, Eq.~\eqref{eq:MEC_heat_res} is the residual and the primary term to minimize. What is more unique to MEC is Eq.~\eqref{eq:MEC_heat_con} which enforces \(\nabla u = p\).
\subsubsection{Stochastic Burger's Equation}

\paragraph{LEC}
As before, the LEC network is simple to define. It is
\begin{equation*}
    u_\theta = \text{LEC}_\text{Burgers'}(t, \vec{x}, \nu, \alpha).
\end{equation*}
A difference to note is the additional terms for the stochastic scaling \(\alpha\). 

The initial and boundary conditions are similarly enforced as constraints in the loss function, with periodic boundaries ensuring continuity in the spatial domain, consistent with the nonlinear nature of Burgers' equation. The periodic boundary conditions are more complex which is reflected in the loss function
\begin{align}
    \mathcal{L}_{\mathcal{R}}^{\text{LEC}} &= \bigg\| \frac{\partial u{\theta}}{\partial t} + u_{\theta} \cdot \nu \nabla u_{\theta} \label{eq:buger_LEC_res} - \nu \Delta u_{\theta} - \alpha u^n W\bigg\|^2_{\Omega \times [0,T]}\\ 
   \mathcal{L}_\mathcal{I}^{\text{LEC}} &= \bigg\| u_{\theta} - \vartheta \prod_{i=1}^{d} \sin(\pi x_i) \bigg\|^2_{\Omega}  \label{eq:buger_LEC_in} \\ 
     \mathcal{L}_\mathcal{B}^{\text{LEC}} &= \sum^d_{i=1} \bigg\| u_{\theta}(x_i = 0) - u_{\theta}(x_i = 1) \bigg\|^2_{\Omega \times [0,T]}. \label{eq:buger_LEC_bound}
\end{align}
As before, Eq.~\eqref{eq:buger_LEC_res} is the primary metric to minimize, Eq.~\eqref{eq:burger_in} enforces the boundary condition Eq.~\eqref{eq:burger_in} and Eq.~\eqref{eq:buger_LEC_bound} enforces the periodic boundary condition form Eq.~\eqref{eq:burger_bound}.
\paragraph{MEC}
The model architecture of MEC is more complex as it needs to capture the periodic boundary condition. We follow the method prescribed by the original authors \cite{mim-cont}.
\begin{align}
    &\vec{x}_\bigcirc = \left[\sin(2\pi j x_i), \cos(2\pi j x_i)\right]_{i=1,j=1}^{d,4} \in \mathbb{R}^{8d}, \label{eq:burger_MEC_bound}\\
    &u_\text{raw}, p_\theta = \text{MEC}_\text{Burgers'}(t, \vec{x}_\bigcirc, \nu, \alpha), \nonumber\\
    &u_\theta = u_\text{raw} \cdot t + \vartheta \prod_{i=1}^{d} \sin(\pi x_i), \label{eq:burger_MEC_in} \\
\end{align}
To enforce the periodic boundary condition defined in Eq.~\eqref{eq:burger_bound}, we introduce \(\vec{x}_\bigcirc\) in Eq.~\eqref{eq:burger_MEC_bound} which adds an additional dimension to the input of the network; in doing so, the boundary condition is enforced because points on the opposite end now equal each other. Additionally, Eq.~\eqref{eq:burger_MEC_in} enforces the initial conditions outlined in Eq.~\eqref{eq:burger_in}.

The loss function is similar to the one for the heat equation, the only difference being the residual portion
\begin{align*}
    \mathcal{L}_\mathcal{R}^{\text{MEC}} &= \bigg\| \frac{\partial u_{\theta}}{\partial t} + u_{\theta} \cdot p_{\theta} - \nu \nabla  p_{\theta} - \alpha u_\theta^d W \bigg\|^2_{\Omega \times [0,T]}   \\
    \mathcal{L}_{\mathcal{G}}^\text{MEC} &=\left\| p_{\theta}(t, \vec{x}) - \nabla u_{\theta}(t, \vec{x}) \right\|^2_{\Omega \times [0,T]}. 
\end{align*}

\subsubsection{Stochastic Kardar–Parisi–Zhang Equation}
\paragraph{LEC}
As with the previous equations, we begin by defining the model approximation for the KPZ equation
\begin{equation*}
    u_{\theta} = \text{LEC}_{\text{KPZ}}(t, \vec{x}, \nu, \lambda, \alpha).
\end{equation*}
Since the KPZ equation introduces the nonlinear \(\frac{\lambda}{2}\|\nabla u(t,\vec{x})\|^2\) term and a more complex noise scaling \(\sigma(u) = \alpha \exp[u(t,\vec{x})^d]\), the resulting residual form is more intricate. In addition to these complexities, we maintain the same periodic boundary conditions as defined in Eq.~\eqref{eq:kpz_bound} and the initial condition from Eq.~\eqref{eq:kpz_in_complex}.

The loss function for LEC includes
\begin{align}
    &\mathcal{L}_{\mathcal{R}}^\text{LEC} = \bigg\| \frac{\partial u_{\theta}}{\partial t} - \nu \Delta u_{\theta} - \frac{\lambda}{2}\|\nabla u_{\theta}\|^2 - \alpha \exp[u_{\theta}^d]W \bigg\|^2_{\Omega \times [0,T]}, \label{eq:kpz_LEC_res}\\[6pt]
    &\mathcal{L}_\mathcal{I}^{\text{LEC}} = \bigg\| u_{\theta}(0, \vec{x}) - \frac{1}{\vartheta} \prod_{i=1}^{d} [\sin(2\pi x_i) + \cos(2\pi x_i)] \bigg\|^2_{\Omega}, \label{eq:kpz_LEC_in}\\[6pt]
    &\mathcal{L}_\mathcal{B}^{\text{LEC}} = \sum_{i=1}^{d} \bigg\| u_{\theta}(x_i=0) - u_{\theta}(x_i=1) \bigg\|^2_{\Omega \times [0,T]}, \label{eq:kpz_LEC_bound}
\end{align}
where Eq.~\eqref{eq:kpz_LEC_res} enforces the KPZ dynamics, Eq.~\eqref{eq:kpz_LEC_in} enforces the chosen initial condition, and Eq.~\eqref{eq:kpz_LEC_bound} ensures the solution respects the periodic boundary conditions.

\paragraph{MEC}
For MEC, we adopt a similar strategy to that used in the stochastic Burgers' equation to handle the periodic boundary conditions. Specifically, we transform the spatial coordinates as follows
\begin{equation}
    \vec{x}_\bigcirc = \left[\sin(2\pi j x_i), \cos(2\pi j x_i)\right]_{i=1,j=1}^{d,4} \in \mathbb{R}^{8d}, \label{eq:kpz_MEC_bound}
\end{equation}
which ensures periodicity is naturally incorporated into the input space of the network. We then define
\begin{align}
    &u_\text{raw}, p_{\theta} = \text{MEC}_{\text{KPZ}}(t, \vec{x}_\bigcirc, \nu, \lambda, \alpha), \nonumber \\[6pt]
    &u_{\theta} = u_\text{raw} \cdot t + \frac{1}{\vartheta} \prod_{i=1}^{d} [\sin(2\pi x_i) + \cos(2\pi x_i)], \label{eq:kpz_MEC_in}
\end{align}
where Eq.~\eqref{eq:kpz_MEC_in} enforces the initial condition given in Eq.~\eqref{eq:kpz_in_complex}. By construction, the periodic boundary conditions stated in Eq.~\eqref{eq:kpz_bound} are respected due to the transformed input \(\vec{x}_\bigcirc\).

The loss functions are defined as:
\begin{align}
    &\mathcal{L}_{\mathcal{R}}^\text{MEC} = \bigg\| \frac{\partial u_{\theta}}{\partial t} - \nu \nabla p_{\theta} - \frac{\lambda}{2}\|p_{\theta}\|^2 - \alpha \exp[u_{\theta}^d]W \bigg\|^2_{\Omega \times [0,T]}, \label{eq:MEC_kpz_res}\\[6pt]
    &\mathcal{L}_{\mathcal{G}}^\text{MEC} = \left\| p_{\theta}(t, \vec{x}) - \nabla u_{\theta}(t, \vec{x}) \right\|^2_{\Omega \times [0,T]}, \label{eq:MEC_kpz_con}
\end{align}
where Eq.~\eqref{eq:MEC_kpz_res} ensures the model satisfies the KPZ equation, and Eq.~\eqref{eq:MEC_kpz_con} enforces the relationship \(p_{\theta} \approx \nabla u_{\theta}\), helping stabilize and improve the training process by aligning the modeled gradient with the actual gradient of the solution.

\subsection{Evaluation and Training Process}
\subsubsection{Performance Metrics}
The models are evaluated based on three metrics, namely 1) The average residual across 10,000 samples, sampled in the same manner as during training, 2) The enforcement of initial and boundary conditions, calculated on the same samples using the loss functions defined in the LEC framework for each SPDE, and, 3) The computation time for both training and inference.

\subsubsection{Training Process for LEC}
The LEC approach parameterizes the solution \( u(t,\vec{x}) \) using a neural network with parameters \( \theta \). At each training epoch, spatial and temporal points are sampled alongside stochastic noise instances derived from the Brownian Sheet. The training algorithm optimizes a composite loss function, which consists of the three components outlined above: \(\mathcal{L}_\mathcal{R}\), \(\mathcal{L}_\mathcal{I}\), \(\mathcal{L}_\mathcal{B}\). The residual loss ensures that the neural network solution satisfies the underlying SPDE. The initial loss enforces that \( u_{\theta} \) correctly reproduces the prescribed initial condition, while the boundary loss ensures that \( u_{\theta} \) adheres to the specified boundary conditions.

The LEC training loop employs Monte Carlo sampling over noise realizations to approximate expectations and to evaluate the residual loss. The learning process iteratively refines $\theta$ using gradient-based optimization. Algorithm~\ref{alg:LEC-train} outlines the overall procedure.
\begin{algorithm}[H]
\caption{Training Process for LEC Model}
\label{alg:LEC-train}
\begin{algorithmic}[1]
\REQUIRE 
    $n$: number of training epochs, \\
    $m$: number of MC samples per epoch, \\
    $d$: number of spatial dimensions, \\
    $p$: number of spatial points per batch, \\
    $\lambda_1 \geq 1$: initial loss scaling, \\
    $\lambda_2 \geq 1$: boundary loss scaling \\
\STATE Initialize model parameters $\theta$ randomly
\FOR{$i = 1$ to $n$}
    \STATE $X \gets \text{Uniform}(0,1)^{p \times d}$ 
    \STATE $\vec{t} \gets \text{Uniform}(0,1)^{p}$
    \STATE $\vec{\nu} \gets \text{LogUniform}(0,1)^{p}$
    \STATE $\vec{\alpha} \gets \text{LogUniform}(0,1)^{p}$ (only for Burgers'/KPZ)
    \STATE $\vec{\lambda} \gets \text{Uniform}(0,1)^{p}$ (only for KPZ)
    \STATE $\text{residual\_loss} \gets 0$
    \FOR{$j = 1$ to $m$}
        \STATE $w \gets [W(t_i, X_i)]_{i=1}^{p}$  \hfill // Noise realization
        \STATE $u_{\theta} \gets \text{LEC}(\vec{t}, X, \vec{\nu}, \vec{\alpha}, \vec{\lambda}; \theta)$
        \STATE $\text{residual\_loss} \gets \text{residual\_loss} + \mathcal{L}_\mathcal{R}(u_{\theta}, \vec{\nu}, \vec{\alpha}, \vec{\lambda}, w)$
    \ENDFOR
    \STATE $\vec{t_0} \gets [0]^p$
    \STATE $u_{\theta,0} \gets LEC(\vec{t_0}, X, \vec{\nu}, \vec{\alpha}; \theta)$
    \STATE $initial\_loss \gets \mathcal{L}_\mathcal{I}(u_{\theta,0}, X)$
    \STATE $X_b \gets \text{ConvertToBoundaryPoints}(X)$
    \STATE $u_{\theta,b} \gets \text{LEC}(\vec{t}, X_b, \vec{\nu}, \vec{\alpha}; \theta)$
    \STATE $boundary\_loss \gets \mathcal{L}_\mathcal{B}(u_{\theta,b}, \vec{t}, X_b)$
    \STATE $\text{loss} \gets \text{residual\_loss} + \lambda_1 \cdot \text{initial\_loss} + \lambda_2 \cdot \text{boundary\_loss}$
    \STATE $\theta \gets \theta - \eta \nabla_{\theta}(loss)$  \hfill // Update parameters
\ENDFOR
\end{algorithmic}
\end{algorithm}
\subsubsection{Training Process for MEC}
The MEC approach parameterizes not only $u(t,\vec{x})$ but also its gradient via a second neural network. This architecture inherently enforces certain properties of the solution and can simplify constraints on the solution's gradient. Similar to the LEC approach, MEC relies on sampling spatial points and noise realizations. However, the primary differences lie in how the constraints are enforced within the network and in the structure of the loss function. In addition to a residual loss, MEC incorporates a \emph{gradient loss}, $L_{\text{gradient}}$, which ensures consistency between $u_{\theta}$ and its modeled gradient $p_{\theta}$. Algorithm ~\ref{alg:MEC-train} describes the process.
\begin{algorithm}[H]
\caption{Training Process for MEC Model}
\label{alg:MEC-train}
\begin{algorithmic}[1]
\REQUIRE 
    $n$: number of training epochs, \\
    $m$: number of MC samples per epoch, \\
    $d$: number of spatial dimensions, \\
    $p$: number of spatial points per batch, \\
    $\lambda_1 \geq 1$: gradient loss scaling, \\
\STATE Initialize model parameters $\theta$ randomly
\FOR{$i = 1$ to $n$}
    \STATE $X \gets \text{Uniform}(0,1)^{p \times d}$ 
    \STATE $\vec{t} \gets \text{Uniform}(0,1)^{p}$
    \STATE $\vec{\nu} \gets \text{LogUniform}(0,1)^{p}$
    \STATE $\vec{\alpha} \gets \text{LogUniform}(0,1)^{p}$ (only for Burgers'/KPZ) % komast af því hvort þetta er log eða ekki
    \STATE $\vec{\lambda} \gets \text{Uniform}(0,1)^{p}$ (only for KPZ)
    \STATE $\text{residual\_loss} \gets 0$
    \FOR{$j = 1$ to $m$}
        \STATE $w \gets [W(t_i, X_i)]_{i=1}^{p}$  \hfill // Noise realization
        \STATE $u_{\theta} \gets \text{MEC}(\vec{t}, X, \vec{\nu}, \vec{\alpha}, \vec{\lambda}; \theta)$
        \STATE $\text{residual\_loss} \gets \text{residual\_loss} + \mathcal{L}_\mathcal{R}(u_{\theta}, \vec{\nu}, \vec{\alpha}, \vec{\lambda}, w)$
    \ENDFOR
    \STATE $u_{\theta}, p_{\theta} \gets \text{MEC}(\vec{t}, X, \vec{\nu}, \vec{\alpha}; \theta)$
    \STATE $\text{gradient\_loss} \gets \mathcal{L}_\mathcal{G}(u_{\theta}, p_{\theta})$
    \STATE $\text{loss} \gets \text{residual\_loss} + \lambda_1 \cdot \text{gradient\_loss}$
    \STATE $\theta \gets \theta - \eta \nabla_{\theta}(loss)$  \hfill // Update parameters
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Experimental Setup}
\subsubsection{Model Definition}
To create the models, we utilize layers defined in the DGM paper, which we well refer to as DGM layers. A DGM layer maps an input tensor \(\mathbf{x} \in \mathbb{R}^d\), a hidden state \(\mathbf{S} \in \mathbb{R}^h\), and an auxiliary state \(\mathbf{S}_1 \in \mathbb{R}^h\) to an updated hidden state \(\mathbf{S}' \in \mathbb{R}^h\) through the following equations:
\begin{align*}
\mathbf{Z} &= \sigma(\mathbf{U}_z \mathbf{x} + \mathbf{W}_z \mathbf{S} + \mathbf{B}_z \mathbf{S}_1)\\
\mathbf{G} &= \sigma(\mathbf{U}_g \mathbf{x} + \mathbf{W}_g \mathbf{S}_1 + \mathbf{B}_g \mathbf{S}) \\
\mathbf{R} &= \sigma(\mathbf{U}_r \mathbf{x} + \mathbf{W}_r \mathbf{S}_1 + \mathbf{B}_r \mathbf{S})\\
\mathbf{H} &= \tanh(\mathbf{U}_h \mathbf{x} + \mathbf{W}_h (\mathbf{S} \odot \mathbf{R}) + \mathbf{B}_h \mathbf{S}_1)\\
\mathbf{S}' &= (1 - \mathbf{G}) \odot \mathbf{H} + \mathbf{Z} \odot \mathbf{S}
\end{align*}
Here, \(\sigma\) is the sigmoid activation function, \(\tanh\) is the hyperbolic tangent, \(\odot\) denotes element-wise multiplication, and \(\mathbf{U}_z, \mathbf{W}_z, \mathbf{B}_z, \dots\) are learnable weight matrices for the respective gates (reset, update, and candidate).

After inputs are passed through DGM layers, they are passed through fully connected dense layers. Each model will have the number of DGM and fully connected layers, and the parameters therein as hyperparameters, which are detailed more thoroughly in the following subsection. The key difference between the setup of LEC and MEC models is that MEC is, in essence, two models, one to estimate \(u_\theta\) and one to estimate \(p_\theta\). Thus, requires the number of layers and parameters set for each sub-model.

\subsubsection{Hyper Parameter Selection}
Hyperparameters are selected by Bayesian optimization using Optuna \cite{optuna}. Table \ref{tab:optuna_search_space} shows the search space for the models; note that this is done twice for MEC to find the optimal parameters for both the \(u_\theta\) and \(p_\theta\) models. Table \ref{tab:optimizer_scheduler} shows the search space for the hyperparameters for general optimization. Note that all the lambdas are included in the search space but excluded to avoid confusion in the tables; they are set between 1 and 10. The final hyperparameters can be found in public \href{https://github.com/izzak98/NN-SPDE}{GitHub repository} associated with this paper.

\begin{table*}[h]
\caption{Optuna Hyperparameter Search Space.\label{tab:optuna_search_space}}
\centering
\begin{tabular}{llll}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Values} & \textbf{Description} \\
\midrule
\texttt{n\_fc\_layers} & Integer & $[1,5]$ & Number of fully connected (FC) layers. \\
\texttt{fc\_layer\_\{i\}\_dim} & Categorical & $\{32,\,64,\,128,\,256,\,512\}$ & Dimension for the $i^\text{th}$ FC layer, where $i=0,\dots,n\_fc\_layers-1$. \\

\texttt{use\_dgm} & Boolean & $\{True,\,False\}$ & Flag to decide whether to use DGM layers. \\

\texttt{dgm\_dims} & Categorical & $\{2,\,4,\,8,\,16,\,32,\,64,\,128\}$ & Dimension for the DGM layer (sampled only if \texttt{use\_dgm} is True). \\

\texttt{n\_dgm\_layers} & Integer & $[1,5]$ & Number of DGM layers (sampled only if \texttt{use\_dgm} is True). \\

\texttt{output\_activation} & Categorical & $\{\texttt{relu},\,\texttt{tanh},\,\texttt{sigmoid},\,\texttt{None}\}$ & Activation function for the output layer. \\

\texttt{hidden\_activation} & Categorical & $\{\texttt{relu},\,\texttt{tanh},\,\texttt{sigmoid}\}$ & Activation function for the hidden layers. \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[h]
\caption{Hyperparameters for Optimizer and Scheduler Setup.}
\label{tab:optimizer_scheduler}
\centering
\begin{tabular}{llll}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Values/Range} & \textbf{Description} \\
\midrule
\texttt{lr} & Float (log-scale) & $[1\times10^{-5},\,1\times10^{-2}]$ & Learning rate for the optimizer. \\

\texttt{l2\_reg} & Float (log-scale) & $[1\times10^{-6},\,1\times10^{-3}]$ & L2 regularization weight decay coefficient. \\

\texttt{use\_scheduler} & Boolean & $\{True,\,False\}$ & Flag to decide whether to use a learning rate scheduler. \\

\texttt{patience} & Integer & $[5,\,20]$ & Number of epochs with no improvement before reducing the learning rate. \\

\texttt{factor} & Float (log-scale) & $[0.1,\,0.9]$ & Factor by which the learning rate will be reduced (if scheduler is used). \\
\bottomrule
\end{tabular}
\end{table*}

\section{Results} \label{sec:Results}
% The results section describes the results you obtained in your research.  Include figures and tables as appropriate to illustrate your results. Figures can show data trends or other visual information. Tables are best to use when the exact values are important.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{loss-curve.png}
    \caption{Loss curves compared across models and dimensions. The loss is shown on logarithmic scale.}
    \label{fig:loss_curves}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{meshes.png}
    \caption{Comparison of LEC and MEC solutions at different time points. With the following parameters $\nu=0.1$, $\alpha = 1$, $\lambda = 0.1$ for the respective equations.}
    \label{fig:solutions}
\end{figure*}

\begin{table}[h]
 \caption{Loss values for different equations, dimensions, and models}
    \label{tab:loss_values}
    \centering
    \begin{tabular}{lclp{1cm}p{1cm}p{1.2cm}}
        \toprule
        \textbf{Equation} & \textbf{Dim.} & \textbf{Model} & \textbf{Initial Loss} & \textbf{Boundary Loss} & \textbf{Residual Loss} \\
        \midrule
       \multirow{4}{*}{Heat}  & \multirow{2}{*}{2}  & LEC & 0.0110  & \textbf{7.06e-04}  & \textbf{2.77e-02} \\
              &    & MEC & \textbf{0}  & 1.31e-01  & 7.06-01 \\
             
              & \multirow{2}{*}{8}   & LEC & 0.3680  & \textbf{4.26e-12}  & \textbf{5.20e-14} \\
              &    & MEC & \textbf{0}  & 6.65e+00  & 4.79e+01 \\
        \midrule
         \multirow{4}{*}{Burger} & \multirow{2}{*}{2}  & LEC & 0.01659  & 2.36e-03  & \textbf{8.53e-03} \\
               &    & MEC & \textbf{0}  & \textbf{1.91e-14}  & 2.44e+02 \\
              
               & \multirow{2}{*}{8}  & LEC & 0.1485  & 7.25e-03  & \textbf{2.66e-02} \\
               &    & MEC & \textbf{0}  & \textbf{9.30e-14}  & 3.51e+02 \\
        \midrule
         \multirow{4}{*}{KPZ}    & \multirow{2}{*}{2}  & LEC & 0.1404  & 1.27e-02  & \textbf{3.08e-01} \\
               &    & MEC & \textbf{0}  & \textbf{4.97e-14}  & 2.00e+01 \\
            
               & \multirow{2}{*}{8}  & LEC & 0.0021  & 1.14e-03  & \textbf{1.046e-02} \\
               &    & MEC & \textbf{0}  & \textbf{5.86e-15}  & 5.13e-01 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]
\caption{Training and inference times for different equations, dimensions, and models. All in Seconds}
    \label{tab:train_inference_time}
    \centering
    \begin{tabular}{lclp{2cm}p{2cm}}
        \toprule
        \textbf{Equation} & \textbf{Dim.} & \textbf{Model} & \textbf{Train Time (seconds)} & \textbf{Inference Time (seconds)} \\
        \midrule
        \multirow{4}{*}{Heat}   & \multirow{2}{*}{2}  & LEC & 1917  & 1.3277 \\
              &    & MEC &  \textbf{863}  & \textbf{0.4732} \\
          
              & \multirow{2}{*}{8}  & LEC & 1664  & \textbf{0.9578} \\
              &    & MEC & \textbf{1372}  & 4.9820 \\
        \midrule
        \multirow{4}{*}{Burger}  & \multirow{2}{*}{2}  & LEC  &  \textbf{2468}  & \textbf{1.1285} \\
               &    & MEC  &  7446  & 1.5130 \\
            
               & \multirow{2}{*}{8}  & LEC  & \textbf{11244}  & \textbf{1.9134} \\
               &    & MEC  & 25931  & 7.4671 \\
        \midrule
        \multirow{4}{*}{KPZ}     & \multirow{2}{*}{2}  & LEC  &  2758  & \textbf{0.8313} \\
               &    & MEC  &  \textbf{1657}  & 1.7529 \\
               & \multirow{2}{*}{8}  & LEC  &  \textbf{5889}  & \textbf{2.1808} \\
               &    & MEC  &  7140  & 7.1282 \\
        \bottomrule
    \end{tabular}
    
\end{table}


\noindent Figure \ref{fig:loss_curves} presents the evolution of the loss when all lambdas are set to 1, over 1000 epochs when training the models with the optimal hyperparameters found using Optuna. The figure shows results for the stochastic heat equation, stochastic Burgers' equation, and stochastic KPZ equation, comparing the performance of LEC and MEC models in both two and eight spatial dimensions. However, in all cases, LEC consistently exhibits a smoother loss trajectory, whereas MEC demonstrates higher variance, particularly in higher dimensions. In the two-dimensional cases, both methods achieve loss reduction, though LEC attains lower final loss values. In eight dimensions, MEC exhibits instability in certain cases, with loss fluctuations remaining several orders of magnitude higher, most notably in the stochastic Burgers' equation.

Figure \ref{fig:solutions} presents the solutions for each equation at \(t = 0.1\) and \(t = 0.9\) in two spatial dimensions. At \(t = 0.1\), the primary differences between LEC and MEC are observed in the scaling of values, with LEC exhibiting faster diffusion in the Burgers’ and heat equations, while diffusing more slowly in KPZ. Despite these scaling differences, the overall shape of the solutions remains similar across methods, with the largest deviation appearing in Burgers', where the central mass shifts upward and to the right under MEC. At \(t = 0.9\), the disparities become more pronounced, both in shape and magnitude. The largest discrepancy occurs in the KPZ equation, where the MEC solution becomes noisy and unstable, exhibiting nearly an order-of-magnitude difference in scale compared to LEC. 


Table \ref{tab:loss_values} provides a quantitative comparison of final loss values, including residual, initial, and boundary losses. MEC achieves perfect initial condition enforcement across all cases, whereas LEC incurs minor deviations. Similarly, MEC achieves near-zero boundary losses, while LEC demonstrates comparable but slightly higher boundary deviations. However, LEC consistently achieves lower residual loss across all equations and dimensions, and the disparity grows more pronounced in higher-dimensional settings. Table \ref{tab:train_inference_time} further summarizes the training and inference times of each model. Although LEC requires more training time in lower-dimensional cases, it scales more efficiently in higher dimensions, whereas MEC experiences a significant increase in inference time as dimensionality increases.

\section{Discussion}\label{sec:Discussion}
% In the discussion section, describe what your results mean and how they are an important contribution to the research field.

\noindent The results highlight key trade-offs between the LEC model and the MEC model, particularly in terms of how they handle initial conditions, boundary conditions, and residual accuracy. The MEC model is explicitly designed to always respect initial conditions by enforcing them directly within the network architecture, which is why it consistently achieves zero initial loss across all equations and dimensions. In contrast, LEC enforces initial conditions only through the loss function, leading to minor deviations. A similar pattern emerges in boundary enforcement, where MEC generally achieves lower boundary losses, except in the heat equation, where the boundary depends on how \(p_\theta\) is estimated, allowing greater error.

The computational trade-offs between the two approaches are evident in training and inference times, as summarized in Table \ref{tab:train_inference_time}. Because MEC is effectively comprised of two models, it requires many more parameters to train. The discrepancy between training time and inference time can be explained by the batch size hyperparameter. 

An important consideration when selecting between these methods is the relative importance of residual accuracy versus constraint enforcement. If initial and boundary conditions are more critical than minimizing residual loss, MEC may be the stronger choice, as it may guarantee perfect enforcement of these constraints. However, this trade-off does not hold uniformly across all equations. In the case of the stochastic Burgers' equation, MEC struggles significantly, exhibiting extreme instability in high dimensions. This is likely due to the non-linearity of the convection term \( u \cdot \nabla u \), which amplifies errors in the gradient approximation. Since MEC models must explicitly track and enforce gradient relationships, small errors in estimating \( \nabla u \) can propagate through training, leading to excessive variance and poor convergence. This effect is less pronounced in the heat equation, where diffusion dominates, but it becomes a significant limitation for Burgers' and KPZ equations. This effect is further reflected in the solution plots in Figure \ref{fig:solutions}, where MEC exhibits greater instability over time, particularly in Burgers’ and KPZ equations. While LEC maintains smoother structures, MEC introduces noticeable distortions, with the KPZ solution becoming highly noisy at later time steps. This suggests that MEC's inability to effectively handle cases where small errors in gradient estimations can quickly propagate through the solution.

Additionally, the scaling of the Brownian Sheet and initial conditions play a role in the observed residual losses. As discussed in the methodology section, the chosen scaling ensures that residual values decrease as the number of dimensions increases. This partially explains why residual losses appear lower in higher-dimensional settings. However, this does not necessarily indicate better approximation quality; it may simply reflect the fact that both the noise intensity and function values shrink due to the chosen scaling. Therefore, while LEC achieves consistently lower residual losses, the practical importance of these improvements must be considered in the context of how the equations and their solutions behave in high dimensions.

\section{Conclusion}\label{sec:Conclusion}
% The conclusion section can highlight potential broader implications of your work and areas that need further study.  Be careful not to inflate your findings.
\noindent This paper introduced a framework for estimating the expected value of solutions to SPDEs using neural networks, comparing LEC and MEC approaches. The results demonstrate that LEC consistently achieves lower residual losses across all tested SPDEs, whereas MEC provides perfect enforcement of initial and excellent enforcement boundary conditions. The trade-offs between these methods depend on the relative importance of enforcing constraints versus minimizing residual errors. While LEC offers more stable training and better generalization, MEC may be preferable when exact constraint adherence is required, except in the case of the stochastic Burgers'’ equation, where gradient approximation errors propagate quickly causing significant instability.

A primary challenge in evaluating these models is the lack of analytical solutions for the tested SPDEs, meaning that effectiveness is assessed solely through residual loss minimization. While numerical approximations could serve as reference solutions, their computational cost makes them infeasible for high-dimensional problems—the very setting where deep learning is most beneficial. Another limitation is the high training cost, especially when incorporating hyperparameter optimization, although inference remains efficient once training is complete. Future work should explore strategies to reduce training time without sacrificing accuracy, such as more efficient architectures or adaptive sampling techniques.
Expanding this framework to a broader range of SPDEs, including those with direct real-world applications in finance, physics, and engineering, would further demonstrate its versatility. Additionally, hybrid approaches that combine the strengths of LEC and MEC may improve both constraint enforcement and residual accuracy, particularly for nonlinear equations like Burgers'. 

\vfill

\bibliographystyle{ieeetr}
\bibliography{lit-rev}


\end{document}


