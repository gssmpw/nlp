% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{makecell}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% \usepackage[noend]{algpseudocode}
% \usepackage{algorithmicx,algorithm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}      
\usepackage{bm}
%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2024}

\usepackage{multirow}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Consistent Meta-Label Correction for Robust Deep Learning: Aligning Training Dynamics Perception  and  Loss Adjustment}
\title{LAC-MTDP: Loss Adjustment Consistency Based Multi-level Training Dynamic Perception for Rubost deep learning}
\title{MDP-LA: Multi-Granularity Data Perception and Loss Alignment for Rubost deep learning }
\title{Logit Perturbation and Label Perturbation Consistency 
\\ for Robust Deep Learning}
\title{CCMLC-Net: Consistent Class-Aware Sample Label Correction \\for Robust Deep Learning}

\title{CMW-Net: Learning a Class-Aware Label Correction Mapping \\ for Robust Deep Learning}

\title{GLC-Net: Learning a Generic Label Correction Mapping \\ for Robust Deep Learning}

\title{TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning}


\author{First Author\\
Institution1\\G
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}

In recent years, meta-label correction algorithms have found widespread applications in noisy learning tasks due to their outstanding performance. Meta-learning enables models to rapidly adapt to new tasks or domains, raising a natural question: can a trained meta-label correction network be transferred to other tasks? To address this question, we conducted experiments on various noise ratios, noise types, models, and datasets. Some intriguing observations emerged: not all transfer tasks exhibited performance degradation. Notably, tasks with higher noise ratios displayed significant performance improvements when transferred to tasks with lower noise ratios. To create a versatile meta-label correction network, we analyzed the reasons behind performance degradation in specific scenarios and introduced the transferable meta-label correction algorithm, TMLC-Net. TMLC-Net addresses distribution shift issues by incorporating temporal modeling during training. It employs class-aware and dataset-aware normalization mechanisms to mitigate dimensionality disparities. Trained TMLC-Net serves as a universal noise label corrector, offering a plug-and-play, model-agnostic, and dataset-agnostic solution. TMLC-Net alleviates the computational burden associated with bi-level optimization. Finally, we validate the transferability and superiority of TMLC-Net on a diverse set of synthetic and real-world datasets.


\end{abstract}




%%%%%%%%% BODY TEXT
\section{Introduction}
In recent years, deep learning has achieved significant success across various domains, owing much of its progress to large-scale, high-quality datasets [GPT][difution]. The collection of high-quality datasets is both time-consuming and expensive, and inevitably, conventional datasets contain samples with noisy labels. Deep neural networks (DNNs) are susceptible to overfitting when trained on data with noisy labels, making noise label learning a critical challenge in the field of deep learning.

\begin{figure}[t] 
\centering
\includegraphics[width=0.45\textwidth]{img/title.png}
\caption{An overview of the pipeline of our TMLC-NET.}
\label{fig5}
\end{figure}

To address the issue of noisy labels, various label correction methods have been proposed, such as label smoothing and bootstrapping. Currently, these traditional label correction approaches typically require the manual pre-definition of label correction rules, which presents challenges due to the significant variability of such strategies for different problems. Recently, meta-learning-based label correction strategies have gained widespread adoption, addressing this issue by identifying and correcting potential noisy labels using a small amount of clean validation data. To tackle the computational challenge, LCN employs a k-step ahead stochastic gradient descent (SGD) update to compute the metagradient. DMLP decouples the label correction process into unsupervised representation learning and a simple meta-label purifier, thereby enhancing efficiency and accuracy.



% \begin{figure*}[t] 
% \centering
% \includegraphics[width=0.98\textwidth]{img/pipline.png} 
% \caption{An overview of the pipeline of our TMLC-NET.}
% \label{fig5}
% \end{figure*}

\begin{figure*}[t]
  \centering
    \begin{subfigure}{0.245\linewidth} 
    \includegraphics[width=0.99\linewidth]{img/dist.png}
    \caption{Distribution drift.}
    \label{fig:short-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.245\linewidth} 
    \includegraphics[width=0.99\linewidth]{img/dist2.png}
    \caption{Distribution drift verification.}
    \label{fig:short-a}
  \end{subfigure}
   \hfill
  \begin{subfigure}{0.245\linewidth} 
    \includegraphics[width=0.99\linewidth]{img/dime1.png}
    \caption{Dimensional inconsistency.}
    \label{fig:short-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.245\linewidth} 
    \includegraphics[width=0.99\linewidth]{img/dime2.png}
    \caption{Normalization Experiment.}
    \label{fig:short-a}
  \end{subfigure}
 

  \caption{Analyzing the Factors Influencing Transferability Performance.}
  \label{fig:short}
\end{figure*}



% \begin{table*}[h]
% \small
% \caption{Transfer Tasks for Evaluation}
% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Task Type} & \textbf{Task ID} & \textbf{Source Task} & \textbf{Target Task} \\
% \hline
% Different Proportions & Task A & 40\% Random + CIFAR10 + ResNet32 & 20\% Random + CIFAR10 + ResNet32 \\
% \hline
%  & Task B & 40\% Random + CIFAR10 + ResNet32 & 60\% Random + CIFAR10 + ResNet32 \\
% \hline
% Different Types & Task C & 40\% Random + CIFAR10 + ResNet32 & 40\% Flipped + CIFAR10 + ResNet32 \\
% \hline
%  & Task D & 40\% Flipped + CIFAR10 + ResNet32 & 40\% Random + CIFAR10 + ResNet32 \\
% \hline
% Different Models & Task E & 40\% Random + CIFAR10 + ResNet32 & 40\% Random + CIFAR10 + ResNet110 \\
% \hline
%  & Task F & 40\% Random + CIFAR10 + ResNet32 & 40\% Random + CIFAR10 + ResNet20 \\
% \hline
% Different Categories & Task G & 40\% Random + CIFAR10 + ResNet32 & 40\% Random + CIFAR100 + ResNet32 \\
% \hline
%  & Task H & 40\% Random + CIFAR100 + ResNet32 & 40\% Random + CIFAR10 + ResNet32 \\
% \hline
% \end{tabular}
% \label{table:experiments}
% \end{table*}

\begin{table*}
\small
\centering
\caption{Attributes and Fixed Conditions Comparison}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Element & Attribute & Fixed Conditions & Source Task & Target Task & PV \\
\hline
Dataset & Proportion &  CIFAR10 / ResNet32 / UNIF / 32*32 / 50K & 0.4 & 0.2 & -0.1 \\
Dataset & Proportion & CIFAR10 / ResNet32 / UNIF / 32*32 / 50K & 0.4 & 0.6 & -1.1 \\
Dataset & Type & CIFAR10 / ResNet32 / 40\% / 32*32 / 50K & UNIF & Flip & 0.9 \\
Dataset & Type & CIFAR10 / ResNet32 / 40\% / 32*32 / 50K & Flip & UNIF & -1.2 \\
Dataset & Category & ResNet32 / UNIF / 40\% / 32*32 / 50K & CIFAR10 & CIFAR100 & 1.9 \\
Dataset & Category & ResNet32 / UNIF/40\% / 32*32 / 50K & CIFAR100 & CIFAR10 & -1.3 \\
Dataset & Quantity & CIFAR10 / ResNet32 / UNIF / 40\%  / 32*32 & 50k & 5k & 2.9 \\
Dataset & Quantity & CIFAR10 / ResNet32 / UNIF / 40\%  / 32*32 & 5k & 50k & -1.4 \\
Dataset & Resolution & CIFAR10 / ResNet32 / UNIF / 40\%  / 50K & 32*32 & 224*224 & 2.9 \\
Dataset & Resolution & CIFAR10 / ResNet32 / UNIF / 40\%  / 50K & 224*224 & 32*32 & -1.4 \\
Model & Performance & CIFAR10 / UNIF / 40\% / 32*32 / 50K & ResNet20 & ResNet110 & -1.4 \\
Model & Performance & CIFAR10 / UNIF / 40\% / 32*32 / 50K & ResNet110 & ResNet20 & -1.4 \\
\hline
\end{tabular}
\end{table*}






Meta-learning inherently possesses the ability to adapt across tasks. In this work, we focus on the potential of meta-label correction networks for cross-task transfer. If the answer is affirmative, we can obtain a universal label correction network, significantly reducing the substantial computational resources required for the dual-optimization process. We empirically analyze the transferability of meta-label correction algorithms across four dimensions: cross-scale, cross-type, cross-model, and cross-category. Among these, cross-category transfer is the most important. The transfer across different category datasets places higher demands on the transferability of meta-label correction networks and greatly broadens the utility of label correction. With this consideration, we choose MSLC as the base network, which generates new soft labels through the combination of predictions, past predictions, and labels. MSLC parameterizes the combination ratio to achieve label correction. It possesses a potential advantage over other meta-label correction algorithms as it is independent of the number of classes. Detailed transfer experiments are presented in Table XXX.





Here, we assess the transferability of a single variable; mixed-type transferability is also evaluated but involves multiple combinations and is presented in the supplementary materials. Figure XXX illustrates the results of the eight experiments in Table XXX, leading to the following conclusions: (1) Cross-type and cross-scale transferability is relatively good but weaker than the baseline. (2) Cross-model and cross-category transferability is poor. (3) And so on.

We further visualize the input conditions of the meta-network, as evident from Table XXX: (1) Distribution shift is the primary hindrance to the decrease in MSLC's transferability. The current MSLC is a static label correction process and lacks temporal modeling capability. (2) Inconsistent scaling is a major factor directly leading to the decrease in MSLC's transferability. When there are unseen loss inputs, MSLC lacks the corresponding handling mechanism.


Based on this analysis, we propose TMLC-Net, a transferable label correction network. Specifically, during the training phase, we transform MSLC's label correction from a static to a dynamic process, enhancing its temporal modeling capability. Additionally, we introduce normalization constraints from two dimensions: class-awareness and dataset-awareness to maintain model inputs consistently. During the testing phase, we utilize TMLC trained on the source task to directly transfer to other tasks for label correction. The contributions of this work can be summarized as follows.
\begin{itemize}
\item We evaluate the transferability of existing meta-label correction algorithms from multiple dimensions and analyze the primary factors constraining their transferability.
\item We introduce transferable TMLC-Net, a universal, model-agnostic, and category-independent meta-label correction network, significantly expanding the utility of label correction and reducing the computational burden of meta-label correction, especially on large datasets.
\item TMLC-Net demonstrates excellent performance improvement and robust transferability on numerous real and synthetic datasets.
\end{itemize}









\section{Related Work}
\subsection{Learning with Label Correction}
The primary ${l_i^t}/\bar{l}_{y_i}^t$ focus of prior research in this domain has been on enhancing the quality of weak labels by introducing assumptions regarding the underlying process of generating noisy labels. In the context of classifying data into k categories, label correction endeavors to estimate a label corruption matrix, denoted as $C_k×k$, where each entry $C_ij$ represents the probability of encountering a noisy label for class i when the true underlying class label is actually j (Han et al., 2018a; Yao et al., 2020; Xia et al., 2019). An illustrative example of this approach is gold loss correction (Hendrycks et al., 2018). However, a notable limitation of this line of research is the ad hoc nature of estimating the label corruption matrix and the separation of this estimation process from the core model. This separation impedes any potential feedback from the main model to the estimation process. Furthermore, the estimated label corruption matrices are typically global in scope, thereby overlooking data-dependent noise patterns—a scenario frequently encountered in real-world label noise scenarios (Xia et al., 2020).


\subsection{Label correction}
Real data usually conform to a skewed or even a long-tail distribution. In long-tail classification, the proportions of tail samples are considerably small compared with those of head samples. Long-tail classification may be divided into two main strategies. The first strategy is to design new network architectures. Zhou et al.~\cite{zhou2020bbn} designed a bilateral-branch network to learn the representations of head and tail samples. The second strategy is to modify the training loss. In this way, the weighting scheme~\cite{fan2017learning} is the most common practice. Relatively larger weights are exerted on the losses of the tail samples. Besides weighting, some recent studies modify the logits to change the whole loss, such as logit adjustment (LA)~\cite{menon2020long}. This new path achieves higher accuracies in benchmark data corpora compared with weighting~\cite{wu2021adversarial}.





\section{Method}

% We define the symbols employed throughout the paper. Let $D=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^N$ be a training set of $N$ samples, where $\boldsymbol{x}_i$ is the $i$-th sample with the label $y_i$. We consider a noise label task setup, involving  $C$ categories. Moreover, the cross entropy (CE) loss is used. Let $\hat{y}^t_i$, $\tilde{y}^t_i$, and $l_i^{t}$ be the prediction, pseudo label, and  CE loss of $\boldsymbol{x}_i$ in the $t$-th epoch, respectively. Let $\boldsymbol{p}_i^{t}$ and $\boldsymbol{u}_i^{t}$ be the softmax output and the logit vector of $\boldsymbol{x}_i$ in the $t$-th epoch, respectively. Obviously, $\boldsymbol{p}_i^{t} = \text{softmax}(\boldsymbol{u}_i^{t})$. $T$ is the maximum epoch. During training, $\boldsymbol{f}_{i}^t=[\zeta_{i,1}^{t},\zeta_{i,2}^{t}]$ is obtained at the $t$-th epoch. $\zeta_{i,k}^{t}$ refers to the $k$-th training dynamics quantity of the $t$-th epoch for $\boldsymbol{x}_i$. 

We introduce the notations used throughout the paper. Let $D=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^N$ represent a training set of $N$ samples, where $\boldsymbol{x}_i$ is the $i$-th sample with label $y_i$. We focus on a noise label task setup involving $C$ categories, with the cross-entropy (CE) loss being employed. In this context, $\hat{y}^t_i$, $\tilde{y}^t_i$, and $l_i^t$ denote the prediction, pseudo-label, and CE loss of $\boldsymbol{x}_i$ in the $t$-th epoch, respectively. Additionally, $\boldsymbol{p}_i^{t}$ and $\boldsymbol{u}_i^{t}$ refer to the softmax output and logit vector of $\boldsymbol{x}_i$ in the $t$-th epoch, with $\boldsymbol{p}_i^{t} = \text{softmax}(\boldsymbol_i^{t})$. The parameter $T$ represents the maximum number of epochs. During training, $\boldsymbol{f}_i^t = [\zeta_{i,1}^{t}, \zeta_{i,2}^{t}]$ is computed at the $t$-th epoch, where $\zeta_{i,k}^{t}$ corresponds to the $k$-th dynamic training quantity of $\boldsymbol{x}_i$ during the $t$-th epoch.



% \begin{figure*}[t]
%   \centering
%     \begin{subfigure}{0.45\linewidth} 
%     \includegraphics[width=0.99\linewidth]{img/tmlc.png}
%     \caption{An overview of the pipeline of our TMLC-NET.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.45\linewidth} 
%     \includegraphics[width=0.99\linewidth]{img/tse.png}
%     \caption{An overview of the pipeline of our TMLC-NET.}
%     \label{fig:short-a}
%   \end{subfigure}
%    \hfill

 

%   \caption{Analyzing the Factors Influencing Transferability Performance.}
%   \label{fig:short}
% \end{figure*}


\begin{figure}[t] 
\centering
\includegraphics[width=0.4\textwidth]{img/tmlc.pdf} 
\caption{An overview of the pipeline of our TMLC-NET.}
\label{fig5}
\end{figure}

\begin{figure}[t] 
\centering
\includegraphics[width=0.4\textwidth]{img/tse.pdf} 
\caption{An overview of the pipeline of our TSE.}
\label{fig5}
\end{figure}





\subsection{TMLC-Net}

The computational graph of TMLC-Net is depicted in Fig.4(a), Let $\mathcal{A} ( \cdot , \cdot ;{\phi _s})$ denote TMLC-Net.
    \begin{equation}
        \tilde y = g(y,I;\Phi )
    \end{equation}



    \begin{equation}
        \tilde y^{t} = g(y,I^{t},\theta^{t} ;\Phi )
    \end{equation}
    
% 为了解决分布不一致的问题，我们对TMLC-Net的输入做了归一化，
\textbf{Normalized Noise Perception} First, inspired by existing relevant studies, the following epoch-wise quantities for a training sample are adopted:


\begin{itemize}

    \item Category-normalized loss: The ratio between the sample loss and average category loss.
    % $\zeta_{i,0}^t = {l_i^t}/\bar{l}_{\mathcal{N}_{i,K}^t}$

    \begin{equation}
        \zeta_{i,0}^t = {l_i^t}/\bar{l}_{\mathcal{N}_{i,K}^t}
    \end{equation}

    \item Global-normalization loss: The ratio between the sample loss and average corpus loss.
    % $\zeta_{i,1}^t = l_i^t/\bar{l}_{Cp}^t$
    \begin{equation}
        \zeta_{i,1}^t = l_i^t/\bar{l}_{Cp}^t
    \end{equation}
\end{itemize}



\textbf{Time-series encoding}


    \begin{align}
    \centering
        \begin{array}{l}
        \left( {\begin{array}{*{20}{c}}
        {{I_t}}\\
        {{F_t}}\\
        {{O_t}}\\
        {{u_t}}
        \end{array}} \right) = \left( {\begin{array}{*{20}{c}}
        \sigma \\
        \sigma \\
        \sigma \\
        {\tanh }
        \end{array}} \right){W_2}\left( {\begin{array}{*{20}{c}}
        {relu}\\
        {relu}
        \end{array}} \right){W_1}\left( {\begin{array}{*{20}{c}}
        {{h_{t - 1}}}\\
        {{f_t}}
        \end{array}} \right),\\
        {c_t} = {F_t} \odot {c_{t - 1}} + {I_t} \odot {u_t}\\
        {h_t} = {O_t} \odot \tanh ({c_t})
        \end{array}
    \end{align}


    
\textbf{Subclass decoding}

    \begin{equation}
        \tilde y^{t} = \sigma ({W_a}[{\bar f_t};{h_t}])
    \end{equation}

\subsection{Learning Algorithm of TMLC-Net}


\textbf{Meta-Train: adapting to the training dynamics of DNN.}
TMLC-Net can be meta-trained to improve the generalization performance of unseen validation data in DNN training by solving the following optimization problem.

\begin{equation}
\begin{aligned}
{L_{\rm{D}}}(\theta ,w) = \frac{1}{N}\sum\limits_{i = 1}^N {l(f({x_i};w),\tilde y_i^t}).
\end{aligned}
\label{meta3}
\end{equation}


\begin{equation}
    \tilde y^{t} = g(y,I^{t},\theta^{t} ;\Phi ), t=0,1,\dots T-1
\end{equation}


\textbf{Updating $\phi$}

\begin{equation}
\begin{aligned}
\boldsymbol{\Omega}^{(t+1)}=\boldsymbol{\Omega}^{(t)}-\left.\beta \frac{1}{m} \sum_{i=1}^{m} \nabla_{\boldsymbol{\Omega}} \ell_{i}^{\text {meta }}\left(\hat{\boldsymbol{\theta}}^{(t)}(\boldsymbol{\Omega})\right)\right|_{\boldsymbol{\Omega}^{(t)}},
\end{aligned}
\label{meta2}
\end{equation}

\textbf{Updating $w$}

\begin{equation}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{(t)}(\boldsymbol{\Omega})=\boldsymbol{\theta}^{(t)}-\alpha \frac{1}{n}\left.\sum_{i=1}^{n} w\left(\boldsymbol{d}_{i} ; \boldsymbol{\Omega}\right) \nabla_{\boldsymbol{\theta}} \ell_{i}^{\text{train}}(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}^{(t)}},
\end{aligned}
\label{meta1}
\end{equation}

\begin{equation}
\begin{aligned}
\boldsymbol{\theta}^{(t+1)}=\boldsymbol{\theta}^{(t)}-\alpha \frac{1}{n}\left.\sum_{i=1}^{n} w\left(\boldsymbol{d}_{i}; \boldsymbol{\Omega}^{(t+1)}\right) \nabla_{\boldsymbol{\theta}} \ell_{i}^{\text {train }}(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}^{(t)}}.
\end{aligned}
\label{meta3}
\end{equation}


\textbf{Meta-Test: generalization to new heterogeneous tasks.}

After the meta-training phase, the meta-learning TMLC-Net with parameter ${\phi _{\rm{T}}} $ will be used to correct the soft labels in a new DNN training task. In order to better preserve proper soft-labeling during DNN training dynamics, we prefer to keep multiple forms of TMLC-Net with parameters ${\phi _{\rm{s}}} $, $s \in S \subset \{ 1, \cdots ,T\}$ (e.g.,${\phi _{T/3}},{\phi _{2T/3}},{\phi _T} $ used in our experiments) and use them as corrected soft-labels in different iterations of the meta-testing phase. The new parameters u of the DNN for the new task are updated as follows (refer to Algorithm 2 for the whole meta-testing process)




\begin{equation}
\begin{aligned}
{L_{\rm{D}}}(\theta ,w) = \frac{1}{N}\sum\limits_{i = 1}^N {l(f({x_i};w),\tilde y_i^t}).
\end{aligned}
\label{meta3}
\end{equation}


\begin{equation}
\begin{aligned}
\boldsymbol{\theta}^{(t+1)}=\boldsymbol{\theta}^{(t)}-\alpha \frac{1}{n}\left.\sum_{i=1}^{n} \nabla_{\boldsymbol{\theta}} \ell_{i}^{\text {train }}(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}^{(t)}}.
\end{aligned}
\label{meta3}
\end{equation}

where ${\phi _{\rm{s}}},s \in S $  are the parameters of the meta-learning TMLC-Net subset. This means that we can reduce multiple LRs This means that we reduce multiple LRs plan to set up the rules and dynamically adopt specific rules over different ranges of DNN training iterations. It can be seen that the meta-learning TMLC-Net is plug-and-play and does not require tuning of additional hyperparameters.


\begin{figure}[t] 
\centering
\includegraphics[width=0.45\textwidth]{img/pipline.pdf} 
\caption{An overview of the pipeline of our TMLC-NET.}
\label{fig5}
\end{figure}


\section{Experiments}
\subsection{Meta-Train: Evaluation of the LR Schedules Meta-learned by TMLC-Net}


{We utilize three benchmark image classification datasets, namely CIFAR10~\cite{cifar10}, Food101~\cite{Food101}, and Clothing1M~\cite{xiao2015learning}, which represent small image scenes, large image scenes, and real-world scenes, respectively. CIFAR10, Food101, and Clothing1M (using only samples re-labeled by humans) contain 60,000 (10 classes), 75,750 (101 classes), and 72,409 (14 classes) samples, respectively. We introduce two types of noise, SN and AN, with noise rates set to 20\% and 40\%. Additionally, to validate the effectiveness of the algorithm in real-world noisy scenarios, we also compile another version of Clothing1M dataset, which consists of 72,409 manually corrected samples, including a training set with 47,570 samples, a validation set with 14,313 samples, and a test set with 10,526 samples. The noise percentage of the training set is 51.79\%, and we use the data with noise for training, while the manually corrected true labels are used to calculate the F1 score.}

The evaluation used for O2UNet~\cite{huang2019o2u} is followed. In each method, the top-$N*r$ samples are selected as the noisy detected samples according to their estimated scores. The detection is repeated three times for each method, and the average F1 scores on the detection results are calculated. The competing methods include Loss (the epoch loss corresponding to the best validation performance), O2UNet~\cite{huang2019o2u}, MentorNet~\cite{bai2021me}, Co-teaching~\cite{cook1995co}, VOG~\cite{agarwal2022estimating}, SPR~\cite{wang2022scalable}, and SimiFeat~\cite{zhu2022detecting}. {VGG16~\cite{simonyan2014very}, ResNet18~\cite{he2016deep}, ResNet34~\cite{he2016deep}, DenseNet~\cite{huang2017densely} are used as backbones, respectively.} And we employ SGD with a momentum of $0.9$ and a weight decay of $5e$-4. The initial learning rate is $0.1$, then decayed by $0.1$ at epochs $60$ and $100$. The number of epochs is $120$.
%The model was trained for 120 epochs. 
The settings of other methods follow their published papers. 



\begin{table*}[t]
\footnotesize
\centering
\caption{Test accuracy~(\%) of the three imbalance datasets on four models.}\vspace{-0.1in}
% \begin{tabular}{|c||p{0.8cm}|p{0.85cm}|p{1.2cm}||p{0.8cm}|p{0.85cm}|p{0.75cm}||p{0.8cm}|p{0.85cm}|p{0.75cm}|p{0.75cm}|}\hline 
\begin{tabular}{|c||c|c|c||c|c|c||c|c|c||c|}\hline 

\multirow{2}*{Method} & \multicolumn{3}{c||}{VGG16} & \multicolumn{3}{c||}{ResNet32}& \multicolumn{3}{c||}{DenseNet} & \multirow{2}*{Mean}  \\ \cline{2-10} 
& C(10:1) & C(100:1)& \footnotesize{EM-By} & C(10:1) & C(100:1)&EM-By & C(10:1) & C(100:1)&EM-By & \\ \cline{2-10}  \hline
    CE Loss &84.50 & 68.97 & 88.34 & 86.18 & 70.14 & 89.67  &86.56    &70.48    &89.94 & 81.64\\
    Focal loss& 84.83& 68.96& 88.15  & 86.66 & 70.38& 89.99  & 87.01 &70.63 &90.39 & 81.89 \\
    CB  & 85.19 &71.32&88.81 & 86.90 & 72.68& 89.87   & 87.33 &73.00   &90.26  & 82.82 \\
    CB Focal loss &85.93 &73.04 &89.41 & 87.48 & 74.57& 90.32 &  87.75 &74.94& 90.66 & 83.79 \\
    CB fine-tuning & 82.27 &  69.66 &  86.91 & 83.17 & 71.34 & 88.51  &83.61 &71.71& 88.78 & 80.66 \\
    L2RW  &80.96& 71.04& 88.14 & 82.12 & 72.23& 89.03  & 82.44 &72.55 &89.34  & 80.87\\
    LDAM &85.94 &72.39& 89.78 & 87.32 & 73.55 & 91.15 &87.54 &73.83& 91.46 & 83.66\\
    LDAM-DRW  & 87.42 &76.96& 90.52 & 88.37 & 78.12 & 91.63 & 88.73& 78.41& 91.93& 85.79 \\
    LPL &87.80 & 76.93& 91.25 & 89.41 &  77.95& 92.03 & 89.70 & 78.31& 92.25& 86.18 \\
    IB loss  &87.38& 76.67& 90.53 & 88.25& 78.26& 91.84  & 88.52  & 78.59 & 92.17 & 85.80 \\
    Meta-Weight-Net &85.90&  71.88 & 89.83 & 87.55& 73.57 & 91.40  & 87.81& 73.87& 91.64& 83.72\\
    Meta-Class-Weight & 87.98  & 75.27& 90.39 &88.85 &76.41& 91.81 &89.33&  76.71& 92.20& 85.44 \\
    \hline
    MWN-TDQ & \underline{88.01} & \underline{78.60} & \underline{91.46}  & \underline{89.58} & \underline{79.94} & \underline{92.42}  & \underline{89.82}& \underline{80.27} &\underline{92.75} & \underline{86.98} \\
    MWN-DRTS  & \textbf{88.62} & \textbf{79.19} & \textbf{92.21} & \textbf{90.32} & \textbf{80.76} & \textbf{93.11} &  \textbf{90.97} & \textbf{81.14} & \textbf{93.78} & \textbf{87.79} \\\hline
\end{tabular}  \vspace{-0.2in}

\label{table:imblance}
\end{table*}

\subsection{Meta-Test: Transferability and Generalization capability of the LR Schedules Meta-learned by TMLC-Net}

Please refer to the author guidelines on the \confName\ \confYear\ web page for a
discussion of the policy on dual submissions.

\subsection{Ablation Study}
Papers, excluding the references section, must be no longer than eight pages in length.
The references section will not be included in the page count, and there is no limit on the length of the references section.
For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
{\bf There will be no extra page charges for \confName\ \confYear.}

\subsection{FURTHER ANALYSIS ON TMLC-Net}

Overlength papers will simply not be reviewed.
This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.




\begin{algorithm}[t]

    \caption{Meta-Train Algotithm of TMLC-Net}
    \label{alg1}
    \textbf{Input}: $D^{\text{train}}$, $D^{\text{meta}}$, max iterations $T$, updating period $T_{meta}$.\\
       \textbf{Output}: Model parameter $w_T$,  TMLC-Net parameter $\Phi_s$, $s \subset S\{ 1, \cdots ,T\}$.
    \begin{algorithmic}[1]\vspace{-0.0in}
\STATE {Initialize Model parameter $w_0$, and TMLC-Net cell $\theta_{0} = (h_{0}, c_{0})^T$, and TMLC-Net parameter $\Phi_0$;}
\FOR{$t = 0$ to $T-1$}
    \STATE{Sample a batch of samples from $D^\text{train}$;}
    \IF{ $t \% {T_{val}} = 0$ }
        \STATE {Sample a batch of samples from $D^\text{meta}$;}
        \STATE {Update $\Phi_{t+1}$ by Eq.[21];}
    \ENDIF
    \STATE{Update $w_{t+1}$ using Eq.[21];}
\ENDFOR
    \end{algorithmic}
% \vspace{-0.05in}
\end{algorithm}



\begin{algorithm}[t]

    \caption{Meta-Test Algotithm of TMLC-Net}
    \label{alg1}
    \textbf{Input}: $D^{\text{train}}$, for new task $\mu$, max iterations $T$, meta-learned TMLC-Net  $\mathcal{A}( \cdot , \cdot ;{\phi _s}),s \in S$.\\
       \textbf{Output}: Model parameter $u_T$
    \begin{algorithmic}[1]\vspace{-0.0in}
\STATE {Initialize Model parameter $u_0$, and TMLC-Net cell $\theta_{0} = (h_{0}, c_{0})^T$, and choose the subset of meta-learned TMLC-Net $\Phi_s,s \subset S\{ 1, \cdots ,T\}$} for test
\FOR{$t = 0$ to $T_{\mu}-1$}
    \STATE{Sample a batch of samples $D_{Tr}^{\mu}$ from $D^\text{train}$;}
    \STATE {Compute the loss  Eq.[21]; and then TMLC-Net predicts the soft label for current iteration}
    \STATE{Update $w_{t+1}$ using Eq.[21];}
\ENDFOR
    \end{algorithmic}
% \vspace{-0.05in}
\end{algorithm}


%------------------------------------------------------------------------
\section{Conclusions}

This study conducts a pilot work on constructing deep representations for the training dynamics of DNNs and applying them for training process mining and practical tasks in image classification. First, a wide range of TD quantities is extracted for training samples in a learning task. Second, a new learning strategy is proposed to generate a deep representation from the raw TD quantities. Subsequently, several potential mining and application studies are discussed. Experiments on 160 (144 + 16) image classification tasks provide initial answers for several interesting problems for the training process of DNNs. Results on noisy label detection and imbalance learning demonstrate that our learned representations are useful, and the pre-trained noisy label detector GNLD is quite promising.  



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
