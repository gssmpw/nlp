\section{Related Work}
\label{sec:related_work}

The problem of learning from noisy labels has a long history in machine learning, and the recent success of deep learning has renewed interest in this area.  Here, we provide a comprehensive review of related work, focusing on the most relevant approaches and highlighting their connections to our proposed TMLC-Net.

\subsection{Noisy Label Learning}
\label{sec:related_noisy}

Methods for handling noisy labels can be broadly classified into the following categories:

\subsubsection{Loss Correction}
\label{sec:related_noisy_loss}

These methods aim to mitigate the impact of noisy labels by modifying the loss function.  Some common strategies include:

Robust Loss Functions: Employing loss functions that are inherently less sensitive to outliers, such as the Mean Absolute Error (MAE) \cite{ghosh2017robust}, Huber loss \cite{huber1964robust}, or generalized cross-entropy loss \cite{zhang2018generalized}. These functions down-weight the contribution of samples with large losses, which are more likely to be noisy.
Loss Adjustment:  Estimating the noise rate or noise transition matrix and using this information to adjust the loss function. Examples include forward and backward correction methods \cite{patrini2017making, hendrycks2018deep, northcutt2017learning, liu2020early}. These approaches try to explicitly model the label corruption process.
Bootstrapping:  A technique where the target labels are modified by combining the given noisy labels with the model's own predictions \cite{reed2014training, arazo2019unsupervised}. This can be seen as a form of self-training.
Regularization: Applying regularization to make the model more noise tolerant \cite{laine2016temporal, miyato2018virtual}.

\subsubsection{Sample Selection}
\label{sec:related_noisy_sample}

These methods focus on identifying and either removing or down-weighting potentially noisy samples during training.

MentorNet:  Jiang et al. \cite{jiang2018mentornet} proposed MentorNet, a meta-learning approach where a "mentor" network learns to select clean samples for training a "student" network.  The mentor network is typically pre-trained on a small, clean dataset.
Co-teaching: Han et al. \cite{han2018co} introduced Co-teaching, where two networks are trained simultaneously, and each network selects small-loss samples to teach the other network. This is based on the idea that networks trained with different initializations will disagree on noisy samples. Co-teaching+ \cite{yu2019does} improves upon this by incorporating a disagreement-based strategy for sample selection.
Iterative Methods:  These methods involve iteratively training a model, identifying potentially noisy samples (e.g., based on high loss or disagreement between multiple models), and either removing or relabeling those samples \cite{wang2018iterative}.
Active Learning: Although traditionally used for selecting the most informative samples to be labeled, active learning principles can be adapted to identify potentially noisy samples for relabeling or closer inspection \cite{settles2009active}.

\subsubsection{Label Correction}
\label{sec:related_noisy_label}

These methods attempt to directly correct the noisy labels in the training data.

Joint Optimization: Some approaches formulate the learning problem as a joint optimization over the model parameters and the true labels \cite{tanaka2018joint, li2020dividemix, yi2019probabilistic}. These methods often involve alternating between updating the model parameters and estimating the true labels.
Meta-Learning for Label Correction:  This is the category most relevant to our work. Several recent papers have explored using meta-learning to learn a label correction function.  Ren et al. \cite{ren2018meta} proposed Meta-Weight-Net, which learns to assign weights to training samples based on their gradients.  Li et al. \cite{li2019learning} proposed a meta-learning approach for learning a label correction function in a few-shot learning setting. Zheng et. al \cite{zheng2020error} introduces a meta-learning module to estimate the instance-dependent label transition matrix.
Graph-Based Methods:  These methods construct a graph where nodes represent samples and edges represent similarity. Noisy labels are then corrected based on the labels of neighboring nodes \cite{wu2020topological, wang2021noise}.
Reweighting methods: These methods focus on re-weighting training samples to minimize the influence of noisy labels on the training.\cite{liu2015classification}

\subsection{Meta-Learning}
\label{sec:related_meta}

Meta-learning, or "learning to learn," aims to develop algorithms that can learn new tasks quickly and efficiently, often with limited data. Key approaches include:

Model-Agnostic Meta-Learning (MAML): MAML \cite{finn2017model} seeks to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will lead to large improvements on new tasks. This is achieved by optimizing for good performance after a few gradient steps on a new task.
Metric-Based Meta-Learning: These methods learn an embedding space where learning is simplified.  Prototypical Networks \cite{snell2017prototypical} learn a metric space where classification can be performed by computing distances to prototype representations of each class.
Recurrent Models:  Recurrent models, such as LSTMs, can be used to process a sequence of data from a new task and learn an update rule or a representation that is suitable for that task \cite{andrychowicz2016learning, munkhdalai2017meta}.

Our work builds upon the meta-learning paradigm, but with a crucial focus on transferability, which has been less explored in the context of noisy label learning.

\subsection{Transfer Learning}
\label{sec:related_transfer}

Transfer learning aims to leverage knowledge learned from one task (the source task) to improve performance on a different but related task (the target task) \cite{pan2009survey, taylor2009transfer}.  While not the central focus of our paper, transfer learning concepts are relevant because our goal is to develop a label correction method that can be transferred to new datasets and noise distributions.  Common transfer learning strategies include fine-tuning pre-trained models, feature extraction, and domain adaptation \cite{ganin2015domain}.

The key distinction of our work is the combination of meta-learning and transfer learning principles to address the problem of noisy labels.  While prior work has explored meta-learning for label correction, our focus on transferability across diverse datasets and noise types, and our specific architectural choices (normalized noise perception, time-series encoding, and subclass decoding), set our approach apart.