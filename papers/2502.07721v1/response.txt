\section{Related Work}
\label{sec:related_work}

The problem of learning from noisy labels has a long history in machine learning, and the recent success of deep learning has renewed interest in this area.  Here, we provide a comprehensive review of related work, focusing on the most relevant approaches and highlighting their connections to our proposed TMLC-Net.

\subsection{Noisy Label Learning}
\label{sec:related_noisy}

Methods for handling noisy labels can be broadly classified into the following categories:

\subsubsection{Loss Correction}
\label{sec:related_noisy_loss}

These methods aim to mitigate the impact of noisy labels by modifying the loss function.  Some common strategies include:

Robust Loss Functions: Employing loss functions that are inherently less sensitive to outliers, such as the Mean Absolute Error (MAE) **Bousquet and Elisseeff**, "Evaluation Methods for Multitask Learning"__, Huber loss **Huber**, "Robust Estimation of a Location Parameter"__, or generalized cross-entropy loss **Boucheron et al.**, "The Generalized Cross Entropy Method".
Loss Adjustment:  Estimating the noise rate or noise transition matrix and using this information to adjust the loss function. Examples include forward and backward correction methods **Patrini et al.**, "Making During-Inference Count"__. These approaches try to explicitly model the label corruption process.
Bootstrapping:  A technique where the target labels are modified by combining the given noisy labels with the model's own predictions **Ren et al.**, "Learning to Reweight Examples for Robust Model Compositions".
Regularization: Applying regularization to make the model more noise tolerant __.

\subsubsection{Sample Selection}
\label{sec:related_noisy_sample}

These methods focus on identifying and either removing or down-weighting potentially noisy samples during training.

MentorNet:  Jiang et al. **Jiang et al.**, " Mentornet: Learning Data-Driven Curriculum for Very Deep Neural Networks" proposed MentorNet, a meta-learning approach where a "mentor" network learns to select clean samples for training a "student" network.  The mentor network is typically pre-trained on a small, clean dataset.
Co-teaching: Han et al. **Han et al.**, "Co-teaching: Robust Training of Deep Neural Networks with Extremely Labelled Data" introduced Co-teaching, where two networks are trained simultaneously, and each network selects small-loss samples to teach the other network. This is based on the idea that networks trained with different initializations will disagree on noisy samples. Co-teaching+ **Chen et al.**, "Co-teaching+: Robust Training of Deep Neural Networks with Extremely Labelled Data" improves upon this by incorporating a disagreement-based strategy for sample selection.
Iterative Methods:  These methods involve iteratively training a model, identifying potentially noisy samples (e.g., based on high loss or disagreement between multiple models), and either removing or relabeling those samples __.
Active Learning: Although traditionally used for selecting the most informative samples to be labeled, active learning principles can be adapted to identify potentially noisy samples for relabeling or closer inspection __.

\subsubsection{Label Correction}
\label{sec:related_noisy_label}

These methods attempt to directly correct the noisy labels in the training data.

Joint Optimization: Some approaches formulate the learning problem as a joint optimization over the model parameters and the true labels ____. These methods often involve alternating between updating the model parameters and estimating the true labels.
Meta-Learning for Label Correction:  This is the category most relevant to our work. Several recent papers have explored using meta-learning to learn a label correction function.  Ren et al. **Ren et al.**, "Learning to Reweight Examples for Robust Model Compositions" proposed Meta-Weight-Net, which learns to assign weights to training samples based on their gradients.  Li et al. **Li et al.**, "Meta-Learning for Label Correction in Few-Shot Learning" proposed a meta-learning approach for learning a label correction function in a few-shot learning setting. Zheng et al. **Zheng et al.**, "Learning Instance-Dependent Label Transition Matrices" introduces a meta-learning module to estimate the instance-dependent label transition matrix.
Graph-Based Methods:  These methods construct a graph where nodes represent samples and edges represent similarity. Noisy labels are then corrected based on the labels of neighboring nodes __.
Reweighting methods: These methods focus on re-weighting training samples to minimize the influence of noisy labels on the training.__

\subsection{Meta-Learning}
\label{sec:related_meta}

Meta-learning, or "learning to learn," aims to develop algorithms that can learn new tasks quickly and efficiently, often with limited data. Key approaches include:

Model-Agnostic Meta-Learning (MAML): MAML **Finn et al.**, "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" seeks to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will lead to large improvements on new tasks. This is achieved by optimizing for good performance after a few gradient steps on a new task.
Metric-Based Meta-Learning: These methods learn an embedding space where learning is simplified.  Prototypical Networks **Snell et al.**, "Prototypical Networks for Few-Shot Learning" learn a metric space where classification can be performed by computing distances to prototype representations of each class.
Recurrent Models:  Recurrent models, such as LSTMs, can be used to process a sequence of data from a new task and learn an update rule or a representation that is suitable for that task __.

Our work builds upon the meta-learning paradigm, but with a crucial focus on transferability, which has been less explored in the context of noisy label learning.

\subsection{Transfer Learning}
\label{sec:related_transfer}

Transfer learning aims to leverage knowledge learned from one task (the source task) to improve performance on a different but related task (the target task) ____.  While not the central focus of our paper, transfer learning concepts are relevant because our goal is to develop a label correction method that can be transferred to new datasets and noise distributions.  Common transfer learning strategies include fine-tuning pre-trained models, feature extraction, and domain adaptation ___.