@article{imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
}

@inproceedings{rded,
  title={On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm},
  author={Sun, Peng and Shi, Bei and Yu, Daiwei and Lin, Tao},
  booktitle={CVPR},
  year={2024}
}
@inproceedings{
datm,
title={Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching},
author={Ziyao Guo and Kai Wang and George Cazenavette and HUI LI and Kaipeng Zhang and Yang You},
booktitle={ICLR},
year={2024},
}
@inproceedings{TESLA,
  title={Scaling up dataset distillation to imagenet-1k with constant memory},
  author={Cui, Justin and Wang, Ruochen and Si, Si and Hsieh, Cho-Jui},
  booktitle={International Conference on Machine Learning},
  year={2023},
}
@inproceedings{gidaris2018dynamic,
  title={Dynamic few-shot visual learning without forgetting},
  author={Gidaris, Spyros and Komodakis, Nikos},
  booktitle={CVPR},
  year={2018}
}
@inproceedings{kaddour2022flat,
  title={When do flat minima optimizers work?},
  author={Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J},
  booktitle={NeurIPS},
  year={2022}
}
@inproceedings{petzka2021relative,
  title={Relative flatness and generalization},
  author={Petzka, Henning and Kamp, Michael and Adilova, Linara and Sminchisescu, Cristian and Boley, Mario},
  booktitle={NeurIPS},
  year={2021}
}
@inproceedings{yang2020distilling,
  title={Distilling knowledge from graph convolutional networks},
  author={Yang, Yiding and Qiu, Jiayan and Song, Mingli and Tao, Dacheng and Wang, Xinchao},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{
he2024multisize,
title={Multisize Dataset Condensation},
author={Yang He and Lingao Xiao and Joey Tianyi Zhou and Ivor Tsang},
booktitle={ICLR},
year={2024},
}

@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}
@inproceedings{gao2021searching,
  title={Searching for robustness: Loss learning for noisy classification tasks},
  author={Gao, Boyan and Gouk, Henry and Hospedales, Timothy M},
  booktitle={ICCV},
  year={2021}
}
@inproceedings{wang2022cafe,
  title={Cafe: Learning to condense dataset by aligning features},
  author={Wang, Kai and Zhao, Bo and Peng, Xiangyu and Zhu, Zheng and Yang, Shuo and Wang, Shuo and Huang, Guan and Bilen, Hakan and Wang, Xinchao and You, Yang},
  booktitle={CVPR},
  year={2022}
}
@inproceedings{lee2022dataset,
  title={Dataset condensation with contrastive signals},
  author={Lee, Saehyung and Chun, Sanghyuk and Jung, Sangwon and Yun, Sangdoo and Yoon, Sungroh},
  booktitle={ICML},
  year={2022},
}
@article{wang2018dataset,
  title={Dataset distillation},
  author={Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A},
  journal={arXiv preprint arXiv:1811.10959},
  year={2018}
}

@article{zhang2024introduction,
  title={An Introduction to Bilevel Optimization: Foundations and applications in signal processing and machine learning},
  author={Zhang, Yihua and Khanduri, Prashant and Tsaknakis, Ioannis and Yao, Yuguang and Hong, Mingyi and Liu, Sijia},
  journal={IEEE Signal Processing Magazine},
  volume={41},
  number={1},
  pages={38--59},
  year={2024},
  publisher={IEEE}
}
@article{sinha2017review,
  title={A review on bilevel optimization: From classical to evolutionary approaches and applications},
  author={Sinha, Ankur and Malo, Pekka and Deb, Kalyanmoy},
  journal={IEEE transactions on evolutionary computation},
  volume={22},
  number={2},
  pages={276--295},
  year={2017},
  publisher={IEEE}
}
@inproceedings{nguyen2021dataset,
  title={Dataset distillation with infinitely wide convolutional networks},
  author={Nguyen, Timothy and Novak, Roman and Xiao, Lechao and Lee, Jaehoon},
  booktitle={NeurIPS},
  year={2021}
}
@inproceedings{
nguyen2022dataset,
title={Dataset Meta-Learning from Kernel Ridge-Regression},
author={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},
booktitle={ICLR},
year={2021},
}
@inproceedings{shaban2019truncated,
  title={Truncated back-propagation for bilevel optimization},
  author={Shaban, Amirreza and Cheng, Ching-An and Hatch, Nathan and Boots, Byron},
  booktitle={AISTATS},
  year={2019},
}
@inproceedings{dong2022privacy,
  title={Privacy for free: How does dataset condensation help privacy?},
  author={Dong, Tian and Zhao, Bo and Lyu, Lingjuan},
  booktitle={ICML},
  year={2022},
}
@inproceedings{shokri2015privacy,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={SIGSAC},
  year={2015}
}
@inproceedings{masarczyk2020reducing,
  title={Reducing catastrophic forgetting with learning on synthetic data},
  author={Masarczyk, Wojciech and Tautkute, Ivona},
  booktitle={CVPR (Workshop)},
  year={2020}
}
@inproceedings{yu2020semantic,
  title={Semantic drift compensation for class-incremental learning},
  author={Yu, Lu and Twardowski, Bartlomiej and Liu, Xialei and Herranz, Luis and Wang, Kai and Cheng, Yongmei and Jui, Shangling and van de Weijer, Joost},
  booktitle={CVPR},
  year={2020}
}


@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={ICML},
  year={2022},
}
@inproceedings{schuhmann2022laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  booktitle={NeurIPs},
  year={2022}
}
@inproceedings{pmlr-v202-li23q,
  title={{BLIP}-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  year={2023},
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  year={2021},
}
@article{gowda2023watt,
  title={Watt For What: Rethinking Deep Learning's Energy-Performance Relationship},
  author={Gowda, Shreyank N and Hao, Xinyue and Li, Gen and Sevilla-Lara, Laura and Gowda, Shashank Narayana},
  journal={arXiv preprint arXiv:2310.06522},
  year={2023}
}
@inproceedings{sangermano2022sample,
  title={Sample condensation in online continual learning},
  author={Sangermano, Mattia and Carta, Antonio and Cossu, Andrea and Bacciu, Davide},
  booktitle={IJCNN},
  year={2022}
}
@inproceedings{hu2023architecture,
  title={Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning},
  author={Hu, Zixuan and Shen, Li and Wang, Zhenyi and Liu, Tongliang and Yuan, Chun and Tao, Dacheng},
  booktitle={CVPR},
  year={2023}
}
@inproceedings{yin2024squeeze,
  title={Squeeze, recover and relabel: Dataset condensation at imagenet scale from a new perspective},
  author={Yin, Zeyuan and Xing, Eric and Shen, Zhiqiang},
  booktitle={NeurIPS},
  year={2024}
}
@inproceedings{kim2022dataset,
  title={Dataset condensation via efficient synthetic-data parameterization},
  author={Kim, Jang-Hyun and Kim, Jinuk and Oh, Seong Joon and Yun, Sangdoo and Song, Hwanjun and Jeong, Joonhyun and Ha, Jung-Woo and Song, Hyun Oh},
  booktitle={ICML},
  year={2022},
}
@article{zhou2022dataset,
  title={Dataset distillation using neural feature regression},
  author={Zhou, Yongchao and Nezhadarya, Ehsan and Ba, Jimmy},
  journal={NeurIPS},
  year={2022}
}
@inproceedings{wang2022improving,
  title={Improving task-free continual learning by distributionally robust memory evolution},
  author={Wang, Zhenyi and Shen, Li and Fang, Le and Suo, Qiuling and Duan, Tiehang and Gao, Mingchen},
  booktitle={ICML},
  year={2022},
}
@inproceedings{paul2021deep,
  title={Deep learning on a data diet: Finding important examples early in training},
  author={Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
  booktitle={NeurIPS},
  year={2021}
}
@article{yang2205dataset,
    title={Dataset pruning: Reducing training data by examining generalization influence},
  author={Yang, S and Xie, Z and Peng, H and Xu, M and Sun, M and Li, P},
  journal={arXiv preprint arXiv:2205.09329},
  year={2022}
}
@inproceedings{har2004coresets,
  title={On coresets for k-means and k-median clustering},
  author={Har-Peled, Sariel and Mazumdar, Soham},
  booktitle={Proceedings of the thirty-sixth annual ACM symposium on Theory of computing},
  pages={291--300},
  year={2004}
}
@inproceedings{gtp_2022,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle={NeurIPS},
  year={2022}
}
@inproceedings{samsegment,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={ICCV},
  year={2023}
}
@article{haba,
  title={Dataset distillation via factorization},
  author={Liu, Songhua and Wang, Kai and Yang, Xingyi and Ye, Jingwen and Wang, Xinchao},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{rosasco2021distilled,
  title={Distilled replay: Overcoming forgetting through synthetic samples},
  author={Rosasco, Andrea and Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
  booktitle={International Workshop on Continual Semi-Supervised Learning},
  pages={104--117},
  year={2021},
  organization={Springer}
}
@inproceedings{gdumb,
  title={Gdumb: A simple approach that questions our progress in continual learning},
  author={Prabhu, Ameya and Torr, Philip HS and Dokania, Puneet K},
  booktitle={ECCV},
  year={2020},
}
@inproceedings{ftd,
  title={Minimizing the accumulated trajectory error to improve dataset distillation},
  author={Du, Jiawei and Jiang, Yidi and Tan, Vincent YF and Zhou, Joey Tianyi and Li, Haizhou},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{dc2021,
  title={Dataset Condensation with Gradient Matching.},
  author={Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  booktitle={ICLR},
  year={2021}
}

%4 DSA2021
@inproceedings{dsa2021,
  title={Dataset condensation with differentiable siamese augmentation},
  author={Zhao, Bo and Bilen, Hakan},
  booktitle={ICML},
  year={2021},
}
@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{Alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={NeurIPS},
  year={2012}
}

@article{vgg,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{tinyimg,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Ya and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}

@article{cifar10,
  title={{CIFAR}-10 and {CIFAR}-100 datasets},
  author={Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  journal={URl: https://www. cs. toronto. edu/kriz/cifar. html},
  volume={6},
  number={1},
  pages={1},
  year={2009}
}

@inproceedings{franceschi2017forward,
  title={Forward and reverse gradient-based hyperparameter optimization},
  author={Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
  booktitle={ICML},
  year={2017}
}

@inproceedings{truncated_gradient,
  title={Truncated back-propagation for bilevel optimization},
  author={Shaban, Amirreza and Cheng, Ching-An and Hatch, Nathan and Boots, Byron},
  booktitle={AISTATS},
  year={2019},
}


@article{duchi2012randomized,
  title={Randomized smoothing for stochastic optimization},
  author={Duchi, John C and Bartlett, Peter L and Wainwright, Martin J},
  journal={SIAM Journal on Optimization},
  publisher={SIAM},
  volume={22},
  number={2},
  pages={674--701},
  year={2012},
}

@inproceedings{onestep_gradient,
  title={One-step differentiation of iterative algorithms},
  author={Bolte, J{\'e}r{\^o}me and Pauwels, Edouard and Vaiter, Samuel},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={ICML},
  year={2016},
}


@article{wen2018smoothout,
  title={Smoothout: Smoothing out sharp minima to improve generalization in deep learning},
  author={Wen, Wei and Wang, Yandan and Yan, Feng and Xu, Cong and Wu, Chunpeng and Chen, Yiran and Li, Hai},
  journal={arXiv preprint arXiv:1805.07898},
  year={2018}
}


@article{haruki2019gradient,
  title={Gradient noise convolution (gnc): Smoothing loss function for distributed large-batch sgd},
  author={Haruki, Kosuke and Suzuki, Taiji and Hamakawa, Yohei and Toda, Takeshi and Sakai, Ryuji and Ozawa, Masahiro and Kimura, Mitsuhiro},
  journal={arXiv preprint arXiv:1906.10822},
  year={2019}
}


@inproceedings{liu2022random,
  title={Random sharpness-aware minimization},
  author={Liu, Yong and Mai, Siqi and Cheng, Minhao and Chen, Xiangning and Hsieh, Cho-Jui and You, Yang},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{dm,
  title={Dataset condensation with distribution matching},
  author={Zhao, Bo and Bilen, Hakan},
  booktitle={WACV},
  year={2023}
}

@inproceedings{mtt,
  title={Dataset distillation by matching training trajectories},
  author={Cazenavette, George and Wang, Tongzhou and Torralba, Antonio and Efros, Alexei A and Zhu, Jun-Yan},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{rajeswaran2019meta,
  title={Meta-learning with implicit gradients},
  author={Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{gao2022loss,
  title={Loss function learning for domain generalization by implicit gradient},
  author={Gao, Boyan and Gouk, Henry and Yang, Yongxin and Hospedales, Timothy},
  booktitle={ICML},
  year={2022},
}

@inproceedings{lorraine2020optimizing,
  title={Optimizing millions of hyperparameters by implicit differentiation},
  author={Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  booktitle={AISTATS},
  year={2020},
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  year={2017},
}

@article{mackay2019self,
  title={Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions},
  author={MacKay, Matthew and Vicol, Paul and Lorraine, Jon and Duvenaud, David and Grosse, Roger},
  journal={arXiv preprint arXiv:1903.03088},
  year={2019}
}


@inproceedings{maclaurin2015gradient,
  title={Gradient-based hyperparameter optimization through reversible learning},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle={ICML},
  year={2015},
}

@inproceedings{domke2012generic,
  title={Generic methods for optimization-based modeling},
  author={Domke, Justin},
  booktitle={AISTATS},
  year={2012},
}

@inproceedings{vasso,
  title={Enhancing sharpness-aware optimization through variance suppression},
  author={Li, Bingcong and Giannakis, Georgios},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={NeurIPS},
  year={2017}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={ICML},
  year={2017},
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}

@inproceedings{gsam,
  title={Surrogate Gap Minimization Improves Sharpness-Aware Training},
  author={Zhuang, Juntang and Gong, Boqing and Yuan, Liangzhe and Cui, Yin and Adam, Hartwig and Dvornek, Nicha C and s Duncan, James and Liu, Ting and others},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{
esam,
title={Efficient Sharpness-aware Minimization for Improved Training of Neural Networks},
author={Jiawei Du and Hanshu Yan and Jiashi Feng and Joey Tianyi Zhou and Liangli Zhen and Rick Siow Mong Goh and Vincent Tan},
booktitle={ICLR},
year={2022},
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{kim2022fisher,
  title={Fisher sam: Information geometry and sharpness aware minimisation},
  author={Kim, Minyoung and Li, Da and Hu, Shell X and Hospedales, Timothy},
  booktitle={ICML},
  year={2022},
}

@inproceedings{zhang2023flatness,
  title={Flatness-aware minimization for domain generalization},
  author={Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Dong, Yancheng and Tian, Pengfei and Cui, Peng},
  booktitle={ICCV},
  year={2023}
}
@inproceedings{cha2021swad,
  title={Swad: Domain generalization by seeking flat minima},
  author={Cha, Junbum and Chun, Sanghyuk and Lee, Kyungjae and Cho, Han-Cheol and Park, Seunghyun and Lee, Yunsung and Park, Sungrae},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={ICML},
  year={2021},
}

@inproceedings{sharpmaml,
  title={Sharp-maml: Sharpness-aware model-agnostic meta learning},
  author={Abbas, Momin and Xiao, Quan and Chen, Lisha and Chen, Pin-Yu and Chen, Tianyi},
  booktitle={ICML},
  year={2022},
}

@inproceedings{luketina2016scalable,
  title={Scalable gradient-based tuning of continuous regularization hyperparameters},
  author={Luketina, Jelena and Berglund, Mathias and Greff, Klaus and Raiko, Tapani},
  booktitle={ICML},
  year={2016},
}

@article{lee2021online,
  title={Online hyperparameter meta-learning with hypergradient distillation},
  author={Lee, Hae Beom and Lee, Hayeon and Shin, Jaewoong and Yang, Eunho and Hospedales, Timothy and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2110.02508},
  year={2021}
}

@article{sam,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}
