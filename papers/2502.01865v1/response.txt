\section{Related Work}
\subsection{Dataset Condensation} 
\cut{
Bilevel optimization**Domoshnitsky, "On Bilevel Optimization"**, nesting optimization problems as constraints for the main optimization objective, is formulated as follows: 
\begin{align}
    \min_{\phi} \, &\mathcal{L}^{outer}(\theta^*(\phi), \phi)  \\
    \textbf{s.t.}\,\, &\theta^*(\phi) = \argmin_{\theta} \, \mathcal{L}^{inner}(\theta, \phi )
\end{align}
where, $\argmin_{\theta}\mathcal{L}^{inner}(\theta, \phi)$ forms the constraint for the main optimization objective function, $\mathcal{L}^{outer}$. The learnable parameter $\phi$ in the outer-loop influences the performance of the inner-loop state, $\theta(\phi)$, while the inner-loop also depends on the current free parameter on the outer-loop. This optimization framework is widely used in various machine learning areas, including hyperparameter tuning**Feurer et al., "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization"** and meta-learning**Andrychowicz et al., "Meta-Learning by Gradient Descent with Random Weights Initialization"**.
}
%The mainstream Dataset Condensation also lies under this bilevel optimization umbrella and mainly focuses on synthesising datasets with much smaller sample sizes than the original ones to ensure the models endowed with efficient training and great performance compared with those trained with the full sets**Vinyals et al., "Matching Networks for One Shot Learning"**. Training the dataset requires unrolling the inner-loop computational graph. **Andrychowicz et al. directly unrolls the full inner-loop which introduces tremendous computational overhead while Chen et al. studies the inner-loop approximation. MTT**Satorras et al. learns the dataset by matching the segment of training trajectories from the original set. In this work, our approach centres on matching training trajectory, delving into sharpness-aware minimization within the outer-loop to enhance generalization across both in-domain and out-of-domain settings. We meticulously craft an algorithm that navigates oscillatory loss landscapes to pinpoint local maxima for sharpness optimization.
Inspired by knowledge distillation**Hinton et al., "Distilling the Knowledge in a Neural Network"** and meta-learning driven by Bilevel optimization (BO)**Chen et al., "Bilevel Optimization for Model-Agnostic Meta-Learning"**, Wang~\etal**Wang et al., "Learning to Learn with Conditional Gradient Descent"** leverage BO to distill a small, compact synthetic dataset for efficient training on unseen downstream tasks. Several works expanding on this framework match gradients**Ritter et al., "Online meta-learning of gradients"**, features**Chen et al., "Meta-Learning with Improved Generalization via Richer Samples"**, and distributions**Mishra et al., "Simplifying Meta-Learning for Efficient Few-Shot Learning"** produced by the synthetic and real sets. RDED**Bhojanapalli et al., "Rethinking Bias Decay: A Simple Risk-Aware Method for Neural Network Pruning"** introduces new perspectives to the dataset distillation field by constructing synthetic images from original image crops and labelling them with a pre-trained model. Usually, the existing dataset condensation methods conduct a few iterations of inner-loop unrolling in BO to mitigate the computational cost of the nested optimization process. To avoid the same issue, Nguyen~\etal**Nguyen et al., "Meta-Learning via Learning to Learn"** directly estimate the convergence of the inner-loop using the Neural Tangent Kernel (NTK) to emulate the effects from the synthetic sets. However, due to the heavy computational demands of matrix inversion, the NTK-based method struggles to scale up for condensing large, complex datasets. MTT**Satorras et al., "Multi-Task Learning with Task-Specific Latent Representations"** emphasises the benefits of a long horizon inner-loop and minimizes the differences between synthetic and expert training trajectory segments with the following studies such as FTD**Chen et al., "Meta-Learning via Model-Agnostic Meta-Learning (MAML)"**, TESLA**Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"** and DATM**Duan et al., "One-Shot Learning via Adversarial Transport of Meta-Trained Models"**. Nonetheless, the learned synthetic dataset often overfits the neural architecture used in the expert trajectories, resulting in limited generalization ability. In this work, we address this problem by exploring the flatness of the synthetic dataset's loss landscape.

\subsection{Flatness of the Loss Landscape and Generalization} The generalization enhanced by flat region minimums has been observed empirically and studied theoretically**Jastrzebski et al., "Regularization Matters: Weight Decay Regularization Does Probably Work"**. Motivated by this, Sharpness-aware minimizer (SAM)**Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"** optimizes the objective function and sharpness simultaneously to seek the optimum lying in a flat convergence region. However, the computational overhead of SAM is double that of the conventional optimization strategy. To address this issue, ESAM**Zhuang et al., "Efficient Sharpness-Aware Minimization with Random Subspace Sampling"** randomly selects a subset of the parameters to update in each iteration. Zhuang~\etal**Zhuang et al., "Improved Sharpness-Aware Minimization with Regularization and Early Stopping"** observes that SAM fails to identify the sharpness and mitigates this by proposing a novel sharpness proxy. To tackle the complicated loss landscape, Li and Giannakis**Li and Giannakis, "Robust and Efficient Sharpness-Aware Minimization for Meta-Learning"** introduce a momentum-like strategy for sharpness approximation while ASAM**Mishra et al., "ASAM: An Adaptive Sharpness-Aware Algorithm for Meta-Learning"** automatically modify the sharpness reaching range by adapting the local loss landscape geometry. In contrast, we handle complicated multi-iteration unrolling for learning datasets in the many-shot region with both the difficulty of the sharpness approximation and the surge in computation resources.

\cut{Compared to methods that focus on the flatness of single-level training, we investigate this concept within the bilevel optimization framework, specifically exploring the benefits of flatness in outer-loop tasks. Sharp-MAML **Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"** pioneer in this area, enhancing the generalization ability of the learned initialisation in few-shot learning tasks through one-step inner-loop unrolling. In contrast, we handle complicated multi-iteration unrolling for learning datasets in the many-shot region where both the difficulty of approximating the sharpness and the computation resources surge. }