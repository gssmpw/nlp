\section{Related Work}
\subsection{Dataset Condensation} 
\cut{
Bilevel optimization~\citep{sinha2017review,zhang2024introduction}, nesting optimization problems as constraints for the main optimization objective, is formulated as follows: 
\begin{align}
    \min_{\phi} \, &\mathcal{L}^{outer}(\theta^*(\phi), \phi)  \\
    \textbf{s.t.}\,\, &\theta^*(\phi) = \argmin_{\theta} \, \mathcal{L}^{inner}(\theta, \phi )
\end{align}
where, $\argmin_{\theta}\mathcal{L}^{inner}(\theta, \phi)$ forms the constraint for the main optimization objective function, $\mathcal{L}^{outer}$. The learnable parameter $\phi$ in the outer-loop influences the performance of the inner-loop state, $\theta(\phi)$, while the inner-loop also depends on the current free parameter on the outer-loop. This optimization framework is widely used in various machine learning areas, including hyperparameter tuning \citep{lorraine2020optimizing, maclaurin2015gradient, mackay2019self} and meta-learning \citep{finn2017model, gao2022loss, rajeswaran2019meta, gao2021searching}.
}
%The mainstream Dataset Condensation also lies under this bilevel optimization umbrella and mainly focuses on synthesising datasets with much smaller sample sizes than the original ones to ensure the models endowed with efficient training and great performance compared with those trained with the full sets~\citep{wang2018dataset, dc2021}. Training the dataset requires unrolling the inner-loop computational graph. \citet{mtt} directly unrolls the full inner-loop which introduces tremendous computational overhead while \citet{mtt} studies the inner-loop approximation. MTT~\citep{mtt} learns the dataset by matching the segment of training trajectories from the original set. In this work, our approach centres on matching training trajectory, delving into sharpness-aware minimization within the outer-loop to enhance generalization across both in-domain and out-of-domain settings. We meticulously craft an algorithm that navigates oscillatory loss landscapes to pinpoint local maxima for sharpness optimization.
Inspired by knowledge distillation~\citep{gou2021knowledge,yang2020distilling} and meta-learning driven by Bilevel optimization (BO)~\citep{lorraine2020optimizing, maclaurin2015gradient, mackay2019self, finn2017model, gao2022loss, rajeswaran2019meta, gao2021searching}, Wang~\etal~\citep{wang2018dataset} leverage BO to distill a small, compact synthetic dataset for efficient training on unseen downstream tasks. Several works expanding on this framework match gradients~\citep{dsa2021, dc2021,lee2022dataset}, features~\citep{wang2022cafe}, and distributions~\citep{dm} produced by the synthetic and real sets. RDED~\citep{rded} introduces new perspectives to the dataset distillation field by constructing synthetic images from original image crops and labelling them with a pre-trained model. Usually, the existing dataset condensation methods conduct a few iterations of inner-loop unrolling in BO to mitigate the computational cost of the nested optimization process. To avoid the same issue, Nguyen~\etal~\citep{nguyen2021dataset, nguyen2022dataset} directly estimate the convergence of the inner-loop using the Neural Tangent Kernel (NTK) to emulate the effects from the synthetic sets. However, due to the heavy computational demands of matrix inversion, the NTK-based method struggles to scale up for condensing large, complex datasets. MTT~\citep{mtt} emphasises the benefits of a long horizon inner-loop and minimizes the differences between synthetic and expert training trajectory segments with the following studies such as FTD~\citep{ftd}, TESLA~\citep{TESLA} and DATM~\citep{datm}. Nonetheless, the learned synthetic dataset often overfits the neural architecture used in the expert trajectories, resulting in limited generalization ability. In this work, we address this problem by exploring the flatness of the synthetic dataset's loss landscape.

\subsection{Flatness of the Loss Landscape and Generalization} The generalization enhanced by flat region minimums has been observed empirically and studied theoretically \citep{dinh2017sharp, keskar2016large,neyshabur2017exploring}. Motivated by this, Sharpness-aware minimizer (SAM)~\citep{sam} optimizes the objective function and sharpness simultaneously to seek the optimum lying in a flat convergence region. However, the computational overhead of SAM is double that of the conventional optimization strategy. To address this issue, ESAM~\citep{esam} randomly selects a subset of the parameters to update in each iteration. Zhuang~\etal~\citep{gsam} observes that SAM fails to identify the sharpness and mitigates this by proposing a novel sharpness proxy. To tackle the complicated loss landscape, Li and Giannakis~\citep{vasso} introduce a momentum-like strategy for sharpness approximation while ASAM~\citep{asam} automatically modify the sharpness reaching range by adapting the local loss landscape geometry. In contrast, we handle complicated multi-iteration unrolling for learning datasets in the many-shot region with both the difficulty of the sharpness approximation and the surge in computation resources.

\cut{Compared to methods that focus on the flatness of single-level training, we investigate this concept within the bilevel optimization framework, specifically exploring the benefits of flatness in outer-loop tasks. Sharp-MAML \citep{sharpmaml} pioneer in this area, enhancing the generalization ability of the learned initialisation in few-shot learning tasks through one-step inner-loop unrolling. In contrast, we handle complicated multi-iteration unrolling for learning datasets in the many-shot region where both the difficulty of approximating the sharpness and the computation resources surge. }