@inproceedings{TESLA,
  title={Scaling up dataset distillation to imagenet-1k with constant memory},
  author={Cui, Justin and Wang, Ruochen and Si, Si and Hsieh, Cho-Jui},
  booktitle={International Conference on Machine Learning},
  year={2023},
}

@inproceedings{asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={ICML},
  year={2021},
}

@inproceedings{dc2021,
  title={Dataset Condensation with Gradient Matching.},
  author={Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={ICML},
  year={2017},
}

@inproceedings{dm,
  title={Dataset condensation with distribution matching},
  author={Zhao, Bo and Bilen, Hakan},
  booktitle={WACV},
  year={2023}
}

@inproceedings{dsa2021,
  title={Dataset condensation with differentiable siamese augmentation},
  author={Zhao, Bo and Bilen, Hakan},
  booktitle={ICML},
  year={2021},
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  year={2017},
}

@inproceedings{ftd,
  title={Minimizing the accumulated trajectory error to improve dataset distillation},
  author={Du, Jiawei and Jiang, Yidi and Tan, Vincent YF and Zhou, Joey Tianyi and Li, Haizhou},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{gao2021searching,
  title={Searching for robustness: Loss learning for noisy classification tasks},
  author={Gao, Boyan and Gouk, Henry and Hospedales, Timothy M},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{gao2022loss,
  title={Loss function learning for domain generalization by implicit gradient},
  author={Gao, Boyan and Gouk, Henry and Yang, Yongxin and Hospedales, Timothy},
  booktitle={ICML},
  year={2022},
}

@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

@inproceedings{gsam,
  title={Surrogate Gap Minimization Improves Sharpness-Aware Training},
  author={Zhuang, Juntang and Gong, Boqing and Yuan, Liangzhe and Cui, Yin and Adam, Hartwig and Dvornek, Nicha C and s Duncan, James and Liu, Ting and others},
  booktitle={ICLR},
  year={2021}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@inproceedings{lee2022dataset,
  title={Dataset condensation with contrastive signals},
  author={Lee, Saehyung and Chun, Sanghyuk and Jung, Sangwon and Yun, Sangdoo and Yoon, Sungroh},
  booktitle={ICML},
  year={2022},
}

@inproceedings{lorraine2020optimizing,
  title={Optimizing millions of hyperparameters by implicit differentiation},
  author={Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  booktitle={AISTATS},
  year={2020},
}

@article{mackay2019self,
  title={Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions},
  author={MacKay, Matthew and Vicol, Paul and Lorraine, Jon and Duvenaud, David and Grosse, Roger},
  journal={arXiv preprint arXiv:1903.03088},
  year={2019}
}

@inproceedings{maclaurin2015gradient,
  title={Gradient-based hyperparameter optimization through reversible learning},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle={ICML},
  year={2015},
}

@inproceedings{mtt,
  title={Dataset distillation by matching training trajectories},
  author={Cazenavette, George and Wang, Tongzhou and Torralba, Antonio and Efros, Alexei A and Zhu, Jun-Yan},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{nguyen2021dataset,
  title={Dataset distillation with infinitely wide convolutional networks},
  author={Nguyen, Timothy and Novak, Roman and Xiao, Lechao and Lee, Jaehoon},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{rajeswaran2019meta,
  title={Meta-learning with implicit gradients},
  author={Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{rded,
  title={On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm},
  author={Sun, Peng and Shi, Bei and Yu, Daiwei and Lin, Tao},
  booktitle={CVPR},
  year={2024}
}

@article{sam,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}

@inproceedings{sharpmaml,
  title={Sharp-maml: Sharpness-aware model-agnostic meta learning},
  author={Abbas, Momin and Xiao, Quan and Chen, Lisha and Chen, Pin-Yu and Chen, Tianyi},
  booktitle={ICML},
  year={2022},
}

@article{sinha2017review,
  title={A review on bilevel optimization: From classical to evolutionary approaches and applications},
  author={Sinha, Ankur and Malo, Pekka and Deb, Kalyanmoy},
  journal={IEEE transactions on evolutionary computation},
  volume={22},
  number={2},
  pages={276--295},
  year={2017},
  publisher={IEEE}
}

@inproceedings{vasso,
  title={Enhancing sharpness-aware optimization through variance suppression},
  author={Li, Bingcong and Giannakis, Georgios},
  booktitle={NeurIPS},
  year={2024}
}

@article{wang2018dataset,
  title={Dataset distillation},
  author={Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A},
  journal={arXiv preprint arXiv:1811.10959},
  year={2018}
}

@inproceedings{wang2022cafe,
  title={Cafe: Learning to condense dataset by aligning features},
  author={Wang, Kai and Zhao, Bo and Peng, Xiangyu and Zhu, Zheng and Yang, Shuo and Wang, Shuo and Huang, Guan and Bilen, Hakan and Wang, Xinchao and You, Yang},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yang2020distilling,
  title={Distilling knowledge from graph convolutional networks},
  author={Yang, Yiding and Qiu, Jiayan and Song, Mingli and Tao, Dacheng and Wang, Xinchao},
  booktitle={CVPR},
  year={2020}
}

@article{zhang2024introduction,
  title={An Introduction to Bilevel Optimization: Foundations and applications in signal processing and machine learning},
  author={Zhang, Yihua and Khanduri, Prashant and Tsaknakis, Ioannis and Yao, Yuguang and Hong, Mingyi and Liu, Sijia},
  journal={IEEE Signal Processing Magazine},
  volume={41},
  number={1},
  pages={38--59},
  year={2024},
  publisher={IEEE}
}

