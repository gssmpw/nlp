\vspace{-0.2cm}
\section{Evaluating Knowledge Unit Effectiveness}

\begin{table*}[ht]
\centering
\small
\vspace*{-0.25cm}
\caption{\textbf{Knowledge Unit Performance Across Domains (Abstract-Level Analysis).} Each column displays the lower-upper bound performance (no context vs. original text) and the Knowledge Unit (KU) performance for different models. Using KUs preserves most information for answering MCQs, perform close to the using the original text (upper bound) across domains and models.}\vspace{0.1cm}
\setlength{\tabcolsep}{4pt}
 \resizebox{0.9\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\textbf{Model} &
\multicolumn{2}{c}{\textbf{Medical}} &
\multicolumn{2}{c}{\textbf{Computer Science}} &
\multicolumn{2}{c}{\textbf{Mathematics}} &
\multicolumn{2}{c}{\textbf{Physics}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& \textbf{[Lower-Upper]} & \textbf{KU} & \textbf{[Lower-Upper]} & \textbf{KU} & \textbf{[Lower-Upper]} & \textbf{KU} & \textbf{[Lower-Upper]} & \textbf{KU} \\
\midrule
Gemini (1.5-Flash 002) & 42.28--97.17 & 93.37 & 58.56--97.27 & 93.62 & 52.26--94.68 & 91.82 & 34.19--95.30 & 92.97 \\
Qwen 2.5 (7B) & 42.76--97.00 & 92.76 & 58.59--97.27 & 93.45 & 52.29--94.79 & 92.87 & 36.80--95.29 & 92.97 \\
Mistral Small (Dense 22B) & 42.46--97.13 & 92.33 & 58.79--97.37 & 94.70 & 52.33--94.74 & 92.91 & 34.58--95.38 & 90.56 \\
Ministral 2410 (3B) & 42.24--97.06 & 88.22 & 58.48--97.36 & 91.91 & 52.21--94.80 & 87.65 & 33.03--95.29 & 87.14 \\
Llama 3.2 (3B) & 42.52--97.10 & 87.08 & 58.63--97.34 & 88.47 & 51.61--94.82 & 86.44 & 36.89--95.33 & 86.90 \\
Llama 3.1 (8B) & 42.69--97.13 & 85.80 & 58.68--97.31 & 87.75 & 52.00--94.81 & 84.21 & 37.04--95.29 & 85.43 \\
\bottomrule
\end{tabular}}
\vspace*{-0.4cm}

\label{tab:performance_models_vertical}
\end{table*}

\begin{table}[ht]
    \centering
    \small
    \vspace{-0.25cm}
    \caption{\textbf{Knowledge Unit Performance in Longer Documents.} Multiple-choice performance with knowledge units (KU) remains far above the no-context baseline and approaches the original-text upper bound, though it is slightly lower for long documents. This indicates that using KU context preserves most information across different domains and models, with small degradation.}\vspace{0.1cm}
    \setlength{\tabcolsep}{3pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & 
        \multicolumn{2}{c}{\textbf{Physics}} & 
        \multicolumn{2}{c}{\textbf{Medical}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & \textbf{[Lower-Upper]} & \textbf{KU} & \textbf{[Lower-Upper]} & \textbf{KU} \\
        \midrule
        Gemini (1.5-Flash 002) & 49.48--90.72 & 83.51 & 46.96--94.13 & 81.76 \\
        Qwen 2.5 (7B) & 52.23--89.69 & 79.04 & 50.45--93.24 & 88.29 \\
        Mistral Small (Dense 22B) & 50.86--89.35 & 81.44 & 48.31--94.59 & 90.20 \\
        \bottomrule
    \end{tabular}}
     \vspace{-0.35cm}
    \label{tab:performance_models_extended_vertical}

    \vspace{-0.35cm}
\end{table}

One major question is whether removing creative expression still preserves enough factual information to be useful. We designed multiple experiments to study these issues.

\vspace{-0.2cm}
\subsection{Experimental Setup and Design}
\label{sec:exp_setup}

\textbf{Design Principles.} Our evaluation requires scalable benchmarks that adapt across different research domains. To achieve this, we adopt a multiple-choice question (MCQ) design with questions, correct answers and distractors generated by a frontier LM. We chose MCQs for three key reasons: (1) They confirm with users querying knowledge contained in specified original text, providing clearer insights than retrieval metrics; (2) Not requiring manual generation allows for easy customization across various disciplines; (3) Repeating benchmark generation and retesting helps capture broader variance and reduces the risk of overfitting.

\textbf{Key Idea.} We tested how well multiple-choice question (MCQ) performance is preserved when we convert the original text into Knowledge Units data. We created MCQs for each text excerpt, asked language models to answer them with no context (lower bound), then asked them again with the original text (upper bound) for sanity check since they are automatically generated. Finally, we tested them with only the Knowledge Units (our method).

\textbf{Datasets.} We used: 
\begin{enumerate}[leftmargin=*,noitemsep,topsep=0em]
    \item \textit{Abstract-level analysis}: 1{,}000 abstracts each from Biology, Mathematics, and Physics of the peS2o dataset~\cite{peS2o} as well as 1{,}000 abstracts from Computer Science from ArXiv~\cite{clement2019arxiv}. For each abstract, we generate three MCQs (Appendix, Table \ref{tab:abstract_medical_mcqs},~\ref{tab:abstract_math_mcqs},~\ref{tab:abstract_cs_mcqs},~\ref{tab:abstract_phy_mcqs}) and one KU.
    \item \textit{Full-paper analysis}: 200 longer papers (100 Medical, 100 Physics)~\cite{DBLP:conf/naacl/CohanDKBKCG18}. We chunked each paper into segments of 200 words and generated Knowledge Units for each chunk, referencing the previous 10 units for context continuity. For each paper we generate 10 MCQs (Appendix, Table \ref{tab:fullpaper_medical_mcqs} and~\ref{tab:fullpaper_phy_mcqs}).
\end{enumerate}

\textbf{Procedure:} The Gemini Pro 1.5 002 model was utilized to generate all MCQs based on the original text, as well as to provide annotations for the correct answer. The questions are designed to assess specific, verifiable elements, such as factual claims, numerical data, definitions, or relational knowledge, to minimize ambiguity. The accuracy of the answers is verified through cloze evaluation, as is standard in the LM-Harness.


\begin{table*}[ht]
    \centering
    \small
    \vspace*{-0.25cm}
    \caption{\textbf{Average and Top 5\% n-gram Overlap and Plagiarism Check Scores for Gemini (1.5-Flash) and Qwen 2.5 (7B).} n-gram overlap measures Jaccard similarity for sequences of n words. Plagiarism scores indicate textual similarity, with values below 20\% considered negligible. Both metrics remain low between Input Texts and Knowledge Unit or corresponding reconstructed text, preliminarily suggesting minimal direct reproduction of the original text in Knowledge Units.
    } \vspace{0.1cm}
\setlength{\tabcolsep}{4pt}
 \resizebox{0.7\linewidth}{!}{
\begin{tabular}{llcccc}
    \toprule
    \textbf{Model} & \textbf{Data} & \textbf{Plagiarism Score} & \textbf{5-gram Overlap} & \textbf{7-gram Overlap} & \textbf{11-gram Overlap} \\
    \midrule
    \multicolumn{6}{c}{Original Text and Knowledge Unit Overlap}\\ \midrule
    \multirow{2}{*}{Gemini-1.5 Flash} & Overall & 2.7 & 0.009 & 0.003 & 0.001 \\
     & Top 5\% & 14.5 & 0.023 & 0.011 & 0.003 \\ \cmidrule{2-6}
    \multirow{2}{*}{Qwen-2.5 (7B)}           & Overall & 5.9 & 0.028 & 0.015 & 0.005 \\
             & Top 5\% & 22.9 & 0.070 & 0.047 & 0.024 \\ \midrule
    \multicolumn{6}{c}{Original Text and Reconstructed Text Overlap}\\ \midrule
   
\multirow{2}{*}{Gemini-1.5 Flash} 
 & Overall & 3.8  & 0.022 & 0.010 & 0.002 \\
 & Top 5\% & 12.1 & 0.047 & 0.030 & 0.013 \\\cmidrule{2-6}
  \multirow{2}{*}{Qwen-2.5 (7B)} 
 & Overall & 17.8 & 0.142 & 0.123 & 0.098 \\
 & Top 5\% & 24.0 & 0.175 & 0.157 & 0.133 \\

    \bottomrule
\end{tabular}}
\vspace*{-0.4cm}
\label{tab:average_sherlock_ngram_scores}
\end{table*}

\vspace{-0.2cm}
\subsection{Results: Information Retention}

We evaluate the effectiveness of Knowledge Units (KUs) in preserving information from original text by addressing two key questions:

\textbf{Q1: Does conversion retain information from the original text chunks (Abstract-Level Analysis)?}  To answer this, we compared multiple-choice question (MCQ) performance using Knowledge Units against performance using the original text. We tested several small language models, first asking them to answer MCQs based on full original passages, then repeating the test with only Knowledge Units. Table~\ref{tab:performance_models_vertical} presents the results.

\textit{Findings.} As expected, models answering without any context (lower bound) performed significantly worse than those given the full original text (upper bound), confirming that context is crucial for correctly answering the MCQs. While language models could occasionally eliminate incorrect answer choices using prior knowledge, their accuracy remained low without context. When provided with Knowledge Units instead of the original text, model performance closely matched the upper bound in nearly all cases. This suggests that Knowledge Units preserve the majority of relevant information needed for answering questions. The pattern held across different models and research domains. Additionally, the variance in performance across different question sets remained between 3–5\%, indicating a statistically significant — but relatively small — difference between using the original text and using Knowledge Units.

\textbf{Q2: Can conversion preserve information in long documents (full paper analysis)?}  To examine this, we assessed model performance on long-document MCQs, where each document was segmented into multiple Knowledge Units. We used models with larger context windows, both to generate the Knowledge Units and to answer the questions. Table~\ref{tab:performance_models_extended_vertical} summarizes the results.

\textit{Findings.} Consistent with Q1, the gap between the lower and upper bounds remained large, reaffirming the validity of our MCQ evaluation approach. However, we observed a slight decline in the upper bound performance, suggesting that longer documents introduced additional complexity, making some questions harder to answer even with access to the full text. Knowledge Units performed slightly worse in long-document scenarios compared to short text segments. This suggests that aggregating multiple Knowledge Units across long contexts introduces some challenges in reasoning. However, performance remained much closer to the upper bound than the lower bound, confirming that Knowledge Units still retained most of the critical information. As with Q1, variance across different question sets remained within 3–5\%, reinforcing the reliability of these findings.

\textbf{Conclusion.} Knowledge Units effectively preserve information across multiple domains. While performance degradation is observed in long-document scenarios, the majority of factual content remains intact. These results are consistent across different model families and scientific disciplines, demonstrating the robustness of Knowledge Units as a structured knowledge representation format. 


\subsection{Results: Assessing Content Overlap}


We have established that key information is preserved in the generated Knowledge Units (KUs). As a next step, we perform empirical checks to detect potential text reuse. In legal contexts, consistent high n-gram overlap across large spans of text is a commonly used measure to prove textual reuse, though not a definitive measure by any means. Here, we compute n-gram overlaps between the original abstracts and our generated KUs on an abstract-level dataset, with results shown in Table~\ref{tab:average_sherlock_ngram_scores} for the two best models.

\textbf{Q1: Is there a significant overlap?} The top portion of Table~\ref{tab:average_sherlock_ngram_scores} reports 5-gram, 7-gram, and 11-gram Jaccard similarities for the entire dataset, as well as for the top 5\% of the most similar original text–KU pairs. The Gemini-1.5 Flash model consistently exhibits very low overlap ($<$3\% in the most conservative 5-gram scenario), even in the top 5\% subset. The Qwen-2.5 model shows slightly higher scores but remains below 7\% in the same scenario. Overall, these findings indicate negligible direct textual reuse.

\textbf{Q2: Does a plagiarism check show different trends?} We additionally use an open-source plagiarism detector~\cite{sherlock}, which attempts to detect more subtle forms of reuse (e.g., paraphrasing, synonym substitutions). Scores below 20\% are typically dismissed as negligible threshold and are not even displayed, according to official documentation. We apply this check instead of the n-gram metric, keeping everything else constant.  Table~\ref{tab:average_sherlock_ngram_scores} reports averages of 3--5\% for the entire dataset, rising to 15--23\% in the top 5\% of most similar pairs. These numbers remain below or close to even automatic dismissal thresholds, let alone a conservative actionable plagiarism check. This reinforces the findings from n-gram scores, indicating no direct text reuse.

\textbf{Q3: Does reconstructing text increase overlaps?} We next tried our best prompting a strong LM (Gemini-1.5 Pro) to regenerate the original abstract from a knowledge graph, using few-shot examples from the same domain. While Qwen-2.5 shows a substantial increase in n-gram overlap in the top 5\% subset, overlaps for both models, especially Gemini-1.5 Flash, remain extremely low in an absolute sense. Sherlock scores also remain unchanged in all but overall the Qwen-2.5 case. Manual inspection of the highest-overlap (11-gram) passages, with examples provided in Appendix  \ref{sec:appsimilarity}, suggests that most identical segments are filler phrases common in scientific writing, rather than substantive stylistic overlap.

\textbf{Conclusion.} Across both n-gram overlap measures and a dedicated plagiarism detector, evidence of direct text copying remains minimal. Even when models are explicitly prompted to reconstruct the source text, stylistic carryover is surprisingly low. While neither n-gram overlap nor plagiarism checks are legal standards for copyright, they provide preliminary empirical reassurance that the original text is not being substantially reproduced. 
