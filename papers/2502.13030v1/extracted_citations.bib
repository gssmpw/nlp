@article{Hendrycks2019,
abstract = {In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
archivePrefix = {arXiv},
arxivId = {1903.12261},
author = {Hendrycks, Dan and Dietterich, Thomas},
eprint = {1903.12261},
file = {::},
journal = {7th International Conference on Learning Representations, ICLR 2019},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Benchmarking neural network robustness to common corruptions and perturbations}},
year = {2019}
}

@article{Lei2021,
abstract = {Evaluating treatment effect heterogeneity widely informs treatment decision making. At the moment, much emphasis is placed on the estimation of the conditional average treatment effect via flexible machine learning algorithms. While these methods enjoy some theoretical appeal in terms of consistency and convergence rates, they generally perform poorly in terms of uncertainty quantification. This is troubling since assessing risk is crucial for reliable decision-making in sensitive and uncertain environments. In this work, we propose a conformal inference-based approach that can produce reliable interval estimates for counterfactuals and individual treatment effects under the potential outcome framework. For completely randomized or stratified randomized experiments with perfect compliance, the intervals have guaranteed average coverage in finite samples regardless of the unknown data generating mechanism. For randomized experiments with ignorable compliance and general observational studies obeying the strong ignorability assumption, the intervals satisfy a doubly robust property which states the following: the average coverage is approximately controlled if either the propensity score or the conditional quantiles of potential outcomes can be estimated accurately. Numerical studies on both synthetic and real data sets empirically demonstrate that existing methods suffer from a significant coverage deficit even in simple models. In contrast, our methods achieve the desired coverage with reasonably short intervals.},
archivePrefix = {arXiv},
arxivId = {2006.06138},
author = {Lei, Lihua and Cand{\`{e}}s, Emmanuel J.},
doi = {10.1111/rssb.12445},
eprint = {2006.06138},
file = {::},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {causal inference,conformal inference,counterfactual,doubly robust,individual treatment effect,uncertainty quantification},
number = {5},
pages = {911--938},
title = {{Conformal inference of counterfactuals and individual treatment effects}},
url = {http://arxiv.org/abs/2006.06138},
volume = {83},
year = {2021}
}

@article{Sadinle2019,
abstract = {In most classification tasks, there are observations that are ambiguous and therefore difficult to correctly label. Set-valued classifiers output sets of plausible labels rather than a single label, thereby giving a more appropriate and informative treatment to the labeling of ambiguous instances. We introduce a framework for multiclass set-valued classification, where the classifiers guarantee user-defined levels of coverage or confidence (the probability that the true label is contained in the set) while minimizing the ambiguity (the expected size of the output). We first derive oracle classifiers assuming the true distribution to be known. We show that the oracle classifiers are obtained from level sets of the functions that define the conditional probability of each class. Then we develop estimators with good asymptotic and finite sample properties. The proposed estimators build on existing single-label classifiers. The optimal classifier can sometimes output the empty set, but we provide two solutions to fix this issue that are suitable for various practical needs. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1609.00451},
author = {Sadinle, Mauricio and Lei, Jing and Wasserman, Larry},
doi = {10.1080/01621459.2017.1395341},
eprint = {1609.00451},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Ambiguous observation,Bayes classifier,Multiclass classification,Nondeterministic classifier,Oracle classifier,Reject option},
number = {525},
pages = {223--234},
publisher = {American Statistical Association},
title = {{Least Ambiguous Set-Valued Classifiers With Bounded Error Levels}},
volume = {114},
year = {2019}
}

@book{Sugiyama2012,
abstract = {This volume focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) changes but the conditional distributions of outputs (answers) is unchanged, and presents machine learning theory algorithms, and applications to overcome this variety of non-stationarity.},
author = {Sugiyama, Masashi and Kawanabe, Motoaki.},
isbn = {9780262017091},
pages = {261},
publisher = {MIT Press},
title = {{Machine learning in non-stationary environments : introduction to covariate shift adaptation}},
year = {2012}
}

@article{Szegedy2014,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
eprint = {1312.6199},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Intriguing properties of neural networks}},
year = {2014}
}

@inproceedings{Vovk2013,
abstract = {Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given confidence level. Inductive conformal predictors are a computationally efficient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have only been known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modifications. In particular, it discusses a convenient expression of one of the modifications in terms of ROC curves. {\textcopyright} 2013 The Author(s).},
archivePrefix = {arXiv},
arxivId = {1209.2673},
author = {Vovk, Vladimir},
booktitle = {Asian conference on machine learning},
doi = {10.1007/s10994-013-5355-6},
eprint = {1209.2673},
file = {::},
issn = {1938-7228},
keywords = {Batch mode of learning,Boosting,Conditional validity,Inductive conformal predictors,MART,ROC curves,Spam detection},
pages = {475--490},
publisher = {PMLR},
title = {{Conditional validity of inductive conformal predictors}},
volume = {25},
year = {2013}
}

@article{Wald1943,
author = {Wald, Abraham},
doi = {10.1214/aoms/1177731491},
file = {::},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {1},
pages = {45--55},
title = {{An Extension of Wilks' Method for Setting Tolerance Limits}},
volume = {14},
year = {1943}
}

@article{Wilks1941,
abstract = {The asymptotic behaviour of the residual life time at time t is investigated (for t rightarrow infty). We derive weak limit laws and their domains of attraction and treat rates of convergence and moment convergence. The presentation exploits the close similarity with extreme value theory.},
author = {Wilks, S. S.},
doi = {10.1214/aoms/1177731788},
file = {::},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {1},
pages = {91--96},
title = {{Determination of Sample Sizes for Setting Tolerance Limits}},
volume = {12},
year = {1941}
}

@article{ai2024not,
  title={Not all distributional shifts are equal: Fine-grained robust conformal inference},
  author={Ai, Jiahao and Ren, Zhimei},
  journal={arXiv preprint arXiv:2402.13042},
  year={2024}
}

@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  pages={151--175},
  year={2010},
  publisher={Springer}
}

@article{bhattacharyya2024group,
  title={Group-weighted conformal prediction},
  author={Bhattacharyya, Aabesh and Barber, Rina Foygel},
  journal={arXiv preprint arXiv:2401.17452},
  year={2024}
}

@article{cauchois2024robust,
  title={Robust validation: Confident predictions even when distributions shift},
  author={Cauchois, Maxime and Gupta, Suyash and Ali, Alnur and Duchi, John C},
  journal={Journal of the American Statistical Association},
  pages={1--66},
  year={2024},
  publisher={Taylor \& Francis}
}

@article{gibbs2021adaptive,
  title={Adaptive conformal inference under distribution shift},
  author={Gibbs, Isaac and Candes, Emmanuel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1660--1672},
  year={2021}
}

@article{gibbs2023conformal,
  title={Conformal prediction with conditional guarantees},
  author={Gibbs, Isaac and Cherian, John J and Cand{\`e}s, Emmanuel J},
  journal={arXiv preprint arXiv:2305.12616},
  year={2023}
}

@article{gui2024distributionally,
  title={Distributionally robust risk evaluation with an isotonic constraint},
  author={Gui, Yu and Barber, Rina Foygel and Ma, Cong},
  journal={arXiv preprint arXiv:2407.06867},
  year={2024}
}

@article{kasa2024adapting,
  title={Adapting Conformal Prediction to Distribution Shifts Without Labels},
  author={Kasa, Kevin and Zhang, Zhiyu and Yang, Heng and Taylor, Graham W},
  journal={arXiv preprint arXiv:2406.01416},
  year={2024}
}

@inproceedings{kaur2022idecode,
  title={iDECODe: In-distribution equivariance for conformal out-of-distribution detection},
  author={Kaur, Ramneet and Jha, Susmit and Roy, Anirban and Park, Sangdon and Dobriban, Edgar and Sokolsky, Oleg and Lee, Insup},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2022}
}

@inproceedings{li2022pac,
  title={PAC-Wrap: Semi-supervised pac anomaly detection},
  author={Li, Shuo and Ji, Xiayan and Dobriban, Edgar and Sokolsky, Oleg and Lee, Insup},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2022}
}

@inproceedings{papadopoulos2002inductive,
  author = {Papadopoulos, Harris and Proedrou, Kostas and Vovk, Volodya and Gammerman, Alex},
  booktitle = {European Conference on Machine Learning},
  date-added = {2021-05-10 12:03:57 -0500},
  date-modified = {2021-05-10 12:03:57 -0500},
  organization = {Springer},
  pages = {345--356},
  title = {Inductive confidence machines for regression},
  year = {2002}}

@inproceedings{park2021pac,
  title={{PAC} Prediction Sets Under Covariate Shift},
  author={Park, Sangdon and Dobriban, Edgar and Lee, Insup and Bastani, Osbert},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{park2022pac,
title={{PAC} Prediction Sets for Meta-Learning},
author={Sangdon Park and Edgar Dobriban and Insup Lee and Osbert Bastani},
booktitle={{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems},
year={2022}
}

@article{qin2024distribution,
  title={Distribution-Free Prediction Intervals Under Covariate Shift, With an Application to Causal Inference},
  author={Qin, Jing and Liu, Yukun and Li, Moming and Huang, Chiung-Yu},
  journal={Journal of the American Statistical Association},
  volume={0},
  number={0},
  pages={1--26},
  year={2024},
  publisher={Taylor \& Francis},
  doi = {10.1080/01621459.2024.2356886},
  URL = { https://doi.org/10.1080/01621459.2024.2356886 },
  eprint = { https://doi.org/10.1080/01621459.2024.2356886 }
}

@article{qiu2023prediction,
  title={Prediction sets adaptive to unknown covariate shift},
  author={Qiu, Hongxiang and Dobriban, Edgar and Tchetgen Tchetgen, Eric},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={85},
  number={5},
  pages={1680--1705},
  year={2023},
  publisher={Oxford University Press US}
}

@book{quinonero2009dataset,
  title={Dataset shift in machine learning},
  author={Qui{\~n}onero-Candela, Joaquin and Sugiyama, Masashi and Lawrence, Neil D and Schwaighofer, Anton},
  year={2009},
  publisher={Mit Press}
}

@inproceedings{saunders1999transduction,
  title={Transduction with confidence and credibility},
  author={Saunders, Craig and Gammerman, Alexander and Vovk, Volodya},
  year={1999},
  booktitle={IJCAI}
}

@article{scheffe1945non,
  title={Non-parametric estimation. I. Validation of order statistics},
  author={Scheffe, Henry and Tukey, John W},
  journal={The Annals of Mathematical Statistics},
  volume={16},
  number={2},
  pages={187--192},
  year={1945},
  publisher={Institute of Mathematical Statistics}
}

@article{sesia2022conformal,
  title={Conformal Frequency Estimation using Discrete Sketched Data with Coverage for Distinct Queries},
  author={Sesia, Matteo and Favaro, Stefano and Dobriban, Edgar},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={348},
  pages={1--80},
  year={2023}
}

@article{shalev2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={2635--2670},
  year={2010},
  publisher={JMLR. org}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{shimodaira2000improving,
  title={Improving predictive inference under covariate shift by weighting the log-likelihood function},
  author={Shimodaira, Hidetoshi},
  journal={Journal of statistical planning and inference},
  volume={90},
  number={2},
  pages={227--244},
  year={2000},
  publisher={Elsevier}
}

@article{si2023pac,
  title={{PAC} prediction sets under label shift},
  author={Si, Wenwen and Park, Sangdon and Lee, Insup and Dobriban, Edgar and Bastani, Osbert},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{taori2020measuring,
  title={Measuring robustness to natural distribution shifts in image classification},
  author={Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18583--18599},
  year={2020}
}

@article{tibshirani2019conformal,
  title={Conformal prediction under covariate shift},
  author={Tibshirani, Ryan J and Foygel Barber, Rina and Cand{\`e}s, Emmanuel J and Ramdas, Aaditya},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{tukey1947non,
  title={Non-Parametric Estimation Ii. Statistically Equivalent Blocks and Tolerance Regions--the Continuous Case},
  author={Tukey, John W},
  journal={The Annals of Mathematical Statistics},
  pages={529--539},
  year={1947},
  publisher={JSTOR}
}

@article{tukey1948nonparametric,
  title={Nonparametric estimation, III. Statistically equivalent blocks and multivariate tolerance regions--the discontinuous case},
  author={Tukey, John W},
  journal={The Annals of Mathematical Statistics},
  pages={30--39},
  year={1948},
  publisher={JSTOR}
}

@inproceedings{vovk1999machine,
  title={Machine-learning applications of algorithmic randomness},
  author={Vovk, Volodya and Gammerman, Alexander and Saunders, Craig},
    booktitle    = "International Conference on Machine Learning",
  year={1999}
}

@book{vovk2005algorithmic,
	author = {Vovk, Vladimir and Gammerman, Alex and Shafer, Glenn},
	date-added = {2021-05-10 11:53:43 -0500},
	date-modified = {2021-05-10 11:53:43 -0500},
	publisher = {Springer Science \& Business Media},
	title = {Algorithmic learning in a random world},
	year = {2005}}

@article{yang2024doubly,
  title={Doubly robust calibration of prediction sets under covariate shift},
  author={Yang, Yachong and Kuchibhotla, Arun Kumar and Tchetgen Tchetgen, Eric},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  pages={qkae009},
  year={2024},
  publisher={Oxford University Press US}
}

