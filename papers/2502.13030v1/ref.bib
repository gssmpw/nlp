@article{ai2024not,
  title={Not all distributional shifts are equal: Fine-grained robust conformal inference},
  author={Ai, Jiahao and Ren, Zhimei},
  journal={arXiv preprint arXiv:2402.13042},
  year={2024}
}

@article{kasa2024adapting,
  title={Adapting Conformal Prediction to Distribution Shifts Without Labels},
  author={Kasa, Kevin and Zhang, Zhiyu and Yang, Heng and Taylor, Graham W},
  journal={arXiv preprint arXiv:2406.01416},
  year={2024}
}

@article{bhattacharyya2024group,
  title={Group-weighted conformal prediction},
  author={Bhattacharyya, Aabesh and Barber, Rina Foygel},
  journal={arXiv preprint arXiv:2401.17452},
  year={2024}
}

@article{qin2024distribution,
  title={Distribution-Free Prediction Intervals Under Covariate Shift, With an Application to Causal Inference},
  author={Qin, Jing and Liu, Yukun and Li, Moming and Huang, Chiung-Yu},
  journal={Journal of the American Statistical Association},
  volume={0},
  number={0},
  pages={1--26},
  year={2024},
  publisher={Taylor \& Francis},
  doi = {10.1080/01621459.2024.2356886},
  URL = { https://doi.org/10.1080/01621459.2024.2356886 },
  eprint = { https://doi.org/10.1080/01621459.2024.2356886 }
}

@article{cauchois2024robust,
  title={Robust validation: Confident predictions even when distributions shift},
  author={Cauchois, Maxime and Gupta, Suyash and Ali, Alnur and Duchi, John C},
  journal={Journal of the American Statistical Association},
  pages={1--66},
  year={2024},
  publisher={Taylor \& Francis}
}

@article{han2024distribution,
  title={Distribution-Free Predictive Inference under Unknown Temporal Drift},
  author={Han, Elise and Huang, Chengpiao and Wang, Kaizheng},
  journal={arXiv preprint arXiv:2406.06516},
  year={2024}
}

@article{taori2020measuring,
  title={Measuring robustness to natural distribution shifts in image classification},
  author={Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18583--18599},
  year={2020}
}

@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  pages={151--175},
  year={2010},
  publisher={Springer}
}

@article{gibbs2021adaptive,
  title={Adaptive conformal inference under distribution shift},
  author={Gibbs, Isaac and Candes, Emmanuel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1660--1672},
  year={2021}
}

@article{lee2024batch,
  title={Batch predictive inference},
  author={Lee, Yonghoon and Tchetgen, Eric Tchetgen and Dobriban, Edgar},
  journal={arXiv preprint arXiv:2409.13990},
  year={2024}
}


@article{dobriban2023symmpi,
  title={SymmPI: Predictive Inference for Data with Group Symmetries},
  author={Dobriban, Edgar and Yu, Mengxin},
  journal={arXiv preprint arXiv:2312.16160},
  year={2023}
}

@misc{UCIrepo,
  author       = {Redmond, Michael},
  title        = {{Communities and Crime}},
  year         = {2002},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C53W3X}
}


@article{gui2024distributionally,
  title={Distributionally robust risk evaluation with an isotonic constraint},
  author={Gui, Yu and Barber, Rina Foygel and Ma, Cong},
  journal={arXiv preprint arXiv:2407.06867},
  year={2024}
}

@inproceedings{yu2020bdd100k,
  title={Bdd100k: A diverse driving dataset for heterogeneous multitask learning},
  author={Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2636--2645},
  year={2020}
}

@article{xu2025wasserstein,
  title={Wasserstein-regularized Conformal Prediction under General Distribution Shift},
  author={Xu, Rui and Chen, Chao and Sun, Yue and Venkitasubramaniam, Parvathinathan and Xie, Sihong},
  journal={arXiv preprint arXiv:2501.13430},
  year={2025}
}

@article{angelopoulos2024theoretical,
  title={Theoretical foundations of conformal prediction},
  author={Angelopoulos, Anastasios N and Barber, Rina Foygel and Bates, Stephen},
  journal={arXiv preprint arXiv:2411.11824},
  year={2024}
}

@book{vovk2022algorithmic,
  title={Algorithmic Learning in a Random World},
  author={Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
  year={2022},
  publisher={Springer Nature}
}

@article{hoeffding1963probability,
  title={Probability Inequalities for Sums of Bounded Random Variables},
  author={Hoeffding, Wassily},
  journal={Journal of the American Statistical Association},
  volume={58},
  number={301},
  pages={13--30},
  year={1963},
  publisher={Taylor \& Francis}
}

@inproceedings{koh2021wilds,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International conference on machine learning},
  pages={5637--5664},
  year={2021},
  organization={PMLR}
}
@article{foygel2021limits,
  title={The limits of distribution-free conditional predictive inference},
  author={Foygel Barber, Rina and Candes, Emmanuel J and Ramdas, Aaditya and Tibshirani, Ryan J},
  journal={Information and Inference: A Journal of the IMA},
  volume={10},
  number={2},
  pages={455--482},
  year={2021},
  publisher={Oxford University Press}
}

@inproceedings{sypetkowski2023rxrx1,
  title={Rxrx1: A dataset for evaluating experimental batch correction methods},
  author={Sypetkowski, Maciej and Rezanejad, Morteza and Saberian, Saber and Kraus, Oren and Urbanik, John and Taylor, James and Mabey, Ben and Victors, Mason and Yosinski, Jason and Sereshkeh, Alborz Rezazadeh and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4285--4294},
  year={2023}
}

@article{dunn2022distribution,
  title={Distribution-free prediction sets for two-layer hierarchical models},
  author={Dunn, Robin and Wasserman, Larry and Ramdas, Aaditya},
  journal={Journal of the American Statistical Association},
  pages={1--12},
  year={2022},
  publisher={Taylor \& Francis}
}



@article{lei2014distribution,
  title={Distribution-free prediction bands for non-parametric regression},
  author={Lei, Jing and Wasserman, Larry},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={76},
  number={1},
  pages={71--96},
  year={2014},
  publisher={Wiley Online Library}
}

@article{lei2018distribution,
  title={Distribution-free predictive inference for regression},
  author={Lei, Jing and G'Sell, Max and Rinaldo, Alessandro and Tibshirani, Ryan and Wasserman, Larry},
  journal={Journal of the American Statistical Association},
  volume={113},
  number={523},
  pages={1094--1111},
  year={2018}
}

@article{sesia2021conformal,
  title={Conformal prediction using conditional histograms},
  author={Sesia, Matteo and Romano, Yaniv},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6304--6315},
  year={2021}
}

@article{guan2022prediction,
  title={Prediction and outlier detection in classification problems},
  author={Guan, Leying and Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B},
  volume={84},
  number={2},
  pages={524--546},
  year={2022}
}

@article{guan2023localized,
  title={Localized conformal prediction: A generalized inference framework for conformal prediction},
  author={Guan, Leying},
  journal={Biometrika},
  volume={110},
  number={1},
  pages={33--50},
  year={2023}
}

@article{guan2023conformal,
  title={A conformal test of linear models via permutation-augmented regressions},
  author={Guan, Leying},
  journal={arXiv preprint arXiv:2309.05482},
  year={2023}
}

@article{romano2020classification,
  title={Classification with valid and adaptive coverage},
  author={Romano, Yaniv and Sesia, Matteo and Candes, Emmanuel},
  journal={{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems},
  year={2020}
}

@article{bates2023testing,
  title={Testing for outliers with conformal p-values},
  author={Bates, Stephen and Cand{\`e}s, Emmanuel and Lei, Lihua and Romano, Yaniv and Sesia, Matteo},
  journal={The Annals of Statistics},
  volume={51},
  number={1},
  pages={149--178},
  year={2023},
  publisher={Institute of Mathematical Statistics}
}

@article{einbinder2022training,
  title={Training uncertainty-aware classifiers with conformalized deep learning},
  author={Einbinder, Bat-Sheva and Romano, Yaniv and Sesia, Matteo and Zhou, Yanfei},
  journal={{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems},
  year={2022}
}

@article{liang2022integrative,
  title={Integrative conformal p-values for powerful out-of-distribution testing with labeled outliers},
  author={Liang, Ziyi and Sesia, Matteo and Sun, Wenguang},
  journal={arXiv preprint arXiv:2208.11111},
  year={2022}
}

@inproceedings{liang2023conformal,
  title={Conformal inference is (almost) free for neural networks trained with early stopping},
  author={Liang, Ziyi and Zhou, Yanfei and Sesia, Matteo},
  booktitle={{I}nternational {C}onference on {M}achine {L}earning},
  year={2023}
}

@book{geisser2017predictive,
  title={Predictive inference: an introduction},
  author={Geisser, Seymour},
  year={2017},
  publisher={Chapman and Hall/CRC}
}


@inproceedings{park2020pac,
  title={{PAC} Confidence Predictions for Deep Neural Network Classifiers},
  author={Park, Sangdon and Li, Shuo and Lee, Insup and Bastani, Osbert},
  journal={arXiv preprint arXiv:2011.00716},
  year={2020}
}

@article{sesia2022conformal,
  title={Conformal Frequency Estimation using Discrete Sketched Data with Coverage for Distinct Queries},
  author={Sesia, Matteo and Favaro, Stefano and Dobriban, Edgar},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={348},
  pages={1--80},
  year={2023}
}

@article{qiu2022distribution,
  title={Distribution-free prediction sets adaptive to covariate shift},
  author={Qiu, Hongxiang and Dobriban, Edgar and Tchetgen Tchetgen, Eric},
  journal={Journal of the Royal Statistical Society Series B},
  year={2022}
}

@inproceedings{li2022pac,
  title={PAC-Wrap: Semi-supervised pac anomaly detection},
  author={Li, Shuo and Ji, Xiayan and Dobriban, Edgar and Sokolsky, Oleg and Lee, Insup},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2022}
}

@inproceedings{kaur2022idecode,
  title={iDECODe: In-distribution equivariance for conformal out-of-distribution detection},
  author={Kaur, Ramneet and Jha, Susmit and Roy, Anirban and Park, Sangdon and Dobriban, Edgar and Sokolsky, Oleg and Lee, Insup},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2022}
}
@article{si2023pac,
  title={{PAC} prediction sets under label shift},
  author={Si, Wenwen and Park, Sangdon and Lee, Insup and Dobriban, Edgar and Bastani, Osbert},
  journal={International Conference on Learning Representations},
  year={2024}
}



@article{schick1986asymptotically,
  title={On asymptotically efficient estimation in semiparametric models},
  author={Schick, Anton},
  journal={The Annals of Statistics},
  pages={1139--1151},
  year={1986},
  publisher={JSTOR}
}

@article{kennedy2022semiparametric,
  title={Semiparametric doubly robust targeted double machine learning: a review},
  author={Kennedy, Edward H},
  journal={arXiv preprint arXiv:2203.06469},
  year={2022}
}

@article{zeng2022bayes,
  title={Bayes-Optimal Classifiers under Group Fairness},
  author={Zeng, Xianli and Dobriban, Edgar and Cheng, Guang},
  journal={arXiv preprint arXiv:2202.09724},
  year={2022}
}

@article{friedman2004multivariate,
  title={On multivariate goodness-of-fit and two-sample testing},
  author={Friedman, Jerome H},
  journal={Statistical Problems in Particle Physics, Astrophysics, and Cosmology},
  volume={1},
  pages={311--313},
  year={2003}
}

@inproceedings{papadopoulos2002inductive,
  author = {Papadopoulos, Harris and Proedrou, Kostas and Vovk, Volodya and Gammerman, Alex},
  booktitle = {European Conference on Machine Learning},
  date-added = {2021-05-10 12:03:57 -0500},
  date-modified = {2021-05-10 12:03:57 -0500},
  organization = {Springer},
  pages = {345--356},
  title = {Inductive confidence machines for regression},
  year = {2002}}


@inproceedings{vovk1999machine,
  title={Machine-learning applications of algorithmic randomness},
  author={Vovk, Volodya and Gammerman, Alexander and Saunders, Craig},
    booktitle    = "International Conference on Machine Learning",
  year={1999}
}

@inproceedings{saunders1999transduction,
  title={Transduction with confidence and credibility},
  author={Saunders, Craig and Gammerman, Alexander and Vovk, Volodya},
  year={1999},
  booktitle={IJCAI}
}

@article{tukey1948nonparametric,
  title={Nonparametric estimation, III. Statistically equivalent blocks and multivariate tolerance regions--the discontinuous case},
  author={Tukey, John W},
  journal={The Annals of Mathematical Statistics},
  pages={30--39},
  year={1948},
  publisher={JSTOR}
}

@article{tukey1947non,
  title={Non-Parametric Estimation Ii. Statistically Equivalent Blocks and Tolerance Regions--the Continuous Case},
  author={Tukey, John W},
  journal={The Annals of Mathematical Statistics},
  pages={529--539},
  year={1947},
  publisher={JSTOR}
}

@article{scheffe1945non,
  title={Non-parametric estimation. I. Validation of order statistics},
  author={Scheffe, Henry and Tukey, John W},
  journal={The Annals of Mathematical Statistics},
  volume={16},
  number={2},
  pages={187--192},
  year={1945},
  publisher={Institute of Mathematical Statistics}
}

@article{cauchois2020robust,
  title={Robust validation: Confident predictions even when distributions shift},
  author={Cauchois, Maxime and Gupta, Suyash and Ali, Alnur and Duchi, John C},
  journal={arXiv preprint arXiv:2008.04267},
  year={2020}
}

@article{lei2015conformal,
  title={A conformal prediction approach to explore functional data},
  author={Lei, Jing and Rinaldo, Alessandro and Wasserman, Larry},
  journal={Annals of Mathematics and Artificial Intelligence},
  volume={74},
  number={1},
  pages={29--43},
  year={2015},
  publisher={Springer}
}

@article{dunn2018distribution,
  title={Distribution-free prediction sets with random effects},
  author={Dunn, Robin and Wasserman, Larry and Ramdas, Aaditya},
  journal={arXiv preprint arXiv:1809.07441},
  year={2018}
}

@article{lei2013distribution,
  title={Distribution-free prediction sets},
  author={Lei, Jing and Robins, James and Wasserman, Larry},
  journal={Journal of the American Statistical Association},
  volume={108},
  number={501},
  pages={278--287},
  year={2013},
  publisher={Taylor \& Francis}
}

@article{bickel1982adaptive,
  title={On adaptive estimation},
  author={Bickel, Peter J},
  journal={The Annals of Statistics},
  pages={647--671},
  year={1982},
  publisher={JSTOR}
}

@article{gneiting2007probabilistic,
  title={Probabilistic forecasts, calibration and sharpness},
  author={Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={69},
  number={2},
  pages={243--268},
  year={2007},
  publisher={Wiley Online Library}
}

@article{degroot1983comparison,
  title={The comparison and evaluation of forecasters},
  author={DeGroot, Morris H and Fienberg, Stephen E},
  journal={Journal of the Royal Statistical Society: Series D (The Statistician)},
  volume={32},
  number={1-2},
  pages={12--22},
  year={1983},
  publisher={Wiley Online Library}
}

@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC},
  year={2003},
  publisher={Cambridge university press}
}

@book{felixkleingeometry,
  title={Elementary Mathematics from an Advanced Standpoint: Geometry},
  author={Felix Klein},
  year={2004},
  publisher={Dover Publications; Dover edition},
  ISBN = {978-0486434810}
}

@inproceedings{park2021pac,
  title={{PAC} Prediction Sets Under Covariate Shift},
  author={Park, Sangdon and Dobriban, Edgar and Lee, Insup and Bastani, Osbert},
  booktitle={International Conference on Learning Representations},
  year={2022}
}


@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}

@incollection{vonNeumann1951,
	address = {Washington, DC},
	author = {von Neumann, John},
	booktitle = {Monte Carlo Method},
	chapter = {13},
	editor = {Householder, A.~S. and Forsythe, G.~E. and Germond, H.~H.},
	pages = {36--38},
	publisher = {US Government Printing Office},
	series = {National Bureau of Standards Applied Mathematics Series},
	title = {Various Techniques Used in Connection with Random Digits},
	volume = {12},
	year = {1951}}
	
	
@article{kanamori2009least,
	author = {Kanamori, Takafumi and Hido, Shohei and Sugiyama, Masashi},
	date-added = {2021-05-10 12:34:00 -0500},
	date-modified = {2021-05-10 12:34:01 -0500},
	journal = {Journal of Machine Learning Research},
	number = {Jul},
	pages = {1391--1445},
	title = {A least-squares approach to direct importance estimation},
	volume = {10},
	year = {2009}}

@inproceedings{kanamori2009efficient,
	author = {Kanamori, Takafumi and Hido, Shohei and Sugiyama, Masashi},
	booktitle = {Advances in neural information processing systems},
	date-added = {2021-05-10 12:34:00 -0500},
	date-modified = {2021-05-10 12:34:01 -0500},
	pages = {809--816},
	title = {Efficient direct density ratio estimation for non-stationarity adaptation and outlier detection},
	year = {2009}}

@inproceedings{cortes2010learning,
	author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
	booktitle = {Advances in neural information processing systems},
	date-added = {2021-05-10 12:34:00 -0500},
	date-modified = {2021-05-10 12:34:01 -0500},
	pages = {442--450},
	title = {Learning bounds for importance weighting},
	year = {2010}}

@article{sugiyama2007covariate,
	author = {Sugiyama, Masashi and Krauledat, Matthias and Muller, Klaus-Robert},
	date-added = {2021-05-10 12:34:00 -0500},
	date-modified = {2021-05-10 12:34:01 -0500},
	journal = {Journal of Machine Learning Research},
	number = {May},
	pages = {985--1005},
	title = {Covariate shift adaptation by importance weighted cross validation},
	volume = {8},
	year = {2007}}

@article{Nguyen_2010,
	author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
	journal = {IEEE Transactions on Information Theory},
	month = {Nov},
	number = {11},
	pages = {5847-5861},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	title = {Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization},
	volume = {56},
	year = {2010},
	}



@inproceedings{sugiyama2007direct,
  title={Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation.},
  author={Sugiyama, Masashi and Nakajima, Shinichi and Kashima, Hisashi and Von Buenau, Paul and Kawanabe, Motoaki},
  booktitle={NIPS},
  volume={7},
  pages={1433--1440},
  year={2007}
}

@article{wang2017sketching,
  title={Sketching meets random projection in the dual: A provable recovery algorithm for big and high-dimensional data},
  author={Wang, Jialei and Lee, Jason D and Mahdavi, Mehrdad and Kolar, Mladen and Srebro, Nathan},
  journal={Electronic Journal of Statistics},
  volume={11},
  number={2},
  pages={4896--4944},
  year={2017},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{mahoney2011randomized,
  title={Randomized algorithms for matrices and data},
  author={Mahoney, Michael W},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={3},
  number={2},
  pages={123--224},
  year={2011},
  publisher={Now Publishers, Inc.}
}


@book{vempala2005random,
  title={The random projection method},
  author={Vempala, Santosh S},
  volume={65},
  year={2005},
  publisher={American Mathematical Soc.}
}

@article{drineas2008relative,
  title={Relative-error CUR matrix decompositions},
  author={Drineas, Petros and Mahoney, Michael W and Muthukrishnan, Shan},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={30},
  number={2},
  pages={844--881},
  year={2008},
  publisher={SIAM}
}

@article{jolliffe1972discarding,
  title={Discarding variables in a principal component analysis. I: Artificial data},
  author={Jolliffe, Ian T},
  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume={21},
  number={2},
  pages={160--173},
  year={1972},
  publisher={Wiley Online Library}
}

@inproceedings{li2013iterative,
  title={Iterative row sampling},
  author={Li, Mu and Miller, Gary L and Peng, Richard},
  booktitle={2013 IEEE 54th Annual Symposium on Foundations of Computer Science},
  pages={127--136},
  year={2013},
  organization={IEEE}
}


@inproceedings{dobriban2019asymptotics,
  title={Asymptotics for sketching in least squares regression},
  author={Dobriban, Edgar and Liu, Sifan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3675--3685},
  year={2019}
}



@article{wang2017sketched,
  title={Sketched ridge regression: Optimization perspective, statistical perspective, and model averaging},
  author={Wang, Shusen and Gittens, Alex and Mahoney, Michael W},
  journal={Journal of Machine Learning Research},
  volume={18},
  pages={1--50},
  year={2018}
  }


@article{raskutti2014statistical,
  title={A statistical perspective on randomized sketching for ordinary least-squares},
  author={Raskutti, Garvesh and Mahoney, Michael W},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={7508--7538},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{cannings2017random,
  title={Random-projection ensemble classification},
  author={Cannings, Timothy I and Samworth, Richard J},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={79},
  number={4},
  pages={959--1035},
  year={2017},
  publisher={Wiley Online Library}
}



@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{breiman1996bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={2},
  pages={123--140},
  year={1996},
  publisher={Springer}
}

@book{efron1982jackknife,
  title={The jackknife, the bootstrap and other resampling plans},
  author={Efron, Bradley},
  year={1982},
  publisher={SIAM}
}

@book{efron1994introduction,
  title={An introduction to the bootstrap},
  author={Efron, Bradley and Tibshirani, Robert J},
  year={1994},
  publisher={CRC press}
}

@book{politis1999subsampling,
  title={Subsampling},
  author={Politis, Dimitris N and Romano, Joseph P and Wolf, Michael},
  year={1999},
  publisher={Springer Science \& Business Media}
}

@article{bishop1995training,
  title={Training with noise is equivalent to Tikhonov regularization},
  author={Bishop, Chris M},
  journal={Neural computation},
  volume={7},
  number={1},
  pages={108--116},
  year={1995},
  publisher={MIT Press}
}


@article{gataric2020sparse,
  title={Sparse principal component analysis via axis-aligned random projections},
  author={Gataric, Milana and Wang, Tengyao and Samworth, Richard J},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year={2020},
  publisher={Wiley Online Library}
}


@article{woodruff2014sketching,
  title={Sketching as a tool for numerical linear algebra},
  author={Woodruff, David P.},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={10},
  number={1--2},
  pages={1--157},
  year={2014},
  publisher={Now Publishers, Inc.}
}


@inproceedings{sarlos2006improved,
  title={Improved approximation algorithms for large matrices via random projections},
  author={Sarlos, Tamas},
  booktitle={Foundations of Computer Science, 2006. FOCS'06. 47th Annual IEEE Symposium on},
  pages={143--152},
  year={2006},
  organization={IEEE}
}

@article{drineas2011faster,
  title={Faster least squares approximation},
  author={Drineas, Petros and Mahoney, Michael W and Muthukrishnan, S and Sarl{\'o}s, Tam{\'a}s},
  journal={Numerische mathematik},
  volume={117},
  number={2},
  pages={219--249},
  year={2011},
  publisher={Springer}
}


@inproceedings{lu2013faster,
  title={Faster ridge regression via the subsampled randomized hadamard transform},
  author={Lu, Yichao and Dhillon, Paramveer and Foster, Dean P and Ungar, Lyle},
  booktitle={Advances in neural information processing systems},
  pages={369--377},
  year={2013}
}


@inproceedings{chen2015fast,
  title={Fast Relative-Error Approximation Algorithm for Ridge Regression.},
  author={Chen, Shouyuan and Liu, Yang and Lyu, Michael R and King, Irwin and Zhang, Shengyu},
  booktitle={UAI},
  pages={201--210},
  year={2015}
}


@article{liu2019ridge,
  title={Ridge Regression: Structure, Cross-Validation, and Sketching},
  author={Liu, Sifan and Dobriban, Edgar},
  journal={arXiv preprint arXiv:1910.02373, International Conference on Learning Representations (ICLR) 2020},
  year={2019}
}

@inproceedings{lopes2011more,
  title={A more powerful two-sample test in high dimensions using random projection},
  author={Lopes, Miles and Jacob, Laurent and Wainwright, Martin J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1206--1214},
  year={2011}
}


@article{srivastava2016raptt,
  title={RAPTT: An exact two-sample test in high dimensions using random projections},
  author={Srivastava, Radhendushka and Li, Ping and Ruppert, David},
  journal={Journal of Computational and Graphical Statistics},
  volume={25},
  number={3},
  pages={954--970},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{pilanci2016iterative,
  title={Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares},
  author={Pilanci, Mert and Wainwright, Martin J},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1842--1879},
  year={2016},
  publisher={JMLR. org}
}


@article{pilanci2015randomized,
  title={Randomized sketches of convex programs with sharp guarantees},
  author={Pilanci, Mert and Wainwright, Martin J},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={9},
  pages={5096--5115},
  year={2015},
  publisher={IEEE}
}

@article{pilanci2017newton,
  title={Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence},
  author={Pilanci, Mert and Wainwright, Martin J},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={1},
  pages={205--245},
  year={2017},
  publisher={SIAM}
}


@inproceedings{ho2019flow++,
  title={Flow++: Improving flow-based generative models with variational dequantization and architecture design},
  author={Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={2722--2730},
  year={2019},
  organization={PMLR}
}

@article{liu2019graph,
  title={Graph normalizing flows},
  author={Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  journal={arXiv preprint arXiv:1905.13177},
  year={2019}
}

@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

@book{minsky2017perceptrons,
  title={Perceptrons: An introduction to computational geometry},
  author={Minsky, Marvin and Papert, Seymour A},
  year={2017},
  publisher={MIT press}
}

@inproceedings{lacotte2020limiting,
  title={Limiting spectrum of randomized hadamard transform and optimal iterative sketching methods},
  author={Lacotte, Jonathan and Liu, Sifan and Dobriban, Edgar and Pilanci, Mert},
  booktitle={Conference on Neural Information Processing Systems},
  year={2020}
}

@article{elisseeff2005stability,
  title={Stability of Randomized Learning Algorithms.},
  author={Elisseeff, Andre and Evgeniou, Theodoros and Pontil, Massimiliano and Kaelbing, Leslie Pack},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={1},
  year={2005}
}

@inproceedings{sinha2016learning,
  title={Learning kernels with random features.},
  author={Sinha, Aman and Duchi, John C},
  booktitle={NIPS},
  pages={1298--1306},
  year={2016}
}

@article{scholkopf2002sampling,
  title={Sampling techniques for kernel methods},
  author={Scholkopf, DAFMB and Achlioptas, F and Bernhard, M},
  journal={Advances in neural information processing systems},
  volume={14},
  pages={335},
  year={2002}
}

@inproceedings{rudi2017generalization,
  title={Generalization Properties of Learning with Random Features.},
  author={Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={NIPS},
  pages={3215--3225},
  year={2017}
}

@article{pao1992functional,
  title={Functional-link net computing: theory, system architecture, and functionalities},
  author={Pao, Y-H and Takefuji, Yoshiyasu},
  journal={Computer},
  volume={25},
  number={5},
  pages={76--79},
  year={1992},
  publisher={IEEE}
}

@inproceedings{gallicchio2017randomized,
  title={Randomized Machine Learning Approaches: Recent Developments and Challenges.},
  author={Gallicchio, Claudio and Mart{\'\i}n-Guerrero, Jos{\'e} David and Micheli, Alessio and Soria-Olivas, Emilio},
  booktitle={ESANN},
  year={2017}
}

@inproceedings{rahimi2008uniform,
  title={Uniform approximation of functions with random bases},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={2008 46th Annual Allerton Conference on Communication, Control, and Computing},
  pages={555--561},
  year={2008},
  organization={IEEE}
}

@inproceedings{rahimi2007random,
  title={Random Features for Large-Scale Kernel Machines.},
  author={Rahimi, Ali and Recht, Benjamin and others},
  booktitle={NIPS},
  year={2007}
}

@article{suganthan2021origins,
  title={On the origins of randomization-based feedforward neural networks},
  author={Suganthan, Ponnuthurai N and Katuwal, Rakesh},
  journal={Applied Soft Computing},
  volume={105},
  pages={107239},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{schmidt1992feed,
  title={Feed forward neural networks with random weights},
  author={Schmidt, Wouter F and Kraaijveld, Martin A and Duin, Robert PW},
  booktitle={International Conference on Pattern Recognition},
  year={1992},
  organization={IEEE COMPUTER SOCIETY PRESS}
}

@article{wang2008comments,
  title={Comments on "The extreme learning machine"},
  author={Wang, Lipo P and Wan, Chunru R},
  journal={IEEE Transactions on Neural Networks},
  volume={19},
  number={8},
  pages={1494--1495},
  year={2008},
  publisher={IEEE}
}

@article{kaminski1997kernel,
  title={Kernel orthonormalization in radial basis function neural networks},
  author={Kaminski, Wladyslaw and Strumillo, Pawel},
  journal={IEEE Transactions on Neural Networks},
  volume={8},
  number={5},
  pages={1177--1183},
  year={1997},
  publisher={IEEE}
}

@article{huang2006extreme,
  title={Extreme learning machine: theory and applications},
  author={Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
  journal={Neurocomputing},
  volume={70},
  number={1-3},
  pages={489--501},
  year={2006},
  publisher={Elsevier}
}

@article{broomhead1988multivariable,
  title={Multivariable functional interpolation and adaptive networks},
  author={Broomhead, David S and Lowe, David},
  journal={Complex Systems},
  volume={2},
  pages={321--355},
  year={1988}
}

@techreport{broomhead1988radial,
  title={Radial basis functions, multi-variable functional interpolation and adaptive networks},
  author={Broomhead, David S and Lowe, David},
  year={1988},
  institution={Royal Signals and Radar Establishment Malvern (United Kingdom)}
}

@article{lukovsevivcius2009reservoir,
  title={Reservoir computing approaches to recurrent neural network training},
  author={Luko{\v{s}}evi{\v{c}}ius, Mantas and Jaeger, Herbert},
  journal={Computer Science Review},
  volume={3},
  number={3},
  pages={127--149},
  year={2009},
  publisher={Elsevier}
}

@article{tanaka2019recent,
  title={Recent advances in physical reservoir computing: A review},
  author={Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  journal={Neural Networks},
  volume={115},
  pages={100--123},
  year={2019},
  publisher={Elsevier}
}

@article{gallicchio2020deep,
  title={Deep randomized neural networks},
  author={Gallicchio, Claudio and Scardapane, Simone},
  journal={Recent Trends in Learning From Data},
  pages={43--68},
  year={2020},
  publisher={Springer}
}

@article{scardapane2017randomness,
  title={Randomness in neural networks: an overview},
  author={Scardapane, Simone and Wang, Dianhui},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={7},
  number={2},
  pages={e1200},
  year={2017},
  publisher={Wiley Online Library}
}

@article{cao2018review,
  title={A review on neural networks with random weights},
  author={Cao, Weipeng and Wang, Xizhao and Ming, Zhong and Gao, Jinzhu},
  journal={Neurocomputing},
  volume={275},
  pages={278--287},
  year={2018},
  publisher={Elsevier}
}

@article{neal1990learning,
  title={Learning stochastic feedforward networks},
  author={Neal, Radford M},
  journal={Department of Computer Science, University of Toronto},
  volume={64},
  number={1283},
  pages={1577},
  year={1990},
  publisher={Citeseer}
}

@article{cannings2021random,
  title={Random projections: Data perturbation for classification problems},
  author={Cannings, Timothy I},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  volume={13},
  number={1},
  pages={e1499},
  year={2021},
  publisher={Wiley Online Library}
}

@article{zhang2016survey,
  title={A survey of randomized algorithms for training neural networks},
  author={Zhang, Le and Suganthan, Ponnuthurai N},
  journal={Information Sciences},
  volume={364},
  pages={146--155},
  year={2016},
  publisher={Elsevier}
}

@article{rosenblatt1958perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}

@inproceedings{tang2013learning,
  title={Learning stochastic feedforward neural networks},
  author={Tang, Charlie and Salakhutdinov, Russ R},
  booktitle={Advances in Neural Information Processing Systems},
  pages={530--538},
  year={2013}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}


@article{beinrucker2016extensions,
  title={Extensions of stability selection using subsamples of observations and covariates},
  author={Beinrucker, Andre and Dogan, {\"U}r{\"u}n and Blanchard, Gilles},
  journal={Statistics and Computing},
  volume={26},
  number={5},
  pages={1059--1077},
  year={2016},
  publisher={Springer}
}

@inproceedings{della2021regularized,
  title={Regularized ERM on random subspaces},
  author={Della Vecchia, Andrea and Mourtada, Jaouad and De Vito, Ernesto and Rosasco, Lorenzo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4006--4014},
  year={2021},
  organization={PMLR}
}


@article{vincent2010stacked,
  title={Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
  author={Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
  journal={Journal of machine learning research},
  volume={11},
  number={12},
  year={2010}
}

@inproceedings{bergman2019classification,
  title={Classification-Based Anomaly Detection for General Data},
  author={Bergman, Liron and Hoshen, Yedid},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{golan2018deep,
  title={Deep anomaly detection using geometric transformations},
  author={Golan, Izhak and El-Yaniv, Ran},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={9781--9791},
  year={2018}
}

@article{InvariantRiskMinimization,
    title={Invariant Risk Minimization},
    author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
    journal={arXiv},
    year={2019}
}

@article{wainwright2008,
  added-at = {2010-03-25T16:35:35.000+0100},
  author = {Wainwright, M.J. and Jordan, M.I.},
  journal = {Foundations and Trends in Machine Learning},
  number = {1-2},
  pages = {1--305},
  timestamp = {2010-03-25T16:35:36.000+0100},
  title = {{Graphical Models, Exponential Families, and Variational Inference}},
  volume = 1,
  year = 2008
}

@InProceedings{pmlr-v32-rezende14,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1278--1286},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rezende14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/rezende14.html},
}

@inproceedings{chapelle2001vicinal,
 author = {Chapelle, Olivier and Weston, Jason and Bottou, L\'{e}on and Vapnik, Vladimir},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Vicinal Risk Minimization},
 volume = {13},
 year = {2001}
}

@article{CAO2015185,
title = {The use of vicinal-risk minimization for training decision trees},
journal = {Applied Soft Computing},
volume = {31},
pages = {185-195},
year = {2015},
author = {Yilong Cao and Peter I. Rockett}
}

@InProceedings{pmlr-v119-finzi20a, 
title = {Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data}, author = {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {3165--3176}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}}

@inproceedings{WeilerHS18,
  author    = {Maurice Weiler and
               Fred A. Hamprecht and
               Martin Storath},
  title     = {Learning Steerable Filters for Rotation Equivariant CNNs},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,  {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {849--858},
  publisher = {{IEEE} Computer Society},
  year      = {2018},
}

@article{zhang2018genboundvrm,
  author    = {Chao Zhang and
               Min{-}Hsiu Hsieh and
               Dacheng Tao},
  title     = {Generalization Bounds for Vicinal Risk Minimization Principle},
  journal   = {CoRR},
  volume    = {abs/1811.04351},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1811.04351}
}

@inproceedings{DBLP:conf/iclr/NagarajanK19,
  author    = {Vaishnavh Nagarajan and
               J. Zico Kolter},
  title     = {Deterministic PAC-Bayesian generalization bounds for deep networks
               via generalizing noise-resilience},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  year      = {2019},
}

@article{Noether_1971,
   title={Invariant variation problems},
   volume={1},
   number={3},
   journal={Transport Theory and Statistical Physics},
   publisher={Informa UK Limited},
   author={Noether, Emmy},
   year={1971},
   month={Jan},
   pages={186–207}
}

@inproceedings{DBLP:conf/iclr/MadryMSTV18,
  author    = {Aleksander Madry and
               Aleksandar Makelov and
               Ludwig Schmidt and
               Dimitris Tsipras and
               Adrian Vladu},
  title     = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  year      = {2018}
}

@article{JMLR:v17:15-290,
  author  = {Pierre Alquier and James Ridgway and Nicolas Chopin},
  title   = {On the properties of variational approximations of Gibbs posteriors},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {236},
  pages   = {1-41}
}

@inproceedings{NIPS2016_84d2004b,
 author = {Germain, Pascal and Bach, Francis and Lacoste, Alexandre and Lacoste-Julien, Simon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {PAC-Bayesian Theory Meets Bayesian Inference},
 volume = {29},
 year = {2016}
}

@book{catoni:hal-00104952,
  TITLE = {{Statistical learning theory and stochastic optimization. Ecole d'{\'e}t{\'e} de probabilit{\'e}s de Saint-Flour XXXI-2001}},
  AUTHOR = {Catoni, O.},
  NOTE = {Lecture notes in mathematics n{\textdegree}1851},
  PUBLISHER = {{Springer}},
  PAGES = {viii-272},
  YEAR = {2004},
  HAL_ID = {hal-00104952},
  HAL_VERSION = {v1},
}

@inproceedings{macallester1998,
author = {McAllester, David A.},
title = {Some PAC-Bayesian Theorems},
year = {1998},
isbn = {1581130570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the Eleventh Annual Conference on Computational Learning Theory},
pages = {230–234},
series = {COLT' 98}
}

@article{williams92reinforce,
author = {Williams, Ronald J.},
title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
journal = {Mach. Learn.},
month = may,
pages = {229–256},
numpages = {28},
keywords = {mathematical analysis, connectionist networks, Reinforcement learning, gradient descent}
}


@article{maddison2016concrete,
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  biburl = {https://www.bibsonomy.org/bibtex/2bdac6b274c0ea2f0f2cfcc361c0f118d/kirk86},
  description = {[1611.00712] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
  interhash = {fc80279dd4ef93cba525f83bac95a40c},
  intrahash = {bdac6b274c0ea2f0f2cfcc361c0f118d},
  keywords = {relaxation stats},
  note = {cite arxiv:1611.00712},
  timestamp = {2019-03-22T13:31:46.000+0100},
  title = {The Concrete Distribution: A Continuous Relaxation of Discrete Random
  Variables},
  url = {http://arxiv.org/abs/1611.00712},
  year = 2016
}

@conference{JanGuPoo17,
  title = {Categorical Reparametrization with Gumbel-Softmax},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  booktitle = {Proceedings International Conference on Learning Representations 2017},
  publisher = {OpenReviews.net},
  month = apr,
  year = {2017},
  url = {https://openreview.net/pdf?id=rkE3y85ee},
  month_numeric = {4}
}

@inproceedings{Kingma2014,
  author = {Kingma, Diederik P. and Welling, Max},
  biburl = {https://www.bibsonomy.org/bibtex/242e5be6faa01cba2587f4907ac99dce8/annakrause},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint = {http://arxiv.org/abs/1312.6114v10},
  eprintclass = {stat.ML},
  eprinttype = {arXiv},
  file = {:http\://arxiv.org/pdf/1312.6114v10:PDF;:KingmaWelling_Auto-EncodingVariationalBayes.pdf:PDF},
  interhash = {a626a9d77a123c52405a08da983203cb},
  intrahash = {42e5be6faa01cba2587f4907ac99dce8},
  keywords = {cs.LG stat.ML vae},
  timestamp = {2021-02-01T17:13:18.000+0100},
  title = {{Auto-Encoding Variational Bayes}},
  year = 2014
}



@article{fraser1966structural,
  title={Structural probability and a generalization},
  author={Fraser, Donald Alexander Stuart},
  journal={Biometrika},
  volume={53},
  number={1-2},
  pages={1--9},
  year={1966},
  publisher={Oxford University Press}
}

@InProceedings{CohenICML2019,
  author       = "T. Cohen and M. Weiler and B. Kicanaoglu and M. Welling",
  title        = "Gauge Equivariant Convolutional Networks and the Icosahedral CNN",
  booktitle    = "International Conference on Machine Learning",
  year         = "2019"
}

@article{kimkimkim,
author = {Kim, Ildoo and Kim, Younghoon and Kim, Sungwoong},
year = {2020},
month = {10},
pages = {},
title = {Learning Loss for Test-Time Augmentation},
journal = {NeurIPS 2020}
}


@inproceedings{novotny2018self,
  title={Self-supervised learning of geometrically stable features through probabilistic introspection},
  author={Novotny, David and Albanie, Samuel and Larlus, Diane and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3637--3645},
  year={2018}
}

@inproceedings{vedaldi2011learning,
  title={Learning equivariant structured output SVM regressors},
  author={Vedaldi, Andrea and Blaschko, Matthew and Zisserman, Andrew},
  booktitle={2011 International Conference on Computer Vision},
  pages={959--966},
  year={2011},
  organization={IEEE}
}


@article{mazaheri2019stochastic,
  title={Stochastic replica voting machine prediction of stable cubic and double perovskite materials and binary alloys},
  author={Mazaheri, T and Sun, Bo and Scher-Zagier, J and Thind, AS and Magee, D and Ronhovde, P and Lookman, T and Mishra, R and Nussinov, Z},
  journal={Physical Review Materials},
  volume={3},
  number={6},
  pages={063802},
  year={2019},
  publisher={APS}
}

@article{chao2017stochastic,
  title={The Stochastic Replica Approach to Machine Learning: Stability and Parameter Optimization},
  author={Chao, Patrick and Mazaheri, Tahereh and Sun, Bo and Weingartner, Nicholas B and Nussinov, Zohar},
  journal={arXiv preprint arXiv:1708.05715},
  year={2017}
}

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}

@article{ho2019population,
  title={Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules},
  author={Ho, Daniel and Liang, Eric and Stoica, Ion and Abbeel, Pieter and Chen, Xi},
  journal={arXiv preprint arXiv:1905.05393},
  year={2019}
}

@inproceedings{jaitly2013vocal,
  title={Vocal tract length perturbation (VTLP) improves speech recognition},
  author={Jaitly, Navdeep and Hinton, Geoffrey E},
  booktitle={Proc. ICML Workshop on Deep Learning for Audio, Speech and Language},
  volume={117},
  year={2013}
}

@article{xie2019unsupervised,
  title={Unsupervised data augmentation},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.12848},
  year={2019}
}

@article{park2019specaugment,
  title={Specaugment: A simple data augmentation method for automatic speech recognition},
  author={Park, Daniel S and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.08779},
  year={2019}
}

@inproceedings{bengio2011deep,
  title={Deep learners benefit more from out-of-distribution examples},
  author={Bengio, Yoshua and Bastien, Fr{\'e}d{\'e}ric and Bergeron, Arnaud and Boulanger--Lewandowski, Nicolas and Breuel, Thomas and Chherawala, Youssouf and Cisse, Moustapha and C{\^o}t{\'e}, Myriam and Erhan, Dumitru and Eustache, Jeremy and others},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={164--172},
  year={2011}
}

@inproceedings{hernandez2018further,
  title={Further advantages of data augmentation on convolutional neural networks},
  author={Hern{\'a}ndez-Garc{\'\i}a, Alex and K{\"o}nig, Peter},
  booktitle={International Conference on Artificial Neural Networks},
  pages={95--103},
  year={2018},
  organization={Springer}
}

@article{hernandez2018data,
  title={Data augmentation instead of explicit regularization},
  author={Hern{\'a}ndez-Garc{\'\i}a, Alex and K{\"o}nig, Peter},
  journal={arXiv preprint arXiv:1806.03852},
  year={2018}
}

@inproceedings{hernandez2018deep,
  title={Deep neural networks trained with heavier data augmentation learn features closer to representations in hIT},
  author={Hern{\'a}ndez-Garc{\'\i}a, Alex and Mehrer, Johannes and Kriegeskorte, Nikolaus and K{\"o}nig, Peter and Kietzmann, Tim C},
  booktitle={Conference on Cognitive Computational Neuroscience},
  year={2018}
}

@inproceedings{simard1992tangent,
  title={Tangent prop-a formalism for specifying selected invariances in an adaptive network},
  author={Simard, Patrice and Victorri, Bernard and LeCun, Yann and Denker, John},
  booktitle={Advances in neural information processing systems},
  pages={895--903},
  year={1992}
}

@inproceedings{bengio2007greedy,
  title={Greedy layer-wise training of deep networks},
  author={Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  booktitle={Advances in neural information processing systems},
  pages={153--160},
  year={2007}
}

@article{liu2019bad,
  title={Bad Global Minima Exist and SGD Can Reach Them},
  author={Liu, Shengchao and Papailiopoulos, Dimitris and Achlioptas, Dimitris},
  journal={arXiv preprint arXiv:1906.02613},
  year={2019}
}

@article{rajput2019does,
  title={Does Data Augmentation Lead to Positive Margin?},
  author={Rajput, Shashank and Feng, Zhili and Charles, Zachary and Loh, Po-Ling and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:1905.03177},
  year={2019}
}

@article{engstrom2017rotation,
  title={A rotation and a translation suffice: Fooling cnns with simple transformations},
  author={Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
  journal={arXiv preprint arXiv:1712.02779},
  year={2017}
}

@article{javadi2019hessian,
  title={A Hessian Based Complexity Measure for Deep Networks},
  author={Javadi, Hamid and Balestriero, Randall and Baraniuk, Richard},
  journal={arXiv preprint arXiv:1905.11639},
  year={2019}
}

@INPROCEEDINGS{resnet18, 
  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}}, 
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016}, 
  volume={}, 
  number={}, 
  pages={770-778}, 
  keywords={image classification;learning (artificial intelligence);neural nets;object detection;COCO segmentation;ImageNet localization;ILSVRC & COCO 2015 competitions;deep residual nets;COCO object detection dataset;visual recognition tasks;CIFAR-10;ILSVRC 2015 classification task;ImageNet test set;VGG nets;residual nets;ImageNet dataset;residual function learning;deeper neural network training;image recognition;deep residual learning;Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}, 
  doi={10.1109/CVPR.2016.90}, 
  ISSN={1063-6919}, 
  month={June},}

@InProceedings{dao19b,
  title =    {A Kernel Theory of Modern Data Augmentation},
  author =   {Dao, Tri and Gu, Albert and Ratner, Alexander and Smith, Virginia and De Sa, Chris and Re, Christopher},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  year =   {2019},
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge University Press}
}

@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@book{fisher1993statistical,
  title={Statistical analysis of spherical data},
  author={Fisher, Nicholas I and Lewis, Toby and Embleton, Brian JJ},
  year={1993},
  publisher={Cambridge university press}
}


@article{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1901.08584},
  year={2019}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{foster2019complexity,
  title={The Complexity of Making the Gradient Small in Stochastic Convex Optimization},
  author={Foster, Dylan and Sekhari, Ayush and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik and Woodworth, Blake},
  journal={arXiv preprint arXiv:1902.04686},
  year={2019}
}

@InProceedings{Esteves_2018_ECCV,
author = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
title = {Learning SO(3) Equivariant Representations with Spherical CNNs},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}


@article{esteves2018cross,
  title={Cross-Domain 3D Equivariant Image Embeddings},
  author={Esteves, Carlos and Sud, Avneesh and Luo, Zhengyi and Daniilidis, Kostas and Makadia, Ameesh},
  journal={arXiv preprint arXiv:1812.02716},
  year={2018}
}

@article{esteves2019equivariant,
  title={Equivariant Multi-View Networks},
  author={Esteves, Carlos and Xu, Yinshuang and Allen-Blanchette, Christine and Daniilidis, Kostas},
  journal={arXiv preprint arXiv:1904.00993},
  year={2019}
}

@book{robert2013monte,
  title={Monte Carlo statistical methods},
  author={Robert, Christian and Casella, George},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{riesenhuber2000models,
  title={Models of object recognition},
  author={Riesenhuber, Maximilian and Poggio, Tomaso},
  journal={Nature neuroscience},
  volume={3},
  number={11s},
  pages={1199},
  year={2000},
  publisher={Nature Publishing Group}
}

@article{lin2017does,
  title={Why does deep and cheap learning work so well?},
  author={Lin, Henry W and Tegmark, Max and Rolnick, David},
  journal={Journal of Statistical Physics},
  volume={168},
  number={6},
  pages={1223--1247},
  year={2017},
  publisher={Springer}
}

@article{wu2007robust,
  title={Robust truncated hinge loss support vector machines},
  author={Wu, Yichao and Liu, Yufeng},
  journal={Journal of the American Statistical Association},
  volume={102},
  number={479},
  pages={974--983},
  year={2007},
  publisher={Taylor \& Francis}
}  

@inproceedings{nguyen2013algorithms,
  title={Algorithms for direct 0--1 loss optimization in binary classification},
  author={Nguyen, Tan and Sanner, Scott},
  booktitle={International Conference on Machine Learning},
  pages={1085--1093},
  year={2013}
}

@article{tanner1987calculation,
  title={The calculation of posterior distributions by data augmentation},
  author={Tanner, Martin A and Wong, Wing Hung},
  journal={Journal of the American statistical Association},
  volume={82},
  number={398},
  pages={528--540},
  year={1987},
  publisher={Taylor \& Francis}
}


@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}


@incollection{baird1992document,
  title={Document image defect models},
  author={Baird, Henry S},
  booktitle={Structured Document Image Analysis},
  pages={546--556},
  year={1992},
  publisher={Springer}
}


@article{cirecsan2010deep,
  title={Deep, big, simple neural nets for handwritten digit recognition},
  author={Cire{\c{s}}an, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={22},
  number={12},
  pages={3207--3220},
  year={2010},
  publisher={MIT Press}
}


@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}


@article{fukushima1980neocognitron,
  title={Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  author={Fukushima, Kunihiko},
  journal={Biological cybernetics},
  volume={36},
  number={4},
  pages={193--202},
  year={1980},
  publisher={Springer}
}

@article{cohen2018general,
  title={A General Theory of Equivariant CNNs on Homogeneous Spaces},
  author={Cohen, Taco and Geiger, Mario and Weiler, Maurice},
  journal={arXiv preprint arXiv:1811.02017},
  year={2018}
}

@article{bloem2019probabilistic,
  title={Probabilistic symmetry and invariant neural networks},
  author={Bloem-Reddy, Benjamin and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1901.06082},
  year={2019}
}

@article{anselmi2019symmetry,
  title={Symmetry-adapted representation learning},
  author={Anselmi, Fabio and Evangelopoulos, Georgios and Rosasco, Lorenzo and Poggio, Tomaso},
  journal={Pattern Recognition},
  volume={86},
  pages={201--208},
  year={2019},
  publisher={Elsevier}
}


@inproceedings{kondor2018clebsch,
  title={Clebsch--gordan nets: a fully fourier space spherical convolutional neural network},
  author={Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10117--10126},
  year={2018}
}

@inproceedings{weiler20183d,
  title={3d steerable cnns: Learning rotationally equivariant features in volumetric data},
  author={Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10381--10392},
  year={2018}
}

@inproceedings{ravanbakhsh2017equivariance,
  title={Equivariance through parameter-sharing},
  author={Ravanbakhsh, Siamak and Schneider, Jeff and Poczos, Barnabas},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year={2017}
}

@article{wiatowski2018mathematical,
  title={A mathematical theory of deep convolutional neural networks for feature extraction},
  author={Wiatowski, Thomas and B{\"o}lcskei, Helmut},
  journal={IEEE Transactions on Information Theory},
  volume={64},
  number={3},
  pages={1845--1866},
  year={2018},
  publisher={IEEE}
}


@article{bruna2013invariant,
  title={Invariant scattering convolution networks},
  author={Bruna, Joan and Mallat, St{\'e}phane},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1872--1886},
  year={2013},
  publisher={IEEE}
}

@article{mallat2012group,
  title={Group invariant scattering},
  author={Mallat, St{\'e}phane},
  journal={Communications on Pure and Applied Mathematics},
  volume={65},
  number={10},
  pages={1331--1398},
  year={2012},
  publisher={Wiley Online Library}
}



@inproceedings{gens2014deep,
  title={Deep symmetry networks},
  author={Gens, Robert and Domingos, Pedro M},
  booktitle={Advances in neural information processing systems},
  pages={2537--2545},
  year={2014}
}


@article{kondor2018generalization,
  title={On the generalization of equivariance and convolution in neural networks to the action of compact groups},
  author={Kondor, Risi and Trivedi, Shubhendu},
  journal={arXiv preprint arXiv:1802.03690},
  year={2018}
}


@article{cohen2018spherical,
  title={Spherical cnns},
  author={Cohen, Taco S and Geiger, Mario and K{\"o}hler, Jonas and Welling, Max},
  journal={arXiv preprint arXiv:1801.10130},
  year={2018}
}

@article{cohen2016steerable,
  title={Steerable cnns},
  author={Cohen, Taco S and Welling, Max},
  journal={arXiv preprint arXiv:1612.08498},
  year={2016}
}

@inproceedings{worrall2017harmonic,
  title={Harmonic networks: Deep translation and rotation equivariance},
  author={Worrall, Daniel E and Garbin, Stephan J and Turmukhambetov, Daniyar and Brostow, Gabriel J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5028--5037},
  year={2017}
}

@inproceedings{lyle2016analysis,
  title={An Analysis of the Effect of Invariance on Generalization in Neural Networks},
  author={Lyle, Clare and Kwiatkowksa, Marta and Gal, Yarin},
  booktitle={International conference on machine learning Workshop on Understanding and Improving Generalization in Deep Learning},
  year={2019}
}



@inproceedings{cohen2016group,
  title={Group equivariant convolutional networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={International conference on machine learning},
  pages={2990--2999},
  year={2016}
}




@article{liao2018sharpening,
  title={Sharpening Jensen's Inequality},
  author={Liao, JG and Berg, Arthur},
  journal={The American Statistician},
  pages={1--4},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{thickstun2018invariances,
  title={Invariances and data augmentation for supervised music transcription},
  author={Thickstun, John and Harchaoui, Zaid and Foster, Dean P and Kakade, Sham M},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2241--2245},
  year={2018},
  organization={IEEE}
}

@book{giri1996group,
  title={Group invariance in statistical inference},
  author={Giri, Narayan C},
  year={1996},
  publisher={World Scientific}
}

@article{helland2004statistical,
  title={Statistical inference under symmetry},
  author={Helland, Inge S},
  journal={International Statistical Review},
  volume={72},
  number={3},
  pages={409--422},
  year={2004},
  publisher={Wiley Online Library}
}


@article{liu17supp,
  title = {{Supplement to ``$e$PCA: High Dimensional Exponential Family PCA''}},
  author = {Lydia T. Liu and Edgar Dobriban and Amit Singer},
  year = {2017}
}

@article{kurta17,
  title = {Correlations in Scattered X-Ray Laser Pulses Reveal Nanoscale Structural Features of Viruses},
  author = {Kurta, Ruslan P. and Donatelli, Jeffrey J. and Yoon, Chun Hong and others},
  journal = {Phys. Rev. Lett.},
  volume = {119},
  issue = {15},
  pages = {158102},
  numpages = {7},
  year = {2017},
  month = {Oct},
  publisher = {American Physical Society}
}

@article{Martin:12,
  author = {A. V. Martin and F. Wang and N. D. Loh and T. Ekeberg and others},
  journal = {Opt. Express},
  keywords = {X-ray imaging; Image reconstruction techniques},
  number = {15},
  pages = {16650--16661},
  publisher = {OSA},
  title = {Noise-robust coherent diffractive imaging with a single diffraction pattern},
  volume = {20},
  month = {Jul},
  year = {2012},
  abstract = {The resolution of single-shot coherent diffractive imaging at X-ray free-electron laser facilities is limited by the low signal-to-noise level of diffraction data at high scattering angles. The iterative reconstruction methods, which phase a continuous diffraction pattern to produce an image, must be able to extract information from these weak signals to obtain the best quality images. Here we show how to modify iterative reconstruction methods to improve tolerance to noise. The method is demonstrated with the hybrid input-output method on both simulated data and single-shot diffraction patterns taken at the Linac Coherent Light Source.},
}

@book{xfelbook,
  editor ="Bergmann, Uwe and Yachandra, Vittal and Yano, Junko",
  title  ="X-Ray Free Electron Lasers",
  subtitle  ="Applications in Materials{,} Chemistry and Biology",
  publisher  ="The Royal Society of Chemistry",
  year  ="2017",
  series  ="Energy and Environment Series",
  edition  ="",
  abstract  ="",
  pages  ="P001-463"
}


@article{pande2015simulations,
  Author = {Pande, K and Schmidt, M and Schwander, P and Saldin, DK},
  Date-Added = {2016-03-15 16:08:49 +0000},
  Date-Modified = {2016-03-15 16:08:49 +0000},
  Journal = {Structural Dynamics},
  Number = {2},
  Pages = {024103},
  Publisher = {American Crystallographic Association, Inc.},
  Title = {Simulations on time-resolved structure determination of uncrystallized biomolecules in the presence of shot noise},
  Volume = {2},
  Year = {2015}}

@article{pande2014deducing,
  Author = {Pande, K and Schwander, P and Schmidt, M and Saldin, DK},
  Date-Added = {2016-03-15 16:02:23 +0000},
  Date-Modified = {2016-03-15 16:02:23 +0000},
  Journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
  Number = {1647},
  Pages = {20130332},
  Publisher = {The Royal Society},
  Title = {Deducing fast electron density changes in randomly orientated uncrystallized biomolecules in a pump--probe experiment},
  Volume = {369},
  Year = {2014}}

@article{josse2016bootstrap,
  title={Bootstrap-based regularization for low-rank matrix estimation},
  author={Josse, Julie and Wager, Stefan},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={124},
  pages={1--29},
  year={2016}
}

@article{scheres2007disentangling,
  title={Disentangling conformational states of macromolecules in {3D-EM} through likelihood optimization},
  author={Scheres, Sjors HW and Gao, Haixiao and Valle, Mikel and Herman, Gabor T and Eggermont, Paul PB and Frank, Joachim and Carazo, Jose-Maria},
  journal={Nature Methods},
  volume={4},
  number={1},
  pages={27--29},
  year={2007},
  publisher={Nature Publishing Group}
}

@article{kam1977determination,
  title={Determination of macromolecular structure in solution by spatial correlation of scattering fluctuations},
  author={Kam, Zvi},
  journal={Macromolecules},
  volume={10},
  number={5},
  pages={927--934},
  year={1977},
  publisher={ACS Publications}
}

@article{Starodub2012,
  Author = {Starodub, D. and Aquila, A. and Bajt, S. and others},
  Day = {11},
  Journal = {Nature Communications},
  Month = {12},
  Publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN  -},
  Title = {Single-particle structure determination by correlations of snapshot X-ray diffraction patterns},
  Ty = {JOUR},
  Volume = {3},
  Year = {2012}
  }

@article{Saldin2009,
  author={D K Saldin and V L Shneerson and R Fung and A Ourmazd},
  title={Structure of isolated biomolecules obtained from ultrashort x-ray pulses: exploiting the symmetry of random orientations},
  journal={Journal of Physics: Condensed Matter},
  volume={21},
  number={13},
  year={2009},
}

@ARTICLE{Makitalo2011, 
  author={M. Makitalo and A. Foi}, 
  journal={IEEE Transactions on Image Processing}, 
  title={Optimal Inversion of the {Anscombe} Transformation in Low-Count {Poisson} Image Denoising}, 
  year={2011}, 
  volume={20}, 
  number={1}, 
  pages={99-109}, 
  keywords={AWGN;image denoising;least mean squares methods;maximum likelihood estimation;additive white Gaussian noise;anscombe transformation;inverse transformation;low-count {Poisson} image denoising;maximum likelihood inverse;minimum mean square error inverse;optimal inversion;three-step procedure;Denoising;{Poisson} noise;photon-limited imaging;variance stabilization}, 
  ISSN={1057-7149}, 
  month={Jan},}

@article{anscombe1948,
author = {Anscombe, F. J.},
title = {THE TRANSFORMATION OF POISSON, BINOMIAL AND NEGATIVE-BINOMIAL DATA},
journal = {Biometrika},
volume = {35},
number = {3-4},
pages = {246},
year = {1948}
}

@article{freeman1950,
  author = {Freeman, Murray F. and Tukey, John W.},
  fjournal = {The Annals of Mathematical Statistics},
  journal = {Ann. Math. Statist.},
  month = {12},
  number = {4},
  pages = {607--611},
  publisher = {The Institute of Mathematical Statistics},
  title = {{Transformations Related to the Angular and the Square Root}},
  volume = {21},
  year = {1950}
}

@book{knott1999latent,
  title={Latent variable models and factor analysis},
  author={Knott, Martin and Bartholomew, David J},
  year={1999},
  publisher={Edward Arnold}
}

@article{huber2004estimation,
  title={Estimation of generalized linear latent variable models},
  author={Huber, Philippe and Ronchetti, Elvezio and Victoria-Feser, Maria-Pia},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={66},
  number={4},
  pages={893--908},
  year={2004},
  publisher={Wiley Online Library}
}

@article{anders2010differential,
  title={Differential expression analysis for sequence count data},
  author={Anders, Simon and Huber, Wolfgang},
  journal={Genome Biology},
  volume={11},
  number={10},
  pages={1},
  year={2010},
  publisher={BioMed Central}
}

@article{ledoit2004well,
  title={A well-conditioned estimator for large-dimensional covariance matrices},
  author={Ledoit, Olivier and Wolf, Michael},
  journal={Journal of multivariate analysis},
  volume={88},
  number={2},
  pages={365--411},
  year={2004},
  publisher={Elsevier}
}

@article {Gaffney1444,
  author = {Gaffney, K. J. and Chapman, H. N.},
  title = {Imaging Atomic Structure and Dynamics with Ultrafast X-ray Scattering},
  volume = {316},
  number = {5830},
  pages = {1444--1448},
  year = {2007},
  doi = {10.1126/science.1135923},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  journal = {Science}
}


@article{duane2009,
  title = {Reconstruction algorithm for single-particle diffraction imaging experiments},
  author = {Loh, Ne-Te Duane and Elser, Veit},
  journal = {Phys. Rev. E},
  volume = {80},
  issue = {2},
  pages = {026705},
  numpages = {20},
  year = {2009},
  month = {Aug},
  publisher = {American Physical Society}
}


@article{zhao2016fast,
  title={Fast steerable principal component analysis},
  author={Zhao, Zhizhen and Shkolnisky, Yoel and Singer, Amit},
  journal={IEEE Transactions on Computational Imaging},
  volume={2},
  number={1},
  pages={1--12},
  year={2016},
  publisher={IEEE}
}

@article{macosko2015highly,
  title={Highly parallel genome-wide expression profiling of individual cells using nanoliter droplets},
  author={Macosko, Evan Z and Basu, Anindita and Satija, Rahul and Nemesh, James and Shekhar, Karthik and Goldman, Melissa and Tirosh, Itay and Bialas, Allison R and Kamitaki, Nolan and Martersteck, Emily M and others},
  journal={Cell},
  volume={161},
  number={5},
  pages={1202--1214},
  year={2015},
  publisher={Elsevier}
}

@article{stein1956some,
  title={Some problems in multivariate analysis},
  author={Stein, Charles},
  journal={Technical Report, Dept of Statistics, Stanford University},
  year={1956}
}

@article{li2008worldwide,
  title={Worldwide human relationships inferred from genome-wide patterns of variation},
  author={Li, Jun Z and Absher, Devin M and Tang, Hua and Southwick, Audrey M and Casto, Amanda M and Ramachandran, Sohini and Cann, Howard M and Barsh, Gregory S and Feldman, Marcus and Cavalli-Sforza, Luigi L and Myers, Richard},
  journal={Science},
  volume={319},
  number={5866},
  pages={1100--1104},
  year={2008},
  publisher={American Association for the Advancement of Science}
}

@article{dobriban2016sharp,
  title={Sharp detection in {PCA} under correlations: all eigenvalues matter},
  author={Dobriban, Edgar},
  journal={The Annals of Statistics}, 
  volume={45},
  number={4 },
  year={2017},
  pages = { 1810--1833}
}

@book{kay1993fundamentals,
  title={Fundamentals of Statistical Signal Processing: Estimation Theory},
  author={Kay, Steven M},
  volume={3},
  year={1993},
  publisher={Prentice Hall}
}

@book{mallat2008wavelet,
  title={A wavelet tour of signal processing: the sparse way},
  author={Mallat, Stephane},
  year={2008},
  publisher={Academic press}
}


@article{marchenko1967distribution,
  Author = {Marchenko, Vladimir A and Pastur, Leonid A},
  Journal = {Mat. Sb.},
  Number = {4},
  Pages = {507--536},
  Publisher = {Russian Academy of Sciences, Branch of Mathematical Sciences},
  Title = {Distribution of eigenvalues for some sets of random matrices},
  Volume = {114},
  Year = {1967}
}
  
@article{grun2014validation,
  title={Validation of noise models for single-cell transcriptomics},
  author={Gr{\"u}n, Dominic and Kester, Lennart and van Oudenaarden, Alexander},
  journal={Nat Methods},
  volume={11},
  number={6},
  pages={637--40},
  year={2014}
}

@inproceedings{donoho1993nonlinear,
  title={Nonlinear wavelet methods for recovery of signals, densities, and spectra from indirect and noisy data},
  author={Donoho, David L},
  booktitle={In Proceedings of Symposia in Applied Mathematics},
  year={1993},
}

@article{nowak1999wavelet,
  title={Wavelet-domain filtering for photon imaging systems},
  author={Nowak, Robert D and Baraniuk, Richard G},
  journal={IEEE Transactions on Image Processing},
  volume={8},
  number={5},
  pages={666--678},
  year={1999},
  publisher={IEEE}
}

@article{milanfar2013tour,
  title={A tour of modern image filtering: New insights and methods, both practical and theoretical},
  author={Milanfar, Peyman},
  journal={IEEE Signal Processing Magazine},
  volume={30},
  number={1},
  pages={106--128},
  year={2013},
  publisher={IEEE}
}

@book{elad2010sparse,
  title={Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing},
  author={Elad, Michael},
  year={2010},
  publisher={Springer Science \& Business Media}
}

@book{starck2010sparse,
  title={Sparse image and signal processing: wavelets, curvelets, morphological diversity},
  author={Starck, Jean-Luc and Murtagh, Fionn and Fadili, Jalal M},
  year={2010},
  publisher={Cambridge university press}
}


@article{worsley2005comparing,
  title={Comparing functional connectivity via thresholding correlations and singular value decomposition},
  author={Worsley, Keith J and Chen, Jen-I and Lerch, Jason and Evans, Alan C},
  journal={Philosophical Transactions of the Royal Society of London B: Biological Sciences},
  volume={360},
  number={1457},
  pages={913--920},
  year={2005},
  publisher={The Royal Society}
}

@article{ostergaard1996high,
  title={High resolution measurement of cerebral blood flow using intravascular tracer bolus passages. Part {I}: Mathematical approach and statistical analysis},
  author={{\O}stergaard, Leif and Weisskoff, Robert M and Chesler, David A and Gyldensted, Carsten and Rosen, Bruce R},
  journal={Magnetic Resonance in Medicine},
  volume={36},
  number={5},
  pages={715--725},
  year={1996},
  publisher={Wiley Online Library}
}

@article{favre2015xtop,
  title={{XTOP: high-resolution X-ray diffraction and imaging}},
  author={Favre-Nicolin, Vincent and Baruchel, Jos{\'e} and Renevier, Hubert and Eymery, Jo{\"e}l and Borb{\'e}ly, Andr{\'a}s},
  journal={Journal of Applied Crystallography},
  volume={48},
  number={3},
  pages={620--620},
  year={2015},
  publisher={International Union of Crystallography}
}

@article{maia2016trickle,
  title={{The trickle before the torrent—diffraction data from X-ray lasers}},
  author={Maia, Filipe RNC and Hajdu, Janos},
  journal={Scientific Data},
  volume={3},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{Furnival2016,
  title = {Denoising time-resolved microscopy image sequences with singular value thresholding},
  journal = {Ultramicroscopy },
  year = {2016},
  issn = {0304-3991},
  author = {Tom Furnival and Rowan K. Leary and Paul A. Midgley},
}


@article{maia2016condor,
  title={Condor: A Simulation Tool for Flash X-Ray Imaging},
  author={Hantke, Max F. and Ekeberg, Tomas and Maia, Filipe R. N. C.  },
  journal={Journal of Applied Crystallography},
  volume={49},
  number={4},
  pages={1356--1362},
  year={2016},
  publisher={PMC}
}

@article{nadakuditi2014optshrink,
  title={OptShrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage},
  author={Nadakuditi, Raj Rao},
  journal={IEEE Transactions on Information Theory},
  volume={60},
  number={5},
  pages={3002--3018},
  year={2014},
  publisher={IEEE}
}

@article{gavish2014optimal,
  title={The optimal hard threshold for singular values is},
  author={Gavish, Matan and Donoho, David L},
  journal={IEEE Transactions on Information Theory},
  volume={60},
  number={8},
  pages={5040--5053},
  year={2014},
  publisher={IEEE}
}

@article{luisier2011image,
  title={Image denoising in mixed {Poisson}--Gaussian noise},
  author={Luisier, Florian and Blu, Thierry and Unser, Michael},
  journal={IEEE Transactions on Image Processing},
  volume={20},
  number={3},
  pages={696--708},
  year={2011},
  publisher={IEEE}
}

@article{luisier2007new,
  title={A new SURE approach to image denoising: Interscale orthonormal wavelet thresholding},
  author={Luisier, Florian and Blu, Thierry and Unser, Michael},
  journal={IEEE Transactions on image processing},
  volume={16},
  number={3},
  pages={593--606},
  year={2007},
  publisher={IEEE}
}

@article{salmon2014poisson,
  title={{Poisson} noise reduction with non-local {PCA}},
  author={Salmon, Joseph and Harmany, Zachary and Deledalle, Charles-Alban and Willett, Rebecca},
  journal={Journal of Mathematical Imaging and Vision},
  volume={48},
  number={2},
  pages={279--294},
  year={2014},
  publisher={Springer}
}

@techreport{landgraf2015generalized,
  title={Generalized principal component analysis: Projection of saturated model parameters},
  author={Landgraf, Andrew J and Lee, Yoonkyung},
  year={2015},
  institution={Technical Report 892, Department of Statistics, The Ohio State University}
}

@inproceedings{sajama2004semi,
  title={Semi-parametric exponential family {PCA}},
  author={Sajama, Sajama and Orlitsky, Alon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1177--1184},
  year={2004}
}

@article{de2006principal,
  title={Principal component analysis of binary data by iterated singular value decomposition},
  author={De Leeuw, Jan},
  journal={Computational statistics \& data analysis},
  volume={50},
  number={1},
  pages={21--39},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{li2010simple,
  title={{Simple exponential family PCA.}},
  author={Li, Jun and Tao, Dacheng},
  booktitle={AISTATS},
  pages={453--460},
  year={2010}
}

@book{hyvarinen2004independent,
  title={Independent Component Analysis},
  author={Hyv{\"a}rinen, Aapo and Karhunen, Juha and Oja, Erkki},
  year={2004},
  publisher={John Wiley \& Sons}
}

@article{qian1994principal,
  title={Principal components selection by the criterion of the minimum mean difference of complexity},
  author={Qian, Guo Qi and Gabor, George and Gupta, RP},
  journal={Journal of Multivariate Analysis},
  volume={49},
  number={1},
  pages={55--75},
  year={1994},
  publisher={Elsevier}
}

@book{yao2015large,
  Author = {Yao, Jianfeng and Bai, Zhidong and Zheng, Shurong},
  Date-Added = {2015-04-28 22:52:15 +0000},
  Date-Modified = {2015-04-28 22:52:15 +0000},
  Publisher = {Cambridge University Press},
  Title = {Large Sample Covariance Matrices and High-Dimensional Data Analysis},
  Year = {2015}}

@article{paul2014random,
  title={Random matrix theory in statistics: A review},
  author={Paul, Debashis and Aue, Alexander},
  journal={Journal of Statistical Planning and Inference},
  volume={150},
  pages={1--29},
  year={2014},
  publisher={Elsevier}
}

@book{anderson1958introduction,
  Author = {Anderson, Theodore W},
  Date-Added = {2015-04-28 22:34:13 +0000},
  Date-Modified = {2015-04-28 22:48:34 +0000},
  Publisher = {Wiley New York},
  Title = {An Introduction to Multivariate Statistical Analysis},
  Year = {2003}}
  
@book{muirhead2009aspects,
  title={Aspects of multivariate statistical theory},
  author={Muirhead, Robb J},
  year={2009},
  publisher={John Wiley \& Sons}
}

  
  
@article{gower1966some,
  title={Some distance properties of latent root and vector methods used in multivariate analysis},
  author={Gower, John C},
  journal={Biometrika},
  volume={53},
  number={3-4},
  pages={325--338},
  year={1966},
  publisher={Biometrika Trust}
}

@article{bhamre2016denoising,
  title={Denoising and covariance estimation of single particle cryo-{EM} images},
  author={Bhamre, Tejal and Zhang, Teng and Singer, Amit},
  journal={Journal of Structural Biology},
  volume={195},
  number={1},
  pages={72--81},
  year={2016},
  publisher={Elsevier}
}

@article{singer2013two,
  title={Two-dimensional tomography from noisy projections taken at unknown random directions},
  author={Singer, A and Wu, H-T},
  journal={SIAM Journal on Imaging Sciences},
  volume={6},
  number={1},
  pages={136--175},
  year={2013},
  publisher={SIAM}
}

@article{lee2010convergence,
  title={Convergence and prediction of principal component scores in high-dimensional settings},
  author={Lee, Seunggeun and Zou, Fei and Wright, Fred A},
  journal={Annals of Statistics},
  volume={38},
  number={6},
  pages={3605--3629},
  year={2010},
}


@article{benaych2011eigenvalues,
  title={The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices},
  author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal={Advances in Mathematics},
  volume={227},
  number={1},
  pages={494--521},
  year={2011},
  publisher={Elsevier}
}

@article{paul2007asymptotics,
  title={Asymptotics of sample eigenstructure for a large dimensional spiked covariance model},
  author={Paul, Debashis},
  journal={Statistica Sinica},
  volume={17},
  number={4},
  pages={1617-1642},
  year={2007}
}

@article{baik2005phase,
  title={Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  author={Baik, Jinho and Ben Arous, G{\'e}rard and P{\'e}ch{\'e}, Sandrine},
  journal={Annals of Probability},
  volume = {33}, 
  number = {5}, 
  pages={1643--1697},
  year={2005},
  publisher={JSTOR}
}

@article{anderson2010data,
  title={Data quality control in genetic case-control association studies},
  author={Anderson, Carl A and Pettersson, Fredrik H and Clarke, Geraldine M and Cardon, Lon R and Morris, Andrew P and Zondervan, Krina T},
  journal={Nature Protocols},
  volume={5},
  number={9},
  pages={1564--1573},
  year={2010},
  publisher={Nature Publishing Group}
}

@book{jolliffe2002principal,
  title={Principal Component Analysis},
  author={Jolliffe, Ian},
  year={2002},
  publisher={Wiley Online Library}
}

@article{dobriban2015efficient,
author = {Dobriban, Edgar},
title = {Efficient computation of limit spectra of sample covariance matrices},
journal = {Random Matrices: Theory and Applications},
volume = {04},
number = {04},
pages = {1550019},
year = {2015},
} 

@book{bai2009spectral,
  Author = {Bai, Zhidong and Silverstein, Jack W},
  Publisher = {Springer},
  Series = {Springer Series in Statistics},
  Title = {Spectral analysis of large dimensional random matrices},
  Year = {2009}}


@article{stegle2015computational,
  title={Computational and analytical challenges in single-cell transcriptomics},
  author={Stegle, Oliver and Teichmann, Sarah A and Marioni, John C},
  journal={Nature Reviews Genetics},
  volume={16},
  number={3},
  pages={133--145},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{visscher2012five,
  title={Five years of {GWAS} discovery},
  author={Visscher, Peter M and Brown, Matthew A and McCarthy, Mark I and Yang, Jian},
  journal={The American Journal of Human Genetics},
  volume={90},
  number={1},
  pages={7--24},
  year={2012},
  publisher={Elsevier}
}

@article{patterson2006population,
  title={Population structure and eigenanalysis},
  author={Patterson, N and Price, AL and Reich, D},
  journal={PLoS Genet},
  volume={2},
  number={12},
  pages={e190},
  year={2006}
}

@article{bigot2016generalized,
  title={Generalized {SURE} for optimal shrinkage of singular values in low-rank matrix denoising},
  author={Bigot, J{\'e}r{\'e}mie and Deledalle, Charles and F{\'e}ral, Delphine},
  journal={arXiv preprint arXiv:1605.07412},
  year={2016}
}

@article{shabalin2013reconstruction,
  title={Reconstruction of a low-rank matrix in the presence of Gaussian noise},
  author={Shabalin, Andrey A and Nobel, Andrew B},
  journal={Journal of Multivariate Analysis},
  volume={118},
  pages={67--76},
  year={2013},
  publisher={Elsevier}
}

@article{hartigan1969linear,
  title={Linear bayesian methods},
  author={Hartigan, JA},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  pages={446--454},
  year={1969},
    volume={31},
  number={3},
  publisher={JSTOR}
}

@article{josse2014stable,
  title={Stable Autoencoding: A Flexible Framework for Regularized Low-Rank Matrix Estimation},
  author={Josse, Julie and Wager, Stefan},
  journal={arXiv preprint arXiv:1410.8275},
  year={2014}
}

@article{deerwester1990indexing,
  title={Indexing by latent semantic analysis},
  author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
  journal={Journal of the American society for Information Science},
  volume={41},
  number={6},
  pages={391},
  year={1990},
  publisher={American Documentation Institute}
}

@article{bloemendal2016principal,
  title={On the principal components of sample covariance matrices},
  author={Bloemendal, Alex and Knowles, Antti and Yau, Horng-Tzer and Yin, Jun},
  journal={Probability Theory and Related Fields},
  volume={164},
  number={1-2},
  pages={459--552},
  year={2016},
  publisher={Springer}
}

@article{lee2014tracy,
  title={Tracy-Widom distribution for the largest eigenvalue of real sample covariance matrices with general population},
  author={Lee, Ji Oon and Schnelli, Kevin},
  journal={arXiv preprint arXiv:1409.4979},
  year={2014}
}

@article{pillai2012edge,
  title={Edge universality of correlation matrices},
  author={Pillai, Natesh S and Yin, Jun},
  journal={The Annals of Statistics},
  pages={1737--1763},
  year={2012},
  publisher={JSTOR}
}

@article{bao2015universality,
  title={Universality for the largest eigenvalue of sample covariance matrices with general population},
  author={Bao, Zhigang and Pan, Guangming and Zhou, Wang},
  journal={The Annals of Statistics},
  volume={43},
  number={1},
  pages={382--421},
  year={2015},
  publisher={Institute of Mathematical Statistics}
}

@article{pillai2014universality,
  title={Universality of covariance matrices},
  author={Pillai, Natesh S and Yin, Jun},
  journal={The Annals of Applied Probability},
  volume={24},
  number={3},
  pages={935--1001},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}

@article{benaych2012singular,
  title={The singular values and vectors of low rank perturbations of large rectangular random matrices},
  author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal={Journal of Multivariate Analysis},
  volume={111},
  pages={120--135},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{anden2015covariance,
  title={Covariance estimation using conjugate gradient for 3D classification in Cryo-{EM}},
  author={And{\'e}n, Joakim and Katsevich, Eugene and Singer, Amit},
  booktitle={Biomedical Imaging (ISBI), 2015 IEEE 12th International Symposium on},
  pages={200--204},
  year={2015},
  organization={IEEE}
}

@article{katsevich2015covariance,
  title={Covariance matrix estimation for the Cryo-{EM} heterogeneity problem},
  author={Katsevich, Eugene and Katsevich, Alexander and Singer, Amit},
  journal={SIAM Journal on Imaging Sciences},
  volume={8},
  number={1},
  pages={126--185},
  year={2015},
  publisher={SIAM}
}

@incollection {johnstone2007high,
    AUTHOR = {Johnstone, Iain M.},
     TITLE = {High dimensional statistical inference and random matrices},
 BOOKTITLE = {International {C}ongress of {M}athematicians. {V}ol. {I}},
     PAGES = {307--333},
 PUBLISHER = {Eur. Math. Soc., Z\"urich},
      YEAR = {2007},
}



@article{Hasinoff,
author = {Hasinoff, Samuel W},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Hasinoff - Unknown - Photon , {Poisson} noise.pdf:pdf},
isbn = {0387307710},
journal = {Computer},
pages = {5--8},
title = {{Photon , {Poisson} noise}}
}
@article{Salmon2014,
abstract = {Photon limitations arise in spectral imaging, nuclear medicine, as- tronomy and night vision. The {Poisson} distribution used to model this noise has variance equal to its mean so blind application of stan- dard noise removals methods yields significant artifacts. Recently, overcomplete dictionaries combined with sparse learning techniques have become extremely popular in image reconstruction. The aim of the present work is to demonstrate that for the task of image de- noising, nearly state-of-the-art results can be achieved using small dictionaries only, provided that they are learned directly from the noisy image. To this end, we introduce patch-based denoising algo- rithms which perform an adaptation of PCA (Principal Component Analysis) for {Poisson} noise. We carry out a comprehensive empiri- cal evaluation of the performance of our algorithms in terms of ac- curacy when the photon count is really low. The results reveal that, despite its simplicity, PCA-flavored denoising appears to be compet- itive with other state-of-the-art denoising algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.0338},
author = {Salmon, Joseph and Harmany, Zachary and Deledalle, Charles Alban and Willett, Rebecca},
doi = {10.1007/s10851-013-0435-6},
eprint = {1206.0338},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Salmon et al. - 2014 - {Poisson} noise reduction with non-local PCA.pdf:pdf},
isbn = {9781467300469},
issn = {09249907},
journal = {Journal of Mathematical Imaging and Vision},
keywords = {Gradient methods,Image denoising,Newton's method,PCA,Signal representations},
number = {2},
pages = {279--294},
title = {{{Poisson} noise reduction with non-local PCA}},
volume = {48},
year = {2014}
}
@article{Ponce2011,
abstract = {We present here an efficient algorithm to compute the Principal Component Analysis (PCA) of a large image set consisting of images and, for each image, the set of its uniform rotations in the plane. We do this by pointing out the block circulant structure of the covariance matrix and utilizing that structure to compute its eigenvectors. We also demonstrate the advantages of this algorithm over similar ones with numerical experiments. Although it is useful in many settings, we illustrate the specific application of the algorithm to the problem of cryo-electron microscopy.},
author = {Ponce, Colin and Singer, Amit},
doi = {10.1109/TIP.2011.2147323},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Ponce, Singer - 2011 - Computing steerable principal components of a large set of images and their rotations.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {EDICS Category: TEC-PRC image and video processing},
number = {11},
pages = {3051--3062},
pmid = {21536533},
title = {{Computing steerable principal components of a large set of images and their rotations}},
volume = {20},
year = {2011}
}
@article{Cai2010,
author = {Cai, Jian-Feng and Cand{\`{e}}s, Emmanuel J. and Shen, Zuowei},
doi = {10.1137/080738970},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Cand{\`{e}}s, Shen - 2010 - A Singular Value Thresholding Algorithm for Matrix Completion.pdf:pdf},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
keywords = {15A83,65K05,90C25,Lagrange dual function,SVT,Uzawa's algorithm,linearized Bregman iteration,matrix completion,nuclear norm minimization,singular value thresholding},
language = {en},
mendeley-tags = {SVT,matrix completion},
month = {jan},
number = {4},
pages = {1956--1982},
publisher = {Society for Industrial and Applied Mathematics},
title = {{A Singular Value Thresholding Algorithm for Matrix Completion}},
volume = {20},
year = {2010}
}
@article{Zhao,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.0781v3},
author = {Zhao, Zhizhen and Shkolnisky, Yoel and Singer, Amit},
eprint = {arXiv:1412.0781v3},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Zhao, Shkolnisky, Singer - Unknown - Fast Steerable Principal Component Analysis.pdf:pdf},
pages = {1--11},
title = {{Fast Steerable Principal Component Analysis}}
}
@article{Singh2008,
author = {Singh, Ajit P and Gordon, Geoffrey J},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Gordon - 2008 - LNAI 5212 - A Unified View of Matrix Factorization Models.pdf:pdf},
journal = {Matrix},
pages = {358--373},
title = {{LNAI 5212 - A Unified View of Matrix Factorization Models}},
year = {2008}
}
@article{Xiuyuan,
annote = {From Jane},
author = {Xiuyuan},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Xiuyuan - Unknown - Chapter 7 Symmetry detection.pdf:pdf},
title = {{Chapter 7 Symmetry detection}}
}
@article{Kam1980,
annote = {From Jane},
author = {Kam, Z},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Kam - 1980 - The reconstruction of structure from electron micrographs of randomly oriented particles.pdf:pdf},
journal = {Journal of Theoretical Biology},
number = {1},
pages = {15--39},
pmid = {7401655},
title = {{The reconstruction of structure from electron micrographs of randomly oriented particles.}},
volume = {82},
year = {1980}
}
@article{Ahmadi2014,
author = {Ahmadi, Amir Ali and Hall, G and Ye, J},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Ahmadi, Hall, Ye - 2014 - Lec13p2 , ORF363 COS323 Why SDP.pdf:pdf},
pages = {1--14},
title = {{Lec13p2 , ORF363 / COS323 Why SDP ?}},
year = {2014}
}
@article{Collins2001,
author = {Collins, Michael and Dasgupta, S and Schapire, Re},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Collins, Dasgupta, Schapire - 2001 - A generalization of principal component analysis to the exponential family.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
title = {{A generalization of principal component analysis to the exponential family}},
year = {2001}
}
@article{Soni,
author = {Soni, Akshay and Haupt, Jarvis},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Soni, Haupt - Unknown - Estimation Error Guarantees for {Poisson} Denoising with Sparse and Structured Dictionary Models.pdf:pdf},
number = {1},
title = {{Estimation Error Guarantees for {Poisson} Denoising with Sparse and Structured Dictionary Models}}
}
@article{Schwander:12, 
  author = {Peter Schwander and Dimitrios Giannakis and Chun Hong Yoon and Abbas Ourmazd}, 
  journal = {Opt. Express}, 
  keywords = {Free-electron lasers (FELs); Three-dimensional microscopy; Inverse scattering; Scattering, molecules; Scattering theory},
  number = {12}, 
  pages = {12827--12849}, 
  publisher = {OSA},
  title = {{The symmetries of image formation by scattering. II. Applications}}, 
  volume = {20}, 
  month = {Jun},
  year = {2012},
}


@inproceedings{Cao2014,
  title={Low-rank matrix recovery in {Poisson} noise},
  author={Cao, Yang and Xie, Yao},
  booktitle={Signal and Information Processing (GlobalSIP), 2014 IEEE Global Conference on},
  pages={384--388},
  year={2014},
  organization={IEEE}
}


@article{baik2006eigenvalues,
  title={Eigenvalues of large sample covariance matrices of spiked population models},
  author={Baik, Jinho and Silverstein, Jack W},
  journal={Journal of Multivariate Analysis},
  volume={97},
  number={6},
  pages={1382--1408},
  year={2006},
  publisher={Elsevier}
}


@article{johnstone2001distribution,
  title={On the distribution of the largest eigenvalue in principal components analysis},
  author={Johnstone, Iain M},
  journal={Annals of Statistics},
    volume={29},
  number={2},
  pages={295--327},
  year={2001},
  publisher={JSTOR}
}

@article{nadler2008finite,
  title={Finite sample approximation results for principal component analysis: A matrix perturbation approach},
  author={Nadler, Boaz},
  journal={The Annals of Statistics},
  volume={36},
  number={6},
  pages={2791--2817},
  year={2008}
}

@article{Tropp2010,
  author = {Tropp, Joel A.},
  title = {User-Friendly Tail Bounds for Sums of Random Matrices},
  journal = {Found. Comput. Math.},
  issue_date = {August 2012},
  volume = {12},
  number = {4},
  month = aug,
  year = {2012},
  issn = {1615-3375},
  pages = {389--434},
  numpages = {46},
  url = {http://dx.doi.org/10.1007/s10208-011-9099-z},
  doi = {10.1007/s10208-011-9099-z},
  acmid = {2347804},
  publisher = {Springer-Verlag New York, Inc.},
  address = {Secaucus, NJ, USA},
  keywords = {Large deviation, Probability inequality, Random matrix, Sum of independent random variables},
} 

@article{Oberg2012,
author = {Oberg, Ann L and Bot, Brian M and Grill, Diane E and Poland, Gregory A and Therneau, Terry M},
title = {{Technical and biological variance structure in mRNA-Seq data : life in the real world}},
year = {2012}
}

@incollection{Vershynin2011,
author = {Vershynin, Roman},
publisher = {Cambridge Univ. Press},
address = {Cambridge},
pages = {210--268},
booktitle = {Compressed sensing},
keywords = {matrices,probability,random},
title = {{Introduction to the non-asymptotic analysis of random matrices}},
year = {2012}
}
@article{Dong2015,
author = {Dong, Kai and Zhao, Hongyu and Wan, Xiang and Tong, Tiejun},
file = {:Users/tingruolydia/Downloads/NBLDA.pdf:pdf},
number = {2011},
pages = {1--7},
title = {{BIOINFORMATICS NBLDA : Negative Binomial Linear Discriminant Analysis for RNA-Seq Data}},
year = {2015}
}

@incollection{Tropp2015,
  author       = {Joel A. Tropp}, 
  title = {{The Expected Norm of a Sum of Independent Random Matrices: An Elementary Approach}},
  booktitle    = {High-Dimensional Probability VII},
  publisher    = {Birkhaeuser},
  year         = 2016,
  series       = {Progress in Probability 71},
}



@article{Gotze1991,
author = {Gotze, F.},
doi = {10.1214/aop/1176990448},
file = {:Users/tingruolydia/Downloads/euclid.aop.1176990448.pdf:pdf},
issn = {0091-1798},
journal = {The Annals of Probability},
number = {2},
pages = {724--739},
title = {{On the Rate of Convergence in the Multivariate CLT}},
volume = {19},
year = {1991}
}
@article{Vershynin2012,
abstract = {Given a probability distribution in R{\^{}}n with general (non-white) covariance, a classical estimator of the covariance matrix is the sample covariance matrix obtained from a sample of N independent points. What is the optimal sample size N = N(n) that guarantees estimation with a fixed accuracy in the operator norm? Suppose the distribution is supported in a centered Euclidean ball of radius $\backslash$sqrt{\{}n{\}}. We conjecture that the optimal sample size is N = O(n) for all distributions with finite fourth moment, and we prove this up to an iterated logarithmic factor. This problem is motivated by the optimal theorem of Rudelson which states that N = O(n $\backslash$log n) for distributions with finite second moment, and a recent result of Adamczak, Litvak, Pajor and Tomczak-Jaegermann which guarantees that N = O(n) for sub-exponential distributions.},
archivePrefix = {arXiv},
arxivId = {1004.3484},
author = {Vershynin, Roman},
doi = {10.1007/s10959-010-0338-z},
eprint = {1004.3484},
file = {:Users/tingruolydia/Documents/PACM/sample-covariance.pdf:pdf},
issn = {08949840},
journal = {Journal of Theoretical Probability},
keywords = {Estimation of covariance matrices,Random matrices with independent columns,Sample covariance matrices},
number = {3},
pages = {655--686},
title = {{How Close is the Sample Covariance Matrix to the Actual Covariance Matrix?}},
volume = {25},
year = {2012}
}
@article{Adamczak2010,
abstract = {Let {\$}X{\_}1,..., X{\_}N\backslashin\backslashR{\^{}}n{\$} be independent centered random vectors with log-concave distribution and with the identity as covariance matrix. We show that with overwhelming probability at least {\$}1 - 3 \backslashexp(-c\backslashsqrt{\{}n{\}}\backslashr){\$} one has {\$} \backslashsup{\_}{\{}x\backslashin S{\^{}}{\{}n-1{\}}{\}} \backslashBig|\backslashfrac{\{}1/N{\}}\backslashsum{\_}{\{}i=1{\}}{\^{}}N (|{\textless}X{\_}i, x{\textgreater}|{\^{}}2 - \backslashE|{\textless}X{\_}i, x{\textgreater}|{\^{}}2\backslashr)\backslashBig| \backslashleq C \backslashsqrt{\{}\backslashfrac{\{}n/N{\}}{\}},{\$} where {\$}C{\$} is an absolute positive constant. This result is valid in a more general framework when the linear forms {\$}({\textless}X{\_}i,x{\textgreater}){\_}{\{}i\backslashleq N, x\backslashin S{\^{}}{\{}n-1{\}}{\}}{\$} and the Euclidean norms {\$}(|X{\_}i|/\backslashsqrt n){\_}{\{}i\backslashleq N{\}}{\$} exhibit uniformly a sub-exponential decay. As a consequence, if {\$}A{\$} denotes the random matrix with columns {\$}(X{\_}i){\$}, then with overwhelming probability, the extremal singular values {\$}\backslashlambda{\_}{\{}\backslashrm min{\}}{\$} and {\$}\backslashlambda{\_}{\{}\backslashrm max{\}}{\$} of {\$}AA{\^{}}\backslashtop{\$} satisfy the inequalities {\$} 1 - C\backslashsqrt{\{}{\{}n/N{\}}{\}} \backslashle {\{}\backslashlambda{\_}{\{}\backslashrm min{\}}/N{\}} \backslashle \backslashfrac{\{}\backslashlambda{\_}{\{}\backslashrm max{\}}/N{\}} \backslashle 1 + C\backslashsqrt{\{}{\{}n/N{\}}{\}} {\$} which is a quantitative version of Bai-Yin theorem $\backslash$cite{\{}BY{\}} known for random matrices with i.i.d. entries.},
archivePrefix = {arXiv},
arxivId = {1012.0294},
author = {Adamczak, Rados{\l}aw and Litvak, Alexander E. and Pajor, Alain and Tomczak-Jaegermann, Nicole},
doi = {10.1016/j.crma.2010.12.014},
eprint = {1012.0294},
file = {:Users/tingruolydia/Documents/PACM/adamczak09.pdf:pdf},
issn = {1631073X},
journal = {Comptes Rendus {\ldots}},
keywords = {approximation of covariance matrices,at the de-,began when this author,convex bodies,held a postdoctoral position,isotropic measures,log-concave measures,norm of random matrices,numbers,random matrices,uniform laws of large,work on this paper},
number = {1},
pages = {1--6},
title = {{Sharp bounds on the rate of convergence of the empirical covariance matrix}},
url = {http://www.sciencedirect.com/science/article/pii/S1631073X10003936$\backslash$nhttp://arxiv.org/abs/1012.0294},
year = {2010}
}
@article{Srivastava2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1106.2775v1},
author = {Srivastava, Nikhil and Vershynin, Roman},
doi = {10.1214/12-AOP760},
eprint = {arXiv:1106.2775v1},
file = {:Users/tingruolydia/Downloads/1106.2775.pdf:pdf},
issn = {00911798},
journal = {Annals of Probability},
keywords = {Covariance matrices,High-dimensional distributions,Log-concave distributions,Random matrices,Stieltjes transform},
number = {5},
pages = {3081--3111},
title = {{Covariance estimation for distributions with 2 + epsilon moments}},
volume = {41},
year = {2013}
}
@article{Rudelson1999,
abstract = {Let {\$}y{\$} be a random vector in $\backslash$rn, satisfying {\$}{\$} $\backslash$Bbb E $\backslash$, $\backslash$tens{\{}y{\}} = id. {\$}{\$} Let {\$}M{\$} be a natural number and let {\$}y{\_}1 \backslashetc y{\_}M{\$} be independent copies of {\$}y{\$}. We prove that for some absolute constant {\$}C{\$} {\$}{\$} $\backslash$enor{\{}$\backslash$frac{\{}1{\}}{\{}M{\}} $\backslash$sum{\_}i $\backslash$tens{\{}y{\_}i{\}} - id{\}} $\backslash$le C $\backslash$cdot $\backslash$frac{\{}$\backslash$sqrt{\{}$\backslash$log M{\}}{\}}{\{}$\backslash$sqrt{\{}M{\}}{\}} $\backslash$cdot $\backslash$left ( $\backslash$enor{\{}y{\}}{\^{}}{\{}$\backslash$log M{\}} $\backslash$right ){\^{}}{\{}1/ $\backslash$log M{\}}, {\$}{\$} provided that the last expression is smaller than 1. We apply this estimate to obtain a new proof of a result of Bourgain concerning the number of random points needed to bring a convex body into a nearly isotropic position.},
archivePrefix = {arXiv},
arxivId = {math/9608208},
author = {Rudelson, Mark},
doi = {10.1006/jfan.1998.3384},
eprint = {9608208},
file = {:Users/tingruolydia/Documents/PACM/rudelson.pdf:pdf},
issn = {00221236},
journal = {Journal of Functional Analysis},
pages = {1--12},
primaryClass = {math},
title = {{Random vectors in the isotropic position}},
url = {http://arxiv.org/abs/math/9608208$\backslash$nhttp://www.sciencedirect.com/science/article/pii/S0022123698933845},
year = {1999}
}
@article{Boucheron2005,
abstract = {A general method for obtaining moment inequalities for functions of independent random variables is presented. It is a generalization of the entropy method which has been used to derive concentration inequalities for such functions [Boucheron, Lugosi and Massart Ann. Probab. 31 (2003) 1583-1614], and is based on a generalized tensorization inequality due to Latala and Oleszkiewicz [Lecture Notes in Math. 1745 (2000) 147-168]. The new inequalities prove to be a versatile tool in a wide range of applications. We illustrate the power of the method by showing how it can be used to effortlessly re-derive classical inequalities including Rosenthal and Kahane-Khinchine-type inequalities for sums of independent random variables, moment inequalities for suprema of empirical processes and moment inequalities for Rademacher chaos and U-statistics. Some of these corollaries are apparently new. In particular, we generalize Talagrand's exponential inequality for Rademacher chaos of order 2 to any order. We also discuss applications for other complex functions of independent random variables, such as suprema of Boolean polynomials which include, as special cases, subgraph counting problems in random graphs.},
archivePrefix = {arXiv},
arxivId = {math/0503651},
author = {Boucheron, St??phane and Bousquet, Olivier and Lugosi, G??bor and Massart, Pascal},
eprint = {0503651},
file = {:Users/tingruolydia/Downloads/document.pdf:pdf},
journal = {Annals of Probability},
keywords = {Concentration inequalities,Empirical processes,Moment inequalities,Random graphs},
number = {2},
pages = {514--560},
primaryClass = {math},
title = {{Moment inequalities for functions of independent random variables}},
volume = {33},
year = {2005}
}
@article{Li2015,
abstract = {We study weighted l(2) fidelity in variational models for {Poisson} noise$\backslash$nrelated image restoration problems. Gaussian approximation to {Poisson}$\backslash$nnoise statistic is adopted to deduce weighted l(2) fidelity. Different$\backslash$nfrom the traditional weighted l(2) approximation, we propose a$\backslash$nreweighted l(2) fidelity with sparse regularization by wavelet frame.$\backslash$nBased on the split Bregman algorithm introduced in {\{}[{\}}21], the proposed$\backslash$nnumerical scheme is composed of three easy subproblems that involve$\backslash$nquadratic minimization, soft shrinkage and matrix vector$\backslash$nmultiplications. Unlike usual least square approximation of {Poisson}$\backslash$nnoise, we dynamically update the underlying noise variance from previous$\backslash$nestimate. The solution of the proposed algorithm is shown to be the same$\backslash$nas the one obtained by minimizing Kullback-Leibler divergence fidelity$\backslash$nwith the same regularization. This reweighted l(2) formulation can be$\backslash$neasily extended to mixed {Poisson}-Gaussian noise case. Finally, the$\backslash$nefficiency and quality of the proposed algorithm compared to other$\backslash$n{Poisson} noise removal methods are demonstrated through denoising and$\backslash$ndeblurring examples. Moreover, mixed {Poisson}-Gaussian noise tests are$\backslash$nperformed on both simulated and real digital images for further$\backslash$nillustration of the performance of the proposed method.},
author = {Li, Jia and Shen, Zuowei and Yin, Rujie and Zhang, Xiaoqun},
doi = {10.3934/ipi.2015.9.875},
file = {:Users/tingruolydia/Documents/PACM/literature/weightedL2{\_}IPI{\_}140226{\_}RY.pdf:pdf},
issn = {1930-8337},
journal = {Inverse Problems and Imaging},
keywords = {{Poisson} noise; {Poisson}-Gaussian mixed noise; inver},
number = {3},
pages = {875--894},
title = {{A REWEIGHTED l(2) METHOD FOR IMAGE RESTORATION WITH POISSON AND MIXED POISSON-GAUSSIAN NOISE}},
volume = {9},
year = {2015}
}

@inproceedings{Sonnleitner2016,
  title={Local retrodiction models for photon-noise-limited images},
  author={Sonnleitner, Matthias and Jeffers, John and Barnett, Stephen M},
  booktitle={SPIE Photonics Europe},
  pages={98960V--98960V},
  year={2016},
  organization={International Society for Optics and Photonics}
}

@article{Basri2003,
author = {Basri, Ronen and Jacobs, David W},
keywords = {Index Terms—Face recognition,Lambertian,illumination,linear subspaces,object recognition,specular,spherical harmonics},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {218--233},
title = {{Lambertian Reflectance and Linear Subspaces}},
volume = {25},
year = {2003}
}

@article{udell2016,
  title={{Generalized Low Rank Models}},
  author={Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen},
  journal={Foundations and Trends in Machine Learning},
  volume={9},
  number={1},
  pages={1--118},
  year={2016},
}

@inproceedings{Udell2014,
  author = {Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen},
  booktitle = {NIPS Workshop on Distributed Machine Learning and Matrix Computations},
  title = {{Generalized Low Rank Models}},
  year = {2014}
}


@article{Donoho2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.0851v1},
author = {Donoho, Dl and Gavish, M and Johnstone, Im},
eprint = {arXiv:1311.0851v1},
file = {:Users/tingruolydia/Documents/PACM/literature/1311.0851v2.pdf:pdf},
journal = {arXiv preprint arXiv:1311.0851},
keywords = {bhattacharya,condition number loss,covariance estimation,divergence loss,echet distance,entropy loss,fr,high-dimensional asymptotics,matusita affinity,optimal nonlinearity,precision estimation,principal compo-,quadratic loss,spiked covariance,stein loss},
pages = {1--35},
title = {{Optimal shrinkage of eigenvalues in the Spiked Covariance Model}},
volume = {0906812},
year = {2013}
}
@article{Luisier2011,
abstract = {We propose a general methodology (PURE-LET) to design and optimize a wide class of transform-domain thresholding algorithms for denoising images corrupted by mixed {Poisson}{\&}{\#}x2013;Gaussian noise. We express the denoising process as a linear expansion of thresholds (LET) that we optimize by relying on a purely data-adaptive unbiased estimate of the mean-squared error (MSE), derived in a non-Bayesian framework (PURE: {Poisson}{\&}{\#}x2013;Gaussian unbiased risk estimate). We provide a practical approximation of this theoretical MSE estimate for the tractable optimization of arbitrary transform-domain thresholding. We then propose a pointwise estimator for undecimated filterbank transforms, which consists of subband-adaptive thresholding functions with signal-dependent thresholds that are globally optimized in the image domain. We finally demonstrate the potential of the proposed approach through extensive comparisons with state-of-the-art techniques that are specifically tailored to the estimation of {Poisson} intensities. We also present denoising results obtained on real images of low-count fluorescence microscopy.},
author = {Luisier, Florian and Blu, Thierry and Unser, Michael},
doi = {10.1109/TIP.2010.2073477},
file = {:Users/tingruolydia/Documents/PACM/literature/Luisier2011.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Filterbank,Gaussian noise,MSE estimation,{Poisson} noise,image denoising,thresholding,unbiased risk estimate},
number = {3},
pages = {696--708},
pmid = {20840902},
title = {{Image denoising in mixed poissongaussian noise}},
volume = {20},
year = {2011}
}


@article{liu2018pca,
  title={$ e $ PCA: High dimensional exponential family PCA},
  author={Liu, Lydia T and Dobriban, Edgar and Singer, Amit},
  journal={The Annals of Applied Statistics},
  volume={12},
  number={4},
  pages={2121--2150},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@article{vonesch2015steerable,
  title={Steerable PCA for rotation-invariant image recognition},
  author={Vonesch, C{\'e}dric and Stauber, Fr{\'e}d{\'e}ric and Unser, Michael},
  journal={SIAM Journal on Imaging Sciences},
  volume={8},
  number={3},
  pages={1857--1873},
  year={2015},
  publisher={SIAM}
}


@article{zhao2018steerable,
  title={Steerable $ e $ PCA},
  author={Zhao, Zhizhen and Liu, Lydia T and Singer, Amit},
  journal={arXiv preprint arXiv:1812.08789},
  year={2018}
}


@article{scheres2005maximum,
  title={Maximum-likelihood multi-reference refinement for electron microscopy images},
  author={Scheres, Sjors HW and Valle, Mikel and Nu{\~n}ez, Rafael and Sorzano, Carlos OS and Marabini, Roberto and Herman, Gabor T and Carazo, Jose-Maria},
  journal={Journal of molecular biology},
  volume={348},
  number={1},
  pages={139--149},
  year={2005},
  publisher={Elsevier}
}


@article{bendory2018bispectrum,
  title={Bispectrum inversion with application to multireference alignment},
  author={Bendory, Tamir and Boumal, Nicolas and Ma, Chao and Zhao, Zhizhen and Singer, Amit},
  journal={IEEE Transactions on Signal Processing},
  volume={66},
  number={4},
  pages={1037--1050},
  year={2018},
  publisher={IEEE}
}

@book{frank2006three,
  title={Three-dimensional electron microscopy of macromolecular assemblies: visualization of biological molecules in their native state},
  author={Frank, Joachim},
  year={2006},
  publisher={Oxford University Press}
}

@article{kam1980reconstruction,
  title={The reconstruction of structure from electron micrographs of randomly oriented particles},
  author={Kam, Zvi},
  journal={Journal of Theoretical Biology},
  volume={82},
  number={1},
  pages={15--39},
  year={1980},
  publisher={Elsevier}
}


@article{bandeira2017estimation,
  title={Estimation under group actions: recovering orbits from invariants},
  author={Bandeira, Afonso S and Blum-Smith, Ben and Kileel, Joe and Perry, Amelia and Weed, Jonathan and Wein, Alexander S},
  journal={arXiv preprint arXiv:1712.10163},
  year={2017}
}

@article{singer2018mathematics,
  title={Mathematics for cryo-electron microscopy},
  author={Singer, Amit},
  journal={arXiv preprint arXiv:1803.06714},
  year={2018}
}


@inproceedings{hauberg2016dreaming,
  title={Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation},
  author={Hauberg, S{\o}ren and Freifeld, Oren and Larsen, Anders Boesen Lindbo and Fisher, John and Hansen, Lars},
  booktitle={Artificial Intelligence and Statistics},
  pages={342--350},
  year={2016}
}


@article{zhang2018dada,
  title={DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification},
  author={Zhang, Xiaofeng and Wang, Zhangyang and Liu, Dong and Ling, Qing},
  journal={arXiv preprint arXiv:1809.00981},
  year={2018}
}

@inproceedings{tran2017bayesian,
  title={A bayesian data augmentation approach for learning deep models},
  author={Tran, Toan and Pham, Trung and Carneiro, Gustavo and Palmer, Lyle and Reid, Ian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2797--2806},
  year={2017}
}
@article{buslaev2018albumentations,
  title={Albumentations: fast and flexible image augmentations},
  author={Buslaev, Alexander and Parinov, Alex and Khvedchenya, Eugene and Iglovikov, Vladimir I and Kalinin, Alexandr A},
  journal={arXiv preprint arXiv:1809.06839},
  year={2018}
}
@InProceedings{cubuk2018autoaugment,
author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
title = {AutoAugment: Learning Augmentation Strategies From Data},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
} 

@article{devries2017dataset,
  title={Dataset augmentation in feature space},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1702.05538},
  year={2017}
}

@article{wang2018low,
  title={Low-Shot Learning from Imaginary Data},
  author={Wang, Yu-Xiong and Girshick, Ross and Hebert, Martial and Hariharan, Bharath},
  journal={arXiv preprint arXiv:1801.05401},
  year={2018}
}

@article{antoniou2017data,
  title={Data augmentation generative adversarial networks},
  author={Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
  journal={arXiv preprint arXiv:1711.04340},
  year={2017}
}

@article{sixt2018rendergan,
  title={Rendergan: Generating realistic labeled data},
  author={Sixt, Leon and Wild, Benjamin and Landgraf, Tim},
  journal={Frontiers in Robotics and AI},
  volume={5},
  pages={66},
  year={2018},
  publisher={Frontiers}
}

@inproceedings{ratner2017learning,
  title={Learning to compose domain-specific transformations for data augmentation},
  author={Ratner, Alexander J and Ehrenberg, Henry and Hussain, Zeshan and Dunnmon, Jared and R{\'e}, Christopher},
  booktitle={Advances in neural information processing systems},
  pages={3236--3246},
  year={2017}
}

@article{mirza2014conditional,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014}
}

@article{gavish-donoho-2017,
  title={Optimal shrinkage of singular values},
  author={Gavish, Matan and Donoho, David L.},
  journal={IEEE Transactions on Information Theory},
  volume={63},
  number={4},
  pages={2137-2152},
  year={2017}
}

@article{huo2018aggregated,
  title={Aggregated inference},
  author={Huo, Xiaoming and Cao, Shanshan},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  pages={e1451},
  publisher={Wiley Online Library}
}

@article{chen2014subadditivity,
  title={Subadditivity of matrix $\phi$-entropy and concentration of random matrices},
  author={Chen, Richard Y and Tropp, Joel A},
  journal={Electron. J. Probab},
  volume={19},
  number={27},
  pages={1--30},
  year={2014}
}

@book{bertsekas1989parallel,
  title={Parallel and distributed computation: numerical methods},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  volume={23},
  year={1989},
  publisher={Prentice hall Englewood Cliffs, NJ}
}

@article{boyd2011distributed,
  title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
  author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
  journal={Foundations and Trends{\textregistered} in Machine learning},
  volume={3},
  number={1},
  pages={1--122},
  year={2011},
  publisher={Now Publishers, Inc.}
}

@book{lynch1996distributed,
  title={Distributed algorithms},
  author={Lynch, Nancy A},
  year={1996},
  publisher={Elsevier}
}

@book{rauber2013parallel,
  title={Parallel programming: For multicore and cluster systems},
  author={Rauber, Thomas and R{\"u}nger, Gudula},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{blelloch2010parallel,
  title={Parallel algorithms},
  author={Blelloch, Guy E and Maggs, Bruce M},
  booktitle={Algorithms and theory of computation handbook},
  pages={25--25},
  year={2010},
  organization={Chapman \& Hall/CRC}
}

@article{koutris2018algorithmic,
  title={Algorithmic Aspects of Parallel Data Processing},
  author={Koutris, Paraschos and Salihoglu, Semih and Suciu, Dan and others},
  journal={Foundations and Trends{\textregistered} in Databases},
  volume={8},
  number={4},
  pages={239--370},
  year={2018},
  publisher={Now Publishers, Inc.}
}

@inproceedings{liu2014distributed,
  title={Distributed estimation, information loss and exponential families},
  author={Liu, Qiang and Ihler, Alexander T},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1098--1106},
  year={2014}
}

@inproceedings{zinkevich2010parallelized,
  title={Parallelized stochastic gradient descent},
  author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J},
  booktitle={Advances in neural information processing systems},
  pages={2595--2603},
  year={2010}
}

@inproceedings{mcdonald2009efficient,
  title={Efficient large-scale distributed training of conditional maximum entropy models},
  author={Mcdonald, Ryan and Mohri, Mehryar and Silberman, Nathan and Walker, Dan and Mann, Gideon S},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1231--1239},
  year={2009}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: a system for large-scale machine learning.},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={OSDI},
  volume={16},
  pages={265--283},
  year={2016}
}

@Manual{nycflights13,
  title = {nycflights13: Flights that Departed NYC in 2013},
  author = {Hadley Wickham},
  year = {2018},
  note = {R package version 1.0.0},
  url = {https://CRAN.R-project.org/package=nycflights13},
}

@article{muller2016random,
  title={Random Matrix Theory Tutorial--Introduction to Deterministic Equivalents},
  author={M{\"u}ller, Axel and Debbah, M{\'e}rouane},
  journal={TRAITEMENT DU SIGNAL},
  volume={33},
  number={2-3},
  pages={223--248},
  year={2016},
  publisher={PRESSES UNIV GRENOBLE 1041 RUE DES RESIDENCES, GRENOBLE, 38040, FRANCE}
}

@article{peacock2008eigenvalue,
  title={Eigenvalue distributions of sums and products of large random matrices via incremental matrix expansions},
  author={Peacock, Matthew JM and Collings, Iain B and Honig, Michael L},
  journal={IEEE Transactions on Information Theory},
  volume={54},
  number={5},
  pages={2123--2138},
  year={2008},
  publisher={IEEE}
}

@article{couillet2014analysis,
  title={Analysis of the limiting spectral measure of large random matrices of the separable covariance type},
  author={Couillet, Romain and Hachem, Walid},
  journal={Random Matrices: Theory and Applications},
  volume={3},
  number={04},
  pages={1450016},
  year={2014},
  publisher={World Scientific}
}

@phdthesis{lixin2007spectral,
  title={Spectral analysis of large dimentional random matrices},
  author={Zhang, Lixin},
  year={2007}
}

@article{paul2009no,
  title={No eigenvalues outside the support of the limiting empirical spectral distribution of a separable covariance matrix},
  author={Paul, Debashis and Silverstein, Jack W},
  journal={Journal of Multivariate Analysis},
  volume={100},
  number={1},
  pages={37--57},
  year={2009},
  publisher={Elsevier}
}
@book{mardia1979multivariate,
  title={Multivariate analysis},
  author={Mardia, Kanti and Kent, John T and Bibby, John M},
  year={1979},
  publisher={Academic Press}
}
@book{white2012hadoop,
  title={Hadoop: The definitive guide},
  author={White, Tom},
  year={2012},
  publisher={" O'Reilly Media, Inc."}
}

@article{zaharia2010spark,
  title={Spark: Cluster computing with working sets.},
  author={Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  journal={HotCloud},
  volume={10},
  number={10-10},
  pages={95},
  year={2010}
}

@article{dean2008mapreduce,
  title={MapReduce: simplified data processing on large clusters},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  journal={Communications of the ACM},
  volume={51},
  number={1},
  pages={107--113},
  year={2008},
  publisher={ACM}
}

@article{zhang2013communication,
  title={Communication-Efficient Algorithms for Statistical Optimization},
  author={Zhang, Yuchen and Duchi, John C and Wainwright, Martin J},
  journal={Journal of Machine Learning Research},
  volume={14},
  pages={3321--3363},
  year={2013}
}

@inproceedings{zhang2012communication,
  title={Communication-efficient algorithms for statistical optimization},
  author={Zhang, Yuchen and Wainwright, Martin J and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1502--1510},
  year={2012}
}

@article{zhu2018distributed,
  title={Distributed Nonparametric Regression under Communication Constraints},
  author={Zhu, Yuancheng and Lafferty, John},
  journal={arXiv preprint arXiv:1803.01302},
  year={2018}
}

@article{fan2017distributed,
  title={Distributed Estimation of Principal Eigenspaces},
  author={Fan, Jianqing and Wang, Dong and Wang, Kaizheng and Zhu, Ziwei},
  journal={arXiv preprint arXiv:1702.06488},
  year={2017}
}

@article{smith2016cocoa,
  title={CoCoA: A general framework for communication-efficient distributed optimization},
  author={Smith, Virginia and Forte, Simone and Ma, Chenxin and Tak{\'a}c, Martin and Jordan, Michael I and Jaggi, Martin},
  journal={arXiv preprint arXiv:1611.02189},
  year={2016}
}

@article{jordan2016communication,
  title={Communication-efficient distributed statistical inference},
  author={Jordan, Michael I and Lee, Jason D and Yang, Yun},
  journal={arXiv preprint arXiv:1605.07689},
  year={2016}
}

@inproceedings{braverman2016communication,
  title={Communication lower bounds for statistical estimation problems via a distributed data processing inequality},
  author={Braverman, Mark and Garg, Ankit and Ma, Tengyu and Nguyen, Huy L and Woodruff, David P},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={1011--1020},
  year={2016},
  organization={ACM}
}

@article{duchi2014optimality,
  title={Optimality guarantees for distributed statistical estimation},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Zhang, Yuchen},
  journal={arXiv preprint arXiv:1405.0782},
  year={2014}
}

@article{lee2017communication,
  title={Communication-efficient sparse regression},
  author={Lee, Jason D and Liu, Qiang and Sun, Yuekai and Taylor, Jonathan E},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={5},
  pages={1--30},
  year={2017}
}

@article{lin2017distributed,
  title={Distributed learning with regularized least squares},
  author={Lin, Shao-Bo and Guo, Xin and Zhou, Ding-Xuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3202--3232},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{zhang2013divide,
  title={Divide and conquer kernel ridge regression},
  author={Zhang, Yuchen and Duchi, John and Wainwright, Martin},
  booktitle={Conference on Learning Theory},
  pages={592--617},
  year={2013}
}

@article{zhang2015divide,
  title={Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates},
  author={Zhang, Yuchen and Duchi, John and Wainwright, Martin},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={3299--3340},
  year={2015},
  publisher={JMLR. org}
}

@article{rosenblatt2016optimality,
  title={On the optimality of averaging in distributed statistical learning},
  author={Rosenblatt, Jonathan D and Nadler, Boaz},
  journal={Information and Inference: A Journal of the IMA},
  volume={5},
  number={4},
  pages={379--404},
  year={2016},
  publisher={Oxford University Press}
}

@article{battey2018distributed,
  title={Distributed testing and estimation under sparse high dimensional models},
  author={Battey, Heather and Fan, Jianqing and Liu, Han and Lu, Junwei and Zhu, Ziwei},
  journal={The Annals of Statistics},
  volume={46},
  number={3},
  pages={1352--1382},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}


@article{dobriban2016weighted,
  title={Weighted mining of massive collections of $p$-values by convex optimization},
  author={Dobriban, Edgar},
  journal={arXiv preprint arXiv:1603.05334},
  year={2016}
}


@article{jurczak2015spectral,
  title={Spectral analysis of high-dimensional sample covariance matrices with missing observations},
  author={Jurczak, Kamil and Rohde, Angelika},
  journal={arXiv preprint arXiv:1507.01615},
  year={2015}
}

@article{tao2013outliers,
  title={Outliers in the spectrum of iid matrices with bounded rank perturbations},
  author={Tao, Terence},
  journal={Probability Theory and Related Fields},
  volume={155},
  number={1-2},
  pages={231--263},
  year={2013},
  publisher={Springer}
}

@article{erdos2012universality,
  title={Universality of local spectral statistics of random matrices},
  author={Erd{\H{o}}s, L{\'a}szl{\'o} and Yau, Horng-Tzer},
  journal={Bulletin of the American Mathematical Society},
  volume={49},
  number={3},
  pages={377--414},
  year={2012}
}

@book{chang2003hyperspectral,
  title={Hyperspectral imaging: techniques for spectral detection and classification},
  author={Chang, Chein-I},
  volume={1},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{fowler2009compressive,
  title={Compressive-projection principal component analysis.},
  author={Fowler, James E},
  journal={IEEE Transactions on Image Processing},
  volume={18},
  number={10},
  pages={2230--2242},
  year={2009},
  publisher={Citeseer}
}

@book{kay1993fundamentals,
  title={Fundamentals of Statistical Signal Processing: Estimation Theory},
  author={Kay, Steven M},
  volume={3},
  year={1993},
  publisher={Prentice Hall}
}

@article{hartigan1969linear,
  title={Linear bayesian methods},
  author={Hartigan, JA},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  pages={446--454},
  year={1969},
  publisher={JSTOR}
}

@book{mallat2008wavelet,
  title={A wavelet tour of signal processing: the sparse way},
  author={Mallat, Stephane},
  year={2008},
  publisher={Academic press}
}

@incollection{frank1996three,
title = "Chapter 3 - Two-Dimensional Averaging Techniques ",
editor = "Frank, Joachim ",
booktitle = "Three-Dimensional Electron Microscopy of Macromolecular Assemblies ",
publisher = "Academic Press",
edition = "",
address = "Burlington",
year = "1996",
pages = "54 - 125",
isbn = "978-0-12-265040-6",
doi = "http://dx.doi.org/10.1016/B978-012265040-6/50003-5",
url = "http://www.sciencedirect.com/science/article/pii/B9780122650406500035",
author = "Joachim Frank"
}


@article{cai2016minimax,
  title={Minimax rate-optimal estimation of high-dimensional covariance matrices with incomplete data},
  author={Cai, T Tony and Zhang, Anru},
  journal={Journal of Multivariate Analysis},
  volume={150},
  pages={55--74},
  year={2016},
  publisher={Elsevier}
}

@article{candes2009exact,
  title={Exact matrix completion via convex optimization},
  author={Cand{\`e}s, Emmanuel J and Recht, Benjamin},
  journal={Foundations of Computational mathematics},
  volume={9},
  number={6},
  pages={717--772},
  year={2009},
  publisher={Springer}
}

@book{schafer1997analysis,
  title={Analysis of incomplete multivariate data},
  author={Schafer, Joseph L},
  year={1997},
  publisher={CRC press}
}

@book{little2014statistical,
  title={Statistical analysis with missing data},
  author={Little, Roderick JA and Rubin, Donald B},
  year={2014},
  publisher={John Wiley \& Sons}
}


@book{petrov2012sums,
  title={Sums of independent random variables},
  author={Petrov, Valentin},
  volume={82},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{lounici2014high,
  title={High-dimensional covariance matrix estimation with missing observations},
  author={Lounici, Karim},
  journal={Bernoulli},
  volume={20},
  number={3},
  pages={1029--1058},
  year={2014},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{lee2010convergence,
  title={Convergence and prediction of principal component scores in high-dimensional settings},
  author={Lee, Seunggeun and Zou, Fei and Wright, Fred A},
  journal={Annals of statistics},
  volume={38},
  number={6},
  pages={3605},
  year={2010},
  publisher={NIH Public Access}
}

@article{srivastava2013covariance,
  title={Covariance estimation for distributions with 2+epsilon moments},
  author={Srivastava, Nikhil and Vershynin, Roman},
  journal={The Annals of Probability},
  volume={41},
  number={5},
  pages={3081--3111},
  year={2013},
  publisher={Institute of Mathematical Statistics}
}

@book{searle2009variance,
  title={Variance components},
  author={Searle, Shayle R and Casella, George and McCulloch, Charles E},
  volume={391},
  year={2009},
  publisher={John Wiley \& Sons}
}

@article{henderson1975best,
  title={Best linear unbiased estimation and prediction under a selection model},
  author={Henderson, Charles R},
  journal={Biometrics},
  pages={423--447},
  year={1975},
  publisher={JSTOR}
}

@article{robinson1991blup,
  title={That BLUP is a good thing: the estimation of random effects},
  author={Robinson, George K},
  journal={Statistical Science},
  pages={15--32},
  year={1991},
  publisher={JSTOR}
}

@article{singer2013two,
  title={Two-dimensional tomography from noisy projections taken at unknown random directions},
  author={Singer, A and Wu, H-T},
  journal={SIAM Journal on Imaging Sciences},
  volume={6},
  number={1},
  pages={136--175},
  year={2013},
  publisher={SIAM}
}

@article{gavish2014optimal,
  title={Optimal shrinkage of singular values},
  author={Gavish, Matan and Donoho, David L},
  journal={arXiv preprint arXiv:1405.7511},
  year={2014}
}


@article{keshavan2010matrix,
  title={Matrix Completion From a Few Entries},
  author={Keshavan, Raghunandan H and Montanari, Andrea and Oh, Sewoong},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={6},
  pages={2980--2998},
  year={2010},
  publisher={IEEE}
}

@article{bai1998no,
  title={No eigenvalues outside the support of the limiting spectral distribution of large-dimensional sample covariance matrices},
  author={Bai, Zhi-Dong and Silverstein, Jack W},
  journal={The Annals of Probability},
    volume={26},
  number={1},
  pages={316--345},
  year={1998},
  publisher={JSTOR}
}

@article{suryaprakash2015consistency,
  title={Consistency and {MSE} Performance of {MUSIC}-Based {DOA} of a Single Source in White Noise With Randomly Missing Data},
  author={Suryaprakash, Raj Tejas and Nadakuditi, Raj Rao},
  journal={IEEE Transactions on Signal Processing},
  volume={63},
  number={18},
  pages={4756--4770},
  year={2015},
  publisher={IEEE}
}

@article{nadakuditi2014optshrink,
  title={OptShrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage},
  author={Nadakuditi, Raj Rao},
  journal={IEEE Transactions on Information Theory},
  volume={60},
  number={5},
  pages={3002--3018},
  year={2014},
  publisher={IEEE}
}

@inproceedings{keshavan2009matrix,
  title={Matrix completion from a few entries},
  author={Keshavan, Raghunandan H and Oh, Sewoong and Montanari, Andrea},
  booktitle={2009 IEEE International Symposium on Information Theory},
  pages={324--328},
  year={2009},
  organization={IEEE}
}

@article{chapon2012outliers,
  title={The outliers among the singular values of large rectangular random matrices with additive fixed rank deformation},
  author={Chapon, Francois and Couillet, Romain and Hachem, Walid and Mestre, Xavier},
  journal={arXiv preprint arXiv:1207.0471},
  year={2012}
}

@article{capitaine2013additive,
  title={Additive/multiplicative free subordination property and limiting eigenvectors of spiked additive deformations of Wigner matrices and spiked sample covariance matrices},
  author={Capitaine, Mireille},
  journal={Journal of Theoretical Probability},
  volume={26},
  number={3},
  pages={595--648},
  year={2013},
  publisher={Springer}
}


@article{bai2007asymptotics,
  title={On asymptotics of eigenvectors of large sample covariance matrix},
  author={Bai, ZD and Miao, BQ and Pan, GM},
  journal={The Annals of Probability},
  volume={35},
  number={4},
  pages={1532--1572},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@article{el2009concentration,
  title={Concentration of measure and spectra of random matrices: applications to correlation matrices, elliptical distributions and beyond},
  author={El Karoui, Noureddine},
  journal={The Annals of Applied Probability},
  volume={19},
  number={6},
  pages={2362--2405},
  year={2009},
  publisher={Institute of Mathematical Statistics}
}

@article{benaych2012singular,
  title={The singular values and vectors of low rank perturbations of large rectangular random matrices},
  author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal={Journal of Multivariate Analysis},
  volume={111},
  pages={120--135},
  year={2012},
  publisher={Elsevier}
}


@article{donoho2013optimal,
  title={Optimal shrinkage of eigenvalues in the spiked covariance model},
  author={Donoho, David L and Gavish, Matan and Johnstone, Iain M},
  journal={arXiv preprint arXiv:1311.0851},
  year={2013}
}

@article{bhamre2016denoising,
  title={Denoising and covariance estimation of single particle cryo-{EM} images},
  author={Bhamre, Tejal and Zhang, Teng and Singer, Amit},
  journal={Journal of Structural Biology},
  volume={195},
  number={1},
  pages={72--81},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{anden2015covariance,
  title={Covariance estimation using conjugate gradient for 3D classification in Cryo-{EM}},
  author={And{\'e}n, Joakim and Katsevich, Eugene and Singer, Amit},
  booktitle={Biomedical Imaging (ISBI), 2015 IEEE 12th International Symposium on},
  pages={200--204},
  year={2015},
  organization={IEEE}
}

@article{katsevich2015covariance,
  title={Covariance matrix estimation for the Cryo-{EM} heterogeneity problem},
  author={Katsevich, Eugene and Katsevich, Alexander and Singer, Amit},
  journal={SIAM Journal on Imaging Sciences},
  volume={8},
  number={1},
  pages={126--185},
  year={2015},
  publisher={SIAM}
}

@article{lehmann1998theory,
  title={Theory of Point Estimation},
  author={Lehmann, EL and Casella, George},
  journal={Springer Texts in Statistics},
  year={1998},
  publisher={New York, NY: Springer New York}
}


@article{hachem2015survey,
  title={A survey on the eigenvalues local behavior of large complex correlated Wishart matrices},
  author={Hachem, Walid and Hardy, Adrien and Najim, Jamal},
  journal={ESAIM: Proceedings and Surveys},
  volume={51},
  pages={150--174},
  year={2015},
  publisher={EDP Sciences}
}

@article{karoui2008spectrum,
  title={Spectrum estimation for large dimensional covariance matrices using random matrix theory},
  author={El Karoui, Noureddine},
  journal={The Annals of Statistics},
  pages={2757--2790},
  year={2008},
  publisher={JSTOR}
}

@book{muirhead2009aspects,
  title={Aspects of multivariate statistical theory},
  author={Muirhead, Robb J},
  volume={197},
  year={2009},
  publisher={John Wiley \& Sons}
}

@incollection {johnstone2007high,
    AUTHOR = {Johnstone, Iain M.},
     TITLE = {High dimensional statistical inference and random matrices},
 BOOKTITLE = {International {C}ongress of {M}athematicians. {V}ol. {I}},
     PAGES = {307--333},
 PUBLISHER = {Eur. Math. Soc., Z\"urich},
      YEAR = {2007},
}
  
  
  
@article{anderson1963asymptotic,
  title={Asymptotic theory for principal component analysis},
  author={Anderson, Theodore Wilbur},
  journal={Annals of Mathematical Statistics},
  pages={122--148},
  year={1963},
  publisher={JSTOR}
}

@article{srivastava2005some,
  title={Some tests concerning the covariance matrix in high dimensional data},
  author={Srivastava, Mu'ni S},
  journal={Journal of the Japan Statistical Society},
  volume={35},
  number={2},
  pages={251--272},
  year={2005},
  publisher={THE JAPAN STATISTICAL SOCIETY}
}

@article{fisher2012testing,
title = "On testing for an identity covariance matrix when the dimensionality equals or exceeds the sample size ",
 author={Fisher, Thomas J},
journal = "Journal of Statistical Planning and Inference ",
volume = "142",
number = "1",
pages = "312 - 326",
year = "2012"
}

@article{paul2007asymptotics,
  title={Asymptotics of sample eigenstructure for a large dimensional spiked covariance model},
  author={Paul, Debashis},
  journal={Statistica Sinica},
  volume={17},
  number={4},
  pages={1617-1642},
  year={2007}
}

@book{jolliffe2002principal,
  title={Principal component analysis},
  author={Jolliffe, Ian},
  year={2002},
  publisher={Wiley Online Library}
}

@article{bai2002determining,
  title={Determining the number of factors in approximate factor models},
  author={Bai, Jushan and Ng, Serena},
  journal={Econometrica},
  volume={70},
  number={1},
  pages={191--221},
  year={2002},
  publisher={Wiley Online Library}
}

@article{price2006principal,
  title={Principal components analysis corrects for stratification in genome-wide association studies},
  author={Price, Alkes L and Patterson, Nick J and Plenge, Robert M and Weinblatt, Michael E and Shadick, Nancy A and Reich, David},
  journal={Nature genetics},
  volume={38},
  number={8},
  pages={904--909},
  year={2006},
  publisher={Nature Publishing Group}
}

@article{zheng2015substitution,
  title={Substitution principle for CLT of linear spectral statistics of high-dimensional sample covariance matrices with applications to hypothesis testing},
  author={Zheng, Shurong and Bai, Zhidong and Yao, Jianfeng},
  journal={The Annals of Statistics},
  volume={43},
  number={2},
  pages={546--591},
  year={2015},
  publisher={Institute of Mathematical Statistics}
}

@article{zheng2015clt,
  title={CLT for linear spectral statistics of large dimensional general Fisher matrices and its applications in high-dimensional data analysis. },
  author={Zheng, Shurong and Bai, Zhidong and Yao, Jianfeng},
  journal={Bernoulli},
  year={2015+},
  note={to appear}
}

@article{li2015testing,
  title={Testing the Sphericity of a covariance matrix when the dimension is much larger than the sample size},
  author={Li, Zeng and Yao, Jianfeng},
  journal={arXiv preprint arXiv:1508.02498},
  year={2015}
}

@article{johnstone2015testing,
  title={Testing in high-dimensional spiked models},
  author={Johnstone, Iain M and Onatski, Alexei},
  journal={arXiv preprint arXiv:1509.07269},
  year={2015}
}

@article{dharmawansa2014local,
  title={Local Asymptotic Normality of the spectrum of high-dimensional spiked F-ratios},
  author={Dharmawansa, Prathapasinghe and Johnstone, Iain M and Onatski, Alexei},
  journal={arXiv preprint arXiv:1411.3875},
  year={2014}
}

@article{li2014hypothesis,
title = "Hypothesis testing for high-dimensional covariance matrices ",
journal = "Journal of Multivariate Analysis ",
author = "Weiming Li and Yingli Qin",
volume = "128",
number = "",
pages = "108 - 119",
year = "2014",
}

@article{paul2014random,
  title={Random matrix theory in statistics: A review},
  author={Paul, Debashis and Aue, Alexander},
  journal={Journal of Statistical Planning and Inference},
  volume={150},
  pages={1--29},
  year={2014},
  publisher={Elsevier}
}

@incollection{cai2014estimating,    
   author = {Cai, T Tony and Ren, Zhao and Zhou, Harrison H},     
   title = {Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation},    
   url = {http://www-stat.wharton.upenn.edu/~tcai/paper/Covariance-Survey.pdf},    
   year = {2014}    
}

@book{bai2008large,
  title={Large dimensional factor analysis},
  author={Bai, Jushan and Ng, Serena},
  year={2008},
  publisher={Now Publishers Inc}
}

@article{bai2012sample,
  title={On sample eigenvalues in a generalized spiked population model},
  author={Bai, Zhidong and Yao, Jianfeng},
  journal={Journal of Multivariate Analysis},
  volume={106},
  pages={167--177},
  year={2012},
  publisher={Elsevier}
}

@book{lehmann2005testing,
  title={Testing statistical hypotheses},
  author={Lehmann, Erich L and Romano, Joseph P},
  year={2005},
  publisher={Springer Science \& Business Media}
}

@book{van1998asymptotic,
  title={Asymptotic statistics},
  author={Van der Vaart, Aad W},
  year={1998},
  publisher={Cambridge University Press}
}

@article{bianchi2011performance,
  title={Performance of statistical tests for single-source detection using random matrix theory},
  author={Bianchi, Pascal and Debbah, Merouane and Ma{\"\i}da, Myl{\`e}ne and Najim, Jamal},
  journal={Information Theory, IEEE Transactions on},
  volume={57},
  number={4},
  pages={2400--2419},
  year={2011},
  publisher={IEEE}
}

@article{nadakuditi2008sample,
  title={Sample eigenvalue based detection of high-dimensional signals in white noise using relatively few samples},
  author={Nadakuditi, Raj Rao and Edelman, Alan},
  journal={Signal Processing, IEEE Transactions on},
  volume={56},
  number={7},
  pages={2625--2638},
  year={2008},
  publisher={IEEE}
}

@article{nadler2008finite,
  title={Finite sample approximation results for principal component analysis: A matrix perturbation approach},
  author={Nadler, Boaz},
  journal={The Annals of Statistics},
  volume={36},
  number={6},
  pages={2791--2817},
  year={2008}
}

@article{kritchman2009non,
  title={Non-parametric detection of the number of signals: Hypothesis testing and random matrix theory},
  author={Kritchman, Shira and Nadler, Boaz},
  journal={Signal Processing, IEEE Transactions on},
  volume={57},
  number={10},
  pages={3930--3941},
  year={2009},
  publisher={IEEE}
}

@article{ahn2013eigenvalue,
  title={Eigenvalue ratio test for the number of factors},
  author={Ahn, Seung C and Horenstein, Alex R},
  journal={Econometrica},
  volume={81},
  number={3},
  pages={1203--1227},
  year={2013},
  publisher={Wiley Online Library}
}

@article{onatski2012asymptotics,
  title={Asymptotics of the principal components estimator of large factor models with weakly influential factors},
  author={Onatski, Alexei},
  journal={Journal of Econometrics},
  volume={168},
  number={2},
  pages={244--258},
  year={2012},
  publisher={Elsevier}
}

@article{bai2012estimation,
  title={Estimation of spiked eigenvalues in spiked models},
  author={Bai, Zhidong and Ding, Xue},
  journal={Random Matrices: Theory and Applications},
  volume={1},
  number={02},
  pages={1150011},
  year={2012},
  publisher={World Scientific}
}

@article{benaych2011eigenvalues,
  title={The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices},
  author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal={Advances in Mathematics},
  volume={227},
  number={1},
  pages={494--521},
  year={2011},
  publisher={Elsevier}
}

@article{Kumar2015limitations,
author = {Krishna Kumar, Siddharth and Feldman, Marcus W. and Rehkopf, David H. and Tuljapurkar, Shripad}, 
title = {Limitations of {GCTA} as a solution to the missing heritability problem},
year = {2015}, 
journal = {Proceedings of the National Academy of Sciences} 
}

@article{dobriban2015high,
  title={High-Dimensional Asymptotics of Prediction: Ridge Regression and Classification},
  author={Dobriban, Edgar and Wager, Stefan},
  journal={arXiv preprint arXiv:1507.03003},
  year={2015}
}

@article{onatski2009testing,
  title={Testing hypotheses about the number of factors in large factor models},
  author={Onatski, Alexei},
  journal={Econometrica},
  volume={77},
  number={5},
  pages={1447--1479},
  year={2009},
  publisher={Wiley Online Library}
}

@book{hald1998history,
  title={A History of Mathematical Statistics from 1750 to 1930},
  author={Hald, Anders},
  year={1998},
  publisher={Wiley-Interscience}
}

@book{groetsch1977generalized,
  title={Generalized inverses of linear operators: representation and approximation},
  author={Groetsch, Charles W},
    publisher={Marcel Dekker},
  year={1977}
}

@book{bogachev2007measure,
  title={Measure theory},
  author={Bogachev, Vladimir I},
  volume={1},
  year={2007},
  publisher={Springer Science \& Business Media}
}

@article{barber2015controlling,
author = {Barber, Rina Foygel and Cand\`es, Emmanuel J.},
fjournal = {The Annals of Statistics},
journal = {Ann. Statist.},
title={Controlling the false discovery rate via knockoffs},
month = {10},
number = {5},
pages = {2055--2085},
volume = {43},
year = {2015}
}


@article{zumbach2011empirical,
  title={Empirical properties of large covariance matrices},
  author={Zumbach, Gilles},
  journal={Quantitative Finance},
  volume={11},
  number={7},
  pages={1091--1102},
  year={2011},
  publisher={Taylor \& Francis}
}

@incollection{bouchaud2009financial,
  title={Financial applications of random matrix theory: a short review},
  author={Bouchaud, Jean-Philippe and Potters, Marc},
  editor      = {Akemann, Gernot and Baik, Jinho and Di Francesco, Phillippe},
  booktitle   = {The Oxford Handbook of Random Matrix Theory},
  publisher   = {Oxford University Press},
  year        = {2011},
}

@article{bryc2013separation,
  title={Separation of the largest eigenvalues in eigenanalysis of genotype data from discrete subpopulations},
  author={Bryc, Katarzyna and Bryc, Wlodek and Silverstein, Jack W},
  journal={Theoretical Population Biology},
  volume={89},
  pages={34--43},
  year={2013},
  publisher={Elsevier}
}

@article{patterson2006population,
  title={Population structure and eigenanalysis},
  author={Patterson, N and Price, AL and Reich, D},
  journal={PLoS Genet},
  volume={2},
  number={12},
  pages={e190},
  year={2006}
}

@book{rudin1987real,
  title={Real and complex analysis},
  author={Rudin, Walter},
  year={1987},
  publisher={McGraw-Hill Education}
}

@article{wang2013sphericity,
  title={On the sphericity test with large-dimensional observations},
  author={Wang, Qinwen and Yao, Jianfeng},
  journal={Electronic Journal of Statistics},
  volume={7},
  pages={2164--2192},
  year={2013},
  publisher={Institute of Mathematical Statistics}
}

@article{wang2014note,
  title={A note on the {CLT} of the {LSS} for sample covariance matrix from a spiked population model},
  author={Wang, Qinwen and Silverstein, Jack W and Yao, Jianfeng},
  journal={Journal of Multivariate Analysis},
  volume={130},
  pages={194--207},
  year={2014},
  publisher={Elsevier}
}

@article{choi2015regularized,
  title={Regularized {LRT} for Large Scale Covariance Matrices: One Sample Problem},
  author={Choi, Young-Geun and Ng, Chi Tim and Lim, Johan},
  journal={arXiv preprint arXiv:1502.00384},
  year={2015}
}

@book{kress2013linear,
  title={Linear Integral Equations},
  author={Kress, Rainer},
  year={2013},
  publisher={Springer}
}

@article{cai2013optimal,
  title={Optimal hypothesis testing for high dimensional covariance matrices},
  author={Cai, T Tony and Ma, Zongming},
  journal={Bernoulli},
  volume={19},
  number={5B},
  pages={2359--2388},
  year={2013},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{johnstone2001distribution,
  title={On the distribution of the largest eigenvalue in principal components analysis},
  author={Johnstone, Iain M},
  journal={Annals of Statistics},
    volume={29},
  number={2},
  pages={295--327},
  year={2001},
  publisher={JSTOR}
}

@article{fisher2010new,
  title={A new test for sphericity of the covariance matrix for high dimensional data},
  author={Fisher, Thomas J and Sun, Xiaoqian and Gallagher, Colin M},
  journal={Journal of Multivariate Analysis},
  volume={101},
  number={10},
  pages={2554--2570},
  year={2010},
  publisher={Elsevier}
}


@article{nagao1973some,
  title={On some test criteria for covariance matrix},
  author={Nagao, Hisao},
  journal={The Annals of Statistics},
    volume={1},
  number={4},
  pages={700--709},
  year={1973},
  publisher={JSTOR}
}

@article{baik2006eigenvalues,
  title={Eigenvalues of large sample covariance matrices of spiked population models},
  author={Baik, Jinho and Silverstein, Jack W},
  journal={Journal of Multivariate Analysis},
  volume={97},
  number={6},
  pages={1382--1408},
  year={2006},
  publisher={Elsevier}
}

@article{john1971optimal,
author = {John, S.}, 
title = {Some optimal multivariate tests},
volume = {58}, 
number = {1}, 
pages = {123-127},
year = {1971}, 
journal = {Biometrika} 
}


@article{baik2005phase,
  title={Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  author={Baik, Jinho and Ben Arous, G{\'e}rard and P{\'e}ch{\'e}, Sandrine},
  journal={Annals of Probability},
  volume = {33}, 
  number = {5}, 
  pages={1643--1697},
  year={2005},
  publisher={JSTOR}
}

@article{mauchly1940significance,
  title={Significance test for sphericity of a normal n-variate distribution},
  author={Mauchly, John W},
  journal={The Annals of Mathematical Statistics},
  volume={11},
  number={2},
  pages={204--209},
  year={1940},
  publisher={JSTOR}
}

@article{onatski2013asymptotic,
  title={Asymptotic power of sphericity tests for high-dimensional data},
  author={Onatski, Alexei and Moreira, Marcelo J and Hallin, Marc},
  journal={The Annals of Statistics},
  volume={41},
  number={3},
  pages={1204--1231},
  year={2013},
  publisher={Institute of Mathematical Statistics}
}


@article{onatski2014signal,
  title={Signal detection in high dimension: The multispiked case},
  author={Onatski, Alexei and Moreira, Marcelo J and Hallin, Marc},
  journal={The Annals of Statistics},
  volume={42},
  number={1},
  pages={225--254},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}


@article{bai2004clt,
  title={{CLT} for linear spectral statistics of large-dimensional sample covariance matrices},
  author={Bai, Zhidong and Silverstein, Jack W},
  journal={The Annals of Probability},
  volume={32},
  number={1A},
  pages={553--605},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}


@article{bai2009corrections,
  title={Corrections to {LRT} on large-dimensional covariance matrix by {RMT}},
  author={Bai, Zhidong and Jiang, Dandan and Yao, Jian-Feng and Zheng, Shurong},
  journal={The Annals of Statistics},
  pages={3822--3840},
    Number = {6B},
  Volume = {37},
    year={2009},
  publisher={JSTOR}
}
 
@article{chen2010tests,
  title={Tests for high-dimensional covariance matrices},
  author={Chen, Song Xi and Zhang, Li-Xin and Zhong, Ping-Shou},
  journal={Journal of the American Statistical Association},
  volume={105}, 
  number={490},
    pages={810--819},
  year={2010}
}


@article{ledoit2002some,
  title={Some hypothesis tests for the covariance matrix when the dimension is large compared to the sample size},
  author={Ledoit, Olivier and Wolf, Michael},
  journal={Annals of Statistics},
  pages={1081--1102},
  year={2002},
      Number = {4},
  Volume = {30},
  publisher={JSTOR}
}


@article{ehrenfeucht1989general,
  title={A general lower bound on the number of examples needed for learning},
  author={Ehrenfeucht, Andrzej and Haussler, David and Kearns, Michael and Valiant, Leslie},
  journal={Information and Computation},
  volume={82},
  number={3},
  pages={247--261},
  year={1989},
  publisher={Elsevier}
}

@inproceedings{wang2012baselines,
  title={Baselines and bigrams: Simple, good sentiment and topic classification},
  author={Wang, Sida and Manning, Christopher D},
  booktitle={Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2},
  pages={90--94},
  year={2012},
  organization={Association for Computational Linguistics}
}

@article{candes2011robust,
  title={Robust principal component analysis?},
  author={Cand{\`e}s, Emmanuel J and Li, Xiaodong and Ma, Yi and Wright, John},
  journal={Journal of the ACM (JACM)},
  volume={58},
  number={3},
  pages={11},
  year={2011},
  publisher={ACM}
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
 booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2015}
}

@article{rifai2011manifold,
  title={The manifold tangent classifier},
  author={Rifai, Salah and Dauphin, Yann and Vincent, Pascal and Bengio, Yoshua and Muller, Xavier},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  pages={2294--2302},
  year={2011}
}

@article{simard2000transformation,
  title={Transformation invariance in pattern recognition: Tangent distance and propagation},
  author={Simard, Patrice Y and Le Cun, Yann A and Denker, John S and Victorri, Bernard},
  journal={International Journal of Imaging Systems and Technology},
  volume={11},
  number={3},
  pages={181--197},
  year={2000}
}

@article{wray2007prediction,
  title={Prediction of individual genetic risk to disease from genome-wide association studies},
  author={Wray, Naomi R and Goddard, Michael E and Visscher, Peter M},
  journal={Genome research},
  volume={17},
  number={10},
  pages={1520--1528},
  year={2007},
  publisher={Cold Spring Harbor Lab}
}

@article{russakovsky2014imagenet,
  title={Image{N}et large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International Journal of Computer Vision},
  pages={1--42},
  year={2014},
  publisher={Springer}
}

@article{kleinberg2015prediction,
  title={Prediction Policy Problems},
  author={Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Obermeyer, Ziad and others},
  journal={American Economic Review},
  volume={105},
  number={5},
  pages={491--95},
  year={2015},
  publisher={American Economic Association}
}

@book{tao2012topics,
  title={Topics in Random Matrix Theory},
  author={Tao, Terence},
  volume={132},
  year={2012},
  publisher={American Mathematical Soc.}
}

@book{anderson2010introduction,
  title={An Introduction to Random Matrices},
  author={Anderson, Greg W and Guionnet, Alice and Zeitouni, Ofer},
  number={118},
  year={2010},
  publisher={Cambridge University Press}
}

@article{witten2009covariance,
  title={Covariance-regularized regression and classification for high dimensional problems},
  author={Witten, Daniela M and Tibshirani, Robert},
  journal={J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  volume={71},
  number={3},
  pages={615--636},
  year={2009},
  publisher={Wiley Online Library}
}

@article{international2005haplotype,
  title={A haplotype map of the human genome},
  author={{International HapMap Consortium}},
  journal={Nature},
  volume={437},
  number={7063},
  pages={1299--1320},
  year={2005},
  publisher={Nature Publishing Group}
}

@article{bernau2015cross-study,
author = {Bernau, Christoph and Riester, Markus and Boulesteix, Anne-Laure and Parmigiani, Giovanni and Huttenhower, Curtis and Waldron, Levi and Trippa, Lorenzo}, 
title = {Cross-study validation for the assessment of prediction algorithms},
volume = {30}, 
number = {12}, 
pages = {i105-i112}, 
year = {2014}, 
journal = {Bioinformatics} 
}

@book{cox1989analysis,
  title={Analysis of {B}inary {D}ata},
  author={Cox, David Roxbee and Snell, E Joyce},
  year={1989},
  publisher={CRC Press},
  edition={2nd}
}

@article{hoerl1970ridge,
  title={Ridge regression: Biased estimation for nonorthogonal problems},
  author={Hoerl, Arthur E and Kennard, Robert W},
  journal={Technometrics},
  volume={12},
  number={1},
  pages={55--67},
  year={1970},
  publisher={Taylor \& Francis Group}
}

@article{sutton2006introduction,
  title={An introduction to conditional random fields for relational learning},
  author={Sutton, Charles and McCallum, Andrew},
  journal={Introduction to statistical relational learning},
  pages={93--128},
  year={2006},
  publisher={MIT press}
}

@inproceedings{toutanova2003feature,
  title={Feature-rich part-of-speech tagging with a cyclic dependency network},
  author={Toutanova, Kristina and Klein, Dan and Manning, Christopher D and Singer, Yoram},
  booktitle={NAACL},
  year={2003}
}

@book{hastie2015statistical,
  title={Statistical Learning with Sparsity: The Lasso and Generalizations},
  author={Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
  year={2015},
  publisher={CRC Press}
}

@article{rubio2012performance,
  title={Performance analysis and optimal selection of large minimum variance portfolios under estimation risk},
  author={Rubio, Francisco and Mestre, Xavier and Palomar, Daniel P},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={6},
  number={4},
  pages={337--350},
  year={2012},
  publisher={IEEE}
}

@article{rubio2011spectral,
  title={Spectral convergence for a general class of random matrices},
  author={Rubio, Francisco and Mestre, Xavier},
  journal={Statistics \& Probability Letters},
  volume={81},
  number={5},
  pages={592--602},
  year={2011},
  publisher={Elsevier}
}

@article{zhang2013finite,
  title={Finite-sample linear filter optimization in wireless communications and financial systems},
  author={Zhang, Mengyi and Rubio, Francisco and Palomar, Daniel P and Mestre, Xavier},
  journal={IEEE Trans. Signal Process.},
  volume={61},
  number={20},
  pages={5014--5025},
  year={2013},
  publisher={IEEE}
}

@article{hachem2008new,
  title={A new approach for mutual information analysis of large dimensional multi-antenna channels},
  author={Hachem, Walid and Khorunzhiy, Oleksiy and Loubaton, Philippe and Najim, Jamal and Pastur, Leonid},
  journal={IEEE Trans. Inform. Theory},
  volume={54},
  number={9},
  pages={3987--4004},
  year={2008},
  publisher={IEEE}
}

@article{hachem2007deterministic,
  title={Deterministic equivalents for certain functionals of large random matrices},
  author={Hachem, Walid and Loubaton, Philippe and Najim, Jamal},
  journal={The Annals of Applied Probability},
  volume={17},
  number={3},
  pages={875--930},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}


@article{raudys1967determining,
  title={On determining training sample size of linear classifier},
  author={Raudys, {\v{S}}ar{\=u}nas},
  journal={Comput. Systems (in Russian)},
  volume={28},
  pages={79–87},
  year={1967}
}

@book{raudys2012statistical,
  title={Statistical and Neural Classifiers: An integrated approach to design},
  author={Raudys, {\v{S}}ar{\=u}nas},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{raudys1995small,
  title={Small sample properties of ridge estimate of the covariance matrix in statistical and neural net classification},
  author={Raudys, {\v{S}}ar{\=u}nas and Skurichina, Marina},
  journal={New Trends in Probability and Statistics},
  volume={3},
  pages={237--245},
  year={1995}
}


@inproceedings{liang2010interaction,
  title={On the interaction between norm and dimensionality: Multiple regimes in learning},
  author={Liang, Percy and Srebro, Nati},
  booktitle={ICML},
  year={2010}
}

@article{donoho2013high,
  title={High dimensional robust {M}-estimation: Asymptotic variance via approximate message passing},
  author={Donoho, David L and Montanari, Andrea},
  journal={arXiv preprint arXiv:1310.7320},
  year={2013}
}

@article{zollanvari2013kolmogorov,
  title={On {K}olmogorov asymptotics of estimators of the misclassification error rate in linear discriminant analysis},
  author={Zollanvari, Amin and Genton, Marc G},
  journal={Sankhya A},
  volume={75},
  number={2},
  pages={300--326},
  year={2013},
  publisher={Springer}
}

@inproceedings{zollanvari2013application,
  title={Application of double asymptotics and random matrix theory in error estimation of regularized linear discriminant analysis},
  author={Zollanvari, Amin and Dougherty, Edward R},
  booktitle={2013 IEEE Global Conference on Signal and Information Processing},
  year={2013}
}

@article{zollanvari2015generalized,
  title={Generalized Consistent Error Estimator of Linear Discriminant Analysis},
  author={Zollanvari, Amin and Dougherty, Edward R},
  journal={IEEE Trans. Signal Process.},
  volume={63},
  number={11},
  year={2015}
}

@article{zollanvari2011analytic,
  title={Analytic study of performance of error estimators for linear discriminant analysis},
  author={Zollanvari, Amin and Braga-Neto, Ulisses M and Dougherty, Edward R},
  journal={IEEE Trans. Signal Process.},
  volume={59},
  number={9},
  pages={4238--4255},
  year={2011},
  publisher={IEEE}
}

@article{li2015two,
  title={On two simple and effective procedures for high dimensional classification of general populations},
  author={Li, Zhaoyuan and Yao, Jianfeng},
  journal={Statist. Papers},
  pages={1--25},
  year={2015},
  publisher={Springer}
}

@article{raudys1972amount,
  title={On the amount of a priori information in designing the classification algorithm},
  author={Raudys, {\v{S}}ar{\=u}nas},
  journal={Technical Cybernetics (in Russian)},
  pages={168--174},
  year={1972},
  volume={4},
}

@article{fujikoshi1985selection,
  title={Selection of variables in discriminant analysis and canonical correlation analysis},
  author={Fujikoshi, Yasunori},
  journal={Multivariate Analysis-VI},
  pages={219--236},
  year={1985},
  publisher={North-Holland Amsterdam}
}

@article{kubokawa2013asymptotic,
  title={Asymptotic expansion and estimation of {EPMC} for linear classification rules in high dimension},
  author={Kubokawa, Tatsuya and Hyodo, Masashi and Srivastava, Muni S},
  journal={J. Multivariate Anal.},
  volume={115},
  pages={496--515},
  year={2013},
  publisher={Elsevier}
}

@article{fujikoshi1998asymptotic,
  title={Asymptotic aproximations for {EPMC}s of the linear and the quadratic discriminant functions when the sample sizes and the dimension are large},
  author={Fujikoshi, Yasunori and Seo, Takashi},
  journal={Random Oper. Stoch. Equ.},
  volume={6},
  number={3},
  pages={269--280},
  year={1998}
}

@book{fujikoshi2011multivariate,
  title={Multivariate Statistics: High-dimensional and Large-sample Approximations},
  author={Fujikoshi, Yasunori and Ulyanov, Vladimir V and Shimizu, Ryoichi},
  year={2011},
  publisher={John Wiley \& Sons}
}


@article{meshalkin1979errors,
  title={Errors in the classification of multi-variate observations},
  author={Meshalkin, LD and Serdobolskii, VI},
  journal={Theory of Probability \& Its Applications},
  volume={23},
  number={4},
  pages={741--750},
  year={1979},
  publisher={SIAM}
}


@inproceedings{serdobolskii1980discriminant,
  title={Discriminant analysis for a large number of variables},
  author={Serdobolskii, Vadim Ivanovich},
  booktitle={Dokl. Akad. Nauk SSSR},
  volume={22},
  pages={314--319},
  year={1980}
}
 
@inproceedings{serdobolskii1983minimum,
  title={On minimum error probability in discriminant analysis},
  author={Serdobolskii, Vadim Ivanovich},
  booktitle={Dokl. Akad. Nauk SSSR},
  volume={27},
  pages={720--725},
  year={1983}
}

@inproceedings{deev1970representation,
  title={Representation of statistics of discriminant analysis and asymptotic expansion when space dimensions are comparable with sample size},
  author={Deev, AD},
  booktitle={Sov. Math. Dokl.},
  volume={11},
  pages={1547--1550},
  year={1970}
}

@article{raudys2004results,
  title={Results in statistical discriminant analysis: A review of the former {S}oviet {U}nion literature},
  author={Raudys, {\v{S}}ar{\=u}nas and Young, Dean M},
  journal={J. Multivariate Anal.},
  volume={89},
  number={1},
  pages={1--35},
  year={2004},
  publisher={Elsevier}
}

@book{serdobolskii2007multiparametric,
  title={Multiparametric {S}tatistics},
  author={Serdobolskii, Vadim Ivanovich},
  year={2007},
  publisher={Elsevier}
}

@article{dobriban2015efficient,
author = {Dobriban, Edgar},
title = {Efficient computation of limit spectra of sample covariance matrices},
journal = {Random Matrices: Theory and Applications},
volume = {04},
number = {04},
pages = {1550019},
year = {2015},
} 

@article{silverstein1992signal,
  Author = {Silverstein, Jack W and Combettes, Patrick L},
  Journal = {IEEE Trans. Signal Process.},
  Number = {8},
  Pages = {2100--2105},
  Publisher = {IEEE},
  Title = {Signal detection via spectral theory of large dimensional random matrices},
  Volume = {40},
  Year = {1992}}

@inproceedings{pang2002thumbs,
  Author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
  Booktitle = {EMNLP},
  Organization = {Association for Computational Linguistics},
  Title = {Thumbs up?: Sentiment classification using machine learning techniques},
  Year = {2002}}

@book{joachims2002learning,
  Author = {Joachims, Thorsten},
  Publisher = {Kluwer Academic Publishers},
  Title = {Learning to classify text using support vector machines: Methods, theory and algorithms},
  Year = {2002}}

@article{goldstein2009common,
  Author = {Goldstein, David B},
  Journal = {New England Journal of Medicine},
  Number = {17},
  Pages = {1696},
  Title = {Common genetic variation and human traits},
  Volume = {360},
  Year = {2009}}

@article{10002012integrated,
  Author = {{1000 Genomes Consortium}},
  Journal = {Nature},
  Number = {7422},
  Pages = {56--65},
  Publisher = {Nature Publishing Group},
  Title = {An integrated map of genetic variation from 1,092 human genomes},
  Volume = {491},
  Year = {2012}}

@article{tulino2004random,
  Author = {Tulino, Antonio M and Verd{\'u}, Sergio},
  Journal = {Communications and Information theory},
  Number = {1},
  Pages = {1--182},
  Publisher = {Now Publishers Inc.},
  Title = {Random matrix theory and wireless communications},
  Volume = {1},
  Year = {2004}}

@book{couillet2011random,
  Author = {Couillet, Romain and Debbah, Merouane},
  Publisher = {Cambridge University Press},
  location={ Cambridge, MA},
  Title = {Random {M}atrix {M}ethods for {W}ireless {C}ommunications},
  Year = {2011}}

@article{pickrell2012inference,
  Author = {Pickrell, Joseph K and Pritchard, Jonathan K},
  Journal = {PLoS genetics},
  Number = {11},
  Pages = {e1002967},
  Publisher = {Public Library of Science},
  Title = {Inference of population splits and mixtures from genome-wide allele frequency data},
  Volume = {8},
  Year = {2012}}

@incollection{NIPS2013_4921,
  Author = {Bartz, Daniel and M\"{u}ller, Klaus-Robert},
  Booktitle = {NIPS},
  Title = {Generalizing Analytic Shrinkage for Arbitrary Covariance Structures},
  Year = {2013}}

@article{Ledoit2015,
  Author = {Olivier Ledoit and Michael Wolf},
  Journal = {J. Multivariate Anal.},
  Title = {Spectrum estimation: A unified framework for covariance matrix estimation and {PCA} in large dimensions},
  Pages = {360--384},
  Volume = {139},
  Year = {2015}}

@article{tibshirani2002diagnosis,
  Author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan, Balasubramanian and Chu, Gilbert},
  Journal = {Proc. Natl. Acad. Sci. USA},
  Number = {10},
  Pages = {6567--6572},
  Publisher = {National Acad Sciences},
  Title = {Diagnosis of multiple cancer types by shrunken centroids of gene expression},
  Volume = {99},
  Year = {2002}}

@article{horvath2014obesity,
  Author = {Horvath, Steve and Erhart, Wiebke and Brosch, Mario and Ammerpohl, Ole and von Schönfels, Witigo and Ahrens, Markus and Heits, Nils and Bell, Jordana T. and Tsai, Pei-Chien and Spector, Tim D. and Deloukas, Panos and Siebert, Reiner and Sipos, Bence and Becker, Thomas and Röcken, Christoph and Schafmayer, Clemens and Hampe, Jochen},
  Journal = {Proc. Natl. Acad. Sci. USA},
  Number = {43},
  Pages = {15538--15543},
  Publisher = {National Acad Sciences},
  Title = {Obesity accelerates epigenetic aging of human liver},
  Volume = {111},
  Year = {2014}}

@article{horvath2013dna,
  Author = {Horvath, Steve},
  Journal = {Genome biology},
  Number = {10},
  Pages = {R115},
  Publisher = {BioMed Central Ltd},
  Title = {{DNA} methylation age of human tissues and cell types},
  Volume = {14},
  Year = {2013}}

@article{Golub15101999,
  Author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S},
  Doi = {10.1126/science.286.5439.531},
  Journal = {Science},
  Number = {5439},
  Pages = {531-537},
  Title = {Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring},
  Volume = {286},
  Year = {1999}}

@article{trippa2015bayesian,
  Author = {Trippa, Lorenzo and Waldron, Levi and Huttenhower, Curtis and Parmigiani, Giovanni},
  Date-Modified = {2015-05-28 22:18:53 +0000},
  Journal = {The Annals of Applied Statistics},
  Number = {1},
  Pages = {402--428},
  Publisher = {Institute of Mathematical Statistics},
  Title = {Bayesian nonparametric cross-study validation of prediction methods},
  Volume = {9},
  Year = {2015}}

@article{Bernau15062014,
  Abstract = {Motivation: Numerous competing algorithms for prediction in high-dimensional settings have been developed in the statistical and machine-learning literature. Learning algorithms and the prediction models they generate are typically evaluated on the basis of cross-validation error estimates in a few exemplary datasets. However, in most applications, the ultimate goal of prediction modeling is to provide accurate predictions for independent samples obtained in different settings. Cross-validation within exemplary datasets may not adequately reflect performance in the broader application context.Methods: We develop and implement a systematic approach to `cross-study validation', to replace or supplement conventional cross-validation when evaluating high-dimensional prediction models in independent datasets. We illustrate it via simulations and in a collection of eight estrogen-receptor positive breast cancer microarray gene-expression datasets, where the objective is predicting distant metastasis-free survival (DMFS). We computed the C-index for all pairwise combinations of training and validation datasets. We evaluate several alternatives for summarizing the pairwise validation statistics, and compare these to conventional cross-validation.Results: Our data-driven simulations and our application to survival prediction with eight breast cancer microarray datasets, suggest that standard cross-validation produces inflated discrimination accuracy for all algorithms considered, when compared to cross-study validation. Furthermore, the ranking of learning algorithms differs, suggesting that algorithms performing best in cross-validation may be suboptimal when evaluated through independent validation.Availability: The survHD: Survival in High Dimensions package (http://www.bitbucket.org/lwaldron/survhd) will be made available through Bioconductor.Contact: levi.waldron@hunter.cuny.eduSupplementary information: Supplementary data are available at Bioinformatics online.},
  Author = {Bernau, Christoph and Riester, Markus and Boulesteix, Anne-Laure and Parmigiani, Giovanni and Huttenhower, Curtis and Waldron, Levi and Trippa, Lorenzo},
  Doi = {10.1093/bioinformatics/btu279},
  Eprint = {http://bioinformatics.oxfordjournals.org/content/30/12/i105.full.pdf+html},
  Journal = {Bioinformatics},
  Number = {12},
  Pages = {i105-i112},
  Title = {Cross-study validation for the assessment of prediction algorithms},
  Url = {http://bioinformatics.oxfordjournals.org/content/30/12/i105.abstract},
  Volume = {30},
  Year = {2014},
  Bdsk-Url-1 = {http://bioinformatics.oxfordjournals.org/content/30/12/i105.abstract},
  Bdsk-Url-2 = {http://dx.doi.org/10.1093/bioinformatics/btu279}}

@book{lynch1998genetics,
  Author = {Lynch, Michael and Walsh, Bruce and others},
  Publisher = {Sinauer Sunderland},
  Title = {Genetics and analysis of quantitative traits},
  Volume = {1},
  Year = {1998}}

@article{bayati2012lasso,
  Author = {Bayati, Mohsen and Montanari, Andrea},
  Journal = {IEEE Trans. Inform. Theory},
  Number = {4},
  Pages = {1997--2017},
  Publisher = {IEEE},
  Title = {The {LASSO} risk for {G}aussian matrices},
  Volume = {58},
  Year = {2012}}

@article{bartlett2003rademacher,
  Author = {Bartlett, Peter L and Mendelson, Shahar},
  Journal = {J. Mach. Learn. Res.},
  Pages = {463--482},
  Publisher = {JMLR. org},
  Title = {Rademacher and {G}aussian complexities: Risk bounds and structural results},
  Volume = {3},
  Year = {2003}}

@article{hsu2014random,
  Author = {Hsu, Daniel and Kakade, Sham M and Zhang, Tong},
  Journal = {Found. Comput. Math.},
  Number = {3},
  Pages = {569--600},
  Publisher = {Springer},
  Title = {Random design analysis of ridge regression},
  Volume = {14},
  Year = {2014}}

@article{dicker2014ridge,
  Author = {Dicker, Lee},
  Journal = {Bernoulli, to appear},
  Title = {Ridge regression and asymptotic minimax estimation over spheres of growing dimension},
  Year = {2014}}

@article{dicker2013optimal,
  Author = {Dicker, Lee},
  Journal = {Electron. J. Stat.},
  Pages = {1806--1834},
  Publisher = {Institute of Mathematical Statistics},
  Title = {Optimal equivariant prediction for high-dimensional linear models with arbitrary predictor covariance},
  Volume = {7},
  Year = {2013}}

@article{karoui2013asymptotic,
  Author = {El Karoui, Noureddine},
  Journal = {arXiv preprint arXiv:1311.2445},
  Title = {Asymptotic behavior of unregularized and ridge-regularized high-dimensional robust regression estimators: rigorous results},
  Year = {2013}}

@article{karoui2011geometric,
  Author = {El Karoui, Noureddine and K{\"o}sters, Holger},
  Journal = {arXiv preprint arXiv:1105.1404},
  Title = {Geometric sensitivity of random matrix results: consequences for shrinkage estimators of covariance and related statistical methods},
  Year = {2011}}

@article{bean2013optimal,
  Author = {Bean, Derek and Bickel, Peter J and El Karoui, Noureddine and Yu, Bin},
  Journal = {Proc. Natl. Acad. Sci. USA},
  Number = {36},
  Pages = {14563--14568},
  Publisher = {National Acad Sciences},
  Title = {Optimal {M}-estimation in high-dimensional regression},
  Volume = {110},
  Year = {2013}}

@article{el2013robust,
  Author = {El Karoui, Noureddine and Bean, Derek and Bickel, Peter J and Lim, Chinghway and Yu, Bin},
  Journal = {Proc. Natl. Acad. Sci. USA},
  Number = {36},
  Pages = {14557--14562},
  Publisher = {National Acad Sciences},
  Title = {On robust regression with high-dimensional predictors},
  Volume = {110},
  Year = {2013}}

@article{couillet2011deterministic,
  Author = {Couillet, Romain and Debbah, M{\'e}rouane and Silverstein, Jack W},
  Journal = {IEEE Trans. Inform. Theory},
  Number = {6},
  Pages = {3493--3514},
  Publisher = {IEEE},
  Title = {A deterministic equivalent for the analysis of correlated MIMO multiple access channels},
  Volume = {57},
  Year = {2011}}

@book{bai2009spectral,
  Author = {Bai, Zhidong and Silverstein, Jack W},
  Publisher = {Springer},
  Series = {Springer Series in Statistics},
  Title = {Spectral analysis of large dimensional random matrices},
  Year = {2009}}

@article{chen2011regularized,
  Author = {Chen, Lin S and Paul, Debashis and Prentice, Ross L and Wang, Pei},
  Journal = {J. Amer. Statist. Assoc.},
  Number = {496},
  Title = {A regularized {Hotelling's} {$T^2$} test for pathway analysis in proteomic studies},
  Volume = {106},
  Year = {2011}}

@inproceedings{sabato2010tight,
  Author = {Sabato, Sivan and Srebro, Nathan and Tishby, Naftali},
  Booktitle = {NIPS},
  Title = {Tight sample complexity of large-margin learning},
  Year = {2010}}

@unpublished{liang2015statistical,
  Author = {Percy Liang},
  Title = {Statistical Learning Theory},
  Url = {https://web.stanford.edu/class/cs229t/notes.pdf},
  Year = {2015},
  Bdsk-Url-1 = {https://web.stanford.edu/class/cs229t/notes.pdf}}

@article{koltchinskii2006local,
  Author = {Koltchinskii, Vladimir and others},
  Journal = {Ann. Statist.},
  Number = {6},
  Pages = {2593--2656},
  Publisher = {Institute of Mathematical Statistics},
  Title = {Local Rademacher complexities and oracle inequalities in risk minimization},
  Volume = {34},
  Year = {2006}}

@article{bartlett2005local,
  Author = {Bartlett, Peter L and Bousquet, Olivier and Mendelson, Shahar},
  Journal = {Ann. Statist.},
  Pages = {1497--1537},
  Publisher = {JSTOR},
  Title = {Local rademacher complexities},
  Year = {2005}}

@article{hastie1995penalized,
  Author = {Hastie, Trevor and Buja, Andreas and Tibshirani, Robert},
  Journal = {Ann. Statist.},
  Number = {1},
  Pages = {73--102},
  Publisher = {JSTOR},
  Title = {Penalized discriminant analysis},
  Volume = {23},
  Year = {1995}}

@article{cai2011direct,
  Author = {Cai, Tony and Liu, Weidong},
  Journal = {J. Amer. Statist. Assoc.},
  Number = {496},
  Title = {A direct estimation approach to sparse linear discriminant analysis},
  Volume = {106},
  Year = {2011}}

@article{silverstein1995strong,
  Author = {Silverstein, Jack W},
  Journal = {J. Multivariate Anal.},
  Number = {2},
  Pages = {331--339},
  Publisher = {Elsevier},
  Title = {Strong convergence of the empirical distribution of eigenvalues of large dimensional random matrices},
  Volume = {55},
  Year = {1995}}

@book{brockwell2009time,
  Author = {Brockwell, Peter J and Davis, Richard A},
  Publisher = {Springer},
  Title = {Time series: theory and methods},
  Year = {2009}}

@misc{grenander1984toeplitz,
  Author = {Grenander, Ulf and Szeg\H{o}, G{\'a}bor},
  Publisher = {Chelsea Pub. Co.(New York)},
  Title = {Toeplitz forms and their applications},
  Year = {1984}}

@article{silverstein1995analysis,
  Author = {Silverstein, Jack W and Choi, Sang-Il},
  Journal = {J. Multivariate Anal.},
  Number = {2},
  Pages = {295--309},
  Publisher = {Elsevier},
  Title = {Analysis of the limiting spectral distribution of large dimensional random matrices},
  Volume = {54},
  Year = {1995}}

@article{marchenko1967distribution,
  Author = {Marchenko, Vladimir A and Pastur, Leonid A},
  Journal = {Mat. Sb.},
  Number = {4},
  Pages = {507--536},
  Publisher = {Russian Academy of Sciences, Branch of Mathematical Sciences},
  Title = {Distribution of eigenvalues for some sets of random matrices},
  Volume = {114},
  Year = {1967}}

@incollection{fan2011high,
  Address = {New Jersey},
  Author = {Fan, Jianqing and Fan, Yingying and Wu, Yichao},
  Booktitle = {High-dimensional Data Analysis},
  Date-Added = {2015-04-28 23:05:13 +0000},
  Date-Modified = {2015-04-28 23:07:40 +0000},
  Editor = {T. Cai and X. Shen},
  Pages = {3--37},
  Publisher = {World Scientific},
  Title = {High dimensional classification},
  Year = {2011}}

@article{friedman1989regularized,
  Author = {Friedman, Jerome H},
  Date-Added = {2015-04-28 23:01:16 +0000},
  Date-Modified = {2015-04-28 23:01:25 +0000},
  Journal = {Journal of the American Statistical Association},
  Number = {405},
  Pages = {165--175},
  Publisher = {Taylor \& Francis},
  Title = {Regularized discriminant analysis},
  Volume = {84},
  Year = {1989}}

@article{guo2007regularized,
  Author = {Guo, Yaqian and Hastie, Trevor and Tibshirani, Robert},
  Date-Added = {2015-04-28 22:55:06 +0000},
  Date-Modified = {2015-04-28 22:55:06 +0000},
  Journal = {Biostatistics},
  Number = {1},
  Pages = {86--100},
  Publisher = {Biometrika Trust},
  Title = {Regularized linear discriminant analysis and its application in microarrays},
  Volume = {8},
  Year = {2007}}

@article{saranadasa1993asymptotic,
  Author = {Saranadasa, Hewa},
  Date-Added = {2015-04-28 22:54:23 +0000},
  Date-Modified = {2015-04-28 22:54:37 +0000},
  Journal = {J. Multivariate Anal.},
  Number = {1},
  Pages = {154--174},
  Publisher = {Elsevier},
  Title = {Asymptotic expansion of the misclassification probabilities of {D}-and {A}-criteria for discrimination from two high dimensional populations using the theory of large dimensional random matrices},
  Volume = {46},
  Year = {1993}}

@article{silverstein1995empirical,
  Author = {Silverstein, Jack W and Bai, ZD},
  Date-Added = {2015-04-28 22:53:21 +0000},
  Date-Modified = {2015-04-28 22:53:21 +0000},
  Journal = {J. Multivariate Anal.},
  Number = {2},
  Pages = {175--192},
  Publisher = {Elsevier},
  Title = {On the empirical distribution of eigenvalues of a class of large dimensional random matrices},
  Volume = {54},
  Year = {1995}}

@article{ledoit2011eigenvectors,
  Author = {Ledoit, Olivier and P{\'e}ch{\'e}, Sandrine},
  Date-Added = {2015-04-28 22:52:43 +0000},
  Date-Modified = {2015-04-28 22:52:43 +0000},
  Journal = {Probab. Theory Related Fields},
  Number = {1-2},
  Pages = {233--264},
  Publisher = {Springer},
  Title = {Eigenvectors of some large sample covariance matrix ensembles},
  Volume = {151},
  Year = {2011}}

@book{yao2015large,
  Author = {Yao, Jianfeng and Bai, Zhidong and Zheng, Shurong},
  Date-Added = {2015-04-28 22:52:15 +0000},
  Date-Modified = {2015-04-28 22:52:15 +0000},
  Publisher = {Cambridge University Press},
  Title = {Large Sample Covariance Matrices and High-Dimensional Data Analysis},
  Year = {2015}}


@article{fan2012road,
  Author = {Fan, Jianqing and Feng, Yang and Tong, Xin},
  Date-Added = {2015-04-28 22:50:01 +0000},
  Date-Modified = {2015-04-28 22:50:01 +0000},
  Journal = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  Number = {4},
  Pages = {745--771},
  Publisher = {Wiley Online Library},
  Title = {A road to classification in high dimensional space: the regularized optimal affine discriminant},
  Volume = {74},
  Year = {2012}}

@article{fan2008high,
  Author = {Fan, Jianqing and Fan, Yingying},
  Date-Added = {2015-04-28 22:46:10 +0000},
  Date-Modified = {2015-04-28 22:48:10 +0000},
  Journal = {Ann. Statist.},
  Number = {6},
  Title = {High dimensional classification using features annealed independence rules},
  Volume = {36},
  Pages = {2605--2637},
  Year = {2008}}


@article{efron1975efficiency,
  Author = {Efron, Bradley},
  Date-Added = {2015-04-28 22:32:57 +0000},
  Date-Modified = {2015-04-28 22:32:57 +0000},
  Journal = {J. Amer. Statist. Assoc.},
  Number = {352},
  Pages = {892--898},
  Publisher = {Taylor \& Francis Group},
  Title = {The efficiency of logistic regression compared to normal discriminant analysis},
  Volume = {70},
  Year = {1975}}

@article{donoho2015variance,
  Author = {Donoho, David L and Montanari, Andrea},
  Journal = {arXiv preprint arXiv:1503.02106},
  Title = {Variance Breakdown of {Huber} ({M})-estimators: $ n/p \rightarrow m \in (1, \, \infty) $},
  Year = {2015}}

@article{bickel2004some,
  Author = {Bickel, Peter J and Levina, Elizaveta},
  Date-Added = {2015-04-28 00:58:15 +0000},
  Date-Modified = {2015-04-28 00:59:24 +0000},
  Journal = {Bernoulli},
  Pages = {989--1010},
  Publisher = {JSTOR},
  Title = {Some theory for {F}isher's linear discriminant function, ``naive {B}ayes'', and some alternatives when there are many more variables than observations},
  Year = {2004}}

@inproceedings{ng2001discriminative,
  Author = {Ng, Andrew and Jordan, Michael},
  Booktitle = {NIPS},
  Date-Added = {2015-04-28 00:57:42 +0000},
  Date-Modified = {2015-04-28 00:57:42 +0000},
  Title = {On discriminative vs. generative classifiers: A comparison of logistic regression and naive {B}ayes},
  Year = {2001}}

@article{chen1998atomic,
  Author = {Chen, Scott Shaobing and Donoho, David L and Saunders, Michael A},
  Date-Added = {2015-04-28 00:28:16 +0000},
  Date-Modified = {2015-04-28 00:28:16 +0000},
  Journal = {SIAM J. Sci. Comput.},
  Number = {1},
  Pages = {33--61},
  Publisher = {SIAM},
  Title = {Atomic decomposition by basis pursuit},
  Volume = {20},
  Year = {1998}}

@article{donoho2006compressed,
  Author = {Donoho, David L},
  Date-Added = {2015-04-28 00:27:30 +0000},
  Date-Modified = {2015-04-28 00:27:30 +0000},
  Journal = {IEEE Trans. Inform. Theory},
  Number = {4},
  Pages = {1289--1306},
  Publisher = {IEEE},
  Title = {Compressed sensing},
  Volume = {52},
  Year = {2006}}

@article{candes2007dantzig,
  Author = {Cand\`es, Emmanuel and Tao, Terence},
  Date-Added = {2015-04-28 00:24:32 +0000},
  Date-Modified = {2015-04-28 00:28:57 +0000},
  Journal = {Ann. Statist.},
  Number = {6},
  Pages = {2313--2351},
  Publisher = {JSTOR},
  Title = {The {D}antzig selector: Statistical estimation when $p$ is much larger than $n$},
  Volume = {35},
  Year = {2007}}

@article{tibshirani1996regression,
  Author = {Tibshirani, Robert},
  Date-Added = {2015-04-28 00:24:05 +0000},
  Date-Modified = {2015-04-28 00:24:05 +0000},
  Journal = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  Number = {1},
  Pages = {267--288},
  Publisher = {JSTOR},
  Title = {Regression shrinkage and selection via the lasso},
  Volume = {58},
  Year = {1996}}

@article{vapnik1971uniform,
  Author = {Vapnik, Vladimir N and Chervonenkis, A Ya},
  Date-Added = {2015-04-27 20:58:36 +0000},
  Date-Modified = {2015-04-27 20:58:36 +0000},
  Journal = {Theory Probab. Appl.},
  Number = {2},
  Pages = {264--280},
  Publisher = {SIAM},
  Title = {On the uniform convergence of relative frequencies of events to their probabilities},
  Volume = {16},
  Year = {1971}}

@article{valiant1984theory,
  Author = {Valiant, Leslie G},
  Journal = {Communications of the ACM},
  Number = {11},
  Pages = {1134--1142},
  Publisher = {ACM},
  Title = {A theory of the learnable},
  Volume = {27},
  Year = {1984}}


@article{negahban2012unified,
  title={A unified framework for high-dimensional analysis of $ M $-estimators with decomposable regularizers},
  author={Negahban, Sahand N and Ravikumar, Pradeep and Wainwright, Martin J and Yu, Bin and others},
  journal={Statistical Science},
  volume={27},
  number={4},
  pages={538--557},
  year={2012},
  publisher={Institute of Mathematical Statistics}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@article{kuchibhotla2018deterministic,
  title={Deterministic inequalities for smooth m-estimators},
  author={Kuchibhotla, Arun Kumar},
  journal={arXiv preprint arXiv:1809.05172},
  year={2018}
}

@book{villani2003topics,
  title={Topics in optimal transportation},
  author={Villani, C{\'e}dric},
  number={58},
  year={2003},
  publisher={American Mathematical Soc.}
}

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}
@inproceedings{kingma2015adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015}}
@article{zhong2017random,
  title={Random erasing data augmentation},
  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  journal={arXiv preprint arXiv:1708.04896},
  year={2017}
}

@article{bae2018perlin,
  title={A Perlin noise-based augmentation strategy for deep learning with small data samples of HRCT images},
  author={Bae, Hyun-Jin and Kim, Chang-Wook and Kim, Namju and Park, BeomHee and Kim, Namkug and Seo, Joon Beom and Lee, Sang Min},
  journal={Scientific reports},
  volume={8},
  number={1},
  pages={17687},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{perlin1985image,
  title={An image synthesizer},
  author={Perlin, Ken},
  journal={ACM Siggraph Computer Graphics},
  volume={19},
  number={3},
  pages={287--296},
  year={1985}
}

@article{lopes2019improving,
  title={Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation},
  author={Lopes, Raphael Gontijo and Yin, Dong and Poole, Ben and Gilmer, Justin and Cubuk, Ekin D},
  journal={arXiv preprint arXiv:1906.02611},
  year={2019}
}

@article{zhang2017mixup,
  title={Mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{sun2018training,
  title={Training augmentation with adversarial examples for robust speech recognition},
  author={Sun, Sining and Yeh, Ching-Feng and Ostendorf, Mari and Hwang, Mei-Yuh and Xie, Lei},
  journal={arXiv preprint arXiv:1806.02782},
  year={2018}
}

@inproceedings{sajjadi2016regularization,
  title={Regularization with stochastic transformations and perturbations for deep semi-supervised learning},
  author={Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1163--1171},
  year={2016}
}
@inproceedings{manzil2017deepsets,
 author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {3391--3401},
 title = {Deep Sets},
 year = {2017}
}
@article{haan2020natural,
title={Natural Graph Networks},
author={Haan, P. and Cohen, T. and Welling M.},
journal={arXiv preprint arXiv:2007.08349},
year={2020}}

@article{cohen2019certified,
  title={Certified adversarial robustness via randomized smoothing},
  author={Cohen, Jeremy M and Rosenfeld, Elan and Kolter, J Zico},
  journal={arXiv preprint arXiv:1902.02918},
  year={2019}
}

@article{blackwell1947conditional,
  title={Conditional expectation and unbiased sequential estimation},
  author={Blackwell, David},
  journal={The Annals of Mathematical Statistics},
  pages={105--110},
  year={1947},
  publisher={JSTOR}
}
 @InProceedings{ioffe2015batchnorm, 
 title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
 author = {Sergey Ioffe and Christian Szegedy}, 
 booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, 
 pages = {448--456}, 
 year = {2015}, 
 editor = {Francis Bach and David Blei}, 
 volume = {37}, 
 series = {Proceedings of Machine Learning Research} } 
@article{srivatsava2014dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958}
}
@incollection{rao1992information,
  title={Information and the accuracy attainable in the estimation of statistical parameters},
  author={Rao, C Radhakrishna},
  booktitle={Breakthroughs in statistics},
  pages={235--247},
  year={1992},
  publisher={Springer}
}

@article{cubuk2019randaugment,
  title={RandAugment: Practical data augmentation with no separate search},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  journal={arXiv preprint arXiv:1909.13719},
  year={2019}
}

@article{hoffer2019augment,
  title={Augment your batch: better training with larger batches},
  author={Hoffer, Elad and Ben-Nun, Tal and Hubara, Itay and Giladi, Niv and Hoefler, Torsten and Soudry, Daniel},
  journal={arXiv preprint arXiv:1901.09335},
  year={2019}
}

@inproceedings{krizhevsky2009learningml,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8139--8148},
  year={2019}
}

@article{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1909.12292},
  year={2019}
}

@article{chen2019much,
  title={How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  author={Chen, Zixiang and Cao, Yuan and Zou, Difan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1911.12360},
  year={2019}
}


@book{tao_2012, 
  place={Providence, R.I}, 
  title={Topics in random matrix theory},
  publisher={American Mathematical Society}, 
  author={Tao, Terence}, 
  year={2012}
}
@article{lecun1998MNIST,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}
@inproceedings{zhou2021metalearning,
title={Meta-learning Symmetries by Reparameterization},
author={Allan Zhou and Tom Knowles and Chelsea Finn},
booktitle={International Conference on Learning Representations},
year={2021}
}

@inproceedings{wilk2018learning,
 author = {van der Wilk, Mark and Bauer, Matthias and John, ST and Hensman, James},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {9938--9948},
 title = {Learning Invariances using the Marginal Likelihood},
 volume = {31},
 year = {2018}
}


@inproceedings{maaten2013learning,
  title={Learning with marginalized corrupted features},
  author={Maaten, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian},
  booktitle={International Conference on Machine Learning},
  pages={410--418},
  year={2013}
}

@article{chen2020group,
  title={A group-theoretic framework for data augmentation},
  author={Chen, Shuxiao and Dobriban, Edgar and Lee, Jane},
  journal={Advances in Neural Information Processing Systems, JMLR},
  volume={33},
  year={2020}
}
@inproceedings{benton2020learning,
 author = {Benton, Gregory and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {17605--17616},
 title = {Learning Invariances in Neural Networks from Training Data},
 volume = {33},
 year = {2020}
}


@inproceedings{maron2018invariant,
  author    = {Haggai Maron and
               Heli Ben{-}Hamu and
               Nadav Shamir and
               Yaron Lipman},
  title     = {Invariant and Equivariant Graph Networks},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  year      = {2019}
}

@InProceedings{pmlr-v97-cohen19d, 
title = {Gauge Equivariant Convolutional Networks and the Icosahedral {CNN}}, 
author = {Cohen, Taco and Weiler, Maurice and Kicanaoglu, Berkay and Welling, Max}, 
booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {1321--1330}, 
year = {2019}, 
editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}
}

@misc{robey2020modelbased,
      title={Model-Based Robust Deep Learning: Generalizing to Natural, Out-of-Distribution Data}, 
      author={Alexander Robey and Hamed Hassani and George J. Pappas},
      year={2020},
      eprint={2005.10247},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{maron2020learning,
  title={On Learning Sets of Symmetric Elements},
  author={Maron, Haggai and Litany, Or and Chechik, Gal and Fetaya, Ethan},
  journal={ICML 2020, arXiv preprint arXiv:2002.08599},
  year={2020}
}

@inproceedings{zhou2019continuity,
  title={On the continuity of rotation representations in neural networks},
  author={Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5745--5753},
  year={2019}
}

@article{chirikjian2001engineering,
  title={Engineering applications of noncommutative harmonic analysis: with emphasis on rotation and motion groups},
  author={Chirikjian, Gregory S and Kyatkin, Alexander B and Buckingham, AC},
  journal={Appl. Mech. Rev.},
  volume={54},
  number={6},
  pages={B97--B98},
  year={2001}
}

@article{peretroukhin2020smooth,
  title={A Smooth Representation of Belief over SO (3) for Deep Rotation Learning with Uncertainty},
  author={Peretroukhin, Valentin and Giamou, Matthew and Rosen, David M and Greene, W Nicholas and Roy, Nicholas and Kelly, Jonathan},
  journal={arXiv preprint arXiv:2006.01031},
  year={2020}
}

@inproceedings{kingma2014auto,
  author = {Kingma, Diederik P. and Welling, Max},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014},
  title = {{Auto-Encoding Variational Bayes}},
  year = 2014
}

@article{falorsi2018explorations,
  title={Explorations in homeomorphic variational auto-encoding},
  author={Falorsi, Luca and de Haan, Pim and Davidson, Tim R and De Cao, Nicola and Weiler, Maurice and Forr{\'e}, Patrick and Cohen, Taco S},
  journal={arXiv preprint arXiv:1807.04689},
  year={2018}
}

@article{dinh2016density,
  title={Density estimation using real nvp},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={arXiv preprint arXiv:1605.08803},
  year={2016}
}

@article{tai2019equivariant,
  title={Equivariant transformer networks},
  author={Tai, Kai Sheng and Bailis, Peter and Valiant, Gregory},
  journal={arXiv preprint arXiv:1901.11399},
  year={2019}
}

@article{rezende2019equivariant,
  title={Equivariant hamiltonian flows},
  author={Rezende, Danilo Jimenez and Racani{\`e}re, S{\'e}bastien and Higgins, Irina and Toth, Peter},
  journal={arXiv preprint arXiv:1909.13739},
  year={2019}
}

@article{zhou2020meta,
  title={Meta-Learning Symmetries by Reparameterization},
  author={Zhou, Allan and Knowles, Tom and Finn, Chelsea},
  journal={arXiv preprint arXiv:2007.02933},
  year={2020}
}

@article{chen2019data,
  title={Data Augmentation, Invariance, and Deep Learning},
  author={Chen, Shuxiao and Dobriban, Edgar and Lee, Jane},
  journal={in preparation},
  year={2019}
}


@article{esteves2018cross,
  title={Cross-Domain 3D Equivariant Image Embeddings},
  author={Esteves, Carlos and Sud, Avneesh and Luo, Zhengyi and Daniilidis, Kostas and Makadia, Ameesh},
  journal={arXiv preprint arXiv:1812.02716},
  year={2018}
}

@article{esteves2019equivariant,
  title={Equivariant Multi-View Networks},
  author={Esteves, Carlos and Xu, Yinshuang and Allen-Blanchette, Christine and Daniilidis, Kostas},
  journal={arXiv preprint arXiv:1904.00993},
  year={2019}
}

@book{robert2013monte,
  title={Monte Carlo statistical methods},
  author={Robert, Christian and Casella, George},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{riesenhuber2000models,
  title={Models of object recognition},
  author={Riesenhuber, Maximilian and Poggio, Tomaso},
  journal={Nature neuroscience},
  volume={3},
  number={11s},
  pages={1199},
  year={2000},
  publisher={Nature Publishing Group}
}

@article{lin2017does,
  title={Why does deep and cheap learning work so well?},
  author={Lin, Henry W and Tegmark, Max and Rolnick, David},
  journal={Journal of Statistical Physics},
  volume={168},
  number={6},
  pages={1223--1247},
  year={2017},
  publisher={Springer}
}

@article{wu2007robust,
  title={Robust truncated hinge loss support vector machines},
  author={Wu, Yichao and Liu, Yufeng},
  journal={Journal of the American Statistical Association},
  volume={102},
  number={479},
  pages={974--983},
  year={2007},
  publisher={Taylor \& Francis}
}  

@inproceedings{nguyen2013algorithms,
  title={Algorithms for direct 0--1 loss optimization in binary classification},
  author={Nguyen, Tan and Sanner, Scott},
  booktitle={International Conference on Machine Learning},
  pages={1085--1093},
  year={2013}
}

@article{tanner1987calculation,
  title={The calculation of posterior distributions by data augmentation},
  author={Tanner, Martin A and Wong, Wing Hung},
  journal={Journal of the American statistical Association},
  volume={82},
  number={398},
  pages={528--540},
  year={1987},
  publisher={Taylor \& Francis}
}


@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}




@article{cirecsan2010deep,
  title={Deep, big, simple neural nets for handwritten digit recognition},
  author={Cire{\c{s}}an, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={22},
  number={12},
  pages={3207--3220},
  year={2010},
  publisher={MIT Press}
}



@article{cohen2018general,
  title={A General Theory of Equivariant CNNs on Homogeneous Spaces},
  author={Cohen, Taco and Geiger, Mario and Weiler, Maurice},
  journal={arXiv preprint arXiv:1811.02017},
  year={2018}
}

@article{bloem2019probabilistic,
  title={Probabilistic symmetry and invariant neural networks},
  author={Bloem-Reddy, Benjamin and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1901.06082},
  year={2019}
}



@inproceedings{kondor2018clebsch,
  title={Clebsch--gordan nets: a fully fourier space spherical convolutional neural network},
  author={Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10117--10126},
  year={2018}
}

@inproceedings{weiler20183d,
  title={3d steerable cnns: Learning rotationally equivariant features in volumetric data},
  author={Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10381--10392},
  year={2018}
}

@inproceedings{ravanbakhsh2017equivariance,
  title={Equivariance through parameter-sharing},
  author={Ravanbakhsh, Siamak and Schneider, Jeff and Poczos, Barnabas},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2892--2901},
  year={2017},
  organization={JMLR. org}
}

@article{wiatowski2018mathematical,
  title={A mathematical theory of deep convolutional neural networks for feature extraction},
  author={Wiatowski, Thomas and B{\"o}lcskei, Helmut},
  journal={IEEE Transactions on Information Theory},
  volume={64},
  number={3},
  pages={1845--1866},
  year={2018},
  publisher={IEEE}
}


@article{bruna2013invariant,
  title={Invariant scattering convolution networks},
  author={Bruna, Joan and Mallat, St{\'e}phane},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1872--1886},
  year={2013},
  publisher={IEEE}
}

@article{mallat2012group,
  title={Group invariant scattering},
  author={Mallat, St{\'e}phane},
  journal={Communications on Pure and Applied Mathematics},
  volume={65},
  number={10},
  pages={1331--1398},
  year={2012},
  publisher={Wiley Online Library}
}



@article{kondor2018generalization,
  title={On the generalization of equivariance and convolution in neural networks to the action of compact groups},
  author={Kondor, Risi and Trivedi, Shubhendu},
  journal={arXiv preprint arXiv:1802.03690},
  year={2018}
}


@article{cohen2018spherical,
  title={Spherical cnns},
  author={Cohen, Taco S and Geiger, Mario and K{\"o}hler, Jonas and Welling, Max},
  journal={arXiv preprint arXiv:1801.10130},
  year={2018}
}

@article{cohen2016steerable,
  title={Steerable cnns},
  author={Cohen, Taco S and Welling, Max},
  journal={arXiv preprint arXiv:1612.08498},
  year={2016}
}

@inproceedings{worrall2017harmonic,
  title={Harmonic networks: Deep translation and rotation equivariance},
  author={Worrall, Daniel E and Garbin, Stephan J and Turmukhambetov, Daniyar and Brostow, Gabriel J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5028--5037},
  year={2017}
}

 @inproceedings{dieleman2016exploiting, 
 title = {Exploiting Cyclic Symmetry in Convolutional Neural Networks}, author = {Sander Dieleman and Jeffrey De Fauw and Koray Kavukcuoglu}, booktitle = {Proceedings of The 33rd International Conference on Machine Learning}, pages = {1889--1898}, year = {2016}} 

@article{liao2018sharpening,
  title={Sharpening Jensen's Inequality},
  author={Liao, JG and Berg, Arthur},
  journal={The American Statistician},
  pages={1--4},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{thickstun2018invariances,
  title={Invariances and data augmentation for supervised music transcription},
  author={Thickstun, John and Harchaoui, Zaid and Foster, Dean P and Kakade, Sham M},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2241--2245},
  year={2018},
  organization={IEEE}
}

@book{giri1996group,
  title={Group invariance in statistical inference},
  author={Giri, Narayan C},
  year={1996},
  publisher={World Scientific}
}

@article{helland2004statistical,
  title={Statistical inference under symmetry},
  author={Helland, Inge S},
  journal={International Statistical Review},
  volume={72},
  number={3},
  pages={409--422},
  year={2004},
  publisher={Wiley Online Library}
}


@article{liu17supp,
  title = {{Supplement to ``$e$PCA: High Dimensional Exponential Family PCA''}},
  author = {Lydia T. Liu and Edgar Dobriban and Amit Singer},
  year = {2017}
}

@article{kurta17,
  title = {Correlations in Scattered X-Ray Laser Pulses Reveal Nanoscale Structural Features of Viruses},
  author = {Kurta, Ruslan P. and Donatelli, Jeffrey J. and Yoon, Chun Hong and others},
  journal = {Phys. Rev. Lett.},
  volume = {119},
  issue = {15},
  pages = {158102},
  numpages = {7},
  year = {2017},
  month = {Oct},
  publisher = {American Physical Society}
}

@article{Martin:12,
  author = {A. V. Martin and F. Wang and N. D. Loh and T. Ekeberg and others},
  journal = {Opt. Express},
  keywords = {X-ray imaging; Image reconstruction techniques},
  number = {15},
  pages = {16650--16661},
  publisher = {OSA},
  title = {Noise-robust coherent diffractive imaging with a single diffraction pattern},
  volume = {20},
  month = {Jul},
  year = {2012},
  abstract = {The resolution of single-shot coherent diffractive imaging at X-ray free-electron laser facilities is limited by the low signal-to-noise level of diffraction data at high scattering angles. The iterative reconstruction methods, which phase a continuous diffraction pattern to produce an image, must be able to extract information from these weak signals to obtain the best quality images. Here we show how to modify iterative reconstruction methods to improve tolerance to noise. The method is demonstrated with the hybrid input-output method on both simulated data and single-shot diffraction patterns taken at the Linac Coherent Light Source.},
}

@book{xfelbook,
  editor ="Bergmann, Uwe and Yachandra, Vittal and Yano, Junko",
  title  ="X-Ray Free Electron Lasers",
  subtitle  ="Applications in Materials{,} Chemistry and Biology",
  publisher  ="The Royal Society of Chemistry",
  year  ="2017",
  series  ="Energy and Environment Series",
  edition  ="",
  abstract  ="",
  pages  ="P001-463"
}


@article{pande2015simulations,
  Author = {Pande, K and Schmidt, M and Schwander, P and Saldin, DK},
  Date-Added = {2016-03-15 16:08:49 +0000},
  Date-Modified = {2016-03-15 16:08:49 +0000},
  Journal = {Structural Dynamics},
  Number = {2},
  Pages = {024103},
  Publisher = {American Crystallographic Association, Inc.},
  Title = {Simulations on time-resolved structure determination of uncrystallized biomolecules in the presence of shot noise},
  Volume = {2},
  Year = {2015}}

@article{pande2014deducing,
  Author = {Pande, K and Schwander, P and Schmidt, M and Saldin, DK},
  Date-Added = {2016-03-15 16:02:23 +0000},
  Date-Modified = {2016-03-15 16:02:23 +0000},
  Journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
  Number = {1647},
  Pages = {20130332},
  Publisher = {The Royal Society},
  Title = {Deducing fast electron density changes in randomly orientated uncrystallized biomolecules in a pump--probe experiment},
  Volume = {369},
  Year = {2014}}

@article{josse2016bootstrap,
  title={Bootstrap-based regularization for low-rank matrix estimation},
  author={Josse, Julie and Wager, Stefan},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={124},
  pages={1--29},
  year={2016}
}

@article{scheres2007disentangling,
  title={Disentangling conformational states of macromolecules in {3D-EM} through likelihood optimization},
  author={Scheres, Sjors HW and Gao, Haixiao and Valle, Mikel and Herman, Gabor T and Eggermont, Paul PB and Frank, Joachim and Carazo, Jose-Maria},
  journal={Nature Methods},
  volume={4},
  number={1},
  pages={27--29},
  year={2007},
  publisher={Nature Publishing Group}
}

@article{kam1977determination,
  title={Determination of macromolecular structure in solution by spatial correlation of scattering fluctuations},
  author={Kam, Zvi},
  journal={Macromolecules},
  volume={10},
  number={5},
  pages={927--934},
  year={1977},
  publisher={ACS Publications}
}

@article{Starodub2012,
  Author = {Starodub, D. and Aquila, A. and Bajt, S. and others},
  Day = {11},
  Journal = {Nature Communications},
  Month = {12},
  Publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN  -},
  Title = {Single-particle structure determination by correlations of snapshot X-ray diffraction patterns},
  Ty = {JOUR},
  Volume = {3},
  Year = {2012}
  }

@article{Saldin2009,
  author={D K Saldin and V L Shneerson and R Fung and A Ourmazd},
  title={Structure of isolated biomolecules obtained from ultrashort x-ray pulses: exploiting the symmetry of random orientations},
  journal={Journal of Physics: Condensed Matter},
  volume={21},
  number={13},
  year={2009},
}

@ARTICLE{Makitalo2011, 
  author={M. Makitalo and A. Foi}, 
  journal={IEEE Transactions on Image Processing}, 
  title={Optimal Inversion of the {Anscombe} Transformation in Low-Count {Poisson} Image Denoising}, 
  year={2011}, 
  volume={20}, 
  number={1}, 
  pages={99-109}, 
  keywords={AWGN;image denoising;least mean squares methods;maximum likelihood estimation;additive white Gaussian noise;anscombe transformation;inverse transformation;low-count {Poisson} image denoising;maximum likelihood inverse;minimum mean square error inverse;optimal inversion;three-step procedure;Denoising;{Poisson} noise;photon-limited imaging;variance stabilization}, 
  ISSN={1057-7149}, 
  month={Jan},}

@article{anscombe1948,
author = {Anscombe, F. J.},
title = {THE TRANSFORMATION OF POISSON, BINOMIAL AND NEGATIVE-BINOMIAL DATA},
journal = {Biometrika},
volume = {35},
number = {3-4},
pages = {246},
year = {1948}
}

@article{freeman1950,
  author = {Freeman, Murray F. and Tukey, John W.},
  fjournal = {The Annals of Mathematical Statistics},
  journal = {Ann. Math. Statist.},
  month = {12},
  number = {4},
  pages = {607--611},
  publisher = {The Institute of Mathematical Statistics},
  title = {{Transformations Related to the Angular and the Square Root}},
  volume = {21},
  year = {1950}
}

@book{knott1999latent,
  title={Latent variable models and factor analysis},
  author={Knott, Martin and Bartholomew, David J},
  year={1999},
  publisher={Edward Arnold}
}

@article{huber2004estimation,
  title={Estimation of generalized linear latent variable models},
  author={Huber, Philippe and Ronchetti, Elvezio and Victoria-Feser, Maria-Pia},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={66},
  number={4},
  pages={893--908},
  year={2004},
  publisher={Wiley Online Library}
}

@article{anders2010differential,
  title={Differential expression analysis for sequence count data},
  author={Anders, Simon and Huber, Wolfgang},
  journal={Genome Biology},
  volume={11},
  number={10},
  pages={1},
  year={2010},
  publisher={BioMed Central}
}

@article{ledoit2004well,
  title={A well-conditioned estimator for large-dimensional covariance matrices},
  author={Ledoit, Olivier and Wolf, Michael},
  journal={Journal of multivariate analysis},
  volume={88},
  number={2},
  pages={365--411},
  year={2004},
  publisher={Elsevier}
}

@article {Gaffney1444,
  author = {Gaffney, K. J. and Chapman, H. N.},
  title = {Imaging Atomic Structure and Dynamics with Ultrafast X-ray Scattering},
  volume = {316},
  number = {5830},
  pages = {1444--1448},
  year = {2007},
  doi = {10.1126/science.1135923},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  journal = {Science}
}


@article{duane2009,
  title = {Reconstruction algorithm for single-particle diffraction imaging experiments},
  author = {Loh, Ne-Te Duane and Elser, Veit},
  journal = {Phys. Rev. E},
  volume = {80},
  issue = {2},
  pages = {026705},
  numpages = {20},
  year = {2009},
  month = {Aug},
  publisher = {American Physical Society}
}


@article{macosko2015highly,
  title={Highly parallel genome-wide expression profiling of individual cells using nanoliter droplets},
  author={Macosko, Evan Z and Basu, Anindita and Satija, Rahul and Nemesh, James and Shekhar, Karthik and Goldman, Melissa and Tirosh, Itay and Bialas, Allison R and Kamitaki, Nolan and Martersteck, Emily M and others},
  journal={Cell},
  volume={161},
  number={5},
  pages={1202--1214},
  year={2015},
  publisher={Elsevier}
}

@article{stein1956some,
  title={Some problems in multivariate analysis},
  author={Stein, Charles},
  journal={Technical Report, Dept of Statistics, Stanford University},
  year={1956}
}

@article{li2008worldwide,
  title={Worldwide human relationships inferred from genome-wide patterns of variation},
  author={Li, Jun Z and Absher, Devin M and Tang, Hua and Southwick, Audrey M and Casto, Amanda M and Ramachandran, Sohini and Cann, Howard M and Barsh, Gregory S and Feldman, Marcus and Cavalli-Sforza, Luigi L and Myers, Richard},
  journal={Science},
  volume={319},
  number={5866},
  pages={1100--1104},
  year={2008},
  publisher={American Association for the Advancement of Science}
}

@article{dobriban2016sharp,
  title={Sharp detection in {PCA} under correlations: all eigenvalues matter},
  author={Dobriban, Edgar},
  journal={The Annals of Statistics}, 
  volume={45},
  number={4 },
  year={2017},
  pages = { 1810--1833}
}

@book{kay1993fundamentals,
  title={Fundamentals of Statistical Signal Processing: Estimation Theory},
  author={Kay, Steven M},
  volume={3},
  year={1993},
  publisher={Prentice Hall}
}

@book{mallat2008wavelet,
  title={A wavelet tour of signal processing: the sparse way},
  author={Mallat, Stephane},
  year={2008},
  publisher={Academic press}
}


@article{marchenko1967distribution,
  Author = {Marchenko, Vladimir A and Pastur, Leonid A},
  Journal = {Mat. Sb.},
  Number = {4},
  Pages = {507--536},
  Publisher = {Russian Academy of Sciences, Branch of Mathematical Sciences},
  Title = {Distribution of eigenvalues for some sets of random matrices},
  Volume = {114},
  Year = {1967}
}
  
@article{grun2014validation,
  title={Validation of noise models for single-cell transcriptomics},
  author={Gr{\"u}n, Dominic and Kester, Lennart and van Oudenaarden, Alexander},
  journal={Nat Methods},
  volume={11},
  number={6},
  pages={637--40},
  year={2014}
}

@inproceedings{donoho1993nonlinear,
  title={Nonlinear wavelet methods for recovery of signals, densities, and spectra from indirect and noisy data},
  author={Donoho, David L},
  booktitle={In Proceedings of Symposia in Applied Mathematics},
  year={1993},
}

@article{nowak1999wavelet,
  title={Wavelet-domain filtering for photon imaging systems},
  author={Nowak, Robert D and Baraniuk, Richard G},
  journal={IEEE Transactions on Image Processing},
  volume={8},
  number={5},
  pages={666--678},
  year={1999},
  publisher={IEEE}
}

@article{milanfar2013tour,
  title={A tour of modern image filtering: New insights and methods, both practical and theoretical},
  author={Milanfar, Peyman},
  journal={IEEE Signal Processing Magazine},
  volume={30},
  number={1},
  pages={106--128},
  year={2013},
  publisher={IEEE}
}

@book{elad2010sparse,
  title={Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing},
  author={Elad, Michael},
  year={2010},
  publisher={Springer Science \& Business Media}
}

@book{starck2010sparse,
  title={Sparse image and signal processing: wavelets, curvelets, morphological diversity},
  author={Starck, Jean-Luc and Murtagh, Fionn and Fadili, Jalal M},
  year={2010},
  publisher={Cambridge university press}
}


@article{worsley2005comparing,
  title={Comparing functional connectivity via thresholding correlations and singular value decomposition},
  author={Worsley, Keith J and Chen, Jen-I and Lerch, Jason and Evans, Alan C},
  journal={Philosophical Transactions of the Royal Society of London B: Biological Sciences},
  volume={360},
  number={1457},
  pages={913--920},
  year={2005},
  publisher={The Royal Society}
}

@article{ostergaard1996high,
  title={High resolution measurement of cerebral blood flow using intravascular tracer bolus passages. Part {I}: Mathematical approach and statistical analysis},
  author={{\O}stergaard, Leif and Weisskoff, Robert M and Chesler, David A and Gyldensted, Carsten and Rosen, Bruce R},
  journal={Magnetic Resonance in Medicine},
  volume={36},
  number={5},
  pages={715--725},
  year={1996},
  publisher={Wiley Online Library}
}

@article{favre2015xtop,
  title={{XTOP: high-resolution X-ray diffraction and imaging}},
  author={Favre-Nicolin, Vincent and Baruchel, Jos{\'e} and Renevier, Hubert and Eymery, Jo{\"e}l and Borb{\'e}ly, Andr{\'a}s},
  journal={Journal of Applied Crystallography},
  volume={48},
  number={3},
  pages={620--620},
  year={2015},
  publisher={International Union of Crystallography}
}

@article{maia2016trickle,
  title={{The trickle before the torrent—diffraction data from X-ray lasers}},
  author={Maia, Filipe RNC and Hajdu, Janos},
  journal={Scientific Data},
  volume={3},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{Furnival2016,
  title = {Denoising time-resolved microscopy image sequences with singular value thresholding},
  journal = {Ultramicroscopy },
  year = {2016},
  issn = {0304-3991},
  author = {Tom Furnival and Rowan K. Leary and Paul A. Midgley},
}


@article{maia2016condor,
  title={Condor: A Simulation Tool for Flash X-Ray Imaging},
  author={Hantke, Max F. and Ekeberg, Tomas and Maia, Filipe R. N. C.  },
  journal={Journal of Applied Crystallography},
  volume={49},
  number={4},
  pages={1356--1362},
  year={2016},
  publisher={PMC}
}

@article{nadakuditi2014optshrink,
  title={OptShrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage},
  author={Nadakuditi, Raj Rao},
  journal={IEEE Transactions on Information Theory},
  volume={60},
  number={5},
  pages={3002--3018},
  year={2014},
  publisher={IEEE}
}

@article{gavish2014optimal,
  title={The optimal hard threshold for singular values is},
  author={Gavish, Matan and Donoho, David L},
  journal={IEEE Transactions on Information Theory},
  volume={60},
  number={8},
  pages={5040--5053},
  year={2014},
  publisher={IEEE}
}

@article{luisier2011image,
  title={Image denoising in mixed {Poisson}--Gaussian noise},
  author={Luisier, Florian and Blu, Thierry and Unser, Michael},
  journal={IEEE Transactions on Image Processing},
  volume={20},
  number={3},
  pages={696--708},
  year={2011},
  publisher={IEEE}
}

@article{luisier2007new,
  title={A new SURE approach to image denoising: Interscale orthonormal wavelet thresholding},
  author={Luisier, Florian and Blu, Thierry and Unser, Michael},
  journal={IEEE Transactions on image processing},
  volume={16},
  number={3},
  pages={593--606},
  year={2007},
  publisher={IEEE}
}

@article{salmon2014poisson,
  title={{Poisson} noise reduction with non-local {PCA}},
  author={Salmon, Joseph and Harmany, Zachary and Deledalle, Charles-Alban and Willett, Rebecca},
  journal={Journal of Mathematical Imaging and Vision},
  volume={48},
  number={2},
  pages={279--294},
  year={2014},
  publisher={Springer}
}

@techreport{landgraf2015generalized,
  title={Generalized principal component analysis: Projection of saturated model parameters},
  author={Landgraf, Andrew J and Lee, Yoonkyung},
  year={2015},
  institution={Technical Report 892, Department of Statistics, The Ohio State University}
}

@inproceedings{sajama2004semi,
  title={Semi-parametric exponential family {PCA}},
  author={Sajama, Sajama and Orlitsky, Alon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1177--1184},
  year={2004}
}

@article{de2006principal,
  title={Principal component analysis of binary data by iterated singular value decomposition},
  author={De Leeuw, Jan},
  journal={Computational statistics \& data analysis},
  volume={50},
  number={1},
  pages={21--39},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{li2010simple,
  title={{Simple exponential family PCA.}},
  author={Li, Jun and Tao, Dacheng},
  booktitle={AISTATS},
  pages={453--460},
  year={2010}
}

@book{hyvarinen2004independent,
  title={Independent Component Analysis},
  author={Hyv{\"a}rinen, Aapo and Karhunen, Juha and Oja, Erkki},
  year={2004},
  publisher={John Wiley \& Sons}
}

@article{qian1994principal,
  title={Principal components selection by the criterion of the minimum mean difference of complexity},
  author={Qian, Guo Qi and Gabor, George and Gupta, RP},
  journal={Journal of Multivariate Analysis},
  volume={49},
  number={1},
  pages={55--75},
  year={1994},
  publisher={Elsevier}
}

@book{yao2015large,
  Author = {Yao, Jianfeng and Bai, Zhidong and Zheng, Shurong},
  Date-Added = {2015-04-28 22:52:15 +0000},
  Date-Modified = {2015-04-28 22:52:15 +0000},
  Publisher = {Cambridge University Press},
  Title = {Large Sample Covariance Matrices and High-Dimensional Data Analysis},
  Year = {2015}}

@article{paul2014random,
  title={Random matrix theory in statistics: A review},
  author={Paul, Debashis and Aue, Alexander},
  journal={Journal of Statistical Planning and Inference},
  volume={150},
  pages={1--29},
  year={2014},
  publisher={Elsevier}
}

  
@book{muirhead2009aspects,
  title={Aspects of multivariate statistical theory},
  author={Muirhead, Robb J},
  year={2009},
  publisher={John Wiley \& Sons}
}

  
  
@article{gower1966some,
  title={Some distance properties of latent root and vector methods used in multivariate analysis},
  author={Gower, John C},
  journal={Biometrika},
  volume={53},
  number={3-4},
  pages={325--338},
  year={1966},
  publisher={Biometrika Trust}
}

@book{searle2009variance,
  title={Variance components},
  author={Searle, Shayle R and Casella, George and McCulloch, Charles E},
  year={2009},
  publisher={John Wiley \& Sons}
}
@article{bhamre2016denoising,
  title={Denoising and covariance estimation of single particle cryo-{EM} images},
  author={Bhamre, Tejal and Zhang, Teng and Singer, Amit},
  journal={Journal of Structural Biology},
  volume={195},
  number={1},
  pages={72--81},
  year={2016},
  publisher={Elsevier}
}

@article{singer2013two,
  title={Two-dimensional tomography from noisy projections taken at unknown random directions},
  author={Singer, A and Wu, H-T},
  journal={SIAM Journal on Imaging Sciences},
  volume={6},
  number={1},
  pages={136--175},
  year={2013},
  publisher={SIAM}
}

@article{lee2010convergence,
  title={Convergence and prediction of principal component scores in high-dimensional settings},
  author={Lee, Seunggeun and Zou, Fei and Wright, Fred A},
  journal={Annals of Statistics},
  volume={38},
  number={6},
  pages={3605--3629},
  year={2010},
}


@article{benaych2011eigenvalues,
  title={The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices},
  author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal={Advances in Mathematics},
  volume={227},
  number={1},
  pages={494--521},
  year={2011},
  publisher={Elsevier}
}

@article{paul2007asymptotics,
  title={Asymptotics of sample eigentructure for a large dimensional spiked covariance model},
  author={Paul, Debashis},
  journal={Statistica Sinica},
  volume={17},
  number={4},
  pages={1617-1642},
  year={2007}
}

@article{baik2005phase,
  title={Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  author={Baik, Jinho and Ben Arous, G{\'e}rard and P{\'e}ch{\'e}, Sandrine},
  journal={Annals of Probability},
  volume = {33}, 
  number = {5}, 
  pages={1643--1697},
  year={2005},
  publisher={JSTOR}
}

@article{anderson2010data,
  title={Data quality control in genetic case-control association studies},
  author={Anderson, Carl A and Pettersson, Fredrik H and Clarke, Geraldine M and Cardon, Lon R and Morris, Andrew P and Zondervan, Krina T},
  journal={Nature Protocols},
  volume={5},
  number={9},
  pages={1564--1573},
  year={2010},
  publisher={Nature Publishing Group}
}

@book{jolliffe2002principal,
  title={Principal Component Analysis},
  author={Jolliffe, Ian},
  year={2002},
  publisher={Wiley Online Library}
}

@article{dobriban2015efficient,
author = {Dobriban, Edgar},
title = {Efficient computation of limit spectra of sample covariance matrices},
journal = {Random Matrices: Theory and Applications},
volume = {04},
number = {04},
pages = {1550019},
year = {2015},
} 

@book{bai2009spectral,
  Author = {Bai, Zhidong and Silverstein, Jack W},
  Publisher = {Springer},
  Series = {Springer Series in Statistics},
  Title = {Spectral analysis of large dimensional random matrices},
  Year = {2009}}


@article{stegle2015computational,
  title={Computational and analytical challenges in single-cell transcriptomics},
  author={Stegle, Oliver and Teichmann, Sarah A and Marioni, John C},
  journal={Nature Reviews Genetics},
  volume={16},
  number={3},
  pages={133--145},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{visscher2012five,
  title={Five years of {GWAS} discovery},
  author={Visscher, Peter M and Brown, Matthew A and McCarthy, Mark I and Yang, Jian},
  journal={The American Journal of Human Genetics},
  volume={90},
  number={1},
  pages={7--24},
  year={2012},
  publisher={Elsevier}
}

@article{patterson2006population,
  title={Population structure and eigenanalysis},
  author={Patterson, N and Price, AL and Reich, D},
  journal={PLoS Genet},
  volume={2},
  number={12},
  pages={e190},
  year={2006}
}

@article{bigot2016generalized,
  title={Generalized {SURE} for optimal shrinkage of singular values in low-rank matrix denoising},
  author={Bigot, J{\'e}r{\'e}mie and Deledalle, Charles and F{\'e}ral, Delphine},
  journal={arXiv preprint arXiv:1605.07412},
  year={2016}
}

@article{shabalin2013reconstruction,
  title={Reconstruction of a low-rank matrix in the presence of Gaussian noise},
  author={Shabalin, Andrey A and Nobel, Andrew B},
  journal={Journal of Multivariate Analysis},
  volume={118},
  pages={67--76},
  year={2013},
  publisher={Elsevier}
}

@article{hartigan1969linear,
  title={Linear bayesian methods},
  author={Hartigan, JA},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  pages={446--454},
  year={1969},
    volume={31},
  number={3},
  publisher={JSTOR}
}

@article{josse2014stable,
  title={Stable Autoencoding: A Flexible Framework for Regularized Low-Rank Matrix Estimation},
  author={Josse, Julie and Wager, Stefan},
  journal={arXiv preprint arXiv:1410.8275},
  year={2014}
}

@article{deerwester1990indexing,
  title={Indexing by latent semantic analysis},
  author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
  journal={Journal of the American society for Information Science},
  volume={41},
  number={6},
  pages={391},
  year={1990},
  publisher={American Documentation Institute}
}

@article{bloemendal2016principal,
  title={On the principal components of sample covariance matrices},
  author={Bloemendal, Alex and Knowles, Antti and Yau, Horng-Tzer and Yin, Jun},
  journal={Probability Theory and Related Fields},
  volume={164},
  number={1-2},
  pages={459--552},
  year={2016},
  publisher={Springer}
}

@article{lee2014tracy,
  title={Tracy-Widom distribution for the largest eigenvalue of real sample covariance matrices with general population},
  author={Lee, Ji Oon and Schnelli, Kevin},
  journal={arXiv preprint arXiv:1409.4979},
  year={2014}
}

@article{pillai2012edge,
  title={Edge universality of correlation matrices},
  author={Pillai, Natesh S and Yin, Jun},
  journal={The Annals of Statistics},
  pages={1737--1763},
  year={2012},
  publisher={JSTOR}
}

@article{bao2015universality,
  title={Universality for the largest eigenvalue of sample covariance matrices with general population},
  author={Bao, Zhigang and Pan, Guangming and Zhou, Wang},
  journal={The Annals of Statistics},
  volume={43},
  number={1},
  pages={382--421},
  year={2015},
  publisher={Institute of Mathematical Statistics}
}

@article{pillai2014universality,
  title={Universality of covariance matrices},
  author={Pillai, Natesh S and Yin, Jun},
  journal={The Annals of Applied Probability},
  volume={24},
  number={3},
  pages={935--1001},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}

@article{benaych2012singular,
  title={The singular values and vectors of low rank perturbations of large rectangular random matrices},
  author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal={Journal of Multivariate Analysis},
  volume={111},
  pages={120--135},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{anden2015covariance,
  title={Covariance estimation using conjugate gradient for 3D classification in Cryo-{EM}},
  author={And{\'e}n, Joakim and Katsevich, Eugene and Singer, Amit},
  booktitle={Biomedical Imaging (ISBI), 2015 IEEE 12th International Symposium on},
  pages={200--204},
  year={2015},
  organization={IEEE}
}

@article{katsevich2015covariance,
  title={Covariance matrix estimation for the Cryo-{EM} heterogeneity problem},
  author={Katsevich, Eugene and Katsevich, Alexander and Singer, Amit},
  journal={SIAM Journal on Imaging Sciences},
  volume={8},
  number={1},
  pages={126--185},
  year={2015},
  publisher={SIAM}
}

@incollection {johnstone2007high,
    AUTHOR = {Johnstone, Iain M.},
     TITLE = {High dimensional statistical inference and random matrices},
 BOOKTITLE = {International {C}ongress of {M}athematicians. {V}ol. {I}},
     PAGES = {307--333},
 PUBLISHER = {Eur. Math. Soc., Z\"urich},
      YEAR = {2007},
}

@article{lehmann1998theory,
  title={Theory of Point Estimation},
  author={Lehmann, EL and Casella, George},
  journal={Springer Texts in Statistics},
  year={1998},
  publisher={New York, NY: Springer New York}
}


@article{Hasinoff,
author = {Hasinoff, Samuel W},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Hasinoff - Unknown - Photon , {Poisson} noise.pdf:pdf},
isbn = {0387307710},
journal = {Computer},
pages = {5--8},
title = {{Photon , {Poisson} noise}}
}
@article{Salmon2014,
abstract = {Photon limitations arise in spectral imaging, nuclear medicine, as- tronomy and night vision. The {Poisson} distribution used to model this noise has variance equal to its mean so blind application of stan- dard noise removals methods yields significant artifacts. Recently, overcomplete dictionaries combined with sparse learning techniques have become extremely popular in image reconstruction. The aim of the present work is to demonstrate that for the task of image de- noising, nearly state-of-the-art results can be achieved using small dictionaries only, provided that they are learned directly from the noisy image. To this end, we introduce patch-based denoising algo- rithms which perform an adaptation of PCA (Principal Component Analysis) for {Poisson} noise. We carry out a comprehensive empiri- cal evaluation of the performance of our algorithms in terms of ac- curacy when the photon count is really low. The results reveal that, despite its simplicity, PCA-flavored denoising appears to be compet- itive with other state-of-the-art denoising algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.0338},
author = {Salmon, Joseph and Harmany, Zachary and Deledalle, Charles Alban and Willett, Rebecca},
doi = {10.1007/s10851-013-0435-6},
eprint = {1206.0338},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Salmon et al. - 2014 - {Poisson} noise reduction with non-local PCA.pdf:pdf},
isbn = {9781467300469},
issn = {09249907},
journal = {Journal of Mathematical Imaging and Vision},
keywords = {Gradient methods,Image denoising,Newton's method,PCA,Signal representations},
number = {2},
pages = {279--294},
title = {{{Poisson} noise reduction with non-local PCA}},
volume = {48},
year = {2014}
}
@article{Ponce2011,
abstract = {We present here an efficient algorithm to compute the Principal Component Analysis (PCA) of a large image set consisting of images and, for each image, the set of its uniform rotations in the plane. We do this by pointing out the block circulant structure of the covariance matrix and utilizing that structure to compute its eigenvectors. We also demonstrate the advantages of this algorithm over similar ones with numerical experiments. Although it is useful in many settings, we illustrate the specific application of the algorithm to the problem of cryo-electron microscopy.},
author = {Ponce, Colin and Singer, Amit},
doi = {10.1109/TIP.2011.2147323},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Ponce, Singer - 2011 - Computing steerable principal components of a large set of images and their rotations.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {EDICS Category: TEC-PRC image and video processing},
number = {11},
pages = {3051--3062},
pmid = {21536533},
title = {{Computing steerable principal components of a large set of images and their rotations}},
volume = {20},
year = {2011}
}
@article{Cai2010,
author = {Cai, Jian-Feng and Cand{\`{e}}s, Emmanuel J. and Shen, Zuowei},
doi = {10.1137/080738970},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Cand{\`{e}}s, Shen - 2010 - A Singular Value Thresholding Algorithm for Matrix Completion.pdf:pdf},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
keywords = {15A83,65K05,90C25,Lagrange dual function,SVT,Uzawa's algorithm,linearized Bregman iteration,matrix completion,nuclear norm minimization,singular value thresholding},
language = {en},
mendeley-tags = {SVT,matrix completion},
month = {jan},
number = {4},
pages = {1956--1982},
publisher = {Society for Industrial and Applied Mathematics},
title = {{A Singular Value Thresholding Algorithm for Matrix Completion}},
volume = {20},
year = {2010}
}
@article{Zhao,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.0781v3},
author = {Zhao, Zhizhen and Shkolnisky, Yoel and Singer, Amit},
eprint = {arXiv:1412.0781v3},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Zhao, Shkolnisky, Singer - Unknown - Fast Steerable Principal Component Analysis.pdf:pdf},
pages = {1--11},
title = {{Fast Steerable Principal Component Analysis}}
}
@article{Singh2008,
author = {Singh, Ajit P and Gordon, Geoffrey J},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Gordon - 2008 - LNAI 5212 - A Unified View of Matrix Factorization Models.pdf:pdf},
journal = {Matrix},
pages = {358--373},
title = {{LNAI 5212 - A Unified View of Matrix Factorization Models}},
year = {2008}
}
@article{Xiuyuan,
annote = {From Jane},
author = {Xiuyuan},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Xiuyuan - Unknown - Chapter 7 Symmetry detection.pdf:pdf},
title = {{Chapter 7 Symmetry detection}}
}
@article{Kam1980,
annote = {From Jane},
author = {Kam, Z},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Kam - 1980 - The reconstruction of structure from electron micrographs of randomly oriented particles.pdf:pdf},
journal = {Journal of Theoretical Biology},
number = {1},
pages = {15--39},
pmid = {7401655},
title = {{The reconstruction of structure from electron micrographs of randomly oriented particles.}},
volume = {82},
year = {1980}
}
@article{Ahmadi2014,
author = {Ahmadi, Amir Ali and Hall, G and Ye, J},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Ahmadi, Hall, Ye - 2014 - Lec13p2 , ORF363 COS323 Why SDP.pdf:pdf},
pages = {1--14},
title = {{Lec13p2 , ORF363 / COS323 Why SDP ?}},
year = {2014}
}
@article{Collins2001,
author = {Collins, Michael and Dasgupta, S and Schapire, Re},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Collins, Dasgupta, Schapire - 2001 - A generalization of principal component analysis to the exponential family.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
title = {{A generalization of principal component analysis to the exponential family}},
year = {2001}
}
@article{Soni,
author = {Soni, Akshay and Haupt, Jarvis},
file = {:Users/tingruolydia/Library/Application Support/Mendeley Desktop/Downloaded/Soni, Haupt - Unknown - Estimation Error Guarantees for {Poisson} Denoising with Sparse and Structured Dictionary Models.pdf:pdf},
number = {1},
title = {{Estimation Error Guarantees for {Poisson} Denoising with Sparse and Structured Dictionary Models}}
}
@article{Schwander:12, 
  author = {Peter Schwander and Dimitrios Giannakis and Chun Hong Yoon and Abbas Ourmazd}, 
  journal = {Opt. Express}, 
  keywords = {Free-electron lasers (FELs); Three-dimensional microscopy; Inverse scattering; Scattering, molecules; Scattering theory},
  number = {12}, 
  pages = {12827--12849}, 
  publisher = {OSA},
  title = {{The symmetries of image formation by scattering. II. Applications}}, 
  volume = {20}, 
  month = {Jun},
  year = {2012},
}


@inproceedings{Cao2014,
  title={Low-rank matrix recovery in {Poisson} noise},
  author={Cao, Yang and Xie, Yao},
  booktitle={Signal and Information Processing (GlobalSIP), 2014 IEEE Global Conference on},
  pages={384--388},
  year={2014},
  organization={IEEE}
}

@book{lehmann2005testing,
  title={Testing statistical hypotheses},
  author={Lehmann, Erich L and Romano, Joseph P},
  year={2005},
  publisher={Springer Science \& Business Media}
}

@article{baik2006eigenvalues,
  title={Eigenvalues of large sample covariance matrices of spiked population models},
  author={Baik, Jinho and Silverstein, Jack W},
  journal={Journal of Multivariate Analysis},
  volume={97},
  number={6},
  pages={1382--1408},
  year={2006},
  publisher={Elsevier}
}


@article{johnstone2001distribution,
  title={On the distribution of the largest eigenvalue in principal components analysis},
  author={Johnstone, Iain M},
  journal={Annals of Statistics},
    volume={29},
  number={2},
  pages={295--327},
  year={2001},
  publisher={JSTOR}
}

@article{nadler2008finite,
  title={Finite sample approximation results for principal component analysis: A matrix perturbation approach},
  author={Nadler, Boaz},
  journal={The Annals of Statistics},
  volume={36},
  number={6},
  pages={2791--2817},
  year={2008}
}

@article{Tropp2010,
  author = {Tropp, Joel A.},
  title = {User-Friendly Tail Bounds for Sums of Random Matrices},
  journal = {Found. Comput. Math.},
  issue_date = {August 2012},
  volume = {12},
  number = {4},
  month = aug,
  year = {2012},
  issn = {1615-3375},
  pages = {389--434},
  numpages = {46},
  url = {http://dx.doi.org/10.1007/s10208-011-9099-z},
  doi = {10.1007/s10208-011-9099-z},
  acmid = {2347804},
  publisher = {Springer-Verlag New York, Inc.},
  address = {Secaucus, NJ, USA},
  keywords = {Large deviation, Probability inequality, Random matrix, Sum of independent random variables},
} 

@article{Oberg2012,
author = {Oberg, Ann L and Bot, Brian M and Grill, Diane E and Poland, Gregory A and Therneau, Terry M},
title = {{Technical and biological variance structure in mRNA-Seq data : life in the real world}},
year = {2012}
}

@incollection{Vershynin2011,
author = {Vershynin, Roman},
publisher = {Cambridge Univ. Press},
address = {Cambridge},
pages = {210--268},
booktitle = {Compressed sensing},
keywords = {matrices,probability,random},
title = {{Introduction to the non-asymptotic analysis of random matrices}},
year = {2012}
}
@article{Dong2015,
author = {Dong, Kai and Zhao, Hongyu and Wan, Xiang and Tong, Tiejun},
file = {:Users/tingruolydia/Downloads/NBLDA.pdf:pdf},
number = {2011},
pages = {1--7},
title = {{BIOINFORMATICS NBLDA : Negative Binomial Linear Discriminant Analysis for RNA-Seq Data}},
year = {2015}
}

@incollection{Tropp2015,
  author       = {Joel A. Tropp}, 
  title = {{The Expected Norm of a Sum of Independent Random Matrices: An Elementary Approach}},
  booktitle    = {High-Dimensional Probability VII},
  publisher    = {Birkhaeuser},
  year         = 2016,
  series       = {Progress in Probability 71},
}



@article{Gotze1991,
author = {Gotze, F.},
doi = {10.1214/aop/1176990448},
file = {:Users/tingruolydia/Downloads/euclid.aop.1176990448.pdf:pdf},
issn = {0091-1798},
journal = {The Annals of Probability},
number = {2},
pages = {724--739},
title = {{On the Rate of Convergence in the Multivariate CLT}},
volume = {19},
year = {1991}
}
@article{Vershynin2012,
abstract = {Given a probability distribution in R{\^{}}n with general (non-white) covariance, a classical estimator of the covariance matrix is the sample covariance matrix obtained from a sample of N independent points. What is the optimal sample size N = N(n) that guarantees estimation with a fixed accuracy in the operator norm? Suppose the distribution is supported in a centered Euclidean ball of radius $\backslash$sqrt{\{}n{\}}. We conjecture that the optimal sample size is N = O(n) for all distributions with finite fourth moment, and we prove this up to an iterated logarithmic factor. This problem is motivated by the optimal theorem of Rudelson which states that N = O(n $\backslash$log n) for distributions with finite second moment, and a recent result of Adamczak, Litvak, Pajor and Tomczak-Jaegermann which guarantees that N = O(n) for sub-exponential distributions.},
archivePrefix = {arXiv},
arxivId = {1004.3484},
author = {Vershynin, Roman},
doi = {10.1007/s10959-010-0338-z},
eprint = {1004.3484},
file = {:Users/tingruolydia/Documents/PACM/sample-covariance.pdf:pdf},
issn = {08949840},
journal = {Journal of Theoretical Probability},
keywords = {Estimation of covariance matrices,Random matrices with independent columns,Sample covariance matrices},
number = {3},
pages = {655--686},
title = {{How Close is the Sample Covariance Matrix to the Actual Covariance Matrix?}},
volume = {25},
year = {2012}
}
@article{Adamczak2010,
abstract = {Let {\$}X{\_}1,..., X{\_}N\backslashin\backslashR{\^{}}n{\$} be independent centered random vectors with log-concave distribution and with the identity as covariance matrix. We show that with overwhelming probability at least {\$}1 - 3 \backslashexp(-c\backslashsqrt{\{}n{\}}\backslashr){\$} one has {\$} \backslashsup{\_}{\{}x\backslashin S{\^{}}{\{}n-1{\}}{\}} \backslashBig|\backslashfrac{\{}1/N{\}}\backslashsum{\_}{\{}i=1{\}}{\^{}}N (|{\textless}X{\_}i, x{\textgreater}|{\^{}}2 - \backslashE|{\textless}X{\_}i, x{\textgreater}|{\^{}}2\backslashr)\backslashBig| \backslashleq C \backslashsqrt{\{}\backslashfrac{\{}n/N{\}}{\}},{\$} where {\$}C{\$} is an absolute positive constant. This result is valid in a more general framework when the linear forms {\$}({\textless}X{\_}i,x{\textgreater}){\_}{\{}i\backslashleq N, x\backslashin S{\^{}}{\{}n-1{\}}{\}}{\$} and the Euclidean norms {\$}(|X{\_}i|/\backslashsqrt n){\_}{\{}i\backslashleq N{\}}{\$} exhibit uniformly a sub-exponential decay. As a consequence, if {\$}A{\$} denotes the random matrix with columns {\$}(X{\_}i){\$}, then with overwhelming probability, the extremal singular values {\$}\backslashlambda{\_}{\{}\backslashrm min{\}}{\$} and {\$}\backslashlambda{\_}{\{}\backslashrm max{\}}{\$} of {\$}AA{\^{}}\backslashtop{\$} satisfy the inequalities {\$} 1 - C\backslashsqrt{\{}{\{}n/N{\}}{\}} \backslashle {\{}\backslashlambda{\_}{\{}\backslashrm min{\}}/N{\}} \backslashle \backslashfrac{\{}\backslashlambda{\_}{\{}\backslashrm max{\}}/N{\}} \backslashle 1 + C\backslashsqrt{\{}{\{}n/N{\}}{\}} {\$} which is a quantitative version of Bai-Yin theorem $\backslash$cite{\{}BY{\}} known for random matrices with i.i.d. entries.},
archivePrefix = {arXiv},
arxivId = {1012.0294},
author = {Adamczak, Rados{\l}aw and Litvak, Alexander E. and Pajor, Alain and Tomczak-Jaegermann, Nicole},
doi = {10.1016/j.crma.2010.12.014},
eprint = {1012.0294},
file = {:Users/tingruolydia/Documents/PACM/adamczak09.pdf:pdf},
issn = {1631073X},
journal = {Comptes Rendus {\ldots}},
keywords = {approximation of covariance matrices,at the de-,began when this author,convex bodies,held a postdoctoral position,isotropic measures,log-concave measures,norm of random matrices,numbers,random matrices,uniform laws of large,work on this paper},
number = {1},
pages = {1--6},
title = {{Sharp bounds on the rate of convergence of the empirical covariance matrix}},
url = {http://www.sciencedirect.com/science/article/pii/S1631073X10003936$\backslash$nhttp://arxiv.org/abs/1012.0294},
year = {2010}
}
@article{Srivastava2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1106.2775v1},
author = {Srivastava, Nikhil and Vershynin, Roman},
doi = {10.1214/12-AOP760},
eprint = {arXiv:1106.2775v1},
file = {:Users/tingruolydia/Downloads/1106.2775.pdf:pdf},
issn = {00911798},
journal = {Annals of Probability},
keywords = {Covariance matrices,High-dimensional distributions,Log-concave distributions,Random matrices,Stieltjes transform},
number = {5},
pages = {3081--3111},
title = {{Covariance estimation for distributions with 2 + epsilon moments}},
volume = {41},
year = {2013}
}
@article{Rudelson1999,
abstract = {Let {\$}y{\$} be a random vector in $\backslash$rn, satisfying {\$}{\$} $\backslash$Bbb E $\backslash$, $\backslash$tens{\{}y{\}} = id. {\$}{\$} Let {\$}M{\$} be a natural number and let {\$}y{\_}1 \backslashetc y{\_}M{\$} be independent copies of {\$}y{\$}. We prove that for some absolute constant {\$}C{\$} {\$}{\$} $\backslash$enor{\{}$\backslash$frac{\{}1{\}}{\{}M{\}} $\backslash$sum{\_}i $\backslash$tens{\{}y{\_}i{\}} - id{\}} $\backslash$le C $\backslash$cdot $\backslash$frac{\{}$\backslash$sqrt{\{}$\backslash$log M{\}}{\}}{\{}$\backslash$sqrt{\{}M{\}}{\}} $\backslash$cdot $\backslash$left ( $\backslash$enor{\{}y{\}}{\^{}}{\{}$\backslash$log M{\}} $\backslash$right ){\^{}}{\{}1/ $\backslash$log M{\}}, {\$}{\$} provided that the last expression is smaller than 1. We apply this estimate to obtain a new proof of a result of Bourgain concerning the number of random points needed to bring a convex body into a nearly isotropic position.},
archivePrefix = {arXiv},
arxivId = {math/9608208},
author = {Rudelson, Mark},
doi = {10.1006/jfan.1998.3384},
eprint = {9608208},
file = {:Users/tingruolydia/Documents/PACM/rudelson.pdf:pdf},
issn = {00221236},
journal = {Journal of Functional Analysis},
pages = {1--12},
primaryClass = {math},
title = {{Random vectors in the isotropic position}},
url = {http://arxiv.org/abs/math/9608208$\backslash$nhttp://www.sciencedirect.com/science/article/pii/S0022123698933845},
year = {1999}
}
@article{Boucheron2005,
abstract = {A general method for obtaining moment inequalities for functions of independent random variables is presented. It is a generalization of the entropy method which has been used to derive concentration inequalities for such functions [Boucheron, Lugosi and Massart Ann. Probab. 31 (2003) 1583-1614], and is based on a generalized tensorization inequality due to Latala and Oleszkiewicz [Lecture Notes in Math. 1745 (2000) 147-168]. The new inequalities prove to be a versatile tool in a wide range of applications. We illustrate the power of the method by showing how it can be used to effortlessly re-derive classical inequalities including Rosenthal and Kahane-Khinchine-type inequalities for sums of independent random variables, moment inequalities for suprema of empirical processes and moment inequalities for Rademacher chaos and U-statistics. Some of these corollaries are apparently new. In particular, we generalize Talagrand's exponential inequality for Rademacher chaos of order 2 to any order. We also discuss applications for other complex functions of independent random variables, such as suprema of Boolean polynomials which include, as special cases, subgraph counting problems in random graphs.},
archivePrefix = {arXiv},
arxivId = {math/0503651},
author = {Boucheron, St??phane and Bousquet, Olivier and Lugosi, G??bor and Massart, Pascal},
eprint = {0503651},
file = {:Users/tingruolydia/Downloads/document.pdf:pdf},
journal = {Annals of Probability},
keywords = {Concentration inequalities,Empirical processes,Moment inequalities,Random graphs},
number = {2},
pages = {514--560},
primaryClass = {math},
title = {{Moment inequalities for functions of independent random variables}},
volume = {33},
year = {2005}
}
@article{Li2015,
abstract = {We study weighted l(2) fidelity in variational models for {Poisson} noise$\backslash$nrelated image restoration problems. Gaussian approximation to {Poisson}$\backslash$nnoise statistic is adopted to deduce weighted l(2) fidelity. Different$\backslash$nfrom the traditional weighted l(2) approximation, we propose a$\backslash$nreweighted l(2) fidelity with sparse regularization by wavelet frame.$\backslash$nBased on the split Bregman algorithm introduced in {\{}[{\}}21], the proposed$\backslash$nnumerical scheme is composed of three easy subproblems that involve$\backslash$nquadratic minimization, soft shrinkage and matrix vector$\backslash$nmultiplications. Unlike usual least square approximation of {Poisson}$\backslash$nnoise, we dynamically update the underlying noise variance from previous$\backslash$nestimate. The solution of the proposed algorithm is shown to be the same$\backslash$nas the one obtained by minimizing Kullback-Leibler divergence fidelity$\backslash$nwith the same regularization. This reweighted l(2) formulation can be$\backslash$neasily extended to mixed {Poisson}-Gaussian noise case. Finally, the$\backslash$nefficiency and quality of the proposed algorithm compared to other$\backslash$n{Poisson} noise removal methods are demonstrated through denoising and$\backslash$ndeblurring examples. Moreover, mixed {Poisson}-Gaussian noise tests are$\backslash$nperformed on both simulated and real digital images for further$\backslash$nillustration of the performance of the proposed method.},
author = {Li, Jia and Shen, Zuowei and Yin, Rujie and Zhang, Xiaoqun},
doi = {10.3934/ipi.2015.9.875},
file = {:Users/tingruolydia/Documents/PACM/literature/weightedL2{\_}IPI{\_}140226{\_}RY.pdf:pdf},
issn = {1930-8337},
journal = {Inverse Problems and Imaging},
keywords = {{Poisson} noise; {Poisson}-Gaussian mixed noise; inver},
number = {3},
pages = {875--894},
title = {{A REWEIGHTED l(2) METHOD FOR IMAGE RESTORATION WITH POISSON AND MIXED POISSON-GAUSSIAN NOISE}},
volume = {9},
year = {2015}
}

@inproceedings{Sonnleitner2016,
  title={Local retrodiction models for photon-noise-limited images},
  author={Sonnleitner, Matthias and Jeffers, John and Barnett, Stephen M},
  booktitle={SPIE Photonics Europe},
  pages={98960V--98960V},
  year={2016},
  organization={International Society for Optics and Photonics}
}

@article{Basri2003,
author = {Basri, Ronen and Jacobs, David W},
keywords = {Index Terms—Face recognition,Lambertian,illumination,linear subspaces,object recognition,specular,spherical harmonics},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {218--233},
title = {{Lambertian Reflectance and Linear Subspaces}},
volume = {25},
year = {2003}
}

@article{udell2016,
  title={{Generalized Low Rank Models}},
  author={Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen},
  journal={Foundations and Trends in Machine Learning},
  volume={9},
  number={1},
  pages={1--118},
  year={2016},
}

@inproceedings{Udell2014,
  author = {Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen},
  booktitle = {NIPS Workshop on Distributed Machine Learning and Matrix Computations},
  title = {{Generalized Low Rank Models}},
  year = {2014}
}


@article{Donoho2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.0851v1},
author = {Donoho, Dl and Gavish, M and Johnstone, Im},
eprint = {arXiv:1311.0851v1},
file = {:Users/tingruolydia/Documents/PACM/literature/1311.0851v2.pdf:pdf},
journal = {arXiv preprint arXiv:1311.0851},
keywords = {bhattacharya,condition number loss,covariance estimation,divergence loss,echet distance,entropy loss,fr,high-dimensional asymptotics,matusita affinity,optimal nonlinearity,precision estimation,principal compo-,quadratic loss,spiked covariance,stein loss},
pages = {1--35},
title = {{Optimal shrinkage of eigenvalues in the Spiked Covariance Model}},
volume = {0906812},
year = {2013}
}
@article{Luisier2011,
abstract = {We propose a general methodology (PURE-LET) to design and optimize a wide class of transform-domain thresholding algorithms for denoising images corrupted by mixed {Poisson}{\&}{\#}x2013;Gaussian noise. We express the denoising process as a linear expansion of thresholds (LET) that we optimize by relying on a purely data-adaptive unbiased estimate of the mean-squared error (MSE), derived in a non-Bayesian framework (PURE: {Poisson}{\&}{\#}x2013;Gaussian unbiased risk estimate). We provide a practical approximation of this theoretical MSE estimate for the tractable optimization of arbitrary transform-domain thresholding. We then propose a pointwise estimator for undecimated filterbank transforms, which consists of subband-adaptive thresholding functions with signal-dependent thresholds that are globally optimized in the image domain. We finally demonstrate the potential of the proposed approach through extensive comparisons with state-of-the-art techniques that are specifically tailored to the estimation of {Poisson} intensities. We also present denoising results obtained on real images of low-count fluorescence microscopy.},
author = {Luisier, Florian and Blu, Thierry and Unser, Michael},
doi = {10.1109/TIP.2010.2073477},
file = {:Users/tingruolydia/Documents/PACM/literature/Luisier2011.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Filterbank,Gaussian noise,MSE estimation,{Poisson} noise,image denoising,thresholding,unbiased risk estimate},
number = {3},
pages = {696--708},
pmid = {20840902},
title = {{Image denoising in mixed poissongaussian noise}},
volume = {20},
year = {2011}
}


@article{liu2018pca,
  title={$ e $ PCA: High dimensional exponential family PCA},
  author={Liu, Lydia T and Dobriban, Edgar and Singer, Amit and others},
  journal={The Annals of Applied Statistics},
  volume={12},
  number={4},
  pages={2121--2150},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@article{vonesch2015steerable,
  title={Steerable PCA for rotation-invariant image recognition},
  author={Vonesch, C{\'e}dric and Stauber, Fr{\'e}d{\'e}ric and Unser, Michael},
  journal={SIAM Journal on Imaging Sciences},
  volume={8},
  number={3},
  pages={1857--1873},
  year={2015},
  publisher={SIAM}
}


@article{zhao2018steerable,
  title={Steerable $ e $ PCA},
  author={Zhao, Zhizhen and Liu, Lydia T and Singer, Amit},
  journal={arXiv preprint arXiv:1812.08789},
  year={2018}
}


@article{scheres2005maximum,
  title={Maximum-likelihood multi-reference refinement for electron microscopy images},
  author={Scheres, Sjors HW and Valle, Mikel and Nu{\~n}ez, Rafael and Sorzano, Carlos OS and Marabini, Roberto and Herman, Gabor T and Carazo, Jose-Maria},
  journal={Journal of molecular biology},
  volume={348},
  number={1},
  pages={139--149},
  year={2005},
  publisher={Elsevier}
}


@book{frank2006three,
  title={Three-dimensional electron microscopy of macromolecular assemblies: visualization of biological molecules in their native state},
  author={Frank, Joachim},
  year={2006},
  publisher={Oxford University Press}
}

@article{kam1980reconstruction,
  title={The reconstruction of structure from electron micrographs of randomly oriented particles},
  author={Kam, Zvi},
  journal={Journal of Theoretical Biology},
  volume={82},
  number={1},
  pages={15--39},
  year={1980},
  publisher={Elsevier}
}

@article{zhao2016fast,
  title={Fast steerable principal component analysis},
  author={Zhao, Zhizhen and Shkolnisky, Yoel and Singer, Amit},
  journal={IEEE transactions on computational imaging},
  volume={2},
  number={1},
  pages={1--12},
  year={2016},
  publisher={IEEE}
}

@article{bandeira2017estimation,
  title={Estimation under group actions: recovering orbits from invariants},
  author={Bandeira, Afonso S and Blum-Smith, Ben and Kileel, Joe and Perry, Amelia and Weed, Jonathan and Wein, Alexander S},
  journal={arXiv preprint arXiv:1712.10163},
  year={2017}
}

@article{singer2018mathematics,
  title={Mathematics for cryo-electron microscopy},
  author={Singer, Amit},
  journal={arXiv preprint arXiv:1803.06714},
  year={2018}
}


@inproceedings{hauberg2016dreaming,
  title={Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation},
  author={Hauberg, S{\o}ren and Freifeld, Oren and Larsen, Anders Boesen Lindbo and Fisher, John and Hansen, Lars},
  booktitle={Artificial Intelligence and Statistics},
  pages={342--350},
  year={2016}
}


@article{zhang2018dada,
  title={DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification},
  author={Zhang, Xiaofeng and Wang, Zhangyang and Liu, Dong and Ling, Qing},
  journal={arXiv preprint arXiv:1809.00981},
  year={2018}
}

@inproceedings{tran2017bayesian,
  title={A bayesian data augmentation approach for learning deep models},
  author={Tran, Toan and Pham, Trung and Carneiro, Gustavo and Palmer, Lyle and Reid, Ian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2797--2806},
  year={2017}
}

@inproceedings{sungbin2019FastAA,
 author = {Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
 booktitle = {Advances in Neural Information Processing Systems},
  pages = {6665--6675},
  title = {Fast AutoAugment},
  volume = {32},
 year = {2019}
}

@article{buslaev2018albumentations,
  title={Albumentations: fast and flexible image augmentations},
  author={Buslaev, Alexander and Parinov, Alex and Khvedchenya, Eugene and Iglovikov, Vladimir I and Kalinin, Alexandr A},
  journal={arXiv preprint arXiv:1809.06839},
  year={2018}
}

@article{devries2017dataset,
  title={Dataset augmentation in feature space},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1702.05538},
  year={2017}
}

@article{wang2018low,
  title={Low-Shot Learning from Imaginary Data},
  author={Wang, Yu-Xiong and Girshick, Ross and Hebert, Martial and Hariharan, Bharath},
  journal={arXiv preprint arXiv:1801.05401},
  year={2018}
}


@article{gavish-donoho-2017,
  title={Optimal shrinkage of singular values},
  author={Gavish, Matan and Donoho, David L.},
  journal={IEEE Transactions on Information Theory},
  volume={63},
  number={4},
  pages={2137-2152},
  year={2017}
}

@article{huo2018aggregated,
  title={Aggregated inference},
  author={Huo, Xiaoming and Cao, Shanshan},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  pages={e1451},
  publisher={Wiley Online Library}
}

@article{chen2014subadditivity,
  title={Subadditivity of matrix $\phi$-entropy and concentration of random matrices},
  author={Chen, Richard Y and Tropp, Joel A},
  journal={Electron. J. Probab},
  volume={19},
  number={27},
  pages={1--30},
  year={2014}
}

@book{bertsekas1989parallel,
  title={Parallel and distributed computation: numerical methods},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  volume={23},
  year={1989},
  publisher={Prentice hall Englewood Cliffs, NJ}
}

@article{boyd2011distributed,
  title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
  author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
  journal={Foundations and Trends{\textregistered} in Machine learning},
  volume={3},
  number={1},
  pages={1--122},
  year={2011},
  publisher={Now Publishers, Inc.}
}

@book{lynch1996distributed,
  title={Distributed algorithms},
  author={Lynch, Nancy A},
  year={1996},
  publisher={Elsevier}
}

@book{rauber2013parallel,
  title={Parallel programming: For multicore and cluster systems},
  author={Rauber, Thomas and R{\"u}nger, Gudula},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{blelloch2010parallel,
  title={Parallel algorithms},
  author={Blelloch, Guy E and Maggs, Bruce M},
  booktitle={Algorithms and theory of computation handbook},
  pages={25--25},
  year={2010},
  organization={Chapman \& Hall/CRC}
}

@article{koutris2018algorithmic,
  title={Algorithmic Aspects of Parallel Data Processing},
  author={Koutris, Paraschos and Salihoglu, Semih and Suciu, Dan and others},
  journal={Foundations and Trends{\textregistered} in Databases},
  volume={8},
  number={4},
  pages={239--370},
  year={2018},
  publisher={Now Publishers, Inc.}
}

@inproceedings{liu2014distributed,
  title={Distributed estimation, information loss and exponential families},
  author={Liu, Qiang and Ihler, Alexander T},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1098--1106},
  year={2014}
}

@inproceedings{zinkevich2010parallelized,
  title={Parallelized stochastic gradient descent},
  author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J},
  booktitle={Advances in neural information processing systems},
  pages={2595--2603},
  year={2010}
}

@inproceedings{mcdonald2009efficient,
  title={Efficient large-scale distributed training of conditional maximum entropy models},
  author={Mcdonald, Ryan and Mohri, Mehryar and Silberman, Nathan and Walker, Dan and Mann, Gideon S},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1231--1239},
  year={2009}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: a system for large-scale machine learning.},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={OSDI},
  volume={16},
  pages={265--283},
  year={2016}
}

@Manual{nycflights13,
  title = {nycflights13: Flights that Departed NYC in 2013},
  author = {Hadley Wickham},
  year = {2018},
  note = {R package version 1.0.0},
  url = {https://CRAN.R-project.org/package=nycflights13},
}

@article{muller2016random,
  title={Random Matrix Theory Tutorial--Introduction to Deterministic Equivalents},
  author={M{\"u}ller, Axel and Debbah, M{\'e}rouane},
  journal={TRAITEMENT DU SIGNAL},
  volume={33},
  number={2-3},
  pages={223--248},
  year={2016},
  publisher={PRESSES UNIV GRENOBLE 1041 RUE DES RESIDENCES, GRENOBLE, 38040, FRANCE}
}

@article{peacock2008eigenvalue,
  title={Eigenvalue distributions of sums and products of large random matrices via incremental matrix expansions},
  author={Peacock, Matthew JM and Collings, Iain B and Honig, Michael L},
  journal={IEEE Transactions on Information Theory},
  volume={54},
  number={5},
  pages={2123--2138},
  year={2008},
  publisher={IEEE}
}

@article{couillet2014analysis,
  title={Analysis of the limiting spectral measure of large random matrices of the separable covariance type},
  author={Couillet, Romain and Hachem, Walid},
  journal={Random Matrices: Theory and Applications},
  volume={3},
  number={04},
  pages={1450016},
  year={2014},
  publisher={World Scientific}
}

@phdthesis{lixin2007spectral,
  title={Spectral analysis of large dimentional random matrices},
  author={Zhang, Lixin},
  year={2007}
}

@article{paul2009no,
  title={No eigenvalues outside the support of the limiting empirical spectral distribution of a separable covariance matrix},
  author={Paul, Debashis and Silverstein, Jack W},
  journal={Journal of Multivariate Analysis},
  volume={100},
  number={1},
  pages={37--57},
  year={2009},
  publisher={Elsevier}
}
@book{mardia1979multivariate,
  title={Multivariate analysis},
  author={Mardia, Kanti and Kent, John T and Bibby, John M},
  year={1979},
  publisher={Academic Press}
}
@book{white2012hadoop,
  title={Hadoop: The definitive guide},
  author={White, Tom},
  year={2012},
  publisher={" O'Reilly Media, Inc."}
}

@article{zaharia2010spark,
  title={Spark: Cluster computing with working sets.},
  author={Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  journal={HotCloud},
  volume={10},
  number={10-10},
  pages={95},
  year={2010}
}

@article{dean2008mapreduce,
  title={MapReduce: simplified data processing on large clusters},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  journal={Communications of the ACM},
  volume={51},
  number={1},
  pages={107--113},
  year={2008},
  publisher={ACM}
}

@article{zhang2013communication,
  title={Communication-Efficient Algorithms for Statistical Optimization},
  author={Zhang, Yuchen and Duchi, John C and Wainwright, Martin J},
  journal={Journal of Machine Learning Research},
  volume={14},
  pages={3321--3363},
  year={2013}
}

@inproceedings{zhang2012communication,
  title={Communication-efficient algorithms for statistical optimization},
  author={Zhang, Yuchen and Wainwright, Martin J and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1502--1510},
  year={2012}
}

@article{zhu2018distributed,
  title={Distributed Nonparametric Regression under Communication Constraints},
  author={Zhu, Yuancheng and Lafferty, John},
  journal={arXiv preprint arXiv:1803.01302},
  year={2018}
}

@article{fan2017distributed,
  title={Distributed Estimation of Principal Eigenspaces},
  author={Fan, Jianqing and Wang, Dong and Wang, Kaizheng and Zhu, Ziwei},
  journal={arXiv preprint arXiv:1702.06488},
  year={2017}
}

@article{smith2016cocoa,
  title={CoCoA: A general framework for communication-efficient distributed optimization},
  author={Smith, Virginia and Forte, Simone and Ma, Chenxin and Tak{\'a}c, Martin and Jordan, Michael I and Jaggi, Martin},
  journal={arXiv preprint arXiv:1611.02189},
  year={2016}
}

@article{jordan2016communication,
  title={Communication-efficient distributed statistical inference},
  author={Jordan, Michael I and Lee, Jason D and Yang, Yun},
  journal={arXiv preprint arXiv:1605.07689},
  year={2016}
}

@inproceedings{braverman2016communication,
  title={Communication lower bounds for statistical estimation problems via a distributed data processing inequality},
  author={Braverman, Mark and Garg, Ankit and Ma, Tengyu and Nguyen, Huy L and Woodruff, David P},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={1011--1020},
  year={2016},
  organization={ACM}
}

@article{duchi2014optimality,
  title={Optimality guarantees for distributed statistical estimation},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Zhang, Yuchen},
  journal={arXiv preprint arXiv:1405.0782},
  year={2014}
}

@article{lee2017communication,
  title={Communication-efficient sparse regression},
  author={Lee, Jason D and Liu, Qiang and Sun, Yuekai and Taylor, Jonathan E},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={5},
  pages={1--30},
  year={2017}
}

@article{lin2017distributed,
  title={Distributed learning with regularized least squares},
  author={Lin, Shao-Bo and Guo, Xin and Zhou, Ding-Xuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3202--3232},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{zhang2013divide,
  title={Divide and conquer kernel ridge regression},
  author={Zhang, Yuchen and Duchi, John and Wainwright, Martin},
  booktitle={Conference on Learning Theory},
  pages={592--617},
  year={2013}
}

@article{zhang2015divide,
  title={Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates},
  author={Zhang, Yuchen and Duchi, John and Wainwright, Martin},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={3299--3340},
  year={2015},
  publisher={JMLR. org}
}

@article{rosenblatt2016optimality,
  title={On the optimality of averaging in distributed statistical learning},
  author={Rosenblatt, Jonathan D and Nadler, Boaz},
  journal={Information and Inference: A Journal of the IMA},
  volume={5},
  number={4},
  pages={379--404},
  year={2016},
  publisher={Oxford University Press}
}

@article{battey2018distributed,
  title={Distributed testing and estimation under sparse high dimensional models},
  author={Battey, Heather and Fan, Jianqing and Liu, Han and Lu, Junwei and Zhu, Ziwei},
  journal={The Annals of Statistics},
  volume={46},
  number={3},
  pages={1352--1382},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}


@article{dobriban2016weighted,
  title={Weighted mining of massive collections of $p$-values by convex optimization},
  author={Dobriban, Edgar},
  journal={arXiv preprint arXiv:1603.05334},
  year={2016}
}


@article{jurczak2015spectral,
  title={Spectral analysis of high-dimensional sample covariance matrices with missing observations},
  author={Jurczak, Kamil and Rohde, Angelika},
  journal={arXiv preprint arXiv:1507.01615},
  year={2015}
}

@article{tao2013outliers,
  title={Outliers in the spectrum of iid matrices with bounded rank perturbations},
  author={Tao, Terence},
  journal={Probability Theory and Related Fields},
  volume={155},
  number={1-2},
  pages={231--263},
  year={2013},
  publisher={Springer}
}

@article{erdos2012universality,
  title={Universality of local spectral statistics of random matrices},
  author={Erd{\H{o}}s, L{\'a}szl{\'o} and Yau, Horng-Tzer},
  journal={Bulletin of the American Mathematical Society},
  volume={49},
  number={3},
  pages={377--414},
  year={2012}
}

@book{chang2003hyperspectral,
  title={Hyperspectral imaging: techniques for spectral detection and classification},
  author={Chang, Chein-I},
  volume={1},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{fowler2009compressive,
  title={Compressive-projection principal component analysis.},
  author={Fowler, James E},
  journal={IEEE Transactions on Image Processing},
  volume={18},
  number={10},
  pages={2230--2242},
  year={2009},
  publisher={Citeseer}
}

@book{kay1993fundamentals,
  title={Fundamentals of Statistical Signal Processing: Estimation Theory},
  author={Kay, Steven M},
  volume={3},
  year={1993},
  publisher={Prentice Hall}
}

@article{hartigan1969linear,
  title={Linear bayesian methods},
  author={Hartigan, JA},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  pages={446--454},
  year={1969},
  publisher={JSTOR}
}

@book{mallat2008wavelet,
  title={A wavelet tour of signal processing: the sparse way},
  author={Mallat, Stephane},
  year={2008},
  publisher={Academic press}
}


@incollection{frank1996three,
title = "Chapter 3 - Two-Dimensional Averaging Techniques ",
editor = "Frank, Joachim ",
booktitle = "Three-Dimensional Electron Microscopy of Macromolecular Assemblies ",
publisher = "Academic Press",
edition = "",
address = "Burlington",
year = "1996",
pages = "54 - 125",
isbn = "978-0-12-265040-6",
doi = "http://dx.doi.org/10.1016/B978-012265040-6/50003-5",
url = "http://www.sciencedirect.com/science/article/pii/B9780122650406500035",
author = "Joachim Frank"
}


@article{cai2016minimax,
  title={Minimax rate-optimal estimation of high-dimensional covariance matrices with incomplete data},
  author={Cai, T Tony and Zhang, Anru},
  journal={Journal of Multivariate Analysis},
  volume={150},
  pages={55--74},
  year={2016},
  publisher={Elsevier}
}

@article{candes2009exact,
  title={Exact matrix completion via convex optimization},
  author={Cand{\`e}s, Emmanuel J and Recht, Benjamin},
  journal={Foundations of Computational mathematics},
  volume={9},
  number={6},
  pages={717--772},
  year={2009},
  publisher={Springer}
}

@book{schafer1997analysis,
  title={Analysis of incomplete multivariate data},
  author={Schafer, Joseph L},
  year={1997},
  publisher={CRC press}
}

@book{little2014statistical,
  title={Statistical analysis with missing data},
  author={Little, Roderick JA and Rubin, Donald B},
  year={2014},
  publisher={John Wiley \& Sons}
}


@book{petrov2012sums,
  title={Sums of independent random variables},
  author={Petrov, Valentin},
  volume={82},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{lounici2014high,
  title={High-dimensional covariance matrix estimation with missing observations},
  author={Lounici, Karim},
  journal={Bernoulli},
  volume={20},
  number={3},
  pages={1029--1058},
  year={2014},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{lee2010convergence,
  title={Convergence and prediction of principal component scores in high-dimensional settings},
  author={Lee, Seunggeun and Zou, Fei and Wright, Fred A},
  journal={Annals of statistics},
  volume={38},
  number={6},
  pages={3605},
  year={2010},
  publisher={NIH Public Access}
}

@article{srivastava2013covariance,
  title={Covariance estimation for distributions with 2+epsilon moments},
  author={Srivastava, Nikhil and Vershynin, Roman},
  journal={The Annals of Probability},
  volume={41},
  number={5},
  pages={3081--3111},
  year={2013},
  publisher={Institute of Mathematical Statistics}
}

@book{searle2009variance,
  title={Variance components},
  author={Searle, Shayle R and Casella, George and McCulloch, Charles E},
  volume={391},
  year={2009},
  publisher={John Wiley \& Sons}
}

@article{henderson1975best,
  title={Best linear unbiased estimation and prediction under a selection model},
  author={Henderson, Charles R},
  journal={Biometrics},
  pages={423--447},
  year={1975},
  publisher={JSTOR}
}

@article{robinson1991blup,
  title={That BLUP is a good thing: the estimation of random effects},
  author={Robinson, George K},
  journal={Statistical Science},
  pages={15--32},
  year={1991},
  publisher={JSTOR}
}

@article{singer2013two,
  title={Two-dimensional tomography from noisy projections taken at unknown random directions},
  author={Singer, A and Wu, H-T},
  journal={SIAM Journal on Imaging Sciences},
  volume={6},
  number={1},
  pages={136--175},
  year={2013},
  publisher={SIAM}
}

@article{gavish2014optimal,
  title={Optimal shrinkage of singular values},
  author={Gavish, Matan and Donoho, David L},
  journal={arXiv preprint arXiv:1405.7511},
  year={2014}
}


@article{keshavan2010matrix,
  title={Matrix Completion From a Few Entries},
  author={Keshavan, Raghunandan H and Montanari, Andrea and Oh, Sewoong},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={6},
  pages={2980--2998},
  year={2010},
  publisher={IEEE}
}

@article{bai1998no,
  title={No eigenvalues outside the support of the limiting spectral distribution of large-dimensional sample covariance matrices},
  author={Bai, Zhi-Dong and Silverstein, Jack W},
  journal={The Annals of Probability},
    volume={26},
  number={1},
  pages={316--345},
  year={1998},
  publisher={JSTOR}
}

@article{suryaprakash2015consistency,
  title={Consistency and {MSE} Performance of {MUSIC}-Based {DOA} of a Single Source in White Noise With Randomly Missing Data},
  author={Suryaprakash, Raj Tejas and Nadakuditi, Raj Rao},
  journal={IEEE Transactions on Signal Processing},
  volume={63},
  number={18},
  pages={4756--4770},
  year={2015},
  publisher={IEEE}
}

@article{nadakuditi2014optshrink,
  title={OptShrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage},
  author={Nadakuditi, Raj Rao},
  journal={IEEE Transactions on Information Theory},
  volume={60},
  number={5},
  pages={3002--3018},
  year={2014},
  publisher={IEEE}
}

@inproceedings{keshavan2009matrix,
  title={Matrix completion from a few entries},
  author={Keshavan, Raghunandan H and Oh, Sewoong and Montanari, Andrea},
  booktitle={2009 IEEE International Symposium on Information Theory},
  pages={324--328},
  year={2009},
  organization={IEEE}
}

@article{chapon2012outliers,
  title={The outliers among the singular values of large rectangular random matrices with additive fixed rank deformation},
  author={Chapon, Francois and Couillet, Romain and Hachem, Walid and Mestre, Xavier},
  journal={arXiv preprint arXiv:1207.0471},
  year={2012}
}

@article{capitaine2013additive,
  title={Additive/multiplicative free subordination property and limiting eigenvectors of spiked additive deformations of Wigner matrices and spiked sample covariance matrices},
  author={Capitaine, Mireille},
  journal={Journal of Theoretical Probability},
  volume={26},
  number={3},
  pages={595--648},
  year={2013},
  publisher={Springer}
}


@article{bai2007asymptotics,
  title={On asymptotics of eigenvectors of large sample covariance matrix},
  author={Bai, ZD and Miao, BQ and Pan, GM},
  journal={The Annals of Probability},
  volume={35},
  number={4},
  pages={1532--1572},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@article{el2009concentration,
  title={Concentration of measure and spectra of random matrices: applications to correlation matrices, elliptical distributions and beyond},
  author={El Karoui, Noureddine},
  journal={The Annals of Applied Probability},
  volume={19},
  number={6},
  pages={2362--2405},
  year={2009},
  publisher={Institute of Mathematical Statistics}
}

@article{benaych2012singular,
  title={The singular values and vectors of low rank perturbations of large rectangular random matrices},
  author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal={Journal of Multivariate Analysis},
  volume={111},
  pages={120--135},
  year={2012},
  publisher={Elsevier}
}


@article{donoho2013optimal,
  title={Optimal shrinkage of eigenvalues in the spiked covariance model},
  author={Donoho, David L and Gavish, Matan and Johnstone, Iain M},
  journal={arXiv preprint arXiv:1311.0851},
  year={2013}
}

@article{bhamre2016denoising,
  title={Denoising and covariance estimation of single particle cryo-{EM} images},
  author={Bhamre, Tejal and Zhang, Teng and Singer, Amit},
  journal={Journal of Structural Biology},
  volume={195},
  number={1},
  pages={72--81},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{anden2015covariance,
  title={Covariance estimation using conjugate gradient for 3D classification in Cryo-{EM}},
  author={And{\'e}n, Joakim and Katsevich, Eugene and Singer, Amit},
  booktitle={Biomedical Imaging (ISBI), 2015 IEEE 12th International Symposium on},
  pages={200--204},
  year={2015},
  organization={IEEE}
}

@article{katsevich2015covariance,
  title={Covariance matrix estimation for the Cryo-{EM} heterogeneity problem},
  author={Katsevich, Eugene and Katsevich, Alexander and Singer, Amit},
  journal={SIAM Journal on Imaging Sciences},
  volume={8},
  number={1},
  pages={126--185},
  year={2015},
  publisher={SIAM}
}

@article{lehmann1998theory,
  title={Theory of Point Estimation},
  author={Lehmann, EL and Casella, George},
  journal={Springer Texts in Statistics},
  year={1998},
  publisher={New York, NY: Springer New York}
}


@article{hachem2015survey,
  title={A survey on the eigenvalues local behavior of large complex correlated Wishart matrices},
  author={Hachem, Walid and Hardy, Adrien and Najim, Jamal},
  journal={ESAIM: Proceedings and Surveys},
  volume={51},
  pages={150--174},
  year={2015},
  publisher={EDP Sciences}
}

@article{karoui2008spectrum,
  title={Spectrum estimation for large dimensional covariance matrices using random matrix theory},
  author={El Karoui, Noureddine},
  journal={The Annals of Statistics},
  pages={2757--2790},
  year={2008},
  publisher={JSTOR}
}

@book{muirhead2009aspects,
  title={Aspects of multivariate statistical theory},
  author={Muirhead, Robb J},
  volume={197},
  year={2009},
  publisher={John Wiley \& Sons}
}

@incollection {johnstone2007high,
    AUTHOR = {Johnstone, Iain M.},
     TITLE = {High dimensional statistical inference and random matrices},
 BOOKTITLE = {International {C}ongress of {M}athematicians. {V}ol. {I}},
     PAGES = {307--333},
 PUBLISHER = {Eur. Math. Soc., Z\"urich},
      YEAR = {2007},
}
  
  
  
@article{anderson1963asymptotic,
  title={Asymptotic theory for principal component analysis},
  author={Anderson, Theodore Wilbur},
  journal={Annals of Mathematical Statistics},
  pages={122--148},
  year={1963},
  publisher={JSTOR}
}

@article{srivastava2005some,
  title={Some tests concerning the covariance matrix in high dimensional data},
  author={Srivastava, Mu'ni S},
  journal={Journal of the Japan Statistical Society},
  volume={35},
  number={2},
  pages={251--272},
  year={2005},
  publisher={THE JAPAN STATISTICAL SOCIETY}
}

@article{fisher2012testing,
title = "On testing for an identity covariance matrix when the dimensionality equals or exceeds the sample size ",
 author={Fisher, Thomas J},
journal = "Journal of Statistical Planning and Inference ",
volume = "142",
number = "1",
pages = "312 - 326",
year = "2012"
}

@article{paul2007asymptotics,
  title={Asymptotics of sample eigenstructure for a large dimensional spiked covariance model},
  author={Paul, Debashis},
  journal={Statistica Sinica},
  volume={17},
  number={4},
  pages={1617-1642},
  year={2007}
}

@book{jolliffe2002principal,
  title={Principal component analysis},
  author={Jolliffe, Ian},
  year={2002},
  publisher={Wiley Online Library}
}

@article{bai2002determining,
  title={Determining the number of factors in approximate factor models},
  author={Bai, Jushan and Ng, Serena},
  journal={Econometrica},
  volume={70},
  number={1},
  pages={191--221},
  year={2002},
  publisher={Wiley Online Library}
}
     @inproceedings{BMVC2016_87,
        	title={Wide Residual Networks},
        	author={Sergey Zagoruyko and Nikos Komodakis},
        	year={2016},
        	month={September},
        	pages={87.1-87.12},
        	articleno={87},
        	numpages={12},
        	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
        	publisher={BMVA Press},
        	editor={Richard C. Wilson, Edwin R. Hancock and William A. P. Smith},

        }
@article{price2006principal,
  title={Principal components analysis corrects for stratification in genome-wide association studies},
  author={Price, Alkes L and Patterson, Nick J and Plenge, Robert M and Weinblatt, Michael E and Shadick, Nancy A and Reich, David},
  journal={Nature genetics},
  volume={38},
  number={8},
  pages={904--909},
  year={2006},
  publisher={Nature Publishing Group}
}

@article{zheng2015substitution,
  title={Substitution principle for CLT of linear spectral statistics of high-dimensional sample covariance matrices with applications to hypothesis testing},
  author={Zheng, Shurong and Bai, Zhidong and Yao, Jianfeng},
  journal={The Annals of Statistics},
  volume={43},
  number={2},
  pages={546--591},
  year={2015},
  publisher={Institute of Mathematical Statistics}
}

@article{zheng2015clt,
  title={CLT for linear spectral statistics of large dimensional general Fisher matrices and its applications in high-dimensional data analysis. },
  author={Zheng, Shurong and Bai, Zhidong and Yao, Jianfeng},
  journal={Bernoulli},
  year={2015+},
  note={to appear}
}

@article{li2015testing,
  title={Testing the Sphericity of a covariance matrix when the dimension is much larger than the sample size},
  author={Li, Zeng and Yao, Jianfeng},
  journal={arXiv preprint arXiv:1508.02498},
  year={2015}
}

@article{johnstone2015testing,
  title={Testing in high-dimensional spiked models},
  author={Johnstone, Iain M and Onatski, Alexei},
  journal={arXiv preprint arXiv:1509.07269},
  year={2015}
}

@article{dharmawansa2014local,
  title={Local Asymptotic Normality of the spectrum of high-dimensional spiked F-ratios},
  author={Dharmawansa, Prathapasinghe and Johnstone, Iain M and Onatski, Alexei},
  journal={arXiv preprint arXiv:1411.3875},
  year={2014}
}

@article{li2014hypothesis,
title = "Hypothesis testing for high-dimensional covariance matrices ",
journal = "Journal of Multivariate Analysis ",
author = "Weiming Li and Yingli Qin",
volume = "128",
number = "",
pages = "108 - 119",
year = "2014",
}

@article{paul2014random,
  title={Random matrix theory in statistics: A review},
  author={Paul, Debashis and Aue, Alexander},
  journal={Journal of Statistical Planning and Inference},
  volume={150},
  pages={1--29},
  year={2014},
  publisher={Elsevier}
}

@incollection{cai2014estimating,    
   author = {Cai, T Tony and Ren, Zhao and Zhou, Harrison H},     
   title = {Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation},    
   url = {http://www-stat.wharton.upenn.edu/~tcai/paper/Covariance-Survey.pdf},    
   year = {2014}    
}

@book{bai2008large,
  title={Large dimensional factor analysis},
  author={Bai, Jushan and Ng, Serena},
  year={2008},
  publisher={Now Publishers Inc}
}

@article{bai2012sample,
  title={On sample eigenvalues in a generalized spiked population model},
  author={Bai, Zhidong and Yao, Jianfeng},
  journal={Journal of Multivariate Analysis},
  volume={106},
  pages={167--177},
  year={2012},
  publisher={Elsevier}
}

@book{lehmann2005testing,
  title={Testing statistical hypotheses},
  author={Lehmann, Erich L and Romano, Joseph P},
  year={2005},
  publisher={Springer Science \& Business Media}
}

@book{van1998asymptotic,
  title={Asymptotic statistics},
  author={Van der Vaart, Aad W},
  year={1998},
  publisher={Cambridge University Press}
}

@article{bianchi2011performance,
  title={Performance of statistical tests for single-source detection using random matrix theory},
  author={Bianchi, Pascal and Debbah, Merouane and Ma{\"\i}da, Myl{\`e}ne and Najim, Jamal},
  journal={Information Theory, IEEE Transactions on},
  volume={57},
  number={4},
  pages={2400--2419},
  year={2011},
  publisher={IEEE}
}

@article{nadakuditi2008sample,
  title={Sample eigenvalue based detection of high-dimensional signals in white noise using relatively few samples},
  author={Nadakuditi, Raj Rao and Edelman, Alan},
  journal={Signal Processing, IEEE Transactions on},
  volume={56},
  number={7},
  pages={2625--2638},
  year={2008},
  publisher={IEEE}
}

@article{nadler2008finite,
  title={Finite sample approximation results for principal component analysis: A matrix perturbation approach},
  author={Nadler, Boaz},
  journal={The Annals of Statistics},
  volume={36},
  number={6},
  pages={2791--2817},
  year={2008}
}

@article{kritchman2009non,
  title={Non-parametric detection of the number of signals: Hypothesis testing and random matrix theory},
  author={Kritchman, Shira and Nadler, Boaz},
  journal={Signal Processing, IEEE Transactions on},
  volume={57},
  number={10},
  pages={3930--3941},
  year={2009},
  publisher={IEEE}
}

@article{ahn2013eigenvalue,
  title={Eigenvalue ratio test for the number of factors},
  author={Ahn, Seung C and Horenstein, Alex R},
  journal={Econometrica},
  volume={81},
  number={3},
  pages={1203--1227},
  year={2013},
  publisher={Wiley Online Library}
}

@article{onatski2012asymptotics,
  title={Asymptotics of the principal components estimator of large factor models with weakly influential factors},
  author={Onatski, Alexei},
  journal={Journal of Econometrics},
  volume={168},
  number={2},
  pages={244--258},
  year={2012},
  publisher={Elsevier}
}

@article{bai2012estimation,
  title={Estimation of spiked eigenvalues in spiked models},
  author={Bai, Zhidong and Ding, Xue},
  journal={Random Matrices: Theory and Applications},
  volume={1},
  number={02},
  pages={1150011},
  year={2012},
  publisher={World Scientific}
}

@article{benaych2011eigenvalues,
  title={The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices},
  author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal={Advances in Mathematics},
  volume={227},
  number={1},
  pages={494--521},
  year={2011},
  publisher={Elsevier}
}

@article{Kumar2015limitations,
author = {Krishna Kumar, Siddharth and Feldman, Marcus W. and Rehkopf, David H. and Tuljapurkar, Shripad}, 
title = {Limitations of {GCTA} as a solution to the missing heritability problem},
year = {2015}, 
journal = {Proceedings of the National Academy of Sciences} 
}

@article{dobriban2015high,
  title={High-Dimensional Asymptotics of Prediction: Ridge Regression and Classification},
  author={Dobriban, Edgar and Wager, Stefan},
  journal={arXiv preprint arXiv:1507.03003},
  year={2015}
}

@article{onatski2009testing,
  title={Testing hypotheses about the number of factors in large factor models},
  author={Onatski, Alexei},
  journal={Econometrica},
  volume={77},
  number={5},
  pages={1447--1479},
  year={2009},
  publisher={Wiley Online Library}
}

@book{hald1998history,
  title={A History of Mathematical Statistics from 1750 to 1930},
  author={Hald, Anders},
  year={1998},
  publisher={Wiley-Interscience}
}

@book{groetsch1977generalized,
  title={Generalized inverses of linear operators: representation and approximation},
  author={Groetsch, Charles W},
    publisher={Marcel Dekker},
  year={1977}
}

@book{bogachev2007measure,
  title={Measure theory},
  author={Bogachev, Vladimir I},
  volume={1},
  year={2007},
  publisher={Springer Science \& Business Media}
}

@article{barber2015controlling,
author = {Barber, Rina Foygel and Cand\`es, Emmanuel J.},
fjournal = {The Annals of Statistics},
journal = {Ann. Statist.},
title={Controlling the false discovery rate via knockoffs},
month = {10},
number = {5},
pages = {2055--2085},
volume = {43},
year = {2015}
}


@article{zumbach2011empirical,
  title={Empirical properties of large covariance matrices},
  author={Zumbach, Gilles},
  journal={Quantitative Finance},
  volume={11},
  number={7},
  pages={1091--1102},
  year={2011},
  publisher={Taylor \& Francis}
}

@incollection{bouchaud2009financial,
  title={Financial applications of random matrix theory: a short review},
  author={Bouchaud, Jean-Philippe and Potters, Marc},
  editor      = {Akemann, Gernot and Baik, Jinho and Di Francesco, Phillippe},
  booktitle   = {The Oxford Handbook of Random Matrix Theory},
  publisher   = {Oxford University Press},
  year        = {2011},
}

@article{bryc2013separation,
  title={Separation of the largest eigenvalues in eigenanalysis of genotype data from discrete subpopulations},
  author={Bryc, Katarzyna and Bryc, Wlodek and Silverstein, Jack W},
  journal={Theoretical Population Biology},
  volume={89},
  pages={34--43},
  year={2013},
  publisher={Elsevier}
}

@article{patterson2006population,
  title={Population structure and eigenanalysis},
  author={Patterson, N and Price, AL and Reich, D},
  journal={PLoS Genet},
  volume={2},
  number={12},
  pages={e190},
  year={2006}
}

@book{rudin1987real,
  title={Real and complex analysis},
  author={Rudin, Walter},
  year={1987},
  publisher={McGraw-Hill Education}
}

@article{wang2013sphericity,
  title={On the sphericity test with large-dimensional observations},
  author={Wang, Qinwen and Yao, Jianfeng},
  journal={Electronic Journal of Statistics},
  volume={7},
  pages={2164--2192},
  year={2013},
  publisher={Institute of Mathematical Statistics}
}

@article{wang2014note,
  title={A note on the {CLT} of the {LSS} for sample covariance matrix from a spiked population model},
  author={Wang, Qinwen and Silverstein, Jack W and Yao, Jianfeng},
  journal={Journal of Multivariate Analysis},
  volume={130},
  pages={194--207},
  year={2014},
  publisher={Elsevier}
}

@article{choi2015regularized,
  title={Regularized {LRT} for Large Scale Covariance Matrices: One Sample Problem},
  author={Choi, Young-Geun and Ng, Chi Tim and Lim, Johan},
  journal={arXiv preprint arXiv:1502.00384},
  year={2015}
}

@book{kress2013linear,
  title={Linear Integral Equations},
  author={Kress, Rainer},
  year={2013},
  publisher={Springer}
}

@article{cai2013optimal,
  title={Optimal hypothesis testing for high dimensional covariance matrices},
  author={Cai, T Tony and Ma, Zongming},
  journal={Bernoulli},
  volume={19},
  number={5B},
  pages={2359--2388},
  year={2013},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{johnstone2001distribution,
  title={On the distribution of the largest eigenvalue in principal components analysis},
  author={Johnstone, Iain M},
  journal={Annals of Statistics},
    volume={29},
  number={2},
  pages={295--327},
  year={2001},
  publisher={JSTOR}
}

@article{fisher2010new,
  title={A new test for sphericity of the covariance matrix for high dimensional data},
  author={Fisher, Thomas J and Sun, Xiaoqian and Gallagher, Colin M},
  journal={Journal of Multivariate Analysis},
  volume={101},
  number={10},
  pages={2554--2570},
  year={2010},
  publisher={Elsevier}
}


@article{nagao1973some,
  title={On some test criteria for covariance matrix},
  author={Nagao, Hisao},
  journal={The Annals of Statistics},
    volume={1},
  number={4},
  pages={700--709},
  year={1973},
  publisher={JSTOR}
}

@article{baik2006eigenvalues,
  title={Eigenvalues of large sample covariance matrices of spiked population models},
  author={Baik, Jinho and Silverstein, Jack W},
  journal={Journal of Multivariate Analysis},
  volume={97},
  number={6},
  pages={1382--1408},
  year={2006},
  publisher={Elsevier}
}

@article{john1971optimal,
author = {John, S.}, 
title = {Some optimal multivariate tests},
volume = {58}, 
number = {1}, 
pages = {123-127},
year = {1971}, 
journal = {Biometrika} 
}


@article{baik2005phase,
  title={Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  author={Baik, Jinho and Ben Arous, G{\'e}rard and P{\'e}ch{\'e}, Sandrine},
  journal={Annals of Probability},
  volume = {33}, 
  number = {5}, 
  pages={1643--1697},
  year={2005},
  publisher={JSTOR}
}

@article{mauchly1940significance,
  title={Significance test for sphericity of a normal n-variate distribution},
  author={Mauchly, John W},
  journal={The Annals of Mathematical Statistics},
  volume={11},
  number={2},
  pages={204--209},
  year={1940},
  publisher={JSTOR}
}

@article{onatski2013asymptotic,
  title={Asymptotic power of sphericity tests for high-dimensional data},
  author={Onatski, Alexei and Moreira, Marcelo J and Hallin, Marc},
  journal={The Annals of Statistics},
  volume={41},
  number={3},
  pages={1204--1231},
  year={2013},
  publisher={Institute of Mathematical Statistics}
}


@article{onatski2014signal,
  title={Signal detection in high dimension: The multispiked case},
  author={Onatski, Alexei and Moreira, Marcelo J and Hallin, Marc},
  journal={The Annals of Statistics},
  volume={42},
  number={1},
  pages={225--254},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}


@article{bai2004clt,
  title={{CLT} for linear spectral statistics of large-dimensional sample covariance matrices},
  author={Bai, Zhidong and Silverstein, Jack W},
  journal={The Annals of Probability},
  volume={32},
  number={1A},
  pages={553--605},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}


@article{bai2009corrections,
  title={Corrections to {LRT} on large-dimensional covariance matrix by {RMT}},
  author={Bai, Zhidong and Jiang, Dandan and Yao, Jian-Feng and Zheng, Shurong},
  journal={The Annals of Statistics},
  pages={3822--3840},
    Number = {6B},
  Volume = {37},
    year={2009},
  publisher={JSTOR}
}
 
@article{chen2010tests,
  title={Tests for high-dimensional covariance matrices},
  author={Chen, Song Xi and Zhang, Li-Xin and Zhong, Ping-Shou},
  journal={Journal of the American Statistical Association},
  volume={105}, 
  number={490},
    pages={810--819},
  year={2010}
}


@article{ledoit2002some,
  title={Some hypothesis tests for the covariance matrix when the dimension is large compared to the sample size},
  author={Ledoit, Olivier and Wolf, Michael},
  journal={Annals of Statistics},
  pages={1081--1102},
  year={2002},
      Number = {4},
  Volume = {30},
  publisher={JSTOR}
}


@article{ehrenfeucht1989general,
  title={A general lower bound on the number of examples needed for learning},
  author={Ehrenfeucht, Andrzej and Haussler, David and Kearns, Michael and Valiant, Leslie},
  journal={Information and Computation},
  volume={82},
  number={3},
  pages={247--261},
  year={1989},
  publisher={Elsevier}
}

@inproceedings{wang2012baselines,
  title={Baselines and bigrams: Simple, good sentiment and topic classification},
  author={Wang, Sida and Manning, Christopher D},
  booktitle={Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2},
  pages={90--94},
  year={2012},
  organization={Association for Computational Linguistics}
}

@article{candes2011robust,
  title={Robust principal component analysis?},
  author={Cand{\`e}s, Emmanuel J and Li, Xiaodong and Ma, Yi and Wright, John},
  journal={Journal of the ACM (JACM)},
  volume={58},
  number={3},
  pages={11},
  year={2011},
  publisher={ACM}
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
 booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2015}
}

@article{rifai2011manifold,
  title={The manifold tangent classifier},
  author={Rifai, Salah and Dauphin, Yann and Vincent, Pascal and Bengio, Yoshua and Muller, Xavier},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  pages={2294--2302},
  year={2011}
}

@article{simard2000transformation,
  title={Transformation invariance in pattern recognition: Tangent distance and propagation},
  author={Simard, Patrice Y and Le Cun, Yann A and Denker, John S and Victorri, Bernard},
  journal={International Journal of Imaging Systems and Technology},
  volume={11},
  number={3},
  pages={181--197},
  year={2000}
}

@article{wray2007prediction,
  title={Prediction of individual genetic risk to disease from genome-wide association studies},
  author={Wray, Naomi R and Goddard, Michael E and Visscher, Peter M},
  journal={Genome research},
  volume={17},
  number={10},
  pages={1520--1528},
  year={2007},
  publisher={Cold Spring Harbor Lab}
}

@article{russakovsky2014imagenet,
  title={Image{N}et large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International Journal of Computer Vision},
  pages={1--42},
  year={2014},
  publisher={Springer}
}

@article{kleinberg2015prediction,
  title={Prediction Policy Problems},
  author={Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Obermeyer, Ziad and others},
  journal={American Economic Review},
  volume={105},
  number={5},
  pages={491--95},
  year={2015},
  publisher={American Economic Association}
}

@book{tao2012topics,
  title={Topics in Random Matrix Theory},
  author={Tao, Terence},
  volume={132},
  year={2012},
  publisher={American Mathematical Soc.}
}

@book{anderson2010introduction,
  title={An Introduction to Random Matrices},
  author={Anderson, Greg W and Guionnet, Alice and Zeitouni, Ofer},
  number={118},
  year={2010},
  publisher={Cambridge University Press}
}

@article{witten2009covariance,
  title={Covariance-regularized regression and classification for high dimensional problems},
  author={Witten, Daniela M and Tibshirani, Robert},
  journal={J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  volume={71},
  number={3},
  pages={615--636},
  year={2009},
  publisher={Wiley Online Library}
}

@article{international2005haplotype,
  title={A haplotype map of the human genome},
  author={{International HapMap Consortium}},
  journal={Nature},
  volume={437},
  number={7063},
  pages={1299--1320},
  year={2005},
  publisher={Nature Publishing Group}
}

@article{bernau2015cross-study,
author = {Bernau, Christoph and Riester, Markus and Boulesteix, Anne-Laure and Parmigiani, Giovanni and Huttenhower, Curtis and Waldron, Levi and Trippa, Lorenzo}, 
title = {Cross-study validation for the assessment of prediction algorithms},
volume = {30}, 
number = {12}, 
pages = {i105-i112}, 
year = {2014}, 
journal = {Bioinformatics} 
}

@book{cox1989analysis,
  title={Analysis of {B}inary {D}ata},
  author={Cox, David Roxbee and Snell, E Joyce},
  year={1989},
  publisher={CRC Press},
  edition={2nd}
}

@article{hoerl1970ridge,
  title={Ridge regression: Biased estimation for nonorthogonal problems},
  author={Hoerl, Arthur E and Kennard, Robert W},
  journal={Technometrics},
  volume={12},
  number={1},
  pages={55--67},
  year={1970},
  publisher={Taylor \& Francis Group}
}

@article{sutton2006introduction,
  title={An introduction to conditional random fields for relational learning},
  author={Sutton, Charles and McCallum, Andrew},
  journal={Introduction to statistical relational learning},
  pages={93--128},
  year={2006},
  publisher={MIT press}
}

@inproceedings{toutanova2003feature,
  title={Feature-rich part-of-speech tagging with a cyclic dependency network},
  author={Toutanova, Kristina and Klein, Dan and Manning, Christopher D and Singer, Yoram},
  booktitle={NAACL},
  year={2003}
}

@book{hastie2015statistical,
  title={Statistical Learning with Sparsity: The Lasso and Generalizations},
  author={Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
  year={2015},
  publisher={CRC Press}
}

@article{rubio2012performance,
  title={Performance analysis and optimal selection of large minimum variance portfolios under estimation risk},
  author={Rubio, Francisco and Mestre, Xavier and Palomar, Daniel P},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={6},
  number={4},
  pages={337--350},
  year={2012},
  publisher={IEEE}
}

@article{rubio2011spectral,
  title={Spectral convergence for a general class of random matrices},
  author={Rubio, Francisco and Mestre, Xavier},
  journal={Statistics \& Probability Letters},
  volume={81},
  number={5},
  pages={592--602},
  year={2011},
  publisher={Elsevier}
}

@article{zhang2013finite,
  title={Finite-sample linear filter optimization in wireless communications and financial systems},
  author={Zhang, Mengyi and Rubio, Francisco and Palomar, Daniel P and Mestre, Xavier},
  journal={IEEE Trans. Signal Process.},
  volume={61},
  number={20},
  pages={5014--5025},
  year={2013},
  publisher={IEEE}
}

@article{hachem2008new,
  title={A new approach for mutual information analysis of large dimensional multi-antenna channels},
  author={Hachem, Walid and Khorunzhiy, Oleksiy and Loubaton, Philippe and Najim, Jamal and Pastur, Leonid},
  journal={IEEE Trans. Inform. Theory},
  volume={54},
  number={9},
  pages={3987--4004},
  year={2008},
  publisher={IEEE}
}

@article{hachem2007deterministic,
  title={Deterministic equivalents for certain functionals of large random matrices},
  author={Hachem, Walid and Loubaton, Philippe and Najim, Jamal},
  journal={The Annals of Applied Probability},
  volume={17},
  number={3},
  pages={875--930},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}


@article{raudys1967determining,
  title={On determining training sample size of linear classifier},
  author={Raudys, {\v{S}}ar{\=u}nas},
  journal={Comput. Systems (in Russian)},
  volume={28},
  pages={79–87},
  year={1967}
}

@book{raudys2012statistical,
  title={Statistical and Neural Classifiers: An integrated approach to design},
  author={Raudys, {\v{S}}ar{\=u}nas},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{raudys1995small,
  title={Small sample properties of ridge estimate of the covariance matrix in statistical and neural net classification},
  author={Raudys, {\v{S}}ar{\=u}nas and Skurichina, Marina},
  journal={New Trends in Probability and Statistics},
  volume={3},
  pages={237--245},
  year={1995}
}


@inproceedings{liang2010interaction,
  title={On the interaction between norm and dimensionality: Multiple regimes in learning},
  author={Liang, Percy and Srebro, Nati},
  booktitle={ICML},
  year={2010}
}

@article{donoho2013high,
  title={High dimensional robust {M}-estimation: Asymptotic variance via approximate message passing},
  author={Donoho, David L and Montanari, Andrea},
  journal={arXiv preprint arXiv:1310.7320},
  year={2013}
}

@article{zollanvari2013kolmogorov,
  title={On {K}olmogorov asymptotics of estimators of the misclassification error rate in linear discriminant analysis},
  author={Zollanvari, Amin and Genton, Marc G},
  journal={Sankhya A},
  volume={75},
  number={2},
  pages={300--326},
  year={2013},
  publisher={Springer}
}

@inproceedings{zollanvari2013application,
  title={Application of double asymptotics and random matrix theory in error estimation of regularized linear discriminant analysis},
  author={Zollanvari, Amin and Dougherty, Edward R},
  booktitle={2013 IEEE Global Conference on Signal and Information Processing},
  year={2013}
}

@article{zollanvari2015generalized,
  title={Generalized Consistent Error Estimator of Linear Discriminant Analysis},
  author={Zollanvari, Amin and Dougherty, Edward R},
  journal={IEEE Trans. Signal Process.},
  volume={63},
  number={11},
  year={2015}
}

@article{zollanvari2011analytic,
  title={Analytic study of performance of error estimators for linear discriminant analysis},
  author={Zollanvari, Amin and Braga-Neto, Ulisses M and Dougherty, Edward R},
  journal={IEEE Trans. Signal Process.},
  volume={59},
  number={9},
  pages={4238--4255},
  year={2011},
  publisher={IEEE}
}

@article{li2015two,
  title={On two simple and effective procedures for high dimensional classification of general populations},
  author={Li, Zhaoyuan and Yao, Jianfeng},
  journal={Statist. Papers},
  pages={1--25},
  year={2015},
  publisher={Springer}
}

@article{raudys1972amount,
  title={On the amount of a priori information in designing the classification algorithm},
  author={Raudys, {\v{S}}ar{\=u}nas},
  journal={Technical Cybernetics (in Russian)},
  pages={168--174},
  year={1972},
  volume={4},
}

@article{fujikoshi1985selection,
  title={Selection of variables in discriminant analysis and canonical correlation analysis},
  author={Fujikoshi, Yasunori},
  journal={Multivariate Analysis-VI},
  pages={219--236},
  year={1985},
  publisher={North-Holland Amsterdam}
}

@article{kubokawa2013asymptotic,
  title={Asymptotic expansion and estimation of {EPMC} for linear classification rules in high dimension},
  author={Kubokawa, Tatsuya and Hyodo, Masashi and Srivastava, Muni S},
  journal={J. Multivariate Anal.},
  volume={115},
  pages={496--515},
  year={2013},
  publisher={Elsevier}
}

@article{fujikoshi1998asymptotic,
  title={Asymptotic aproximations for {EPMC}s of the linear and the quadratic discriminant functions when the sample sizes and the dimension are large},
  author={Fujikoshi, Yasunori and Seo, Takashi},
  journal={Random Oper. Stoch. Equ.},
  volume={6},
  number={3},
  pages={269--280},
  year={1998}
}

@book{fujikoshi2011multivariate,
  title={Multivariate Statistics: High-dimensional and Large-sample Approximations},
  author={Fujikoshi, Yasunori and Ulyanov, Vladimir V and Shimizu, Ryoichi},
  year={2011},
  publisher={John Wiley \& Sons}
}


@article{meshalkin1979errors,
  title={Errors in the classification of multi-variate observations},
  author={Meshalkin, LD and Serdobolskii, VI},
  journal={Theory of Probability \& Its Applications},
  volume={23},
  number={4},
  pages={741--750},
  year={1979},
  publisher={SIAM}
}


@inproceedings{serdobolskii1980discriminant,
  title={Discriminant analysis for a large number of variables},
  author={Serdobolskii, Vadim Ivanovich},
  booktitle={Dokl. Akad. Nauk SSSR},
  volume={22},
  pages={314--319},
  year={1980}
}
 
@inproceedings{serdobolskii1983minimum,
  title={On minimum error probability in discriminant analysis},
  author={Serdobolskii, Vadim Ivanovich},
  booktitle={Dokl. Akad. Nauk SSSR},
  volume={27},
  pages={720--725},
  year={1983}
}

@inproceedings{deev1970representation,
  title={Representation of statistics of discriminant analysis and asymptotic expansion when space dimensions are comparable with sample size},
  author={Deev, AD},
  booktitle={Sov. Math. Dokl.},
  volume={11},
  pages={1547--1550},
  year={1970}
}

@article{raudys2004results,
  title={Results in statistical discriminant analysis: A review of the former {S}oviet {U}nion literature},
  author={Raudys, {\v{S}}ar{\=u}nas and Young, Dean M},
  journal={J. Multivariate Anal.},
  volume={89},
  number={1},
  pages={1--35},
  year={2004},
  publisher={Elsevier}
}

@book{serdobolskii2007multiparametric,
  title={Multiparametric {S}tatistics},
  author={Serdobolskii, Vadim Ivanovich},
  year={2007},
  publisher={Elsevier}
}

@article{dobriban2015efficient,
author = {Dobriban, Edgar},
title = {Efficient computation of limit spectra of sample covariance matrices},
journal = {Random Matrices: Theory and Applications},
volume = {04},
number = {04},
pages = {1550019},
year = {2015},
} 

@article{silverstein1992signal,
  Author = {Silverstein, Jack W and Combettes, Patrick L},
  Journal = {IEEE Trans. Signal Process.},
  Number = {8},
  Pages = {2100--2105},
  Publisher = {IEEE},
  Title = {Signal detection via spectral theory of large dimensional random matrices},
  Volume = {40},
  Year = {1992}}

@inproceedings{pang2002thumbs,
  Author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
  Booktitle = {EMNLP},
  Organization = {Association for Computational Linguistics},
  Title = {Thumbs up?: Sentiment classification using machine learning techniques},
  Year = {2002}}

@book{joachims2002learning,
  Author = {Joachims, Thorsten},
  Publisher = {Kluwer Academic Publishers},
  Title = {Learning to classify text using support vector machines: Methods, theory and algorithms},
  Year = {2002}}

@article{goldstein2009common,
  Author = {Goldstein, David B},
  Journal = {New England Journal of Medicine},
  Number = {17},
  Pages = {1696},
  Title = {Common genetic variation and human traits},
  Volume = {360},
  Year = {2009}}

@article{10002012integrated,
  Author = {{1000 Genomes Consortium}},
  Journal = {Nature},
  Number = {7422},
  Pages = {56--65},
  Publisher = {Nature Publishing Group},
  Title = {An integrated map of genetic variation from 1,092 human genomes},
  Volume = {491},
  Year = {2012}}

@article{tulino2004random,
  Author = {Tulino, Antonio M and Verd{\'u}, Sergio},
  Journal = {Communications and Information theory},
  Number = {1},
  Pages = {1--182},
  Publisher = {Now Publishers Inc.},
  Title = {Random matrix theory and wireless communications},
  Volume = {1},
  Year = {2004}}

@book{couillet2011random,
  Author = {Couillet, Romain and Debbah, Merouane},
  Publisher = {Cambridge University Press},
  location={ Cambridge, MA},
  Title = {Random {M}atrix {M}ethods for {W}ireless {C}ommunications},
  Year = {2011}}

@article{pickrell2012inference,
  Author = {Pickrell, Joseph K and Pritchard, Jonathan K},
  Journal = {PLoS genetics},
  Number = {11},
  Pages = {e1002967},
  Publisher = {Public Library of Science},
  Title = {Inference of population splits and mixtures from genome-wide allele frequency data},
  Volume = {8},
  Year = {2012}}

@incollection{NIPS2013_4921,
  Author = {Bartz, Daniel and M\"{u}ller, Klaus-Robert},
  Booktitle = {NIPS},
  Title = {Generalizing Analytic Shrinkage for Arbitrary Covariance Structures},
  Year = {2013}}

@article{Ledoit2015,
  Author = {Olivier Ledoit and Michael Wolf},
  Journal = {J. Multivariate Anal.},
  Title = {Spectrum estimation: A unified framework for covariance matrix estimation and {PCA} in large dimensions},
  Pages = {360--384},
  Volume = {139},
  Year = {2015}}

@article{tibshirani2002diagnosis,
  Author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan, Balasubramanian and Chu, Gilbert},
  Journal = {Proc. Natl. Acad. Sci. USA},
  Number = {10},
  Pages = {6567--6572},
  Publisher = {National Acad Sciences},
  Title = {Diagnosis of multiple cancer types by shrunken centroids of gene expression},
  Volume = {99},
  Year = {2002}}

@article{horvath2014obesity,
  Author = {Horvath, Steve and Erhart, Wiebke and Brosch, Mario and Ammerpohl, Ole and von Schönfels, Witigo and Ahrens, Markus and Heits, Nils and Bell, Jordana T. and Tsai, Pei-Chien and Spector, Tim D. and Deloukas, Panos and Siebert, Reiner and Sipos, Bence and Becker, Thomas and Röcken, Christoph and Schafmayer, Clemens and Hampe, Jochen},
  Journal = {Proc. Natl. Acad. Sci. USA},
  Number = {43},
  Pages = {15538--15543},
  Publisher = {National Acad Sciences},
  Title = {Obesity accelerates epigenetic aging of human liver},
  Volume = {111},
  Year = {2014}}

@article{horvath2013dna,
  Author = {Horvath, Steve},
  Journal = {Genome biology},
  Number = {10},
  Pages = {R115},
  Publisher = {BioMed Central Ltd},
  Title = {{DNA} methylation age of human tissues and cell types},
  Volume = {14},
  Year = {2013}}

@article{Golub15101999,
  Author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S},
  Doi = {10.1126/science.286.5439.531},
  Journal = {Science},
  Number = {5439},
  Pages = {531-537},
  Title = {Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring},
  Volume = {286},
  Year = {1999}}

@article{trippa2015bayesian,
  Author = {Trippa, Lorenzo and Waldron, Levi and Huttenhower, Curtis and Parmigiani, Giovanni},
  Date-Modified = {2015-05-28 22:18:53 +0000},
  Journal = {The Annals of Applied Statistics},
  Number = {1},
  Pages = {402--428},
  Publisher = {Institute of Mathematical Statistics},
  Title = {Bayesian nonparametric cross-study validation of prediction methods},
  Volume = {9},
  Year = {2015}}

@article{Bernau15062014,
  Abstract = {Motivation: Numerous competing algorithms for prediction in high-dimensional settings have been developed in the statistical and machine-learning literature. Learning algorithms and the prediction models they generate are typically evaluated on the basis of cross-validation error estimates in a few exemplary datasets. However, in most applications, the ultimate goal of prediction modeling is to provide accurate predictions for independent samples obtained in different settings. Cross-validation within exemplary datasets may not adequately reflect performance in the broader application context.Methods: We develop and implement a systematic approach to `cross-study validation', to replace or supplement conventional cross-validation when evaluating high-dimensional prediction models in independent datasets. We illustrate it via simulations and in a collection of eight estrogen-receptor positive breast cancer microarray gene-expression datasets, where the objective is predicting distant metastasis-free survival (DMFS). We computed the C-index for all pairwise combinations of training and validation datasets. We evaluate several alternatives for summarizing the pairwise validation statistics, and compare these to conventional cross-validation.Results: Our data-driven simulations and our application to survival prediction with eight breast cancer microarray datasets, suggest that standard cross-validation produces inflated discrimination accuracy for all algorithms considered, when compared to cross-study validation. Furthermore, the ranking of learning algorithms differs, suggesting that algorithms performing best in cross-validation may be suboptimal when evaluated through independent validation.Availability: The survHD: Survival in High Dimensions package (http://www.bitbucket.org/lwaldron/survhd) will be made available through Bioconductor.Contact: levi.waldron@hunter.cuny.eduSupplementary information: Supplementary data are available at Bioinformatics online.},
  Author = {Bernau, Christoph and Riester, Markus and Boulesteix, Anne-Laure and Parmigiani, Giovanni and Huttenhower, Curtis and Waldron, Levi and Trippa, Lorenzo},
  Doi = {10.1093/bioinformatics/btu279},
  Eprint = {http://bioinformatics.oxfordjournals.org/content/30/12/i105.full.pdf+html},
  Journal = {Bioinformatics},
  Number = {12},
  Pages = {i105-i112},
  Title = {Cross-study validation for the assessment of prediction algorithms},
  Url = {http://bioinformatics.oxfordjournals.org/content/30/12/i105.abstract},
  Volume = {30},
  Year = {2014},
  Bdsk-Url-1 = {http://bioinformatics.oxfordjournals.org/content/30/12/i105.abstract},
  Bdsk-Url-2 = {http://dx.doi.org/10.1093/bioinformatics/btu279}}

@book{lynch1998genetics,
  Author = {Lynch, Michael and Walsh, Bruce and others},
  Publisher = {Sinauer Sunderland},
  Title = {Genetics and analysis of quantitative traits},
  Volume = {1},
  Year = {1998}}

@article{bayati2012lasso,
  Author = {Bayati, Mohsen and Montanari, Andrea},
  Journal = {IEEE Trans. Inform. Theory},
  Number = {4},
  Pages = {1997--2017},
  Publisher = {IEEE},
  Title = {The {LASSO} risk for {G}aussian matrices},
  Volume = {58},
  Year = {2012}}

@article{bartlett2003rademacher,
  Author = {Bartlett, Peter L and Mendelson, Shahar},
  Journal = {J. Mach. Learn. Res.},
  Pages = {463--482},
  Publisher = {JMLR. org},
  Title = {Rademacher and {G}aussian complexities: Risk bounds and structural results},
  Volume = {3},
  Year = {2003}}

@article{hsu2014random,
  Author = {Hsu, Daniel and Kakade, Sham M and Zhang, Tong},
  Journal = {Found. Comput. Math.},
  Number = {3},
  Pages = {569--600},
  Publisher = {Springer},
  Title = {Random design analysis of ridge regression},
  Volume = {14},
  Year = {2014}}

@article{dicker2014ridge,
  Author = {Dicker, Lee},
  Journal = {Bernoulli, to appear},
  Title = {Ridge regression and asymptotic minimax estimation over spheres of growing dimension},
  Year = {2014}}

@article{dicker2013optimal,
  Author = {Dicker, Lee},
  Journal = {Electron. J. Stat.},
  Pages = {1806--1834},
  Publisher = {Institute of Mathematical Statistics},
  Title = {Optimal equivariant prediction for high-dimensional linear models with arbitrary predictor covariance},
  Volume = {7},
  Year = {2013}}

@article{karoui2013asymptotic,
  Author = {El Karoui, Noureddine},
  Journal = {arXiv preprint arXiv:1311.2445},
  Title = {Asymptotic behavior of unregularized and ridge-regularized high-dimensional robust regression estimators: rigorous results},
  Year = {2013}}

@article{karoui2011geometric,
  Author = {El Karoui, Noureddine and K{\"o}sters, Holger},
  Journal = {arXiv preprint arXiv:1105.1404},
  Title = {Geometric sensitivity of random matrix results: consequences for shrinkage estimators of covariance and related statistical methods},
  Year = {2011}}

@article{bean2013optimal,
  Author = {Bean, Derek and Bickel, Peter J and El Karoui, Noureddine and Yu, Bin},
  Journal = {Proc. Natl. Acad. Sci. USA},
  Number = {36},
  Pages = {14563--14568},
  Publisher = {National Acad Sciences},
  Title = {Optimal {M}-estimation in high-dimensional regression},
  Volume = {110},
  Year = {2013}}

@article{el2013robust,
  Author = {El Karoui, Noureddine and Bean, Derek and Bickel, Peter J and Lim, Chinghway and Yu, Bin},
  Journal = {Proc. Natl. Acad. Sci. USA},
  Number = {36},
  Pages = {14557--14562},
  Publisher = {National Acad Sciences},
  Title = {On robust regression with high-dimensional predictors},
  Volume = {110},
  Year = {2013}}

@article{couillet2011deterministic,
  Author = {Couillet, Romain and Debbah, M{\'e}rouane and Silverstein, Jack W},
  Journal = {IEEE Trans. Inform. Theory},
  Number = {6},
  Pages = {3493--3514},
  Publisher = {IEEE},
  Title = {A deterministic equivalent for the analysis of correlated MIMO multiple access channels},
  Volume = {57},
  Year = {2011}}

@book{bai2009spectral,
  Author = {Bai, Zhidong and Silverstein, Jack W},
  Publisher = {Springer},
  Series = {Springer Series in Statistics},
  Title = {Spectral analysis of large dimensional random matrices},
  Year = {2009}}

@article{chen2011regularized,
  Author = {Chen, Lin S and Paul, Debashis and Prentice, Ross L and Wang, Pei},
  Journal = {J. Amer. Statist. Assoc.},
  Number = {496},
  Title = {A regularized {Hotelling's} {$T^2$} test for pathway analysis in proteomic studies},
  Volume = {106},
  Year = {2011}}

@inproceedings{sabato2010tight,
  Author = {Sabato, Sivan and Srebro, Nathan and Tishby, Naftali},
  Booktitle = {NIPS},
  Title = {Tight sample complexity of large-margin learning},
  Year = {2010}}

@unpublished{liang2015statistical,
  Author = {Percy Liang},
  Title = {Statistical Learning Theory},
  Url = {https://web.stanford.edu/class/cs229t/notes.pdf},
  Year = {2015},
  Bdsk-Url-1 = {https://web.stanford.edu/class/cs229t/notes.pdf}}

@article{koltchinskii2006local,
  Author = {Koltchinskii, Vladimir and others},
  Journal = {Ann. Statist.},
  Number = {6},
  Pages = {2593--2656},
  Publisher = {Institute of Mathematical Statistics},
  Title = {Local Rademacher complexities and oracle inequalities in risk minimization},
  Volume = {34},
  Year = {2006}}

@article{bartlett2005local,
  Author = {Bartlett, Peter L and Bousquet, Olivier and Mendelson, Shahar},
  Journal = {Ann. Statist.},
  Pages = {1497--1537},
  Publisher = {JSTOR},
  Title = {Local rademacher complexities},
  Year = {2005}}

@article{cai2011direct,
  Author = {Cai, Tony and Liu, Weidong},
  Journal = {J. Amer. Statist. Assoc.},
  Number = {496},
  Title = {A direct estimation approach to sparse linear discriminant analysis},
  Volume = {106},
  Year = {2011}}

@article{silverstein1995strong,
  Author = {Silverstein, Jack W},
  Journal = {J. Multivariate Anal.},
  Number = {2},
  Pages = {331--339},
  Publisher = {Elsevier},
  Title = {Strong convergence of the empirical distribution of eigenvalues of large dimensional random matrices},
  Volume = {55},
  Year = {1995}}

@book{brockwell2009time,
  Author = {Brockwell, Peter J and Davis, Richard A},
  Publisher = {Springer},
  Title = {Time series: theory and methods},
  Year = {2009}}

@misc{grenander1984toeplitz,
  Author = {Grenander, Ulf and Szeg\H{o}, G{\'a}bor},
  Publisher = {Chelsea Pub. Co.(New York)},
  Title = {Toeplitz forms and their applications},
  Year = {1984}}

@article{silverstein1995analysis,
  Author = {Silverstein, Jack W and Choi, Sang-Il},
  Journal = {J. Multivariate Anal.},
  Number = {2},
  Pages = {295--309},
  Publisher = {Elsevier},
  Title = {Analysis of the limiting spectral distribution of large dimensional random matrices},
  Volume = {54},
  Year = {1995}}

@article{marchenko1967distribution,
  Author = {Marchenko, Vladimir A and Pastur, Leonid A},
  Journal = {Mat. Sb.},
  Number = {4},
  Pages = {507--536},
  Publisher = {Russian Academy of Sciences, Branch of Mathematical Sciences},
  Title = {Distribution of eigenvalues for some sets of random matrices},
  Volume = {114},
  Year = {1967}}

@incollection{fan2011high,
  Address = {New Jersey},
  Author = {Fan, Jianqing and Fan, Yingying and Wu, Yichao},
  Booktitle = {High-dimensional Data Analysis},
  Date-Added = {2015-04-28 23:05:13 +0000},
  Date-Modified = {2015-04-28 23:07:40 +0000},
  Editor = {T. Cai and X. Shen},
  Pages = {3--37},
  Publisher = {World Scientific},
  Title = {High dimensional classification},
  Year = {2011}}

@article{friedman1989regularized,
  Author = {Friedman, Jerome H},
  Date-Added = {2015-04-28 23:01:16 +0000},
  Date-Modified = {2015-04-28 23:01:25 +0000},
  Journal = {J. Amer. Statist. Assoc.},
  Number = {405},
  Pages = {165--175},
  Publisher = {Taylor \& Francis},
  Title = {Regularized discriminant analysis},
  Volume = {84},
  Year = {1989}}

@article{guo2007regularized,
  Author = {Guo, Yaqian and Hastie, Trevor and Tibshirani, Robert},
  Date-Added = {2015-04-28 22:55:06 +0000},
  Date-Modified = {2015-04-28 22:55:06 +0000},
  Journal = {Biostatistics},
  Number = {1},
  Pages = {86--100},
  Publisher = {Biometrika Trust},
  Title = {Regularized linear discriminant analysis and its application in microarrays},
  Volume = {8},
  Year = {2007}}

@article{saranadasa1993asymptotic,
  Author = {Saranadasa, Hewa},
  Date-Added = {2015-04-28 22:54:23 +0000},
  Date-Modified = {2015-04-28 22:54:37 +0000},
  Journal = {J. Multivariate Anal.},
  Number = {1},
  Pages = {154--174},
  Publisher = {Elsevier},
  Title = {Asymptotic expansion of the misclassification probabilities of {D}-and {A}-criteria for discrimination from two high dimensional populations using the theory of large dimensional random matrices},
  Volume = {46},
  Year = {1993}}

@article{silverstein1995empirical,
  Author = {Silverstein, Jack W and Bai, ZD},
  Date-Added = {2015-04-28 22:53:21 +0000},
  Date-Modified = {2015-04-28 22:53:21 +0000},
  Journal = {J. Multivariate Anal.},
  Number = {2},
  Pages = {175--192},
  Publisher = {Elsevier},
  Title = {On the empirical distribution of eigenvalues of a class of large dimensional random matrices},
  Volume = {54},
  Year = {1995}}

@article{ledoit2011eigenvectors,
  Author = {Ledoit, Olivier and P{\'e}ch{\'e}, Sandrine},
  Date-Added = {2015-04-28 22:52:43 +0000},
  Date-Modified = {2015-04-28 22:52:43 +0000},
  Journal = {Probab. Theory Related Fields},
  Number = {1-2},
  Pages = {233--264},
  Publisher = {Springer},
  Title = {Eigenvectors of some large sample covariance matrix ensembles},
  Volume = {151},
  Year = {2011}}

@book{yao2015large,
  Author = {Yao, Jianfeng and Bai, Zhidong and Zheng, Shurong},
  Date-Added = {2015-04-28 22:52:15 +0000},
  Date-Modified = {2015-04-28 22:52:15 +0000},
  Publisher = {Cambridge University Press},
  Title = {Large Sample Covariance Matrices and High-Dimensional Data Analysis},
  Year = {2015}}


@article{fan2012road,
  Author = {Fan, Jianqing and Feng, Yang and Tong, Xin},
  Date-Added = {2015-04-28 22:50:01 +0000},
  Date-Modified = {2015-04-28 22:50:01 +0000},
  Journal = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  Number = {4},
  Pages = {745--771},
  Publisher = {Wiley Online Library},
  Title = {A road to classification in high dimensional space: the regularized optimal affine discriminant},
  Volume = {74},
  Year = {2012}}

@article{fan2008high,
  Author = {Fan, Jianqing and Fan, Yingying},
  Date-Added = {2015-04-28 22:46:10 +0000},
  Date-Modified = {2015-04-28 22:48:10 +0000},
  Journal = {Ann. Statist.},
  Number = {6},
  Title = {High dimensional classification using features annealed independence rules},
  Volume = {36},
  Pages = {2605--2637},
  Year = {2008}}


@article{efron1975efficiency,
  Author = {Efron, Bradley},
  Date-Added = {2015-04-28 22:32:57 +0000},
  Date-Modified = {2015-04-28 22:32:57 +0000},
  Journal = {J. Amer. Statist. Assoc.},
  Number = {352},
  Pages = {892--898},
  Publisher = {Taylor \& Francis Group},
  Title = {The efficiency of logistic regression compared to normal discriminant analysis},
  Volume = {70},
  Year = {1975}}

@article{donoho2015variance,
  Author = {Donoho, David L and Montanari, Andrea},
  Journal = {arXiv preprint arXiv:1503.02106},
  Title = {Variance Breakdown of {Huber} ({M})-estimators: $ n/p \rightarrow m \in (1, \, \infty) $},
  Year = {2015}}

@article{bickel2004some,
  Author = {Bickel, Peter J and Levina, Elizaveta},
  Date-Added = {2015-04-28 00:58:15 +0000},
  Date-Modified = {2015-04-28 00:59:24 +0000},
  Journal = {Bernoulli},
  Pages = {989--1010},
  Publisher = {JSTOR},
  Title = {Some theory for {F}isher's linear discriminant function, ``naive {B}ayes'', and some alternatives when there are many more variables than observations},
  Year = {2004}}

@inproceedings{ng2001discriminative,
  Author = {Ng, Andrew and Jordan, Michael},
  Booktitle = {NIPS},
  Date-Added = {2015-04-28 00:57:42 +0000},
  Date-Modified = {2015-04-28 00:57:42 +0000},
  Title = {On discriminative vs. generative classifiers: A comparison of logistic regression and naive {B}ayes},
  Year = {2001}}

@article{chen1998atomic,
  Author = {Chen, Scott Shaobing and Donoho, David L and Saunders, Michael A},
  Date-Added = {2015-04-28 00:28:16 +0000},
  Date-Modified = {2015-04-28 00:28:16 +0000},
  Journal = {SIAM J. Sci. Comput.},
  Number = {1},
  Pages = {33--61},
  Publisher = {SIAM},
  Title = {Atomic decomposition by basis pursuit},
  Volume = {20},
  Year = {1998}}

@article{donoho2006compressed,
  Author = {Donoho, David L},
  Date-Added = {2015-04-28 00:27:30 +0000},
  Date-Modified = {2015-04-28 00:27:30 +0000},
  Journal = {IEEE Trans. Inform. Theory},
  Number = {4},
  Pages = {1289--1306},
  Publisher = {IEEE},
  Title = {Compressed sensing},
  Volume = {52},
  Year = {2006}}

@article{candes2007dantzig,
  Author = {Cand\`es, Emmanuel and Tao, Terence},
  Date-Added = {2015-04-28 00:24:32 +0000},
  Date-Modified = {2015-04-28 00:28:57 +0000},
  Journal = {Ann. Statist.},
  Number = {6},
  Pages = {2313--2351},
  Publisher = {JSTOR},
  Title = {The {D}antzig selector: Statistical estimation when $p$ is much larger than $n$},
  Volume = {35},
  Year = {2007}}

@inproceedings{jaegle2016fast,
  Author = {Jaegle, Andrew and Phillips, Stephen and Daniilidis, Kostas},
  Booktitle = {IEEE ICRA},
  Title = {Fast, Robust, Continuous Monocular Egomotion Computation},
  Year = {2016}}

@inproceedings{mabel16iros,
  Author = {Zhang, Mabel M and Kennedy, Monroe D and Hsieh, M Ani and Daniilidis, Kostas},
  Booktitle = {Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
  Organization = {IEEE},
  Pages = {4931--4938},
  Title = {A triangle histogram for object classification by tactile sensing},
  Year = {2016}}

 @inproceedings{mabel17iros,
  Author = {M.M. Zhang and N. Atanasov and K. Daniilidis},
  Booktitle = {Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on},
  Title = {Active Touch Sensing for Object Recognition by Monte Carlo Tree Search},
  Year = {2017}}

 @inproceedings{jaegle18iclr,
  Author = {Andrew Jaegle and Stephen Phillips and Daphne Ippolito and Kostas Daniilidis},
  Booktitle = {International Conference on Learning Representations},
  Title = {Understanding image motion with group representations},
  Url = {https://openreview.net/forum?id=SJLlmG-AZ},
  Year = {2018},
  Bdsk-Url-1 = {https://openreview.net/forum?id=SJLlmG-AZ}}


@inproceedings{esteves18iclr,
  Author = {Esteves, Carlos and Allen-Blanchette, Christine and Zhou, Xiaowei and Daniilidis, Kostas},
  Booktitle = {Int. Conf. Learning Representations},
  Title = {Polar Transformer Networks},
  Year = {2018}}


@book{jean19nc,
title={Aspects of Harmonic Analysis and Representation Theory},
author={Jean Gallier and Jocelyn Quaintance},
publisher={Self-published},
year={2019}
}

@book{chirikjian2000,
  title={Engineering applications of noncommutative harmonic analysis: with emphasis on rotation and motion groups},
  author={Chirikjian, Gregory S and Kyatkin, Alexander B},
  year={2000},
  publisher={CRC press}
}

@article{dicarlo12,
  title={How does the brain solve visual object recognition?},
  author={DiCarlo, James J and Zoccolan, Davide and Rust, Nicole C},
  journal={Neuron},
  volume={73},
  number={3},
  pages={415--434},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{gens2014deep,
  title={Deep symmetry networks},
  author={Gens, Robert and Domingos, Pedro M},
  booktitle={Advances in neural information processing systems},
  pages={2537--2545},
  year={2014}
}

@inproceedings{lenc2015understanding,
  title={Understanding image representations by measuring their equivariance and equivalence},
  author={Lenc, Karel and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={991--999},
  year={2015}
}

@article{dieleman2015rotation,
  title={Rotation-invariant convolutional neural networks for galaxy morphology prediction},
  author={Dieleman, Sander and Willett, Kyle W and Dambre, Joni},
  journal={Monthly notices of the royal astronomical society},
  volume={450},
  number={2},
  pages={1441--1459},
  year={2015},
  publisher={Oxford University Press}
}

@article{granlund78,
  title={In search of a general picture processing operator},
  author={Granlund, Goesta H},
  journal={Computer Graphics and Image Processing},
  volume={8},
  number={2},
  pages={155--173},
  year={1978},
  publisher={Elsevier}
}

@article{casasent1976scale,
  title={Scale invariant optical transform},
  author={Casasent, David and Psaltis, Demetri},
  journal={Optical Engineering},
  volume={15},
  number={3},
  pages={153258--153258},
  year={1976},
  publisher={International Society for Optics and Photonics}
}

@article{zwicke1983new,
  title={A new implementation of the Mellin transform and its application to radar classification of ships},
  author={Zwicke, Philip E and Kiss, Imre},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
volume=4,
  number={2},
  pages={191--199},
  year={1983},
  publisher={IEEE}
}

@article{rubinstein1991recognition,
  title={Recognition of distorted patterns by invariance kernels},
  author={Rubinstein, Jacob and Segman, Joseph and Zeevi, Yehoshua},
  journal={Pattern Recognition},
  volume={24},
  number={10},
  pages={959--967},
  year={1991},
  publisher={Elsevier}
}

@article{ferraro1988relationship,
  title={Relationship between integral transform invariances and Lie group theory},
  author={Ferraro, Mario and Caelli, Terry M},
  journal={JOSA A},
  volume={5},
  number={5},
  pages={738--742},
  year={1988},
  publisher={Optical Society of America}
}


@article{cohen2014learning,
  title={Learning the Irreducible Representations of Commutative Lie Groups},
    author={Cohen, Taco and Welling, Max},
      journal={arXiv preprint arXiv:1402.4437},
        year={2014}
  }

@article{lecun2015deep,
  title={Deep learning},
    author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
      journal={Nature},
        volume={521},
    number={7553},
      pages={436--444},
        year={2015},
          publisher={Nature Research}
    }
    
  
@inproceedings{hel1996canonical,
  title={Canonical decomposition of steerable functions},
    author={Hel-Or, Yacov and Teo, Patrick C},
      booktitle={Computer Vision and Pattern Recognition, 1996. Proceedings CVPR'96, 1996 IEEE Computer Society Conference on},
        pages={809--816},
    year={1996},
      organization={IEEE}
      }
      
@inproceedings{teo1998design,
  title={Design of multi-parameter steerable functions using cascade basis reduction},
    author={Teo, Patrick C and Hel-Or, Yacov},
      booktitle={Computer Vision, 1998. Sixth International Conference on},
        pages={187--192},
    year={1998},
      organization={IEEE}
      }
@inproceedings{bansal2014steerability,
title={Steerability for Lie Transformation Groups},
author={Bansal, Mayank},
year={2014}
}

@article{kyatkin2000algorithms,
  title={Algorithms for fast convolutions on motion groups},
  author={Kyatkin, Alexander B and Chirikjian, Gregory S},
  journal={Applied and Computational Harmonic Analysis},
  volume={9},
  number={2},
  pages={220--241},
  year={2000},
  publisher={Elsevier}
}
@article{miao2007learning,
  title={Learning the lie groups of visual invariance},
    author={Miao, Xu and Rao, Rajesh PN},
      journal={Neural computation},
        volume={19},
    number={10},
      pages={2665--2693},
        year={2007},
          publisher={MIT Press}
    }

@inproceedings{oyallon2015deep,
  title={Deep roto-translation scattering for object classification},
    author={Oyallon, Edouard and Mallat, St{\'e}phane},
      booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
        pages={2865--2873},
    year={2015}
    }

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
    author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
      journal={arXiv preprint arXiv:1312.6199},
        year={2013}
  }

@inproceedings{manmatha1994framework,
  title={A framework for recovering affine transforms using points, lines, or image brightness},
    author={Manmatha, R},
    year={1994}
    }

@inproceedings{kai2005steerable,
title={Steerable Filters in Motion Estimation},
author={Kai Krajsek},
year={2005}
}

@article{freeman1991design,
  title={The design and use of steerable filters},
    author={Freeman, William T and Adelson, Edward H and others},
      journal={IEEE Transactions on Pattern analysis and machine intelligence},
        volume={13},
    number={9},
      pages={891--906},
        year={1991}
        }

@article{perona1995deformable,
  title={Deformable kernels for early vision},
  author={Perona, Pietro},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={17},
  number={5},
  pages={488--499},
  year={1995},
  publisher={IEEE}
}

@article{simoncelli1992shiftable,
  title={Shiftable multiscale transforms},
    author={Simoncelli, Eero P and Freeman, William T and Adelson, Edward H and Heeger, David J},
      journal={IEEE transactions on Information Theory},
        volume={38},
    number={2},
      pages={587--607},
        year={1992},
          publisher={IEEE}
    }

@article{morel2011sift,
  title={Is SIFT scale invariant?},
    author={Morel, Jean-Michel and Yu, Guoshen}
    }

@inproceedings{yu2009fully,
  title={A fully affine invariant image comparison method},
    author={Yu, Guoshen and Morel, Jean-Michel},
      booktitle={Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on},
        pages={1597--1600},
    year={2009},
      organization={IEEE}
      }

@article{hinterstoisser2011learning,
  title={Learning real-time perspective patch rectification},
    author={Hinterstoisser, Stefan and Lepetit, Vincent and Benhimane, Selim and Fua, Pascal and Navab, Nassir},
      journal={International Journal of Computer Vision},
        volume={91},
    number={1},
      pages={107--130},
        year={2011},
          publisher={Springer}
    }

@inproceedings{gvili2003complete,
  title={A complete system of measurement invariants for abelian lie transformation groups},
    author={Gvili, Yaron and Sochen, Nir},
      booktitle={International Conference on Scale-Space Theories in Computer Vision},
        pages={72--85},
    year={2003},
      organization={Springer}
      }

@inproceedings{wang2007real,
  title={Real-time image matching based on multiple view kernel projection},
    author={Wang, Quan and You, Suya},
      booktitle={Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on},
        pages={1--8},
    year={2007},
      organization={IEEE}
      }

@inproceedings{nordberg1996equivariance,
  title={Equivariance and invariance-an approach based on Lie groups},
  author={Nordberg, Klas and Granlund, Gosta},
  booktitle={Image Processing, 1996. Proceedings., International Conference on},
  volume={3},
  pages={181--184},
  year={1996},
  organization={IEEE}
}

@inproceedings{sifre2013rotation,
  title={Rotation, scaling and deformation invariant scattering for texture discrimination},
    author={Sifre, Laurent and Mallat, St{\'e}phane},
      booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
        pages={1233--1240},
    year={2013}
    }

@article{bruna2013invariant,
  title={Invariant scattering convolution networks},
    author={Bruna, Joan and Mallat, St{\'e}phane},
      journal={IEEE transactions on pattern analysis and machine intelligence},
        volume={35},
    number={8},
      pages={1872--1886},
        year={2013},
          publisher={IEEE}
    }

@article{mallat2012group,
  title={Group invariant scattering},
    author={Mallat, St{\'e}phane},
      journal={Communications on Pure and Applied Mathematics},
        volume={65},
    number={10},
      pages={1331--1398},
        year={2012},
          publisher={Wiley Online Library}
    }

@inproceedings{mallat2010recursive,
  title={Recursive interferometric representation},
    author={Mallat, St{\'e}phane},
      year={2010}
      }

@article{segman1992canonical,
  title={The canonical coordinates method for pattern deformation: Theoretical and computational considerations},
    author={Segman, Joseph and Rubinstein, Jacob and Zeevi, Yehoshua Y},
      journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
        volume={14},
    number={12},
      pages={1171--1183},
        year={1992},
          publisher={IEEE}
    }

@article{bruna2013learning,
  title={Learning stable group invariant representations with convolutional networks},
    author={Bruna, Joan and Szlam, Arthur and LeCun, Yann},
      journal={arXiv preprint arXiv:1301.3537},
        year={2013}
  }

@article{henriques2016warped,
  title={Warped Convolutions: Efficient Invariance to Spatial Transformations},
    author={Henriques, Jo{\~a}o F and Vedaldi, Andrea},
      journal={arXiv preprint arXiv:1609.04382},
        year={2016}
  }

@inproceedings{jaderberg2015spatial,
  title={Spatial transformer networks},
    author={Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and others},
      booktitle={Advances in Neural Information Processing Systems},
        pages={2017--2025},
    year={2015}
    }

@inproceedings{chomat2000local,
  title={Local scale selection for Gaussian based description techniques},
    author={Chomat, Olivier and de Verdi{\`e}re, Vincent Colin and Hall, Daniela and Crowley, James L},
      booktitle={European Conference on Computer Vision},
        pages={117--134},
    year={2000},
      organization={Springer}
      }
    
@article{lindeberg1994scale,
  title={Scale-space theory: A basic tool for analyzing structures at different scales},
    author={Lindeberg, Tony},
      journal={Journal of applied statistics},
        volume={21},
    number={1-2},
      pages={225--270},
        year={1994},
          publisher={Taylor \& Francis}
    }
    
@article{lowe2004distinctive,
  title={Distinctive image features from scale-invariant keypoints},
    author={Lowe, David G},
      journal={International journal of computer vision},
        volume={60},
    number={2},
      pages={91--110},
        year={2004},
          publisher={Springer}
    }

@book{thurston97geotop,
 author = {William P. Thurston},
 publisher = {Princeton University Press},
 title = {Three-Dimensional Geometry and Topology, Volume 1: Volume 1},
 year = {1997}
}

@book{arfken1966mathematical,
  title={Mathematical Methods for Physicists},
  author={Arfken, G.B.},
  number={v. 2},
  lccn={65027740},
  series={Mathematical Methods for Physicists},
  url={https://books.google.com/books?id=zJdj5AAACAAJ},
  year={1966},
  publisher={Academic Press}
}

@article{driscoll1994computing,
  title = {Computing Fourier transforms and convolutions on the 2-sphere},
  author = {Driscoll, James R and Healy, Dennis M},
  journal = {Advances in applied mathematics},
  volume = {15},
  number = {2},
  pages = {202--250},
  year = {1994},
  publisher = {Elsevier},
}


@article{worrall17cvpr,
  title={Harmonic Networks: Deep Translation and Rotation Equivariance},
  author={Worrall, Daniel E and Garbin, Stephan J and Turmukhambetov, Daniyar and Brostow, Gabriel J},
  journal={arXiv preprint arXiv:1612.04642},
  year={2016}
}

@article{granlund1978search,
  title={In search of a general picture processing operator},
    author={Granlund, Goesta H},
      journal={Computer Graphics and Image Processing},
        volume={8},
    number={2},
      pages={155--173},
        year={1978},
          publisher={Elsevier}
    }

@incollection{soatto2013actionable,
  title={Actionable information in vision},
    author={Soatto, Stefano},
      booktitle={Machine learning for computer vision},
        pages={17--48},
    year={2013},
      publisher={Springer}
      }
      
@inproceedings{larochelle2007empirical,
  title = {An empirical evaluation of deep architectures on problems with many factors of variation},
  author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
  booktitle = {Proceedings of the 24th international conference on Machine learning},
  pages = {473--480},
  year = {2007},
  organization = {ACM},
}

@InProceedings{Toshev_2014_CVPR,
author = {Toshev, Alexander and Szegedy, Christian},
title = {DeepPose: Human Pose Estimation via Deep Neural Networks},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
}

@incollection{NIPS2014_5573,
title = {Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation},
author = {Tompson, Jonathan J and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {1799--1807},
year = {2014},
publisher = {Curran Associates, Inc.},
}


@article{newell16_stack_hourg_networ_human_pose_estim,
  title = {Stacked Hourglass Networks for Human Pose Estimation},
  author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
  archivePrefix = {arXiv},
  year = {2016},
  eprint = {1603.06937},
  primaryClass = {cs.CV},
  abstract = {This work introduces a novel convolutional network architecture for the task
of human pose estimation. Features are processed across all scales and
consolidated to best capture the various spatial relationships associated with
the body. We show how repeated bottom-up, top-down processing used in
conjunction with intermediate supervision is critical to improving the
performance of the network. We refer to the architecture as a "stacked
hourglass" network based on the successive steps of pooling and upsampling that
are done to produce a final set of predictions. State-of-the-art results are
achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
}

@InProceedings{Zhou_2017_CVPR,
author = {Zhou, Yanzhao and Ye, Qixiang and Qiu, Qiang and Jiao, Jianbin},
title = {Oriented Response Networks},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
} 

@InProceedings{Laptev_2016_CVPR,
author = {Laptev, Dmitry and Savinov, Nikolay and Buhmann, Joachim M. and Pollefeys, Marc},
title = {TI-Pooling: Transformation-Invariant Pooling for Feature Learning in Convolutional Neural Networks},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@inproceedings{wu20153d,
  title = {3d shapenets: A deep representation for volumetric shapes},
  author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages = {1912--1920},
  year = {2015},
}


@inproceedings{maturana2015voxnet,
  title = {Voxnet: A 3d convolutional neural network for real-time object recognition},
  author = {Maturana, Daniel and Scherer, Sebastian},
  booktitle = {Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on},
  pages = {922--928},
  year = {2015},
  organization = {IEEE},
}

@inproceedings{su2015multi,
  title = {Multi-view convolutional neural networks for 3D shape recognition},
  author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  pages = {945--953},
  year = {2015},
}

@article{sedaghat16_orien_boost_voxel_nets_objec_recog,
  author = {Sedaghat, Nima and Zolfaghari, Mohammadreza and
                  Brox, Thomas},
  title = {Orientation-Boosted Voxel Nets for 3d Object
                  Recognition},
  journal = {CoRR},
  year = 2016,
  archivePrefix = {arXiv},
  eprint = {1604.03351},
  primaryClass = {cs.CV},
}

@article{qi16_volum_multi_view_cnns_objec_class_data,
  title = {Volumetric and Multi-View CNNs for Object Classification on 3D Data},
  author = {Qi, Charles R. and Su, Hao and Niessner, Matthias and Dai, Angela and Yan, Mengyuan and Guibas, Leonidas J.},
  archivePrefix = {arXiv},
  year = {2016},
  eprint = {1604.03265},
  primaryClass = {cs.CV},
}

@article{marcos16_rotat_equiv_vector_field_networ,
  author = {Marcos, Diego and Volpi, Michele and Komodakis,
                  Nikos and Tuia, Devis},
  title = {Rotation Equivariant Vector Field Networks},
  journal = {CoRR},
  year = 2016,
  archivePrefix = {arXiv},
  eprint = {1612.09346},
  primaryClass = {cs.CV},
}

@inproceedings{netzer2011reading,
  title = {Reading digits in natural images with unsupervised feature learning},
  author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  booktitle = {NIPS workshop on deep learning and unsupervised feature learning},
  volume = {2011},
  number = {2},
  pages = {5},
  year = {2011},
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{jacobsen17_dynam_steer_block_deep_resid_networ,
  author = {Jacobsen, J{\"o}rn-Henrik and Brabandere, Bert de
                  and Smeulders, Arnold W. M.},
  title = {Dynamic Steerable Blocks in Deep Residual Networks},
  journal = {CoRR},
  year = 2017,
  url = {http://arxiv.org/abs/1706.00598v2},
  archivePrefix = {arXiv},
  eprint = {1706.00598},
  primaryClass = {cs.CV},
}

@article{cohen16_steer_cnns,
  title = {Steerable CNNs},
  author = {Cohen, Taco S. and Welling, Max},
  journal = {arXiv},
  year = {2016},
  eprint = {1612.08498},
  primaryClass = {cs.LG},
  url = {http://arxiv.org/abs/1612.08498v1},
}



@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}
@inproceedings{fuchs2020se3transformers,
    title={SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks},
    author={Fabian B. Fuchs and Daniel E. Worrall and Volker Fischer and Max Welling},
    year={2020},
    booktitle = {Advances in Neural Information Processing Systems 34 (NeurIPS)},
}
@misc{thomas2018tensorfieldnet,
title	= {Tensor Field Networks:  Rotation- and Translation-Equivariant Neural Networks for 3D Point Clouds},
author	= {Nathaniel Cabot Thomas and Tess Smidt and Steven Kearnes and Lusann Yang and Li Li and Kai Kohlhoff and Patrick Riley},
journal={arXiv preprint arXiv:1802.08219 },
year	= {2018}
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@inproceedings{Weiler2019E2Equiv,
 author = {Weiler, Maurice and Cesa, Gabriele},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {General E(2)-Equivariant Steerable CNNs},
 volume = {32},
 year = {2019}
}
 @inproceedings{zagoruyko2016WideresNet,
    	title={Wide Residual Networks},
    	author={Sergey Zagoruyko and Nikos Komodakis},
    	year={2016},
    	month={September},
    	pages={87.1-87.12},
    	articleno={87},
    	numpages={12},
    	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
    	publisher={BMVA Press},
    	editor={Richard C. Wilson, Edwin R. Hancock and William A. P. Smith}
    }
@inproceedings{jang2017gumbel,
  author    = {Eric Jang and
               Shixiang Gu and
               Ben Poole},
  title     = {Categorical Reparameterization with Gumbel-Softmax},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},

}
@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.}
}

@inproceedings{guo17a,
  title = 	 {On Calibration of Modern Neural Networks},
  author =       {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1321--1330},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher = {PMLR}
}

@inproceedings{naei15,
  title={Obtaining well calibrated probabilities using bayesian binning},
  author={Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@inproceedings{Netzer2011ReadingDI,
  title={Reading Digits in Natural Images with Unsupervised Feature Learning},
  author={Yuval Netzer and Tiejie Wang and Adam Coates and A. Bissacco and Bo Wu and A. Ng},
  year={2011}
}
  


@inproceedings{levit1974optimality,
  title={On optimality of some statistical estimates},
  author={Levit, B Ya},
  booktitle={Proceedings of the Prague symposium on asymptotic statistics},
  volume={2},
  pages={215--238},
  year={1974},
  organization={Charles University Prague}
}

@inproceedings{gammerman1998learning,
  title={Learning by transduction},
  author={Gammerman, A and Vovk, V and Vapnik, V},
  booktitle={Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence},
  pages={148--155},
  year={1998}
}

@book{vovk2005algorithmic,
	author = {Vovk, Vladimir and Gammerman, Alex and Shafer, Glenn},
	date-added = {2021-05-10 11:53:43 -0500},
	date-modified = {2021-05-10 11:53:43 -0500},
	publisher = {Springer Science \& Business Media},
	title = {Algorithmic learning in a random world},
	year = {2005}}



@article{shafer2008tutorial,
	author = {Shafer, Glenn and Vovk, Vladimir},
	date-added = {2021-05-10 12:34:00 -0500},
	date-modified = {2021-05-10 12:34:01 -0500},
	journal = {Journal of Machine Learning Research},
	number = {Mar},
	pages = {371--421},
	title = {A tutorial on conformal prediction},
	volume = {9},
	year = {2008}}


@book{cp,
	author = {Balasubramanian, Vineeth and Ho, Shen-Shyang and Vovk, Vladimir},
	publisher = {Newnes},
	title = {Conformal prediction for reliable machine learning: theory, adaptations and applications},
	year = {2014}
}

@article{shimodaira2000improving,
  title={Improving predictive inference under covariate shift by weighting the log-likelihood function},
  author={Shimodaira, Hidetoshi},
  journal={Journal of statistical planning and inference},
  volume={90},
  number={2},
  pages={227--244},
  year={2000},
  publisher={Elsevier}
}
@book{quinonero2009dataset,
  title={Dataset shift in machine learning},
  author={Qui{\~n}onero-Candela, Joaquin and Sugiyama, Masashi and Lawrence, Neil D and Schwaighofer, Anton},
  year={2009},
  publisher={Mit Press}
}
@article{Vaart2011,
  title={A local maximal inequality under uniform entropy},
  author={Van Der Vaart, Aad and Wellner, Jon A},
  journal={Electronic Journal of Statistics},
  volume={5},
  number={2011},
  pages={192},
  year={2011},
  publisher={NIH Public Access}
}
@article{VanderLaan2006,
  title={Targeted maximum likelihood learning},
  author={{Van der Laan}, Mark J and Rubin, Daniel},
  journal={The international journal of biostatistics},
  volume={2},
  number={1},
  year={2006},
  publisher={De Gruyter}
}
@book{VanderLaan2018,
  title={Targeted learning in data science: causal inference for complex longitudinal studies},
  author={{Van der Laan}, Mark J and Rose, Sherri},
  year={2018},
  publisher={Springer}
}
@book{Pfanzagl1985,
address = {New York, NY},
author = {Pfanzagl, J.},
booktitle = {Statistics and Risk Modeling},
doi = {10.1524/strm.1985.3.34.379},
isbn = {978-0-387-90776-5},
issn = {21967040},
pages = {379--388},
publisher = {Springer New York},
series = {Lecture Notes in Statistics},
title = {{Contributions to a general asymptotic statistical theory}},
volume = {3},
year = {1985}
}

@book{Pfanzagl1990,
author = {Pfanzagl, Johann},
doi = {10.1007/978-1-4612-3396-1_5},
pages = {17--22},
publisher = {Springer, New York, NY},
title = {Estimation in semiparametric models},
series = {Lecture Notes in Statistics},
volume = {63},
year = {1990}
}

@article{Clopper1934,
  title={The use of confidence or fiducial limits illustrated in the case of the binomial},
  author={Clopper, Charles J and Pearson, Egon S},
  journal={Biometrika},
  pages={404--413},
  year={1934},
  publisher={JSTOR}
}
@article{Cai2005,
  title={One-sided confidence intervals in discrete distributions},
  author={Cai, T Tony},
  journal={Journal of Statistical planning and inference},
  volume={131},
  number={1},
  pages={63--88},
  year={2005},
  publisher={Elsevier}
}
@article{Park2020,
  title={PAC Confidence Predictions for Deep Neural Network Classifiers},
  author={Park, Sangdon and Li, Shuo and Lee, Insup and Bastani, Osbert},
  journal={arXiv preprint arXiv:2011.00716},
  year={2020}
}
@book{Hall2013,
  title={The bootstrap and Edgeworth expansion},
  author={Hall, Peter},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{Agresti1998,
  title={Approximate is better than “exact” for interval estimation of binomial proportions},
  author={Agresti, Alan and Coull, Brent A},
  journal={The American Statistician},
  volume={52},
  number={2},
  pages={119--126},
  year={1998},
  publisher={Taylor \& Francis}
}
@article{Bates2021,
  title={Distribution-free, risk-controlling prediction sets},
  author={Bates, Stephen and Angelopoulos, Anastasios and Lei, Lihua and Malik, Jitendra and Jordan, Michael I},
  journal={arXiv preprint arXiv:2101.02703},
  year={2021}
}
@book{vandervaart1996,
  title={Weak convergence and empirical processes: with applications to statistics},
  author={{van der Vaart}, Adrianus Willem and Wellner, Jon},
  year={1996},
  publisher={Springer Science \& Business Media}
}
@article{Wilson1927,
author = {Wilson, Edwin B.},
doi = {10.1080/01621459.1927.10502953},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {158},
pages = {209--212},
title = {{Probable Inference, the Law of Succession, and Statistical Inference}},
volume = {22},
year = {1927}
}
@article{Thulin2014,
abstract = {When computing a confidence interval for a binomial proportion p one must choose between using an exact interval, which has a coverage probability of at least 1-$\alpha$ for all values of p, and a shorter approximate interval, which may have lower coverage for some p but that on average has coverage equal to 1-$\alpha$. We investigate the cost of using the exact one and two-sided Clopper-Pearson confidence intervals rather than shorter approximate intervals, first in terms of increased expected length and then in terms of the increase in sample size required to obtain a desired expected length. Using asymptotic expansions, we also give a closed-form formula for determining the sample size for the exact Clopper-Pearson methods. For two-sided intervals, our investigation reveals an interesting connection between the frequentist Clopper-Pearson interval and Bayesian intervals based on noninformative priors.},
archivePrefix = {arXiv},
arxivId = {1303.1288},
author = {Thulin, M{\aa}ns},
doi = {10.1214/14-EJS909},
eprint = {1303.1288},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Asymptotic expansion,Binomial distribution,Confidence interval,Expected length,Proportion,Sample size determination},
number = {1},
pages = {817--840},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
title = {{The cost of using exact confidence intervals for a binomial proportion}},
volume = {8},
year = {2014}
}
@article{Lei2021,
abstract = {Evaluating treatment effect heterogeneity widely informs treatment decision making. At the moment, much emphasis is placed on the estimation of the conditional average treatment effect via flexible machine learning algorithms. While these methods enjoy some theoretical appeal in terms of consistency and convergence rates, they generally perform poorly in terms of uncertainty quantification. This is troubling since assessing risk is crucial for reliable decision-making in sensitive and uncertain environments. In this work, we propose a conformal inference-based approach that can produce reliable interval estimates for counterfactuals and individual treatment effects under the potential outcome framework. For completely randomized or stratified randomized experiments with perfect compliance, the intervals have guaranteed average coverage in finite samples regardless of the unknown data generating mechanism. For randomized experiments with ignorable compliance and general observational studies obeying the strong ignorability assumption, the intervals satisfy a doubly robust property which states the following: the average coverage is approximately controlled if either the propensity score or the conditional quantiles of potential outcomes can be estimated accurately. Numerical studies on both synthetic and real data sets empirically demonstrate that existing methods suffer from a significant coverage deficit even in simple models. In contrast, our methods achieve the desired coverage with reasonably short intervals.},
archivePrefix = {arXiv},
arxivId = {2006.06138},
author = {Lei, Lihua and Cand{\`{e}}s, Emmanuel J.},
doi = {10.1111/rssb.12445},
eprint = {2006.06138},
file = {::},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {causal inference,conformal inference,counterfactual,doubly robust,individual treatment effect,uncertainty quantification},
number = {5},
pages = {911--938},
title = {{Conformal inference of counterfactuals and individual treatment effects}},
url = {http://arxiv.org/abs/2006.06138},
volume = {83},
year = {2021}
}
@article{Bickel2007,
abstract = {We address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution - -problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. We formulate the general problem of learning under covariate shift as an integrated optimization problem. We derive a kernel logistic regression classifier for differing training and test distributions.},
author = {Bickel, Steffen and Br{\"{u}}ckner, Michael and Scheffer, Tobias},
doi = {10.1145/1273496.1273507},
file = {::},
journal = {ACM International Conference Proceeding Series},
pages = {81--88},
title = {{Discriminative learning for differing training and test distributions}},
volume = {227},
year = {2007}
}

@article{Bonferroni1936,
author = {Bonferroni, Carlo},
journal = {Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commericiali di Firenze},
pages = {3--62},
title = {{Teoria statistica delle classi e calcolo delle probabilit{\`{a}}}},
volume = {8},
year = {1936}
}
@article{Dunn1961,
abstract = {Methods for constructing simultaneous confidence intervals for all possible linear contrasts among several means of normally distributed variables have been given by Scheff{\'{e}} and Tukey. In this paper the possibility is considered of picking in advance a number (say m) of linear contrasts among k means, and then estimating these m linear contrasts by confidence intervals based on a Student t statistic, in such a way that the overall confidence level for the m intervals is greater than or equal to a preassigned value. It is found that for some values of k, and for m not too large, intervals obtained in this way are shorter than those using the F distribution or the Studentized range. When this is so, the experimenter may be willing to select the linear combinations in advance which he wishes to estimate in order to have m shorter intervals instead of an infinite number of longer intervals. {\textcopyright} Taylor & Francis Group, LLC.},
author = {Dunn, Olive Jean},
doi = {10.1080/01621459.1961.10482090},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {293},
pages = {52--64},
title = {{Multiple Comparisons among Means}},
volume = {56},
year = {1961}
}
@article{Bender1999,
author = {Bender, R. and Lange, S.},
journal = {British Medical Journal},
number = {7183},
pages = {600--601},
pmid = {10037651},
publisher = {BMJ Publishing Group},
title = {{Multiple test procedures other than Bonferroni's deserve wider use}},
volume = {318},
year = {1999}
}
@article{Bland1995,
abstract = {Many published papers include large numbers of significance tests. These may be difficult to interpret because if we go on testing long enough we will inevitably find something which is “significant.” We must beware of attaching too much importance to a lone significant result among a mass of non-significant ones. It may be the one in 20 which we expect by chance alone.

Lee et al simulated a clinical trial of the treatment of coronary artery disease by allocating 1073 patient records from past cases into two “treatment” groups at random.1 They then analysed the outcome as if it were a genuine trial of two treatments. The analysis was quite detailed and thorough. As we would expect, it failed to show any significant difference in survival between those patients allocated to the two treatments. Patients were then subdivided by two variables which affect prognosis, the number of diseased coronary vessels and whether the left ventricular contraction pattern was normal or abnormal. A significant difference in survival between the two “treatment” groups was found in those patients with three diseased vessels (the maximum) and abnormal ventricular contraction. As this would be the subset of patients with the worst prognosis, the finding would be easy to account for by saying that the superior “treatment” {\ldots}},
author = {Bland, J. Martin and Altman, Douglas G.},
doi = {10.1136/bmj.310.6973.170},
file = {::},
issn = {14685833},
journal = {Bmj},
number = {6973},
pages = {170},
pmid = {7833759},
publisher = {British Medical Journal Publishing Group},
title = {{Multiple significance tests: The Bonferroni method}},
volume = {310},
year = {1995}
}
@article{Moran2003,
author = {Moran, Matthew D.},
doi = {10.1034/j.1600-0706.2003.12010.x},
issn = {00301299},
journal = {Oikos},
number = {2},
pages = {403--405},
publisher = {John Wiley & Sons, Ltd},
title = {{Arguments for rejecting the sequential bonferroni in ecological studies}},
volume = {100},
year = {2003}
}
@book{Kincaid1996,
abstract = {3rd ed. This book introduces students with diverse backgrounds to various types of mathematical analysis that are commonly needed in scientific computing. The subject of numerical analysis is treated from a mathematical point of view, offering a complete analysis of methods for scientific computing with appropriate motivations and careful proofs. In an engaging and informal style, the authors demonstrate that many computational procedures and intriguing questions of computer science arise from theorems and proofs. Algorithms are presented in pseudocode, so that students can immediately write computer. 1. Mathematical Preliminaries -- 2. Computer Arithmetic -- 3. Solution of Nonlinear Equations -- 4. Solving Systems of Linear Equations -- 5. Selected Topics in Numerical Linear Algebra -- 6. Approximating Functions -- 7. Numerical Differentiation and Integration -- 8. Numerical Solution of Ordinary Differential Equations -- 9. Numerical Solution of Partial Differential Equations -- 10. Linear Programming and Related Topics -- 11. Optimization -- Appendix A. Overview of Mathematical Software.},
author = {Kincaid, David and Cheney, Ward},
isbn = {978-1-4704-1115-2},
pages = {xii, 804 p.},
publisher = {American Mathematical Society},
title = {{Numerical Analysis: Mathematics of Scientific Computing (2nd ed.)}},
year = {1996}
}
@article{Huang2007,
abstract = {We consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. Most algorithms for this setting try to first recover sampling distributions and then make appropriate corrections based on the distribution estimate. We present a nonparametric method which directly produces resampling weights without distribution estimation. Our method works by matching distributions between training and testing sets in feature space. Experimental results demonstrate that our method works well in practice.},
author = {Huang, Jiayuan and Smola, Alexander J. and Gretton, Arthur and Borgwardt, Karsten M. and Sch{\"{o}}lkopf, Bernhard},
doi = {10.7551/mitpress/7503.003.0080},
isbn = {9780262195683},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {601--608},
publisher = {MIT Press},
title = {{Correcting sample selection bias by unlabeled data}},
year = {2007}
}
@inproceedings{Gopalan2011,
abstract = {Adapting the classifier trained on a source domain to recognize instances from a new target domain is an important problem that is receiving recent attention. In this paper, we present one of the first studies on unsupervised domain adaptation in the context of object recognition, where we have labeled data only from the source domain (and therefore do not have correspondences between object categories across domains). Motivated by incremental learning, we create intermediate representations of data between the two domains by viewing the generative subspaces (of same dimension) created from these domains as points on the Grassmann manifold, and sampling points along the geodesic between them to obtain subspaces that provide a meaningful description of the underlying domain shift. We then obtain the projections of labeled source domain data onto these subspaces, from which a discriminative classifier is learnt to classify projected data from the target domain. We discuss extensions of our approach for semi-supervised adaptation, and for cases with multiple source and target domains, and report competitive results on standard datasets. {\textcopyright} 2011 IEEE.},
author = {Gopalan, Raghuraman and Li, Ruonan and Chellappa, Rama},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126344},
file = {::},
isbn = {9781457711015},
pages = {999--1006},
title = {{Domain adaptation for object recognition: An unsupervised approach}},
year = {2011}
}
@inproceedings{Gong2012,
abstract = {In real-world applications of visual recognition, many factors such as pose, illumination, or image quality can cause a significant mismatch between the source domain on which classifiers are trained and the target domain to which those classifiers are applied. As such, the classifiers often perform poorly on the target domain. Domain adaptation techniques aim to correct the mismatch. Existing approaches have concentrated on learning feature representations that are invariant across domains, and they often do not directly exploit low-dimensional structures that are intrinsic to many vision datasets. In this paper, we propose a new kernel-based method that takes advantage of such structures. Our geodesic flow kernel models domain shift by integrating an infinite number of subspaces that characterize changes in geometric and statistical properties from the source to the target domain. Our approach is computationally advantageous, automatically inferring important algorithmic parameters without requiring extensive cross-validation or labeled data from either domain. We also introduce a metric that reliably measures the adaptability between a pair of source and target domains. For a given target domain and several source domains, the metric can be used to automatically select the optimal source domain to adapt and avoid less desirable ones. Empirical studies on standard datasets demonstrate the advantages of our approach over competing methods. {\textcopyright} 2012 IEEE.},
author = {Gong, Boqing and Shi, Yuan and Sha, Fei and Grauman, Kristen},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247911},
file = {::},
isbn = {9781467312264},
issn = {10636919},
pages = {2066--2073},
title = {{Geodesic flow kernel for unsupervised domain adaptation}},
year = {2012}
}
@inproceedings{Gong2013,
abstract = {Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-of-the-art significantly. Copyright 2013 by the author(s).},
author = {Gong, Boqing and Grauman, Kristen and Sha, Fei},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
issn = {1938-7228},
number = {PART 1},
pages = {222--230},
publisher = {PMLR},
title = {{Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation}},
year = {2013}
}
@inproceedings{Ganin2015,
abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard back-propagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
archivePrefix = {arXiv},
arxivId = {1409.7495},
author = {Ganin, Yaroslav and Lempitsky, Victor},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1409.7495},
file = {::},
isbn = {9781510810587},
issn = {1938-7228},
pages = {1180--1189},
publisher = {PMLR},
title = {{Unsupervised domain adaptation by backpropagation}},
volume = {2},
year = {2015}
}
@book{Sugiyama2012,
abstract = {This volume focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) changes but the conditional distributions of outputs (answers) is unchanged, and presents machine learning theory algorithms, and applications to overcome this variety of non-stationarity.},
author = {Sugiyama, Masashi and Kawanabe, Motoaki.},
isbn = {9780262017091},
pages = {261},
publisher = {MIT Press},
title = {{Machine learning in non-stationary environments : introduction to covariate shift adaptation}},
year = {2012}
}
@article{Lei2018,
abstract = {We develop a general framework for distribution-free predictive inference in regression, using conformal inference. The proposed methodology allows for the construction of a prediction band for the response variable using any estimator of the regression function. The resulting prediction band preserves the consistency properties of the original estimator under standard assumptions, while guaranteeing finite-sample marginal coverage even when these assumptions do not hold. We analyze and compare, both empirically and theoretically, the two major variants of our conformal framework: full conformal inference and split conformal inference, along with a related jackknife method. These methods offer different tradeoffs between statistical accuracy (length of resulting prediction intervals) and computational efficiency. As extensions, we develop a method for constructing valid in-sample prediction intervals called rank-one-out conformal inference, which has essentially the same computational efficiency as split conformal inference. We also describe an extension of our procedures for producing prediction bands with locally varying length, to adapt to heteroscedasticity in the data. Finally, we propose a model-free notion of variable importance, called leave-one-covariate-out or LOCO inference. Accompanying this article is an R package conformalInference that implements all of the proposals we have introduced. In the spirit of reproducibility, all of our empirical results can also be easily (re)generated using this package.},
archivePrefix = {arXiv},
arxivId = {1604.04173},
author = {Lei, Jing and G'Sell, Max and Rinaldo, Alessandro and Tibshirani, Ryan J. and Wasserman, Larry},
doi = {10.1080/01621459.2017.1307116},
eprint = {1604.04173},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Distribution-free,Model misspecification,Prediction band,Regression,Variable importance},
number = {523},
pages = {1094--1111},
publisher = {Taylor & Francis},
title = {{Distribution-Free Predictive Inference for Regression}},
volume = {113},
year = {2018}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
volume = {39},
year = {2017}
}
@article{Moja2014,
abstract = {We systematically reviewed randomized controlled trials (RCTs) assessing the effectiveness ofcomputerized decision support systems (CDSSs) featuring rule- or algorithm-based software integrated with electronic health records (EHRs) and evidence-based knowledge. We searched MEDLINE, EMBASE, Cochrane Central Register of Controlled Trials, and Cochrane Database of Abstracts of Reviews of Effects. Information on system design, capabilities, acquisition, implementation context, and effects on mortality, morbidity, and economic outcomes were extracted. Twenty-eight RCTs were included. CDSS use did not affect mortality (16 trials, 37395 patients; 2282 deaths; risk ratio [RR] = 0.96; 95%confidence interval [CI] = 0.85, 1.08; I2 = 41%).Astatistically significant effect was evident in the prevention ofmorbidity, any disease (9 RCTs; 13868 patients;RR = 0.82; 95% CI = 0.68, 0.99; I2 = 64%), but selectiveoutcomereporting or publication bias cannot be excluded. We observed differences for costs and health service utilization, although these were often small in magnitude. Across clinical settings, new generation CDSSs integrated with EHRs do not affect mortality and might moderately improvemorbidity outcomes.},
author = {Moja, Lorenzo and Kwag, Koren H. and Lytras, Theodore and Bertizzolo, Lorenzo and Brandt, Linn and Pecoraro, Valentina and Rigon, Giulio and Vaona, Alberto and Ruggiero, Francesca and Mangia, Massimo and Iorio, Alfonso and Kunnamo, Ilkka and Bonovas, Stefanos},
doi = {10.2105/AJPH.2014.302164},
file = {::},
issn = {15410048},
journal = {American Journal of Public Health},
keywords = {AJPH,APHA,American,Association,Health,Journal,Public,ethics,infrastructure,policy,practice},
number = {12},
pages = {e12--e22},
pmid = {25322302},
publisher = {American Public Health Association},
title = {{Effectiveness of computerized decision support systems linked to electronic health records: A systematic review and meta-analysis}},
volume = {104},
year = {2014}
}
@article{Kitani2012,
abstract = {We address the task of inferring the future actions of people from noisy visual input. We denote this task activity forecasting. To achieve accurate activity forecasting, our approach models the effect of the physical environment on the choice of human actions. This is accomplished by the use of state-of-the-art semantic scene understanding combined with ideas from optimal control theory. Our unified model also integrates several other key elements of activity analysis, namely, destination forecasting, sequence smoothing and transfer learning. As proof-of-concept, we focus on the domain of trajectory-based activity analysis from visual input. Experimental results demonstrate that our model accurately predicts distributions over future actions of individuals. We show how the same techniques can improve the results of tracking algorithms by leveraging information about likely goals and trajectories. {\textcopyright} 2012 Springer-Verlag.},
author = {Kitani, Kris M. and Ziebart, Brian D. and Bagnell, James Andrew and Hebert, Martial},
doi = {10.1007/978-3-642-33765-9_15},
file = {::},
isbn = {9783642337642},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {activity forecasting,inverse optimal control},
number = {PART 4},
pages = {201--214},
publisher = {Springer, Berlin, Heidelberg},
title = {{Activity forecasting}},
volume = {7575 LNCS},
year = {2012}
}
@article{Bojarski2016,
abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
archivePrefix = {arXiv},
arxivId = {1604.07316},
author = {Bojarski, Mariusz and {Del Testa}, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
eprint = {1604.07316},
file = {::},
journal = {arXiv preprint arXiv:1604.07316v1},
title = {{End to End Learning for Self-Driving Cars}},
year = {2016}
}
@inproceedings{Gal2017,
abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolu-tional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
archivePrefix = {arXiv},
arxivId = {1703.02910},
author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
doi = {10.17863/CAM.11070},
eprint = {1703.02910},
file = {::},
isbn = {9781510855144},
issn = {2640-3498},
pages = {1923--1932},
publisher = {PMLR},
title = {{Deep Bayesian active learning with image data}},
volume = {3},
year = {2017}
}
@inproceedings{Malik2019,
abstract = {Estimates of predictive uncertainty are important for accurate model-based planning and reinforcement learning. However, predictive uncertainties-especially ones derived from modern deep learning systems-can be inaccurate and impose a bottleneck on performance. This paper explores which uncertainties are needed for model-based reinforcement learning and argues that good uncertainties must be calibrated, i.e. their probabilities should match empirical frequencies of predicted events. We describe a simple way to augment any model-based reinforcement learning agent with a calibrated model and show that doing so consistently improves planning, sample complexity , and exploration. On the HALFCHEETAH MuJoCo task, our system achieves state-of-the-art performance using 50% fewer samples than the current leading approach. Our findings suggest that calibration can improve the performance of model-based reinforcement learning with minimal computational and implementation overhead.},
author = {Malik, Ali and Kuleshov, Volodymyr and Song, Jiaming and Nemer, Danny and Seymour, Harlan and Ermon, Stefano},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
file = {::},
issn = {2640-3498},
pages = {4314--4323},
publisher = {PMLR},
title = {{Calibrated Model-Based Deep Reinforcement Learning}},
year = {2019}
}
@article{Berkenkamp2017,
abstract = {Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.},
archivePrefix = {arXiv},
arxivId = {1705.08551},
author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela P. and Krause, Andreas},
eprint = {1705.08551},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {909--919},
publisher = {Neural information processing systems foundation},
title = {{Safe model-based reinforcement learning with stability guarantees}},
volume = {2017-Decem},
year = {2017}
}
@article{VanEngelen2020,
abstract = {Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
author = {van Engelen, Jesper E. and Hoos, Holger H.},
doi = {10.1007/s10994-019-05855-6},
file = {::},
issn = {15730565},
journal = {Machine Learning},
keywords = {Classification,Machine learning,Semi-supervised learning},
number = {2},
pages = {373--440},
publisher = {Springer},
title = {{A survey on semi-supervised learning}},
volume = {109},
year = {2020}
}
@inproceedings{Chernozhukov2018,
abstract = {We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and accounts for potential serial dependence by including block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods. When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score.},
archivePrefix = {arXiv},
arxivId = {1802.06300},
author = {Chernozhukov, Victor and Wuthrich, Kaspar and Zhu, Yinchu},
booktitle = {Proceedings of the 31st Conference On Learning Theory, PMLR},
eprint = {1802.06300},
file = {::},
issn = {2640-3498},
keywords = {Conformal inference,dependent data,groups,permutation and randomization},
pages = {732--749},
publisher = {PMLR},
title = {{Exact and Robust Conformal Inference Methods for Predictive Machine Learning With Dependent Data}},
url = {http://arxiv.org/abs/1802.06300},
volume = {75},
year = {2018}
}
@article{Hendrycks2019,
abstract = {In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
archivePrefix = {arXiv},
arxivId = {1903.12261},
author = {Hendrycks, Dan and Dietterich, Thomas},
eprint = {1903.12261},
file = {::},
journal = {7th International Conference on Learning Representations, ICLR 2019},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Benchmarking neural network robustness to common corruptions and perturbations}},
year = {2019}
}
@article{Szegedy2014,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
eprint = {1312.6199},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Intriguing properties of neural networks}},
year = {2014}
}

@article{Cauchois2020,
abstract = {While the traditional viewpoint in machine learning and statistics assumes training and testing samples come from the same population, practice belies this fiction. One strategy---coming from robust statistics and optimization---is thus to build a model robust to distributional perturbations. In this paper, we take a different approach to describe procedures for robust predictive inference, where a model provides uncertainty estimates on its predictions rather than point predictions. We present a method that produces prediction sets (almost exactly) giving the right coverage level for any test distribution in an $f$-divergence ball around the training population. The method, based on conformal inference, achieves (nearly) valid coverage in finite samples, under only the condition that the training data be exchangeable. An essential component of our methodology is to estimate the amount of expected future data shift and build robustness to it; we develop estimators and prove their consistency for protection and validity of uncertainty estimates under shifts. By experimenting on several large-scale benchmark datasets, including Recht et al.'s CIFAR-v4 and ImageNet-V2 datasets, we provide complementary empirical results that highlight the importance of robust predictive validity.},
archivePrefix = {arXiv},
arxivId = {2008.04267},
author = {Cauchois, Maxime and Gupta, Suyash and Ali, Alnur and Duchi, John C.},
eprint = {2008.04267},
file = {::},
journal = {arXiv preprint arXiv:2008.04267v1},
title = {{Robust Validation: Confident Predictions Even When Distributions Shift}},
year = {2020}
}
@article{Parzen1962,
abstract = {T},
author = {Parzen, Emanuel},
doi = {10.1214/aoms/1177704472},
file = {::},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {1065--1076},
title = {{On Estimation of a Probability Density Function and Mode}},
volume = {33},
year = {1962}
}
@article{Rosenblatt1956,
abstract = {This note discusses some aspects of the estimation of the density function of a univariate probability distribution. All estimates of the density function satisfying relatively mild conditions are shown to be biased. The asymptotic mean square error of a particular class of estimates is evaluated.},
author = {Rosenblatt, Murray},
doi = {10.1214/aoms/1177728190},
file = {::},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {832--837},
publisher = {Institute of Mathematical Statistics},
title = {{Remarks on Some Nonparametric Estimates of a Density Function}},
volume = {27},
year = {1956}
}
@article{Marron1994,
author = {Marron, J. S.},
doi = {10.1080/10618600.1994.10474657},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Density estimation,Nonparametric regression,Smoothing},
mendeley-groups = {Sieve Correction},
number = {4},
pages = {447--458},
title = {{Visual understanding of higher-order kernels}},
volume = {3},
year = {1994}
}
@article{Rosenbaum1983,
abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot. {\textcopyright} 1983 Biometrika Trust.},
author = {Rosenbaum, Paul R. and Rubin, Donald B.},
doi = {10.1093/biomet/70.1.41},
file = {::},
issn = {00063444},
journal = {Biometrika},
keywords = {Covariance adjustment,Direct adjustment,Discriminant matching,Matched sampling,Nonrandomized study,Standardization,Stratification,Subclassification},
number = {1},
pages = {41--55},
publisher = {Oxford Academic},
title = {{The central role of the propensity score in observational studies for causal effects}},
volume = {70},
year = {1983}
}
@article{VanderLaan2007,
abstract = {When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox. A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression. Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners. Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner. This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners. In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions. This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.},
author = {van der Laan, Mark J. and Polley, Eric C and Hubbard, Alan E.},
doi = {10.2202/1544-6115.1309},
issn = {2194-6302},
journal = {Statistical Applications in Genetics and Molecular Biology},
mendeley-groups = {IV individualized treatment rule},
number = {1},
publisher = {De Gruyter},
title = {{Super Learner}},
volume = {6},
year = {2007}
}
@book{Hastie1990,
author = {Hastie, T. J. and Tibshirani, R. J.},
isbn = {9780412343902},
publisher = {Chapman and Hall/CRC},
title = {{Generalized Additive Models}},
year = {1990}
}
@book{Bishop1995,
author = {Bishop, Christopher M.},
doi = {10.1016/S0065-2458(08)60404-0},
file = {::},
issn = {00652458},
mendeley-groups = {IV individualized treatment rule},
publisher = {Oxford University Press},
title = {{Neural Networks for Pattern Recognition}},
year = {1995}
}
@article{Cortes1995,
abstract = {In this paper, the optimal margin algorithm is generalized\nto non-separable problems by the introduction of slack\nvariables in the statement of the optimization problem.},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/bf00994018},
file = {::},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
mendeley-groups = {IV individualized treatment rule},
number = {3},
pages = {273--297},
publisher = {Springer Science and Business Media LLC},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Sadinle2019,
abstract = {In most classification tasks, there are observations that are ambiguous and therefore difficult to correctly label. Set-valued classifiers output sets of plausible labels rather than a single label, thereby giving a more appropriate and informative treatment to the labeling of ambiguous instances. We introduce a framework for multiclass set-valued classification, where the classifiers guarantee user-defined levels of coverage or confidence (the probability that the true label is contained in the set) while minimizing the ambiguity (the expected size of the output). We first derive oracle classifiers assuming the true distribution to be known. We show that the oracle classifiers are obtained from level sets of the functions that define the conditional probability of each class. Then we develop estimators with good asymptotic and finite sample properties. The proposed estimators build on existing single-label classifiers. The optimal classifier can sometimes output the empty set, but we provide two solutions to fix this issue that are suitable for various practical needs. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1609.00451},
author = {Sadinle, Mauricio and Lei, Jing and Wasserman, Larry},
doi = {10.1080/01621459.2017.1395341},
eprint = {1609.00451},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Ambiguous observation,Bayes classifier,Multiclass classification,Nondeterministic classifier,Oracle classifier,Reject option},
number = {525},
pages = {223--234},
publisher = {American Statistical Association},
title = {{Least Ambiguous Set-Valued Classifiers With Bounded Error Levels}},
volume = {114},
year = {2019}
}
@article{Wald1943,
author = {Wald, Abraham},
doi = {10.1214/aoms/1177731491},
file = {::},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {1},
pages = {45--55},
title = {{An Extension of Wilks' Method for Setting Tolerance Limits}},
volume = {14},
year = {1943}
}
@article{Wilks1941,
abstract = {The asymptotic behaviour of the residual life time at time t is investigated (for t rightarrow infty). We derive weak limit laws and their domains of attraction and treat rates of convergence and moment convergence. The presentation exploits the close similarity with extreme value theory.},
author = {Wilks, S. S.},
doi = {10.1214/aoms/1177731788},
file = {::},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {1},
pages = {91--96},
title = {{Determination of Sample Sizes for Setting Tolerance Limits}},
volume = {12},
year = {1941}
}
@article{Qiu2021,
abstract = {Suppose that we wish to estimate a finite-dimensional summary of one or more function-valued features of an underlying data-generating mechanism under a nonparametric model. One approach to estimation is by plugging in flexible estimates of these features. Unfortunately, in general, such estimators may not be asymptotically efficient, which often makes these estimators difficult to use as a basis for inference. Though there are several existing methods to construct asymptotically efficient plug-in estimators, each such method either can only be derived using knowledge of efficiency theory or is only valid under stringent smoothness assumptions. Among existing methods, sieve estimators stand out as particularly convenient because efficiency theory is not required in their construction, their tuning parameters can be selected data adaptively, and they are universal in the sense that the same fits lead to efficient plug-in estimators for a rich class of estimands. Inspired by these desirable properties, we propose two novel universal approaches for estimating function-valued features that can be analyzed using sieve estimation theory. Compared to traditional sieve estimators, these approaches are valid under more general conditions on the smoothness of the function-valued features by utilizing flexible estimates that can be obtained, for example, using machine learning.},
archivePrefix = {arXiv},
arxivId = {2003.01856},
author = {Qiu, Hongxiang and Luedtke, Alex and Carone, Marco},
doi = {10.3150/20-BEJ1309},
eprint = {2003.01856},
issn = {13507265},
journal = {Bernoulli},
keywords = {Asymptotic efficiency,Nonparametric inference,Sieve estimation},
number = {4},
pages = {2300--2336},
title = {{Universal sieve-based strategies for efficient estimation using machine learning tools}},
url = {https://arxiv.org/abs/2003.01856},
volume = {27},
year = {2021}
}
@article{Chen2007,
abstract = {Often researchers find parametric models restrictive and sensitive to deviations from the parametric specifications; semi-nonparametric models are more flexible and robust, but lead to other complications such as introducing infinite-dimensional parameter spaces that may not be compact and the optimization problem may no longer be well-posed. The method of sieves provides one way to tackle such difficulties by optimizing an empirical criterion over a sequence of approximating parameter spaces (i.e., sieves); the sieves are less complex but are dense in the original space and the resulting optimization problem becomes well-posed. With different choices of criteria and sieves, the method of sieves is very flexible in estimating complicated semi-nonparametric models with (or without) endogeneity and latent heterogeneity. It can easily incorporate prior information and constraints, often derived from economic theory, such as monotonicity, convexity, additivity, multiplicity, exclusion and nonnegativity. It can simultaneously estimate the parametric and nonparametric parts in semi-nonparametric models, typically with optimal convergence rates for both parts. This chapter describes estimation of semi-nonparametric econometric models via the method of sieves. We present some general results on the large sample properties of the sieve estimates, including consistency of the sieve extremum estimates, convergence rates of the sieve M-estimates, pointwise normality of series estimates of regression functions, root-n asymptotic normality and efficiency of sieve estimates of smooth functionals of infinite-dimensional parameters. Examples are used to illustrate the general results. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Chen, Xiaohong},
doi = {10.1016/S1573-4412(07)06076-X},
isbn = {9780444532008},
issn = {15734412},
journal = {Handbook of Econometrics},
keywords = {endogeneity in semi-nonparametric models,semiparametric two-step estimation,series,sieve extremum estimation,sieve minimum distance},
mendeley-groups = {Sieve Correction},
number = {SUPPL. PART B},
pages = {5549--5632},
title = {{Chapter 76: Large Sample Sieve Estimation of Semi-Nonparametric Models}},
volume = {6},
year = {2007}
}
@article{Shen1997,
abstract = {We develop a general theory which provides a unied treatment for the asymptotic normality and efciency of the maximum likelihood estimates (MLE's) in parametric, semiparametric and nonparametric models. We nd that the asymptotic behavior of substitution estimates for estimating smooth functionals are essentially governed by two indices: the degree of smoothness of the functional and the local size of the underlying parameter space. We show that when the local size of the parameter space is not very large, the substitution standard (nonsieve), substitution sieve and substitution penalized MLE's are asymptotically efcient in the Fisher sense, under certain stochastic equicontinuity conditions of the loglikelihood. Moreover, when the convergence rate of the estimate is slow, the degree of smoothness of the functional needs to compensate for the slowness of the rate in order to achieve efciency. When the size of the parameter space is very large, the standard and penalized maximum likelihood procedures may be inefcient, whereas the method of sieves may be able to overcome this difculty. This phenomenon is particularly manifested when the functional of interest is very smooth, especially in the semiparametric case.},
author = {Shen, Xiaotong},
doi = {10.1214/aos/1030741085},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Asymptotic normality,Constraints,Efficiency,Maximum likelihood estimation,Methods of sieves and penalization,Nonparametric and semiparametric models,Substitution},
mendeley-groups = {Sieve Correction},
number = {6},
pages = {2555--2591},
title = {{On methods of sieves and penalization}},
volume = {25},
year = {1997}
}
@article{Newey2004,
author = {Newey, Whitney K. and Hsieh, Fushing and Robins, James M.},
doi = {10.1111/j.1468-0262.2004.00518.x},
journal = {Econometrica},
mendeley-groups = {Sieve Correction},
number = {3},
pages = {947--962},
publisher = {John Wiley & Sons, Ltd (10.1111)},
title = {{Twicing Kernels and a Small Bias Property of Semiparametric Estimators}},
volume = {72},
year = {2004}
}
@article{Bickel2003,
abstract = {We consider nonparametric estimation of an object such as a probability density or a regression function. Can such an estimator achieve the ratewise minimax rate of convergence on suitable function spaces, while, at the same time, when "plugged-in," estimate efficiently (at a rate of$\sim$n−1/2 with the best constant) many functionals of the object? For example, can we have a density estimator whose definite integrals are efficient estimators of the cumulative distribution function? We show that this is impossible for very large sets, for example, expectations of all functions bounded by M<∞. However, we also show that it is possible for sets as large as indicators of all quadrants, that is, distribution functions. We give appropriate constructions of such estimates.},
author = {Bickel, Peter J. and Ritov, Ya'acov},
doi = {10.1214/aos/1059655904},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Density estimation,Efficient estimator,Nonparametric regression},
mendeley-groups = {Sieve Correction},
number = {4},
pages = {1033--1053},
title = {{Nonparametric estimators which can be ``plugged-in''}},
volume = {31},
year = {2003}
}
@article{Newey1998,
author = {Newey, Whitney and Hsieh, Fushing and Robins, James},
mendeley-groups = {Sieve Correction},
publisher = {Massachusetts Institute of Technology (MIT), Department of Economics},
title = {{Undersmoothing and Bias Corrected Functional Estimation}},
year = {1998}
}
@article{VanDerLaan2014,
author = {{Van Der Laan}, Mark J and Luedtke, Alexander R},
file = {::},
journal = {U.C. Berkeley Division of Biostatistics Working Paper Series.},
title = {{Targeted Learning of an Optimal Dynamic Treatment, and Statistical Inference for its Mean Outcome}},
year = {2014}
}
@article{VanderLaan2017,
abstract = {Suppose we observe},
author = {van der Laan, Mark},
doi = {10.1515/ijb-2015-0097},
issn = {15574679},
journal = {International Journal of Biostatistics},
keywords = {Donsker class,asymptotic linear estimator,canonical gradient,cross-validated targeted minimum loss estimation (,efficient estimator,efficient influence curve,empirical process,entropy,highly adaptive Lasso,influence curve,one-step TMLE,super-learning,targeted minimum loss estimation (TMLE)},
mendeley-groups = {IV individualized treatment rule,Sieve Correction,Discussion: Entropy Learning for Dynamic Treatment Regimes,BIOST572},
number = {2},
pmid = {29023235},
publisher = {NIH Public Access},
title = {{A Generally Efficient Targeted Minimum Loss Based Estimator based on the Highly Adaptive Lasso}},
volume = {13},
year = {2017}
}
@article{Luedtke2016,
abstract = {An individualized treatment rule (ITR) is a treatment rule which assigns treatments to individuals based on (a subset of) their measured covariates. An optimal ITR is the ITR which maximizes the population mean outcome. Previous works in this area have assumed that treatment is an unlimited resource so that the entire population can be treated if this strategy maximizes the population mean outcome. We consider optimal ITRs in settings where the treatment resource is limited so that there is a maximum proportion of the population which can be treated. We give a general closed-form expression for an optimal stochastic ITR in this resource-limited setting, and a closed-form expression for the optimal deterministic ITR under an additional assumption. We also present an estimator of the mean outcome under the optimal stochastic ITR in a large semiparametric model that at most places restrictions on the probability of treatment assignment given covariates. We give conditions under which our estimator is efficient among all regular and asymptotically linear estimators. All of our results are supported by simulations.},
author = {Luedtke, Alexander R. and {Van Der Laan}, Mark J.},
doi = {10.1515/ijb-2015-0007},
issn = {15574679},
journal = {International Journal of Biostatistics},
keywords = {Asymptotic linearity,efficient influence curve,individualized treatments,influence curve,resource constraint},
mendeley-groups = {IV individualized treatment rule,Discussion: Entropy Learning for Dynamic Treatment Regimes},
number = {1},
pages = {283--303},
publisher = {De Gruyter},
title = {{Optimal Individualized Treatments in Resource-Limited Settings}},
volume = {12},
year = {2016}
}
@article{Qiu2021ITR,
abstract = {There is an extensive literature on the estimation and evaluation of optimal individualized treatment rules in settings where all confounders of the effect of treatment on outcome are observed. We study the development of individualized decision rules in settings where some of these confounders may not have been measured but a valid binary instrument is available for a binary treatment. We first consider individualized treatment rules, which will naturally be most interesting in settings where it is feasible to intervene directly on treatment. We then consider a setting where intervening on treatment is infeasible, but intervening to encourage treatment is feasible. In both of these settings, we also handle the case that the treatment is a limited resource so that optimal interventions focus the available resources on those individuals who will benefit most from treatment. Given a reference rule, we evaluate an optimal individualized rule by its average causal effect relative to a prespecified reference rule. We develop methods to estimate optimal individualized rules and construct asymptotically efficient plug-in estimators of the corresponding average causal effect relative to a prespecified reference rule. Supplementary materials for this article are available online.},
author = {Qiu, Hongxiang and Carone, Marco and Sadikova, Ekaterina and Petukhova, Maria and Kessler, Ronald C. and Luedtke, Alex},
doi = {10.1080/01621459.2020.1745814},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Individualized treatment,Limited resources,Unmeasured confounders},
mendeley-groups = {IV individualized treatment rule},
number = {533},
pages = {174--191},
publisher = {American Statistical Association},
title = {{Optimal Individualized Decision Rules Using Instrumental Variable Methods}},
volume = {116},
year = {2021}
}
@book{vanderVaart1998,
abstract = {applicability for this approach.},
author = {van der Vaart, A. W.},
booktitle = {Asymptotic Statistics},
doi = {10.1017/cbo9780511802256},
mendeley-groups = {Gamma-minimax},
publisher = {Cambridge University Press},
title = {{Asymptotic Statistics}},
year = {1998}
}
@book{Kosorok2008,
abstract = {This book provides a self-contained, linear, and unified introduction to empirical processes and semiparametric inference.},
author = {Kosorok, Michael R.},
booktitle = {International Statistical Review},
doi = {10.1007/978-0-387-74978-5},
isbn = {978-0-387-74977-8},
issn = {03067734},
keywords = {Convergence,Empiricism,Estimation theory,Mathematical statistics,Probabilities,Sampling (Statistics),Stochastic processes},
mendeley-groups = {Gamma-minimax},
pages = {318--318},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{Introduction to Empirical Processes and Semiparametric Inference}},
url = {http://doi.wiley.com/10.1111/j.1751-5823.2009.00085_20.x%0Ahttp://link.springer.com/10.1007/978-0-387-74978-5},
volume = {77},
year = {2008}
}

@book{LeCam1969,
  title={Th{\'e}orie asymptotique de la d{\'e}cision statistique},
  author={Le Cam, Lucien Marie},
  volume={33},
  year={1969},
  publisher={Presses de l'Universit{\'e} de Montr{\'e}al}
}

@book{Tsiatis2006,
author = {Tsiatis, Anastasios A.},
publisher = {Springer},
title = {{Semiparametric theory and missing data}},
year = {2006}
}
@article{Han2019,
abstract = {This paper derives Berry-Esseen bounds for an important class of non-standard asymptotics in nonparametric statistics with Chernoff-type limiting distributions, with a focus on the isotonic regression model. In the canonical situation of cube-root asymptotics, we obtain a cube-root Berry-Esseen bound (up to multiplicative logarithmic factors) for the speed of distributional approximation of the isotonic estimate to its Chernoff limit. Our method of proof relies on localization techniques in isotonic regression and an anti-concentration inequality for the supremum of a Brownian motion with a polynomial drift. These techniques extend to various Chernoff-type limiting distributions in isotonic regression with (i) general local smoothness conditions, (ii) both interior and boundary points, and (iii) general designs of covariates, where the Berry-Esseen bounds in each scenario match that of the oracle local average estimator with optimal bandwidth, up to multiplicative logarithmic factors.},
archivePrefix = {arXiv},
arxivId = {1910.09662},
author = {Han, Qiyang and Kato, Kengo},
eprint = {1910.09662},
file = {::},
issn = {2331-8422},
journal = {arXiv preprint arXiv:1910.09662v2},
title = {{Berry-Esseen bounds for Chernoff-type non-standard asymptotics in isotonic regression}},
year = {2019}
}
@article{Zhang2011,
abstract = {Consider the heteroscedastic semi-parametric model yi=xiΒ+g(ti)+$\sigma$iei (1≤i≤n), where $\sigma$i2=f(ui), the design points (xi,ti,ui) are known and nonrandom, the functions g({\textperiodcentered}) and f({\textperiodcentered}) are defined on closed interval [0,1]. When the random errors {ei} are assumed to be a sequence of stationary $\alpha$-mixing random variables, we derive the Berry-Esseen type bounds for the estimators of Β and g({\textperiodcentered}) under f({\textperiodcentered}) is known, respectively. When f({\textperiodcentered}) is unknown, the Berry-Esseen type bounds for the estimators of Β, g({\textperiodcentered}) and f({\textperiodcentered}) are discussed under the errors {ei} are assumed to be independent but not necessarily identically distributed. As corollary, by choosing suitable weighted functions, the Berry-Esseen type bounds for the estimators of Β, g({\textperiodcentered}) and f({\textperiodcentered}) can achieve O(n-1/6+$\pi${variant}/3), O(n-1/12+$\pi${variant}/6) and O(n-1/12+$\pi${variant}/6), respectively, where 0<$\pi${variant}<1/2. {\textcopyright} 2011 Elsevier B.V.},
author = {Zhang, Jing Jing and Liang, Han Ying},
doi = {10.1016/j.jspi.2011.05.001},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Berry-Esseen type bound,Heteroscedastic semi-parametric model,Least-squares estimator,$\alpha$-Mixing},
number = {11},
pages = {3447--3462},
publisher = {North-Holland},
title = {{Berry-Esseen type bounds in heteroscedastic semi-parametric model}},
volume = {141},
year = {2011}
}
@book{Gine2016,
abstract = {"In nonparametric and high-dimensional statistical models, the classical Gauss-Fisher-Le Cam theory of the optimality of maximum likelihood estimators and Bayesian posterior inference does not apply, and new foundations and ideas have been developed in the past several decades. This book gives a coherent account of the statistical theory in infinite-dimensional parameter spaces. The mathematical foundations include self-contained 'mini-courses' on the theory of Gaussian and empirical processes, on approximation and wavelet theory, and on the basic theory of function spaces. The theory of statistical inference in such models - hypothesis testing, estimation and confidence sets - is then presented within the minimax paradigm of decision theory. This includes the basic theory of convolution kernel and projection estimation, but also Bayesian nonparametrics and nonparametric maximum likelihood estimation. In the final chapter, the theory of adaptive inference in nonparametric models is developed, including Lepski's method, wavelet thresholding, and adaptive inference for self-similar functions."--Publisher's description. 1. Nonparametric statistical models -- 2. Gaussian processes -- 3. Empirical processes -- 4. Function spaces and approximation theory -- 5. Linear nonparametric estimators -- 6. The minimax paradigm -- 7. Likelihood-based procedures -- 8. Adaptive inference.},
author = {Gine, Evarist and Nickl, Richard},
booktitle = {Mathematical Foundations of Infinite-Dimensional Statistical Models},
doi = {10.1017/cbo9781107337862},
isbn = {9781107043169},
publisher = {Cambridge University Press},
title = {{Mathematical Foundations of Infinite-Dimensional Statistical Models}},
year = {2016}
}
@inproceedings{Benkeser2016,
abstract = {Estimation of a regression functions is a common goal of statistical learning. We propose a novel nonparametric regression estimator that, in contrast to many existing methods, does not rely on local smoothness assumptions nor is it constructed using local smoothing techniques. Instead, our estimator respects global smoothness constraints by virtue of falling in a class of right-hand continuous functions with left-hand limits that have variation norm bounded by a constant. Using empirical process theory, we establish a fast minimal rate of convergence of our proposed estimator and illustrate how such an estimator can be constructed using standard software. In simulations, we show that the finite-sample performance of our estimator is competitive with other popular machine learning techniques across a variety of data generating mechanisms. We also illustrate competitive performance in real data examples using several publicly available data sets.},
author = {Benkeser, David and van der Laan, Mark},
booktitle = {Data Science and Advanced Analytics (DSAA), 2016 IEEE International Conference on},
doi = {10.1109/DSAA.2016.93},
file = {::},
mendeley-groups = {IV individualized treatment rule,Sieve Correction,Discussion: Entropy Learning for Dynamic Treatment Regimes,BIOST572},
organization = {IEEE},
pages = {689--696},
title = {{The Highly Adaptive Lasso Estimator}},
year = {2016}
}
@book{Wakefield2013,
author = {Wakefield, Jon},
doi = {10.1007/978-1-4419-0925-1},
isbn = {978-1-4419-0924-4},
publisher = {Springer Science \& Business Media},
title = {{Bayesian and Frequentist Regression Methods}},
year = {2013}
}
@inproceedings{Menon2016,
abstract = {Given samples from two densities p and q, density ratio estimation (DRE) is the problem of estimating the ratio p/q. In this paper, we formally relate DRE and class-probability estimation (CPE), and theoretically justify the use of existing losses from one problem for the other. In the CPE to DRE direction, we show that essentially any CPE loss (e.g. logistic, exponential) minimises a Bregman divergence to the true density ratio, and thus can be used for DRE. We also show how different losses focus on accurately modelling different ranges of the density ratio, and use this to design new CPE losses for DRE. In the DRE to CPE direction, we argue that the least squares importance fitting method has potential use for bipartite ranking of instances with maximal accuracy at the head of the ranking. Our analysis relies on a novel Bregman divergence identity that may be of independent interest.},
author = {Menon, Aditya Krishna and Ong, Cheng Soon},
booktitle = {33rd International Conference on Machine Learning, ICML 2016},
file = {::},
isbn = {9781510829008},
issn = {1938-7228},
pages = {484--504},
publisher = {PMLR},
title = {{Linking losses for density ratio and class-probability estimation}},
volume = {1},
year = {2016}
}
@article{Sugiyama2008,
abstract = {A situation where training and test samples follow different input distributions is called covariate shift. Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent-weighted variants according to the ratio of test and training input densities are consistent. Therefore, accurately estimating the density ratio, called the importance, is one of the key issues in covariate shift adaptation. A naive approach to this task is to first estimate training and test input densities separately and then estimate the importance by taking the ratio of the estimated densities. However, this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional cases. In this paper, we propose a direct importance estimation method that does not involve density estimation. Our method is equipped with a natural cross validation procedure and hence tuning parameters such as the kernel width can be objectively optimized. Furthermore, we give rigorous mathematical proofs for the convergence of the proposed algorithm. Simulations illustrate the usefulness of our approach. {\textcopyright} 2008 The Institute of Statistical Mathematics, Tokyo.},
author = {Sugiyama, Masashi and Suzuki, Taiji and Nakajima, Shinichi and Kashima, Hisashi and {Von B{\"{u}}nau}, Paul and Kawanabe, Motoaki},
doi = {10.1007/s10463-008-0197-x},
file = {::},
issn = {00203157},
journal = {Annals of the Institute of Statistical Mathematics},
keywords = {Covariate shift,Importance sampling,Kullback-Leibler divergence,Likelihood cross validation,Model misspecification},
number = {4},
pages = {699--746},
title = {{Direct importance estimation for covariate shift adaptation}},
volume = {60},
year = {2008}
}
@article{Lei2014,
abstract = {We study distribution-free, non-parametric prediction bands with a focus on their finite sample behaviour. First we investigate and develop different notions of finite sample coverage guarantees. Then we give a new prediction band by combining the idea of 'conformal prediction' with non-parametric conditional density estimation. The proposed estimator, called COPS (conformal optimized prediction set), always has a finite sample guarantee. Under regularity conditions the estimator converges to an oracle band at a minimax optimal rate. A fast approximation algorithm and a data-driven method for selecting the bandwidth are developed. The method is illustrated in simulated and real data examples. {\textcopyright} 2013 Royal Statistical Society.},
author = {Lei, Jing and Wasserman, Larry},
doi = {10.1111/rssb.12021},
file = {::},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Conformal prediction,Finite sample property,Kernel density,Prediction bands},
number = {1},
pages = {71--96},
title = {{Distribution-free prediction bands for non-parametric regression}},
volume = {76},
year = {2014}
}

@techreport{Mason1999,
abstract = {Much recent attention, both experimental and theoretical, has been focussed on classii-cation algorithms which produce voted combinations of classiiers. Recent theoretical work has shown that the impressive generalization performance of algorithms like AdaBoost can be attributed to the classiier having large margins on the training data. We present abstract algorithms for nding linear and convex combinations of functions that minimize arbitrary cost functionals (i.e functionals that do not necessarily depend on the margin). Many existing voting methods can be shown to be special cases of these abstract algorithms. Then, following previous theoretical results bounding the generalization performance of convex combinations of classiiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations. Margin distribution plots verify that DOOM II is willing t{\`{o}}give up' on examples that are too hard in order to avoid overrtting. We also show that the overrtting behavior exhibited by AdaBoost can be quantiied in terms of our proposed cost function.},
author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter and Frean, Marcus},
mendeley-groups = {IV individualized treatment rule,Sieve Correction},
title = {{Boosting Algorithms as Gradient Descent in Function Space}},
volume={12},
pages={512--518},
year = {1999}
}
@techreport{Mason2000,
abstract = {We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions.},
archivePrefix = {arXiv},
arxivId = {arXiv:hep-ph/0607324v2},
author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter L. and Frean, Marcus},
booktitle = {In Advances in Neural Information Processing Systems 12},
doi = {10.1109/5.58323},
eprint = {0607324v2},
isbn = {0-7695-2000-6},
issn = {09205632},
mendeley-groups = {IV individualized treatment rule,Sieve Correction},
pages = {512--518},
pmid = {15852500},
primaryClass = {arXiv:hep-ph},
title = {{Boosting Algorithms as Gradient Descent}},
year = {2000}
}

@article{Friedman2002,
abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current "pseudo" -residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:hep-ph/0607324v2},
author = {Friedman, Jerome H.},
doi = {10.1016/S0167-9473(01)00065-2},
eprint = {0607324v2},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
mendeley-groups = {IV individualized treatment rule,Sieve Correction},
number = {4},
pages = {367--378},
pmid = {15852500},
primaryClass = {arXiv:hep-ph},
publisher = {Elsevier Science Publishers B. V.},
title = {{Stochastic gradient boosting}},
volume = {38},
year = {2002}
}
@techreport{Friedman2001,
abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization iti function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitives highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
author = {Friedman, Jerome H.},
booktitle = {Annals of Statistics},
doi = {10.1214/aos/1013203451},
issn = {00905364},
keywords = {Boosting,Decision trees,Function estimation,Robust nonparametric regression},
mendeley-groups = {IV individualized treatment rule,Sieve Correction},
number = {5},
pages = {1189--1232},
title = {{Greedy function approximation: A gradient boosting machine}},
volume = {29},
year = {2001}
}

@inproceedings{Vovk2013,
abstract = {Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given confidence level. Inductive conformal predictors are a computationally efficient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have only been known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modifications. In particular, it discusses a convenient expression of one of the modifications in terms of ROC curves. {\textcopyright} 2013 The Author(s).},
archivePrefix = {arXiv},
arxivId = {1209.2673},
author = {Vovk, Vladimir},
booktitle = {Asian conference on machine learning},
doi = {10.1007/s10994-013-5355-6},
eprint = {1209.2673},
file = {::},
issn = {1938-7228},
keywords = {Batch mode of learning,Boosting,Conditional validity,Inductive conformal predictors,MART,ROC curves,Spam detection},
pages = {475--490},
publisher = {PMLR},
title = {{Conditional validity of inductive conformal predictors}},
volume = {25},
year = {2013}
}
@article{Yang2018,
abstract = {Causal inference with observational studies often relies on the assumptions of unconfoundedness and overlap of covariate distributions in different treatment groups. The overlap assumption is violated when some units have propensity scores close to 0 or 1, so both practical and theoretical researchers suggest dropping units with extreme estimated propensity scores. However, existing trimming methods often do not incorporate the uncertainty in this design stage and restrict inference to only the trimmed sample, due to the nonsmoothness of the trimming. We propose a smooth weighting, which approximates sample trimming and has better asymptotic properties. An advantage of our estimator is its asymptotic linearity, which ensures that the bootstrap can be used to make inference for the target population, incorporating uncertainty arising from both design and analysis stages. We extend the theory to the average treatment effect on the treated, suggesting trimming samples with estimated propensity scores close to 1.},
author = {Yang, S. and Ding, P.},
doi = {10.1093/biomet/asy008},
file = {::},
issn = {14643510},
journal = {Biometrika},
keywords = {Bootstrap,Limited overlap,Nonsmooth estimator,Potential outcome,Unconfoundedness},
mendeley-groups = {Sieve Correction},
number = {2},
pages = {487--493},
publisher = {Oxford Academic},
title = {{Asymptotic inference of causal effects with observational studies trimmed by the estimated propensity scores}},
volume = {105},
year = {2018}
}
@article{Chung2006,
abstract = {We examine a number of generalized and extended versions of concentration inequalities and martingale inequalities. These inequalities are effective for analyzing processes with quite general conditions as illustrated in an example for an infinite Polya process and web graphs.},
author = {Chung, Fan and Lu, Linyuan},
doi = {10.1080/15427951.2006.10129115},
file = {::},
issn = {15427951},
journal = {Internet Mathematics},
number = {1},
pages = {79--127},
title = {{Concentration inequalities and martingale inequalities: A survey}},
volume = {3},
year = {2006}
}
@article{Tanser2013,
author = {Tanser, Frank and B{\"{a}}rnighausen, Till and Grapsa, Erofili and Zaidi, Jaffer and Newell, Marie Louise},
doi = {10.1126/science.1228160},
file = {::},
issn = {10959203},
journal = {Science},
number = {6122},
pages = {966--971},
publisher = {American Association for the Advancement of Science},
title = {{High coverage of ART associated with decline in risk of HIV acquisition in rural KwaZulu-Natal, South Africa}},
volume = {339},
year = {2013}
}
@article{Groenwold2012,
abstract = {Appendix 1: Explanation of bias when using the missing-indicator method (as supplied by the authors) Suppose one wants to predict blood pressure as a function of age and sex. Further, suppose the covariate age is missing in a number of observations in the study population. When using the indicator method to handle missing covariate data, the missing values are set to 0 (or any other value as long as the same number is used for all missing observations), and a new variable (indicating missingness) is defined and set to 1 if age is missing and 0 otherwise. Then, not only age and sex but also the new variable (the missingness indicator) are included in a multivariable model: Blood pressure = b 0 + b 1 .age + b 2 .sex + b 3 .Indicator For those subjects without missing data, the Indicator is zero and the model fitted to the data is: Blood pressure = b 0 + b 1 .age + b 2 .sex. For those subjects with missing data on the variable age, the Indicator is one and the model fitted to the data is: Blood pressure = b 0 + b 1 .age + b 2 .sex + b 3. Since all missing values on age are set to zero, this model is in fact: Blood pressure = b 0 + b 2 .sex + b 3. Clearly, when estimating the regression coefficient of sex (b 2), the relation between age and sex (i.e., the confounding effect of age on the association between sex and blood pressure) is not taken into account in the latter model. This is no problem if age and sex are not related and missingness of age is not related to the outcome conditional on (or given) sex. Then, the regression coefficient of sex (b 2) is correctly estimated, even though age is not included (adjusted for) in the model. However, when age and sex are related, the estimated regression coefficient of sex (b 2) will be biased, since the relation between sex and age is not adequately taken into account. Formally,},
author = {Groenwold, Rolf H.H. and White, Ian R. and Donders, A. Rogier T. and Carpenter, James R. and Altman, Douglas G. and Moons, Karel G.M.},
doi = {10.1503/cmaj.110977},
file = {::},
issn = {14882329},
journal = {Cmaj},
number = {11},
pages = {1265--1269},
pmid = {22371511},
publisher = {Canadian Medical Association},
title = {{Missing covariate data in clinical research: When and when not to use the missing-indicator method for analysis}},
volume = {184},
year = {2012}
}
@article{Yang2022,
abstract = {Conformal prediction has received tremendous attention in recent years and has offered new solutions to problems in missing data and causal inference; yet these advances have not leveraged modern semiparametric efficiency theory for more robust and efficient uncertainty quantification. In this paper, we consider the problem of obtaining distribution-free prediction regions accounting for a shift in the distribution of the covariates between the training and test data. Under an explainable covariate shift assumption analogous to the standard missing at random assumption, we propose three variants of a general framework to construct well-calibrated prediction regions for the unobserved outcome in the test sample. Our approach is based on the efficient influence function for the quantile of the unobserved outcome in the test population combined with an arbitrary machine learning prediction algorithm, without compromising asymptotic coverage. Next, we extend our approach to account for departure from the explainable covariate shift assumption in a semiparametric sensitivity analysis for potential latent covariate shift. In all cases, we establish that the resulting prediction sets eventually attain nominal average coverage in large samples. This guarantee is a consequence of the product bias form of our proposal which implies correct coverage if either the propensity score or the conditional distribution of the response is estimated sufficiently well. Our results also provide a framework for construction of doubly robust prediction sets of individual treatment effects, under both unconfoundedness and allowing for some degree of unmeasured confounding. Finally, we discuss aggregation of prediction sets from different machine learning algorithms for optimal prediction and illustrate the performance of our methods in both synthetic and real data.},
archivePrefix = {arXiv},
arxivId = {2203.01761},
author = {Yang, Yachong and Kuchibhotla, Arun Kumar and Tchetgen, Eric Tchetgen},
doi = {10.48550/arxiv.2203.01761},
eprint = {2203.01761},
file = {::},
journal = {arXiv preprint arXiv:2203.01761},
title = {{Doubly Robust Calibration of Prediction Sets under Covariate Shift}},
year = {2022}
}
@book{Little2019,
abstract = {Praise for the First Edition of Statistical Analysis with Missing Data “An important contribution to the applied statistics literature.... I give the book high marks for unifying and making accessible much of the past and current work in this important area.”—William E. Strawderman, Rutgers University “This book...provide[s] interesting real-life examples, stimulating end-of-chapter exercises, and up-to-date references. It should be on every applied statistician's bookshelf.”—The Statistician “The book should be studied in the statistical methods department in every statistical agency.”—Journal of Official Statistics Statistical analysis of data sets with missing values is a pervasive problem for which standard methods are of limited value. The first edition of Statistical Analysis with Missing Data has been a standard reference on missing-data methods. Now, reflecting extensive developments in Bayesian methods for simulating posterior distributions, this Second Edition by two acknowledged experts on the subject offers a thoroughly up-to-date, reorganized survey of current methodology for handling missing-data problems. Blending theory and application, authors Roderick Little and Donald Rubin review historical approaches to the subject and describe rigorous yet simple methods for multivariate analysis with missing values. They then provide a coherent theory for analysis of problems based on likelihoods derived from statistical models for the data and the missing-data mechanism and apply the theory to a wide range of important missing-data problems. The new edition now enlarges its coverage to include: Expanded coverage of Bayesian methodology, both theoretical and computational, and of multiple imputation Analysis of data with missing values where inferences are based on likelihoods derived from formal statistical models for the data-generating and missing-data mechanisms Applications of the approach in a variety of contexts including regression, factor analysis, contingency table analysis, time series, and sample survey inference Extensive references, examples, and exercises Amstat News asked three review editors to rate their top five favorite books in the September 2003 issue. Statistical Analysis With Missing Data was among those chosen.},
author = {Little, Roderick J.A. and Rubin, Donald B.},
booktitle = {Statistical Analysis with Missing Data},
doi = {10.1002/9781119013563},
isbn = {9781119013563},
publisher = {John Wiley \& Sons, Ltd},
title = {{Statistical analysis with missing data}},
year = {2019}
}
@article{Scharfstein1999,
abstract = {Consider a study whose design calls for the study subjects to be followed from enrollment (time t = 0) to time t = T, at which point a primary endpoint of interest Y is to be measured. The design of the study also calls for measurements on a vector Vt) of covariates to be made at one or more times t during the interval [0, T). We are interested in making inferences about the marginal mean $\mu$0 of Y when some subjects drop out of the study at random times Q prior to the common fixed end of follow-up time T. The purpose of this article is to show how to make inferences about $\mu$0 when the continuous drop-out time Q is modeled semiparametrically and no restrictions are placed on the joint distribution of the outcome and other measured variables. In particular, we consider two models for the conditional hazard of drop-out given (V(T), Y), where V(t) denotes the history of the process Vt) through time t, t ∈ [0, T). In the first model, we assume that $\lambda$Q(t|V(T), Y) exp($\alpha$0Y), where $\alpha$0 is a scalar parameter and $\lambda$0(t|V(t)) is an unrestricted positive function of t and the process V(t). When the process Vt) is high dimensional, estimation in this model is not feasible with moderate sample sizes, due to the curse of dimensionality. For such situations, we consider a second model that imposes the additional restriction that $\lambda$0(t|V(t)) = $\lambda$0(t) exp($\gamma$′0(t)), where $\lambda$0t) is an unspecified baseline hazard function, W(t) = w(t, V(t)), w({\textperiodcentered},{\textperiodcentered}) is a known function that maps (t, V(t)) to Rq, and $\gamma$0 is a q × 1 unknown parameter vector. When $\alpha$0 ≠ 0, then drop-out is nonignorable. On account of identifiability problems, joint estimation of the mean $\mu$0 of Y and the selection bias parameter $\alpha$0 may be difficult or impossible. Therefore, we propose regarding the selection bias parameter $\alpha$0 as known, rather than estimating it from the data. We then perform a sensitivity analysis to see how inference about $\alpha$0 changes as we vary $\alpha$0 over a plausible range of values. We apply our approach to the analysis of ACTG 175, an AIDS clinical trial. {\textcopyright} 1999 Taylor & Francis Group, LLC.},
author = {Scharfstein, Daniel O. and Rotnitzky, Andrea and Robins, James M.},
doi = {10.1080/01621459.1999.10473862},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Augmented inverse probability of censoring weighted estimators,Cox proportional hazards model,Identification; Missing data,Noncompliance; Nonparametric methods,Randomized trials,Sensitivity analysis,Time-dependent covariates},
number = {448},
pages = {1096--1120},
title = {{Adjusting for Nonignorable Drop-Out Using Semiparametric Nonresponse Models}},
volume = {94},
year = {1999}
}
@article{Bang2005,
abstract = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
author = {Bang, Heejung and Robins, James M.},
doi = {10.1111/j.1541-0420.2005.00377.x},
file = {::},
issn = {15410420},
journal = {Biometrics},
keywords = {Causal inference,Doubly robust estimation,Longitudinal data,Marginal structural model,Missing data,Semiparametrics},
number = {4},
pages = {962--973},
pmid = {16401269},
publisher = {John Wiley & Sons, Ltd},
title = {{Doubly robust estimation in missing data and causal inference models}},
volume = {61},
year = {2005}
}
@inproceedings{Robins2000,
author = {Robins, James M},
booktitle = {Proceedings of the American Statistical Association},
pages = {6--10},
title = {{Robust estimation in sequentially ignorable missing data and causal inference models}},
volume = {1999},
year = {2000}
}
@article{Rotnitzky2021,
abstract = {We study a class of parameters with the so-called mixed bias property. For parameters with this property, the bias of the semiparametric efficient one-step estimator is equal to the mean of the product of the estimation errors of two nuisance functions. In nonparametric models, parameters with the mixed bias property admit so-called rate doubly robust estimators, i.e., estimators that are consistent and asymptotically normal when one succeeds in estimating both nuisance functions at sufficiently fast rates, with the possibility of trading off slower rates of convergence for the estimator of one of the nuisance functions against faster rates for the estimator of the other nuisance function. We show that the class of parameters with the mixed bias property strictly includes two recently studied classes of parameters which, in turn, include many parameters of interest in causal inference. We characterize the form of parameters with the mixed bias property and of their influence functions. Furthermore, we derive two functional loss functions, each being minimized at one of the two nuisance functions. These loss functions can be used to derive loss-based penalized estimators of the nuisance functions.},
archivePrefix = {arXiv},
arxivId = {1904.03725},
author = {Rotnitzky, A. and Smucler, E. and Robins, J. M.},
doi = {10.1093/biomet/asaa054},
eprint = {1904.03725},
issn = {14643510},
journal = {Biometrika},
keywords = {Average treatment effect,Causal inference,Doubly robust estimation,Influence function,One-step estimator},
number = {1},
pages = {231--238},
publisher = {Oxford Academic},
title = {{Characterization of parameters with a mixed bias property}},
volume = {108},
year = {2021}
}
@book{Efron1994,
author = {Efron, Bradley and Tibshirani, Robert J},
isbn = {0412042312},
publisher = {CRC Press},
title = {{An Introduction to the Bootstrap}},
year = {1994}
}
@book{Bickel1993,
abstract = {Asymptotic Inference for (Finite-Dimensional) Parametric Models -- Regular parametric models in the I.I.D. case -- Regular estimates of Euclidean parameters -- The information bound and the Hajek-Le Cam convolution and asymptotic minimax theorems -- Nuisance parameters, adaptation, and some geometry -- Construction of [square root]n-consistent and efficient estimates -- Information Bounds for Euclidean Parameters in Infinite-Dimensional Models -- Tangent spaces -- Information bounds via derivatives of functions: the nonparametric approach -- Information bound calculations via scores: the semiparametric approach -- Euclidean Parameters: Further Examples -- Introduction: models -- Semiparametric group models -- Regression models -- Biased sampling models -- Mixture models -- Missing data models -- Transformation models -- Information Bounds for Infinite-Dimensional Parameters -- Convolution theorems for regular estimates of infinite-dimensional parameters -- Differentiability of functions -- The "calculus" of efficient score and influence operators -- Infinite-Dimensional Parameters: Further Examples -- Constrained families -- Group models -- Biased sampling models -- Mixture models and models with monotonicity constraints -- Missing data and censoring -- Transformation models -- Construction of Estimates -- M-estimates for Euclidean parameters -- Generalized M-estimates for Euclidean parameters -- GMC- and GM-estimates corresponding to convex D.},
author = {Bickel, Peter and Klaassen, Chris AJ and Ritov, Ya'acov and Wellner, Jon A},
isbn = {9780387984735},
pages = {560},
publisher = {Johns Hopkins University Press},
title = {{Efficient and adaptive estimation for semiparametric models}},
year = {1993}
}
@book{Bickel2015,
abstract = {Mathematical Statistics: Basic Ideas and Selected Topics, Volume I, Second Edition presents fundamental, classical statistical concepts at the doctorate level. It covers estimation, prediction, testing, confidence sets, Bayesian analysis, and the general approach of decision theory. This edition gives careful proofs of major results and explains how the theory sheds light on the properties of practical methods. The book first discusses non- and semiparametric models before covering parameters and parametric models. It then offers a detailed treatment of maximum likelihood estimates (MLEs) and examines the theory of testing and confidence regions, including optimality theory for estimation and elementary robustness considerations. It next presents basic asymptotic approximations with one-dimensional parameter models as examples. The book also describes inference in multivariate (multiparameter) models, exploring asymptotic normality and optimality of MLEs, Wald and Rao statistics, generalized linear models, and more. Mathematical Statistics: Basic Ideas and Selected Topics, Volume II will be published in 2015. It will present important statistical concepts, methods, and tools not covered in Volume I.},
author = {Bickel, Peter J. and Doksum, Kjell A.},
booktitle = {Mathematical Statistics: Basic Ideas and Selected Topics, Second Edition},
doi = {10.1201/b18312},
file = {::},
isbn = {9781498723817},
pages = {1--547},
publisher = {Chapman and Hall/CRC},
title = {{Mathematical statistics: Basic ideas and selected topics, second edition}},
volume = {1},
year = {2015}
}
@article{Chernozhukov2018debiasedML,
abstract = {We revisit the classic semi&#8208;parametric problem of inference on a low&#8208;dimensional parameter &#952; in the presence of high&#8208;dimensional nuisance parameters &#951;. We depart from the classical setting by allowing for &#951; to be so high&#8208;dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate &#951;, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high&#8208;dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating &#951; cause a heavy bias in estimators of &#952; that are obtained by naively plugging ML estimators of &#951; into estimating equations for &#952;. This bias results in the naive estimator failing to be consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest &#952; can be removed by using two simple, yet critical, ingredients: (1) using Neyman&#8208;orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate &#952;; (2) making use of cross&#8208;fitting, which provides an efficient form of data&#8208;splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an &#8208;neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1111/ectj.12097},
issn = {1368423X},
journal = {Econometrics Journal},
mendeley-groups = {Sieve Correction},
number = {1},
pages = {C1--C68},
publisher = {Narnia},
title = {{Double/debiased machine learning for treatment and structural parameters}},
volume = {21},
year = {2018}
}
@article{Newey1990,
abstract = {Semiparametric models are those where the functional form of some components is unknown. Efficiency bounds are of fundamental importance for such models. They provide a guide to estimation methods and give an asymptotic efficiency standard. The purpose of this paper is to provide an introduction to research methods and problems for semiparametric efficiency bounds. The nature of the bounds is discussed, as well as ways of calculating them. Their uses in solving estimation problems are outlined, including construction of semiparametric estimators and calculation of their limiting distribution. The paper includes new results as well as survey material. Copyright {\textcopyright} 1990 John Wiley & Sons, Ltd.},
author = {Newey, Whitney K.},
doi = {10.1002/jae.3950050202},
issn = {10991255},
journal = {Journal of Applied Econometrics},
number = {2},
pages = {99--135},
publisher = {John Wiley & Sons, Ltd},
title = {{Semiparametric efficiency bounds}},
volume = {5},
year = {1990}
}
@article{vandervaart1991,
abstract = {Given a sample of size $n$ from a distribution $P_\lambda$, one wants to estimate a functional $\psi(\lambda)$ of the (typically infinite-dimensional) parameter $\lambda$. Lower bounds on the performance of estimators can be based on the concept of a differentiable functional $P_\lambda \rightarrow \psi(\lambda)$. In this paper we relate a suitable definition of differentiable functional to differentiability of $\alpha \rightarrow dP^{1/2}_\lambda$ and $\lambda \rightarrow \psi(\lambda)$. Moreover, we show that regular estimability of a functional implies its differentiability.},
author = {{Van Der Vaart}, Aad},
doi = {10.1214/aos/1176347976},
journal = {The Annals of Statistics},
number = {1},
pages = {178----204},
title = {{On Differentiable Functionals}},
volume = {19},
year = {1991}
}
@article{Shah2020,
abstract = {It is a common saying that testing for conditional independence, that is, testing whether whether two random vectors X and Y are independent, given Z, is a hard statistical problem if Z is a continuous random variable (or vector). In this paper, we prove that conditional independence is indeed a particularly difficult hypothesis to test for. Valid statistical tests are required to have a size that is smaller than a pre-defined significance level, and different tests usually have power against a different class of alternatives. We prove that a valid test for conditional independence does not have power against any alternative. Given the nonexistence of a uniformly valid conditional independence test, we argue that tests must be designed so their suitability for a particular problem may be judged easily. To address this need, we propose in the case where X and Y are univariate to nonlinearly regress X on Z, and Y on Z and then compute a test statistic based on the sample covariance between the residuals, which we call the generalised covariance measure (GCM). We prove that validity of this form of test relies almost entirely on the weak requirement that the regression procedures are able to estimate the conditional means X given Z, and Y given Z, at a slow rate. We extend the methodology to handle settings where X and Y may be multivariate or even high dimensional. While our general procedure can be tailored to the setting at hand by combining it with any regression technique, we develop the theoretical guarantees for kernel ridge regression. A simulation study shows that the test based on GCM is competitive with state of the art conditional independence tests. Code is available as the R package GeneralisedCovarianceMeasure on CRAN.},
archivePrefix = {arXiv},
arxivId = {1804.07203},
author = {Shah, Rajen D. and Peters, Jonas},
doi = {10.1214/19-AOS1857},
eprint = {1804.07203},
file = {::},
isbn = {1804.07203v6},
issn = {21688966},
journal = {Annals of Statistics},
keywords = {Conditional independence,Hypothesis testing,Kernel ridge regression,Testability,Wild bootstrap},
number = {3},
pages = {1514--1538},
title = {{The hardness of conditional independence testing and the generalised covariance measure}},
volume = {48},
year = {2020}
}
}
@article{Chernozhukov2020,
abstract = {We provide an adversarial approach to estimating Riesz representers of linear functionals within arbitrary function spaces. We prove oracle inequalities based on the localized Rademacher complexity of the function space used to approximate the Riesz representer and the approximation error. These inequalities imply fast finite sample mean-squared-error rates for many function spaces of interest, such as high-dimensional sparse linear functions, neural networks and reproducing kernel Hilbert spaces. Our approach offers a new way of estimating Riesz representers with a plethora of recently introduced machine learning techniques. We show how our estimator can be used in the context of de-biasing structural/causal parameters in semi-parametric models, for automated orthogonalization of moment equations and for estimating the stochastic discount factor in the context of asset pricing.},
archivePrefix = {arXiv},
arxivId = {2101.00009},
author = {Chernozhukov, Victor and Newey, Whitney and Singh, Rahul and Syrgkanis, Vasilis},
doi = {10.48550/arxiv.2101.00009},
eprint = {2101.00009},
file = {::},
journal = {arXiv preprint arXiv:2101.00009v1},
title = {{Adversarial Estimation of Riesz Representers}},
year = {2020}
}
@inproceedings{Vovk2012,
abstract = {Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given confidence level. Inductive conformal predictors are a computationally efficient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have only been known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modifications. In particular, it discusses a convenient expression of one of the modifications in terms of ROC curves. {\textcopyright} 2013 The Author(s).},
archivePrefix = {arXiv},
arxivId = {1209.2673},
author = {Vovk, Vladimir},
booktitle = {Asian conference on machine learning},
doi = {10.1007/s10994-013-5355-6},
eprint = {1209.2673},
file = {::},
issn = {1938-7228},
keywords = {Batch mode of learning,Boosting,Conditional validity,Inductive conformal predictors,MART,ROC curves,Spam detection},
pages = {475--490},
publisher = {PMLR},
title = {{Conditional validity of inductive conformal predictors}},
volume = {25},
year = {2012}
}
@article{Angelopoulos2021,
abstract = {We introduce a framework for calibrating machine learning models so that their predictions satisfy explicit, finite-sample statistical guarantees. Our calibration algorithms work with any underlying model and (unknown) data-generating distribution and do not require model refitting. The framework addresses, among other examples, false discovery rate control in multi-label classification, intersection-over-union control in instance segmentation, and the simultaneous control of the type-1 error of outlier detection and confidence set coverage in classification or regression. Our main insight is to reframe the risk-control problem as multiple hypothesis testing, enabling techniques and mathematical arguments different from those in the previous literature. We use the framework to provide new calibration methods for several core machine learning tasks, with detailed worked examples in computer vision and tabular medical data.},
archivePrefix = {arXiv},
arxivId = {2110.01052},
author = {Angelopoulos, Anastasios N. and Bates, Stephen and Cand{\`{e}}s, Emmanuel J. and Jordan, Michael I. and Lei, Lihua},
doi = {10.48550/arxiv.2110.01052},
eprint = {2110.01052},
file = {::},
isbn = {2110.01052v5},
journal = {arXiv preprint arXiv:2110.01052v5},
title = {{Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control}},
year = {2021}
}
@book{Gyofi2002,
abstract = {This book provides a systematic in-depth analysis of nonparametric regression with random design.},
address = {New York, NY},
author = {Gy{\"o}rfi, L{\'a}szl{\'o} and Kohler, Michael and Krzyzak, Adam and Walk, Harro},
doi = {10.1007/B97848},
isbn = {978-0-387-95441-7},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{A Distribution-Free Theory of Nonparametric Regression}},
year = {2002}
}
@article{Hajek1962,
abstract = {Having observed Xi = $\alpha$ + $\beta$ ci + $\sigma$ Yi, we test the hypothesis $\beta$ = 0 against the alternative $\beta &gt; 0$ . We suppose that the square root of the probability density f(x) of the residuals Yi possesses a quadratically integrable derivative and define a class of rank order tests, which are asymptotically most powerful for given f. The main result is exposed in the following succession: theorem, corollaries and examples, comments, preliminaries and proof. The proof is based on results by H{\'{a}}jek [6] and LeCam [8], [9]. Section 6 deals with asymptotic efficiency of rank-order tests, which is shown, on the basis of Mikulski&apos;s results [10], to be presumably never less than the asymptotic efficiency of corresponding parametric tests of Neyman&apos;s type [11]. This would extend the well-known result obtained by Chernoff and Savage [2] for the Student t-test. Furthermore, it is shown that the efficiency may be negative, i.e., asymptotic power may be less than the asymptotic size. In Section 7 we consider parallel rank-order tests of symmetry for judging paired comparisons. Section 8 is devoted to rank-order tests for densities such that (f(x))1/2 does not possess a quadratically integrable derivative. In Section 9, we construct a test which is asymptotically most powerful simultaneously for all densities f(x) such that (f(x))1/2 possesses a quadratically integrable derivative.},
author = {Hajek, Jaroslav},
doi = {10.1214/aoms/1177704476},
file = {::},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {1124--1147},
publisher = {Institute of Mathematical Statistics},
title = {{Asymptotically Most Powerful Rank-Order Tests}},
volume = {33},
year = {1962}
}
@article{Westling2021,
abstract = {In the absence of data from a randomized trial, researchers often aim to use observational data to draw causal inference about the effect of a treatment on a time-to-event outcome. In this context, interest often focuses on the treatment-specific survival curves; that is, the survival curves were the entire population under study to be assigned to receive the treatment or not. Under certain causal conditions, including that all confounders of the treatment-outcome relationship are observed, the treatment-specific survival can be identified with a covariate-adjusted survival function. Several estimators of this function have been proposed, including estimators based on outcome regression, inverse probability weighting, and doubly robust estimators. In this article, we propose a new cross-fitted doubly-robust estimator that incorporates data-adaptive (e.g. machine learning) estimators of the conditional survival functions. We establish conditions on the nuisance estimators under which our estimator is consistent and asymptotically linear, both pointwise and uniformly in time. We also propose a novel ensemble learner for combining multiple candidate estimators of the conditional survival estimators. Notably, our methods and results accommodate events occurring in discrete or continuous time (or both). We investigate the practical performance of our methods using numerical studies and an application to the effect of a surgical treatment to prevent metastases of parotid carcinoma on mortality.},
archivePrefix = {arXiv},
arxivId = {2106.06602},
author = {Westling, Ted and Luedtke, Alex and Gilbert, Peter and Carone, Marco},
eprint = {2106.06602},
journal = {arXiv preprint arXiv:2106.06602v1},
mendeley-groups = {Sequential doubly robust survival},
pages = {1--49},
title = {{Inference for treatment-specific survival curves using machine learning}},
url = {http://arxiv.org/abs/2106.06602},
year = {2021}
}
@article{Hudson2021,
abstract = {It is often of interest to make inference on an unknown function that is a local parameter of the data-generating mechanism, such as a density or regression function. Such estimands can typically only be estimated at a slower-than-parametric rate in nonparametric and semiparametric models, and performing calibrated inference can be challenging. In many cases, these estimands can be expressed as the minimizer of a population risk functional. Here, we propose a general framework that leverages such representation and provides a nonparametric extension of the score test for inference on an infinite-dimensional risk minimizer. We demonstrate that our framework is applicable in a wide variety of problems. As both analytic and computational examples, we describe how to use our general approach for inference on a mean regression function under (i) nonparametric and (ii) partially additive models, and evaluate the operating characteristics of the resulting procedures via simulations.},
archivePrefix = {arXiv},
arxivId = {2105.06646},
author = {Hudson, Aaron and Carone, Marco and Shojaie, Ali},
doi = {10.48550/arxiv.2105.06646},
eprint = {2105.06646},
file = {::},
isbn = {2105.06646v1},
journal = {arXiv preprint arXiv:2105.06646v1},
title = {{Inference on function-valued parameters using a restricted score test}},
year = {2021}
}
@article{Nadaraya1964,
author = {Nadaraya, Elizbar A},
journal = {Theory of Probability \& Its Applications},
mendeley-groups = {Sieve Correction,BIOST572},
number = {1},
pages = {141--142},
publisher = {SIAM},
title = {{On estimating regression}},
volume = {9},
year = {1964}
}
@article{romano2019conformalized,
  title={Conformalized quantile regression},
  author={Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{Watson1964,
author = {Watson, Geoffrey S},
journal = {Sankhyā: The Indian Journal of Statistics, Series A},
mendeley-groups = {BIOST572},
pages = {359--372},
publisher = {JSTOR},
title = {{Smooth regression analysis}},
year = {1964}
}
@article{Petersen2012,
abstract = {The assumption of positivity or experimental treatment assignment requires that observed treatment levels vary within confounder strata. This article discusses the positivity assumption in the context of assessing model and parameter-specific identifiability of causal effects. Positivity violations occur when certain subgroups in a sample rarely or never receive some treatments of interest. The resulting sparsity in the data may increase bias with or without an increase in variance and can threaten valid inference. The parametric bootstrap is presented as a tool to assess the severity of such threats and its utility as a diagnostic is explored using simulated and real data. Several approaches for improving the identifiability of parameters in the presence of positivity violations are reviewed. Potential responses to data sparsity include restriction of the covariate adjustment set, use of an alternative projection function to define the target parameter within a marginal structural working model, restriction of the sample, and modification of the target intervention. All of these approaches can be understood as trading off proximity to the initial target of inference for identifiability; we advocate approaching this tradeoff systematically. {\textcopyright} The Author(s) 2010.},
author = {Petersen, Maya L. and Porter, Kristin E. and Gruber, Susan and Wang, Yue and {Van Der Laan}, Mark J.},
doi = {10.1177/0962280210386207},
issn = {09622802},
journal = {Statistical Methods in Medical Research},
keywords = {causal inference,counterfactual,double robust,experimental treatment assignment,inverse probability weight,marginal structural model,parametric bootstrap,positivity,realistic treatment rule,stabilised weights,trimming,truncation},
month = {feb},
number = {1},
pages = {31--54},
pmid = {21030422},
publisher = {Stat Methods Med Res},
title = {{Diagnosing and responding to violations in the positivity assumption}},
volume = {21},
year = {2012}
}
@article{Tran2018,
abstract = {We consider a longitudinal data structure consisting of baseline covariates, time-varying treatment variables, intermediate time-dependent covariates, and a possibly time dependent outcome. Previous studies have shown that estimating the variance of asymptotically linear estimators using empirical influence functions in this setting result in anti-conservative estimates with increasing magnitudes of positivity violations, leading to poor coverage and uncontrolled Type I errors. In this paper, we present two alternative approaches of estimating the variance of these estimators: (i) a robust approach which directly targets the variance of the influence function as a counterfactual mean outcome, and (ii) a non-parametric bootstrap based approach that is theoretically valid and lowers the computational cost, thereby increasing the feasibility in non-parametric settings using complex machine learning algorithms. The performance of these approaches are compared to that of the empirical influence function in simulations across different levels of positivity violations and treatment effect sizes.},
archivePrefix = {arXiv},
arxivId = {1810.03030},
author = {Tran, Linh and Petersen, Maya and Schwab, Joshua and van der Laan, Mark J},
eprint = {1810.03030},
file = {::},
journal = {arXiv preprint arXiv:1810.03030},
title = {{Robust variance estimation and inference for causal effect estimation}},
url = {http://arxiv.org/abs/1810.03030},
year = {2018}
}
@article{Hahn1998,
abstract = {In this paper, the role of the propensity score in the efficient estimation of average treatment effects is examined. Under the assumption that the treatment is ignorable given some observed characteristics, it is shown that the propensity score is ancillary for estimation of the average treatment effects. The propensity score is not ancillary for estimation of average treatment effects on the treated. It is suggested that the marginal value of the propensity score lies entirely in the "dimension reduction." Efficient semi- parametric estimators of average treatment effects and average treatment effects on the treated are shown to take the form of relevant sample averages of the data completed by the nonparametric imputation method. It is shown that the projection on the propensity score is not necessar for efficient semiparametric estimation of average treatment effects on the treated even if the propensity score is known. An application to the experimental data reveals that conditioning on the propensity score may even result in a loss of efficiency.},
author = {Hahn, Jinyong},
doi = {10.2307/2998560},
issn = {00129682},
journal = {Econometrica},
mendeley-groups = {Proximal synthetic control},
number = {2},
pages = {315},
title = {{On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects}},
volume = {66},
year = {1998}
}
@article{Matsouaka2023,
abstract = {Common causal estimands include the average treatment effect, the average treatment effect of the treated, and the average treatment effect on the controls. Using augmented inverse probability weighting methods, parametric models are judiciously leveraged to yield doubly robust estimators, that is, estimators that are consistent when at least one the parametric models is correctly specified. Three sources of uncertainty are associated when we evaluate these estimators and their variances, that is, when we estimate the treatment and outcome regression models as well as the desired treatment effect. In this article, we propose methods to calculate the variance of the normalized, doubly robust average treatment effect of the treated and average treatment effect on the controls estimators and investigate their finite sample properties. We consider both the asymptotic sandwich variance estimation, the standard bootstrap as well as two wild bootstrap methods. For the asymptotic approximations, we incorporate the aforementioned uncertainties via estimating equations. Moreover, unlike the standard bootstrap procedures, the proposed wild bootstrap methods use perturbations of the influence functions of the estimators through independently distributed random variables. We conduct an extensive simulation study where we vary the heterogeneity of the treatment effect as well as the proportion of participants assigned to the active treatment group. We illustrate the methods using an observational study of critical ill patients on the use of right heart catherization.},
author = {Matsouaka, Roland A. and Liu, Yi and Zhou, Yunji},
doi = {10.1177/09622802221142532},
issn = {14770334},
journal = {Statistical Methods in Medical Research},
keywords = {M-theory,Robust variance estimation,propensity score weighting,treatment effect on the controls,treatment effect on the treated,wild bootstrap},
number = {2},
pages = {389--403},
pmid = {36476035},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Variance estimation for the average treatment effects on the treated and on the controls}},
volume = {32},
year = {2023}
}
@article{Rothe2017,
author = {Rothe, Christoph},
doi = {10.3982/ecta13141},
file = {::},
issn = {0012-9682},
journal = {Econometrica},
keywords = {Average treatment effect,causality,overlap,propensity score,treatment effect heterogeneity,unconfoundedness},
number = {2},
pages = {645--660},
publisher = {John Wiley & Sons, Ltd},
title = {{Robust Confidence Intervals for Average Treatment Effects Under Limited Overlap}},
volume = {85},
year = {2017}
}
@article{Bindele2018,
abstract = {In this paper, a regression semi-parametric model is considered where responses are assumed to be missing at random. From the empirical likelihood function defined based on the rank-based estimating equation, robust confidence intervals/regions of the true regression coefficient are derived. Monte Carlo simulation experiments show that the proposed approach provides more accurate confidence intervals/regions compared to its normal approximation counterpart under different model error structure. The approach is also compared with the least squares approach, and its superiority is shown whenever the error distribution in the simulation study is heavy tailed or contaminated. Finally, a real data example is given to illustrate our proposed method.},
author = {Bindele, Huybrechts F. and Abebe, Asheber and Meyer, Nicole K.},
doi = {10.1080/02331888.2018.1467419},
issn = {10294910},
journal = {Statistics},
keywords = {62G05,62G20,Primary 62J02,Secondary: 62F12,Wilcoxon estimator,confidence regions,empirical likelihood,imputation,missing at random},
number = {4},
pages = {885--900},
publisher = {Taylor & Francis},
title = {{Robust confidence regions for the semi-parametric regression model with responses missing at random}},
volume = {52},
year = {2018}
}
@article{Robins1994,
abstract = {In applied problems it is common to specify a model for the conditional mean of a response given a set of regressors. A subset of the regressors may be missing for some study subjects either by design or happenstance. In this article we propose a new class of semiparametric estimators, based on inverse probability weighted estimating equations, that are consistent for parameter vector $\alpha$0 of the conditional mean model when the data are missing at random in the sense of Rubin and the missingness probabilities are either known or can be parametrically modeled. We show that the asymptotic variance of the optimal estimator in our class attains the semiparametric variance bound for the model by first showing that our estimation problem is a special case of the general problem of parameter estimation in an arbitrary semiparametric model in which the data are missing at random and the probability of observing complete data is bounded away from 0, and then deriving a representation for the efficient score, the semiparametric variance bound, and the influence function of any regular, asymptotically linear estimator in this more general estimation problem. Because the optimal estimator depends on the unknown probability law generating the data, we propose locally and globally adaptive semiparametric efficient estimators. We compare estimators in our class with previously proposed estimators. We show that each previous estimator is asymptotically equivalent to some, usually inefficient, estimator in our class. This equivalence is a consequence of a proposition stating that every regular asymptotic linear estimator of $\alpha$0 is asymptotically equivalent to some estimator in our class. We compare various estimators in a small simulation study and offer some practical recommendations. {\textcopyright} 1994 Taylor & Francis Group, LLC.},
author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
doi = {10.1080/01621459.1994.10476818},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Cox proportional hazards model,Linear regression,Logistic regression,Measurement error,Missing covariates,Missing data,Nonlinear regression,Semiparametric efficiency,Survey sampling,Two-stage case-control studies,Validation study},
mendeley-groups = {Proximal synthetic control},
number = {427},
pages = {846--866},
publisher = { Taylor & Francis Group },
title = {{Estimation of regression coefficients when some regressors are not always observed}},
volume = {89},
year = {1994}
}


@inproceedings{park2022pac,
title={{PAC} Prediction Sets for Meta-Learning},
author={Sangdon Park and Edgar Dobriban and Insup Lee and Osbert Bastani},
booktitle={{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems},
year={2022}
}

@article{tibshirani2019conformal,
  title={Conformal prediction under covariate shift},
  author={Tibshirani, Ryan J and Foygel Barber, Rina and Cand{\`e}s, Emmanuel J and Ramdas, Aaditya},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{yang2024doubly,
  title={Doubly robust calibration of prediction sets under covariate shift},
  author={Yang, Yachong and Kuchibhotla, Arun Kumar and Tchetgen Tchetgen, Eric},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  pages={qkae009},
  year={2024},
  publisher={Oxford University Press US}
}

@article{qiu2023prediction,
  title={Prediction sets adaptive to unknown covariate shift},
  author={Qiu, Hongxiang and Dobriban, Edgar and Tchetgen Tchetgen, Eric},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={85},
  number={5},
  pages={1680--1705},
  year={2023},
  publisher={Oxford University Press US}
}

@article{qiu2024efficient,
  title={Efficient and multiply robust risk estimation under general forms of dataset shift},
  author={Qiu, Hongxiang and Tchetgen Tchetgen, Eric and Dobriban, Edgar},
  journal={The Annals of Statistics},
  volume={52},
  number={4},
  pages={1796--1824},
  year={2024},
  publisher={Institute of Mathematical Statistics}
}

@article{angelopoulos2021gentle,
  title={A gentle introduction to conformal prediction and distribution-free uncertainty quantification},
  author={Angelopoulos, Anastasios N and Bates, Stephen},
  journal={arXiv preprint arXiv:2107.07511},
  year={2021}
}

@article{gibbs2023conformal,
  title={Conformal prediction with conditional guarantees},
  author={Gibbs, Isaac and Cherian, John J and Cand{\`e}s, Emmanuel J},
  journal={arXiv preprint arXiv:2305.12616},
  year={2023}
}

@article{shalev2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={2635--2670},
  year={2010},
  publisher={JMLR. org}
}

@article{mei2018landscape,
  title={The landscape of empirical risk for nonconvex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={The Annals of Statistics},
  volume={46},
  number={6A},
  pages={2747--2774},
  year={2018},
  publisher={JSTOR}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{bastani2022practical,
  title={Practical adversarial multivalid conformal prediction},
  author={Bastani, Osbert and Gupta, Varun and Jung, Christopher and Noarov, Georgy and Ramalingam, Ramya and Roth, Aaron},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29362--29373},
  year={2022}
}

@inproceedings{jung2023batch,
  title={Batch Multivalid Conformal Prediction},
  author={Jung, Christopher and Noarov, Georgy and Ramalingam, Ramya and Roth, Aaron},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{kiyani2024length,
  title={Length optimization in conformal prediction},
  author={Kiyani, Shayan and Pappas, George and Hassani, Hamed},
  journal={arXiv preprint arXiv:2406.18814},
  year={2024}
}

@article{kiyani2024conformal,
  title={Conformal Prediction with Learned Features},
  author={Kiyani, Shayan and Pappas, George and Hassani, Hamed},
  journal={arXiv preprint arXiv:2404.17487},
  year={2024}
}

@article{noorani2024conformal,
  title={Conformal Risk Minimization with Variance Reduction},
  author={Noorani, Sima and Romero, Orlando and Fabbro, Nicolo Dal and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2411.01696},
  year={2024}
}


@inproceedings{StutzICLR2022,
    title={Learning Optimal Conformal Classifiers},
    author={David Stutz and Krishnamurthy Dj Dvijotham and Ali Taylan Cemgil and Arnaud Doucet},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=t8O-4LKFVx}
}


@article{bai2022efficient,
  title={Efficient and differentiable conformal prediction with general function classes},
  author={Bai, Yu and Mei, Song and Wang, Huan and Zhou, Yingbo and Xiong, Caiming},
  journal={arXiv preprint arXiv:2202.11091},
  year={2022}
}
