    \begin{table}[t]
    \caption{Ablation study on HotpotQA citation recall, precision, and F1 (R, P, F1) and citation length for finetuned models.}
    \label{tab:ablation_finetuning}
    \centering
    \small
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l|ccc|c}
    \toprule
    \multirow{2}{*}{\bf Fine-tuning Methods} & \multicolumn{3}{c|}{\bf HotpotQA} & \multicolumn{1}{c}{\bf Citation} \\
    & \bf R & \bf P & \bf F1 & \bf Length \\
    \midrule
    LongCite-8B (Our repro.) & 60.8 & 77.9 & 64.1 & 83.5 \\
    \midrule
    + SimPO                  & 69.4 & 82.3 & 71.5 & 105.7 \\
    + SimPO + BoN            & 72.0 & 82.7 & 72.9 & 126.9 \\
    \midrule
    \multicolumn{5}{l}{\it + SimPO w/ or w/o length balancing} \\
    \midrule
     w/ length balancing      & 69.4 & 82.3 & 71.5 & 105.7 \\
     w/o length balancing     & 64.4 & 62.9 & 60.5 & 152.9 \\
    \midrule
    \multicolumn{5}{l}{\it + SimPO w/ varying data sizes} \\
    \midrule
     1K examples              & 62.5 & 78.9 & 65.7 & 90.1 \\
     2K examples              & 69.4 & 82.3 & 71.5 & 105.7 \\
     4K examples              & 68.5 & 80.4 & 70.3 & 134.1 \\
     8K examples              & 63.6 & 75.3 & 64.4 & 195.2 \\
    \midrule
    + {\it SFT on BoN responses}      & 68.8 & 77.3 & 68.4 & 98.7 \\
    \midrule
    \multicolumn{5}{l}{\it + SimPO by denoising perturbed citations} \\
    \midrule
    On original responses & 40.5 & 50.5 & 41.6 & 88.8 \\
    On BoN responses & 42.6 & 50.7 & 42.3 & 79.7 \\
    \bottomrule
    \end{tabular}
    }
    \end{table}