\begin{table*}[t!]
\caption{An example of differences in the citation from baseline vs BoN. Related information are highlighted in the context/response.}
\label{tab:q1}
\centering
\small
\resizebox{\linewidth}{!}{
\renewcommand{\arraystretch}{1.0} % row height
\begin{tabular}{p{0.15\textwidth} p{0.85\textwidth}}
\toprule
\bf Sent. ID & \textbf{Context Sentences} (only showing cited sentences due to space) \\
\midrule
\textbf{23 (\ding{51})} 
  & We explored using \hl{a simple and cost-effective procedure to instruction finetune our continually pretrained long models without any human-annotated data.}\\
\midrule
\textbf{42 (\ding{55})} 
  & Collecting human demonstration and preference labels for LLM alignment is a cumbersome and expensive process (Ouyang et al., 2022; Touvron et al., 2023).\\
\midrule
\textbf{45 (\ding{51})} 
  & In this work, we found that a simple and cheap approach which leverages \hl{a pre-built large and diverse short-prompt dataset} works surprisingly well on long-context benchmarks.\\
\midrule
\textbf{46 (\ding{51})} 
  & Specifically, we take the RLHF dataset used in LLAMA 2 CHAT and \hl{augment it with synthetic self-instruct} (Wang et al., 2022) \hl{long data generated by LLAMA 2 CHAT itself}, in the hope that the model can learn a diverse set of skills through the large amount of RLHF data and transfer that knowledge to long-context scenarios via self-instruct data.\\
\midrule
\textbf{47-50 (\ding{55})} 
  & The data generation process focuses on QA-format tasks: starting from a long document in our pretraining corpus, we select a random chunk and prompt LLAMA 2 CHAT to write question-answer pairs based on information in the text chunk. [...]\\
\midrule
\midrule
\bf Query & What aspects of the LLAMA Long model proposed above have changed relative to the LLAMA-based model? What improvements have been made? \\
\midrule
\textbf{Response} (single statement due to space)
 & [...] 3. Instruction Tuning: The paper proposes \hl{a simple and cost-effective procedure to instruction finetune the continually pretrained long models without any human-annotated data.} This involves using \hl{a pre-built large and diverse short-prompt dataset} and \hl{augmenting it with synthetic self-instruct long data generated by LLAMA CHAT itself.} [...]\\
\midrule
\multicolumn{2}{l}{\textbf{Citation Strings (\textcolor{ForestGreen}{green: correct}; \textcolor{red}{red: wrong})}} \\
\midrule
\textbf{Baseline} 
 & \texttt{\textcolor{red}{[42-42]}\textcolor{ForestGreen}{[45}\textcolor{red}{-50]}} \\
\textbf{\ours BoN} 
 & \texttt{\textcolor{ForestGreen}{[23-23][45-45][46-46]}} \\
\midrule
\end{tabular}
}
\end{table*}