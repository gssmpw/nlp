\begin{table*}[t]
	\caption{Citation recall (R), citation precision (P), citation F1 (F1), and citation length evaluated on LongBench-Cite benchmark. The best of our results are bolded. The best of previous state of the art are underlined. $^\dagger$ indicates the results taken from~\citet{zhang2024longcite}. Our repro.\ means our reproduced results.
	}
	\label{tab:main_cite}
	\centering
	\small
	\resizebox{\linewidth}{!}{
		\setlength{\tabcolsep}{5pt}
		\begin{tabular}{l|ccc|ccc|ccc|ccc|ccc|c|c}
			\toprule
			\multirow{2}{*}{\bf Model}           & \multicolumn{3}{c|}{\bf Longbench-Chat} & \multicolumn{3}{c|}{\bf MultifieldQA} & \multicolumn{3}{c|}{\bf HotpotQA} & \multicolumn{3}{c|}{\bf Dureader} & \multicolumn{3}{c|}{\bf GovReport} & \bf Avg. & \bf Citation                                                                               \\
			                                 & R                                       & P                                     & F1                                & R                                 & P                                  & F1       & R            & P    & F1   & R    & P    & F1   & R    & P    & F1   & \bf F1 & \bf Length \\ \midrule
			\multicolumn{18}{l}{\textit{\bf Proprietary models}}                                                                                                                                                                                                                                                                                    \\\midrule
			GPT-4o$^\dagger$                 & 46.7                                    & 53.5                                  & 46.7                              & \underline{79.0}                  & 87.9                               & 80.6     & 55.7         & 62.3 & 53.4 & 65.6 & 74.2 & 67.4 & 73.4 & 90.4 & 79.8 & 65.6   & 220        \\
			Claude-3-sonnet$^\dagger$        & 52.0                                    & 67.8                                  & 55.1                              & 64.7                              & 85.8    & 71.3 & 46.4 & 65.8 & 49.9 & 67.7 &\underline{89.2} & \underline{75.5} & 77.4 & \underline{\bf 93.9} & 84.1 & 67.2   & 132        \\
			GLM-4$^\dagger$                  & 47.6                                    & 53.9                                  & 47.1                              & 72.3           & 80.1                               & 73.6 & 47.0 & 50.1 & 44.4 & 73.4 & 82.3 & 75.0 & \underline{82.8} & 93.4 &        \underline{87.1} & 65.4   & 169        \\ \midrule
			\multicolumn{18}{l}{\textit{\bf Open-source models }}                                                                                                                                                                                                                                                                                   \\\midrule
			GLM-4-9B-chat$^\dagger$          & 25.9                                    & 20.5                                  & 16.7                              & 51.1                              & 60.6                               & 52.0     & 22.9         & 28.8 & 20.1 & 45.4 & 48.3 & 40.9 & 5.7  & 8.2  & 6.3  & 27.2   & 96         \\
			Llama-3.1-8B-Instruct$^\dagger$  & 14.1                                    & 19.5                                  & 12.4                              & 29.8                              & 44.3                               & 31.6     & 20.2         & 30.9 & 20.9 & 22.0 & 25.1 & 17.0 & 16.2 & 25.3 & 16.8 & 19.7   & 100        \\
			Llama-3.1-70B-Instruct$^\dagger$ & 25.8                                    & 32.0                                  & 23.2                              & 53.2                              & 65.2                               & 53.9     & 29.6         & 37.3 & 28.6 & 38.2 & 46.0 & 35.4 & 53.4 & 77.5 & 60.7 & 40.4   & 174        \\
			Mistral-Large-Instruct$^\dagger$ & 19.8                                    & 23.9                                  & 19.0                              & 71.8                              & 80.7                               & 73.8     & 34.5         & 40.9 & 32.1 & 58.3 & 67.0 & 60.1 & 67.9 & 79.6 & 72.5 & 51.5   & 132        \\
			\midrule

			\multicolumn{18}{l}{\textit{\bf Contributive context attribution} (\textit{with Llama-3.1-8B-Instruct})}                                                                                                                                                                                                                                \\\midrule
			ContextCite (32 calls)           & 56.7                                    & 76.8                                  & 58.0                              & 76.1                              & 87.2                               & 78.9     & 40.5         & 54.7 & 43.9 & 58.0 & 82.4 & 65.0 & 67.1 & 88.8 & 75.6 & 64.3   & 92.7       \\
			ContextCite (256 calls)          & \underline{63.5}                        & \underline{\bf 83.1}                  & 64.7                              & 78.8                              & 89.8                       & \underline{81.8} & 46.5         & 60.8 & 49.2 & 61.7 & 89.1 & 70.1 & 69.1 & 93.5 & 78.8 & 68.9   & 100.8      \\
			\midrule

			\multicolumn{18}{l}{\textit{\bf Fine-tuned models}}                                                                                                                                                                                                                                                                                     \\\midrule
			LongCite-9B$^\dagger$            & 57.6                                    & 78.1                                  & 63.6                              & 67.3        & 91.0      & 74.8 & \underline{61.8}  & \underline{78.8} & \underline{64.8} & 67.6           & \underline{89.2} & 74.4 & 63.4 & 76.5 & 68.2 & 69.2   & 91         \\
			LongCite-8B$^\dagger$            & 62.0                                    & 79.7                                  & \underline{67.4}                  & 74.7                              & \underline{93.0}       & 80.8     & 59.2 & 72.1 & 60.3 & \underline{68.3} & 85.6 & 73.1 & 74.0 & 86.6 & 78.5 &\underline{72.0}& 85         \\ \midrule
            % \hline
			\multicolumn{18}{l}{\textit{\bf Ours: \ours}}                                                                                                                                                                                                                                                                                           \\\midrule

			LongCite-8B (Our repro.)         & 67.0                                    & 78.1                                  & 66.6                              & 74.8                              & 90.7                               & 79.9     & 60.8         & 77.9 & 64.1 & 67.1 & 87.2 & 73.7 & 81.6 & 89.3 & 84.5 & 73.8   & 83.5       \\
			+ BoN                            & 68.4                                    & 81.3                                  & 71.2                              & 76.1                              & 92.8                               & 81.2     & 67.2 & 81.0   & 68.8 & 70.6 & 90.9 & 76.9 & \bf 87.6 & 92.4 & \bf 89.3 & 77.5   & 93.4       \\
			+ SimPO                          & 68.1                                    & 79.5                                  & 69.1                              & 75.5                              & 92.6                               & 81.0     & \bf 69.4 & 82.3 & \bf 71.5 & 72.7 & 91.6 & 78.9 & 86.4 & 92.9 & 89.1 & 77.9   & 105.7      \\
			+ SimPO then BoN                 & \bf 73.3                                & 79.4                                  & \bf 72.8                          & 76.7                              & \bf 93.2    & 82.2 & \bf 69.4 & \bf 83.0 & 71.1 & \bf 74.2 & \bf 92.2 &                \bf 80.3 & 86.7 & 92.7 & 89.2 & \bf 79.1   & 94.7       \\

			\midrule
			\multicolumn{18}{l}{Llama-3.1-8B-Instruct (\textit{fully self-supervised setting})}                                                                                                                                                                                                                                                                                                         \\
			+ SFT on ContextCite             & 52.3                                    & 70.6                                  & 56.5                              & 79.1                              & 90.5                               & 82.0     & 54.5         & 72.3 & 56.3 & 54.9 & 79.0 & 61.6 & 63.7 & 84.9 & 72.3 & 65.7   & 83.0       \\
			\hspace{4mm} + BoN               & 54.8                                    & 67.6                                  & 58.1                              & 80.4                              & 90.5                               & 83.0     & 58.3         & 70.0 & 57.5 & 57.6 & 79.0 & 63.1 & 67.2 & 84.8 & 74.6 & 67.3   & 80.4       \\
			\hspace{4mm} + SimPO             & 63.3                                    & 74.3                                  & 64.6                              & 80.2                              & 88.9                               & 82.4     & 59.7         & 76.9 & 61.0 & 59.0 & 80.9 & 65.4 & 68.5 & 86.6 & 76.1 & 69.9   & 90.2       \\
			\hspace{4mm} + SimPO then BoN    & 66.0                                    & 82.4                                  & 71.1                              & \bf 81.5                          & 90.7                               & \bf 83.2 & 61.3         & 70.0 & 59.9 & 62.1 & 81.4 & 67.4 & 68.8 & 86.2 & 76.1 & 71.5   & 87.4       \\
			\bottomrule
		\end{tabular}
	}
\end{table*}