\section{Conclusion and Limitations}

In this work, we introduced \ours, a self-supervised framework that aligns LLMs to generate more accurate, fine-grained citations by directly leveraging their own probabilities for necessity and sufficiency rewards through context ablation. With best-of-N sampling and preference optimization, \ours significantly improves citation correctness on the LongBench-Cite benchmark without requiring human annotation, offering a promising self-improving direction towards verifiable and trustworthy LLMs.

\ours also has limitations: 1) While achieving good results with SimPO, integrating other alignment algorithms remains unexplored. 2) While we focus on self-supervision in preference optimization, our attempt to make the SFT stage self-supervised via ContextCite is still preliminary. Better unsupervised ways to kick-start LLMs' ability in generating structured citations can be further explored.

\section*{Acknowledgements}

We thank Jiajie Zhang and Yushi Bai for their help in providing implementation details of LongCite. We also thank Andrei Barbu, Linlu Qiu, Nour Jedidi, Weijia Shi, and Tianyu Gao for their valuable discussions. At MIT, Yung-Sung was sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.