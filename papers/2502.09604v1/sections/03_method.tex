\section{Method}
\label{method}

In this section, we describe the \ours framework.
We begin by introducing the task of generating responses with context attributions (\ref{sec:problem_formulation}), referred to as \emph{citations} for brevity.
We then design a reward for providing feedback on citation quality \emph{without} human annotations (\ref{sec:reward}) as illustrated in Fig.~\ref{main-figure}.
Finally, we discuss two approaches for utilizing this reward to improve citation quality: best-of-N sampling (\ref{sec:best_of_n_sampling}) and preference optimization (\ref{sec:preference_optimization}).

\subsection{Problem Formulation}
\label{sec:problem_formulation}

We first formalize the task of generating responses with context attributions and the metrics to self-evaluate context attributions within the \ours framework, inspired by previous papers~\citep{zhang2024longcite, cohen2024contextcite} but adapted to our proposed self-supervised reward.

\paragraph{Setup.} Consider employing an autoregressive language model (LM) to generate a response to a specific query given a context of relevant information. 
Specifically, given an LM $p_{\text{LM}}$, let $p_{\text{LM}}(t_i \mid t_1, \hdots, t_{i-1})$ denote its output distribution over the next token $t_i$ based on a sequence of preceding tokens $t_1,\hdots,t_{i-1}$.
Next, let $C$ represent the context of relevant information.
This context is partitioned into $|C|$ sentences: $c_1, c_2, \dots, c_{|C|}$.
Each sentence $c_j$ is prepended with a unique identifier (e.g. sentence index $j$) as a way for the model to reference the sentence when generating citations. 
The context $C$ is followed by a query $Q$, a question or instruction for the model.
A response $R$ is then sampled from the LM $p_\text{LM}$.

\paragraph{Generating Responses with Context Attributions.}

In \ours, following prior work on generating responses with context attributions~\citep{zhang2024longcite}, each statement $r_i$ in the response $R$ is followed by a citation sequence $e_i$ consisting of the identifiers of sentences from the context $C$.
Thus, the entire response sequence $R$ is $\{r_{1}, e_{1}, r_{2}, e_{2}, \dots, r_{S}, e_{S}\}$, where $S$ is the total number of generated statements.
The citation $e_i$ is intended to reference sentences that support the generation of $r_i$. Formally, for each response statement $r_i$, the model outputs a citation sequence $e_i = \{e_{i}^{1}, e_{i}^{2}, \dots, e_{i}^{m}\}$, where each $e_{i}^{j} \in \{1, 2, \dots, |C|\}$ corresponds to a specific sentence number in the context $C$, and $m$ sentences are cited.
Note that this citation sequence may be empty.
The entire response $R$ consisting of statements $r_i$ followed by citations $e_i$ is sampled from the LM $p_\text{LM}$ as follows:
\begin{align*}
r_i &\sim p_{\text{LM}}\left(\cdot \mid c_1, \hdots, c_{|C|}, Q, r_1, e_1, \hdots, r_{i-1}, e_{i-1}\right), \\
e_i &\sim p_{\text{LM}}\left(\cdot \mid c_1, \hdots, c_{|C|}, Q, r_1, e_1, \hdots, r_{i-1}, e_{i-1}, r_{i}\right).
\end{align*}

The objective of optimizing the LM is to ensure that the citation sequence $e_i$ accurately reflects the evidence from the context that supports the generation of $r_i$. 
In the SFT setting~\citep{zhang2024longcite}, the probability of a ``ground truth'' annotated responses and citations $\{\hat{r}_1, \hat{e}_1, ..., \hat{r}_{S}, \hat{e}_{S}\}$ will be maximized, given the input $C$ and $Q$, but it is not trivial to do further alignment with feedback after the SFT data is used up. 
To achieve this, we introduce \ours that can evaluate the quality of these citations based on context ablation as a reward for further preference optimization.

\subsection{Self-Supervised Reward via Context Ablation} 
\label{sec:reward}

We measure the quality of a citation sequence $e_i$ by the \emph{changes} in the LMâ€™s probability of generating $r_i$ when the cited sentences are either removed from or isolated within the context. To simplify the notation, let all the cited context sentences be $E_i = \{c_{e_i^1}, c_{e_i^2}, \dots, c_{e_i^m}\}$. We define two key metrics: \emph{necessity score} and \emph{sufficiency score}, and finally combine them into the final reward, as shown in Fig.~\ref{main-figure}.

\paragraph{Necessity Score: Probability Drop.} This metric quantifies the decrease in the probability of generating $r_i$ when the cited sentences in $E_i$ are all removed from the context (denoted as set minus $\setminus$ operator). Formally, it is defined as:
\[
\text{Prob-Drop}(e_i) = \log p_{\text{LM}}(r_i \mid C) - \log p_{\text{LM}}\left(r_i \mid C \setminus E_i\right).
\]
To keep the equation concise, we ignore $Q$ and $\{r_1, e_1, ..., r_{i-1}, e_{i-1}\}$ in the equation, but they are staying in the context history when computing the probabilities.
A larger probability drop indicates that the removal of $E_i$ significantly diminishes the likelihood of generating $r_i$, thereby validating the necessity of the cited evidence.

\paragraph{Sufficiency Score: Probability Hold.} Conversely, this metric measures if the probability of generating $r_i$ is still kept large when \emph{only} the cited sentences are kept in the context, effectively testing the sufficiency of the citation to support the response statement. Formally:
\[
\text{Prob-Hold}(e_i) = \log p_{\text{LM}}\left(r_i \mid E_i\right) - \log p_{\text{LM}}(r_i \mid C).
\]
A more positive value of probability hold indicates that the cited sentences alone are sufficient to support the generation of $r_i$, while removing all the other irrelevant context. Please note that the values of probability drop or hold can be either positive or negative. For example, if the citation is not relevant to $r_i$ or even distracting, it is possible for $p(r_i \mid E_i)$ to be lower than $p(r_i \mid C)$.

\paragraph{Final Reward.} To comprehensively evaluate the necessity and sufficiency of the generated citations, we add the two metrics together, where the opposing terms cancel out:
\begin{equation}
\label{eq:reward}
\text{Reward}(e_i) = \log p_{\text{LM}}\left(r_i | E_i\right) - \log p_{\text{LM}}\left(r_i | C \setminus E_i\right).
\end{equation}
The combined reward measures if the citations are both necessary and sufficient for generating the response $r_i$.

\subsection{Best-of-N Sampling}
\label{sec:best_of_n_sampling}

To leverage the self-supervised reward computed via context ablation, we employ a \emph{best-of-N} sampling strategy, which is a common way to test the effectiveness of a reward design~\citep{gao2023scaling,lightman2024lets} as a performance oracle without any confounders from training. After generating the full response, we locate the position where the citation tags \texttt{<cite>...</cite>} are generated. Within the citation tags, we sample $N$ candidate citation sequences and select the citation set that maximizes the combined reward metric,~Eq.~\eqref{eq:reward}. The corresponding procedure is shown in Algorithm~\ref{alg:best_of_n}. After obtaining the selected citations $\{e_1^*, \dots, e_{S}^*\}$, we replace the original citation sequence $e_i$ with the optimal citation set $e_i^*$ for the response statement $r_i$, while keeping the response statements $\{r_1, \dots, r_{S}\}$ unchanged. This process is repeated for each statement in the response $R$ to obtain the final, citation-improved output $R^* = \{r_1, e_1^*, \dots, r_{S}, e_{S}^*\}$. To prevent the model from citating too many sentences, we exclude the BoN candidates that cites more than 384 tokens in total, unless they are all from a single sentence.
\begin{algorithm}[t!]
\caption{\ours Best-of-N Sampling for Citations}
\label{alg:best_of_n}
\begin{algorithmic}
\REQUIRE LM $p_{\text{LM}}$, context $C$, query $Q$, response $R$, number of candidates $N$
\FOR{$r_i \in R$}
  \FOR{$k=1,\dots,N$}
    \STATE $e_i^{(k)} \sim p_{\text{LM}}(\cdot \mid r_i, C, Q)$
    \STATE $\mathrm{Reward}(e_i^{(k)})$
        $ = \log p_{\text{LM}}\bigl(r_i \mid E_i^{(n)}\bigr) 
    - \log p_{\text{LM}}\bigl(r_i \mid C \setminus E_i^{(n)}\bigr)$
  \ENDFOR
  \STATE $e_i^* = \arg\max_{k}\;\mathrm{reward}\bigl(e_i^{(k)}\bigr)$
\ENDFOR
\STATE \textbf{return} $R^* = \{r_1, e_1^*, \dots, r_{S}, e_{S}^*\}$
\end{algorithmic}
\end{algorithm}

\subsection{Preference Optimization}
\label{sec:preference_optimization}

Best-of-N sampling is a straightforward way to obtain better citations, but at the additional inference time of generating candidates and reranking. Thus, we try to internalize the ability of generating better citations back to the LM itself.

Given documents and queries, we can prompt the LM to generate the responses along with the citations $R = \{r_1, e_1, ..., r_{S}, e_{S}\}$. By further applying best-of-N sampling, we can obtain new responses of the same statements but with better citations $R^* = \{r_1, e^*_1, ..., r_{S}, e^*_{S}\}$. Such preference data can be used in direct preference optimization (DPO)~\citep{rafailov2024direct} to align the model based on the preference between the original outputs and improved outputs.
DPO typically requires more memory usage than SFT due to the need of a reference model. Also, optimizing with preference data pairs inherently makes the per-GPU batch size to be at least 2, limiting the maximum context length that can be used.
To address these challenges, we use SimPO~\citep{meng2024simpo}, a variant of DPO that does not require a reference model, freeing up memory for long-context fine-tuning.
We further apply Liger-Kernel~\citep{hsu2024ligerkernelefficienttriton}, a collection of efficient Triton kernels, to optimize memory usage and scale the context length to 25.6K, as detailed in Appendix~\ref{appx:details}.
Through this self-supervised alignment process, which does not require ground-truth answers or human annotations, the model learns to generate more accurate and contextually grounded citations on its own.
