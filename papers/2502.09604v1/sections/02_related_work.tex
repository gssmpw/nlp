\section{Related Work}
\label{sec:related_work}

\paragraph{Citations for Language Models.}

Recent work has explored various approaches to teaching language models to generate citations, including fine-tuning with direct human feedback or annotations~\citep{nakano2021webgpt,menick2022teaching,slobodkin2024attribute}, annotation/feedback from external models~\citep{huang2024training}, and prompting-based methods~\citep{gao2022rarr, gao2023enabling} to explicitly incorporate relevant retrieved documents. To avoid the human annotation processes, \citet{zhang2024longcite} introduced \textbf{CoF} (“\textbf{Co}arse to \textbf{F}ine”), an automated multi-stage pipeline that simulates human annotations. 
This approach leverages proprietary LLMs for chunk-level retrieval and sentence-level citation extraction, achieving high citation quality through supervised fine-tuning. However, it depends on two powerful proprietary APIs—GLM-4 for the LLM and Zhipu Embedding-v2 for retrieval\footnote{\url{https://open.bigmodel.cn/pricing}}—with carefully designed prompting, effectively distilling the capabilities of these powerful proprietary APIs into much smaller models in 8B/9B.
In contrast, our \ours aims at completely eliminating the reliance on annotations for citation, either from human or proprietary APIs. Instead, our method enables a small 8B model to assess citation quality itself using self-supervised reward signal from context ablation, effectively self-improving without external supervision.

\paragraph{Contributive Context Attribution.}

Besides being self-supervised, \ours also adopts the view that citations should reference the sources from the context that a model actually \emph{uses} when generating a statement--known as \emph{contributive} attribution~\citep{worledge2023unifying}--rather than any sources that merely \emph{support} the claim.
Our reward signal naturally aligns with this attribution framework, as context ablation identifies the sources that \emph{cause} the model to produce a statement. Existing contributive attribution methods for LLMs typically require extensive context ablations or other computationally expensive techniques, such as gradient-based analysis during inference~\citep{cohen2024contextcite,Qi2024ModelIA,phukan2024peering}.
In contrast, \ours simply generate the citation tags, and refine citation candidates by preference optimization with reward signals from context ablations, effectively teaching the model to perform contributive context attribution itself.

\paragraph{Self- or Weakly-Supervised Alignment.}
Another relevant area is self- or weakly-supervised approaches for aligning LLMs without human supervision~\citep{kim2023aligning,yuan2024selfrewarding}, reducing the need for explicit human feedback~\citep{ouyang2022training}, or curating high-quality data for supervised fine-tuning~\citep{zhou2023lima}. \ours  shares the same spirit by computing simple probability \emph{differences} under context ablation as rewards, eliminating the need for additional annotation process. 