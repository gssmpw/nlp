\section{Experiments}

We evaluate the effectiveness of \ours by applying the best-of-N sampling and preference optimization methods to existing models that generate responses with citations.

\subsection{Model Details}
\label{subsec:model}

We use the Llama-3.1-8B model~\citep{dubey2024llama} fine-tuned on LongCite-45K SFT data, namely the LongCite-8B model~\citep{zhang2024longcite} as the start point 
for both best-of-N sampling and preference optimization. We adopt the same text segmentation strategy from \citet{zhang2024longcite}: each document is split into individual sentences using NLTK~\citep{bird2006nltk} and Chinese punctuations, and each sentence is prepended with a unique identifier in \texttt{\small <C\{$i$\}>} format. These identifiers serve as the \emph{citation indices}, enabling the model to cite relevant context right after the statements with the format of \texttt{\small <statement> \{content ...\} <cite>[}$i_1-i_2$\texttt{\small ][}$i_3-i_4$\texttt{\small ]...</cite></statement>}. This format allows the model to cite a single sentence (e.g. $i_1 = i_2$) or a span (e.g. $i_1 < i_2$) efficiently within several tokens. The responses are generated via top-p sampling~\citep{Holtzman2020The} with p=0.7 and temperature=0.95. We set p=0.9 and temperature=1.2 when doing best-of-N sampling for the citation strings to increase the diversity. We set N=10 in all the experiments considering the limited diversity in citations.\footnote{After deduplicating repeated citation candidates, on average there are only 4.8 candidates left per statement in the BoN experiment on LongBench-Cite, with a standard deviation of 3.2.}

\subsection{Preference Optimization}
\label{sec:po}

\paragraph{LongCite-45K.} 

Best-of-N sampling (Section~\ref{sec:best_of_n_sampling}) requires no training, so no training data is used. For preference optimization with SimPO (Section~\ref{sec:preference_optimization}), we use 2K document–question pairs from LongCite-45K~\citep{zhang2024longcite} as the training set but we do not use its ground-truth responses with high-quality citations for SFT. Instead, we generate model responses from the documents and queries, then apply best-of-N to refine citations. We label the original responses as \emph{rejected} and replace their citations with BoN-refined ones to create the \emph{chosen} responses, forming preference pairs to build the dataset for SimPO.

\paragraph{Data Construction and Length Balancing}

Since best-of-N responses tend to have slightly longer citations, directly fine-tuning on them can lead the model to adopt a shortcut—generating longer citations instead of improving citation quality. To prevent this, we introduce \emph{length balancing}: if an original response has a shorter citation length than the best-of-N response, we insert random citations from nearby sentences. This encourages the model to focus on \emph{where} to cite rather than simply citing \emph{more}. Details are provided in Appendix~\ref{appx:length}, with an ablation study in Section~\ref{sec:balance}.


\subsection{Evaluation}
\label{subsec:datasets}

\paragraph{Benchmark.}
We evaluate our approach on \textbf{LongBench-Cite}~\citep{zhang2024longcite}, a comprehensive benchmark specifically designed for \emph{long-context QA with citations (LQAC)}. Given a long context $C$ and a query $Q$, the model must produce a multi-statement answer with each statement cites relevant supporting sentences in $C$. 
Unlike chunk-level citation schemes~\citep{gao2023enabling} which cites short paragraphs, LongBench-Cite adopts \emph{sentence-level} citations to ensure semantic integrity and finer-grained evidence tracking.
LongBench-Cite assesses two main aspects:
\begin{itemize}
\item \textbf{Citation Quality:} Whether each statement is fully supported by relevant and \emph{only} relevant sentences. GPT-4o measures \emph{citation recall} (extent to which a statement is fully or partially supported by the cited text) and \emph{citation precision} (whether each cited text truly supports the statement). These are combined into a \emph{citation F1} score. Additionally, we track \emph{average citation length} (tokens per citation) to promote fine-grained citations over unnecessarily long passages.
\item \textbf{Correctness:} How accurately and comprehensively the response answers the query disregarding the citations. This is scored by GPT-4o in a zero-/few-shot fashion based on the query and reference answers.
\end{itemize}

The benchmark contains five datasets, including single-doc QA \textit{MultiFieldQA-en/zh}~\citep{bai2023longbench}, multi-doc QA \textit{HotpotQA}~\citep{yang2018hotpotqa} and \textit{DuReader}~\citep{he2018dureader}, one summarization dataset \textit{GovReport}~\citep{huang2021efficient}, and LongBench-Chat~\citep{bai2024longalign} which covers diverse real-world queries with long contexts such as document QA, summarization, and coding.

\paragraph{Baselines.} \ours is compared with these baselines.
\begin{itemize}
    \item \textbf{Prompting}: \citet{zhang2024longcite} propose the baseline of prompting LLMs with an one-shot example. This can be applied to proprietary models including GPT-4o~\citep{openai2023gpt4}, Claude-3-sonnet~\citep{claude-3}, and GLM-4~\citep{glm2024chatglm}, as well as open-source models including GLM-4-9B-chat~\citep{glm2024chatglm}, Llama-3.1-\{8,70\}B-Instruct~\citep{dubey2024llama}, and Mistral-Large-Instruct~\citep{mistral}.
    \item \textbf{Contributive context attribution}: 
    Contributive context attribution seeks to directly identify the parts of the context that \emph{cause} the model to generate a particular statement.
    We consider ContextCite \citep{cohen2024contextcite}, a contributive context attribution method that performs several random context ablations to model the effect of ablating different parts of the context on a generated statement.
    We apply ContextCite with 32 and 256 times of random context ablations, with the details described in Appendix~\ref{appx:cc}.
    \item \textbf{Fine-tuned models}: LongCite-8B and 9B released by \citet{zhang2024longcite}, trained on LongCite-45K, fine-tuned from Llama-3.1-8B~\citep{dubey2024llama} and GLM-4-9B~\citep{glm2024chatglm}, respectively.
\end{itemize}

\input{tables/main_cite}

\subsection{Main Results}
\paragraph{Citation Quality.}
Table~\ref{tab:main_cite} presents our main results. Our best-of-N sampling (BoN) consistently improves both \textbf{citation recall} and \textbf{citation precision} across tasks, increasing the overall F1 score from 73.8 to 77.5. Using SimPO to internalize BoN’s gains—\textbf{eliminating the need for BoN sampling}—achieves a similar improvement, with an F1 of 77.9. Applying BoN again to the SimPO fine-tuned model further boosts \textbf{F1 by 5.3 points to 79.1}, the highest across the datasets, suggesting room for further gains.
Our results surpass LongCite-8B/9B at similar citation lengths and outperform proprietary model prompting while producing shorter citations. Compared to ContextCite, which also relies on context ablation, our approach is significantly better. A key reason is that ContextCite estimates sentence importance from scratch using linear regression, while we rerank existing LLM-generated citation candidates, leading to more efficient and accurate citation quality estimation. Additionally, we evaluate the latest released \emph{Claude Citations} API, as shown in Appendix~\ref{appx:claude} that \textbf{\ours achieves strong results very close to this commercial-level API} that specialized for providing citations.

\paragraph{Fully Self-Supervised Setting.}

In our main experiment, we start from the Llama-3.1-8B model fine-tuned on the LongCite-45K SFT data, which effectively kick-starts its ability to generate structured citations for best-of-N sampling. The subsequent SimPO alignment stage is entirely self-supervised. We are also curious if it is possible to start from a fully self-supervised SFT model and then apply our self-supervised alignment after that. To begin with, we automatically generate 11K citation SFT data using ContextCite (see Appendix~\ref{appx:cc} for details) to replace the LongCite-45K annotations in the training data, as shown in the results at the bottom of Table~\ref{tab:main_cite}. We can see that SFT on ContextCite can achieve decent initial results (65.7 F1) but still far from LongCite-8B (73.8 F1). BoN helps improving F1 to 67.3. After SimPO training, it achieves 69.9 F1, and additionally applying BoN can boost its F1 by 5.8 to 71.5, significantly closing the gap to LongCite-8B, showing our alignment method not only improve the supervised models, but also enhance the models purely trained from self-supervision.

\input{tables/main_corr_small}

\paragraph{Answer Correctness.}
For best-of-N sampling, only the citation parts are modified, so the responses it generates to answer the questions are the same as those of the original LongCite-8B model, maintaining the same correctness. For the SimPO finetuned models, we test their answer correctness by the evaluation in \citet{zhang2024longcite}, which contains two settings: answering with/without citations. If answering with citations, the citation parts will be removed when evaluating the answer correctness.
The results in Table~\ref{tab:correctness} show that the SimPO finetuning \textbf{does not change the correctness of the LongCite-8B model} much. The correctness is similar to LongSFT-8B/9B~\citep{zhang2024longcite}, which are ablation baselines finetuned on LongCite-45k QA pairs but without the citation parts. When finetuning Llama-3.1-8B-Instruct with ContextCite SFT data, the answer correctness slightly degrades. It is probably caused by the fact that we do not mix any instruction following dataset in the SFT stage as \citet{zhang2024longcite} does. However, the further SimPO step does not change the answer correctness significantly, while lead to better citations for user verification. Since the SFT stage is not our main method here, we leave the better data mixture strategy in SFT stage in the future work.
