\section{Introduction}
\label{introduction}

Assistants built using large language models (LLMs) have become ubiquitous in helping users gather information and acquire knowledge~\citep{chatgpt2023,openai2023gpt4}. For instance, when asked about recent news, an assistant can read through dozens of relevant articles---potentially more than a user could comb through themselves---and use these articles as \emph{context} to provide a clear, specific answer to the user's query. While this ability can greatly accelerate information gathering, LLMs often produce hallucinations—content that sounds plausible but is actually fabricated~\citep{ji2023survey}. Even when provided with accurate context, models may misinterpret the data or include details that are not supported by the context~\citep{shi2024trusting, chuang2024lookback}.

Although completely eliminating hallucinations remains difficult, existing approaches have sought to enhance the reliability of LLMs by providing context attributions--commonly referred to as \emph{citations}--which are fine-grained references to relevant evidences from the context, alongside generated responses for user verification~\citep{menick2022teaching, slobodkin2024attribute, zhang2024longcite}. 
While they have shown promise in generating citations, an outstanding challenge is their reliance on annotated data either from human~\citep{menick2022teaching,slobodkin2024attribute} or costly proprietary APIs~\citep{zhang2024longcite,huang2024training} to train models to generate citations. Collecting annotations can be time-consuming and costly, especially with long-context documents.

To address this challenge, we introduce \ours, a novel alignment approach designed to autonomously enhance the quality of citations generated by LLMs without the need for any annotations in the alignment process. 
Drawing inspiration from model interpretability techniques~\citep{lei2016rationalizing,cohen2024contextcite},
\ours leverages the inherent capabilities of LLMs to provide feedback through \emph{context ablation}—a process to evaluate the necessity and sufficiency of a citation.
If removing the cited text prevents the LLM from assigning high probability to the same response, we can infer that it is \emph{necessary} for the LLM. Conversely, if the response remains highly probable despite removing all context other than the cited text, this indicates that the citation is \emph{sufficient} for the LLM to make the claim. This self-evaluation mechanism enables \ours to calculate a reward signal without relying on the annotation processes.

Building on this intuition, we design a reward that can be cheaply computed by the LLM itself, composed by \emph{probability drop} and \emph{probability hold} in context ablation. 
By integrating this reward function into a best-of-N sampling strategy, \ours achieves substantial improvements in citation quality.
Furthermore, we employ this reward for preference optimization using SimPO~\citep{meng2024simpo}, which not only maintains these improvements but also eliminates the need for best-of-N sampling. We outperform the previous state of the art on the LongBench-Cite benchmark~\citep{zhang2024longcite} by up to 5.3 points in F1 scores, and showing a promising direction to bootstrap the citation quality from LLMs via self-rewarding.

\begin{figure}[!t]
\includegraphics[width=\textwidth]{figures/SelfCite.pdf}
\caption{The \ours framework calculates rewards based on two metrics: \emph{necessity score} (probability drop) and \emph{sufficiency score} (probability hold). First, the full context is used to generate a response. Then, the framework evaluates the probability of generating the same response after (1) removing the cited sentences from the context and (2) using only the cited sentences in the context. The probability drop and hold are computed from these probability differences, and their sum is used as the final reward.}
\label{main-figure}
\end{figure}