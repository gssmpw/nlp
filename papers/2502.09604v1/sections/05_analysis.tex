\section{Analysis}
\subsection{Ablation Study on Rewards}
\label{sec:ablation}

To better understand our final reward design, we explore various reward strategies in the BoN sampling process. Here, all BoN candidates are pre-generated and fixed, the reward is the only factor affecting results. Table~\ref{tab:ablation_decoding} presents our ablation results on HotpotQA, while citation lengths are computed across all LongBench-Cite datasets for direct comparison with Table~\ref{tab:main_cite}.
We evaluate four alternative reward designs. \textit{BoN by LM log prob} re-ranks candidates simply by the probability of the citation string, \texttt{\small <cite>[}$i_1-i_2$\texttt{\small ][}$i_3-i_4$\texttt{\small ]...</cite>}, which is similar to the effects of beam search. We observe that this strategy slightly boosts recall while reducing precision, resulting in a minor reduction in F1. 
\textit{BoN by max citation length} always selects the candidates with the longest citations, i.e. citing the greatest number of sentences. Although it improves recall, it significantly reduces precision from 77.9 to 73.6 and inflates the citation length from 83.5 to 139.8. 
By contrast, both \textit{BoN by Prob-Drop} and \textit{BoN by Prob-Hold} improve recall without sacrificing precision. 
Finally, by combining both Prob-Drop and Prob-Hold into our final \ours reward, we achieve the best outcome, increasing \textbf{both recall and precision and a 4-point improvement in F1}.

We also explored different token-length limits for citations in the bottom of Table~\ref{tab:ablation_decoding}, as discussed in \Cref{sec:best_of_n_sampling}. By default, we exclude candidates citing more than 384 tokens, unless the citation contains only a single sentence. Lowering the cap to 256 tokens slightly hurts F1, while raising it to 512 tokens has negligible impact. Completely removing length limits inflates citation length to 121.9 tokens and yields worse precision (79.3) but slightly improved recall (67.9). We also notice that the 256 length limit still outperforms the LongCite-8B baseline (66.4 vs 64.1) while having almost equally long citation length (84.5 vs 83.5), showing that \textbf{the improvement of \ours correlates less with the citation length}.
Overall, using a 384-token limit achieves a good balance for short citation lengths and strong performance.
\input{tables/ablation_decoding}

\input{tables/ablation_finetuning}

\subsection{Citation Length Balance}
\label{sec:balance}

As noted in Section~\ref{sec:po}, BoN selects slightly longer citations, making it easy for a model trained directly on BoN-preferred data to adopt the shortcut of generating longer citations without improving quality. To counter this, we apply \emph{length balancing}, injecting random citations into examples where length bias exists to equalize the number of cited sentences. Table~\ref{tab:ablation_finetuning} (see w/ vs. w/o length balancing) highlights its critical role in length balancing. Without length balancing, the model overextends citations (average length 152.9), leading to lower precision (62.9) and F1 (60.5). In contrast, enabling length balancing maintains high precision (82.3) and recall (69.4), achieving a better F1 of 71.5 while keeping citation length reasonable (105.7). These results confirm that \textbf{length balancing prevents shortcut learning, ensuring the model truly learns to cite accurately}.

\subsection{Training Size of SimPO}

In prior study~\citep{zhou2023lima}, 1K examples are sufficient to align user preferences effectively. Table~\ref{tab:ablation_finetuning} presents SimPO results with 1K to 8K examples. 1K examples already bring a moderate improvement, raising F1 from 64.1 to 65.7, with gains in precision and recall. Using 2K examples further boosts F1 to 71.5, while 4K leads to saturated improvement. However, at 8K examples, performance declines, and citation length rises to 195.2. We attribute this to SimPO’s off-policy nature, especially because it lacks a reference model to constrain the output distributions to be similar to the collected data. As training steps grow, the model may drift from the collected data, potential overfitting to the biases in preference data. Thus, further fine-tuning may degrade citation quality. To address this, we show initial results from iterative SimPO in \Cref{sec:iter}.

\subsection{SimPO vs.\ SFT on Best-of-N responses}

We also show the effect of applying standard supervised fine-tuning (SFT) on the responses selected by best-of-N sampling, which is a simplified alternative of preference optimization. As the result shown in the last row in Table~\ref{tab:ablation_finetuning}, SFT also improves the F1 score from 64.1 to 68.4, but it still falls behind 71.5 of SimPO. This result confirms that it is necessary to train the model via SimPO with preference data, which enables the model to distinguish between bad and good citations, and thus improve the citation quality.

\subsection{Off-policy Denoising Perturbed Citations}
\label{sec:offpolicy}

We explored a purely \emph{off-policy} alternative approach. Specifically, given a model-generated response, we randomly shift its citation spans to create perturbed variants. SimPO training pairs were then constructed by preferring the \emph{original} citation over the \emph{perturbed} one, encouraging the model to ``denoise'' citations by restoring their original spans.
However, as shown at the bottom of Table~\ref{tab:ablation_finetuning}, this approach \emph{degrades} performance, both when applied to original and best-of-N responses. We attribute this to a mismatch between the training data and the model’s natural error distribution—since random shifts do not reflect typical citation errors, they fail to provide useful guidance for improvement.

\begin{figure}[t!]
\centering
\includegraphics[width=0.5\linewidth]{figures/merged_figure.png}
\caption{Iteratively applying SimPO for three iterations.}
\label{fig:iter}
\end{figure}

\subsection{Iterative Preference Optimization}
\label{sec:iter}

It has been discussed that an \emph{on-policy} alignment process can be beneficial to avoid reward exploitation~\citep{bai2022training} and maintains consistency between the generated data and the model’s evolving output distribution.
We thus experiment with iteratively performing SimPO, similar to the concepts of recent studies~\citep{pang2024iterative, yasunaga2024alma}, to maintain the consistency between the generated data and the model’s evolving output distribution. 
Specifically, after fine-tuning with SimPO, we generate a new dataset via BoN, which is also 2K in size but not overlapped with previous iterations. We continue training the model and repeat the process for three rounds.
As shown in Figure~\ref{fig:iter}, while the largest improvement occurs in the first round, improvements continue over three iterations, which further validates the reliability of our reward signal.
Iterative SimPO is still not perfect since it remains an off-policy method. Given that our reward can be cheaply computed, we believe that on-policy methods like PPO~\citep{schulman2017proximal} could further enhance performance. We leave the exploration of such approaches for future work.

\subsection{Qualitative Study}

Finally, we examine an example that requires citing multiple context sentences to support a complex response. As shown in Table~\ref{tab:qual}, the response integrates information from sentences 302, 303, and 306.
Direct sampling (2) omits sentence 302 while incorrectly including 305. In contrast, the best-of-N candidate (1) correctly includes 302 and excludes 305, achieving a slightly higher reward (0.578 vs.\ 0.547), demonstrating the effectiveness of our reward design. We also present candidates (3) and (4), which cite more irrelevant sentences and miss key citations, leading to even lower rewards. Additional qualitative examples are provided in Appendix~\ref{appx:qual}.

\input{tables/qualitative}
