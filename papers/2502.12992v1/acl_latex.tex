% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{bbding}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{subcaption}
%\usepackage{wasysym}
\usepackage{enumitem}% noitemsep

\usepackage{csquotes}



% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{xcolor}         % colors
\definecolor{ao}{rgb}{0.0, 0.5, 0.0}
\definecolor{forestgreen}{RGB}{0, 150, 0}


\newcommand{\jl}[1]{\textcolor{ao}{#1}}
\newcommand{\TODO}[1]{\textcolor{red}{\textbf{TODO:} #1}}


\title{B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability}
% Alternative titles:
% B-cos LM: Empowering Language Models to Make Faithful and Plausible Predictions 

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
Yifan Wang\textsuperscript{1,3},~
Sukrut Rao\textsuperscript{2,3},~
Ji-Ung Lee\textsuperscript{1,3},~
\\
\textbf{
Mayank Jobanputra\textsuperscript{1}~
and Vera Demberg\textsuperscript{1,2,3}}
\\
\textsuperscript{1}Saarland University, Saarbrücken, Germany~\\
\textsuperscript{2}Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany\\
\textsuperscript{3}RTG Neuroexplicit Models of Language, Vision, and Action, Saarbrücken, Germany\\
\texttt{\{yifwang,mayank,vera\}@lst.uni-saarland.de,}\\
\texttt{sukrut.rao@mpi-inf.mpg.de,~ji-ung.lee@uni-saarland.de}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural models. 
Meanwhile, B-cos networks have been introduced to improve model explainability through architectural and computational adaptations, but their application has so far been limited to computer vision models and their associated training pipelines. 
In this work, we introduce B-cos LMs, i.e., B-cos networks empowered for NLP tasks. % proposing B-cos LMs.
Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous B-cos methods.
Our automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post hoc methods, while maintaining task performance comparable to conventional fine-tuning.
Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. 
Finally, we provide practical guidelines for effectively building B-cos LMs based on our findings. Our code is available at \href{https://anonymous.4open.science/r/bcos_lm}{{https://anonymous.4open.science/r/bcos\_lm}}. 
%Finally, we summarize our findings into practical guidelines for effectively building B-cos LMs.

% Post-hoc explanation methods for black-box neural models have been shown to often yield explanations that unfaithful to the model being explained and not human interpretable. B-cos networks aim to address this by enforcing architectural constraints, but usually require costly retraining of models. Recently, `B-cosification' was proposed as a means to efficiently transform conventional models to their B-cos variants in the context of architectures used in computer vision. In this work, we extend B-cos networks to NLP by adapting the B-cosification procedure and obtain B-cos LMs. Our approach directly transforms pre-trained language models into B-cos LMs by integrating B-cos conversion with task fine-tuning, making it more efficient than prior work. Our automatic and human evaluation results demonstrate that B-cos LMs produce explanations that are both more faithful and human interpretable than post hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores the differences between B-cos LMs and conventionally fine-tuned models. Based on our findings, we provide practical guidelines for effectively building B-cos LMs.

\end{abstract}

\section{Introduction}

Pre-trained language models (PLMs) such as BERT~\citep{Devlin-2019-BERT} and GPT~\citep{Radford-2019-GPT, NEURIPS2020_1457c0d6, DBLP:journals/corr/abs-2303-08774} have significantly advanced performance across a plethora of NLP tasks~\citep{wang-etal-2018-glue, Eval-2023-Harness}. 
However, their complex architectures and black-box nature make understanding their behavior a persistent challenge~\citep{Bommasani-2021-FoundationModels}. To address this, research has increasingly focused on explaining model predictions, particularly in relation to the input. These input-based explanations, often referred to as local explanations or rationales, aim to reveal how specific inputs influence a model’s predictions~\citep{arras-etal-2019-evaluating, atanasova-etal-2020-diagnostic, lyu-etal-2024-towards}.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{compare_bcos_baseline.pdf}
    \caption{\textbf{Visualization of $\mathbf{W(x)x}$ in a conventionally fine-tuned model (Conventional LM) and a B-cos LM.} \textcolor{forestgreen}{Green} (\textcolor{red}{red}) indicates the \textcolor{forestgreen}{positive} (\textcolor{red}{negative}) impact of tokens on the prediction. In both examples, both models correctly predict \textit{not toxic}. In the Conventional LM, ``funny'' is \textcolor{blue}{incorrectly} assigned a negative attribution in example (a), while in example (b), \textcolor{blue}{irrelevant} words like ``why'' and ``smell'' are highlighted, making the explanations unfaithful and less interpretable.}
    \label{fig:bcos_vs_baseline}
\end{figure}

Most explanation methods for neural models are post-hoc, meaning that they attempt to explain a model’s behavior only after it has been trained and deployed~\citep{Sundararajan-2017-IntegratedGrad, Ribeiro-2016-LIME}. 
While these methods are widely used and easy to apply, they have been shown to produce unfaithful and less interpretable explanations~\citep{Smilkov-2017-SmoothGrad, Kindermans-2019-Reliability, Slack-2020-FoolingLimeSHAP, pruthi-etal-2020-learning}.\footnote{Considering the evolving definition of these terms in past literature, we provide a detailed definition in Appendix~\ref{appendix:definitions}.} %; as past literature has used these terms with changing definitions~\citep{jacovi-goldberg-2020-towards}.} %, we provide a detailed definition of the terms used in this work in Appendix \ref{appendix:definitions}.} 
Prior research has attributed these shortcomings to the lack of explainability in contemporary neural models~\citep{Kindermans-2018-PatternNet, NEURIPS2018_3e9f0fc9, rudin2019stop}. 
Figure \ref{fig:bcos_vs_baseline} provides examples illustrating this issue.

To overcome these limitations, we introduce \textbf{B-cos LM}, a dynamic linear model that learns task-relevant patterns through increased input-weight alignment pressure. 
Building upon B-cos networks from computer vision~\citep{Bohle_2022_CVPR, arya24bcosification}, we improve explainability of B-cos LMs through mathematically grounded architectural and computational adaptations.
%propose architectural modifications and training pipelines that result in improved model explainability through mathematically grounded architectural and computational adaptations. 
%B-cos LMs improve model explainability through mathematically grounded architectural and computational adaptations. 
Furthermore, we tailor B-cos LMs for NLP by incorporating specialized architectural modifications and training pipelines.
Our contributions are as follows:

\begin{enumerate}
    \item We propose B-cos LM, a novel model with enhanced explainability. 
    Automatic and human evaluations demonstrate that B-cos LMs generate more faithful and human interpretable explanations than post-hoc explanations while maintaining a strong task performance.
    \item We investigate different strategies for transforming PLMs into task-specific B-cos LMs. 
    Our findings show that combining task fine-tuning and B-cos conversion is the most efficient approach, leading to faster convergence than previous B-cos methods and conventional fine-tuning.
    \item We thoroughly investigate how B-cos LMs differ from conventionally fine-tuned models and examine how alignment pressure influences their behavior.
    \item Based on our findings, we provide practical guidelines for building effective B-cos LMs.
\end{enumerate}



\section{Related Work}


\paragraph{Post-hoc Explanation Methods} 
Various methods have been proposed to provide post-hoc explanations for neural model predictions~\citep{atanasova-etal-2020-diagnostic}. 
These methods can be broadly categorized based on how they generate explanations: gradient-based~\citep{DBLP:journals/corr/SimonyanVZ13, DBLP:journals/corr/KindermansSMD16, Sundararajan-2017-IntegratedGrad}, propagation-based~\citep{bach2015pixel, shrikumar2017learning, DBLP:journals/corr/SpringenbergDBR14}, and perturbation-based methods~\citep{DBLP:journals/corr/LiMJ16a, Ribeiro-2016-LIME, Lundberg-2017-SHAP}. 
Besides, the attention mechanism~\citep{DBLP:journals/corr/BahdanauCB14} is often viewed as an explanation, particularly in transformer-based models~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}. 

Although post-hoc methods can be applied to generate explanations for existing models, numerous studies have shown that they lack faithfulness, often failing to capture the true decision-making process of the model~\citep{Kindermans-2019-Reliability, jain-wallace-2019-attention, Slack-2020-FoolingLimeSHAP, pruthi-etal-2020-learning}. 
%and exhibit low consistency across different techniques and instances~\citep{neely2022song, Dasgupta-2022-FaithfulnessEval}. 
Furthermore, they may generate noisy explanations that focus on irrelevant information, making them difficult for humans to interpret~\citep{Smilkov-2017-SmoothGrad, NEURIPS2021_e0cd3f16}. 

\paragraph{From Post-hoc Explanations to Explainable Models}

The limitations of post-hoc explanation methods may be attributed to the inherent lack of explainability in contemporary neural models, which are typically optimized solely for task performance~\citep{Kindermans-2018-PatternNet, rudin2019stop, atanasova2022diagnostics}. 
For instance, studies have shown that existing models struggle to provide faithful explanations~\citep{NEURIPS2018_3e9f0fc9} or tend to learn noisy patterns, resulting in less interpretable explanations~\citep{NEURIPS2021_e0cd3f16}. 

%and \cite{Kindermans-2018-PatternNet} showed that these models often fail to learn patterns recognizable to humans. In response, efforts have been made to enhance model explainability.

In response, various efforts have been made to enhance model explainability. 
Some work has introduced constraints that improve specific explanation properties, such as faithfulness~\citep{tutek2022toward, moradi-etal-2020-training, moradi-etal-2021-measuring}, consistency~\citep{atanasova2022diagnostics}, locality~\citep{NEURIPS2018_3e9f0fc9}, and plausibility~\citep{NEURIPS2021_e0cd3f16}. 
However, as these constraints are typically imposed as regularizers, their effectiveness in improving explanation quality is not guaranteed~\citep{pruthi-etal-2020-learning}. 
Others have proposed self-explanatory model architectures such as rationale-based models that utilize an ``explain-then-predict'' pipeline, where one module selects rationales for another to make predictions based on them~\citep{lei-etal-2016-rationalizing}.
Although seemingly transparent, both components rely on neural networks, making the rationale extraction and utilization processes opaque~\citep{zheng-etal-2022-irrationality, jacovi-goldberg-2021-aligning}. 
Besides, such models may face optimization challenges that limit their practicality in real-world tasks~\citep{lyu-etal-2024-towards}.
\looseness=-1

To tackle these shortcomings, \citet{Bohle_2022_CVPR} proposed B-cos networks. 
Unlike methods that impose external constraints, B-cos networks improve explainability through mathematically grounded architectural and computational adaptations. 
Moreover, these adaptations are designed as drop-in replacements for conventional model components, making B-cos networks easy to train with minimal performance loss. Most recently, \citet{arya24bcosification} explored \textit{B-cosification} techniques to convert existing models into B-cos models, which reduces the training costs of adopting B-cos architectures. 

Despite their successful application in vision tasks, B-cos networks have yet to be explored in NLP, where input modalities and training paradigms differ significantly. 
In this work, we adapt B-cos models for the language domain, integrating them efficiently into NLP pipelines. 

%Specifically, we propose a direct transformation of PLMs into task-specific B-cos LMs, which substantially improves training efficiency compared to previous B-cos methods~\citep{arya24bcosification} and achieves faster convergence than conventional fine-tuning. In addition, our in-depth analysis reveals how B-cos LMs behave differently from conventional models under varying alignment pressures, offering crucial insights into their behavior and guiding future improvements in their design.
\begin{table*}[ht]
    \centering
    \resizebox{\textwidth}{!}{
    \tabcolsep=2pt
    \begin{tabular}{lC{5cm}C{6cm}C{6cm}}
        \toprule
        \textbf{Property} & \textbf{Conventional Fine-tuning} & \textbf{B-cosification}~\citep{arya24bcosification} & \textbf{B-cos LM (ours)} \\
        \midrule
        %\textbf{Modality} & NLP (text) & CV (image) & NLP (text) \\
        %\textbf{Input} & sequential word embeddings & image embeddings & sequential word embeddings \\
        %\midrule
        %\midrule
        \textbf{Bias terms} & yes & no & no \\
        \textbf{B (alignment pressure)} & 1 & 2 & 1.5 \\
        \textbf{Pred. Head Activations} & tanh  & n/a\footnotemark & identity \\
        %\bottomrule
        \midrule
       \textbf{Prior task abilities} & no & yes  & no  \\
        \textbf{Training objectives} & Task fine-tuning & B-cos conversion & Task fine-tuning \& B-cos conversion \\
        \bottomrule
    \end{tabular}
    }
    \caption{Comparison between conventional fine-tuning, B-cosification in computer vision and B-cosification in NLP (B-cos LM). 
    Conventional fine-tuning and B-cosification follow the configuration of BERT for sequence classification and CLIP~\citep{radford2021learning}, respectively (cf. \S~\ref{sec:methodology} for details).}
    \label{tab:comparison}
\end{table*}

\footnotetext{\citet{arya24bcosification} used a single linear layer on top of CLIP so the prediction head activation is not applicable in their setup.}

\section{Methodology}
\label{sec:methodology}
In this section, we outline the architecture and training process of B-cos LMs and how their design ensures faithful and human interpretable explanations.
We first introduce B-cos networks (\S~\ref{bcos background}) and then describe how we transform PLMs to task-specific B-cos LMs (\S~\ref{b-cosification in nlp}). 
Finally, we demonstrate how to generate explanations from B-cos LMs (\S~\ref{generate explanation}).
Notations used in the work are detailed in Appendix~\ref{appendix:notation}.
\looseness=-1

\subsection{B-cos Networks}
\label{bcos background}

Complex neural networks can be interpreted as generalized linear models~\citep{DBLP:conf/icml/NairH10, NEURIPS2018_3e9f0fc9, NEURIPS2019_80537a94}. 
For each input $\mathbf{x}$, the network applies a linear transformation: $\mathbf{f(x) = W(x)x + b(x)}$, where both the weight $\mathbf{W(x)}$ and bias $\mathbf{b(x)}$ depend on $\mathbf{x}$. 
Given that many activation functions are (approximately) piecewise linear, the overall network can be viewed as (approximately) piecewise affine~\citep{NEURIPS2018_3e9f0fc9}. 
Earlier work refers to such models as dynamic linear models~\citep{bohle2021convolutional, Bohle_2022_CVPR}, highlighting the fact that the weight and bias terms dynamically change according to $\mathbf{x}$. 

Under this dynamic linear perspective, the linear mapping $\mathbf{W(x)}$ can be seen as attributing model predictions to individual input features. 
However, two challenges hinder the direct use of this interpretation. 
First, $\mathbf{W(x)}$ alone provides an incomplete and unfaithful explanation since $\mathbf{f(x) \neq W(x)x}$ due to the presence of the bias term $\mathbf{b(x)}$, and incorporating $\mathbf{b(x)}$ into explanations is highly non-trivial~\citep{pmlr-v97-wang19p}. 
Second, $\mathbf{W(x)}$ is often difficult for humans to interpret, as it does not necessarily align only with task-relevant input patterns~\citep{Smilkov-2017-SmoothGrad} and therefore yields noisy and irrelevant explanations. 
Figure~\ref{fig:bcos_vs_baseline} illustrates these challenges. % with concrete examples. 
To address these issues, \citet{Bohle_2022_CVPR} introduced B-cos networks by replacing the conventional linear transformation:
%Specifically, instead of the conventional linear transformation:
\begin{equation}
\mathbf{f(x;w, \text{b}) = w^T x+\text{b} = \|w\|\|x\|\text{cos}(x,w)+\text{b}}
\end{equation}
with a B-cos transformation: % B-cos networks use a 
\begin{align}
\text{B-cos}&\mathbf{(x;w)}=\mathbf{\hat{w}^T x} \times |\text{cos}\mathbf{(x, \hat{w})|^{B-1}} \label{eq:b-cos}\\
&=\|\mathbf{\hat{w}\|\|x\|}|\text{cos}\mathbf{(x,\hat{w})|^B}\nonumber \times \mathbf{\text{sgn}(\text{cos}(x, \hat{w}))} \nonumber
\end{align}
where $\mathbf{\hat{w}}$ is a scaled version of $\mathbf{w}$ with unit norm and $\text{sgn}$ denotes the sign function.
%The B-cos transformation 

B-cos$\mathbf{(x;w)}$ can be seen as a linear transformation of $\mathbf{x}$ with the dynamic linear weight $\mathbf{w(x)=|\text{cos}(x,\hat{w})|^{B-1} \times \hat{w}}$. 
The absence of $\mathbf{b(x)}$ ensures the completeness of summary $\mathbf{w(x)}$.
We demonstrate that this completeness extends to an entire network composed of bias-free dynamic linear modules in~\ref{generate explanation}.
Moreover, with additional alignment pressure (B>1), the weight $\mathbf{w}$ is forced to align closely with task-relevant patterns to achieve a high cosine similarity and strong activation within the B-cos module. 
As a result, only the most relevant features are highlighted in explanations, making them more interpretable to humans.

While early B-cos models were trained from scratch, \citet{arya24bcosification} recently introduced B-cosification, an efficient method to obtain B-cos models. 
This approach first modifies conventional models with task capacities to adopt the B-cos architecture, followed by fine-tuning on downstream datasets for B-cos conversion. 
B-cosified models generate explanations as faithful and interpretable as B-cos models trained from scratch but at a much lower training cost. 
However, directly applying B-cosification to NLP models is non-trivial and inefficient due to the significant differences in model architectures and training pipelines.

%method that fine-tunes existing image classifiers into B-cos models, significantly reducing the cost of obtaining them. However, NLP typically follows a "pre-train then fine-tune" pipeline, where an additional task fine-tuning step is required for downstream applications. Building on this, we also explore how to integrate task fine-tuning with B-cos fine-tuning to efficiently convert PLMs into B-cos LMs. 



\subsection{B-cosification in NLP}
\label{b-cosification in nlp}
In this section, we present our B-cosification approach for NLP.
%, detailing the architectural and computational adaptations applied to PLMs (\ref{b-cosification-architecture}), as well as the fine-tuning process used to transform them into task-specific B-cos LMs (\ref{b-cosification-finetuning}). 
We summarize the differences between B-cosification in NLP, its counterpart in vision, and conventional fine-tuning in Table \ref{tab:comparison}. 
We provide an extensive ablation study in Appendix~\ref{appendix:ablation}.

\subsubsection{B-cos Adaptations}
\label{b-cosification-architecture}
Given a conventional model, we first modify its architecture and computation to integrate the B-cos framework.
%\looseness=-1

\paragraph{Architectural Adaptations} 
%As discussed in Section \ref{bcos background}, bias terms in the network can lead to incomplete and thereof unfaithful explanations. 
For completeness and faithfulness of explanations, we follow \citet{arya24bcosification} and remove all bias terms in models, including those in the affine transformations of layer normalization and attention blocks. 
%Additionally, to accommodate the unique architecture of NLP models, we remove non-linear activation functions that are not (approximately) piecewise linear, such as sigmoid and tanh, in the prediction heads. These activations reduce the locality of explanations and introduce numerical instability when computing them.
Additionally, a prediction head is typically added on top of the transformer before fine-tuning for downstream tasks in the NLP pipeline. This head often includes activation functions that are not (approximately) piecewise linear, such as sigmoid and tanh.
To accommodate the unique architecture of NLP models, we remove all activation functions in the prediction heads, as they reduce the locality of explanations and introduce numerical instability during their generation. 
We expect the added non-linearity from B>1 to compensates for this removal. %The additional non-linearity introduced by B>1 is expected to compensate for this removal.

\paragraph{Introducing B-cos Computation} 
To promote input-weight alignment and improve human interpretability of explanations, we follow \citet{arya24bcosification} and replace all linear transformations with B-cos transformations in \S~\ref{bcos background}. 
%The weight matrix 
For a more efficient B-cosification, B-cos layers are initialized with the corresponding weights $\mathbf{W}$ of the original model.
%$\mathbf{W}$ in B-cos layers is initialized using the corresponding weights in the original model for a more efficient B-cosification.

\subsubsection{Fine-tuning}
\label{b-cosification-finetuning}
The B-cos adaptations above modify the architecture and computation of models, requiring fine-tuning to restore their capabilities and adapt to alignment pressure.
Following the NLP-typical ``pre-train then fine-tune'' paradigm, we directly transform PLMs to B-cos LMs, rather than adapting task-specific models % that already have task-specific capacities, 
as done in previous work~\citep{arya24bcosification}. 
This fundamental difference in the training pipeline adds complexity to B-cosification in NLP, as the objective involves both B-cos conversion and task fine-tuning. 
While there are multiple ways to conjoin these two steps (cf. \S~\ref{sec: different setups}), we find that the most efficient way is to combine them by first applying B-cos adaptations to a PLM and then fine-tuning it on a downstream task. 
Following \citet{Bohle_2022_CVPR}, we use the binary cross-entropy (BCE) loss instead of conventional cross-entropy loss, as it explicitly maximizes the absolute target logits and strengthens alignment pressure.
%Due to architectural constraints and alignment pressure, the model is expected to only acquire the most task-relevant patterns and generate faithful and human interpretable explanations after fine-tuning. 
We provide an extensive comparison of B-cosification setups in \S~\ref{sec: different setups}. % compare the effectiveness and training efficiency of other possible B-cosification setups in Section \ref{sec: different setups}.

%Building on this, we also explore how to integrate task fine-tuning with B-cos fine-tuning to efficiently convert PLMs into B-cos LMs. 


%Besides, instead of the conventional cross-entropy loss, we follow \citet{Bohle_2022_CVPR} and use binary cross-entropy (BCE) loss during fine-tuning. 
%Since BCE explicitly maximizes the absolute target logits, it strengthens alignment pressure and guides B-cos LMs to produce more desirable explanations.

\subsection{Computing B-cos Explanations}
\label{generate explanation}
Once trained, the B-cos LM can generate explanations that faithfully summarize its decision-making process during inference. 
As all components are dynamic linear with no bias terms (cf. Appendix~\ref{appendix:dynamic linear components}), the entire model computation can be expressed as a sequence of dynamic linear transformations:
\begin{equation}
\mathbf{\hat{W}}_L(\mathbf{A}_L)\mathbf{\hat{W}}_{L-1}(\mathbf{A}_{L-1})...\mathbf{\hat{W}}_1(\mathbf{A}_1=\mathbf{X})\mathbf{X}
\end{equation}
which can be completely summarized as a single dynamic linear function $\Pi_{j=1}^{L}\mathbf{\hat{W}}_j(\mathbf{A}_j)$.\footnote{Note that a residual connection of $\mathbf{W(x)x+x}$ with $\mathbf{x} \in \mathbb{R}^n$ and $\mathbf{W(x)} \in \mathbb{R}^{n \times n}$ is mathematically equivalent to a single dynamic linear transformation of $(\mathbf{W(x)+I}_n)\mathbf{x}$.} 
Considering the textual inputs specific to NLP, we attribute the model's predictions to the embedding representations. Specifically, to quantify the contribution of a token $i$ to a model prediction, we compute the dot product $\mathbf{W(x}_i)\mathbf{x}_i$ between its embedding $\mathbf{x}_i$ and the corresponding dynamic linear weight $\mathbf{W(x}_i)$ for the predicted class logit. 
For the remainder of the paper, we will refer to such explanations as \textit{B-cos explanations}.
%These explanations derived directly from dynamic linear B-cos LMs are referred to as \textit{B-cos explanations} throughout the remainder of the paper.


%\paragraph{Explaining B-cos LMs}



\begin{table*}[ht]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{llccccccccc}
     \toprule
      \multirow{2}{*}{\textbf{Model}}&\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{AG News}} & \multicolumn{3}{c}{\textbf{IMDB}} & \multicolumn{3}{c}{\textbf{HateXplain}} \\
      \cmidrule(lr){3-5}\cmidrule(lr){6-8} \cmidrule(lr){9-11}
      & & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) \\
      \midrule

      \multirow{6}{*}{(a) Conv. BERT}&Attention &24.40 & 8.09 & 50 & 26.84 & 14.56 & 50 & 27.64 & 13.83 & 50 \\
      &IxG & 15.28 & 10.19 & 45.41 & 18.29 & 16.96 & 49.42 & 19.16 & 18.90
      & 47.24 \\
      &SIG & 27.02 & 3.40 & 64.77 & 29.34 & 14.05 & 59.09 & 37.31 & 5.10 & 66.38 \\
      &DecompX & 52.16 & 0.92 & 84.48 & 57.94 & 2.41 & 63.27 & 44.86 & 2.72 & 66.76 \\
      &ShapSampl & 43.96 & 0.46 & 82.87 & 58.29 & 2.44 & \textbf{71.29} & 44.86 & 2.43 & 67.17 \\
      &LIME & 44.95 & 0.06 & 80.28 & 51.45 & 6.07 & 60.15 & 22.64 & 14.30 & 57.61 \\
    \midrule
      \multirow{6}{*}{(b) B-cos BERT}&Attention & 33.51 & 2.71 & 50 & 32.91 & 5.31 & 50 & 39.3 & 3.93 & 50 \\
      &IxG & 26.89 & 1.31 & 48.24 & 61.74 & -2.39 & 52.95 & 44.93 & -0.60 & 53.57 \\
      &SIG & 14.39 & 4.65 & 19.64 & 29.06 & 3.11 & 56.82 & 35.04 & 1.96 & 60.23 \\
      &DecompX & - & - & - & - & - & - & - & - & - \\
      &ShapSampl & 15.90 & 3.91 & 52.71 & 35.95 & 0.66 & 52.14 & 39.6 & 0.66 & 65.02 \\
      &LIME & 57.99 & 0.07 & 79.30 & 70.05 & -1.55 & 60.03 & 40.84 & 3.84 & 59.14 \\
      %\cmidrule(lr){2-11}
      %&B-cos & \textbf{64.22} & \textbf{-1.26} & \textbf{87.92} & \textbf{75.33} & \textbf{-2.95} &  \textbf{70.27} & \textbf{59.66} & \textbf{-4.89} & \textbf{77.57} \\  


      %\cmidrule(lr){2-11}
      \midrule
(c) B-cos BERT & B-cos & \textbf{64.22} & \textbf{-1.26} & \textbf{87.92} & \textbf{75.33} & \textbf{-2.95} & 
      70.27 & \textbf{59.66} & \textbf{-4.89} & \textbf{77.57} \\
    \bottomrule
  \end{tabular}
  }
  \caption{Faithfulness evaluation for conventionally fine-tuned BERT and B-cos BERT across three datasets. 
  %We highlight the best results in \textbf{bold} and repeat the B-cos explanations results for easy comparison. 
  The best results are in \textbf{bold}.
  We find that B-cos explanations are consistently more faithful than post-hoc explanations from both models.}
  \label{tab:faithfulness}
\end{table*}
\section{Experiments}

We evaluate the task performance of B-cos LMs and faithfulness of B-cos explanations against conventional models and baseline explanation methods across various tasks, PLMs, and metrics.

\paragraph{Datasets and Models} Our experiments include three sequence classification datasets: AG News (topic classification, \citealp{NIPS2015_250cf8b5}), IMDB (sentiment analysis, \citealp{maas-etal-2011-learning}), and Hate\-Xplain (hate speech detection, \citealp{mathew2021HateXplain}). 
We use BERT \citep{Devlin-2019-BERT}, RoBERTa \citep{DBLP:journals/corr/abs-1907-11692}, and DistilBERT \citep{DBLP:journals/corr/abs-1910-01108} as the basis for conventional fine-tuning and for obtaining B-cos LMs (B=1.5) with the same training hyperparameters (cf. Appendix~\ref{appendix: implementation details} for details on fine-tuning, B-cosification, and data splits). 
%We provide details on fine-tuning, B-cosification, and data splits in Appendix~\ref{appendix: implementation details}.

\paragraph{Faithfulness Metrics} For a more comprehensive evaluation, we employ two different methods to assess faithfulness. 
First, we report two perturbation-based metrics~\citep{deyoung-etal-2020-eraser}:
\begin{itemize}
    \item \textbf{Comprehensiveness} (Comp) measures the average drop in predicted class probability after masking out the top $k\%$ most important tokens in the explanation. A higher score indicates better faithfulness.
    \item \textbf{Sufficiency} (Suff) measures the average drop in predicted class probability after keeping only the top $k\%$ tokens. A lower score indicates better faithfulness.
\end{itemize}

To avoid arbitrary choices of $k$, we compute Comp and Suff for multiple values ($k=10, 20, ..., 90$) and summarize them using the \underline{A}rea \underline{O}ver the \underline{P}erturbation \underline{C}urve (AOPC, \citealp{deyoung-etal-2020-eraser}).

In addition, we introduce a new faithfulness metric called \underline{Seq}uence \underline{P}ointing \underline{G}ame (Seq\-PG), inspired by the grid pointing game in vision tasks~\citep{bohle2021convolutional}:
%\looseness=-1
\begin{itemize}
    \item \textbf{Sequence Pointing Game} (SeqPG). We evaluate models on synthetic sequences composed of segments associated with different classes. To assess faithfulness, we measure the proportion of positive attribution assigned to the corresponding segment of each class and compute their average. A higher score indicates better faithfulness.
\end{itemize}

Compared to perturbation-based metrics, SeqPG does not rely on perturbations and thus avoids the potential distortions introduced by token masking. 
When constructing SeqPG examples, we truncate each segment to a fixed length and randomize segment order to control for length and position effects. 
%Following \citet{bohle2021convolutional}, 
We generate synthetic examples using correctly and most confidently classified test instances.
SeqPG can be seen as a standardized version of hybrid document evaluation~\citep{poerner-etal-2018-evaluating}. %, and an example of SeqPG is provided in Appendix \ref{appendix:seqpg example}. 
We provide an example of SeqPG in Figure~\ref{fig:seqpg} and more details in Appendix~\ref{appendix:seqpg example}.







\paragraph{Baselines} We compare B-cos explanations against a diverse set of post-hoc explanation methods: Attention \citep{DBLP:journals/corr/BahdanauCB14}, InputXGradient (IxG, \citealp{DBLP:journals/corr/KindermansSMD16}), Sequential Integrated Gradients (SIG, \citealp{enguehard-2023-sequential}), DecompX \citep{modarressi-etal-2023-decompx}, Shapley Value Sampling (ShapSampl, \citealp{strumbelj2010efficient}), and LIME \citep{Ribeiro-2016-LIME}. 
For a fair comparison against embedding-level methods, we aggregate attributions by summing across all embedding dimensions (cf. Appendix~\ref{appendix: implementation details}).
%For methods that produce explanations at the embedding level rather than the token level, we aggregate attributions by summing across all embedding dimensions, as this yields better faithfulness results (cf. Appendix~\ref{appendix: implementation details}). %Further details on baseline configurations are provided in Appendix \ref{appendix: implementation details}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{bert_performance_comparison.pdf}
    \caption{Mean accuracy of conventionally fine-tuned and B-cos BERT models averaged over three runs. B-cos models perform comparably to conventional models on most tasks.}
    \label{fig:bert performance}
\end{figure}

\paragraph{Task Performance}
\label{sec:task performance}

Figure~\ref{fig:bert performance} shows the accuracy of conventionally fine-tuned and B-cos BERT across three datasets (we provide results for DistilBERT and RoBERTa in Appendix~\ref{appendix:accuracy other plms}).
We find that all B-cos LMs performs on par with conventional models on AG News and HateXplain, with only a minor drop ($\sim$1\%) in accuracy. 
Only for IMDB, we find a slightly larger drop of 4.21\%, though the performance remains strong overall. 
%Results for other DistilBERT and RoBERTa are consistent and are provided in Appendix \ref{appendix:accuracy other plms} 

\paragraph{Faithfulness Results}

Table~\ref{tab:faithfulness} shows the faithfulness scores for post-hoc explanation methods on (a) conventionally fine-tuned BERT models and (b) B-cos BERT models, as well as (c) B-cos explanations extracted from B-cos BERT.
The results show that B-cos explanations are consistently and substantially more faithful than post-hoc methods across all datasets. This improvement holds both across different models and within the same model. 
B-cos explanations outperform the strongest post-hoc methods on conventional models by an average of 14.63 points in Comp score and achieve negative Suff scores, indicating that the identified important tokens alone enable even more confident predictions. 
Additionally, B-cos explanations show a considerable improvement in SeqPG.
Similar trends are observed for DistilBERT and RoBERTa (Appendix~\ref{appendix:other PLMs}), further strengthening our findings.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{human_evaluation.pdf}
    \caption{Human evaluation reveals that B-cos explanations have better human interpretability and human agreement than baseline methods and the improvements are statistically significant.}
    \label{fig:human}
\end{figure}

\section{Human Evaluation}
We conduct a human study to evaluate the human interpretability and agreement of B-cos explanations, comparing them against three strong post-hoc explanation methods on the conventional BERT model.
%As we aim to measure how understanding the explanations are to humans instead of agreement with human rationales, we rely on a human study to compare B-cos explanations with those of the three strongest post-hoc explanation methods from the baseline BERT model on the AG News and HateXplain datasets.
%As the human explanation datasets suffer from poor quality and  low consistency across instructions~\citep{tan-2022-diversity}, we rely on a human study to compare the plausibility of B-cos explanations with those of the three strongest post-hoc explanation methods from the baseline BERT model on the AG News and HateXplain datasets.
For the study, we randomly select 50 instances from AG News and HateXplain where the B-cos and conventional model predict the same label.
We then ask five annotators to rate the respective explanations in terms of human interpretability (how well they understand it) and human agreement (how much they agree with the it) on a scale of 1-5. 
We provide further details of the human evaluation in Appendix~\ref{appendix:human evaluation}.



\paragraph{Human Evaluation Results}
Figure~\ref{fig:human} shows that B-cos explanations have a better human interpretability and exhibit greater alignment with human reasoning than post-hoc methods. 
Conducting paired t-tests with a Bonferroni-corrected $\alpha=\frac{0.05}{6}=0.008\overline{3}$~\citep{bonferroni1936teoria} shows that the improvements of B-cos explanations are statistically significant ($p<\alpha$) for both metrics.




\section{Qualitative Analysis}
\label{sec:qualitative}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{baselines.png}
    \caption{Examples of B-cos explanations (B-cos BERT) as well as ShapSampl and DecompX explanations (BERT) from AG News.
    \textcolor{forestgreen}{Green} (\textcolor{red}{red}) indicates the \textcolor{forestgreen}{positive} (\textcolor{red}{negative}) impact of tokens on the prediction. 
    The B-cos explanation highlights only relevant tokens and is more interpretable to humans (cf. Appendix~\ref{appendix:more examples} for more examples).}
    \label{fig:examples}
\end{figure*}

Figure~\ref{fig:examples} provides an example of B-cos and other (post-hoc) explanations.
It can be seen the B-cos explanation highlights important tokens well with little focus on irrelevant ones. 
In contrast, ShapSampl attributes the highest importance to the [SEP] token and provides only little useful information. 
Meanwhile, DecompX extracts a significant amount of irrelevant information. Overall, we find that the B-cos explanation provides clearer and more relevant attributions compared to the post-hoc explanations.


\section{Comparison of B-cosification Setups}
\label{sec: different setups}
Transforming PLMs into task-specific B-cos LMs involves two key objectives: task fine-tuning and B-cos conversion. While our main experiments combine these two phases, they can also be performed separately. To assess their effects, we compare two alternative training setups:
\begin{itemize}
    \item Task then B-cos: PLMs are first fine-tuned on a downstream task. % for task abilities. 
    B-cos adaptations are then applied, followed by further fine-tuning on the same task for B-cos conversion. 
    This setup is equivalent to \citet{arya24bcosification} who apply B-cosification to models with task capabilities.
    \item B-cos then task: B-cos adaptations are applied to PLMs first, followed by pre-training on unsupervised texts to enhance B-cosification (cf.~Appendix~\ref{appendix: implementation details}).
    The pre-trained B-cos models are then fine-tuned on the downstream task. 
\end{itemize}

We evaluate these setups against the B-cosification approach used in our main experiments (B-cos LM) and compare task performance, faithfulness, and training efficiency. 
Additionally, we report results for conventional fine-tuning (Conv. LM) and training a randomly initialized B-cos LM (B-cos from scratch). 
Experiments are conducted on IMDB, with results averaged over three runs.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \tabcolsep=2pt
    \begin{tabular}{lcccc}
    \toprule 
    \textbf{Setup} & \textbf{Epochs} & \textbf{Acc ($\uparrow$)} & \textbf{SeqPG ($\uparrow$)} & \textbf{Steps (K)} \\
    \midrule
    Conv. LM & 5 & 94.06 & - & 7.33 \\
    \midrule
    B-cos LM & 5 & 89.85 & 70.41 & 3.67 \\
    \midrule
    B-cos from scratch & 5 & 88.36 & 60.92 & 4.33 \\
    \midrule
    \multirow{5}{*}{Task then B-cos} & 1+4 & 90.14 & 70.28 & 1+4.33 \\
     & 2+3 & 90.33 & 70.36 & 3+3.33 \\
     & 3+2 & 90.07 & 69.94 & 4+3 \\
     & 4+1 & 88.19 & 70.36 & 5+1 \\
     & 5+5* & 90.33 & 69.65 & 6.67+3.33 \\
     \midrule
    \multirow{7}{*}{B-cos then task} & 1+4 & 89.78 & 65.58 & 1+5.67 \\
    & 2+3 & 89.81 & 66.01 & 3+4 \\
    & 3+2 & 89.38 & 66.95 & 4+3 \\
    & 4+1 & 87.42 & 67.9 & 6+1 \\
    & 5+5* & 90.38 & 71.16 & 7+3 \\
    & 10+5* & 91.08 & 75.06 & 15+3.67 \\
    & 20+5* & 91.75 & 76.66 & 31+6.33 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Different B-cosification setups. % Training epochs, accuracy, explanation faithfulness, and convergence steps for 
    For two-phase methods, we report epoch distribution and convergence steps per phase. 
    * marks additional training epochs.}
    \label{tab:different setup}
\end{table}

Table~\ref{tab:different setup} shows that B-cos LM requires fewer training steps to reach optimal validation performance than conventional fine-tuning. 
Training B-cos LM from scratch results in worse task performance and faithfulness, emphasizing the importance of good parameter initialization.
Among the two setups that separate task fine-tuning and B-cos conversion, \textit{Task then B-cos} achieves results comparable to B-cos LM but requires more training steps. 
\textit{B-cos then task} initially performs worse under the same training budget. 
However, with additional pre-training epochs, it surpasses other B-cosification setups in both task performance and faithfulness.
Overall, we find that combining task fine-tuning and B-cos conversion is the most efficient approach. 
However, with sufficient pre-training, \textit{B-cos then task} can produce more performant and explainable models. 

\section{Effects of B-cosification and B Values}
\label{sec:B}

For a deeper understanding of how B-cosification and parameter B affect model performance and behavior, we compare conventional and B-cos BERT trained on HateXplain across different B values. 
We also provide an empirical analysis of the impact of B on input-weight alignment in Appendix~\ref{appendix:alignment}.

\paragraph{Model Performance}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{b_analysis_w_comp.pdf}
    \caption{Varying B for B-cos BERT (HateXplain). Accuracy and Comp both peak around B=1.5, while explanation entropy negatively correlates with B.}
    \label{fig:b analysis}
\end{figure}

Figure~\ref{fig:b analysis} shows the effects of varying B on the task performance and explanation faithfulness. 
Classification accuracy initially improves slightly as B increases from 1 to 1.25, benefiting from the extra non-linearity introduced by B>1. 
However, beyond this point, accuracy declines as higher alignment pressure reduces model flexibility. 
A similar trend is observed for Comp, peaking around B=1.5 before decreasing. This differs from previous findings in vision models~\citep{Bohle_2022_CVPR}, which we attribute to the high sparsity of explanations at larger B values. 
As alignment pressure increases, fewer tokens receive attribution scores that are not close to zero, leading to poor token importance calibration and lower Comp scores. 
The effects of B on other metrics are similar and can be found in Appendix~\ref{appendix:different b all metrics}.
%As alignment pressure increases, fewer tokens receive attribution scores not close to zero, and B-cos LMs can attribute predictions to a single segment in some SeqPG examples, leading to numerical instability when computing the positive attribution ratio. Besides, faithfulness measured by perturbation-based metrics also declines with larger B due to poorer token importance calibration caused by overly sparse explanations (cf. Appendix~\ref{appendix:different b all metrics}).

\paragraph{Explanation Entropy}

%The choice of B also influences the structure of B-cos explanations. We measure the entropy of explanations from B-cos LMs across different B values (Figure \ref{fig:b analysis}). 
Figure~\ref{fig:b analysis} also reveals a negative correlation between explanation entropy and B, indicating that higher alignment pressure leads to sparser explanations.
This aligns with our expectations: a larger B amplifies the differences between dimensions in $\mathbf{|\text{cos}(x,\hat{W})|^{B-1}}$ of B-cos layers (Equation~\ref{eq:b-cos}) and the dynamic linear weight assigns more distinct attributions to input features. As a result, explanations become more concentrated, where only a few tokens receive high attributions, while most remain close to zero (cf. Appendix~\ref{appendix:examples different b} for an example).
% $\mathbf{W(x)=|\text{cos}(x,\hat{W})|^{B-1}\otimes \hat{W}}$
%An example of explanations with different B values is provided in Appendix \ref{appendix:examples different b}.




\paragraph{Model Bias}
\label{sec:bias}
Since B-cos LMs with larger B values rely on fewer tokens for prediction, we investigate whether this may cause them to learn biases in the data. 
For this, we examine label bias and word-level spurious correlations using the HateXplain dataset, where approximately 60\% of training and test examples have positive labels and societal biases are present.
Figure~\ref{fig:label bias} shows that a larger B value (B=2.5) reduces the model capacity, leading to a substantially higher prediction positive rate and lower balanced accuracy. 
Moreover, the B=2.5 model assigns higher attributions to non-semantic [CLS] and [SEP] tokens, indicating a reduced reliance on meaningful content. 
Notably, this label bias is not observed in the balanced datasets.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{model_bias.pdf}
    \caption{Comparison of conv. BERT and B-cos BERT with different B values. 
    The attributions to [CLS] and [SEP] tokens ($\color{red}\blacksquare$) indicate that B-cos LMs with large B overfit to the non-semantic label distribution.}
    \label{fig:label bias}
\end{figure}

We also find that B-cosification---particularly with large B---amplifies the reliance on spurious correlations. 
%For example, we observe that bias against Black individuals becomes more pronounced in B-cos LMs with increasing B: 
For example, the prediction positive rate for examples with the word ``black'' rises from 49.02\% in the test set and 52.94\% in the conventional model to 59.80\%, 56.86\%, and 73.53\% in B-cos LMs with B=1, 1.5, and, 2.5, respectively (we provide an example in Appendix~\ref{appendix:bias}).
%See Appendix~\ref{appendix:bias} for an example of such bias.
However, the faithfulness and interpretability of B-cos explanations facilitate the identification of spurious correlations and can effectively guide models toward reducing them~\citep{Rao_2023_ICCV}. 
We leave the exploration of B-cos LMs for bias detection and mitigation to future work.





\section{Explanation Efficiency}

Beyond improved faithfulness and human interpretability, B-cos explanations are also efficient to extract. 
Comparing their computational costs with strong post-hoc methods shows that B-cos explanations are the most efficient in both time and memory usage (Table~\ref{tab:efficiency}).


\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \tabcolsep=2pt
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Conv. BERT}} & \multicolumn{2}{c}{\textbf{B-cos BERT}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
     & \textbf{Time (s)} & \textbf{Memory (GB)} & \textbf{Time (s)} & \textbf{Memory (GB)} \\
    \midrule
    ShapSampl & 37.22 & 21.95 & 70.49 & 22.95 \\
    LIME & 6.82 & 21.96 & 8.92 & 22.95 \\
    SIG & 67.46 & 29.09 & 108.48 & 69.32 \\
    DecompX & 0.76 & 48.38 & - & - \\
    B-cos & - & - & \textbf{0.08} & \textbf{2.78} \\
    \bottomrule
    \end{tabular}
    }
    \caption{Computational costs of generating explanations for 100 instances randomly sampled from IMDB (test) using an NVIDIA H100 GPU (batch size 1).
    We see that the B-cos explanations (\textbf{bold}) are at least 9x faster and require at most $\frac{1}{8}$ of VRAM.
    %All computations were done on .
    %Results are based on 100 IMDB test examples using an NVIDIA H100 GPU with a batch size of 1. 
    %B-cos explanations are substantially more efficient to extract.
    }
    \label{tab:efficiency}
\end{table}

\section{Conclusion}

In this work, we introduce B-cos LM, a dynamic linear model that learns task-relevant patterns through increased input-weight alignment pressure. 
B-cos LMs generate more faithful and human interpretable explanations while maintaining strong task performance and fast convergence. 
Based on our in-depth analysis of B-cosification, we provide three recommendations for effectively transforming PLMs into B-cos LMs: (1) combine B-cos conversion and task fine-tuning for efficient B-cosification. 
If resources allow, additional B-cos pre-training can further improve task performance and explanation faithfulness; (2) carefully select the parameter B, as excessively large values can reduce model capacity and lead to overly sparse explanations; and (3) be mindful of biases in training data, especially at high B values, as B-cosification may amplify existing biases.

%\begin{enumerate}
%    \item Combine B-cos conversion and task fine-tuning for efficient B-cosification. If resources allow, additional pre-training can further improve task performance and explanation faithfulness.
%    \item Carefully select the parameter B, as excessively large values can reduce model capacity, degrade faithfulness, and lead to overly sparse explanations.
%   \item Be mindful of biases in training data, especially at high B values, as B-cosification may amplify existing biases.
%\end{enumerate}


\section{Limitations}

This study has certain limitations that should be acknowledged.

Firstly, the automatic evaluation metrics we use may not fully capture the faithfulness of different explanation methods~\citep{feng-etal-2018-pathologies, lapuschkin2019unmasking}. 
However, since there is no universal consensus on the most reliable evaluation metrics, this remains an open challenge in explainability research.

Secondly, our study does not include a direct comparison with other methods designed to enhance model explainability, which may limit the scope of our findings. 
This omission is due to two reasons: (1) existing explainable models often provide only marginal improvements over post-hoc explanation methods~\citep{brinner-zarriess-2024-rationalizing}, and (2) incorporating them into our study would require substantial computational resources, as many baseline explanation methods are computationally expensive.

Finally, although B-cos LMs can be applied to different model architectures and tasks, our experiments focus only on encoder-only models for sequence classification tasks. Extending our approach to other architectures and tasks remains an avenue for future work.

\section{Ethical Considerations}

As discussed in \S~\ref{sec:bias}, B-cos LMs can overfit to biases present in the training data. 
Although their more faithful and human interpretable explanations make biased predictions easier to detect, this does not eliminate the risk of unintended bias amplification. 
We encourage users to carefully assess potential biases in their specific use cases before deploying B-cos LMs and to incorporate bias mitigation strategies where necessary.

All models and datasets used in this work comply with their respective licenses. Their usage aligns with their intended purpose as specified by their creators.

The human study complies with all ethical research guidelines set by our institutes. 
All participants of the human evaluation study were master's or doctoral students with backgrounds in computer science or computational linguistics and were proficient in English.
They were volunteers and were compensated with the standard hourly salary set by the university (at least 5\% above minimum wage). 
Before participation, all participants were informed about the content and purpose of the study, the collected data and its usage.
They were instructed on how they could access, modify, or delete their data post-study and provided their informed consent.

\section*{Acknowledgements}
This work was funded in part by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -- GRK 2853/1 “Neuroexplicit Models of Language, Vision, and Action” - project number 471607914. We also thank all those who provided valuable feedback and suggestions during the development of this work. Their input has been instrumental in improving its quality.
%We are grateful to the anonymous reviewers and area chairs for their exceptionally detailed and helpful feedback.
%\paragraph{Data collection.} 
%The study fulfills all conditions of our university's guidelines for ethical research. 
%To ensure a GDPR-conform data collection, we do not collect any personal data of our participants or any data that could allow a profiling of our participants. 
%Before participation, all participants were informed about the content and purpose of the study, the collected data and its usage.
%They were instructed on how they could access, modify, or delete their data post-study.
%All participants were employed for the duration of the study and received a compensation of $\sim$\$XX (XX\% above the minimum wage). 
%All data is anonymized for publication. 

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix


\section{Terminology}
\label{appendix:definitions}

To ensure clarity, we define key terms used in this work as follows:

\begin{itemize}
    \item \textbf{Faithfulness}. The extent to which an explanation accurately reflects the model’s actual reasoning process~\citep{jacovi-goldberg-2020-towards}. A faithful explanation should directly correspond to the internal mechanisms that led to the model’s prediction.
    \item \textbf{Human Interpretability}. The ease with which a person can understand the model’s reasoning from the explanation~\citep{DBLP:journals/corr/abs-1902-00006}. A highly interpretable explanation should be clear, concise, and focused on relevant information while avoiding unnecessary or distracting information. However, an explanation that is easy for humans to interpret may not necessarily reflect the model’s actual reasoning process or align with human reasoning patterns.
    \item \textbf{Human Agreement}. The degree to which a model’s explanation aligns with the reasoning a human would use for the same prediction. A high-agreement explanation should follow intuitive, logical reasoning patterns similar to human decision-making.
    \item \textbf{Explainability}. The extent to which a model’s computations can be faithfully explained and its learned patterns are understandable to humans. A highly explainable model should yield explanations that are both faithful to its actual reasoning process and interpretable to humans.
\end{itemize}


\section{Notation}
\label{appendix:notation}
In this paper, we use lowercase letters for scalars (e.g., $\text{b}$), bold lowercase letters for vectors (e.g., $\mathbf{w}$, $\mathbf{x}$), and bold uppercase letters ($\mathbf{W}$) for matrices. Additionally, we use bold uppercase letters $\mathbf{X}$ and $\mathbf{A}$ to denote a sequence of model inputs or hidden state activations. In \S~\ref{sec:methodology}, we use $\mathbf{x}$ to denote the input when a function is applied to each element of the input sequence separately. In contrast, we use $\mathbf{X}$ or $\mathbf{A}$ when the function involves interactions between elements, such as in the attention mechanism.


\section{Ablation Study}
\label{appendix:ablation}

To gain deeper insights into B-cosification, we conduct an ablation study to evaluate the effects of key design choices on model performance. 
Table~\ref{tab:ablation} reports the effects of these modifications.

Consistent with \S~\ref{sec:B}, B=1 results in worse task performance and lower explanation faithfulness. Using binary cross-entropy (BCE) loss instead of conventional cross-entropy loss has minimal impact on classification accuracy, but leads to better faithfulness results in perturbation-based evaluations. Additionally, architectural adaptations, including removing bias terms and eliminating activation functions in prediction heads, play a crucial role in improving both model performance and explainability in B-cos LMs. Besides, we encountered numerical instability when generating explanations without these architectural adaptations, as the dynamic linear weight for tanh ($\frac{\text{tanh}(\mathbf{x})}{(\mathbf{x})}$) becomes unstable when $\mathbf{x}$ is close to 0.

Beyond ablating components in model design and training, we also examine different explanation methods across models. First, replacing dynamic linear weights $\mathbf{W(x)}$ with gradients for computing input contributions (equivalent to InputXGradient, \citealp{DBLP:journals/corr/KindermansSMD16}) results in less faithful explanations. Moreover, directly extracting B-cos-like explanations, $\mathbf{W(x)x}$, from a conventional model results in worse faithfulness compared to those from B-cos LMs..\footnote{Extracting $\mathbf{W(x)x}$ from conventional models follows the same approach as in B-cos LMs (cf. \S~\ref{appendix:dynamic linear components}), except that in standard linear transformations, the dynamic linear weight is replaced by the fixed weight matrix $\mathbf{W}$.}

\begin{table*}[h]
    \centering
    
    \begin{tabular}{lcccc}
    \hline
     & Acc ($\uparrow$) & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) \\
     \hline
    Full system     & 78.64 & 59.66 & -4.89 & 77.57 \\
    \hline
    w/o alignment pressure (B=1) &  78.07 & 57.19 & -2.57 & 70.18 \\
    w/o BCE training & 79.00 & 49.22 & -7.91 & 79.21 \\
    w/o architectural adaptations & 77.65 & 52.23 & -3.80 & 74.30 \\
    \hline
    w/o dynamic linear weights (IxG) & 78.64 & 44.93 & -0.60 & 53.57 \\
    $\mathbf{W(x)x}$ from conv. model & 80.77 & 44.92 & 2.80 & 70.20 \\
    \hline
    
    \end{tabular}
    
    \caption{Ablation study of key designs in B-cos BERT model on HateXplain. Results are averaged over three runs.}
    \label{tab:ablation}
\end{table*}

\section{Dynamic Linear Representation of Model Components}
\label{appendix:dynamic linear components}
Here we describe how each model component functions as a dynamic linear module in B-cos LMs.

\paragraph{B-cos Layers}

B-cos layers are designed as dynamic linear modules with a dynamic linear weight matrix $\mathbf{W(x)=|\text{cos}(x,\hat{W})|^{B-1}\otimes \hat{W}}$. Here, $\mathbf{\otimes}$ scales the rows of the matrix $\mathbf{\hat{W}}$ to its right by the scalar entries of the vector to its left.

\paragraph{Non-linear activation functions}

In transformer models, non-linearity is typically introduced using (approximately) piecewise linear activation functions, such as ReLU \citep{DBLP:conf/icml/NairH10} and GELU \citep{DBLP:journals/corr/HendrycksG16}. These functions can be easily interpreted as linear transformations with input-dependent weights. For example, $\text{GELU}(\mathbf{x})=\mathbf{x} \times (0.5 + 0.5 \times  \text{erf}(\mathbf{x}/\sqrt{2}))$ can be interpreted as a linear transformation where the second term acts as a dynamic linear weight.  


\paragraph{Attention block}

\citet{bohle2024b} showed that attention computations can be seamlessly integrated into B-cos networks as a dynamic linear module:
\begin{align}
\text{Att}(\mathbf{X;Q,}&\mathbf{K,V})=\text{softmax}(\mathbf{X^TQ^TKX)VX} \nonumber \\
&=\mathbf{A(X)VX}=\mathbf{W(X)X}
\end{align}

For multi-head self-attention (MSA), the output can be viewed as the concatenation of the outputs from $H$ attention heads, followed by a linear projection with matrix $\mathbf{U}$:
\begin{equation}
    \text{MSA}\mathbf{(X)=U[W}_1\mathbf{(X)X, ..., W}_H\mathbf{(X)X)]}
\end{equation}
Since this operation maintains a dynamic linear structure, the multi-head attention block remains a dynamic linear module.

\section{Implementation Details}
\label{appendix: implementation details}

\paragraph{Fine-tuning Setups} For all PLMs used in the experiments, we use the uncased base version from huggingface~\citep{wolf-etal-2020-transformers}. 
For both conventional models and B-cos LMs, we train them for 5 epochs with 10\% linear warm-up steps on the downstream task datasets. 
The learning rates are set to 2e-5 for IMDB and HateXplain, and 3e-5 for AG News. 
All models use a batch size of 16 and a maximum sequence length of 512. 
For validation, we randomly sample half of the test set from IMDB and AG News.

\paragraph{Post-hoc Explanation Baselines}
For IxG and ShapSampl, we use the Captum~\citep{DBLP:journals/corr/abs-2009-07896} implementations.\footnote{\url{https://captum.ai/api/}} We implement the Attention method ourselves, and LIME is sourced from the lit library\footnote{\url{https://github.com/PAIR-code/lit}}. For DecompX\footnote{\url{https://github.com/mohsenfayyaz/DecompX}} and SIG\footnote{\url{https://github.com/josephenguehard/time_interpret}}, we use their official implementations with default configurations. The number of samples is set to 25 for ShapSampl and 3,000 for LIME, with [MASK] as the baseline token. For all explanation methods at the embedding level, model predictions are attributed to the combined sum of word, position, and token type embeddings (if applicable). In the main experiments, we compute token attribution scores by summing over all embedding dimensions, as this approach demonstrates better faithfulness results than using the L2 norm.

\paragraph{SeqPG Examples} When constructing examples for SeqPG, we set the sequence length to 50 for AG News, 256 for IMDB, and 25 for HateXplain, aligning with their median lengths. Only examples longer than these thresholds are selected, and they are truncated to construct synthetic examples. Additionally, we only use examples that are correctly predicted with a minimum confidence of 75\% after truncation. For a fair comparison, we evaluate B-cos LMs on the same sets of examples constructed based on the predictions of the corresponding conventional models.

\paragraph{Evaluation Setups}

For task performance evaluation, we use the complete test set for each task. For faithfulness evaluation, we conduct perturbation-based evaluations on 2000 test examples and SeqPG on 500 test examples for AG News and IMDB. For HateXplain, we use the full test set for perturbation-based evaluation (1,924 examples) and construct 269, 310, and 308 SeqPG examples from it using BERT, DistilBERT, and RoBERTa, respectively.

\paragraph{B-cos Pre-training}
For B-cos pre-training in \S~\ref{sec: different setups}, we further pre-train the model on the Wikipedia dataset\footnote{\url{https://huggingface.co/datasets/wikimedia/wikipedia}} using masked language modeling loss with a learning rate of 1e-4 and a 15\% masking ratio.

\paragraph{Compute Infrastructure} 
Unless stated otherwise, all experiments are conducted on a single NVIDIA H100 GPU. 
Training one epoch of B-cos BERT takes approximately 40 minutes on AG News, 10 minutes on IMDB, and 5 minutes on HateXplain.

\section{SeqPG Example}
\label{appendix:seqpg example}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{seqpg_example.png}
    \caption{An example of SeqPG from AG News (using B-cos BERT). \textcolor{forestgreen}{Green} (\textcolor{red}{red}) indicates the \textcolor{forestgreen}{positive} (\textcolor{red}{negative}) impact of tokens on the prediction. The example consists of two sequences with different labels (Sports and Sci/tech), separated by the [SEP] token after the first sequence. Explanations are generated for each label, and the proportion of correctly attributed positive tokens is averaged across both labels to compute the SeqPG score for this example.}
    \label{fig:seqpg}
\end{figure*}

Figure~\ref{fig:seqpg} presents a SeqPG example from AG News using B-cos BERT. 
For better visualization, each segment is truncated to 20 tokens instead of 50 used in the experiments. 
Unlike the hybrid document evaluation proposed by \citet{poerner-etal-2018-evaluating}, our approach explicitly controls segment length and position to ensure a fair comparison. 
Additionally, we measure the proportion of correctly assigned positive attributions rather than relying solely on the highest attribution value.
%As can be seen, changing the target class label flips the positive/negative contributions for many tokens.  

\section{Task Performance of Other B-cos LMs}
\label{appendix:accuracy other plms}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{distilbert_performance_comparison.pdf}
    \caption{Mean accuracy of conventionally fine-tuned and B-cos DistilBERT models averaged over three runs. B-cos models perform comparably to conventional models on most tasks.}
    \label{fig:distilbert performance}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{roberta_performance_comparison.pdf}
    \caption{Mean accuracy of conventionally fine-tuned and B-cos RoBERTa models averaged over three runs. B-cos models perform comparably to conventional models on most tasks.}
    \label{fig:roberta performance}
\end{figure}

Figures~\ref{fig:distilbert performance} and~\ref{fig:roberta performance} illustrate the task performance of conventional and B-cos DistilBERT and RoBERTa across datasets. 
Consistent with findings from BERT models (cf. Figure~\ref{fig:bert performance}), B-cos LMs exhibit strong performance comparable to conventionally fine-tuned models.

\section{Faithfulness Evaluation of Other B-cos LMs}
\label{appendix:other PLMs}
Tables~\ref{tab:faithfulness distilbert} and~\ref{tab:faithfulness roberta} present the faithfulness evaluation results for DistilBERT and RoBERTa. 
The findings are consistent with our main experiments (cf. Table~\ref{tab:faithfulness}), confirming that B-cos LMs produce more faithful explanations compared to post-hoc explanation methods.


\begin{table*}[ht]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{llccccccccc}
     \toprule
      \multirow{2}{*}{\textbf{Model}}&\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{AG News}} & \multicolumn{3}{c}{\textbf{IMDB}} & \multicolumn{3}{c}{\textbf{HateXplain}} \\
      \cmidrule(lr){3-5}\cmidrule(lr){6-8} \cmidrule(lr){9-11}
      & & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) \\
      \midrule
    \multirow{6}{*}{(a) Conv. DistilBERT}&Attention & 26.36 & 5.37 & 50 & 31.62 & 10.46 & 50 & 30.56 & 14.67 & 50 \\
      &IxG & 19.29 & 6.21 & 53.71 & 23.78 & 12.38 & 49.23 & 25.13 & 18.08 & 46.60 \\
      &SIG & 30.78 & 1.63 & 67.87 & 47.16 & 5.48 & 60.66 & 41.11 & 4.23 & 58.55 \\
      &DecompX & - & - & - & - & - & - & - & - & - \\
      &ShapSampl & 52.56 & -0.56 & 82.64 & 63.29 & 2.91 & 70.27 & 48.73 & 0.87 & 64.44 \\
      &LIME & 52.59 & -0.56 & 77.64 & 58.6 & 5.12 & 61.11 & 31.61 & 12.94 & 56.49 \\
      %\cmidrule(lr){2-11}
        %&B-cos & \textbf{61.93} & \textbf{-1.01} & \textbf{86.78} & \textbf{75.73} & \textbf{-2.57} & \textbf{71.95} & \textbf{57.2} & \textbf{-4.49} & \textbf{74.89} \\
      

    \midrule

  \multirow{6}{*}{(b) B-cos DistilBERT}&Attention & 28.47 & 3.05 & 50 & 31.36 & 4.15 & 50 & 37.33 & 6.49 & 50 \\
      &IxG & 22.33 & 9.09 & 58.63 & 51.02 & -1.44 & 53.76 & 41.62 & 0.29 & 56.03 \\
      &SIG & 14.73 & 5.62 & 53.09 & 39.75 & -0.11 & 64.18 & 28.68 & 7.27 & 60.75 \\
      &DecompX & - & - & - & - & - & - & - & - & - \\
      &ShapSampl & 31.78 & 1.77 & 62.60 & 64.65 & -2.42 &  56.89 & 34.64 & 4.56 & 55.8 \\
      &LIME & 58.25 & 0.31 & 77.65 & 69.96 & -0.43 & 61.08 & 44.66 & 1.66 & 59.27 \\
      %\cmidrule(lr){2-11}
      \midrule
      (c) B-cos DistilBERT  &B-cos & \textbf{61.93} & \textbf{-1.01} & \textbf{86.78} & \textbf{75.73} & \textbf{-2.57} & \textbf{71.95} & \textbf{57.2} & \textbf{-4.49} & \textbf{74.89} \\


    \bottomrule
  \end{tabular}
  }
  \caption{Faithfulness evaluation for conventionally fine-tuned DistilBERT and B-cos DistilBERT across three datasets. 
  %We highlight the best results in \textbf{bold} and repeat the B-cos explanations results for easy comparison. 
  The best results are in \textbf{bold}.
  We find that B-cos explanations are consistently more faithful than post-hoc explanations from both models.}
  \label{tab:faithfulness distilbert}
\end{table*}





\begin{table*}[h]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{llccccccccc}
     \toprule
      \multirow{2}{*}{\textbf{Model}}&\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{AG News}} & \multicolumn{3}{c}{\textbf{IMDB}} & \multicolumn{3}{c}{\textbf{HateXplain}} \\
      \cmidrule(lr){3-5}\cmidrule(lr){6-8} \cmidrule(lr){9-11}
      & & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) & Comp ($\uparrow$) & Suff ($\downarrow$) & SeqPG ($\uparrow$) \\
      \midrule
      
      \multirow{6}{*}{(a) Conv. RoBERTa}&Attention & 22.17 & 3.80 & 50 & 25.26 & 5.84 & 50 & 32.94 & 7.52 & 50 \\
      &IxG & 11.33 & 7.54 & 44.15 & 16.15 & 11.53 & 47.20 & 24.40 & 15.16 & 50.59 \\
      &SIG & 19.64 & 1.63 & 66.43 & 38.14 & 2.13 & 59.04 & 44.21 & -1.42 & 66.73 \\
      &DecompX & 50.00 & -0.84 & \textbf{90.38} & 49.24 & 0.65 & 72.80 & 46.94 & -1.42 & 70.16 \\
      &ShapSampl & 35.63 & -0.68 & 78.31 & 43.32 & 1.83 & 65.85 & 44.83 & -1.30 & 67.15 \\
      &LIME & 19.28 & 2.85 & 66.73 & 21.07 & 8.32 & 50.81 & 27.97 & 11.38 & 58.59 \\
      %\cmidrule(lr){2-11}
%&B-cos & \textbf{62.47} & \textbf{-1.18} & 86.63 & \textbf{75.15} & \textbf{-2.39} & \textbf{75.83} & \textbf{51.33} & \textbf{-5.18} & \textbf{74.01} \\
    \midrule
  




\multirow{6}{*}{(b) B-cos RoBERTa}&Attention & 16.07 & 6.83 & 50 & 29.83 & 2.85 & 50 & 27.35 & 8.39 & 50 \\
      &IxG & 22.25 & 2.39 & 56.15 & 67.2 & -2.26 & 56.95 & 40.69 & -1.11 & 58.59 \\
      &SIG & 44.35 & -0.95 & 51.70 & 74.70 & \textbf{-2.39} & 58.03 & 51.20 & \textbf{-5.80} & 57.62 \\
      &DecompX & - & - & - & - & - & - & - & - & - \\
      &ShapSampl & 55.26 & -0.92 & 72.65 & 74.3 & \textbf{-2.39} & 62.74 & \textbf{51.54} & -5.64 & 70.58 \\
      &LIME & 23.01 & 2.46 & 63.11 & 37.14 & 0.78 & 53.15 & 29.86 & 5.74 & 63.61 \\
      %\cmidrule(lr){2-11}
      \midrule
(c) B-cos RoBERTa & B-cos & \textbf{62.47} & \textbf{-1.18} & 86.63 & \textbf{75.15} & \textbf{-2.39} & \textbf{75.83} & 51.33 & -5.18 & \textbf{74.01} \\
    \bottomrule
  \end{tabular}
  }
  \caption{Faithfulness evaluation for conventionally fine-tuned RoBERTa and B-cos RoBERTa across three datasets. 
  %We highlight the best results in \textbf{bold} and repeat the B-cos explanations results for easy comparison.
  The best results are in \textbf{bold}.
  We find that B-cos explanations are consistently more faithful than post-hoc explanations from both models.}
  \label{tab:faithfulness roberta}
\end{table*}

\section{Human Evaluation Details}
\label{appendix:human evaluation}

In the human study, we select only examples shorter than 25 tokens for HateXplain and 40 tokens for AG News to improve visualization. Additionally, we replace [CLS] and [SEP] with \#\# to make the examples more understandable for lay users. Below, we provide the instructions along with a detailed description of the criteria and scoring used in our human evaluation.


\begin{displayquote}
\noindent \textbf{WARNING: SOME CONTENT IN THIS QUESTIONNAIRE IS HIGHLY OFFENSIVE.}

\noindent \textbf{Prerequisites:} Proficiency in English is required for this evaluation task. If you do not meet this criterion, please do not proceed.

\noindent We invite you to review 50 examples where NLP models perform classification tasks and provide explanations for their predictions.
\begin{itemize}
    \item The first 25 examples come from a hate speech detection task, where the model predicts whether a text is toxic or not toxic.  
    \item The last 25 examples come from a topic classification task, where the model categorizes a text into one of four topics: sports, world, business, or sci/tech. 
\end{itemize}

\noindent For each example:
\begin{itemize}
    \item The model’s prediction is shown along with four explanations justifying the prediction.
    \item The order of the explanations is randomized to prevent bias.
    \item Words highlighted in green indicate words that had a positive influence on the prediction, while words in red indicate words that had a negative influence. The intensity of the color reflects the strength of the impact.
    \item \textbf{Important:} The model’s prediction may be incorrect. Your task is to evaluate the explanations based on how well they support the model’s prediction, not the true labels. 
\end{itemize}

\noindent Evaluation Task:

\noindent After reviewing each example, please rate the the \textbf{human interpretability} and \textbf{human agreement} of the four explanations on a scale of 1 to 5. Refer to the definitions and rating scales provided below when making your assessments.

\paragraph{Human Interpretability:} How easily a person can \textbf{understand the model’s reasoning} based on the explanation. A highly interpretable explanation should be clear and easy to follow, focus on relevant words and avoid unnecessary or distracting details.
\begin{enumerate}
    \item \textbf{Not Interpretable:} The explanation is unclear, noisy, or provides no meaningful insight.
    \item \textbf{Slightly Interpretable:} Some clues are present, but the explanation is too sparse, irrelevant, or confusing.
    \item \textbf{Moderately Interpretable:} The explanation contains useful information but is cluttered with noise or irrelevant details.
    \item \textbf{Highly Interpretable:} The explanation is mostly clear, with minimal irrelevant highlights.
    \item \textbf{Completely Interpretable:} The explanation is fully transparent, highlighting only the most relevant words, making the model’s reasoning fully clear.
\end{enumerate}

\paragraph{Human Agreement:} How closely the model’s explanation \textbf{aligns with the reasoning a human would use} for the same prediction. A high-agreement explanation should follow logical, intuitive reasoning and align with typical human decision-making patterns.
\begin{enumerate}
    \item \textbf{No Agreement:} The explanation contradicts human reasoning or lacks logic.
    \item \textbf{Low Agreement:} The explanation bears some resemblance to human reasoning but includes major inconsistencies.
    \item \textbf{Moderate Agreement:} The explanation partially aligns with human reasoning, yet contains notable differences.
    \item \textbf{High Agreement:} The explanation largely aligns with human reasoning, showing only minor discrepancies.
    \item \textbf{Complete Agreement:} The explanation fully matches human reasoning, following a logical and intuitive path that a human would naturally use.
\end{enumerate}
\end{displayquote}

We also provide participants with examples to illustrate the reasoning behind rating explanations.
One such example is shown in Figure~\ref{fig:rationale}.
Additionally, Figure~\ref{fig:interface1} presents an example of a model prediction and its explanations as displayed to participants during the study.
%The user interface is provided in Figures \ref{fig:interface1}, \ref{fig:interface2}, and \ref{fig:interface3}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{user_interface.png}
    \caption{An example shown to participants that demonstrates how to rate explanations.}
    \label{fig:rationale}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{interface1.png}
    \caption{ An examples of a model prediction and its explanations presented to participants.}
    \label{fig:interface1}
\end{figure}

\iffalse
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{interface2.png}
    \caption{Instructions to rate human interpretability of each explanation.}
    \label{fig:interface2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{interface3.png}
    \caption{Instructions to rate human agreement of each explanation.}
    \label{fig:interface3}
\end{figure}
\fi

\section{More Examples of B-cos Explanations}
\label{appendix:more examples}

We provide two more examples of B-cos and other (post-hoc) explanations from AG News in Figure~\ref{fig:more examples}. Consistent with our findings in \S~\ref{sec:qualitative}, B-cos LMs provide more human interpretable explanations.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{more_examples.png}
    \caption{More examples of B-cos explanations (B-cos BERT) as well as ShapSampl and DecompX explanations (BERT) from the AG News dataset.
    \textcolor{forestgreen}{Green} (\textcolor{red}{red}) indicates the \textcolor{forestgreen}{positive} (\textcolor{red}{negative}) impact of tokens on the prediction. 
    As can be seen, the B-cos explanation highlights only relevant tokens and is more interpretable to humans.}
    \label{fig:more examples}
\end{figure*}

\section{Impact of B on Input-weight Alignment}
\label{appendix:alignment}
%\paragraph{Input-weight alignment}

To analyze how B-cosification and alignment pressure influence the behavior of B-cos LMs, we compute the alignment (cosine similarity) between each input and its corresponding weight in B-cos modules across all layers. This analysis is performed on 100 examples from the HateXplain dataset. In Figure~\ref{fig:alignment}, we plot different percentiles of input-weight alignment for conventional and B-cos BERT models with varying B values. For better visualization, we display only the 10th to 90th percentiles.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{alignment_distribution.png}
    \caption{Percentiles of input-weight alignment in B-cos modules across selected layers of conventional and B-cos BERT models with different B values (HateXplain).}
    \label{fig:alignment}
\end{figure}

Overall, larger B values generally lead to stronger input-weight alignment compared to smaller B and conventional models, as evidenced by the curves for B=1.5 and B=2.5 lying above those for the conventional model and B=1. However, the alignment pattern becomes more complex when comparing B=1.5 and B=2.5. Specifically, at B=2.5, the most aligned input-weight pairs exhibit higher alignment than in other models, but some pairs show very low alignment. This result may arise because certain weights are highly optimized for specific input patterns, leading to poor alignment with others, particularly in later layers where input features become more anisotropic \citep{ethayarajh-2019-contextual, li-etal-2020-sentence}. As a result, some outputs from the B-cos layers are highly negative. When these outputs are fed into GELU activation functions, their dynamic weights approach zero, making the explanations more sparse.



\section{Effects of B on Other Metrics}
\label{appendix:different b all metrics}
Table~\ref{tab:b values} presents the complete results on how B values affect task performance, explanation faithfulness and explanation entropy, as shown in Figure~\ref{fig:b analysis}. 
Similar to Comp, SeqPG scores also decline with higher alignment pressure. 
This could also be attributed to the high sparsity of explanations. 
As B increases, fewer tokens receive attribution scores that are not close to zero, and in some SeqPG examples, B-cos LMs may attribute predictions to a single segment. This can lead to numerical instability when computing the positive attribution ratio.

\begin{table}[h]
    \centering
    \begin{small}
        

    \resizebox{\linewidth}{!}{
        \tabcolsep=2pt
    \begin{tabular}{lrrrrrrr}
    \toprule
      \textbf{B}   & 1.00 & 1.25 & 1.50 & 1.75 & 2.00 & 2.25 & 2.50 \\
      \midrule
      Acc ($\uparrow$) & 78.57 & \textbf{79.23} & 78.10 & 77.41 & 77.48 & 70.44 & 73.55 \\
      Comp ($\uparrow$) & 55.09 & 58.99 & \textbf{59.64} & 59.23 & 54.44 & 35.80 & 27.11 \\
      Suff ($\downarrow$) & -4.25 & -5.71 & -5.47 & -5.84 & -6.69 & \textbf{-7.23} & -5.47 \\
      SeqPG ($\uparrow$) & 69.75 & 77.26 & \textbf{77.79} & 77.67 & 76.79 & 76.68 & 77.25 \\
      Entropy & 3.09 & 2.79 & 2.58 & 2.35 & 2.28 & 1.98 & 1.89 \\
      \bottomrule
    \end{tabular}
    }
    \end{small}
    \caption{Task performance, explanation faithfulness, and explanation entropy of B-cos BERT models on HateXplain with different B values. Results are averaged over three runs. Similar to Figure~\ref{fig:b analysis}, task performance and explanation faithfulness peak around B=1.5, while explanation entropy negatively correlates with B.}
    \label{tab:b values}
\end{table}


\section{B-cos Explanations with Different B Values}
\label{appendix:examples different b}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{different_b_values.png}
    \caption{B-cos explanations (B-cos BERT) on AG News with different B values. \textcolor{forestgreen}{Green} (\textcolor{red}{red}) indicates the \textcolor{forestgreen}{positive} (\textcolor{red}{negative}) impact of tokens on the prediction. As B increases, B-cos LMs produce sparser explanations, with fewer tokens receiving significant attribution scores.}
    \label{fig:explanation different b}
\end{figure*}

Figure~\ref{fig:explanation different b} illustrates that with increased alignment pressure, B-cos LMs learn fewer but more task-relevant features. Consequently, they produce sparser explanations, with fewer tokens receiving significant attribution. This finding aligns with the statistics presented in \S~\ref{sec:B}.


\section{Example of Model Bias}
\label{appendix:bias}

In the example shown in Figure~\ref{fig:word bias}, models become increasingly confident in the incorrect prediction as B increases, with attributions primarily assigned to the word ``blacks''. Moreover, simply replacing ``blacks'' with ``whites'' results in a sharp drop in confidence, which demonstrates a growing reliance on spurious correlations with increased alignment pressure. The observation further confirms our findings in \S\ref{sec:B}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{bias.png}
    \caption{Example of how larger B values lead B-cos LMs to learn word-level spurious correlations. \textcolor{forestgreen}{Green} (\textcolor{red}{red}) indicates the \textcolor{forestgreen}{positive} (\textcolor{red}{negative}) impact of tokens on the prediction. Higher alignment pressure increases the reliance of B-cos LMs on spurious correlations in the data. In this example, perturbation involves changing ``blacks'' to ``whites''.}
    \label{fig:word bias}
\end{figure*}


\end{document}
