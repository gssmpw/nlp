\section{Related Work}
\paragraph{Post-hoc Explanation Methods} 
Various methods have been proposed to provide post-hoc explanations for neural model predictions~\citep{atanasova-etal-2020-diagnostic}. 
These methods can be broadly categorized based on how they generate explanations: gradient-based~\citep{DBLP:journals/corr/SimonyanVZ13, DBLP:journals/corr/KindermansSMD16, Sundararajan-2017-IntegratedGrad}, propagation-based~\citep{bach2015pixel, shrikumar2017learning, DBLP:journals/corr/SpringenbergDBR14}, and perturbation-based methods~\citep{DBLP:journals/corr/LiMJ16a, Ribeiro-2016-LIME, Lundberg-2017-SHAP}. 
Besides, the attention mechanism~\citep{DBLP:journals/corr/BahdanauCB14} is often viewed as an explanation, particularly in transformer-based models~\citep{DBLP:conf/nips/VaswaniSPUJGKP17}. 

Although post-hoc methods can be applied to generate explanations for existing models, numerous studies have shown that they lack faithfulness, often failing to capture the true decision-making process of the model~\citep{Kindermans-2019-Reliability, jain-wallace-2019-attention, Slack-2020-FoolingLimeSHAP, pruthi-etal-2020-learning}. 
%and exhibit low consistency across different techniques and instances~\citep{neely2022song, Dasgupta-2022-FaithfulnessEval}. 
Furthermore, they may generate noisy explanations that focus on irrelevant information, making them difficult for humans to interpret~\citep{Smilkov-2017-SmoothGrad, NEURIPS2021_e0cd3f16}. 

\paragraph{From Post-hoc Explanations to Explainable Models}

The limitations of post-hoc explanation methods may be attributed to the inherent lack of explainability in contemporary neural models, which are typically optimized solely for task performance~\citep{Kindermans-2018-PatternNet, rudin2019stop, atanasova2022diagnostics}. 
For instance, studies have shown that existing models struggle to provide faithful explanations~\citep{NEURIPS2018_3e9f0fc9} or tend to learn noisy patterns, resulting in less interpretable explanations~\citep{NEURIPS2021_e0cd3f16}. 

%and \cite{Kindermans-2018-PatternNet} showed that these models often fail to learn patterns recognizable to humans. In response, efforts have been made to enhance model explainability.

In response, various efforts have been made to enhance model explainability. 
Some work has introduced constraints that improve specific explanation properties, such as faithfulness~\citep{tutek2022toward, moradi-etal-2020-training, moradi-etal-2021-measuring}, consistency~\citep{atanasova2022diagnostics}, locality~\citep{NEURIPS2018_3e9f0fc9}, and plausibility~\citep{NEURIPS2021_e0cd3f16}. 
However, as these constraints are typically imposed as regularizers, their effectiveness in improving explanation quality is not guaranteed~\citep{pruthi-etal-2020-learning}. 
Others have proposed self-explanatory model architectures such as rationale-based models that utilize an ``explain-then-predict'' pipeline, where one module selects rationales for another to make predictions based on them~\citep{lei-etal-2016-rationalizing}.
Although seemingly transparent, both components rely on neural networks, making the rationale extraction and utilization processes opaque~\citep{zheng-etal-2022-irrationality, jacovi-goldberg-2021-aligning}. 
Besides, such models may face optimization challenges that limit their practicality in real-world tasks~\citep{lyu-etal-2024-towards}.
\looseness=-1

To tackle these shortcomings, \citet{Bohle_2022_CVPR} proposed B-cos networks. 
Unlike methods that impose external constraints, B-cos networks improve explainability through mathematically grounded architectural and computational adaptations. 
Moreover, these adaptations are designed as drop-in replacements for conventional model components, making B-cos networks easy to train with minimal performance loss. Most recently, \citet{arya24bcosification} explored \textit{B-cosification} techniques to convert existing models into B-cos models, which reduces the training costs of adopting B-cos architectures. 

Despite their successful application in vision tasks, B-cos networks have yet to be explored in NLP, where input modalities and training paradigms differ significantly. 
In this work, we adapt B-cos models for the language domain, integrating them efficiently into NLP pipelines. 

%Specifically, we propose a direct transformation of PLMs into task-specific B-cos LMs, which substantially improves training efficiency compared to previous B-cos methods~\citep{arya24bcosification} and achieves faster convergence than conventional fine-tuning. In addition, our in-depth analysis reveals how B-cos LMs behave differently from conventional models under varying alignment pressures, offering crucial insights into their behavior and guiding future improvements in their design.
\begin{table*}[ht]
    \centering
    \resizebox{\textwidth}{!}{
    \tabcolsep=2pt
    \begin{tabular}{lC{5cm}C{6cm}C{6cm}}
        \toprule
        \textbf{Property} & \textbf{Conventional Fine-tuning} & \textbf{B-cosification}~\citep{arya24bcosification} & \textbf{B-cos LM (ours)} \\
        \midrule
        %\textbf{Modality} & NLP (text) & CV (image) & NLP (text) \\
        %\textbf{Input} & sequential word embeddings & image embeddings & sequential word embeddings \\
        %\midrule
        %\midrule
        \textbf{Bias terms} & yes & no & no \\
        \textbf{B (alignment pressure)} & 1 & 2 & 1.5 \\
        \textbf{Pred. Head Activations} & tanh  & n/a\footnotemark & identity \\
        %\bottomrule
        \midrule
       \textbf{Prior task abilities} & no & yes  & no  \\
        \textbf{Training objectives} & Task fine-tuning & B-cos conversion & Task fine-tuning \& B-cos conversion \\
        \bottomrule
    \end{tabular}
    }
    \caption{Comparison between conventional fine-tuning, B-cosification in computer vision and B-cosification in NLP (B-cos LM). 
    Conventional fine-tuning and B-cosification follow the configuration of BERT for sequence classification and CLIP~\citep{radford2021learning}, respectively (cf. \S~\ref{sec:methodology} for details).}
    \label{tab:comparison}
\end{table*}

\footnotetext{\citet{arya24bcosification} used a single linear layer on top of CLIP so the prediction head activation is not applicable in their setup.}