% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{lyu-etal-2024-towards,
    title = "Towards Faithful Model Explanation in {NLP}: A Survey",
    author = "Lyu, Qing  and
      Apidianaki, Marianna  and
      Callison-Burch, Chris",
    journal = "Computational Linguistics",
    volume = "50",
    number = "2",
    month = jun,
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.cl-2.6/",
    doi = "10.1162/coli_a_00511",
    pages = "657--723",
    abstract = "End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, that is, an explanation should accurately represent the reasoning process behind the model's prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP."
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@misc{Eval-2023-Harness,
	author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
	title        = {A framework for few-shot language model evaluation},
	month        = 12,
	year         = 2023,
	publisher    = {Zenodo},
	version      = {v0.4.0},
	doi          = {10.5281/zenodo.10256836},
	url          = {https://zenodo.org/records/10256836}
}


@inproceedings{abnar-zuidema-2020-quantifying,
    title = "Quantifying Attention Flow in Transformers",
    author = "Abnar, Samira  and
      Zuidema, Willem",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.385/",
    doi = "10.18653/v1/2020.acl-main.385",
    pages = "4190--4197",
    abstract = "In the Transformer model, ``self-attention'' combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients."
}


@InProceedings{Sundararajan-2017-IntegratedGrad,
  title = 	 {Axiomatic Attribution for Deep Networks},
  author =       {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3319--3328},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/sundararajan17a.html},
}

@inproceedings{DBLP:journals/corr/BahdanauCB14,
  author       = {Dzmitry Bahdanau and
                  Kyunghyun Cho and
                  Yoshua Bengio},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1409.0473},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ribeiro-2016-LIME,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@article{shapley1953value,
  title={A value for n-person games},
  author={Shapley, Lloyd S},
  journal={Contribution to the Theory of Games},
  volume={2},
  year={1953}
}

@inproceedings{DBLP:journals/corr/SimonyanVZ13,
  author       = {Karen Simonyan and
                  Andrea Vedaldi and
                  Andrew Zisserman},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Deep Inside Convolutional Networks: Visualising Image Classification
                  Models and Saliency Maps},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings},
  year         = {2014},
  url          = {http://arxiv.org/abs/1312.6034},
  timestamp    = {Thu, 25 Jul 2019 14:36:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SimonyanVZ13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/KindermansSMD16,
  author       = {Pieter{-}Jan Kindermans and
                  Kristof Sch{\"{u}}tt and
                  Klaus{-}Robert M{\"{u}}ller and
                  Sven D{\"{a}}hne},
  title        = {Investigating the influence of noise and distractors on the interpretation
                  of neural networks},
  journal      = {CoRR},
  volume       = {abs/1611.07270},
  year         = {2016},
  url          = {http://arxiv.org/abs/1611.07270},
  eprinttype    = {arXiv},
  eprint       = {1611.07270},
  timestamp    = {Mon, 13 Aug 2018 16:48:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KindermansSMD16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bach2015pixel,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{shrikumar2017learning,
  title={Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={International conference on machine learning},
  pages={3145--3153},
  year={2017},
  organization={PMlR}
}

@inproceedings{DBLP:conf/eccv/ZeilerF14,
  author       = {Matthew D. Zeiler and
                  Rob Fergus},
  editor       = {David J. Fleet and
                  Tom{\'{a}}s Pajdla and
                  Bernt Schiele and
                  Tinne Tuytelaars},
  title        = {Visualizing and Understanding Convolutional Networks},
  booktitle    = {Computer Vision - {ECCV} 2014 - 13th European Conference, Zurich,
                  Switzerland, September 6-12, 2014, Proceedings, Part {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {8689},
  pages        = {818--833},
  publisher    = {Springer},
  year         = {2014},
  url          = {https://doi.org/10.1007/978-3-319-10590-1\_53},
  doi          = {10.1007/978-3-319-10590-1\_53},
  timestamp    = {Sat, 30 Sep 2023 09:39:19 +0200},
  biburl       = {https://dblp.org/rec/conf/eccv/ZeilerF14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/SpringenbergDBR14,
  author       = {Jost Tobias Springenberg and
                  Alexey Dosovitskiy and
                  Thomas Brox and
                  Martin A. Riedmiller},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Striving for Simplicity: The All Convolutional Net},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6806},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SpringenbergDBR14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{atanasova-etal-2020-diagnostic,
    title = "A Diagnostic Study of Explainability Techniques for Text Classification",
    author = "Atanasova, Pepa  and
      Simonsen, Jakob Grue  and
      Lioma, Christina  and
      Augenstein, Isabelle",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.263/",
    doi = "10.18653/v1/2020.emnlp-main.263",
    pages = "3256--3274",
    abstract = "Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques."
}

@article{bonferroni1936teoria,
  title={Teoria statistica delle classi e calcolo delle probabilita},
  author={Bonferroni, Carlo},
  journal={Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commericiali di Firenze},
  volume={8},
  pages={3--62},
  year={1936},
  doi = "http://dx.doi.org/10.4135/9781412961288.n455"
}

@inproceedings{arras-etal-2019-evaluating,
    title = "Evaluating Recurrent Neural Network Explanations",
    author = {Arras, Leila  and
      Osman, Ahmed  and
      M\"uller, Klaus-Robert  and
      Samek, Wojciech},
    editor = "Linzen, Tal  and
      Chrupa\l a, Grzegorz  and
      Belinkov, Yonatan  and
      Hupkes, Dieuwke",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4813/",
    doi = "10.18653/v1/W19-4813",
    pages = "113--126",
    abstract = "Recently, several methods have been proposed to explain the predictions of recurrent neural networks (RNNs), in particular of LSTMs. The goal of these methods is to understand the network's decisions by assigning to each input variable, e.g., a word, a relevance indicating to which extent it contributed to a particular prediction. In previous works, some of these methods were not yet compared to one another, or were evaluated only qualitatively. We close this gap by systematically and quantitatively comparing these methods in different settings, namely (1) a toy arithmetic task which we use as a sanity check, (2) a five-class sentiment prediction of movie reviews, and besides (3) we explore the usefulness of word relevances to build sentence-level representations. Lastly, using the method that performed best in our experiments, we show how specific linguistic phenomena such as the negation in sentiment analysis reflect in terms of relevance patterns, and how the relevance visualization can help to understand the misclassification of individual samples."
}

@article{castro2009polynomial,
  title={Polynomial calculation of the Shapley value based on sampling},
  author={Castro, Javier and G{\'o}mez, Daniel and Tejada, Juan},
  journal={Computers \& operations research},
  volume={36},
  number={5},
  pages={1726--1730},
  year={2009},
  publisher={Elsevier}
}

@article{strumbelj2010efficient,
  title={An efficient explanation of individual classifications using game theory},
  author={Strumbelj, Erik and Kononenko, Igor},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={1--18},
  year={2010},
  publisher={JMLR. org}
}


@inproceedings{pruthi-etal-2020-learning,
    title = "Learning to Deceive with Attention-Based Explanations",
    author = "Pruthi, Danish  and
      Gupta, Mansi  and
      Dhingra, Bhuwan  and
      Neubig, Graham  and
      Lipton, Zachary C.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.432/",
    doi = "10.18653/v1/2020.acl-main.432",
    pages = "4782--4793",
}

@inproceedings{Slack-2020-FoolingLimeSHAP,
  title={Fooling lime and shap: Adversarial attacks on post hoc explanation methods},
  author={Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={180--186},
  year={2020}
}

@inproceedings{jain-wallace-2019-attention,
    title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1357/",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
    abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful ``explanations'' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do."
}

@inproceedings{wiegreffe-pinter-2019-attention,
    title = "Attention is not not Explanation",
    author = "Wiegreffe, Sarah  and
      Pinter, Yuval",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1002/",
    doi = "10.18653/v1/D19-1002",
    pages = "11--20",
    abstract = "Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability."
}

@inproceedings{ferrando-etal-2022-measuring,
    title = "Measuring the Mixing of Contextual Information in the Transformer",
    author = "Ferrando, Javier  and
      G\'allego, Gerard I.  and
      Costa-juss\`a, Marta R.",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.595/",
    doi = "10.18653/v1/2022.emnlp-main.595",
    pages = "8698--8714",
    abstract = "The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block --multi-head attention, residual connection, and layer normalization-- and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods."
}

@inproceedings{serrano-smith-2019-attention,
    title = "Is Attention Interpretable?",
    author = "Serrano, Sofia  and
      Smith, Noah A.",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M\`arquez, Llu\'\i s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1282/",
    doi = "10.18653/v1/P19-1282",
    pages = "2931--2951",
    abstract = "Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator."
}

@inproceedings{wang-etal-2020-gradient,
    title = "Gradient-based Analysis of {NLP} Models is Manipulable",
    author = "Wang, Junlin  and
      Tuyls, Jens  and
      Wallace, Eric  and
      Singh, Sameer",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.24/",
    doi = "10.18653/v1/2020.findings-emnlp.24",
    pages = "247--258",
    abstract = "Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, the fact that they directly reflect the model internals. In this paper, however, we demonstrate that the gradients of a model are easily manipulable, and thus bring into question the reliability of gradient-based analyses. In particular, we merge the layers of a target model with a Facade Model that overwhelms the gradients without affecting the predictions. This Facade Model can be trained to have gradients that are misleading and irrelevant to the task, such as focusing only on the stop words in the input. On a variety of NLP tasks (sentiment analysis, NLI, and QA), we show that the merged model effectively fools different analysis tools: saliency maps differ significantly from the original model's, input reduction keeps more irrelevant input tokens, and adversarial perturbations identify unimportant tokens as being highly important."
}

@inproceedings{DBLP:conf/icml/LopardoPG24,
  author       = {Gianluigi Lopardo and
                  Fr{\'{e}}d{\'{e}}ric Precioso and
                  Damien Garreau},
  title        = {Attention Meets Post-hoc Interpretability: {A} Mathematical Perspective},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=wnkC5T11Z9},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/LopardoPG24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Kindermans-2019-Reliability,
  title={The (un) reliability of saliency methods},
  author={Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"u}tt, Kristof T and D{\"a}hne, Sven and Erhan, Dumitru and Kim, Been},
  journal={Explainable AI: Interpreting, explaining and visualizing deep learning},
  pages={267--280},
  year={2019},
  publisher={Springer}
}

@article{DBLP:journals/corr/abs-2105-03287,
  author       = {Michael Neely and
                  Stefan F. Schouten and
                  Maurits J. R. Bleeker and
                  Ana Lucic},
  title        = {Order in the Court: Explainable {AI} Methods Prone to Disagreement},
  journal      = {CoRR},
  volume       = {abs/2105.03287},
  year         = {2021},
  url          = {https://arxiv.org/abs/2105.03287},
  eprinttype    = {arXiv},
  eprint       = {2105.03287},
  timestamp    = {Sun, 02 Oct 2022 15:32:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2105-03287.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@incollection{neely2022song,
  title={A song of (dis) agreement: Evaluating the evaluation of explainable artificial intelligence in natural language processing},
  author={Neely, Michael and Schouten, Stefan F and Bleeker, Maurits and Lucic, Ana},
  booktitle={HHAI2022: Augmenting Human Intellect},
  pages={60--78},
  year={2022},
  publisher={IOS Press}
}

@inproceedings{zheng-etal-2022-irrationality,
    title = "The Irrationality of Neural Rationale Models",
    author = "Zheng, Yiming  and
      Booth, Serena  and
      Shah, Julie  and
      Zhou, Yilun",
    editor = "Verma, Apurv  and
      Pruksachatkun, Yada  and
      Chang, Kai-Wei  and
      Galstyan, Aram  and
      Dhamala, Jwala  and
      Cao, Yang Trista",
    booktitle = "Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022)",
    month = jul,
    year = "2022",
    address = "Seattle, U.S.A.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.trustnlp-1.6/",
    doi = "10.18653/v1/2022.trustnlp-1.6",
    pages = "64--73",
    abstract = "Neural rationale models are popular for interpretable predictions of NLP tasks. In these, a selector extracts segments of the input text, called rationales, and passes these segments to a classifier for prediction. Since the rationale is the only information accessible to the classifier, it is plausibly defined as the explanation. Is such a characterization unconditionally correct? In this paper, we argue to the contrary, with both philosophical perspectives and empirical evidence suggesting that rationale models are, perhaps, less rational and interpretable than expected. We call for more rigorous evaluations of these models to ensure desired properties of interpretability are indeed achieved. The code for our experiments is at \url{https://github.com/yimingz89/Neural-Rationale-Analysis}."
}

@inproceedings{bibal-etal-2022-attention,
    title = "Is Attention Explanation? An Introduction to the Debate",
    author = "Bibal, Adrien  and
      Cardon, R{\'e}mi  and
      Alfter, David  and
      Wilkens, Rodrigo  and
      Wang, Xiaoou  and
      Fran{\c{c}}ois, Thomas  and
      Watrin, Patrick",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.269/",
    doi = "10.18653/v1/2022.acl-long.269",
    pages = "3889--3900",
    abstract = "The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation."
}


@inproceedings{bastings-filippova-2020-elephant,
    title = "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?",
    author = "Bastings, Jasmijn  and
      Filippova, Katja",
    editor = "Alishahi, Afra  and
      Belinkov, Yonatan  and
      Chrupa\l a, Grzegorz  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.14/",
    doi = "10.18653/v1/2020.blackboxnlp-1.14",
    pages = "149--155",
    abstract = "There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations."
}

@inproceedings{DBLP:conf/nips/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Attention is All you Need},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {5998--6008},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/BrunnerLPRCW20,
  author       = {Gino Brunner and
                  Yang Liu and
                  Damian Pascual and
                  Oliver Richter and
                  Massimiliano Ciaramita and
                  Roger Wattenhofer},
  title        = {On Identifiability in Transformers},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=BJg1f6EFDB},
  timestamp    = {Thu, 07 May 2020 17:11:47 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/BrunnerLPRCW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{tutek2022toward,
  title={Toward practical usage of the attention mechanism as a tool for interpretability},
  author={Tutek, Martin and {\v{S}}najder, Jan},
  journal={IEEE access},
  volume={10},
  pages={47011--47030},
  year={2022},
  publisher={IEEE}
}

@inproceedings{atanasova2022diagnostics,
  title={Diagnostics-guided explanation generation},
  author={Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={10445--10453},
  year={2022}
}

@inproceedings{moradi-etal-2020-training,
    title = "Training with Adversaries to Improve Faithfulness of Attention in Neural Machine Translation",
    author = "Moradi, Pooya  and
      Kambhatla, Nishant  and
      Sarkar, Anoop",
    editor = "Shmueli, Boaz  and
      Huang, Yin Jou",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-srw.14/",
    doi = "10.18653/v1/2020.aacl-srw.14",
    pages = "93--100",
    abstract = "Can we trust that the attention heatmaps produced by a neural machine translation (NMT) model reflect its true internal reasoning? We isolate and examine in detail the notion of faithfulness in NMT models. We provide a measure of faithfulness for NMT based on a variety of stress tests where model parameters are perturbed and measuring faithfulness based on how often the model output changes. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and it also seems to have a useful regularization effect on the NMT model and can even improve translation quality in some cases."
}

@inproceedings{moradi-etal-2021-measuring,
    title = "Measuring and Improving Faithfulness of Attention in Neural Machine Translation",
    author = "Moradi, Pooya  and
      Kambhatla, Nishant  and
      Sarkar, Anoop",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.243/",
    doi = "10.18653/v1/2021.eacl-main.243",
    pages = "2791--2802",
    abstract = "While the attention heatmaps produced by neural machine translation (NMT) models seem insightful, there is little evidence that they reflect a model's true internal reasoning. We provide a measure of faithfulness for NMT based on a variety of stress tests where attention weights which are crucial for prediction are perturbed and the model should alter its predictions if the learned weights are a faithful explanation of the predictions. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and has a useful regularization effect on the NMT model and can even improve translation quality in some cases."
}

@inproceedings{NEURIPS2021_e0cd3f16,
 author = {Ismail, Aya Abdelsalam and Corrada Bravo, Hector and Feizi, Soheil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26726--26739},
 publisher = {Curran Associates, Inc.},
 title = {Improving Deep Learning Interpretability by Saliency Guided Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e0cd3f16f9e883ca91c2a4c24f47b3d9-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{NEURIPS2018_3e9f0fc9,
 author = {Alvarez Melis, David and Jaakkola, Tommi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{lei-etal-2016-rationalizing,
    title = "Rationalizing Neural Predictions",
    author = "Lei, Tao  and
      Barzilay, Regina  and
      Jaakkola, Tommi",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1011/",
    doi = "10.18653/v1/D16-1011",
    pages = "107--117"
}

@InProceedings{Bohle_2022_CVPR,
    author    = {B\"ohle, Moritz and Fritz, Mario and Schiele, Bernt},
    title     = {B-Cos Networks: Alignment Is All We Need for Interpretability},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10329-10338}
}

@article{bohle2024b,
  title={B-cos Alignment for Inherently Interpretable CNNs and Vision Transformers},
  author={B{\"o}hle, Moritz and Singh, Navdeeppal and Fritz, Mario and Schiele, Bernt},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{DBLP:journals/corr/abs-2411-00715,
  author       = {Shreyash Arya and
                  Sukrut Rao and
                  Moritz B{\"{o}}hle and
                  Bernt Schiele},
  title        = {B-cosification: Transforming Deep Neural Networks to be Inherently
                  Interpretable},
  journal      = {CoRR},
  volume       = {abs/2411.00715},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2411.00715},
  doi          = {10.48550/ARXIV.2411.00715},
  eprinttype    = {arXiv},
  eprint       = {2411.00715},
  timestamp    = {Wed, 11 Dec 2024 17:23:25 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2411-00715.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{modarressi-etal-2023-decompx,
    title = "{D}ecomp{X}: Explaining Transformers Decisions by Propagating Token Decomposition",
    author = "Modarressi, Ali  and
      Fayyaz, Mohsen  and
      Aghazadeh, Ehsan  and
      Yaghoobzadeh, Yadollah  and
      Pilehvar, Mohammad Taher",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.149/",
    doi = "10.18653/v1/2023.acl-long.149",
    pages = "2649--2664",
    abstract = "An emerging solution for explaining Transformer-based models is to use vector-based analysis on how the representations are formed. However, providing a faithful vector-based explanation for a multi-layer model could be challenging in three aspects: (1) Incorporating all components into the analysis, (2) Aggregating the layer dynamics to determine the information flow and mixture throughout the entire model, and (3) Identifying the connection between the vector-based analysis and the model's predictions. In this paper, we present DecompX to tackle these challenges. DecompX is based on the construction of decomposed token representations and their successive propagation throughout the model without mixing them in between layers. Additionally, our proposal provides multiple advantages over existing solutions for its inclusion of all encoder components (especially nonlinear feed-forward networks) and the classification head. The former allows acquiring precise vectors while the latter transforms the decomposition into meaningful prediction-based values, eliminating the need for norm- or summation-based vector aggregation. According to the standard faithfulness evaluations, DecompX consistently outperforms existing gradient-based and vector-based approaches on various datasets. Our code is available at \url{https://github.com/mohsenfayyaz/DecompX}."
}

@inproceedings{deyoung-etal-2020-eraser,
    title = "{ERASER}: {A} Benchmark to Evaluate Rationalized {NLP} Models",
    author = "DeYoung, Jay  and
      Jain, Sarthak  and
      Rajani, Nazneen Fatema  and
      Lehman, Eric  and
      Xiong, Caiming  and
      Socher, Richard  and
      Wallace, Byron C.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.408/",
    doi = "10.18653/v1/2020.acl-main.408",
    pages = "4443--4458",
    abstract = "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the `reasoning' behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the \textbf{E}valuating \textbf{R}ationales \textbf{A}nd \textbf{S}imple \textbf{E}nglish \textbf{R}easoning (\textbf{ERASER} a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of ``rationales'' (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how \textit{faithful} these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at \url{https://www.eraserbenchmark.com/}"
}

@inproceedings{poerner-etal-2018-evaluating,
    title = "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement",
    author = {Poerner, Nina  and
      Sch\"utze, Hinrich  and
      Roth, Benjamin},
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1032/",
    doi = "10.18653/v1/P18-1032",
    pages = "340--350",
    abstract = "The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP."
}

@inproceedings{abnar-zuidema-2020-quantifying,
    title = "Quantifying Attention Flow in Transformers",
    author = "Abnar, Samira  and
      Zuidema, Willem",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.385/",
    doi = "10.18653/v1/2020.acl-main.385",
    pages = "4190--4197",
    abstract = "In the Transformer model, ``self-attention'' combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients."
}

@inproceedings{enguehard-2023-sequential,
    title = "Sequential Integrated Gradients: a simple but effective method for explaining language models",
    author = "Enguehard, Joseph",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.477/",
    doi = "10.18653/v1/2023.findings-acl.477",
    pages = "7555--7565",
    abstract = "Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. Moreover, inspired by the training procedure of language models, we also propose to replace the baseline token ``pad'' with the trained token ``mask''. While being a simple improvement over the original IG method, we show on various models and datasets that SIG proves to be a very effective method for explaining language models."
}

@inproceedings{jacovi-goldberg-2020-towards,
    title = "Towards Faithfully Interpretable {NLP} Systems: How Should We Define and Evaluate Faithfulness?",
    author = "Jacovi, Alon  and
      Goldberg, Yoav",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.386/",
    doi = "10.18653/v1/2020.acl-main.386",
    pages = "4198--4205",
    abstract = "With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is ``defined'' by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility."
}

@inproceedings{Lundberg-2017-SHAP,
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {1--10},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{zhang2018top,
  title={Top-down neural attention by excitation backprop},
  author={Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
  journal={International Journal of Computer Vision},
  volume={126},
  number={10},
  pages={1084--1102},
  year={2018},
  publisher={Springer}
}

@article{jacovi-goldberg-2021-aligning,
    title = "Aligning Faithful Interpretations with their Social Attribution",
    author = "Jacovi, Alon  and
      Goldberg, Yoav",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.18/",
    doi = "10.1162/tacl_a_00367",
    pages = "294--310",
    abstract = "We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations."
}


@InProceedings{Dasgupta-2022-FaithfulnessEval,
  title = 	 {Framework for Evaluating Faithfulness of Local Explanations},
  author =       {Dasgupta, Sanjoy and Frost, Nave and Moshkovitz, Michal},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {4794--4815},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/dasgupta22a/dasgupta22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/dasgupta22a.html},
}


@inproceedings{Devlin-2019-BERT,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {4171--4186},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1423},
  doi          = {10.18653/V1/N19-1423},
  timestamp    = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Radford-2019-GPT,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{Ji-2023-Hallucination,
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	title = {Survey of Hallucination in Natural Language Generation},
	year = {2023},
	issue_date = {December 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {55},
	number = {12},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3571730},
	doi = {10.1145/3571730},
	journal = {ACM Comput. Surv.},
	month = {mar},
	articleno = {248},
	numpages = {38},
	keywords = {extrinsic hallucination, consistency in NLG, Hallucination, intrinsic hallucination, faithfulness in NLG, factuality in NLG}
}

@article{Gallegos-2024-Bias,
    author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K.},
    title = {Bias and Fairness in Large Language Models: A Survey},
    journal = {Computational Linguistics},
    volume = {50},
    number = {3},
    pages = {1097-1179},
    year = {2024},
    month = {09},
    issn = {0891-2017},
    doi = {10.1162/coli_a_00524},
    url = {https://doi.org/10.1162/coli\_a\_00524},
    eprint = {https://direct.mit.edu/coli/article-pdf/50/3/1097/2471010/coli\_a\_00524.pdf},
}

@inproceedings{Meng-2023-Massediting,
title={Mass-Editing Memory in a Transformer},
author={Kevin Meng and Arnab Sen Sharma and Alex J Andonian and Yonatan Belinkov and David Bau},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=MkbcAHIYgyS}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{DBLP:journals/corr/abs-2303-08774,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.08774},
  doi          = {10.48550/ARXIV.2303.08774},
  eprinttype    = {arXiv},
  eprint       = {2303.08774},
  timestamp    = {Mon, 28 Aug 2023 21:26:19 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-08774.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jain-etal-2020-learning,
    title = "{L}earning to Faithfully Rationalize by Construction",
    author = "Jain, Sarthak  and
      Wiegreffe, Sarah  and
      Pinter, Yuval  and
      Wallace, Byron C.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.409/",
    doi = "10.18653/v1/2020.acl-main.409",
    pages = "4459--4473",
    abstract = "In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text `responsible for' corresponding model output; when such a snippet comprises tokens that indeed informed the model's prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to `end-to-end' approaches, while being more general and easier to train. Code is available at \url{https://github.com/successar/FRESH}."
}

@inproceedings{glockner-etal-2020-think,
    title = "Why do you think that? Exploring Faithful Sentence-Level Rationales Without Supervision",
    author = "Glockner, Max  and
      Habernal, Ivan  and
      Gurevych, Iryna",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.97/",
    doi = "10.18653/v1/2020.findings-emnlp.97",
    pages = "1080--1095",
    abstract = "Evaluating the trustworthiness of a model's prediction is essential for differentiating between `right for the right reasons' and `right for the wrong reasons'. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training--framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart's performance in two cases. We further exploit the transparent decision--making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale--level."
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa\l a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446/",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
}

@article{Bommasani-2021-FoundationModels,
  author       = {Rishi Bommasani and
                  Drew A. Hudson and
                  Ehsan Adeli and
                  Russ B. Altman and
                  Simran Arora and
                  Sydney von Arx and
                  Michael S. Bernstein and
                  Jeannette Bohg and
                  Antoine Bosselut and
                  Emma Brunskill and
                  Erik Brynjolfsson and
                  Shyamal Buch and
                  Dallas Card and
                  Rodrigo Castellon and
                  Niladri S. Chatterji and
                  Annie S. Chen and
                  Kathleen Creel and
                  Jared Quincy Davis and
                  Dorottya Demszky and
                  Chris Donahue and
                  Moussa Doumbouya and
                  Esin Durmus and
                  Stefano Ermon and
                  John Etchemendy and
                  Kawin Ethayarajh and
                  Li Fei{-}Fei and
                  Chelsea Finn and
                  Trevor Gale and
                  Lauren E. Gillespie and
                  Karan Goel and
                  Noah D. Goodman and
                  Shelby Grossman and
                  Neel Guha and
                  Tatsunori Hashimoto and
                  Peter Henderson and
                  John Hewitt and
                  Daniel E. Ho and
                  Jenny Hong and
                  Kyle Hsu and
                  Jing Huang and
                  Thomas Icard and
                  Saahil Jain and
                  Dan Jurafsky and
                  Pratyusha Kalluri and
                  Siddharth Karamcheti and
                  Geoff Keeling and
                  Fereshte Khani and
                  Omar Khattab and
                  Pang Wei Koh and
                  Mark S. Krass and
                  Ranjay Krishna and
                  Rohith Kuditipudi and
                  et al.},
  title        = {On the Opportunities and Risks of Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2108.07258},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.07258},
  eprinttype    = {arXiv},
  eprint       = {2108.07258},
  timestamp    = {Fri, 08 Nov 2024 20:52:57 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-07258.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1907-11692,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 14 Dec 2023 18:03:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1910-01108,
  author       = {Victor Sanh and
                  Lysandre Debut and
                  Julien Chaumond and
                  Thomas Wolf},
  title        = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
                  and lighter},
  journal      = {CoRR},
  volume       = {abs/1910.01108},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.01108},
  eprinttype    = {arXiv},
  eprint       = {1910.01108},
  timestamp    = {Tue, 02 Jun 2020 12:48:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/NairH10,
  author       = {Vinod Nair and
                  Geoffrey E. Hinton},
  editor       = {Johannes F{\"{u}}rnkranz and
                  Thorsten Joachims},
  title        = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle    = {Proceedings of the 27th International Conference on Machine Learning
                  (ICML-10), June 21-24, 2010, Haifa, Israel},
  pages        = {807--814},
  publisher    = {Omnipress},
  year         = {2010},
  url          = {https://icml.cc/Conferences/2010/papers/432.pdf},
  timestamp    = {Wed, 03 Apr 2019 17:43:37 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/NairH10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2019_80537a94,
 author = {Srinivas, Suraj and Fleuret, Fran\c{c}ois},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Full-Gradient Representation for Neural Network Visualization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/80537a945c7aaa788ccfcdf1b99b5d8f-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{bohle2021convolutional,
  title={Convolutional dynamic alignment networks for interpretable classifications},
  author={BÃ¶hle, Moritz and Fritz, Mario and Schiele, Bernt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10029--10038},
  year={2021}
}

@inproceedings{razzhigaev-etal-2024-transformer,
    title = "Your Transformer is Secretly Linear",
    author = "Razzhigaev, Anton  and
      Mikhalchuk, Matvey  and
      Goncharova, Elizaveta  and
      Gerasimenko, Nikolai  and
      Oseledets, Ivan  and
      Dimitrov, Denis  and
      Kuznetsov, Andrey",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.293/",
    doi = "10.18653/v1/2024.acl-long.293",
    pages = "5376--5384",
    abstract = "This paper reveals a novel linear characteristic exclusive to transformer decoders, including models like GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering an almost perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed, due to a consistently low transformer layer output norm. Our experiments show that pruning or linearly approximating some of the layers does not impact loss or model performance significantly. Moreover, we introduce a cosine-similarity-based regularization in our pretraining experiments on smaller models, aimed at reducing layer linearity. This regularization not only improves performance metrics on benchmarks like Tiny Stories and SuperGLUE but as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed."
}

@inproceedings{Kindermans-2018-PatternNet,
  author       = {Pieter{-}Jan Kindermans and
                  Kristof T. Sch{\"{u}}tt and
                  Maximilian Alber and
                  Klaus{-}Robert M{\"{u}}ller and
                  Dumitru Erhan and
                  Been Kim and
                  Sven D{\"{a}}hne},
  title        = {Learning how to explain neural networks: PatternNet and PatternAttribution},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=Hkn7CBaTW},
  timestamp    = {Thu, 25 Jul 2019 14:25:51 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/KindermansSAMEK18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v97-wang19p,
  title = 	 {Bias Also Matters: Bias Attribution for Deep Neural Network Explanation},
  author =       {Wang, Shengjie and Zhou, Tianyi and Bilmes, Jeff},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6659--6667},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wang19p/wang19p.pdf},
  url = 	 {https://proceedings.mlr.press/v97/wang19p.html},
  abstract = 	 {The gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features and has been widely studied to assist in interpreting DNNs. In a linear model (i.e., g(x) = wx + b), the gradient corresponds to the weights w. Such a model can reasonably locally-linearly approximate a smooth nonlinear DNN, and hence the weights of this local model are the gradient. The bias b, however, is usually overlooked in attribution methods. In this paper, we observe that since the bias in a DNN also has a non-negligible contribution to the correctness of predictions, it can also play a significant role in understanding DNN behavior. We propose a backpropagation-type algorithm âbias back-propagation (BBp)â that starts at the output layer and iteratively attributes the bias of each layer to its input nodes as well as combining the resulting bias term of the previous layer. Together with the backpropagation of the gradient generating w, we can fully recover the locally linear model g(x) = wx + b. In experiments, we show that BBp can generate complementary and highly interpretable explanations.}
}


@inproceedings{NIPS2015_250cf8b5,
 author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Character-level Convolutional Networks for Text Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
 volume = {28},
 year = {2015}
}

@inproceedings{maas-etal-2011-learning,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    editor = "Lin, Dekang  and
      Matsumoto, Yuji  and
      Mihalcea, Rada",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1015/",
    pages = "142--150"
}

@inproceedings{mathew2021hatexplain,
  title={Hatexplain: A benchmark dataset for explainable hate speech detection},
  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  pages={14867--14875},
  year={2021}
}


@inproceedings{NIPS2017_8a20a862,
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{ethayarajh-2019-contextual,
    title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
    author = "Ethayarajh, Kawin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1006/",
    doi = "10.18653/v1/D19-1006",
    pages = "55--65",
    abstract = "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5{\%} of the variance in a word`s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations."
}

@inproceedings{li-etal-2020-sentence,
    title = "On the Sentence Embeddings from Pre-trained Language Models",
    author = "Li, Bohan  and
      Zhou, Hao  and
      He, Junxian  and
      Wang, Mingxuan  and
      Yang, Yiming  and
      Li, Lei",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.733/",
    doi = "10.18653/v1/2020.emnlp-main.733",
    pages = "9119--9130",
    abstract = "Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at \url{https://github.com/bohanli/BERT-flow}."
}


@article{DBLP:journals/corr/HendrycksG16,
  author       = {Dan Hendrycks and
                  Kevin Gimpel},
  title        = {Bridging Nonlinearities and Stochastic Regularizers with Gaussian
                  Error Linear Units},
  journal      = {CoRR},
  volume       = {abs/1606.08415},
  year         = {2016},
  url          = {http://arxiv.org/abs/1606.08415},
  eprinttype    = {arXiv},
  eprint       = {1606.08415},
  timestamp    = {Mon, 13 Aug 2018 16:46:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HendrycksG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lapuschkin2019unmasking,
  title={Unmasking Clever Hans predictors and assessing what machines really learn},
  author={Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1096},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{feng-etal-2018-pathologies,
    title = "Pathologies of Neural Models Make Interpretations Difficult",
    author = "Feng, Shi  and
      Wallace, Eric  and
      Grissom II, Alvin  and
      Iyyer, Mohit  and
      Rodriguez, Pedro  and
      Boyd-Graber, Jordan",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1407/",
    doi = "10.18653/v1/D18-1407",
    pages = "3719--3728",
    abstract = "One way to interpret neural model predictions is to highlight the most important input features{---}for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word`s importance is determined by either input perturbation{---}measuring the decrease in model confidence when that word is removed{---}or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction, without accuracy loss on regular examples."
}

@inproceedings{brinner-zarriess-2024-rationalizing,
    title = "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training",
    author = "Brinner, Marc Felix  and
      Zarrie{\ss}, Sina",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.664/",
    doi = "10.18653/v1/2024.emnlp-main.664",
    pages = "11894--11907",
    abstract = "We propose an end-to-end differentiable training paradigm for stable training of a rationalized transformer classifier. Our approach results in a single model that simultaneously classifies a sample and scores input tokens based on their relevance to the classification. To this end, we build on the widely-used three-player-game for training rationalized models, which typically relies on training a rationale selector, a classifier and a complement classifier. We simplify this approach by making a single model fulfill all three roles, leading to a more efficient training paradigm that is not susceptible to the common training instabilities that plague existing approaches. Further, we extend this paradigm to produce class-wise rationales while incorporating recent advances in parameterizing and regularizing the resulting rationales, thus leading to substantially improved and state-of-the-art alignment with human annotations without any explicit supervision."
}

@article{Smilkov-2017-SmoothGrad,
  author       = {Daniel Smilkov and
                  Nikhil Thorat and
                  Been Kim and
                  Fernanda B. Vi{\'{e}}gas and
                  Martin Wattenberg},
  title        = {SmoothGrad: removing noise by adding noise},
  journal      = {CoRR},
  volume       = {abs/1706.03825},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03825},
  eprinttype    = {arXiv},
  eprint       = {1706.03825},
  timestamp    = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SmilkovTKVW17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/LiMJ16a,
  author       = {Jiwei Li and
                  Will Monroe and
                  Dan Jurafsky},
  title        = {Understanding Neural Networks through Representation Erasure},
  journal      = {CoRR},
  volume       = {abs/1612.08220},
  year         = {2016},
  url          = {http://arxiv.org/abs/1612.08220},
  eprinttype    = {arXiv},
  eprint       = {1612.08220},
  timestamp    = {Sun, 12 Mar 2023 00:56:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/LiMJ16a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chrysostomou-aletras-2021-enjoy,
    title = "Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience",
    author = "Chrysostomou, George  and
      Aletras, Nikolaos",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.645/",
    doi = "10.18653/v1/2021.emnlp-main.645",
    pages = "8189--8200",
    abstract = "Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SaLoss; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SaLoss consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SaLoss models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks."
}

@inproceedings{tan-2022-diversity,
    title = "On the Diversity and Limits of Human Explanations",
    author = "Tan, Chenhao",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.158/",
    doi = "10.18653/v1/2022.naacl-main.158",
    pages = "2173--2188",
    abstract = "A growing effort in NLP aims to build datasets of human explanations. However, it remains unclear whether these datasets serve their intended goals. This problem is exacerbated by the fact that the term explanation is overloaded and refers to a broad range of notions with different properties and ramifications. Our goal is to provide an overview of the diversity of explanations, discuss human limitations in providing explanations, and ultimately provide implications for collecting and using human explanations in NLP.Inspired by prior work in psychology and cognitive sciences, we group existing human explanations in NLP into three categories: proximal mechanism, evidence, and procedure. These three types differ in nature and have implications for the resultant explanations. For instance, procedure is not considered explanation in psychology and connects with a rich body of work on learning from instructions. The diversity of explanations is further evidenced by proxy questions that are needed for annotators to interpret and answer {\textquotedblleft}why is [input] assigned [label]{\textquotedblright}. Finally, giving explanations may require different, often deeper, understandings than predictions, which casts doubt on whether humans can provide valid explanations in some tasks."
}

@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature machine intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group UK London}
}


@inproceedings{arya24bcosification,
 author = {Arya, Shreyash and Rao, Sukrut and B\"{o}hle, Moritz and Schiele, Bernt},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {62756--62786},
 publisher = {Curran Associates, Inc.},
 title = {B-cosification: Transforming Deep Neural Networks to be Inherently LInterpretable},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/72d50a87b218d84c175d16f4557f7e12-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}


@InProceedings{Rao_2023_ICCV,
    author    = {Rao, Sukrut and B\"ohle, Moritz and Parchami-Araghi, Amin and Schiele, Bernt},
    title     = {Studying How to Efficiently and Effectively Guide Models with Explanations},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {1922-1933}
}

@article{DBLP:journals/corr/abs-1902-00006,
  author       = {Isaac Lage and
                  Emily Chen and
                  Jeffrey He and
                  Menaka Narayanan and
                  Been Kim and
                  Sam Gershman and
                  Finale Doshi{-}Velez},
  title        = {An Evaluation of the Human-Interpretability of Explanation},
  journal      = {CoRR},
  volume       = {abs/1902.00006},
  year         = {2019},
  url          = {http://arxiv.org/abs/1902.00006},
  eprinttype    = {arXiv},
  eprint       = {1902.00006},
  timestamp    = {Tue, 21 May 2019 18:03:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1902-00006.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2009-07896,
  author       = {Narine Kokhlikyan and
                  Vivek Miglani and
                  Miguel Martin and
                  Edward Wang and
                  Bilal Alsallakh and
                  Jonathan Reynolds and
                  Alexander Melnikov and
                  Natalia Kliushkina and
                  Carlos Araya and
                  Siqi Yan and
                  Orion Reblitz{-}Richardson},
  title        = {Captum: {A} unified and generic model interpretability library for
                  PyTorch},
  journal      = {CoRR},
  volume       = {abs/2009.07896},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.07896},
  eprinttype    = {arXiv},
  eprint       = {2009.07896},
  timestamp    = {Tue, 03 Aug 2021 17:00:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-07896.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
