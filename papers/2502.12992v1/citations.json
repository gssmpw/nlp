[
  {
    "index": 0,
    "papers": [
      {
        "key": "atanasova-etal-2020-diagnostic",
        "author": "Atanasova, Pepa  and\nSimonsen, Jakob Grue  and\nLioma, Christina  and\nAugenstein, Isabelle",
        "title": "A Diagnostic Study of Explainability Techniques for Text Classification"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "DBLP:journals/corr/SimonyanVZ13",
        "author": "Karen Simonyan and\nAndrea Vedaldi and\nAndrew Zisserman",
        "title": "Deep Inside Convolutional Networks: Visualising Image Classification\nModels and Saliency Maps"
      },
      {
        "key": "DBLP:journals/corr/KindermansSMD16",
        "author": "Pieter{-}Jan Kindermans and\nKristof Sch{\\\"{u}}tt and\nKlaus{-}Robert M{\\\"{u}}ller and\nSven D{\\\"{a}}hne",
        "title": "Investigating the influence of noise and distractors on the interpretation\nof neural networks"
      },
      {
        "key": "Sundararajan-2017-IntegratedGrad",
        "author": "Mukund Sundararajan and Ankur Taly and Qiqi Yan",
        "title": "Axiomatic Attribution for Deep Networks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bach2015pixel",
        "author": "Bach, Sebastian and Binder, Alexander and Montavon, Gr{\\'e}goire and Klauschen, Frederick and M{\\\"u}ller, Klaus-Robert and Samek, Wojciech",
        "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation"
      },
      {
        "key": "shrikumar2017learning",
        "author": "Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul",
        "title": "Learning important features through propagating activation differences"
      },
      {
        "key": "DBLP:journals/corr/SpringenbergDBR14",
        "author": "Jost Tobias Springenberg and\nAlexey Dosovitskiy and\nThomas Brox and\nMartin A. Riedmiller",
        "title": "Striving for Simplicity: The All Convolutional Net"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "DBLP:journals/corr/LiMJ16a",
        "author": "Jiwei Li and\nWill Monroe and\nDan Jurafsky",
        "title": "Understanding Neural Networks through Representation Erasure"
      },
      {
        "key": "Ribeiro-2016-LIME",
        "author": "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",
        "title": "\" Why should i trust you?\" Explaining the predictions of any classifier"
      },
      {
        "key": "Lundberg-2017-SHAP",
        "author": "Lundberg, Scott M and Lee, Su-In",
        "title": "A Unified Approach to Interpreting Model Predictions"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "DBLP:journals/corr/BahdanauCB14",
        "author": "Dzmitry Bahdanau and\nKyunghyun Cho and\nYoshua Bengio",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "DBLP:conf/nips/VaswaniSPUJGKP17",
        "author": "Ashish Vaswani and\nNoam Shazeer and\nNiki Parmar and\nJakob Uszkoreit and\nLlion Jones and\nAidan N. Gomez and\nLukasz Kaiser and\nIllia Polosukhin",
        "title": "Attention is All you Need"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "Kindermans-2019-Reliability",
        "author": "Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\\\"u}tt, Kristof T and D{\\\"a}hne, Sven and Erhan, Dumitru and Kim, Been",
        "title": "The (un) reliability of saliency methods"
      },
      {
        "key": "jain-wallace-2019-attention",
        "author": "Jain, Sarthak  and\nWallace, Byron C.",
        "title": "{A}ttention is not {E}xplanation"
      },
      {
        "key": "Slack-2020-FoolingLimeSHAP",
        "author": "Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu",
        "title": "Fooling lime and shap: Adversarial attacks on post hoc explanation methods"
      },
      {
        "key": "pruthi-etal-2020-learning",
        "author": "Pruthi, Danish  and\nGupta, Mansi  and\nDhingra, Bhuwan  and\nNeubig, Graham  and\nLipton, Zachary C.",
        "title": "Learning to Deceive with Attention-Based Explanations"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "neely2022song",
        "author": "Neely, Michael and Schouten, Stefan F and Bleeker, Maurits and Lucic, Ana",
        "title": "A song of (dis) agreement: Evaluating the evaluation of explainable artificial intelligence in natural language processing"
      },
      {
        "key": "Dasgupta-2022-FaithfulnessEval",
        "author": "Dasgupta, Sanjoy and Frost, Nave and Moshkovitz, Michal",
        "title": "Framework for Evaluating Faithfulness of Local Explanations"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Smilkov-2017-SmoothGrad",
        "author": "Daniel Smilkov and\nNikhil Thorat and\nBeen Kim and\nFernanda B. Vi{\\'{e}}gas and\nMartin Wattenberg",
        "title": "SmoothGrad: removing noise by adding noise"
      },
      {
        "key": "NEURIPS2021_e0cd3f16",
        "author": "Ismail, Aya Abdelsalam and Corrada Bravo, Hector and Feizi, Soheil",
        "title": "Improving Deep Learning Interpretability by Saliency Guided Training"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Kindermans-2018-PatternNet",
        "author": "Pieter{-}Jan Kindermans and\nKristof T. Sch{\\\"{u}}tt and\nMaximilian Alber and\nKlaus{-}Robert M{\\\"{u}}ller and\nDumitru Erhan and\nBeen Kim and\nSven D{\\\"{a}}hne",
        "title": "Learning how to explain neural networks: PatternNet and PatternAttribution"
      },
      {
        "key": "rudin2019stop",
        "author": "Rudin, Cynthia",
        "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead"
      },
      {
        "key": "atanasova2022diagnostics",
        "author": "Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle",
        "title": "Diagnostics-guided explanation generation"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "NEURIPS2018_3e9f0fc9",
        "author": "Alvarez Melis, David and Jaakkola, Tommi",
        "title": "Towards Robust Interpretability with Self-Explaining Neural Networks"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "NEURIPS2021_e0cd3f16",
        "author": "Ismail, Aya Abdelsalam and Corrada Bravo, Hector and Feizi, Soheil",
        "title": "Improving Deep Learning Interpretability by Saliency Guided Training"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "Kindermans-2018-PatternNet",
        "author": "Pieter{-}Jan Kindermans and\nKristof T. Sch{\\\"{u}}tt and\nMaximilian Alber and\nKlaus{-}Robert M{\\\"{u}}ller and\nDumitru Erhan and\nBeen Kim and\nSven D{\\\"{a}}hne",
        "title": "Learning how to explain neural networks: PatternNet and PatternAttribution"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "tutek2022toward",
        "author": "Tutek, Martin and {\\v{S}}najder, Jan",
        "title": "Toward practical usage of the attention mechanism as a tool for interpretability"
      },
      {
        "key": "moradi-etal-2020-training",
        "author": "Moradi, Pooya  and\nKambhatla, Nishant  and\nSarkar, Anoop",
        "title": "Training with Adversaries to Improve Faithfulness of Attention in Neural Machine Translation"
      },
      {
        "key": "moradi-etal-2021-measuring",
        "author": "Moradi, Pooya  and\nKambhatla, Nishant  and\nSarkar, Anoop",
        "title": "Measuring and Improving Faithfulness of Attention in Neural Machine Translation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "atanasova2022diagnostics",
        "author": "Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle",
        "title": "Diagnostics-guided explanation generation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "NEURIPS2018_3e9f0fc9",
        "author": "Alvarez Melis, David and Jaakkola, Tommi",
        "title": "Towards Robust Interpretability with Self-Explaining Neural Networks"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "NEURIPS2021_e0cd3f16",
        "author": "Ismail, Aya Abdelsalam and Corrada Bravo, Hector and Feizi, Soheil",
        "title": "Improving Deep Learning Interpretability by Saliency Guided Training"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "pruthi-etal-2020-learning",
        "author": "Pruthi, Danish  and\nGupta, Mansi  and\nDhingra, Bhuwan  and\nNeubig, Graham  and\nLipton, Zachary C.",
        "title": "Learning to Deceive with Attention-Based Explanations"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "lei-etal-2016-rationalizing",
        "author": "Lei, Tao  and\nBarzilay, Regina  and\nJaakkola, Tommi",
        "title": "Rationalizing Neural Predictions"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zheng-etal-2022-irrationality",
        "author": "Zheng, Yiming  and\nBooth, Serena  and\nShah, Julie  and\nZhou, Yilun",
        "title": "The Irrationality of Neural Rationale Models"
      },
      {
        "key": "jacovi-goldberg-2021-aligning",
        "author": "Jacovi, Alon  and\nGoldberg, Yoav",
        "title": "Aligning Faithful Interpretations with their Social Attribution"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "lyu-etal-2024-towards",
        "author": "Lyu, Qing  and\nApidianaki, Marianna  and\nCallison-Burch, Chris",
        "title": "Towards Faithful Model Explanation in {NLP}: A Survey"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "Bohle_2022_CVPR",
        "author": "B\\\"ohle, Moritz and Fritz, Mario and Schiele, Bernt",
        "title": "B-Cos Networks: Alignment Is All We Need for Interpretability"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "arya24bcosification",
        "author": "Arya, Shreyash and Rao, Sukrut and B\\\"{o}hle, Moritz and Schiele, Bernt",
        "title": "B-cosification: Transforming Deep Neural Networks to be Inherently LInterpretable"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "arya24bcosification",
        "author": "Arya, Shreyash and Rao, Sukrut and B\\\"{o}hle, Moritz and Schiele, Bernt",
        "title": "B-cosification: Transforming Deep Neural Networks to be Inherently LInterpretable"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "arya24bcosification",
        "author": "Arya, Shreyash and Rao, Sukrut and B\\\"{o}hle, Moritz and Schiele, Bernt",
        "title": "B-cosification: Transforming Deep Neural Networks to be Inherently LInterpretable"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "arya24bcosification",
        "author": "Arya, Shreyash and Rao, Sukrut and B\\\"{o}hle, Moritz and Schiele, Bernt",
        "title": "B-cosification: Transforming Deep Neural Networks to be Inherently LInterpretable"
      }
    ]
  }
]