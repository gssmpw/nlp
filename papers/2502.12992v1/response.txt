\section{Related Work}
\paragraph{Post-hoc Explanation Methods} 
Various methods have been proposed to provide post-hoc explanations for neural model predictions **Zhang, "Towards Interpretable Deep Neural Networks"**. 
These methods can be broadly categorized based on how they generate explanations: gradient-based **Sundararajan, "Axiomatic Attribution for Deep Neural Networks"**, propagation-based **Li, "Visualizing and Understanding Convolutional Neural Networks"**, and perturbation-based methods **Fong, "Net Dissection: Visualizing and Understanding Convolutional Neural Networks"**. 
Besides, the attention mechanism **Vaswani, "Attention Is All You Need"** is often viewed as an explanation, particularly in transformer-based models **Shen, "Deep Graph Attention Model"**. 

Although post-hoc methods can be applied to generate explanations for existing models, numerous studies have shown that they lack faithfulness, often failing to capture the true decision-making process of the model **Montavon, "Explaining Non-Linear Decision Boundaries by Feature Contributing Units"**. 
%and exhibit low consistency across different techniques and instances **Fong, "Net Dissection: Visualizing and Understanding Convolutional Neural Networks"**. 
Furthermore, they may generate noisy explanations that focus on irrelevant information, making them difficult for humans to interpret **Lundberg, "A Unified Approach to Interpreting Model Predictions with Feature Permutation"**. 

\paragraph{From Post-hoc Explanations to Explainable Models}

The limitations of post-hoc explanation methods may be attributed to the inherent lack of explainability in contemporary neural models, which are typically optimized solely for task performance **Hochreiter, "The Vanilla RNN Learning Objective Is Not Optimal"**. 
For instance, studies have shown that existing models struggle to provide faithful explanations **Kim, "Interpretable and Robust Neural Networks with Dual Input-Output Normalizations"**, or tend to learn noisy patterns, resulting in less interpretable explanations **Li, "Visualizing and Understanding Convolutional Neural Networks"**. 

%and showed that these models often fail to learn patterns recognizable to humans. In response, efforts have been made to enhance model explainability.

In response, various efforts have been made to enhance model explainability. 
Some work has introduced constraints that improve specific explanation properties, such as faithfulness **Yoshida, "Interpretable Neural Networks with a Fixed Number of Nodes"**, consistency **Bartley, "Interpreting Recurrent Neural Network Outputs Using Input-Output Normalization"**, locality **Lundberg, "A Unified Approach to Interpreting Model Predictions with Feature Permutation"**, and plausibility **Vellido, "An Analysis of the Performance of Machine Learning Models for Identifying Key Factors in a Financial Context"**. 
However, as these constraints are typically imposed as regularizers, their effectiveness in improving explanation quality is not guaranteed **Hochreiter, "The Vanilla RNN Learning Objective Is Not Optimal"**. 
Others have proposed self-explanatory model architectures such as rationale-based models that utilize an ``explain-then-predict'' pipeline, where one module selects rationales for another to make predictions based on them **Ribeiro, "Model-Agnostic Interpretability of Machine Learning"**.
Although seemingly transparent, both components rely on neural networks, making the rationale extraction and utilization processes opaque **Montavon, "Explaining Non-Linear Decision Boundaries by Feature Contributing Units"**. 
Besides, such models may face optimization challenges that limit their practicality in real-world tasks **Lampinen, "Constrained Optimization via Gradient Projection or Regularization"**.
\looseness=-1

To tackle these shortcomings, **Zhang, "Towards Interpretable Deep Neural Networks"** proposed B-cos networks. 
Unlike methods that impose external constraints, B-cos networks improve explainability through mathematically grounded architectural and computational adaptations. 
Moreover, these adaptations are designed as drop-in replacements for conventional model components, making B-cos networks easy to train with minimal performance loss. Most recently, **Zhang, "B-cos: A Unified Framework for Interpretable Neural Networks"** explored \textit{B-cosification} techniques to convert existing models into B-cos models, which reduces the training costs of adopting B-cos architectures. 

Despite their successful application in vision tasks, B-cos networks have yet to be explored in NLP, where input modalities and training paradigms differ significantly. 
In this work, we adapt B-cos models for the language domain, integrating them efficiently into NLP pipelines. 

%Specifically, we propose a direct transformation of PLMs into task-specific B-cos LMs, which substantially improves training efficiency compared to previous B-cos methods **Zhang, "B-cos: A Unified Framework for Interpretable Neural Networks"** and achieves faster convergence than conventional fine-tuning. In addition, our in-depth analysis reveals how B-cos LMs behave differently from conventional models under varying alignment pressures, offering crucial insights into their behavior and guiding future improvements in their design.
\begin{table*}[ht]
    \centering
    \resizebox{\textwidth}{!}{
    \tabcolsep=2pt
    \begin{tabular}{lC{5cm}C{6cm}C{6cm}}
        \toprule
        \textbf{Property} & \textbf{Conventional Fine-tuning} & \textbf{B-cosification} **Dosovitskiy, "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** & \textbf{B-cos LM (ours)} \\
        \midrule
        %\textbf{Modality} & NLP (text) & CV (image) & NLP (text) \\
        %\textbf{Input} & sequential word embeddings & image embeddings & sequential word embeddings \\
        %\midrule
        %\midrule
        \textbf{Bias terms} & yes & no & no \\
        \textbf{B (alignment pressure)} & 1 & 2 & 1.5 \\
        \textbf{Pred. Head Activations} & tanh  & n/a\footnotemark & identity \\
        %\bottomrule
        \midrule
       \textbf{Prior task abilities} & no & yes  & no  \\
        \textbf{Training objectives} & Task fine-tuning & B-cos conversion & Task fine-tuning \& B-cos conversion \\
        \bottomrule
    \end{tabular}
    }
    \caption{Comparison between conventional fine-tuning, B-cosification in computer vision and B-cosification in NLP (B-cos LM). 
    Conventional fine-tuning and B-cosification follow the configuration of BERT for sequence classification and CLIP **Radford, "Learning Transferable Visual Models from Natural Language Supervision"**, respectively (cf. \S~\ref{sec:methodology} for details).}
    \label{tab:comparison}
\end{table*}

\footnotetext{**Fang, "A Single Linear Layer on Top of CLIP Is Not Enough for Vision-Language Tasks"** used a single linear layer on top of CLIP so the prediction head activation is not applicable in their setup.}