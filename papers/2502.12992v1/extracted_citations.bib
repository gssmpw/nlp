@InProceedings{Bohle_2022_CVPR,
    author    = {B\"ohle, Moritz and Fritz, Mario and Schiele, Bernt},
    title     = {B-Cos Networks: Alignment Is All We Need for Interpretability},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10329-10338}
}

@inproceedings{DBLP:conf/nips/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Attention is All you Need},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {5998--6008},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/BahdanauCB14,
  author       = {Dzmitry Bahdanau and
                  Kyunghyun Cho and
                  Yoshua Bengio},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1409.0473},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/KindermansSMD16,
  author       = {Pieter{-}Jan Kindermans and
                  Kristof Sch{\"{u}}tt and
                  Klaus{-}Robert M{\"{u}}ller and
                  Sven D{\"{a}}hne},
  title        = {Investigating the influence of noise and distractors on the interpretation
                  of neural networks},
  journal      = {CoRR},
  volume       = {abs/1611.07270},
  year         = {2016},
  url          = {http://arxiv.org/abs/1611.07270},
  eprinttype    = {arXiv},
  eprint       = {1611.07270},
  timestamp    = {Mon, 13 Aug 2018 16:48:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KindermansSMD16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/LiMJ16a,
  author       = {Jiwei Li and
                  Will Monroe and
                  Dan Jurafsky},
  title        = {Understanding Neural Networks through Representation Erasure},
  journal      = {CoRR},
  volume       = {abs/1612.08220},
  year         = {2016},
  url          = {http://arxiv.org/abs/1612.08220},
  eprinttype    = {arXiv},
  eprint       = {1612.08220},
  timestamp    = {Sun, 12 Mar 2023 00:56:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/LiMJ16a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/SimonyanVZ13,
  author       = {Karen Simonyan and
                  Andrea Vedaldi and
                  Andrew Zisserman},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Deep Inside Convolutional Networks: Visualising Image Classification
                  Models and Saliency Maps},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings},
  year         = {2014},
  url          = {http://arxiv.org/abs/1312.6034},
  timestamp    = {Thu, 25 Jul 2019 14:36:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SimonyanVZ13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/SpringenbergDBR14,
  author       = {Jost Tobias Springenberg and
                  Alexey Dosovitskiy and
                  Thomas Brox and
                  Martin A. Riedmiller},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Striving for Simplicity: The All Convolutional Net},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6806},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SpringenbergDBR14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Dasgupta-2022-FaithfulnessEval,
  title = 	 {Framework for Evaluating Faithfulness of Local Explanations},
  author =       {Dasgupta, Sanjoy and Frost, Nave and Moshkovitz, Michal},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {4794--4815},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/dasgupta22a/dasgupta22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/dasgupta22a.html},
}

@inproceedings{Kindermans-2018-PatternNet,
  author       = {Pieter{-}Jan Kindermans and
                  Kristof T. Sch{\"{u}}tt and
                  Maximilian Alber and
                  Klaus{-}Robert M{\"{u}}ller and
                  Dumitru Erhan and
                  Been Kim and
                  Sven D{\"{a}}hne},
  title        = {Learning how to explain neural networks: PatternNet and PatternAttribution},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=Hkn7CBaTW},
  timestamp    = {Thu, 25 Jul 2019 14:25:51 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/KindermansSAMEK18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Kindermans-2019-Reliability,
  title={The (un) reliability of saliency methods},
  author={Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"u}tt, Kristof T and D{\"a}hne, Sven and Erhan, Dumitru and Kim, Been},
  journal={Explainable AI: Interpreting, explaining and visualizing deep learning},
  pages={267--280},
  year={2019},
  publisher={Springer}
}

@inproceedings{Lundberg-2017-SHAP,
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {1--10},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{NEURIPS2018_3e9f0fc9,
 author = {Alvarez Melis, David and Jaakkola, Tommi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{NEURIPS2021_e0cd3f16,
 author = {Ismail, Aya Abdelsalam and Corrada Bravo, Hector and Feizi, Soheil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26726--26739},
 publisher = {Curran Associates, Inc.},
 title = {Improving Deep Learning Interpretability by Saliency Guided Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e0cd3f16f9e883ca91c2a4c24f47b3d9-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{Ribeiro-2016-LIME,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{Slack-2020-FoolingLimeSHAP,
  title={Fooling lime and shap: Adversarial attacks on post hoc explanation methods},
  author={Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={180--186},
  year={2020}
}

@article{Smilkov-2017-SmoothGrad,
  author       = {Daniel Smilkov and
                  Nikhil Thorat and
                  Been Kim and
                  Fernanda B. Vi{\'{e}}gas and
                  Martin Wattenberg},
  title        = {SmoothGrad: removing noise by adding noise},
  journal      = {CoRR},
  volume       = {abs/1706.03825},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03825},
  eprinttype    = {arXiv},
  eprint       = {1706.03825},
  timestamp    = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SmilkovTKVW17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Sundararajan-2017-IntegratedGrad,
  title = 	 {Axiomatic Attribution for Deep Networks},
  author =       {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3319--3328},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/sundararajan17a.html},
}

@inproceedings{arya24bcosification,
 author = {Arya, Shreyash and Rao, Sukrut and B\"{o}hle, Moritz and Schiele, Bernt},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {62756--62786},
 publisher = {Curran Associates, Inc.},
 title = {B-cosification: Transforming Deep Neural Networks to be Inherently LInterpretable},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/72d50a87b218d84c175d16f4557f7e12-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@inproceedings{atanasova-etal-2020-diagnostic,
    title = "A Diagnostic Study of Explainability Techniques for Text Classification",
    author = "Atanasova, Pepa  and
      Simonsen, Jakob Grue  and
      Lioma, Christina  and
      Augenstein, Isabelle",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.263/",
    doi = "10.18653/v1/2020.emnlp-main.263",
    pages = "3256--3274",
    abstract = "Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques."
}

@inproceedings{atanasova2022diagnostics,
  title={Diagnostics-guided explanation generation},
  author={Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={10445--10453},
  year={2022}
}

@article{bach2015pixel,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{jacovi-goldberg-2021-aligning,
    title = "Aligning Faithful Interpretations with their Social Attribution",
    author = "Jacovi, Alon  and
      Goldberg, Yoav",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.18/",
    doi = "10.1162/tacl_a_00367",
    pages = "294--310",
    abstract = "We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations."
}

@inproceedings{jain-wallace-2019-attention,
    title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1357/",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
    abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful ``explanations'' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do."
}

@inproceedings{lei-etal-2016-rationalizing,
    title = "Rationalizing Neural Predictions",
    author = "Lei, Tao  and
      Barzilay, Regina  and
      Jaakkola, Tommi",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1011/",
    doi = "10.18653/v1/D16-1011",
    pages = "107--117"
}

@article{lyu-etal-2024-towards,
    title = "Towards Faithful Model Explanation in {NLP}: A Survey",
    author = "Lyu, Qing  and
      Apidianaki, Marianna  and
      Callison-Burch, Chris",
    journal = "Computational Linguistics",
    volume = "50",
    number = "2",
    month = jun,
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.cl-2.6/",
    doi = "10.1162/coli_a_00511",
    pages = "657--723",
    abstract = "End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, that is, an explanation should accurately represent the reasoning process behind the model's prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP."
}

@inproceedings{moradi-etal-2020-training,
    title = "Training with Adversaries to Improve Faithfulness of Attention in Neural Machine Translation",
    author = "Moradi, Pooya  and
      Kambhatla, Nishant  and
      Sarkar, Anoop",
    editor = "Shmueli, Boaz  and
      Huang, Yin Jou",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-srw.14/",
    doi = "10.18653/v1/2020.aacl-srw.14",
    pages = "93--100",
    abstract = "Can we trust that the attention heatmaps produced by a neural machine translation (NMT) model reflect its true internal reasoning? We isolate and examine in detail the notion of faithfulness in NMT models. We provide a measure of faithfulness for NMT based on a variety of stress tests where model parameters are perturbed and measuring faithfulness based on how often the model output changes. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and it also seems to have a useful regularization effect on the NMT model and can even improve translation quality in some cases."
}

@inproceedings{moradi-etal-2021-measuring,
    title = "Measuring and Improving Faithfulness of Attention in Neural Machine Translation",
    author = "Moradi, Pooya  and
      Kambhatla, Nishant  and
      Sarkar, Anoop",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.243/",
    doi = "10.18653/v1/2021.eacl-main.243",
    pages = "2791--2802",
    abstract = "While the attention heatmaps produced by neural machine translation (NMT) models seem insightful, there is little evidence that they reflect a model's true internal reasoning. We provide a measure of faithfulness for NMT based on a variety of stress tests where attention weights which are crucial for prediction are perturbed and the model should alter its predictions if the learned weights are a faithful explanation of the predictions. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and has a useful regularization effect on the NMT model and can even improve translation quality in some cases."
}

@incollection{neely2022song,
  title={A song of (dis) agreement: Evaluating the evaluation of explainable artificial intelligence in natural language processing},
  author={Neely, Michael and Schouten, Stefan F and Bleeker, Maurits and Lucic, Ana},
  booktitle={HHAI2022: Augmenting Human Intellect},
  pages={60--78},
  year={2022},
  publisher={IOS Press}
}

@inproceedings{pruthi-etal-2020-learning,
    title = "Learning to Deceive with Attention-Based Explanations",
    author = "Pruthi, Danish  and
      Gupta, Mansi  and
      Dhingra, Bhuwan  and
      Neubig, Graham  and
      Lipton, Zachary C.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.432/",
    doi = "10.18653/v1/2020.acl-main.432",
    pages = "4782--4793",
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature machine intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{shrikumar2017learning,
  title={Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={International conference on machine learning},
  pages={3145--3153},
  year={2017},
  organization={PMlR}
}

@article{tutek2022toward,
  title={Toward practical usage of the attention mechanism as a tool for interpretability},
  author={Tutek, Martin and {\v{S}}najder, Jan},
  journal={IEEE access},
  volume={10},
  pages={47011--47030},
  year={2022},
  publisher={IEEE}
}

@inproceedings{zheng-etal-2022-irrationality,
    title = "The Irrationality of Neural Rationale Models",
    author = "Zheng, Yiming  and
      Booth, Serena  and
      Shah, Julie  and
      Zhou, Yilun",
    editor = "Verma, Apurv  and
      Pruksachatkun, Yada  and
      Chang, Kai-Wei  and
      Galstyan, Aram  and
      Dhamala, Jwala  and
      Cao, Yang Trista",
    booktitle = "Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022)",
    month = jul,
    year = "2022",
    address = "Seattle, U.S.A.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.trustnlp-1.6/",
    doi = "10.18653/v1/2022.trustnlp-1.6",
    pages = "64--73",
    abstract = "Neural rationale models are popular for interpretable predictions of NLP tasks. In these, a selector extracts segments of the input text, called rationales, and passes these segments to a classifier for prediction. Since the rationale is the only information accessible to the classifier, it is plausibly defined as the explanation. Is such a characterization unconditionally correct? In this paper, we argue to the contrary, with both philosophical perspectives and empirical evidence suggesting that rationale models are, perhaps, less rational and interpretable than expected. We call for more rigorous evaluations of these models to ensure desired properties of interpretability are indeed achieved. The code for our experiments is at \url{https://github.com/yimingz89/Neural-Rationale-Analysis}."
}

