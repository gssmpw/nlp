\section{Similarity between AI agents}
\label{sec:method:agent}

It would be better if we could say that an AI agent is acceptable when it is sufficiently close to an AI agent that we empirically know is strongly acceptable, regardless of how those agents are developed (i.e., independent of their configurations including the amount of computation for training).  A question is how to evaluate the similarity between two AI agents.  If the two AI agents consist of identical base FMs and identical planning modules, their difference may be simply characterized by the amount of computation for inference.  However, AI agents can consist of different base FMs or different planning modules.
%(e.g., the number of search steps performed in MCTS).

A possible approach is to measure the similarity between two AI agents, $G$ and $G'$, based on their output.  For example, using a distance $d$ between probability distributions, we may for example define the distance between $G$ and $G'$ with $\sup_x d(g(G,x),g(G',x))$, where $g(G,x)$ is the distribution of the output (e.g., document) of $G$ when it perceives $x$ (e.g., prompt).  This is similar to the motivation of reinforcement learning from human feedback \cite{ouyang2022training,bai2022training} and direct policy optimization \cite{rafailov2023direct}, where the regularization with Kullbackâ€“Leibler divergence is used to mitigate catastrophic forgetting or alignment tax \cite{kotha2024understanding,ouyang2022training,bai2022training}, which refer to the phenomena that fine-tuned models lose the skills that pre-trained models had.

Likewise, we may mitigate catastrophic forgetting of the strong acceptability of an AI agent $G'$ and retain the acceptability in a new AI agent $G$ by ensuring $\sup_x d(g(G,x),g(G',x))\le \varepsilon$.  A difficulty is that it is unclear how to evaluate the supremum, since there can be infinitely many possible perceptions $x$.  Alternatively, one may consider $\E[d(g(G,X),g(G',X))]$, where $\E$ is the expectation with respect to some distribution of the random perception $X$.  However, such a guarantee based on expectation may be unsuitable for addressing safety concerns related to existential risks, which involve events with extremely low probabilities and extremely high impacts.

\section{Expanding the strongly acceptable set}
\label{sec:method:expand}

The set of strongly acceptable solutions may be expanded gradually.  For example, we may choose an acceptable solution $z\in\bar\calZ_0$ and keep using $z$ for a certain period of time.  If it turns out that $z$ can be considered strongly acceptable based on its behavior during that period, we may expand the set of strongly acceptable solutions by adding $z$ into $\calZ_0$.  This process of expanding the acceptable set $\calZ_0$ could also be performed jointly as a community.  

This expansion of acceptable set is similar in spirit to safe exploration in RL \cite{garcia2015comprehensive}.  Here, the safety set is gradually expand, starting from a seed set, for example based on the assumption of Lipshitz continuity and Gaussian process \cite{sui2015safe}.  In control theory, safety is often guaranteed with barrier certificates often based on some prior knowledge about the environment \cite{ames2019control,luo2021learning,bansal2017hamilton}.  Such ideas have also been exploited in safe exploration in RL with generative modeling \cite{wang2023enforcing}.

%barrier certificate or function in control theory: unsafe states are specified by users; given dynamics, can guarantee that it never goes to unsafe states; 
%prior knowledge of the environment dynamics \cite{ames2019control}
%use confidence interval of the dynamics \cite{luo2021learning}.
%Hamilton-Jacobi reachability analysis \cite{bansal2017hamilton} in control theory. 
%- set of states that can lead to an unsafe state
%neural network based controller

\subsection{Rationale for similarity-based measures}
\label{sec:method:rationale}

Here, we provide some rationale on such similarity-based measures.  Let a solution $z$ denote a configuration, an AI agent, or an action-sequence; we discuss the acceptability of $z$.

Suppose that there exists an unknown function $g_0$ such that a solution $z$ is acceptable iff $g_0(z)\le 0$.  We cannot make strong assumptions about $g_0$, since we know little about $g_0$. Since we cannot deal with $g_0$ without any assumptions, let us make a minimal assumption that the solution space $\calZ$ is equipped with a metric $d$, and that $g_0$ is 1-Lipschitz.\footnote{This does not lose generality, since $L$-Lipschitz functions under a metric $d$ can be made 1-Lipshitz by redefining $d$.}  Let $\calG$ be a class of 1-Lipschitz functions.  

We say that a solution $z_0$ is strongly acceptable if all of its $\varepsilon$-neighbors are acceptable.  Let 
$\calZ_0
\subseteq
\left\{
    z\in\calZ
    \mid 
    g(z_0)\le - \varepsilon
\right\}$
be the set of known strongly acceptable solutions.  Then we know that
%\begin{align}
    $\bar\calZ_0
    \coloneqq \left\{
    z \in\calZ
    \mid
    \min_{z_0\in \calZ_0} d(z,z_0) \le \varepsilon
    \right\}$
%\end{align}
is a set of acceptable solutions.  One would typically choose the solution $z$ that maximizes an objective function under the constraint of $z\in\bar\calZ_0$.  In this way, the selected solution is guaranteed to be acceptable under the assumptions made.