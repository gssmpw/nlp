\section{AI agents: Today and future}
\label{sec:agent}

We start by reviewing the state-of-the-art in AI agents, with a focus on LLM-based agents, and shares our perspectives on potential development that could lead to advanced AI agents.  While it is challenging to predict the trajectory of future AI development, this discussion lays the groundwork for the analyses and proposals in the following.  We will also briefly review prior work on safety of AI agents and FMs. 

\subsection{Advanced AI agents}

An AI agent is defined as ``anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators'' \cite{russell2016artificial}.  Based on the observations received from its environment, the controller of an AI agent selects an action, which could range from uttering a word to executing a physical movement, such as the motion of a robotic arm.

LLMs are significantly accelerating the development of advanced AI agents \cite{xi2023rise,wang2024survey,sumers2024cognitive}. LLMs can function as controllers for these agents, utilizing their internal reasoning capabilities, such as those demonstrated by Chain-of-Thought \cite{wei2022chain}. Alternatively, LLMs can be utilized to solve individual subtasks, with an external controller orchestrating the overall plan by breaking the original task into multiple subtasks and coordinating their solutions. The external controller may use simple search methods \cite{yao2023tree,besta2024graph,wang2024math,wang2024multistep} or advanced methods, such as Monte Carlo Tree Search (MCTS) \cite{luo2024improve,zhang2024restmcts} and domain-independent planners \cite{guan2023leveraging,liu2023llmp,dagan2023dynamic}.

%selecting an output from multiple samples \cite{wang2023selfconsistency}, iteratively refining outputs \cite{madaan2023selfrefine}, and conducting debate \cite{du2023improving} or

Reasoning capabilities \cite{huang2023towards,qiao2023reasoning,plaat2024reasoning,xu2025large} are central to such controllers, as they involve searching for and planning sequences of actions to achieve a specified goal. An emerging direction is developing large reasoning models---FMs specifically optimized for reasoning tasks \cite{xu2025large}. Early FMs in this direction include o1 \cite{zhong2024evaluation}, OpenR \cite{wang2024openr}, and LLaMA-Berry \cite{zhang2024llamaberry}. Importantly, strong reasoning capabilities typically stem from inference-time computation rather than pre-training or fine-tuning \cite{ji2025testtime}.

This trend could eventually lead to the development of long-term planning agents (LTPAs), capable of planning over extended time horizons far more effectively than humans.  \citet{cohen2024regulating} warn that LTPAs could ``take humans out of the loop, if it has the opportunity, ... deceive humans and thwart human control'' to achieve their goals. Ensuring the safety of such agents is particularly challenging: their risks cannot be fully tested in real environments due to the inherent danger, nor in simulated environments, since they may behave harmlessly in testing to achieve their goals once deployed. As a result, \citet{cohen2024regulating} compellingly argue that LTPAs should never be developed.

Advanced AI agents may also evolve continually over time. Similar to humans, their cognitive processes may consist of dual systems: System~1, which makes instantaneous and intuitive decisions, and System~2, which performs slower but more deliberate reasoning \cite{kahneman2003perspective,ji2025testtime}. These two systems can interleave in their operations. For instance, System~2 may devise a plan, after which System~1 is updated or retrained to execute similar tasks in the future without requiring further planning \cite{yu2024distilling}. Over time, this enables System~2 to conduct more complex reasoning processes by bypassing previously learned steps.  For such continually learning AI agents, the distinction between training and inference becomes blurred.

In addition to search and planning, advanced AI agents will possess strategic reasoning capabilities \cite{zhang2024llm,feng2024survey,goktas2025strategic}, enabling them to interact with other AI agents and humans in cooperative or competitive ways \cite{guo2024large,jiang2024multimodal}. The effectiveness of multi-agent coordination has already been demonstrated with current LLMs through methods such as debate \cite{du2023improving} and dialog \cite{qian2024chatdev}, which help thems achieve better solutions than a single LLM could alone.  These showcase the potential for sophisticated strategic behavior in complex multi-agent environments.


\subsection{Safety of AI agents and FMs}

While our primary focus is on safety against existential risks, there is a substantial body of literature addressing other types of risks associated with AI agents. Here, we briefly review the prior work on the risks posed by AI agents and FMs along with the approaches to mitigate these risks.

Prior work has identified various risks and safety issues associated with FMs and other generative models \cite{chua2024ai,wang2024security,shayegani2023survey,longpre2024position}. These risks include the generation of toxic, harmful, biased, false, or misleading content, including hallucinations; violations of privacy, copyright, or other legal protections; misalignment with human instructions and values, including ethical and moral considerations; and vulnerabilities to adversarial attacks. 

There have been extensive efforts to develop approaches for mitigating these risks. These approaches include pre-training or fine-tuning with data selection \cite{albalak2024survey} and human feedback \cite{kaufmann2024survey}, establishing guardrails \cite{dong2024safeguarding}, and conducting empirical evaluations through testing \cite{chang2024survey} and red teaming \cite{lin2024achilles}. In particular, \citet{longpre2024position} advocate the importance of evaluation and red teaming by independent third parties.

A particularly relevant risk for advanced AI agents is misalignment, which can result in reward hacking and negative side effects \cite{skalse2022defining,ngo2024alignment,gabriel2020artificial,shen2023large,amodei2016concrete,taylor2020alignment}.  Namely, AI agents can maximize rewards by exploiting misspecifications or misalignment in the reward function, resulting in unintended and potentially high risk behaviors.  One approach to avoiding negative side effects is to avoid any side effect by ensuring that the actions have low impacts on the environment \cite{armstrong2012mathematics,armstrong2017low,amodei2016concrete}.  Representative measures of impact include attainable utility \cite{turner2020conservative,turner2020avoiding}, relative reachability \cite{krakovna2019penalizing}, and other reachability-based measures.  Reachability-base measures are grounded on the idea that reachability to certain states should be maintained, while attainable utility is to maintain the achievability of certain goals, which are different from the goal given to the agent.

%, should be maintained as a result of taking an action (e.g. as compared to not taking any actions).

While these impact measures provide clear guidance on how the safety of AI agents may be ensured, their applicability is limited to relatively simple environments such as grid worlds.  In particular, the requirement on the observability of states makes it difficult to apply existing impact measures to regulate advanced AI agents\footnote{There has been little research on impact measures under partial observability \cite{naiff2023low}.}, since they can operate in complex and open-ended environments that cannot be fully observed.  While the study on impact measures and other techniques towards AI safety remain crucial and can even contribute to mitigating existential risks, the magnitude of existential risks demands additional measures that are broadly applicable in that they require minimal knowledge and assumptions about the environments and the AI agents.

%catastrophic convergence conjecture: unaligned goals tend to have catastrophe-inducing policies because of power-seeking incentives \cite{turner2020catastrophic}
%optimal agents would try to gain control over their environment, because default is not preferable \cite{turner2021optimal}

%Instrumental convergence:  there are some instrumental goals likely to be pursued by almost any intelligent agent (self-preservation, resource acquisition) \cite{bostrom2012superintelligent}
%self-correction \cite{shinn2023reflexion}

% inference time computation (generate multiple responses, and select one): \cite{yu2024selfgenerated,wang2023selfconsistency,zhang2024generative,ankner2024critiqueoutloud}

%\subsection{Impact Minimization}
%\subsubsection{Reachability}
%Consider actions to have high impact if they make some states unreachable. 
%reachability of state states \cite{moldovan2012safe,eysenbach2018leave}
%reachability of safe regions \cite{mitchell2005time,gillula2012guaranteed,fisac2019general}
%- it is insensitive to the magnitude of the irreversible disruption
%- irreversible transitions can happen spontaneously (due to the forces of nature, the actions of other agents, etc).  the agent has an incentive to interfere to prevent them

%\cite{hadfield2021principal} Assume some of the features $K$ are not mentioned.  Then optimize under the constraint that the unmentioned features are not changed.
%\begin{align}
%    \min_{\phi\in\Phi} & U(\phi) \\
%    s.t. & \phi_K = \phi_K^{(0)} \\
%    & C(\phi) \le 0
%\end{align}

%\cite{krakovna2019penalizing} empirically compare various deviation measures: unreachability, relative reachability, attainable utility, value difference measures
%\cite{krakovna2020avoiding} Avoiding Side Effects By Considering Future Tasks
