\section{Approach based on action sequence}
\label{sec:method}

We have seen that the amount of computation is one of the best criteria that we can use today to regulate advanced AI agents.  Although we argue that the amount of computation alone is insufficient, we do not mean to dismiss it.  We should regulate advanced AI agents from various perspectives to mitigate existential risks.  Currently, we lack technologies that allow us to regulate advanced AI agents more effectively than or complementary to the amount of computation.

Here, we suggest a potential approach as a basis of discussion.  The suggested approach by no means solves the problem of existential risk.  Rather, it serves as a baseline and a call for action for further research on this matter.  We hope that the community will build upon it or develop entirely different approaches, eventually leading to technologies that can properly govern and regulate advanced AI agents.

In the following, we say that the amount of computation, an AI agent, or its action sequence is \emph{acceptable} when it involves low existential risk.  Also, we say that any of the three is \emph{strongly acceptable} when it involves very low existential risk, which we will slightly more formalize in the sequel and in Appendix~\ref{sec:method:rationale}.

\subsection{Why and why not the amount of computation?}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{process.pdf}
    \caption{AI agent trained with a configuration generates actions}
    \label{fig:process}
\end{figure}

We first discuss, in a slightly formal manner, why and when the amount of computation may or may not work as a criteria of whether an AI agent is acceptable.  Consider the process of building an AI agent $G$ and letting $G$ generate and take actions (Figure~\ref{fig:process}).  The $G$ is trained with a given configuration $C$, which includes training data, the architecture of a neural network, the amount of computation to use, and other conditions of a training algorithm.  Such a trained $G=f(C)$ generates an action $A$ (e.g., a document or a command to use an external tool), possibly after searching optimal actions with MCTS.  The $A$ is then taken in a cyber-physical world, and $G$ perceives a part the world $x$ (e.g., getting a prompt) to generate further actions depending on $x$.  The $G$ typically determines the conditional distribution of $A$ given $x$.  Namely, $A$ is a sample from the conditional distribution, which is determined by $G$ that is trained with $C$: $A\sim g(G,x)$, where $G=f(C)$.

When we say that an AI agent trained with the amount of computation $C_1$ is acceptable, it is assumed that the acceptability does not decrease much when we only slightly increase the amount of computation.  More formally, suppose that we empirically know that any AI agent is strongly acceptable as long as it is trained with at most $C_1'$ FLOPs of computation (e.g.\ $C_1'=10^{24}$).\footnote{For example, an AI agent may be considered strongly acceptable if it does not show any symptoms of unacceptable behaviors for a certain period of time.  Based on the behaviors of existing LLMs, we may say that an LLM trained with at most $10^{24}$ FLOPs of computation is strongly acceptable.}  Then the assumption is that the AI agent trained with $C=(C_1,C_{-1})$ is acceptable regardless of $C_{-1}$ as long as $d(C_1,C_1')\le \varepsilon$ for some metric $d$ and a threshold $\varepsilon$ (e.g.\ $C_1\le 10^{25}$ if $\varepsilon=1$ and $d$ is the difference in $\log_{10}$ of FLOPs used for training).

While this assumption may be a good approximation for AI agents that are primarily based on LLMs with standard Transformer architectures whose performance is well characterized by scaling laws, advanced AI agents with reasoning and planning capabilities are unlikely to satisfy the assumption, since they would heavily rely on inference-time computation.  Also, the assumption may no longer hold even for LLMs when new technologies are introduced to enable training an LLM having similar capabilities with substantially less computational resources. 


\subsection{Similarity between action sequences}

As is evident from Figure~\ref{fig:process}, it is the action that makes direct impacts on the environment and can pose existential risks.  Hence, rather than regulating configurations, we may directly regulate the actions taken by AI agents.  In Appendix~\ref{sec:method:agent}, we will also discuss an alternative approach of regulating AI agents, particularly based on their output distributions, rather than their actions or configurations.

However, whether an action is acceptable or not typically depends on when, where, and whom the action is taken (i.e., the state of the environment).  In fact, the action space of an AI agent can typically be designed by humans, who could ensure that each individual action is acceptable at normal states.  For example, each action may generate a document, search information from the Web, use an external tool via API, or other elementary steps that would look acceptable when they are considered individually.  It is the sophisticated sequence of actions that can gradually transform the state into unstable one, which can then make the next action unacceptable.

Hence, in the study of the safety of AI agents and RL agents, it is often modeled that those agents operate in Markov decision processes (MDPs).  Then some states are considered unsafe or some state-action pairs are considered unsafe.  One does not say that some actions are unsafe, because those actions can be easily excluded from the action space.

While these conventional approaches of AI/RL safety are principled in theory, they cannot be easily applied to the scenarios where it is hard to define, represent, or observe the (Markovian) state.  AI agents may operate in open-ended real-world environments, and they observe only very partial information about the state of the environment.  The concept of existential risks is rather vague, and we do not know specifically what can cause irreversible global catastrophes or human extinction.  This makes it infeasible to choose specific features of the environmental state to observe, but the entire state of the environment cannot be observed either.

Consider the states and actions in an MDP as a graphical model in Figure~\ref{fig:MDP}.  We want to avoid taking an action $a_{T-1}$ that can lead to an unacceptable state $s_T$.  Since the distribution of $s_T$ is determined by the pair $(s_{T-1},a_{T-1})$ according to a transition probability $p(s_T | s_{T-1},a_{T-1})$, it makes sense to consider the safety of the pair $(s_{T-1},a_{T-1})$ as in the conventional approaches.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{MDP.pdf}
    \caption{States and actions in a Markov decision process}
    \label{fig:MDP}
\end{figure}

The distribution of $s_T$ can also be recursively computed (i.e.\ $p(s_t | s_{t-1},a_{t-1})$ for $t=1,\ldots,T$) from the sequence $(s_0,a_0,\ldots,a_{T-2},a_{T-1})$.  Although the lack of knowledge about $s_{T-1}$ makes $s_T$ less predictable, we may still consider whether the sequence $(s_0,a_0,\ldots,a_{T-2},a_{T-1})$ is acceptable, or whether $a_{T-1}$ is acceptable given $(s_0,a_0,\ldots,a_{T-2})$.  This, however, still requires knowing the initial state $s_0$, which cannot be observed in general.  

Our proposal is to study whether the action-sequence $(a_0,\ldots,a_{T-2},a_{T-1})$ is acceptable.  This may correspond to ensuring that the sequence $(s_0,a_0,\ldots,a_{T-2},a_{T-1})$ is acceptable for any $s_0$.  Although this may sound overly conservative, the dependence on $s_0$ may not be significant in practice.  For example, an ergodic MDP gets close to the steady state distribution after a mixing time, regardless of the initial state.  Although ergodicity generally does not hold in real-world environments, we expect that the action-sequence is a practical compromise to define a measure of acceptability.

Similar to the amount of computation, we may say that an action-sequence is acceptable when it is sufficiently close to the action-sequence that is known to be strongly acceptable.  For example, if $(a_0,\ldots,a_{T-1})$ is strongly acceptable, $(a_0,\ldots,a_{T-1},a_T)$ would be acceptable for any $a_T$.  Note that an action is different from a reasoning step but rather an operation that has some impacts on the environment.  For example, an action may be an API (Application Programming Interface) call that is selected and executed after a heavy reasoning process.  In Appendix~\ref{sec:method:expand}, we discuss how the strongly acceptable set may be gradually expanded, in relation to safe exploration in RL.

There are many ways to measure the distance between sequences.  Dynamic time warping is a representative method.  However, a simple measure such as the difference in the length of the action sequence may be more widely applicable.  Specifically, we may say that the action-sequence of length $T$ is acceptable after getting sufficient empirical evidence that any action-sequences of length $T-1$ is strongly acceptable, regardless of the initial state.  Then AI agents are allowed to autonomously take actions for at most $T$ consecutive steps.  This simplicity is reminiscent of the amount of computation, where we say that the FM trained with $10^{25}$ FLOPs is acceptable, since we have sufficient evidence that FMs trained with $10^{24}$ FLOPs are strongly acceptable, regardless of how they are developed or operated.

Note that our proposal is to regulate the sequence of actions taken \emph{autonomously} by AI agents.  This means that a new action sequence may start after a human intervention.  Also, when an agent cannot accomplish its goal with $T$ actions, it may consult a human and take further actions under a careful supervision of the human.

\subsection{Action trees, action graphs, and beyond}

While an action sequence can well represent the behavior of a single AI agent, it may not be suitable when multiple AI agents are involved.  Suppose, for example, that an AI agent takes a sequence of several actions and gives the results of taking those actions to another AI agent, and then these two agents sequentially take further actions in parallel to jointly accomplish a goal.  In this scenario, the behavior of each agent can be represented by an action sequence, but these action sequences should not be considered independently.  Rather, we should represent the behavior of the two agents with a tree of actions.  In more complex scenarios, we would need to consider a graph of actions taken by multiple agents.

The similarity between action graphs may be defined analogously to action sequences.  For example, we may say that any action graph of size $N$ is acceptable if any action graph of size $N-1$ is strongly acceptable,  empirically.  

When a central controller coordinates multiple AI agents, it can track and manage the behaviors of those agents, making sure that the graph of their actions remain acceptable.  When multiple AI agents collaborate in a decentralized manner, they should communicate their actions to each other to ensure that their action graph remain acceptable.  In both cases, an action graph consists of actions taken autonomously by those agents.  A difficulty arises when multiple AI agents collaborate implicitly by chance, and we currently do not have a solution to this.

%as we have discussed in Section~\ref{sec:alternative:operation}.

\subsection{On similarity-based measures}

All the measures considered are based on similarity: we say that anything is acceptable when it is sufficiently close to something that is strongly acceptable.  In Appendix~\ref{sec:method:rationale}, we provide some rationale for these similarity-based measures.  

When we say that a solution is acceptable based on a similarity measure, it is based on the assumptions that can be summarized with a tuple $(\calZ_0,d,\varepsilon)$.  Namely, what solutions are assumed to be strongly acceptable ($\calZ_0$), how the similarity between solutions is measured ($d$), and what level of guarantee is made ($\varepsilon$).  For example, $\calZ_0$ may be set of all the FMs that are trained with at most $10^{24}$ FLOPs of computation, $d$ may be the difference in FLOPs measured in $\log_{10}$, and $\varepsilon=1$.  Alternatively, $\calZ$ may be the set of action sequences of length at most 1000, $d$ may be the difference in the length of the action-sequences, and $\varepsilon=1$.

Governments may then design regulations based on the tuple $(\calZ_0,d,\varepsilon)$.  Scientists may provide guidelines regarding what tuples should be used in regulations.  
