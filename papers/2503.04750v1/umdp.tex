\section{Unobservable Markov decision processes}

There has been a limited amount of work on unobservable Markov decision processes (UMDPs)\footnote{UMDPs have also been studied under the name of Markov decision processes (MDPs) with no observations, non-observable MDPs (NOMDPs), and no observation MDPs (NOMDPs).}.  An UMDP is a special case of a partially observable Markov decision process (POMDP) in that the agent always makes a null observation in the UMDP.  A standard approach to finding the optimal policy for a POMDP is to recursively compute its value as a function of the belief state, which is updated on the basis of the Bayes rule.  This is substantially simplified when there are no observations.  In this section, we provide a comprehensive survey of the prior work on UMDPs.

\citet{madani1999computability,madani2003undecidability} establish the undecidability of some decision problems associated with POMDPs over infinite horizons by establishing undecidability for the special case of UMDPs.  Such undecidability for UMDPs can be established by reducing an UMDP to a probabilistic finite-state automaton.  The undecidability also holds for a restricted class of UMDPs \cite{balle2017bisimulation,balle2022bisimulation}. While approximate decision problems are still undecidable for general UMDPs over infinite horizons, \citet{chatterjee2024ergodic} study a special case of UMDPs whose approximate decision problems are decidable.

\citet{burago1996complexity} prove that computing the optimal policy for a POMDP over a finite horizon is NP-hard but showing that it is NP-hard for an UMDP.  \citet{wu2020optimal} study special cases of UMDPs over finite horizons whose optimal policies can be computed in polynomial time.

UMDPs have also been used as approximations of POMDPs \cite{hauskrecht2000value,brechtel2015dynamic,lauri2016sequential} or simply discussed as a special case of POMDPs \cite{valkanova2009algorithms}.  For a given POMDP, the corresponding UMDP can given a lower bound on the value function, while the corresponding (fully observable) MDP can give an upper bound on the value function.  This relation between UMDP and POMDP can be exploited to efficiently find approximately optimal policies for POMDPs.  Notice that an UMDP can allow more efficient optimization than the corresponding POMDP, since the UMDP does not need to deal with observations.  For exemple, \citet{king2018robust} studies an MCTS method for UMDPs.

UMDPs have also been studied as a simple special case of POMDPs to study the effectiveness of planning methods in belief states to study the relative performance of different planning methods for POMDPs \cite{littlefield2020efficient,littlefield2018importance,kimmel2019belief}.  UMDPs have also been simply discussed as a special case of POMDPs \cite{boutilier1999decision,csaji2008adaptive,verma2005graphical}.

UMDP also appears in the study of planning for multiple distributed agents to optimize a single objective under partial observability.  Specifically, planning for a decentralized POMDP (DecPOMDP) \cite{oliehoek2016concise} can be reduced to planning for a (centralized) UMDP \cite{oliehoek2014decpomdps,roijers2016multi,roijers2020multi}, where the state in the UMDP is the pair of the state and the history of observations in the DecPOMDP, and the action in the UMDP is the decision rule that maps the history of observations into the actions in the POMDP.  In DecPOMDPs, the belief (distribution) over the pair of the state and the history of observations is the sufficient statistic, and planning can be performed in the space of the belief states.

\citet{evendar2007value} consider an UMDP in the context of studying what values observations can provide in a POMDP.  They introduce a parameter that ranges from 0 to 1.  When the parameter is 0, the observation provides no information about the state (hence, the POMDP reduces to an UMDP); when the parameter is 1, the observation provides full information about the state (hence, the POMDP reduces to an MDP).  The prior work also extends UMDPs to allow often costly actions that enable partial or full observations of the state \cite{fox2007reinforcement,kamar2009modeling,kamar2010reasoning,wang2025ocmdpob}.

% NOMDP is mentioned as a model used in classical method https://publications.polymtl.ca/5550/1/2020_Yves_Alain_Mbeutcha.pdf