\section{Discussion and conclusion}
\label{sec:conclude}

We have argued that advanced AI agents should be regulated based on their action sequences or more generally action graphs. More broadly, we highlight the insufficiency of regulations that focus on the computational resources used for training AI models. Given the trend of increasing emphasis on sophisticated reasoning at inference time, we contend that the capabilities and existential risks of AI agents will stem largely from their inference-time computation.

A key motivation for action sequences is the fundamental challenge of defining, representing, and observing the state of the open-ended environments in which AI agents operate. This limitation renders existing impact measures inadequate for managing existential risks. Since we cannot exhaustively identify all potential direct causes of global catastrophes or human extinction, it is impossible to determine a set of environmental features sufficient to predict whether a given action poses an existential risk in a specific state.

The simplest form of the regulation based on action sequences would impose limits on their length, prohibiting AI agents from autonomously operating beyond the limit where their safety against existential risks is known empirically.  This is neither a novel idea nor a sufficient solution; it should be seen as a baseline and a call to action for further research on this critical issue.  Action sequences can allow more elaborate regulations, but more research is needed to better understand what they can or cannot guarantee.  Further research is also needed to address myriad of questions, including what to be allowed as a single action, how to manage complex scenarios where multiple AI agents are involved, and how to ensure that the agents stop acting when their action sequence reaches the limit.

The suggested approach of empirically verifying the strong acceptability of action sequences would inevitably delay the expansion of the range in which AI agents can operate autonomously.  This is our intention.  Although AI agents might acquire general or even superhuman intelligence with all the common sense that humans have, they can still pose existential risks, similar to humans.  Humans, however, can often correct mistakes before they escalate, since our limited speed and scale allow us to identify and address errors---though there have been rare exceptions, such as those that led to world wars. In contrast, AI agents can operate at far greater speed and scale, preventing them from learning from mistakes in a controlled manner. Therefore, regulations should be seen not as obstacles to innovation and technological progress but as guidelines that accelerate research on making AI agents more controllable, verifiable, and governable, ensuring they truly benefit our society and the future.


%Some might argue that artificial general intelligence (AGI), with its incorporation of common sense, could eventually mitigate existential risks in a manner similar to humans. However, this assumption is overly optimistic. Humans can often correct mistakes because our limited speed and scale allow us to identify and address errors before they escalate---though there have been rare exceptions, such as those that led to world wars. In contrast, AI agents operate at far greater speed and scale, preventing them from learning from mistakes in a controlled manner. This heightened pace and scope significantly increase their potential for catastrophic outcomes.

%This has some similarity to how humans ensure safety in our society. We do not immediately take completely unknown (sequences of) actions. Instead, we begin by taking actions that are sufficiently similar to those known to be safe. If the action proves to be safe, we can then gradually take actions that are increasingly more complex or varied, but still within the bounds of what we've already determined to be safe. Even if the actions turn out to be unsafe, it typically does not lead to disaster, as it closely resembles some actions that are known to be safe.  In this way, humans typically learn from mistakes before committing to actions that could result in catastrophic consequences.  

% the frame problem \cite{mccarthy1969some}