\documentclass{article}

\usepackage[margin=45mm]{geometry}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\input{definition}

% modification to natbib citations
\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}

% Change citation commands to be more like old ICML styles
\newcommand{\yrcite}[1]{\citeyearpar{#1}}
\renewcommand{\cite}[1]{\citep{#1}}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}

\title{\textbf{Position: AI agents should be regulated based on autonomous action sequences}}
\author{
    Takauki Osogami\\
    IBM Research -- Tokyo\\
    {\tt osogami@jp.ibm.com}
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\input{abstract}
\end{abstract}

\keywords{AI agent, Long-term planning agent, Existential risk, Irreversible global catastrophe, Human extinction, Inference-time computation, Reasoning, Autonomous action graph, Impact measure, Safety, Regulation}

\input{intro}
\input{agent}
\input{alternative}
\input{method}
\input{conclusion}

\clearpage

\section*{Impact Statement}

This paper proposes a new framework on regulating advanced AI agents, who can pose existential risks.  The paper is thus intended to mitigate the negative impacts and implications that AI technologies might have.  However, the paper also discusses how those advanced AI agents might be developed.  While all the discussion is based on existing technologies that are already known in public, the unified treatment in this paper might motivate malicious readers to develop and deploy such advanced AI agents without sufficient considerations of their safety.

\bibliography{principal-agent}
\bibliographystyle{icml2025}

\newpage
\appendix
\input{method-agent}
\input{umdp}


\end{document}
