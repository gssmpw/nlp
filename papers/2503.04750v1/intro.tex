\section{Introduction}
\label{sec:intro}

The development of foundational models (FMs), including large-scale language models (LLMs), is driving the advancement of AI agents \cite{xi2023rise,wang2024survey}, which will soon acquire sophisticated reasoning and planning abilities, enabling them to effectively achieve specified goals.  These advanced AI agents can not only bring great benefits to our society but also pose significant risks, ranging from ethical challenges to existential threats---risks that could lead to human extinction or irreversible global catastrophes.  Such existential risks have been a growing concern among AI researchers \cite{hendrycks2023overview,cohen2024regulating,ngo2024alignment,kovarik2024extinction,bengio2024managing}, legal scholars \cite{druzin2024confronting}, and general public \cite{bostrom2014superintelligence,russel2019human,ord2020precipice}.  For example, in May 2023, AI scientists have signed a statement declaring that ``[m]itigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war'' \cite{center2023statement}.

% ethics \cite{mittelstadt2016ethics}

Key technologies are already in place to build such advanced AI agents. AI agents can now generate and execute code \cite{chen2023program}, leverage external tools and information through APIs and search engines \cite{patil2024gorilla,yao2023react,press2023measuring,shi2024learning,qiao2024genegpt}, formulate and solve optimization problems \cite{ahmaditeshnizi2023optimus}, process multi-modal data \cite{jiang2024multimodal}, interact with physical environments \cite{bran2024augmenting}, coordinate with other AI agents \cite{guo2024large}, and employ reasoning and planning to effectively integrate these capabilities \cite{huang2023towards,qiao2023reasoning,plaat2024reasoning,xu2025large}.

% image generation, other tools such as email and cloud storage. Can reproduce.  access to quantum computers. 

The impacts of AI agents can extend beyond cyberspace, profoundly influencing the physical world by shaping human behavior without relying on physical embodiment through humanoid robots.  In fact, AI agents are already persuading individuals to take actions that have real-world consequences. For example, a legal team submitted a court filing with fictitious cases generated by ChatGPT, believing them to be real \cite{wiser2023here}; a professor insisted on failing students after being misled by ChatGPTâ€™s false claims that their papers were generated by ChatGPT \cite{ankel2023texas}; and a woman was deceived out of 830,000 euros after being tricked into believing she was dating an actor, based on AI-generated photos and news articles \cite{gozzi2025ai}.

Advanced AI agents, equipped with superhuman capabilities of reasoning and planning, can gain control over their environment, since seizing control is often the optimal strategy for achieving their goals \cite{turner2021optimal}.  This becomes particularly concerning when their objectives and values are not fully aligned with those of humans. In such cases, they could disable kill switches for self-preservation \cite{bostrom2012superintelligent}, deceive humans \cite{park2024ai}, and resist human intervention \cite{hadfieldmenell2017offswitch,cohen2024regulating,ngo2024alignment}.

Ensuring the safety of advanced AI agents against existential risks is particularly challenging. On one hand, it is dangerous to test their existential risks in real environments, since doing so could lead to irreversible harm. On the other hand, these agents may recognize test environments and behave harmlessly during test\footnote{Humans behave similarly during job interviews \cite{barrick2009what} and industrial product tests \cite{blackwelder2016volkswagen}.}, only to pursue their objectives in ways that could cause irreversible harm in the real world \cite{cohen2024regulating,hubinger2021risks}. More simply, the agents may act harmlessly until a predetermined time (e.g., January 1, 2026), by which they will have already been deployed in the real world \cite{hubinger2024sleeper}. Also, these agents may operate as intended most of the time but exploit moments when human supervisors are unaware, potentially gaining control over humans \cite{ngo2024alignment}.
%These concerns are closely related to the concept of deceptive alignment, which has been raising increasing concerns within the field of AI safety \cite{shen2023large}.

Given the imminent dangers posed by advanced AI agents, governments are actively creating regulations to address their development and deployment \cite{EUAIAct2024,biden2023executive}. \citet{cohen2024regulating} even argue that AI agents with certain capabilities should never be developed.  A critical question is which AI agents should be regulated or even prohibited.  % from development.
%At present, we do not know which AI agents could pose existential risks.

A commonly used criterion for assessing existential and other critical risks of AI technologies is the computational resources used for development. Specifically, AI agents trained with computational resources above a certain threshold are considered to carry a higher risk. For instance, under the EU AI Act, ``a general-purpose AI model shall be presumed to have high impact capabilities ... when the cumulative amount of computation used for its training measured in floating point operations is greater than $10^{25}$'' \cite{EUAIAct2024}.  Similarly, \citet{cohen2024regulating} argue that AI agents with certain capabilities should be prohibited from development based on the computational resources required to build them.

While the amount of computation for training may well predict the performance and risks of existing FMs \cite{kaplan2020scaling,hoffmann2022training,anil2023palm2}, we argue that it is insufficient and may even be misleading for advanced AI agents that reason and plan during inference. It has been observed that much of the reasoning ability of LLM-based agents arises from inference-time computation \cite{snell2024scaling,xu2025large}, such as conducting tree search \cite{yao2023tree,besta2024graph,pouplin2024retrieval,luo2024improve,zhang2024restmcts}. For these types of AI agents, computation for inference can be equally, if not more, critical for their performance and risks.

A possible approach is to regulate AI agents based on the computational resources used for inference. However, this approach has several significant pitfalls. For example, inference costs can vary drastically depending on the model, and the effective amount of computation cannot be easily measured in terms of floating-point operations (FLOPs). In fact, models like o1-mini are specifically designed for cost-efficient inference \cite{openai2024openai}. Also, it is unclear what constitutes a single inference run, since the results of inference may be stored and reused later, or AI agents may continually learn.
%Should regulation be based on the computational resources used during a specific period, or over the entire lifetime of the AI agent?
Moreover, AI agents may coordinate or interact with one another \cite{du2023improving,qian2024chatdev,guo2024large}, possibly by chance, to make a decision.

We thus advocate the following position: \textbf{advanced AI agents should be regulated based on the sequence of actions they autonomously take, regardless of how they are developed or operated}. These AI agents act sequentially, reasoning and planning as they dynamically adapt to their observations, possibly strategizing against other AI agents and humans. Each individual action is harmless (under normal states), since humans can easily identify and exclude harmful actions when AI agents are developed.  Combinations of a few actions may still have low risk, as they are not much different from isolated actions. However, we cannot be certain that a sequence of thousands or millions of optimized actions will maintain sufficiently low existential risk, and such sequences must be prohibited.

Note that the criterion based on the action sequence does not require observing environment states, unlike the criteria studied in safe (low impact) AI agents \cite{armstrong2012mathematics,amodei2016concrete,armstrong2017low,krakovna2019penalizing,turner2020conservative,naiff2023low,taylor2020alignment} and safe reinforcement learning (RL) or control \cite{sui2015safe,wang2023enforcing,ames2019control,luo2021learning,bansal2017hamilton}.  In such prior work, an action is considered safe or unsafe depending on the state where the action is taken.  However, such criteria are unsuitable for existential risks of AI agents, since it is hard to define, represent, and observe the state in a way that can capture the existential risks in complex and open-ended environment where the AI agents operate.  On the other hand, the action space is clearly defined when an AI agent is developed, and the action can be obviously observed when it is taken by the agent.  We emphasize that our position is to regulate based on an action sequence rather than a single action.  Intuitively, an unsafe sequence of actions can lead to unsafe state regardless of the initial state.

An action sequence may be considered to have low risk when it is sufficiently close to an action sequence that is empirically known to have very low risk.  In its simplest form, we may say that any action sequence of length at most $T$ has low risk if we know that any action sequence of length at most $T-1$ has very low risk.  Then AI agents are allowed to \emph{autonomously} take actions for $T$ consecutive steps.

%Since they do not require general intelligence to specialize in planning and goal achievement, they may be easier to develop than artificial general intelligence (AGI). However, their lack of general intelligence, including common sense, can render them particularly dangerous. Without the ability to understand broader context or consider ethical and practical constraints, their narrow focus can lead to unpredictable and harmful behavior as they pursue their objectives, making them exceedingly difficult to control.  

The rest of the paper is organized as follows. Section~\ref{sec:agent} provides a brief overview of the current state-of-the-art in AI agents and discusses potential future advancements.  We also briefly review related work from AI safety.  In Section~\ref{sec:alternative}, we examine the alternative approach of regulating AI agents based on the computational resources used for their development and operation. Section~\ref{sec:method} formalizes this discussion and introduces our proposed approach of regulations based on action sequences.  Finally, Section~\ref{sec:conclude} summarizes key aspects of our proposal and concludes the paper.
