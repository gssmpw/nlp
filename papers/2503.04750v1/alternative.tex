\section{Alternative views}
\label{sec:alternative}

One broadly applicable metric for regulating AI agents is the amount of computation they require. Here, we explore how existing regulations and recommendations from AI researchers often center on computational resources. While these efforts primarily address the computational resources for training, we expand the discussion to include computational demands for inference. We then argue that limiting the focus to these aspects alone is inadequate for the existential risks associated with advanced AI agents.

\subsection{Computational resources for development}

In this section, we briefly review existing regulations, focusing on the EU AI Act and President Biden's executive order, along with key recommendations from AI researchers.

\subsubsection{EU AI Act}
%https://yourlearning.ibm.com/activity/UDEMY-5863828
% https://www.noandt.com/wp-content/uploads/2021/04/technology_no6.pdf

\citet{EUAIAct2024} has established a set of rules (the EU AI Act) for the development, deployment, and use of AI within European Union.  Its Chapter 5 defines ``general-purpose AI models with systemic risk'' and lists obligations for providers of such models.  Here, general-purpose AI models essentially refer to FMs, which are pre-trained with self-supervised learning and can (be adapted to) perform a wide range of downstream tasks (see Article 3(63)).  Also, systemic risk refers to ``a risk that is specific to the high-impact capabilities of general-purpose AI models, having a significant impact ... on public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scale across the value chain'' (see Article 3(65)).

In particular, a ``general-purpose AI model shall be presumed to have high impact capabilities ... when the cumulative amount of computation used for its training measured in [FLOPs] is greater than $10^{25}$'' (see Article 51(2)).  When this or other specified conditions are met, the provider of a general-purpose AI model is required to fulfill certain obligations, such as providing technical documentation about the model.

The advanced AI agent that we consider can certainly be classified as general-purpose AI.  Also, the existential risks that we consider can be considered as systemic risks, although human extinction and irreversible global catastrophes are not discussed in the EU AI Act.

\subsubsection{Biden's executive order and related bills}

In October 2023, Joe Biden, then president of the US, signed the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence \cite{biden2023executive}\footnote{This executive order was repealed by President Trump.}.  Its Section 4.2 is dedicated to ensuring safe and reliable AI.  In particular, it requires companies to report on ``any model that was trained using a quantity of computing power greater than $10^{26}$ integer or [FLOPs]'' until a set of technical conditions for models are defined by specified authorities (see Section 4.2(b)).

In this executive order, particular attention is paid to a dual-use FM, which refers to a FM that exhibits ``high levels of performance at tasks that pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters, such as by ... permitting the evasion of human control or oversight through means of deception or obfuscation'' (see Section 3(k)).

% https://web.archive.org/web/20250118020619/https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/

Following this executive order, almost 700 AI-related bills are introduced in 45 states across the United States in 2024 \cite{bsa2024state}.  A particularly interesting one is California Senate Bill 1047 (Safe and Secure Innovation for Frontier Artificial Intelligence Models Act) \cite{wiener2024senate}\footnote{The bill had passed the state legislature but was later vetoed by the Governor.  The technical feasibility of the requirements has also been questioned by the community \cite{ai2024statement}.}.  Its Chapter 22.6 is devoted to safe and secure innovation for frontier artificial intelligence models, which cover ``[a]n artificial intelligence model trained using a quantity of computing power greater than $10^{26}$ integer or floating-point operations.''  In particular, the senate bill requires that ``[b]efore beginning to initially train a covered model, the developer shall ... [i]mplement the capability to promptly enact a full shutdown,'' which completely halts the operations of the model.

% https://leginfo.legislature.ca.gov/faces/billStatusClient.xhtml?bill_id=202320240SB1047

The necessity of such an off-switch \cite{hadfieldmenell2017offswitch} is motivated to prevent ``critical harms,'' which include ``[m]ass casualties or at least five hundred million dollars (\$500,000,000) of damage resulting from an artificial intelligence model engaging in conduct that ... [a]cts with limited human oversight, intervention, or
supervision.''

% Key 2024 Statistics: State lawmakers across the United States introduced almost 700 AI-related bills in 2024 across 45 states % https://www.bsa.org/news-events/news/2025-state-ai-wave-building-after-700-bills-in-2024

\subsubsection{Recommendations by scientists}

We have seen that the computational resources used for training is one of the major criteria to judge whether an AI model can have unacceptably high risk.  Although specific values of the threshold such as $10^{25}$ or $10^{26}$ FLOPs are subject to change, the amount of computation for training is one of the most reliable metrics that AI researchers can currently provide for approximating the performance and potential risks of a wide range of FMs.

For example, \citet{anderljung2023frontier} recommend to identify sufficiently dangerous frontier AI models on the basis of whether they are trained with more than $10^{26}$ FLOPs of computation.  More recently, \citet{cohen2024regulating} argue that ``[s]ystems should be considered `dangerously capable' if they are trained with enough resources to potentially exhibit those dangerous capabilities, and regulators should not permit the development of dangerously capable LTPAs.''  Although they do not specify exactly what is considered as enough resources, the amount of computation for training is the only specific criterion that they suggest to determine whether an LTPA can have existential risk. 

\citet{future2023policymaking} have also provided recommendations for governments on managing AI risks.  The recommendations include mechanisms such as auditing, certification, and regulation, grounded in the assumption that ``[t]he amount of compute used to train a general-purpose system largely correlates with ... the magnitude of its risks'' \cite{future2023policymaking}.  These recommendations have been made by following an open letter that called for ``all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4,'' which was issued in response to the severe societal risks posed by advanced AI systems and signed by AI researchers \cite{future2023pause}.

In fact, the amount of computation used for training may serve as a sufficiently reliable predictor of the performance and risks of today’s LLMs. This is because most existing LLMs share the same Transformer architecture, differing primarily in their size and the volume of training data. Several studies have examined scaling laws that describe the relationship between a model's optimal size, the amount of training data, and the computational budget required \cite{kaplan2020scaling,hoffmann2022training}.  Research has also shown that various abilities, such as multi-step reasoning, tend to emerge as the computational resources used for training increase \cite{wei2022emergent}.

These scaling laws can, in turn, be used to estimate the FLOPs needed to train existing LLMs.  For example, \citet{anil2023palm2} propose a heuristic suggesting that an LLM should be trained with $6\,N\,D$ FLOPs, where $D$ is the amount of training data, and $N$ is the model size.  Using this heuristics, Llama 3.1 405B trained on 15 trillion tokens is estimated to require approximately $4 \times 10^{25}$ FLOPs---an amount closely aligning with the thresholds specified in the EU AI Act and President Biden's executive order.


\subsection{Computational resources for operation}
\label{sec:alternative:operation}

We have seen that the amount of computation used for training is one of the key criteria that is used today to regulate highly capable FMs.  Although such regulations may be effective for AI agents whose capabilities primarily and directly stem from traditional FMs, they obviously fail to regulate advanced AI agents that gain substantial reasoning capabilities from computation at inference time.  

A natural approach is to regulate advanced AI agents based on the amount of computation used not only for training but also for inference. While this approach may be effective for some AI agents, we argue that it is insufficient for advanced AI agents, at least for the following four reasons.

First, scaling laws for inference-time computation are much less established than those for training-time computation. Recently, scaling laws have been studied for a limited number of inference strategies in LLMs \cite{chen2024simple,brown2024large,snell2024scaling,wu2024inference}. However, there are many potential inference strategies, and their performance can scale very differently. The scaling law for inference also depends on how difficult the task is and what LLM is used \cite{snell2024scaling,openai2024openai}. 
%As a result, we cannot simply assume that inference is safe simply because it does not require heavy computation.

%\cite{chen2024simple}: inference scaling law for a particular inference strategy (sample many, and select one with tournament)
%\cite{brown2024large}: inference scaling law for a particular inference strategy (sample many, and select one)
%\cite{snell2024scaling,wu2024inference}: inference scaling laws for several selected inference strategies

Second, inference can be performed in parallel by multiple entities. For example, several entities may operate the same AI agent that perform reasoning with MCTS \cite{luo2024improve,zhang2024restmcts}, either collaboratively or independently, possibly without knowing each other. Since MCTS is a randomized algorithm, the likelihood that one of the agents optimally solves the task increases with the number of agents. However, this also means that this agent may exploit a loophole, solving the task super-optimally in a way that violates critical constraints, potentially leading to catastrophic outcomes. 
%Therefore, even if each entity’s computation is negligible, a group of entities could collectively exceed a threshold that ensures the safety of inference.

Third, it is not always clear what constitutes a single run of inference. For instance, the results of one inference run may be stored and used in another. Intermediate results could also be stored and later retrieved by a different AI agent, who may or may not be aware that the information is from a previous inference run. More broadly, reasoning can be enhanced through retrieval augmentation \cite{pouplin2024retrieval}, where retrieved information may have been generated with substantial computational resources.
%Therefore, regulating the computational resources spent on a single, arbitrarily defined inference run may not be sufficient.

Finally, AI agents may continually learn over time, which makes it difficult to separate inference from training. For example, an AI agent may perform reasoning with MCTS, with an LLM performing a step in the process. Once the agent identifies a good sequence of steps, it may fine-tune the LLM in a way that the LLM can perform the entire sequence in a single step, bypassing the reasoning process.  As this learning progresses, the agent will gain the ability to perform high-level reasoning with limited computation.
%However, since training and inference can be performed by different entities, tracking the total computational resources used by an AI agent becomes challenging.

% MCTS \cite{kocsis2006bandit,browne2012survey} at run time.  counterfactual regret minimization \cite{zinkevich2007regret}.

% 民間企業においては、AI の開発に際し、政府機関に安全性テストの報告等が必要になる。ただし、EU の AI 規制（案）のように一部の AI の開発・利用を禁止するものではなく、AI 大統領令は政府機関が策定する基準やベストプラクティスに基づいた対応を求めるにとどまる。
% 。いずれも連邦当局に対して基準やベストプラクティスの策定、一定の対象者に報告義務を課す等の緩やかな規制であり、EU の AI 規則（案）7のように非常にリスクの高い AI を禁止する等の AI の利用そのものを制限するものではない。
% https://www.dir.co.jp/report/research/law-research/law-others/20231130_024115.pdf

% いずれも連邦当局に対して基準やベストプラクティスの策定、一定の対象者に報告義務を課す等の緩やかな規制であり、EU の AI 規則（案）7のように非常にリスクの高い AI を禁止する等の AI の利用そのものを制限するものではない。いずれも連邦当局に対して基準やベストプラクティスの策定、一定の対象者に報告義務を課す等の緩やかな規制であり、EU の AI 規則（案）7のように非常にリスクの高い AI を禁止する等の AI の利用そのものを制限するものではない。
% dual use FMs：悪用されると安全保障、国家経済安全保障、国家公衆衛生・安全に対する深刻なリスクをもたらしうるAIモデル dual use FMsの開発者に対し、AI red-teamingテストの結果やトレーニングに関する情報の報告義務を課す 4.2項(a)(i)
% https://www.noandt.com/wp-content/uploads/2023/11/technology_no43_1.pdf
